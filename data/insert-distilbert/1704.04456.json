{"id": "1704.04456", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Liquid Splash Modeling with Neural Networks", "abstract": "this paper proposes a new data - driven classification approach for modeling detailed splashes for liquid simulations supplemented with neural networks. our model learns to generate small - scale virtual splash detail for standard fluid - implicit - wave particle methods using training data acquired from physically accurate, high - resolution simulations. we use neural networks to model the regression of splash point formation using a classifier together with a velocity modification term. more specifically, we employ a heteroscedastic model for the velocity updates. our simulation results demonstrate that our gradient model significantly improves visual fidelity with a large amount of realistic momentum droplet formation and yields splash detail much more efficiently than finer discretizations. we accordingly show this for two different spatial scales and simulation analysis setups.", "histories": [["v1", "Fri, 14 Apr 2017 15:28:37 GMT  (3192kb,D)", "http://arxiv.org/abs/1704.04456v1", null]], "reviews": [], "SUBJECTS": "cs.GR cs.LG", "authors": ["kiwon um", "xiangyu hu", "nils thuerey"], "accepted": false, "id": "1704.04456"}, "pdf": {"name": "1704.04456.pdf", "metadata": {"source": "META", "title": "Liquid Splash Modeling with Neural Networks", "authors": ["KIWON UM", "XIANGYU HU", "NILS THUEREY"], "emails": [], "sections": [{"heading": null, "text": "Liquid Splash Modeling with Neural Networks\nKIWON UM, Technical University of Munich XIANGYU HU, Technical University of Munich NILS THUEREY, Technical University of Munich\n(a) breaking dam (b) sloshing wave\nFig. 1. Our data-driven splash model improves the visual fidelity of the basic FLIP simulation in two examples: a breaking dam and sloshing wave simulations. Each example shows the visual comparisons between FLIP (le ) and a simulation employing our model (right).\nThis paper proposes a new data-driven approach for modeling detailed splashes for liquid simulations with neural networks. Our model learns to generate small-scale splash detail for uid-implicit-particle methods using training data acquired from physically accurate, high-resolution simulations. We use neural networks to model the regression of splash formation using a classi er together with a velocity modi cation term. More speci cally, we employ a heteroscedastic model for the velocity updates. Our simulation results demonstrate that our model signi cantly improves visual delity with a large amount of realistic droplet formation and yields splash detail much more e ciently than ner discretizations. We show this for two di erent spatial scales and simulation setups.\nAdditional Key Words and Phrases: machine learning, neural network, liquid simulation, uid implicit particle, smoothed particle hydrodynamics"}, {"heading": "1 INTRODUCTION", "text": "For large-scale liquid simulations, it is crucial for visual delity that a numerical simulation can produce su cient amounts of very small droplets. However, it is di cult to capture such splashes in practical simulations due to their small-scale structure. Simulations of detailed structures typically require the use of very ne spatial discretizations, which in turn lead to high computational cost. Thus, it is often challenging to generate vivid splashes in liquid simulations because these require resolving the small-scale dispersive motions that lead to droplets forming and being ejected from the bulk volume.\nThis paper proposes a new data-driven splash model that improves the visual delity of hybrid particle-grid liquid simulations. By learning the formation of small-scale splashes from physically accurate simulations, our model e ectively approximates the subgrid scale e ects that lead to droplets being generated. This enables us to generate realistic splashes in coarse simulations without the need for manually tweaking parameters or increased computational costs induced by high-resolution simulations. We realize our model using machine learning techniques with neural networks (NNs) and integrate the model into the uid-implicit-particle (FLIP) algorithm.\nThis work is supported by the ERC Starting Grant 637014.\nFigure 1 shows the outcome of our model, which we will denote with MLFLIP in the following."}, {"heading": "2 RELATED WORK", "text": "The behavior of liquids is typically modeled as Navier-Stokes equations:\n\u2202u \u2202t + u \u00b7 \u2207u = g \u2212 1 \u03c1 \u2207P + \u03bd\u22072u and \u2207 \u00b7 u = 0, (1)\nwhere u is the velocity, g is the gravity, \u03c1 is the density, P is the pressure, and \u03bd is the viscosity coe cient. There exist many numerical methods for solving these equations. Those solvers are commonly categorized as Eulerian and Lagrangian approaches (Bridson 2015; Ihmsen et al. 2014).\nFLIP is a particularly popular method for liquid simulations (Zhu and Bridson 2005), and it is the most widely used one in movie visual e ects at the moment. Its e ective combination of Lagrangian and Eulerian properties enables the e cient solve of liquid motions. While FLIP has become a practical solution for liquid simulations, the core method has been extended in various ways to improve its simulation quality and e ciency. For example, di erent position correction methods were introduced to improve the distribution of particles (Ando et al. 2012; Um et al. 2014). In addition, Ferstl et al. (2016) proposed a narrow band method that improves the e ciency by sampling the volume with particles only near the surface.\nThe goal of our model is to improve the visual delity of liquid simulations with splash detail. Apart from FLIP, the smoothed particle hydrodynamics (SPH) approach is a popular alternative in graphics (M\u00fcller et al. 2003; Solenthaler and Pajarola 2009). Ihmsen et al. (2012) proposed a exible model for secondary particle e ects for SPH simulations. With enough manual tuning, such a secondary particle method can yield realistic results, but in contrast to their work, we focus on an automated approach that captures splash e ects for physically-parametrized real world scales. Our model does not require any parameter tuning on the user side. At the same time, one of our goals is to demonstrate that neural networks are a suitable tool to detect and generate these splashes.\nar X\niv :1\n70 4.\n04 45\n6v 1\n[ cs\n.G R\n] 1\n4 A\npr 2\n01 7\nA method that shares our goal to enable splashes with FLIP is the unilateral incompressibility solver (Gerszewski and Bargteil 2013). While it also aims at letting FLIP particles disperse more easily, our approach targets a very di erent direction. Instead of modifying the pressure solve, we incorporate a statistical model with the help of machine learning. Given a trained model, our approach is easily integrated into existing solvers.\nMachine learning: As we employ a machine learning technique for our splash model, we will also brie y review previous work on machine learning. In general, the learning process aims for the approximation of a general function f using a given data set (i.e., input x and output y) in the form of y = f (x,w)wherew is the set of weights and biases to be trained. Using NNs, the general function f is modeled by networks of multiple layers where each layer contains multiple nodes (i.e., arti cial neurons). These networks consist of layers with connected nodes. The output vector yL from a layer L is typically computed with yL = \u03a6(wLyL\u22121 + bL) where \u03a6(\u00b7) is the activation function that is applied to each component, wL is the weight matrix of the layer, and bL is the bias vector of the layer. Here, the activation function \u03a6 makes it possible to capture non-linearities in the approximated function. We will demonstrate that the NNs technique, which so far have rarely been used for uid simulations, can be used for realization of our data-driven splash model.\nSuch NNs were previously used to compute local pressure approximations (Yang et al. 2016) while others employed networks with convolutional layers to regress the whole pressure eld (Tompson et al. 2016). Moreover, an approach using regression forests, which represent an alternative to neural networks, was proposed to e ciently compute forces in SPH (Ladick\u00fd et al. 2015). More recently, NNs were also successfully employed for patch-based smoke simulations (Chu and Thuerey 2017) and fast generation of liquid animations with space-time deformations (Bonev et al. 2017). In the engineering community, approximating e ects smaller than the discretization resolution is known as coarse graining (Hij\u00f3n et al. 2009), but this idea has not been used to model splash formation. We propose to use machine learning techniques to represent accurate and high-resolution data in order to approximate complex small-scale e ects with high e ciency. To the best of our knowledge, splash modeling using machine learning techniques has not been studied before."}, {"heading": "3 DATA-DRIVEN SPLASH MODELING", "text": "The following section details our data-driven approach for generating splashes. The principal idea of our approach is to infer statistics about splash formation based on data from simulations that are parametrized to capture the droplet formation in nature. Our de - nition of a splash is a small disconnected region of liquid material that is not globally coupled with the main liquid volume. Thus, we treat individual splashing droplets as particles that only experience gravitational acceleration but no other NS forces. This modeling is in line with the secondary particle e ects often employed in movies (Losure and Baer 2012). The key novelty of our approach is that it does not require manually chosen parameters, such as velocity or curvature thresholds, to generate the splashes. Rather, it uses a\nstatistical model and data extracted from a series of highly detailed and pre-computed simulations.\nOur approach consists of two components: a detachment classi - cation and a velocity modi cation step. Based on a feature descriptor consisting of localized ow information, the classi er predicts whether a certain volume of liquid forms a detached droplet within a chosen duration \u2206t (typically, on the order of a single frame of animation). For droplets that are classi ed as such, our modi er predicts its future velocity based on a probability distribution for velocity modi cations. We use NNs to represent both components, and the following sections describe our statistical model and the corresponding machine learning approach."}, {"heading": "3.1 Neural Network Model", "text": "The input to our model is a feature descriptor x \u2208 RM that encapsulates enough information of the ow such that a machine learning model can infer whether the region in question will turn into a splash. For this binary decision \u201dsplash or no-splash\u201d, we will use an indicator value l \u2208 {1, 0} in the following. Each descriptor is typically generated for a position p. The M individual components of x consist of ow motion and geometry in the vicinity of p. In practice, we use 33 samples of the velocity and level set. The discussion of this choice will be given in Section 3.3 in more detail.\nWe train our models with a given data set that consists of feature vectors X = {x1, x2, \u00b7 \u00b7 \u00b7 , xN } and corresponding detachment indicator values L = {l1, l2, \u00b7 \u00b7 \u00b7 , lN }; they are generated during a pre-processing phase at locations {p1, p2, \u00b7 \u00b7 \u00b7 , pN }. Then, our classi er aims for inferring the probability Ps that a feature vector xi is in the class indicated by li . Considering a probability distribution function ys that Ps follows, we will approximate the function ys from the given data. For this task, we can follow established procedures from the machine learning eld (Bishop 2006).\nThe probability distribution ys (xi ,ws ) is the target function that is represented by the weights ws . The weights are the actual degrees of freedom that will be adjusted in accordance to the data during the learning phase. We can express Ps in terms of ys as:\nPs (li |xi ) \u223c P (li |ys (xi ,ws )) , (2)\nwhich yields the following likelihood function that we wish to maximize:\nLd (L|X) = N\u220f i=1 P (li |ys (xi ,ws )) . (3)\nIn order to maximize this likelihood, we use the well-established softmax (i.e., normalized exponential function) for the loss of our classi cation networks. A successfully trained model will encode ys ; thus, we can evaluate with new feature vectors at any position in a ow to predict whether the region will turn into a detached droplet within the time frame \u2206t .\nLet\u2206v be an instance of a velocity change for a splash with respect to the motion of the bulk liquid in its vicinity. We will afterward consider this velocity change of a droplet relative to the bulk as the velocity modi cation of a particle in our simulation. Similar to the classi er above, our goal is to infer the set of velocity modi cations \u2206V = {\u2206v1,\u2206v2, \u00b7 \u00b7 \u00b7 ,\u2206vN } based on the corresponding set of feature vectors X. From the statistics of our training data, we found\nthat it is reasonable to assume that the velocity modi cations follow a normal distribution relative to the mean ow direction. Accordingly, we model the modi er as a modi cation function fm (\u2206vi |xi ) following a normal distribution with mean \u00b5 and variance \u03c32:\nfm (\u2206vi |xi ) \u223c N ( \u2206vi |\u00b5(xi ,w\u00b5 ),\u03c32(xi ,w\u03c3 2 ) ) , (4)\nthus,\nfm (\u2206vi |xi ) \u223c 1\u221a 2\u03c0\u03c32i exp\n( \u2212(\u2206vi \u2212 \u00b5i )2\n2\u03c32i\n) , (5)\nwhere, for the sake of simplicity, \u00b5i and \u03c32i denote \u00b5(xi ,w\u00b5 ) and \u03c32(xi ,w\u03c3 2 ), respectively. Then, the negative log likelihood function Lm (also known as loss function) for the given data is de ned as follows:\nLm (\u2206V|X) = 1 2 N\u2211 i=1 d\u2211 j=1\n[ (\u2206vi, j \u2212 \u00b5i, j )2\n\u03c32i, j + ln\u03c32i, j\n] , (6)\nwhere j denotes the spatial index. As Equation (6) indicates, we model the modi er as a mean variance estimation (MVE) problem (Khosravi et al. 2011; Nix and Weigend 1994). Instead of directly estimating the mean of targets, the MVE problem assumes that errors are normally distributed around the mean and estimates both the mean and heteroscedastic variance. Note that \u00b5(xi ,w\u00b5 ) and \u03c32(xi ,w\u03c3 2 ) are the target functions that are approximated with each set of weights w\u00b5 and w\u03c3 2 .\nIn our approach, the mean and variance functions are represented by NNs and approximated by estimating the two sets of weights such that they minimize the loss function Lm for the given data {X,\u2206V}. We want to point out that several machine learning algorithms are available to solve this problem (Bishop 2006), but NNs have proven themselves to be robust in this problem, and we found that they work su ciently well in our tests.\nThe overall structure of our NNs is illustrated in Figure 2. The NNs learn for two separate components: classi er and modi er. Sharing the input vector x, both components are represented as separate two-layer NNs. The size of output from the rst layer is double the input vector\u2019s, and the output is fully connected to the nal output. All outputs from each layer are activated using the hyperbolic tangent function. Note that there is a large variety of\ndi erent network layouts that could tried here, but we found that this simple structure worked su ciently well in our tests.\nWe realized our NNs approach with the TensorFlow framework (2016) using its ADAM optimizer (Kingma and Ba 2014) with a learning rate of 10\u22124. We also employ weight decay and dropout (both with strength 10\u22121) to avoid over- tting. Additionally, we found that the batch normalization technique (Io e and Szegedy 2015) signi cantly improves the learning rate and accuracy."}, {"heading": "3.2 FLIP Integration", "text": "Our NN-based splash generation model easily integrates into an existing FLIP simulation pipeline; we will denote the new FLIP method integrated with our splash model as MLFLIP. After the pressure projection step, we run classi cation on all particles in a narrow band of one cell around the surface. For those that generate a positive outcome, we evaluate our velocity modi cation network to compute component-wise mean and variance. Then, we generate random numbers from correspondingly parameterized normal distributions to update the velocity of the new splash particle. All splashes are treated as ballistic particles and do not participate in the FLIP simulation until they impact the main volume again.\nLook-ahead correction: While our splash generation algorithm reliably works in our tests, we found that a small percentage of particle, about 0.05-0.5%, can be misclassi ed. This can happen, for instance, when the side of an obstacle is just outside the region of our input feature vector. To minimize the in uence of such misclassi cations, we implemented a look-ahead step that reverts the classi cation of individual splashes if necessary. For this look-ahead check, we advance all bulk volume particles, i.e., those that were not classi ed as splashes, by \u2206t using the current grid velocities. We separately integrate positions of the new splash particles for a time interval of \u2206t with their updated velocities. If a new splash particle ends up inside of the bulk liquid or an obstacle, we revert its classi cation and modi cation."}, {"heading": "3.3 Training Data", "text": "Our NNs require a large set of input feature vectors with target outputs for training. In our model, the latter consists of the classi - cation result l indicating whether a ow region forms a splash and the velocity modi cation \u2206v predicting the trajectory of a splash. We generate the training data from a series of simulations with randomized initial conditions designed to incite droplet formation. The randomized initial conditions are the number of droplets and their initial positions and velocities. We choose the ranges of each condition such that they yield su cient variability for data generation. The snapshots of the randomized example simulations are shown in Figure 3. Note that at this stage any available \u201caccurate\u201d NS solver could be employed. However, we demonstrate that FLIP can bootstrap itself by using high-resolution simulations with correctly parameterized surface tension.\nFor the training data simulations, it is crucial that they resolve important sub-grid e ects that are probably not resolved on coarse resolutions to which our model will be applied later. We test our model with two small-scale physics where the surface tension is dominant thus generates many droplets. Our training simulations\nare performed using FLIP with the ghost uid method (Hong and Kim 2005) for surface tension e ects. The two scales use a grid spacing of 1cm and 0.3cm, and they are parametrized with the surface tension of water, i.e., 0.073 N/m. Both scales use a simulation grid of 1003. Several example frames for both scales are shown in Figure 4. Our networks successfully extract the dynamics of droplet formation based on coarse data and thus e ectively model small-scale dynamics that would be costly to resolve with a regular simulation.\nNote that the simulation data for training will be used to encode the desired sub-grid e ects for much larger scales afterwards. When applying our model in new simulation setups, we typically have scales that are much coarser than those used for generating the training data. Thus, the feature descriptor (i.e., xi ) needs to be dened at this coarse simulation scale, and our networks need to infer their outputs (i.e., li and \u2206vi ) based only on this coarse input. For this purpose, we make use of a coarse grid at data generation time. This coarse grid represents the scale to which the model will be applied afterward. For every time step, we down-sample the necessary high-resolution elds from the data generation simulation to this lower resolution and extract the feature descriptors for training.\nAs indicated in Section 3.1, we de ne a feature descriptor xi using 33 samples of the velocity and level set values interpolated with a sampling spacing of h; i.e., the feature vector consists of 108 components containing 33 \u00d7 3 velocity values and 33 \u00d7 1 level set values. Because the splash formation mostly relies on the local ow physics\nnear the liquid surface, we focus on the localized ow information and the surface region where the splash is likely initiated. From pilot experiments, we found that the improvement is negligible when more samples or more features such as obstacle information are used for the feature vector.\nIn order to extract the splash indicator value l , we analyze the particle\u2019s spatial motion for a chosen duration \u2206t (i.e., a single frame of animation in our experiments). Using an auxiliary grid, the separate volumes are recognized by computing the isolated liquid regions from the level set eld or cell ags. We then identify the splashing particles (i.e., l=1) as those ending up in a new disconnected component that falls below a given volume threshold at time t + \u2206t . In our case, if a disconnected component consists of less than 8 cells, the volume is marked as droplet volume. All particles in such a droplet are marked as splashing if the droplet did not exist as a disconnected component at time t .\nThe velocity modi er of our model predicts the trajectory for a splash. We evaluate this prediction after updating the velocity of particle from the divergence-free velocity. Thus, the new velocity vt+\u2206tm for a splash particle is de ned as vt+\u2206tm = vt+\u2206t + \u2206v, and we compute the velocity modi cation \u2206v for training with:\n\u2206v = pt+\u2206t \u2212 pt\n\u2206t \u2212 vt+\u2206t . (7)\nThe splashes are initiated near the liquid surface in general; thus, we extract the training data only from the surface particles. The surface particles are recognized by slightly expanding the area of empty (or air) cells. Note that we use the data from splashing as well as non-splashing particles as training data. For the latter, we set \u2206v = 0. It is crucial for training that the networks see su ciently large amounts of samples from both classes.\nFor training, we used 500K samples of the 1cm scale and 102K samples of the 0.3cm scale. They were extracted from ten training simulations per scale. The data contain the same number of both splashing and non-splashing samples. We randomly split them into 75% for the training data set and 25% for the test data set. The graphs in Figure 5 illustrate the progress of the learning phase in\nour experiments. The training performed for 40K iterations with 5K samples as a training batch. When the data were fully used, we randomly shu ed the samples and then continued with the training."}, {"heading": "4 RESULTS", "text": "This section demonstrates that our data-driven splash model (i.e., MLFLIP) improves the visual delity of liquid simulations for two examples: a breaking dam and sloshing wave tank setups. The former should represent ca. three meters of real world size, while the sloshing wave tank is ca. one meter in length. Hence, we use the 1cm splash model for the breaking dam and 0.3cm splash model for the sloshing wave. Figure 6 shows the comparisons between FLIP and MLFLIP for the breaking dam example. Our model leads to a signi cant increase in violent and detailed splashes for this large scale ow. Despite the large number of splashes, the plausibility of the overall ow is preserved. Likewise, our model robustly introduces splashes also in the smaller sloshing wave example as shown in Figure 7. The smaller real world size in conjunction with the smaller velocities leads to fewer splashes being generated for this setup.\nOur model requires additional calculations for generation of the splashes, and consequently, this results in an additional increased runtime. In the breaking dam example, the average computation time per simulation step is 0.5s for FLIP, while it is 1.6s for MLFLIP. Both simulations use the same grid resolution of 160\u00d7150\u00d750. In the sloshing wave example, it is 0.2s for FLIP and 0.5s for MLFLIP. In this case, the grid resolution is 150\u00d784\u00d710. The breaking dam setup requires ca. 5 simulation steps per frame of animation, while the sloshing wave requires ca. 8 on average.\nThe most time consuming part of MLFLIP is the evaluation step for the classi er and modi er. In the breaking dam example, its computation time is 1.0s on average out of the 1.6s per simulation step. The evaluation step leaves signi cant room for optimizations as we simply employ the CPU evaluation of the NN library here. As GPUs are very well suited for evaluating NNs, we expect that our MLFLIP would not impose a signi cant overhead compared to a regular FLIP simulation when evaluating the model on GPUs. The cost for computing feature vectors is negligible with 0.03s per simulation step because it only a ects a narrow band near the surface.\nWe observed that the splashes of our MLFLIP simulation are very di cult to achieve with regular FLIP simulations even with high resolutions. For the breaking dam example, Figure 8 shows a visual comparison of three simulations: FLIP and MLFLIP with the same resolution and FLIP with a doubled resolution of 320\u00d7300\u00d7100. Despite taking 6.5 times longer per frame of animation (i.e., 54s for\nFLIP and 8.3s for MLFLIP), this high resolution simulation fails to resolve the splashing e ects of our MLFLIP simulation. Thus, our model successfully generates splash details from a low resolution simulation, while the high resolution simulation barely improves the amount of detail despite its high computational cost."}, {"heading": "5 CONCLUSIONS AND FUTURE WORK", "text": "This paper introduces a new data-driven splash model that is realized using machine learning techniques with NNs. We demonstrate that our NNs and training process successfully learn the formation of splashes at a certain physical scale from our training data. Our model leads to improved splashing liquids and thus successfully extracts the relevant mechanisms for droplet formation from the pre-computed data.\nSo far, we only experimented with two di erent spatial scales. The training data were generated independently for both scales, and the neural networks were trained separately. However, we envision that our model could be enlarged and trained with data from a variety of spatial scales leading to a more generic model that could be applicable to a broad range of targeted real world scales. Additionally, other complex e ects such as bubbles, capillary waves, and foam could be incorporated into our model in the future."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper proposes a new data-driven approach for modeling detailed splashes for liquid simulations with neural networks. Our model learns to generate small-scale splash detail for uid-implicit-particle methods using training data acquired from physically accurate, high-resolution simulations. We use neural networks to model the regression of splash formation using a classi er together with a velocity modi cation term. More speci cally, we employ a heteroscedastic model for the velocity updates. Our simulation results demonstrate that our model signi cantly improves visual delity with a large amount of realistic droplet formation and yields splash detail much more e ciently than ner discretizations. We show this for two di erent spatial scales and simulation setups.", "creator": "LaTeX with hyperref package"}}}