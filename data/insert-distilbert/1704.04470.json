{"id": "1704.04470", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Lean From Thy Neighbor: Stochastic & Adversarial Bandits in a Network", "abstract": "an individual's decisions are often guided by those of his or her peers, i. e., neighbors in a social network. presumably, being privy councillors to the experiences of others aids in learning and decision making, but how much advantage does an individual gain by observing her neighbors? such problems make appearances in sociology and economics and, thus in this paper, we present a novel model to capture such decision - making processes and appeal to the classical naive multi - armed bandit framework to analyze it. each individual, in addition to her own actions, can observe the actions and rewards obtained by her neighbors, and can use all of this information in order to minimize her own regret. we provide algorithms for compiling this setting, both for stochastic and adversarial bandits, estimates and show that their regret smoothly interpolates between placing the regret in the classical bandit setting and that of mapping the full - information setting possibly as a function of the neighbors'exploration. in the stochastic setting the additional information must simply be incorporated into the usual estimation of storing the rewards, while in the adversarial setting this is attained by constructing a new unbiased estimator for the chosen rewards and appropriately bounding the amount of additional information provided by the neighbors. these algorithms are optimal up to log factors ; despite the fact that the agents act independently and selfishly, this implies that it is an approximate nash equilibria for all agents to use our algorithms. further, we show via empirical simulations that our algorithms, often significantly, outperform existing algorithms that one could apply to this setting.", "histories": [["v1", "Fri, 14 Apr 2017 16:24:58 GMT  (905kb,D)", "http://arxiv.org/abs/1704.04470v1", "This article was first circulated in January 2015 and presented at ISMP 2015 under the title \"Bandit in a Network\" (this https URL&amp;mmnno=264&amp;ppnno=85856)"]], "COMMENTS": "This article was first circulated in January 2015 and presented at ISMP 2015 under the title \"Bandit in a Network\" (this https URL&amp;mmnno=264&amp;ppnno=85856)", "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["l elisa celis", "farnood salehi"], "accepted": false, "id": "1704.04470"}, "pdf": {"name": "1704.04470.pdf", "metadata": {"source": "CRF", "title": "Lean From Thy Neighbor: Stochastic & Adversarial Bandits in a Network", "authors": ["L. Elisa Celis", "Farnood Salehi"], "emails": ["elisa.celis@epfl.ch", "farnood.salehi@epfl.ch"], "sections": [{"heading": "1 Introduction", "text": "Individuals often have access to information, via their social or economic network, that they can use to make improved decisions. This phenomenon has been observed widely in the social and natural sciences. For instance, a recent work ([Yoo12]) studies farmers who, every year, have to decide which kind of seed to plant (not just what kind of crop, but which variety of seed) in order to attain the most profit (i.e., revenue - cost). In their study, [Yoo12] finds that farmers\u2019 decisions are based on a) their own experience in previous years of how different varieties performed, and b) the experiences of peers attained either directly (explicitly via conversations with social contacts) or indirectly (implicitly by observing the farming practices of peers). Moreover, the information farmers used is primarily from peers in their physical neighborhood \u2013 not only because these are where their contacts are most likely to be, but also because the profit is correlated due to similar soil and weather conditions. These connections between peers then form a network of farmers across the country, where locally, each farmer is trying to learn the best seed for their farm using her own information and that of her neighbors. As another example, consider WI-FI networks in which nodes want to send their data across the best frequency band. Nodes could obtain the current quality of the band their peers are using indirectly through capacity estimation or directly by message passing, and use this information to determine which band to use. Similar social learning phenomena appear in many other areas in various disguises \u2013 e.g., in the acquisition\nar X\niv :1\n70 4.\n04 47\n0v 1\n[ cs\n.L G\n] 1\nof consumer products by individuals, the adoption of new technologies, the prevalence and spread of corruption, and in the behavior of animals such as squirrels; see [LBG48, KL55, ZAA07, San06, ASC12]. A natural question then is: How should an individual incorporate the information from their neighbors in order to make the best decisions, and how much improvement can such information bring?\nTowards this, one approach could be to consider the framework of bandits with side observations for which, in the adversarial setting, variants of the multiplicativeweight update algorithm have been developed with success. Informally, side observations just mean that at each time step, in addition to observing the reward of a selected action a(t), one may observe (but not receive) rewards from a set of other actions S(t). A recent body of work\nhas explored how to minimize regret for various different models of S(t). In the free observation model, the individual is allowed to select S(t) up to some cardinality (e.g., [AKTT15]). However, if one tried to apply such algorithms to the social settings considered above, it would require an individual to decide which actions her neighbors should take, and hence is not feasible as a solution in this setting. In another line of work (e.g., [ACBDK15]) an action-network model has been studied: Here, the actions form a network and the individual observes the rewards of the neighbors of the action she selects (as opposed to the rewards of the actions that her neighbors select). The action-network is often taken to be exogenous and can be changing over time. Thus, one can apply the algorithms developed in the action-network setting to the social setting above by defining S(t) to be the set of actions selected by the individual\u2019s neighbors; however, this may not always be optimal for the social setting as neighbors can provide even more information (see Section 4.5).\nThe above approaches have been developed in independent contexts and hence geared towards different settings. Towards obtaining optimal results in the social setting described above, the challenge is to adequately model the information from neighbors that can aid in learning, leverage it appropriately, and quantify the advantage it provides. In this paper we present such a model and show how one can incorporate the additional social information in order to obtain optimal results. More specifically, in the stochastic setting we show that incorporating side information in a simple way gives rise to a near-optimal algorithm. Furthermore, in the adversarial setting, we present a modified multiplicative-weight update algorithm that uses an new unbiased estimator to incorporate this information appropriately into the estimation of the reward of each action. The proofs requires us to overcome some additional hurdles in order to bound the amount of information gleaned from the neighbors and attain optimal bounds on the regret. Our algorithms outperform other state-of-the-art bandit algorithms both theoretically (Section 4) and empirically (Section 5)."}, {"heading": "1.1 Summary of Our Results", "text": "Since our model appeals to the bandit framework, we start by recalling its salient features: In the bandit optimization setting, there are K potential actions (arms), and the individual selects one arm at each time step. Each arm j has an\n(unknown) reward gj(t) at time t = 1, . . . , T . The individual receives the reward ga(t)(t) for the selected arm a(t), while the rewards gj(t) for all other actions j 6= a(t) remain unknown. Ideally, the individual would like to select the arm with the best overall reward, i.e., j? = arg maxj E [\u2211T t=1 gj(t) ] . However, in the absence of knowledge about the reward structure, this is not feasible. Instead, as is prevalent in the online learning literature, j? is used as a benchmark and the regret R (i.e., the difference between the individual\u2019s rewards and those of the best arm) is minimized. Formally,\nR def = E [ T\u2211 t=1 gj?(t)\u2212 T\u2211 t=1 ga(t)(t) ] ,\nwhere the expectation is taken over the randomness in the rewards (if stochastic) and the randomness in the algorithm.1 An important divide arises from how the rewards are decided: for stochastic bandits, the rewards gj(t) are i.i.d. from an (unknown) distribution Fj , and for adversarial bandits (also known as non-stochastic bandits), the rewards gj(t) are set in an arbitrary manner by an adversary that knows the individual\u2019s algorithm and past coin flips. In realistic settings, such as the examples mentioned in the introduction, the situation is often somewhere in-between. Thus, in this work we consider both extremes by analyzing both the stochastic and adversarial settings. In either case, algorithms must carefully tradeoff between exploration (gaining new knowledge about the rewards) and exploitation (using current knowledge to maximize rewards) in order to minimize regret. In either case, algorithms must carefully tradeoff between exploration (gaining new knowledge about the rewards) and exploitation (using current knowledge to maximize rewards) in order to minimize regret.\nThe Model. Firstly, we assume all individuals play against the same multi-armed bandit: in the stochastic setting, the reward distributions Fi is the same for arm i for all individuals (although the realizations at any given time may differ), and in the adversarial setting the reward vector g(t) selected by the adversary at time t is the same for all individuals. Clearly, there must be some similarity in the rewards between neighbors for social learning to occur. Our results also extend to the setting in which the distributions or rewards are correlated, e.g., via 0-mean noise (i.e., each individual i receives the reward as above + individual noise); for ease of presentation we omit this extension, the proofs follow analogously. Secondly, we assume that each individual can observe the following for all neighbors i:\n(1) the actions ai(t), (2) the rewards gai(t)(t), and (3) (for the adversarial setting only) the probability distribution each neighbor used to select an arm at the\nprevious time step. Assumptions (1) and (2) are natural and directly inspired by applications such as those mentioned in the introduction; individuals either directly or indirectly observe their neighbors\u2019 actions and rewards. (3) additionally assumes a limited knowledge of how neighbors made their decisions on a step-by-step basis, without having to assume we know their overall algorithm or restricting their behavior in any way. All individuals are free to select their probability distributions arbitrarily (and depend arbitrarily on each other), and each one can draw her decision independently of the rest. While it would be nice to drop assumption (3) entirely, this would prevent us from attaining optimal regret bounds (see Proposition 4.5 and the discussion in Appendix A.4).2\nImportantly, an individual (4) does not know about the actions and rewards of individuals beyond her neighbors, (5) does not know any global properties of the network, (6) does not know which algorithm other individuals (including neighbors) are using, and (7) cannot dictate or coerce other individuals to act a certain way.\nRemoving any of assumptions (4)-(7) would be unnatural for the social learning setting described above: If (4) does not hold, we would simply consider such an individual a neighbor, removing (5) is unnatural as the network can be very large and we cannot expect to have knowledge of distant individuals, removing (6) seems impractical as it would mean that individuals have a detailed knowledge of how neighbors select actions, and allowing individuals to be coerced as in\n1This is often referred to as pseudo-regret; in this paper we simply refer to it as regret. 2Alternatively, under different assumptions (e.g., if we assume the neighbors are using an algorithm such as EXP3) we can estimate these\ndistributions which would suffice.\n(7) would be in conflict with the idea that every individual seeks to improve her own performance. Hence, given (1)-(7), any improvement in the individual\u2019s regret arises solely from passive observation of local information.\nRemark 1.1. We emphasize that, by setting up the model as above, agents can act independently and work to selfishly minimize their own regret. Hence, (as our algorithms are near-optimal) it is always an individual\u2019s (approximate) best-response to use our algorithms, regardless of what her neighbors do. Putting it another way, when the action space is the set of algorithms, it is an (approximate) Nash equilibrium for all nodes to use ours \u2013 no individual can significantly improve their regret by deviating to use an alternate algorithm. This gives rise to an interesting set of questions regarding the average regret of a network (depending on its structure) in equilibrium, or the expected regret in equilibrium of a node as a function of its position in the network. Some immediate implications about the properties of such equilibria follow directly from our results.\nThe Algorithm in the Stochastic Setting. Algorithms for the classical stochastic bandit setting [AG12, ACBF02, GC11] maintain metrics about the observed samples, and determine which arm to select based on these metrics. Hence, a natural strategy would be for an individual in the networked setting to take one of these algorithms and incorporate samples obtained from neighbors along with her own without differentiating between the two. Indeed, we this simple insight suffices to get near-optimal results. Our UCBN algorithm extends the classic UCB algorithm by incorporating all observed samples indiscriminately. We show that this suffices to improve performance, often dramatically, both asymptotically and in silico. These results are presented in Section 3 and Appendix B. We show that the regret of our algorithm interpolates between O(1) and the O(K lnT ) regret for the classic bandit setting depending on the amount of exploration conducted by the neighbors (see Theorem 3.1), and these bounds are asymptotically optimal (see Theorem B.3). As a corollary, in the complete network with b vertices, if all neighbors use our algorithm (i.e., in equilibrium) the regret reduces to O(K lnTb ), which is optimal as even a fully centralized approach can speed up learning by at most a factor of the increase in the number of available samples, i.e., b (see Corollary B.2). The theoretical results are presented in Section 3 and Appendix B), and the empirical results in Section 5.2).\nThe Algorithm in the Adversarial Setting. Algorithms for classic adversarial bandits are, typically, variations of the multiplicative weights update method. Such algorithms maintain a weight for each arm, and (multiplicatively) update that weight based on the observed reward(s). An individual then selects an action proportionally to the weight vector. It is easy to verify that if we na\u00efvely incorporate samples from neighbors as if they were our own into the weight vector there is no improvement in the regret. Our variation of the multiplicative-weights update algorithm a) incorporates information in a clever manner (that allows for new improved bounds) by using a different unbiased estimator for the rewards and then b) adapts according to the behavior of its neighbors by tuning its update parameters. We show that the regret of our algorithm provably interpolates between the O( \u221a KT lnK) regret for the classical bandit setting and the O( \u221a T lnK) regret for the full-information setting (where the full vector g(t) is observed at each time step) depending on the amount of exploration conducted by the neighbors (see Theorem 4.1), and is optimal up to log factors (see Theorem 4.2 & 4.5). Moreover, our algorithm improves performance over the state-of-the art in silico. The theoretical results are presented in Section 4 and Appendix A), and the empirical results in Section 5.1)."}, {"heading": "2 Related Work", "text": "Distributed learning in a network is a broad topic and has been studied under various names in several disciplines. However, to the best of our knowledge, our model for learning from neighbors, along with its assumptions and nonassumptions (1)-(7) which are motivated by relevant settings in sociology and economics, is novel. Here we briefly survey the closest relatives to our work.\nIn the study of non-strategic learning on networks, individuals are connected via a network, and each individual has a finite set of actions with probabilistic rewards whose distributions depend on the state of the world (see [Goy05], Chapter 2 for a survey). Indeed, would be similar to our model in the stochastic setting. However, work in this area has focused on studying variants of a greedy algorithm, and answering the question of whether learning (i.e., discovery of the state of the world, and hence convergence to the best action) occurs asymptotically (see, e.g., [BG98, EF93, BG01, GK03, GJ10]). Instead, we are concerned with regret, which could be loosely interpreted as the rate of convergence.\nRecall that in models of bandits with side observations, in addition to observing ga(t)(t), one may observe (but not receive) additional rewards gS(t)(t). The set of arms S(t) depends on the particular model of side observations. In the free observations model, the individual can select B additional arms to observe at each time step; i.e., |S(t)| = B and the individual selects S(t) for all t. Such models have been studied both for stochastic ([YM09]) and adversarial ([AMS12, AKTT15]) bandits. Without assumption (7), we could apply such algorithms directly because an individual could dictate which actions her neighbors should take. In the social setting we cannot hope to control our neighbor\u2019s decisions in this manner. Still, we show that the performance of our algorithm is equivalent empirically to such algorithms (see Section 5). In the arm-network (or action-network) setting, the individual observes the rewards of the neighbors of the arm she selects. Such stochastic ([CKLB12, BES14]) and adversarial ([MS11, ACBGM13, KNVM14, ACBDK15]) bandit settings have been studied. While one could apply these algorithms to the social setting, some social information, in particular from assumption (3), is left on the table. Leveraging this allows us to provably outperform such approaches (see Section 4.5), and empirically the difference can be dramatic (see Figure 3).\nOther work has considered bounding the cumulative regret of all individuals, rather than individuals minimizing their own regret. Towards this, centralized algorithms for various versions of stochastic bandits have been studied, in particular for the complete graph ([BES13, SBFH+13, CBGZ13]). Although the centralized setting is not the object of our study, as a corollary, we obtain a centralized algorithm for adversarial bandits that is optimal on the complete network (see Section 4.6)."}, {"heading": "3 Technical Overview for the Stochastic Setting", "text": "To describe our algorithm, let us first revisit the UCB algorithm, first introduced by Auer et. al. [ACBF02] and since widely extended and studied (see, e.g., [Bub10, MMS11, GC11]). UCB is an asymptotically optimal algorithm for stochastic bandits with many well-studied variants (see, e.g., [BCB12] for an overview). The idea behind the algorithm uses the principle of optimism in the face of uncertainty; the algorithm maintains an optimistic upper bound on the mean reward of each arm, and selects an arm with maximal upper bound. As is standard, we assume the probability distributions satisfy Hoeffding\u2019s lemma. Then, arm j at time t has an upper bound\nUj(t) = \u00b5\u0302j(t) +\n\u221a \u03b1 ln(t)\n2nj(t) (1)\nwhich holds with probability at least 1 \u2212 t\u2212\u03b1 when \u00b5\u0302j(t) is the sample mean of arm j, and nj(t) is the number of samples we have for j. At time t, an arm a(t) \u2208 arg maxj{Uj(t)} is selected.\nWe make a simple extension to UCB for an agent on a network: the agent simply incorporates all samples and all rewards into nj and \u00b5\u0302j regardless of whether it came from her action or was observed from one of her neighbors. We denote this algorithm by UCBN, and note that it can be implemented by an individual irrespective of the graph structure and the algorithm(s) her neighbors may employ. The regret of UCBN algorithm is upper bounded as follows.\nTheorem 3.1. Consider an agent with neighbors who play arbitrarily. Let n\u2032i(t) be the number times arm i has been selected by one of her neighbors by time t. Then, the regret of UCBN for any \u03b1 > 2 is\nR \u2264 \u2211\ni,\u2206i>0\n( max { max t=1,..,T { 2\u03b1 ln t \u2206i \u2212 n\u2032i(t)\u2206i } , 0 } + \u03b1 \u03b1\u2212 2 ) , (2)\nwhere \u2206i is the difference between \u00b5i? and \u00b5i.\nThis result is asymptotically optimal (see Theorem B.3). The regret differs from the regret of the classic UCB regret by the \u2212n\u2032j(T )\u2206j term, and, depending on the behavior of the neighbors, can potentially take the agent from logarithmic to constant regret.\nClearly, the performance of an agent must depend on the behavior of her neighbors. In the worst case, if there are clumsy agents who always select the same arm, then our regret is not improved much. However, as long as the agent has at least one neighbor who explores an arm uniformly at random with probability \u03b5t \u2208 \u03c9(K ln tt ) at time t (e.g., this occurs if a neighbor uses an adaptive greedy algorithm), then the regret is O(1)! Hence, this allows us to\ninterpret neighbor behavior to our regret seamlessly. As an instructive example, consider the setting where all agents use UCBN in a complete graph. The regret in this setting is O ( K lnT b ) . In other words, the regret of an agent using UCBN is a factor O(1/b) less than that of an agent using UCB \u2013 indeed we cannot hope to do better, even in a completely centralized setting. The proof parallels the proofs for the original UCB results (see, e.g., [BCB12] for a template), and can be found along with further discussion in Appendix B. While the story for the stochastic setting turns out to be simple and easy to manage, the adversarial setting, as we see below, turns out to be more challenging."}, {"heading": "4 Technical Overview for the Adversarial Setting", "text": ""}, {"heading": "4.1 Preliminaries", "text": "The multiplicative weight update method has been discovered many times in many fields over the past century (see [AHK12] for an overview). It is a simple yet surprisingly powerful way to conservatively update beliefs about the benefit of a given arm and is extremely effective for adversarial bandits and is asymptotically optimal up to log factors (see, e.g., [ACBFS02, FKM05, ACBGM13]). Such algorithms for the full information setting (where all rewards are observed at each time step) maintain a vector of weights wj for each arm j, and (multiplicatively) update it at each time step: wj(t + 1) = wj(t)e\u03b4gj(t), where 0 \u2264 gj(t) \u2264 1 is the reward observed for arm j at time t and \u03b4 is the update parameter. The probability of choosing arm j at time t is proportional to the weight wj(t), namely, pj(t) =\nwj(t) Wt where wj(0) = 1/N, and Wt = \u2211N j=1 wj(t). This algorithm, for an optimal choice of \u03b4 has regret \u0398( \u221a T lnK). Extending to the bandit setting uses a simple trick; instead of using gj(t) to update, we use an unbiased estimator g\u0302j(t) for gj(t) (see, e.g., [ACBFS02, FKM05]). Since we no longer observe the reward of all of the arms, we must also ensure some exploration should be added to the algorithm. This is achieved by setting a lower bound \u03b7 \u2208 [0, 1] (the exploration parameter) on the probability of exploration: pj(t) = (1\u2212 \u03b7)wj(t)Wt + \u03b7 K . This algorithm, also known as\nEXP3 [ACBFS02], achieves regret O( \u221a TK lnK), and is optimal up to log factors for the right choice of parameters (see [BCB12] for an exposition)."}, {"heading": "4.2 Formal Statement of Results", "text": "We call our algorithm in the adversarial setting EXPN. Recall that pj(t) is the probability that an individual selects arm j at time t. Let qij(t) be the probability that her neighbor i selects arm j at time t. We denote the number of an individual\u2019s neighbors by b. Note that the number of nodes in a network, denoted by N , may be much larger, but the remaining network does not play a role in the algorithm or main results.\nTheorem 4.1. Given an individual with b neighbors who are playing arbitrarily, the regret when using the EXPN algorithm is\nREXPN \u2264 E 2 \u221a\u221a\u221a\u221a(T + T\u2211\nt=1\n\u03b3t\n) lnK  (3) where \u03b3t = \u2211K j=1 pj(t) pj(t)+ \u2211b `=1 q ` j(t) .\nBefore discussing the proof, we first re-write the results in a way that makes them easier to interpret. For ease of presentation, momentarily assume that for all arms j \u2208 [K], neighbors i \u2208 [b], and times t \u2208 [T ] we have that qij(t) \u2265 \u03b5iK for some \u03b5i \u2208 (0, 1]. 3 We can then reinterpret REXPN as a function of the bandit regret (REXP3) and full information information regret (RFULL) as follows:\nREXPN =  RFULL \u00b7 \u221a \u03b2 \u0398 \u2264 1\u2212 \u221a lnK \u03b2T REXP3 \u00b7 \u221a \u03b2+K/\u0398 2K 1\u2212 \u221a lnK\n(\u03b2+K\u0398 )T \u2264 \u0398\n(4)\n3This assumption is not required for the proof of Theorem 4.1, only for the ease of interpretation in Equation 4. Note that if a neighbor is running any variant of the multiplicative weight update method, this condition is satisfied. Removing this assumption requires the number of non-zero \u03b5j to be tracked for each j, and these numbers would appear in the regret bound.\nwhere \u0398 = \u03a0bi=1 ( 1\u2212 \u03b5iK ) and \u03b2 = 1\n1\u2212(1\u2212 1K )\u0398 + 1.\nIn particular, note that when \u0398 = 1, none of the individual\u2019s neighbors maintain a probability distribution that is bounded away from 0 for all arms. In other words, the neighbors are not exploring effectively. In this case, \u03b2 = K + 1 and REXPN \u2208 O( \u221a TK lnK), the same as in the classical bandit setting. On the other hand, for example, when\n\u0398 \u2264 1/2, then \u03b2 \u2264 3 and hence REXPN \u2208 O( \u221a T lnK), the same as in the full-information setting. Hence, this algorithm smoothly interpolates between bandit regret and full information regret as a function of the neighbors\u2019 exploration.\nThe proofs of Theorem 4.1 and Equation 4 appear in Section 4.4 and Appendix A respectively. At first, the proofs parallel standard approaches to analyze the multiplicative-weight update method; the crucial difference is a new unbiased estimator that is used in order to incorporate the neighbors\u2019 information (see Section 4.3). This leads to the following bound on the regret:\nR \u2264 lnK \u03b4 + \u03b7T + \u03b4T K\u2211 j=1 pj(t) p\u2032j(t) .\nThe technical obstacle then becomes attaining tight bounds on the \u2211K j=1 pj(t) p\u2032j(t)\nterm (see Lemmas A.2 and A.4). What remains is then a straightforward (albeit tedious) optimization problem on the parameters \u03b7 and \u03b4.\nWe can further show that Theorem 4.1 is optimal up to log factors.\nTheorem 4.2. Given an individual with b neighbors who are playing arbitrarily, the regret when using the EXPN algorithm is\nREXPN = \u2126\n \u221a\u221a\u221a\u221aT + T\u2211\nt=1\n\u03b3t  where \u03b3t = \u2211K j=1 pj(t) pj(t)+ \u2211b `=1 q ` j(t) as defined above.\nThis information theoretic lower bound follows by extending the lower bound attained for the classic bandit setting (see, e.g., [ACBF02]), and is given in Appendix A.3. This theorem shows that the analysis of our algorithm is tight up to log factors and will help us establish dominance over other potential approaches as discussed in Section 4.5 \u2013 a weaker lower bound that is not algorithm-dependent (which matches the above bound for pathological cases such as when all neighbors always play the same action) is presented in Theorem 4.5.\n4.3 The EXPN Algorithm Key to our EXPN algorithm is the following new unbiased estimator for the rewards:\ng\u0302j(t) =\n{ gj(t) p\u2032j(t) if some individuals selects action j at time t\n0 otherwise, (5)\nwhere p\u2032j(t) is the probability that at least one individual selects action j, i.e.,\np\u2032j(t) def = 1\u2212 (1\u2212 pj(t))(1\u2212 q1j (t)) \u00b7 \u00b7 \u00b7 (1\u2212 qbj(t)). (6)\nThe algorithm then updates the weights according to wj(t+1) = wj(t)e\u03b4g\u0302j(t) = wj(0)e\u03b4 \u2211t s=1 g\u0302j(s), where wj(0) = 1, and updates the probability distributions according to pj(t) = (1 \u2212 \u03b7)wj(t)Wt + \u03b7 K where Wt = \u2211 j wj(t). Note that this algorithm can be implemented irrespective of the network structure and depends only on the information obtained locally from neighbors as defined in our model. In essence, the key to our algorithm is two fold:\n1. Design a new unbiased estimator g\u0302j(t) that incorporates the side observations obtained from neighbors: Unlike for stochastic bandits, na\u00efve estimators do not suffice, and a new approach is required.4\n4We must ensure that in bounding E[g\u03022j ], we get some improvement over the usual bandit setting; it is easy to verify that such bounds do not hold for na\u00efve estimators such as the average of the neighbors\u2019 estimators.\n2. Decouple the exploration and exploitation parameters: When an individual\u2019s neighbors explore a lot, she could benefit by free-riding off of the exploration of her neighbors; this is accomplished by decreasing her exploration parameter \u03b7. However, if we take \u03b4 = \u03b7/K as in EXP3, this dampens our updates. Hence we need \u03b7 and \u03b4 to act independently.\nThis analysis is presented in Appendix A.1."}, {"heading": "4.4 Proof of Theorem 4.1", "text": "While the above version of the algorithm gives a natural interpretation of the parameters, in order to attain the stronger regret bound in Theorem 4.1 we take a slightly different approach. Instead of decoupling the parameters \u03b7 and \u03b4, we instead use an adaptive \u03b4t that changes based on the amount of information received from the neighbors.5 In particular, we let \u03b4t = \u221a lnK\u2211t\nc=1(1+\u03b3c) and can take \u03b7 = 0. Importantly, note that we use the same unbiased estimator in either\nversion of the algorithm.\nProof of Theorem 4.1. The first part of proof (from Equation (7) to Equation (12)) parallels the traditional analysis for analyzing multiplicative weight update algorithms; for completeness we present the steps without going into the details (see [BCB12] for an exposition). We first write E[g\u0302j(t)] as follows:\nE[g\u0302j(t)] = 1\n\u03b4t (lnE [exp (\u2212\u03b4t (g\u0302j(t)\u2212 E[g\u0302j(t)]))]\u2212 lnE [exp (\u2212\u03b4tg\u0302j(t))]) (7)\nwhere the expectation is over the randomness of the estimator and choice of the arm. We will now consider the right-hand side of the equation and upper bound the two terms separately.\n1 \u03b4t lnE [exp (\u2212\u03b4t(g\u0302j(t)\u2212 E[g\u0302j(t)]))] = 1 \u03b4t lnE [exp(\u2212\u03b4tg\u0302j(t)] + E[g\u0302j(t)])\n\u2264 1 \u03b4t E[exp(\u2212\u03b4tg\u0302j(t))\u2212 1 + \u03b4tg\u0302j(t)] \u2264 \u03b4t 2 E[g\u03022j (t)],\n(8)\nwhere we use the inequalities lnx \u2264 x\u2212 1 and exp(\u2212x)\u2212 1 + x \u2264 x2/2 for x \u2265 0. Now, let G\u0302j(t) = \u2211t a=1 g\u0302j(t),\nand let \u03c8(t) = 1\u03b4t ln ( 1 K \u2211K j=1 exp(\u2212\u03b4tG\u0302j(t)) ) . Hence,\n\u2212 1 \u03b4t ln E at\u223cp\u2032t E j\u223cp\u2032t exp(\u2212\u03b4tg\u0302j(t)) \u2264 \u2212 1 \u03b4t E at\u223cp\u2032t ln E j\u223cp\u2032t exp(\u2212\u03b4tg\u0302j(t))\n\u2212 1 \u03b4t E at\u223cp\u2032t ln  K\u2211 j=1 exp(\u2212\u03b4tG\u0302j(t))\u2211K c=1 exp(\u2212\u03b4tG\u0302c(t\u2212 1))  \u2264 \u03c8(t\u2212 1)\u2212 \u03c8(t) (9)\nwhere we recall that \u03b7 = 0, the first inequality we use Jensen\u2019s inequality, and note that \u03b4t is decreasing in t. By summing up the terms in Equation (7) and (9) over all t we get\nT\u2211 t=1 E[g\u0302j(t)] \u2264 T\u2211 t=1 \u03b4t 2 E[g\u03022j (t)]\u2212 E at\u223cp\u2032t \u03c8(T ). (10)\nWe now bound \u2212\u03c8(T ) as follows:\n\u2212\u03c8(T ) = lnK \u03b4T \u2212 1 \u03b4T ln  K\u2211 j=1 exp(\u2212\u03b4T G\u0302j(T ))  \u2264 lnK \u03b4T \u2212 1 \u03b4T ln ( exp(\u2212\u03b4T G\u0302k(T )) ) = lnK \u03b4T + G\u0302k(T ).\n(11) 5Adaptive \u03b4t are often used when T is unknown; here the adaptive \u03b4t serves a different function by allowing us to explicitly respond to the\nneighbors\u2019 actions.\nPlugging this into in the above inequality yields\nT\u2211 t=1 E[g\u0302j(t)] \u2264 T\u2211 t=1 \u03b4t 2 E[g\u03022j (t)] + lnK \u03b4T + E at\u223cp\u2032t [G\u0302k(T )]. (12)\nNow, we note that given pj(t) at time t we have\nE [g\u0302j(t)] = gj(t), (13a)\nE [ T\u2211 t=1 K\u2211 i=1 pj(t)g\u0302j(t) ] = T\u2211 t=1 K\u2211 i=1 pj(t)gj(t) = E [ T\u2211 t=1 ga(t)(t) ] , and (13b)\nE [ T\u2211 t=1 K\u2211 i=1 pj(t)g\u0302 2 j (t) ] = T\u2211 t=1 K\u2211 i=1 pj(t) p\u2032j(t) g2j (t) \u2264 T\u2211 t=1 K\u2211 i=1 pj(t) p\u2032j(t) (13c)\nas gj(t) \u2264 1, and where the expectation is over randomness of the algorithm. Now, attaining a good bound on the regret boils down to attaining a good bound on \u2211K i=1 pj(t) p\u2032j(t)\n. Towards this, we need a technical lemma that, in effect, allows bounds the amount of information received from neighbors.\nLemma 4.3. \u2211K j=1\npj 1\u2212(1\u2212pj)(1\u2212q1j )\u00b7\u00b7\u00b7(1\u2212qbj )\n\u2264 \u2211K j=1\npj pj+q1j+q 2 j+\u00b7\u00b7\u00b7+qbj + 1.\nThe proof is presented in Appendix 4.4. Using this Lemma and combining all of the above, we get\nR \u2264 lnK \u03b4T + 1 2 T\u2211 t=1 \u03b4t(1 + \u03b3t). (14)\nTo conclude the proof, recall that \u03b4t = \u221a\nlnK\u2211t c=1(1+\u03b3c)\n, use Lemma 3.5 of [ACBG02], and take expectation of the both sides of Equation (14).\n4.5 Comparison to Alternate Approaches\nInstead of developing a new algorithm, we could have attempted to leverage an existing one. The most natural one to try is from the arm-network setting which is as follows: there is a single individual and the bandit\u2019s arms form an arm-network which can change over time. An edge from arm u to arm v means that by choosing arm u we observe the reward of arm v. Thus we could, in retrospect at each time step, recreate an arm-network (see Figure 2) and apply an arm-network algorithm. We consider EXP3G ([ACBDK15]), which is the state-of-the-art solution for such problems, and performed best amongst arm-network algorithms in our empirical simulations. However, we prove in Appendix A.4 that our algorithm is at least as good.\nProposition 4.4. REXPN = O(REXP3G).\nMoreover, as we will see in Section 5, the regret of EXPN is often drastically better empirically. This because EXP3G, and other similar algorithms, were developed from different settings in which it was not possible to make use of the probability distributions afforded to us by assumption (3). Indeed, without this assumption, one can get a stronger lower bound than the one presented above.\nProposition 4.5. Let nt be the size of the set of arms selected (arbitrarily) by all of the individual\u2019s neighbors at time t. Then, the regret RA for any algorithm A in our setting without assumption (3) is RA = \u2126 (\u221a T + \u2211T t=1(K \u2212 nt) ) .\nThe proof follows from Theorem 5 of [ACBG+14]. Our EXPN algorithm is often able to beat this bound by leveraging (3). For example, this proposition implies that if we have a complete network on b vertices where\nlogK b K, then REXP3G = \u2126 (\u221a (K \u2212 b)T ) while in our case REXPN = O (\u221a K b T ) (see Corollary 4.7)."}, {"heading": "4.6 A Centralized Solution for the Network", "text": "Our model and algorithm are formulated for an individual because this allows us to draw the most general conclusions \u2013 bounding the individual\u2019s regret as a function of the neighbors\u2019 behavior. However, a surprising feature is that it can also be made into a centralized solution. In the general case, this requires assuming there is an external coordinator that can select a maximum-degree individual to lead and direct the rest on how to act as follows: Let v? be the maximum degree node selected. The coordinator directs v? to use the EXPN algorithm. The remaining nodes u are each assigned a neighbor vu that lies on the shortest path between them and v?, and are directed to copy the probability distribution that vu used in the previous time step.\nTheorem 4.6. Using the above centralized algorithm, the regret of all individuals is at most\nR = O ( \u2206 + \u221a( 1 +\nK\n1 + bmax\n) T lnK ) , (15)\nwhere bmax is the degree of v? and \u2206 is the diameter of the network.\nThe proof follows, with minor modifications, from the proof of Theorem 4.1; the main difference regards accounting for the delay (of at most \u2206 time steps) for the farthest node from v? to update their probability distribution. By replacing \u03b3t with K1+bmax , this gives us the resulting regret bound. In the simple case of a complete network on N nodes, no coordinator is required, and we obtain the following corollary. Corollary 4.7. On a complete network with b nodes, if all nodes use the EXPN algorithm, then they attain R = O (\u221a( 1 + Kb ) T lnK ) , which is optimal (up to log factors) for any centralized solution.\nThis again follows from the proof of Theorem 4.1 using the fact that the number of neighbors is b\u2212 1 on a complete network, and that a centralized solution has average regret \u2126( \u221a( 1 + Kb ) T ) as shown in [AKTT15]."}, {"heading": "5 Empirical Results", "text": ""}, {"heading": "5.1 Adversarial Setting", "text": "Benchmarks. We compare our algorithm against the bandit algorithms developed for various settings with sideinformation, namely EXP3G ([ACBDK15]), EXP.IX ([KNVM14]) and BEXP ([AKTT15]). The first two are designed for the arm-network setting as described in Section 4.5, while the latter is designed for the free-exploration setting described in Section 2. Recall that in free-exploration there are no neighbors; rather there is a budget B, and at each time step the individual can choose up to B arms to select. In order to attain a fair comparison, we assume we have budget B = b+ 1 for BEXP, where b is the number of neighbors.\nExperimental Setup. For the simulations we use the decoupled version of the EXPN algorithm as presented in Section 4.3; the results for the adaptive algorithm version would be even better. We consider a bandit with Bernoulli rewards that has a single good arm with mean 0.7, while the remaining arms have mean 0.5. This is similar to the worst case (minimax) bandit; the difficulty arises from the fact that it is hard, in an information theoretic sense, to distinguish the single good arm from the rest with few samples. Indeed the performance for our algorithm in comparison to our benchmarks is only improved for all other settings we attempted.\nPerformance in Networks. In addition to exploring the effect of the various algorithm on a single individual, we are able to consider various network topologies and consider the regret as a whole. Towards this, in the first set of simulations, all nodes in the specified networks use the same algorithm. We first compare the regret of EXPN against the benchmarks in a complete network on 5 nodes (Figure 3(a)); even on such a small network the difference in regret is dramatic.6 We significantly outperform arm-network algorithms (EXP3G and EXP.IX), which empirically are initially worse than even EXP3. Asymptotically EXP3G eventually outperforms EXP3, although EXP.IX does not. Surprisingly, our algorithm performs as well as BEXP, which would be equivalent to identifying a single node as the leader and having them dictate the action of all other nodes. Hence, our distributed algorithm is as good as a centralized one. For comparison, we also consider a random 5-regular graph on 50 nodes (Figure 3(b)), and observe that the performance of all algorithms is roughly equivalent to the complete network on 5 nodes; i.e., the primary determining factor in the regret appears to be the number of neighbors rather than the topology of the network.\nWe also consider the regret of EXPN on various network topologies on 10 vertices: the complete network, a random 5-regular network, a cycle, and a star network (Figure 3(c)). When the number of neighbors differ in a topology, the regret of the nodes may differ; the star is the extreme example and we depict the minimum (for the center node), maximum (for one of the leaves) and average regret. As expected, the more neighbors one has, the better the regret is, with the internal node of star outperforming all. We also observe that there is an advantage to having neighbors that are not well-connected; despite a node in the complete network having the same degree as the center node of the star, the former has more regret. Because the nodes that are not well-connected receive less information, they must explore more \u2013 this is advantageous for their neighbors.\nPerformance of Individuals. Moving back to analyzing the performance for an individual, consider a setting where her neighbors all use the EXP3 algorithm. We measure the regret ratio, i.e., the ratio between the regret of bandit algorithm A when the node has b neighbors divided by the regret of A when the node has 0 neighbors. This allows us to better visualize the improvement in regret that each algorithm obtains as a function of the number of neighbors. We vary time T (Figure 3(d)), the number of arms K (Figure 3(e) and the number of neighbors b (Figure 3(f)). We observe that, in all cases, our EXPN algorithm always matches or outperforms the benchmarks. The fact that the performance of our EXPN is comparable to that of BEXP is surprising, as we could not hope to do any better."}, {"heading": "5.2 Stochastic Setting", "text": "The setup for the empirical results in this section parallel that of Section 5.1. Recall that we make no assumption in our algorithm about our neighbors or how they play. We simply observe their actions and rewards. We let \u03b1 = 2.5 in the UCBN algorithm; the performance could be improved by optimizing \u03b1. We first observe that more neighbors leads to less regret (Figure 4(f)).\nWe then consider the regret of UCBN on various network topologies on 10 vertices: the complete network, a random 5-regular network, and a star network (Figure 4(c)). Similar to the previous experiments, for networks in which all vertices have the same number of neighbors (all but the star network), all agents attain the same regret and hence we report the average regret. However, this is not the case if the number of neighbors differ; the star is the extreme example and we depict the minimum (for the center node), maximum (for one of the leaves) and average regret. As expected, the more neighbors one has, the better the regret is, with the complete network and center of star outperforming all. We also observe that there is advantage to having neighbors that are not well-connected; despite a node in the complete network having the same degree as the center node of the star, the former has slightly more regret. The reason is a neighbor with lower degree attains less information from neighbors and explore the suboptimal arms more which itself is in the favor of its neighbors (here the center of star).\nWe then consider the regret ratio, i.e., the ratio between the regret of bandit algorithm A when the agent has b neighbors divided by the regret ofA when the agent has 0 neighbors. This allows us to better visualize the improvement in regret that each algorithm obtains as a function of the number of neighbors. We vary the number of arms K (Figure 4(e) and the number of neighbors b (Figure 4(f)). We observe that, in all cases, our algorithm UCBN attains the theoretical regret ratio, i.e, in the complete graph when all agents use UCBN the regret ration Ab \u2192 1/b as T \u2192\u221e.\n6Indeed, on larger networks the differences are only more pronounced \u2013 we present the results on a small network in order to be able to visualize them adequately.\nFinally, we compare our algorithm to the one proposed in [CBGZ13] (GOB.LIN); see Figure 4(d). Although this algorithm is a centralized and developed for a different setting (namely, for linear contextual bandits), it can be adapted to our setting by assuming that individuals are cooperative instead of selfish. Despite the centralized nature of GOB.LIN, our algorithm outperforms its regret."}, {"heading": "6 Conclusion & Future Work", "text": "In this paper, we consider a model for social learning that puts the problem in the the bandit framework This model allows the problem to be analyzed both in the stochastic and adversarial bandit setting, and we provide algorithms for both cases. The regret of our algorithms interpolates between the regret of the traditional bandit setting (e.g., when an individual has no neighbors) and the regret of the full information setting (e.g., when the number of neighbors goes to infinity), and are optimal up to log factors. We show, both theoretically and empirically, that we outperform state-of-the-art bandit algorithms that one could also apply to this setting, and illustrate how our approach could also lead to centralized algorithms of interest.\nWith respect to improvements to the social learning model, relaxing assumption (3) would be ideal. As we have shown (see Proposition 4.5), removing it entirely results in strictly weaker regret bounds. Would an alternate relaxed assumption suffice? Lastly, it remains to formally study the effect of arbitrary network topologies on the regret, both for the individual (based on their position in the network) and on average."}, {"heading": "A Adversarial Bandits", "text": "A.1 Regret bound for Equation 4 We first state and prove a slightly simpler regret upper bound that is more intuitive, and then show how to enhance the proof to give Theorem 4.1 in Appendix A.2. In this regret bound the algorithm is as described in the main body of the paper, and we use a fixed (as opposed to adaptive) parameters \u03b4 and \u03b7.\nTheorem A.1. Given an agent with b neighbors who are playing arbitrarily, the regret of an agent using the EXPN algorithm is R \u2208 O( \u221a \u03b2T lnK)\nwhere \u0398 = \u03a0bi=1 ( 1\u2212 \u03b5i\nK\n) and \u03b2 =\n1 1\u2212 ( 1\u2212 1K ) \u0398 + 1.\nfor an optimal choice of \u03b7 and \u03b4 that depends on \u0398.\nMore precisely, we show that\nR \u2264  2 \u221a \u03b2T lnK if \u0398 \u2264 1\u2212 \u221a lnK \u03b2T , for \u03b7 = 0, \u03b4 = \u221a lnK \u03b2T , \u221a \u03b2T lnK + \u221a (\u03b2 + K\u0398 )T lnK if 1\u2212 \u221a lnK \u03b2T \u2264 \u0398 \u2264 1\u2212 \u221a lnK (\u03b2+K\u0398 )T , for \u03b7 = 0, \u03b4 = 1\u2212\u0398, 2 \u221a( \u03b2 + K\u0398 ) T lnK if 1\u2212 \u221a lnK (\u03b2+K\u0398 )T \u2264 \u0398 for \u03b7 = K\u0398 (\u221a\nlnK (\u03b2+K\u0398 )T\n+ (\u0398\u2212 1) ) , \u03b4 = \u221a lnK\n(\u03b2+K\u0398 )T .\nWe can now directly reinterpret this regret (REXPN) as a function of the bandit regret (REXP3) and full information information regret (RMUA) as in Equation 4. In particular, note that when \u0398 = 1, none of our neighbors maintain a probability distribution that is bounded away from 0 for all arms. Hence, our neighbors are effectively not exploring. In this case, \u03b2 = K + 1 and REXPN \u2208 O( \u221a TK lnK), the same as in the classical bandit setting. On the other hand,\nwhen \u0398 \u2264 1/2, then \u03b2 \u2264 3 and hence REXPN \u2208 O( \u221a T lnK), the same as in the full-information setting. Hence, this algorithm smoothly interpolates between the bandit regret to full information regret as a function of the neighbor\u2019s exploration.\nProof. First, note that E[g\u0302j(t)] = ( 1\u2212 p\u2032j(t) ) \u00b70+p\u2032j(t)\ngj(t) p\u2032j(t) = gj(t); hence we are indeed using an unbiased estimator\nfor the rewards. From the definition of wj(t) (see Section 4.3), we see that\nln WT+1 W0 \u2265 ln wj(T + 1) W0 \u2265 \u03b4 T\u2211 t=1 g\u0302j(t)\u2212 lnK. (16)\nMoreover, using the definition of pj(t) (see Section 4.3),\nWt+1 Wt = K\u2211 j=1 wj(t) Wt e\u03b4g\u0302j(t) = K\u2211 j=1 ( pj(t)\u2212 \u03b7K 1\u2212 \u03b7 ) e\u03b4g\u0302j(t).\nSince the algorithm selects a \u03b4 such that \u03b4g\u0302j(t) < 1, we can use the inequality ez \u2264 1 + z + z2, which holds for all z \u2264 1. Therefore,\nWt+1 Wt \u2264 K\u2211 j=1 ( pj(t)\u2212 \u03b7K 1\u2212 \u03b7 ) (1 + \u03b4g\u0302j(t) + (\u03b4g\u0302j(t)) 2)\n\u2264 1 + \u03b4 1\u2212 \u03b7 K\u2211 j=1 pj(t)g\u0302j + \u03b42 1\u2212 \u03b7 K\u2211 j=1 pj(t)g\u0302j 2.\n(17)\nTaking logarithms of Equation (17) and using the inequality ln(1 + x) < x which holds for all x > 0, we get\nln Wt+1 Wt \u2264 \u03b4 1\u2212 \u03b7 K\u2211 j=1 pj(t)g\u0302j + \u03b42 1\u2212 \u03b7 K\u2211 j=1 pj(t)g\u0302j 2. (18)\nSince\nln WT+1 W0 = T\u2211 t=0 ln Wt+1 Wt ,\ncombining Equations (16) and (18) with above and noting that g\u0302i(0) = 0 we have that\n\u03b4 T\u2211 t=1 g\u0302j(t)\u2212 lnK \u2264 \u03b4 1\u2212 \u03b7 T\u2211 t=1 K\u2211 j=1 pj(t)g\u0302j(t) + \u03b42 1\u2212 \u03b7 T\u2211 t=1 K\u2211 j=1 pj(t)(g\u0302j(t)) 2. (19)\nNow, we note that given pj(t) at time t we have\nE [g\u0302j(t)] = gj(t), (20a)\nE [ T\u2211 t=1 K\u2211 i=1 pj(t)g\u0302j(t) ] = T\u2211 t=1 K\u2211 j=1 pj(t)gj(t) = E [ T\u2211 t=1 ga(t)(t) ] , and (20b)\nE  T\u2211 t=1 K\u2211 j=1 pj(t)g\u0302 2 j (t)  = T\u2211 t=1 K\u2211 j=1 pj(t) p\u2032j(t) g2j (t). (20c)\nWhere the expectation is over randomness of the algorithm. We will upper bound Equation (49c). Recall that gj(t) \u2264 1 for all j, t. Hence, for all t,\nK\u2211 j=1 pj(t) p\u2032j(t) g2j (t) \u2264 max  K\u2211 j=1 pj(t) p\u2032j(t)  . Where the maximization is over the space of valid probabilities for actions. This is upper bounded by Lemma A.2\n\u03b2 def =\n1\n1\u2212 (1\u2212 1K )\u0398 + 1.\nSince Equation (19) holds for all j, by (49), and Lemma A.2, we get\n\u03b4max j [ T\u2211 t=1 gj(t) ] \u2212 lnK \u2264 \u03b4 1\u2212 \u03b7 E [ T\u2211 t=1 ga(t)(t) ] + \u03b42 1\u2212 \u03b7 \u03b2T.\nSince gi(t) \u2264 1, we rearrange to get the following upper bound on the regret of our algorithm\nR \u2264 lnK \u03b4 + \u03b7T + \u03b4\u03b2T.\nWhat remains is then an optimization problem in \u03b4 and \u03b7 which is subject to the following two constraints:\n\u03b7 \u2208 [0, 1] and \u03b4 1\u2212 (1\u2212 \u03b7K )\u0398 \u2208 [0, 1]. (22)\nIn this optimization problem the only assumption made on the algorithm of agents is that they are select arms randomly and that the probability of selecting an arm has a minimum value \u03b5.\nmin \u03b4,\u03b7\nf(\u03b4, \u03b7) = lnK\n\u03b4 + \u03b7 \u00b7 T + \u03b4\u03b2T\nSubject to g1(\u03b4, \u03b7) \u2264 0, g2(\u03b4, \u03b7) \u2264 0,\n(23)\nwhere g1(\u03b4, \u03b7) = \u2212\u03b7,\ng2(\u03b4, \u03b7) = \u03b4\n1\u2212\u0398(1\u2212 \u03b7N ) \u2212 1,\n(24)\nand \u0398 = \u03a0bi=2(1\u2212\n\u03b7i K ),\n\u03b2 = 1\n1\u2212 (1\u2212 1K )\u0398 + 1,\n(25)\nIf \u03b4? and \u03b7? is a local minimum that satisfies Karush-Kuhn-Tucker (KKT) conditions(see below). Stationary:\n\u2212\u2207f(\u03b4?, \u03b7?) = \u00b51 \u00b7 \u2207g1(\u03b4?, \u03b7?) + \u00b52 \u00b7 \u2207g2(\u03b4?, \u03b7?), (26)\nPrimal feasibility: g1(\u03b4\n?, \u03b7?) \u2264 0 g2(\u03b4 ?, \u03b7?) \u2264 0, (27)\nDual feasibility: \u00b51 \u2265 0 \u00b52 \u2265 0\n(28)\nComplementary slackness: \u00b51 \u00b7 g1(\u03b4?, \u03b7?) = 0 \u00b52 \u00b7 g2(\u03b4?, \u03b7?) = 0\n(29)\nIn each step we will assume that some of these constraints are active (i.e.,gi(\u03b4, \u03b7) = 0), and find the points that satisfy the KKT conditions, \u2212\u2207f(\u03b4, \u03b7) = (\nlnK \u03b42 \u2212 T\u03b2 \u2212T\n) (30)\nFirst, let us assume that only first constraint is active,\ng1(\u03b4, \u03b7) = 0.\nWhich yields that \u03b7? = 0.\nThe only stationary point in this case is\n\u03b4? =\n\u221a lnK\n\u03b2T . (31)\nAnd the \u0398 that primal and dual feasibility holds for is\n\u0398 \u2264 1\u2212\n\u221a lnK\n\u03b2T . (32)\nSince \u03b4? and \u03b7? satisfy KKT conditions they are valid answers for this interval. The regret is\nR \u2264 \u221a \u03b2 lnKT. (33)\nSecond, let us assume that only the second constraint is active. In this case the stationary point is\n\u03b4? = 1\u2212\u0398(1\u2212 \u03b7 K )\n\u03b7? = K\n\u0398\n\u221a lnK\n(\u03b2 +K/\u0398)T + K(\u0398 \u2212 1) \u0398 .\n(34)\nThe interval that this answer is valid for is as follows, \u0398 \u2265 1\u2212 \u221a lnK\n(\u03b2 + K\u0398 )T . (35)\nThe regret is\nR \u2264 2 \u221a (\u03b2 + K\n\u0398 )T lnK + K(\u0398 \u2212 1)T \u0398 . (36)\nFinally, let us assume that all of the constraints are active. In this case the stationary point is\n\u03b4? = 1\u2212\u0398 \u03b7? = 0 .\n(37)\nAnd the interval that this answer is valid is as follows,\n\u0398 \u2265 1\u2212\n\u221a lnK\n\u03b2T ,\n\u0398 \u2264 1\u2212 \u221a lnK\n(\u03b2 + K\u0398 )T .\n(38)\nThe regret is\nR \u2264 lnK 1\u2212\u0398 + (1\u2212\u0398)\u03b2T . (39)\nAfter solving the inequalities (32), (35) and (38) we get the desired bound for the regret.\nLemma A.2. \u2211K j=1 pj(t) p\u2032j(t) \u2264 1 1\u2212(1\u2212 1K )\u0398 + 1 def = \u03b2 for all t.\nProof of Lemma A.2. By definition (see Equations 5 and 6),\nS def = K\u2211 j=1 pj(t) p\u2032j(t) = K\u2211 j=1\npj(t)\n1\u2212 (1\u2212 pj(t)) \u00b7 \u00b7 \u00b7 (1\u2212 qbj(t)) .\nSince qij \u2265 \u03b5i/K, clearly\nS \u2264 K\u2211 j=1\npj(t)\n1\u2212 (1\u2212 pj(t))\u0398 .\nNotice that the right-hand side is strictly concave with respect to each pi. Thus, the maximum is achieved either on the boundary of the feasible region of p, or at a single point in the interior. Recall that pi \u2208 [\u03b7/K, 1\u2212 (K \u2212 1) \u03b7K ], and that p is a probability distribution. Hence, boundary points are of the form \u03b7/K for all except one entry, which is 1\u2212 (K \u2212 1) \u03b7K . If p is of this form, then\nS \u2264 (K \u2212 1) K \u00b7 \u03b7 1\u2212 (1\u2212 \u03b7K )\u0398 + 1\u2212 (K\u22121)\u03b7K 1\u2212 (K\u22121)\u03b7\u0398K \u2264 \u03b7 1\u2212 (1\u2212 \u03b7K )\u0398 + 1. (40)\nIf, instead, the maximum is achieved on the interior, it must be symmetric, i.e., the probability of playing all actions is 1/K; otherwise, since the equation is symmetric, there would be more than one maximal distribution which contradicts strict concavity. If p is of this form, then\nS \u2264 1 1\u2212 (1\u2212 1K )\u0398 . (41)\nSince \u03b7 \u2208 [0, 1], we can upper bound both the right-hand sides of Equations 40 and 41 to attain the desired bound:\nK\u2211 i=1 pi(t) p\u2032i(t) \u2264 1 1\u2212 (1\u2212 1K )\u0398 + 1.\nA.2 Proof of Main Theorem 4.1 For this result, we use the same unbiased estimator as presented in the main body of the paper, but take a different approach in the analysis. Instead of having an explicit exploration parameter \u03b7 > 0, we instead will allow the update parameter \u03b4t to vary with time t (in fact, in this case \u03b7 = 0). This sort of analysis is common (see, e.g., [BCB12] and [KNVM14]), in particular in settings where T is unknown. In our case the adaptive \u03b4t functions to incorporate information from neighbors as we see it.\nProof of Main Theorem 4.1. The first part of proof (from Equation (42) to Equation (48)) parallels the format of the proof of Theorem 3.1 in [BCB12] for the usual bandit setting. We first write the E[l\u0302j(t)] as Equation (42), then we upper bound the first term and lower bound the second term, which leads to Equation (48).\nE[l\u0302j(t)] = 1\n\u03b4\n( lnE [ exp ( \u2212\u03b4 ( l\u0302j(t)\u2212 E[l\u0302j(t)] ))] \u2212 lnE [ exp ( \u2212\u03b4l\u0302j(t) )]) . (42)\nwhere the expectation is over the randomness of the estimator and choice of the arm.\nWe now diverge from the usual proof template: In the next step we find an upper bound for the first term in right-hand side of above equation.\n1 \u03b4 lnE\n[ exp ( \u2212\u03b4(l\u0302j(t)\u2212 E[l\u0302j(t)]) )] = 1\n\u03b4 lnE\n[ exp(\u2212\u03b4l\u0302j(t) ] + E[l\u0302j(t)])\n\u2264 1 \u03b4 E[exp(\u2212\u03b4l\u0302j(t))\u2212 1 + \u03b4l\u0302j(t)] \u2264 \u03b4 2 E[l\u03022j (t)],\n(43)\nwhere in the second inequality we use lnx \u2264 x \u2212 1 and in the last inequality we use exp(\u2212x) \u2212 1 + x \u2264 x2/2 for x \u2265 0. Defining L\u0302j(t) = \u2211t a=1 l\u0302j(t) we have\n\u22121 \u03b4 lnEat\u223cp\u2032tEj\u223cp\u2032t exp(\u2212\u03b4l\u0302j(t)) \u2264 \u2212 1 \u03b4 Eat\u223cp\u2032t lnEj\u223cp\u2032t exp(\u2212\u03b4l\u0302j(t))\n= \u22121 \u03b4 Eat\u223cp\u2032t ln K\u2211 j=1 p\u2032j(t) exp(\u2212\u03b4l\u0302j(t))\n= \u22121 \u03b4 Eat\u223cp\u2032t ln  K\u2211 j=1 (1\u2212 \u03b7) exp(\u2212\u03b4L\u0302j(t))\u2211K c=1 exp(\u2212\u03b4L\u0302c(t\u2212 1)) + \u03b7 K exp(\u2212\u03b4l\u0302j(t))  \u2264 \u22121\n\u03b4 Eat\u223cp\u2032t  \u03b7 K K\u2211 j=1 (\u2212\u03b4l\u0302j(t)) + (1\u2212 \u03b7) ln  K\u2211 j=1 exp(\u2212\u03b4L\u0302j(t))\u2211K c=1 exp(\u2212\u03b4L\u0302c(t\u2212 1))  ,\n(44)\nwhere in the first and last inequality we used the Jensen\u2019s inequality. Now we want to upper bound the term \u2212 1\u03b4 ln (\u2211K i=j exp(\u2212\u03b4L\u0302j(t))\u2211K c=1 exp(\u2212\u03b4L\u0302c(t\u22121)) ) in the above inequality.\n\u22121 \u03b4 ln  K\u2211 j=1 exp(\u2212\u03b4L\u0302j(t))\u2211K c=1 exp(\u2212\u03b4L\u0302c(t\u2212 1))  = \u03c8(t\u2212 1)\u2212 \u03c8(t), (45) where \u03c8(t) = 1\u03b4 ln ( 1 K \u2211K c=1 exp(\u2212\u03b4L\u0302c(t)) ) . By summing up these terms in Equation (42), (44), and (45) we get\nT\u2211 t=1 E[l\u0302j(t)] \u2264 T\u2211 t=1 \u03b4 2 E[l\u03022j (t)] + T\u2211 t=1 \u03b7 K Eat\u223cp\u2032t K\u2211 c=1 l\u0302c(t)\u2212 Eat\u223cp\u2032t\u03c8(T ). (46)\nBy bounding \u2212\u03c8(T ) we are able to find an upper bound on the regret.\n\u2212\u03c8(T ) = lnK \u03b4 \u2212 1 \u03b4 ln  K\u2211 j=1 exp(\u2212\u03b4L\u0302j(T ))  \u2264 lnK\n\u03b4 \u2212 1 \u03b4\nln ( exp(\u2212\u03b4L\u0302k(T )) )\n= lnK\n\u03b4 + L\u0302k(T ).\n(47)\nPlugging this in the above inequality yields\nT\u2211 t=1 E[l\u0302j(t)] \u2264 T\u2211 t=1 \u03b4 2 E[l\u03022j (t)] + T\u2211 t=1 \u03b7 K Eat\u223cp\u2032t K\u2211 c=1 l\u0302c(t) + lnK \u03b4 + Eat\u223cp\u2032t [L\u0302k(T )]. (48)\nIn fact, having more neighbors help us to get better bounds for E[l\u03022j (t)]. Now, we note that given pj(t) at time t we have\nE [g\u0302i(t)] = gi(t), (49a)\nE [ T\u2211 t=1 K\u2211 i=1 pi(t)g\u0302i(t) ] = T\u2211 t=1 K\u2211 i=1 pi(t)gi(t) = E [ T\u2211 t=1 ga(t)(t) ] , and (49b)\nE [ T\u2211 t=1 K\u2211 i=1 pi(t)g\u0302 2 i (t) ] = T\u2211 t=1 K\u2211 i=1 pi(t) p\u2032i(t) g2i (t). (49c)\nWhere the expectation is over randomness of the algorithm. Note that gi(t) is less than 1, as a result we have,\nT\u2211 t=1 K\u2211 i=1 pi(t) p\u2032i(t) g2i (t) \u2264 T\u2211 t=1 K\u2211 i=1 pi(t) p\u2032i(t) .\nSetting \u03b7 = 0 and using the Lemma A.4 we have\nR \u2264 lnK \u03b4T + 1 2 T\u2211 t=1 \u03b4t(1 + \u03b3t). (50)\nTo conclude the proof, let \u03b4t = \u221a\nlnK\u2211t c=1(1+\u03b3t)\nand use Lemma 3.5 of [ACBG02], which gives an upper bound on the regret to get the expected regret, simply take expectation of the both sides of Equation (50).\nLemma A.3. For pi \u2208 [0, 1] we want to show that \u03a0bi=1(1\u2212 pi) \u2264 11+\u2211bi=1 pi . Proof. The following formula holds for all positive pi,\n\u03a0bi=1(1 + p i) \u2265 1 + b\u2211 i=1 pi, (51)\nwhich implies 1\n\u03a0bi=1(1 + p i) \u2264 1 1 + \u2211b i=1 p i . (52)\nAs we have \u03a0bi=1(1\u2212 pi) \u00b7\u03a0bi=1(1 + pi) = \u03a0bi=1(1\u2212 (pi)2) \u2264 1,\n(53)\nWe can conclude \u03a0bi=1(1\u2212 pi) \u2264\n1\n\u03a0bi=1(1 + p i) . (54)\nFrom (52) and (54) we get\n\u03a0bi=1(1\u2212 pi) \u2264 1\n\u03a0bi=1(1 + p i)\n\u2264 1 1 + \u2211b i=1 p i .\n(55)\nLemma A.4. \u2211K j=1\npj 1\u2212(1\u2212pj)(1\u2212q1j )\u00b7\u00b7\u00b7(1\u2212qbj )\n\u2264 \u2211K j=1\npj pj+q1j+q 2 j+\u00b7\u00b7\u00b7+qbj + 1.\nProof. From Lemma A.3 we have\n(1\u2212 pj)(1\u2212 q1j ) \u00b7 \u00b7 \u00b7 (1\u2212 qbj) \u2264 1\n1 + pj + q1j + q 2 j + \u00b7 \u00b7 \u00b7+ qbj\n, (56)\nsubstituting this bound in the statement of lemma yields\nK\u2211 j=1 pj 1\u2212 (1\u2212 pj)(1\u2212 q1j ) \u00b7 \u00b7 \u00b7 (1\u2212 qbj) \u2264 K\u2211 j=1\npj\n1\u2212 1 1+pj+q1j+q 2 j+\u00b7\u00b7\u00b7+qbj\n\u2264 K\u2211 j=1 pj pj + q1j + q 2 j + \u00b7 \u00b7 \u00b7+ qbj + 1,\n(57)\nA.3 Proof of Regret Lower Bound Proof of Theorem 4.2. This proof follows, with some adjustments, from the proof of Theorem 5.1 in [ACBFS03]; we simply point out the key differences here. In this proof a random distribution of rewards is constructed, one of the actions is chosen uniformly at random to be the \u201cgood\u201d action, which is 1 with probability 1/2 + and 0 otherwise for some small fixed \u2208 (0, 1/2]. The remaining actions are 0 or 1 with probability 1/2.\nIn Equation (30) of [ACBFS03], where the relative entropy between two Bernoulli random variables with parameters P and Q is calculated, the total number of past observation is T . In our case, it is between \u2211T t=1 nt and T + \u2211T t=1 nt where nt is the size of the set of arms selected (arbitrarily) by all of the individual?s neighbors at time t. Note that, stopping here, this would lead directly to the proof of Theorem 4.5. Here, we dig into the proof to break up the regret in a way that allows us to write the regret with respect to our \u03b3t, which will result in a tighter bound.\nMore precisely, let r denote the entire sequence of rewards, let f is an arbitrary function on such histories r to a fixed range [0, T ], and let Ni be a random variable denoting the number of times an algorithm selects arm i. We let Ei be the expectation taken conditioned on i being the good action, and Eunif denotes the expectation taken over a uniformly random choice of rewards for all actions (including the good action). Then, Lemma A.1 in [ACBFS03] becomes\nEi[f(r)] \u2264 Eunif [f(r)] + T\n2\n\u221a \u2212(Eunif [Ni]) ln(1\u2212 4 2),\nwhen f(r) is the number of times that we choose arm i. Note that, for our algorithm, Eunif [Ni] = \u2211T t=1 1 \u2212 (1 \u2212\npi(t))(1\u2212 q1i (t)) \u00b7 \u00b7 \u00b7 (1\u2212 qbi (t)). Using the lemma, if we set = c \u221a K\u2211K i=1 Eunif [Ni] , and continue to follow the proof of\nTheorem 5.1 in [ACBFS03]. By observing that \u2126 ( T \u221a\nK\u2211K i=1 Eunif [Ni]\n) = \u2126 ( T + \u2211T t=1 \u03b3t ) , we recover the desired\nlower bound.\nA.4 Comparison to Arm-Network Algorithms We compare our algorithm to arm-network algorithms which, though developed for a different setting, could be applied to ours. These algorithms are given for a multi-armed-bandit (MAB) with side observations encoded as an arm-network. In these papers, authors assume the arms form a arm-network Gt (the arm-network can change over time). The arm-network is a directed graph, whose vertices represent the arms, and an edge from arm i to arm j means that by choosing arm i we observe the reward of arm j (see Figure 2). First, let recall our interpretation of our problem as a multi-armed bandit problem with an arm-network (see Figure 2). The players choose arms according to their algorithm, and after selecting and revealing the rewards. Each player i individually can construct a arm-network. If we show the arms selected by neighbors of player i by A[N(i)], in arm-network there is an edge from all arms (vertices) to A[N(i)]. In addition, if we denote the cardinality of set A[N(i)] by Ci, the independence number \u03b1 of the underlying graph is K + 1\u2212 Ci.\nWe show that our algorithm\u2019s regret is at most that of EXP3G, the state-of-the-art algorithm for the arm-network setting. From Theorem 4.1, we know that that the regret of our algorithm is O\u0303 (\u221a T + \u2211T t=1 \u03b3t ) and from [ACBDK15]\nthe regret of EXP3G is O\u0303 (\u221a\u2211T\nt=1 \u03b1t\n) .\nWith some abuse of notation, let A be the arms chosen by neighbors of 0 let C be its cardinality, and let 1{j \u2208 A} be an indicator random variable that is 1 if and only if j is in A. Lastly, let 1{j = a`t} be an indicator random variable that is 1 if and only if player ` (one of the neighbors) chooses arm j at round t. First, we lower bound \u03b1 using the indicator random variable defined above, then we show that in expectation this term is greater than \u03b3t.\nLemma A.5. The independence number \u03b1 is lower bounded as follows\n\u03b1t \u2265 K\u2211 j=1\npj(t) pj(t) + \u2211b `=1 1{j = a`t} . (58)\nProof. First, we show the following\n\u03b1t \u2265 K\u2211 j=1\npj(t)\npj(t) + 1{j \u2208 A} . (59)\nDecomposing the sum we have\nK\u2211 j=1\npj(t) pj(t) + 1{j \u2208 A} = \u2211 j\u2208A pj(t) pj(t) + 1 + \u2211 j /\u2208A pj(t) pj(t) , (60)\nplugging the cardinality of A we get\n= \u2211 j\u2208A pj(t) pj(t) + 1 +K \u2212 C, (61)\nwe know \u03b1 = 1 +K \u2212 C (this comes from the fact that arms in A are connected to all arms, see Figure 2), knowing that \u2211 j\u2208A pj(t) pj(t)+1 is less than 1 completes the first part of lemma.\nSecond, we have 1{j \u2208 A} \u2264 \u2211b `=1 1{j = a`t}, which yields the lemma.\nIn the Lemma A.5, the term 1{j = a`t} can be seen as an unbiased estimator for q`j(t) (we denote it by q\u0302`j(t)). As a final step, we show the following lemma.\nLemma A.6. For a multinomial distribution q and their unbiased estimator q\u0302`j(t) = 1{j = a`t}, we have\u221a\u221a\u221a\u221a T\u2211 t=1 (1 + \u03b3t) \u2264 Eq\u0302\u223cq  \u221a\u221a\u221a\u221a2 T\u2211 t=1 \u03b1t  (62) Proof. Because \u03b1t \u2265 1, we know \u2211T t=1 \u03b1t \u2265 T . As a result by showing \u221a\u2211T t=1(\u03b3t) \u2264 Eq\u0302\u223cq [\u221a\u2211T t=1 \u03b1t ] , we can\nconclude \u221a T + \u2211T t=1(\u03b3t) \u2264 Eq\u0302\u223cq [\u221a 2 \u2211T t=1 \u03b1t ] .\nFrom Lemma A.5, we have\nEq\u0302\u223cq\n \u221a\u221a\u221a\u221a T\u2211\nt=1\n\u03b1t  \u2265 Eq\u0302\u223cq \u221a\u221a\u221a\u221a T\u2211\nt=1 K\u2211 j=1\npj(t) pj(t) + \u2211b `=1 q\u0302 ` j(t)\n . (63)\nIn the next step, we want to show\nEq\u0302\u223cq \u221a\u221a\u221a\u221a T\u2211 t=1 K\u2211 j=1\npj(t) pj(t) + \u2211b `=1 q\u0302 ` j(t)\n \u2265 \u221a\u221a\u221a\u221a T\u2211\nt=1\n\u03b3t.\nLet \u03c6(q\u0302) = \u03c6(q\u03021(1), q\u03022(1), \u00b7 \u00b7 \u00b7 , q\u0302b(1), q\u03021(2), \u00b7 \u00b7 \u00b7 , q\u0302b(T )) = \u221a\u2211T\nt=1 \u2211K j=1\npj(t) pj(t)+ \u2211b `=1 q\u0302 ` j(t) . This function is convex\n(it is convex along every arbitrary line with positive entires, so it is convex), we can use Jensen\u2019s inequality to swap the order of expectation and \u03c6 to get the following.\nEq\u0302\u223cq [\u03c6(q\u0302)] \u2265 \u03c6 (Eq\u0302\u223cq [q\u0302]) = \u03c6 (q) = \u221a\u221a\u221a\u221a T\u2211 t=1 \u03b3t. (64)\nThe first inequality is Jensen\u2019s inequality and the last equality comes from definition of \u03b3t."}, {"heading": "B Stochastic Bandits", "text": "In this section, we first formally state and prove the results for the stochatic setting. Let us first recall our result:\nTheorem B.1 (Theorem 3.1). Consider an agent with neighbors who play arbitrarily. Let n\u2032i(t) be the number times arm i has been selected by one of her neighbors by time t. Then, the regret of UCBN for any \u03b1 > 2 is\nR \u2264 \u2211\ni,\u2206i>0\n( max { max t=1,..,T { 2\u03b1 ln t \u2206i \u2212 n\u2032i(t)\u2206i } , 0 } + \u03b1 \u03b1\u2212 2 ) , (65)\nwhere \u2206i is the difference between \u00b5i? and \u00b5i.\nCorollary B.2. On a complete graph with b nodes, if all agents use UCBN then under the same conditions as in Theorem 3.1, the regret of an agent is\nR \u2264 \u2211\ni,\u2206i>0\n( 2\u03b1 lnT\nb\u2206i +\n\u03b1\n\u03b1\u2212 2\n) \u2208 O ( K lnT\nb\n) . (66)\nThe following lower bound yields same behavior for getting free observation from neighbors.\nTheorem B.3. Consider a strategy that satisfies E[ni(T )] = o(T a), any arm i with \u2206i > 0, and any a > 0. Let ct(i) be the number of times arm i selected (arbitrarily) by all of the agent\u2019s neighbors up to time t, then, for any set of Bernoulli reward distributions the following inequality holds\nlim T\u2212\u2192+\u221e\ninf R\nlnT \u2265 \u2211 i,\u2206i>0 1 2\u2206i \u2212 lim T\u2212\u2192+\u221e inf\n\u2211 i,\u2206i>0 cT (i)\u2206i\nlnT . (67)\nOur proofs parallel, with additional bookkeeping, the proofs for the original UCB results (see, e.g., [BCB12] for a template). Note that the results for stochastic bandits hold when the reward distributions satisfy the following standard conditions.\nDefinition B.1 (Conditions on Fi). Every reward distribution Fi satisfies Hoefding\u2019s lemma, i.e., there exists a convex function \u03c8 on the reals such that, for all \u03bb \u2265 0, we have ln [ E [ e\u03bb|X\u2212E[X]| ]] \u2264 \u03c8(\u03bb) where X \u223c Fi.\nFor example, when X \u2208 [0, 1], one can take \u03c8(\u03bb) = \u03bb 2\n8 ; indeed the results in the main body of the paper take this \u03c8. The results can be easily generalized for other \u03c8 in the usual manner.\nWe first prove a lemma that will be of assistance in the proof of Theorem 3.1. Recall that a(t) is the arm the agent selects at time t.\nLemma B.4. If a(t) = i, at least one of the three following inequalities is true:\n\u00b5\u0302i?,ni? (t\u22121) +\n\u221a \u03b1 ln t\n2ni?(t\u2212 1) \u2264 \u00b5?, (68a)\n\u00b5\u0302i,ni(t\u22121) > \u00b5i +\n\u221a \u03b1 ln t\n2ni(t\u2212 1) , (68b)\nni(t\u2212 1) < 2\u03b1 ln t\n42i . (68c)\nProof. We prove the contrapositive. Assume that a(t) = i and that none of the inequalities (68a), (68b) or (68c) are true.\n\u00b5\u0302i?,ni? (t\u22121) +\n\u221a \u03b1 ln t\n2ni?(t\u2212 1) > \u00b5?, (69a)\n\u00b5\u0302i,ni(t\u22121) < \u00b5i +\n\u221a \u03b1 ln t\n2ni(t\u2212 1) , (69b)\nni(t\u2212 1) > 2\u03b1 ln t\n42i . (69c)\nBy plugging \u00b5? = \u00b5i +4i in Equation (69a) we obtain\n\u00b5\u0302i?,ni? (t\u22121) +\n\u221a \u03b1 ln t\n2ni?(t\u2212 1) > \u00b5i +4i. (70)\nFrom Equation (69c) we have\n\u00b5i +4i > \u00b5i +\n\u221a 2\u03b1 ln t\nni(t\u2212 1) (71)\nand plugging (71) in (70) yields\n\u00b5\u0302i?,ni? (t\u22121) +\n\u221a \u03b1 ln t\n2ni?(t\u2212 1) > \u00b5\u0302i,ni(t\u22121) +\n\u221a \u03b1 ln t\n2ni(t\u2212 1) . (72)\nBy our criteria for selecting arms given by Equation (1), this implies a(t) 6= i.\nProof of Theorem 3.1 . Let ni(t) = n 1 i (t) + n \u2032 i(t), (73) where n1i (t) is the number of times the agent selects the arm i and n \u2032 i(t) is the number of times her neighbors select arm i. Using Lemma B.4, we will first find an upper bound for n1i (t) for a suboptimal arm i. Lemma B.4 states that at least one of the three inequalities (68a), (68b) and (68c) must be true. If Equation (68c) holds, then from (73) we obtain\nn1i (t) \u2264 2\u03b1\n42i ln t\u2212 n\u2032i(t). (74)\nLet U be the maximum of right hand side of (74) for t = 1, .., T :\nU = max { max t=1,..,T { 2\u03b1 42i ln t\u2212 n\u2032i(t) } 4i, 0 } , (75)\nas a result if the Equation (68c) holds for some instance k, then n1i (k) is bounded by U , i.e.,\nn1i (k) \u2264 U. (76)\nFor bounding the regret we find an upper bound on the number of times we select a suboptimal arm i:\nE[n1i (T )] = E [ T\u2211 t=1 1a(t)=i ] = E [ U\u2211 t=1 1a(t)=i ] + E [ T\u2211 t=U+1 1a(t)=i ] . (77)\nSince E [\u2211U\nt=1 1a(t)=i\n] \u2264 U , we can deduce\nE[n1i (T )] \u2264 U + E\n[ T\u2211\nt=U+1\n1a(t)=i\n] , (78)\nas we saw in (B.4), 1{a(t)=i} = 1 requires that at least one of the three equations (68a), (68b) and (68c) is true. Assume the last time that (68c) is true is at time \u03b6, hence\nn1i (\u03b6) \u2264 2\u03b1\n42i ln \u03b6 \u2212 n\u2032i(\u03b6), (79)\nand since \u03b6 is the last time that (68c) holds, we can upper bound E[n1i (T )] by\nE[n1i (T )] \u2264 2\u03b1\n42i ln \u03b6 \u2212 n\u2032i(\u03b6) + E  T\u2211 t=\u03b6+1 1{(68a) or (68b) is true and (68c) is false}  . (80) According to the definition of U in (75) and Equation (80) we have\nE[n1i (T )] \u2264 U + E\n[ T\u2211\nt=U+1\n1{(68a) or (68b) is true and (68c) is false}\n] (81)\n\u2264 U + T\u2211\nt=U+1\nP[(68a) is true] + P[(68b) is true]. (82)\nIt suffices to bound the probability (68a) and (68b):\nP [(68a) is true] = \u2211 ni(t) P [(68a) is true|ni(t)] \u00b7 P [ni(t)] . (83)\nRecall that P [(68a) is true|ni(t)] \u2264 1\nt\u03b1 . (84)\nPlugging (84) in (83) yields that\nP [(68a) is true] \u2264 1 t\u03b1 \u2211 ni(t) P [ni(t)] = 1 t\u03b1 . (85)\nThen we take integral of 1t\u03b1 for t from 1 to T , which is smaller than \u03b1 2(\u03b1\u22122) . The same upper bound holds for (68b). Thus, the regret is\nR \u2264 \u2211\ni,4i>0\n( max { max t=1,..,T { 2\u03b1 42i ln t\u2212 n\u2032i(t) } 4i, 0 } + \u03b1 \u03b1\u2212 2 ) (86)\nas desired.\nB.1 UCBN on Complete Graphs In this section, we analyze the regret in a complete graph when all agents use UCBN. The following curious lemma will assist in the proof.\nLemma B.5. Given a complete graph of agents, if all agents use UCBN with a deterministic common tie breaking scheme, then in every time step all agents select the same action.\nProof. Since the graph is complete, all agents see the rewards of other agents at every time step; hence the sample means s\u0302i(t) and number of samples ni(t) at time t are the same for all agents. Furthermore, every agent selects an arm according to criteria (1) and a common deterministic tie breaking rule. Therefore, the arm selected at time t will be same for all agents.\nProof of Corollary B.2. Let\nU =\n[ 2\u03b1 lnT\nb \u00b7 42i\n] . (87)\nWe bound the number of times action i other than the best arm is selected. Following the proof of Theorem 3.1,\nE[n1i (T )] \u2264 U + T\u2211\nt=U+1\nP[(68a) is true] + P[(68b) is true]. (88)\nThe upper bound of the probabilities (68a) and (68b) are same as before. Hence, the regret bound is\nR \u2264 \u2211\ni,4i>0\n( 2\u03b1\nb \u00b7 4i lnT +\n\u03b1\n\u03b1\u2212 2\n) . (89)\nB.2 Lower Bound The lower bound for UCB [LR85] is\nlim T\u2212\u2192+\u221e\ninf R\nlnT \u2265 \u2211 i,4i>0 4i kl(\u00b5i, \u00b5?) . (90)\nOur proof again follows the same template.\nProof of Theorem B.3. As in [LR85], we assume the rewards are drawn from a Bernoulli distribution. From their proof it follows that the expected number of times that a suboptimal arm must be selected in order to distinguish between best arm and other arms is at least\nE[ni(T )] + cT (i) \u2265 (1 + o(1)) 1\u2212 \u03b5 1 + \u03b5 lnT kl(\u00b5i, \u00b5?) . (91)\nwhere the second term is the information coming from the neighbors (that is, the number of times neighbors selected arm i up to time t), \u00b5? is the mean of the best arm, and kl(p, q) is the Kullback-Leibler divergence between a Bernoulli variable with parameter p and a Bernoulli variable with parameter q, defined to be\nkl(p, q) def = p ln\n( p\nq\n) + (1\u2212 p) ln ( 1\u2212 p 1\u2212 q ) . (92)\nAs the number of rounds increases, \u03b5 can be taken to be smaller. As T goes to infinity, \u03b5 can be taken zero; as a result we can write the following lower bound for the regret\nlim T\u2212\u2192+\u221e\ninf R+\n\u2211 i,\u2206i>0 cT (i)\u2206i\nlnT \u2265 \u2211 i,4i>0 4i kl(\u00b5i, \u00b5?) . (93)\nApplying Pinkser\u2019s inequality yields the lower bound."}], "references": [{"title": "Online learning with feedback graphs: Beyond bandits", "author": ["N. Alon", "N. Cesa-Bianchi", "O. Dekel", "T. Koren"], "venue": "Conference on Learning Theory (COLT)", "citeRegEx": "ACBDK15", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, 47:235\u2013256", "citeRegEx": "ACBF02", "shortCiteRegEx": null, "year": 2002}, {"title": "The non-stochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing, 32:47\u201377", "citeRegEx": "ACBFS02", "shortCiteRegEx": null, "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM Journal on Computing", "citeRegEx": "ACBFS03", "shortCiteRegEx": null, "year": 2003}, {"title": "Journal of Computer and System Sciences", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Claudio Gentile. Adaptive", "self-confident on-line learning algorithms"], "venue": "64(1):48\u201375,", "citeRegEx": "ACBG02", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "S. Mannor", "Y. Mansour"], "venue": "Arxiv", "citeRegEx": "ACBG14", "shortCiteRegEx": null, "year": 2014}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "Proceedings of the 26th Conference on Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "ACBGM13", "shortCiteRegEx": null, "year": 2013}, {"title": "In COLT", "author": ["Shipra Agrawal", "Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem"], "venue": "pages 39\u20131,", "citeRegEx": "AG12", "shortCiteRegEx": null, "year": 2012}, {"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["S. Arora", "E. Hazan", "S. Kale"], "venue": "Theory of Computing, 8:121\u2013164", "citeRegEx": "AHK12", "shortCiteRegEx": null, "year": 2012}, {"title": "Budgeted prediction with expert advice", "author": ["Kareem Amin", "Satyen Kale", "Gerald Tesauro", "Deepak Turaga"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "AKTT15", "shortCiteRegEx": null, "year": 2015}, {"title": "Decoupling exploration and exploitation in multi-armed bandits", "author": ["O. Avener", "S. Mannor", "O. Shamir"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML)", "citeRegEx": "AMS12", "shortCiteRegEx": null, "year": 2012}, {"title": "Corruption driven by imitative behavior", "author": ["E. Accinelli", "J. S\u00e1nchez-Carrera"], "venue": "Econ. Letters, 117:84\u201387", "citeRegEx": "ASC12", "shortCiteRegEx": null, "year": 2012}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "volume 5. Foundations and Trends in Machine Learning", "citeRegEx": "BCB12", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-armed bandits in the presence of side observations in networks", "author": ["S. Buccapatnam", "A. Eryilmaz", "N.B. Shroff"], "venue": "Proceedings of the 2014 SIGMETRICS conference", "citeRegEx": "BES13", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic bandits with side observations on networks", "author": ["S. Buccapatnam", "A. Eryilmaz", "N.B. Shroff"], "venue": "Proceedings of the 52nd IEEE Conference on Decision and Control", "citeRegEx": "BES14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning from neighbours", "author": ["V. Bala", "S. Goyal"], "venue": "Review of Econ. Studies, 65:595\u2013621", "citeRegEx": "BG98", "shortCiteRegEx": null, "year": 1998}, {"title": "Conformism and diversity under social learning", "author": ["V. Bala", "S. Goyal"], "venue": "Econ. Theory, 17:101\u2013120", "citeRegEx": "BG01", "shortCiteRegEx": null, "year": 2001}, {"title": "Bandits games and clustering foundations", "author": ["S. Bubeck"], "venue": "PhD thesis, Universite Lille", "citeRegEx": "Bub10", "shortCiteRegEx": null, "year": 2010}, {"title": "A gang of bandits", "author": ["N. Cesa-Bianchi", "C. Gentile", "G. Zappella"], "venue": "Proceedings of the 26th Conference on Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "CBGZ13", "shortCiteRegEx": null, "year": 2013}, {"title": "Leveraging side observations in stochastic bandits", "author": ["S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat"], "venue": "Proceedings of Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "CKLB12", "shortCiteRegEx": null, "year": 2012}, {"title": "Rules of thumb for social learning", "author": ["G. Ellison", "D. Fudenberg"], "venue": "J. of Political Economy, 101", "citeRegEx": "EF93", "shortCiteRegEx": null, "year": 1993}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "Proceedings of the 16th ACM/SIAM symposium on Discrete algorithms (SODA)", "citeRegEx": "FKM05", "shortCiteRegEx": null, "year": 2005}, {"title": "The kl-ucb algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "Proceedings of the Conference on Learning Theory (COLT)", "citeRegEx": "GC11", "shortCiteRegEx": null, "year": 2011}, {"title": "Naive learning in social networks and the wisdom of crowds", "author": ["B. Golub", "M.O. Jackson"], "venue": "American Econ. Jourmal: MicroEcon., 2", "citeRegEx": "GJ10", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian learning in social networks", "author": ["D. Gale", "S. Kariv"], "venue": "Games and Econ. Behavior, 45", "citeRegEx": "GK03", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning in networks", "author": ["S. Goyal"], "venue": "G. Demange and M. Wooders, editors, Group formation in Econ.: networks, clubs, and coalitions, chapter 4, pages 122\u2013167. Cambridge University Press", "citeRegEx": "Goy05", "shortCiteRegEx": null, "year": 2005}, {"title": "Personal Influence", "author": ["E. Katz", "P. Lazersfeld"], "venue": "The Free Press", "citeRegEx": "KL55", "shortCiteRegEx": null, "year": 1955}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko", "R. Munos"], "venue": "Proceedings of the 27th Conference on Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "KNVM14", "shortCiteRegEx": null, "year": 2014}, {"title": "The People\u2019s Choice", "author": ["P. Lazarsfeld", "B. Berelson", "H. Gaudet"], "venue": "Columbia University Press", "citeRegEx": "LBG48", "shortCiteRegEx": null, "year": 1948}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6", "citeRegEx": "LR85", "shortCiteRegEx": null, "year": 1985}, {"title": "A finite-time analysis of multiarmed bandits problems with kullback-leibler divergences", "author": ["O.A. Maillard", "R. Munos", "G. Stoltz"], "venue": "Proceedings of the Conference on Learning Theory (COLT)", "citeRegEx": "MMS11", "shortCiteRegEx": null, "year": 2011}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), pages 684\u2013692", "citeRegEx": "MS11", "shortCiteRegEx": null, "year": 2011}, {"title": "Essays on Social Learning and Imitation", "author": ["B. Sanditov"], "venue": "PhD thesis, Universitaire Pers Maastricht", "citeRegEx": "San06", "shortCiteRegEx": null, "year": 2006}, {"title": "In 30th International Conference on Machine Learning (ICML 2013)", "author": ["Balazs Szorenyi", "R\u00f3bert Busa-Fekete", "Istv\u00e1n Heged\u00fcs", "R\u00f3bert Orm\u00e1ndi", "M\u00e1rk Jelasity", "Bal\u00e1zs K\u00e9gl. Gossip-based distributed stochastic bandit algorithms"], "venue": "volume 28, pages 19\u201327. Acm Press,", "citeRegEx": "SBFH13", "shortCiteRegEx": null, "year": 2013}, {"title": "Piecewise-stationary bandit problems with side observations", "author": ["J.Y. Yu", "S. Mannor"], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML)", "citeRegEx": "YM09", "shortCiteRegEx": null, "year": 2009}, {"title": "Individual and social learning in bio-technology adoption: The case of gm corn in the u.s", "author": ["D. Yoo"], "venue": "In Agricultural & Applied Econ. Association\u2019s Annual Meeting (AAEA),", "citeRegEx": "Yoo.,? \\Q2012\\E", "shortCiteRegEx": "Yoo.", "year": 2012}, {"title": "Expertise networks in online communities: Structures and algorithms", "author": ["J. Zhang", "M.S. Ackerman", "L. Adamic"], "venue": "Proceedings of the World Wide Web Conference (WWW)", "citeRegEx": "ZAA07", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": ", [AKTT15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": ", [ACBDK15]) an action-network model has been studied: Here, the actions form a network and the individual observes the rewards of the neighbors of the action she selects (as opposed to the rewards of the actions that her neighbors select).", "startOffset": 2, "endOffset": 11}, {"referenceID": 25, "context": "In the study of non-strategic learning on networks, individuals are connected via a network, and each individual has a finite set of actions with probabilistic rewards whose distributions depend on the state of the world (see [Goy05], Chapter 2 for a survey).", "startOffset": 226, "endOffset": 233}, {"referenceID": 34, "context": "Such models have been studied both for stochastic ([YM09]) and adversarial ([AMS12, AKTT15]) bandits.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "[ACBF02] and since widely extended and studied (see, e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": ", [BCB12] for an overview).", "startOffset": 2, "endOffset": 9}, {"referenceID": 12, "context": ", [BCB12] for a template), and can be found along with further discussion in Appendix B.", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": "1 Preliminaries The multiplicative weight update method has been discovered many times in many fields over the past century (see [AHK12] for an overview).", "startOffset": 129, "endOffset": 136}, {"referenceID": 2, "context": "This algorithm, also known as EXP3 [ACBFS02], achieves regret O( \u221a TK lnK), and is optimal up to log factors for the right choice of parameters (see [BCB12] for an exposition).", "startOffset": 35, "endOffset": 44}, {"referenceID": 12, "context": "This algorithm, also known as EXP3 [ACBFS02], achieves regret O( \u221a TK lnK), and is optimal up to log factors for the right choice of parameters (see [BCB12] for an exposition).", "startOffset": 149, "endOffset": 156}, {"referenceID": 1, "context": ", [ACBF02]), and is given in Appendix A.", "startOffset": 2, "endOffset": 10}, {"referenceID": 12, "context": "The first part of proof (from Equation (7) to Equation (12)) parallels the traditional analysis for analyzing multiplicative weight update algorithms; for completeness we present the steps without going into the details (see [BCB12] for an exposition).", "startOffset": 225, "endOffset": 232}, {"referenceID": 4, "context": "5 of [ACBG02], and take expectation of the both sides of Equation (14).", "startOffset": 5, "endOffset": 13}, {"referenceID": 0, "context": "We consider EXP3G ([ACBDK15]), which is the state-of-the-art solution for such problems, and performed best amongst arm-network algorithms in our empirical simulations.", "startOffset": 19, "endOffset": 28}, {"referenceID": 5, "context": "The proof follows from Theorem 5 of [ACBG14].", "startOffset": 36, "endOffset": 44}, {"referenceID": 9, "context": "1 using the fact that the number of neighbors is b\u2212 1 on a complete network, and that a centralized solution has average regret \u03a9( \u221a( 1 + Kb ) T ) as shown in [AKTT15].", "startOffset": 159, "endOffset": 167}, {"referenceID": 0, "context": "We compare our algorithm against the bandit algorithms developed for various settings with sideinformation, namely EXP3G ([ACBDK15]), EXP.", "startOffset": 122, "endOffset": 131}, {"referenceID": 27, "context": "IX ([KNVM14]) and BEXP ([AKTT15]).", "startOffset": 4, "endOffset": 12}, {"referenceID": 9, "context": "IX ([KNVM14]) and BEXP ([AKTT15]).", "startOffset": 24, "endOffset": 32}, {"referenceID": 18, "context": "Finally, we compare our algorithm to the one proposed in [CBGZ13] (GOB.", "startOffset": 57, "endOffset": 65}], "year": 2017, "abstractText": "An individual\u2019s decisions are often guided by those of his or her peers, i.e., neighbors in a social network. Presumably, being privy to the experiences of others aids in learning and decision making, but how much advantage does an individual gain by observing her neighbors? Such problems make appearances in sociology and economics and, in this paper, we present a novel model to capture such decision-making processes and appeal to the classical multi-armed bandit framework to analyze it. Each individual, in addition to her own actions, can observe the actions and rewards obtained by her neighbors, and can use all of this information in order to minimize her own regret. We provide algorithms for this setting, both for stochastic and adversarial bandits, and show that their regret smoothly interpolates between the regret in the classical bandit setting and that of the full-information setting as a function of the neighbors\u2019 exploration. In the stochastic setting the additional information must simply be incorporated into the usual estimation of the rewards, while in the adversarial setting this is attained by constructing a new unbiased estimator for the rewards and appropriately bounding the amount of additional information provided by the neighbors. These algorithms are optimal up to log factors; despite the fact that the agents act independently and selfishly, this implies that it is an approximate Nash equilibria for all agents to use our algorithms. Further, we show via empirical simulations that our algorithms, often significantly, outperform existing algorithms that one could apply to this setting.", "creator": "LaTeX with hyperref package"}}}