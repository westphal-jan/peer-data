{"id": "1704.05119", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Exploring Sparsity in Recurrent Neural Networks", "abstract": "large recurrent neural networks ( rnn ) are widely used to solve a variety of problems and as the quantity distributions of data and the amount of available compute have increased, so have model sizes. the number of parameters in some recent state - of - the - art networks consistently makes them hard to deploy, for especially worn on mobile phones and embedded sensor devices. half the challenge is due to both affecting the size of the model and the time frames it technically takes to evaluate it. in order to deploy these rnns efficiently, we could propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. at the end of training, the parameters observed of the network are sparse while accuracy is still close to the original shortest dense neural network. the network size is however reduced annually by 8x and the time required to again train the model remains constant. additionally, we can prune a larger dense network to achieve better yield than baseline performance while still reducing the total number of parameters significantly. simply pruning rnns reduces the size requirements of the model and can also help achieve significant inference time speed - up using sparse matrix multiply. naive benchmarks show that using our technique simple model size can initially be reduced by 90 % drastically and speed - up is around 2x to 7x.", "histories": [["v1", "Mon, 17 Apr 2017 20:42:05 GMT  (259kb,D)", "http://arxiv.org/abs/1704.05119v1", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Published as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["sharan narang", "gregory diamos", "shubho sengupta", "erich elsen"], "accepted": true, "id": "1704.05119"}, "pdf": {"name": "1704.05119.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Sharan Narang", "Greg Diamos", "Shubho Sengupta", "Erich Elsen"], "emails": ["sharan@baidu.com", "gdiamos@baidu.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (Jo\u0301zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets.\nFor example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al. (2015). And in language modeling the size of the non-embedding parameters (mostly in the recurrent layers) have exploded even as various ways of hand engineering sparsity into the embeddings have been explored in Jo\u0301zefowicz et al. (2016) and Chen et al. (2015a).\nThese large models face two significant challenges in deployment. Mobile phones and embedded devices have limited memory and storage and in some cases network bandwidth is also a concern. In addition, the evaluation of these models requires a significant amount of computation. Even in cases when the networks can be evaluated fast enough, it will still have a significant impact on battery life in mobile devices (Han et al., 2015).\nInference performance of RNNs is dominated by the memory bandwidth of the hardware, since most of the work is simply reading in the parameters at every time step. Moving from a dense calculation to a sparse one comes with a penalty, but if the sparsity factor is large enough, then the smaller amount of data required by the sparse routines becomes a win. Furthermore, this suggests that if the parameter sizes can be reduced to fit in cache or other very fast memory, then large speedups could be realized, resulting in a super-linear increase in performance.\n\u2217Now at Facebook AI Research ssengupta@fb.com \u2020Now at Google Brain eriche@google.com\nar X\niv :1\n70 4.\n05 11\n9v 1\n[ cs\n.L G\n] 1\n7 A\npr 2\n01 7\nThe more powerful server class GPUs used in data centers can generally perform inference quickly enough to serve one user, but in the data center performance per dollar is very important. Techniques that allow models to be evaluated faster enable more users to be served per GPU increasing the effective performance per dollar.\nWe propose a method to reduce the number of weights in recurrent neural networks. While the network is training we progressively set more and more weights to zero using a monotonically increasing threshold. By controlling the shape of the function that maps iteration count to threshold value, we can control how sparse the final weight matrices become. We prune all the weights of a recurrent layer; other layer types with significantly fewer parameters are not pruned. Separate threshold functions can be used for each layer, although in practice we use one threshold function per layer type. With this approach, we can achieve sparsity of 90% with a small loss in accuracy. We show this technique works with Gated Recurrent Units (GRU) (Cho et al., 2014) as well as vanilla RNNs.\nIn addition to the benefits of less storage and faster inference, this technique can also improve the accuracy over a dense baseline. By starting with a larger dense matrix than the baseline and then pruning it down, we can achieve equal or better accuracy compared to the baseline but with a much smaller number of parameters.\nThis approach can be implemented easily in current training frameworks and is agnostic to the optimization algorithm. Furthermore, training time does not increase unlike previous approaches such as in Han et al. (2015). State of the art results in speech recognition generally require days to weeks of training time, so a further 3-4\u00d7 increase in training time is undesirable."}, {"heading": "2 RELATED WORK", "text": "There have been several proposals to reduce the memory footprint of weights and activations in neural networks. One method is to use a fixed point representation to quantize weights to signed bytes and activations to unsigned bytes (Vanhoucke et al., 2011). Another technique that has been tried in the past is to learn a low rank factorization of the weight matrices. One method is to carefully construct one of the factors and learn the other (Denil et al., 2013). Inspired by this technique, a low rank approximation for the convolution layers achieves twice the speed while staying within 1% of the original model in terms of accuracy (Denton et al., 2014). The convolution layer can also be approximated by a smaller set of basis filters (Jaderberg et al., 2014). By doing this they achieve a 2.5x speedup with no loss in accuracy. Quantization techniques like k-means clustering of weights can also reduce the storage size of the models by focusing only on the fully connected layers (Gong et al., 2014). A hash function can also reduce memory footprint by tying together weights that fall in the same hash bucket (Chen et al., 2015b). This reduces the model size by a factor of 8.\nYet another approach to reduce compute and network size is through network pruning. One method is to use several bias techniques to decay weights (Hanson & Pratt, 1989). Yet another approach is to use the diagonal terms of a Hessian matrix to construct a saliency threshold and used this to drop weights that fall below a given saliency threshold (LeCun et al., 1989). In this technique, once a weight has been set to 0, the network is retrained with these weights frozen at 0. Optimal Brain Surgeon is another work in the same vein that prunes weights using the inverse of a Hessian matrix with the additional advantage of no re-training after pruning (Hassibi et al., 1993).\nBoth pruning and quantization techniques can be combined to get impressive gains on AlexNet trained on the ImageNet dataset (Han et al., 2015). In this case, pruning, quantization and subsequent Huffman encoding results in a 35x reduction in model size without affecting accuracy. There has also been some recent work to shrink model size for recurrent and LSTM networks used in automatic speech recognition (ASR) (Lu et al., 2016). By using a hybrid strategy of using Toeplitz matrices for the bottom layer and shared low-rank factors on the top layers, they were able to reduce the parameters of a LSTM by 75% while incurring a 0.3% increase in word error rate (WER).\nOur method is a pruning technique that is computationally efficient for large recurrent networks that have become the norm for automatic speech recognition. Unlike the methods that need to approximate a Hessian (LeCun et al., 1989; Hassibi et al., 1993) our method uses a simple heuristic to choose the threshold used to drop weights. Yet another advantage, when compared to methods that need re-training (Han et al., 2015), is that our pruning technique is part of training and needs\nno additional re-training. Even though our technique requires judicious choice of pruning hyperparameters, we feel that it is easier than choosing the structure of matrices to guide the sparsification for recurrent networks (Lu et al., 2016). Another approach for pruning feed forward neural networks for speech recognition is using simple threshold to prune all weights (Yu et al., 2012) at a particular epoch. However, we find that gradual pruning produces better results than hard pruning."}, {"heading": "3 IMPLEMENTATION", "text": "Our pruning approach involves maintaining a set of masks, a monotonically increasing threshold and a set of hyper parameters that are used to determine the threshold. During model initialization, we create a set of binary masks, one for each weight in the network that are all initially set to one. After every optimizer update step, each weight is multiplied with its corresponding mask. At regular intervals, the masks are updated by setting all parameters that are lower than the threshold to zero.\nThe threshold is computed using hyper-parameters shown in Table 1. The hyper-parameters control the duration, rate and frequency of pruning the parameters for each layer. We use a different set of hyper-parameters for each layer type resulting in a different threshold for each layer type. The threshold is updated at regular intervals using the hyper-parameters according to Algorithm 1. We don\u2019t modify the gradients in the back-propagation step. It is possible for the updates of a pruned weight to be larger than the threshold of that layer. In this case, the weight will be involved in the forward pass again.\nWe provide heuristics to help determine start itr, ramp itr and end itr in table 1. After picking these hyper parameters and assuming that ramp slope(\u03c6) is 1.5\u00d7 start slope (\u03b8), we calculate (\u03b8) using equation 1.\n\u03b8 = 2 \u2217 q \u2217 freq\n2 \u2217 (ramp itr \u2212 start itr) + 3 \u2217 (end itr \u2212 ramp itr) (1)\nIn order to determine q in equation 1, we use an existing weight array from a previously trained model. The weights are sorted using absolute values and we pick the weight corresponding to the 90th percentile as q. This allows us to pick reasonable values for the hyper-parameters required for pruning. A validation set can be used to fine tune these parameters.\nWe only prune the weights of the recurrent and linear layers but not the biases or batch norm parameters since they are much fewer in number compared to the weights. For the recurrent layers, we prune both the input weight matrix and the recurrent weight matrix. Similarly, we prune all the weights in gated recurrent units including those of the reset and update gates.\nAlgorithm 1 Pruning Algorithm current itr = 0 while training do\nfor all parameters do param = (param and mask ) if current itr > start itr and current itr < end itr then\nif (current itr mod freq) == 0 then if current itr < ramp itr then = \u03b8 \u2217 (current itr \u2212 start itr + 1)/freq\nelse = (\u03b8 \u2217 (ramp itr \u2212 start itr + 1) + \u03c6 \u2217 (current itr \u2212 ramp itr + 1))/freq end if mask = abs(param) <\nend if end if\nend for current itr += 1\nend while"}, {"heading": "4 EXPERIMENTS", "text": "We run all our experiments on a training set of 2100 hours of English speech data and a validation set of 3.5 hours of multi-speaker data. This is a small subset of the datasets that we use to train our state-of-the-art automatic speech recognition models. We train the models using Nesterov SGD for 20 epochs. Besides the hyper-parameters for determining the threshold, all other hyper-parameters remain unchanged between the dense and sparse training runs. We find that our pruning approach works well for vanilla bidirectional recurrent layers and forward only gated recurrent units."}, {"heading": "4.1 BIDIRECTIONAL RNNS", "text": "We use the Deep Speech 2 model for these experiments. As shown in Table 2, this model has 2 convolution layers, followed by 7 bidirectional recurrent layers and a CTC cost layer. Each recurrent linear layer has 1760 hidden units, creating a network of approximately 67 million parameters. For these experiments, we prune the linear layers that feed into the recurrent layers, the forward and backward recurrent layers and fully connected layer before the CTC layer. These experiments use clipped rectified-linear units (ReLU) \u03c3(x) = min(max(x, 0), 20) as the activation function.\nIn the sparse run, the pruning begins shortly after the first epoch and continues until the 10th epoch. We chose these hyper-parameters so that the model has an overall sparsity of 88% at the end of pruning, which is 8x smaller than the original dense model. The character error rate (CER) on the devset is about 20% worse relative to the dense model as shown in Table 3.\nAn argument against this sparsity result might be that we are taking advantage of a large model that overfits our relatively small dataset. In order to test this hypothesis, we train a dense model with 704 hidden units in each layer, that has approximately the same number of parameters as the final sparse model. Table 3 shows that this model performs worse than the sparse models. Thus sparse model is a better approach to reduce parameters than using a dense model with fewer hidden units.\nIn order to recover the loss in accuracy, we train sparse models with larger recurrent layers with 2560 and 3072 hidden units. Figure 1a shows the training and dev curves for these sparse models compared to the dense baseline model. These experiments use the same hyper-parameters (except for small changes in the pruning hyper-parameters) and the same dataset as the baseline model. As we see in Table 3, the model with 2560 hidden units achieves a 0.75% relative improvement compared to the dense baseline model, while the model with 3072 hidden units has a 3.95% improvement. The dense 2560 model also improves the CER by 11.85% relative to the dense baseline model. The sparse 2560 model is about 12% worse than the corresponding dense model. Both these large models are pruned to achieve a final sparsity of around 92%. These sparse larger models have significantly fewer parameters than the baseline dense model.\nWe also compare our gradual pruning approach to the hard pruning approach proposed in Yu et al. (2012). In their approach, all parameters below a certain threshold are pruned at particular epoch. Table 4 shows the results of pruning the RNN dense baseline model at different epochs to achieve final parameter count ranging from 8 million to 11 million. The network is trained for the same number of epochs as the gradual pruning experiments. These hard threshold results are compared with the RNN Sparse 1760 model in Table 3. For approximately same number of parameters, gradual pruning is 7% to 9% better than hard pruning.\nWe conclude that pruning models to achieve sparsity of around 90% reduces the relative accuracy of the model by 10% to 20%. However, for a given performance requirement, it is better to prune a larger model than to use a smaller dense model. Gradually pruning a model produces better results than hard pruning."}, {"heading": "4.2 GATED RECURRENT UNITS", "text": "We also experimented with GRU models shown in Table 5, that have 2560 hidden units in the GRU layer and a total of 115 million parameters. For these experiments, we prune all layers except the convolution layers since they have relatively fewer parameters.\nFigure 1b compares the training and dev curves of a sparse GRU model a dense GRU model. The sparse GRU model has a 13.8% drop in the accuracy relative to the dense model. As shown in Table 3, the sparse model has an overall sparsity of 88.6% with 13 million parameters. Similar to the RNN models, we train a sparse GRU model with 3568 hidden units. The dataset and the hyperparameters are not changed from the previous GRU experiments. This model has an overall sparsity of 91.82% with 17.8 million parameters. As shown in Table 3, the model with 3568 hidden units is only 2.2% worse than the baseline dense GRU model. We expect to match the performance of the GRU dense network by slightly lowering the sparsity of this network or by increasing the hidden units for the layers.\nIn addition, we experimented with pruning only the GRU layers and keeping all the parameters in fully connected layers. The accuracy for these experiments is around 7% worse than the baseline dense model. However, this model only achieves 50% compression due to the size of the fully connected layers.\nTable 6: GEMM times for recurrent layers with different sparsity\nLAYER SIZE SPARSITY LAYER TYPE TIME (\u00b5sec) SPEEDUP\n1760 0% RNN 56 1 1760 95% RNN 20 2.8 2560 95% RNN 29 1.93 3072 95% RNN 48 1.16 2560 0% GRU 313 1 2560 95% GRU 46 6.80 3568 95% GRU 89 3.5"}, {"heading": "5 PERFORMANCE", "text": ""}, {"heading": "5.1 COMPUTE TIME", "text": "The success of deep learning in recent years have been driven by large models trained on large datasets. However this also increases the inference time after the models have been deployed. We can mitigate this effect by using sparse layers.\nA General Matrix-Matrix Multiply (GEMM) is the most compute intensive operation in evaluating a neural network model. Table 6 compares times for GEMM for recurrent layers with different number of hidden units that are 95% sparse. The performance benchmark was run using NVIDIA\u2019s CUDNN and cuSPARSE libraries on a TitanX Maxwell GPU and compiled using CUDA 7.5. All experiments are run on a minibatch of 1 and in this case, the operation is known as a sparse matrix-vector product (SpMV). We can achieve speed-ups ranging from 3x to 1.15x depending on the size of the recurrent layer. Similarly, for the GRU models, the speed-ups range from 7x to 3.5x. However, we notice that cuSPARSE performance is substantially lower than the approximately 20x speedup that we would expect by comparing the bandwidth requirements of the 95% sparse and dense networks. State of the art SpMV routines can achieve close to device memory bandwidth for a wide array of matrix shapes and sparsity patterns (see Baxter (2016) and Liu et al. (2013)). This means that the performance should improve by the factor that parameter counts are reduced. Additionally, we find that the cuSPARSE performance degrades with larger batch sizes. It should be possible for a better implementation to further exploit the significant reuse of the weight matrix provided by large batch sizes."}, {"heading": "5.2 COMPRESSION", "text": "Pruning allows us to reduce the memory footprint of a model which allows them to be deployed on phones and other embedded devices. The Deep Speech 2 model can be compressed from 268 MB to around 32 MB (1760 hidden units) or 64 MB (3072 hidden units). The GRU model can be compressed from 460 MB to 50 MB. These pruned models can be further quantized down to float16 or other smaller datatypes to further reduce the memory requirements without impacting accuracy."}, {"heading": "6 DISCUSSION", "text": ""}, {"heading": "6.1 PRUNING CHARACTERISTICS", "text": "Figure 2a shows the sparsity of all the recurrent layers with the same hyper-parameters used to prune the layers. The layers are ordered such that layer 1 is closest to input and layer 14 is the final recurrent layer before the cost layer. We see that the initial layers are pruned more aggressively compared to the final layers. We also performed experiments where the hyper parameters are different for the recurrent layers resulting in equal sparsity for all the layers. However, we get higher CER for these experiments. We conclude that to get good accuracy, it is important to prune the final layers slightly less than the initial ones.\nIn Figure 2b, we plot the pruning schedule of a 95% sparse recurrent layer of the bidirectional model trained for 20 epochs (55000 iterations). We begin pruning the network at the start of the second epoch at 2700 iterations. We stop pruning a layer after 10 epochs (half the total epochs) are complete at 27000 iterations. We see that nearly 25000 weights are pruned before 5 epochs are complete at around 15000 iterations. In our experiments, we\u2019ve noticed that pruning schedules that are a convex curve tend to outperform schedules with a linear slope."}, {"heading": "6.2 PERSISTENT KERNELS", "text": "Persistent Recurrent Neural Networks (Diamos et al., 2016) is a technique that increases the computational intensity of evaluating an RNN by caching the weights in on-chip memory such as caches, block RAM, or register files across multiple timesteps. A high degree of sparsity allows significantly large Persistent RNNs to be stored in on-chip memory. When all the weights are stored in float16, a NVIDIA P100 GPU can support a vanilla RNN size of about 2600 hidden units. With the same datatype, at 90% sparsity, and 99% sparsity, a P100 can support RNNs with about 8000, and 24000 hidden units respectively. We expect these kernels to be bandwidth limited out of the memory that is used to store the parameters. This offers the potential of a 146x speedup compared to the TitanX GPU if the entire RNN layer can be stored in registers rather than the GPU DRAM of a TitanX.\nAdditionally, sparse matrix multiplication involves scheduling and load balancing phases to divide the work up evenly over thousands of threads and to route corresponding weights and activations to individual threads. Since the sparsity patterns for RNNs are fixed over many timesteps these scheduling and load balancing operations can be factored outside of the loop, performed once, and reused many times."}, {"heading": "7 CONCLUSION AND FUTURE WORK", "text": "We have demonstrated that by pruning the weights of RNNs during training we can find sparse models that are more accurate than dense models while significantly reducing model size. These sparse models are especially suited for deployment on mobile devices and on back-end server farms due to their small size and increased computational efficiency. Even with existing sub-optimal sparse matrix-vector libraries we realize speed-ups with these models. This technique is orthogonal to quantization techniques which would allow for even further reductions in model size and corresponding increase in performance.\nWe wish to investigate whether these techniques can generalize to language modeling tasks and if they can effectively reduce the size of embedding layers. We also wish to compare the sparsity generated by our pruning technique to that obtained by L1 regularization.\nWe are investigating training techniques that don\u2019t require maintaining dense matrices for a significant portion of the calculation. Further work remains to implement optimal small batch sparse matrix-dense vector routine for GPUs and ARM processors that would help in deployment."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Bryan Catanzaro for helpful discussions related to this work."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "Moderngpu, 2016. URL https://nvlabs.github.io/moderngpu/ segreduce.html", "author": ["Sean Baxter"], "venue": null, "citeRegEx": "Baxter.,? \\Q2016\\E", "shortCiteRegEx": "Baxter.", "year": 2016}, {"title": "Strategies for training large vocabulary neural language models", "author": ["Welin Chen", "David Grangier", "Michael Auli"], "venue": "CoRR, abs/1512.04906,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "CoRR, abs/1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "CoRR, abs/1306.0543,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "CoRR, abs/1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Persistent rnns: Stashing recurrent weights onchip", "author": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Diamos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diamos et al\\.", "year": 2016}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir D. Bourdev"], "venue": "CoRR, abs/1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In ICML,", "citeRegEx": "Graves and Jaitly.,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Advances in neural information processing systems 1. chapter Comparing Biases for Minimal Network Construction with Back-propagation, pp. 177\u2013185", "author": ["Stephen Jos\u00e9 Hanson", "Lorien Pratt"], "venue": "URL http://dl.acm.org/citation.cfm?id=89851.89872", "citeRegEx": "Hanson and Pratt.,? \\Q1989\\E", "shortCiteRegEx": "Hanson and Pratt.", "year": 1989}, {"title": "Optimal brain surgeon and general network pruning", "author": ["Babak Hassibi", "David G Stork", "Gregory J Wolff"], "venue": "In Neural Networks,", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "CoRR, abs/1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "CoRR, abs/1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In NIPs,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Efficient sparse matrix-vector multiplication on x86-based many-core processors", "author": ["Xing Liu", "Mikhail Smelyanskiy", "Edmond Chow", "Pradeep Dubey"], "venue": "In Proceedings of the 27th International ACM Conference on International Conference on Supercomputing,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Learning compact recurrent neural networks", "author": ["Zhiyun Lu", "Vikas Sindhwani", "Tara N. Sainath"], "venue": "CoRR, abs/1604.02594,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z. Mao"], "venue": "In Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "venue": "In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al.", "startOffset": 62, "endOffset": 106}, {"referenceID": 15, "context": ", 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 10, "context": "Even in cases when the networks can be evaluated fast enough, it will still have a significant impact on battery life in mobile devices (Han et al., 2015).", "startOffset": 136, "endOffset": 154}, {"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets. For example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al.", "startOffset": 86, "endOffset": 439}, {"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets. For example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al. (2015). And in language modeling the size of the non-embedding parameters (mostly in the recurrent layers) have exploded even as various ways of hand engineering sparsity into the embeddings have been explored in J\u00f3zefowicz et al.", "startOffset": 86, "endOffset": 617}, {"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets. For example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al. (2015). And in language modeling the size of the non-embedding parameters (mostly in the recurrent layers) have exploded even as various ways of hand engineering sparsity into the embeddings have been explored in J\u00f3zefowicz et al. (2016) and Chen et al.", "startOffset": 86, "endOffset": 848}, {"referenceID": 0, "context": "Recent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (J\u00f3zefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets. For example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al. (2015). And in language modeling the size of the non-embedding parameters (mostly in the recurrent layers) have exploded even as various ways of hand engineering sparsity into the embeddings have been explored in J\u00f3zefowicz et al. (2016) and Chen et al. (2015a). These large models face two significant challenges in deployment.", "startOffset": 86, "endOffset": 872}, {"referenceID": 4, "context": "We show this technique works with Gated Recurrent Units (GRU) (Cho et al., 2014) as well as vanilla RNNs.", "startOffset": 62, "endOffset": 80}, {"referenceID": 4, "context": "We show this technique works with Gated Recurrent Units (GRU) (Cho et al., 2014) as well as vanilla RNNs. In addition to the benefits of less storage and faster inference, this technique can also improve the accuracy over a dense baseline. By starting with a larger dense matrix than the baseline and then pruning it down, we can achieve equal or better accuracy compared to the baseline but with a much smaller number of parameters. This approach can be implemented easily in current training frameworks and is agnostic to the optimization algorithm. Furthermore, training time does not increase unlike previous approaches such as in Han et al. (2015). State of the art results in speech recognition generally require days to weeks of training time, so a further 3-4\u00d7 increase in training time is undesirable.", "startOffset": 63, "endOffset": 653}, {"referenceID": 19, "context": "One method is to use a fixed point representation to quantize weights to signed bytes and activations to unsigned bytes (Vanhoucke et al., 2011).", "startOffset": 120, "endOffset": 144}, {"referenceID": 5, "context": "One method is to carefully construct one of the factors and learn the other (Denil et al., 2013).", "startOffset": 76, "endOffset": 96}, {"referenceID": 6, "context": "Inspired by this technique, a low rank approximation for the convolution layers achieves twice the speed while staying within 1% of the original model in terms of accuracy (Denton et al., 2014).", "startOffset": 172, "endOffset": 193}, {"referenceID": 14, "context": "The convolution layer can also be approximated by a smaller set of basis filters (Jaderberg et al., 2014).", "startOffset": 81, "endOffset": 105}, {"referenceID": 8, "context": "Quantization techniques like k-means clustering of weights can also reduce the storage size of the models by focusing only on the fully connected layers (Gong et al., 2014).", "startOffset": 153, "endOffset": 172}, {"referenceID": 16, "context": "Yet another approach is to use the diagonal terms of a Hessian matrix to construct a saliency threshold and used this to drop weights that fall below a given saliency threshold (LeCun et al., 1989).", "startOffset": 177, "endOffset": 197}, {"referenceID": 13, "context": "Optimal Brain Surgeon is another work in the same vein that prunes weights using the inverse of a Hessian matrix with the additional advantage of no re-training after pruning (Hassibi et al., 1993).", "startOffset": 175, "endOffset": 197}, {"referenceID": 10, "context": "Both pruning and quantization techniques can be combined to get impressive gains on AlexNet trained on the ImageNet dataset (Han et al., 2015).", "startOffset": 124, "endOffset": 142}, {"referenceID": 18, "context": "There has also been some recent work to shrink model size for recurrent and LSTM networks used in automatic speech recognition (ASR) (Lu et al., 2016).", "startOffset": 133, "endOffset": 150}, {"referenceID": 16, "context": "Unlike the methods that need to approximate a Hessian (LeCun et al., 1989; Hassibi et al., 1993) our method uses a simple heuristic to choose the threshold used to drop weights.", "startOffset": 54, "endOffset": 96}, {"referenceID": 13, "context": "Unlike the methods that need to approximate a Hessian (LeCun et al., 1989; Hassibi et al., 1993) our method uses a simple heuristic to choose the threshold used to drop weights.", "startOffset": 54, "endOffset": 96}, {"referenceID": 10, "context": "Yet another advantage, when compared to methods that need re-training (Han et al., 2015), is that our pruning technique is part of training and needs", "startOffset": 70, "endOffset": 88}, {"referenceID": 18, "context": "Even though our technique requires judicious choice of pruning hyperparameters, we feel that it is easier than choosing the structure of matrices to guide the sparsification for recurrent networks (Lu et al., 2016).", "startOffset": 197, "endOffset": 214}, {"referenceID": 20, "context": "Another approach for pruning feed forward neural networks for speech recognition is using simple threshold to prune all weights (Yu et al., 2012) at a particular epoch.", "startOffset": 128, "endOffset": 145}, {"referenceID": 20, "context": "We also compare our gradual pruning approach to the hard pruning approach proposed in Yu et al. (2012). In their approach, all parameters below a certain threshold are pruned at particular epoch.", "startOffset": 86, "endOffset": 103}, {"referenceID": 1, "context": "State of the art SpMV routines can achieve close to device memory bandwidth for a wide array of matrix shapes and sparsity patterns (see Baxter (2016) and Liu et al.", "startOffset": 137, "endOffset": 151}, {"referenceID": 1, "context": "State of the art SpMV routines can achieve close to device memory bandwidth for a wide array of matrix shapes and sparsity patterns (see Baxter (2016) and Liu et al. (2013)).", "startOffset": 137, "endOffset": 173}, {"referenceID": 7, "context": "Persistent Recurrent Neural Networks (Diamos et al., 2016) is a technique that increases the computational intensity of evaluating an RNN by caching the weights in on-chip memory such as caches, block RAM, or register files across multiple timesteps.", "startOffset": 37, "endOffset": 58}], "year": 2017, "abstractText": "Recurrent Neural Networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8\u00d7 and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse matrix multiply. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2\u00d7 to 7\u00d7.", "creator": "LaTeX with hyperref package"}}}