{"id": "1704.05228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees", "abstract": "prominent applications of sentiment analysis are countless, including areas such as marketing, customer service and institutional communication. the conventional bag - of - words approach for measuring sentiment merely counts term frequencies ; however, it neglects the position of the terms within the discourse. as a remedy, we thus develop a discourse - aware method that builds upon roughly the discourse structure of documents. for precisely this purpose, we utilize rhetorical structure theory to label ( sub - ) clauses according to their hierarchical relationships and then assign polarity scores to individual leaves. \u2022 to learn from the resulting synthetic rhetoric structure, we propose a tensor - based, tree - structured narrative deep neural network ( named rst - lstm ) in order to process the largest complete discourse tree. the underlying attention mechanism infers the salient passages of narrative materials. in addition, we suggest two algorithms for data augmentation ( node reordering and artificial leaf insertion ) that uniquely increase our training set and reduce overfitting. also our benchmarks demonstrate the superior performance of our approach. ultimately, this work advances the status quo explored in natural language processing by developing algorithms that incorporate semantic information.", "histories": [["v1", "Tue, 18 Apr 2017 08:24:20 GMT  (395kb,D)", "http://arxiv.org/abs/1704.05228v1", null], ["v2", "Mon, 9 Oct 2017 08:03:06 GMT  (663kb,D)", "http://arxiv.org/abs/1704.05228v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mathias kraus", "stefan feuerriegel"], "accepted": false, "id": "1704.05228"}, "pdf": {"name": "1704.05228.pdf", "metadata": {"source": "CRF", "title": "Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees", "authors": ["Mathias Krausa", "Stefan Feuerriegel"], "emails": ["mathias.kraus@is.uni-freiburg.de;", "mathias.kraus@is.uni-freiburg.de", "stefan.feuerriegel@is.uni-freiburg.de"], "sections": [{"heading": null, "text": "Prominent applications of sentiment analysis are countless, including areas such as marketing, customer service and communication. The conventional bag-ofwords approach for measuring sentiment merely counts term frequencies; however, it neglects the position of the terms within the discourse. As a remedy, we thus develop a discourse-aware method that builds upon the discourse structure of documents. For this purpose, we utilize rhetorical structure theory to label (sub-)clauses according to their hierarchical relationships and then assign polarity scores to individual leaves. To learn from the resulting rhetoric structure, we propose a tensor-based, tree-structured deep neural network (named RST-LSTM) in order to process the complete discourse tree. The underlying attention mechanism infers the salient passages of narrative materials. In addition, we suggest two algorithms for data augmentation (node reordering and artificial leaf insertion) that increase our training set and reduce overfitting. Our benchmarks demonstrate the superior performance of our approach. Ultimately, this work advances the status quo in natural language processing by developing algorithms that incorporate semantic information.\nKeywords: Sentiment analysis, Rhetorical structure theory, Discourse tree, Tree-structured network, Long short-term memory, Attention mechanism\n\u2217Corresponding author. Mail: mathias.kraus@is.uni-freiburg.de; Tel: +49 761 203 2395; Fax: +49 761 203 2416.\nEmail addresses: mathias.kraus@is.uni-freiburg.de (Mathias Kraus), stefan.feuerriegel@is.uni-freiburg.de (Stefan Feuerriegel)\nPreprint submitted to arXiv April 19, 2017\nar X\niv :1\n70 4.\n05 22\n8v 1\n[ cs\n.C L\n] 1\n8 A\npr 2"}, {"heading": "1. Introduction", "text": "User-generated content reveals personal opinions towards entities such as products, services or events, which can benefit organizations and businesses in improving their marketing, communication, production and procurement. In this context, sentiment analysis facilitates the extraction of subjective information from user-generated content, or narrative materials in general, by quantifying the positivity or negativity of natural language [1, 2]. Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].\nSentiment analysis traditionally utilizes bag-of-words approaches, which merely\ncount the frequency of words (and tuples thereof) to obtain a mathematical representation of documents in matrix form [1, 2]. As such, these approaches are not capable of taking into consideration semantic relationships between sections and sentences of a document. Let us, for instance, consider the following two examples, which express opposite polarities: \u201cthe movie was [good]+, although reviews are [bad]\u2212\u201d and \u201cthe movie was [bad]\u2212, although reviews are [good]+\u201d. Both constitute identical representations in na\u0308\u0131ve bag-of-words models, which are unable to account for information regarding semantics and discourse.\nA remedy to this problem may be found in the form of n-grams, which construct tuples of n contiguous words from a text and thus incorporate the contextual information of words into the representation of text [11]. Here research commonly utilizes n-grams of size two (bi-grams) and three (tri-grams), which are often enriched with additional features such as part-of-speech [1]. This approach allows for implicitly formalizing dependencies between adjacent words. Specific use cases are the detection of negation scopes, e. g. \u201cnot bad\u201d, or compound nouns, e. g. \u201ccomputer science\u201d. Yet n-grams cannot encode relationships between different sections of a text. For instance, bi-grams cannot\n2\ncorrectly infer the sentiment of the previous example.1\nTo reliably analyze the sentiment of lengthy documents, it is essential to incorporate the semantic structure. A recent review in Science points out that novel techniques are required to leverage semantics [12]. Hence, we propose an innovative method, based on rhetorical structure theory (RST), that incorporates the discourse structures of natural language. RST structures documents hierarchically [13] by splitting the content into (sub-)clauses called elementary discourse units (EDUs). The EDUs are then connected to form a binary discourse tree. The edges are further labeled according to the type of discourse, for instance, whether it is an elaboration or an argument. The RST tree thus helps localize essential information within documents; e. g. the concluding section of a newspaper article is typically relevant as it reports the opinion of the author. Figure 1 illustrates the discourse tree for the previous example, in which RST renders it possible to identify the intended sentiment (i. e. the subjective statement conveyed by the main clause). Hence, the goal of this work is to develop a novel approach that identifies salient passages in a document based on their position in the discourse tree and incorporates their importance in the form of weights when computing sentiment scores.\n1Bi-grams contained in the first example are \u201cThe movie\u201d, \u201cmovie was\u201d, \u201cwas [good]+\u201d, \u201c[good]+ although\u201d, \u201calthough reviews\u201d, \u201creviews are\u201d, \u201care [bad]\u2212\u201d. The second example includes the bi-grams \u201cThe movie\u201d, \u201cmovie was\u201d, \u201cwas [bad]\u2212\u201d, \u201c[bad]\u2212 although\u201d, \u201calthough reviews\u201d, \u201creviews are\u201d, \u201care [good]+\u201d. Hence, the vector representations differ for each sentence, but both still suggest the same, albeit false, neutral rating.\n3\nPrevious research has demonstrated that discourse-related information can successfully improve the performance of sentiment analysis (see Section 2 for details). For instance, one can reweigh the importance of passages based on their relation type [14] or depth [15] in the discourse tree. Some methods prune the discourse trees at certain thresholds to yield a tree of fixed depth, e. g. 2 or 4 levels [15]. Other approaches train machine learning classifiers based on the relation types as input features [16]. What the previous references have in common is that they try to map the tree structure onto mathematically simpler representations, since it is virtually impossible to encode unstructured data of arbitrary complexity in a fixed-length vector.\nWe overcome the limitations of previous works and propose a specific neural network, named RST-LSTM, for representation learning. The RST-LSTM utilizes multiple tensors to build an attention mechanism that can localize salient passages within documents by following the discourse structure. In contrast to previous works, the RST-LSTM is thus capable of exploiting the complete structure of the discourse tree. In addition, we propose two techniques for data augmentation that facilitate training and yield higher predictive accuracy.\nIn brief, our approach is as follows: we utilize rhetorical structure theory to represent the semantic structure of a document in the form of a hierarchical discourse tree. We then follow a dictionary-based approach to obtain sentiment scores for each leaf. The resulting tree is subsequently traversed by the RST-LSTM, thereby aggregating the sentiment scores based on the discourse structure in order to compute a sentiment score at document level. This approach thus weighs the importance of (sub-)clauses based on their position and relation in the discourse tree, which is learned during the training phase. As a consequence, this allows us to improve sentiment analysis with discourse information.\nThe RST-LSTM also differs from the attention mechanism in [17], which can only exploit the relation type and not the hierarchy. Furthermore, former approaches are based on traditional recursive neural networks, which are limited by the fact that they can persist information for only a few iterations [18].\n4\nTherefore, these methods struggle with complex discourses, while ours can handle even very deep tree structures.\nThe remainder of this paper is structured as follows. Section 2 reviews discourse parsing and RST-based sentiment analysis. To overcome the limitations of previous approaches, Section 3 introduces the RST-LSTM, as well as our algorithms for data augmentation. Section 4 describes our experimental setup, for which we evaluate the performance of our deep learning methods in comparison to common baselines (Section 5). Section 6 concludes with a summary and suggestions for future research."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Rhetorical structure theory", "text": "Rhetorical structure theory formalizes the discourse in narrative materials by organizing sub-clauses, sentences and paragraphs into a hierarchy [13]. The premise is that a document is split into elementary discourse units, which constitute the smallest, indivisible segments. These EDUs are then connected by one of 18 different relation types, which represent edges in the discourse tree; see Table 1 for a list [19]. Each relation is further labeled by a hierarchy type, i. e. either as a nucleus (N) or a satellite (S), where a nucleus denotes a more essential unit of information, while a satellite indicates a supporting or background unit of information. We note that this hierarchy type is not necessarily an exclusive-or, since both children can be labeled as nuclei or satellites at the same time. Figure 2 presents an example of a discourse tree.\n5\n6\nPrevious research has proposed various methods for automating the parsing of discourse trees of documents. Common implementations for documents consisting of multiple paragraphs are represented by the high-level discourse analyzer HILDA [20] and the DPLP parser [21], of which the DPLP parser currently achieves the better F1-score in identifying relation types . Since this is regarded as the most challenging subtask of RST parsing, we specifically decided to utilize the DPLP parser in this work."}, {"heading": "2.2. Sentiment analysis with RST", "text": "Previous studies have advocated different approaches for sentiment analysis\nthat utilize the discourse tree; see Table 2.\nThe underlying sentiment scores of EDUs are almost exclusively calculated on the basis of pre-defined sentiment dictionaries. Common examples include\n7\nSentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15]. A recent approach calculates ex ante vector representations for the EDUs based on word embeddings [17]. This essentially resembles a high-dimensional dictionary that is, consistent with the pre-defined dictionaries, applied prior to learning the weights of the classifier. For reasons of comparability, we also utilize a dictionary-based approach in order to compute sentiment features from the content of elementary discourse units.\nAmong the most common methods are simple weighting rules that aggregate the sentiment scores of EDUs based on the tree structure [19, 14]. However, the weights are frequently pre-determined and hand-crafted. A different stream of research also considers hierarchy labels (nucleus or satellite) of the nodes and updates the weights based on these. Examples include approaches that focus on the top-split (i. e. the root node) of the discourse tree and scale the relative importance based on (hand-crafted) weights [23, 19, 14]. The underlying weights can also be optimized using logistic regression [24]. In contrast, other works specifically focus on the hierarchy labels at leaf level, arguing that this choice facilitates a more fine-grained evaluation [14], while neglecting the discourse tree from above. To fill the gap between top-level and leaf-level analysis, recent research also applies a recursive weighting scheme that utilizes a scaling factor to diminish the influence of increasing depth [14, 15]. Alternatively, one can prune the discourse tree at certain thresholds in order to yield a tree of fixed depth, e. g. 2 or 4 levels [15]. Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].\nThe papers discussed above predominantly consider depth-related information of the RST nodes. Accordingly, they neglect the links between nodes within the tree structure. A potential remedy is to traverse the RST tree with a recursive neural network [25]; however, this approach only incorporates the edges and lacks information regarding the relation type. Closest to our work is an approach that traverses the RST tree with the help of a recursive neural network, while utilizing relation-specific composition matrices in order to build an\n8\nattention mechanism [17]. However, the recursive neural network is known to struggle with complex tree structures because of vanishing or exploding gradients and, instead, we utilize a long short-term memory. Moreover, the approach sums the representations in each recursion and, hence, cannot distinguish the hierarchy; i. e. between nucleus and satellite. Hence, the objective of this paper is to extend the previous works by advancing representation learning in order to incorporate the complete discourse tree, including relation types, tree depth and hierarchy labels."}, {"heading": "2.3. Representation learning for sequential and tree data", "text": "Recent advances in deep neural networks have rendered it possible to learn representations of unstructured data such as sequences or text [26]. This can, for instance, be achieved by recurrent neural networks, which entail an internal architecture in the form of a directed cycle, thereby creating an internal state with which to learn dependent structures [27]. Based on these, one can process texts of arbitrary length in sequential order, while the internal state encodes the complete sequence. The underlying structure thus allows for the passage of information from one word to the next. However, in practice, information only persists for a few iterations [18]. A viable remedy is provided by the long shortterm memory (LSTM) network. These enhance recurrent neural networks by capturing long dependencies among input signals [28]. We thus utilize LSTMs as part of our baselines later, as well as inside all tree-structured networks.\nPrevious research has proposed a Tree-LSTM that can deal with representation learning, for instance for trees from RST, and we thus rely upon it as another baseline. This tree-structured LSTM network traverses trees bottomup in order to generate representations of the underlying structure [29]. The Tree-LSTM computes a representation for each parent node based on its immediate children and does so recursively until the root of the tree is reached. It thereby stacks individual LSTMs such that they reflect the tree structure from the input. However, the Tree-LSTM provides no possibility of incorporating additional information from the discourse trees, such as the relation type. We\n9\nR e fe\nr e n c e\nD a t a s e t s\nP r e d ic\nt e d\nW e ig\nh t\no p t im\niz a t io\nn s\nE D\nU fe\na t u r e s\nR S T\np a r s e r\nM o d e l\nC o n s id\ne r e d\nR S T\nfe a t u r e s\nv a r ia\nb le\nR e la\nt io\nn T\nr e e\nN u c le\nu s /\nt y p\ne d e p t h\ns a t e ll it\ne\nB h a t ia\ne t\na l.\n[2 5 ]\nR o t t e n\nt o m\na t o e s\nr e v ie\nw s ,\nU s e r\nr a t in\ng H\na n d c r a ft\ne d\nw e ig\nh t in\ng fa\nc t o r\nD ic\nt io\nn a r y -b\na s e d\nD P\nL P\nH ie\nr a r c h ic\na l\nw e ig\nh t in\ng r u le\n7 3\n7\nIM D\nb r e v ie\nw s\no r\nc la\ns s if\nie r -b\na s e d\ns e n t im\ne n t\ns c o r e\nR e c u r s iv\ne ly\nt r a v e r s a l\no f\nR S T\nt r e e\n7 3\n3\nw it\nh r e c u r r e n t\nn e u r a l\nn e t w\no r k\nC h e n lo\ne t\na l.\n[2 4 ]\nB L\nO G\nS 0 6\nB lo\ng p o s t\nr a n k in\ng W\ne ig\nh t s\no p t im\niz e d\nD ic\nt io\nn a r y -b\na s e d\nS P A\nD E\nT o p -s\np li t\nw e ig\nh t in\ng 3\n7 3\nb y\nlo g is\nt ic\nr e g r e s s io\nn s e n t im\ne n t\ns c o r e\nH e e r s c h o p\ne t\na l.\n[1 9 ]\nR o t t e n\nt o m\na t o e s\nr e v ie\nw s\nU s e r\nr a t in\ng H\na n d c r a ft\ne d\nw e ig\nh t in\ng fa\nc t o r\nD ic\nt io\nn a r y -b\na s e d\nS P A\nD E\nP o s it\nio n -b\na s e d\nw e ig\nh t in\ng r u le\n7 7\n7\no r\no p t im\niz e d\nb y\ns e n t im\ne n t\ns c o r e\nT o p -s\np li t\nw e ig\nh t in\ng r u le\n7 U\np t o\nle v e l\n1 3\ng e n e t ic\na lg\no r it\nh m\nR e la\nt io\nn -t\ny p e\nw e ig\nh t in\ng 3\n7 3\nH o g e n b o o m\ne t\na l.\n[1 4 ]\nR o t t e n\nt o m\na t o e s\nr e v ie\nw s\nU s e r\nr a t in\ng H\na n d c r a ft\ne d\nw e ig\nh t in\ng fa\nc t o r\nD ic\nt io\nn a r y -b\na s e d\nH IL\nD A\nP o s it\nio n -b\na s e d\nw e ig\nh t in\ng 7\n7 7\no r\np a r t ic\nle s w\na r m\no p t im\niz a t io\nn s e n t im\ne n t\ns c o r e\nS P A\nD E\nT o p -s\np li t\nw e ig\nh t in\ng 7\nU p\nt o\nle v e l\n1 3\nB o t t o m\n-s p li t\nw e ig\nh t in\ng 7\n7 3\nH ie\nr a r c h ic\na l\nw e ig\nh t in\ng 7\n3 3\nH o g e n b o o m\ne t\na l.\n[1 6 ]\nR o t t e n\nt o m\na t o e s\nr e v ie\nw s ,\nU s e r\nr a t in\ng W\ne ig\nh t s\no p t im\niz e d\nb y\nS V\nM D\nic t io\nn a r y -b\na s e d\nS P A\nD E\nT o p -\na n d\nle a f-\ns p li t\nw e ig\nh t in\ng 3\n7 7\nM u lt\nid o m\na in\nc o ll e c t io\nn s e n t im\ne n t\ns c o r e\no f\n1 4\nr e la\nt io\nn t y p e s\nJ i\na n d\nS m\nit h\n[1 7 ]\nY e lp\nr e v ie\nw s ,\nU s e r\nr a t in\ng o r\nT r a n s fo\nr m\na t io\nn o f\nR S T\nt r e e\nH id\nd e n\ns t a t e s\no f\nD P\nL P\nA v e r a g e\no f\na ll\nE D\nU s\n7 7\n7\nR o t t e n\nt o m\na t o e s\nr e v ie\nw s ,\nc la\ns s\nla b e ls\nin t o\nd e p e n d e n c y\ns t r u c t u r e\nb id\nir e c t io\nn a l\nL S T\nM S e le\nc t\nr o o t\nn o d e\n7 U\np t o\nle v e l\n1 7\nC o n g r e s s io\nn a l\nd e b a t e s ,\no n\nw o r d\ne m\nb e d d in\ng s\no f\nd e p e n d e n c y\nR S T\nt r e e\nC o n g r e s s io\nn a l\nb il l\nc o r p u s\nT r e e -s\nt r u c t u r e d\nn e u r a l\n3 3\n7\nM e d ia\nfr a m\ne s\nc o r p u s\nn e t w\no r k\nw it\nh a t t e n t io\nn -w\ne ig\nh t s\nM a\u0308 r k le\n-H u \u00df\ne t\na l.\n[1 5 ]\nF in\na n c ia\nl d is\nc lo\ns u r e s\nD ir\ne c t io\nn a l\ns t o c k\nW e ig\nh t s\no p t im\niz e d\nb y\ng r id\n-s e a r c h\nD ic\nt io\nn a r y -b\na s e d\nH IL\nD A\nH ie\nr a r c h ic\na l\nw e ig\nh t in\ng r u le\n7 3\n3\np r ic\ne c h a n g e\ns e n t im\ne n t\ns c o r e\nR a n d o m\nfo r e s t\no n\np r u n e d\nt r e e\n7 U\np t o\nle v e l\n3\n2 o r\n4\nT a b o a d a\ne t\na l.\n[2 3 ]\nO p in\nio n s\nr e v ie\nw s ,\nU s e r\nr a t in\ng H\na n d c r a ft\ne d\nw e ig\nh t s\nD ic\nt io\nn a r y -b\na s e d\nS P A\nD E\nT o p -s\np li t\nw e ig\nh t in\ng 7\nU p\nt o\nle v e l\n1 3\nR o t t e n\nt o m\na t o e s\nr e v ie\nw s\ns e n t im\ne n t\ns c o r e\nZ ir\nn e t\na l.\n[2 2 ]\nM D\nS A\nU s e r\nr a t in\ng D\nic t io\nn a r y -b\na s e d\nM a r k o v\nlo g ic\nn e t w\no r k\nC o n t r a s t iv\ne a n d\n7 7\ns e n t im\ne n t\ns c o r e\nn o n -c\no n t r a s t iv\ne\nT h is\np a p\ne r\nR o t t e n\nt o m\na t o e s\nr e v ie\nw s ,\nU s e r\nr a t in\ng A\nd d it\nio n a l\nd a t a\nD ic\nt io\nn a r y -b\na s e d\nD P\nL P\nR S T\n-L S T\nM 3\n3 3\nIM D\nb r e v ie\nw s\na u g m\ne n t a t io\nn s e n t im\ne n t\ns c o r e\nTable 2: Comparison of methods for sentiment analysis utilizing the discourse structure.\nthus later extend the na\u0308\u0131ve Tree-LSTM through two additional tensor-based attention mechanisms, resulting in an RST-LSTM that allows us to utilize the complete set of information encoded in discourse tree."}, {"heading": "3. Discourse-based sentiment analysis with deep learning", "text": "This section introduces our discourse-based methodology, which infers sentiment scores from textual materials. Figure 3 illustrates the underlying framework and divides the procedure into steps for discourse parsing, computing low-level polarity features, data augmentation and prediction. The prediction phase implements either one of the baselines (e. g. LSTM or Tree-LSTM) or our proposed RST-LSTM."}, {"heading": "3.1. Discourse parsing", "text": "We generate discourse trees for our datasets by utilizing the DPLP parser [21].\nFor sake of simplicity, we introduce the following notation. We denote the relation type of node i as \u03c1i \u2208 {elaboration, argument, . . .}. The complete list of relation types is given in Table 1. Furthermore, let \u03c0i specify a path from the root to a certain node i in the tree given by a continuous list of nucleus (N) or satellite (S) traversals. We then denote the relation type of i as \u03c1\u03c0i . For example, \u03c1SN refers to the relation type of node SN , which is the nucleus\n11\nin the first satellite branch. In this regard, the symbol R represents the root node of a discourse tree and, hence, \u03c1R provides the relation type of the root. Furthermore, we denote the list with the relation types of all nodes in an RST tree as \u03c1RST. We also introduce \u03c4i \u2208 {nucleus, satellite} as the hierarchy type of node i. Additionally, we specify the list of all hierarchy types in an RST tree as \u03c4RST."}, {"heading": "3.2. Polarity features", "text": "We follow common procedures in sentiment analysis and utilize a pre-defined dictionary that labels terms as positive or negative [1, 2]. This approach has multiple advantages, as it is domain-independent and works reliably even with few training observations. In addition, one can easily exchange the underlying dictionary for one that not only measures polarity or negativity, but is concerned with other language concepts such as subjectivity, certainty or the domainspecific tone. Our experimental results are based on the SentiWordNet 3.0 dictionary [30], which provides sentiment labels for 117,659 words.\nBased on the sentiment labels at word level, we then proceed to compute a\nsentiment score \u03c3i for each EDU i via\n\u03c3i = 1 |{w |w \u2208 i}| \u2211 w\u2208i pos(w)\u2212 neg(w), (1)\nwhere we iterate over the words w in EDU i, while pos(w) and neg(w) are the positivity and negativity scores for word w according to SentiWordNet. The resulting sentiment value \u03c3i thus represent the low-level features that later serve as input to our predictive models."}, {"heading": "3.3. Tree-LSTM baseline", "text": "We draw upon the Tree-LSTM as a baseline, since it is widely regarded as the status quo for tree learning [29]. The Tree-LSTM takes a discourse tree as input and then processes EDU features while incorporating their position in the tree. For this purpose, it stacks individual LSTMs in the form of that tree (cf.\n12\nFigure 4) and adapts the ideas of both a memory cell and gates from traditional LSTMs, but extends these concepts to tree structures [29]. Here the underlying LSTM helps to overcome the problem of exploding gradients.\nIn the Tree-LSTM, each node j from the discourse tree is translated into a single LSTM unit, which comprises an input gate ij , an output gate oj , a memory cell cj and hidden state hj . In contrast to the standard LSTM, the Tree-LSTM contains not a single forget gate, but one forget gate fjk for each child k. This allows each parent node to recursively compute a representation from its immediate children. The input vectors to each LSTM unit are given by the hidden state hk and the memory cell ck from each child k \u2208 C(j), where C(j) is the set of children of parent j. This layout of arranging connections renders it possible for the Tree-LSTM to pass information upward in the tree, since every node can incorporate selected information from each child-LSTM. Figure 5 details the connection between the gates in a Tree-LSTM.\n13\nOur experiments later compare the performance of two different architectures of Tree-LSTM models, namely, the child-sum and N -ary Tree-LSTM [29]. Both are common in research, but vary in their connections between input and output gates. The former, the child-sum Tree-LSTM, sums the hidden states hk from the children k \u2208 C(j) in order to obtain a single input to the hidden state h\u0303parent of the parent. This approach loses any information regarding the order of the children, since it uses the same weights U (i), U (f), U (o) and U (u) for all children. In contrast, the N -ary Tree-LSTM requires a fixed, pre-defined number of N = |C(j)| children for each inner node. It then combines the child nodes by weighting their hidden states based on parameters U (i) m , U (f) km , U (o) m and U (u) m dependent on the index m = 1, . . . , N of the child.\n14\nMathematically, the child-sum Tree-LSTM transition equations are defined\nas\nh\u0303j = \u2211\nk\u2208C(j)\nhk, (2)\nij = sigmoid ( W (i)xj + U (i)h\u0303j + b (i) ) , (3)\nfjk = sigmoid ( W (f)xj + U (f)hk + b (f) ) , (4)\noj = sigmoid ( W (o)xj + U (o)h\u0303j + b (o) ) , (5)\nuj = tanh ( W (u)xj + U (u)h\u0303j + b (u) ) , (6)\ncj = ij uj + \u2211\nk\u2208C(j)\nfjk ck, (7)\nhj = oj tanh(cj), (8)\nwhere denotes the element-wise multiplication. Moreover, the above equations contain the weights W (i), W (f), W (o), W (u) and b(i), b(f), b(o), b(u) that must be learned from the data. Similarly, the N -ary Tree-LSTM obtains its memory cell and hidden state via\nij = sigmoid ( W (i)xj +\nN\u2211 m=1 U (i)m hjm + b (i)\n) , (9)\nfjk = sigmoid ( W (f)xj +\nN\u2211 m=1 U (f) kmhjm + b (f)\n) , (10)\noj = sigmoid ( W (o)xj +\nN\u2211 m=1 U (o)m hjm + b (o)\n) , (11)\nuj = tanh ( W (u)xj +\nN\u2211 m=1 U (u)m hjm + b (u)\n) , (12)\ncj = ij uj + N\u2211 m=1 fjm cjm, (13) hj = oj tanh(cj). (14)\nIn order to make sentiment predictions from the Tree-LSTM at the root\n15\nnode, we introduce an additional feedforward classification layer. Here we utilize a softmax classifier that predicts a class label y from the hidden state hroot of the root node. The softmax layer entails further weights W (s) and b(s), based on which it computes the probability p(y\u0302 |hroot) of the tree belonging to class y\u0302 via\ny = arg max y\u0302 p(y\u0302 | hroot) = arg max y\u0302\nsoftmax ( W (s) hroot + b (s) ) , (15)\nwith the negative log-likelihood of the true class label y as the cost function [26]."}, {"heading": "3.4. RST-LSTM", "text": "This section extends the previous Tree-LSTMs through an additional attention mechanism. As a result, this yields two variants of tensor-based TreeLSTMs, which we refer to as RST-LSTM. The RST-LSTM introduces two modifications that allow us to incorporate (1) the relation type between two nodes and (2) the hierarchy type (i. e. nucleus or satellite).\n16\nIn order to include the relation type, we replace the global LSTM that serves all nodes with one that is dependent on the relation type r \u2208 {1, . . . , n}. Figure 6 visualizes the idea schematically. We then select an LSTM\u03c1i for each node depending on its relation type \u03c1i.\nWe incorporate the hierarchy type \u03c4i (i. e. nucleus or satellite) by additionally weighting the cell state cj and the hidden state hj before they enter the above tensor-based LSTM. For this purpose, we introduce tensor-based weights\nW (c) = [ W\n(c) nucleus;W (c) satellite\n] , (16)\nW (h) = [ W\n(h) nucleus;W (h) satellite\n] . (17)\nWe then choose the weights according to the hierarchy type \u03c4i in the tree. This allows us to additionally discriminate the influence of nuclei and satellites.\nAccordingly, the RST-LSTM must simultaneously optimize both the tensorbased LSTM, as well as the hierarchy-related tensors W (c) and W (h) based on a combined objective function. We thus rearrange them as rank-3 tensors as follows: let W (x)[r, :] indicate the weight tensor for relation type r and W (x)[l, :] denote the weight tensor for a hierarchy type l \u2208 {nucleus, satellite}. On this basis, we now specify the new, updated equations for calculating the cell and\n17\nhidden state. As such, the child-sum RST-LSTM computes\nh\u0302j = W (h)[l, :]hj , (18) c\u0302j = W (c)[l, :] cj , (19)\nh\u0303j = \u2211\nk\u2208C(j)\nh\u0302k, (20)\nij = sigmoid ( W (i)xj + U (i)[r, :] h\u0303j + b (i)[r, :] ) , (21)\nfjk = sigmoid ( W (f)xj + U (f)[r, :] h\u0302k + b (f)[r, :] ) , (22)\noj = sigmoid ( W (o)xj + U (o)[r, :] h\u0303j + b (o)[r, :] ) , (23)\nuj = tanh ( W (u)xj + U (u)[r, :] h\u0303j + b (u)[r, :] ) , (24)\ncj = ij uj + \u2211\nk\u2208C(j)\nfjk c\u0302k, (25)\nhj = oj tanh(cj). (26)\nSimilarly, the N -ary RST-LSTM computes its representations via\nh\u0302j = W (h)[l, :]hj , (27) c\u0302j = W (c)[l, :] cj , (28)\nij = sigmoid ( W (i)xj +\nN\u2211 m=1 U (i)m [r, :] h\u0302jm + b (i)[r, :]\n) , (29)\nfjk = sigmoid ( W (f)xj +\nN\u2211 m=1 U (f) km [r, :] h\u0302jm + b (f)[r, :]\n) , (30)\noj = sigmoid ( W (o)xj +\nN\u2211 m=1 U (o)m [r, :] h\u0302jm + b (o)[r, :]\n) , (31)\nuj = tanh ( W (u)xj +\nN\u2211 m=1 U (u)m [r, :] h\u0302jm + b (u)[r, :]\n) , (32)\ncj = ij uj + N\u2211 m=1 fjm c\u0302jm, (33) hj = oj tanh(cj). (34)\n18\nAs a result, both the N -ary and child-sum RST-LSTM integrate the complete discourse tree into the neural network. As opposed to the works in the literature review, this approach allows us to encode both the relation type and the hierarchy type."}, {"heading": "3.5. Training data augmentation", "text": "Deep neural networks typically feature a complex structure with thousands of weights that need to be trained, which makes them prone to overfitting. A viable remedy is to artificially increase the number of training samples in order to better tune parameters. Such approaches are common in computer vision, where one extracts different crops from the same image and later considers each as a training instance. We thus propose similar techniques for tree structures that enlarge our training set. These algorithms take a tree as input and then slightly modify its structure in each epoch of training (one full training cycle on the training set). The first variant, called node reordering, swaps sub-trees, while the second, artificial leaf insertion, randomly exchanges a leaf for a node with two new children."}, {"heading": "3.5.1. Node reordering", "text": "Node reordering rearranges inner nodes while trying to preserve the inherent structure. That is, the text passages inside the nodes must keep their original order since the content might otherwise change its meaning or grammatical structure. Our approach thus randomly chooses an inner node n and relocates it to the position of its sibling m in the tree. The sibling m is then moved down the tree and becomes a child of n. Afterwards, the previous position of n is filled by one of its former children. As a result, the order of l, r and m from left to right is unchanged. The corresponding algorithm for an inner node n is sketched in Figure 7.\nAltogether, this approach for data augmentation tries to modify the structure slightly, thereby generating potentially different representations of the same tree. The extent of reordering depends on the level of n, since a reordering of\n19\na node at a higher level usually has a larger effect on the overall tree structure compared to a node at a lower level."}, {"heading": "3.5.2. Artificial leaf insertion", "text": "Artificial leaf insertion allows us to grow larger trees and helps our method to better learn complex trees. The insertion of leaves into a sub-tree is depicted in Figure 8. This approach randomly picks a leaf n from the tree and appends two newly created child nodes l and r which, subsequently present the leaves, while the n becomes an inner node. We compute \u03c3l and \u03c3r by multiplying \u03c3n by random weights \u03c9 \u2208 [0, 1] and (1\u2212 \u03c9), i. e.\n\u03c3l = \u03c9 \u03c3n, (35) \u03c3r = (1\u2212 \u03c9) \u03c3n. (36)\nThese update rules thus try to keep the overall information unchanged, but distribute the values from n into two separate children given a certain ratio \u03c9. We finally choose the relation type \u03c1n and the hierarchy type \u03c4n randomly.\n20"}, {"heading": "4. Experimental setup", "text": ""}, {"heading": "4.1. Datasets", "text": "We build upon earlier work and utilize two common datasets comprising movie reviews and corresponding user ratings. The first consists of 2000 movie reviews from Rotten Tomatoes [31], for which we perform 10-fold cross-validation and then average the predictive performance across splits. The second dataset comprises 50,000 reviews from the Internet Movie Database (IMDb), which are split evenly into 25,000 reviews for training and 25,000 for testing [32]. It includes, at most, 30 reviews for any one movie, since reviews for the same movie tend to have correlated ratings. Furthermore, the training and test sets contain a disjoint set of movies to avoid correlation based on movie-specific terms. Within the training and test sets, 12,500 reviews are labeled as positive and 12,500 reviews as negative in order to provide a balanced sample.\nAll reviews are preprocessed as follows: we perform tokenization, convert all characters to lowercase, and conduct stemming. The latter maps inflected words onto a base form, e. g. \u201cenjoyed\u201d and \u201cenjoying\u201d are both reduced to \u201cenjoy\u201d [33].\n21"}, {"heading": "4.2. Descriptive statistics", "text": "In total, the Rotten Tomatoes corpus contains 15,462 positive, 20,153 negative and 842,537 neutral terms as defined by the SentiWordNet dictionary. Hence, 1.76 % of the words are labeled positive, while 2.29 % convey a negative connotation. The IMDb reviews include 213,723 positive, 255,335 negative and 9,031,977 neutral words, resulting in 2.25 % of the words being positive and 2.69 % being negative.\nThe resulting discourse trees exhibit the following characteristics. In the case of reviews from Rotten Romatoes, they entail 51.09 EDUs on average, while this number plummets to 19.79 EDUs for IMDb reviews. The largest discourse tree contains 154 levels. Table 3 reports the relation types and corresponding frequencies in the corpus.\n22"}, {"heading": "4.3. Bag-of-words baseline", "text": "We construct na\u0308\u0131ve benchmarks with bag-of-words as follows. We count term frequencies (tf) and convert the numerical features into a document-term matrix. As a second baseline, we also scale the term frequencies using the term frequency-inverse document frequency approach (tf-idf), which puts stronger weights on characteristic terms [11]. Both feature spaces are then inserted a random forest, since this traditional machine learning classifier can detect highly non-linear relationships but still yields a satisfactory performance out-of-thebox. These benchmarks allow us to distinguish the sentiment conveyed by words from that conveyed by the discourse structure.\n23"}, {"heading": "4.4. Model evaluation", "text": "We proceed as follows in order to tune the model parameters (see Table 4). In the case of the random forest baseline, we identify the optimal parameters utilizing a grid search together with 10-fold cross-validation applied to the training set. In contrast, we reduce the computational requirements of the deep learning architectures by taking 20 % of the training data as a validation set in each epoch. After each epoch, we shuffle the observations and enlarge our training set by constructing additional samples based on our technique for data augmentation."}, {"heading": "5. Results", "text": "In this section, we evaluate the performance of our RST-LSTM and compare it to the previous baselines. The evaluation provides evidence that incorporating semantic structure into the task of sentiment analysis improves the predictive performance."}, {"heading": "5.1. Dataset 1: movie reviews from Rotten Tomatoes", "text": "Table 5 details the prediction results for the dataset featuring movie reviews from Rotten Tomatoes. The na\u0308\u0131ve benchmark with tf-idf features yields\n24\na balanced accuracy of 0.746 and an F1-score of 0.763. The approach with tffeatures, as well as the LSTM, achieves a similar performance. Here we see no clear indication that one of the baselines is consistently superior to another.\nWhen comparing the pruned RST trees, we observe that a higher pruning depth also increases the predictive performance. For instance, the best results stem from an RST tree pruned at level 4, thereby obtaining a balanced accuracy of 0.758 and an F1-score of 0.753. However, this still ranges below the bestperforming na\u0308\u0131ve benchmarks. Moreover, the inclusion of the relation type \u03c1R diminishes the predictive power compared to the approach with only sentiment scores [\u03c3N , \u03c3S ], amounting to a decrease of 0.007 in the balanced accuracy and 0.006 in the F1-score.\nThe simple tree learning based on the Tree-LSTM outperforms all of the previous benchmarks. It achieves a balanced accuracy of up to 0.774 and an F1-score of 0.781. Nevertheless, the Tree-LSTM is further surpassed by the RST-LSTM, which achieves an equal balanced accuracy of 0.777, but boosts the F1-score to 0.796. This amounts to an additional improvement of 0.033 (i. e. 4.2 %) in the F1-score. Altogether, the RST-LSTM benefits from the discourse-related information and thus performs best overall.\nFinally, we additionally note the following patterns: there is no consistent indication that either the child-sum or N -ary variant is consistently superior. However, the N -ary models usually benefit more strongly from data augmentation due to their additional degrees of freedom. By comparing the underlying algorithms for data augmentation, the results indicate a greater increase in predictive power from leaf insertion as compared to node reordering. Yet the best performing models generally facilitate both techniques, which is not surprising given that the dataset consists of only 2000 samples. The RST-based approaches also outperform models utilizing actual words as features. This suggests that a large portion of sentiment-related information is encoded in the discourse structure.\n25"}, {"heading": "5.2. Dataset 2: IMDb movie reviews", "text": "Table 6 reports the predictive results for the larger of the two datasets, which is based on 50,000 IMDb movie reviews. The random forest with tf-idf achieves a performance superior to the previous task, yielding an accuracy of 0.825 and an F1-score of 0.823. With regard to deep learning, the LSTM baseline improves the accuracy by 0.006 to 0.831, but shrinks the F1-score slightly by 0.002.\nThe accuracy is further increased by utilizing pruned discourse trees. Here the best-performing feature set is again obtained by cutting the tree at level 4, yielding an accuracy of 0.839 (an increase of 0.014 over the na\u0308\u0131ve benchmarks) and an F1-score of 0.837 (an improvement of 0.006). We also observe mixed results in terms of performance when incorporating the relation type \u03c1R.\nAgain, tree-structured LSTMs outperform all previous baselines. For instance, the N -ary Tree-LSTM raises both the accuracy and the F1-score of the na\u0308\u0131ve baselines by 0.024. This results in an accuracy of 0.849 (i. e. 2.9 % compared to the Tree-LSTM) and an F1-score of 0.847 (i. e. also 2.9 %).\nOur RST-LSTMs achieve the best performance overall, with an accuracy of 0.850 and an F1-score of 0.849, by utilizing data augmentation. However, we again find no general pattern indicating that one technique for enlarging the training set scores better than the other. A potential reason for the smaller improvements obtained from data augmentation compared to the previous dataset might be the larger number of training samples."}, {"heading": "5.3. Comparison", "text": "We additionally compare our RST-LSTM to the relation-specific attention mechanism in [17], which, in contrast, sums the representations in each recursive call and hence cannot distinguish between nucleus and satellite. In addition, their approach utilizes a recursive neural network, which is known to suffer from vanishing or exploding gradients [18]. In response to such shortcomings, we decided to draw upon a long short-term memory.\nWe proceed as follows in order to specifically evaluate the attention mechanism itself and leave all other parameters unchanged (i. e. identical to the\n27\nprevious experiments). That is, we feed the networks by utilizing EDU-level features that originate from the previous dictionary-based sentiment scores. The performance measurements indicate that the resulting predictive accuracy is inferior to the RST-LSTM. For the dataset from Rotten Tomatoes, their approach achieves a balanced accuracy of 0.761 and thus represents a decline of 0.016 (i. e. \u22122.1 %) compared to the best-performing N -ary RST-LSTM. In the case of IMDb reviews, their approach yields an accuracy of 0.831, while the N -ary RST-LSTM achieves 0.850. Hence, this work results in an improvement of 0.019 (i. e. 2.3 %)."}, {"heading": "5.4. Discussion", "text": "We now investigate the trained weights of our tensor-based attention mechanism inside the RST-LSTM. This facilitates insights into how the neural network processes the discourse and infers the sentiment from the semantic structure of textual materials. Figure 9 compares the normalized weights of the tensors U (u) m across different relation types m. The values result from using a child-sum RST-LSTM without data augmentation. Overall, the tensor weights between both datasets are highly correlated. The corresponding correlation coefficient stands at 0.640, statistically significant at the 99 % level. However, we observe large differences in the relative importance across the relation types. For instance, relation types such as background and textual-organization entail only a marginal importance, consistent with initial expectations. In contrast, the joint relation yields among the highest weights across both datasets.\n29\nWith regard to the hierarchy-related tensors, we find a greater importance (i. e. higher weights) for nuclei, as compared to satellites. For instance, the IMDb movie reviews lead to a nucleus weight of 0.738, whereas the weight of satellites totals a mere 0.588. This is in keeping with our intuition and the idea of RST since, in most cases, nuclei are more essential to the writer\u2019s purpose than satellites.\nFigure 10 illustrates the attention mechanism with an example movie review drawn from the Rotten Tomatoes dataset. Here we color the text according to the attention values inside the child-sum RST-LSTM without data augmentation. A darker text color refers to more essential pieces of information. For instance, the RST-LSTM assigns among the highest relevance to the passage \u201cthe plot is deceptively simple\u201d, whereas \u201ca popular-but-slow jock to run\u201d ranges among the lowest.\n30\nThe above discussion confirms that the tensors build an attention mechanism that learns to weight the importance of sentences based on their position and relations in the discourse tree. As a result, the RST-LSTM can localize the relevant parts of the document and ascertain the relative importance of sentiment scores"}, {"heading": "6. Conclusion", "text": "Deep learning for natural language predominantly builds upon sequential models such as LSTMs in order to incorporate context. While these models usually achieve a high predictive power when applied to short texts, the complexity of linguistic discourse hampers performance in relation to longer documents. As a remedy, our paper proposes an innovative, discourse-aware approach: we first parse the semantic structure based on rhetorical structure theory, thereby mapping the document onto a discourse tree that encodes its storyline. We then\n31\napply tailored tree-structured deep neural networks with an additional attention mechanism that enables us to directly learn the complete discourse tree. Each of the architectures entails more than 10,000 parameters, empowering the models to learn highly non-linear relationships.\nOur findings reveal that our RST-LSTM substantially outperforms the baselines. For instance, the best-performing RST-LSTMs achieve an improvement of 4.33 % and 3.16 % in the F1-score for both datasets as compared to using word features. These gains are partially owed to our techniques for data augmentation, which slightly alter existing trees in order to enlarge the size of the training set. Evidently, data augmentation presents a viable option to reduce the risk of overfitting. Furthermore, the underlying attention mechanism learns the relative importance of passages based on their position in the discourse tree. This facilitates insights into which discourse units convey essential pieces of information. Altogether, our work contributes to research in the field of text mining by advancing deep learning methods towards including semantic information when processing natural language.\nFuture research could benefit from a neural network in which the dictionarybased approach is replaced by sentiment scores that are learned from the content itself. For instance, one could think of an alternating approach where, in one step, the algorithm trains the RST-LSTM and, in every second step, optimizes an autoencoder based on the content of the EDUs. This could theoretically yield additional improvements in the predictive performance."}], "references": [{"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval 2 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Techniques and applications for sentiment analysis", "author": ["R. Feldman"], "venue": "Communications of the ACM 56 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining comparative opinions from 32  customer reviews for competitive intelligence", "author": ["K. Xu", "S.S. Liao", "J. Li", "Y. Song"], "venue": "Decision Support Systems 50 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "An empirical analysis of the antecedents of electronic commerce service continuance", "author": ["A. Bhattacherjee"], "venue": "Decision Support Systems 32 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "in: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL \u201905)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining online reviews for predicting sales performance: A case study in the movie domain", "author": ["X. Yu", "Y. Liu", "X. Huang", "A. An"], "venue": "IEEE Transactions on Knowledge and Data Engineering 24 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Does chatter really matter? Dynamics of usergenerated content and stock performance", "author": ["S. Tirunillai", "G.J. Tellis"], "venue": "Marketing Science 31 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "News-based trading strategies", "author": ["S. Feuerriegel", "H. Prendinger"], "venue": "Decision Support Systems 90 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Negation scope detection in sentiment analysis: Decision support for news-driven trading", "author": ["N. Pr\u00f6llochs", "S. Feuerriegel", "D. Neumann"], "venue": "Decision Support Systems 88 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Whose and what chatter matters? The effect of tweets on movie sales", "author": ["H. Rui", "Y. Liu", "A. Whinston"], "venue": "Decision Support Systems 55 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Foundations Of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "MIT Press, Cambridge MA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Advances in natural language processing", "author": ["J. Hirschberg", "C.D. Manning"], "venue": "Science 349 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["W.C. Mann", "S.A. Thompson"], "venue": "Text-Interdisciplinary Journal for the Study of Discourse 8 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1988}, {"title": "F", "author": ["A. Hogenboom", "F. Frasincar"], "venue": "de Jong, U. Kaymak, Using rhetorical structure in sentiment analysis, Communications of the ACM 58 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving sentiment analysis with document-level semantic relationships from rhetoric discourse structures", "author": ["J. M\u00e4rkle-Hu\u00df", "S. Feuerriegel", "H. Prendinger"], "venue": "50th Hawaii International Conference on System Sciences ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "F", "author": ["A. Hogenboom", "F. Frasincar"], "venue": "de Jong, U. Kaymak, Polarity classification using structure-based vector representations of text, Decision Support Systems 74 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural discourse structure for text categorization", "author": ["Y. Ji", "N. Smith"], "venue": "arXiv preprint arXiv:1702.01829 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks 5 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "F", "author": ["B. Heerschop", "F. Goossen", "A. Hogenboom", "F. Frasincar", "U. Kaymak"], "venue": "de Jong, Polarity analysis of texts using discourse structure, in: Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM \u201911)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Hilda: A discourse parser using support vector machine classification", "author": ["H. Hernault", "H. Prendinger", "D.A. DuVerle", "M. Ishizuka", "T. Paek"], "venue": "Dialogue and Discourse 1 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Representation learning for text-level discourse parsing", "author": ["Y. Ji", "J. Eisenstein"], "venue": "in: Proceedings of the 52nd Annual Meeting Annual Meeting on Association for Computational Linguistics (ACL \u201914)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Fine-grained sentiment analysis with structural features", "author": ["C. Zirn", "M. Niepert", "H. Stuckenschmidt", "M. Strube"], "venue": "in: Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP \u201911)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Extracting sentiment as a function of discourse structure and topicality", "author": ["M. Taboada", "K. Voll", "J. Brooke"], "venue": "Simon Fraser Univeristy School of Computing Science Technical Report ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Rhetorical structure theory for polarity estimation: An experimental study", "author": ["J.M. Chenlo", "A. Hogenboom", "D.E. Losada"], "venue": "Data & Knowledge Engineering 94 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Better document-level sentiment analysis from RST discourse parsing", "author": ["P. Bhatia", "Y. Ji", "J. Eisenstein"], "venue": "in: Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \u201915)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Learning: Adaptive Computation And Machine Learning Series", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press, Cambridge MA", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning representations by back-propagating errors", "author": ["D. Williams", "G.E. Hinton"], "venue": "Nature 323 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1986}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL \u201915)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "F", "author": ["S. Baccianella", "A. Esuli"], "venue": "Sebastiani, SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining, in: Proceedings of the International Conference on Language Resources and Evaluation (LREC \u201910)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "in: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL \u201904)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "in: Proceedings of the 49th Annual Meeting on Association for Computational Linguistics (ACL \u201911)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program 14 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1980}], "referenceMentions": [{"referenceID": 0, "context": "In this context, sentiment analysis facilitates the extraction of subjective information from user-generated content, or narrative materials in general, by quantifying the positivity or negativity of natural language [1, 2].", "startOffset": 217, "endOffset": 223}, {"referenceID": 1, "context": "In this context, sentiment analysis facilitates the extraction of subjective information from user-generated content, or narrative materials in general, by quantifying the positivity or negativity of natural language [1, 2].", "startOffset": 217, "endOffset": 223}, {"referenceID": 2, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 139, "endOffset": 148}, {"referenceID": 5, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 139, "endOffset": 148}, {"referenceID": 6, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 139, "endOffset": 148}, {"referenceID": 7, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 178, "endOffset": 184}, {"referenceID": 8, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 178, "endOffset": 184}, {"referenceID": 5, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 206, "endOffset": 213}, {"referenceID": 9, "context": "Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon financial news [8, 9] and predicting sales [6, 10].", "startOffset": 206, "endOffset": 213}, {"referenceID": 0, "context": "Sentiment analysis traditionally utilizes bag-of-words approaches, which merely count the frequency of words (and tuples thereof) to obtain a mathematical representation of documents in matrix form [1, 2].", "startOffset": 198, "endOffset": 204}, {"referenceID": 1, "context": "Sentiment analysis traditionally utilizes bag-of-words approaches, which merely count the frequency of words (and tuples thereof) to obtain a mathematical representation of documents in matrix form [1, 2].", "startOffset": 198, "endOffset": 204}, {"referenceID": 10, "context": "A remedy to this problem may be found in the form of n-grams, which construct tuples of n contiguous words from a text and thus incorporate the contextual information of words into the representation of text [11].", "startOffset": 208, "endOffset": 212}, {"referenceID": 0, "context": "Here research commonly utilizes n-grams of size two (bi-grams) and three (tri-grams), which are often enriched with additional features such as part-of-speech [1].", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "A recent review in Science points out that novel techniques are required to leverage semantics [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "RST structures documents hierarchically [13] by splitting the content into (sub-)clauses called elementary discourse units (EDUs).", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "For instance, one can reweigh the importance of passages based on their relation type [14] or depth [15] in the discourse tree.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "For instance, one can reweigh the importance of passages based on their relation type [14] or depth [15] in the discourse tree.", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "2 or 4 levels [15].", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Other approaches train machine learning classifiers based on the relation types as input features [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "The RST-LSTM also differs from the attention mechanism in [17], which can only exploit the relation type and not the hierarchy.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Furthermore, former approaches are based on traditional recursive neural networks, which are limited by the fact that they can persist information for only a few iterations [18].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "Rhetorical structure theory Rhetorical structure theory formalizes the discourse in narrative materials by organizing sub-clauses, sentences and paragraphs into a hierarchy [13].", "startOffset": 173, "endOffset": 177}, {"referenceID": 18, "context": "These EDUs are then connected by one of 18 different relation types, which represent edges in the discourse tree; see Table 1 for a list [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Table 1: Overview of the different relation types that connect elementary discourse units [19].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "Common implementations for documents consisting of multiple paragraphs are represented by the high-level discourse analyzer HILDA [20] and the DPLP parser [21], of which the DPLP parser currently achieves the better F1-score in identifying relation types .", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "Common implementations for documents consisting of multiple paragraphs are represented by the high-level discourse analyzer HILDA [20] and the DPLP parser [21], of which the DPLP parser currently achieves the better F1-score in identifying relation types .", "startOffset": 155, "endOffset": 159}, {"referenceID": 21, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 13, "endOffset": 29}, {"referenceID": 18, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 13, "endOffset": 29}, {"referenceID": 13, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 13, "endOffset": 29}, {"referenceID": 15, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 13, "endOffset": 29}, {"referenceID": 22, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "SentiWordNet [22, 19, 14, 16], hand-crafted dictionaries [23] or domain-specific dictionaries [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "A recent approach calculates ex ante vector representations for the EDUs based on word embeddings [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "Among the most common methods are simple weighting rules that aggregate the sentiment scores of EDUs based on the tree structure [19, 14].", "startOffset": 129, "endOffset": 137}, {"referenceID": 13, "context": "Among the most common methods are simple weighting rules that aggregate the sentiment scores of EDUs based on the tree structure [19, 14].", "startOffset": 129, "endOffset": 137}, {"referenceID": 22, "context": "the root node) of the discourse tree and scale the relative importance based on (hand-crafted) weights [23, 19, 14].", "startOffset": 103, "endOffset": 115}, {"referenceID": 18, "context": "the root node) of the discourse tree and scale the relative importance based on (hand-crafted) weights [23, 19, 14].", "startOffset": 103, "endOffset": 115}, {"referenceID": 13, "context": "the root node) of the discourse tree and scale the relative importance based on (hand-crafted) weights [23, 19, 14].", "startOffset": 103, "endOffset": 115}, {"referenceID": 23, "context": "The underlying weights can also be optimized using logistic regression [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "In contrast, other works specifically focus on the hierarchy labels at leaf level, arguing that this choice facilitates a more fine-grained evaluation [14], while neglecting the discourse tree from above.", "startOffset": 151, "endOffset": 155}, {"referenceID": 13, "context": "To fill the gap between top-level and leaf-level analysis, recent research also applies a recursive weighting scheme that utilizes a scaling factor to diminish the influence of increasing depth [14, 15].", "startOffset": 194, "endOffset": 202}, {"referenceID": 14, "context": "To fill the gap between top-level and leaf-level analysis, recent research also applies a recursive weighting scheme that utilizes a scaling factor to diminish the influence of increasing depth [14, 15].", "startOffset": 194, "endOffset": 202}, {"referenceID": 14, "context": "2 or 4 levels [15].", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].", "startOffset": 56, "endOffset": 68}, {"referenceID": 23, "context": "Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].", "startOffset": 56, "endOffset": 68}, {"referenceID": 13, "context": "Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].", "startOffset": 56, "endOffset": 68}, {"referenceID": 21, "context": "Some works also incorporate relation types between EDUs [19, 24, 14] or categorize them into contrastive or non-contrastive relations, which are then weighted separately [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "A potential remedy is to traverse the RST tree with a recursive neural network [25]; however, this approach only incorporates the edges and lacks information regarding the relation type.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "attention mechanism [17].", "startOffset": 20, "endOffset": 24}, {"referenceID": 25, "context": "Representation learning for sequential and tree data Recent advances in deep neural networks have rendered it possible to learn representations of unstructured data such as sequences or text [26].", "startOffset": 191, "endOffset": 195}, {"referenceID": 26, "context": "This can, for instance, be achieved by recurrent neural networks, which entail an internal architecture in the form of a directed cycle, thereby creating an internal state with which to learn dependent structures [27].", "startOffset": 213, "endOffset": 217}, {"referenceID": 17, "context": "However, in practice, information only persists for a few iterations [18].", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "These enhance recurrent neural networks by capturing long dependencies among input signals [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "This tree-structured LSTM network traverses trees bottomup in order to generate representations of the underlying structure [29].", "startOffset": 124, "endOffset": 128}, {"referenceID": 1, "context": "[2 5 ] R o t t e n t o m a t o e s r e v ie w s , U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d D P L P H ie r a r c h ic a l w e ig h t in g r u le 7 3 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[2 5 ] R o t t e n t o m a t o e s r e v ie w s , U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d D P L P H ie r a r c h ic a l w e ig h t in g r u le 7 3 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 4 ] B L O G S 0 6 B lo g p o s t r a n k in g W e ig h t s o p t im iz e d D ic t io n a r y -b a s e d S P A D E T o p -s p li t w e ig h t in g 3 7 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[2 4 ] B L O G S 0 6 B lo g p o s t r a n k in g W e ig h t s o p t im iz e d D ic t io n a r y -b a s e d S P A D E T o p -s p li t w e ig h t in g 3 7 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1 9 ] R o t t e n t o m a t o e s r e v ie w s U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d S P A D E P o s it io n -b a s e d w e ig h t in g r u le 7 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[1 9 ] R o t t e n t o m a t o e s r e v ie w s U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d S P A D E P o s it io n -b a s e d w e ig h t in g r u le 7 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1 4 ] R o t t e n t o m a t o e s r e v ie w s U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d H IL D A P o s it io n -b a s e d w e ig h t in g 7 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[1 4 ] R o t t e n t o m a t o e s r e v ie w s U s e r r a t in g H a n d c r a ft e d w e ig h t in g fa c t o r D ic t io n a r y -b a s e d H IL D A P o s it io n -b a s e d w e ig h t in g 7 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1 6 ] R o t t e n t o m a t o e s r e v ie w s , U s e r r a t in g W e ig h t s o p t im iz e d b y S V M D ic t io n a r y -b a s e d S P A D E T o p a n d le a fs p li t w e ig h t in g 3 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[1 6 ] R o t t e n t o m a t o e s r e v ie w s , U s e r r a t in g W e ig h t s o p t im iz e d b y S V M D ic t io n a r y -b a s e d S P A D E T o p a n d le a fs p li t w e ig h t in g 3 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "J i a n d S m it h [1 7 ] Y e lp r e v ie w s , U s e r r a t in g o r T r a n s fo r m a t io n o f R S T t r e e H id d e n s t a t e s o f D P L P A v e r a g e o f a ll E D U s 7 7 7", "startOffset": 19, "endOffset": 25}, {"referenceID": 6, "context": "J i a n d S m it h [1 7 ] Y e lp r e v ie w s , U s e r r a t in g o r T r a n s fo r m a t io n o f R S T t r e e H id d e n s t a t e s o f D P L P A v e r a g e o f a ll E D U s 7 7 7", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "[1 5 ] F in a n c ia l d is c lo s u r e s D ir e c t io n a l s t o c k W e ig h t s o p t im iz e d b y g r id -s e a r c h D ic t io n a r y -b a s e d H IL D A H ie r a r c h ic a l w e ig h t in g r u le 7 3 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[1 5 ] F in a n c ia l d is c lo s u r e s D ir e c t io n a l s t o c k W e ig h t s o p t im iz e d b y g r id -s e a r c h D ic t io n a r y -b a s e d H IL D A H ie r a r c h ic a l w e ig h t in g r u le 7 3 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 3 ] O p in io n s r e v ie w s , U s e r r a t in g H a n d c r a ft e d w e ig h t s D ic t io n a r y -b a s e d S P A D E T o p -s p li t w e ig h t in g 7 U p t o le v e l 1 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[2 3 ] O p in io n s r e v ie w s , U s e r r a t in g H a n d c r a ft e d w e ig h t s D ic t io n a r y -b a s e d S P A D E T o p -s p li t w e ig h t in g 7 U p t o le v e l 1 3", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 2 ] M D S A U s e r r a t in g D ic t io n a r y -b a s e d M a r k o v lo g ic n e t w o r k C o n t r a s t iv e a n d 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[2 2 ] M D S A U s e r r a t in g D ic t io n a r y -b a s e d M a r k o v lo g ic n e t w o r k C o n t r a s t iv e a n d 7 7", "startOffset": 0, "endOffset": 6}, {"referenceID": 20, "context": "Discourse parsing We generate discourse trees for our datasets by utilizing the DPLP parser [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Polarity features We follow common procedures in sentiment analysis and utilize a pre-defined dictionary that labels terms as positive or negative [1, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 1, "context": "Polarity features We follow common procedures in sentiment analysis and utilize a pre-defined dictionary that labels terms as positive or negative [1, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 29, "context": "0 dictionary [30], which provides sentiment labels for 117,659 words.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "Tree-LSTM baseline We draw upon the Tree-LSTM as a baseline, since it is widely regarded as the status quo for tree learning [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "Figure 4) and adapts the ideas of both a memory cell and gates from traditional LSTMs, but extends these concepts to tree structures [29].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Our experiments later compare the performance of two different architectures of Tree-LSTM models, namely, the child-sum and N -ary Tree-LSTM [29].", "startOffset": 141, "endOffset": 145}, {"referenceID": 25, "context": "with the negative log-likelihood of the true class label y as the cost function [26].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "We compute \u03c3l and \u03c3r by multiplying \u03c3n by random weights \u03c9 \u2208 [0, 1] and (1\u2212 \u03c9), i.", "startOffset": 61, "endOffset": 67}, {"referenceID": 30, "context": "The first consists of 2000 movie reviews from Rotten Tomatoes [31], for which we perform 10-fold cross-validation and then average the predictive performance across splits.", "startOffset": 62, "endOffset": 66}, {"referenceID": 31, "context": "The second dataset comprises 50,000 reviews from the Internet Movie Database (IMDb), which are split evenly into 25,000 reviews for training and 25,000 for testing [32].", "startOffset": 164, "endOffset": 168}, {"referenceID": 32, "context": "\u201cenjoyed\u201d and \u201cenjoying\u201d are both reduced to \u201cenjoy\u201d [33].", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "As a second baseline, we also scale the term frequencies using the term frequency-inverse document frequency approach (tf-idf), which puts stronger weights on characteristic terms [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 16, "context": "Comparison We additionally compare our RST-LSTM to the relation-specific attention mechanism in [17], which, in contrast, sums the representations in each recursive call and hence cannot distinguish between nucleus and satellite.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "In addition, their approach utilizes a recursive neural network, which is known to suffer from vanishing or exploding gradients [18].", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "Prominent applications of sentiment analysis are countless, including areas such as marketing, customer service and communication. The conventional bag-ofwords approach for measuring sentiment merely counts term frequencies; however, it neglects the position of the terms within the discourse. As a remedy, we thus develop a discourse-aware method that builds upon the discourse structure of documents. For this purpose, we utilize rhetorical structure theory to label (sub-)clauses according to their hierarchical relationships and then assign polarity scores to individual leaves. To learn from the resulting rhetoric structure, we propose a tensor-based, tree-structured deep neural network (named RST-LSTM) in order to process the complete discourse tree. The underlying attention mechanism infers the salient passages of narrative materials. In addition, we suggest two algorithms for data augmentation (node reordering and artificial leaf insertion) that increase our training set and reduce overfitting. Our benchmarks demonstrate the superior performance of our approach. Ultimately, this work advances the status quo in natural language processing by developing algorithms that incorporate semantic information.", "creator": "LaTeX with hyperref package"}}}