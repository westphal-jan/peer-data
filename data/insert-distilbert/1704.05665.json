{"id": "1704.05665", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "CNN based music emotion classification", "abstract": "music emotion recognition ( mer ) is usually regarded as a classic multi - label tagging task, observing and each segment of music can inspire specific emotion tags. most researchers extract acoustic features from music and explore the relations between these features traits and their corresponding emotion tags. considering the inconsistency of emotions inspired by the same music segment used for human beings, seeking for the key acoustic features that really affect on emotions is really a challenging task. in this paper, suppose we propose a novel mer method by using deep convolutional neural network ( cnn ) on the distributed music spectrograms that emerge contains both the original time and frequency encoding domain information. by the proposed lame method, no additional effort on extracting specific features required, which is left to the training adaptation procedure of the cnn model. experiments are conducted on the standard cal500 and cal500exp dataset. results show that, for both datasets, the proposed method outperforms state - of - the - art methods.", "histories": [["v1", "Wed, 19 Apr 2017 09:28:39 GMT  (2887kb,D)", "http://arxiv.org/abs/1704.05665v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.MM cs.LG", "authors": ["xin liu", "qingcai chen", "xiangping wu", "yan liu", "yang liu"], "accepted": false, "id": "1704.05665"}, "pdf": {"name": "1704.05665.pdf", "metadata": {"source": "CRF", "title": "CNN BASED MUSIC EMOTION CLASSIFICATION", "authors": ["Xin Liu", "Qingcai Chen", "Xiangping Wu", "Yan Liu", "Yang Liu"], "emails": ["wxpleduole}@gmail.com,", "csyliu@comp.polyu.edu.hk,", "csygliu@comp.hkbu.edu.hk"], "sections": [{"heading": null, "text": "Index Terms\u2014 music emotion recognition, convolutional neural network, spectrogram\n1. INTRODUCTION\nIt is well known that different type of music make quit different influences on our emotion. Researchers have shown that, music, explained as organized sound, can resonate with our nerve tissue [1]. As researchers mentioned, for little babies who still do not know what music is, what language is and even what they see is, can make responds to what they hear. It is more likely a biological instinct, just the interaction between sound rhythm or melody and their brain [2, 3]. In fact, the delicate relationship between music and emotion has already been explored by numerous researchers for a long period [4]. Barthet et al. [5] gave a detail introduction about music emotion recognition (MER) task. Wieczorkowska et al. [6] firstly formulated MER as a multi-label classification problem. And then many researchers follow this opinion. In multi-label classification, each training or test music segment\nsample is a sequence of features that have been assigned multiple labels. Each label indicates one type of emotion.\nFor a classification task, feature selection is one of the most important tasks. Though there is still no standard guidance for selecting features that contribute most to the representation of music [7], acoustic features are still most prevalent in the feature selecting procedure [8]. Acoustic features mainly consist of rhythmic features, timbre features and spectral features [9]. Rhythmic features are derived by extracting periodic changes from a beat histogram [9]. Timbre features consist of a series of Zero Crossing Rate, MFCC [10] and Chroma [11]. Spectral features include Spectral Flatness Measure, Spectral Centroid, Spectral Crest Factor, Spectral Rolloff and Spectral Flux. All these features can represent music respectively or mutually in a numeric way. In addition to features selection, the other important task of MER is the choosing of classifier. Researchers have tried some classifiers. For example, Calibrated Label Ranking classifier using a Support Vector Machine (CLRSVM) [12], Random k-Labelsets (RAkEL) [13], Back-propagation for Multi-Label Learning (BPMLL), Multi-Label K-Nearest Neighbor (MLkNN) [14] and Binary Relevance kNN (BRkNN) etc. Among these classifiers, in most cases, CLRSVM outperforms the rest [5].\nThough great improvements have been made by researchers, the state-of-the-art of MER is still far from satisfactory. Considering the inconsistency of emotions inspired by the same music segment for human beings, seeking for the key acoustic features that actually affect on emotions is a really challenging task and is a crisis obstacle of solving MER problem. To address this problem, in this work, we propose a novel model based on deep CNN architecture. This model directly uses the spectrogram of music audio without complex artificially selected features. Our main contributions include: 1) a novel CNN based MER model that only uses the music audio spectrogram as input is proposed. By this model, manually selection of complicate features is avoided, which not only simplifies the process of model construction, but also keeps most of the original time and frequency domain information. 2) The convolution method on local time ar X\niv :1\n70 4.\n05 66\n5v 1\n[ cs\n.M M\n] 1\n9 A\npr 2\nand frequency of spectrogram is proposed to address the issue of variance in time length for different music segments. 3) MER experiments are conducted on standard CAL500 and CAL500exp dataset and the results show that the proposed method outperforms existing methods and gets obvious improvements on state-of-the-art F1 measure.\nThis work is organized as follows. In Section 2 we detail describe the spectrogram for specific data and the structures of our deep neural network. In Section 3 we discuss the experiments on the dataset CAL500 [15] and CAL500exp [16] and compare the results with state-of-the-arts algorithms. Finally, some conclusions and future work are drawn in Section 4.\n2. PROPOSED METHOD"}, {"heading": "2.1. CNN framework for music emotion classification", "text": "Fig.1 shows the CNN framework for the music emotion classification. Unique from existing methods, this framework uses only the spectrogram of the audio signal as input. And a network with few convolution-pooling layers, hidden layer and a SOFTMAX classifier is then constructed to extract features and classify the emotions of given music segment. As well known that emotions inspired by a segment of music are close relevant to both the rhythm and melody of music [17], which are mainly determined by the distribution and variance of signal energy on time and frequency domains. Spectrogram, as a type of representation for variances of frequency spectrum with time, not only presents a visualization tool, but also an important type of rich-information feature for audio signal analysis [18, 19]. The spectrogram is computed via the Short-Time-Fourier-Transformation of audio signal along the time axis as below formula [20]:\nSpec(t, f) = |nfft(t, f)|2, (1)\nThough the phase information is lost by transform audio signals into their spectrogram, the power variance information along both frequency and time axis are well presented [19]. As an example, Fig. 2 shows the waveforms and spectrograms of two music segments with emotion tags of emotional, exciting, happy and powerful and calming, tender, mellow and pleasant respectively. It is obvious that from Fig. 2 we can more easily tell the differences between the two music segments via the spectrograms than though their waveforms.\nConsidering the capabilities shown by CNN on the capturing of complexity features for 2 or 3-dimensional pictures, using CNN to extract features from spectrogram may also be an effective way. To verify it, in this paper, the general CNN layers for feature extraction of music spectrogram are introduced. As shown in Fig. 1, each spectrogram is convolved with n convolution kernel functions and n feature map matrices are produced correspondingly. The pooling operations are then executed on each feature map with multiple kernels\nto generate self-adapting features by neural network. More convolution and pooling operations are further conducted according to the application requirements.\nIn fact, CNN operations executed on spectrogram are independent from the emotion classification and thus applicable in any music processing tasks. For the goal of this paper, a multi-layer perceptions (MLP) with hidden layers and softmax operations for specific emotion tag set are presented as the last part of the framework. The input layer of the MLP is generated by stretching and concatenating the matrixes of the last CNN layer. To tuning the whole CNN based network, supervised learning is conducted by the differences of the softmax outputs with standard emotion tags of the input music segment.\nIn following sections, we detail describe the main processing of different stages in above framework."}, {"heading": "2.2. Preprocessing of music spectrogram", "text": "The preprocessing of spectrogram is a key point of successfully applying CNN on music spectrogram. It is because that the input to CNN is required as a fix dimensional matrix, while the length of music segment given for emotion classification is usually variant duration.\nAs Eq. (1) shows that, the spectrogram of a music segment is produced by computing a series value sets of the discrete-time STFT (t, f) at different time points of the music segment along the time axis. At each time point, the number of values produced along frequency axis is determined by the needs of frequency resolution for a given application. For this reason, we firstly determine the frequency points K of the STFT (t, f). Then the number of time points is set as the same of the frequency points to generate a K\u00d7K matrix. Since we directly using matlab to generate the spectrogram, in this paper, we select the equal-distance time point by computing the overlap signal number of two concatenated windows for a given music segment as below:\nNoverlap = (M + 1) \u2217Nwin szie \u2212Nmusic len\nM (2)\nHere, M , Nwin szie and Nmusic len denote the frequency dimension, window length and number of signals contained in the given music segment. Negative overlap signal means number of skipping signal from previous window to the next window. To simplify the process, the window length is equal to nfft, the number of points for the FFT. The frequency dimension M = nfft/2 + 1.\nTo construct the fixed dimensions of input matrix for CNN network, there must be more sophisticate methods existing. But our goal is focused on the effectiveness of CNN architecture for spectrogram based music emotion classification, so which is left for further study."}, {"heading": "2.3. Convolutional neural network for music spectrogram", "text": "As mentioned in the previous section, the emotions formed when people listen to music mostly will go through a cumulative process as time goes on. Then this is another reason why we choose spectrogram to represent one music. In convolutional neural network, the input feature maps will be calculated through the operation called local field to generate new feature maps. The concrete implementation in the network is that the nodes for calculating in the same layers are neighboring and the nodes in two adjective layers are not fully connected. So this operation can satisfy the need of calculating the relevance of adjacent time and frequency.\nAs shown in Fig.3, it shows the local convolution operation. In the stage of spectrogram, there are a lot of windows like area A and B. The values in one area are calculated by some filters through convolution computing, each filter will\ngenerate one feature map. The convolution calculation [21] is\nzli = \u03c3(w (l,fk)zl\u22121j\u2217 + b (l,fk)) (3)\nwhere zli means the output of feature map in layer l at location i and corresponds to the one point of An or Bn in Fig.3, w(l,fk) is the parameters of k-th filter from layer l\u22121 to l and fk is some filter in Fn = [f0, f1, ..., fn], b(l,fk) is the bias for layer l, zl\u22121j\u2217 means the areas of location j in layer l \u2212 1 and corresponds to the area A or B in Fig.3, \u03c3(.)is the activation function (e.g., Sigmoid or Relu [22]).\nFor example, area A is a square of time varying from t0 to t1 and frequency varying from f0 to f0. Through different filters, we will get the subarea A1 A2 to An in next feature maps. Area B is the same as A. Other convolutions also act as these steps. This is also simulating the procedure of generating emotions over time and frequency. After each convolution, we take a max-pooling in every unit window for every filter as\nzl,fki = max(z l\u22121,fk 2i\u22121 , z l\u22121,fk 2i ) (4)\nthis means the output of pooling map is the maximum value selected from the pooling window of its feature map. In Fig.3, after max-pooling, we will get such output an or bn in pooling maps. In convolutional layers, input feature map is calculated through repeated convolution-pooling operation. The difference among them is that the different filters for generating new feature map. In the final step, the output size of feature maps is a vector, so we just need to reshape these nodes as the final output features of convolutional layers for hidden layer."}, {"heading": "2.4. Emotion classification based on CNN of music spectrogram", "text": "In hidden layer and classifier layer, the input is the same as the reshaped vectors in previous layer. The nodes in hidden layer are fully connected. Then after a few full-connected layers, we may get fixed vector as the input of softmax to classify. For softmax, the dimension of output equals to the number of emotion tags. Each dimension corresponds to one tag. Once the numerical value exceeds a certain threshold, we will conclude that the tag belongs to this music. We also train our neural network in some valid techniques, such as dropout [22], activation function and so on.\n3. EXPERIMENTS"}, {"heading": "3.1. Dataset", "text": "We tested the music emotion recognition performance of the proposed approach. A series of experiments were performed on CAL500exp and CAL500. CAL500exp is an enriched version of the well-known CAL500. Wang et al. [16] published the dataset. Labels of CAL500exp are annotated in the segment level instead of track level in CAL500. In the other words, in CAL500exp, each song contains several segments split from itself. And each segment is annotated as a dependent data from 18 emotion tags. So in CAL500exp dataset, there are total 3223 items. While in CAL500, each whole song is regarded as one train or test data and each song is scored on 18 emotions form 1 to 5 by different listeners, then we confirm the emotion labels of this song by the means that at least 80 percent of all listeners agree the score [15]. The number in CAL500 is quite less than CAL500exp. There are about 502 items."}, {"heading": "3.2. Evaluation criterion", "text": "As a multi-label task, some common criteria were used to evaluate the performance, namely label-based metrics and example-based metrics. Most researchers use macro average and micro average to evaluate the overall performance across multiple labels.\nIn this paper, we will focus on the criteria of precision (P), recall (R) and F1 score which considers both the precision and the recall. As a contrast, some other criteria such as hamming\nloss, AUC score, average precision score(AP) and one error will also be introduced in brief. The computational method can be found from python-sklearn which is a python module for machine learning and data mining at [23]. For some criteria including F1 score, AUC score and average precision, the larger the metric value is, the better performance it shows. As for another two metrics hamming loss and one error, it is just the opposite. Smaller values indicate the better performance. For each train set, we stop the training procedure until CNN reached certain epoch iteration."}, {"heading": "3.3. Cross validation", "text": "we constructed train set, validation set and test set based on these segments and whole songs. We proposed ten-fold cross validation for train, validation and test. In both data set, we performed ten-fold cross validation randomly. 3223 segments and 502 songs were first disrupted into a random order, then we selected 10 percent of the same unordered set as test set and the rest as train or validation set for ten times on each dataset respectively.\nTen-fold cross validation aims to make a contrast with those state-of-the-arts already known. In CAL500exp, each fold contains the similar number of train and test set with the average differences in 5, namely about 2902 and 321. While in CAL500, each fold contains 452 songs for train and 50 for test. Since CNN model generates a series of parameters in each epoch iteration and each will bring different results. So parts of both train set are used for validation to find proper parameters. In our experiments we choose ten epochs as benchmark based on validation set. And the average value of ten epochs represents the performance of this fold."}, {"heading": "3.4. Model structure", "text": "As shown in Fig. 1, the initial input is the spectrogram with fixed size. Compared to the general input size of CNN, our input structure may be a little complex. But such a scale can retain more information of a long music. In price, more calculations and memory space will be cost. Besides we need to know that the size is not the more complex the better. If more complex, there will be more redundant information which will lead to a negative impact on the final result. In order to balance the loss of information and the cost of time and space complexity, we need to select a balanced size. In our model, there are two key points that we need to plan. One is the size of input spectrogram, the other is the structure of CNN.\nFirst we need to know is that the size of spectrogram is associated with the length of music. After analyzing music in both dataset, we find that one song in CAL500 is about 5 minutes long while 5 seconds more or less for one segment in CAL500exp. So some test experiments are conducted to determine the size of spectrogram. In most case, we will make\nit square matrix. Since the vertical dimension is calculated based on nffts by half of nffts plus one, we may try different sizes by changing nffts to 256, 512, 1024 and other else. We set the initial size as 257 \u00d7 257 by rule of thumb, so all test sizes are 1025\u00d7 1025, 513\u00d7 513 and 129\u00d7 129 in total. We take final performance and time cost into consideration for each input size in a same structure.\nTable 1 shows these experiments, we find that if we regard 257\u00d7257 as baseline at both considerations, bigger size such as 1025\u00d71025 and 513\u00d7513 can get better performance and smaller size shows poor performance, but 1025 \u00d7 1025 take much more time cost that is far beyond the other size. With overall consideration, the spectrogram in CAL500 is 513 \u00d7 513 and in CAL500exp it is 257\u00d7 257.\nNext one we need to know is the structure of CNN including total layers and nodes in each layer. We first define two kinds of neural network, one simple network contains four convolutional layers with nodes no more than 50 in each and one hidden layer, the other one seems more complex with nodes from 100 to 200 in each and three or more hidden layers. The simple contrasts on macro metric of both dataset on different network are given in Table 2. From Table 2, we can see that simple network outperforms complex network nearly on all metrics for both dataset. Then through these experiments, we confirm the size of spectrogram and structure of CNN."}, {"heading": "3.5. Results", "text": "In previous parts, we introduce the size of spectrogram and structure of CNN. During training our model, we need to adjust the parameters in the network. We train the model with iterations(also called epoch). And in each iteration, we will get a cost to indicate the processing of current epoch. The cost can not only tell us whether the model is working but also help us to select the parameters for testing.\nFig.4 gives an example of cost reduction with the increasing of iterations when training the model. Since there may be hundreds of epochs, we choose the average cost of ten epochs as one coordinate. The trend of cost indicates the validity of model and allows us to do test on different epochs to find the optimal parameters.\nAfter a series of certain iterations, we finish the train. According to the validation set and parameter adjustment, we obtain the performance of all metrics on both dataset. The final results of ten-fold cross validation with our model are shown in Table 3. Since we cannot try all parameters that represent convergence, there will be max 0.002 errors for the result. Wang et al. [16] report the results when they published the CAL500exp dataset. The results are as shown in Table 3. And on CAL500, the results are shown in Table 4, compared with [16] and [1].\nFrom Table 3 and 4, we can see that through the contrast of those results published, our proposed model outperforms the state-of-the-art on the metrics of macro F1 and micro F1.\nTable 5 gives the results of the rest metrics on CAL500 and CAL500exp. From Table 5, we can also see that the variable trend of the rest metrics is also in accordance to F1 score on both dataset. But the D-value differs greatly in each metric\nbetween CAL500 and CAL500exp. We guess that the reason mainly lies in the following two aspects. One point is the data quantity, especially for CAL500. After the whole set was divided into validation and test set in the proportion of 10%, the number for train was greatly reduced which actually is inadequate for deep neural network. The other one is that the criterion to ascertain labels for music is ambiguous in CAL500. Not like explicit labels of each music in CAL500exp, there is only a series of scores between 1 and 5 on each label from several different users. Then we need to stipulate labels in some way. This may cause inaccurate compared to CAL500exp. Both aspects may be the exact cause of the results above.\n4. CONCLUSIONS AND DISCUSSIONS\nIn this work, we propose a novel method combining original music spectrogram with deep convolutional neural network (CNN) to predict the emotion tags. Though we have got a obvious performance gain by this model, there are still a lot works to do since the final result of the method is only 0.709 on micro F1 measure. First, this CNN architecture is not fine-tuned, there is still a lot of improved space on both the convolution step and the feedback training step. Second, for a song or a segment of song with tagged emotions, only fixed numbers of segments are sampled and there is no detail study on the selection of time or frequency points. Though the advantages of CNN on extracting useful features from raw data, there is still no research on the meaning of CNN outputs, which makes it difficult to further understand and deduce the source of inspired emotions for a given music.\n5. REFERENCES\n[1] Yang Liu, Yan Liu, Yu Zhao, and Kien A Hua, \u201cWhat strikes the strings of your heart?\u2014feature mining for music emotion analysis,\u201d IEEE Transactions on Affective Computing, vol. 6, no. 3, pp. 247\u2013260, 2015.\n[2] Teruhiro Nakada, Yukihiko Fujii, Kiyotaka Suzuki, and Ingrid L Kwee, \u201cMusical brain revealed by high-field (3 tesla) functional mri,\u201d Neuro report, vol. 9, no. 17, pp. 3853\u20133856, 1998.\n[3] John A Sloboda and Patrik N Juslin, \u201cPsychological perspectives on music and emotion,\u201d 2001.\n[4] Patrik N Juslin and Petri Laukka, \u201cExpression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening,\u201d Journal of New Music Research, vol. 33, no. 3, pp. 216\u2013237, 2010.\n[5] Mathieu Barthet, Gyorgy Fazekas, and Mark Sandler, \u201cMusic emotion recognition: From content- to contextbased models,\u201d Computer music modeling and retrieval, pp. 228\u2013252, 2012.\n[6] Alicja A Wieczorkowska, Piotr Synak, and Zbigniew W Ras\u0301, \u201cMulti-label classification of emotions in music,\u201d in Proc. of Intelligent Information Processing and Web Mining, vol. 35, pp. 307\u2013315, 2006.\n[7] Chris Sanden and John Z Zhang, \u201cAn empirical study of multi-label classifiers for music tag annotation,\u201d in Proc. of the 12th International Society for Music Information Retrieval (ISMIR) Conference, pp. 717\u2013722, 2011.\n[8] Florian Eyben, Glaucia L Salomao, Johan Sundberg, Klaus R Scherer, and Bjorn Schuller, \u201cEmotion in the singing voice\u2014a deeperlook at acoustic features in the light of automatic classification,\u201d Eurasip Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, pp. 1\u20139, 2015.\n[9] Mudiana Mokhsin Misron, Nurlaila Binti Rosli, Norehan Abdul Manaf, and Hamizan Abdul Halim, \u201cMusic emotion classification (mec): Exploiting vocal and instrumental sound features,\u201d Recent Advances on Soft Computing and Data Mining, pp. 539\u2013549, 2014.\n[10] Beth Logan, \u201cMel frequency cepstral coefficients for music modeling,\u201d in Proc. of the 1st International Symposium on Music Information Retrieval (ISMIR 2000)Conference, 2000.\n[11] Erik M Schmidt, Douglas Turnbull, and Youngmoo E Kim, \u201cFeature selection for content-based, time-varying musical emotion regression,\u201d in Proc. of the 11th ACM SIGMM International Conference on Multimedia Information Retrieval (MIR), pp. 267\u2013274, 2010.\n[12] Johannes Furnkranz, Eyke Hullermeier, Eneldo Loza Mencia, and Klaus Brinker, \u201cMultilabel classification via calibrated label ranking,\u201d Machine Learning, vol. 73, no. 2, pp. 133\u2013153, 2008.\n[13] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas, \u201cRandom k-labelsets for multilabel classification,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 23, no. 7, pp. 1079\u20131089, 2011.\n[14] Minling Zhang and Zhihua Zhou, \u201cMl-knn: A lazy learning approach to multi-label learning,\u201d Pattern Recognition, vol. 40, no. 7, pp. 2038\u20132048, 2007.\n[15] Douglas Turnbull, Luke Barrington, David Torres, and Gert Lanckriet, \u201cTowards musical query-by-semanticdescription using the cal500 data set,\u201d in Proc. of Annual International Acm Sigir Conference on Research Development in Information Retrieval, pp. 439\u2013446, 2007.\n[16] Shuoyang Wang, Juchiang Wang, Yihsuan Yang, and Hsinmin Wang, \u201cTowards time-varying music autotagging based on cal500 expansion,\u201d IEEE International Coference on Multimedia and Expo(ICME), pp. 1\u20136, 2014.\n[17] Christian Alexander Mikutta, Andreas Altorfer, Werner Strik, and Thomas Koenig, \u201cEmotions, arousal, and frontal alpha rhythm asymmetry during beethoven\u2019s 5th symphony,\u201d Brain Topography, vol. 25, no. 4, pp. 423\u2013 430, 2012.\n[18] Yandre M G Costa, Luiz S Oliveira, Alessandro L Koericb, and Fabien Gouyon, \u201cMusic genre recognition using spectrograms,\u201d IEEE International Conference on Systems, Signals and Image Processing(IWSSIP), pp. 1\u2013 4, 2011.\n[19] Mark French and Rod Handy, \u201cSpectrograms: turning signals into pictures,\u201d Journal of Engineering Technology, vol. 24, pp. 32\u201335, 2007.\n[20] Akshay Krishnegowda Hebbal, \u201cReal time implementation of audio spectrogram on field programmable gate array(fpga),\u201d Dissertations Theses Gradworks, 2014.\n[21] Yoshua Bengio, Yann Lecun, and Yann Lecun, \u201cConvolutional networks for images, speech, and time-series,\u201d 1995.\n[22] George E Dahl, Tara N Sainath, and Geoffrey E Hinton, \u201cImproving deep neural networks for lvcsr using rectified linear units and dropout,\u201d IEEE International Conference on Acoustics, Speech adn Signal Processing, pp. 8609\u20138613, 2013.\n[23] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron J Weiss, Vincent Dubourg, et al., \u201cScikit-learn: Machine learning in python,\u201d Journal of Machine Learning Research, vol. 12, no. 10, pp. 2825\u20132830, 2011."}], "references": [{"title": "What strikes the strings of your heart?\u2014feature mining for music emotion analysis", "author": ["Yang Liu", "Yan Liu", "Yu Zhao", "Kien A Hua"], "venue": "IEEE Transactions on Affective Computing, vol. 6, no. 3, pp. 247\u2013260, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Musical brain revealed by high-field (3 tesla) functional mri", "author": ["Teruhiro Nakada", "Yukihiko Fujii", "Kiyotaka Suzuki", "Ingrid L Kwee"], "venue": "Neuro report, vol. 9, no. 17, pp. 3853\u20133856, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Psychological perspectives on music and emotion", "author": ["John A Sloboda", "Patrik N Juslin"], "venue": "2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening", "author": ["Patrik N Juslin", "Petri Laukka"], "venue": "Journal of New Music Research, vol. 33, no. 3, pp. 216\u2013237, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Music emotion recognition: From content- to contextbased models", "author": ["Mathieu Barthet", "Gyorgy Fazekas", "Mark Sandler"], "venue": "Computer music modeling and retrieval, pp. 228\u2013252, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label classification of emotions in music", "author": ["Alicja A Wieczorkowska", "Piotr Synak", "Zbigniew W Ra\u015b"], "venue": "Proc. of Intelligent Information Processing and Web Mining, vol. 35, pp. 307\u2013315, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "An empirical study of multi-label classifiers for music tag annotation", "author": ["Chris Sanden", "John Z Zhang"], "venue": "Proc. of the 12th International Society for Music Information Retrieval (ISMIR) Conference, pp. 717\u2013722, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Emotion in the singing voice\u2014a deeperlook at acoustic features in the light of automatic classification", "author": ["Florian Eyben", "Glaucia L Salomao", "Johan Sundberg", "Klaus R Scherer", "Bjorn Schuller"], "venue": "Eurasip Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, pp. 1\u20139, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Music emotion classification (mec): Exploiting vocal and instrumental sound features", "author": ["Mudiana Mokhsin Misron", "Nurlaila Binti Rosli", "Norehan Abdul Manaf", "Hamizan Abdul Halim"], "venue": "Recent Advances on Soft Computing and Data Mining, pp. 539\u2013549, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["Beth Logan"], "venue": "Proc. of the 1st International Symposium on Music Information Retrieval (ISMIR 2000)Conference, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Feature selection for content-based, time-varying musical emotion regression", "author": ["Erik M Schmidt", "Douglas Turnbull", "Youngmoo E Kim"], "venue": "Proc. of the 11th ACM SIGMM International Conference on Multimedia Information Retrieval (MIR), pp. 267\u2013274, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Multilabel classification via calibrated label ranking", "author": ["Johannes Furnkranz", "Eyke Hullermeier", "Eneldo Loza Mencia", "Klaus Brinker"], "venue": "Machine Learning, vol. 73, no. 2, pp. 133\u2013153, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Random k-labelsets for multilabel classification", "author": ["Grigorios Tsoumakas", "Ioannis Katakis", "Ioannis Vlahavas"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 23, no. 7, pp. 1079\u20131089, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Ml-knn: A lazy learning approach to multi-label learning", "author": ["Minling Zhang", "Zhihua Zhou"], "venue": "Pattern Recognition, vol. 40, no. 7, pp. 2038\u20132048, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards musical query-by-semanticdescription using the cal500 data set", "author": ["Douglas Turnbull", "Luke Barrington", "David Torres", "Gert Lanckriet"], "venue": "Proc. of Annual International Acm Sigir Conference on Research Development in Information Retrieval, pp. 439\u2013446, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards time-varying music autotagging based on cal500 expansion", "author": ["Shuoyang Wang", "Juchiang Wang", "Yihsuan Yang", "Hsinmin Wang"], "venue": "IEEE International Coference on Multimedia and Expo(ICME), pp. 1\u20136, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Emotions, arousal, and frontal alpha rhythm asymmetry during beethoven\u2019s 5th symphony", "author": ["Christian Alexander Mikutta", "Andreas Altorfer", "Werner Strik", "Thomas Koenig"], "venue": "Brain Topography, vol. 25, no. 4, pp. 423\u2013 430, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Music genre recognition using spectrograms", "author": ["Yandre M G Costa", "Luiz S Oliveira", "Alessandro L Koericb", "Fabien Gouyon"], "venue": "IEEE International Conference on Systems, Signals and Image Processing(IWSSIP), pp. 1\u2013 4, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectrograms: turning signals into pictures", "author": ["Mark French", "Rod Handy"], "venue": "Journal of Engineering Technology, vol. 24, pp. 32\u201335, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Real time implementation of audio spectrogram on field programmable gate array(fpga)", "author": ["Akshay Krishnegowda Hebbal"], "venue": "Dissertations Theses Gradworks, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional networks for images, speech, and time-series", "author": ["Yoshua Bengio", "Yann Lecun", "Yann Lecun"], "venue": "1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["George E Dahl", "Tara N Sainath", "Geoffrey E Hinton"], "venue": "IEEE International Conference on Acoustics, Speech adn Signal Processing, pp. 8609\u20138613, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Gael Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron J Weiss", "Vincent Dubourg"], "venue": "Journal of Machine Learning Research, vol. 12, no. 10, pp. 2825\u20132830, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Researchers have shown that, music, explained as organized sound, can resonate with our nerve tissue [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "It is more likely a biological instinct, just the interaction between sound rhythm or melody and their brain [2, 3].", "startOffset": 109, "endOffset": 115}, {"referenceID": 2, "context": "It is more likely a biological instinct, just the interaction between sound rhythm or melody and their brain [2, 3].", "startOffset": 109, "endOffset": 115}, {"referenceID": 3, "context": "In fact, the delicate relationship between music and emotion has already been explored by numerous researchers for a long period [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "[5] gave a detail introduction about music emotion recognition (MER) task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] firstly formulated MER as a multi-label classification problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Though there is still no standard guidance for selecting features that contribute most to the representation of music [7], acoustic features are still most prevalent in the feature selecting procedure [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "Though there is still no standard guidance for selecting features that contribute most to the representation of music [7], acoustic features are still most prevalent in the feature selecting procedure [8].", "startOffset": 201, "endOffset": 204}, {"referenceID": 8, "context": "Acoustic features mainly consist of rhythmic features, timbre features and spectral features [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "Rhythmic features are derived by extracting periodic changes from a beat histogram [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "Timbre features consist of a series of Zero Crossing Rate, MFCC [10] and Chroma [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Timbre features consist of a series of Zero Crossing Rate, MFCC [10] and Chroma [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "For example, Calibrated Label Ranking classifier using a Support Vector Machine (CLRSVM) [12], Random k-Labelsets (RAkEL) [13], Back-propagation for Multi-Label Learning (BPMLL), Multi-Label K-Nearest Neighbor (MLkNN) [14] and Binary Relevance kNN (BRkNN) etc.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "For example, Calibrated Label Ranking classifier using a Support Vector Machine (CLRSVM) [12], Random k-Labelsets (RAkEL) [13], Back-propagation for Multi-Label Learning (BPMLL), Multi-Label K-Nearest Neighbor (MLkNN) [14] and Binary Relevance kNN (BRkNN) etc.", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "For example, Calibrated Label Ranking classifier using a Support Vector Machine (CLRSVM) [12], Random k-Labelsets (RAkEL) [13], Back-propagation for Multi-Label Learning (BPMLL), Multi-Label K-Nearest Neighbor (MLkNN) [14] and Binary Relevance kNN (BRkNN) etc.", "startOffset": 218, "endOffset": 222}, {"referenceID": 4, "context": "Among these classifiers, in most cases, CLRSVM outperforms the rest [5].", "startOffset": 68, "endOffset": 71}, {"referenceID": 14, "context": "In Section 3 we discuss the experiments on the dataset CAL500 [15] and CAL500exp [16] and compare the results with state-of-the-arts algorithms.", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "In Section 3 we discuss the experiments on the dataset CAL500 [15] and CAL500exp [16] and compare the results with state-of-the-arts algorithms.", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "As well known that emotions inspired by a segment of music are close relevant to both the rhythm and melody of music [17], which are mainly determined by the distribution and variance of signal energy on time and frequency domains.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "Spectrogram, as a type of representation for variances of frequency spectrum with time, not only presents a visualization tool, but also an important type of rich-information feature for audio signal analysis [18, 19].", "startOffset": 209, "endOffset": 217}, {"referenceID": 18, "context": "Spectrogram, as a type of representation for variances of frequency spectrum with time, not only presents a visualization tool, but also an important type of rich-information feature for audio signal analysis [18, 19].", "startOffset": 209, "endOffset": 217}, {"referenceID": 19, "context": "The spectrogram is computed via the Short-Time-Fourier-Transformation of audio signal along the time axis as below formula [20]:", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "nals into their spectrogram, the power variance information along both frequency and time axis are well presented [19].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "The convolution calculation [21] is", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": ", Sigmoid or Relu [22]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "We also train our neural network in some valid techniques, such as dropout [22], activation function and so on.", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "[16] published the dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "While in CAL500, each whole song is regarded as one train or test data and each song is scored on 18 emotions form 1 to 5 by different listeners, then we confirm the emotion labels of this song by the means that at least 80 percent of all listeners agree the score [15].", "startOffset": 265, "endOffset": 269}, {"referenceID": 22, "context": "The computational method can be found from python-sklearn which is a python module for machine learning and data mining at [23].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "The first line of numerical parts shows the result of [16].", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "[16] Macro average 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] report the results when they published the CAL500exp dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "And on CAL500, the results are shown in Table 4, compared with [16] and [1].", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "And on CAL500, the results are shown in Table 4, compared with [16] and [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 15, "context": "The first line of numerical parts shows the result of [16].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "The second and third line of numerical parts shows the result of [1].", "startOffset": 65, "endOffset": 68}, {"referenceID": 15, "context": "[16] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "417 Macro average [1] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "444 Micro average [1] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "423 CAL500 [1] 0.", "startOffset": 11, "endOffset": 14}], "year": 2017, "abstractText": "Music emotion recognition (MER) is usually regarded as a multi-label tagging task, and each segment of music can inspire specific emotion tags. Most researchers extract acoustic features from music and explore the relations between these features and their corresponding emotion tags. Considering the inconsistency of emotions inspired by the same music segment for human beings, seeking for the key acoustic features that really affect on emotions is really a challenging task. In this paper, we propose a novel MER method by using deep convolutional neural network (CNN) on the music spectrograms that contains both the original time and frequency domain information. By the proposed method, no additional effort on extracting specific features required, which is left to the training procedure of the CNN model. Experiments are conducted on the standard CAL500 and CAL500exp dataset. Results show that, for both datasets, the proposed method outperforms state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}