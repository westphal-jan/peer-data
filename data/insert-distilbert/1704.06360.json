{"id": "1704.06360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "SwellShark: A Generative Model for Biomedical Named Entity Recognition without Labeled Data", "abstract": "we present swellshark, a framework for building biomedical named entity recognition ( now ner ) systems quickly and without hand - labeled data. our approach views biomedical resources like lexicons as function primitives for autogenerating weak supervision. we propose then use a generative model to unify and denoise this supervision and construct large - scale, probabilistically labeled datasets for training the high - accuracy ner query taggers. in three biomedical ner tasks, swellshark achieves competitive scores culminating with state - of - the - art supervised benchmarks using no hand - tape labeled training data. in a drug name extraction task performing using patient medical records, one domain expert using swellshark achieved within diameter 5. 1 % of a crowdsourced annotation approach - - which originally utilized 20 teams managed over the course of several weeks - - starting in 24 hours.", "histories": [["v1", "Thu, 20 Apr 2017 23:02:14 GMT  (1047kb,D)", "http://arxiv.org/abs/1704.06360v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jason fries", "sen wu", "alex ratner", "christopher r\\'e"], "accepted": false, "id": "1704.06360"}, "pdf": {"name": "1704.06360.pdf", "metadata": {"source": "CRF", "title": "SWELLSHARK: A Generative Model for Biomedical Named Entity Recognition without Labeled Data", "authors": ["Jason Fries", "Sen Wu", "Alex Ratner", "Christopher R\u00e9"], "emails": ["jfries@cs.stanford.edu", "senwu@cs.stanford.edu", "ajratner@cs.stanford.edu", "chrismre@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Named-entity recognition (NER) is a foundational NLP task that is traditionally approached as a supervised learning problem. In this setting, state-of-theart NER systems often require considerable manual feature engineering to learn robust models using hand-labeled training data. Recent success in deep learning for NER (Lample et al., 2016) suggests that automatic feature extraction will largely replace this process. However, this shifts the burden to constructing the massive hand-labeled training sets needed for robust deep models.\nHow do we obtain enough training data to fit these complex models? Crowdsourcing offers one way of generating large-scale labeled data, but the process is expensive, especially when annotators require specialized domain knowledge or data has\nprivacy concerns preventing distribution (Sabou et al., 2012; Gokhale et al., 2014). Furthermore, even expert inter-annotator agreement rates can be low for certain tasks.\nIn NLP, another common approach is distant supervision (Mintz et al., 2009) where structured resources like ontologies and knowledge bases are used to heuristically label training data. While noisy, this technique has shown empirical success. Distant supervision is commonly used with a few, canonical structured resources like Freebase (Bollacker et al., 2008), weighting each resource equally when labeling data. However, in biomedicine we are faced with a wide space of curated resources; NCBO Bioportal (Whetzel et al., 2011) currently houses 541 distinct biomedical ontologies. These resources contain different hierarchical structures, concept granularities, and otherwise overlap or conflict in their definitions of 8 million entities. Any single ontology may have widely varying accuracy depending on the target task, making them difficult to combine using simple methods like majority vote.\nWe present SWELLSHARK, a framework for quickly building biomedical NER taggers using lexicons, heuristics, and other forms of weak supervision instead of hand-labeled data. Our approach effectively subsumes both crowdsourcing and distant supervision, automatically modeling all such inputs as a generative labeling process. This functional view allows us to take advantage of recent advances in denoising weak supervision, more effectively unifying large-scale biomedical resources.\nOur approach greatly simplifies supervision in an NER system. Traditional distant supervision pipelines consist of three basic components: (1) candidate generation, i.e., identifying potential entities for labeling and classification; (2) labeling heuristics for generating noisy labels; and (3) features describing candidates for classification. Pre-\nar X\niv :1\n70 4.\n06 36\n0v 1\n[ cs\n.C L\n] 2\n0 A\npr 2\n01 7\nviously, each of these has required human engineering or supervision. In SWELLSHARK we show that in the presence of structured resources like lexicons and ontologies, components (1) and (2) can be largely automated. When coupled with automatic feature extraction models like LSTMs, we create a powerful end-to-end pipeline that requires dramatically less human input and can train high-accuracy taggers using unlabeled data.\nThe central argument of this work is that modeling noise in supervision resources is such a powerful strategy that it enables tremendous performance gains from even simple NER techniques. This allows us to focus exclusively on the resources used to supervise a model instead of diffusing human effort across an entire extraction pipeline.\nOur three core contributions are summarized as: \u2022 Automatic Candidate Generation: SWELL-\nSHARK automatically generates potential or candidate entity mentions in documents, a heuristic process that traditionally required non-trivial engineering. Since candidates define the space over which we both provide supervision and learn, selecting the right approach is critical to overall performance. \u2022 Autogenerated Supervision: SWELL-\nSHARK only requires a set of positive and negative lexicons as baseline input. Several classes of automatic supervision generators apply transformations to these lexicons and efficiently generate a large space of noisy supervision with minimal human input. \u2022 Weakly-supervising Sequences: SWELL-\nSHARK allows us to \u201ccompile\u201d supervision inputs, like lexicons and other heuristic rules, directly into a sequence prediction model. We propose a multinomial generative model to explicitly learn entity boundaries, a key part of NER, and model the accuracies of our underlying supervision sources.\nModeling noise while generating data is critical for scaling, where we improve tagger accuracy by using more unlabeled data to train our models. With SWELLSHARK, we construct weakly-labeled training sets of up to 100K documents, providing boosts of up to 6.7% (4.9 F1 points) over the same models trained on small ( \u2264 1K) document collections. With scaling, we can achieve competitive results to state-of-the-art supervised models.\nFinally, as an applied validation challenge, we used SWELLSHARK to build an NER system in\nanother biomedical domain: tagging drug names in clinical discharge summaries. We report the performance achieved by a single domain expert given 24 hours for development and model training. SWELLSHARK scored within 5.1% of a crowdsourced annotation approach, which originally utilized 20 teams over the course of several weeks."}, {"heading": "2 Related Work", "text": "Domain-specific NER tasks are a well-studied NLP problem. The best performing systems use supervised or semi-supervised learning and require handannotated training data. In biomedical NER, supervised methods using CRFs are the standard (Settles, 2004; Leaman et al., 2008) though RNNs/LSTMs are increasingly common (Sahu and Anand, 2016; Dernoncourt et al., 2016). Semi-supervised methods that augment labeled datasets with word embeddings (Tang et al., 2014; Kuksa and Qi, 2010) or bootstrapping techniques (Vlachos and Gasperin, 2006) have been shown to outperform supervised baselines in tasks like gene name recognition. Unlike these existing approaches, SWELLSHARK does not require hand-labeled training data and is agnostic to the choice of discriminative model.\nLeveraging existing resources to heuristically label data has received considerable research interest. Distant supervision (Craven et al., 1999; Mintz et al., 2009) uses knowledge bases to supervise relation extraction tasks. Recent methods incorporate more generalized knowledge into extraction systems. Natarajan et al.(2016) used Markov Logic Networks to encode commonsense domain knowledge like \u201chome teams are more likely to win a game\u201d and generate weak training examples. SWELLSHARK is informed by these methods, but uses a generative model to unify and model noise across different supervision sources."}, {"heading": "3 Background", "text": "Biomedical NER Identifying named entities is a core component of applied biomedical information extraction systems and a critical subtask in normalization, where entities are mapped to canonical identifiers, and relation extraction, where we identify n-arity semantic connections between entities. Ontologies are key artifacts in formalizing biological concepts for normalization and use in computational systems. Biomedical NER focuses on identifying these concepts. For example we would label a sentence as: \u201cPrimary pulmonary\nhypertension is a rare, progressive and incurable disease.\u201d to identify a disease name.\nFor simplicity, in this work we assume each entity is a binary classification task, i.e., each tagger predicts one entity type, although our method generalizes to multi-class settings without extensive changes. We focus on the recognition part of named entity extraction and do not address normalization.\nData programming: Ratner et al. (2016) proposed data programming as a method for programmatic training set creation. In data programming, a collection of user-provided rules called labeling functions (LFs) are modeled as a generative process of training set labeling. Labeling functions may overlap and conflict in their labels, as long as the majority have accuracies greater than 50%. By default, it\u2019s assumed that labeling functions are conditionally independent. Fitting a generative model allows us to automatically estimate these accuracies without ground truth data. The resulting model is then used to construct large-scale training sets with probabilistic labels.\nFormally, labeling functions are black box functions which label some subset of data. In our setting, given a set of candidates for a single entity class (e.g., disease names) and corresponding binary labels, (x, y) \u2208 X \u00d7{\u22121, 1}, where the y are unseen, a labeling function \u03bbi maps:\n\u03bbi : X 7\u2192 {\u22121, 0, 1}\nwhere 0 means a labeling function abstains from providing a label. The output of a set of M labeling functions applied to N candidates is a matrix \u039b \u2208 {\u22121, 0, 1}N\u00d7M . Each labeling function is represented by a single accuracy parameter, learned based on observing the agreements and disagreements between overlapping labeling functions.\nInstead of binary ground-truth training labels (x, y) with y \u2208 {\u22121, 1}, we now utilize the marginal probabilities of our generative model, P\u00b5(Y | \u039b) \u2208 [0, 1], as training labels for a discriminative model, such as logistic regression or an LSTM. This requires using a noise-aware version of our loss function. This can be implemented analytically in the discriminative model or simulated by creating a sampled dataset based on our marginal probabilities."}, {"heading": "4 Methods", "text": "The SWELLSHARK pipeline is outlined in Figure 1 and consists of the following stages: 1) providing unlabeled documents and defining weak supervision input; 2) using generators to transform documents into a set of candidates for classification; 3) autogenerating labeling functions using structured resources or user heuristics; 4) fitting a multinomial generative model using the output of all labeling functions as applied to candidates; and 5) generating probabilistically labeled data, which can then be used with any off-the-shelf classification model. Details for each stage are described below."}, {"heading": "4.1 SwellShark Input", "text": "SWELLSHARK requires as input a collection of unlabeled documents and some form of weak supervision. This is typically a collection of lexicons, ontologies, and optional heuristic rules. Supervision largely consists of specifying positive and negative lexicons. As a toy example, a minimal drug tagger specification could be (1: antibotic, -1: amino acid, peptide, or protein, gene or genome), with each semantic category mapping to source lexicons in the Unified Medical Language System (UMLS) (Bodenreider, 2004) or other external dictionaries."}, {"heading": "4.2 Candidate Generators", "text": "Our approach requires first identifying a set of potential or candidate mentions in documents. We define a candidate generator, \u0393\u03c6, as a function that transforms a document collection D into a candidate set: \u0393\u03c6 : D 7\u2192 {x1, ..., xN}. Each candidate x is defined as a character-level span within a document sentence. Candidate generators are heuristics that can be restrictive e.g., the set of all dictionary matches, or permissive, such as all overlapping kgram spans. The choice of heuristic impacts overall\nperformance, since candidates define the space over which we both provide supervision and learn. We explore the following simple automated generators: \u2022 Noun Phrases All noun phrases as matched\nusing regular expressions over POS tags. \u2022 Dictionary Domain dictionaries with no\ndomain-specific stopword lists or other lexical curation.\nEach heuristic emits k-grams candidates; in our experiments k = [1-10]. Choosing a heuristic involves some trade-off between development time and performance. Hand-tuned matchers require more engineering, but generate more accurate candidate sets. This requires less weak supervision to train the discriminative model, since the candidate generation step acts as an implicit hard filter. In contrast, dictionary and noun phrase candidates are generated automatically, but create additional challenges during learning. Dictionary matches limit recall, impacting generalizability; noun phrase candidates generate larger sets, introducing more noise during labeling function application."}, {"heading": "4.3 Labeling Function Generators", "text": "Labeling functions are a generalization of strategies used in distant supervision. For example, in disease name tagging we can define a labeling function that outputs 1 if a candidate occurs in a disease or syndrome lexicon and another function which outputs -1 if it\u2019s found in a gene or genome lexicon. Several examples are shown in Figure 2.\ndef LF_in_lexicon(c): t = c.text() return 1 if t in umls_disease else 0\ndef LF_idf_filter(c): return -1 if idf(c) <= 4.0 else 0\ndef LF_temporal_modifiers(c): head = c.tokens(\"words\")[0] return -1 if head in temp_mod else 0\nFigure 2: Example labeling functions : (top) tests for membership in a lexicon; (middle) filters by candidate inverse document frequency; and (bottom) defines a compositional grammar rule for rejecting candidates beginning with a temporal modifier, e.g., *recurrent* carcinoma, *childhood* cancer.\nLabeling functions using structured resources assume predictable froms, meaning most lexicalbased supervision can be autogenerated. We define a labeling function generator (LFG), \u0393\u03bb, as a func-\ntion which accepts a weak supervision resource, R, e.g., an ontology or empirical attributes like term frequencies, and automatically generates one or more labeling functions, \u0393\u03bb : R 7\u2192 {\u03bb1, ..., \u03bbN}. LFGs automate a large space of supervision from minimal manual input, such as a collection of positive and negative lexicons. We implemented LFGs as described in Table 1.\nA single generator can instantiate multiple positive and negative labeling functions. This occurs in the case of overlapping candidate spans, where we allow supervision to cascade to nested child entities. In Figure 3 some children of \u201cprimary pulmonary hypertension\u201d are dictionary members and are assigned positive labels. However, if we wish to enforce longest match, we can define a dictionary generator that votes positive on the longest match and negative on all child nodes. Other complex primitives are possible using compositions, such as synonym or negation detection. Our goal is to make it easy and fast to use combinations of LFGs and LFs to provide supervision for a new tagger."}, {"heading": "4.4 Multinomial Generative Model", "text": "A key challenge of our simple, unified framework for accepting weak supervision is that it involves overlapping candidates. The generative model originally proposed by data programming assumes candidates are independent, and does not account for dependencies induced by overlapping spans. Figure 3 shows how this can lead to incorrect marginal probability estimates. In this example, all variants of \u201chypertension\u201d are found in a single lexi-\ncon, overestimating positive label probabilities for nested candidates. To address this bias, we extend the generative model to learn mutual exclusion constraints. For NER, this change is vital because it allows us to learn entity boundaries while maintaining simple labeling function semantics, i.e., voting on discrete candidates. In our experiments, modeling these dependencies improves overall F1 score by up-to 4.7% (3.4 points).\nWe define a spanset, s, as a collection of overlapping candidates within a sentence as partitioned by the labeling matrix \u039b, where each candidate\u2019s row contains the labels generated by all labeling function. Candidates with \u2265 1 positive, overlapping labels form the basis of each spanset. Candidates with \u2265 1 negative labels and 0 positives are added if such addition does not join two disconnected spansets. Unlabeled candidates are removed.\nEach spanset is represented as a matrix X\u0302 \u2208 RM\u00d7K where M is the number of labeling functions and K is the number of overlapping candidates per spanset plus the NONE class. In our datasets, these matrices are column sparse with small K. Pathologically long cases are filtered out.\nPositive label semantics remain unchanged in this framing. However negative labels are now underspecified since they don\u2019t provide enough supervision to perfectly map to a sequence prediction task. A candidate may be completely wrong (the NONE class) or a partial match. For example, in Figure 3 a negative vote for \u201cprimary\u201d doesn\u2019t tell us if the whole spanset is wrong or if it\u2019s a subset of a correct entity (as is the case). Our convention is if the ith LF votes negatively on a candidate j, this is expressed as having uniform distribution over all K columns except j in row i of X\u0302 .\nLearning is otherwise unchanged from the binary case, except we now apply the softmax function to each spanset matrix X\u0302 to compute marginals.\nP (Y = j|X\u0302;w) = exp(w T X\u0302j)\u2211K\nk=1 exp(w T X\u0302k)"}, {"heading": "4.5 Sampling for Dataset Construction", "text": "After training, each spanset now defines a multinomial distribution si = {(cij , pij); j = 1...k}, where p is the probability of each candidate within the spanset. We treat all spansets in a sentence, S, as as sampling distribution S \u223c s1 \u00d7 s2 \u00d7 ...sN to generate noisily labeled tag sequences. We assume spansets are independent and generate 10 samples per observed sentence, sampling once per spanset."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Weakly-supervised Taggers", "text": "As our experimental testbed, we built three biomedical taggers: two for disease names (\u201costeoarthritis\u201d) and one for chemical names (\u201cbupropion hydrochloride\u201d), with each tagger trained using a CRF or LSTM. Each model configuration is evaluated using 25K to 100K unlabeled training documents.\nOur primary experimental questions are: 1) what are the performance trade-offs of different candidate generation heuristics; 2) how well does autogenerated supervision perform; and 3) how quickly can we write a tagger for use in other domains?\nComparison Systems: For our baseline comparison system, we use reported benchmarks from TaggerOne (Leaman and Lu, 2016) a state-of-theart general purpose biomedical NER tagger. Their approach uses a CRF with manually engineered features. We compute two simple baselines: a string matching score using all positive lexicons (LEX) and a domain-agnostic stop word list; and the majority vote (MV) across all labeling functions.\n1) Tuning Candidate Generation: We explored the trade-offs of two automated candidate generation methods compared to a manually-engineered\nbaseline. For each tagger, we implemented an optimized, hand-tuned generator using regular expressions, dictionaries, and other fuzzy matching heuristics. These generators are task-specific, carefully maximizing recall while minimizing false positives. We compute precision-recall curves for each heuristic and report F1 scores when those methods are scaled. We empirically evaluated precision/recall tradeoffs of different k-gram token lengths and use k=6 for all candidates. Here we use the CDR disease task as our motivating example.\n2) Autogenerating Supervision: We trained models using only autogenerated, lexicon-based labeling functions and NounPhrase candidate generators. Here supervision consists of specifying positive and negative semantic categories selected from the 133 semantic definitions provided by the UMLS. Lexicons provide a strong baseline supervision framework, but they are usually incapable of modeling the entire dataset generation process. The definition of ground truth often depends on dataset-specific annotation guidelines. For example, in our clinical notes task, mentions of drug names that are negated or describe patient allergies are not considered true mentions; a lexicon alone cannot encode this form of supervision. Choosing what affixes, modifiers, and prepositional phrases constitute an entity can vary greatly across datasets.\nOne advantage of SWELLSHARK is that this type of dataset-specific supervision is easily introduced into our system by adding more labeling functions. For each PubMed tagger, we extended our lexical supervision with labeling functions to capture annotation guidelines and other dataset-specific supervision. This required writing 20-50 additional labeling functions per task.\n3) Building a Tagger in 24-hours: We tested out ability to quickly build a high-performance tagger in another biomedical domain. Given a time budget of 24 hours, we used our tools to build a drug name tagger for clinical discharge summaries. For the autogenerated LF model, our positive supervision consisted of 13 chemical semantic types from the UMLS. This setup required about one hour. We spent the remaining time examining training data to identify syntactical patterns and rules for labeling functions. Due to time constraints and the lack of additional unlabeled data, we did not explore scale-up performance.\nLabeling Function Development: All labeling functions and domain stop word lists were developed iteratively by inspecting unlabeled training documents. Manually written LFs were refined based on empirical accuracy scores from a small set of held-out labeled training instances. No ground truth labels were used to fit our final discriminative models; we only use noise-aware labels produced by the generative model."}, {"heading": "5.2 Materials and Setup", "text": "Datasets: We evaluate performance on three bioinformatics datasets: (1) the NCBI Disease corpus (Dog\u0306an et al., 2014); (2) the BioCreative V Chemical Disease Relation task (CDR) corpus (Wei et al., 2015); and the i2b2-2009 Medication Extraction Challenge dataset (Uzuner et al., 2010a). NCBI Disease contains 792 PubMed abstracts separated into training, development, and test subsets (n=592/100/100); CDR contains 1,500 PubMed abstracts (n=500/500/500); and i2b2-2009 contains 1249 electronic health record (EHR) discharge summaries (n=1000/124/125). PubMed datasets are annotated with mention-level disease and chemical entities and i2b2 data with drug names.\nFor unlabeled PubMed data, we use a 100K document sample chosen uniformly at random from the BioASQ Task 4a challenge (Tsatsaronis et al., 2015) dataset. All LSTM experiments use the 200- dim word2vec embeddings provided as part of this dataset. For clinical text embeddings, we generated embeddings using 2.4M clinical narratives made available as part of the MIMIC-III critical care database (Johnson et al., 2016).\nOntologies used in this work include those provided as part of the 2014AB release of the UMLS, and various other disease/chemical ontologies (Schriml et al., 2012; Davis et al., 2015; Rath et al., 2012; Ko\u0308hler et al., 2013).\nData Preprocessing: All datasets are preprocessed using Stanford CoreNLP1 with default English models for tokenization, sentence boundary detection, POS tagging, and dependency parsing.\nDiscriminative Models: We use two external sequence models: CRFsuite (Okazaki, 2007) and a bidirectional LSTM-CRF hybrid (Lample et al., 2016) which makes use of both word and characterlevel embeddings. Our LSTM-CRF uses automatic feature extraction based on word-level input. CRFs\n1http://stanfordnlp.github.io/CoreNLP/\nuse a generic feature library (see \u00a7Appendix) based on TaggerOne.\nEvaluation Measures: Precision (P), Recall (R), F1-score (F1)."}, {"heading": "6 Results / Discussion", "text": "6.1 Candidate Generation\nFigure 4 shows performance trade-offs at scale of our three heuristics in CDR disease with extended labeling functions. Using a hand-tuned candidate generator allows our system to surpass supervised performance, however, completely automated methods also perform very well. A NounPhrase generator scores within 2% of the handtuned heuristic with CRF/emb and 1.25% using\nLSTM-CRF/emb (see Table 2). The bump in performance seen around 1K documents is an artifact of scale-up using a CRF with embeddings, where there is some early overfitting.\nTables 2 contains scores for PubMed taggers, comparing manual and automated candidate generation heuristics at scale. In both CDR tasks, we can closely match or exceed benchmark scores reported by TaggerOne (from +0.5 to -0.1 F1 points). In chemicals, NounPhrase candidate generation demonstrates better recall and improves on handtuned matchers by 0.7 points. In contrast, the NCBI task performs far below baseline using NounPhrase generators. This is due to that dataset\u2019s more complex definition of mentions, including conjunctions (\u201cbreast and ovarian cancer\u201d) and prepositional phrases (\u201cdeficiency of beta-hexosaminidase A\u201d). These increase partial matches, hurting overall performance. With a hand-tuned candidate generator, we can account for these specific cases and dramatically improve performance and scoring -0.7 F1 points within the benchmark."}, {"heading": "6.2 Autogenerating Supervision", "text": "Table 3 shows performance measures for our models when using only lexical resources for supervision, without any annotation guideline or datasetspecific labeling functions. In all cases, we find the LSTM-CRF/emb models outperform majority vote by 1.7 to 5.4 F1 points. In chemical tagging, we come within 1 F1 point of published TaggerOne\u2019s benchmark score; in NCBI we do much worse due to candidate issues outlined above.\nScale-up & Automatic Feature Extraction: Figure 5 gives a broader picture of the scale-up curve and the convergence differences between the human-generated feature library used for our CRF and LSTM-CRF models. Pretrained word embeddings give the LSTM-CRF an advantage in smaller document settings, converging quickly to the best score after 10K additional unlabeled documents. In contrast, without embeddings, the LSTM-CRF is always dominated by CRF models, requiring over 3x more unlabeled data to learn features that approach the same score. Models trained on 100k documents are similar in performance, although the LSTM-CRF is the best overall performer."}, {"heading": "6.3 A Tagger in 24 Hours", "text": "Autogenerated labeling functions provide a strong baseline system in this task, scoring 6.5 F1 points over majority vote and boosting recall by 12.2 points. We extending this core system with 21 customized regular expression rules and other guideline specific labeling functions for an overall score within 7% (6 F1 points) of the same model trained on hand-labeled data. Our approach quickly\nachieved good performance and, most likely, would improve with more unlabeled training documents which unfortunately are not available for this task.\nComparing our performance to the same task as done with crowdsourcing (Uzuner et al., 2010b), we are within 5.1% (4.4 F1 points) of the crowd macro average achieved by 79 annotators. This required 2 phases of labeling and adjudication over several weeks, although an exact time estimate for drug names alone is difficult as it was one of 1 of 7 annotated subtasks."}, {"heading": "7 Conclusion", "text": "In this work, we\u2019ve demonstrated that programmatic supervision, provided by biomedical lexicons and other heuristics, can achieve competitive performance to state-of-the-art systems trained on hand-labeled data. SWELLSHARK accepts much weaker forms of supervision, allowing NER taggers to be built in less time and in a more intuitive fashion for domain experts. Our approach intrinsically scales to automatically construct large training sets, allowing SWELLSHARK to train high performance taggers using state of recent deep learning models."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Mobilize Center, a National Institutes of Health Big Data to Knowledge (BD2K) Center of Excellence supported through Grant U54EB020405."}, {"heading": "8 Appendix", "text": ""}, {"heading": "8.1 Materials", "text": "Feature Library: Our CRF models use feature templates defined over the mention and it\u2019s parent dependency parse tree. Features includes context window features, part of speech tags, word shape, word embeddings, character n-grams, morphology, domain dictionary membership, and the lemmatized form of a mention\u2019s dependency tree parent word. We expand in-document abbreviations, sharing features across any identical mention linked within a document by a parenthetical mention."}, {"heading": "8.2 Results", "text": "Candidate Generation Precision/Recall Curves: Figure 6 shows the precision-recall curves of candidate generation methods in detail. Note how domain-engineered matchers suffer recall problems, only generalizing a small amount beyond dictionaries and missing 20% of all mentions.\nMultinomial Generative Model: Figure 7 shows the performance difference between the multinomial and binary generative models when sampled data is used to train noise-aware implementations of a CRF and logistic regression. We see the multinomial CRF model performs best overall for all choices of k-gram. For sequence models, k=6 scored best.\nScaling: Tables 5 and 6 show some of the benefits of scale, where we see gains of up to 3.1 F1 points, or 7.6 point improvement over majority vote. Scale-up improvements were smaller in our other tasks, but still beating majority vote in all systems using automatic feature extraction."}], "references": [{"title": "The unified medical language system (UMLS): integrating biomedical terminology", "author": ["Olivier Bodenreider."], "venue": "Nucleic acids research 32(suppl 1):D267\u2013D270.", "citeRegEx": "Bodenreider.,? 2004", "shortCiteRegEx": "Bodenreider.", "year": 2004}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Mark Craven", "Johan Kumlien"], "venue": "In ISMB. volume", "citeRegEx": "Craven and Kumlien,? \\Q1999\\E", "shortCiteRegEx": "Craven and Kumlien", "year": 1999}, {"title": "The Comparative Toxicogenomics Database\u2019s 10th year anniversary: update", "author": ["Allan Peter Davis", "Cynthia J Grondin", "Kelley LennonHopkins", "Cynthia Saraceni-Richards", "Daniela Sciaky", "Benjamin L King", "Thomas C Wiegers", "Carolyn J Mattingly"], "venue": null, "citeRegEx": "Davis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2015}, {"title": "De-identification of patient notes with recurrent neural networks", "author": ["Franck Dernoncourt", "Ji Young Lee", "Ozlem Uzuner", "Peter Szolovits."], "venue": "Journal of the American Medical Informatics Association page ocw156.", "citeRegEx": "Dernoncourt et al\\.,? 2016", "shortCiteRegEx": "Dernoncourt et al\\.", "year": 2016}, {"title": "Ncbi disease corpus: a resource for disease name recognition and concept normalization", "author": ["Rezarta Islamaj Do\u011fan", "Robert Leaman", "Zhiyong Lu."], "venue": "Journal of biomedical informatics 47:1\u201310.", "citeRegEx": "Do\u011fan et al\\.,? 2014", "shortCiteRegEx": "Do\u011fan et al\\.", "year": 2014}, {"title": "Corleone: Hands-off crowdsourcing for entity matching", "author": ["Chaitanya Gokhale", "Sanjib Das", "AnHai Doan", "Jeffrey F Naughton", "Narasimhan Rampalli", "Jude Shavlik", "Xiaojin Zhu."], "venue": "Proceedings of the 2014 ACM SIGMOD international conference", "citeRegEx": "Gokhale et al\\.,? 2014", "shortCiteRegEx": "Gokhale et al\\.", "year": 2014}, {"title": "Mimic-iii, a freely accessible critical care database", "author": ["Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "Liwei H Lehman", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark."], "venue": "Scientific", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Semi-supervised bio-named entity recognition with word-codebook learning", "author": ["Pavel P Kuksa", "Yanjun Qi."], "venue": "Proceedings of the 2010 SIAM International Conference on Data Mining. SIAM, pages 25\u201336.", "citeRegEx": "Kuksa and Qi.,? 2010", "shortCiteRegEx": "Kuksa and Qi.", "year": 2010}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "arXiv preprint arXiv:1603.01360 .", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Banner: an executable survey of advances in biomedical named entity recognition", "author": ["Robert Leaman", "Graciela Gonzalez"], "venue": "In Pacific symposium on biocomputing", "citeRegEx": "Leaman and Gonzalez,? \\Q2008\\E", "shortCiteRegEx": "Leaman and Gonzalez", "year": 2008}, {"title": "Taggerone: Joint named entity recognition and normalization with semi-markov models", "author": ["Robert Leaman", "Zhiyong Lu."], "venue": "Bioinformatics page btw343.", "citeRegEx": "Leaman and Lu.,? 2016", "shortCiteRegEx": "Leaman and Lu.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Deep distant supervision: Learning statistical relational models for weak supervision in natural language extraction", "author": ["Sriraam Natarajan", "Ameet Soni", "Anurag Wazalwar", "Dileep Viswanathan", "Kristian Kersting."], "venue": "Solving Large Scale Learning", "citeRegEx": "Natarajan et al\\.,? 2016", "shortCiteRegEx": "Natarajan et al\\.", "year": 2016}, {"title": "Crfsuite: a fast implementation of conditional random fields (crfs)", "author": ["Naoaki Okazaki."], "venue": "http://www.chokkan.org/software/crfsuite/.", "citeRegEx": "Okazaki.,? 2007", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "Representation of rare diseases in health information systems: the orphanet approach to serve a wide range of end users", "author": ["Ana Rath", "Annie Olry", "Ferdinand Dhombres", "Maja Mili\u010di\u0107 Brandt", "Bruno Urbero", "Segolene Ayme."], "venue": "Human mutation", "citeRegEx": "Rath et al\\.,? 2012", "shortCiteRegEx": "Rath et al\\.", "year": 2012}, {"title": "Data programming: Creating large training sets, quickly", "author": ["Alexander Ratner", "Christopher De Sa", "Sen Wu", "Daniel Selsam", "Christopher R\u00e9."], "venue": "arXiv preprint arXiv:1605.07723 .", "citeRegEx": "Ratner et al\\.,? 2016", "shortCiteRegEx": "Ratner et al\\.", "year": 2016}, {"title": "Crowdsourcing research opportunities: lessons from natural language processing", "author": ["Marta Sabou", "Kalina Bontcheva", "Arno Scharl."], "venue": "Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technolo-", "citeRegEx": "Sabou et al\\.,? 2012", "shortCiteRegEx": "Sabou et al\\.", "year": 2012}, {"title": "Recurrent neural network models for disease name recognition using domain invariant features", "author": ["Sunil Kumar Sahu", "Ashish Anand."], "venue": "arXiv preprint arXiv:1606.09371 .", "citeRegEx": "Sahu and Anand.,? 2016", "shortCiteRegEx": "Sahu and Anand.", "year": 2016}, {"title": "Disease ontology: a backbone for disease semantic integration", "author": ["Lynn Marie Schriml", "Cesar Arze", "Suvarna Nadendla", "Yu-Wei Wayne Chang", "Mark Mazaitis", "Victor Felix", "Gang Feng", "Warren Alden Kibbe."], "venue": "Nucleic acids research 40(D1):D940\u2013D946.", "citeRegEx": "Schriml et al\\.,? 2012", "shortCiteRegEx": "Schriml et al\\.", "year": 2012}, {"title": "Biomedical named entity recognition using conditional random fields and rich feature sets", "author": ["Burr Settles."], "venue": "Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. Association for", "citeRegEx": "Settles.,? 2004", "shortCiteRegEx": "Settles.", "year": 2004}, {"title": "Evaluating word representation features in biomedical named entity recognition tasks", "author": ["Buzhou Tang", "Hongxin Cao", "Xiaolong Wang", "Qingcai Chen", "Hua Xu."], "venue": "BioMed research international 2014.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "An overview of the BIOASQ", "author": ["George Tsatsaronis", "Georgios Balikas", "Prodromos Malakasiotis", "Ioannis Partalas", "Matthias Zschunke", "Michael R Alvers", "Dirk Weissenborn", "Anastasia Krithara", "Sergios Petridis", "Dimitris Polychronopoulos"], "venue": null, "citeRegEx": "Tsatsaronis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsatsaronis et al\\.", "year": 2015}, {"title": "Extracting medication information from clinical text", "author": ["\u00d6zlem Uzuner", "Imre Solti", "Eithon Cadag."], "venue": "Journal of the American Medical Informatics Association 17(5):514\u2013518.", "citeRegEx": "Uzuner et al\\.,? 2010a", "shortCiteRegEx": "Uzuner et al\\.", "year": 2010}, {"title": "Community annotation experiment for ground truth generation for the i2b2 medication challenge", "author": ["\u00d6zlem Uzuner", "Imre Solti", "Fei Xia", "Eithon Cadag."], "venue": "Journal of the American Medical Informatics Association 17(5):519\u2013523.", "citeRegEx": "Uzuner et al\\.,? 2010b", "shortCiteRegEx": "Uzuner et al\\.", "year": 2010}, {"title": "Bootstrapping and evaluating named entity recognition in the biomedical domain", "author": ["Andreas Vlachos", "Caroline Gasperin."], "venue": "Proceedings of the HLTNAACL BioNLP Workshop on Linking Natural Language and Biology. Association for Computational", "citeRegEx": "Vlachos and Gasperin.,? 2006", "shortCiteRegEx": "Vlachos and Gasperin.", "year": 2006}, {"title": "Overview of the biocreative v chemical disease relation (cdr) task", "author": ["Chih-Hsuan Wei", "Yifan Peng", "Robert Leaman", "Allan Peter Davis", "Carolyn J Mattingly", "Jiao Li", "Thomas C Wiegers", "Zhiyong Lu."], "venue": "Proceedings of the fifth BioCre-", "citeRegEx": "Wei et al\\.,? 2015", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Bioportal: enhanced functionality via new web services from the national center for biomedical ontology to access", "author": ["Patricia L Whetzel", "Natalya F Noy", "Nigam H Shah", "Paul R Alexander", "Csongor Nyulas", "Tania Tudorache", "Mark A Musen"], "venue": null, "citeRegEx": "Whetzel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Whetzel et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "Recent success in deep learning for NER (Lample et al., 2016) suggests that automatic feature extraction will largely replace this process.", "startOffset": 40, "endOffset": 61}, {"referenceID": 17, "context": "How do we obtain enough training data to fit these complex models? Crowdsourcing offers one way of generating large-scale labeled data, but the process is expensive, especially when annotators require specialized domain knowledge or data has privacy concerns preventing distribution (Sabou et al., 2012; Gokhale et al., 2014).", "startOffset": 283, "endOffset": 325}, {"referenceID": 6, "context": "How do we obtain enough training data to fit these complex models? Crowdsourcing offers one way of generating large-scale labeled data, but the process is expensive, especially when annotators require specialized domain knowledge or data has privacy concerns preventing distribution (Sabou et al., 2012; Gokhale et al., 2014).", "startOffset": 283, "endOffset": 325}, {"referenceID": 12, "context": "In NLP, another common approach is distant supervision (Mintz et al., 2009) where structured resources like ontologies and knowledge bases are used to heuristically label training data.", "startOffset": 55, "endOffset": 75}, {"referenceID": 1, "context": "Distant supervision is commonly used with a few, canonical structured resources like Freebase (Bollacker et al., 2008), weighting each resource equally when labeling data.", "startOffset": 94, "endOffset": 118}, {"referenceID": 27, "context": "rated resources; NCBO Bioportal (Whetzel et al., 2011) currently houses 541 distinct biomedical ontologies.", "startOffset": 32, "endOffset": 54}, {"referenceID": 20, "context": "vised methods using CRFs are the standard (Settles, 2004; Leaman et al., 2008) though RNNs/LSTMs are increasingly common (Sahu and Anand, 2016; Dernoncourt et al.", "startOffset": 42, "endOffset": 78}, {"referenceID": 18, "context": ", 2008) though RNNs/LSTMs are increasingly common (Sahu and Anand, 2016; Dernoncourt et al., 2016).", "startOffset": 50, "endOffset": 98}, {"referenceID": 4, "context": ", 2008) though RNNs/LSTMs are increasingly common (Sahu and Anand, 2016; Dernoncourt et al., 2016).", "startOffset": 50, "endOffset": 98}, {"referenceID": 21, "context": "Semi-supervised methods that augment labeled datasets with word embeddings (Tang et al., 2014; Kuksa and Qi, 2010) or bootstrapping techniques (Vlachos and Gasperin, 2006) have been shown to outperform supervised baselines in tasks like gene name recognition.", "startOffset": 75, "endOffset": 114}, {"referenceID": 8, "context": "Semi-supervised methods that augment labeled datasets with word embeddings (Tang et al., 2014; Kuksa and Qi, 2010) or bootstrapping techniques (Vlachos and Gasperin, 2006) have been shown to outperform supervised baselines in tasks like gene name recognition.", "startOffset": 75, "endOffset": 114}, {"referenceID": 25, "context": ", 2014; Kuksa and Qi, 2010) or bootstrapping techniques (Vlachos and Gasperin, 2006) have been shown to outperform supervised baselines in tasks like gene name recognition.", "startOffset": 56, "endOffset": 84}, {"referenceID": 12, "context": "Distant supervision (Craven et al., 1999; Mintz et al., 2009) uses knowledge bases to supervise relation extraction tasks.", "startOffset": 20, "endOffset": 61}, {"referenceID": 12, "context": ", 1999; Mintz et al., 2009) uses knowledge bases to supervise relation extraction tasks. Recent methods incorporate more generalized knowledge into extraction systems. Natarajan et al.(2016) used Markov Logic Networks to encode commonsense domain knowledge like \u201chome teams are more likely to win a game\u201d and generate weak training examples.", "startOffset": 8, "endOffset": 191}, {"referenceID": 16, "context": "Data programming: Ratner et al. (2016) proposed data programming as a method for programmatic training set creation.", "startOffset": 18, "endOffset": 39}, {"referenceID": 0, "context": "As a toy example, a minimal drug tagger specification could be (1: antibotic, -1: amino acid, peptide, or protein, gene or genome), with each semantic category mapping to source lexicons in the Unified Medical Language System (UMLS) (Bodenreider, 2004) or other external dictionaries.", "startOffset": 233, "endOffset": 252}, {"referenceID": 11, "context": "Comparison Systems: For our baseline comparison system, we use reported benchmarks from TaggerOne (Leaman and Lu, 2016) a state-of-theart general purpose biomedical NER tagger.", "startOffset": 98, "endOffset": 119}, {"referenceID": 5, "context": "Datasets: We evaluate performance on three bioinformatics datasets: (1) the NCBI Disease corpus (Do\u011fan et al., 2014); (2) the BioCreative V Chemical Disease Relation task (CDR) corpus (Wei et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 26, "context": ", 2014); (2) the BioCreative V Chemical Disease Relation task (CDR) corpus (Wei et al., 2015); and the i2b2-2009 Medication Ex-", "startOffset": 75, "endOffset": 93}, {"referenceID": 23, "context": "traction Challenge dataset (Uzuner et al., 2010a).", "startOffset": 27, "endOffset": 49}, {"referenceID": 22, "context": "For unlabeled PubMed data, we use a 100K document sample chosen uniformly at random from the BioASQ Task 4a challenge (Tsatsaronis et al., 2015) dataset.", "startOffset": 118, "endOffset": 144}, {"referenceID": 7, "context": "4M clinical narratives made available as part of the MIMIC-III critical care database (Johnson et al., 2016).", "startOffset": 86, "endOffset": 108}, {"referenceID": 19, "context": "Ontologies used in this work include those provided as part of the 2014AB release of the UMLS, and various other disease/chemical ontologies (Schriml et al., 2012; Davis et al., 2015; Rath et al., 2012; K\u00f6hler et al., 2013).", "startOffset": 141, "endOffset": 223}, {"referenceID": 3, "context": "Ontologies used in this work include those provided as part of the 2014AB release of the UMLS, and various other disease/chemical ontologies (Schriml et al., 2012; Davis et al., 2015; Rath et al., 2012; K\u00f6hler et al., 2013).", "startOffset": 141, "endOffset": 223}, {"referenceID": 15, "context": "Ontologies used in this work include those provided as part of the 2014AB release of the UMLS, and various other disease/chemical ontologies (Schriml et al., 2012; Davis et al., 2015; Rath et al., 2012; K\u00f6hler et al., 2013).", "startOffset": 141, "endOffset": 223}, {"referenceID": 14, "context": "Discriminative Models: We use two external sequence models: CRFsuite (Okazaki, 2007) and a bidirectional LSTM-CRF hybrid (Lample et al.", "startOffset": 69, "endOffset": 84}, {"referenceID": 9, "context": "Discriminative Models: We use two external sequence models: CRFsuite (Okazaki, 2007) and a bidirectional LSTM-CRF hybrid (Lample et al., 2016) which makes use of both word and characterlevel embeddings.", "startOffset": 121, "endOffset": 142}, {"referenceID": 24, "context": "Comparing our performance to the same task as done with crowdsourcing (Uzuner et al., 2010b), we are within 5.", "startOffset": 70, "endOffset": 92}], "year": 2017, "abstractText": "We present SWELLSHARK, a framework for building biomedical named entity recognition (NER) systems quickly and without hand-labeled data. Our approach views biomedical resources like lexicons as function primitives for autogenerating weak supervision. We then use a generative model to unify and denoise this supervision and construct large-scale, probabilistically labeled datasets for training high-accuracy NER taggers. In three biomedical NER tasks, SWELLSHARK achieves competitive scores with state-of-the-art supervised benchmarks using no hand-labeled training data. In a drug name extraction task using patient medical records, one domain expert using SWELLSHARK achieved within 5.1% of a crowdsourced annotation approach \u2013 which originally utilized 20 teams over the course of several weeks \u2013 in 24 hours.", "creator": "LaTeX with hyperref package"}}}