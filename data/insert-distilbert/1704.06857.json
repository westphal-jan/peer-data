{"id": "1704.06857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "A Review on Deep Learning Techniques Applied to Semantic Segmentation", "abstract": "image semantic segmentation is more and more being of interest for computer & vision managers and machine learning researchers. many applications on the rise need accurate and efficient segmentation mechanisms : autonomous driving, indoor navigation, and even virtual or augmented reality systems ought to name a few. this demand coincides with experiencing the rise of deep learning approaches in almost every computational field or application target related to computer vision, including semantic segmentation functionality or scene understanding. this paper provides a lively review on introducing deep learning methods for semantic segmentation software applied to various traditional application areas. firstly, we describe the philosophical terminology of this field as well as mandatory background concepts. next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. in then, existing dynamic methods are reviewed, individually highlighting their ongoing contributions and their significance in the field. however finally, quantitative results are given for the described methods themselves and the datasets in which reality they were evaluated, following keeping up with a discussion of the results. \u201c at last, here we point out a set of promising future works and successfully draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.", "histories": [["v1", "Sat, 22 Apr 2017 23:37:43 GMT  (8639kb,D)", "http://arxiv.org/abs/1704.06857v1", "Submitted to TPAMI on Apr. 22, 2017"]], "COMMENTS": "Submitted to TPAMI on Apr. 22, 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["alberto garcia-garcia", "sergio orts-escolano", "sergiu oprea", "victor villena-martinez", "jose garcia-rodriguez"], "accepted": false, "id": "1704.06857"}, "pdf": {"name": "1704.06857.pdf", "metadata": {"source": "CRF", "title": "A Review on Deep Learning Techniques Applied to Semantic Segmentation", "authors": ["A. Garcia-Garcia", "S. Orts-Escolano", "S.O. Oprea", "V. Villena-Martinez", "J. Garcia-Rodriguez"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Semantic Segmentation, Deep Learning, Scene Labeling, Object Segmentation\nF"}, {"heading": "1 INTRODUCTION", "text": "NOWADAYS, semantic segmentation \u2013 applied to still2D images, video, and even 3D or volumetric data \u2013 is one of the key problems in the field of computer vision. Looking at the big picture, semantic segmentation is one of the high-level task that paves the way towards complete scene understanding. The importance of scene understanding as a core computer vision problem is highlighted by the fact that an increasing number of applications nourish from inferring knowledge from imagery. Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few. Such problem has been addressed in the past using various traditional computer vision and machine learning techniques. Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency. However, deep learning is far from the maturity achieved by other old-established branches of computer vision and machine learning. Because of that, there is a lack of unifying works and state of the art reviews. The ever-changing state of the field makes initiation difficult and keeping up with its evolution pace is an incredibly time-consuming task due to the sheer amount of new literature being produced. This makes it hard to keep track of the works dealing with se-\n\u2022 A. Garcia-Garcia, S.O. Oprea, V. Villena-Martinez, and J. GarciaRodriguez are with the Department of Computer Technology, University of Alicante, Spain. E-mail: {agarcia, soprea, vvillena, jgarcia}@dtic.ua.es \u2022 S. Orts-Escolano is with the Department of Computer Science and Artificial Intelligence, Universit of Alicante, Spain. E-mail: sorts@ua.es.\nmantic segmentation and properly interpret their proposals, prune subpar approaches, and validate results.\nTo the best of our knowledge, this is the first review to focus explicitly on deep learning for semantic segmentation. Various semantic segmentation surveys already exist such as the works by Zhu et al. [12] and Thoma [13], which do a great work summarizing and classifying existing methods, discussing datasets and metrics, and providing design choices for future research directions. However, they lack some of the most recent datasets, they do not analyze frameworks, and none of them provide details about deep learning techniques. Because of that, we consider our work to be novel and helpful thus making it a significant contribution for the research community.\nar X\niv :1\n70 4.\n06 85\n7v 1\n[ cs\n.C V\n] 2\n2 A\npr 2\n01 7\n2 The key contributions of our work are as follows:\n\u2022 We provide a broad survey of existing datasets that might be useful for segmentation projects with deep learning techniques. \u2022 An in-depth and organized review of the most significant methods that use deep learning for semantic segmentation, their origins, and their contributions. \u2022 A thorough performance evaluation which gathers quantitative metrics such as accuracy, execution time, and memory footprint. \u2022 A discussion about the aforementioned results, as well as a list of possible future works that might set the course of upcoming advances, and a conclusion summarizing the state of the art of the field.\nThe remainder of this paper is organized as follows. Firstly, Section 2 introduces the semantic segmentation problem as well as notation and conventions commonly used in the literature. Other background concepts such as common deep neural networks are also reviewed. Next, Section 3 describes existing datasets, challenges, and benchmarks. Section 4 reviews existing methods following a bottomup complexity order based on their contributions. This section focuses on describing the theory and highlights of those methods rather than performing a quantitative evaluation. Finally, Section 5 presents a brief discussion on the presented methods based on their quantitative results on the aforementioned datasets. In addition, future research directions are also laid out. At last, Section 6 summarizes the paper and draws conclusions about this work and the state of the art of the field."}, {"heading": "2 TERMINOLOGY AND BACKGROUND CONCEPTS", "text": "In order to properly understand how semantic segmentation is tackled by modern deep learning architectures, it is important to know that it is not an isolated field but rather a natural step in the progression from coarse to fine inference. The origin could be located at classification, which consists of making a prediction for a whole input, i.e., predicting which is the object in an image or even providing a ranked list if there are many of them. Localization or detection is the next step towards fine-grained inference, providing not only the classes but also additional information regarding the spatial location of those classes, e.g., centroids or bounding boxes. Providing that, it is obvious that semantic segmentation is the natural step to achieve fine-grained inference, its goal: make dense predictions inferring labels for every pixel; this way, each pixel is labeled with the class of its enclosing object or region. Further improvements can be made, such as instance segmentation (separate labels for different instances of the same class) and even part-based segmentation (low-level decomposition of already segmented classes into their components). Figure 1 shows the aforementioned evolution. In this review, we will mainly focus on generic scene labeling, i.e., per-pixel class segmentation, but we will also review the most important methods on instance and part-based segmentation.\nIn the end, the per-pixel labeling problem can be reduced to the following formulation: find a way to assign a state from the label space L = {l1, l2, ..., lk} to each one of the\nelements of a set of random variables X = {x1, x2, ..., xN}. Each label l represents a different class or object, e.g., aeroplane, car, traffic sign, or background. This label space has k possible states which are usually extended to k + 1 and treating l0 as background or a void class. Usually, X is a 2D image of W \u00d7H = N pixels x. However, that set of random variables can be extended to any dimensionality such as volumetric data or hyperspectral images.\nApart from the problem formulation, it is important to remark some background concepts that might help the reader to understand this review. Firstly, common networks, approaches, and design decisions that are often used as the basis for deep semantic segmentation systems. In addition, common techniques for training such as transfer learning. At last, data pre-processing and augmentation approaches."}, {"heading": "2.1 Common Deep Network Architectures", "text": "As we previously stated, certain deep networks have made such significant contributions to the field that they have become widely known standards. It is the case of AlexNet, VGG-16, GoogLeNet, and ResNet. Such was their importance that they are currently being used as building blocks for many segmentation architectures. For that reason, we will devote this section to review them."}, {"heading": "2.1.1 AlexNet", "text": "AlexNet was the pioneering deep CNN that won the ILSVRC-2012 with a TOP-5 test accuracy of 84.6% while the closest competitor, which made use of traditional techniques instead of deep architectures, achieved a 73.8% accuracy in the same challenge. The architecture presented by Krizhevsky et al. [14] was relatively simple. It consists of five convolutional layers, max-pooling ones, Rectified Linear Units (ReLUs) as non-linearities, three fully-connected layers, and dropout. Figure 2 shows that CNN architecture."}, {"heading": "2.1.2 VGG", "text": "Visual Geometry Group (VGG) is a CNN model introduced by the Visual Geometry Group (VGG) from the University of Oxford. They proposed various models and configurations of deep CNNs [15], one of them was submitted to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)-2013. That model, also known as VGG-16 due to the fact that it is composed by 16 weight layers, became popular thanks to its achievement of 92.7% TOP-5 test accuracy. Figure 3 shows the configuration of VGG-16. The main difference between VGG-16 and its predecessors is the use of a stack of convolution layers with small receptive fields in the first layers instead of few layers with big\n3 receptive fields. This leads to less parameters and more nonlinearities in between, thus making the decision function more discriminative and the model easier to train."}, {"heading": "2.1.3 GoogLeNet", "text": "GoogLeNet is a network introduced by Szegedy et al. [16] which won the ILSVRC-2014 challenge with a TOP-5 test accuracy of 93.3%. This CNN architecture is characterized by its complexity, emphasized by the fact that it is composed by 22 layers and a newly introduced building block called inception module (see Figure 4). This new approach proved that CNN layers could be stacked in more ways than a typical sequential manner. In fact, those modules consist of a Network in Network (NiN) layer, a pooling operation, a large-sized convolution layer, and small-sized convolution layer. All of them are computed in parallel and followed by 1 \u00d7 1 convolution operations to reduce dimensionality. Thanks to those modules, this network puts special consideration on memory and computational cost by significantly reducing the number of parameters and operations."}, {"heading": "2.1.4 ResNet", "text": "Microsoft\u2019s ResNet [17] is specially remarkable thanks to winning ILSVRC-2016 with 96.4% accuracy. Apart from that fact, the network is well-known due to its depth (152 layers) and the introduction of residual blocks (see Figure 5). The residual blocks address the problem of training a really deep architecture by introducing identity skip connections so that layers can copy their inputs to the next layer.\nweight layer\nweight layer\n+F (\u03c7) + \u03c7\n\u03c7\nF (\u03c7) \u03c7\nidentity\nrelu\nFig. 5: Residual block from the ResNet architecture. Figure reproduced from [17].\nThe intuitive idea behind this approach is that it ensures that the next layer learns something new and different from what the input has already encoded (since it is provided with both the output of the previous layer and its unchanged input). In addition, this kind of connections help overcoming the vanishing gradients problem."}, {"heading": "2.1.5 ReNet", "text": "In order to extend Recurrent Neural Networks (RNNs) architectures to multi-dimensional tasks, Graves et al. [18] proposed a Multi-dimensional Recurrent Neural Network (MDRNN) architecture which replaces each single recurrent connection from standard RNNs with d connections, where d is the number of spatio-temporal data dimensions. Based on this initial approach, Visin el al. [19] proposed ReNet architecture in which instead of multidimensional RNNs, they have been using usual sequence RNNs. In this way, the number of RNNs is scaling linearly at each layer regarding to the number of dimensions d of the input image (2d). In this approach, each convolutional layer (convolution + pooling) is replaced with four RNNs sweeping the image vertically and horizontally in both directions as we can see in Figure 6.\n4"}, {"heading": "2.2 Transfer Learning", "text": "Training a deep neural network from scratch is often not feasible because of various reasons: a dataset of sufficient size is required (and not usually available) and reaching convergence can take too long for the experiments to be worth. Even if a dataset large enough is available and convergence does not take that long, it is often helpful to start with pre-trained weights instead of random initialized ones [20] [21]. Fine-tuning the weights of a pre-trained network by continuing with the training process is one of the major transfer learning scenarios.\nYosinski et al. [22] proved that transferring features even from distant tasks can be better than using random initialization, taking into account that the transferability of features decreases as the difference between the pre-trained task and the target one increases.\nHowever, applying this transfer learning technique is not completely straightforward. On the one hand, there are architectural constraints that must be met to use a pretrained network. Nevertheless, since it is not usual to come up with a whole new architecture, it is common to reuse already existing network architectures (or components) thus enabling transfer learning. On the other hand, the training process differs slightly when fine-tuning instead of training from scratch. It is important to choose properly which layers to fine-tune \u2013 usually the higher-level part of the network, since the lower one tends to contain more generic features \u2013 and also pick an appropriate policy for the learning rate, which is usually smaller due to the fact that the pre-trained weights are expected to be relatively good so there is no need to drastically change them.\nDue to the inherent difficulty of gathering and creating per-pixel labelled segmentation datasets, their scale is not as large as the size of classification datasets such as ImageNet [23] [24]. This problem gets even worse when dealing with RGB-D or 3D datasets, which are even smaller. For that reason, transfer learning, and in particular fine-tuning from pre-trained classification networks is a common trend for segmentation networks and has been successfully applied in the methods that we will review in the following sections."}, {"heading": "2.3 Data Preprocessing and Augmentation", "text": "Data augmentation is a common technique that has been proven to benefit the training of machine learning models in general and deep architectures in particular; either speeding up convergence or acting as a regularizer, thus avoiding overfitting and increasing generalization capabilities [25].\nIt typically consist of applying a set of transformations in either data or feature spaces, or even both. The most common augmentations are performed in the data space. That kind of augmentation generates new samples by applying transformations to the already existing data. There are many transformations that can be applied: translation, rotation, warping, scaling, color space shifts, crops, etc. The goal of those transformations is to generate more samples to create a larger dataset, preventing overfitting and presumably regularizing the model, balance the classes within that database, and even synthetically produce new samples that are more representative for the use case or task at hand.\nAugmentations are specially helpful for small datasets, and have proven their efficacy with a long track of success stories. For instance, in [26], a dataset of 1500 portrait images is augmented synthesizing four new scales (0.6, 0.8, 1.2, 1.5), four new rotations (\u221245,\u221222, 22, 45), and four gamma variations (0.5, 0.8, 1.2, 1.5) to generate a new dataset of 19000 training images. That process allowed them to raise the accuracy of their system for portrait segmentation from 73.09 to 94.20 Intersection over Union (IoU) when including that augmented dataset for fine-tuning."}, {"heading": "3 DATASETS AND CHALLENGES", "text": "Two kinds of readers are expected for this type of review: either they are initiating themselves in the problem, or either they are experienced enough and they are just looking for the most recent advances made by other researchers in the last few years. Although the second kind is usually aware of two of the most important aspects to know before starting to research in this problem, it is critical for newcomers to get a grasp of what are the top-quality datasets and challenges. Therefore, the purpose of this section is to kickstart novel scientists, providing them with a brief summary of datasets that might suit their needs as well as data augmentation and preprocessing tips. Nevertheless, it can also be useful for hardened researchers who want to review the fundamentals or maybe discover new information.\nArguably, data is one of the most \u2013 if not the most \u2013 important part of any machine learning system. When dealing with deep networks, this importance is increased even more. For that reason, gathering adequate data into a dataset is critical for any segmentation system based on deep learning techniques. Gathering and constructing an appropriate dataset, which must have a scale large enough and represent the use case of the system accurately, needs time, domain expertise to select relevant information, and infrastructure to capture that data and transform it to a representation that the system can properly understand and learn. This task, despite the simplicity of its formulation in comparison with sophisticated neural network architecture definitions, is one of the hardest problems to solve in this context. Because of that, the most sensible approach usually means using an existing standard dataset which is representative enough for the domain of the problem. Following this approach has another advantage for the community: standardized datasets enable fair comparisons between systems; in fact, many datasets are part of a challenge which reserves some data \u2013 not provided to developers to test their algorithms \u2013 for a competition in which many methods are tested, generating a fair ranking of methods according to their actual performance without any kind of data cherrypicking.\nIn the following lines we describe the most popular large-scale datasets currently in use for semantic segmentation. All datasets listed here provide appropriate pixelwise or point-wise labels. The list is structured into three parts according to the nature of the data: 2D or plain RGB datasets, 2.5D or RGB-Depth (RGB-D) ones, and pure volumetric or 3D databases. Table 1 shows a summarized view, gathering all the described datasets and providing\n5 useful information such as their purpose, number of classes, data format, and training/validation/testing splits."}, {"heading": "3.1 2D Datasets", "text": "Throughout the years, semantic segmentation has been mostly focused on two-dimensional images. For that reason, 2D datasets are the most abundant ones. In this section we describe the most popular 2D large-scale datasets for semantic segmentation, considering 2D any dataset that contains any kind of two-dimensional representations such as gray-scale or Red Green Blue (RGB) images.\n\u2022 PASCAL Visual Object Classes (VOC) [27]1: this challenge consists of a ground-truth annotated dataset of images and five different competitions: classification, detection, segmentation, action classification, and person layout. The segmentation one is specially interesting since its goal is to predict the object class of each pixel for each test image. There are 21 classes categorized into vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Background is also considered if the pixel does not belong to any of those classes. The dataset is divided into two subsets: training and validation with 1464 and 1449 images respectively. The test set is private for the challenge. This dataset is arguably the most popular for semantic segmentation so almost every remarkable method in the literature is being submitted to its performance evaluation server to validate against their private test set. Methods can be trained either using only the dataset or either using additional information. Furthermore, its leaderboard is public and can be consulted online2. \u2022 PASCAL Context [28]3: this dataset is an extension of the PASCAL VOC 2010 detection challenge which contains pixel-wise labels for all training images (10103). It contains a total of 540 classes \u2013 including the original 20 classes plus background from PASCAL VOC segmentation \u2013 divided into three categories (objects, stuff, and hybrids). Despite the large number of categories, only the 59 most frequent are remarkable. Since its classes follow a power law distribution, there are many of them which are too sparse throughout the dataset. In this regard, this subset of 59 classes is usually selected to conduct studies on this dataset, relabeling the rest of them as background. \u2022 PASCAL Part [29]4: this database is an extension of the PASCAL VOC 2010 detection challenge which goes beyond that task to provide per-pixel segmentation masks for each part of the objects (or at least silhouette annotation if the object does not have a\n1. http://host.robots.ox.ac.uk/pascal/VOC/voc2012/ 2. http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?\nchallengeid=11&compid=6 3. http://www.cs.stanford.edu/\u223croozbeh/pascal-context/ 4. http://www.stat.ucla.edu/\u223cxianjie.chen/pascal part dataset/ pascal part.html\nconsistent set of parts). The original classes of PASCAL VOC are kept, but their parts are introduced, e.g., bicycle is now decomposed into back wheel, chain wheel, front wheel, handlebar, headlight, and saddle. It contains labels for all training and validation images from PASCAL VOC as well as for the 9637 testing images. \u2022 Semantic Boundaries Dataset (SBD) [30]5: this dataset is an extended version of the aforementioned PASCAL VOC which provides semantic segmentation ground truth for those images that were not labelled in VOC. It contains annotations for 11355 images from PASCAL VOC 2011. Those annotations provide both category-level and instance-level information, apart from boundaries for each object. Since the images are obtained from the whole PASCAL VOC challenge (not only from the segmentation one), the training and validation splits diverge. In fact, SBD provides its own training (8498 images) and validation (2857 images) splits. Due to its increased amount of training data, this dataset is often used as a substitute for PASCAL VOC for deep learning. \u2022 Microsoft Common Objects in Context (COCO) [31]6: is another image recognition, segmentation, and captioning large-scale dataset. It features various challenges, being the detection one the most relevant for this field since one of its parts is focused on segmentation. That challenge, which features more than 80 classes, provides more than 82783 images for training, 40504 for validation, and its test set consist of more than 80000 images. In particular, the test set is divided into four different subsets or splits: test-dev (20000 images) for additional validation, debugging, test-standard (20000 images) is the default test data for the competition and the one used to compare state-of-the-art methods, testchallenge (20000 images) is the split used for the challenge when submitting to the evaluation server, and test-reserve (20000 images) is a split used to protect against possible overfitting in the challenge (if a method is suspected to have made too many submissions or trained on the test data, its results will be compared with the reserve split). Its popularity and importance has ramped up since its appearance thanks to its large scale. In fact, the results of the challenge are presented yearly on a joint workshop at the European Conference on Computer Vision (ECCV)7 together with ImageNet\u2019s ones. \u2022 SYNTHetic Collection of Imagery and Annotations (SYNTHIA) [32]8: is a large-scale collection of photorealistic renderings of a virtual city, semantically segmented, whose purpose is scene understanding in the context of driving or urban scenarios.The dataset provides fine-grained pixel-level annotations for 11 classes (void, sky, building, road, sidewalk, fence, vegetation, pole, car, sign, pedestrian, and cyclist). It\n5. http://home.bharathh.info/home/sbd 6. http://mscoco.org/ 7. http://image-net.org/challenges/ilsvrc+coco2016 8. http://synthia-dataset.net/\n6\n7 \u2022 Materials in Context (MINC) [43]: this work is a dataset for patch material classification and full scene material segmentation. The dataset provides segment annotations for 23 categories: wood, painted, fabric, glass, metal, tile, sky, foliage, polished stone, carpet, leather, mirror, brick, water, other, plastic, skin, stone, ceramic, hair, food, paper, and wallpaper. It contains 7061 labeled material segmentations for training, 5000 for test, and 2500 for validation. The main source for these images is the OpenSurfaces dataset [58], which was augmented using other sources of imagery such as Flickr or Houzz. For that reason, image resolution for this dataset varies. On average, image resolution is approximately 800\u00d7500 or 500\u00d7 800. \u2022 Densely-Annotated VIdeo Segmentation (DAVIS) [44] [45]12: this challenge is purposed for video object segmentation. Its dataset is composed by 50 highdefinition sequences which add up to 4219 and 2023 frames for training and validation respectively. Frame resolution varies across sequences but all of them were downsampled to 480p for the challenge. Pixel-wise annotations are provided for each frame for four different categories: human, animal, vehicle, and object. Another feature from this dataset is the presence of at least one target foreground object in each sequence. In addition, it is designed not to have many different objects with significant motion. For those scenes which do have more than one target foreground object from the same class, they provide separated ground truth for each one of them to allow instance segmentation. \u2022 Stanford background [40]13: dataset with outdoor scene images imported from existing public datasets: LabelMe, MSRC, PASCAL VOC and Geometric Context. The dataset contains 715 images (size of 320 \u00d7 240 pixels) with at least one foreground object and having the horizon position within the image. The dataset is pixel-wise annotated (horizon location, pixel semantic class, pixel geometric class and image region) for evaluating methods for semantic scene understanding. \u2022 SiftFlow [41]: contains 2688 fully annotated images which are a subset of the LabelMe database [59]. Most of the images are based on 8 different outdoor scenes including streets, mountains, fields, beaches and buildings. Images are 256 \u00d7 256 belonging to one of the 33 semantic classes. Unlabeled pixels, or pixels labeled as a different semantic class are treated as unlabeled."}, {"heading": "3.2 2.5D Datasets", "text": "With the advent of low-cost range scanners, datasets including not only RGB information but also depth maps are gaining popularity and usage. In this section, we review the most well-known 2.5D databases which include that kind of depth data.\n12. http://davischallenge.org/index.html 13. http://dags.stanford.edu/data/iccv09Data.tar.gz\n\u2022 NYUDv2 [46]14: this database consists of 1449 indoor RGB-D images captured with a Microsoft Kinect device. It provides per-pixel dense labeling (category and instance levels) which were coalesced into 40 indoor object classes by Gupta et al. [60] for both training (795 images) and testing (654) splits. This dataset is specially remarkable due to its indoor nature, this makes it really useful for certain robotic tasks at home. However, its relatively small scale with regard to other existing datasets hinders its application for deep learning architectures. \u2022 SUN3D [47]15: similar to the NYUDv2, this dataset contains a large-scale RGB-D video database, with 8 annotated sequences. Each frame has a semantic segmentation of the objects in the scene and information about the camera pose. It is still in progress and it will be composed by 415 sequences captured in 254 different spaces, in 41 different buildings. Moreover, some places have been captured multiple times at different moments of the day. \u2022 SUNRGBD [48]16: captured with four RGB-D sensors, this dataset contains 10000 RGB-D images, at a similar scale as PASCAL VOC. It contains images from NYU depth v2 [46], Berkeley B3DO [61], and SUN3D [47]. The whole dataset is densely annotated, including polygons, bounding boxes with orientation as well as a 3D room layout and category, being suitable for scene understanding tasks. \u2022 The Object Segmentation Database (OSD) [62]17\nthis database has been designed for segmenting unknown objects from generic scenes even under partial occlusions. This dataset contains 111 entries, and provides depth image and color images together withper-pixel annotations for each one to evaluate object segmentation approaches. However, the dataset does not differentiate the category of different objects so its classes are reduced to a binary set of objects and not objects. \u2022 RGB-D Object Dataset [49]18: this dataset is composed by video sequences of 300 common household objects organized in 51 categories arranged using WordNet hypernym-hyponym relationships. The dataset has been recorded using a Kinect style 3D camera that records synchronized and aligned 640 \u00d7 480 RGB and depth images at 30Hz. For each frame, the dataset provides, the RGB-D and depth images, a cropped ones containing the object, the location and a mask with per-pixel annotation. Moreover, each object has been placed on a turntable, providing isolated video sequences around 360 degrees. For the validation process, 22 annotated video sequences of natural indoor scenes containing the objects are provided.\n14. http://cs.nyu.edu/\u223csilberman/projects/indoor scene seg sup. html\n15. http://sun3d.cs.princeton.edu/ 16. http://rgbd.cs.princeton.edu/ 17. http://www.acin.tuwien.ac.at/?id=289 18. http://rgbd-dataset.cs.washington.edu/\n8"}, {"heading": "3.3 3D Datasets", "text": "Pure three-dimensional databases are scarce, this kind of datasets usually provide Computer Aided Design (CAD) meshes or other volumetric representations, such as point clouds. Generating large-scale 3D datasets for segmentation is costly and difficult, and not many deep learning methods are able to process that kind of data as it is. For those reasons, 3D datasets are not quite popular at the moment. In spite of that fact, we describe the most promising ones for the task at hand.\n\u2022 ShapeNet Part [50]19: is a subset of the ShapeNet [63] repository which focuses on fine-grained 3D object segmentation. It contains 31, 693 meshes sampled from 16 categories of the original dataset (airplane, earphone, cap, motorbike, bag, mug, laptop, table, guitar, knife, rocket, lamp, chair, pistol, car, and skateboard). Each shape class is labeled with two to five parts (totalling 50 object parts across the whole dataset), e.g., each shape from the airplane class is labeled with wings, body, tail, and engine. Groundtruth labels are provided on points sampled from the meshes. \u2022 Stanford 2D-3D-S [51]20: is a multi-modal and largescale indoor spaces dataset extending the Stanford 3D Semantic Parsing work [64]. It provides a variety of registered modalities \u2013 2D (RGB), 2.5D (depth maps and surface normals), and 3D (meshes and point clouds) \u2013 with semantic annotations. The database is composed of 70, 496 full high-definition RGB images (1080\u00d71080 resolution) along with their corresponding depth maps, surface normals, meshes, and point clouds with semantic annotations (perpixel and per-point). That data were captured in six indoor areas from three different educational and office buildings. That makes a total of 271 rooms and approximately 700 million points annotated with labels from 13 categories: ceiling, floor, wall, column, beam, window, door, table, chair, bookcase, sofa, board, and clutter. \u2022 A Benchmark for 3D Mesh Segmentation [52]21: this benchmark is composed by 380 meshes classified in 19 categories (human, cup, glasses, airplane, ant, chair, octopus, table, teddy, hand, plier, fish, bird, armadillo, bust, mech, bearing, vase, fourleg). Each mesh has been manually segmented into functional parts, the main goal is to provide a sample distribution over \u201dhow humans decompose each mesh into functional parts\u201d. \u2022 Sydney Urban Objects Dataset [53]22: this dataset contains a variety of common urban road objects scanned with a Velodyne HDK-64E LIDAR. There are 631 individual scans (point clouds) of objects across classes of vehicles, pedestrians, signs and trees. The interesting point of this dataset is that, for each\n19. http://cs.stanford.edu/\u223cericyi/project page/part annotation/ 20. http://buildingparser.stanford.edu 21. http://segeval.cs.princeton.edu/ 22. http://www.acfr.usyd.edu.au/papers/\nSydneyUrbanObjectsDataset.shtml\nobject, apart from the individual scan, a full 360- degrees annotated scan is provided. \u2022 Large-Scale Point Cloud Classification Benchmark [54]23: this benchmark provides manually annotated 3D point clouds of diverse natural and urban scenes: churches, streets, railroad tracks, squares, villages, soccer fields, castles among others. This dataset features statically captured point clouds with very fine details and density. It contains 15 large-scale point clouds for training and another 15 for testing. Its scale can be grasped by the fact that it totals more than one billion labelled points."}, {"heading": "4 METHODS", "text": "The relentless success of deep learning techniques in various high-level computer vision tasks \u2013 in particular, supervised approaches such as Convolutional Neural Networks (CNNs) for image classification or object detection [14] [15] [16] \u2013 motivated researchers to explore the capabilities of such networks for pixel-level labelling problems like semantic segmentation. The key advantage of these deep learning techniques, which gives them an edge over traditional methods, is the ability to learn appropriate feature representations for the problem at hand, e.g., pixel labelling on a particular dataset, in an end-to-end fashion instead of using hand-crafted features that require domain expertise, effort, and often too much fine-tuning to make them work on a particular scenario.\n23. http://www.semantic3d.net/\n9\nCurrently, the most successful state-of-the-art deep learning techniques for semantic segmentation stem from a common forerunner: the Fully Convolutional Network (FCN) by Long et al. [65]. The insight of that approach was to take advantage of existing CNNs as powerful visual models that are able to learn hierarchies of features. They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores. Those maps are upsampled using fractionally strided convolutions (also named deconvolutions [90] [91]) to produce dense per-pixel labeled outputs. This work is considered a milestone since it showed how CNNs can be trained end-to-end for this problem, efficiently learning how to make dense predictions for semantic segmentation with inputs of arbitrary sizes. This approach achieved a significant improvement in segmentation accuracy over traditional methods on standard datasets like PASCAL VOC, while preserving efficiency at inference. For all those reasons, and other significant contributions, the FCN is the cornerstone of deep learning applied to semantic segmentation. The convolutionalization process is shown in Figure 7. Despite the power and flexibility of the FCN model, it still lacks various features which hinder its application to certain problems and situations: its inherent spatial invariance does not take into account useful global context information, no instance-awareness is present by default, efficiency is still far from real-time execution at high resolutions, and it is not completely suited for unstructured data such as 3D point clouds or models. Those problems will be reviewed in this section, as well as the state-of-the-art solutions that have been proposed in the literature to overcome those hurdles. Table 2 provides a summary of that review. It shows all reviewed methods (sorted by appearance order in the section), their base architecture, their main contribution, and a classification depending on the target of the work: accuracy, efficiency, training simplicity, sequence processing, multi-modal inputs, and 3D data. Each target is graded from\n10\none to three stars (?) depending on how much focus puts the work on it, and a mark (7) if that issue is not addressed. In addition, Figure 8 shows a graph of the reviewed methods for the sake of visualization."}, {"heading": "4.1 Decoder Variants", "text": "Apart from the FCN architecture, other variants were developed to transform a network whose purpose was classification to make it suitable for segmentation. Arguably, FCN-based architectures are more popular and successful, but other alternatives are also remarkable. In general terms, all of them take a network for classification, such as VGG16, and remove its fully connected layers. This part of the new segmentation network often receives the name of encoder and produce low-resolution image representations or feature maps. The problem lies on learning to decode or map those low-resolution images to pixel-wise predictions for segmentation. This part is named decoder and it is usually the divergence point in this kind of architectures.\nSegNet [66] is a clear example of this divergence (see Figure 9). The decoder stage of SegNet is composed by a set of upsampling and convolution layers which are at last followed by a softmax classifier to predict pixel-wise labels for an output which has the same resolution as the input image. Each upsampling layer in the decoder stage corresponds to a max-pooling one in the encoder part. Those layers upsample feature maps using the max-pooling indices from their corresponding feature maps in the encoder phase. The upsampled maps are then convolved with a set of trainable filter banks to produce dense feature maps. When the feature maps have been restored to the original resolution, they are fed to the softmax classifier to produce the final segmentation.\nOn the other hand, FCN-based architectures make use of learnable deconvolution filters to upsample feature maps. After that, the upsampled feature maps are added elementwise to the corresponding feature map generated by the convolution layer in the encoder part. Figure 10 shows a comparison of both approaches."}, {"heading": "4.2 Integrating Context Knowledge", "text": "Semantic segmentation is a problem that requires the integration of information from various spatial scales. It also implies balancing local and global information. On the one hand, fine-grained or local information is crucial to achieve good pixel-level accuracy. On the other hand, it is also important to integrate information from the global context of the image to be able to resolve local ambiguities.\nFig. 10: Comparison of SegNet (left) and FCN (right) decoders. While SegNet uses max-pooling indices from the corresponding encoder stage to upsample, FCN learns deconvolution filters to upsample (adding the corresponding feature map from the encoder stage). Figure reproduced from [66].\nVanilla CNNs struggle with this balance. Pooling layers, which allow the networks to achieve some degree of spatial invariance and keep computational cost at bay, dispose of the global context information. Even purely CNNs \u2013 without pooling layers \u2013 are limited since the receptive field of their units can only grow linearly with the number of layers.\nMany approaches can be taken to make CNNs aware of that global information: refinement as a post-processing step with Conditional Random Fields (CRFs), dilated convolutions, multi-scale aggregation, or even defer the context modeling to another kind of deep networks such as RNNs."}, {"heading": "4.2.1 Conditional Random Fields", "text": "As we mentioned before, the inherent invariance to spatial transformations of CNN architectures limits the very same spatial accuracy for segmentation tasks. One possible and common approach to refine the output of a segmentation system and boost its ability to capture fine-grained details is to apply a post-processing stage using a Conditional Random Field (CRF). CRFs enable the combination of lowlevel image information \u2013 such as the interactions between pixels [92] [93] \u2013 with the output of multi-class inference systems that produce per-pixel class scores. That combination is especially important to capture long-range dependencies, which CNNs fail to consider, and fine local details.\nThe DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kra\u0308henbu\u0308hl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result. It models each pixel as a node in the field and employs one pairwise term for each pair of pixels no matter how far they lie (this model is known as dense or fully connected factor graph). By using this model, both short and long-range interactions are taken into account, rendering the system able to recover detailed structures in the segmentation that were lost due to the spatial invariance of the CNN. Despite the fact that usually fully connected models are inefficient, this model can be efficiently approximated via probabilistic inference. Figure 11 shows the effect of this CRF-based post-processing on the score and belief maps produced by the DeepLab model.\nThe material recognition in the wild network by Bell et al. [43] makes use of various CNNs trained to identify patches in the MINC database. Those CNNs are used on a sliding\n11\nwindow fashion to classify those patches. Their weights are transferred to the same networks converted into FCNs by adding the corresponding upsampling layers. The outputs are averaged to generate a probability map. At last, the same CRF from DeepLab, but discretely optimized, is applied to predict and refine the material at every pixel.\nAnother significant work applying a CRF to refine the segmentation of a FCN is the CRFasRNN by Zheng et al. [70]. The main contribution of that work is the reformulation of the dense CRF with pairwise potentials as an integral part of the network. By unrolling the mean-field inference steps as RNNs, they make it possible to fully integrate the CRF with a FCN and train the whole network end-to-end. This work demonstrates the reformulation of CRFs as RNNs to form a part of a deep network, in contrast with Pinheiro et al. [81] which employed RNNs to model large spatial dependencies."}, {"heading": "4.2.2 Dilated Convolutions", "text": "Dilated convolutions, also named a\u0300-trous convolutions, are a generalization of Kronecker-factored convolutional filters [96] which support exponentially expanding receptive fields without losing resolution. In other words, dilated convolutions are regular ones that make use of upsampled filters. The dilation rate l controls that upsampling factor. As shown in Figure 12, stacking l-dilated convolution makes the receptive fields grow exponentially while the number of parameters for the filters keeps a linear growth. This means that dilated convolutions allow efficient dense feature extraction on any arbitrary resolution. As a side note, it is important to remark that typical convolutions are just 1- dilated convolutions.\nIn practice, it is equivalent to dilating the filter before doing the usual convolution. That means expanding its size, according to the dilation rate, while filling the empty elements with zeros. In other words, the filter weights are matched to distant elements which are not adjacent if the dilation rate is greater than one. Figure 13 shows examples of dilated filters.\nThe most important works that make use of dilated convolutions are the multi-scale context aggregation module by Yu et al. [71], the already mentioned DeepLab (its improved version) [69], and the real-time network ENet [72]. All of them use combinations of dilated convolutions with increasing dilation rates to have wider receptive fields with no additional cost and without overly downsampling the feature maps. Those works also show a common trend: dilated convolutions are tightly coupled to multi-scale context aggregation as we will explain in the following section."}, {"heading": "4.2.3 Multi-scale Prediction", "text": "Another possible way to deal with context knowledge integration is the use of multi-scale predictions. Almost every single parameter of a CNN affects the scale of the generated feature maps. In other words, the very same architecture will have an impact on the number of pixels of the input image which correspond to a pixel of the feature map. This means that the filters will implicitly learn to detect features at specific scales (presumably with certain invariance degree). Furthermore, those parameters are usually tightly coupled to the problem at hand, making it difficult for the models to generalize to different scales. One possible way to overcome that obstacle is to use multi-scale networks which generally make use of multiple networks that target different scales and then merge the predictions to produce a single output.\nRaj et al. [73] propose a multi-scale version of a fully convolutional VGG-16. That network has two paths, one that processes the input at the original resolution and another one which doubles it. The first path goes through a shallow convolutional network. The second one goes through the fully convolutional VGG-16 and an extra convolutional layer. The result of that second path is upsampled and combined with the result of the first path. That concatenated output then goes through another set of convolutional layers to generate the final output. As a result, the network becomes more robust to scale variations.\nRoy et al. [75] take a different approach using a network composed by four multi-scale CNNs. Those four networks\n12\nhave the same architecture introduced by Eigen et al. [74]. One of those networks is devoted to finding semantic labels for the scene. That network extracts features from a progressively coarse-to-fine sequence of scales (see Figure 14).\nAnother remarkable work is the network proposed by Bian et al. [76]. That network is a composition of n FCNs which operate at different scales. The features extracted from the networks are fused together (after the necessary upsampling with an appropriate padding) and then they go through an additional convolutional layer to produce the final segmentation. The main contribution of this architecture is the two-stage learning process which involves, first, training each network independently, then the networks are combined and the last layer is fine-tuned. This multi-scale model allows to add an arbitrary number of newly trained networks in an efficient manner."}, {"heading": "4.2.4 Feature Fusion", "text": "Another way of adding context information to a fully convolutional architecture for segmentation is feature fusion. This technique consists of merging a global feature (extracted from a previous layer in a network) with a more local feature map extracted from a subsequent layer. Common architectures such as the original FCN make use of skip connections to perform a late fusion by combining the feature maps extracted from different layers (see Figure 15).\nAnother approach is performing early fusion. This approach is taken by ParseNet [77] in their context module. The global feature is unpooled to the same spatial size as the local feature and then they are concatenated to generate a combined feature that is used in the next layer or to learn a classifier. Figure 16 shows a representation of that process.\nFig. 15: Skip-connection-like architecture, which performs late fusion of feature maps as if making independent predictions for each layer and merging the results. Figure extracted from [84].\nFig. 16: ParseNet context module overview in which a global feature (from a previous layer) is combined with the feature of the next layer to add context information. Figure extracted from [77].\nThis feature fusion idea was continued by Pinheiro et al. in their SharpMask network [84], which introduced a progressive refinement module to incorporate features from the previous layer to the next in a top-down architecture. This work will be reviewed later since it is mainly focused on instance segmentation."}, {"heading": "4.2.5 Recurrent Neural Networks", "text": "As we noticed, CNNs have been successfully applied to multi-dimensional data, such as images. Nevertheless, these networks rely on hand specified kernels limiting the architecture to local contexts. Taking advantage of its topological structure, Recurrent Neural Networks have been successfully applied for modeling short- and long-temporal sequences. In this way and by linking together pixel-level and local information, RNNs are able to successfully model global contexts and improve semantic segmentation. However, one important issue is the lack of a natural sequential structure in images and the focus of standard vanilla RNNs architectures on one-dimensional inputs.\nBased on ReNet model for image classification Visin et al. [19] proposed an architecture for semantic segmentation called ReSeg [78] represented in Figure 17. In this approach, the input image is processed with the first layers of the\n13\nFig. 17: Representation of ReSeg network. VGG-16 convolutional layers are represented by the blue and yellow first layers. The rest of the architecture is based on the ReNet approach with fine-tuning purposes. Figure extracted from [78].\nVGG-16 network [15], feeding the resulting feature maps into one or more ReNet layers for fine-tuning. Finally, feature maps are resized using upsampling layers based on transposed convolutions. In this approach Gated Recurrent Units (GRUs) have been used as they strike a good performance balance regarding memory usage and computational power. Vanilla RNNs have problems modeling longterm dependencies mainly due to the vanishing gradients problem. Several derived models such as Long Short-Term Memory (LSTM) networks [97] and GRUs [98] are the stateof-art in this field to avoid such problem.\nInspired on the same ReNet architecture, a novel Long Short-Term Memorized Context Fusion (LSTM-CF) model for scene labeling was proposed by [99]. In this approach, they use two different data sources: RGB and depth. The RGB pipeline relies on a variant of the DeepLab architecture [29] concatenating features at three different scales to enrich feature representation (inspired by [100]). The global context is modeled vertically over both, depth and photometric data sources, concluding with a horizontal fusion in both direction over these vertical contexts.\nAs we noticed, modeling image global contexts is related to 2D recurrent approaches by unfolding vertically and horizontally the network over the input images. Based on the same idea, Byeon et al. [80] purposed a simple 2D LSTMbased architecture in which the input image is divided into non-overlapping windows which are fed into four separate LSTMs memory blocks. This work emphasizes its low computational complexity on a single-core CPU and the model simplicity.\nAnother approach for capturing global information relies on using bigger input windows in order to model larger contexts. Nevertheless, this reduces images resolution and also implies several problems regarding to window overlapping. However, Pinheiro et al. [81] introduced Recurrent Convolutional Neural Networks (rCNNs) which recurrently train with different input window sizes taking into account previous predictions by using a different input window sizes. In this way, predicted labels are automatically smoothed increasing the performance.\nUndirected cyclic graphs (UCGs) were also adopted to model image contexts for semantic segmentation [82]. Nevertheless, RNNs are not directly applicable to UCG and the solution is decomposing it into several directed graphs (DAGs). In this approach, images are processed by\nthree different layers: image feature map produced by CNN, model image contextual dependencies with DAG-RNNs, and deconvolution layer for upsampling feature maps. This work demonstrates how RNNs can be used together with graphs to successfully model long-range contextual dependencies, overcoming state-of-the-art approaches in terms of performance."}, {"heading": "4.3 Instance Segmentation", "text": "Instance segmentation is considered the next step after semantic segmentation and at the same time the most challenging problem in comparison with the rest of lowlevel pixel segmentation techniques. Its main purpose is to represent objects of the same class splitted into different instances. The automation of this process is not straightforward, thus the number of instances is initially unknown and the evaluation of performed predictions is not pixelwise such as in semantic segmentation. Consequently, this problem remains partially unsolved but the interest in this field is motivated by its potential applicability. Instance labeling provides us extra information for reasoning about occlusion situations, also counting the number of elements belonging to the same class and for detecting a particular object for grasping in robotics tasks, among many other applications.\nFor this purpose, Hariharan et al. [10] proposed a Simultaneous Detection and Segmentation (SDS) method in order to improve performance over already existing works. Their pipeline uses, firstly, a bottom-up hierarchical image segmentation and object candidate generation process called Multi-scale COmbinatorial Grouping (MCG) [101] to obtain region proposals. For each region, features are extracted by using an adapted version of the Region-CNN (R-CNN) [102], which is fine-tuned using bounding boxes provided by the MCG method instead of selective search and also alongside region foreground features. Then, each region proposal is classified by using a linear Support Vector Machine (SVM) on top of the CNN features. Finally, and for refinement purposes, Non-Maximum Suppression (NMS) is applied to the previous proposals.\nLater, Pinheiro et al. [83] presented DeepMask model, an object proposal approach based on a single ConvNet. This model predicts a segmentation mask for an input patch and the likelihood of this patch for containing an object. The two tasks are learned jointly and computed by a single network,\n14\nsharing most of the layers except last ones which are taskspecific.\nBased on the DeepMask architecture as a starting point due to its effectiveness, the same authors presented a novel architecture for object instance segmentation implementing a top-down refinement process [84] and achieving a better performance in terms of accuracy and speed. The goal of this process is to efficiently merge low-level features with highlevel semantic information from upper network layers. The process consisted in different refinement modules stacked together (one module per pooling layer), with the purpose of inverting pooling effect by generating a new upsampled object encoding. Figure 18 shows the refinement module in SharpMask.\nAnother approach, based on Fast R-CNN as a starting point and using DeepMask object proposals instead of Selective Search was presented by Zagoruyko et al [85]. This combined system called MultiPath classifier, improved performance over COCO dataset and supposed three modifications to Fast R-CNN: improving localization with an integral loss, provide context by using foveal regions and finally skip connections to give multi-scale features to the network. The system achieved a 66% improvement over the baseline Fast R-CNN.\nAs we have seen, most of the methods mentioned above rely on existing object detectors limiting in this way model performance. Even so, instance segmentation process remains an unresolved research problem and the mentioned works are only a small part of this challenging research topic."}, {"heading": "4.4 RGB-D Data", "text": "As we noticed, a significant amount of work has been done in semantic segmentation by using photometric data. Nevertheless, the use of structural information was spurred on with the advent of low-cost RGB-D sensors which provide useful geometric cues extracted from depth information.\nSeveral works focused on RGB-D scene segmentation have reported an improvement in the fine-grained labeling precision by using depth information and not only photometric data. Using depth information for segmentation is considered more challenging because of the unpredictable variation of scene illumination alongside incomplete representation of objects due to complex occlusions. However, various works have successfully made use of depth information to increase accuracy.\nThe use of depth images with approaches focused on photometric data is not straightforward. Depth data needs to be encoded with three channels at each pixel as if it was an RGB images. Different techniques such as Horizontal Height Angle (HHA) [11] are used for encoding the depth into three channels as follows: horizontal disparity, height above ground, and the angle between local surface normal and the inferred gravity direction. In this way, we can input depth images to models designed for RGB data and improve in this way the performance by learning new features from structural information. Several works such as [99] are based on this encoding technique.\nIn the literature, related to methods that use RGB-D data, we can also find some works that leverage a multi-view approach to improve existing single-view works.\nZeng et al. [103] present an object segmentation approach that leverages multi-view RGB-D data and deep learning techniques. RGB-D images captured from each viewpoint are fed to a FCN network which returns a 40-class probability for each pixel in each image. Segmentation labels are threshold by using three times the standard deviation above the mean probability across all views. Moreover, in this work, multiple networks for feature extraction were trained (AlexNet [14] and VGG-16 [15]), evaluating the benefits of using depth information. They found that adding depth did not yield any major improvements in segmentation performance, which could be caused by noise in the depth information. The described approach was presented during the 2016 Amazon Picking Challenge. This work is a minor contribution towards multi-view deep learning systems since RGB images are independently fed to a FCN network.\nMa et al. [104] propose a novel approach for object-class segmentation using a multi-view deep learning technique. Multiple views are obtained from a moving RGB-D camera. During the training stage, camera trajectory is obtained using an RGB-D SLAM technique, then RGB-D images are warped into ground-truth annotated frames in order to enforce multi-view consistency for training. The proposed approach is based on FuseNet [105], which combines RGB and depth images for semantic segmentation, and improves the original work by adding multi-scale loss minimization."}, {"heading": "4.5 3D Data", "text": "3D geometric data such as point clouds or polygonal meshes are useful representations thanks to their additional dimension which provides methods with rich spatial information that is intuitively useful for segmentation. However, the vast majority of successful deep learning segmentation architectures \u2013 CNNs in particular \u2013 are not originally engineered to deal with unstructured or irregular inputs such as the aforementioned ones. In order to enable weight sharing\n15\nand other optimizations in convolutional architectures, most researchers have resorted to 3D voxel grids or projections to transform unstructured and unordered point clouds or meshes into regular representations before feeding them to the networks. For instance, Huang et al. [86] (see Figure 19 take a point cloud and parse it through a dense voxel grid, generating a set of occupancy voxels which are used as input to a 3D CNN to produce one label per voxel. They then map back the labels to the point cloud. Although this approach has been applied successfully, it has some disadvantages like quantization, loss of spatial information, and unnecessarily large representations. For that reason, various researchers have focused their efforts on creating deep architectures that are able to directly consume unstructured 3D point sets or meshes.\nPointNet [87] is a pioneering work which presents a deep neural network that takes raw point clouds as input, providing a unified architecture for both classification and segmentation. Figure 20 shows that two-part network which is able to consume unordered point sets in 3D.\nAs we can observe, PointNet is a deep network architecture that stands out of the crowd due to the fact that it is based on fully connected layers instead of convolutional ones. The architecture features two subnetworks: one for classification and another for segmentation. The classification subnetwork takes a point cloud and applies a set of transforms and Multi Layer Perceptrons (MLPs) to generate features which are then aggregated using max-pooling to generate a global feature which describes the original input cloud. That global feature is classified by another MLP to produce output scores for each class. The segmentation subnetwork concatenates the global feature with the perpoint features extracted by the classification network and applies another two MLPs to generate features and produce output scores for each point."}, {"heading": "4.6 Video Sequences", "text": "As we have observed, there has been a significant progress in single-image segmentation. However, when dealing with image sequences, many systems rely on the na\u0131\u0308ve application of the very same algorithms in a frame-by-frame\nmanner. This approach works, often producing remarkable results. Nevertheless, applying those methods frame by frame is usually non-viable due to computational cost. In addition, those methods completely ignore temporal continuity and coherence cues which might help increase the accuracy of the system while reducing its execution time.\nArguably, the most remarkable work in this regard is the clockwork FCN by Shelhamer et al. [88]. This network is an adaptation of a FCN to make use of temporal cues in video to decrease inference time while preserving accuracy. The clockwork approach relies on the following insight: feature velocity \u2013 the temporal rate of change of features in the network \u2013 across frames varies from layer to layer so that features from shallow layers change faster than deep ones. Under that assumption, layers can be grouped into stages, processing them at different update rates depending on their depth. By doing this, deep features can be persisted over frames thanks to their semantic stability, thus saving inference time. Figure 21 shows the network architecture of the clockwork FCN.\nIt is important to remark that the authors propose two kinds of update rates: fixed and adaptive. The fixed schedule just sets a constant time frame for recomputing the features for each stage of the network. The adaptive schedule fires each clock on a data-driven manner, e.g., depending on the amount of motion or semantic change. Figure 22 shows an example of this adaptive scheduling.\nZhang et al. [106] took a different approach and made use of a 3DCNN, which was originally created for learning features from volumes, to learn hierarchical spatio-temporal features from multi-channel inputs such as video clips. In parallel, they over-segment the input clip into supervoxels. Then they use that supervoxel graph and embed the learned features in it. The final segmentation is obtained by applying graph-cut [107] on the supervoxel graph.\nAnother remarkable method, which builds on the idea of using 3D convolutions, is the deep end-to-end voxel-tovoxel prediction system by Tran et al. [89]. In that work, they make use of the Convolutional 3D (C3D) network introduced by themselves on a previous work [108], and extend it for semantic segmentation by adding deconvolutional layers at the end. Their system works by splitting the input into clips of 16 frames, performing predictions for each clip separately. Its main contribution is the use of 3D convolutions. Those convolutions make use of three-dimensional filters which are suitable for spatio-temporal feature learning across multiple channels, in this case frames. Figure 23 shows the difference between 2D and 3D convolutions applied to multi-channel inputs, proving the usefulness of the 3D ones for video segmentation."}, {"heading": "5 DISCUSSION", "text": "In the previous section we reviewed the existing methods from a literary and qualitative point of view, i.e., we did not take any quantitative result into account. In this Section we are going to discuss the very same methods from a numeric standpoint. First of all, we will describe the most popular evaluation metrics that can be used to measure the performance of semantic segmentation systems from three aspects: execution time, memory footprint, and accuracy.\n16\nFig. 20: The PointNet unified architecture for point cloud classification and segmentation. Figure reproduced from [87].\nNext, we will gather the results of the methods on the most representative datasets using the previously described metrics. After that, we will summarize and draw conclusions about those results. At last, we enumerate possible future research lines that we consider significant for the field."}, {"heading": "5.1 Evaluation Metrics", "text": "For a segmentation system to be useful and actually produce a significant contribution to the field, its performance must be evaluated with rigor. In addition, that evaluation must be performed using standard and well-known metrics that enable fair comparisons with existing methods. Furthermore, many aspects must be evaluated to assert the validity and usefulness of a system: execution time, memory footprint, and accuracy. Depending on the purpose or the context of the system, some metrics might be of more importance than others, i.e., accuracy may be expendable up to a certain point in favor of execution speed for a real-time application. Nevertheless, for the sake of scientific rigor it is of utmost importance to provide all the possible metrics for a proposed method."}, {"heading": "5.1.1 Execution Time", "text": "Speed or runtime is an extremely valuable metric since the vast majority of systems must meet hard requirements on how much time can they spend on the inference pass. In some cases it might be useful to know the time needed for training the system, but it is usually not that significant, unless it is exaggeratedly slow, since it is an offline process. In any case, providing exact timings for the methods can be seen as meaningless since they are extremely dependant on the hardware and the backend implementation, rendering some comparisons pointless.\nHowever, for the sake of reproducibility and in order to help fellow researchers, it is useful to provide timings with a thorough description of the hardware in which the system was executed on, as well as the conditions for the benchmark. If done properly, that can help others estimate if the method is useful or not for the application as well as perform fair comparisons under the same conditions to check which are the fastest methods.\n17"}, {"heading": "5.1.2 Memory Footprint", "text": "Memory usage is another important factor for segmentation methods. Although it is arguably less constraining than execution time \u2013 scaling memory capacity is usually feasible \u2013 it can also be a limiting element. In some situations, such as onboard chips for robotic platforms, memory is not as abundant as in a high-performance server. Even high-end Graphics Processing Units (GPUs), which are commonly used to accelerate deep networks, do not pack a copious amount of memory. In this regard, and considering the same implementation-dependent aspects as with runtime, documenting the peak and average memory footprint of a method with a complete description of the execution conditions can be extraordinarily helpful."}, {"heading": "5.1.3 Accuracy", "text": "Many evaluation criteria have been proposed and are frequently used to assess the accuracy of any kind of technique for semantic segmentation. Those metrics are usually variations on pixel accuracy and IoU. We report the most popular metrics for semantic segmentation that are currently used to measure how per-pixel labeling methods perform on this task. For the sake of the explanation, we remark the following notation details: we assume a total of k+1 classes (from L0 to Lk including a void class or background) and pij is the amount of pixels of class i inferred to belong to class j. In other words, pii represents the number of true positives, while pij and pji are usually interpreted as false positives and false negatives respectively (although either of them can be the sum of both false positives and false negatives)..\n\u2022 Pixel Accuracy (PA): it is the simplest metric, simply computing a ratio between the amount of properly classified pixels and the total number of them.\nPA =\nk\u2211 i=0 pii\nk\u2211 i=0 k\u2211 j=0 pij\n\u2022 Mean Pixel Accuracy (MPA): a slightly improved PA in which the ratio of correct pixels is computed in a per-class basis and then averaged over the total number of classes.\nMPA = 1\nk + 1 k\u2211 i=0 pii k\u2211\nj=0\npij\n\u2022 Mean Intersection over Union (MIoU): this is the standard metric for segmentation purposes. It computes a ratio between the intersection and the union of two sets, in our case the ground truth and our predicted segmentation. That ratio can be reformulated as the number of true positives (intersection) over the sum of true positives, false negatives, and\nfalse positives (union). That IoU is computed on a per-class basis and then averaged.\nMIoU = 1\nk + 1 k\u2211 i=0 pii k\u2211\nj=0\npij + k\u2211 j=0 pji \u2212 pii\n\u2022 Frequency Weighted Intersection over Union (FWIoU): it is an improved over the raw MIoU which weights each class importance depending on their appearance frequency.\nFWIoU = 1 k\u2211 i=0 k\u2211 j=0 pij\nk\u2211 i=0\nk\u2211 j=0 pijpii\nk\u2211 j=0 pij + k\u2211 j=0 pji \u2212 pii\nOf all metrics described above, the MIoU stands out of the crowd as the most used metric due to its representativeness and simplicity. Most challenges and researchers make use of that metric to report their results."}, {"heading": "5.2 Results", "text": "As we stated before, Section 4 provided a functional description of the reviewed methods according to their targets. Now we gathered all the quantitative results for those methods as stated by their authors in their corresponding papers. These results are organized into three parts depending on the input data used by the methods: 2D RGB or 2.5D RGB-D images, volumetric 3D, or video sequences.\nThe most used datasets have been selected for that purpose. It is important to remark the heterogeneity of the papers in the field when reporting results. Although most of them try to evaluate their methods in standard datasets and provide enough information to reproduce their results, also expressed in widely known metrics, many others fail to do so. That leads to a situation in which it is hard or even impossible to fairly compare methods.\nFurthermore, we also came across the fact few authors provide information about other metrics rather than accuracy. Despite the importance of other metrics, most of the papers do not include any data about execution time nor memory footprint. In some cases that information is provided, but no reproducibility information is given so it is impossible to know the setup that produced those results which are of no use."}, {"heading": "5.2.1 RGB", "text": "For the single 2D image category we have selected seven datasets: PASCAL VOC2012, PASCAL Context, PASCAL Person-Part, CamVid, CityScapes, Stanford Background, and SiftFlow. That selection accounts for a wide range of situations and targets.\nThe first, and arguably the most important dataset, in which the vast majority of methods are evaluated is PASCAL VOC-2012. Table 3 shows the results of those reviewed methods which provide accuracy results on the PASCAL VOC-2012 test set. This set of results shows a clear improvement trend from the firs proposed methods (SegNet\n18\nand the original FCN) to the most complex models such as CRFasRNN and the winner (DeepLab) with 79.70 IoU.\nApart from the widely known VOC we also collected metrics of its Context counterpart. Table 4 shows those results in which DeepLab is again the top scorer (45.70 IoU).\nIn addition, we also took into account the PASCAL Part dataset, whose results are shown in Table 5. In this case, the only analyzed method that provided metrics for this dataset is DeepLab which achieved a 64.94 IoU.\nMoving from a general-purpose dataset such as PASCAL VOC, we also gathered results for two of the most important urban driving databases. Table 6 shows the results of those methods which provide accuracy metrics for the CamVid dataset. In this case, an RNN-based approach (DAG-RNN) is the top one with a 91.60 IoU.\nTable 7 shows the results on a more challenging and currently more in use database: CityScapes. The trend on this dataset is similar to the one with PASCAL VOC with DeepLab leading with a 70.40 IoU.\nTable 8 shows the results of various recurrent networks on the Stanford Background dataset. The winner, rCNN, achieves a maximum accuracy of 80.20 IoU.\nAt last, results for another popular dataset such as SiftFlow are shown in Table 9. This dataset is also dominated by\nrecurrent methods. In particular DAG-RNN is the top scorer with 85.30 IoU."}, {"heading": "5.2.2 2.5D", "text": "Regarding the 2.5D category, i.e., datasets which also include depth information apart from the typical RGB channels, we have selected three of them for the analysis: SUNRGB-D and NYUDv2. Table 10 shows the results for SUNRGB-D that are only provided by LSTM-CF, which achieves 48.10 IoU.\nTable 11 shows the results for NYUDv2 which are exclusive too for LSTM-CF. That method reaches 49.40 IoU.\nAt last, Table 12 gathers results for the last 2.5D dataset: SUN-3D. Again, LSTM-CF is the only one which provides information for that database, in this case a 58.50 accuracy."}, {"heading": "5.2.3 3D", "text": "Two 3D datasets have been chosen for this discussion: ShapeNet Part and Stanford-2D-3D-S. In both cases, only one of the analyzed methods actually scored on them. It is the case of PointNet which achieved 83.80 and 47.71 IoU on ShapeNet Part (Table 13) and Stanford-2D-3D-S (Table 14) respectively.\n19"}, {"heading": "5.2.4 Sequences", "text": "The last category included in this discussion is video or sequences. For that part we gathered results for two datasets which are suitable for sequence segmentation: CityScapes and YouTube-Objects. Only one of the reviewed methods for video segmentation provides quantitative results on those datasets: Clockwork Convnet. That method reaches 64.40 IoU on CityScapes (Table 15), and 68.50 on YouTube-Objects (Table 16)."}, {"heading": "5.3 Summary", "text": "In light of the results, we can draw various conclusions. The most important of them is related to reproducibility. As we have observed, many methods report results on nonstandard datasets or they are not even tested at all. That makes comparisons impossible. Furthermore, some of them do not describe the setup for the experimentation or do not provide the source code for the implementation, thus significantly hurting reproducibility. Methods should report their results on standard datasets, exhaustively describe the training procedure, and also make their models and weights publicly available to enable progress.\nAnother important fact discovered thanks to this study is the lack of information about other metrics such as execution time and memory footprint. Almost no paper reports this kind of information, and those who do suffer from the reproducibility issues mentioned before. This void is due to the fact that most methods focus on accuracy without any concern about time or space. However, it is important to think about where are those methods being applied. In practice, most of them will end up running on embedded devices, e.g., self-driving cars, drones, or robots, which are fairly limited from both sides: computational power and memory.\nRegarding the results themselves, we can conclude that DeepLab is the most solid method which outperforms the rest on almost every single RGB images dataset by a significant margin. The 2.5D or multimodal datasets are dominated by recurrent networks such as LSTM-CF. 3D data segmentation still has a long way to go with PointNet paving the way for future research on dealing with unordered point clouds without any kind of preprocessing or discretization. Finally, dealing with video sequences is another green area with no clear direction, but Clockwork Convnets are the most promising approach thanks to their efficiency and accuracy duality. 3D convolutions are worth remarking due to their power and flexibility to process multichannel inputs, making them successful at capturing both spatial and temporal information."}, {"heading": "5.4 Future Research Directions", "text": "Based on the reviewed research, which marks the state of the art of the field, we present a list of future research directions that would be interesting to pursue.\n\u2022 3D datasets: methods that make full use of 3D information are starting to rise but, even if new proposals and techniques are engineered, they still lack one of the most important components: data. There is a strong need for large-scale datasets for 3D semantic segmentation, which are harder to create than their lower dimensional counterparts. Although there are already some promising works, there is still room for more, better, and varied data. It is important to remark the importance of real-world 3D data since most of the already existing works are synthetic databases. A proof of the importance of 3D is the fact that the ILSVRC will feature 3D data in 2018. \u2022 Sequence datasets: the same lack of large-scale data that hinders progress on 3D segmentation also impacts video segmentation. There are only a few datasets that are sequence-based and thus helpful for developing methods which take advantage of temporal information. Bringing up more high-quality data from this nature, either 2D or 3D, will unlock new research lines without any doubt. \u2022 Point cloud segmentation using Graph Convolutional Networks (GCNs): as we already mentioned, dealing with 3D data such as point clouds poses an unsolved challenge. Due to its unordered and unstructured nature, traditional architectures such as CNNs cannot be applied unless some sort of discretization process is applied to structure it. One promising line of research aims to treat point clouds as graphs and apply convolutions over them [109] [110] [111]. This has the advantage of preserving spatial cues in every dimension without quantizing data. \u2022 Context knowledge: while FCNs are a consolidated approach for semantic segmentation, they lack several features such as context modelling that help increasing accuracy. The reformulation of CRFs as RNNs to create end-to-end solutions seems to be a promising direction to improve results on real-life data. Multi-scale and feature fusion approaches have also shown remarkable progress. In general, all those works represent important steps towards achieving the ultimate goal, but there are some problems that still require more research. \u2022 Real-time segmentation: In many applications, precision is important; however, it is also crucial that these implementations are able to cope with common camera frame rates (at least 25 frames per second). Most of the current methods are far from that framerate, e.g., FCN-8s takes roughly 100 ms to process a lowresolution PASCAL VOC image whilst CRFasRNN needs more than 500 ms. Therefore, during the next years, we expect a stream of works coming out, focusing more on real-time constraints. These future works will have to find a trade-off between accuracy and runtime.\n20\n\u2022 Memory: some platforms are bounded by hard memory constraints. Segmentation networks usually do need significant amounts of memory to be executed for both inference and training. In order to fit them in some devices, networks must be simplified. While this can be easily accomplished by reducing their complexity (often trading it for accuracy), another approaches can be taken. Pruning is a promising research line that aims to simplify a network, making it lightweight while keeping the knowledge, and thus the accuracy, of the original network architecture [112] [113] [114]. \u2022 Temporal coherency on sequences: some methods have addressed video or sequence segmentation but either taking advantage of that temporal cues to increase accuracy or efficiency. However, none of them have explicitly tackled the coherency problem. For a segmentation system to work on video streams it is important, not only to produce good results frame by frame, but also make them coherent through the whole clip without producing artifacts by smoothing predicted per-pixel labels along the sequence. \u2022 Multi-view integration: Use of multiple views in recently proposed segmentation works is mostly limited to RGB-D cameras and in particular focused on single-object segmentation."}, {"heading": "6 CONCLUSION", "text": "To the best of our knowledge, this is the first review paper in the literature which focuses on semantic segmentation using deep learning. In comparison with other surveys, this paper is devoted to such a rising topic as deep learning, covering the most advanced and recent work on that front. We formulated the semantic segmentation problem and provided the reader with the necessary background knowledge about deep learning for such task. We covered the contemporary literature of datasets and methods, providing a comprehensive survey of 28 datasets and 27 methods. Datasets were carefully described, stating their purposes and characteristics so that researchers can easily pick the one that best suits their needs. Methods were surveyed from two perspectives: contributions and raw results, i.e., accuracy. We also presented a comparative summary of the datasets and methods in tabular forms, classifying them according to various criteria. In the end, we discussed the results and provided useful insight in shape of future research directions and open problems in the field. In conclusion, semantic segmentation has been approached with many success stories but still remains an open problem whose solution would prove really useful for a wide set of realworld applications. Furthermore, deep learning has proved to be extremely powerful to tackle this problem so we can expect a flurry of innovation and spawns of research lines in the upcoming years."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work has been funded by the Spanish Government TIN2016-76515-R grant for the COMBAHO project, supported with Feder funds. It has also been supported by\na Spanish national grant for PhD studies FPU15/04516. In addition, it was also funded by the grant Ayudas para Estudios de Ma\u0301ster e Iniciacio\u0301n a la Investigacio\u0301n from the University of Alicante."}], "references": [{"title": "Segmentationbased urban traffic scene understanding.", "author": ["A. Ess", "T. M\u00fcller", "H. Grabner", "L.J. Van Gool"], "venue": "in BMVC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition, June 2012, pp. 3354\u20133361.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3213\u20133223.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Hands deep in deep learning for hand pose estimation", "author": ["M. Oberweger", "P. Wohlhart", "V. Lepetit"], "venue": "arXiv preprint arXiv:1502.06807, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a deep convolutional network for light-field image superresolution", "author": ["Y. Yoon", "H.-G. Jeon", "D. Yoo", "J.-Y. Lee", "I. So Kweon"], "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, 2015, pp. 24\u201332.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for content-based image retrieval: A comprehensive study", "author": ["J. Wan", "D. Wang", "S.C.H. Hoi", "P. Wu", "J. Zhu", "Y. Zhang", "J. Li"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 157\u2013166.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward automatic phenotyping of developing embryos from videos", "author": ["F. Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P.E. Barbano"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 9, pp. 1360\u20131371, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep neural networks segment neuronal membranes in electron microscopy images", "author": ["D. Ciresan", "A. Giusti", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2012, pp. 2843\u20132851.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1915\u2013 1929, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1915}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 297\u2013312.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning rich features from rgb-d images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbel\u00e1ez", "J. Malik"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 345\u2013360.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond pixels: A comprehensive survey from bottom-up to semantic image segmentation and cosegmentation", "author": ["H. Zhu", "F. Meng", "J. Cai", "S. Lu"], "venue": "Journal of Visual Communication and Image Representation, vol. 34, pp. 12 \u2013 27, 2016. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S1047320315002035", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A survey of semantic segmentation", "author": ["M. Thoma"], "venue": "CoRR, vol. abs/1602.06541, 2016. [Online]. Available: http://arxiv.org/abs/ 1602.06541", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Multidimensional recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "J. Schmidhuber"], "venue": "CoRR, vol. abs/0705.2011, 2007. [Online]. Available: http://arxiv.org/ abs/0705.2011  21", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Renet: A recurrent neural network based alternative to convolutional networks", "author": ["F. Visin", "K. Kastner", "K. Cho", "M. Matteucci", "A.C. Courville", "Y. Bengio"], "venue": "CoRR, vol. abs/1505.00393, 2015. [Online]. Available: http://arxiv.org/abs/1505.00393", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks", "author": ["A. Ahmed", "K. Yu", "W. Xu", "Y. Gong", "E. Xing"], "venue": "European Conference on Computer Vision. Springer, 2008, pp. 69\u201382.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 1717\u20131724.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "How transferable are features in deep neural networks?", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 248\u2013255.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding data augmentation for classification: when to warp?", "author": ["S.C. Wong", "A. Gatt", "V. Stamatescu", "M.D. McDonnell"], "venue": "CoRR, vol. abs/1609.08764,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Automatic portrait segmentation for image stylization", "author": ["X. Shen", "A. Hertzmann", "J. Jia", "S. Paris", "B. Price", "E. Shechtman", "I. Sachs"], "venue": "Computer Graphics Forum, vol. 35, no. 2. Wiley Online Library, 2016, pp. 93\u2013102.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, vol. 111, no. 1, pp. 98\u2013136, Jan. 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Detect what you can: Detecting and representing objects using holistic models and body parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "2011 International Conference on Computer Vision. IEEE, 2011, pp. 991\u2013998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 740\u2013755.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes", "author": ["G. Ros", "L. Sellart", "J. Materzynska", "D. Vazquez", "A.M. Lopez"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3234\u20133243.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "The cityscapes dataset", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Scharw\u00e4chter", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR Workshop on The Future of Datasets in Vision, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic object classes in video: A high-definition ground truth database", "author": ["G.J. Brostow", "J. Fauqueur", "R. Cipolla"], "venue": "Pattern Recognition Letters, vol. 30, no. 2, pp. 88\u201397, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Combining appearance and structure from motion features for road scene understanding", "author": ["P. Sturgess", "K. Alahari", "L. Ladicky", "P.H. Torr"], "venue": "BMVC 2012-23rd British Machine Vision Conference. BMVA, 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Road scene segmentation from a single image", "author": ["J.M. Alvarez", "T. Gevers", "Y. LeCun", "A.M. Lopez"], "venue": "European Conference on Computer Vision. Springer, 2012, pp. 376\u2013389.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised image transformation for outdoor semantic labelling", "author": ["G. Ros", "J.M. Alvarez"], "venue": "Intelligent Vehicles Symposium (IV), 2015 IEEE. IEEE, 2015, pp. 537\u2013542.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Vision-based offline-online perception paradigm for autonomous driving", "author": ["G. Ros", "S. Ramos", "M. Granados", "A. Bakhtiary", "D. Vazquez", "A.M. Lopez"], "venue": "Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on. IEEE, 2015, pp. 231\u2013 238.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Sensor fusion for semantic segmentation of urban scenes", "author": ["R. Zhang", "S.A. Candra", "K. Vetter", "A. Zakhor"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 2015, pp. 1850\u20131857.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["S. Gould", "R. Fulton", "D. Koller"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 1\u20138.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonparametric scene parsing: Label transfer via dense scene alignment", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 1972\u20131979.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervoxel-consistent foreground propagation in video", "author": ["S.D. Jain", "K. Grauman"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 656\u2013671.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Material recognition in the wild with the materials in context database", "author": ["S. Bell", "P. Upchurch", "N. Snavely", "K. Bala"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3479\u20133487.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A benchmark dataset and evaluation methodology for video object segmentation", "author": ["F. Perazzi", "J. Pont-Tuset", "B. McWilliams", "L. Van Gool", "M. Gross", "A. Sorkine-Hornung"], "venue": "Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "The 2017 davis challenge on video object segmentation", "author": ["J. Pont-Tuset", "F. Perazzi", "S. Caelles", "P. Arbel\u00e1ez", "A. Sorkine- Hornung", "L. Van Gool"], "venue": "arXiv:1704.00675, 2017.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2017}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "European Conference on Computer Vision. Springer, 2012, pp. 746\u2013760.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "Sun3d: A database of big spaces reconstructed using sfm and object labels", "author": ["J. Xiao", "A. Owens", "A. Torralba"], "venue": "2013 IEEE International Conference on Computer Vision, Dec 2013, pp. 1625\u2013 1632.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Sun rgb-d: A rgb-d scene understanding benchmark suite", "author": ["S. Song", "S.P. Lichtenberg", "J. Xiao"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 567\u2013 576.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "A large-scale hierarchical multi-view rgb-d object dataset", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "Robotics and Automation (ICRA), 2011 IEEE International Conference on. IEEE, 2011, pp. 1817\u20131824.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "A scalable active framework for region annotation in 3d shape collections", "author": ["L. Yi", "V.G. Kim", "D. Ceylan", "I.-C. Shen", "M. Yan", "H. Su", "C. Lu", "Q. Huang", "A. Sheffer", "L. Guibas"], "venue": "SIGGRAPH Asia, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint 2D-3D- Semantic Data for Indoor Scene Understanding", "author": ["I. Armeni", "A. Sax", "A.R. Zamir", "S. Savarese"], "venue": "ArXiv e-prints, Feb. 2017.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2017}, {"title": "A benchmark for 3D mesh segmentation", "author": ["X. Chen", "A. Golovinskiy", "T. Funkhouser"], "venue": "ACM Transactions on Graphics (Proc. SIGGRAPH), vol. 28, no. 3, Aug. 2009.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "An occlusionaware feature for range images", "author": ["A. Quadros", "J. Underwood", "B. Douillard"], "venue": "Robotics and Automation, 2012. ICRA\u201912. IEEE International Conference on. IEEE, May 14-18 2012.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Contour detection in unstructured 3d point clouds", "author": ["T. Hackel", "J.D. Wegner", "K. Schindler"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1610\u20131618.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Segmentation and recognition using structure from motion point clouds", "author": ["G.J. Brostow", "J. Shotton", "J. Fauqueur", "R. Cipolla"], "venue": "European Conference on Computer Vision. Springer, 2008, pp. 44\u201357.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2008}, {"title": "Vision meets robotics: The kitti dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "The International Journal of Robotics Research, vol. 32, no. 11, pp. 1231\u20131237, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning object class detectors from weakly annotated video", "author": ["A. Prest", "C. Leistner", "J. Civera", "C. Schmid", "V. Ferrari"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 3282\u20133289.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "OpenSurfaces: A richly annotated catalog of surface appearance", "author": ["S. Bell", "P. Upchurch", "N. Snavely", "K. Bala"], "venue": "ACM Trans. on Graphics (SIGGRAPH), vol. 32, no. 4, 2013.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Labelme: a database and web-based tool for image annotation", "author": ["B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "International journal of computer vision, vol. 77, no. 1, pp. 157\u2013173, 2008.  22", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2008}, {"title": "Perceptual organization and recognition of indoor scenes from rgb-d images", "author": ["S. Gupta", "P. Arbelaez", "J. Malik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 564\u2013571.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "A Category-Level 3D Object Dataset: Putting the Kinect to Work", "author": ["A. Janoch", "S. Karayev", "Y. Jia", "J.T. Barron", "M. Fritz", "K. Saenko", "T. Darrell"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "The object segmentation database (osd)", "author": ["A. Richtsfeld"], "venue": "2012.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2012}, {"title": "Shapenet: An information-rich 3d model repository", "author": ["A.X. Chang", "T. Funkhouser", "L. Guibas", "P. Hanrahan", "Q. Huang", "Z. Li", "S. Savarese", "M. Savva", "S. Song", "H. Su"], "venue": "arXiv preprint arXiv:1512.03012, 2015.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "3d semantic parsing of large-scale indoor spaces", "author": ["I. Armeni", "O. Sener", "A.R. Zamir", "H. Jiang", "I. Brilakis", "M. Fischer", "S. Savarese"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1534\u20131543.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431\u20133440.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2015}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561, 2015.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding", "author": ["A. Kendall", "V. Badrinarayanan", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.02680, 2015.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1412.7062, 2014.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["\u2014\u2014"], "venue": "arXiv preprint arXiv:1606.00915, 2016.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1529\u20131537.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122, 2015.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2015}, {"title": "Enet: A deep neural network architecture for real-time semantic segmentation", "author": ["A. Paszke", "A. Chaurasia", "S. Kim", "E. Culurciello"], "venue": "arXiv preprint arXiv:1606.02147, 2016.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-scale convolutional architecture for semantic segmentation", "author": ["A. Raj", "D. Maturana", "S. Scherer"], "venue": "2015.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2650\u20132658.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-scale cnn for affordance segmentation in rgb images", "author": ["A. Roy", "S. Todorovic"], "venue": "European Conference on Computer Vision. Springer, 2016, pp. 186\u2013201.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiscale fully convolutional network with application to industrial inspection", "author": ["X. Bian", "S.N. Lim", "N. Zhou"], "venue": "Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on. IEEE, 2016, pp. 1\u20138.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2016}, {"title": "Parsenet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "arXiv preprint arXiv:1506.04579, 2015.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2015}, {"title": "Reseg: A recurrent neural network-based model for semantic segmentation", "author": ["F. Visin", "M. Ciccone", "A. Romero", "K. Kastner", "K. Cho", "Y. Bengio", "M. Matteucci", "A. Courville"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2016.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2016}, {"title": "LSTM-CF: Unifying Context Modeling and Fusion with LSTMs for RGB-D Scene Labeling", "author": ["Z. Li", "Y. Gan", "X. Liang", "Y. Yu", "H. Cheng", "L. Lin"], "venue": "Cham: Springer International Publishing,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2016}, {"title": "Scene labeling with lstm recurrent neural networks", "author": ["W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3547\u20133555.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural networks for scene labeling.", "author": ["P.H. Pinheiro", "R. Collobert"], "venue": "in ICML,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2014}, {"title": "Dag-recurrent neural networks for scene labeling", "author": ["B. Shuai", "Z. Zuo", "G. Wang", "B. Wang"], "venue": "CoRR, vol. abs/1509.00552, 2015. [Online]. Available: http://arxiv.org/abs/1509.00552", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to segment object candidates", "author": ["P.O. Pinheiro", "R. Collobert", "P. Dollar"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1990\u20131998.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to refine object segments", "author": ["P.O. Pinheiro", "T.-Y. Lin", "R. Collobert", "P. Doll\u00e1r"], "venue": "European Conference on Computer Vision. Springer, 2016, pp. 75\u201391.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2016}, {"title": "A multipath network for object detection", "author": ["S. Zagoruyko", "A. Lerer", "T.-Y. Lin", "P.O. Pinheiro", "S. Gross", "S. Chintala", "P. Doll\u00e1r"], "venue": "arXiv preprint arXiv:1604.02135, 2016.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2016}, {"title": "Point cloud labeling using 3d convolutional neural network", "author": ["J. Huang", "S. You"], "venue": "Proc. of the International Conf. on Pattern Recognition (ICPR), vol. 2, 2016.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2016}, {"title": "Pointnet: Deep learning on point sets for 3d classification and segmentation", "author": ["C.R. Qi", "H. Su", "K. Mo", "L.J. Guibas"], "venue": "arXiv preprint arXiv:1612.00593, 2016.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2016}, {"title": "Clockwork convnets for video semantic segmentation", "author": ["E. Shelhamer", "K. Rakelly", "J. Hoffman", "T. Darrell"], "venue": "Computer Vision\u2013 ECCV 2016 Workshops. Springer, 2016, pp. 852\u2013868.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep end2end voxel2voxel prediction", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2016, pp. 17\u201324.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 2018\u20132025.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European conference on computer vision. Springer, 2014, pp. 818\u2013833.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2014}, {"title": "Grabcut: Interactive foreground extraction using iterated graph cuts", "author": ["C. Rother", "V. Kolmogorov", "A. Blake"], "venue": "ACM transactions on graphics (TOG), vol. 23, no. 3. ACM, 2004, pp. 309\u2013314.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2004}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "International Journal of Computer Vision, vol. 81, no. 1, pp. 2\u201323, 2009.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["V. Koltun"], "venue": "Adv. Neural Inf. Process. Syst, vol. 2, no. 3, p. 4, 2011.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter learning and convergent inference for dense random fields.", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": null, "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2013}, {"title": "Exploiting local structures with the kronecker layer in convolutional networks", "author": ["S. Zhou", "J.-N. Wu", "Y. Wu", "X. Zhou"], "venue": "arXiv preprint arXiv:1512.09194, 2015.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2014}, {"title": "RGB-D scene labeling with long short-term memorized fusion model", "author": ["Z. Li", "Y. Gan", "X. Liang", "Y. Yu", "H. Cheng", "L. Lin"], "venue": "CoRR, vol. abs/1604.05000, 2016. [Online]. Available: http://arxiv.org/abs/1604.05000", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep contrast learning for salient object detection", "author": ["G. Li", "Y. Yu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 478\u2013487.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiscale combinatorial grouping", "author": ["P. Arbel\u00e1ez", "J. Pont-Tuset", "J.T. Barron", "F. Marques", "J. Malik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 328\u2013335.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580\u2013587.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge", "author": ["A. Zeng", "K. Yu", "S. Song", "D. Suo", "E.W. Jr.", "A. Rodriguez", "J. Xiao"], "venue": "CoRR, vol. abs/1609.09475, 2016. [Online]. Available: http: //arxiv.org/abs/1609.09475", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-view deep learning for consistent semantic mapping with rgb-d cameras", "author": ["L. Ma", "J. Stuckler", "C. Kerl", "D. Cremers"], "venue": "arXiv:1703.08866, Mar 2017.  23", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2017}, {"title": "Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture", "author": ["C. Hazirbas", "L. Ma", "C. Domokos", "D. Cremers"], "venue": "Proc. ACCV, vol. 2, 2016.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative feature learning for video semantic segmentation", "author": ["H. Zhang", "K. Jiang", "Y. Zhang", "Q. Li", "C. Xia", "X. Chen"], "venue": "Virtual Reality and Visualization (ICVRV), 2014 International Conference on. IEEE, 2014, pp. 321\u2013326.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on pattern analysis and machine intelligence, vol. 23, no. 11, pp. 1222\u20131239, 2001.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 4489\u20134497.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv preprint arXiv:1506.05163, 2015.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "arXiv preprint arXiv:1609.02907, 2016.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning convolutional neural networks for graphs", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "Proceedings of the 33rd annual international conference on machine learning. ACM, 2016.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2016}, {"title": "Structured pruning of deep convolutional neural networks", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": "arXiv preprint arXiv:1512.08571, 2015.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "arXiv preprint arXiv:1510.00149, 2015.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "Some of those applications include autonomous driving [1] [2] [3], human-machine interaction [4], computational photography [5], image search engines [6], and augmented reality to name a few.", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 264, "endOffset": 267}, {"referenceID": 7, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 268, "endOffset": 271}, {"referenceID": 8, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 272, "endOffset": 275}, {"referenceID": 9, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 276, "endOffset": 280}, {"referenceID": 10, "context": "Despite the popularity of those kind of methods, the deep learning revolution has turned the tables so that many computer vision problems \u2013 semantic segmentation among them \u2013 are being tackled using deep architectures, usually Convolutional Neural Networks (CNNs) [7] [8] [9] [10] [11], which are surpassing other approaches by a large margin in terms of accuracy and sometimes even efficiency.", "startOffset": 281, "endOffset": 285}, {"referenceID": 11, "context": "[12] and Thoma [13], which do a great work summarizing and classifying existing methods, discussing datasets and metrics, and providing design choices for future research directions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] and Thoma [13], which do a great work summarizing and classifying existing methods, discussing datasets and metrics, and providing design choices for future research directions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "[14] was relatively simple.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Figure reproduced from [14].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "They proposed various models and configurations of deep CNNs [15], one of them was submitted to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)-2013.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "[16] which won the ILSVRC-2014 challenge with a TOP-5 test accuracy of 93.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Figure reproduced from [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Microsoft\u2019s ResNet [17] is specially remarkable thanks to winning ILSVRC-2016 with 96.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "Figure reproduced from [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "[18] proposed a Multi-dimensional Recurrent Neural Network (MDRNN) architecture which replaces each single recurrent connection from standard RNNs with d connections, where d is the number of spatio-temporal data dimensions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposed ReNet architecture in which instead of multidimensional RNNs, they have been using usual sequence RNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Extracted from [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "Even if a dataset large enough is available and convergence does not take that long, it is often helpful to start with pre-trained weights instead of random initialized ones [20] [21].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "Even if a dataset large enough is available and convergence does not take that long, it is often helpful to start with pre-trained weights instead of random initialized ones [20] [21].", "startOffset": 179, "endOffset": 183}, {"referenceID": 21, "context": "[22] proved that transferring features even from distant tasks can be better than using random initialization, taking into account that the transferability of features decreases as the difference between the pre-trained task and the target one increases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Due to the inherent difficulty of gathering and creating per-pixel labelled segmentation datasets, their scale is not as large as the size of classification datasets such as ImageNet [23] [24].", "startOffset": 183, "endOffset": 187}, {"referenceID": 23, "context": "Due to the inherent difficulty of gathering and creating per-pixel labelled segmentation datasets, their scale is not as large as the size of classification datasets such as ImageNet [23] [24].", "startOffset": 188, "endOffset": 192}, {"referenceID": 24, "context": "Data augmentation is a common technique that has been proven to benefit the training of machine learning models in general and deep architectures in particular; either speeding up convergence or acting as a regularizer, thus avoiding overfitting and increasing generalization capabilities [25].", "startOffset": 289, "endOffset": 293}, {"referenceID": 25, "context": "For instance, in [26], a dataset of 1500 portrait images is augmented synthesizing four new scales (0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "\u2022 PASCAL Visual Object Classes (VOC) [27]1: this challenge consists of a ground-truth annotated dataset of images and five different competitions: classification, detection, segmentation, action classification, and person layout.", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "\u2022 PASCAL Context [28]3: this dataset is an extension of the PASCAL VOC 2010 detection challenge which contains pixel-wise labels for all training images (10103).", "startOffset": 17, "endOffset": 21}, {"referenceID": 28, "context": "\u2022 PASCAL Part [29]4: this database is an extension of the PASCAL VOC 2010 detection challenge which goes beyond that task to provide per-pixel segmentation masks for each part of the objects (or at least silhouette annotation if the object does not have a", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "\u2022 Semantic Boundaries Dataset (SBD) [30]5: this dataset is an extended version of the aforementioned PASCAL VOC which provides semantic segmentation ground truth for those images that were not labelled in VOC.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "\u2022 Microsoft Common Objects in Context (COCO) [31]6: is another image recognition, segmentation, and captioning large-scale dataset.", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "\u2022 SYNTHetic Collection of Imagery and Annotations (SYNTHIA) [32]8: is a large-scale collection of photorealistic renderings of a virtual city, semantically segmented, whose purpose is scene understanding in the context of driving or urban scenarios.", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "Name and Reference Purpose Year Classes Data Resolution Sequence Synthetic/Real Samples (training) Samples (validation) Samples (test) PASCAL VOC 2012 Segmentation [27] Generic 2012 21 2D Variable 7 R 1464 1449 Private PASCAL-Context [28] Generic 2014 540 (59) 2D Variable 7 R 10103 N/A 9637", "startOffset": 164, "endOffset": 168}, {"referenceID": 27, "context": "Name and Reference Purpose Year Classes Data Resolution Sequence Synthetic/Real Samples (training) Samples (validation) Samples (test) PASCAL VOC 2012 Segmentation [27] Generic 2012 21 2D Variable 7 R 1464 1449 Private PASCAL-Context [28] Generic 2014 540 (59) 2D Variable 7 R 10103 N/A 9637", "startOffset": 234, "endOffset": 238}, {"referenceID": 28, "context": "PASCAL-Part [29] Generic-Part 2014 20 2D Variable 7 R 10103 N/A 9637", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "SBD [30] Generic 2011 21 2D Variable 7 R 8498 2857 N/A Microsoft COCO [31] Generic 2014 +80 2D Variable 7 R 82783 40504 81434", "startOffset": 4, "endOffset": 8}, {"referenceID": 30, "context": "SBD [30] Generic 2011 21 2D Variable 7 R 8498 2857 N/A Microsoft COCO [31] Generic 2014 +80 2D Variable 7 R 82783 40504 81434", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "SYNTHIA [32] Urban (Driving) 2016 11 2D 960\u00d7 720 7 S 13407 N/A N/A Cityscapes (fine) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 2975 500 1525", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "SYNTHIA [32] Urban (Driving) 2016 11 2D 960\u00d7 720 7 S 13407 N/A N/A Cityscapes (fine) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 2975 500 1525", "startOffset": 85, "endOffset": 89}, {"referenceID": 32, "context": "Cityscapes (coarse) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 22973 500 N/A CamVid [34] Urban (Driving) 2009 32 2D 960\u00d7 720 3 R 701 N/A N/A CamVid-Sturgess [35] Urban (Driving) 2009 11 2D 960\u00d7 720 3 R 367 100 233", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "Cityscapes (coarse) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 22973 500 N/A CamVid [34] Urban (Driving) 2009 32 2D 960\u00d7 720 3 R 701 N/A N/A CamVid-Sturgess [35] Urban (Driving) 2009 11 2D 960\u00d7 720 3 R 367 100 233", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "Cityscapes (coarse) [33] Urban 2015 30 (8) 2D 2048\u00d7 1024 3 R 22973 500 N/A CamVid [34] Urban (Driving) 2009 32 2D 960\u00d7 720 3 R 701 N/A N/A CamVid-Sturgess [35] Urban (Driving) 2009 11 2D 960\u00d7 720 3 R 367 100 233", "startOffset": 155, "endOffset": 159}, {"referenceID": 35, "context": "KITTI-Layout [36] [37] Urban/Driving 2012 3 2D Variable 7 R 323 N/A N/A KITTI-Ros [38] Urban/Driving 2015 11 2D Variable 7 R 170 N/A 46", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "KITTI-Layout [36] [37] Urban/Driving 2012 3 2D Variable 7 R 323 N/A N/A KITTI-Ros [38] Urban/Driving 2015 11 2D Variable 7 R 170 N/A 46", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "KITTI-Layout [36] [37] Urban/Driving 2012 3 2D Variable 7 R 323 N/A N/A KITTI-Ros [38] Urban/Driving 2015 11 2D Variable 7 R 170 N/A 46", "startOffset": 82, "endOffset": 86}, {"referenceID": 38, "context": "KITTI-Zhang [39] Urban/Driving 2015 10 2D/3D 1226\u00d7 370 7 R 140 N/A 112", "startOffset": 12, "endOffset": 16}, {"referenceID": 39, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 20, "endOffset": 24}, {"referenceID": 40, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 77, "endOffset": 81}, {"referenceID": 41, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 148, "endOffset": 152}, {"referenceID": 25, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 229, "endOffset": 233}, {"referenceID": 42, "context": "Stanford background [40] Outdoor 2009 8 2D 320\u00d7 240 7 R 725 N/A N/A SiftFlow [41] Outdoor 2011 33 2D 256\u00d7 256 7 R 2688 N/A N/A Youtube-Objects-Jain [42] Objects 2014 10 2D 480\u00d7 360 3 R 10167 N/A N/A Adobe\u2019s Portrait Segmentation [26] Portrait 2016 2 2D 600\u00d7 800 7 R 1500 300 N/A MINC [43] Materials 2015 23 2D Variable 7 R 7061 2500 5000", "startOffset": 284, "endOffset": 288}, {"referenceID": 43, "context": "DAVIS [44] [45] Generic 2016 4 2D 480p 3 R 4219 2023 2180", "startOffset": 6, "endOffset": 10}, {"referenceID": 44, "context": "DAVIS [44] [45] Generic 2016 4 2D 480p 3 R 4219 2023 2180", "startOffset": 11, "endOffset": 15}, {"referenceID": 45, "context": "NYUDv2 [46] Indoor 2012 40 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 46, "context": "5D 480\u00d7 640 7 R 795 654 N/A SUN3D [47] Indoor 2013 \u2013 2.", "startOffset": 34, "endOffset": 38}, {"referenceID": 47, "context": "5D 640\u00d7 480 3 R 19640 N/A N/A SUNRGBD [48] Indoor 2015 37 2.", "startOffset": 38, "endOffset": 42}, {"referenceID": 48, "context": "RGB-D Object Dataset [49] Household objects 2011 51 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 49, "context": "5D 640\u00d7 480 3 R 207920 N/A N/A ShapeNet Part [50] Object/Part 2016 16/50 3D N/A 7 S 31, 963 N/A N/A Stanford 2D-3D-S [51] Indoor 2017 13 2D/2.", "startOffset": 45, "endOffset": 49}, {"referenceID": 50, "context": "5D 640\u00d7 480 3 R 207920 N/A N/A ShapeNet Part [50] Object/Part 2016 16/50 3D N/A 7 S 31, 963 N/A N/A Stanford 2D-3D-S [51] Indoor 2017 13 2D/2.", "startOffset": 117, "endOffset": 121}, {"referenceID": 51, "context": "5D/3D 1080\u00d7 1080 3 R 70469 N/A N/A 3D Mesh [52] Object/Part 2009 19 3D N/A 7 S 380 N/A N/A Sydney Urban Objects Dataset [53] Urban (Objects) 2013 26 3D N/A 7 R 41 N/A N/A Large-Scale Point Cloud Classification Benchmark [54] Urban/Nature 2016 8 3D N/A 7 R 15 N/A 15", "startOffset": 43, "endOffset": 47}, {"referenceID": 52, "context": "5D/3D 1080\u00d7 1080 3 R 70469 N/A N/A 3D Mesh [52] Object/Part 2009 19 3D N/A 7 S 380 N/A N/A Sydney Urban Objects Dataset [53] Urban (Objects) 2013 26 3D N/A 7 R 41 N/A N/A Large-Scale Point Cloud Classification Benchmark [54] Urban/Nature 2016 8 3D N/A 7 R 15 N/A 15", "startOffset": 120, "endOffset": 124}, {"referenceID": 53, "context": "5D/3D 1080\u00d7 1080 3 R 70469 N/A N/A 3D Mesh [52] Object/Part 2009 19 3D N/A 7 S 380 N/A N/A Sydney Urban Objects Dataset [53] Urban (Objects) 2013 26 3D N/A 7 R 41 N/A N/A Large-Scale Point Cloud Classification Benchmark [54] Urban/Nature 2016 8 3D N/A 7 R 15 N/A 15", "startOffset": 220, "endOffset": 224}, {"referenceID": 32, "context": "\u2022 Cityscapes [33]9: is a large-scale database which focuses on semantic understanding of urban street scenes.", "startOffset": 13, "endOffset": 17}, {"referenceID": 54, "context": "\u2022 CamVid [55] [34]10: is a road/driving scene understanding database which was originally captured as five video sequences with a 960 \u00d7 720 resolution camera mounted on the dashboard of a car.", "startOffset": 9, "endOffset": 13}, {"referenceID": 33, "context": "\u2022 CamVid [55] [34]10: is a road/driving scene understanding database which was originally captured as five video sequences with a 960 \u00d7 720 resolution camera mounted on the dashboard of a car.", "startOffset": 14, "endOffset": 18}, {"referenceID": 34, "context": "[35] which divided the dataset into 367/100/233 training, validation, and testing images respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "\u2022 KITTI [56]: is one of the most popular datasets for use in mobile robotics and autonomous driv-", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": "[36] [37] generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[36] [37] generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky.", "startOffset": 5, "endOffset": 9}, {"referenceID": 38, "context": "[39] annotated 252 (140 for training and 112 for testing) acquisitions \u2013 RGB and Velodyne scans \u2013 from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "\u2022 Youtube-Objects [57] is a database of videos collected from YouTube which contain objects from ten PASCAL VOC classes: aeroplane, bird, boat, car, cat, cow, dog, horse, motorbike, and train.", "startOffset": 18, "endOffset": 22}, {"referenceID": 41, "context": "[42] manually annotated a subset of 126 sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "\u2022 Adobe\u2019s Portrait Segmentation [26]11: this is a dataset of 800 \u00d7 600 pixels portrait images collected from Flickr, mainly captured with mobile frontfacing cameras.", "startOffset": 32, "endOffset": 36}, {"referenceID": 42, "context": "\u2022 Materials in Context (MINC) [43]: this work is a dataset for patch material classification and full scene material segmentation.", "startOffset": 30, "endOffset": 34}, {"referenceID": 57, "context": "The main source for these images is the OpenSurfaces dataset [58], which was augmented using other sources of imagery such as Flickr or Houzz.", "startOffset": 61, "endOffset": 65}, {"referenceID": 43, "context": "\u2022 Densely-Annotated VIdeo Segmentation (DAVIS) [44] [45]12: this challenge is purposed for video object segmentation.", "startOffset": 47, "endOffset": 51}, {"referenceID": 44, "context": "\u2022 Densely-Annotated VIdeo Segmentation (DAVIS) [44] [45]12: this challenge is purposed for video object segmentation.", "startOffset": 52, "endOffset": 56}, {"referenceID": 39, "context": "\u2022 Stanford background [40]13: dataset with outdoor scene images imported from existing public datasets: LabelMe, MSRC, PASCAL VOC and Geometric Context.", "startOffset": 22, "endOffset": 26}, {"referenceID": 40, "context": "\u2022 SiftFlow [41]: contains 2688 fully annotated images which are a subset of the LabelMe database [59].", "startOffset": 11, "endOffset": 15}, {"referenceID": 58, "context": "\u2022 SiftFlow [41]: contains 2688 fully annotated images which are a subset of the LabelMe database [59].", "startOffset": 97, "endOffset": 101}, {"referenceID": 45, "context": "gz \u2022 NYUDv2 [46]14: this database consists of 1449 indoor RGB-D images captured with a Microsoft Kinect device.", "startOffset": 12, "endOffset": 16}, {"referenceID": 59, "context": "[60] for both training (795 images) and testing (654) splits.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "\u2022 SUN3D [47]15: similar to the NYUDv2, this dataset contains a large-scale RGB-D video database, with 8 annotated sequences.", "startOffset": 8, "endOffset": 12}, {"referenceID": 47, "context": "\u2022 SUNRGBD [48]16: captured with four RGB-D sensors, this dataset contains 10000 RGB-D images, at a similar scale as PASCAL VOC.", "startOffset": 10, "endOffset": 14}, {"referenceID": 45, "context": "It contains images from NYU depth v2 [46], Berkeley B3DO [61], and SUN3D [47].", "startOffset": 37, "endOffset": 41}, {"referenceID": 60, "context": "It contains images from NYU depth v2 [46], Berkeley B3DO [61], and SUN3D [47].", "startOffset": 57, "endOffset": 61}, {"referenceID": 46, "context": "It contains images from NYU depth v2 [46], Berkeley B3DO [61], and SUN3D [47].", "startOffset": 73, "endOffset": 77}, {"referenceID": 61, "context": "\u2022 The Object Segmentation Database (OSD) [62]17 this database has been designed for segmenting unknown objects from generic scenes even under partial occlusions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 48, "context": "\u2022 RGB-D Object Dataset [49]18: this dataset is composed by video sequences of 300 common household objects organized in 51 categories arranged using WordNet hypernym-hyponym relationships.", "startOffset": 23, "endOffset": 27}, {"referenceID": 49, "context": "\u2022 ShapeNet Part [50]19: is a subset of the ShapeNet [63] repository which focuses on fine-grained 3D object segmentation.", "startOffset": 16, "endOffset": 20}, {"referenceID": 62, "context": "\u2022 ShapeNet Part [50]19: is a subset of the ShapeNet [63] repository which focuses on fine-grained 3D object segmentation.", "startOffset": 52, "endOffset": 56}, {"referenceID": 50, "context": "\u2022 Stanford 2D-3D-S [51]20: is a multi-modal and largescale indoor spaces dataset extending the Stanford 3D Semantic Parsing work [64].", "startOffset": 19, "endOffset": 23}, {"referenceID": 63, "context": "\u2022 Stanford 2D-3D-S [51]20: is a multi-modal and largescale indoor spaces dataset extending the Stanford 3D Semantic Parsing work [64].", "startOffset": 129, "endOffset": 133}, {"referenceID": 51, "context": "\u2022 A Benchmark for 3D Mesh Segmentation [52]21: this benchmark is composed by 380 meshes classified in 19 categories (human, cup, glasses, airplane, ant, chair, octopus, table, teddy, hand, plier, fish, bird, armadillo, bust, mech, bearing, vase, fourleg).", "startOffset": 39, "endOffset": 43}, {"referenceID": 52, "context": "\u2022 Sydney Urban Objects Dataset [53]22: this dataset contains a variety of common urban road objects scanned with a Velodyne HDK-64E LIDAR.", "startOffset": 31, "endOffset": 35}, {"referenceID": 53, "context": "\u2022 Large-Scale Point Cloud Classification Benchmark [54]23: this benchmark provides manually annotated 3D point clouds of diverse natural and urban scenes: churches, streets, railroad tracks, squares, villages, soccer fields, castles among others.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "The relentless success of deep learning techniques in various high-level computer vision tasks \u2013 in particular, supervised approaches such as Convolutional Neural Networks (CNNs) for image classification or object detection [14] [15] [16] \u2013 motivated researchers to explore the capabilities of such networks for pixel-level labelling problems like semantic segmentation.", "startOffset": 224, "endOffset": 228}, {"referenceID": 14, "context": "The relentless success of deep learning techniques in various high-level computer vision tasks \u2013 in particular, supervised approaches such as Convolutional Neural Networks (CNNs) for image classification or object detection [14] [15] [16] \u2013 motivated researchers to explore the capabilities of such networks for pixel-level labelling problems like semantic segmentation.", "startOffset": 229, "endOffset": 233}, {"referenceID": 15, "context": "The relentless success of deep learning techniques in various high-level computer vision tasks \u2013 in particular, supervised approaches such as Convolutional Neural Networks (CNNs) for image classification or object detection [14] [15] [16] \u2013 motivated researchers to explore the capabilities of such networks for pixel-level labelling problems like semantic segmentation.", "startOffset": 234, "endOffset": 238}, {"referenceID": 64, "context": "[65].", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 159, "endOffset": 163}, {"referenceID": 65, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 210, "endOffset": 214}, {"referenceID": 66, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 285, "endOffset": 289}, {"referenceID": 67, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 346, "endOffset": 350}, {"referenceID": 68, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 351, "endOffset": 355}, {"referenceID": 42, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 439, "endOffset": 443}, {"referenceID": 69, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 514, "endOffset": 518}, {"referenceID": 70, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 580, "endOffset": 584}, {"referenceID": 71, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 638, "endOffset": 642}, {"referenceID": 72, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 733, "endOffset": 737}, {"referenceID": 73, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 817, "endOffset": 821}, {"referenceID": 74, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 903, "endOffset": 907}, {"referenceID": 75, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1010, "endOffset": 1014}, {"referenceID": 76, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1085, "endOffset": 1089}, {"referenceID": 77, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1153, "endOffset": 1157}, {"referenceID": 78, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1242, "endOffset": 1246}, {"referenceID": 79, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1352, "endOffset": 1356}, {"referenceID": 80, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1410, "endOffset": 1414}, {"referenceID": 81, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1487, "endOffset": 1491}, {"referenceID": 9, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1574, "endOffset": 1578}, {"referenceID": 82, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1665, "endOffset": 1669}, {"referenceID": 83, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1745, "endOffset": 1749}, {"referenceID": 84, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1823, "endOffset": 1827}, {"referenceID": 85, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 1930, "endOffset": 1934}, {"referenceID": 86, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 2007, "endOffset": 2011}, {"referenceID": 87, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 2102, "endOffset": 2106}, {"referenceID": 88, "context": "Targets Name and Reference Architecture Accuracy Efficiency Training Instance Sequences Multi-modal 3D Source Code Contribution(s) Fully Convolutional Network [65] VGG-16(FCN) ? ? ? 7 7 7 7 3 Forerunner SegNet [66] VGG-16 + Decoder ? ? ? ?? ? 7 7 7 7 3 Encoder-decoder Bayesian SegNet [67] SegNet ? ? ? ? ? 7 7 7 7 3 Uncertainty modeling DeepLab [68] [69] VGG-16/ResNet-101 ? ? ? ? ? 7 7 7 7 3 Standalone CRF, atrous convolutions MINC-CNN [43] GoogLeNet(FCN) ? ? ? 7 7 7 7 3 Patchwise CNN, Standalone CRF CRFasRNN [70] FCN-8s ? ?? ? ? ? 7 7 7 7 3 CRF reformulated as RNN Dilation [71] VGG-16 ? ? ? ? ? 7 7 7 7 3 Dilated convolutions ENet [72] ENet bottleneck ?? ? ? ? ? 7 7 7 7 3 Bottleneck module for efficiency Multi-scale-CNN-Raj [73] VGG-16(FCN) ? ? ? ? ? 7 7 7 7 7 Multi-scale architecture Multi-scale-CNN-Eigen [74] Custom ? ? ? ? ? 7 7 7 7 3 Multi-scale sequential refinement Multi-scale-CNN-Roy [75] Multi-scale-CNN-Eigen ? ? ? ? ? 7 7 ?? 7 7 Multi-scale coarse-to-fine refinement Multi-scale-CNN-Bian [76] FCN ?? ? ?? 7 7 7 7 7 Independently trained multi-scale FCNs ParseNet [77] VGG-16 ? ? ? ? ? 7 7 7 7 3 Global context feature fusion ReSeg [78] VGG-16 + ReNet ?? ? ? 7 7 7 7 3 Extension of ReNet to semantic segmentation LSTM-CF [79] Fast R-CNN + DeepMask ? ? ? ? ? 7 7 7 7 3 Fusion of contextual information from multiple sources 2D-LSTM [80] MDRNN ?? ?? ? 7 7 7 7 7 Image context modelling rCNN [81] MDRNN ? ? ? ?? ? 7 7 7 7 3 Different input sizes, image context DAG-RNN [82] Elman network ? ? ? ? ? 7 7 7 7 3 Graph image structure for context modelling SDS [10] R-CNN + Box CNN ? ? ? ? ? ?? 7 7 7 3 Simultaneous detection and segmentation DeepMask [83] VGG-A ? ? ? ? ? ?? 7 7 7 3 Proposals generation for segmentation SharpMask [84] DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Top-down refinement module MultiPathNet [85] Fast R-CNN + DeepMask ? ? ? ? ? ? ? ? 7 7 7 3 Multi path information flow through network Huang-3DCNN [86] Own 3DCNN ? ? ? 7 7 7 ? ? ? 7 3DCNN for voxelized point clouds PointNet [87] Own MLP-based ?? ? ? 7 7 7 ? ? ? 3 Segmentation of unordered point sets Clockwork Convnet [88] FCN ?? ?? ? 7 ? ? ? 7 7 3 Clockwork scheduling for sequences 3DCNN-Zhang Own 3DCNN ?? ? ? 7 ? ? ? 7 7 3 3D convolutions and graph cut for sequences End2End Vox2Vox [89] C3D ?? ? ? 7 ? ? ? 7 7 7 3D convolutions/deconvolutions for sequences", "startOffset": 2271, "endOffset": 2275}, {"referenceID": 64, "context": "[65].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores.", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "They transformed those existing and well-known classification models \u2013 AlexNet [14], VGG (16-layer net) [15], GoogLeNet [16], and ResNet [17] \u2013 into fully convolutional ones by replacing the fully connected layers with convolutional ones to output spatial maps instead of classification scores.", "startOffset": 137, "endOffset": 141}, {"referenceID": 89, "context": "Those maps are upsampled using fractionally strided convolutions (also named deconvolutions [90] [91]) to produce dense per-pixel labeled outputs.", "startOffset": 92, "endOffset": 96}, {"referenceID": 90, "context": "Those maps are upsampled using fractionally strided convolutions (also named deconvolutions [90] [91]) to produce dense per-pixel labeled outputs.", "startOffset": 97, "endOffset": 101}, {"referenceID": 65, "context": "SegNet [66] is a clear example of this divergence (see Figure 9).", "startOffset": 7, "endOffset": 11}, {"referenceID": 65, "context": "Figure extracted from [66].", "startOffset": 22, "endOffset": 26}, {"referenceID": 65, "context": "Figure reproduced from [66].", "startOffset": 23, "endOffset": 27}, {"referenceID": 91, "context": "CRFs enable the combination of lowlevel image information \u2013 such as the interactions between pixels [92] [93] \u2013 with the output of multi-class inference systems that produce per-pixel class scores.", "startOffset": 100, "endOffset": 104}, {"referenceID": 92, "context": "CRFs enable the combination of lowlevel image information \u2013 such as the interactions between pixels [92] [93] \u2013 with the output of multi-class inference systems that produce per-pixel class scores.", "startOffset": 105, "endOffset": 109}, {"referenceID": 67, "context": "The DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kr\u00e4henb\u00fchl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result.", "startOffset": 19, "endOffset": 23}, {"referenceID": 68, "context": "The DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kr\u00e4henb\u00fchl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result.", "startOffset": 24, "endOffset": 28}, {"referenceID": 93, "context": "The DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kr\u00e4henb\u00fchl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result.", "startOffset": 99, "endOffset": 103}, {"referenceID": 94, "context": "The DeepLab models [68] [69] make use of the fully connected pairwise CRF by Kr\u00e4henb\u00fchl and Koltun [94] [95] as a separated post-processing step in their pipeline to refine the segmentation result.", "startOffset": 104, "endOffset": 108}, {"referenceID": 42, "context": "[43] makes use of various CNNs trained to identify patches in the MINC database.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "11: CRF refinement per iteration as shown by the authors of DeepLab [68].", "startOffset": 68, "endOffset": 72}, {"referenceID": 69, "context": "[70].", "startOffset": 0, "endOffset": 4}, {"referenceID": 80, "context": "[81] which employed RNNs to model large spatial dependencies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 95, "context": "Dilated convolutions, also named \u00e0-trous convolutions, are a generalization of Kronecker-factored convolutional filters [96] which support exponentially expanding receptive fields without losing resolution.", "startOffset": 120, "endOffset": 124}, {"referenceID": 70, "context": "12: As shown in [71], dilated convolution filters with various dilation rates: (a) 1-dilated convolutions in which each unit has a 3\u00d7 3 receptive fields, (b) 2-dilated ones with 7 \u00d7 7 receptive fields, and (c) 3-dilated convolutions with 15\u00d7 15 receptive fields.", "startOffset": 16, "endOffset": 20}, {"referenceID": 70, "context": "[71], the already mentioned DeepLab (its improved version) [69], and the real-time network ENet [72].", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "[71], the already mentioned DeepLab (its improved version) [69], and the real-time network ENet [72].", "startOffset": 59, "endOffset": 63}, {"referenceID": 71, "context": "[71], the already mentioned DeepLab (its improved version) [69], and the real-time network ENet [72].", "startOffset": 96, "endOffset": 100}, {"referenceID": 72, "context": "[73] propose a multi-scale version of a fully convolutional VGG-16.", "startOffset": 0, "endOffset": 4}, {"referenceID": 74, "context": "[75] take a different approach using a network composed by four multi-scale CNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "[74].", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "[74].", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "Figure extracted from [74].", "startOffset": 22, "endOffset": 26}, {"referenceID": 75, "context": "[76].", "startOffset": 0, "endOffset": 4}, {"referenceID": 76, "context": "This approach is taken by ParseNet [77] in their context module.", "startOffset": 35, "endOffset": 39}, {"referenceID": 83, "context": "Figure extracted from [84].", "startOffset": 22, "endOffset": 26}, {"referenceID": 76, "context": "Figure extracted from [77].", "startOffset": 22, "endOffset": 26}, {"referenceID": 83, "context": "in their SharpMask network [84], which introduced a progressive refinement module to incorporate features from the previous layer to the next in a top-down architecture.", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "[19] proposed an architecture for semantic segmentation called ReSeg [78] represented in Figure 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 77, "context": "[19] proposed an architecture for semantic segmentation called ReSeg [78] represented in Figure 17.", "startOffset": 69, "endOffset": 73}, {"referenceID": 77, "context": "Figure extracted from [78].", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "VGG-16 network [15], feeding the resulting feature maps into one or more ReNet layers for fine-tuning.", "startOffset": 15, "endOffset": 19}, {"referenceID": 96, "context": "Several derived models such as Long Short-Term Memory (LSTM) networks [97] and GRUs [98] are the stateof-art in this field to avoid such problem.", "startOffset": 70, "endOffset": 74}, {"referenceID": 97, "context": "Several derived models such as Long Short-Term Memory (LSTM) networks [97] and GRUs [98] are the stateof-art in this field to avoid such problem.", "startOffset": 84, "endOffset": 88}, {"referenceID": 98, "context": "Inspired on the same ReNet architecture, a novel Long Short-Term Memorized Context Fusion (LSTM-CF) model for scene labeling was proposed by [99].", "startOffset": 141, "endOffset": 145}, {"referenceID": 28, "context": "The RGB pipeline relies on a variant of the DeepLab architecture [29] concatenating features at three different scales to enrich feature representation (inspired by [100]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 99, "context": "The RGB pipeline relies on a variant of the DeepLab architecture [29] concatenating features at three different scales to enrich feature representation (inspired by [100]).", "startOffset": 165, "endOffset": 170}, {"referenceID": 79, "context": "[80] purposed a simple 2D LSTMbased architecture in which the input image is divided into non-overlapping windows which are fed into four separate LSTMs memory blocks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 80, "context": "[81] introduced Recurrent Convolutional Neural Networks (rCNNs) which recurrently train with different input window sizes taking into account previous predictions by using a different input window sizes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 81, "context": "Undirected cyclic graphs (UCGs) were also adopted to model image contexts for semantic segmentation [82].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "[10] proposed a Simultaneous Detection and Segmentation (SDS) method in order to improve performance over already existing works.", "startOffset": 0, "endOffset": 4}, {"referenceID": 100, "context": "Their pipeline uses, firstly, a bottom-up hierarchical image segmentation and object candidate generation process called Multi-scale COmbinatorial Grouping (MCG) [101] to obtain region proposals.", "startOffset": 162, "endOffset": 167}, {"referenceID": 101, "context": "For each region, features are extracted by using an adapted version of the Region-CNN (R-CNN) [102], which is fine-tuned using bounding boxes provided by the MCG method instead of selective search and also alongside region foreground features.", "startOffset": 94, "endOffset": 99}, {"referenceID": 82, "context": "[83] presented DeepMask model, an object proposal approach based on a single ConvNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 83, "context": "Based on the DeepMask architecture as a starting point due to its effectiveness, the same authors presented a novel architecture for object instance segmentation implementing a top-down refinement process [84] and achieving a better performance in terms of accuracy and speed.", "startOffset": 205, "endOffset": 209}, {"referenceID": 82, "context": "Figure extracted from [83].", "startOffset": 22, "endOffset": 26}, {"referenceID": 84, "context": "Another approach, based on Fast R-CNN as a starting point and using DeepMask object proposals instead of Selective Search was presented by Zagoruyko et al [85].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "Different techniques such as Horizontal Height Angle (HHA) [11] are used for encoding the depth into three channels as follows: horizontal disparity, height above ground, and the angle between local surface normal and the inferred gravity direction.", "startOffset": 59, "endOffset": 63}, {"referenceID": 98, "context": "Several works such as [99] are based on this encoding technique.", "startOffset": 22, "endOffset": 26}, {"referenceID": 102, "context": "[103] present an object segmentation approach that leverages multi-view RGB-D data and deep learning techniques.", "startOffset": 0, "endOffset": 5}, {"referenceID": 13, "context": "Moreover, in this work, multiple networks for feature extraction were trained (AlexNet [14] and VGG-16 [15]), evaluating the benefits of using depth information.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "Moreover, in this work, multiple networks for feature extraction were trained (AlexNet [14] and VGG-16 [15]), evaluating the benefits of using depth information.", "startOffset": 103, "endOffset": 107}, {"referenceID": 103, "context": "[104] propose a novel approach for object-class segmentation using a multi-view deep learning technique.", "startOffset": 0, "endOffset": 5}, {"referenceID": 104, "context": "The proposed approach is based on FuseNet [105], which combines RGB and depth images for semantic segmentation, and improves the original work by adding multi-scale loss minimization.", "startOffset": 42, "endOffset": 47}, {"referenceID": 85, "context": "[86] (see Figure 19 take a point cloud and parse it through a dense voxel grid, generating a set of occupancy voxels which are used as input to a 3D CNN to produce one label per voxel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "[86] for semantic labeling of point clouds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "Figure extracted from [86].", "startOffset": 22, "endOffset": 26}, {"referenceID": 86, "context": "PointNet [87] is a pioneering work which presents a deep neural network that takes raw point clouds as input, providing a unified architecture for both classification and segmentation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 87, "context": "[88].", "startOffset": 0, "endOffset": 4}, {"referenceID": 105, "context": "[106] took a different approach and made use of a 3DCNN, which was originally created for learning features from volumes, to learn hierarchical spatio-temporal features from multi-channel inputs such as video clips.", "startOffset": 0, "endOffset": 5}, {"referenceID": 106, "context": "The final segmentation is obtained by applying graph-cut [107] on the supervoxel graph.", "startOffset": 57, "endOffset": 62}, {"referenceID": 88, "context": "[89].", "startOffset": 0, "endOffset": 4}, {"referenceID": 107, "context": "In that work, they make use of the Convolutional 3D (C3D) network introduced by themselves on a previous work [108], and extend it for semantic segmentation by adding deconvolutional layers at the end.", "startOffset": 110, "endOffset": 115}, {"referenceID": 86, "context": "Figure reproduced from [87].", "startOffset": 23, "endOffset": 27}, {"referenceID": 87, "context": "Figure extracted from [88].", "startOffset": 22, "endOffset": 26}, {"referenceID": 87, "context": "[88].", "startOffset": 0, "endOffset": 4}, {"referenceID": 87, "context": "Figure extracted from [88].", "startOffset": 22, "endOffset": 26}, {"referenceID": 68, "context": "# Method Accuracy (IoU) 1 DeepLab [69] 79.", "startOffset": 34, "endOffset": 38}, {"referenceID": 70, "context": "2 Dilation [71] 75.", "startOffset": 11, "endOffset": 15}, {"referenceID": 69, "context": "3 CRFasRNN [70] 74.", "startOffset": 11, "endOffset": 15}, {"referenceID": 76, "context": "4 ParseNet [77] 69.", "startOffset": 11, "endOffset": 15}, {"referenceID": 64, "context": "5 FCN-8s [65] 67.", "startOffset": 9, "endOffset": 13}, {"referenceID": 73, "context": "6 Multi-scale-CNN-Eigen [74] 62.", "startOffset": 24, "endOffset": 28}, {"referenceID": 66, "context": "7 Bayesian SegNet [67] 60.", "startOffset": 18, "endOffset": 22}, {"referenceID": 68, "context": "# Method Accuracy (IoU) 1 DeepLab [69] 45.", "startOffset": 34, "endOffset": 38}, {"referenceID": 69, "context": "2 CRFasRNN [70] 39.", "startOffset": 11, "endOffset": 15}, {"referenceID": 64, "context": "3 FCN-8s [65] 39.", "startOffset": 9, "endOffset": 13}, {"referenceID": 68, "context": "# Method Accuracy (IoU) 1 DeepLab [69] 64.", "startOffset": 34, "endOffset": 38}, {"referenceID": 81, "context": "# Method Accuracy (IoU) 1 DAG-RNN [82] 91.", "startOffset": 34, "endOffset": 38}, {"referenceID": 66, "context": "2 Bayesian SegNet [67] 63.", "startOffset": 18, "endOffset": 22}, {"referenceID": 65, "context": "3 SegNet [66] 60.", "startOffset": 9, "endOffset": 13}, {"referenceID": 77, "context": "4 ReSeg [78] 58.", "startOffset": 8, "endOffset": 12}, {"referenceID": 71, "context": "5 ENet [72] 55.", "startOffset": 7, "endOffset": 11}, {"referenceID": 68, "context": "# Method Accuracy (IoU) 1 DeepLab [69] 70.", "startOffset": 34, "endOffset": 38}, {"referenceID": 70, "context": "2 Dilation10 [71] 67.", "startOffset": 13, "endOffset": 17}, {"referenceID": 64, "context": "3 FCN-8s [65] 65.", "startOffset": 9, "endOffset": 13}, {"referenceID": 69, "context": "4 CRFasRNN [70] 62.", "startOffset": 11, "endOffset": 15}, {"referenceID": 71, "context": "5 ENet [72] 58.", "startOffset": 7, "endOffset": 11}, {"referenceID": 80, "context": "# Method Accuracy (IoU) 1 rCNN [81] 80.", "startOffset": 31, "endOffset": 35}, {"referenceID": 79, "context": "2 2D-LSTM [80] 78.", "startOffset": 10, "endOffset": 14}, {"referenceID": 81, "context": "# Method Accuracy (IoU) 1 DAG-RNN [82] 85.", "startOffset": 34, "endOffset": 38}, {"referenceID": 80, "context": "2 rCNN [81] 77.", "startOffset": 7, "endOffset": 11}, {"referenceID": 79, "context": "3 2D-LSTM [80] 70.", "startOffset": 10, "endOffset": 14}, {"referenceID": 78, "context": "# Method Accuracy (IoU) 1 LSTM-CF [79] 48.", "startOffset": 34, "endOffset": 38}, {"referenceID": 78, "context": "# Method Accuracy (IoU) 1 LSTM-CF [79] 49.", "startOffset": 34, "endOffset": 38}, {"referenceID": 78, "context": "# Method Accuracy (IoU) 1 LSTM-CF [79] 58.", "startOffset": 34, "endOffset": 38}, {"referenceID": 86, "context": "# Method Accuracy (IoU) 1 PointNet [87] 83.", "startOffset": 35, "endOffset": 39}, {"referenceID": 86, "context": "# Method Accuracy (IoU) 1 PointNet [87] 47.", "startOffset": 35, "endOffset": 39}, {"referenceID": 87, "context": "# Method Accuracy (IoU) 1 Clockwork Convnet [88] 64.", "startOffset": 44, "endOffset": 48}, {"referenceID": 87, "context": "# Method Accuracy (IoU) 1 Clockwork Convnet [88] 68.", "startOffset": 44, "endOffset": 48}, {"referenceID": 108, "context": "One promising line of research aims to treat point clouds as graphs and apply convolutions over them [109] [110] [111].", "startOffset": 101, "endOffset": 106}, {"referenceID": 109, "context": "One promising line of research aims to treat point clouds as graphs and apply convolutions over them [109] [110] [111].", "startOffset": 107, "endOffset": 112}, {"referenceID": 110, "context": "One promising line of research aims to treat point clouds as graphs and apply convolutions over them [109] [110] [111].", "startOffset": 113, "endOffset": 118}, {"referenceID": 111, "context": "Pruning is a promising research line that aims to simplify a network, making it lightweight while keeping the knowledge, and thus the accuracy, of the original network architecture [112] [113] [114].", "startOffset": 181, "endOffset": 186}, {"referenceID": 112, "context": "Pruning is a promising research line that aims to simplify a network, making it lightweight while keeping the knowledge, and thus the accuracy, of the original network architecture [112] [113] [114].", "startOffset": 187, "endOffset": 192}], "year": 2017, "abstractText": "Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.", "creator": "LaTeX with hyperref package"}}}