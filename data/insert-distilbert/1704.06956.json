{"id": "1704.06956", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Naturalizing a Programming Language via Interactive Learning", "abstract": "our goal is to create a convenient natural language users interface for not performing well - specified but complex actions such as analyzing data, manipulating text, and querying databases. however, existing natural language interfaces for such tasks are quite primitive compared to producing the power one wields with a programming language. to bridge this gap, we start with a core programming language and allow users to \" naturalize \" the core language, incrementally by adding defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. in a voxel optimization world, we begin show that a community thousands of users can simultaneously teach a common system a diverse language and use it to build hundreds of moderately complex voxel organizational structures. over the course of three days, these users went from using only the core language to using the naturalized language in 85. 9 \\ % of the last 10k utterances.", "histories": [["v1", "Sun, 23 Apr 2017 18:13:10 GMT  (3699kb,D)", "http://arxiv.org/abs/1704.06956v1", "10 pages, ACL2017"]], "COMMENTS": "10 pages, ACL2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.HC cs.LG", "authors": ["sida i wang", "samuel ginn", "percy liang", "christopher d manning"], "accepted": true, "id": "1704.06956"}, "pdf": {"name": "1704.06956.pdf", "metadata": {"source": "CRF", "title": "Naturalizing a Programming Language via Interactive Learning", "authors": ["Sida I. Wang", "Samuel Ginn", "Percy Liang", "Christopher D. Manning"], "emails": ["manning}@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In tasks such as analyzing and plotting data (Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al., 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al., 2017) and robots (Tellex et al., 2011), people need computers to perform well-specified but complex actions. To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for experts because the syntax is uncompromising and all statements have to be precise. Another route is to convert natural language into a formal lan-\nCubes: initial \u2013 select left 6 \u2013 select front 8 \u2013 black 10x10x10 frame \u2013 black 10x10x10 frame \u2013 move front 10 \u2013 red cube size 6 \u2013 move bot 2 \u2013 blue cube size 6 \u2013 green cube size 4 \u2013 (some steps are omitted)\nMonsters, Inc: initial \u2013 move forward \u2013 add green monster \u2013 go down 8 \u2013 go right and front \u2013 add brown floor \u2013 add girl \u2013 go back and down \u2013 add door \u2013 add black column 30 \u2013 go up 9 \u2013 finish door \u2013 (some steps for moving are omitted) Deer: initial \u2013 bird\u2019s eye view \u2013 deer head; up; left 2; back 2; { left antler }; right 2; {right antler} \u2013 down 4; front 2; left 3; deer body; down 6; {deer leg front}; back 7; {deer leg back}; left 4; {deer leg back}; front 7; {deer leg front} \u2013 (some steps omitted)\nFigure 1: Some examples of users building structures using a naturalized language in Voxelurn:\nhttp://www.voxelurn.com\nguage, which has been the subject of work in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011, 2013; Pasupat and Liang, 2015). However, the capability of semantic parsers is still quite primitive compared to the power one wields with a programming language. This gap is increasingly limiting the potential of\nar X\niv :1\n70 4.\n06 95\n6v 1\n[ cs\n.C L\n] 2\n3 A\npr 2\n01 7\nboth text and voice interfaces as they become more ubiquitous and desirable.\nIn this paper, we propose bridging this gap with an interactive language learning process which we call naturalization. Before any learning, we seed a system with a core programming language that is always available to the user. As users instruct the system to perform actions, they augment the language by defining new utterances \u2014 e.g., the user can explicitly tell the computer that \u2018X\u2019 means \u2018Y\u2019. Through this process, users gradually and interactively teach the system to understand the language that they want to use, rather than the core language that they are forced to use initially. While the first users have to learn the core language, later users can make use of everything that is already taught. This process accommodates both users\u2019 preferences and the computer action space, where the final language is both interpretable by the computer and easier to produce by human users.\nCompared to interactive language learning with weak denotational supervision (Wang et al., 2016), definitions are critical for learning complex actions (Figure 1). Definitions equate a novel utterance to a sequence of utterances that the system already understands. For example, \u2018go left 6 and go front\u2019 might be defined as \u2018repeat 6 [go left]; go front\u2019, which eventually can be traced back to the expression \u2018repeat 6 [select left of this]; select front of this\u2019 in the core language. Unlike function definitions in programming languages, the user writes concrete values rather than explicitly declaring arguments. The system automatically extracts arguments and learns to produce the correct generalizations. For this, we propose a grammar induction algorithm tailored to the learning from definitions setting. Compared to standard machine learning, say from demonstrations, definitions provide a much more powerful learning signal: the system is told directly that \u2018a 3 by 4 red square\u2019 is \u20183 red columns of height 4\u2019, and does not have to infer how to generalize from observing many structures of different sizes.\nWe implemented a system called Voxelurn, which is a command language interface for a voxel world initially equipped with a programming language supporting conditionals, loops, and variable scoping etc. We recruited 70 users from Amazon Mechanical Turk to build 230 voxel structures using our system. All users teach the system at once, and what is learned from one user\ncan be used by another user. Thus a community of users evolves the language to becomes more efficient over time, in a distributed way, through interaction. We show that the user community defined many new utterances\u2014short forms, alternative syntax, and also complex concepts such as \u2018add green monster, add yellow plate 3 x 3\u2019. As the system learns, users increasingly prefer to use the naturalized language over the core language: 85.9% of the last 10K accepted utterances are in the naturalized language."}, {"heading": "2 Voxelurn", "text": "World. A world state in Voxelurn contains a set of voxels, where each voxel has relations \u2018row\u2019, \u2018col\u2019, \u2018height\u2019, and \u2018color\u2019. There are two domainspecific actions, \u2018add\u2019 and \u2018move\u2019, one domainspecific relation \u2018direction\u2019. In addition, the state contains a selection, which is a set of positions. While our focus is Voxelurn, we can think more generally about the world as a set of objects equiped with relations \u2014 events on a calendar, cells of a spreadsheet, or lines of text.\nCore language. The system is born understanding a core language called Dependency-based Action Language (DAL), which we created (see Table 1 for an overview).\nThe language composes actions using the usual but expressive control primitives such as \u2018if\u2019, \u2018foreach\u2019, \u2018repeat\u2019, etc. Actions usually take sets as arguments, which are represented using lambda dependency-based compositional semantics (lambda DCS) expressions (Liang, 2013). Besides standard set operations like union, intersec-\ntion and complement, lambda DCS leverages the tree dependency structure common in natural language: for the relation \u2018color\u2019, \u2018has color red\u2019 refers to the set of voxels that have color red, and its reverse \u2018color of has row 1\u2019 refers to the set of colors of voxels having row number 1. Treestructured joins can be chained without using any variables, e.g., \u2018has color [yellow or color of has row 1]\u2019.\nWe protect the core language from being redefined so it is always precise and usable.1 In addition to expressivity, the core language interpolates well with natural language. We avoid explicit variables by using a selection, which serves as the default argument for most actions.2 For example, \u2018select has color red; add yellow top; remove\u2019 adds yellow on top of red voxels and then removes the red voxels.\nTo enable the building of more complex struc1Not doing so resulted in ambiguities that propagated uncontrollably, e.g., once \u2018red\u2019 can mean many different colors. 2The selection is like the turtle in LOGO, but can be a set.\ntures in a more modular way, we introduce a notion of scoping. Suppose one is operating on one of the palm trees in Figure 2. The user might want to use \u2018select all\u2019 to select only the voxels in that tree rather than all of the voxels in the scene. In general, an action A can be viewed as taking a set of voxels v and a selection s, and producing an updated set of voxels v\u2032 and a modified selection s\u2032. The default scoping is \u2018[A]\u2019, which is the same as \u2018A\u2019 and returns (v\u2032, s\u2032). There are two constructs that alter the flow: First, \u2018{A}\u2019 takes (v, s) and returns (v\u2032, s), thus restoring the selection. This allows A to use the selection as a temporary variable without affecting the rest of the program. Second, \u2018isolate [A]\u2019 takes (v, s), calls A with (s, s) (restricting the set of voxels to just the selection) and returns (v\u2032\u2032, s), where v\u2032\u2032 consists of voxels in v\u2032 and voxels in v that occupy empty locations in v\u2032. This allows A to focus only on the selection (e.g., one of the palm trees). Although scoping can be explicitly controlled via\n\u2018[ ]\u2019, \u2018isolate\u2019, and \u2018{ }\u2019, it is an unnatural concept for non-programmers. Therefore when the choice is not explicit, the parser generates all three possible scoping interpretations, and the model learns which is intended based on the user, the rule, and potentially the context."}, {"heading": "3 Learning interactively from definitions", "text": "The goal of the user is to build a structure in Voxelurn. In Wang et al. (2016), the user provided interactive supervision to the system by selecting from a list of candidates. This is practical when there are less than tens of candidates, but is completely infeasible for a complex action space such as Voxelurn. Roughly, 10 possible colors over the 3\u00d7 3\u00d7 4 box containing the palm tree in Figure 2 yields 1036 distinct denotations, and many more programs. Obtaining the structures in Figure 1 by selecting candidates alone would be infeasible.\nThis work thus uses definitions in addition to selecting candidates as the supervision signal. Each definition consists of a head utterance and a body, which is a sequence of utterances that the system understands. One use of definitions is paraphrasing and defining alternative syntax, which helps naturalize the core language (e.g., defining \u2018add brown top 3 times\u2019 as \u2018repeat 3 add brown top\u2019). The second use is building up complex concepts hierarchically. In Figure 2, \u2018add yellow palm tree\u2019 is defined as a sequence of steps for building the palm tree. Once the system understands an utterance, it can be used in the body of other definitions. For example, Figure 3 shows the full definition tree of \u2018add palm tree\u2019. Unlike function definitions in a programming language, our definitions do not specify the exact arguments; the system has to learn to extract arguments to achieve the correct generalization.\nThe interactive definition process is described in Figure 4. When the user types an utterance x, the system parses x into a list of candidate programs. If the user selects one of them (based on its denotation), then the system executes the resulting program. If the utterance is unparsable or the user rejects all candidate programs, the user is asked to provide the definition body for x. Any utterances in the body not yet understood can be defined recursively. Alternatively, the user can first execute a sequence of commands X , and then provide a head utterance for body X .\nWhen constructing the definition body, users\ncan type utterances with multiple parses; e.g., \u2018move forward\u2019 could either modify the selection (\u2018select front\u2019) or move the voxel (\u2018move front\u2019). Rather than propagating this ambiguity to the head, we force the user to commit to one interpretation by selecting a particular candidate. Note that we are using interactivity to control the exploding ambiguity."}, {"heading": "4 Model and learning", "text": "Let us turn to how the system learns and predicts. This section contains prerequisites before we describe definitions and grammar induction in Section 5.\nSemantic parsing. Our system is based on a semantic parser that maps utterances x to programs z, which can be executed on the current state s (set of voxels and selection) to produce the next state s\u2032 = JzKs. Our system is implemented as the interactive package in SEMPRE (Berant et al., 2013);\nsee Liang (2016) for a gentle exposition. A derivation d represents the process by which an utterance x turns into a program z = prog(d). More precisely, d is a tree where each node contains the corresponding span of the utterance (start(d), end(d)), the grammar rule rule(d), the grammar category cat(d), and a list of child derivations [d1, . . . , dn].\nFollowing Zettlemoyer and Collins (2005), we define a log-linear model over derivations d given an utterance x produced by the user u:\np\u03b8(d | x, u) \u221d exp(\u03b8T\u03c6(d, x, u)), (1)\nwhere \u03c6(d, x, u) \u2208 Rp is a feature vector and \u03b8 \u2208 Rp is a parameter vector. The user u does not appear in previous work on semantic parsing, but we use it to personalize the semantic parser trained on the community.\nWe use a standard chart parser to construct a chart. For each chart cell, indexed by the start and end indices of a span, we construct a list of partial derivations recursively by selecting child derivations from subspans and applying a grammar rule. The resulting derivations are sorted by model score and only the top K are kept. We use chart(x) to denote the set of all partial derivations across all chart cells. The set of grammar rules starts with the set of rules for the core language (Table 1), but grows via grammar induction when users add definitions (Section 5). Rules in the grammar are stored in a trie based on the righthand side to enable better scalability to a large number of rules.\nFeatures. Derivations are scored using a weighted combination of features. There are three types of features, summarized in Table 2.\nRule features fire on each rule used to construct a derivation. ID features fire on specific rules (by ID). Type features track whether a rule is part of the core language or induced, whether it has been\nused again after it was defined, if it was used by someone other than its author, and if the user and the author are the same (5 + #rules features).\nSocial features fire on properties of rules that capture the unique linguistic styles of different users and their interaction with each other. Author features capture the fact that some users provide better, and more generalizable definitions that tend to be accepted. Friends features are cross products of author ID and user ID, which captures whether rules from a particular author are systematically preferred or not by the current user, due to stylistic similarities or differences (#users+#users\u00d7#users features).\nSpan features include conjunctions of the category of the derivation and the leftmost/rightmost token on the border of the span. In addition, span features include conjunctions of the category of the derivation and the 1 or 2 adjacent tokens just outside of the left/right border of the span. These capture a weak form of context-dependence that is generally helpful (<\u2248 V 4 \u00d7 #cats features for a vocabulary of size V ).\nScoping features track how the community, as well as individual users, prefer each of the 3 scoping choices (none, selection only \u2018{A}\u2019, and voxels+selection \u2018isolate {A}\u2019), as described in Section 2. 3 global indicators, and 3 indicators for each user fire every time a particular scoping choice is made (3 + 3\u00d7 #users features).\nParameter estimation. When the user types an utterance, the system generates a list of candidate next states. When the user chooses a particular next state s\u2032 from this list, the system performs an online AdaGrad update (Duchi et al., 2010) on the parameters \u03b8 according to the gradient of the following loss function:\n\u2212 log \u2211\nd:Jprog(d)Ks=s\u2032 p\u03b8(d | x, u) + \u03bb||\u03b8||1,\nwhich attempts to increase the model probability on derivations whose programs produce the next state s\u2032."}, {"heading": "5 Grammar induction", "text": "Recall that the main form of supervision is via user definitions, which allows creation of user-defined concepts. In this section, we show how to turn\nthese definitions into new grammar rules that can be used by the system to parse new utterances.\nPrevious systems of grammar induction for semantic parsing were given utterance-program pairs (x, z). Both the GENLEX (Zettlemoyer and Collins, 2005) and higher-order unification (Kwiatkowski et al., 2010) algorithms overgenerate rules that liberally associate parts of x with parts of z. Though some rules are immediately pruned, many spurious rules are undoubtedly still kept. In the interactive setting, we must keep the number of candidates small to avoid a bad user experience, which means a higher precision bar for new rules.\nFortunately, the structure of definitions makes the grammar induction task easier. Rather than being given an utterance-program (x, z) pair, we are given a definition, which consists of an utterance x (head) along with the body X = [x1, . . . , xn], which is a sequence of utterances. The body X is fully parsed into a derivation d, while the head x is likely only partially parsed. These partial derivations are denoted by chart(x).\nAt a high-level, we find matches\u2014partial derivations chart(x) of the head x that also occur in the full derivation d of the body X . A grammar rule is produced by substituting any set of nonoverlapping matches by their categories. As an example, suppose the user defines\n\u2018add red top times 3\u2019 as \u2018repeat 3 [add red top]\u2019.\nThen we would be able to induce the following two grammar rules:\nA\u2192 add C D times N : \u03bbCDN.repeat N [add C D]\nA\u2192 A times N : \u03bbAN.repeat N [A]\nThe first rule substitutes primitive values (\u2018red\u2019, \u2018top\u2019, and \u20183\u2019) with their respective pre-terminal categories (C, D, N ). The second rule contains compositional categories like actions (A), which require some care. One might expect that greedily substituting the largest matches or the match that covers the largest portion of the body would work, but the following example shows that this is not the case:\nA1 A1 A1\ufe37 \ufe38\ufe38 \ufe37 \ufe37 \ufe38\ufe38 \ufe37 \ufe37 \ufe38\ufe38 \ufe37 add red left and here = add red left; add red\ufe38 \ufe37\ufe37 \ufe38 \ufe38 \ufe37\ufe37 \ufe38\nA2 A2\nHere, both the highest coverage substitution (A1: \u2018add red\u2019, which covers 4 tokens of the body), and the largest substitution available (A2: \u2018add red left\u2019) would generalize incorrectly. The correct grammar rule only substitutes the primitive values (\u2018red\u2019, \u2018left\u2019)."}, {"heading": "5.1 Highest scoring abstractions", "text": "We now propose a grammar induction procedure that optimizes a more global objective and uses the learned semantic parsing model to choose substitutions. More formally, let M be the set of partial derivations in the head whose programs appear in the derivation dX of the body X:\nM def = {d \u2208 chart(x) : \u2203d\u2032 \u2208 desc(dX) \u2227 prog(d) = prog(d\u2032)},\nwhere desc(dX) are the descendant derivations of dX . Our goal is to find a packing P \u2286 M , which is a set of derivations corresponding to nonoverlapping spans of the head. We say that a packing P is maximal if no other derivations may be added to it without creating an overlap.\nLet packings(M) denote the set of maximal packings, we can frame our problem as finding the maximal packing that has the highest score under our current semantic parsing model:\nP \u2217L = argmax P\u2208packings(M); \u2211 d\u2208P score(d). (2)\nFinding the highest scoring packing can be done using dynamic programming on P \u2217i for i = 0, 1, . . . , L, whereL is the length of x and P \u22170 = \u2205. Since d \u2208M , start(d) and end(d) (exclusive) refer to span in the head x. To obtain this dynamic program, let Di be the highest scoring maximal packing containing a derivation ending exactly at position i (if it exists):\nDi = {di} \u222a P \u2217start(di), (3)\ndi = argmax d\u2208M ;end(d)=i\nscore(d \u222a P \u2217start(d)). (4)\nThen the maximal packing of up to i can be defined recursively as\nP \u2217i = argmax D\u2208{Ds(i)+1,Ds(i)+2,...,Di} score(D) (5)\ns(i) = max d:end(d)\u2264i start(d), (6)\nInput : x, dX , P \u2217 Output: rule r \u2190 x; f \u2190 dX ; for d \u2208 P \u2217 do\nr \u2190 r[cat(d)/ span(d)] f \u2190 \u03bb cat(d).f [cat(d)/d]\nreturn rule (cat(dX)\u2192 r : f)\nAlgorithm 1: Extract a rule r from a derivation dX of body X and a packing P \u2217. Here, f [t/s] means substituting s by t in f , with the usual care about names of bound variables.\nwhere s(i) is the largest index such thatDs(i) is no longer maximal for the span (0, i) (i.e. there is a d \u2208M on the span start(d) \u2265 s(i) \u2227 end(d) \u2264 i.\nOnce we have a packing P \u2217 = P \u2217L, we can go through d \u2208 P \u2217 in order of start(d), as in Algorithm 1. This generates one high precision rule per packing per definition. In addition to the highest scoring packing, we also use a \u201csimple packing\u201d, which includes only primitive values (in Voxelurn, these are colors, numbers, and directions). Unlike the simple packing, the rule induced from the highest scoring packing does not always generalize correctly. However, a rule that often generalizes incorrectly should be down-weighted, along with the score of its packings. As a result, a different rule might be induced next time, even with the same definition."}, {"heading": "5.2 Extending the chart via alignment", "text": "Algorithm 1 yields high precision rules, but fails to generalize in some cases. Suppose that \u2018move up\u2019 is defined as \u2018move top\u2019, where \u2018up\u2019 does not parse, and does not match anything. We would like to infer that \u2018up\u2019 means \u2018top\u2019. To handle this, we leverage a property of definitions that we have not used thus far: the utterances themselves. If we align the head and body, then we would intuitively expect aligned phrases to correspond to the same derivations. Under this assumption, we can then transplant these derivations from dX to chart(x) to create new matches. This is more constrained than the usual alignment problem (e.g., in machine translation) since we only need to consider spans of X which corresponds to derivations in desc(dX).\nAlgorithm 2 provides the algorithm for extending the chart via alignments. The aligned function is implemented using the following two heuristics:\nInput : x,X, dX for d \u2208 desc(dX), x\u2032 \u2208 spans(x) do\nif aligned(x\u2032, d, (x,X)) then d\u2032 \u2190 d; start(d\u2032)\u2190 start(x\u2032); end(d\u2032)\u2190 end(x\u2032); chart(x)\u2190 chart(x) \u222a d\u2032\nend end\nAlgorithm 2: Extending the chart by alignment: If d is aligned with x\u2032 based on the utterance, then we pretend that x\u2032 should also parse to d, and d is transplanted to chart(x) as if it parsed from x\u2032.\n\u2022 exclusion: if all but 1 pair of short spans (1 or 2 tokens) are matched, the unmatched pair is considered aligned.\n\u2022 projectivity: if d1, d2 \u2208 desc(dX) \u2229 chart(x), then ances(d1, d2) is aligned to the corresponding span in x.\nWith the extended chart, we can run the algorithm from Section 5.1 to induce rules. The transplanted derivations (e.g., \u2018up\u2019) might now form new matches which allows the grammar induction to induce more generalizable rules. We only perform this extension when the body consists of one utterance, which tend to be a paraphrase. Bodies with multiple utterances tend to be new concepts (e.g., \u2018add green monster\u2019), for which alignment is impossible. Because users have to select from candidates parses in the interactive setting, inducing low precision rules that generate many parses degrade the user experience. Therefore, we induce alignment-based rules conservatively\u2014only when all but 1 or 2 tokens of the head aligns to the body and vice versa."}, {"heading": "6 Experiments", "text": "Setup. Our ultimate goal is to create a community of users who can build interesting structures in Voxelurn while naturalizing the core language. We created this community using Amazon Mechanical Turk (AMT) in two stages. First, we had qualifier tasks, in which an AMT worker was instructed to replicate a fixed target exactly (Figure 5), ensuring that the initial users are familiar with at least some of the core language, which is the starting point of the naturalization process.\nNext, we allowed the workers who qualified to enter the second freebuilding task, in which they were asked to build any structure they wanted in 30 minutes. This process was designed to give users freedom while ensuring quality. The analogy of this scheme in a real system is that early users (or a small portion of expert users) have to make some learning investment, so the system can learn and become easier for other users.\nStatistics. 70 workers passed the qualifier task, and 42 workers participated in the final freebuilding experiment. They built 230 structures. There were over 103,000 queries consisting of 5,388 distinct token types. Of these, 64,075 utterances were tried and 36,589 were accepted (so an action was performed). There were 2,495 definitions combining over 15,000 body utterances with 6.5 body utterances per head on average (96 max). From these definitions, 2,817 grammar rules were induced, compared to less than 100 core rules. Over all queries, there were 8.73 parses per utterance on average (starting from 1 for core).\nIs naturalization happening? The answer is yes according to Figure 6, which plots the cummulative percentage of utterances that are core, induced, or unparsable. To rule out that more induced utterances are getting rejected, we consider only accepted utterances in the middle of Figure 6, which plots the percentage of induced rules among accepted utterances for the entire community, as well as for the 5 heaviest users. Since unparsable utterances cannot be accepted, accepted core (which is not shown) is the complement of accepted induced. At the conclusion of the experiment, 72.9% of all accepted utterances are induced\u2014this becomes 85.9% if we only consider the final 10,000 accepted utterances.\nThree modes of naturalization are outlined in Table 3. For very common operations, like moving the selection, people found \u2018select left\u2019 too verbose and shorterned this to l, left, >, sel l. One user preferred \u2018go down and right\u2019 instead of \u2018select bot; select right\u2019 in core and defined it as \u2018go down; go right\u2019. Definitions for high-level\nconcepts tend to be whole objects that are not parameterized (e.g., \u2018dancer\u2019). The bottom plot of Figure 6 suggests that users are defining and using higher level concepts, since programs become longer relative to utterances over time.\nAs a result of the automatic but implicit grammar induction, some concepts do not generalize correctly. In definition head \u20183 tall 9 wide white tower centered here\u2019, arguments do not match the body; for \u2018black 10x10x10 frame\u2019, we failed to tokenize.\nLearned parameters. Training using L1 regularization, we obtained 1713 features with nonzero parameters. One user defined many concepts consisting of a single short token, and the Social.Author feature for that user has the most negative weight overall. With user compatibility (Social.Friends), some pairs have large positive weights and others large negative weights. The \u2018isolate\u2019 scoping choice (which allows easier hierarchical building) received the most positive weights, both overall and for many users. The 2 highest scoring induced rules correspond to \u2018add row red right 5\u2019 and \u2018select left 2\u2019.\nIncentives. Having complex structures show that the actions in Voxelurn are expressive and that hierarchical definitions are useful. To incentivize this behavior, we created a leaderboard which ranked structures based on recency and upvotes (like Hacker News). Over the course of 3 days, we picked three prize categories to be released daily. The prize categories for each day were bridge, house, animal; tower, monster, flower; ship, dancer, and castle.\nTo incentivize more definitions, we also track citations. When a rule is used in an accepted utterance by another user, the rule (and its author) receives a citation. We pay bonuses to top users according to their h-index. Most cited definitions are also displayed on the leaderboard. Our qualitative results should be robust to the incentives scheme, because the users do not overfit to the incentives\u2014e.g., around 20% of the structures are\nnot in the prize categories and users define complex concepts that are rarely cited."}, {"heading": "7 Related work and discussion", "text": "This work is an evolution of Wang et al. (2016), but differs crucially in several ways: While Wang et al. (2016) starts from scratch and relies on selecting candidates, this work starts with a programming language (PL) and additionally relies on definitions, allowing us to scale. Instead of having a private language for each user, the user community in this work shares one language.\nAzaria et al. (2016) presents Learning by Instruction Agent (LIA), which also advocates learning from users. They argue that developers cannot anticipate all the actions that users want, and that the system cannot understand the corresponding natural language even if the desired action is built-in. Like Jia et al. (2017), Azaria et al. (2016) starts with an ad-hoc set of initial slot-filling commands in natural language as the basis of further instructions\u2014our approach starts with a more expressive core PL designed to interpolate with natural language. Compared to previous work, this work studied interactive learning in a shared community setting and hierarchical definitions resulting in more complex concepts.\nAllowing ambiguity and a flexible syntax is a key reason why natural language is easier to produce\u2014this cannot be achieved by PLs such as Inform and COBOL which look like natural language. In this work, we use semantic parsing techniques that can handle ambiguity (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Liang et al., 2011; Pasupat and Liang, 2015). In semantic parsing, the semantic representation and action space is usually designed to accommodate the natural language that is considered constant. In contrast, the action space is considered constant in the naturalizing PL approach, and the language adapts to be more natural while accommodating the action space.\nOur work demonstrates that interactive definitions is a strong and usable form of supervision. In the future, we wish to test these ideas in more domains, naturalize a real PL, and handle paraphrasing and implicit arguments. In the process of naturalization, both data and the semantic grammar have important roles in the evolution of a language that is easier for humans to produce while still parsable by computers.\nAcknowledgments. We thank our reviewers, Panupong (Ice) Pasupat for helpful suggestions and discussions on lambda DCS, DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF-15-1-0462, and NSF CAREER Award no. IIS-1552635.\nReproducibility. All code, data, and experiments for this paper are available on the CodaLab platform: https://worksheets.\ncodalab.org/worksheets/\n0xbf8f4f5b42e54eba9921f7654b3c5c5d and a demo: http://www.voxelurn.com"}], "references": [{"title": "Bootstrapping semantic parsers from conversations", "author": ["Y. Artzi", "L. Zettlemoyer."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 421\u2013432.", "citeRegEx": "Artzi and Zettlemoyer.,? 2011", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2011}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 1:49\u201362.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Instructable intelligent personal agent", "author": ["A. Azaria", "J. Krishnamurthy", "T.M. Mitchell."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI). pages 2681\u20132689.", "citeRegEx": "Azaria et al\\.,? 2016", "shortCiteRegEx": "Azaria et al\\.", "year": 2016}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Almond: The architecture of an open, crowdsourced, privacy-preserving, programmable virtual assistant", "author": ["G. Campagna", "R. Ramesh", "S. Xu", "M. Fischer", "M.S. Lam."], "venue": "World Wide Web (WWW). pages 341\u2013350.", "citeRegEx": "Campagna et al\\.,? 2017", "shortCiteRegEx": "Campagna et al\\.", "year": 2017}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer."], "venue": "Conference on Learning Theory (COLT).", "citeRegEx": "Duchi et al\\.,? 2010", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "NLyze: interactive programming by natural language for spreadsheet data analysis and manipulation", "author": ["S. Gulwani", "M. Marron."], "venue": "International Conference on Management of Data, SIGMOD. pages 803\u2013814.", "citeRegEx": "Gulwani and Marron.,? 2014", "shortCiteRegEx": "Gulwani and Marron.", "year": 2014}, {"title": "Learning concepts through conversations in spoken dialogue systems", "author": ["R. Jia", "L. Heck", "D. Hakkani-T\u00fcr", "G. Nikolov."], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP).", "citeRegEx": "Jia et al\\.,? 2017", "shortCiteRegEx": "Jia et al\\.", "year": 2017}, {"title": "Using semantic unification to generate regular expressions from natural language", "author": ["N. Kushman", "R. Barzilay."], "venue": "Human Language Technology and", "citeRegEx": "Kushman and Barzilay.,? 2013", "shortCiteRegEx": "Kushman and Barzilay.", "year": 2013}, {"title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "author": ["T. Kwiatkowski", "L. Zettlemoyer", "S. Goldwater", "M. Steedman."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1223\u20131233.", "citeRegEx": "Kwiatkowski et al\\.,? 2010", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Lambda dependency-based compositional semantics", "author": ["P. Liang."], "venue": "arXiv preprint arXiv:1309.4408 .", "citeRegEx": "Liang.,? 2013", "shortCiteRegEx": "Liang.", "year": 2013}, {"title": "Learning executable semantic parsers for natural language understanding", "author": ["P. Liang."], "venue": "Communications of the ACM 59.", "citeRegEx": "Liang.,? 2016", "shortCiteRegEx": "Liang.", "year": 2016}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein."], "venue": "Association for Computational Linguistics (ACL). pages 590\u2013599.", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S.J. Teller", "N. Roy."], "venue": "Association for the Advancement of Artificial Intelligence", "citeRegEx": "Tellex et al\\.,? 2011", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Learning language games through interaction", "author": ["S.I. Wang", "P. Liang", "C. Manning."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["M. Zelle", "R.J. Mooney."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI). pages 1050\u20131055.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins."], "venue": "Uncertainty in Artificial Intelligence (UAI). pages 658\u2013 666.", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins."], "venue": "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL). pages 678\u2013687.", "citeRegEx": "Zettlemoyer and Collins.,? 2007", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}], "referenceMentions": [{"referenceID": 6, "context": "(Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 16, "context": "(Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al., 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al.", "startOffset": 47, "endOffset": 92}, {"referenceID": 3, "context": "(Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al., 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al.", "startOffset": 47, "endOffset": 92}, {"referenceID": 8, "context": ", 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al.", "startOffset": 27, "endOffset": 55}, {"referenceID": 4, "context": ", 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al., 2017) and robots (Tellex et al.", "startOffset": 95, "endOffset": 118}, {"referenceID": 14, "context": ", 2017) and robots (Tellex et al., 2011), peo-", "startOffset": 19, "endOffset": 40}, {"referenceID": 17, "context": "guage, which has been the subject of work in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011, 2013; Pasupat and Liang, 2015).", "startOffset": 62, "endOffset": 153}, {"referenceID": 13, "context": "guage, which has been the subject of work in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011, 2013; Pasupat and Liang, 2015).", "startOffset": 62, "endOffset": 153}, {"referenceID": 15, "context": "Compared to interactive language learning with weak denotational supervision (Wang et al., 2016), definitions are critical for learning complex ac-", "startOffset": 77, "endOffset": 96}, {"referenceID": 10, "context": "Actions usually take sets as arguments, which are represented using lambda dependency-based compositional semantics (lambda DCS) expressions (Liang, 2013).", "startOffset": 141, "endOffset": 154}, {"referenceID": 15, "context": "In Wang et al. (2016), the user provided interactive supervision to the system by selecting from a list of candidates.", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "Our system is implemented as the interactive package in SEMPRE (Berant et al., 2013);", "startOffset": 63, "endOffset": 84}, {"referenceID": 10, "context": "see Liang (2016) for a gentle exposition.", "startOffset": 4, "endOffset": 17}, {"referenceID": 17, "context": "Following Zettlemoyer and Collins (2005), we define a log-linear model over derivations d given an utterance x produced by the user u:", "startOffset": 10, "endOffset": 41}, {"referenceID": 5, "context": "When the user chooses a particular next state s\u2032 from this list, the system performs an online AdaGrad update (Duchi et al., 2010) on the parameters \u03b8 according to the gradient of the following loss function:", "startOffset": 110, "endOffset": 130}, {"referenceID": 17, "context": "Both the GENLEX (Zettlemoyer and Collins, 2005) and higher-order unification (Kwiatkowski et al.", "startOffset": 16, "endOffset": 47}, {"referenceID": 9, "context": "Both the GENLEX (Zettlemoyer and Collins, 2005) and higher-order unification (Kwiatkowski et al., 2010) algorithms overgenerate rules that liberally associate parts of x with parts of z.", "startOffset": 77, "endOffset": 103}, {"referenceID": 15, "context": "This work is an evolution of Wang et al. (2016), but differs crucially in several ways: While Wang et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 15, "context": "This work is an evolution of Wang et al. (2016), but differs crucially in several ways: While Wang et al. (2016) starts from scratch and relies on selecting candidates, this work starts with a programming language (PL) and additionally relies on definitions, allowing us to scale.", "startOffset": 29, "endOffset": 113}, {"referenceID": 6, "context": "Like Jia et al. (2017), Azaria et al.", "startOffset": 5, "endOffset": 23}, {"referenceID": 2, "context": "(2017), Azaria et al. (2016)", "startOffset": 8, "endOffset": 29}, {"referenceID": 9, "context": "In this work, we use semantic parsing techniques that can handle ambiguity (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Liang et al., 2011; Pasupat and Liang, 2015).", "startOffset": 75, "endOffset": 183}, {"referenceID": 12, "context": "In this work, we use semantic parsing techniques that can handle ambiguity (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Liang et al., 2011; Pasupat and Liang, 2015).", "startOffset": 75, "endOffset": 183}, {"referenceID": 13, "context": "In this work, we use semantic parsing techniques that can handle ambiguity (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Liang et al., 2011; Pasupat and Liang, 2015).", "startOffset": 75, "endOffset": 183}], "year": 2017, "abstractText": "Our goal is to create a convenient natural language interface for performing wellspecified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to \u201cnaturalize\u201d the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9% of the last 10K utterances.", "creator": "LaTeX with hyperref package"}}}