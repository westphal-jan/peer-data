{"id": "1704.06960", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Translating Neuralese", "abstract": "several approaches have recently been proposed for learning decentralized interactions deep multiagent policies that coordinate via a differentiable communication channel. while these policies are effective for many tasks, interpretation of communicating their induced communication strategies efficiently has remained a challenge. here have we propose to interpret agents'messages by translating them. unlike in countless typical machine translation problems, informally we have no parallel communicated data to continuously learn tasks from. instead more we develop utilizing a translation model based on assessing the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. we present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics effect of messages by ensuring that players communicating through a translation layer don't suffer a substantial loss in reward relative to assisting players with a more common language.", "histories": [["v1", "Sun, 23 Apr 2017 18:46:42 GMT  (7288kb,D)", "http://arxiv.org/abs/1704.06960v1", "To appear in ACL 2017"], ["v2", "Mon, 25 Sep 2017 21:28:48 GMT  (7285kb,D)", "http://arxiv.org/abs/1704.06960v2", null], ["v3", "Thu, 28 Sep 2017 15:10:24 GMT  (7472kb,D)", "http://arxiv.org/abs/1704.06960v3", null]], "COMMENTS": "To appear in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jacob andreas", "anca d dragan", "dan klein"], "accepted": true, "id": "1704.06960"}, "pdf": {"name": "1704.06960.pdf", "metadata": {"source": "CRF", "title": "Translating Neuralese", "authors": ["Jacob Andreas Anca Dragan", "Dan Klein"], "emails": ["jda@cs.berkeley.edu", "anca@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents\u2019 messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.1"}, {"heading": "1 Introduction", "text": "Several recent papers have described approaches for learning deep communicating policies (DCPs): decentralized representations of behavior that enable multiple agents to communicate via a differentiable channel that can be formulated as a recurrent neural network. DCPs have been shown to solve a variety of coordination problems, including reference games (Lazaridou et al., 2016b), logic puzzles (Foerster et al., 2016), and simple control (Sukhbaatar et al., 2016). Appealingly, the agents\u2019 communication protocol can be learned via direct\n1 We have released code and data at http://github. com/jacobandreas/neuralese.\nbackpropagation through the communication channel, avoiding many of the challenging inference problems associated with learning in classical decentralized decision processes (Roth et al., 2005).\nBut analysis of the strategies induced by DCPs has remained a challenge. As an example, Figure 1 depicts a driving game in which two cars, which are unable to see each other, must both cross an intersection without colliding. In order to ensure success, it is clear that the cars must communicate with each other. But a number of successful communication strategies are possible\u2014for example, they might report their exact (x, y) coordinates at every timestep, or they might simply announce whenever they are entering and leaving the intersection. If these messages were communicated in natural language, it would be straightforward to determine which strategy was being employed. However, DCP agents instead communicate with an automatically induced protocol of unstructured, real-valued recurrent state vectors\u2014an artificial language we might call \u201cneuralese,\u201d which superficially bears little resemblance to natural language, and thus frustrates attempts at direct interpretation. ar X iv :1 70 4.\n06 96\n0v 1\n[ cs\n.C L\n] 2\n3 A\npr 2\n01 7\nWe propose to understand neuralese messages by translating them. In this work, we present a simple technique for inducing a dictionary that maps between neuralese message vectors and short natural language strings, given only examples of DCP agents interacting with other agents, and humans interacting with other humans. Natural language already provides a rich set of tools for describing beliefs, observations, and plans\u2014our thesis is that these tools provide a useful complement to the visualization and ablation techniques used in previous work on understanding complex models (Strobelt et al., 2016; Ribeiro et al., 2016).\nWhile structurally quite similar to the task of machine translation between pairs of human languages, interpretation of neuralese poses a number of novel challenges. First, there is no natural source of parallel data: there are no bilingual \u201cspeakers\u201d of both neuralese and natural language. Second, there may not be a direct correspondence between the strategy employed by humans and DCP agents: even if it were constrained to communicate using natural language, an automated agent might choose to produce a different message from humans in a given state. We tackle both of these challenges by appealing to the grounding of messages in gameplay. Our approach is based on one of the core insights in natural language semantics: messages (whether in neuralese or natural language) have similar meanings when they induce similar beliefs about the state of the world.\nBased on this intuition, we introduce a translation criterion that matches neuralese messages with natural language strings by minimizing statistical distance in a common representation space of distributions over speaker states. We explore several related questions:\n\u2022 What makes a good translation, and under what conditions is translation possible at all? (Section 4)\n\u2022 How can we build a model to translate between neuralese and natural language? (Section 5)\n\u2022 What kinds of theoretical guarantees can we provide about the behavior of agents communicating via this translation model? (Section 6)\nOur translation model and analysis are general, and in fact apply equally to human\u2013computer and\nhuman\u2013human translation problems grounded in gameplay. In this paper, we focus our experiments specifically on the problem of interpreting communication in deep policies, and apply our approach to the driving game in Figure 1 and two reference games of the kind shown in Figure 2. We find that this approach outperforms a more conventional machine translation criterion both when attempting to interoperate with neuralese speakers and when predicting their state."}, {"heading": "2 Related work", "text": "A variety of approaches for learning deep policies with communication were proposed essentially simultaneously in the past year. We have broadly labeled these as \u201cdeep communicating policies\u201d; concrete examples include Lazaridou et al. (2016b), Foerster et al. (2016), and Sukhbaatar et al. (2016). The policy representation we employ in this paper is similar to the latter two of these, although the general framework is agnostic to low-level modeling details and could be straightforwardly applied to other architectures. Analysis of communication strategies in all these papers has been largely adhoc, obtained by clustering states from which similar messages are emitted and attempting to manually assign semantics to these clusters. The present work aims at developing tools for performing this analysis automatically.\nMost closely related to our approach is that of Lazaridou et al. (2016a), who also develop a model for assigning natural language interpretations to learned messages; however, this approach relies on supervised cluster labels and is targeted specifically towards referring expression games. Here we attempt to develop an approach that can handle general multiagent interactions without assuming a prior discrete structure in space of observations.\nThe literature on learning decentralized multiagent policies in general is considerably larger (Bernstein et al., 2002; Dibangoye et al., 2016). This includes work focused on communication in multiagent settings (Roth et al., 2005) and even communication using natural language messages (Vogel et al., 2013b). All of these approaches employ structured communication schemes with manually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference.\nOur evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game. Communication strategies for reference games were previously explored by Vogel et al. (2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference games specifically featuring end-to-end communication protocols by Yu et al. (2016). On the control side, a long line of work considers nonverbal communication strategies in multiagent policies (Dragan and Srinivasa, 2013).\nAnother group of related approaches focuses on the development of more general machinery for interpreting deep models in which messages have no explicit semantics. This includes both visualization techniques (Zeiler and Fergus, 2014; Strobelt et al., 2016), and approaches focused on generating explanations in the form of natural language (Hendricks et al., 2016; Vedantam et al., 2017)."}, {"heading": "3 Problem formulation", "text": "Games Consider a cooperative game with two players a and b of the form given in Figure 3. At every step t of this game, player a makes an observation x(t)a and receives a message z (t\u22121) b from b. It then takes an action u(t)a and sends a message z (t) a to b. (The process is symmetric for b.) The distributions p(ua|xa, zb) and p(za|xa) together define a policy \u03c0 which we assume is shared by both players, i.e. p(ua|xa, zb) = p(ub|xb, za) and p(za|xa) = p(zb|xb). As in a standard Markov decision process, the actions (u(t)a , u (t) b ) alter the world state, generating new observations for both players and a reward shared by both.\nThe distributions p(z|x) and p(u|x, z) may also be viewed as defining a language: they specify how a speaker will generate messages based on world states, and how a listener will respond to these mes-\nsages. Our goal in this work is to learn to translate between pairs of languages generated by different policies. Specifically, we assume that we have access to two policies for the same game: a \u201crobot policy\u201d \u03c0r and a \u201chuman policy\u201d \u03c0h. We would like to use the representation of \u03c0h, the behavior of which is transparent to human users, in order to understand the behavior of \u03c0r (which is in general an uninterpretable learned model); we will do this by inducing bilingual dictionaries that map message vectors zr of \u03c0r to natural language strings zh of \u03c0h and vice-versa.\nLearned agents \u03c0r Our goal is to present tools for interpretation of learned messages that are agnostic to the details of the underlying algorithm for acquiring them. We use a generic DCP model as a basis for the techniques developed in this paper. Here each agent policy is represented as a deep recurrent Q network (Hausknecht and Stone, 2015). This network is built from communicating cells of the kind depicted in Figure 4. At every timestep, this agent receives three pieces of information: an\nobservation of the current state of the world, the agent\u2019s memory vector from the previous timestep, and a message from the other player. It then produces three outputs: a predicted Q value for every possible action, a new memory vector for the next timestep, and a message to send to the other agent.\nSukhbaatar et al. (2016) observe that models of this form may be viewed as specifying a single RNN in which weight matrices have a particular block structure. Such models may thus be trained using the standard recurrent Q-learning objective, with communication protocol learned end-to-end.\nHuman agents \u03c0h The translation model we develop requires a representation of the distribution over messages p(za|xa) employed by human speakers (without assuming that humans and agents produce equivalent messages in equivalent contexts). We model the human message generation process as categorical, and fit a simple multilayer perceptron model to map from observations to words and phrases used during human gameplay."}, {"heading": "4 What\u2019s in a translation?", "text": "What does it mean for a message zh to be a \u201ctranslation\u201d of a message zr? In standard machine translation problems, the answer is that zh is likely to co-occur in parallel data with zr; that is, p(zh|zr) is large. Here we have no parallel data: even if we could observe natural language and neuralese messages produced by agents in the same state, we would have no guarantee that these messages actually served the same function. Our answer must instead appeal to the fact that both natural language and neuralese messages are grounded in a common environment. For a given neuralese message zr, we will first compute a grounded representation of that message\u2019s meaning; to translate, we find a natural-language message whose meaning is most similar. The key question is then what form this grounded meaning representation should take. The existing literature suggests two broad approaches:\nSemantic representation The meaning of a message za is given by its denotations: that is, by the set of world states of which za may be felicitously predicated, given the existing context available to a listener. In probabilistic terms, this says that the meaning of a message za is represented by the distribution p(xa|za, xb) it induces over speaker states. Examples of this approach include Guerin and Pitt (2001) and Pasupat and Liang (2016).\nPragmatic representation The meaning of a message za is given by the behavior it induces in a listener. In probabilistic terms, this says that the meaning of a message za is represented by the distribution p(ub|za, xb) it induces over actions given the listener\u2019s observation xb. Examples of this approach include Vogel et al. (2013a) and Gauthier and Mordatch (2016).\nThese two approaches can give rise to rather different behaviors. Consider the following example:\nsquare hexagon circle\nfew many many\nThe top language (in blue) has a unique name for every kind of shape, while the bottom language (in red) only distinguishes between shapes with few sides and shapes with many sides. Now imagine a simple reference game with the following form: player a is covertly assigned one of these three shapes as a reference target, and communicates that reference to b; b must then pull a lever labeled large or small depending on the size of the target shape. Blue language speakers can achieve perfect success at this game, while red language speakers can succeed at best two out of three times.\nHow should we translate the blue word hexagon into the red language? The semantic approach suggests that we should translate hexagon as many: while many does not uniquely identify the hexagon, it produces a distribution over shapes that is closest to the truth. The pragmatic approach instead suggests that we should translate hexagon as few, as this is the only message that guarantees that the listener will pull the correct lever large. So in order to produce a correct listener action, the translator might have to \u201clie\u201d and produce a maximally inaccurate listener belief.\nIf we were exclusively concerned with building a translation layer that allowed humans and DCP agents to interoperate as effectively as possible, it would be natural to adopt a pragmatic representation strategy. But our goals here are broader: we also want to facilitate understanding, and specifically to help users of learned systems form true beliefs about the systems\u2019 computational processes and representational abstractions. The example above demonstrates that \u201cpragmatically\u201d optimizing directly for task performance can sometimes lead to translations that produce inaccurate beliefs.\nWe instead build our approach around semantic representations of meaning. By preserving semantics, we allow listeners to reason accurately about the content and interpretation of messages. We might worry that by adopting a semantics-first view, we have given up all guarantees of effective interoperation between humans and agents using a translation layer. Fortunately, this is not so: as we will see in Section 6, it is possible to show that players communicating via a semantic translator perform only boundedly worse (and sometimes better!) than pairs of players with a common language."}, {"heading": "5 Translation models", "text": "In this section, we build on the intuition that messages should be translated via their semantics to define a concrete translation model\u2014a procedure for constructing a natural language \u2194 neuralese dictionary given agent and human interactions.\nWe understand the meaning of a message za to be represented by the distribution p(xa|za, xb) it induces over speaker states given listener context. We can formalize this by defining the belief distribution \u03b2 for a message z and context xb as:\n\u03b2(za, xb) = p(xa|za, xb) = p(za|xa)p(xb|xa)\u2211 x\u2032a p(za|x\u2032a)p(xb|x\u2032a)\nHere we have modeled the listener as performing a single step of Bayesian inference, using the listener state and the message generation model (by assumption shared between players) to compute the posterior over speaker states. While in general neither humans nor DCP agents compute explicit representations of this posterior, past work has found that both humans and suitably-trained neural networks can be modeled as Bayesian reasoners (Frank et al., 2009; Paige and Wood, 2016).\nThis provides a context-specific representation of belief, but for messages z and z\u2032 to have the same semantics, they must induce the same belief over all contexts in which they occur. In our probabilistic formulation, this introduces an outer expectation over contexts, providing a final measure q of the quality of a translation from z to z\u2032:\nq(z, z\u2032) = E [ DKL(\u03b2(z,Xb) || \u03b2(z\u2032, Xb)) | z, z\u2032 ] = \u2211 xa,xb p(xa, xb|z, z\u2032)DKL(\u03b2(z, xb) || \u03b2(z\u2032, xb))\n\u221d \u2211 xa,xb p(xa, xb) \u00b7 p(z|xa) \u00b7 p(z\u2032|xa)\n\u00b7 DKL(\u03b2(z, xb) || \u03b2(z\u2032, xb)); (1)\nAlgorithm 1 Translating messages\ngiven: a phrase inventory L function TRANSLATE(z)\nreturn argminz\u2032\u2208L q\u0302(z, z\u2032)\nfunction q\u0302(z, z\u2032) // sample contexts and distractors xai, xbi \u223c p(Xa, Xb) for i = 1..n x\u2032ai \u223c p(Xa|xbi) // compute context weights w\u0303i \u2190 p(z|xai) \u00b7 p(z\u2032|xai) wi \u2190 w\u0303i/ \u2211 j w\u0303j\n// compute divergences ki \u2190 \u2211 x\u2208{xa,x\u2032a} p(z|x) log p(z|x) p(z\u2032|x)\nreturn \u2211\niwiki\nrecalling that in this setting\nDKL(\u03b2 || \u03b2\u2032) = \u2211 xa p(xa|z, xb) log p(xa|z, xb) p(xa|z\u2032, xb)\n\u221d \u2211 xa p(xa|xb)p(z|xa) log p(z|xa) p(z\u2032|xa)\n(2)\nwhich is zero when the messages z and z\u2032 give rise to identical belief distributions and increases as they grow more dissimilar. To translate, we would like to compute tr(zr) = argminzh q(zr, zh) and tr(zh) = argminzr q(zh, zr). Intuitively, Equation 1 says that we will measure the quality of a proposed translation z 7\u2192 z\u2032 by asking the following question: in contexts where z is likely to be used, how frequently does z\u2032 induce the same belief about speaker states as z?\nWhile this translation criterion directly encodes the semantic notion of meaning described in Section 4, it is doubly intractable: the KL divergence and outer expectation involve a sum over all observations xa and xb respectively; these sums are not in general possible to compute efficiently. To avoid this, we approximate Equation 1 by sampling. We draw a collection of samples (xa, xb) from the prior over world states, and then generate for each sample a sequence of distractors (x\u2032a, xb) from p(x \u2032 a|xb) (we assume access to both of these distributions from the problem representation). The KL term in Equation 1 is computed over each true sample and its distractors, which are then normalized and averaged to compute the final score.\nSampling accounts for the outer p(xa, xb) in Equation 1 and the inner p(xa|xb) in Equation 2.\nThe only quantities remaining are of the form p(z|xa). In the case of neuralese, this distribution already is part of the definition of the agent policy \u03c0r and can be reused directly. For natural language, we use transcripts of human interactions to fit a model that maps from world states to a distribution over frequent utterances as discussed in Section 3. Details of these model implementations are provided in Appendix B, and the full translation procedure is given in Algorithm 1."}, {"heading": "6 Belief and behavior", "text": "The translation criterion in the previous section makes no reference to listener actions at all. The shapes example in Section 4 shows that some model performance might be lost under translation. It is thus reasonable to ask whether this translation model of Section 5 can make any guarantees about the effect of translation on behavior. In this section we explore the relationship between beliefpreserving translations and the behaviors they produce, by examining the effect of belief accuracy and strategy mismatch on the reward obtained by cooperating agents.\nTo facilitate this analysis, we consider a simplified family of communication games with the structure depicted in Figure 5. These games can be viewed as a subset of the family depicted in Figure 3; and consist of two steps: a listener makes an observation xa and sends a single message z to a speaker, which makes its own observation xb, takes a single action u, and receives a reward. We emphasize that the results in this section concern the theoretical properties of idealized games, and are presented to provide intuition about high-level properties of our approach. Section 8 investigates empirical behavior of this approach on real-world tasks where these ideal conditions do not hold.\nOur first result is that translations that minimize semantic dissimilarity q cause the listener to take near-optimal actions:2\n2Proof is provided in Appendix A.\nProposition 1. Semantic translations reward rational listeners. Define a rational listener as one that chooses the best action in expectation over the speaker\u2019s state:\nU(z, xb) = argmax u \u2211 xa p(xa|xb, z)r(xa, xb, u)\nfor a reward function r \u2208 [0, 1] that depends only on the two observations and the action.3 Now let a be a speaker of a language r, b be a listener of the same language r, and b\u2032 be a listener of a different language h. Suppose that we wish for a and b\u2032 to interact via the translator tr : zr 7\u2192 zh (so that a produces a message zr, and b\u2032 takes an action U(zh = tr(zr), xb\u2032)). If tr respects the semantics of zr, then the bilingual pair a and b\u2032 achieves only boundedly worse reward than the monolingual pair a and b. Specifically, if q(zr, zh) \u2264 D, then\nEr(Xa, Xb, U(tr(Z))\n\u2265 Er(Xa, Xb, U(Z))\u2212 \u221a 2D (3)\nSo as discussed in Section 4, even by committing to a semantic approach to meaning representation, we have still succeeded in (approximately) capturing the nice properties of the pragmatic approach.\nSection 4 examined the consequences of a mismatch between the set of primitives available in two languages. In general we would like some measure of our approach\u2019s robustness to the lack of an exact correspondence between two languages. In the case of humans in particular we expect that a variety of different strategies will be employed, many of which will not correspond to the behavior of the learned agent. It is natural to want some assurance that we can identify the DCP\u2019s strategy as long as some human strategy mirrors it. Our second observation is that it is possible to exactly recover a translation of a DCP strategy from a mixture of humans playing different strategies:\nProposition 2. Semantic translations find hidden correspondences. Consider a fixed robot policy \u03c0r and a set of human policies {\u03c0h1 , \u03c0h2 , . . . } (recalling from Section 3 that each \u03c0 is defined by distributions\n3This notion of rationality is a fairly weak one: it permits many suboptimal communication strategies, and requires only that the listener do as well as possible given a fixed speaker\u2014 a first-order optimality criterion likely to be satisfied by any richly-parameterized model trained via gradient descent.\np(z |xa) and p(u |z , xb)). Suppose further that the messages employed by these human strategies are disjoint; that is, if phi(z |xa) > 0, then phj (z |xa) = 0 for all j 6= i. Now suppose that all q(zr , zh) = 0 for all messages in the support of some phi(z |xa) and > 0 for all j 6= i. Then every message zr is translated into a message produced by \u03c0hi , and messages from other strategies are ignored.\nThis observation follows immediately from the definition of q(zr, zh), but demonstrates one of the key distinctions between our approach and a conventional machine translation criterion. Maximizing p(zh|zr) will produce the natural language message most often produced in contexts where zr is observed, regardless of whether that message is useful or informative. By contrast, minimizing q(zh, zr) will find the zh that corresponds most closely to zr even when zh is rarely used.\nThe disjointness condition, while seemingly quite strong, in fact arises naturally in many circumstances\u2014for example, players in the driving game reporting their spatial locations in absolute vs. relative coordinates, or speakers in a color reference game (Figure 6) discriminating based on lightness vs. hue. It is also possible to relax the above condition to require that strategies be only locally disjoint (i.e. with the disjointness condition holding for each fixed xa), in which case overlapping human strategies are allowed, and the recovered robot strategy is a context-weighted mixture of these."}, {"heading": "7 Evaluation", "text": ""}, {"heading": "7.1 Tasks", "text": "In the remainder of the paper, we evaluate the empirical behavior of our approach to translation. Our evaluation considers two kinds of tasks: reference games and navigation games. In a reference game (e.g. Figure 6a), both players observe a pair of candidate referents. A speaker is assigned a target referent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target. In this paper we consider two variants on the reference game: a simple color-naming task, and a more complex task involving natural images of birds. For examples of human communication strategies for these tasks, we obtain the XKCD color dataset (McMahan and Stone, 2015; Monroe et al., 2016) and the Caltech Birds dataset (Welinder et al., 2010) with accom-\npanying natural language descriptions (Reed et al., 2016). We use standard train / validation / test splits for both of these datasets.\nThe final task we consider is the driving task (Figure 6c) first discussed in the introduction. In this task, two cars, invisible to each other, must each navigate between randomly assigned start and goal positions without colliding. This task takes a number of steps to complete, and potentially involves a much broader range of communication strategies. To obtain human annotations for this task, we recorded both actions and messages generated by pairs of human Amazon Mechanical Turk workers playing the driving game with each other. We collected close to 400 games, with a total of more than 2000 messages exchanged, from which we held out 100 game traces as a test set."}, {"heading": "7.2 Metrics", "text": "A mechanism for understanding the behavior of a learned model should allow a human user both to correctly infer its beliefs and to successfully interoperate with it; we accordingly report results of both \u201cbelief\u201d and \u201cbehavior\u201d evaluations.\nTo support easy reproduction and comparison (and in keeping with standard practice in machine\ntranslation), we focus on developing automatic measures of system performance. We use the available training data to develop simulated models of human decisions; by first showing that these models track well with human judgments, we can be confident that their use in evaluations will correlate with human understanding. We employ the following two metrics:\nBelief evaluation This evaluation focuses on the denotational perspective in semantics that motivated the initial development of our model. We have successfully understood the semantics of a message zr if, after translating zr 7\u2192 zh, a human listener can form a correct belief about the state in which zr was produced. We construct a simple state-guessing game where the listener is presented with a translated message and two state observations, and must guess which state the speaker was in when the message was emitted.\nWhen translating from natural language to neuralese, we use the learned agent model to directly guess the hidden state. For neuralese to natural language we must first construct a \u201cmodel human listener\u201d to map from strings back to state representations; we do this by using the training data to fit a simple regression model that scores (state, sentence) pairs using a bag-of-words sentence representation. We find that our \u201cmodel human\u201d matches the judgments of real humans 83% of the time on the colors task, 77% of the time on the birds task, and 77% of the time on the driving task. This gives us confidence that the model human gives a reasonably accurate proxy for human interpretation.\nBehavior evaluation This evaluation focuses on the cooperative aspects of interpretability: we measure the extent to which learned models are able to interoperate with each other by way of a translation layer. In the case of reference games, the goal of this semantic evaluation is identical to the goal of the game itself (to identify the hidden state of the speaker), so we perform this additional pragmatic evaluation only for the driving game. We found that the most data-efficient and reliable way to make use of human game traces was to construct a \u201cdeaf\u201d model human. The evaluation selects a full game trace from a human player, and replays both the human\u2019s actions and messages exactly (disregarding any incoming messages); the evaluation measures the quality of the natural-language-toneuralese translator, and the extent to which the\nlearned agent model can accommodate a (real) human given translations of the human\u2019s messages.\nBaselines We compare our approach to two baselines: a random baseline that chooses a translation of each input uniformly from messages observed during training, and a direct baseline that directly maximizes p(z\u2032|z) (by analogy to a conventional machine translation system). This is accomplished by sampling from a DCP speaker in training states labeled with natural language strings."}, {"heading": "8 Results", "text": "In all below, \u201cR\u201d indicates a DCP agent, \u201cH\u201d indicates a real human, and \u201cH*\u201d indicates a model human player.\nReference games Results for the two reference games are shown in Table 1. The end-to-end trained model achieves nearly perfect accuracy in both\ncases, while a model trained to communicate in natural language achieves somewhat lower performance. Regardless of whether the speaker is a DCP and the listener a model human or vice-versa, translation based on the belief-matching criterion in Section 5 achieves the best performance; indeed, when translating neuralese color names to natural language, the listener is able to achieve a slightly higher score than it is natively. This suggests that the automated agent has discovered a more effective strategy than the one demonstrated by humans in the dataset, and that the effectiveness of this strategy is preserved by translation. Example translations from the reference games are depicted in Figure 2 and Figure 7.\nDriving game Behavior evaluation of the driving game is shown in Table 3, and belief evaluation is shown in Table 2. Translation of messages in the driving game is considerably more challenging than in the reference games, and scores are uniformly lower; however, a clear benefit from the beliefmatching model is still visible. Belief matching leads to higher scores on the belief evaluation in both directions, and allows agents to obtain a higher reward on average (though task completion rates remain roughly the same across all agents). Some example translations of driving game messages are shown in Figure 8."}, {"heading": "9 Conclusion", "text": "We have investigated the problem of interpreting message vectors from deep networks by translating them. After introducing a translation criterion based on matching listener beliefs about speaker states, we presented both theoretical and empirical evidence that this criterion outperforms a conventional machine translation approach at recovering the content of message vectors and facilitating collaboration between humans and learned agents.\nWhile our evaluation has focused on understanding the behavior of deep communicating policies, the framework proposed in this paper could be much more generally applied. Any encoder\u2013 decoder model (Sutskever et al., 2014) can be thought of as a kind of communication game played between the encoder and the decoder, so we can analogously imagine computing and translating \u201cbeliefs\u201d induced by the encoding to explain what features of the input are being transmitted. The current work has focused on learning a purely categorical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch. More broadly, the work here shows that the denotational perspective from formal semantics provides a framework for precisely framing the demands of interpretable machine learning (Wilson et al., 2016), and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors."}, {"heading": "Acknowledgments", "text": "JA is supported by a Facebook Graduate Fellowship and a Berkeley AI / Huawei Fellowship. We are grateful to Lisa Anne Hendricks for assistance with the Caltech Birds dataset."}, {"heading": "A Proofs", "text": "Proof of Proposition 1 We know that\nU(z, xb) := argmax u \u2211 xa p(xa|xb, z)r(xa, xb, z)\nand that for all translations (z, z\u2032 = t(r)) D \u2265 \u2211 xb p(xb|z, z\u2032)DKL(\u03b2(z, xb) || \u03b2(z\u2032, xb)) .\nApplying Pinsker\u2019s inequality: \u2265 2 \u2211 xb p(xb|z, z\u2032)\u03b4(\u03b2(z, xb), \u03b2(z\u2032, xb))2\nand Jensen\u2019s inequality: \u2265 2 (\u2211\nxb\np(xb|z, z\u2032)\u03b4(\u03b2(z, xb), \u03b2(z\u2032, xb))) )2\nso \u221a D/2 \u2265 \u2211 xb p(xb|z, z\u2032)\u03b4(\u03b2(z, xb), \u03b2(z\u2032, xb)) .\nThe next step relies on the following well-known property of the total variation distance: for distributions p and q and a function f bounded by [0, 1],\n|Epf(x)\u2212 Eqf(x)| \u2264 \u03b4(p, q) . (*)\nFor convenience we will write\n\u03b4 := \u03b4(\u03b2(z, xb), \u03b2(z \u2032, xb)) .\nA listener using the speaker\u2019s language expects a reward of\u2211 xb p(xb) \u2211 xa p(xa|xb, z)r(xa, xb, U(z, xb))\n\u2264 \u2211 xb p(xb) (\u2211 xa p(xa|xb, z\u2032)r(xa, xb, U(z, xb)) + \u03b4 )\nvia (*). From the assumption of player rationality:\n\u2264 \u2211 xb p(xb) (\u2211 xa p(xa|xb, z\u2032)r(xa, xb, U(z\u2032, xb)) + \u03b4 )\nusing (*) again:\n\u2264 \u2211 xb p(xb) (\u2211 xa p(xa|xb, z)r(xa, xb, U(z\u2032, xb)) + 2\u03b4 )\n\u2264 \u2211 xa,xb p(xa, xb|z)r(xa, xb, U(z\u2032, xb)) + \u221a 2D .\nSo the true reward achieved by a z\u2032-speaker receiving a translated code is only additively worse than the native z-speaker reward:( \u2211\nxa,xb\np(xa, xb|z)r(xa, xb, U(z, xb)) ) \u2212 \u221a 2D\nB Implementation details\nB.1 Agents\nLearned agents have the following form:\nx(t)a\nz (t 1) b\nh(t 1)a h (t) a\nu(t)a\nz(t)aMLP GRU\nwhere h is a hidden state, z is a message from the other agent, u is a distribution over actions, and x is an observation of the world. A single hidden layer with 256 units and a tanh nonlinearity is used for the MLP. The GRU hidden state is also of size 256, and the message vector is of size 64.\nAgents are trained via interaction with the world as in Hausknecht and Stone (2015) using the ADAM optimizer (Kingma and Ba, 2014) and a discount factor of 0.9. The step size was chosen as 0.003 for reference games and 0.0003 for the driving game. An -greedy exploration strategy is employed, with the exploration parameter for timestep t given by:\n= max  (1000\u2212 t)/1000 (5000\u2212 t)/50000 0\nAs in Foerster et al. (2016), we found it useful to add noise to the communication channel: in this case, isotropic Gaussian noise with mean 0 and standard deviation 0.3. This also helps smooth p(z|xa) when computing the translation criterion.\nB.2 Representational models\nAs discussed in Section 5, the translation criterion is computed based on the quantity p(z|x). The policy representation above actually defines a distribution p(z|x, h), additionally involving the agent\u2019s hidden state h from a previous timestep. While in principle it is possible to eliminate the dependence on h by introducing an additional sampling step into Algorithm 1, we found that it simplified inference to simply learn an additional model of p(z|x) directly. This model is trained alongside the learned agent to imitate its decisions, but does not get to observe the recurrent state, like so:\nx(t)a\nz (t 1) b z(t)aMLP\nHere the multilayer perceptron has a single hidden layer with tanh nonlinearities and size 128. It is also trained with ADAM and a step size of 0.0003.\nWe use exactly the same model and parameters to implement representations of p(z|x) for human speakers, but in this case the vector z is taken to be a distribution over messages in the natural language inventory, and the model is trained to maximize the likelihood of labeled human traces.\nB.3 Tasks Colors We use the version of the XKCD dataset prepared by McMahan and Stone (2015). Here the input feature vector is simply the LAB representation of each color, and the message inventory taken to be all unigrams that appear at least five times.\nBirds We use the dataset of Welinder et al. (2010) with natural language annotations from Reed et al. (2016). The model\u2019s input feature representations are a final 256-dimensional hidden feature vector from a compact bilinear pooling model (Gao et al., 2016) pre-trained for classification. The message inventory consists of the 50 most frequent bigrams to appear in natural language descriptions; example human traces are generated by for every frequent (bigram, image) pair in the dataset.\nDriving Driving data is collected from pairs of human workers on Mechanical Turk. Workers received the following description of the task:\nYour goal is to drive the red car onto the red square. Be careful! You\u2019re driving in a thick fog, and there is another car on the road that you cannot see. However, you can talk to the other driver to make sure you both reach your destinations safely.\nPlayers were restricted to messages of 1\u20133 words, and required to send at least one message per game. Each player was paid $0.25 per game. 382 games were collected with 5 different road layouts, each represented as an 8x8 grid presented to players as in Figure 8. The action space is discrete: players can move forward, back, turn left, turn right, or wait. These were divided into a 282-game training set and 100-game test set. The message inventory consists of all messages sent more than 3 times. Input features consists of indicators on the agent\u2019s current position and orientation, goal position, and map identity. Data is available for download at http://github.com/jacobandreas/neuralese."}], "references": [{"title": "Reasoning about pragmatics with neural listeners and speakers", "author": ["Jacob Andreas", "Dan Klein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Andreas and Klein.,? 2016", "shortCiteRegEx": "Andreas and Klein.", "year": 2016}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["Daniel S Bernstein", "Robert Givan", "Neil Immerman", "Shlomo Zilberstein."], "venue": "Mathematics of operations research 27(4):819\u2013840.", "citeRegEx": "Bernstein et al\\.,? 2002", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Optimally solving Dec-POMDPs as continuous-state MDPs", "author": ["Jilles Steeve Dibangoye", "Christopher Amato", "Olivier Buffet", "Fran\u00e7ois Charpillet."], "venue": "Journal of Artificial Intelligence Research 55:443\u2013 497.", "citeRegEx": "Dibangoye et al\\.,? 2016", "shortCiteRegEx": "Dibangoye et al\\.", "year": 2016}, {"title": "Generating legible motion", "author": ["Anca Dragan", "Siddhartha Srinivasa."], "venue": "Robotics: Science and Systems.", "citeRegEx": "Dragan and Srinivasa.,? 2013", "shortCiteRegEx": "Dragan and Srinivasa.", "year": 2013}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["Jakob Foerster", "Yannis M Assael", "Nando de Freitas", "Shimon Whiteson."], "venue": "Advances in Neural Information Processing Systems. pages 2137\u20132145.", "citeRegEx": "Foerster et al\\.,? 2016", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Informative communication in word production and word learning", "author": ["Michael C Frank", "Noah D Goodman", "Peter Lai", "Joshua B Tenenbaum."], "venue": "Proceedings of the 31st annual conference of the cognitive science society. pages 1228\u20131233.", "citeRegEx": "Frank et al\\.,? 2009", "shortCiteRegEx": "Frank et al\\.", "year": 2009}, {"title": "Compact bilinear pooling", "author": ["Yang Gao", "Oscar Beijbom", "Ning Zhang", "Trevor Darrell."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 317\u2013326.", "citeRegEx": "Gao et al\\.,? 2016", "shortCiteRegEx": "Gao et al\\.", "year": 2016}, {"title": "A paradigm for situated and goal-driven language learning", "author": ["Jon Gauthier", "Igor Mordatch."], "venue": "arXiv preprint arXiv:1610.03585 .", "citeRegEx": "Gauthier and Mordatch.,? 2016", "shortCiteRegEx": "Gauthier and Mordatch.", "year": 2016}, {"title": "Denotational semantics for agent communication language", "author": ["Frank Guerin", "Jeremy Pitt."], "venue": "Proceedings of the fifth international conference on Autonomous agents. ACM, pages 497\u2013504.", "citeRegEx": "Guerin and Pitt.,? 2001", "shortCiteRegEx": "Guerin and Pitt.", "year": 2001}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Matthew Hausknecht", "Peter Stone."], "venue": "arXiv preprint arXiv:1507.06527 .", "citeRegEx": "Hausknecht and Stone.,? 2015", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Generating visual explanations", "author": ["Lisa Anne Hendricks", "Zeynep Akata", "Marcus Rohrbach", "Jeff Donahue", "Bernt Schiele", "Trevor Darrell."], "venue": "European Conference on Computer Vision. Springer, pages 3\u201319.", "citeRegEx": "Hendricks et al\\.,? 2016", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "ReferItGame: Referring to objects in photographs of natural scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L Berg."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 787\u2013798.", "citeRegEx": "Kazemzadeh et al\\.,? 2014", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["Angeliki Lazaridou", "Alexander Peysakhovich", "Marco Baroni."], "venue": "arXiv preprint arXiv:1612.07182 .", "citeRegEx": "Lazaridou et al\\.,? 2016a", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2016}, {"title": "Towards multi-agent communication-based language learning", "author": ["Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni."], "venue": "arXiv preprint arXiv:1605.07133 .", "citeRegEx": "Lazaridou et al\\.,? 2016b", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2016}, {"title": "A Bayesian model of grounded color semantics", "author": ["Brian McMahan", "Matthew Stone."], "venue": "Transactions of the Association for Computational Linguistics 3:103\u2013115.", "citeRegEx": "McMahan and Stone.,? 2015", "shortCiteRegEx": "McMahan and Stone.", "year": 2015}, {"title": "Learning to generate compositional color descriptions", "author": ["Will Monroe", "Noah D Goodman", "Christopher Potts."], "venue": "arXiv preprint arXiv:1606.03821 .", "citeRegEx": "Monroe et al\\.,? 2016", "shortCiteRegEx": "Monroe et al\\.", "year": 2016}, {"title": "Inference networks for sequential monte carlo in graphical models", "author": ["Brooks Paige", "Frank Wood."], "venue": "volume 48.", "citeRegEx": "Paige and Wood.,? 2016", "shortCiteRegEx": "Paige and Wood.", "year": 2016}, {"title": "Inferring logical forms from denotations", "author": ["Panupong Pasupat", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.06900 .", "citeRegEx": "Pasupat and Liang.,? 2016", "shortCiteRegEx": "Pasupat and Liang.", "year": 2016}, {"title": "Learning deep representations of fine-grained visual descriptions", "author": ["Scott Reed", "Zeynep Akata", "Honglak Lee", "Bernt Schiele."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 49\u201358.", "citeRegEx": "Reed et al\\.,? 2016", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Why should I trust you?: Explaining the predictions of any classifier", "author": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin."], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "Ribeiro et al\\.,? 2016", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Reasoning about joint beliefs for executiontime communication decisions", "author": ["Maayan Roth", "Reid Simmons", "Manuela Veloso."], "venue": "Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems. ACM,", "citeRegEx": "Roth et al\\.,? 2005", "shortCiteRegEx": "Roth et al\\.", "year": 2005}, {"title": "Visual analysis of hidden state dynamics in recurrent neural networks", "author": ["Hendrik Strobelt", "Sebastian Gehrmann", "Bernd Huber", "Hanspeter Pfister", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.07461 .", "citeRegEx": "Strobelt et al\\.,? 2016", "shortCiteRegEx": "Strobelt et al\\.", "year": 2016}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sainbayar Sukhbaatar", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sukhbaatar and Fergus,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar and Fergus", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context-aware captions from context-agnostic supervision", "author": ["Ramakrishna Vedantam", "Samy Bengio", "Kevin Murphy", "Devi Parikh", "Gal Chechik."], "venue": "arXiv preprint arXiv:1701.02870 .", "citeRegEx": "Vedantam et al\\.,? 2017", "shortCiteRegEx": "Vedantam et al\\.", "year": 2017}, {"title": "Emergence of Gricean maxims from multi-agent decision theory", "author": ["Adam Vogel", "Max Bodoia", "Christopher Potts", "Daniel Jurafsky."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Asso-", "citeRegEx": "Vogel et al\\.,? 2013a", "shortCiteRegEx": "Vogel et al\\.", "year": 2013}, {"title": "Implicatures and nested beliefs in approximate Decentralized-POMDPs", "author": ["Adam Vogel", "Christopher Potts", "Dan Jurafsky."], "venue": "ACL (2). pages 74\u201380.", "citeRegEx": "Vogel et al\\.,? 2013b", "shortCiteRegEx": "Vogel et al\\.", "year": 2013}, {"title": "Caltech-UCSD Birds 200", "author": ["P. Welinder", "S. Branson", "T. Mita", "C. Wah", "F. Schroff", "S. Belongie", "P. Perona."], "venue": "Technical Report CNS-TR-2010-001, California Institute of Technology.", "citeRegEx": "Welinder et al\\.,? 2010", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "Proceedings of nips 2016 workshop on interpretable machine learning for complex systems", "author": ["Andrew Gordon Wilson", "Been Kim", "William Herlands."], "venue": "arXiv preprint arXiv:1611.09139 .", "citeRegEx": "Wilson et al\\.,? 2016", "shortCiteRegEx": "Wilson et al\\.", "year": 2016}, {"title": "A joint speaker-listener-reinforcer model for referring expressions", "author": ["Licheng Yu", "Hao Tan", "Mohit Bansal", "Tamara L Berg."], "venue": "arXiv preprint arXiv:1612.09542 .", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus."], "venue": "European conference on computer vision. Springer, pages 818\u2013833.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "DCPs have been shown to solve a variety of coordination problems, including reference games (Lazaridou et al., 2016b), logic puzzles (Foerster et al.", "startOffset": 92, "endOffset": 117}, {"referenceID": 5, "context": ", 2016b), logic puzzles (Foerster et al., 2016), and simple control (Sukhbaatar et al.", "startOffset": 24, "endOffset": 47}, {"referenceID": 22, "context": "backpropagation through the communication channel, avoiding many of the challenging inference problems associated with learning in classical decentralized decision processes (Roth et al., 2005).", "startOffset": 174, "endOffset": 193}, {"referenceID": 23, "context": "Natural language already provides a rich set of tools for describing beliefs, observations, and plans\u2014our thesis is that these tools provide a useful complement to the visualization and ablation techniques used in previous work on understanding complex models (Strobelt et al., 2016; Ribeiro et al., 2016).", "startOffset": 260, "endOffset": 305}, {"referenceID": 21, "context": "Natural language already provides a rich set of tools for describing beliefs, observations, and plans\u2014our thesis is that these tools provide a useful complement to the visualization and ablation techniques used in previous work on understanding complex models (Strobelt et al., 2016; Ribeiro et al., 2016).", "startOffset": 260, "endOffset": 305}, {"referenceID": 13, "context": "We have broadly labeled these as \u201cdeep communicating policies\u201d; concrete examples include Lazaridou et al. (2016b), Foerster et al.", "startOffset": 90, "endOffset": 115}, {"referenceID": 5, "context": "(2016b), Foerster et al. (2016), and Sukhbaatar et al.", "startOffset": 9, "endOffset": 32}, {"referenceID": 5, "context": "(2016b), Foerster et al. (2016), and Sukhbaatar et al. (2016). The policy representation we employ in this paper is similar to the latter two of these, although the general framework is agnostic to low-level modeling details and could be straightforwardly applied to other architectures.", "startOffset": 9, "endOffset": 62}, {"referenceID": 5, "context": "(2016b), Foerster et al. (2016), and Sukhbaatar et al. (2016). The policy representation we employ in this paper is similar to the latter two of these, although the general framework is agnostic to low-level modeling details and could be straightforwardly applied to other architectures. Analysis of communication strategies in all these papers has been largely adhoc, obtained by clustering states from which similar messages are emitted and attempting to manually assign semantics to these clusters. The present work aims at developing tools for performing this analysis automatically. Most closely related to our approach is that of Lazaridou et al. (2016a), who also develop a model for assigning natural language interpretations to learned messages; however, this approach relies on supervised cluster labels and is targeted specifically towards referring expression games.", "startOffset": 9, "endOffset": 661}, {"referenceID": 1, "context": "The literature on learning decentralized multiagent policies in general is considerably larger (Bernstein et al., 2002; Dibangoye et al., 2016).", "startOffset": 95, "endOffset": 143}, {"referenceID": 3, "context": "The literature on learning decentralized multiagent policies in general is considerably larger (Bernstein et al., 2002; Dibangoye et al., 2016).", "startOffset": 95, "endOffset": 143}, {"referenceID": 22, "context": "This includes work focused on communication in multiagent settings (Roth et al., 2005) and even communication using natural language messages (Vogel et al.", "startOffset": 67, "endOffset": 86}, {"referenceID": 28, "context": ", 2005) and even communication using natural language messages (Vogel et al., 2013b).", "startOffset": 63, "endOffset": 84}, {"referenceID": 4, "context": "On the control side, a long line of work considers nonverbal communication strategies in multiagent policies (Dragan and Srinivasa, 2013).", "startOffset": 109, "endOffset": 137}, {"referenceID": 32, "context": "This includes both visualization techniques (Zeiler and Fergus, 2014; Strobelt et al., 2016), and approaches focused on generating explanations in the form of natural language (Hendricks et al.", "startOffset": 44, "endOffset": 92}, {"referenceID": 23, "context": "This includes both visualization techniques (Zeiler and Fergus, 2014; Strobelt et al., 2016), and approaches focused on generating explanations in the form of natural language (Hendricks et al.", "startOffset": 44, "endOffset": 92}, {"referenceID": 11, "context": ", 2016), and approaches focused on generating explanations in the form of natural language (Hendricks et al., 2016; Vedantam et al., 2017).", "startOffset": 91, "endOffset": 138}, {"referenceID": 26, "context": ", 2016), and approaches focused on generating explanations in the form of natural language (Hendricks et al., 2016; Vedantam et al., 2017).", "startOffset": 91, "endOffset": 138}, {"referenceID": 0, "context": "The literature on learning decentralized multiagent policies in general is considerably larger (Bernstein et al., 2002; Dibangoye et al., 2016). This includes work focused on communication in multiagent settings (Roth et al., 2005) and even communication using natural language messages (Vogel et al., 2013b). All of these approaches employ structured communication schemes with manually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference. Our evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game. Communication strategies for reference games were previously explored by Vogel et al. (2013a), Andreas and Klein (2016) and Kazemzadeh et al.", "startOffset": 96, "endOffset": 832}, {"referenceID": 0, "context": "(2013a), Andreas and Klein (2016) and Kazemzadeh et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 0, "context": "(2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference games specifically featuring end-to-end communication protocols by Yu et al.", "startOffset": 9, "endOffset": 63}, {"referenceID": 0, "context": "(2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference games specifically featuring end-to-end communication protocols by Yu et al. (2016). On the control side, a long line of work considers nonverbal communication strategies in multiagent policies (Dragan and Srinivasa, 2013).", "startOffset": 9, "endOffset": 162}, {"referenceID": 10, "context": "Here each agent policy is represented as a deep recurrent Q network (Hausknecht and Stone, 2015).", "startOffset": 68, "endOffset": 96}, {"referenceID": 2, "context": "MLP denotes a multilayer perceptron; GRU denotes a gated recurrent unit (Cho et al., 2014).", "startOffset": 72, "endOffset": 90}, {"referenceID": 4, "context": "(2016) and Foerster et al. (2016)).", "startOffset": 11, "endOffset": 34}, {"referenceID": 8, "context": "Examples of this approach include Guerin and Pitt (2001) and Pasupat and Liang (2016).", "startOffset": 34, "endOffset": 57}, {"referenceID": 8, "context": "Examples of this approach include Guerin and Pitt (2001) and Pasupat and Liang (2016). Pragmatic representation The meaning of a message za is given by the behavior it induces in a listener.", "startOffset": 34, "endOffset": 86}, {"referenceID": 8, "context": "Examples of this approach include Guerin and Pitt (2001) and Pasupat and Liang (2016). Pragmatic representation The meaning of a message za is given by the behavior it induces in a listener. In probabilistic terms, this says that the meaning of a message za is represented by the distribution p(ub|za, xb) it induces over actions given the listener\u2019s observation xb. Examples of this approach include Vogel et al. (2013a) and Gauthier and Mordatch (2016).", "startOffset": 34, "endOffset": 422}, {"referenceID": 8, "context": "(2013a) and Gauthier and Mordatch (2016).", "startOffset": 12, "endOffset": 41}, {"referenceID": 6, "context": "While in general neither humans nor DCP agents compute explicit representations of this posterior, past work has found that both humans and suitably-trained neural networks can be modeled as Bayesian reasoners (Frank et al., 2009; Paige and Wood, 2016).", "startOffset": 210, "endOffset": 252}, {"referenceID": 18, "context": "While in general neither humans nor DCP agents compute explicit representations of this posterior, past work has found that both humans and suitably-trained neural networks can be modeled as Bayesian reasoners (Frank et al., 2009; Paige and Wood, 2016).", "startOffset": 210, "endOffset": 252}, {"referenceID": 16, "context": "For examples of human communication strategies for these tasks, we obtain the XKCD color dataset (McMahan and Stone, 2015; Monroe et al., 2016) and the Caltech Birds dataset (Welinder et al.", "startOffset": 97, "endOffset": 143}, {"referenceID": 17, "context": "For examples of human communication strategies for these tasks, we obtain the XKCD color dataset (McMahan and Stone, 2015; Monroe et al., 2016) and the Caltech Birds dataset (Welinder et al.", "startOffset": 97, "endOffset": 143}, {"referenceID": 29, "context": ", 2016) and the Caltech Birds dataset (Welinder et al., 2010) with accom(a) (b)", "startOffset": 38, "endOffset": 61}, {"referenceID": 20, "context": "panying natural language descriptions (Reed et al., 2016).", "startOffset": 38, "endOffset": 57}, {"referenceID": 25, "context": "Any encoder\u2013 decoder model (Sutskever et al., 2014) can be thought of as a kind of communication game played between the encoder and the decoder, so we can analogously imagine computing and translating \u201cbeliefs\u201d induced by the encoding to explain what features of the input are being transmitted.", "startOffset": 27, "endOffset": 51}, {"referenceID": 30, "context": "More broadly, the work here shows that the denotational perspective from formal semantics provides a framework for precisely framing the demands of interpretable machine learning (Wilson et al., 2016), and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors.", "startOffset": 179, "endOffset": 200}], "year": 2017, "abstractText": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents\u2019 messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.1", "creator": "LaTeX with hyperref package"}}}