{"id": "1704.06970", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Differentiable Scheduled Sampling for Credit Assignment", "abstract": "we demonstrate that a continuous relaxation of by the argmax operation can be used to create a differentiable time approximation to greedy decoding for sequence - to - sequence ( seq2seq ) models. by just incorporating this approximation time into the scheduled sampling training procedure ( bengio et al., holm 2015 ) - - a well - known technique for correcting exposure bias - - we introduce a new training objective tree that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their preferred value. in addition, directly by using a related approximation, we demonstrate a similar approach to employing sampled - based ensemble training. finally, we later show that primarily our approach outperforms typical cross - entropy training and scheduled sampling procedures in two sequence prediction tasks : named entity template recognition and machine data translation.", "histories": [["v1", "Sun, 23 Apr 2017 20:05:36 GMT  (533kb,D)", "http://arxiv.org/abs/1704.06970v1", "Accepted at ACL2017 (this http URL)"]], "COMMENTS": "Accepted at ACL2017 (this http URL)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["kartik goyal", "chris dyer", "taylor berg-kirkpatrick"], "accepted": true, "id": "1704.06970"}, "pdf": {"name": "1704.06970.pdf", "metadata": {"source": "CRF", "title": "Differentiable Scheduled Sampling for Credit Assignment", "authors": ["Kartik Goyal", "Chris Dyer"], "emails": ["kartikgo@cs.cmu.edu", "cdyer@google.com", "tberg@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Sequence-to-Sequence (seq2seq) models have demonstrated excellent performance in several tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), dialogue generation (Serban et al., 2015), and image captioning (Xu et al., 2015). However, the standard cross-entropy training procedure for these models suffers from the well-known problem of exposure bias: because cross-entropy training always uses gold contexts, the states and contexts encountered during training do not match those encountered at test time. This issue has been addressed using several approaches that try to incorporate awareness of decoding choices into the training optimization. These include reinforcement learning (Ranzato et al., 2016; Bahdanau\net al., 2017), imitation learning (Daume\u0301 et al., 2009; Ross et al., 2011; Bengio et al., 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al., 2016; Daume\u0301 III and Marcu, 2005). In this paper, we focus on one the simplest to implement and least computationally expensive approaches, scheduled sampling (Bengio et al., 2015), which stochastically incorporates contexts from previous decoding decisions into training.\nWhile scheduled sampling has been empirically successful, its training objective has a drawback: because the procedure directly incorporates greedy decisions at each time step, the objective is discontinuous at parameter settings where previous decisions change their value. As a result, gradients near these points are non-informative and scheduled sampling has difficulty assigning credit for errors. In particular, the gradient does not provide information useful in distinguishing between local errors without future consequences and cascading errors which are more serious.\nHere, we propose a novel approach based on scheduled sampling that uses a differentiable approximation of previous greedy decoding decisions inside the training objective by incorporating a continuous relaxation of argmax. As a result, our end-to-end relaxed greedy training objective is differentiable everywhere and fully continuous. By making the objective continuous at points where previous decisions change value, our approach provides gradients that can respond to cascading errors. In addition, we demonstrate a related approximation and reparametrization for sample-based training (another training scenario considered by scheduled sampling (Bengio et al., 2015)) that can yield stochastic gradients with lower variance than in standard scheduled sampling. In our experiments on two different tasks, machine translation (MT) and named entity recognition (NER), we show that our approach outperforms both cross-entropy training and standard ar X iv :1 70 4.\n06 97\n0v 1\n[ cs\n.C L\n] 2\n3 A\npr 2\n01 7\nscheduled sampling procedures with greedy and sampled-based training."}, {"heading": "2 Discontinuity in Scheduled Sampling", "text": "While scheduled sampling (Bengio et al., 2015) is an effective way to rectify exposure bias, it cannot differentiate between cascading errors, which can lead to a sequence of bad decisions, and local errors, which have more benign effects. Specifically, scheduled sampling focuses on learning optimal behavior in the current step given the fixed decoding decision of the previous step. If a previous bad decision is largely responsible for the current error, the training procedure has difficulty adjusting the parameters accordingly. The following machine translation example highlights this credit assignment issue:\nRef: The cat purrs . Pred: The dog barks .\nAt step 3, the model prefers the word \u2018barks\u2019 after incorrectly predicting \u2018dog\u2019 at step 2. To correct this error, the scheduled sampling procedure would increase the score of \u2018purrs\u2019 at step 3, conditioned on the fact that the model predicted (incorrectly) \u2018dog\u2019 at step 2, which is not the ideal learning behaviour. Ideally, the model should be able to backpropagate the error from step 3 to the source of the problem which occurred at step 2, where \u2018dog\u2019 was predicted instead of \u2018cat\u2019.\nThe lack of credit assignment during training is a result of discontinuity in the objective function used by scheduled sampling, as illustrated in Figure 1. We denote the ground truth target symbol at step i by y\u2217i , the embedding representation of word y by e(y), and the hidden state of a seq2seq decoder at step i as hi. Standard crossentropy training defines the loss at each step to be log p(y\u2217i |hi(e(y\u2217i\u22121), hi\u22121)), while scheduled sampling uses loss log p(y\u2217i |hi(e(y\u0302i\u22121), hi\u22121)), where\ny\u0302i\u22121 refers the model\u2019s prediction at the previous step.1 Here, the model prediction y\u0302i\u22121 is obtained by argmaxing over the output softmax layer. Hence, in addition to the intermediate hidden states and final softmax scores, the previous model prediction, y\u0302i\u22121, itself depends on the model parameters, \u03b8, and ideally, should be backpropagated through, unlike the gold target symbol y\u2217i\u22121 which is independent of model parameters. However, the argmax operation is discontinuous, and thus the training objective (depicted in Figure 1 as the red line) exhibits discontinuities at parameter settings where the previous decoding decisions change value (depicted as changes from \u2018kitten\u2019 to \u2018dog\u2019 to \u2018cat\u2019). Because these change points represent discontinuities, their gradients are undefined and the effect of correcting an earlier mistake (for example \u2018dog\u2019 to \u2018cat\u2019) as the training procedure approaches such a point is essentially hidden.\nIn our approach, described in detail in the next section, we attempt to fix this problem by incorporating a continuous relaxation of the argmax operation into the scheduled sampling procedure in order to form an approximate but fully continuous objective. Our relaxed approximate objective is depicted in Figure 1 as blue and purple lines, depending on temperature parameter \u03b1which tradesoff smoothness and quality of approximation."}, {"heading": "3 Credit Assignment via Relaxation", "text": "In this section we explain in detail the continuous relaxation of greedy decoding that we will use to build a fully continuous training objective. We also introduce a related approach for sample-based training.\n1For the sake of simplicity, the \u2018always sample\u2019 variant of scheduled sampling is described (Bengio et al., 2015)."}, {"heading": "3.1 Soft Argmax", "text": "In scheduled sampling, the embedding for the best scoring word at the previous step is passed as an input to the current step. This operation2 can be expressed as\ne\u0302i\u22121 = \u2211\ny\ne(y)1[\u2200y\u2032 6= y si\u22121(y) > si\u22121(y\u2032)]\nwhere y is a word in the vocabulary, si\u22121(y) is the output score of that word at the previous step, and e\u0302i\u22121 is the embedding passed to the next step. This operation can be relaxed by replacing the indicator function with a peaked softmax function with hyperparameter \u03b1 to define a soft argmax procedure:\ne\u0304i\u22121 = \u2211\ny\ne(y) \u00b7 exp (\u03b1 si\u22121(y))\u2211 y\u2032 exp (\u03b1 si\u22121(y \u2032))\nAs \u03b1 \u2192 \u221e, the equation above approaches the true argmax embedding. Hence, with a finite and large \u03b1, we get a linear combination of all the words (and therefore a continuous function of the parameters) that is dominated heavily by the word with maximum score."}, {"heading": "3.2 Soft Reparametrized Sampling", "text": "Another variant of scheduled sampling is to pass a sampled embedding from the softmax distribution at the previous step to the current step instead of the argmax. This is expected to enable better exploration of the search space during optimization due to the added randomness and hence result in a more robust model. In this section, we discuss and review an approximation to the Gumbel reparametrization trick that we use as a module in our sample-based decoder. This approximation was proposed by Maddison et al. (2017) and Jang et al. (2016), who showed that the same soft argmax operation introduced above can be used for reducing variance of stochastic gradients when sampling from softmax distributions. Unlike soft argmax, this approach is not a fully continuous approximation to the sampling operation, but it does result in much more informative gradients compared to naive scheduled sampling procedure.\nThe Gumbel reparametrization trick shows that sampling from a categorical distribution can be refactored into sampling from a simple distribution followed by a deterministic transformation\n2Assuming there are no ties for the sake of simplicity.\nas follows: (i) sampling an independent Gumbel noise G for each element in the categorical distribution, typically done by transforming a sample from the uniform distribution: U \u223c Uniform(0, 1) as G = \u2212log(\u2212log U), then (ii) adding it componentwise to the unnormalized score of each element, and finally (iii) taking an argmax over the vector. Using the same argmax softening procedure as above, they arrive at an approximation to the reparametrization trick which mitigates some of the gradient\u2019s variance introduced by sampling. The approximation is3:\ne\u0303i\u22121 = \u2211\ny\ne(y) \u00b7 exp (\u03b1 (si\u22121(y) +Gy))\u2211 y\u2032 exp (\u03b1 (si\u22121(y \u2032) +Gy\u2032))\nWe will use this \u2018concrete\u2019 approximation of softmax sampling in our relaxation of scheduled sampling with a sample-based decoder. We discuss details in the next section. Note that our original motivation based on removing discontinuity does not strictly apply to this sampling procedure, which still yields a stochastic gradient due to sampling from the Gumbel distribution. However, this approach is conceptually related to greedy relaxations since, here, the soft argmax reparametrization reduces gradient variance which may yield a more informative training signal. Intuitively, this approach results in the gradient of the loss to be more aware of the sampling procedure compared to naive scheduled sampling and hence carries forward information about decisions made at previous steps. The empirical results, discussed later, show similar gains to the greedy scenario."}, {"heading": "3.3 Differentiable Relaxed Decoders", "text": "With the argmax relaxation introduced above, we have a recipe for a fully differentiable greedy decoder designed to produce informative gradients near change points. Our final training network for scheduled sampling with relaxed greedy decoding is shown in Figure 2. Instead of conditioning the current hidden state, hi, on the argmax embedding from the previous step, e\u0302i\u22121, we use the \u03b1-soft argmax embedding, e\u0304i\u22121, defined in Section 3.1. This removes the discontinuity in the original greedy scheduled sampling objective by passing a linear combination of embeddings, dominated by the argmax, to the next step. Figure 1\n3This is different from using the expected softmax embedding because our approach approximates the actual sampling process instead of linearly weighting the embeddings by their softmax probabilities\nillustrates the effect of varying \u03b1. As \u03b1 increases, we more closely approximate the greedy decoder.\nAs in standard scheduled sampling, here we minimize the cross-entropy based loss at each time step. Hence the computational complexity of our approach is comparable to standard seq2seq training. As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training (Bengio et al., 2015; Daume\u0301 et al., 2009; Ross et al., 2011), while annealing the probability of using the ground truth with each epoch, results in better models and more stable training. As a result, training is reliant on the annealing schedule of two important hyperparameters: i) ground truth mixing probability and ii) the \u03b1 parameter used for approximating the argmax function. For output prediction, at each time step, we can still output the hard argmax, depicted in Figure 2.\nFor the case of scheduled sampling with sample-based training\u2013where decisions are sampled rather than chosen greedily (Bengio et al., 2015)\u2013we conduct experiments using a related training procedure. Instead of using soft argmax, we use the soft sample embedding, e\u0303i\u22121, defined in Section 3.2. Apart from this difference, training is carried out using the same procedure."}, {"heading": "4 Related Work", "text": "Gormley et al. (2015)\u2019s approximation-aware training is conceptually related, but focuses on variational decoding procedures. Hoang et al. (2017) also propose continuous relaxations of decoders, but are focused on developing better inference procedures. Grefenstette et al. (2015) successfully use a soft approximation to argmax in neural stack mechanisms. Finally, Ranzato et al. (2016) experiment with a similarly motivated objective that was not fully continuous, but found it performed worse than the standard training."}, {"heading": "5 Experimental Setup", "text": "We perform experiments with machine translation (MT) and named entity recognition (NER).\nData: For MT, we use the same dataset (the German-English portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014)), preprocessing and data splits as Ranzato et al. (2016). For named entity recognition, we use the CONLL 2003 shared task data (Tjong\nKim Sang and De Meulder, 2003) for German language and use the provided data splits. We perform no preprocessing on the data.The output vocabulary length for MT is 32000 and 10 for NER.\nImplementation details: For MT, we use a seq2seq model with a simple attention mechanism (Bahdanau et al., 2015), a bidirectional LSTM encoder (1 layer, 256 units), and an LSTM decoder (1 layer, 256 units). For NER, we use a seq2seq model with an LSTM encoder (1 layer, 64 units) and an LSTM decoder (1 layer, 64 units) with a fixed attention mechanism that deterministically attends to the ith input token when decoding the ith output, and hence does not involve learning of attention parameters. 4\nHyperparameter tuning: We start by training with actual ground truth sequences for the first epoch and decay the probability of selecting the ground truth token as an inverse sigmoid (Bengio et al., 2015) of epochs with a decay strength parameter k. We also tuned for different values of \u03b1 and explore the effect of varying \u03b1 exponentially (annealing) with the epochs. In table 1, we report results for the best performing configuration of decay parameter and the \u03b1 parameter on the validation set. To account for variance across randomly started runs, we ran multiple random restarts (RR) for all the systems evaluated and always used the RR with the best validation set score to calculate test performance.\nComparison We report validation and test metrics for NER and MT tasks in Table 1, F1 and BLEU respectively. \u2018Greedy\u2019 in the table refers to scheduled sampling with soft argmax decisions (either soft or hard) and \u2018Sample\u2019 refers the corresponding reparametrized sample-based decoding scenario. We compare our approach with two baselines: standard cross-entropy loss minimization for seq2seq models (\u2018Baseline CE\u2019) and the standard scheduled sampling procedure (Bengio et al. (2015)). We report results for two variants of our approach: one with a fixed \u03b1 parameter throughout the training procedure (\u03b1-soft fixed), and the other in which we vary \u03b1 exponentially with the number of epochs (\u03b1-soft annealed).\n4Fixed attention refers to the scenario when we use the bidirectional LSTM encoder representation of the source sequence token at time step t while decoding at time step t instead of using a linear combination of all the input sequences weighted according to the attention parameters in the standard attention mechanism based models."}, {"heading": "6 Results", "text": "All three approaches improve over the standard cross-entropy based seq2seq training. Moreover, both approaches using continuous relaxations (greedy and sample-based) outperform standard scheduled sampling (Bengio et al., 2015). The best results for NER were obtained with the relaxed greedy decoder with annealed \u03b1 which yielded an F1 gain of +3.1 over the standard seq2seq baseline and a gain of +1.5 F1 over standard scheduled sampling. For MT, we obtain the best results with the relaxed sample-based decoder, which yielded a gain of +1.5 BLEU over standard seq2seq and a gain of +0.75 BLEU over standard scheduled sampling.\nWe observe that the reparametrized samplebased method, although not fully continuous endto-end unlike the soft greedy approach, results in good performance on both the tasks, particularly MT. This might be an effect of stochastic exploration of the search space over the output sequences during training and hence we expect MT to benefit from sampling due to a much larger search space associated with it. We also observe that annealing \u03b1 results in good performance which suggests that a smoother approximation to the loss function in the initial stages of training is helpful in guiding the learning in the right direction. However, in our experiments we noticed that\nthe performance while annealing \u03b1 was sensitive to the hyperparameter associated with the annealing schedule of the mixing probability in scheduled sampling during training.\nThe computational complexity of our approach is comparable to that of standard seq2seq training. However, instead of a vocabulary-sized max and lookup, our approach requires a matrix multiplication. Practically, we observed that on GPU hardware, all the models for both the tasks had similar speeds which suggests that our approach leads to accuracy gains without compromising run-time. Moreover, as shown in Table 2, we observe that a gradual decay of mixing probability consistently compared favorably to more aggressive decay schedules. We also observed that the \u2018always sample\u2019 case of relaxed greedy decoding, in which we never mix in ground truth inputs (see Bengio et al. (2015)), worked well for NER but resulted in unstable training for MT. We reckon that this is an effect of large difference between the search space associated with NER and MT."}, {"heading": "7 Conclusion", "text": "Our positive results indicate that mechanisms for credit assignment can be useful when added to the models that aim to ameliorate exposure bias. Further, our results suggest that continuous relaxations of the argmax operation can be used as effective approximations to hard decoding during training."}, {"heading": "Acknowledgements", "text": "We thank Graham Neubig for helpful discussions. We also thank the three anonymous reviewers for their valuable feedback."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2017", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."], "venue": "Advances in Neural Information Processing Systems. pages 1171\u20131179.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Report on the 11th iwslt evaluation campaign, iwslt 2014", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico."], "venue": "Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam.", "citeRegEx": "Cettolo et al\\.,? 2014", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9", "John Langford", "Daniel Marcu."], "venue": "Machine learning 75(3):297\u2013325.", "citeRegEx": "Daum\u00e9 et al\\.,? 2009", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2009}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Proceedings of the 22nd international conference on Machine learning. ACM, pages 169\u2013176.", "citeRegEx": "III and Marcu.,? 2005", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Approximation-aware dependency parsing by belief propagation", "author": ["Matthew R. Gormley", "Mark Dredze", "Jason Eisner."], "venue": "Transactions of the Association for Computational Linguistics (TACL) .", "citeRegEx": "Gormley et al\\.,? 2015", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1828\u20131836.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Decoding as continuous optimization in neural machine translation", "author": ["Cong Duy Vu Hoang", "Gholamreza Haffari", "Trevor Cohn."], "venue": "arXiv preprint arXiv:1701.02854 .", "citeRegEx": "Hoang et al\\.,? 2017", "shortCiteRegEx": "Hoang et al\\.", "year": 2017}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Jang et al\\.,? 2016", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Maddison et al\\.,? 2017", "shortCiteRegEx": "Maddison et al\\.", "year": 2017}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell."], "venue": "AISTATS. volume 1, page 6.", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "AAAI\u201916 Proceedings of the Thirtieth AAAI Conference on Artifi-", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Erik F Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML. volume 14, pages 77\u201381.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)\u2013a well-known technique for correcting exposure bias\u2013we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value.", "startOffset": 83, "endOffset": 104}, {"referenceID": 16, "context": "Sequence-to-Sequence (seq2seq) models have demonstrated excellent performance in several tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al.", "startOffset": 125, "endOffset": 149}, {"referenceID": 14, "context": ", 2014), summarization (Rush et al., 2015), dialogue generation (Serban et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 15, "context": ", 2015), dialogue generation (Serban et al., 2015), and image captioning (Xu et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 19, "context": ", 2015), and image captioning (Xu et al., 2015).", "startOffset": 30, "endOffset": 47}, {"referenceID": 12, "context": "These include reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2017), imitation learning (Daum\u00e9 et al.", "startOffset": 37, "endOffset": 82}, {"referenceID": 1, "context": "These include reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2017), imitation learning (Daum\u00e9 et al.", "startOffset": 37, "endOffset": 82}, {"referenceID": 5, "context": ", 2017), imitation learning (Daum\u00e9 et al., 2009; Ross et al., 2011; Bengio et al., 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al.", "startOffset": 28, "endOffset": 88}, {"referenceID": 13, "context": ", 2017), imitation learning (Daum\u00e9 et al., 2009; Ross et al., 2011; Bengio et al., 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al.", "startOffset": 28, "endOffset": 88}, {"referenceID": 3, "context": ", 2017), imitation learning (Daum\u00e9 et al., 2009; Ross et al., 2011; Bengio et al., 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al.", "startOffset": 28, "endOffset": 88}, {"referenceID": 18, "context": ", 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al., 2016; Daum\u00e9 III and Marcu, 2005).", "startOffset": 35, "endOffset": 106}, {"referenceID": 0, "context": ", 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al., 2016; Daum\u00e9 III and Marcu, 2005).", "startOffset": 35, "endOffset": 106}, {"referenceID": 3, "context": "In this paper, we focus on one the simplest to implement and least computationally expensive approaches, scheduled sampling (Bengio et al., 2015), which stochastically incorporates contexts from previous decoding decisions into training.", "startOffset": 124, "endOffset": 145}, {"referenceID": 3, "context": "In addition, we demonstrate a related approximation and reparametrization for sample-based training (another training scenario considered by scheduled sampling (Bengio et al., 2015)) that can yield stochastic gradients with lower variance than in standard scheduled sampling.", "startOffset": 160, "endOffset": 181}, {"referenceID": 3, "context": "While scheduled sampling (Bengio et al., 2015) is an effective way to rectify exposure bias, it cannot differentiate between cascading errors, which can lead to a sequence of bad decisions, and local errors, which have more benign effects.", "startOffset": 25, "endOffset": 46}, {"referenceID": 3, "context": "For the sake of simplicity, the \u2018always sample\u2019 variant of scheduled sampling is described (Bengio et al., 2015).", "startOffset": 91, "endOffset": 112}, {"referenceID": 10, "context": "This approximation was proposed by Maddison et al. (2017) and Jang et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 10, "context": "(2017) and Jang et al. (2016), who showed that the same soft argmax operation introduced above can be used for reducing variance of stochastic gradients when sampling from softmax distributions.", "startOffset": 11, "endOffset": 30}, {"referenceID": 3, "context": "As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training (Bengio et al., 2015; Daum\u00e9 et al., 2009; Ross et al., 2011), while annealing the probability of using the ground truth with each epoch, results in better models and more stable training.", "startOffset": 104, "endOffset": 164}, {"referenceID": 5, "context": "As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training (Bengio et al., 2015; Daum\u00e9 et al., 2009; Ross et al., 2011), while annealing the probability of using the ground truth with each epoch, results in better models and more stable training.", "startOffset": 104, "endOffset": 164}, {"referenceID": 13, "context": "As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training (Bengio et al., 2015; Daum\u00e9 et al., 2009; Ross et al., 2011), while annealing the probability of using the ground truth with each epoch, results in better models and more stable training.", "startOffset": 104, "endOffset": 164}, {"referenceID": 3, "context": "For the case of scheduled sampling with sample-based training\u2013where decisions are sampled rather than chosen greedily (Bengio et al., 2015)\u2013we conduct experiments using a related training procedure.", "startOffset": 118, "endOffset": 139}, {"referenceID": 4, "context": "Data: For MT, we use the same dataset (the German-English portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014)), preprocessing and data splits as Ranzato et al.", "startOffset": 124, "endOffset": 146}, {"referenceID": 4, "context": "Data: For MT, we use the same dataset (the German-English portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014)), preprocessing and data splits as Ranzato et al. (2016). For named entity recognition, we use the CONLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003) for German language and use the provided data splits.", "startOffset": 125, "endOffset": 204}, {"referenceID": 2, "context": "Implementation details: For MT, we use a seq2seq model with a simple attention mechanism (Bahdanau et al., 2015), a bidirectional LSTM encoder (1 layer, 256 units), and an LSTM decoder (1 layer, 256 units).", "startOffset": 89, "endOffset": 112}, {"referenceID": 3, "context": "Hyperparameter tuning: We start by training with actual ground truth sequences for the first epoch and decay the probability of selecting the ground truth token as an inverse sigmoid (Bengio et al., 2015) of epochs with a decay strength parameter k.", "startOffset": 183, "endOffset": 204}, {"referenceID": 3, "context": "We compare our approach with two baselines: standard cross-entropy loss minimization for seq2seq models (\u2018Baseline CE\u2019) and the standard scheduled sampling procedure (Bengio et al. (2015)).", "startOffset": 167, "endOffset": 188}, {"referenceID": 3, "context": "We compare our approach (\u03b1-soft argmax with fixed and annealed temperature) with standard cross entropy training (Baseline CE) and discontinuous scheduled sampling (Bengio et al. (2015)).", "startOffset": 165, "endOffset": 186}, {"referenceID": 3, "context": "Moreover, both approaches using continuous relaxations (greedy and sample-based) outperform standard scheduled sampling (Bengio et al., 2015).", "startOffset": 120, "endOffset": 141}, {"referenceID": 3, "context": "We also observed that the \u2018always sample\u2019 case of relaxed greedy decoding, in which we never mix in ground truth inputs (see Bengio et al. (2015)), worked well for NER but resulted in unstable training for MT.", "startOffset": 125, "endOffset": 146}], "year": 2017, "abstractText": "We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-tosequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)\u2013a well-known technique for correcting exposure bias\u2013we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.", "creator": "LaTeX with hyperref package"}}}