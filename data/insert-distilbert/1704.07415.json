{"id": "1704.07415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "abstract": "to answer the question in machine comprehension ( mc ) task, the conceptual models need software to establish the observed interaction between the question tag and the context. to tackle the problem that the single - pass display model cannot reflect on and correct its answer, we present ruminating reader. ruminating reader adds a select second pass of attention and a novel information fusion component to the bi - directional attention relationship flow model ( bidaf ). we propose novel layer structures that construct each an query - based aware context vector representation and fuse encoding representation with intermediate representation on ~ top slope of bidaf model. next we show that a multi - hop attention mechanism can be applied to specify a bi - directional attention structure. in experiments on squad, whereas we find that the reader outperforms the bidaf image baseline by a substantial margin, and matches is or surpasses the performance of all other published systems.", "histories": [["v1", "Mon, 24 Apr 2017 18:49:38 GMT  (498kb,D)", "http://arxiv.org/abs/1704.07415v1", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yichen gong", "samuel r bowman"], "accepted": false, "id": "1704.07415"}, "pdf": {"name": "1704.07415.pdf", "metadata": {"source": "CRF", "title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "authors": ["Yichen Gong", "Samuel R. Bowman"], "emails": ["yichen.gong@nyu.edu", "bowman@nyu.edu"], "sections": [{"heading": null, "text": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BIDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BIDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BIDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems."}, {"heading": "1 Introduction", "text": "The majority of recorded human knowledge is circulated in unstructured natural language. It is tremendously valuable to allow machines to read and comprehend the text knowledge. Machine comprehension (MC)\u2014especially in the form of question answering (QA)\u2014is therefore attracting a significant amount of attention from the machine learning community. Recently introduced large-scale datasets like CNN/Daily Mail (Hermann et al., 2015), the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016) and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO; Nguyen et al., 2016) have\nallow data-driven methods, including deep learning, to become viable.\nRecent approaches toward solving machine comprehension tasks using neural networks can be viewed as falling into two broad categories: single-pass reasoners and multiple-pass reasoners. Single-pass models read a question and a source text once and often adopt the differentiable attention mechanism that emphasizes important parts of the context related to the question.\nBIDAF (Seo et al., 2017) represents one of\nar X\niv :1\n70 4.\n07 41\n5v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nthe state-of-the-art single-pass models in Machine Comprehension. BIDAF uses a bi-directional attention matrix which calculates the correlations between each word pair in context and query to build query-aware context representation. However, BIDAF and some similar models miss some questions because they don\u2019t have the capacity to reflect on problematic candidate answers and revise their decisions.\nWhen humans are reading a text with the goal of answering a question, they tend to read it multiple times to get a better understanding of the context and question, and to give a better response.\nWith this intuition, recent multi-pass models revisit the question and the context passage (or ruminate) to infer the relations between the context, the question and the answer.\nWe propose an extension of BIDAF, called Ruminating Reader, which uses a second pass of reading and reasoning to allow it to learn to avoid mistakes and to ensure that it is able to effectively use the full context when selecting an answer. In addition to adding a second pass, we also introduce two novel layer types, the ruminate layers, which use gating mechanisms to fuse the obtained from the first and second passes. We observe a surprising phenomenon that when an LSTM layer in the context ruminate layer takes same input in each timestep, it can produce useful representation for the gates. In addition, we introduce an answer\u2013question similarity loss to penalize overlap between question and predicted answer, a common feature in the errors of our base model. This allows us to achieve an F1 score of 79.5 and Exact Match (EM) score of 70.6 on hidden test set,1 an improvement of 2.2 F1 score and 2.9 EM on BIDAF. Figure 1 shows a high-level comparison between BIDAF and Ruminating Reader.\nThis paper is organized as follows: In Section 2 we define the problem to be solved and introduce the SQuAD task. In Section 3 we introduce Ruminating Reader, focusing on the informationextracting and information-digesting components and how they integrate. Section 4 discusses related work. Section 5 presents the experimental setting, results and analysis. Section 6 concludes."}, {"heading": "2 Question Answering", "text": "The task of the Ruminate Reader is to answer a question by reading and understanding a paragraph of text and selecting a span of words within the context. Formally, the Training and development data consist of tuples (Q, P, A), where Q = (q1, ..., qi, ...q|Q|) is the question, a sequence of words with length |Q|, C = (c1, ...cj , ..., c|C|) is the context, a sequence of words with length |C|, and A = (ab, ae) is the answer span marking the beginning and end indices of the the answer in the context (1 <= ab <= ae <= |C|).\nSQuAD The SQuAD corpus is built using 536 articles randomly selected from English Wikipedia. Images, figures, tables are stripped and any paragraphs shorter than 500 characters are discarded. Unlike other datasets that such as CNN/Daily Mail whose questions are synthesized, Rajpurkar et al. (2016) uses a crowdsourcing platform to generate realistic question and answer pairs. SQuAD contains 107,785 question-\n1The latest results are listed at https://rajpurkar. github.io/SQuAD-explorer/\nanswer pairs. The typical context length spans from 50 tokens to 250 tokens. The typical length of a question is around 10 tokens. The answer be any span of words from the context, resulting in O(|C|2) possible outputs."}, {"heading": "3 Our Model", "text": ""}, {"heading": "3.1 Ruminating Reader", "text": "In this section, we review the BIDAF model (Seo et al., 2017) and introduce our extension, the Ruminating Reader.\nOur additions to the base model are motivated by the intuition that adding an additional pass of reading will allow the model to better integrate information from the question and answer and to better weigh possible answers, and that by interpolating the results of the second pass with those of the first pass through gating, we can prevent the additional complexity that we add to the model from substantially increasing the difficulty of training. The structure of our model is shown in Figure 2 and explained in the following sections.\nCharacter Embedding Layer Just as in the base BIDAF model, the character embedding layer maps each word to a high dimensional vector using character features. It does so using a convolutional neural network with max pooling over learned character vectors (Lee et al., 2017; Kim et al., 2016). Thus we have a context character representation M \u2208 Rf\u00d7C and a query representation N \u2208 Rf\u00d7Q, where C is the sequence length of the context, Q is the sequence length of the query and f is the number of 1D convolutional neural network filters.\nWord Embedding Layer Again as in the base model, the word embedding layer uses pretrained word vectors (the 6B GloVe vectors of Pennington et al., 2014) to map the word into a high dimensional vector space. We do not update the word embeddings during training. The character embedding and the word embedding are concatenated and passed into a two-layer highway network (Srivastava et al., 2015) to obtain a d dimensional vector representation of each single word. Hence, we have a context representation H \u2208 Rd\u00d7C and a query representation U \u2208 Rd\u00d7Q.\nSequence Encoding Layers As in BIDAF, we use two LSTM RNNs (Hochreiter and Schmidhuber, 1997) with d-dimensional outputs to encode\nthe context and query representations in both directions. Therefore, we obtain a context encoding matrix C \u2208 R2d\u00d7C , and a query encoding matrix Q \u2208 R2d\u00d7Q.\nAttention Flow Layer As in BIDAF, the attention flow layer constructs a query-aware context representation G from inputs C and Q. This layer takes two steps. In the first step, an interaction matrix I \u2208 RC\u00d7Q is computed, which indicates the affinities between each context word encoding and each query word encoding. Icq indicates the correlation between the c-th word in context and q-th word in query. The interaction matrix is computed by\nIcq = w > (I)[Cc;Qq;Cc \u25e6Qq] (1)\nwhere wI \u2208 R6d is a trainable parameter, Cc is c-th column of context encoding and Qq is q-th column of query encoding, \u25e6 is elementwise multiplication, and [; ] is vector concatenation.\nContext-to-query Attention As in BIDAF, the context-to-query attention component generates, for each context word, an attention-weighted sum of query word encodings. Let Q\u0303 \u2208 R2d\u00d7C represent the context-to-query attention matrix. For column c in Q\u0303 is defined by Q\u0303c = \u2211 (acqQq), where a is the attention weight. a is computed by ac = softmax(Ic) \u2208 RQ.\nQuery-to-context Attention Query-to-context attention indicates the most relevant context words to query. The most relevant word vector representation is an attention-weighted sum defined by c\u0303 = \u2211 bcCc where b, is an attention weight which is calculated by b = softmax(maxcol(I)) \u2208 RC . c\u0303 is replicated C times across the column, therefore giving C\u0303 \u2208 R2d\u00d7C .\nWe then obtain the final query-aware context representation by\nGc = [Cc; Q\u0303c;Cc \u25e6 Q\u0303c;Cc \u25e6 C\u0303c] (2)\nwhere Gc \u2208 R8d\u00d7C .\nSummarization Layer We propose summarization layer which produces a vector representation that summarizes the information in the queryaware context representation. The input to summarization layer is G. We use one bi-directional LSTM network to model the learned information. We select the final states from both directions and concatenate them together as s = [sf ; sb] . where\ns \u2208 R2d represents the representation summarized from the reading of context and query, sf is the final state of LSTM in forward direction, and sb is the final state of LSTM in backward direction.\nQuery Ruminate Layer The query ruminate layer fuses the summarization vector representation with the query encoding Q, helping reformulate the query representation in order to maximize the chance of retrieving the correct answer. The input to this layer is s tiled Q times (SQ \u2208 R2d\u00d7Q). A gating function then fuses this with the existing query encoding:\nzi = tanh(W 1> Qz SQi +W 2> Qz Qi + bQz) (3)\nfi = \u03c3(W 1> Qf SQi +W 2> QfQi + bQf ) (4)\nQ\u0303i = fi \u25e6Qi + (1\u2212 fi) \u25e6 zi (5)\nwhere W 1Qz,W 2 Qz,W 1 Qf ,W 2 Qf \u2208 R2d\u00d72d and bQz, bQf \u2208 R2d are trainable parameters, SQiis the i-th column of the SQ, Qi is the i-th column of Q.\nContext Ruminate Layer Context ruminate layer digests the summarization and integrates it with the context encoding C to facilitate answer extraction. In this layer, we tile s C times and we have SC \u2208 R2d\u00d7C . To incorporate the positional information into this relatively long tiled sequence, we feed it into an additional bidirectional LSTM with output size d in each direction. This approach, while somewhat inefficient, proves to be an valuable addition to the model and allows it to better track position information, loosely following the positional encoding strategy of Sukhbaatar et al. (2015). Hence we obtain S\u0303C \u2208 R2d\u00d7C , which is fused with context encoding C via a gate:\nzi = tanh(W 1> Cz S\u0303Ci +W 2> Cz Ci + bCz) (6)\nfi = \u03c3(W 1> Cf S\u0303Ci +W 2> Cf Ci + bCf ) (7)\nC\u0303i = fi \u25e6Ci + (1\u2212 fi) \u25e6 zi (8)\nwhere W 1Cz,W 2 Cz,W 1 Cf ,W 2 Cf \u2208 R2d\u00d72d and bCz, bCf \u2208 R2d are trainable parameters, S\u0303Ciis the i-th column of the S\u0303C , Ci is the i-th column of C.\nSecond Hop Attention Flow Layer We take Q\u0303 \u2208 R2d\u00d7Q and C\u0303 \u2208 R2d\u00d7C as the input to another attention flow layer with the same structure as described above, yielding G(2) \u2208 R8d\u00d7C .\nModeling Layer We use two layers of bidirectional LSTM with output size d in each direction to aggregate the information in G(2), yielding a pre-output matrix M s \u2208 R2d\u00d7C .\nOutput Layer As in BIDAF, our output layer independently models the probability of each word being selected as the start or end of an answer span. We calculate the probability distribution of the start index of the answer span by\nps = softmax(w>p1 [G;M s]) (9)\nwhere w(p1) \u2208 R10d is a trainable parameter. We pass the matrix M s to another bi-directional LSTM with output size d in single direction yielding M e. We obtain the probability distribution of the end index of the answer span by\npe = softmax(w>(p2)[G;M e]) (10)\nTraining Loss We define the training loss as the sum of three components: negative log likelihood loss, L2 regularization loss, and a novel answer\u2013 question similarity loss.\nAnswer\u2013Question Similarity Loss We observe that a version of our model trained only on the two standard loss terms often selects answers that overlap substantially in content with their corresponding questions, and that this nearly always results\nin an error. A sample error of this kind is shown in Table 1. This motivates an additional loss term at training time: We penalize the similarity between the question and the selected answer. Formally, the answer question similarity loss is defined as\ns = Argmax(p1) (11)\ne = Argmax(p2) (12)\n~qBoW = Sumrow(Q)\nQ (13)\nAQSL(\u03b8) = cos(Cs, ~qBoW ) + cos(Ce, ~qBoW ) (14)\nwhere s refers to the start index of answer span, e refers to the end index of the answer span, ~qBoW is the bag of words representation of query encoding, cos(a, b) is the cosine similarity between a and b, Cs and Ce are the s-th and e-th vector representation of context encoding.\nPrediction During prediction, we use a local search strategy that for token indices a and a\u2032, we maximize psa \u00d7 pea\u2032 , where 0 \u2264 a\u2032 \u2212 a \u2264 15 . Dynamic programming is applied during search, resulting in O(C) time complexity."}, {"heading": "4 Related Work", "text": "Recently, both QA and Cloze-style machine comprehension tasks like CNN/Daily Mail have seen fast progress. Much of this recent work has been based on end-to-end trained neural network models, and within that, most have used recurrent neural networks with soft attention (Bahdanau et al., 2015), which emphasizes one part of the data over the others. These models can be coarsely divided into two categories: single-pass and multi-pass reasoners.\nMost papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context. Xiong et al. (2017) propose the Dynamic Coattention Network, which uses co-dependent representations of the question and the context, and iteratively updates the\nstart and end indices to recover from local maxima and to find the optimal answer span. Wang et al. (2016) propose the Multi-Perspective Context Matching model that matches the encoded context with query by combining various matching strategies, aggregates matching vector with bidirectional LSTM, and predict start and end positions. In order to merge the entity score during its multiple appearence, Kadlec et al. (2016) propose attention-sum reader who computes dot product between context and query encoding, does a softmax operation over context and sums the probability over the same entity to favor the frequent entities over rare ones. Chen et al. (2016) propose to use a bilinear term to calculate the attentional alignment between context and query.\nAmong multi-hop reasoning systems: Hill et al. (2015) apply attention on window-based memory, by extending multi-hop end-to-end memory network (Sukhbaatar et al., 2015). Dhingra et al. (2016) extend attention-sum reader to multi-turn reasoning with an added gating mechanism. The Iterative Alternative (IA) reader (Sordoni et al., 2016) produces query glimpse and document glimpse in each iterations and uses both glimpses to update recurrent state in each iteration. Shen et al. (2017) propose a multi-hop attention model that used reinforcement learning to dynamically determine when to stop digesting intermediate information and produce an answer."}, {"heading": "5 Evaluation", "text": ""}, {"heading": "5.1 Implementation details", "text": "Our model configuration closely follows that of Seo et al. (2017) did: In the character encoding layer, we use 100 filters of width 5. In the remainder of the model, we set the hidden layer dimension (d) to 100. We use pretrained 100D GloVe vectors (6B-token version) as word embeddings. Out-of-vocobulary tokens are represented by an UNK symbol in the word embedding layer, but treated normally by the character embedding layer. The BiLSTMs in context and query encoding layers share same weights. We use the AdaDelta optimizer (Zeiler, 2012) for optimization.\nWe selected hyperparameter values through random search (Bergstra and Bengio, 2012). Batch size is 30. Learning rate starts at 0.5, and decreases to 0.2 once the model stops improving. The L2-regularization weight is 1e-4, AQSL weight is 1 and dropout with a drop rate of 0.2 is\n\u2021\nUnpublished\napplied to all forward connections in the CNN, the LSTMs, and all feedforward layers.\nA typical model run converges in about 40k steps. This takes two days using Tensorflow (Abadi et al., 2015) and a single NVIDIA K80 GPU ."}, {"heading": "5.2 Evaluation Method", "text": "Rajpurkar et al. (2016) provide an official evaluation script that allows us to measure F1 score and EM score by comparing the prediction and ground truth answers. Three answers are provided for each question. The prediction is compared to each of the answer and best score is selected. F1 score is defined by recall and precision of words and EM score, as Exact Match score, is defined as the score of 100% accuracy in prediction. We do not use any kind of ensembling, and compare our results primarily with other single-model (non-ensemble) results. The test set performance is evaluated at CodaLab by administrator."}, {"heading": "5.3 Results", "text": "At the time of submission, our model is tied in accuracy on the hidden test set with the bestperforming published single model (Zhang et al., 2017). We achieve an F1 score of 79.5 and EM score of 70.6. The current leaderboard is displayed in Table 2. The leaderboard is listed in descending order of F1 score, but if an entry\u2019s F1 score is better than the adjacent entry\u2019s, while its EM score is worse, then these two entries are considered tied."}, {"heading": "5.4 Analysis", "text": "Layer Ablation Analysis To analyze how each component contribute to the model, we run a layer ablation experiment. We present results for twelve versions of the model on the development set, each missing some or all of the major components of the full Ruminating Reader. The precise definition of each of the twelve ablated models can be found in Appendix A.1.\nThe results of the ablation experiment are shown in Table 3. The ablation experiments show how each component contribute to the model. Experiments 3 and 4 show that the two ruminate layers are both important and helpful in contributing performance. It is worth noting that the BiLSTM in the context ruminate layer contributes substantially to model performance. We find this somewhat surprising, since it takes the same input in each timestep, but it nonetheless successfully digests the summarization information representation and produces a useful input for the gating component. Experiments 7 and 8 show that the\nmodeled summarization vector representation can provide information to gates reasonably well. The drop in performance in both experiments 9 and 10 shows that the key information for new query and context representation are the are first stage query and context encodings. Experiments 11 and 12 shows that the summarization vector representation does help the later stage of reasoning.\nVisualization Figure 3 provides a visualization of the first hop and second hop attention interaction matrix I . We also provide a sample of visualization for the L2 sum of gate value in context and query ruminate layers in Figure 4.\nFrom Figure 3 we see that though the structures of two hops of attention flow layer are the same, they function quite differently in typical cases. The first hop attention appears to be primarily concerned with identifying the key informative word (or words, as here) in the query. Though in Figure 3 four key words are signified, one or two words are attended to in the first hop in the common case. The second hop is then responsible for finding candidate answers that are relevant to those key words and generating a query-aware context representation. We observe the first hop attention shows a consistent attention pattern across context words, suggesting that there may be room to make the first hop component more efficient in future work.\nFrom Figure 4, we see the gate value on both query ruminate layer and context ruminate layer\nshows that the gates are working to fuse information to original query encoding and context encoding. We observe that in most of the case the gates in ruminate layers uses more information from encoding than from summarization representation. The observation matches our expectation that the gates modify and improve on the encoding representation.\nWe also provide a comparison of F1 score between BIDAF and Ruminating Reader on question with different ground truth answer length and different types of questions in Figure 5. Exact match score is highly correlated with F1 score so we omit it for clarity. We observe that the Ruminating Reader outperforms BIDAF on most of the questions with respect of different answer length. On the question with long answer length, of 5, 8 and 9, Ruminating Reader outperforms BIDAF by a great margin. Questions with longer reference answers appear to be more difficult to answer. In\naddition, the Ruminating Reader does better on each type of question. Both models work best for when questions\u2014these question are answerable by temporal expressions, which are relatively easy to recognize. The Why questions are hardest to answer\u2014they tend to have long answers with no purely lexical cues marking their beginnings or ends. Ruminating Reader outperforms BIDAF model on why questions by a substantial margin. Performance Breakdown Following Zhang et al. (2017), we break down Ruminating Reader\u2019s 79.5% F1 score on the development set into three sub-scores, representing failures, partial successes, and successes. On 13.5% of development set examples, Ruminate Reader fails, yielding 0% F1. On 70.6% of examples, Ruminate Reader achieves a perfect F1 score. On the remaining 15.9%, Ruminate Reader got only partial matches (i.e., answers that partially overlapped with reference answers), with an average F1 score of 56.0%. Comparing to the jNet (Zhang et al., 2017) whose success answers occupy 69.1% of all answers, failure score answers 14.9% and partial success 16.01% with an average F1 score of 58.0%, our model works better on increasing successes and reducing failures."}, {"heading": "6 Conclusion", "text": "We propose the Ruminating Reader, an extension to the BIDAF model with two-hop attention. The model surpasses the original BIDAF model\u2019s performance on Stanford Question Answering Dataset (SQuAD) by a large margin, and ties with the best published system. These results and our qualitative analysis both suggest that the model successfully fuses the information from two passes of reading using gating and uses the result to identify appropriate answers to Wikipedia questions. An ablation experiment shows that each of components of this complex model contribute substantially. In future work, we aim to find ways to simplify this model without impacting performance, to explore the possibility of yet deeper models, and to expand our study to machine comprehension tasks more broadly."}, {"heading": "Acknowledgments", "text": "We thank Pranav Rajpurkar for testing Ruminate Reader on SQuAD hidden test set."}, {"heading": "A Appendix", "text": "A.1 Layer Ablation Experiments setup In this section we show the setup of layer ablation experiment details in Table 3.\n1. Vanilla BIDAF\n2. Ruminating Reader without context and query ruminate layer. Therefore, the model is equivalent to original BIDAF model with L2-regurization, answer-question similarity penalization and local search prediction feature.\n3. Ruminating Reader without query ruminate layer. The query encoding Q is directly fed into the second hop attention flow layer.\n4. Ruminating Reader without context ruminate layer. The context encoding C is directly connected to the second hop attention flow layer without digesting newly acquired information.\n5. Ruminating Reader with BiLSTM modeling in query ruminate layer. Formally, we have S\u0303Q = BiLSTM(SQ) in query ruminate layer. Therefore, the query ruminate layer is defined by\nzi = tanh(W 1> Qz S\u0303Qi +W 2> Qz Qi + bQz)\n(15)\nfi = \u03c3(W 1> Qf S\u0303Qi +W 2> QfQi + bQf ) (16)\nQ\u0303i = fi \u25e6Qi + (1\u2212 fi) \u25e6 zi (17)\n6. Ruminating Reader without BiLSTM modeling in context ruminate layer. Formally, we have S\u0303C = SC in query ruminate layer and all other components remains the same.\n7. Ruminating Reader without query input at z, f in query ruminate layer. While all other components remain the same as in Ruminating Reader, the gate in query ruminate layer is defined by\nzi = tanh(W 1> Qz SQi + bQz) (18)\nfi = \u03c3(W 1> Qf SQi + bQf ) (19)\nQ\u0303i = fi \u25e6Qi + (1\u2212 fi) \u25e6 zi (20)\n8. Ruminating Reader without context input at z, f in context ruminate layer. While all other components remain the same as in Ruminating Reader, the gate in context ruminate layer is defined by\nzi = tanh(W 1> Cz S\u0303Ci + bCz) (21)\nfi = \u03c3(W 1> f S\u0303Ci + bCf ) (22)\nC\u0303i = fi \u25e6Ci + (1\u2212 fi) \u25e6 zi (23)\n9. Ruminating Reader without query input in query ruminate layer. In this version, we discard query encoding input Q in the gate of query ruminate layer. Formally, the gate in Query Ruminate layer is\nzi = tanh(W 1> Qz SQi + bQz) (24)\nfi = \u03c3(W 1> Qf SQi + bQf ) (25)\nQ\u0303i = (1\u2212 fi) \u25e6 zi (26)\n10. Ruminating Reader without context encoding input in context ruminate layer. We ablate the context encoding input C in the gate of context ruminate layer. Therefore, the gate in context ruminate layer is\nzi = tanh(W 1> Cz S\u0303Ci + bCz) (27)\nfi = \u03c3(W 1> f S\u0303Ci + bCf ) (28)\nC\u0303i = (1\u2212 fi) \u25e6 zi (29)\n11. Ruminating Reader without summarization information input in query ruminate layer. In case that the summarization do not help the encoding, while on the other hand, the gate contributes to the learning, we design the experiment that allows to eliminate the influence of summarization. We discard the summarization input in query ruminate layer.\nFormally, the gate in query ruminate layer is defined as\nzi = tanh(W 2> Qz Qi + bQz) (30)\nfi = \u03c3(W 2> QfQi + bQf ) (31)\nQ\u0303i = fi \u25e6Qi + (1\u2212 fi) \u25e6 zi (32)\n12. Ruminating Reader without summarization information input in context ruminate layer. The summarization information is not included in zi,fi\nzi = tanh(W 2> Cz Ci + bCz) (33)\nfi = \u03c3(W 2> Cf Ci + bCf ) (34)\nC\u0303i = fi \u25e6Ci + (1\u2212 fi) \u25e6 zi (35)"}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["van", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "van et al\\.,? \\Q2015\\E", "shortCiteRegEx": "van et al\\.", "year": 2015}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Random Search for Hyper-Parameter Optimization", "author": ["James Bergstra", "Yoshua Bengio."], "venue": "Proc. JMLR.", "citeRegEx": "Bergstra and Bengio.,? 2012", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."], "venue": "Proc. ACL.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Reading Wikipedia to Answer OpenDomain Questions", "author": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."], "venue": "Proc. ACL.", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Gated-Attention Readers for Text Comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "CoRR.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Teaching Machines to Read and Comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Proc. NIPS.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The Goldilocks Principle: Reading Children\u2019s Books with Explicit Memory Representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "Proc. ICLR.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "volume 9, pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text Understanding with the Attention Sum Reader Network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "Proc. ACL.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "Proc. AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "Proc. ACL.", "citeRegEx": "Lee et al\\.,? 2017", "shortCiteRegEx": "Lee et al\\.", "year": 2017}, {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "author": ["Kenton Lee", "Shimi Salant", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das", "Jonathan Berant."], "venue": "ArXiv:1611.01436.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "Proc. CoCo@NIPS.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD - 100, 000+ Questions for Machine Comprehension of Text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proc. EMNLP.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional Attention Flow for Machine Comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "Proc. ICLR.", "citeRegEx": "Seo et al\\.,? 2017", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "ReasoNet: Learning to Stop Reading in Machine Comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."], "venue": "Proc. NIPS.", "citeRegEx": "Shen et al\\.,? 2017", "shortCiteRegEx": "Shen et al\\.", "year": 2017}, {"title": "Iterative Alternating Neural Attention for Machine Reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio."], "venue": "CoRR.", "citeRegEx": "Sordoni et al\\.,? 2016", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Highway Networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Proc. ICML.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "End-To-End Memory Networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "Proc. NIPS.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Pointer Networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Proc. NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "ArXiv:1608.07905.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Multi-Perspective Context Matching for Machine Comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "ArXiv:1612.04211.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Making Neural QA as Simple as Possible but not Simpler", "author": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe."], "venue": "ArXiv:1703.04816.", "citeRegEx": "Weissenborn et al\\.,? 2017", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "Dynamic Coattention Networks For Question Answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "Proc. ICLR.", "citeRegEx": "Xiong et al\\.,? 2017", "shortCiteRegEx": "Xiong et al\\.", "year": 2017}, {"title": "Words or Characters? Fine-grained Gating for Reading Comprehension", "author": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "Proc. ICLR.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou."], "venue": "ArXiv:1610.09996.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D Zeiler."], "venue": "ArXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering", "author": ["Junbei Zhang", "Xiaodan Zhu", "Qian Chen", "Lirong Dai", "Hui Jiang."], "venue": "ArXiv:1703.04617.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 6, "context": "Recently introduced large-scale datasets like CNN/Daily Mail (Hermann et al., 2015), the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 15, "context": ", 2015), the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016) and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO; Nguyen et al.", "startOffset": 49, "endOffset": 80}, {"referenceID": 13, "context": ", 2016) and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO; Nguyen et al., 2016) have (a) The high-level structure of BIDAF.", "startOffset": 64, "endOffset": 95}, {"referenceID": 16, "context": "BIDAF (Seo et al., 2017) represents one of ar X iv :1 70 4.", "startOffset": 6, "endOffset": 24}, {"referenceID": 15, "context": "Unlike other datasets that such as CNN/Daily Mail whose questions are synthesized, Rajpurkar et al. (2016) uses a crowdsourcing platform to generate realistic question and answer pairs.", "startOffset": 83, "endOffset": 107}, {"referenceID": 16, "context": "In this section, we review the BIDAF model (Seo et al., 2017) and introduce our extension, the Ruminating Reader.", "startOffset": 43, "endOffset": 61}, {"referenceID": 11, "context": "It does so using a convolutional neural network with max pooling over learned character vectors (Lee et al., 2017; Kim et al., 2016).", "startOffset": 96, "endOffset": 132}, {"referenceID": 10, "context": "It does so using a convolutional neural network with max pooling over learned character vectors (Lee et al., 2017; Kim et al., 2016).", "startOffset": 96, "endOffset": 132}, {"referenceID": 19, "context": "The character embedding and the word embedding are concatenated and passed into a two-layer highway network (Srivastava et al., 2015) to obtain a d dimensional vector representation of each single word.", "startOffset": 108, "endOffset": 133}, {"referenceID": 8, "context": "Sequence Encoding Layers As in BIDAF, we use two LSTM RNNs (Hochreiter and Schmidhuber, 1997) with d-dimensional outputs to encode the context and query representations in both directions.", "startOffset": 59, "endOffset": 93}, {"referenceID": 20, "context": "This approach, while somewhat inefficient, proves to be an valuable addition to the model and allows it to better track position information, loosely following the positional encoding strategy of Sukhbaatar et al. (2015). Hence we obtain S\u0303C \u2208 R2d\u00d7C , which is fused with context encoding C via a gate:", "startOffset": 196, "endOffset": 221}, {"referenceID": 1, "context": "Much of this recent work has been based on end-to-end trained neural network models, and within that, most have used recurrent neural networks with soft attention (Bahdanau et al., 2015), which emphasizes one part of the data over the others.", "startOffset": 163, "endOffset": 186}, {"referenceID": 21, "context": "Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context.", "startOffset": 240, "endOffset": 262}, {"referenceID": 18, "context": "Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 18, "context": "Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context. Xiong et al. (2017) propose the Dynamic Coattention Network, which uses co-dependent representations of the question and the context, and iteratively updates the start and end indices to recover from local maxima and to find the optimal answer span.", "startOffset": 241, "endOffset": 328}, {"referenceID": 18, "context": "Most papers on single-pass reasoning systems propose novel ways to use the attention mechanism: Wang and Jiang (2016) propose matchLSTM to model the interaction between context and query, as well as introducing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context. Xiong et al. (2017) propose the Dynamic Coattention Network, which uses co-dependent representations of the question and the context, and iteratively updates the start and end indices to recover from local maxima and to find the optimal answer span. Wang et al. (2016) propose the Multi-Perspective Context Matching model that matches the encoded context with query by combining various matching strategies, aggregates matching vector with bidirectional LSTM, and predict start and end positions.", "startOffset": 241, "endOffset": 577}, {"referenceID": 7, "context": "In order to merge the entity score during its multiple appearence, Kadlec et al. (2016) propose attention-sum reader who computes dot product between context and query encoding, does a softmax operation over context and sums the probability over the same entity to favor the frequent entities over rare ones.", "startOffset": 67, "endOffset": 88}, {"referenceID": 3, "context": "Chen et al. (2016) propose to use a bilinear term to calculate the attentional alignment between context and query.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "(2015) apply attention on window-based memory, by extending multi-hop end-to-end memory network (Sukhbaatar et al., 2015).", "startOffset": 96, "endOffset": 121}, {"referenceID": 18, "context": "The Iterative Alternative (IA) reader (Sordoni et al., 2016) produces query glimpse and document glimpse in each iterations and uses both glimpses to update recurrent state in each iteration.", "startOffset": 38, "endOffset": 60}, {"referenceID": 6, "context": "Among multi-hop reasoning systems: Hill et al. (2015) apply attention on window-based memory, by extending multi-hop end-to-end memory network (Sukhbaatar et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 5, "context": "Dhingra et al. (2016) extend attention-sum reader to multi-turn reasoning with an added gating mechanism.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "Dhingra et al. (2016) extend attention-sum reader to multi-turn reasoning with an added gating mechanism. The Iterative Alternative (IA) reader (Sordoni et al., 2016) produces query glimpse and document glimpse in each iterations and uses both glimpses to update recurrent state in each iteration. Shen et al. (2017) propose a multi-hop attention model that used reinforcement learning to dynamically determine when to stop digesting intermediate information and produce an answer.", "startOffset": 0, "endOffset": 317}, {"referenceID": 28, "context": "We use the AdaDelta optimizer (Zeiler, 2012) for optimization.", "startOffset": 30, "endOffset": 44}, {"referenceID": 16, "context": "Our model configuration closely follows that of Seo et al. (2017) did: In the character encoding layer, we use 100 filters of width 5.", "startOffset": 48, "endOffset": 66}, {"referenceID": 2, "context": "We selected hyperparameter values through random search (Bergstra and Bengio, 2012).", "startOffset": 56, "endOffset": 83}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al.", "startOffset": 2, "endOffset": 46}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al. (2017); d Wang and Jiang (2016); e Xiong et al.", "startOffset": 2, "endOffset": 68}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al. (2017); d Wang and Jiang (2016); e Xiong et al.", "startOffset": 2, "endOffset": 93}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al. (2017); d Wang and Jiang (2016); e Xiong et al. (2017); f Seo et al.", "startOffset": 2, "endOffset": 116}, {"referenceID": 11, "context": "a Rajpurkar et al. (2016); b Yu et al. (2016); c Yang et al. (2017); d Wang and Jiang (2016); e Xiong et al. (2017); f Seo et al. (2017); g Lee et al.", "startOffset": 2, "endOffset": 137}, {"referenceID": 9, "context": "(2017); g Lee et al. (2016); h Wang et al.", "startOffset": 10, "endOffset": 28}, {"referenceID": 9, "context": "(2017); g Lee et al. (2016); h Wang et al. (2016); i Weissenborn et al.", "startOffset": 10, "endOffset": 50}, {"referenceID": 9, "context": "(2017); g Lee et al. (2016); h Wang et al. (2016); i Weissenborn et al. (2017); j Chen et al.", "startOffset": 10, "endOffset": 79}, {"referenceID": 3, "context": "(2017); j Chen et al. (2017); k Shen et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 3, "context": "(2017); j Chen et al. (2017); k Shen et al. (2017); l Zhang et al.", "startOffset": 10, "endOffset": 51}, {"referenceID": 3, "context": "(2017); j Chen et al. (2017); k Shen et al. (2017); l Zhang et al. (2017); \u2021 Unpublished", "startOffset": 10, "endOffset": 74}, {"referenceID": 29, "context": "At the time of submission, our model is tied in accuracy on the hidden test set with the bestperforming published single model (Zhang et al., 2017).", "startOffset": 127, "endOffset": 147}, {"referenceID": 29, "context": "Comparing to the jNet (Zhang et al., 2017) whose success answers occupy 69.", "startOffset": 22, "endOffset": 42}, {"referenceID": 29, "context": "Performance Breakdown Following Zhang et al. (2017), we break down Ruminating Reader\u2019s 79.", "startOffset": 32, "endOffset": 52}], "year": 2017, "abstractText": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BIDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BIDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BIDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems.", "creator": "LaTeX with hyperref package"}}}