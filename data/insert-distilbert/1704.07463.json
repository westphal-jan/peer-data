{"id": "1704.07463", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Streaming Word Embeddings with the Space-Saving Algorithm", "abstract": "... we develop a streaming ( coherent one - pass, bounded - memory ) word embedding algorithm based on the canonical skip - gram calculation with his negative sampling algorithm implemented later in prototype word2vec. we essentially compare additionally our streaming algorithm to word2vec empirically by measuring the cosine similarity between available word copy pairs under each algorithm and by applying each algorithm in the downstream task of hashtag prediction on within a two - month interval of the twitter sample stream. we then discuss the results of these tracking experiments, concluding they provide partial validation of our approach as a streaming replacement for word2vec. finally, we discuss potential failure modes and suggest directions for future work.", "histories": [["v1", "Mon, 24 Apr 2017 20:55:33 GMT  (3077kb,D)", "http://arxiv.org/abs/1704.07463v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chandler may", "kevin duh", "benjamin van durme", "ashwin lall"], "accepted": false, "id": "1704.07463"}, "pdf": {"name": "1704.07463.pdf", "metadata": {"source": "CRF", "title": "Streaming Word Embeddings with the Space-Saving Algorithm", "authors": ["Chandler May", "Kevin Duh", "Benjamin Van Durme", "Ashwin Lall"], "emails": ["cjmay@jhu.edu", "kevinduh@cs.jhu.edu", "vandurme@cs.jhu.edu", "lalla@denison.edu"], "sections": [{"heading": "1 Introduction", "text": "Word embedding algorithms such as the skip-gram with negative sampling (SGNS) (Mikolov et al., 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011). Existing word embedding training algorithms process the training data in a batch fashion, passing over it once to estimate a fixed-size vocabulary and then one or more additional times to learn a word embedding model based on that vocabulary. In this study we augment the word2vec word embedding training algorithm to process training data\n1 https://code.google.com/archive/p/ word2vec/\nin a streaming fashion, performing all processing in one pass with bounded memory usage. This mode of operation facilitates applications in which the data is too large to store on disk or to scan more than once, and is potentially even infinitely large.\nWe assume the data comprises a stream of sentences (si)i = (s1, s2, . . .). The sentence at time t, denoted st, consists of a tuple of nt words( w\n(t) 1 , w (t) 2 , w (t) 3 , . . . , w (t) nt\n) ; hereafter we drop the\nindex t for simplicity. Our goal is to derive a onepass, bounded-memory algorithm that, at any time step t, can return word embeddings\u2014vector representations of words inRD\u2014that approximately capture the same semantics as word embeddings learned by a batch algorithm trained on (s1, s2, . . . , st). Here we defer a rigorous definition of those semantics but illustrate our intent through intrinsic and extrinsic experiments that measure desirable properties of a streaming word embedding training algorithm with respect to the corresponding batch algorithm.\nExisting word embedding algorithms often use a first pass through the data to find a K-word vocabulary of words to embed, then use a second pass to learn the embeddings of those words, ignoring outof-vocabulary (OOV) words. Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities. Conversely, treating words as atomic units precludes test-time inference of unrecognized words in the absence of surrounding context. We ar X\niv :1\n70 4.\n07 46\n3v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\ndo not address this trade-off in the present study but suggest incorporating sub-word unit representations into our approach in future work.\nIn the present study, we augment word2vec to handle a potentially unbounded vocabulary in bounded memory using the space-saving algorithm (Metwally et al., 2005), updating an unsmoothed negative sampling distribution online using reservoir sampling (Vitter, 1985). We call this augmented, one-pass, bounded-memory algorithm spacesaving-word2vec."}, {"heading": "1.1 Incremental SGNS", "text": "The method introduced in this paper, spacesaving-word2vec, is similar to the recently proposed incremental SGNS (Kaji and Kobayashi, 2017), developed independently. There are three main differences.\nFirst, whereas spacesaving-word2vec uses the space-saving algorithm to maintain an approximate vocabulary, incremental SGNS uses the MisraGries algorithm (Misra and Gries, 1982). Prior work found the space-saving algorithm to outperform a modern implementation of the Misra-Gries algorithm (Demaine et al., 2002) and many other streaming frequent-item algorithms in both error and runtime (Cormode and Hadjieleftheriou, 2008).\nSecond, whereas spacesaving-word2vec uses standard reservoir sampling to estimate an unsmoothed negative sampling distribution, Kaji and Kobayashi (2017) develop a modified reservoir sampling algorithm to estimate a smoothed negative sampling distribution. In prior work, smoothing the negative sampling distribution was shown to increase word embedding quality consistently (Levy et al., 2015).\nThird, whereas spacesaving-word2vec employs a separate, thresholded, linearly decaying learning rate for each embedding, resetting the learning rate for an embedding each time the corresponding word in the space-saving data structure is replaced, incremental SGNS employs AdaGrad to adaptively set per-word learning rates (Duchi et al., 2011). The vanishing learning rates estimated by AdaGrad (Zeiler, 2012) may be inappropriate in the streaming setting if the data is not i.i.d. and we desire a model that gives similar weight to data from the beginning, middle, and end of the stream.\nIn light of the previous discussion, we cast our contribution as a complementary implementation and analysis to that of Kaji and Kobayashi (2017). We also release an open-source C++ implementation of our algorithm in hopes of enabling comparison and future work.\nIn what follows, we first review word2vec, the space-saving algorithm, and reservoir sampling in Section 2. We then introduce our algorithm, referred to as spacesaving-word2vec, in Section 3. In Section 4 we describe and report the results of intrinsic experiments on word embeddings learned by word2vec and spacesaving-word2vec; then, in Section 5, we describe and report on the results of an application of word2vec and spacesaving-word2vec in a downstream task of hashtag prediction on Twitter. We discuss modeling and implementation questions along with the results of our experiments in Section 6, also offering suggestions for future work, and conclude in Section 7.\n2 Background\n2.1 word2vec\nThe intuition behind word2vec is to build on the distributional hypothesis (Harris, 1954; Sahlgren, 2008) and map words used in similar contexts to nearby vectors in Euclidean space. Let w be a word in the vocabulary and let vw \u2208 RD denote the targetword embedding of w in Euclidean space. Let x be a word in a context of w (for example, a word cooccurring with w in a sentence) and let v\u2032x \u2208 RD denote the context-word embedding of x. Starting with a random initialization of all target-word and context-word embeddings, the SGNS algorithm of word2vec uses stochastic gradient descent to try to maximize \u03c3 (\u3008vw, v\u2032x\u3009) for co-occurring words w and x, where \u03c3 is the sigmoid function, \u03c3(t) = 1/(1 + exp(\u2212t)). Simultaneously, to constrain the problem, SGNS seeks to minimize \u03c3 ( \u3008vw, v\u2032y\u3009 ) for words w and y that do not co-occur. Thus, if both w and z occur in similar contexts (for example, both co-occur with x), word2vec will learn similar target-word embeddings for them.\nThe complete objective function of SGNS then comprises the sum over all co-occurring target-\nword\u2013context-word pairs (w, x) of the expression log \u03c3 ( \u3008vw, v\u2032x\u3009 ) + S Ey\u223cP [ log \u03c3 ( \u2212\u3008vw, v\u2032y\u3009 )] (1)\nwhere S is a fixed pre-specified integer, P is a negative sampling distribution over words, and the expectation over y is estimated by the mean of S i.i.d. samples from P. The negative sampling distribution represents random noise and is implemented in practice as a smoothed empirical unigram distribution, where smoothing is accomplished by raising all word probabilities to the 0.75 power and renormalizing.\nWhile Equation (1) can be optimized in an online fashion, one word context at a time, word2vec first scans through the entire data to compute the vocabulary (the set of all word types occurring in the data, often truncated in practice to only those words occurring at least five or so times) and negative sampling distribution. To augment word2vec to learn word embeddings in a single pass, using bounded memory, we must develop an approach to handle an unknown and potentially unbounded vocabulary and estimate a usable negative sampling distribution P over it."}, {"heading": "2.2 Space-Saving Algorithm", "text": "The space-saving algorithm is a one-pass algorithm that estimates the most frequent items in a stream in bounded memory (Metwally et al., 2005). It does so by maintaining an array of K items (for a prespecified number K) and corresponding array of K counts; when K unique items have been seen and a new item is encountered, the item with smallest count is replaced while the corresponding count is incremented. The space-saving algorithm thus overestimates item counts but enjoys a bound of n/K on the error in each count, where n is the total number of items (including repeats) seen so far.\nConsider a stream of items (xi)i. To initialize the space-saving algorithm, a size-K array of samples (z1, . . . , zK) is initialized with empty values and a corresponding array of their respective counts (c1, . . . , cK) is initialized with zeroes. At each time step i, item xi is observed and one of three actions is taken.\n1. If xi is in the array (z1, . . . , zK) then increment the corresponding count in (c1, . . . , cK)\nby one.\n2. If xi is not in the array (z1, . . . , zK) but the array is not full (we have not seenK unique items yet) then set the next empty array slot to xi and increment the corresponding count by one.\n3. Otherwise replace the element in (z1, . . . , zK) with smallest count with xi and increment the corresponding count by one.\nAt any time i in the stream the space-saving data structure contains all items with true count greater than i/K seen so far and over-estimates all counts by at most i/K (Metwally et al., 2005). The spacesaving data structure can be efficiently implemented with a min-heap, with multiple linked lists, or with multiple arrays with additional pointers (emulating linked lists for the special case of the space-saving algorithm)."}, {"heading": "2.3 Reservoir Sampling", "text": "Reservoir sampling is a one-pass algorithm that computes a uniform subsample of a stream in bounded memory (Vitter, 1985). To initialize the sampler, an array of K items (r1, . . . , rK) is initialized with empty values and a counter n is initialized to zero. Then, when stream item xi is seen, n is incremented by one and one of two actions is taken.\n1. If i \u2264 K, set ri to xi.\n2. Otherwise draw an integer k uniformly from {1, 2, . . . , n} and, if k is less than or equal to K, replace rk with xi.\nAt any time i in the stream the reservoir contains a uniform sample of the items seen so far (Vitter, 1985)."}, {"heading": "3 spacesaving-word2vec", "text": "To learn word embeddings in one pass using bounded memory we use the space-saving algorithm to create and update a vocabulary of K frequent words and we use reservoir sampling to maintain a negative sampling distribution over that vocabulary. Specifically, we start the learning process by initializing all word embeddings randomly and initializing the space-saving data structure and negative sampling reservoir as empty. For each sentence\nData: Stream of sentences (si)i, vocabulary size K, negative sampling reservoir size N . Result: Any-time word embeddings indexed against space-saving data structure. initialize empty size-K space-saving data structure and size-N negative sampling reservoir ; initialize input, output word embeddings ; for ever do\nread sentence s = (w1, . . . , wJ) (a tuple of words) ; subsample sentence s\u2032 = (w\u20321, . . . , w \u2032 J \u2032) ; insert words from s\u2032 into space-saving data structure and their space-saving data structure indices into reservoir ; for each skip-gram context in s\u2032 do if all words in context are in space-saving data structure then\ntake SGNS gradient steps on space-saving data structure indices corresponding to words in this context ;\nend end\nend Algorithm 1: The spacesaving-word2vec algorithm, simplified for ease of exposition. See the text for details, and see Algorithm 2 for a full algorithm listing.\nin the data stream we first subsample the words in the sentence, following word2vec. We then insert all retained word tokens into the space-saving data structure and negative sampling reservoir. In each fixed-length context window, we then check if all words are present in the space-saving data structure. If so, we iterate over the left and right context words of the central target word, performing a gradient step of Equation (1) for each context-word\u2013 target-word pair in turn. Simplified pseudo-code of the spacesaving-word2vec algorithm is given in Algorithm 1; more detailed pseudo-code is provided in Algorithm 2.\nWhile the high-level description of spacesaving-word2vec is straightforward there are several important implementation choices; we summarize them here. We take care to distinguish between a word w and its index in the space-saving data structure, which we denote by k. By nature of the space-saving algorithm, a given index k can be associated with different words at different points during training.\n\u2022 Whenever a word w is ejected from the spacesaving data structure its target-word embedding vk and context-word embedding v\u2032k are re-initialized as draws from N (0, 1)D. This means that if w appears in the future and is inserted again into space-saving data struc-\nture, training of its embedding starts over from scratch.\n\u2022 Sentence subsampling retains all words not in the space-saving data structure, and retains each word w in the space-saving data structure with probability min(1, \u221a \u03b4/fk), where fk\nis the count of space-saving data structure element k. All other words in the sentence are discarded.\n\u2022 When words are discarded by sentence subsampling, we do not add them to the spacesaving data structure and negative sampling reservoir. When a sentence smaller (after subsampling) than a single context window is encountered, its words are first added to the space-saving data structure and negative sampling reservoir, and then the algorithm moves on to the next sentence, skipping the embedding gradient updates.\n\u2022 When a skip-gram context contains an outof-vocabulary word, the entire skip-gram is ignored. (However, for small context window sizes we expect this event to occur only rarely, as the space-saving data structure empirically has a long tail, and when the outof-vocabulary check is performed all words in\nthe context would have just received an incremented count.)\n\u2022 Our negative sampling approach differs from that of word2vec in that we draw from the empirical distribution over words in the stream instead of over a smoothed empirical distribution.\n\u2022 We maintain a separate, thresholded, linearly decaying learning rate for each of the K bins in the space-saving data structure, resetting a learning rate when the corresponding word is ejected from the space-saving data structure.\nWe leave a thorough investigation of the impacts of these design choices to future work. Our C++ code implementing the spacesaving-word2vec algorithm with these choices is released as open-source software to facilitate reproducibility and future work.2"}, {"heading": "4 Intrinsic Evaluation", "text": "To inspect the empirical operation of the spacesaving-word2vec algorithm, we perform two intrinsic evaluations. First, we estimate the errors in word counts in the space-saving data structure for different space-saving data structure sizes, noting that those errors affect both the vocabulary and the negative sampling distribution. Second, for a large number of word pairs in the shared vocabulary, we compare the cosine similarity of each pair under word2vec to the cosine similarity under spacesaving-word2vec. The aim of this second experiment is to measure a rudimentary formulation of the degree to which spacesaving-word2vec embeddings approximate word2vec embeddings.\nFor both intrinsic evaluations, we use text8,3 the first 100 MB of cleaned text from a Wikipedia dump dated 3 March 2006.4 To estimate the errors in space-saving data structure word counts, we treat the data set as a stream of words and apply the spacesaving algorithm to that stream separately for spacesaving data structure sizes (K) of 7000, 70 000, and\n2 https://github.com/cjmay/athena 3 http://mattmahoney.net/dc/text8.zip 4 http://mattmahoney.net/dc/textdata.\nhtml\n700 000, learning three different sets of approximate word counts. There are a total of 253 854 word types in the text8 data set, so vocabularies of size 7000 or 70 000 are truncated and represent streaming approximations, whereas a vocabulary of size 700 000 contains all word types.\nFor each space-saving data structure size, we compute the true word counts in the data set and plot the relative error of the approximate count of each word with respect to its rank in the true ordering by frequency in the data set (descending). When computing and plotting these errors we estimate the count of words dropped from the spacesaving data structure as the value of the smallest counter in the space-saving data structure. Separate plots for the three streaming approximations (three values of K) are shown in Figure 1. For K = 7000 and K = 70000, we observe small error for highfrequency words and a slow increase in error with respect to rank initially, followed by a distinctive linear increase in error with respect to rank for lowerfrequency words. In both cases the relative error is around one or two for many words; however, we note there are ten times as many words portrayed in the K = 70000 plot, hence the accumulated error is much smaller. In the K = 7000 case the dropped counts lie on the positive-slope line to the right of the kink; in the K = 70000 case the dropped counts are the points displayed as horizontal lines emanating rightward from a similar positive-slope line. The K = 70000 relative errors, omitting dropped words, are depicted in Figure 2 for the sake of comparison. For K = 700000 we find zero error for all word ranks, reflecting the fact that the entire vocabulary now fits in the space-saving data structure.\nNext, we assess whether the embeddings learned by spacesaving-word2vec have similar pairwise distances as the embeddings learned by word2vec. We interpret this evaluation as a coarse measurement of how much the spacesaving-word2vec embeddings approximate the word2vec embeddings. As before, we perform this experiment separately for vocabulary sizes 7000, 70 000, and 700 000; word2vec and spacesaving-word2vec are each trained on text8 using each vocabulary size in turn. For each vocabulary size, to help illustrate how the spacesaving-word2vec handles different\nclasses of words, three 100-word intervals are selected from the true vocabulary ordered by frequency (descending), namely: words with ranks 1 to 100, words with ranks 801 to 900, and words with ranks 6401 to 6500. For each pair of these intervals, word pairs (comprising one word from the first interval in the pair and another word from the second) are drawn uniformly at random and their cosine similarity under the spacesaving-word2vec model is plotted against their cosine similarity under the word2vec model. These plots are shown in Figure 3 for K = 7000, Figure 4 for K = 70000, and Figure 5 for K = 700000. Pearson correlation coefficients are reported above each plot; for K = 70000 and K = 700000 we find correlation coefficients in the range of 0.8 for all pairs of intervals, suggesting that the word similarities of spacesaving-word2vec approximate those of word2vec. For the more aggressive vocabulary size K = 7000 the correlation coefficients are lower, even near zero for low-frequency word intervals. Moreover, when the correlation coefficient of the words in the space-saving data structure is near zero the fraction of word similarities that are undefined due to words being dropped from the space-saving data structure is also high, near 0.6 or 0.85 depending on the particular word intervals under consideration. Considering the true vocabulary size of 253 854, the K = 7000 case, in which these deficiencies manifest, is perhaps most interesting because it yields an appreciable memory savings\nrelative to the true vocabulary. Interestingly, the similarities computed by spacesaving-word2vec are downward-biased for a trivially large space-saving data structure size of 700 000, approximately unbiased for a non-trivial space-saving data structure size of 70 000, and substantially upward-biased for the aggressive space-saving data structure size of 7000. We leave the investigation of the causes and effects of this differential bias to future work."}, {"heading": "5 Extrinsic Evaluation", "text": "We now compare the embeddings learned by word2vec and spacesaving-word2vec in an experiment designed to test the benefits of the online update afforded by spacesaving-word2vec. In particular, we apply the learned embeddings in the downstream task of hashtag prediction (Ding et al., 2012; Godin et al., 2013; Weston et al., 2014). Hashtags are user-defined and user-applied textual labels on social media posts. The task of hashtag prediction can be formalized as the prediction of zero more hashtags from the non-hashtag content of a post such that the post\u2019s author approves of the predicted hashtags for application to the post. This task can be operationalized using historical data by learning to predict the hashtags of a post that were in fact applied to it.\nIn our experiment, both word2vec and spacesaving-word2vec were trained on a sample of 1 123 701 Tweets from January 2016. Tweets were tokenized using the Tift Twitter tokenizer and then normalized by stripping hashtags, user mentions, and URLs and lower-casing the remaining text. In parallel with the word2vec and spacesaving-word2vec training on Tweet text, the space-saving algorithm was applied with 10 000 slots to track the top (lower-cased) hashtags in the data and a reservoir of size 100 000 was used to maintain a uniform sample of Tweets for each hashtag in that space-saving data structure. That collection of hashtags was filtered to the top 100 hashtags by frequency, then each hashtag\u2019s reservoir was truncated to 1000 samples to reduce skew. The resulting data set, comprising 88 413 Tweets, was used to train the classifiers. Then, holding the classifiers fixed, training of the embedding models was\nresumed and 100-dimensional hashtag predictions (a binary choice for each hashtag) were made by the respective classifiers on a sample of Tweets from February 2016. For simplicity, predictions were not made on Tweets that did not contain at least one of the 100 hashtags to be predicted; this simplification resulted in a test set of 680 579 Tweets.\nAs we are comparing the word2vec SGNS implementation to our spacesaving-word2vec SGNS implementation designed to mimic it\u2014and considering that we wish to assess performance in the non-i.i.d. streaming setting, in which we cannot practically tune hyperparameters on the stream and re-train\u2014we used the same hyperparameters for both methods, choosing the particular values according to recommendations for word2vec from prior work (Levy et al., 2015). Specifically, we used a context window size of two, dynamic context windows, five negative samples, and subsampling with a threshold of 10\u22123. The most consistently high-performing hyperparameter setting in prior work was context distribution smoothing of 0.75; we used this value in word2vec but in spacesaving-word2vec we sampled words from the unsmoothed empirical distribution for computational efficiency.\nThe remaining hyperparameters are the learning rate, the maximum size of the vocabulary (in spacesaving-word2vec, the size of the space-saving data structure), the number of points used to estimate the negative sampling distribution (in spacesaving-word2vec, the reservoir size), and the embedding dimension. Following word2vec, we used a linear learning rate starting at 2.5 \u00d7 10\u22122 and decaying to 2.5 \u00d7 10\u22126 over the first part of the stream, then fixed at 2.5 \u00d7 10\u22126 for the rest of the stream; we used a vocabulary of one hundred thousand; we used a negative sampling discretization (reservoir) of one hundred million; and we used an embedding dimension of one hundred.\nThe classification problem is formulated as a multi-label classification task: given a tuple of words representing a Tweet, transformed to a tuple of vectors by a given embedding model, the task is to output a 100-dimensional binary prediction in which 1 represents the presence of a hashtag and 0 represents its absence. Note that the classifier does not update the word vectors (as would be done in\nsupervised embedding approaches) but merely accepts them from the embedding model (which is updated in an unsupervised fashion) as input. Thus we can translate a dynamic and potentially infinite vocabulary to a static classifier defined on a finitedimensional input space.\nOur classifier model uses a convolutional neural network architecture that was found effective for short text classification in prior work (Kim, 2014; Kurata et al., 2016). Input word embeddings are convolved along the time dimension, then max-pooled and passed through a fully connected layer to generate the output vector, as in Figure 6. We use convolution window lengths of one, two, and three words, and a filtermap size of one hundred. We train the model using binary cross-entropy loss (Kurata et al., 2016; Nam et al., 2014). We denote the classifier trained on the word2vec model by static-cnn and the classifier trained on the spacesaving-word2vec model by stream-cnn. The training and testing schemes were the same for both classifiers; in particular, both word2vec and spacesaving-word2vecwere updated online during the test phase (on the February interval of tweets). However, in that test phase, word2vec marked all words not in its fixed vocabulary as OOV while spacesaving-word2vec updated its vocabulary online.\nAt the end of the February stream we separately measured aggregate classification performance for static-cnn and stream-cnn. For each of three classification metrics, namely precision, recall, and F1, we computed scores for each of the 100 labels, then aggregated them with a weighted average, weighting\nby the number of samples per label in the test set. The aggregated precision, recall, and F1 scores are shown in Table 1. We found stream-cnn achieved an F1 score of 0.42, similar to the F1 score of 0.43 achieved by static-cnn. Our original hypothesis was that stream-cnn would perform better than staticcnn due to its updated vocabulary; this result does not support that hypothesis. This point estimate of the performance difference may however be interpreted as a sanity check of the stream-cnn approach, suggesting only a small performance degradation."}, {"heading": "6 Discussion", "text": "We first emphasize that both experiments are works in progress and not meant to provide conclusive validation of spacesaving-word2vec in their current forms. Though the intrinsic experiment measured the error in the cosine similarities of spacesaving-word2vec with respect to word2vec, it is not clear what amount of error (and on what words) is acceptable to maintain a desired degree of qualitative fidelity in the model; this is a basic matter of sensitivity analysis. In particular, it is conceivable that the nearest neighbors of many words could change even in the presence of relatively high overall word similarity correlation. The downstream applications of word embedding models are complex, and we do not know what sensitivities or invariances they may have without measuring them.\nMeanwhile, though the extrinsic experiment appropriated a real-world task on real-world data, the particular experimental design is only illustrative of the motivating application. Virtually by construction, the hashtag set defining the target variable of the prediction class included many hashtags constituting spam, the Tweets of which were repetitive and arguably simplistic. The distribution of those hashtags over the classifier training data was also\nmarkedly skewed. Accordingly, the specific extrinsic experiment reported in this study might be construed as an artificial spam categorization task, and we might question whether word embeddings and a convolutional neural network classifier are necessary to perform the task in the first place. The extrinsic experiment requires substantial refinement in order to reflect the natural real-world hashtag prediction task, and a closer analysis of results (including comparison against strong baselines) is necessary before the validation potential of the experiment can be known.\nThere were several design choices in the extrinsic experiment on Twitter. These included the choice of time span of Tweets to use for initialization of the embedding models and for prediction by the classifier; the frequency at which predictions were made by the classifier (for example, predicting after every Tweet, after every B Tweets, or after all Tweets in the prediction set); the filtering, normalization, and segmentation of the Tweets; the construction of the set of hashtags to be predicted; the treatment of Tweets with no hashtags; the choice to update spacesaving-word2vec and word2vec at test time rather than keeping one or both embedding models fixed; and the estimation by each embedding algorithm of its own vocabulary (rather than an ablation analysis in which word2vec was seeded with the overall spacesaving-word2vec vocabulary). Our solutions to these choices are by no means the best solutions in terms of most thoroughly evaluating spacesaving-word2vec against word2vec, but for the sake of scope we must defer more indepth investigation to future work.\nIn particular, we did not analyze the dynamics of spacesaving-word2vec on non-stationary data in our experiments: though the extrinsic experiment used a real-world non-stationary data stream, we truncated the stream to a relatively short interval and we only measured aggregate performance at the end of that interval. Therefore we must leave empirical testing of the ability of spacesaving-word2vec to accommodate a highly dynamic stream, including a stream exhibiting sudden shifts in distribution, to future work. In particular, if embeddings of common words that stay in the space-saving data structure rotate during train-\ning, those word embeddings would not be useful in off-the-shelf downstream models, which often expect the embedding of a given word to be static.\nFuture work may benefit from studying the relationship between context distribution smoothing, in which the negative sampling distribution is made a smoothed version of the empirical unigram language model, and subsampling, in which words observed by the embedding model are subsampled from the input according to frequency. These investigations are particularly interesting in the streaming setting, in which context distribution smoothing is computationally expensive whereas subsampling is easy.\nThough prior work found context distribution smoothing consistently beneficial (Levy et al., 2015), in the preliminary experiments reported in this study we have not seen such a strong preference. Though this lack of confirmation may be due to the rudimentary nature of our experiments, we speculate that the over-counting of low-frequency words by the space-saving algorithm, and resultant overrepresentation of low-frequency words in the negative sampling distribution, may constitute context distribution smoothing as a happy side effect.\nThe frequency of ejections in the tail of the spacesaving data structure impact not only runtime but the utility of the learned embeddings, as rapid ejections could result in a word towards the beginning of a sentence being ejected before the last word of the sentence is added to the space-saving data structure and the embedding model is updated. However, if the space-saving data structure has a long tail, this event may occur only rarely. That is, if there is a long tail, after a target item is inserted in the spacesaving data structure a large number of subsequent items (the items with lowest count) must be inserted before the target item has a chance of being ejected. It may be useful to study the empirical frequency and impact of ejections of recently inserted words in future work.\nThe spacesaving-word2vec algorithm handles an unrecognized word at training time by adding it to the space-saving data structure, ejecting an existing word and resetting its embedding, and updating the new embedding based on the context of the unrecognized word. (Subsequent contexts of that word are then used to further update that embedding, assuming the word is not ejected from\nthe space-saving data structure in the meantime.) This approach treats each word type atomically and is thus agnostic to morphological structure; it cannot infer embeddings for unrecognized words out of context, and may yield a low-quality embedding for an unrecognized word the first time (or first few times) it is seen. We leave the measurement and resolution of these issues to future work; it may be beneficial in particular to back off to sub-word unit representations.\nThe algorithm proposed here is complementary to incremental SGNS, sharing a similar motivation and high-level approach while exhibiting several different implementation choices. We developed our algorithm independently, and do not compare it empirically to incremental SGNS, but doing so would be an interesting direction for future work. We moreover suggest an ablation study, evaluating each design decision in isolation in order to better understand the empirical operation of these algorithms and perhaps develop a superior third implementation by combining the best-performing components from spacesaving-word2vec and incremental SGNS.\nIn incremental SGNS, the modified reservoir sampling approach to estimating the smoothed negative sampling distribution either requires memory linear in the true vocabulary (in order to maintain cumulative weights of all words seen so far) or suffers an approximation error in computing the cumulative weights of low-probability words. Our approach faces the same memory-bias tradeoff as incremental SGNS, and we choose to achieve constant memory usage while over-estimating the frequency of low-probability words in the negative sampling distribution. A second source of bias is introduced in incremental SGNS by deterministically making the expected number of insertions into the reservoir, instead of sampling the number of insertions, for computational efficiency. The spacesaving-word2vec negative sampling distribution is not smoothed, hence we afford constant-time reservoir updates without incurring this additional source of bias.5\n5 Incidentally, the original word2vec implementation uses a deterministic construction of the negative sampling table and is also biased with respect to the smoothed empirical unigram distribution.\nCompellingly, the optimal solution of incremental SGNS is shown to have an objective value under the batch SGNS objective function that converges in probability to the optimal batch SGNS objective value, given i.i.d. data (Kaji and Kobayashi, 2017). (However, the incremental SGNS objective function in this analysis employs an unbiased incremental negative sampling distribution, and so differs from the objective function that is implemented.) Empirically, when the effective vocabularies are constrained to be similar in size, incremental SGNS emulates the performance of batch SGNS and word2vec on semantic similarity and word analogy tasks. Updating a pre-trained incremental SGNS model on new data also yields a significant runtime improvement over re-training a batch SGNS model (or word2vec) on the combined old and new data.\nWe would be remiss not to comment on the practical performance of our approach. Despite implementing spacesaving-word2vec in C++ and closely following the tricks employed by word2vec, in informal experiments we found word2vec consistently faster by a factor of two or more in the main loop (after computing the vocabulary and negative sampling distribution), while the time taken by word2vec to compute the vocabulary and negative sampling distribution was relatively small. Indeed, the practical performance of word2vec is quantified by the runtime experiments comparing incremental SGNS to its batch version and word2vec in prior work (Kaji and Kobayashi, 2017). Specifically, the three algorithms are compared in runtime on the task of updating a pre-trained model based on new data. For batch SGNS and word2vec this is operationalized as retraining the model on both old and new data sets, while incremental SGNS is run only on the new data set. When the old data set comprises ten million words and the new data set comprises one million, incremental SGNS is found to achieve an overall speed-up of 7.3 over word2vec. However, in this experiment incremental SGNS is processing oneeleventh as much data; it is thus only two-thirds as fast as word2vec per word. Moreover, while batch SGNS and word2vec each scan the combined (old and new) data set twice in that experiment, effectively corresponding to 22 scans of the\nnew data set, incremental SGNS only scans the new data set once. We do not argue this is a shortcoming in the implementation of incremental SGNS; on the contrary, this performance gap reflects our experience with our own implementation and appears to point to clever manual optimization evident in the word2vec code. We therefore caution practitioners and implore other researchers to carefully consider whether the theoretical benefits of streaming algorithms such as these are nullified by practical inefficiencies.\nOn the note of applications, our extrinsic experiment used hashtag prediction as a motivating downstream task. A hashtag prediction model can be used to suggest hashtags to users writing new Tweets. It could also be used to provide additional (inferred) data to downstream analytics. However, this latter application raises a dual-use concern: if a user intentionally refrains from using a hashtag in order to escape categorization or publicity, a hashtag prediction model could reduce or remove that particular form of privacy potentially without the user\u2019s consent. The technology developed in the current work, enabling embeddings to be learned at greater scale, could thus be used to help a user or to harm them."}, {"heading": "7 Conclusion", "text": "We have developed a one-pass, bounded-memory variant of the popular SGNS algorithm for training word embeddings. Our approach, called spacesaving-word2vec after the word2vec implementation of SGNS on which it is based, leverages the space-saving algorithm and reservoir sampling in order to maintain an approximate, dynamic vocabulary and negative sampling distribution. Though preliminary experiments provide some evidence for the fidelity of spacesaving-word2vec to word2vec, there are still many open questions to be addressed. While we cannot yet wholeheartedly endorse the use of spacesaving-word2vec in real-world applications, we hope that thoughtful future research drawing on insights gleaned from spacesaving-word2vec and incremental SGNS (Kaji and Kobayashi, 2017) will close this gap."}, {"heading": "A Appendix", "text": "The complete spacesaving-word2vec learning algorithm is listed in Algorithm 2.\nData: Stream of sentences (si)i, subsampling threshold \u03b4, vocabulary size K, negative sampling reservoir size N , negative sample size S, embedding dimension D, context radius C, learning rate offset \u03c4 , learning rate exponent \u03ba. Result: Any-time word embeddings (vk : k \u2208 [K]) indexed against space-saving data structure \u03c6 : [K]\u2192 \u03a3\u2217. initialize empty size-K space-saving data structure and size-N negative sampling reservoir ; initialize input, output word embeddings (vk \u223c N (0, 1)D, v\u2032k \u223c N (0, 1)D) for all k \u2208 [K] ; initialize tk \u2190 1 for all k \u2208 [K] ; for ever do\nread sentence s = (w1, . . . , wJ) (a tuple of words) ; subsample sentence s\u2032 = (w\u20321, . . . , w \u2032 J \u2032) ; for 1 \u2264 j \u2264 J \u2032 do insert w\u2032j into space-saving data structure and its index into reservoir ; end for 1 \u2264 j \u2264 J \u2032 \u2212 2C do\nm\u2190 j + 2C ; // j is start of current context; m is end if all words w\u2032j , . . . , w\u2032m are in space-saving data structure then\nlet kj , . . . , km be space-saving data structure indices of w\u2032j , . . . , w \u2032 m ; // iterate over output word w\u2032` for j \u2264 ` \u2264 m do\n// w\u2032j+C is the input word if ` 6= j + C then // take gradient step on output word \u03b1\u2190 1\u2212 \u03c3(\u3008vkj+C , v\u2032k`\u3009) ; v\u2032k` \u2190 v \u2032 k`\n+ \u03c1k`\u03b1vkj+C ; u\u2190 \u03c1kj+C\u03b1v\u2032k` ; // take gradient steps on negative samples for 1 \u2264 j\u2032 \u2264 S do\ndraw k(\u2212) from negative sampling reservoir ; \u03b1\u2190 \u2212\u03c3(\u3008vkj+C , v\u2032k(\u2212)\u3009) ; v\u2032k(\u2212) \u2190 v \u2032 k(\u2212) + \u03c1k(\u2212)\u03b1vkj+C ;\nu\u2190 u+ \u03c1kj+C\u03b1v\u2032k(\u2212) ; end // take gradient step on input word vkj+C \u2190 vkj+C + u ; for indices k of union of input, output, negative sample words in this context do\ntk \u2190 tk + 1 ; end\nend end\nend end\nend Algorithm 2: Complete spacesaving-word2vec algorithm."}], "references": [{"title": "Factored neural language models", "author": ["Andrei Alexandrescu", "Katrin Kirchhoff."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Alexandrescu and Kirchhoff.,? 2006", "shortCiteRegEx": "Alexandrescu and Kirchhoff.", "year": 2006}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "Proceedings of the 31st International Conference on Machine Learning.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u2013 2537, Aug.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Methods for finding frequent items in data streams", "author": ["Graham Cormode", "Marios Hadjieleftheriou."], "venue": "Proceedings of the 34th International Conference on Very Large Data Bases (VLDB), pages 1530\u20131541.", "citeRegEx": "Cormode and Hadjieleftheriou.,? 2008", "shortCiteRegEx": "Cormode and Hadjieleftheriou.", "year": 2008}, {"title": "Frequency estimation of internet packet streams with limited space", "author": ["Erik D. Demaine", "Alejandro L\u00f3pez-Ortiz", "J. Ian Munro."], "venue": "Proceedings of the European Symposium on Algorithms (ESA), pages 348\u2013 360.", "citeRegEx": "Demaine et al\\.,? 2002", "shortCiteRegEx": "Demaine et al\\.", "year": 2002}, {"title": "Automatic hashtag recommendation for microblogs using topic-specific translation model", "author": ["Zhuoye Ding", "Qi Zhang", "Xuanjing Huang."], "venue": "Proceedings of the 24th International Conference on Computational Linguistics (COLING).", "citeRegEx": "Ding et al\\.,? 2012", "shortCiteRegEx": "Ding et al\\.", "year": 2012}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero Dos Santos", "Bianca Zadrozny."], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Using topic models for Twitter hashtag recommendation", "author": ["Fr\u00e9deric Godin", "Viktor Slavkovikj", "Wesley De Neve", "Benjamin Schrauwen", "Rik Van de Walle."], "venue": "Proceedings of the 22nd International Conference on World Wide Web, pages 593\u2013596.", "citeRegEx": "Godin et al\\.,? 2013", "shortCiteRegEx": "Godin et al\\.", "year": 2013}, {"title": "Distributional structure", "author": ["Zellig Harris."], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Incremental skip-gram model with negative sampling", "author": ["Nobuhiro Kaji", "Hayato Kobayashi."], "venue": "Preprint, arXiv:1704.03956, April.", "citeRegEx": "Kaji and Kobayashi.,? 2017", "shortCiteRegEx": "Kaji and Kobayashi.", "year": 2017}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Improved neural network-based multi-label classification with better initialization leveraging label cooccurrence", "author": ["Gakuto Kurata", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "citeRegEx": "Kurata et al\\.,? 2016", "shortCiteRegEx": "Kurata et al\\.", "year": 2016}, {"title": "Compositional-ly derived representations of morphologically complex words in distributional semantics", "author": ["Angeliki Lazaridou", "Marco Marelli", "Roberto Zamparelli", "Marco Baroni."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the 17th Conference on Computational Natural Language Learning, pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient computation of frequent and topk elements in data streams", "author": ["Ahmed Metwally", "Divyakant Agrawal", "Amr El Abbadi."], "venue": "Proceedings of the 10th International Conference on Database Theory, pages 398\u2013412.", "citeRegEx": "Metwally et al\\.,? 2005", "shortCiteRegEx": "Metwally et al\\.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Finding repeated elements", "author": ["Jayadev Misra", "David Gries."], "venue": "Science of Computer Programming, 2(2):143\u2013152.", "citeRegEx": "Misra and Gries.,? 1982", "shortCiteRegEx": "Misra and Gries.", "year": 1982}, {"title": "Largescale multi-label text classification \u2014 revisiting neural networks", "author": ["Jinseok Nam", "Jungi Kim", "Eneldo Loza Menc\u0131\u0301a", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "venue": null, "citeRegEx": "Nam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2014}, {"title": "The distributional hypothesis", "author": ["Magnus Sahlgren."], "venue": "Rivista di Linguistica, 20(1):33\u201353.", "citeRegEx": "Sahlgren.,? 2008", "shortCiteRegEx": "Sahlgren.", "year": 2008}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455\u2013465. Asso-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1627\u2013", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384\u2013394, Uppsala,", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Random sampling with a reservoir", "author": ["Jeffrey S Vitter."], "venue": "ACM Transactions on Mathematical Software (TOMS), 11(1):37\u201357.", "citeRegEx": "Vitter.,? 1985", "shortCiteRegEx": "Vitter.", "year": 1985}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Mengqiu Wang", "Christopher D Manning."], "venue": "Proceedings of the 4th International Joint Conference on Natural Language Processing, pages 1285\u2013 1291.", "citeRegEx": "Wang and Manning.,? 2013", "shortCiteRegEx": "Wang and Manning.", "year": 2013}, {"title": "TagSpace: Semantic embeddings from hashtags", "author": ["Jason Weston", "Sumit Chopra", "Keith Adams."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1822\u20131827, Doha, Qatar, October. Association", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "Preprint, arXiv:1212.5701, December.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "Word embedding algorithms such as the skip-gram with negative sampling (SGNS) (Mikolov et al., 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 23, "context": ", 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011).", "startOffset": 121, "endOffset": 211}, {"referenceID": 25, "context": ", 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011).", "startOffset": 121, "endOffset": 211}, {"referenceID": 21, "context": ", 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011).", "startOffset": 121, "endOffset": 211}, {"referenceID": 2, "context": ", 2013) method of word2vec1 have led to improvements in the performance of many natural language processing applications (Turian et al., 2010; Wang and Manning, 2013; Socher et al., 2013; Collobert et al., 2011).", "startOffset": 121, "endOffset": 211}, {"referenceID": 15, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 22, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 0, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 1, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 13, "context": "Methods for inferring embeddings of OOV words generally back off to sub-word unit representations (Luong et al., 2013; Dos Santos and Zadrozny, 2014; Soricut and Och, 2015; Alexandrescu and Kirchhoff, 2006; Botha and Blunsom, 2014; Lazaridou et al., 2013), but this approach may falter on important classes of words like named entities.", "startOffset": 98, "endOffset": 255}, {"referenceID": 16, "context": "In the present study, we augment word2vec to handle a potentially unbounded vocabulary in bounded memory using the space-saving algorithm (Metwally et al., 2005), updating an un-", "startOffset": 138, "endOffset": 161}, {"referenceID": 24, "context": "smoothed negative sampling distribution online using reservoir sampling (Vitter, 1985).", "startOffset": 72, "endOffset": 86}, {"referenceID": 10, "context": "recently proposed incremental SGNS (Kaji and Kobayashi, 2017), developed independently.", "startOffset": 35, "endOffset": 61}, {"referenceID": 18, "context": "First, whereas spacesaving-word2vec uses the space-saving algorithm to maintain an approximate vocabulary, incremental SGNS uses the MisraGries algorithm (Misra and Gries, 1982).", "startOffset": 154, "endOffset": 177}, {"referenceID": 4, "context": "algorithm (Demaine et al., 2002) and many other streaming frequent-item algorithms in both error and runtime (Cormode and Hadjieleftheriou, 2008).", "startOffset": 10, "endOffset": 32}, {"referenceID": 3, "context": ", 2002) and many other streaming frequent-item algorithms in both error and runtime (Cormode and Hadjieleftheriou, 2008).", "startOffset": 84, "endOffset": 120}, {"referenceID": 10, "context": "Second, whereas spacesaving-word2vec uses standard reservoir sampling to estimate an unsmoothed negative sampling distribution, Kaji and Kobayashi (2017) develop a modified reservoir sampling algorithm to estimate a smoothed negative", "startOffset": 128, "endOffset": 154}, {"referenceID": 14, "context": "In prior work, smoothing the negative sampling distribution was shown to increase word embedding quality consistently (Levy et al., 2015).", "startOffset": 118, "endOffset": 137}, {"referenceID": 7, "context": "Third, whereas spacesaving-word2vec employs a separate, thresholded, linearly decaying learning rate for each embedding, resetting the learning rate for an embedding each time the corresponding word in the space-saving data structure is replaced, incremental SGNS employs AdaGrad to adaptively set per-word learning rates (Duchi et al., 2011).", "startOffset": 322, "endOffset": 342}, {"referenceID": 27, "context": "The vanishing learning rates estimated by AdaGrad (Zeiler, 2012) may be inappropriate in the streaming setting if the data is not i.", "startOffset": 50, "endOffset": 64}, {"referenceID": 7, "context": "Third, whereas spacesaving-word2vec employs a separate, thresholded, linearly decaying learning rate for each embedding, resetting the learning rate for an embedding each time the corresponding word in the space-saving data structure is replaced, incremental SGNS employs AdaGrad to adaptively set per-word learning rates (Duchi et al., 2011). The vanishing learning rates estimated by AdaGrad (Zeiler, 2012) may be inappropriate in the streaming setting if the data is not i.i.d. and we desire a model that gives similar weight to data from the beginning, middle, and end of the stream. In light of the previous discussion, we cast our contribution as a complementary implementation and analysis to that of Kaji and Kobayashi (2017). We also release an open-source C++ implementation", "startOffset": 323, "endOffset": 734}, {"referenceID": 9, "context": "The intuition behind word2vec is to build on the distributional hypothesis (Harris, 1954; Sahlgren, 2008) and map words used in similar contexts to nearby vectors in Euclidean space.", "startOffset": 75, "endOffset": 105}, {"referenceID": 20, "context": "The intuition behind word2vec is to build on the distributional hypothesis (Harris, 1954; Sahlgren, 2008) and map words used in similar contexts to nearby vectors in Euclidean space.", "startOffset": 75, "endOffset": 105}, {"referenceID": 16, "context": "that estimates the most frequent items in a stream in bounded memory (Metwally et al., 2005).", "startOffset": 69, "endOffset": 92}, {"referenceID": 16, "context": "At any time i in the stream the space-saving data structure contains all items with true count greater than i/K seen so far and over-estimates all counts by at most i/K (Metwally et al., 2005).", "startOffset": 169, "endOffset": 192}, {"referenceID": 24, "context": "Reservoir sampling is a one-pass algorithm that computes a uniform subsample of a stream in bounded memory (Vitter, 1985).", "startOffset": 107, "endOffset": 121}, {"referenceID": 24, "context": "At any time i in the stream the reservoir contains a uniform sample of the items seen so far (Vitter, 1985).", "startOffset": 93, "endOffset": 107}, {"referenceID": 5, "context": "In particular, we apply the learned embeddings in the downstream task of hashtag prediction (Ding et al., 2012; Godin et al., 2013; Weston et al., 2014).", "startOffset": 92, "endOffset": 152}, {"referenceID": 8, "context": "In particular, we apply the learned embeddings in the downstream task of hashtag prediction (Ding et al., 2012; Godin et al., 2013; Weston et al., 2014).", "startOffset": 92, "endOffset": 152}, {"referenceID": 26, "context": "In particular, we apply the learned embeddings in the downstream task of hashtag prediction (Ding et al., 2012; Godin et al., 2013; Weston et al., 2014).", "startOffset": 92, "endOffset": 152}, {"referenceID": 14, "context": "and re-train\u2014we used the same hyperparameters for both methods, choosing the particular values according to recommendations for word2vec from prior work (Levy et al., 2015).", "startOffset": 153, "endOffset": 172}, {"referenceID": 12, "context": "We train the model using binary cross-entropy loss (Kurata et al., 2016; Nam et al., 2014).", "startOffset": 51, "endOffset": 90}, {"referenceID": 19, "context": "We train the model using binary cross-entropy loss (Kurata et al., 2016; Nam et al., 2014).", "startOffset": 51, "endOffset": 90}, {"referenceID": 14, "context": "Though prior work found context distribution smoothing consistently beneficial (Levy et al., 2015), in the preliminary experiments reported in this study we have not seen such a strong preference.", "startOffset": 79, "endOffset": 98}, {"referenceID": 10, "context": "data (Kaji and Kobayashi, 2017).", "startOffset": 5, "endOffset": 31}, {"referenceID": 10, "context": "iments comparing incremental SGNS to its batch version and word2vec in prior work (Kaji and Kobayashi, 2017).", "startOffset": 82, "endOffset": 108}, {"referenceID": 10, "context": "While we cannot yet wholeheartedly endorse the use of spacesaving-word2vec in real-world applications, we hope that thoughtful future research drawing on insights gleaned from spacesaving-word2vec and incremental SGNS (Kaji and Kobayashi, 2017) will close this gap.", "startOffset": 218, "endOffset": 244}], "year": 2017, "abstractText": "We develop a streaming (one-pass, boundedmemory) word embedding algorithm based on the canonical skip-gram with negative sampling algorithm implemented in word2vec. We compare our streaming algorithm to word2vec empirically by measuring the cosine similarity between word pairs under each algorithm and by applying each algorithm in the downstream task of hashtag prediction on a two-month interval of the Twitter sample stream. We then discuss the results of these experiments, concluding they provide partial validation of our approach as a streaming replacement for word2vec. Finally, we discuss potential failure modes and suggest directions for future work.", "creator": "LaTeX with hyperref package"}}}