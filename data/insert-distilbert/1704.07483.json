{"id": "1704.07483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Continuously Differentiable Exponential Linear Units", "abstract": "exponential linear units ( elus ) are a useful rectifier for constructing deep learning architectures, as they may speed up and otherwise improve learning by virtue if of not have vanishing pitch gradients and by having mean activations near zero. however, the elu activation as parametrized in [ 1 ] is not continuously differentiable with respect to its input when the shape distribution parameter alpha is not typically equal to 1. we present there an alternative generic parametrization which is c1 continuous for all values of alpha, making the rectifier easier to reason about and making alpha easier simply to tune. this alternative parametrization has several other useful properties that the original parametrization of elu does not not : 1 ) its derivative with respect to x is bounded, 2 ) it contains both partially the linear transfer function and relu as special cases, and 3 ) it is scale - similar with respect to linear alpha.", "histories": [["v1", "Mon, 24 Apr 2017 22:37:08 GMT  (196kb,D)", "http://arxiv.org/abs/1704.07483v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jonathan t barron"], "accepted": false, "id": "1704.07483"}, "pdf": {"name": "1704.07483.pdf", "metadata": {"source": "CRF", "title": "Continuously Differentiable Exponential Linear Units", "authors": ["Jonathan T. Barron"], "emails": ["barron@google.com"], "sections": [{"heading": null, "text": "continuous for all values of \u03b1, making the rectifier easier to reason about and making \u03b1 easier to tune. This alternative parametrization has several other useful properties that the original parametrization of ELU does not: 1) its derivative with respect to x is bounded, 2) it contains both the linear transfer function and ReLU as special cases, and 3) it is scale-similar with respect to \u03b1.\nThe Exponential Linear Unit as described in [1] is as follows:\nELU(x, \u03b1) = { x if x \u2265 0 \u03b1(exp(x)\u2212 1) otherwise\n(1)\nWhere x is the input to the function, and \u03b1 is a shape parameter. The derivative of this function with respect to x is:\nd\ndx ELU(x, \u03b1) = { 1 if x \u2265 0 \u03b1 exp(x) otherwise\n(2)\nIn Figures 1a and 1b we plot this activation and its derivative with respect to x for different values of \u03b1. We see that when \u03b1 6= 1, the activation\u2019s derivative is discontinuous at x = 0. Additionally we see that large values of \u03b1 can cause a large (\u201cexploding\u201d) gradient for small negative values of x, which may make training difficult.\nOur alternative parametrization of the ELU, which we dub \u201cCELU\u201d, is simply the ELU where the activation for negative values has been modified to ensure that the derivative at x = 0 for all values of \u03b1 is 1:\nCELU(x, \u03b1) = { x if x \u2265 0 \u03b1 ( exp ( x \u03b1 ) \u2212 1 ) otherwise (3)\nNote that ELU and CELU are identical when \u03b1 = 1:\n\u2200x ELU(x, 1) = CELU(x, 1) (4)\nThe derivative of the activation with respect to x and \u03b1 are as follows:\nd\ndx CELU(x, \u03b1) = { 1 if x \u2265 0 exp ( x \u03b1 ) otherwise\n(5)\nd\nd\u03b1 CELU(x, \u03b1) = { 0 if x \u2265 0 exp ( x \u03b1 ) ( 1\u2212 x\u03b1 ) \u2212 1 otherwise\nLike in ELU, derivatives for CELU can be computed efficiently by precomputing exp ( x \u03b1 ) and using it for the activation and its derivatives. Unlike ELU, CELU is scale-similar as a function of x and \u03b1:\nCELU(x, \u03b1) = 1\nc CELU(cx, c\u03b1) (6)\nThe CELU also converges to ReLU as \u03b1 approaches 0 from the right and converges to a linear \u201cno-op\u201d activation as \u03b1 approaches\u221e:\nlim \u03b1\u21920+ CELU(x, \u03b1) = max(0, x) (7)\nlim \u03b1\u2192\u221e CELU(x, \u03b1) = x (8)\nThis gives the CELU a nice interpretation as a way to interpolate between a ReLU and a linear function using \u03b1. Naturally, CELU can be slightly shifted in x and y such that it converges to any arbitrary shifted ReLU, in case negative activations are desirable even for small values of \u03b1."}], "references": [{"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "CoRR, abs/1511.07289,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Exponential Linear Units (ELUs) are a useful rectifier for constructing deep learning architectures, as they may speed up and otherwise improve learning by virtue of not have vanishing gradients and by having mean activations near zero [1].", "startOffset": 236, "endOffset": 239}, {"referenceID": 0, "context": "However, the ELU activation as parametrized in [1] is not continuously differentiable with respect to its input when the shape parameter \u03b1 is not equal to 1.", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "The Exponential Linear Unit as described in [1] is as follows:", "startOffset": 44, "endOffset": 47}], "year": 2017, "abstractText": "Exponential Linear Units (ELUs) are a useful rectifier for constructing deep learning architectures, as they may speed up and otherwise improve learning by virtue of not have vanishing gradients and by having mean activations near zero [1]. However, the ELU activation as parametrized in [1] is not continuously differentiable with respect to its input when the shape parameter \u03b1 is not equal to 1. We present an alternative parametrization which is C continuous for all values of \u03b1, making the rectifier easier to reason about and making \u03b1 easier to tune. This alternative parametrization has several other useful properties that the original parametrization of ELU does not: 1) its derivative with respect to x is bounded, 2) it contains both the linear transfer function and ReLU as special cases, and 3) it is scale-similar with respect to \u03b1. The Exponential Linear Unit as described in [1] is as follows: ELU(x, \u03b1) = { x if x \u2265 0 \u03b1(exp(x)\u2212 1) otherwise (1) Where x is the input to the function, and \u03b1 is a shape parameter. The derivative of this function with respect to x is: d dx ELU(x, \u03b1) = { 1 if x \u2265 0 \u03b1 exp(x) otherwise (2) In Figures 1a and 1b we plot this activation and its derivative with respect to x for different values of \u03b1. We see that when \u03b1 6= 1, the activation\u2019s derivative is discontinuous at x = 0. Additionally we see that large values of \u03b1 can cause a large (\u201cexploding\u201d) gradient for small negative values of x, which may make training difficult. Our alternative parametrization of the ELU, which we dub \u201cCELU\u201d, is simply the ELU where the activation for negative values has been modified to ensure that the derivative at x = 0 for all values of \u03b1 is 1: CELU(x, \u03b1) = { x if x \u2265 0 \u03b1 ( exp ( x \u03b1 ) \u2212 1 ) otherwise (3) Note that ELU and CELU are identical when \u03b1 = 1: \u2200x ELU(x, 1) = CELU(x, 1) (4) The derivative of the activation with respect to x and \u03b1 are as follows: d dx CELU(x, \u03b1) = { 1 if x \u2265 0 exp ( x \u03b1 ) otherwise (5) d d\u03b1 CELU(x, \u03b1) = { 0 if x \u2265 0 exp ( x \u03b1 ) ( 1\u2212 x \u03b1 ) \u2212 1 otherwise Like in ELU, derivatives for CELU can be computed efficiently by precomputing exp ( x \u03b1 ) and using it for the activation and its derivatives. Unlike ELU, CELU is scale-similar as a function of x and \u03b1: CELU(x, \u03b1) = 1 c CELU(cx, c\u03b1) (6) The CELU also converges to ReLU as \u03b1 approaches 0 from the right and converges to a linear \u201cno-op\u201d activation as \u03b1 approaches\u221e: lim \u03b1\u21920+ CELU(x, \u03b1) = max(0, x) (7) lim<lb>\u03b1\u2192\u221e<lb>CELU(x, \u03b1) = x<lb>(8) This gives the CELU a nice interpretation as a way to in-<lb>terpolate between a ReLU and a linear function using \u03b1.<lb>Naturally, CELU can be slightly shifted in x and y such that<lb>it converges to any arbitrary shifted ReLU, in case negative<lb>activations are desirable even for small values of \u03b1.<lb>References<lb>[1] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accu-<lb>rate deep network learning by exponential linear units (elus).<lb>CoRR, abs/1511.07289, 2015. 1<lb>ar<lb>X<lb>iv<lb>:1<lb>70<lb>4.<lb>07<lb>48<lb>3v<lb>1<lb>[<lb>cs<lb>.L<lb>G<lb>]<lb>2<lb>4<lb>A<lb>pr<lb>2<lb>01<lb>7", "creator": "LaTeX with hyperref package"}}}