{"id": "1704.08352", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "From Characters to Words to in Between: Do We Capture Morphology?", "abstract": "words can be represented by composing the coarse representations successively of subword units such something as short word row segments, characters, and / or each character n - grams. while such representations are effective and may capture the morphological vocabulary regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological stylistic typologies. on a language modeling task, we present experiments that systematically vary ( 1st 1 ) the basic primitive unit of representation, ( 2 ) match the composition of these representations, and ( 3 3 ) the specific morphological typology of the language modeled. because our results considerably extend previous findings that character mesh representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi - lstms outperforms most as others. but we also find room for improvement : typically none of the character - level models consistently match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.", "histories": [["v1", "Wed, 26 Apr 2017 21:10:53 GMT  (90kb,D)", "http://arxiv.org/abs/1704.08352v1", "Accepted at ACL 2017"]], "COMMENTS": "Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["clara vania", "adam lopez"], "accepted": true, "id": "1704.08352"}, "pdf": {"name": "1704.08352.pdf", "metadata": {"source": "CRF", "title": "From Characters to Words to in Between: Do We Capture Morphology?", "authors": ["Clara Vania", "Adam Lopez"], "emails": ["c.vania@ed.ac.uk,", "alopez@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Continuous representations of words learned by neural networks are central to many NLP tasks (Cho et al., 2014; Chen and Manning, 2014; Dyer et al., 2015). However, directly mapping a finite set of word types to a continuous representation has well-known limitations. First, it makes a closed vocabulary assumption, enabling only generic out-of-vocabulary handling. Second, it cannot exploit systematic functional relationships in learning. For example, cat and cats stand in the\nsame relationship as dog and dogs. While this relationship might be discovered for these specific frequent words, it does not help us learn that the same relationship also holds for the much rarer words sloth and sloths.\nThese functional relationships reflect the fact that words are composed from smaller units of meaning, or morphemes. For instance, cats consists of two morphemes, cat and -s, with the latter shared by the words dogs and tarsiers. Modeling this effect is crucial for languages with rich morphology, where vocabulary sizes are larger, many more words are rare, and many more such functional relationships exist. Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schu\u0308tze, 2015). A downside of these models is that they depend on morphological segmenters or analyzers.\nMorphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014). These models are compact, can represent rare and unknown words, and do not require morphological analyzers. They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters?\nThe relative merits of word, subword. and character-level models are not fully understood because each new model has been compared on\nar X\niv :1\n70 4.\n08 35\n2v 1\n[ cs\n.C L\n] 2\n6 A\npr 2\n01 7\ndifferent tasks and datasets, and often compared against word-level models. A number of questions remain open:\n1. How do representations based on morphemes compare with those based on characters?\n2. What is the best way to compose subword representations?\n3. Do character-level models capture morphology in terms of predictive utility?\n4. How do different representations interact with languages of different morphological typologies?\nThe last question is raised by Bender (2013): languages are typologically diverse, and the behavior of a model on one language may not generalize to others. Character-level models implicitly assume concatenative morphology, but many widely-spoken languages feature nonconcatenative morphology, and it is unclear how such models will behave on these languages.\nTo answer these questions, we performed a systematic comparison across different models for the simple and ubiquitous task of language modeling. We present experiments that vary (1) the type of subword unit; (2) the composition function; and (3) morphological typology. To understand the extent to which character-level models capture true morphological regularities, we present oracle experiments using human morphological annotations instead of automatic morphological segments. Our results show that:\n1. For most languages, character-level representations outperform the standard word representations. Most interestingly, a previously unstudied combination of character trigrams composed with bi-LSTMs performs best on the majority of languages.\n2. Bi-LSTMs and CNNs are more effective composition functions than addition.\n3. Character-level models learn functional relationships between orthographically similar words, but don\u2019t (yet) match the predictive accuracy of models with access to true morphological analyses.\n4. Character-level models are effective across a range of morphological typologies, but orthography influences their effectiveness."}, {"heading": "2 Morphological Typology", "text": "A morpheme is the smallest unit of meaning in a word. Some morphemes express core meaning (roots), while others express one or more dependent features of the core meaning, such as person, gender, or aspect. A morphological analysis identifies the lemma and features of a word. A morph is the surface realization of a morpheme (Morley, 2000), which may vary from word to word. These distinctions are shown in Table 1.\nMorphological typology classifies languages based on the processes by which morphemes are composed to form words. While most languages will exhibit a variety of such processes, for any given language, some processes are much more frequent than others, and we will broadly identify our experimental languages with these processes.\nWhen morphemes are combined sequentially, the morphology is concatenative. However, morphemes can also be composed by nonconcatenative processes. We consider four broad categories of both concatenative and nonconcatenative processes in our experiments.\nFusional languages realize multiple features in a single concatenated morpheme. For example, English verbs can express number, person, and tense in a single morpheme:\nwanted (English) want + ed\nwant + VB+1st+SG+Past Agglutinative languages assign one feature per morpheme. Morphemes are concatenated to form a word and the morpheme boundaries are clear. For example (Haspelmath, 2010):\nokursam (Turkish) oku+r+sa+m\n\u201cread\u201d+AOR+COND+1SG Root and Pattern Morphology forms words by inserting consonants and vowels of dependent morphemes into a consonantal root based on a given pattern. For example, the Arabic root ktb (\u201cwrite\u201d) produces (Roark and Sproat, 2007):\nkatab \u201cwrote\u201d (Arabic)\ntakaatab \u201cwrote to each other\u201d (Arabic) Reduplication is a process where a word form is produced by repeating part or all of the root to express new features. For example:\nanak \u201cchild\u201d (Indonesian) anak-anak \u201cchildren\u201d (Indonesian)\nbuah \u201cfruit\u201d (Indonesian) buah-buahan \u201cvarious fruits\u201d (Indonesian)"}, {"heading": "3 Representation Models", "text": "We compare ten different models, varying subword units and composition functions that have commonly been used in recent work, but evaluated on various different tasks (Table 2). Given word w, we compute its representation w as:\nw = f(Ws, \u03c3(w)) (1)\nwhere \u03c3 is a deterministic function that returns a sequence of subword units; Ws is a parameter matrix of representations for the vocabulary of subword units; and f is a composition function which takes \u03c3(w) and Ws as input and returns w. All of the representations that we consider take this form, varying only in f and \u03c3."}, {"heading": "3.1 Subword Units", "text": "We consider four variants of \u03c3 in Equation 1, each returning a different type of subword unit: character, character trigram, or one of two types of morph. Morphs are obtained from Morfessor (Smit et al., 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al., 2016). BPE works by iteratively replacing frequent pairs of characters with a single unused character. For Morfessor, we use default parameters while for BPE we set the number of merge operations to 10,000.1 When we segment into character trigrams, we consider all trigrams in the word, including those covering notional beginning and end of word characters, as in Sperr et al. (2013). Example output of \u03c3 is shown in Table 3."}, {"heading": "3.2 Composition Functions", "text": "We use three variants of f in Eq. 1. The first constructs the representation w of word w by adding\n1BPE takes a single parameter: the number of merge operations. We tried different parameter values (1k, 10k, 100k) and manually examined the resulting segmentation on the English dataset. Qualitatively, 10k gave the most plausible segmentation and we used this setting across all languages.\nthe representations of its subwords s1, . . . , sn = \u03c3(w), where the representation of si is vector si.\nw = n\u2211\ni=1\nsi (2)\nThe only subword unit that we don\u2019t compose by addition is characters, since this will produce the same representation for many different words.\nOur second composition function is a bidirectional long-short-term memory (bi-LSTM), which we adapt based on its use in the characterlevel model of Ling et al. (2015) and its widespread use in NLP generally. Given si and the previous LSTM hidden state hi\u22121, an LSTM (Hochreiter and Schmidhuber, 1997) computes the following outputs for the subword at position i:\nhi = LSTM(si,hi\u22121) (3)\ns\u0302i+1 = g(VT \u00b7 hi) (4)\nwhere s\u0302i+1 is the predicted target subword, g is the softmax function and V is a weight matrix.\nA bi-LSTM (Graves et al., 2005) combines the final state of an LSTM over the input sequence with one over the reversed input sequence. Given the hidden state produced from the final input of the forward LSTM, hfwn and the hidden state produced from the final input of the backward LSTM hbw0 , we compute the word representation as:\nwt = Wf \u00b7 hfwn + Wb \u00b7 hbw0 + b (5)\nwhere Wf , Wb, and b are parameter matrices and hfwn and hbw0 are forward and backward LSTM states, respectively.\nThe third composition function is a convolutional neural network (CNN) with highway layers, as in Kim et al. (2016). Let c1, . . . , ck be the sequence of characters of word w. The character embedding matrix is C \u2208 Rd\u00d7k, where the i-th column corresponds to the embeddings of ci. We first apply a narrow convolution between C and a filter F \u2208 Rd\u00d7n of width n to obtain a feature map f \u2208 Rk\u2212n+1. In particular, the computation of the j-th element of f is defined as\nf [j] = tanh(\u3008C[\u2217, j : j + n\u2212 1],F\u3009+ b) (6)\nwhere \u3008A,B\u3009 = Tr(ABT ) is the Frobenius inner product and b is a bias. The CNN model applies filters of varying width, representing features\nof character n-grams. We then calculate the maxover-time of each feature map.\nyj = max j f [j] (7)\nand concatenate them to derive the word representation wt = [y1, . . . , ym], where m is the number of filters applied. Highway layers allow some dimensions of wt to be carried or transformed. Since it can learn character n-grams directly, we only use the CNN with character input."}, {"heading": "3.3 Language Model", "text": "We use language models (LM) because they are simple and fundamental to many NLP applications. Given a sequence of text s = w1, . . . , wT , our LM computes the probability of s as:\nP (w1, . . . , wT ) = T\u220f t=1 P (yt|w1, . . . , wt\u22121) (8)\nwhere yt = wt if wt is in the output vocabulary and yt = UNK otherwise.\nOur language model is an LSTM variant of recurrent neural network language (RNN) LM (Mikolov et al., 2010). At time step t, it receives input wt and predicts yt+1. Using Eq. 1, it first computes representation wt of wt. Given this representation and previous state ht\u22121, it produces a new state ht and predicts yt+1:\nht = LSTM(wt,ht\u22121) (9)\ny\u0302t+1 = g(VT \u00b7 ht) (10)\nwhere g is a softmax function over the vocabulary yielding the probability in Equation 8. Note that this design means that we can predict only words\nfrom a finite output vocabulary, so our models differ only in their representation of context words. This design makes it possible to compare language models using perplexity, since they have the same event space, though open vocabulary word prediction is an interesting direction for future work.\nThe complete architecture of our system is shown in Figure 1, showing segmentation function \u03c3 and composition function f from Equation 1."}, {"heading": "4 Experiments", "text": "We perform experiments on ten languages (Table 4). We use datasets from Ling et al. (2015) for English and Turkish. For Czech and Russian we use Universal Dependencies (UD) v1.3 (Nivre et al., 2015). For other languages, we use preprocessed Wikipedia data (Al-Rfou et al., 2013).2 For each dataset, we use approximately 1.2M tokens to train, and approximately 150K tokens each for development and testing. Preprocessing involves lowercasing (except for character models) and removing hyperlinks.\nTo ensure that we compared models and not implementations, we reimplemented all models in a single framework using Tensorflow (Abadi et al., 2015).3 We use a common setup for all experiments based on that of Ling et al. (2015), Kim et al. (2016), and Miyamoto and Cho (2016). In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM\n2The Arabic and Hebrew dataset are unvocalized. Japanese mixes Kanji, Katakana, Hiragana, and Latin characters (for foreign words). Hence, a Japanese character can correspond to a character, syllable, or word. The preprocessed dataset is already word-segmented.\n3Our implementation of these models can be found at https://github.com/claravania/subword-lstm-lm\nmodels of Ling et al. (2015). Even following detailed discussion with Ling (p.c.), we were unable to reproduce their perplexities exactly\u2014our English reimplementation gives lower perplexities; our Turkish higher\u2014but we do reproduce their general result that character bi-LSTMs outperform word models. We suspect that different preprocessing and the stochastic learning explains differences in perplexities. Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al., 1993), preprocessed by Mikolov et al. (2010)."}, {"heading": "4.1 Training and Evaluation", "text": "Our LSTM-LM uses two hidden layers with 200 hidden units and representation vectors for words, characters, and morphs all have dimension 200. All parameters are initialized uniformly at random from -0.1 to 0.1, trained by stochastic gradient descent with mini-batch size of 32, time steps of 20, for 50 epochs. To avoid overfitting, we apply dropout with probability 0.5 on the input-tohidden layer and all of the LSTM cells (including those in the bi-LSTM, if used). For all models which do not use bi-LSTM composition, we start with a learning rate of 1.0 and decrease it by half if the validation perplexity does not decrease by 0.1 after 3 epochs. For models with bi-LSTMs composition, we use a constant learning rate of 0.2 and stop training when validation perplexity does not improve after 3 epochs. For the character CNN model, we use the same settings as the small model of Kim et al. (2016).\nTo make our results comparable to Ling et al. (2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token. To learn to predict unknown words, we follow Ling et al. (2015): in training, words that occur only once are stochastically replaced with the unknown token with probability 0.5. To evaluate the models, we compute perplexity on the test data."}, {"heading": "5 Results and Analysis", "text": "Table 5 presents our main results. In six of ten languages, character-trigram representations composed with bi-LSTMs achieve the lowest perplexities. As far as we know, this particular model has not been tested before, though it is similar\nto (but more general than) the model of Sperr et al. (2013). We can see that the performance of character, character trigrams, and BPE are very competitive. Composition by bi-LSTMs or CNN is more effective than addition, except for Turkish. We also observe that BPE always outperforms Morfessor, even for the agglutinative languages. We now turn to a more detailed analysis by morphological typology.\nFusional languages. For these languages, character trigrams composed with bi-LSTMs outperformed all other models, particularly for Czech and Russian (up to 20%), which is unsurprising since both are morphologically richer than English.\nAgglutinative languages. We observe different results for each language. For Finnish, character trigrams composed with bi-LSTMs achieves the best perplexity. Surprisingly, for Turkish character trigrams composed via addition is best and addition also performs quite well for other representations, potentially useful since the addition function is simpler and faster than bi-LSTMs. We suspect that this is due to the fact that Turkish morphemes are reasonably short, hence wellapproximated by character trigrams. For Japanese, we improvements from character models are more modest than in other languages.\nRoot and Pattern. For these languages, character trigrams composed with bi-LSTMs also achieve the best perplexity. We had wondered whether CNNs would be more effective for root-and-pattern morphology, but since these data are unvocalized, it is more likely that nonconcatenative effects are minimized, though we do\nstill find morphological variants with consonantal inflections that behave more like concatenation. For example, maktab (root:ktb) is written as mktb. We suspect this makes character trigrams quite effective since they match the tri-consonantal root patterns among words which share the same root.\nReduplication. For Indonesian, BPE morphs composed with bi-LSTMs model obtain the best perplexity. For Malay, the character CNN outperforms other models. However, these improvements are small compared to other languages. This likely reflects that Indonesian and Malay are only moderately inflected, where inflection involves both concatenative and non-concatenative processes."}, {"heading": "5.1 Effects of Morphological Analysis", "text": "In the experiments above, we used unsupervised morphological segmentation as a proxy for morphological analysis (Table 3). However, as discussed in Section 2, this is quite approximate, so it is natural to wonder what would happen if we had the true morphological analysis. If characterlevel models are powerful enough to capture the effects of morphology, then they should have the predictive accuracy of a model with access to this analysis. To find out, we conducted an oracle experiment using the human-annotated morphological analyses provided in the UD datasets for Czech and Russian, the only languages in our set for which these analyses were available. In these experiments we treat the lemma and each morphological feature as a subword unit.\nThe results (Table 6) show that bi-LSTM composition of these representations outperforms all\nother models for both languages. These results demonstrate that neither character representations nor unsupervised segmentation is a perfect replacement for manual morphological analysis, at least in terms of predictive accuracy. In light of character-level results, they imply that current unsupervised morphological analyzers are poor substitutes for real morphological analysis.\nHowever, we can obtain much more unannotated than annotated data, and we might guess that the character-level models would outperform those based on morphological analyses if trained on larger data. To test this, we ran experiments that varied the training data size on three representation models: word, character-trigram bi-LSTM, and character CNN. Since we want to see how much training data is needed to reach perplexity obtained using annotated data, we use the same output vocabulary derived from the original training. While this makes it possible to compare perplexities across models, it is unfavorable to the models trained on larger data, which may focus on other words. This is a limitation of our experimental setup, but does allow us to draw some tentative conclusions. As shown in Table 7, a characterlevel model trained on an order of magnitude more data still does not match the predictive accuracy of a model with access to morphological analysis."}, {"heading": "5.2 Automatic Morphological Analysis", "text": "The oracle experiments show promising results if we have annotated data. But these annotations are expensive, so we also investigated the use of automatic morphological analysis. We obtained analyses for Arabic with the MADAMIRA (Pasha et al., 2014).4 As in the experiment using annotations, we treated each morphological feature as a subword unit. The resulting perplexities of 71.94 and 42.85 for addition and bi-LSTMs, respectively, are worse than those obtained with character trigrams (39.87), though it approaches the best perplexities.\n4We only experimented with Arabic since MADAMIRA disambiguates words in contexts; most other analyzers we found did not do this, and would require additional work to add disambiguation."}, {"heading": "5.3 Targeted Perplexity Results", "text": "A difficulty in interpreting the results of Table 5 with respect to specific morphological processes is that perplexity is measured for all words. But these processes do not apply to all words, so it may be that the effects of specific morphological processes are washed out. To get a clearer picture, we measured perplexity for only specific subsets of words in our test data: specifically, given target word wi, we measure perplexity of word wi+1. In other words, we analyze the perplexities when the inflected words of interest are in the most recent history, exploiting the recency bias of our LSTM-LM. This is the perplexity most likely to be strongly affected by different representations, since we do not vary representations of the predicted word itself.\nWe look at several cases: nouns and verbs in Czech and Russian, where word classes can be identified from annotations, and reduplication in Indonesian, which we can identify mostly automatically. For each analysis, we also distinguish between frequent cases, where the inflected word occurs more than ten times in the training data, and rare cases, where it occurs fewer than ten times. We compare only bi-LSTM models.\nFor Czech and Russian, we again use the UD annotation to identify words of interest. The results (Table 8), show that manual morphological analysis uniformly outperforms other subword models, with an especially strong effect for Czech nouns, suggesting that other models do not capture useful predictive properties of a morphological analysis. We do however note that character trigrams achieve low perplexities in most cases, similar to overall results (Table 5). We also observe that the subword models are more effective for rare words.\nFor Indonesian, we exploit the fact that the hyphen symbol \u2018-\u2019 typically separates the first and second occurrence of a reduplicated morpheme, as in the examples of Section 2. We use the presence of word tokens containing hyphens to estimate the percentage of those exhibiting reduplication. As shown in Table 9, the numbers are quite low.\nTable 10 shows results for reduplication. In contrast with the overall results, the BPE bi-LSTM model has the worst perplexities, while character bi-LSTM has the best, suggesting that these models are more effective for reduplication.\nLooking more closely at BPE segmentation of reduplicated words, we found that only 6 of 252 reduplicated words have a correct word segmentation, with the reduplicated morpheme often combining differently with the notional start-of-word or hyphen character. One the other hand BPE correctly learns 8 out of 9 Indonesian prefixes and 4 out of 7 Indonesian suffixes.5 This analysis supports our intuition that the improvement from BPE might come from its modeling of concatenative morphology."}, {"heading": "5.4 Qualitative Analysis", "text": "Table 11 presents nearest neighbors under cosine similarity for in-vocabulary, rare, and out-of-\n5We use Indonesian affixes listed in Larasati et al. (2011)\nvocabulary (OOV) words.6 For frequent words, standard word embeddings are clearly superior for lexical meaning. Character and morph representations tend to find words that are orthographically similar, suggesting that they are better at modeling dependent than root morphemes. The same pattern holds for rare and OOV words. We suspect that the subword models outperform words on language modeling because they exploit affixes to signal word class. We also noticed similar patterns in Japanese.\nWe analyze reduplication by querying reduplicated words to find their nearest neighbors using the BPE bi-LSTM model. If the model were sensitive to reduplication, we would expect to see morphological variants of the query word among its nearest neighbors. However, from Table 12, this is not so. With the partially reduplicated query berlembah-lembah, we do not find the lemma lembah."}, {"heading": "6 Conclusion", "text": "We presented a systematic comparison of word representation models with different levels of morphological awareness, across languages with different morphological typologies. Our results confirm previous findings that character-level models are effective for many languages, but these models do not match the predictive accuracy of model with explicit knowledge of morphology, even after we increase the training data size by ten times. Moreover, our qualitative analysis suggests that they learn orthographic similarity of affixes, and lose the meaning of root morphemes.\nAlthough morphological analyses are available\n6https://radimrehurek.com/gensim/\nin limited quantities, our results suggest that there might be utility in semi-supervised learning from partially annotated data. Across languages with different typologies, our experiments show that the subword unit models are most effective on agglutinative languages. However, these results do not generalize to all languages, since factors such as morphology and orthography affect the utility of these representations. We plan to explore these effects in future work."}, {"heading": "Acknowledgments", "text": "Clara Vania is supported by the Indonesian Endowment Fund for Education (LPDP), the Centre for Doctoral Training in Data Science, funded by the UK EPSRC (grant EP/L016427/1), and the University of Edinburgh. We thank Sameer Bansal, Toms Bergmanis, Marco Damonte, Federico Fancellu, Sorcha Gilroy, Sharon Goldwater, Frank Keller, Mirella Lapata, Felicia Liu, Jonathan Mallinson, Joana Ribeiro, Naomi Saphra, Ida Szubert, and the anonymous reviewers for helpful discussion of this work and comments on previous drafts of the paper."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["sudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "sudevan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "sudevan et al\\.", "year": 2015}, {"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for Computa-", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax", "author": ["Emily M. Bender."], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "Bender.,? 2013", "shortCiteRegEx": "Bender.", "year": 2013}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "CoRR abs/1607.04606. http://arxiv.org/abs/1607.04606.", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Compositional Morphology for Word Representations and Language Modeling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML). Beijing, China.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Morphological word-embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Com-", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "A new algorithm for data compression", "author": ["Philip Gage."], "venue": "C Users J. 12(2):23\u201338. http://dl.acm.org/citation.cfm?id=177910.177914.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Gillick et al\\.,? 2016", "shortCiteRegEx": "Gillick et al\\.", "year": 2016}, {"title": "Bidirectional lstm networks for improved phoneme classification and recognition", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of the 15th International Conference on Artificial Neu-", "citeRegEx": "Graves et al\\.,? 2005", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Understanding Morphology", "author": ["Martin Haspelmath."], "venue": "Understanding Language Series. Arnold, London, second edition.", "citeRegEx": "Haspelmath.,? 2010", "shortCiteRegEx": "Haspelmath.", "year": 2010}, {"title": "An extensive empirical evaluation of character-based morphological tagging for 14 languages", "author": ["Georg Heigold", "Guenter Neumann", "Josef van Genabith."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Com-", "citeRegEx": "Heigold et al\\.,? 2017", "shortCiteRegEx": "Heigold et al\\.", "year": 2017}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780. https://doi.org/10.1162/neco.1997.9.8.1735.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, Association for Computational Linguistics, chapter MED: The LMU", "author": ["Katharina Kann", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Kann and Sch\u00fctze.,? \\Q2016\\E", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander Rush."], "venue": "Proceedings of the 2016 Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Indonesian Morphology Tool (MorphInd): Towards an Indonesian Corpus, Springer Berlin Heidelberg, Berlin, Heidelberg, pages 119\u2013 129", "author": ["Septina Dian Larasati", "Vladislav Kubo\u0148", "Daniel Zeman."], "venue": "https://doi.org/10.1007/978-3-642-23138-4 8.", "citeRegEx": "Larasati et al\\.,? 2011", "shortCiteRegEx": "Larasati et al\\.", "year": 2011}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "CoRR abs/1610.03017. http://arxiv.org/abs/1610.03017.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Comput. Linguist. 19(2):313\u2013330. http://dl.acm.org/citation.cfm?id=972470.972475.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Gated word-character recurrent language model", "author": ["Yasumasa Miyamoto", "Kyunghyun Cho."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Lin-", "citeRegEx": "Miyamoto and Cho.,? 2016", "shortCiteRegEx": "Miyamoto and Cho.", "year": 2016}, {"title": "Syntax in Functional Grammar: An Introduction to Lexicogrammar in Systemic Linguistics", "author": ["G. David Morley."], "venue": "Continuum.", "citeRegEx": "Morley.,? 2000", "shortCiteRegEx": "Morley.", "year": 2000}, {"title": "Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of ara", "author": ["Arfath Pasha", "Mohamed Al-Badrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "TieYan Liu."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers.", "citeRegEx": "Qiu et al\\.,? 2014", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Attending to characters in neural sequence labeling models", "author": ["Marek Rei", "Gamal Crichton", "Sampo Pyysalo."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016", "citeRegEx": "Rei et al\\.,? 2016", "shortCiteRegEx": "Rei et al\\.", "year": 2016}, {"title": "Computational Approach to Morphology and Syntax", "author": ["Brian Roark", "Richard Sproat."], "venue": "Oxford University Press.", "citeRegEx": "Roark and Sproat.,? 2007", "shortCiteRegEx": "Roark and Sproat.", "year": 2007}, {"title": "Learning character-level representations for partof-speech tagging", "author": ["Cicero Dos Santos", "Bianca Zadrozny."], "venue": "Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning. PMLR,", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Morfessor 2.0: Toolkit", "author": ["Mikko Kurimo"], "venue": null, "citeRegEx": "Kurimo.,? \\Q2014\\E", "shortCiteRegEx": "Kurimo.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "word units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 64, "endOffset": 138}, {"referenceID": 4, "context": "word units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 64, "endOffset": 138}, {"referenceID": 7, "context": "word units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 64, "endOffset": 138}, {"referenceID": 19, "context": "Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al.", "startOffset": 157, "endOffset": 194}, {"referenceID": 16, "context": "Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al.", "startOffset": 157, "endOffset": 194}, {"referenceID": 3, "context": ", 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al.", "startOffset": 27, "endOffset": 119}, {"referenceID": 4, "context": ", 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al.", "startOffset": 27, "endOffset": 119}, {"referenceID": 26, "context": ", 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al.", "startOffset": 40, "endOffset": 82}, {"referenceID": 10, "context": ", 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al.", "startOffset": 40, "endOffset": 82}, {"referenceID": 30, "context": ", 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014).", "startOffset": 33, "endOffset": 78}, {"referenceID": 27, "context": ", 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014).", "startOffset": 33, "endOffset": 78}, {"referenceID": 2, "context": "The last question is raised by Bender (2013): languages are typologically diverse, and the behavior of a model on one language may not generalize to others.", "startOffset": 31, "endOffset": 45}, {"referenceID": 24, "context": "A morph is the surface realization of a morpheme (Morley, 2000), which may vary from word to word.", "startOffset": 49, "endOffset": 63}, {"referenceID": 12, "context": "For example (Haspelmath, 2010):", "startOffset": 12, "endOffset": 30}, {"referenceID": 29, "context": "For example, the Arabic root ktb (\u201cwrite\u201d) produces (Roark and Sproat, 2007): katab \u201cwrote\u201d (Arabic)", "startOffset": 52, "endOffset": 76}, {"referenceID": 31, "context": ", 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al., 2016).", "startOffset": 170, "endOffset": 193}, {"referenceID": 9, "context": ", 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al.", "startOffset": 65, "endOffset": 77}, {"referenceID": 14, "context": "Given si and the previous LSTM hidden state hi\u22121, an LSTM (Hochreiter and Schmidhuber, 1997) computes the following outputs for the subword at position i:", "startOffset": 58, "endOffset": 92}, {"referenceID": 18, "context": "Our second composition function is a bidirectional long-short-term memory (bi-LSTM), which we adapt based on its use in the characterlevel model of Ling et al. (2015) and its widespread use in NLP generally.", "startOffset": 148, "endOffset": 167}, {"referenceID": 11, "context": "A bi-LSTM (Graves et al., 2005) combines the", "startOffset": 10, "endOffset": 31}, {"referenceID": 16, "context": "The third composition function is a convolutional neural network (CNN) with highway layers, as in Kim et al. (2016). Let c1, .", "startOffset": 98, "endOffset": 116}, {"referenceID": 12, "context": "(2013) words, character n-grams addition Luong et al. (2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al.", "startOffset": 39, "endOffset": 117}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al.", "startOffset": 39, "endOffset": 179}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al.", "startOffset": 39, "endOffset": 230}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al.", "startOffset": 39, "endOffset": 292}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al.", "startOffset": 39, "endOffset": 328}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al. (2015) characters bi-LSTM Wieting et al.", "startOffset": 39, "endOffset": 362}, {"referenceID": 3, "context": "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN Cotterell and Sch\u00fctze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al. (2015) characters bi-LSTM Wieting et al. (2016) character n-grams addition Bojanowski et al.", "startOffset": 39, "endOffset": 403}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al.", "startOffset": 34, "endOffset": 109}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al.", "startOffset": 34, "endOffset": 177}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al. (2016) words, characters bi-LSTM Lee et al.", "startOffset": 34, "endOffset": 221}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al. (2016) words, characters bi-LSTM Lee et al. (2016) characters CNN Kann and Sch\u00fctze (2016) characters, morphological analyses none Heigold et al.", "startOffset": 34, "endOffset": 265}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al. (2016) words, characters bi-LSTM Lee et al. (2016) characters CNN Kann and Sch\u00fctze (2016) characters, morphological analyses none Heigold et al.", "startOffset": 34, "endOffset": 304}, {"referenceID": 3, "context": "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Rei et al. (2016) words, characters bi-LSTM Lee et al. (2016) characters CNN Kann and Sch\u00fctze (2016) characters, morphological analyses none Heigold et al. (2017) words, characters bi-LSTM, CNN", "startOffset": 34, "endOffset": 366}, {"referenceID": 22, "context": "Our language model is an LSTM variant of recurrent neural network language (RNN) LM (Mikolov et al., 2010).", "startOffset": 84, "endOffset": 106}, {"referenceID": 1, "context": "For other languages, we use preprocessed Wikipedia data (Al-Rfou et al., 2013).", "startOffset": 56, "endOffset": 78}, {"referenceID": 18, "context": "We use datasets from Ling et al. (2015) for English and Turkish.", "startOffset": 21, "endOffset": 40}, {"referenceID": 18, "context": "3 We use a common setup for all experiments based on that of Ling et al. (2015), Kim et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 16, "context": "(2015), Kim et al. (2016), and Miyamoto and Cho (2016).", "startOffset": 8, "endOffset": 26}, {"referenceID": 16, "context": "(2015), Kim et al. (2016), and Miyamoto and Cho (2016). In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM", "startOffset": 8, "endOffset": 55}, {"referenceID": 19, "context": "com/claravania/subword-lstm-lm models of Ling et al. (2015). Even following de-", "startOffset": 41, "endOffset": 60}, {"referenceID": 21, "context": "Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al., 1993), preprocessed by Mikolov et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 21, "context": "Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 21, "context": "Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al., 1993), preprocessed by Mikolov et al. (2010).", "startOffset": 178, "endOffset": 238}, {"referenceID": 16, "context": "For the character CNN model, we use the same settings as the small model of Kim et al. (2016).", "startOffset": 76, "endOffset": 94}, {"referenceID": 19, "context": "To make our results comparable to Ling et al. (2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token.", "startOffset": 34, "endOffset": 53}, {"referenceID": 19, "context": "To make our results comparable to Ling et al. (2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token. To learn to predict unknown words, we follow Ling et al. (2015): in training, words that occur only once are stochastically replaced with the unknown token with probability 0.", "startOffset": 34, "endOffset": 237}, {"referenceID": 25, "context": "We obtained analyses for Arabic with the MADAMIRA (Pasha et al., 2014).", "startOffset": 50, "endOffset": 70}, {"referenceID": 17, "context": "We use Indonesian affixes listed in Larasati et al. (2011) Language type-level (%) token-level (%)", "startOffset": 36, "endOffset": 59}], "year": 2017, "abstractText": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.", "creator": "LaTeX with hyperref package"}}}