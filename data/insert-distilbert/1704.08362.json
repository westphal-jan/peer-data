{"id": "1704.08362", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "A New Type of Neurons for Machine Learning", "abstract": "in machine learning, the use of an artificial neural network is the mainstream approach. such a network consists of layers of unrelated neurons. these neurons are of the same type characterized by preserving the two features : ( 1 ) an inner product of an input vector and a matching differential weighting vector of trainable parameters and ( 2 ) a nonlinear excitation function. here we investigate the several possibility factors of replacing the inner product with a inverse quadratic function of the assigned input vector, thereby upgrading the 1st order neuron to supplement the 2nd order impulse neuron, empowering individual neurons, and facilitating the optimization of computational neural networks. also, numerical examples are provided to first illustrate the feasibility features and merits of the 2nd order neurons. about finally, further complexity topics are discussed.", "histories": [["v1", "Wed, 26 Apr 2017 22:02:25 GMT  (890kb)", "http://arxiv.org/abs/1704.08362v1", "5 pages, 8 figures, 11 references"]], "COMMENTS": "5 pages, 8 figures, 11 references", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["fenglei fan", "wenxiang cong", "ge wang"], "accepted": false, "id": "1704.08362"}, "pdf": {"name": "1704.08362.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "network is the mainstream approach. Such a network consists of layers of neurons. These neurons are of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function. Here we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the 1st order neuron to the 2nd order neuron, empowering individual neurons, and facilitating the optimization of neural networks. Also, numerical examples are provided to illustrate the feasibility and merits of the 2nd order neurons. Finally, further topics are discussed.\nIndex Terms \u2014 Machine learning, artificial neural network\n(ANN), 2nd order neuron, convolutional neural network (CNN).\nI. INTRODUCTION\nN the field of machine learning, artificial neural networks (ANNs) especially deep neural networks (CNNs) have recently achieved remarkable successes in various types of applications such as classification, unsupervised learning, prediction, image processing and analysis [1-3]. Excited by the tremendous potential of machine learning, major efforts are being made to improve machine learning methods [4]. An important aspect of the methodological research is how to optimize the topology of a neural network. For example, generative adversarial networks (GANs) were proposed to utilize some deep results of game theory. In particular, the Wasserstein distance was introduced for effective, efficient and stable performance of GAN [5, 6].\nTo our best knowledge, all ANNs/CNNs are currently constructed with neurons of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function [7-9]. Although these neurons can be interconnected to approximate any general function, the topology of the network is not unique. On another side of this non-uniqueness, our hypothesis is that the type of neurons suitable for general machine learning is not unique either, which is a new dimension of machine learning research.\nThe above hypothesis is well motivated. It is commonly known that the current structure of artificial neurons was inspired by the inner working of biological neurons. Biologically speaking, a core principle is that diversity brings synergy and prosperity at all levels. In biology, multiple types of cells are available to make various organisms [10]. Indeed, many genomic\nThis work is partially supported by the Clark & Crossan Endowment Fund at\nRensselaer Polytechnic Institute, Troy, New York, USA.\ncompensate for others\u2019 weaknesses [11]. Most of cell types are highly dedicated to desirable forms and functions. Synthetic biological research may enhance the biological diversity further. This observation makes us wonder if we could have multiple types of neurons for machine learning either in general or for specific tasks.\nAs the first step along this direction, here we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the 1st order neuron to the 2nd order neuron, empowering individual neurons, and facilitating the optimization of neural networks.\nThe model of the current single neurons, which are also referred to as perceptrons, has been applied to solve linearly separable problems. For linearly inseparable tasks, multi-layers of neurons are needed to perform multi-scale nonlinear analysis. In other words, existing neurons can only perform linear classification individually, and the linearly-limited cellular function can be only enhanced via cellular interconnection into an artificial organism. Our curiosity is to produce such an artificial organism with neurons capable of performing quadratic classification.\nIn the next section, we describe the 2nd order neurons. Given n inputs, the new type of neurons could have 3n parameters to approximate a general quadratic function, or have n(n+1)/2 degrees of freedom to admit any quadratic function. Furthermore, we formulate how to train the 2nd order neuron. In the third section, we present numerical simulation results for fuzzy logic operations. In the last section, we discuss relevant issues and conclude the paper."}, {"heading": "II. SECOND ORDER MODEL AND OPTIMIZATION", "text": "The structure of a current neuron is shown in Fig. 1, where we define 0w := b and 0x =1. The linear function of the input vector produce the output [8]:\n1\n( ) n\ni i\ni f x w x b    (1)\nand then ( )f x will be nonlinearly processed, such as by a\nsigmoid function. Clearly, the single neuron can separate two sets of inputs that are linearly separable. In contrast, for linearly inseparable groups of inputs, the single neuron is subject to classification errors. For example, a single neuron is incapable of simulating the function of the XOR gate.\nWe introduce a new type of neurons in Fig. 2 where the input vector is turned into two inner products and one norm term for summation before feeding to the nonlinear excitation function.\nI\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n2\nAgain, for compact notation, we define that 0 1:rw b ,\n0 2:gw b and 0x =1. Then, the output function is expressed as:\n2\n1 2\n1 1 1\n( ) ( )( ) + . n n n\nir i ig i ib i\ni i i f x w x b w x b w x c         \n(2)\nIn this embodiment, the threshold is chosen by a sigmoid function. Eq. (2) is quadric, and has the current neuron type as a special case. Due to its added nonlinearity, our proposed neuron is intrinsically superior to the current neuron in terms of representation power such as for classification.\nThe training algorithm can be formulated for the proposed 2nd order neuron as follows, assuming the sigmoid function [8]\n1\n( ) 1 exp( ) x x      \uff0c (3)\nwhere \u03b2=1 in this pilot study. Let us denote a training dataset of m samples by  1 2, , ,k k k knX x x x , along with the ideal\noutput data ky , 1,2, ,k m . Then, the output of the 2nd order\nneuron can be modeled as\n    \n \n1 2\n2\n1 2\n1 1 1\n, , , w , ,\n+ .\nk\nr g b\nn n n k k k\nir i ig i ib i\ni i i\nh X w w b b f x\nw x b w x b w x c\n\n   \n\n          \n      \n(4)\nLet us define the error function as\n     2\n1 2 1 2\n1\n1 , , , , , , , w , ,\n2\nm k k\nr g b r g b\nk E w w w b b h X w w b b y    (5)\nThe error function depends on the structural parameters:\n1b \uff0c 2b \uff0cc ,  1 2, , ,r r r nrw w w w \uff0c  1 2, , ,g g g ngw w w w and\n 1 2, , ,b b b nbw w w w ,\nThe goal of machine learning is to find optimal parameters to minimize the objective function [9]. The optimal parameters can be found using the gradient descent method with an\nappropriate initial guess. Thus, we can iteratively update rw ,\ngw , bw , 1b , 2b and c in the form of , E\n   \n   \n where \ndenotes a generic variable of the objective function, and the step size  is typically set between zero and one. The gradient\nof the object function for any sample can be computed as follows:\n2\n1\n( ( ) ) ( ) n\ni i i ig i\niir\nE h x y x w x b\nw x\n\n\n    \n  \n1\n1\n( ( ) ) ( ) n\ni i i ir i\niig\nE h x y x w x b\nw x\n\n\n    \n  \n2( ( ) )i i ib i ib\nE h x y w x\nw x\n   \n \n2\n11\n( ( ) ) ( ) n\ni i ig i\ni\nE h x y w x b b x \n\n    \n  \n1\n12\n( ( ) ) ( ) n\ni i ir i\ni\nE h x y w x b b x \n\n    \n  \n( ( ) )i i E\nh x y c x\n     \nA general optimization flowchart is in Fig. 3."}, {"heading": "III. NUMERICAL RESULTS", "text": "In our pilot study, the proposed 2nd order neuron of two input variables was individually applied in fuzzy logic experiments. Extending classic Boolean logic, fuzzy logic was extensively\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n3\nstudied and applied over the past decades, which manipulates vague or imprecise logic statements. Compared with Boolean logic, the value of a fuzzy logic variable is on the interval [0, 1], since a practical judgement may not be purely black (false or 0) or white (true or 1). The fuzziness of a logic variable can be easily reflected in a continuous variable by its closeness to either 0 or 1.\nFor visualization, the color map \u201ccool\u201d in MATLAB was used to represent the functional value at every point. We used \u201co\u201d for 0 and \u201c+\u201d for 1. With the proposed 2nd order neuron, the training process kept refining a quadratic contour to separate labeled points for the highest classification accuracy. By the nature of the 2nd order neuron, the contour can be two lines or curves including parabolic and elliptical boundaries.\nA. XOR Gate\nFirst, the training dataset was simply the XOR logic table, which is not separable by a single neuron of the conventional type. To implement the XOR-gate operation with our proposed 2nd order neuron, the initial parameters could be randomly selected in the framework of evolutionary computation. For example, an initial seed were set to rw =[-0.4, -0.4], gw =[0.2,\n1], bw =[0, 0], 1b =-0.9095, 2b =-0.6426, c=0. The trained logic map is shown in Fig. 4, yielding a perfect result.\nThen, we generated an XOR-like pattern, the initial parameters were set to rw =[0.07994,-0.2119], gw =[0.06049,-0.144], bw\n=[0 0], 1b =-0.9095, 2b =-0.6426. c=0. As shown in Fig. 5, the deformed XOR pattern was perfectly segmented by our proposed 2nd order neuron after training."}, {"heading": "B. NAND and NOR Gates", "text": "While the XOR gate is exemplary, it is not a universal logic gate. To further show the power of the proposed 2nd order neuron, additional datasets were generated that respectively demand fuzzy NAND and NOR operations for correct classification. Then, training steps similar to those in Subsection A were repeated to simulate NAND and NOR operations respectively. In this pilot study, the initial\nparameters were set to rw =[0.4, -0.1], gw =[0.3, 0.1], bw =[0,\n0], 1b =0, 2b =0, c=1.3 and rw =[-1,1], gw =[1,-2], bw =[0,0],\n1b =-0.5, 2b =1, c=0, for NAND and NOR tasks respectively. The results are shown in Figs. 6 and 7.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n4"}, {"heading": "C. Concentric Rings", "text": "Yet another type of natural patterns linearly inseparable is\nconcentric rings. As an example, we generated two concentric\nrings, which were respectively assigned to two classes. With\nthe initial parameters rw =[0.04,0.01], gw =[0.03,-0.01], bw =[0,0.4], 1b =0.1, and 2b =0.2, c=1.3, our 2 nd order neuron was trained, producing an ideal outcome as shown in Fig. 8, which\ncompletely separates the inner ring from the outer ring."}, {"heading": "IV. DISCUSSIONS AND CONCLUSION", "text": "As demonstrated in this paper, the proposed 2nd order neuron works well in solving basic fuzzy logic problems, and qualified as a different building block for machine learning. The sufficiency of the proposed neuron type can be trivially argued.\nClearly, the traditional neuron type is a special case of the proposed 2nd order neuron. Since the traditional neuron is sufficient for a general machine learning task, the proposed neuron type should be sufficient as well. Furthermore, the superiority of the proposed neuron type can be argued in analogy to the functional approximation. The traditional neuron linearly synthesizes inputs into a single number, which is the 1st order Taylor approximation. What we have proposed is the 2nd order Taylor approximation. This flexibility should simplify the overall architecture of a neural network in certain types of applications including but not limited to fuzzy logic tasks.\nFrom the perspective of digital/fuzzy logic, the proposed 2nd order neurons can directly implement all typical logic gates with or without fuzzyness, especially the so-called universal gates such as NAND and NOR. This comment is a hint that the 2nd order neurons may be appropriate modules for fuzzy logic processing or soft-computing when logic values are not binary, which means the correct logic judgements cannot be directly made with the common neuron that is the 1st order but can be efficiently made with the proposed 2nd order neuron.\nWhile the proposal 2nd order neuron has important new representation capabilities, the general 2nd order neuron should use more parameters:\n, 1, 1\n( ) n n\nij i j k k\ni j i j k f x a x x b x c       \nFrom a training set  px and  py , the parameters  ija ,  kb and c can be updated using the gradient descent method in the evolutionary computing framework, where the gradient can be computed as follows:\n( ( ) )p p i j ij\nE h x y x x a x       ,\n( ( ) )p p k k\nE h x y x b x       ,\n( ( ) )p p E\nh x y c x\n   \n  .\nThis general 2nd order neuron can be similarly trained. For example, this general neuron was adapted for an OR-like operation with the initial parameters 11 22 21 0.1a a a   ,\n1 2 1b b  , 0.1c  , yielding the result in Fig. 8.\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <\n5\nDespite the fact that the number of parameters is now in the order of n2, this should not present a major computational challenge when these 2nd order neurons are used in CNNs.\nIt is underlined that with new types of neurons, we have broadened the space for the network optimization to adjust not only neural interconnections but also neural inner makings. The 2nd order neuron is only an example. Other types of neurons can be imagined as well. Preferred neuron structures may depend on specific applications.\nTo have a greater impact of the 2nd order neurons, we would need to connect them into multi-layers and compare the power and efficiency of competing networks made with 1st and 2nd order neurons respectively. To train a network consisting of 2nd order neurons, we will formulate a generalized back propagation algorithm and optimize its performance. Importantly, we need to find specific niche applications of 2nd order neurons.\nA complete theory remains missing for the neural network consisting of 1st order neurons, and needless to say about theoretical results on the network with 2nd order neurons. However, this challenge is also our opportunities to perform further research on solution optimality, robustness, etc.\nIn conclusion, a new type of artificial neurons, which we call the 2nd order neurons, has been proposed for an enhanced expressing capability than what the current neuron has. Our pilot results show some encouraging results, but much efforts are needed to demonstrate a real-world utility and establish a rigorous theory."}], "references": [{"title": "Brain Tumor Segmentation using Convolutional Neural Networks in MRI Images", "author": ["S. Pereira", "A. Pinto", "V. Alves", "C.A. Silva"], "venue": "IEEE Trans Med Imaging, Mar 04, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Convolutional Neural  Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning", "author": ["H.C. Shin", "H.R. Roth", "M. Gao", "L. Lu", "Z. Xu", "I. Nogues", "J. Yao", "D. Mollura", "R.M. Summers"], "venue": "IEEE Trans Med Imaging, vol. 35, no. 5, pp. 1285-98, May, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network", "author": ["M. Anthimopoulos", "S. Christodoulidis", "L. Ebner", "A. Christe", "S. Mougiakakou"], "venue": "IEEE Trans Med Imaging, vol. 35, no. 5, pp. 1207-1216, May, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "A logical calculus of the ideas immanent in nervous activity. 1943", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "Bull Math Biol, vol. 52, no. 1-2, pp. 99-115; discussion 73-97, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Shape Classification Using Wasserstein Distance for Brain Morphometry Analysis", "author": ["Z. Su", "W. Zeng", "Y. Wang", "Z.L. Lu", "X. Gu"], "venue": "Inf Process Med Imaging, vol. 24, pp. 411-23, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv: 1701.07875, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep learning in neural networks: an overview", "author": ["J. Schmidhuber"], "venue": "Neural Netw, vol. 61, pp. 85-117, Jan, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning in medical imaging", "author": ["D. Shen", "G. Wu", "D. Zhang", "K. Suzuki", "F. Wang", "P. Yan"], "venue": "Comput Med Imaging Graph, vol. 41, pp. 1-2, Apr, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Training redundant artificial neural networks: imposing biology on technology", "author": ["D.A. Medler", "M.R. Dawson"], "venue": "Psychol Res, vol. 57, no. 1, pp. 54-62, 1994.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "Artificial neural networks in biology and chemistry: the evolution of a new analytical tool", "author": ["H.M. Cartwright"], "venue": "Methods Mol Biol, vol. 458, pp. 1-13, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION N the field of machine learning, artificial neural networks (ANNs) especially deep neural networks (CNNs) have recently achieved remarkable successes in various types of applications such as classification, unsupervised learning, prediction, image processing and analysis [1-3].", "startOffset": 285, "endOffset": 290}, {"referenceID": 1, "context": "INTRODUCTION N the field of machine learning, artificial neural networks (ANNs) especially deep neural networks (CNNs) have recently achieved remarkable successes in various types of applications such as classification, unsupervised learning, prediction, image processing and analysis [1-3].", "startOffset": 285, "endOffset": 290}, {"referenceID": 2, "context": "INTRODUCTION N the field of machine learning, artificial neural networks (ANNs) especially deep neural networks (CNNs) have recently achieved remarkable successes in various types of applications such as classification, unsupervised learning, prediction, image processing and analysis [1-3].", "startOffset": 285, "endOffset": 290}, {"referenceID": 3, "context": "Excited by the tremendous potential of machine learning, major efforts are being made to improve machine learning methods [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "In particular, the Wasserstein distance was introduced for effective, efficient and stable performance of GAN [5, 6].", "startOffset": 110, "endOffset": 116}, {"referenceID": 5, "context": "In particular, the Wasserstein distance was introduced for effective, efficient and stable performance of GAN [5, 6].", "startOffset": 110, "endOffset": 116}, {"referenceID": 6, "context": "To our best knowledge, all ANNs/CNNs are currently constructed with neurons of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function [7-9].", "startOffset": 264, "endOffset": 269}, {"referenceID": 7, "context": "To our best knowledge, all ANNs/CNNs are currently constructed with neurons of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function [7-9].", "startOffset": 264, "endOffset": 269}, {"referenceID": 8, "context": "In biology, multiple types of cells are available to make various organisms [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "components contribute complementary strengths and compensate for others\u2019 weaknesses [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "The goal of machine learning is to find optimal parameters to minimize the objective function [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "Compared with Boolean logic, the value of a fuzzy logic variable is on the interval [0, 1], since a practical judgement may not be purely black (false or 0) or white (true or 1).", "startOffset": 84, "endOffset": 90}, {"referenceID": 0, "context": "After the training, the outputs at [0,0], [0,1], [1,0] and [1,1] are 0.", "startOffset": 42, "endOffset": 47}, {"referenceID": 0, "context": "After the training, the outputs at [0,0], [0,1], [1,0] and [1,1] are 0.", "startOffset": 49, "endOffset": 54}, {"referenceID": 0, "context": "After the training, the outputs at [0,0], [0,1], [1,0] and [1,1] are 0.", "startOffset": 59, "endOffset": 64}, {"referenceID": 0, "context": "After the training, the outputs at [0,0], [0,1], [1,0] and [1,1] are 0.", "startOffset": 59, "endOffset": 64}, {"referenceID": 0, "context": "3 and r w =[-1,1], g w =[1,-2], b w =[0,0],", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "3 and r w =[-1,1], g w =[1,-2], b w =[0,0],", "startOffset": 24, "endOffset": 30}], "year": 2017, "abstractText": "\uf020 Abstract \u2014 In machine learning, the use of an artificial neural network is the mainstream approach. Such a network consists of layers of neurons. These neurons are of the same type characterized by the two features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function. Here we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the 1 order neuron to the 2 order neuron, empowering individual neurons, and facilitating the optimization of neural networks. Also, numerical examples are provided to illustrate the feasibility and merits of the 2 order neurons. Finally, further topics are discussed.", "creator": "Microsoft\u00ae Word 2016"}}}