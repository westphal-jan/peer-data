{"id": "1704.09028", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Time-Sensitive Bandit Learning and Satisficing Thompson Sampling", "abstract": "hence the literature on optimal bandit compensation learning and regret analysis has focused on contexts where the best goal is to converge on discovering an optimal action in a sufficient manner that limits exploration costs. one shortcoming imposed purely by this orientation is that it does not treat time preference in a coherent manner. time preference plays an important role when the optimal action efficiency is costly to learn relative to socially near - ideal optimal actions. this noise limitation has not only restricted the accuracy relevance of theoretical results thereof but has also influenced the design of algorithms. indeed, later popular approaches such as thompson accountability sampling and ucb can fare poorly in such situations. in this paper, we consider discounted posterior rather than cumulative regret, where a discount factor encodes time preference. we propose satisficing thompson sampling - - a variation of thompson sampling - - and establish a strong discounted regret bound for this new algorithm.", "histories": [["v1", "Fri, 28 Apr 2017 17:54:59 GMT  (189kb,D)", "http://arxiv.org/abs/1704.09028v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel russo", "david tse", "benjamin van roy"], "accepted": false, "id": "1704.09028"}, "pdf": {"name": "1704.09028.pdf", "metadata": {"source": "CRF", "title": "Time-Sensitive Bandit Learning and Satisficing Thompson Sampling", "authors": ["Daniel Russo", "David Tse", "Benjamin Van Roy"], "emails": ["daniel.russo@kellogg.northwestern.edu", "bvr@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "As high level motivation, consider a recommendation system that interacts sequentially with a single user. The system faces the classic tradeoff between exploration and exploitation: by experimenting with different recommendations the system can learn to offer more effective personalized recommendations in the future, but this may require some degradation of current performance. While recommendation systems are often used as a motivating example for studying the multiarmed bandit problem, this problem has several salient features that are not addressed well by standard bandit algorithms and analysis (e.g. the UCB1 algorithm and analysis of Auer et al. [1]). First, modern recommendation systems have an enormous number of products, but when begining to interact with a new user, the system has a great deal of historical data from interactions with different users, and therefore begins with significant prior knowledge about the products. This prior knowledge presents itself in multiple ways. As certain products are typically much more popular than others, the system begins with evidence that certain types of recommendations will be more successful than others. In addition, data can be used to uncover relevant features of items and users, for example through matrix-completion. As a result, experience offering one type of item to a user can provide significant information about whether they will like a different type of product. Another distinguishing feature of this problem is the presence of a limited and uncertain horizon. The limited number of interactions means that a recommendation system likely won\u2019t have enough experience with each single user to perfectly tailor its recommendations to them. Instead, it is natural to hope for a system that quickly learns to offer highly effective, but still suboptimal, recommendations to its users. The uncertain horizon means that one can\u2019t predict a priori how many\nar X\niv :1\n70 4.\n09 02\n8v 1\n[ cs\n.L G\n] 2\n8 A\npr 2\ntimes the system will interact with a single user. As a result it is especially valuable to have strong performance during early interactions.\nThis work focuses on developing algorithms and a framework for theoretical analysis to address problems with these salient features. We build on the Thompson sampling algorithm (TS) [16] and a recent information theoretic analysis of its performance [14], but offer substantial advances to this thread of theoretical work. TS is able to leverage very general forms of prior information, including rich statistical models that encode a relationship between actions, and prior knowledge that some actions are more likely to offer strong performance than others. The information theoretic analysis of TS yields regret bounds that scale with the entropy of the optimal action distribution. This dependence reflects the performance benefits of prior information but also points to a substantial potential weakness. In particular, entropy generally increases with the number of actions and becomes infinite when they form a continuum. Such regret bounds can therefore be irrelevant when action spaces are very large or infinite. At the heart of this issue is the emphasis Thompson sampling and this information theoretic analysis place on identification of an optimal action. There are circumstances when a near-optimal action can be identified quickly even though an optimal one proves elusive.\nInstead of focusing on cumulative regret, we will compare algorithms based on their expected discounted regret, where the discount factor encodes time preferences. Note that minimizing expected discounted regret is equivalent to minimizing expected undiscounted regret in a problem where the horizon is a geometric random variable, and hence is uncertain. We introduce satisficing Thompson sampling (STS), a modified form of Thompson sampling designed to address problems with limited horizon. We bound discounted-regret by leveraging the information-theoretic concept of rate-distortion, which offers a means for reasoning about the value of information that is useful for identifying near-optimal, not just optimal, actions. Through simulation results, and instantiating these regret bounds on specific examples, we show STS can dramatically outperform TS and standard UCB algorithms when the optimal action is costly to learn relative to high-performing suboptimal actions.\nMany papers [5, 12, 13] have studied bandit problems with continuous action spaces, where it is also necessary to learn only approximately optimal actions. However, because these papers focus on the asymptotic growth rate of regret they implicitly emphasize later stages of learning, where the algorithm has already identified extremely high performing actions but exploration is needed to identify even better actions. Our discounted framework instead focuses on the initial cost of learning to attain good, but not perfect, performance. Recent papers [9, 10] study several heuristics for a discounted objective, though without an orientation toward formal regret analysis. The Knowledge Gradient algorithm of [15] also takes time horizon into account and can learn suboptimal actions when its not worthwhile to identify the optimal action. This algorithm tries to directly approximate the optimal Bayesian policy using a one-step lookahead heuristic, but unfortunately there are no performance guarantees for this method. Deshpande and Montanari [8] consider a linear bandit problem with dimension that is too large relative to the desired horizon. They propose an algorithm that limits exploration and learns something useful within this short time frame. Berry et al. [2], Wang et al. [17] and Bonald and Proutiere [3] study an infinitely-armed bandit problem in which it\u2019s impossible to identify an optimal action and propose algorithms to minimizes the asymptotic growth rate of regret. While we will instantiate our general regret bound for STS on the infinitely-armed bandit problem, we use this example mostly to provide a simple analytic illustration. We hope that the flexibility of STS and our analysis framework allow this work to be applied to more complicated time-sensitive learning problems."}, {"heading": "2 Problem Formulation", "text": "An agent sequentially chooses actions (At)t\u2208N0 from the action set A and observes the corresponding outcomes (Yt,At)t\u2208N0 . There is a random outcome Yt,a \u2208 Y associated with each a \u2208 A and time t \u2208 N0 \u2261 {0, 1, 2..}. Let Yt \u2261 (Yt,a)a\u2208A be the vector of outcomes at time t \u2208 N0. There is a random variable \u03b8 such that, conditioned on \u03b8, (Yt)t\u2208N0 is an iid sequence. Ours can be thought of as a Bayesian formulation, in which the distribution of \u03b8 represents the agent\u2019s prior uncertainty about the true characteristics of the system, and conditioned on \u03b8, the remaining randomness in Yt represents idiosyncratic noise in observed outcomes.\nThe agent associates a reward R(y) with each outcome y \u2208 Y. Let Rt,a \u2261 R(Yt,a) denote the reward corresponding to outcome Yt,a. The history available when selecting action At is\nFt = (A0, Y0,A0 , . . . , At\u22121, Yt\u22121,At\u22121).\nThe agent selects actions according to a policy, which is a sequence of functions (\u03c0t : t \u2208 N0), each mapping a history and an exogenous random variable \u03be to an action. In particular At = \u03c0t(Ft, \u03be) for each t, where \u03be is some random variable that is independent of \u03b8 and (Yt : t \u2208 N0).\nWe denote by Y\u221e an independent copy of Yt. In particular, P(Y\u221e \u2208 \u00b7|\u03b8) = P(Yt \u2208 \u00b7|\u03b8) but conditioned on \u03b8, Y\u221e is drawn independently of (Yt : t \u2208 N0). Let A\u2217 \u2208 argmaxa\u2208A E[R(Y\u221e,a)|\u03b8] denote the true optimal action and let R\u2217 = maxa\u2208AE[R(Y\u221e,a)|\u03b8] denote the corresponding reward. As a performance metric, we consider expected discounted regret, defined by\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] ,\nwhich measures a discounted sum of the expected performance gap between a benchmark policy which always chooses the optimal action A\u2217 and the chosen actions (At : t \u2208 N0). This deviates from the typical notion of expected regret in its dependence on a discount factor \u03b1 \u2208 [0, 1]. Regular expected regret corresponds to the case of \u03b1 = 1. Smaller values of \u03b1 convey time preference by weighting gaps in nearer-term performance higher than gaps in longer-term performance.\nIt is worth noting that minimizing expected discounted regret is equivalent to maximizing expected discounted reward, which is the objective used in the classical Bayesian formulation of the multi-armed bandit problem [11]. For problems of the kind we consider, expected discounted reward can in principle be maximized via dynamic programming. However, solving the associated dynamic programs is computationally intractable. As such, similarly with the bulk of the recent bandit learning literature, we consider heuristic policies and aim to bound regret, though in this paper we consider a discounted variation of regret."}, {"heading": "3 Algorithms", "text": "Thompson sampling (TS) is a popular algorithm that implements a useful decision policy. Over each tth period, TS selects an action At as follows:\n1. Sample \u03b8\u0302t \u223c P(\u03b8|Ft)\n2. Let At \u2208 arg maxa\u2208AE [ Rt,a|\u03b8 = \u03b8\u0302t ] We will assume that actions are indexed and that ties are broken by selecting the action with the smallest index. Note that, as should be the case for any decision policy, we can write TS\nas At = \u03c0t(Ft, \u03be), for an appropriately defined (\u03c0t : t \u2208 N0), where \u03be is independent of \u03b8 and (Yt : t \u2208 N0).\nAs one key contribution of this paper, we introduce a modification of TS, which we will call satisficing Thompson sampling (STS). While TS aims to identify an optimal action, STS is designed to identify an action that is sufficiently satisfying, or close enough to optimal. Over each tth period, STS selects an action At as follows:\n1. Sample \u03b8\u0302t \u223c P(\u03b8|Ft)\n2. Let At \u2208 arg maxa\u2208AE [ Rt,a|\u03b8 = \u03b8\u0302t ] 3. Let \u03c4\u0302 = min { \u03c4 \u2208 {1, . . . , t\u2212 1} : E [ Rt,A\u03c4 |\u03b8 = \u03b8\u0302t ] + \u2265 E [ Rt,At |\u03b8 = \u03b8\u0302t\n]} 4. If \u03c4\u0302 is not null then let At = A\u03c4\u0302\nNote that \u2265 0 is supplied to the algorithm as a tolerance parameter. When = 0, STS is equivalent to TS. Otherwise, STS attributes preference to selecting previously selected actions. As we will further discuss and formalize, this can result in substantial benefit in the face of time preference. In particular, when the optimal action requires a long time to learn but an -optimal action can be learned quickly, STS can quickly achieve -optimal performance where Thompson sampling would continue to incur significant losses deploying resources toward eventual identification of the optimal action.\nIt is worth mentioning that STS can be applied efficiently across the wide variety of problems that are amenable to Thompson sampling. This includes, for example, complex parametric bandit problems. For example, we present in Section 5 computational results involving a linear bandit problem with many arms and many parameters to learn.\nA probability matching property: Thompson sampling satisfies a powerful probability matching property: under TS, Pt(At = a) = Pt(A\u2217 = a) for all a \u2208 A, and so action-sampling probabilities are matched to the posterior distribution of the optimal action. Under STS, actionsampling probabilities instead are essentially matched to the posterior-distribution of the first \u2013optimal action sampled by the algorithm. More precisely, if \u03c4 = inf{t|E[Rt,At |\u03b8] \u2265 R\u2217 \u2212 } then at time t STS sets Pt(At = Ak) = Pt(\u03c4 = k) for each k < t. With probability Pt(\u03c4 \u2265 t) STS selects a new, or previously un-sampled action. In this way, the algorithm aims to identify a satisfactory action while concentrating exploration effort on the smallest number of arms required to do so."}, {"heading": "4 Example: Infinitely-Armed Deterministic Bandit", "text": "To clarify our motivation, we now provide a simple analytic illustration of advantages enjoyed by STS. Consider a problem with a countable action space A = {1, 2, . . .} in which each action a \u2208 A yields reward \u03b8a. Our prior over each \u03b8a is independent and uniform over the interval [0, 1]. The optimal reward is almost surely R\u2217 = 1.\nFor this problem, which we refer to as the infinitely-armed deterministic bandit problem, TS never selects the same action twice. This is because, with probability one, no action selected within a finite time horizon yields reward 1, and as such, at any point in time, there are better actions than those previously selected by TS. STS, in contrast, stops searching after finding an action that generates reward exceeding 1 \u2212 . After such an action is identified, STS repeatedly selects that action.\nThe benefits of STS can be formalized in terms of bounds on expected discounted regret. The following result, proved in the appendix, provides an expression for the expected discounted regret of TS in our infinitely-armed deterministic bandit problem.\nTheorem 1. For all \u03b1 \u2208 [0, 1], under Thompson sampling in the infinitely-armed deterministic bandit problem then\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] = 12(1\u2212 \u03b1) .\nIt is enlightening to compare this to the following bound on expected discounted regret of STS in our infinite deterministic bandit problem, which is also proved in the appendix. Theorem 2. For all \u03b1 \u2208 [0, 1], under satisficing Thompson sampling with tolerance = \u221a\n1\u2212 \u03b1 in the infinitely-armed deterministic bandit problem,\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] \u2264 1\u221a 1\u2212 \u03b1 .\nFor \u03b1 close to 1, 1/ \u221a\n1\u2212 \u03b1 1/(1\u2212 \u03b1), and therefore STS vastly outperforms TS. In fact, as \u03b1 approaches 1, the ratio between expected regret of TS and that of STS goes to infinity."}, {"heading": "5 Computational Examples", "text": "Computational studies involving a broader range of bandit problems further illustrate potential benefits afforded by STS. In this section, we present results from experiments with four bandit problems. Each case is designed so that near-optimal actions can be identified far sooner than the optimal action. In each case, the per-period regret of STS diminishes more rapidly than that of TS over early time periods.\nOur first is a deterministic bandit problem with 250 actions. The mean reward associated with each action is independently sampled from unif([0, 1]). When an action is sampled the realized reward is equal to the mean reward; in other words, there is no observation noise. Figure 1(a) plots per-period regret of TS and STS over 500 time periods, averaged over 5000 simulations, each with an independently sampled problem instance. For STS, we used a tolerance parameter of 0.05.\nWe next consider a problem that is the same as our previous one except with observation noise. In particular, instead of observing the mean reward, after selecting an action, we observe a binary reward that is one with probability equal to the mean reward. Figure 1(b) plots average per-period regret over 5000 simulations. For STS, we used a tolerance parameter of 0.05.\nWe now consider another bandit problem with 250 actions, each with a mean reward sampled independently independently from N(0, 1). Upon taking an action, we observe the sum of the action\u2019s mean reward and an independent N(0, 1) sample that represents observation noise. Figure 1(c) plots per-period regret of TS and STS over 500 time periods, averaged over 5000 simulations, each with an independently sampled problem instance. For STS, we used a tolerance parameter of 0.5.\nFinally, we consider a bandit problem with mean rewards given by a linear function. In particular, mean rewards are given by a vector L\u03b8 \u2208 <|A|, where L \u2208 <|A|\u00d7M is a randomly generated loadings matrix, with each row independently drawn uniformly from the unit sphere, and \u03b8 \u2208 <M is sampled from N(0, I). For our computational study, we let A = {1, . . . , 250} and M = 250. The decision-maker knows L and begins with a N(0, I) prior on \u03b8. Upon taking an action, we observe the sum of the action\u2019s mean reward and an independent N(0, 2) sample that represents observation\nnoise. Figure 1(d) plots per-period regret of TS and STS over 500 time periods, averaged over 5000 simulations, each with an independently sampled problem instance. For STS, we used a tolerance parameter of 1.0."}, {"heading": "6 A General Regret Bound", "text": "This section provides a general discounted regret bound and a new information-theoretic analysis technique. We\u2019ll leverage this general regret bound when analyzing STS in the next section. We begin by reviewing the information-theoretic analysis of Thompson sampling of Russo and Van Roy [14], on which our analysis builds. Before beginning, let us first introduce some additional notation."}, {"heading": "6.1 Notation", "text": "We denote by Et[\u00b7] = E[\u00b7|Ft] the expectation operator conditioned on the history up to time t and similarly define Pt(\u00b7) = P(\u00b7|Ft). We denote the entropy of a discrete random variable X by H(X),\nthe mutual information between two random variablesX and Y by I(X;Y ) and the Kullback-Leibler divergence between probability distributions P and Q by D(P ||Q). The definitions of entropy and mutual information depend on a base measure. We use Ht(\u00b7) and It(\u00b7 , \u00b7) to denote entropy and mutual-information when the base-measure is the posterior distribution Pt. For example, if X is a discrete random variable taking values in a set X ,\nHt(X) = \u2212 \u2211 x\u2208X Pt(X = x) log Pt(X = x).\nDue to its dependence on the realized history Ft, Ht(X) is a random variable. The standard definition of conditional entropy integrates over this randomness, and in particular, E[Ht(X)] = H(X|Ft)."}, {"heading": "6.2 Information Theoretic Analysis of Thompson Sampling", "text": "The regret analysis in [14] relates the regret an algorithm incurs to the information it acquires about the identity of optimal action A\u2217 \u2208 arg maxa E[R(Y\u221e,a)|\u03b8]. They define the information ratio in a given period to be the ratio between the square of single-period expected regret and the information acquired about the optimal action:\nEt[R\u2217 \u2212Rt,At ]2\nIt(A\u2217;Yt,At |\u03be) . (1)\nIt\u2019s shown that every algorithm satisfies a bound on un-discounted expected-regret up period T in terms of its average information ratio over the first T periods and the entropy of the optimal action H(A\u2217). Here the information ratio roughly captures the cost-per-bit of information the algorithm acquires about the optimum, and the entropy H(A\u2217) measures the magnitude of the decision-maker\u2019s initial uncertainty about the identity of the optimal action. For a number of widely studied classes of online optimization problems, strong regret bounds for Thompson sampling can be derived by bounding the algorithm\u2019s information ratio. Subsequent work by Bubeck et al. [6] and Bubeck and Eldan [4] bounds the information-ratio for bandit problems with convex reward functions."}, {"heading": "6.3 A Modified Information Ratio", "text": "This section introduces a modified information ratio, which is more appropriate for time-sensitive online learning problems. As motivation, consider the infinitely-armed deterministic bandit of Section 4. While no algorithm could identify an optimal action in that example, STS is able to efficiently converge to a satisfactory level of performance. In this sense, although the algorithm can\u2019t identify the true optimum, it seems to acquire enough information to identify some highreward action A\u0303. Building on this intuition, our information-theoretic analysis will aim to formally relate regret to the information the algorithm acquires about this A\u0303. To help ground this discussion, consider two examples of such an A\u0303 arising from different problem settings.\nExample 1. Consider the infinitely-armed deterministic bandit of Section 4. As time progresses, STS samples a sequence of actions (A0, A1, A2, ...). Let \u03c4 = min{t|\u03b8At \u2265 1 \u2212 } denote the first time the algorithm samples an action that is \u2013optimal and set A\u0303 := A\u03c4 to be the corresponding action. In this example, there are many \u2013optimal, or \u201dsatisfactory\u201d actions, and A\u0303 is taken to be the first one discovered by the algorithm.\nExample 2. Consider a bandit problem where mean-rewards are given by a linear function. In particular, A \u2282 Rd, and E[Rt,a|\u03b8] = aT \u03b8 for an unknown vector \u03b8. Suppose that \u03b8 \u223c N(0, I) and A consists of n vectors spread out uniformly along boundary of the d dimensional unit sphere {a \u2208 Rd : \u2016a\u20162 = 1}. The optimal action A\u2217 = arg maxa\u2208A \u03b8Ta is then uniformly distributed over A, and hence H(A\u2217) = logn. Here entropy tends to infinity the number of actions grows, and it takes an enormous amount of information to exactly identify A\u2217. For this example, we might take A\u0303 to be a coarser version of A\u2217. In particular, for m n, let A\u0303 consist of m vectors spread out uniformly along boundary of the d dimensional unit sphere {a \u2208 Rd : \u2016a\u20162 = 1} and let A\u0303 = arg maxa\u2208A \u03b8Ta. This can be viewed as a form of lossy-compression, where one may have H(A\u0303) H(A\u2217) but E[Rt,A\u0303] \u2265 E[Rt,A\u2217 ] + for some small > 0.\nIn each of these examples, the action A\u0303 can be viewed as some random variable taking values in the action set A. In the second example, A\u0303 is a deterministic function of \u03b8, and is random only because of the randomness in \u03b8. In the first example, A\u0303 also depends on the algorithm\u2019s internal randomness, which determines the order in which actions are sampled.\nTo address problems of this form, we introduce the following modified information ratio. For any (random) action A\u0303 and (random) action process {At : t \u2208 N0}, define\n\u0393 ( A\u0303, {At : t \u2208 N0} ) = (1\u2212 \u03b12) \u221e\u2211 t=0 \u03b12tE [ Et[R\u0303\u2212Rt,At ]2 It(A\u0303;Yt,At |\u03be) ] , (2)\nwhere R\u0303 = R(Y\u221e,A\u0303). Recall that Y\u221e denotes an independent sample of the action-outcome vector. The subscript of Et and It indicates that the random variables are drawn from the probability space conditioned on Ft. The ratio Et[R\u0303 \u2212 Rt,At ]2/It(A\u0303;Yt,At |\u03be) relates the current shortfall in performance relative to the benchmark action A\u0303 to the amount of information acquired about the benchmark action. The right-hand-side of (2) is the discounted average of these single-period ratios. The square in the discount factor \u03b1 is consistent with the problem\u2019s original discount rate, since Et[\u03b1t(R\u0303\u2212Rt,At)]2 = \u03b12tEt[R\u0303\u2212Rt,At ]2."}, {"heading": "6.4 General Regret Bound", "text": "The following theorem bounds the expected discounted regret of any algorithm, or action process, {At : t \u2208 N0}, in terms of the information ratio (2).\nTheorem 3. For any action process {At : t \u2208 N0} and A\u0303 : \u2126\u2192 A\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] \u2264 E[R \u2217 \u2212 R\u0303] 1\u2212 \u03b1 + \u221a\u221a\u221a\u221a\u0393 (A\u0303, {At : t \u2208 N0})H(A\u0303|\u03be) 1\u2212 \u03b12 . (3)\nwhere R\u0303 = R(Y\u221e,A\u0303).\nThis bound decomposes regret into the sum of two terms; one which captures the discounted performance shortfall of the benchmark action A\u0303 relative toA\u2217, and one which bounds the additional regret incurred while learning to identify A\u0303. Breaking things down further, the entropy H(A\u0303|\u03be) measures the magnitude of the decision-maker\u2019s initial uncertainty about A\u0303, and the information ratio measures the regret incurred in reducing this uncertainty. It\u2019s worth highlighting that for any given action process, this bound holds simultaneously for all possible choices of A\u0303, and in particular, it holds for the A\u0303 minimizing the right hand side of (3)."}, {"heading": "6.5 Connections to Rate Distortion Theory", "text": "In information-theory, the entropy of a source characterizes the length of an optimal lossless encoding. The celebrated rate-distortion theory [7, Chapter 10] characterizes the number of bits required for an encoding to be close in some loss metric. This theory resolves when it is possible to to derive a satisfactory lossy compression scheme while transmitting far less information than required for a lossless compression. At a high level, the developments in this paper represent a shift from entropy to the use of rate-distortion function. Whereas prior results depend on the entropy of A\u2217, Theorem 3 depends on a naturally defined rate distortion function for compressing the optimal decision A\u2217:\nR(D) := min E[R\u2217\u2212R\u0303]\u2264D\nI(A\u0303;A\u2217).\nWhen A\u0303 depends deterministically on A\u2217, I(A\u0303;A\u2217) = H(A\u0303), and hence the rate-distortion function describes the optimal tradeoff between the loss in reward E[R\u2217\u2212 R\u0303] and the entropy of A\u0303, precisely what is needed in minimizing the right hand side of (3)."}, {"heading": "7 Information Ratio Analysis of the Infinitely-Armed Bandit", "text": "The general regret bound of the previous section can be instantiated on two variants of the infinitearmed bandit problem. The next subsection revisits the deterministic infinite-armed bandit of Section 4, and shows how to derive a regret bound for STS using Theorem 3. Subsection 7.2 studies an extension of the infinite-armed bandit problem in which reward-observations are noisy. Again, in this setting Theorem 3 can be specialized to derive a regret bound for STS."}, {"heading": "7.1 Infinitely\u2013Armed Bandit with Deterministic Observations", "text": "We now revisit the infinitely-armed deterministic bandit problem of Section 4. By specializing our general regret bound this setting, we will effectively recover the bound of Theorem 2 that was derived from direct analysis. Because there is no observation noise in this example, once STS samples an action with reward exceeding 1 \u2212 , it will sample it in all subsequent periods. Before that point, the algorithm knows with certainty that no previously-sampled action generates reward exceeding 1 \u2212 , and so a new action will be selected in every period. Let \u03c4 = min{t|\u03b8At \u2265 1 \u2212 } denote the first time an \u2013optimal action is sampled. The next result applies the general regret bound of Theorem 3 to this problem with A\u0303 = A\u03c4 , so the benchmark action is the first \u2013optimal action sampled by STS.\nTheorem 4. For any \u03b1 \u2208 (0, 1), if STS is applied to the deterministic infinite bandit problem with tolerance \u2208 (0, 1) then\nH(A\u0303|\u03be) = H(\u03c4) and \u0393 ( A\u0303, {At : t \u2208 N0} ) \u2264 14 H(\u03c4)\nwhere \u03c4 = min{t|\u03b8At \u2265 1\u2212 } follows a Geometric distribution with parameter and A\u0303 = A\u03c4 . This implies that if = \u221a (1\u2212 \u03b1)/2,\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] \u2264 \u221a 2 1\u2212 \u03b1."}, {"heading": "7.2 Infinitely\u2013Armed Bandit with Noisy Observations", "text": "Now consider a generalization of the problem treated in the previous section that allows for noisy observations and non-uniform priors. We again assume there is a countable action space A = {1, 2, . . .}. Each action a \u2208 A yields expected reward E[Rt,a|\u03b8] = \u03b8a where the \u03b8a are drawn independently from a distribution whose support is the unit interval [0, 1]. Assume rewards are bounded in [0, 1] almost surely.\nWe\u2019ll study the discounted regret incurred by STS with parameter \u2208 (0, 1). Each action sampled by STS is \u2013optimal with probability \u03b4 \u2261 P(\u03b8a > 1 \u2212 ), but because observations are noisy, the algorithm may be uncertain about the quality of the actions it has sampled. The next result provides a regret bound for STS in this more complicated setting. The proof again leverages Theorem 3 with the benchmark action A\u0303 taken to be the first \u2013optimal action sampled by the algorithm. By bounding the problem\u2019s information-ratio, we relate the regret incurred by STS to the information it acquires about the identity of A\u0303.\nTheorem 5. Suppose STS with tolerance parameter \u2208 (0, 1) is applied to the infinite-armed bandit with noisy observations. Then, with probability 1, there exists t \u2208 N0 with \u03b8At \u2265 1\u2212 . If A\u0303 = A\u03c4 where \u03c4 = min{t : \u03b8At \u2265 1\u2212 }, then\nH(A\u0303|\u03be) \u2264 1 + log(1/\u03b4) and \u0393 ( A\u0303, {At : t \u2208 N0} ) \u2264 6 + 4/\u03b4 + (2/\u03b4) log ( 1 1\u2212 \u03b12 ) .\nTogether with Theorem 3 this implies\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] \u2264 1\u2212 \u03b1 + \u221a\u221a\u221a\u221a(6 + 4/\u03b4 + (2/\u03b4) log ( 11\u2212\u03b12)) (1 + log(1/\u03b4)) 1\u2212 \u03b12\n= O\u0303  1\u2212 \u03b1 + \u221a 1/\u03b4 1\u2212 \u03b12  ."}, {"heading": "8 Conclusion", "text": "This paper introduces satisficing Thompson sampling \u2013 a variation of Thompson sampling that can offer vastly superior performance when the optimal action is costly to identify relative to high performing suboptimal actions. We have also developed a general information-theoretic framework for analyzing discounted regret. This framework provides a novel link between optimal decisionmaking with time preferences and the study of lossy data compression. Important questions remain open, but we hope this link will open up many paths for future research."}, {"heading": "A Proof of Theorem 1: Regret of TS on the Infinitely-Armed Deterministic Bandit", "text": "Proof. In every period t, TS samples a previously un-sampled action At /\u2208 {A1, ..., At\u22121}, which generates expected reward E[\u03b8At ] = E[\u03b81] = 1/2. The optimal expected reward is 1, and therefore the expected discounted-regret of TS is\n\u221e\u2211 t=0 \u03b1t(1\u2212 1/2) = 12(1\u2212 \u03b1) ."}, {"heading": "B Proof of Theorem 2: Direct Analysis of the Infinitely-Armed Deterministic Bandit", "text": "Proof. Let \u03c4 = min{t : \u03b8At \u2265 1\u2212 }.\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt) ] = E [ \u221e\u2211 t=0 \u03b1t(1\u2212Rt) ]\n= E [ E [ \u03c4\u22121\u2211 t=0 \u03b1t(1\u2212Rt) + \u221e\u2211 t=\u03c4 \u03b1t(1\u2212Rt,At) \u2223\u2223\u2223\u03c4]]\n= E [(1\u2212 \u03b1\u03c4 )(1\u2212 )\n2(1\u2212 \u03b1) + \u03b1\u03c4 2(1\u2212 \u03b1) ] = E [(1\u2212 \u03b1\u03c4 )(1\u2212 ) 2(1\u2212 \u03b1) + 2(1\u2212 \u03b1) \u2212 (1\u2212 \u03b1\u03c4 ) 2(1\u2212 \u03b1)\n] = E [\n2(1\u2212 \u03b1) + (1\u2212 \u03b1\u03c4 )(1\u2212 2 ) 2(1\u2212 \u03b1)\n] .\nNote that\nE[1\u2212 \u03b1\u03c4 ] = 1\u2212 \u221e\u2211 t=0 (1\u2212 )t\u03b1t = 1\u2212 1\u2212 \u03b1+ \u03b1 = 1\u2212 \u03b1\u2212 + \u03b1 1\u2212 \u03b1+ \u03b1 = (1\u2212 \u03b1)(1\u2212 ) 1\u2212 \u03b1(1\u2212 ) .\nTherefore E [(1\u2212 \u03b1\u03c4 )(1\u2212 2 )\n2(1\u2212 \u03b1)\n] = (1\u2212 )(1\u2212 2 )2(1\u2212 \u03b1(1\u2212 )) = (1\u2212 )(1\u2212 2 ) 2( + (1\u2212 \u03b1)(1\u2212 )) .\nNow consider an upper bound that follows from choosing as a function of \u03b1. We can simplify our upper bound on regret as\n2(1\u2212 \u03b1) + (1\u2212 )(1\u2212 2 ) 2( + (1\u2212 \u03b1)(1\u2212 )) \u2264 2(1\u2212 \u03b1) + 1 2\nThe minimizer of the right hand side is \u2217 = \u221a\n1\u2212 \u03b1. Plugging this in shows that Thompson sampling with a confidence bonus of \u2217 satisfies the discounted regret bound\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] \u2264 1\u221a 1\u2212 \u03b1 ."}, {"heading": "C Proof of Theorem 3", "text": "Proof. We first show that entropy bounds the expected accumulation of mutual-information. By the chain rule for mutual information, for any T ,\nE [ T\u22121\u2211 t=0 It(A\u0303;Yt,At |\u03be) ] = T\u22121\u2211 t=0 I(A\u0303;Yt,At |\u03be,Ht)\n= T\u22121\u2211 t=0 I(A\u0303;Yt,At |\u03be, A0, Y0,A0 , . . . , At\u22121, Yt\u22121,At\u22121)\n= T\u22121\u2211 t=0 I(A\u0303;Yt,At |\u03be, A0, Y0,A0 , . . . , At\u22121, Yt\u22121,At\u22121 , At)\n= I(A\u0303; (A0, Y0,A0 , . . . , At, Yt,At)|\u03be) = H(A\u0303|\u03be)\u2212H(A\u0303|A0, Y0,A0 , . . . , At, Yt,At , \u03be) \u2264 H(A\u0303|\u03be).\nTaking a the limit as T \u2192\u221e implies\nE [ \u221e\u2211 t=0 It(A\u0303;Yt,At |\u03be) ] = lim T\u2192\u221e E [ T\u2211 t=0 It(A\u0303;Yt,At |\u03be) ] \u2264 H(A\u0303|\u03be),\nwhere the monotone convergence theorem justifies the exchange of limit and expectation. Now, fix any A\u0303 and {At : t \u2208 N0}, and let\n\u0393t \u2261 Et[R\u0303\u2212Rt,At ]2\nIt(A\u0303;Yt,At |\u03be))\ndenote the (random) information ratio at time t under the benchmark action A\u0303 and action process {At : t \u2208 N0}. Then we have\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] = E [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212 R\u0303) ] + E [ \u221e\u2211 t=0 \u03b1t(R\u0303\u2212Rt,At) ]\n= E [ R\u2217 \u2212 R\u0303 ] 1\u2212 \u03b1 + E [ \u221e\u2211 t=0 \u221a \u03b12t\u0393t \u221a It(A\u0303;Yt,At)|\u03be) ]\n\u2264 E [ R\u2217 \u2212 R\u0303 ] 1\u2212 \u03b1 + \u221a\u221a\u221a\u221aE [ \u221e\u2211 t=0 \u03b12t\u0393t ]\u221a\u221a\u221a\u221aE [ \u221e\u2211 t=0 It(A\u0303;Yt,At |\u03be) ]\n\u2264 E [ R\u2217 \u2212 R\u0303 ] 1\u2212 \u03b1 + \u221a\u221a\u221a\u221aE [ \u221e\u2211 t=0 \u03b12t\u0393t ]\u221a H(A\u0303|\u03be)\n= E [ R\u2217 \u2212 R\u0303 ] 1\u2212 \u03b1 + \u221a\u221a\u221a\u221a\u0393 (A\u0303, {At : t \u2208 N0})H(A\u0303|\u03be) 1\u2212 \u03b12 ,\nwhere the first inequality follows from the Cauchy-Schwarz inequality and the second was established earlier in this proof."}, {"heading": "D Proof of Theorem 4: Information-Ratio Analysis of Infinitely\u2013 Armed Deterministic Bandit", "text": "Lemma 6. Under STS with tolerance \u2208 (0, 1) in the infinitely\u2013armed deterministic bandit problem, \u03c4 = min{t|\u03b8At \u2265 1 \u2212 } follows a Geometric distribution with parameter , and if A\u0303 = A\u03c4 then\nIt(A\u0303;Yt,At |\u03be) = { H(\u03c4) if Et[R\u2217 \u2212Rt,At ] > 0 otherwise.\nProof. As time progresses, STS samples actions A1, A2, A3.... At each time t <= \u03c4 , it selects a previously un-sampled action At /\u2208 {A1, ...At\u22121}. It selects the actions At = A\u03c4 in each period t > \u03c4 . Because P(\u03b8a \u2265 1\u2212 ) = for each a, we have that \u03c4 follows a Geometric distribution with parameter . Conditioned on \u03c4 \u2265 t, the identity of At is determined by the algorithm\u2019s internal random bits \u03be. That is, the order of the new actions sampled by the algorithm is a function only of \u03be. Therefore, H(A\u0303|\u03be) = H(\u03c4).\nUnder STS, if Et[R\u2217\u2212Rt,At ] \u2264 then At = A\u0303, and \u0393t = 0 since Et[R\u0303\u2212Rt,At ] = 0. On the other hand, if Et[R\u2217 \u2212Rt,At ] > then At /\u2208 {A1, ...At\u22121} and\nIt(A\u0303;Yt,At |\u03be) = Ht(A\u0303|\u03be)\u2212Ht(A\u0303|\u03be, Yt,At)\n= Ht(A\u0303|\u03be)\u2212 \u02c6 1 y=0 Ht(A\u0303|\u03be, Yt,At \u2208 dy)\n= Ht(A\u0303|\u03be)\u2212 \u02c6 1\u2212 y=0 Ht(A\u0303|\u03be, Yt,At \u2208 dy)\u2212 \u02c6 1 y=1\u2212 Ht(A\u0303|\u03be, Yt,At \u2208 dy)\n= Ht(A\u0303|\u03be)\u2212 \u02c6 1\u2212 y=0 Ht(A\u0303|\u03be, Yt,At \u2208 dy)\n= Ht(A\u0303|\u03be)\u2212 (1\u2212 )H(A\u0303|\u03be) = Ht(A\u0303|\u03be) = Ht(\u03c4).\nTogether with the previous lemma, our general regret bound implies Theorem 4.\nProof of Theorem 4. We have\n\u0393 ( A\u0303, {At : t \u2208 N0} ) = (1\u2212 \u03b12)E [ \u221e\u2211 t=0 \u03b12t Et[R\u0303\u2212Rt,At ]2 It(A\u0303;Yt,At |\u03be) ]\n= (1\u2212 \u03b12)E [ \u03c4\u22121\u2211 t=0 \u03b12t (1\u2212 )2 4 H(\u03c4) + \u221e\u2211 t=\u03c4 \u03b12t 02 0 ]\n= E [ 1\u2212 \u03b12\u03c4 ] (1\u2212 )2 4 H(\u03c4) = (1\u2212 ) 2(1\u2212 \u03b12 + \u03b12 \u2212 )\n4 H(\u03c4)(1\u2212 \u03b12 + \u03b12)\n\u2264 14 H(\u03c4) .\nIt follows from Theorem 3 that\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] \u2264 E[R \u2217 \u2212 R\u0303] 1\u2212 \u03b1 + \u221a\u221a\u221a\u221a\u0393 (A\u0303, {At : t \u2208 N0}) I(\u03b8; A\u0303|\u03be) 1\u2212 \u03b12\n\u2264 1\u2212 \u03b1 + \u221a (1\u2212 )2(1\u2212 \u03b12 + \u03b12 \u2212 ) 4 (1\u2212 \u03b12 + \u03b12)(1\u2212 \u03b12)\n= 1\u2212 \u03b1 + \u221a 1\u2212 3 4 2 + (1\u2212 )(1\u2212 \u03b12))\n\u2264 1\u2212 \u03b1 + 1 2 .\nNow we consider an upper bound that follows from choosing as a function of \u03b1. The minimizer of 1\u2212\u03b1 + 1 2 is \u2217 = \u221a\n(1\u2212 \u03b1)/2. If {At : t \u2208 N0} is generated STS with parameter \u2217, the bound on regret becomes\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] \u2264 \u221a 2 1\u2212 \u03b1."}, {"heading": "E Proof of Theorem 5: Information Ratio Analysis of the InfinitelyArmed Bandit with Noisy Observations", "text": "The proof of Theorem 5 leverages the probability matching property of STS highlighted in Section 3. Recall that A\u0303 = A\u03c4 where \u03c4 = min{t|\u03b8At \u2265 }. Throughout this proof, let At \u2261 {A1, A2, ...At\u22121} denote the set of previously sampled actions. Under STS, P(At = a|Ft) = P(A\u0303 = a|Ft) for all a \u2208 At, and P(At /\u2208 At|Ft) = P(A\u0303 /\u2208 At|Ft). The algorithm essentially performs a kind of probability matching on A\u0303.\nTheorem 5. Suppose STS with tolerance parameter \u2208 (0, 1) is applied to the infinite-armed bandit with noisy observations. Then, with probability 1, there exists t \u2208 N0 with \u03b8At \u2265 1\u2212 . If A\u0303 = A\u03c4 where \u03c4 = min{t : \u03b8At \u2265 1\u2212 }, then\nH(A\u0303|\u03be) \u2264 1 + log(1/\u03b4) and \u0393 ( A\u0303, {At : t \u2208 N0} ) \u2264 6 + 4/\u03b4 + (2/\u03b4) log ( 1 1\u2212 \u03b12 ) .\nTogether with Theorem 3 this implies\nE [ \u221e\u2211 t=0 \u03b1t(R\u2217 \u2212Rt,At) ] \u2264 1\u2212 \u03b1 + \u221a\u221a\u221a\u221a(6 + 4/\u03b4 + (2/\u03b4) log ( 11\u2212\u03b12)) (1 + log(1/\u03b4)) 1\u2212 \u03b12\n= O\u0303  1\u2212 \u03b1 + \u221a 1/\u03b4 1\u2212 \u03b12  . We begin with a lemma establishing that with probability 1 STS will eventually sample an \u2013 optimal action. At an intuitive level, this result follows from the algorithm\u2019s probability matching property, which guarantees that whenever its likely that no \u2013optimal action has been sampled previously, the algorithm is likely to select a previously un-sampled action. With probability \u03b4 this new action is \u2013optimal.\nLemma 7. If STS with tolerance parameter \u2208 (0, 1) is applied to the infinite-armed bandit with noisy observations, then, with probability 1, there exists t \u2208 N0 with \u03b8At \u2265 1\u2212 .\nProof. Our goal is to show P(\u03c4 <\u221e) = 1 where\n\u03c4 = inf{t : \u03b8At \u2265 1\u2212 }.\nBy the so-called continuity of measure,\nP(\u03c4 <\u221e) = lim t\u2192\u221e P(\u03c4 \u2264 t) = 1\u2212 lim t\u2192\u221e P(\u03c4 \u2265 t).\nNow set \u03b2 \u2261 lim\nt\u2192\u221e P(\u03c4 \u2265 t)\nBecause P(\u03c4 \u2265 t) is a decreasing bounded sequence, this limit exists, and \u03b2 = inft\u2208N0 P(\u03c4 \u2265 t). The proof shows \u03b2 = 0.\nBy the probability matching property of STS P(At /\u2208 At|Ft) = P(\u03c4 \u2265 t|Ft). Then, by the definition of \u03c4 and the independence among the components of (\u03b81, \u03b82, ...)\nP(\u03c4 = t|Ft) = P(A\u0303 /\u2208 At \u2227At /\u2208 At|Ft)\u03b4 = P(A\u0303 /\u2208 At|Ft)2\u03b4 = P(\u03c4 \u2265 t|Ft)2\u03b4.\nTaking expectations implies\nP(\u03c4 = t) = E [ P(\u03c4 \u2265 t|Ft)2 ] \u03b4 \u2265 E [P(\u03c4 \u2265 t|Ft)]2 \u03b4 = P(\u03c4 \u2265 t)2\u03b4.\nThen P(\u03c4 \u2265 t)\u2212P(\u03c4 \u2265 t+ 1) = P(\u03c4 = t) \u2265 P(\u03c4 \u2265 t)2\u03b4.\nSince P(\u03c4 \u2265 t) converges,\n0 = lim t\u2192\u221e (P(\u03c4 \u2265 t)\u2212P(\u03c4 \u2265 t+ 1)) \u2265 lim t\u2192\u221e\nP(\u03c4 \u2265 t)2\u03b4 = \u03b22\u03b4.\nSince \u03b2 \u2208 [0, 1] by definition, this implies \u03b2 = 0.\nThe remaining proof will follow from a sequence of lemmas. We now bound the entropy of A\u0303.\nLemma 8. H(A\u0303|\u03be) \u2264 1 + log(1/\u03b4)\nProof. Because the order in which new actions are sampled is completely determined given \u03be,\nH(A\u0303|\u03be) = H(N) where N \u223c Geom(\u03b4) is a geometric random variable. This implies\nH(A\u0303|\u03be) = H (N)\n= \u2212 \u221e\u2211 k=1 \u03b4(1\u2212 \u03b4)k\u22121 log(\u03b4(1\u2212 \u03b4)k\u22121)\n= \u2212 \u221e\u2211 k=1 \u03b4(1\u2212 \u03b4)k\u22121 log(\u03b4)\u2212 \u221e\u2211 k=1 \u03b4(1\u2212 \u03b4)k\u22121 log((1\u2212 \u03b4)k\u22121)\n= \u221e\u2211 k=1 P(N = k) log(1/\u03b4)\u2212 log(1\u2212 \u03b4) \u221e\u2211 k=1 \u03b4(1\u2212 \u03b4)k\u22121(k \u2212 1)\n= log(1/\u03b4) + log ( 1\n1\u2212 \u03b4\n) (E[N ]\u2212 1)\n= log(1/\u03b4) + log (\n1 + \u03b41\u2212 \u03b4 )(1\u2212 \u03b4 \u03b4 ) \u2264 1 + log(1/\u03b4).\nThe bound on entropy yields a regret bound when combined with a bound the information ratio. The next lemma gives bounds the one-step information ratio.\nLemma 9. Et[\u03b8A\u0303 \u2212 \u03b8At ]2\nIt(A\u0303;Yt,At |\u03be) \u2264 2|At|+ 2/\u03b4\nwhere At = \u222at\u22121s=1{As} is the set of actions that were sampled before period t, and \u03b4 \u2261 P(\u03b8i \u2265 1\u2212 ) is the prior probability an arm is \u2013optimal.\nProof. Define L \u2261 E[\u03b8i|\u03b8i \u2265 1\u2212 ]\u2212E[\u03b8i]\nand \u03b4 \u2261 P(\u03b8i \u2265 1\u2212 ).\nHere \u03b4 is the probability an unsampled arm is optimal, and L is the difference between the expected reward of an optimal arm and that of an arm sampled uniformly at random. In the case where \u03b8i \u223c Unif(0, 1), \u03b4 = and L = (1\u2212 )/2.\nWe can write expected regret as\nEt[\u03b8A\u0303 \u2212 \u03b8At ] = \u2211 a\u2208A Pt(A\u0303 = a)Et[\u03b8a|A\u0303 = a]\u2212 \u2211 a\u2208A Pt(At = a)Et[\u03b8a]\n= \u2211 a\u2208At Pt(A\u0303 = a) ( Et[\u03b8a|A\u0303 = a]\u2212Et[\u03b8a] ) + \u2211 a/\u2208At Pt(A\u0303 = a)Et[\u03b8a|A\u0303 = a]\u2212 \u2211 a/\u2208At Pt(At = a)Et[\u03b8a]\n= \u2211 a\u2208At Pt(A\u0303 = a) ( Et[\u03b8a|A\u0303 = a]\u2212Et[\u03b8a] ) + Pt(A\u0303 /\u2208 At)(E[\u03b8a|\u03b8a \u2265 1\u2212 ]\u2212E[\u03b8a])\n= \u2211 a\u2208At Pt(A\u0303 = a) ( Et[\u03b8a|A\u0303 = a]\u2212Et[\u03b8a] ) \ufe38 \ufe37\ufe37 \ufe38\n\u2206t,1\n+ Pt(A\u0303 /\u2208 At)L\ufe38 \ufe37\ufe37 \ufe38 \u2206t,2\nThis decomposes regret into the sum of two terms: one which captures the regret due to suboptimal selection within the set of previously sampled actions At, and one due to the remaining possibility that none of the sampled actions are optimal. The proof develops a similar decomposition for mutual information, and then lower bounds both terms.\nWe can express mutual information as follows:\nIt(A\u0303;Yt,At |\u03be) = \u2211 a\u2208A Pt(At = a)It(A\u0303;Yt,a|At = a)\n= \u2211 a\u2208A Pt(A\u0303 = a)It(A\u0303;Yt,a|At = a)\n= \u2211 a\u2208At Pt(A\u0303 = a)It(A\u0303;Yt,a) + Pt(A\u0303 /\u2208 At)It(A\u0303;Yt,aN |At = aN )\nwhere aN \u2208 Act is an arbitrary action that has not yet been sampled. (N stands for \u201cnew\u201d) Now, using the shorthand Pt(X) = Pt(X \u2208 \u00b7) to denote the posterior distribution of a random variable X, we have\nIt(A\u0303;Yt,aN |At = aN ) = \u2211 a\u2208A Pt(A\u0303 = a|At = aN )D ( Pt(Yt,aN |A\u0303 = a,At = aN )||Pt(Yt,aN ) ) \u2265 Pt(A\u0303 = aN |At = aN )D ( Pt(Yt,aN |A\u0303 = aN , At = aN )||Pt(Yt,aN )\n) = Pt(A\u0303 = aN |At = aN )D (Pt(Yt,aN |\u03b8aN \u2265 1\u2212 )||Pt(Yt,aN )) \u2265 2Pt(A\u0303 = aN |At = aN ) (Et[R(Yt,aN )|\u03b8aN \u2265 1\u2212 ]\u2212Et[R(Yt,aN )]) 2 = 2Pt(A\u0303 = aN |At = aN )L2 = 2Pt(A\u0303 /\u2208 At)P(A\u0303 = aN |At = aN , A\u0303 /\u2208 At)L2 = 2Pt(A\u0303 /\u2208 At)\u03b4L2.\nFollowing the analysis from [14] shows\u2211 a\u2208At Pt(A\u0303 = a)It(A\u0303;Yt,a) = \u2211 a\u2208At Pt(A\u0303 = a) \u2211 a\u0303\u2208A D ( Pt(Yt,a||A\u0303 = a\u0303)||Pt(Yt,a) ) \u2265\n\u2211 a\u2208At Pt(A\u0303 = a)2D ( Pt(Yt,a||A\u0303 = a)||Pt(Yt,a) ) \u2265 2\n\u2211 a\u2208At Pt(A\u0303 = a)2 ( Et[\u03b8a|A\u0303 = a]\u2212Et[\u03b8a] )2\n\u2265 2 |At| \u2211 a\u2208At Pt(A\u0303 = a) ( Et[\u03b8a|A\u0303 = a]\u2212Et[\u03b8a] )2 . Therefore\nIt(A\u0303;Yt,At |\u03be) \u2265 2 |At| \u2211 a\u2208At Pt(A\u0303 = a) ( Et[\u03b8a|A\u0303 = a]\u2212Et[\u03b8a] )2 \ufe38 \ufe37\ufe37 \ufe38\nGt,1\n+ 2Pt(A\u0303 /\u2208 At)2\u03b4L2\ufe38 \ufe37\ufe37 \ufe38 Gt,2 ,\nis lower bounded by the sum of two terms: one which captures the information gain due to refining knowledge about previously sampled actions, and one that captures the expected information gathered about previously unexplored actions.\nTo bound the information ratio we\u2019ll separately consider two cases. If \u22061 \u2265 \u22062, then\nEt[\u03b8A\u0303 \u2212 \u03b8At ]2 It(A\u0303;Yt,At |\u03be) \u2264 (2\u2206t,1) 2 Gt,1 +Gt,2 \u2264 4(\u2206t,1) 2 Gt,1 = 2|At|.\nIf instead \u22061 < \u22062, then\nEt[\u03b8A\u0303 \u2212 \u03b8At ]2 It(A\u0303;Yt,At |\u03be) \u2264 (2\u2206t,2) 2 Gt,1 +Gt,2 \u2264 4(\u2206t,2) 2 Gt,2 = 2 \u03b4 .\nThis shows Et[\u03b8A\u0303 \u2212 \u03b8At ]2\nIt(A\u0303; \u03b8;Yt,At |\u03be) \u2264 2|At|+ 2/\u03b4.\nWe\u2019d now like to use the previous result to bound\n\u0393 ( A\u0303, {At : t \u2208 N0} ) = (1\u2212 \u03b12) \u221e\u2211 t=0 \u03b12tE [ Et[R\u0303\u2212Rt,At ]2 It(A\u0303;Yt,At |\u03be) ]\n\u2264 2/\u03b4 + 2(1\u2212 \u03b12) \u221e\u2211 t=0 \u03b12tE[|At|].\nWe begin by bounding E[|At|].\nLemma 10. |A0| = 0 and for each T \u2208 N0, E[|AT |] \u2264 2 + log(T )/\u03b4.\nProof. Let \u03c4k = min{t \u2264 T ||At| \u2265 k} denote the first period before T in which k actions have been sampled. Then\nE[|AT |] = E[|A\u03c4k |] + E[|AT | \u2212 |A\u03c4k |] \u2264 E[|A\u03c4k |] + E[|A\u03c4k+T | \u2212 |A\u03c4k |]\n\u2264 k + E \u03c4k+T\u22121\u2211 t=\u03c4k 1(At /\u2208 At)\n= k + E T\u22121\u2211 s=0 P(A\u03c4k+s /\u2208 A\u03c4k+s|H\u03c4k+s)\n= k + E T\u22121\u2211 s=0 P(A\u0303 /\u2208 A\u03c4k+s|H\u03c4k+s)\n= k + T\u22121\u2211 s=0 P(A\u0303 /\u2208 A\u03c4k+s)\n\u2264 k + TP(A\u0303 /\u2208 A\u03c4k) = k + TP(Geom(\u03b4) > k) = k + T (1\u2212 \u03b4)k\n\u2264 k + Te\u2212\u03b4k.\nChoosing k = dlog(T )/\u03b4e \u2264 1 + log(T )/\u03b4, implies\nE[|AT |] \u2264 2 + log(T )/\u03b4.\nThe next technical lemma shows \u2211\u221e t=1 \u03b3\n\u2212t log(t) = O((1/\u03b3) log(1/\u03b3)). Lemma 11. For any \u03b3 \u2208 (0, 1),\n\u221e\u2211 t=1 \u03b3\u2212t log(t) \u2264 11\u2212 \u03b3 [ 1 + log ( 1 1\u2212 \u03b3 )] .\nProof. \u221e\u2211 t=1 \u03b3\u2212t log(t) \u2264 \u221e\u2211 t=1 e\u2212(1\u2212\u03b3)t log(t)\n= \u221e\u2211 t=2 e\u2212(1\u2212\u03b3)t log(t)\n\u2217 \u2264 \u02c6 \u221e\n1 e\u2212(1\u2212\u03b3)x log(x+ 1)dx\n= 11\u2212 \u03b3 \u02c6 \u221e 1 e\u2212u log ( u 1\u2212 \u03b3 + 1 ) du \u2264 11\u2212 \u03b3 ([ 1 + log ( 1 1\u2212 \u03b3 )]\u02c6 \u221e 1 e\u2212udu+ \u02c6 \u221e 1 e\u2212u log(u)du\n) = 11\u2212 \u03b3 ([ 1 + log ( 1 1\u2212 \u03b3 )] (1/e) + \u02c6 \u221e 1 e\u2212u log(u)du ) \u2264 11\u2212 \u03b3 [ 1 + log ( 1 1\u2212 \u03b3\n)] where the last step uses a numerical approximation to the indefinite integral\u02c6 \u221e\n1 e\u2212u log(u)du \u2248 .22\nalong with the fact that 1/e+ .22 \u2248 .57 < 1. The inequality (*) uses that for any t \u2265 2\ne\u2212(1\u2212\u03b3)t log(t) \u2264 t\u02c6\nt\u22121\ne\u2212(1\u2212\u03b3)x log(x+ 1)\nsince e\u2212(1\u2212\u03b3)x is decreasing in x and log(x) is increasing in x.\nFinally we can conclude with the proof of Theorem 5. We have\n\u0393 ( A\u0303, {At : t \u2208 N0} ) = (1\u2212 \u03b12) \u221e\u2211 t=0 \u03b12tE [ Et[R\u0303\u2212Rt,At ]2 It(A\u0303;Yt,At |\u03be) ]\n\u2264 2/\u03b4 + 2(1\u2212 \u03b12) \u221e\u2211 t=0 \u03b12tE[|At|]\nSince\n(1\u2212 \u03b12) \u221e\u2211 t=0 \u03b12tE[|At|] \u2264 (1\u2212 \u03b12) \u221e\u2211 t=1 \u03b12t (2 + log(t)/\u03b4)\n\u2264 3 + (1/\u03b4)(1\u2212 \u03b12) \u221e\u2211 t=1 \u03b12t log(t)\n\u2264 3 + (1/\u03b4) [ 1 + log ( 1 1\u2212 \u03b12 )] ,\nthis implies\n\u0393 ( A\u0303, {At : t \u2208 N0} ) \u2264 6 + 4/\u03b4 + (2/\u03b4) log ( 1 1\u2212 \u03b12 ) = O ( (1/\u03b4) log ( 1 1\u2212 \u03b12 )) and concludes the proof of Theorem 5."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>The literature on bandit learning and regret analysis has focused on contexts where the goal<lb>is to converge on an optimal action in a manner that limits exploration costs. One shortcoming<lb>imposed by this orientation is that it does not treat time preference in a coherent manner.<lb>Time preference plays an important role when the optimal action is costly to learn relative to<lb>near-optimal actions. This limitation has not only restricted the relevance of theoretical results<lb>but has also influenced the design of algorithms. Indeed, popular approaches such as Thompson<lb>sampling and UCB can fare poorly in such situations. In this paper, we consider discounted<lb>rather than cumulative regret, where a discount factor encodes time preference. We propose<lb>satisficing Thompson sampling \u2013 a variation of Thompson sampling \u2013 and establish a strong<lb>discounted regret bound for this new algorithm.", "creator": "LaTeX with hyperref package"}}}