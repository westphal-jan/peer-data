{"id": "1705.00364", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2017", "title": "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings", "abstract": "as we consider the problem of learning performing general - purpose, paraphrastic sentence embeddings, revisiting the setting of edward wieting et al. ( 2016b ). while they found lstm recurrent networks to underperform word averaging, we present nevertheless several developments that together produce the opposite conclusion. these features include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. these improve lstms in both transfer learning and supervised reporting settings. we also introduce a new recurrent architecture, the gated recurrent averaging network, that is completely inspired differently by averaging and lstms while outperforming them both. we analyze our learned models, finding evidence associations of preferences for particular parts of conscious speech and dependency relations.", "histories": [["v1", "Sun, 30 Apr 2017 19:18:22 GMT  (33kb)", "http://arxiv.org/abs/1705.00364v1", "Published as a long paper at ACL 2017"]], "COMMENTS": "Published as a long paper at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john wieting", "kevin gimpel"], "accepted": true, "id": "1705.00364"}, "pdf": {"name": "1705.00364.pdf", "metadata": {"source": "CRF", "title": "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings", "authors": ["John Wieting", "Kevin Gimpel"], "emails": ["jwieting@ttic.edu", "kgimpel@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n00 36\n4v 1\n[ cs\n.C L\n] 3\n0 A\npr 2\n01 7\ning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the GATED RECURRENT AVERAGING NETWORK, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations. 1"}, {"heading": "1 Introduction", "text": "Modeling sentential compositionality is a fundamental aspect of natural language semantics. Researchers have proposed a broad range of compositional functional architectures (Mitchell and Lapata, 2008; Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated them on a large variety of applications. Our goal is to learn a general-purpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) (Agirre et al., 2012) and can also serve as a useful initialization for downstream tasks. We wish to learn this embedding function such that sentences\n1Trained models and code are available at http://ttic.uchicago.edu/\u02dcwieting.\nwith high semantic similarity have high cosine similarity in the embedding space. In particular, we focus on the setting of Wieting et al. (2016b), in which models are trained on noisy paraphrase pairs and evaluated on both STS and supervised semantic tasks.\nSurprisingly, Wieting et al. found that simple embedding functions\u2014those based on averaging word vectors\u2014outperform more powerful architectures based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997). In this paper, we revisit their experimental setting and present several techniques that together improve the performance of the LSTM to be superior to word averaging.\nWe first change data sources: rather than train on noisy phrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), we use noisy sentence pairs obtained automatically by aligning Simple English to standard English Wikipedia (Coster and Kauchak, 2011). Even though this data was intended for use by text simplification systems, we find it to be efficient and effective for learning sentence embeddings, outperforming much larger sets of examples from PPDB.\nWe then show how we can modify and regularize the LSTM to further improve its performance. The main modification is to simply average the hidden states instead of using the final one. For regularization, we experiment with two kinds of dropout and also with randomly scrambling the words in each input sequence. We find that these techniques help in the transfer learning setting and on two supervised semantic similarity datasets as well. Further gains are obtained on the supervised tasks by initializing with our models from the transfer setting.\nInspired by the strong performance of both averaging and LSTMs, we introduce a novel recurrent neural network architecture which we call\nthe GATED RECURRENT AVERAGING NETWORK (GRAN). The GRAN outperforms averaging and the LSTM in both the transfer and supervised learning settings, forming a promising new recurrent architecture for semantic modeling."}, {"heading": "2 Related Work", "text": "Modeling sentential compositionality has received a great deal of attention in recent years. A comprehensive survey is beyond the scope of this paper, but we mention popular functional families: neural bag-ofwords models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), recursive neural networks using syntactic parses (Socher et al., 2011, 2012, 2013; I\u0307rsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015). Simple operations based on vector addition and multiplication typically serve as strong baselines (Mitchell and Lapata, 2008, 2010; Blacoe and Lapata, 2012).\nMost work cited above uses a supervised learning framework, so the composition function is learned discriminatively for a particular task. In this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for supervised sentence tasks rather than to capture semantic similarity.\nAn exception is the work of Wieting et al. (2016b). We closely follow their experimental setup and directly address some outstanding questions in their experimental results. Here we briefly summarize their main findings and their attempts at explaining them. They made the surprising discovery that word averaging outperforms LSTMs by a wide margin in the transfer learning setting. They proposed several hypotheses for why this occurs. They first considered that the LSTM was unable to adapt to the differences in sequence length between phrases in training and sentences in test. This was ruled out by showing that neither model\nshowed any strong correlation between sequence length and performance on the test data.\nThey next examined whether the LSTM was overfitting on the training data, but then showed that both models achieve similar values of the training objective and similar performance on indomain held-out test sets. Lastly, they considered whether their hyperparameters were inadequately tuned, but extensive hyperparameter tuning did not change the story. Therefore, the reason for the performance gap, and how to correct it, was left as an open problem. This paper takes steps toward addressing that problem."}, {"heading": "3 Models and Training", "text": ""}, {"heading": "3.1 Models", "text": "Our goal is to embed a word sequence s into a fixed-length vector. We focus on three compositional models in this paper, all of which use words as the smallest unit of compositionality. We denote the tth word in s as st, and we denote its word embedding by xt.\nOur first two models have been well-studied in prior work, so we describe them briefly. The first, which we call AVG, simply averages the embeddings xt of all words in s. The only parameters learned in this model are those in the word embeddings themselves, which are stored in the word embedding matrix Ww. This model was found by Wieting et al. (2016b) to perform very strongly for semantic similarity tasks.\nOur second model uses a long shortterm memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997) to embed s. We use the LSTM variant from Gers et al. (2003) including its \u201cpeephole\u201d connections. We consider two ways to obtain a sentence embedding from the LSTM. The first uses the final hidden vector, which we denote h\u22121. The second, denoted LSTMAVG, averages all hidden vectors of the LSTM. In both variants, the learnable parameters include both the LSTM parameters Wc and the word embeddings Ww.\nInspired by the success of the two models above, we propose a third model, which we call the GATED RECURRENT AVERAGING NETWORK (GRAN). The GATED RECURRENT AVERAGING NETWORK combines the benefits of AVG and LSTMs. In fact it reduces to AVG if the output of the gate is all ones. We first use an LSTM to generate a hidden vector, ht, for each word st in\ns. Then we use ht to compute a gate that will be elementwise-multiplied with xt, resulting in a new, gated hidden vector at for each step t:\nat = xt \u2299 \u03c3(Wxxt +Whht + b) (1)\nwhere Wx and Wh are parameter matrices, b is a parameter vector, and \u03c3 is the elementwise logistic sigmoid function. After all at have been generated for a sentence, they are averaged to produce the embedding for that sentence. This model includes as learnable parameters those of the LSTM, the word embeddings, and the additional parameters in Eq. (1). For both the LSTM and GRAN models, we use Wc to denote the \u201ccompositional\u201d parameters, i.e., all parameters other than the word embeddings.\nThe motivation for the GRAN is that we are contextualizing the word embeddings prior to averaging. The gate can be seen as an attention, attending to the prior context of the sentence.2\nWe also experiment with four other variations of this model, though they generally were more complex and showed inferior performance. In the first, GRAN-2, the gate is applied to ht (rather than xt) to produce at, and then these at are averaged as before.\nGRAN-3 and GRAN-4 use two gates: one applied to xt and one applied to at\u22121. We tried two different ways of computing these gates: for each gate i, \u03c3(Wxixt+Whiht+bi) (GRAN-3) or \u03c3(Wxixt + Whiht + Waiat\u22121 + bi) (GRAN-4). The sum of these two terms comprised at. In this model, the last average hidden state, a\u22121, was used as the sentence embedding after dividing it by the length of the sequence. In these models, we are additionally keeping a running average of the embeddings that is being modified by the context at every time step. In GRAN-4, this running average is also considered when producing the contextualized word embedding.\nLastly, we experimented with a fifth GRAN, GRAN-5, in which we use two gates, calculated by \u03c3(Wxixt + Whiht + bi) for each gate i. The first is applied to xt and the second is applied to ht. The output of these gates is then summed. Therefore GRAN-5 can be reduced to either wordaveraging or averaging LSTM states, depending on the behavior of the gates. If the first gate\n2We tried a variant of this model without the gate. We obtain at from f(Wxxt+Whht+b), where f is a nonlinearity, tuned over tanh and ReLU. The performance of the model is significantly worse than the GRAN in all experiments.\nis all ones and the second all zeros throughout the sequence, the model is equivalent to wordaveraging. Conversely, if the first gate is all zeros and the second is all ones throughout the sequence, the model is equivalent to averaging the LSTM states. Further analysis of these models is included in Section 4."}, {"heading": "3.2 Training", "text": "We follow the training procedure of Wieting et al. (2015) and Wieting et al. (2016b), described below. The training data consists of a set S of phrase or sentence pairs \u3008s1, s2\u3009 from either the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) or the aligned Wikipedia sentences (Coster and Kauchak, 2011) where s1 and s2 are assumed to be paraphrases. We optimize a margin-based loss:\nmin Wc,Ww\n1\n|S|\n(\n\u2211\n\u3008s1,s2\u3009\u2208S\nmax(0, \u03b4 \u2212 cos(g(s1), g(s2))\n+ cos(g(s1), g(t1))) + max(0, \u03b4 \u2212 cos(g(s1), g(s2))\n+ cos(g(s2), g(t2)))\n)\n+ \u03bbc \u2016Wc\u2016 2 + \u03bbw \u2016Wwinitial \u2212Ww\u2016 2\n(2)\nwhere g is the model in use (e.g., AVG or LSTM), \u03b4 is the margin, \u03bbc and \u03bbw are regularization parameters, Wwinitial is the initial word embedding matrix, and t1 and t2 are carefully-selected negative examples taken from a mini-batch during optimization. The intuition is that we want the two phrases to be more similar to each other (cos(g(s1), g(s2))) than either is to their respective negative examples t1 and t2, by a margin of at least \u03b4."}, {"heading": "3.2.1 Selecting Negative Examples", "text": "To select t1 and t2 in Eq. (2), we simply choose the most similar phrase in some set of phrases (other than those in the given phrase pair). For simplicity we use the mini-batch for this set, but it could be a different set. That is, we choose t1 for a given \u3008s1, s2\u3009 as follows:\nt1 = argmax t:\u3008t,\u00b7\u3009\u2208Sb\\{\u3008s1,s2\u3009} cos(g(s1), g(t))\nwhere Sb \u2286 S is the current mini-batch. That is, we want to choose a negative example ti that is similar to si according to the current model. The downside is that we may occasionally choose a phrase ti that is actually a true paraphrase of si."}, {"heading": "4 Experiments", "text": "Our experiments are designed to address the empirical question posed by Wieting et al. (2016b): why do LSTMs underperform AVG for transfer learning? In Sections 4.1.2-4.2, we make progress on this question by presenting methods that bridge the gap between the two models in the transfer setting. We then apply these same techniques to improve performance in the supervised setting, described in Section 4.3. In both settings we also evaluate our novel GRAN architecture, finding it to consistently outperform both AVG and the LSTM."}, {"heading": "4.1 Transfer Learning", "text": ""}, {"heading": "4.1.1 Datasets and Tasks", "text": "We train on large sets of noisy paraphrase pairs and evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. We report the average Pearson\u2019s r over these 22 sentence similarity tasks.\nEach STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Further details are provided in the official task descriptions (Agirre et al., 2012, 2013, 2014, 2015)."}, {"heading": "4.1.2 Experiments with Data Sources", "text": "We first investigate how different sources of training data affect the results. We try two data sources. The first is phrase pairs from the Paraphrase Database (PPDB). PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any manual annotation (Ganitkevitch and Callison-Burch, 2014).\nThe second source of data is a set of sentence pairs automatically extracted from Simple English Wikipedia and English Wikipedia articles by Coster and Kauchak (2011). This data was extracted for developing text simplification systems, where each instance pairs a simple and complex sentence representing approximately the same information. Though the data was obtained for simplification, we use it as a source of training data for learning paraphrastic sentence embeddings. The dataset, which we call SimpWiki, consists of 167,689 sentence pairs.\nTo ensure a fair comparison, we select a sample of pairs from PPDB XL such that the number of tokens is approximately the same as the number of tokens in the SimpWiki sentences.3\nWe use PARAGRAM-SL999 embeddings (Wieting et al., 2015) to initialize the word embedding matrix (Ww) for all models. For all experiments, we fix the mini-batch size to 100, and \u03bbc to 0. We tune the margin \u03b4 over {0.4, 0.6, 0.8} and \u03bbw over {10\n\u22124, 10\u22125, 10\u22126, 10\u22127, 10\u22128, 0}. We train AVG for 7 epochs, and the LSTM for 3, since it converges much faster and does not benefit from 7 epochs. For optimization we use Adam (Kingma and Ba, 2015) with a learning rate of 0.001. We use the 2016 STS tasks (Agirre et al., 2016) for model selection, where we average the Pearson\u2019s r over its 5 datasets. We refer to this type of model selection as test. For evaluation, we report the average Pearson\u2019s r over the 22 other sentence similarity tasks.\nThe results are shown in Table 1. We first note that, when training on PPDB, we find the same result as Wieting et al. (2016b): AVG outperforms the LSTM bymore than 13 points. However, when training both on sentence pairs, the gap shrinks to about 9 points. It appears that part of the inferior performance for the LSTM in prior work was due\n3The PPDB data consists of 1,341,188 phrase pairs and contains 3 more tokens than the SimpWiki data.\nto training on phrase pairs rather than on sentence pairs. The AVG model also benefits from training on sentences, but not nearly as much as the LSTM.4\nOur hypothesis explaining this result is that in PPDB, the phrase pairs are short fragments of text which are not necessarily constituents or phrases in any syntactic sense. Therefore, the sentences in the STS test sets are quite different from the fragments seen during training. We hypothesize that while word-averaging is relatively unaffected by this difference, the recurrent models are much more sensitive to overall characteristics of the word sequences, and the difference between train and test matters much more.\nThese results also suggest that the SimpWiki data, even though it was developed for text simplification, may be useful for other researchers working on semantic textual similarity tasks."}, {"heading": "4.1.3 Experiments with LSTM Variations", "text": "We next compare LSTM and LSTMAVG. The latter consists of averaging the hidden vectors of the LSTM rather than using the final hidden vector as in prior work (Wieting et al., 2016b). We hypothesize that the LSTM may put more emphasis on the words at the end of the sentence than those at the beginning. By averaging the hidden states, the impact of all words in the sequence is better taken into account. Averaging also makes the LSTM more like AVG, which we know to perform strongly in this setting.\nThe results on AVG and the LSTM models are shown in Table 1. When training on PPDB, moving from LSTM to LSTMAVG improves performance by 10 points, closing most of the gap with AVG. We also find that LSTMAVG improves by moving from PPDB to SimpWiki, though in both cases it still lags behind AVG.\n4We experimented with adding EOS tags at the end of training and test sentences, SOS tags at the start of training and test sentences, adding both, and adding neither. We treated adding these tags as hyperparameters and tuned over these four settings along with the other hyperparameters in the original experiment. Interestingly, we found that adding these tags, especially EOS, had a large effect on the LSTM when training on SimpWiki, improving performance by 6 points. When training on PPDB, adding EOS tags only improved performance by 1.6 points.\nThe addition of the tags had a smaller effect on LSTMAVG. Adding EOS tags improved performance by 0.3 points on SimpWiki and adding SOS tags on PPDB improved performance by 0.9 points."}, {"heading": "4.2 Experiments with Regularization", "text": "We next experiment with various forms of regularization. Previous work (Wieting et al., 2016b,a) only used L2 regularization. Wieting et al. (2016b) also regularized the word embeddings back to their initial values. Here we use L2 regularization as well as several additional regularization methods we describe below.\nWe try two forms of dropout. The first is just standard dropout (Srivastava et al., 2014) on the word embeddings. The second is \u201cword dropout\u201d, which drops out entire word embeddings with some probability (Iyyer et al., 2015).\nWe also experiment with scrambling the inputs. For a given mini-batch, we go through each sentence pair and, with some probability, we shuffle the words in each sentence in the pair. When scrambling a sentence pair, we always shuffle both sentences in the pair. We do this before selecting negative examples for the mini-batch. The motivation for scrambling is to make it more difficult for the LSTM to memorize the sequences in the training data, forcing it to focus more on the identities of the words and less on word order. Hence it will be expected to behave more like the word averaging model.5\nWe also experiment with combining scrambling and dropout. In this setting, we tune over scrambling with either word dropout or dropout.\nThe settings for these experiments are largely the same as those of the previous section with the exception that we tune \u03bbw over a smaller set of values: {10\u22125, 0}. When using L2 regularization, we tune \u03bbc over {10\n\u22123, 10\u22124, 10\u22125, 10\u22126}. When using dropout, we tune the dropout rate over {0.2, 0.4, 0.6}. When using scrambling, we tune the scrambling rate over {0.25, 0.5, 0.75}. We also include a bidirectional model (\u201cBi\u201d) for both LSTMAVG and the GATED RECURRENT AVERAGING NETWORK. We tune over two ways to combine the forward and backward hidden states; the first simply adds them together and the second uses a single feedforward layer with a tanh activation.\nWe try two approaches for model selection. The first, test, is the same as was done in Section 4.1.2,\n5We also tried some variations on scrambling that did not yield significant improvements: scrambling after obtaining the negative examples, partially scrambling by performing n swaps where n comes from a Poisson distribution with a tunable \u03bb, and scrambling individual sentences with some probability instead of always scrambling both in the pair.\nwhere we use the average Pearson\u2019s r on the 5 2016 STS datasets. The second tunes based on the average Pearson\u2019s r of all 22 datasets in our evaluation. We refer to this as oracle.\nThe results are shown in Table 2. They show that dropping entire word embeddings and scrambling input sequences is very effective in improving the result of the LSTM, while neither type of dropout improves AVG. Moreover, averaging the hidden states of the LSTM is the most effective modification to the LSTM in improving performance. All of these modifications can be combined to significantly improve the LSTM, finally allowing it to overtake AVG.\nIn Table 3, we compare the various GRAN architectures. We find that the GRAN provides a small improvement over the best LSTM configuration, possibly because of its similarity to AVG. It also outperforms the other GRAN models, despite being the simplest.\nIn Table 4, we show results on all individual STS evaluation datasets after using STS 2016 for model selection (unidirectional models only). The\nLSTMAVG and GATED RECURRENT AVERAGING NETWORK are more closely correlated in performance, in terms of Spearman\u2019s \u03c1 and Pearson\u2019r r, than either is to AVG. But they do differ significantly in some datasets, most notably in those comparing machine translation output with its reference. Interestingly, both the LSTMAVG and GATED RECURRENT AVERAGING NETWORK significantly outperform AVG in the datasets focused on comparing glosses like OnWN and FNWN. Upon examination, we found that these datasets, especially 2013 OnWN, contain examples of low similarity with high word overlap. For example, the pair \u3008the act of preserving or protecting something., the act of decreasing or reducing something.\u3009 from 2013 OnWN has a gold similarity score of 0.4. It appears that AVG was fooled by the high amount of word overlap in such pairs, while the other two models were better able to recognize the semantic differences."}, {"heading": "4.3 Supervised Text Similarity", "text": "We also investigate if these techniques can improve LSTM performance on supervised semantic textual similarity tasks. We evaluate on two supervised datasets. For the first, we start with the 20 SemEval STS datasets from 2012-2015 and then\nuse 40% of each dataset for training, 10% for validation, and the remaining 50% for testing. There are 4,481 examples in training, 1,207 in validation, and 6,060 in the test set. The second is the SICK 2014 dataset, using its standard training, validation, and test sets. There are 4,500 sentence pairs in the training set, 500 in the development set, and 4,927 in the test set. The SICK task is an easier learning problem since the training examples are all drawn from the same distribution, and they are mostly shorter and use simpler language. As these are supervised tasks, the sentence pairs in the training set contain manually-annotated semantic similarity scores.\nWe minimize the loss function6 from Tai et al. (2015). Given a score for a sentence pair in the range [1,K], whereK is an integer, with sentence representations hL and hR, and model parameters \u03b8, they first compute:\nh\u00d7 = hL \u2299 hR, h+ = |hL \u2212 hR|,\nhs = \u03c3 ( W (\u00d7)h\u00d7 +W (+)h+ + b (h) ) ,\np\u0302\u03b8 = softmax ( W (p)hs + b (p) ) ,\ny\u0302 = rT p\u0302\u03b8,\nwhere rT = [1 2 . . . K]. They then define a sparse target distribution p that satisfies y = rT p:\npi =\n\n \n \ny \u2212 \u230ay\u230b, i = \u230ay\u230b+ 1\n\u230ay\u230b \u2212 y + 1, i = \u230ay\u230b\n0 otherwise\nfor 1 \u2264 i \u2264 K . Then they use the following loss, the regularized KL-divergence between p and p\u0302\u03b8:\nJ(\u03b8) = 1\nm\nm \u2211\nk=1\nKL ( p(k) \u2225 \u2225\n\u2225 p\u0302 (k) \u03b8\n)\n,\nwhere m is the number of training pairs.\nWe experiment with the LSTM, LSTMAVG, and AVG models with dropout, word dropout, and scrambling tuning over the same hyperparameter as in Section 4.2. We again regularize the word embeddings back to their initial state, tuning \u03bbw over {10\u22125, 0}. We used the validation set for each respective dataset for model selection.\nThe results are shown in Table 5. The GATED RECURRENT AVERAGING NETWORK has the best\n6This objective function has been shown to perform very strongly on text similarity tasks, significantly better than squared or absolute error.\nperformance on both datasets. Dropout helps the word-averaging model in the STS task, unlike in the transfer learning setting. The LSTM benefits slightly from dropout, scrambling, and averaging on their own individually with the exception of word dropout on both datasets and averaging on the SICK dataset. However, when combined, these modifications are able to significantly improve the performance of the LSTM, bringing it much closer in performance to AVG. This experiment indicates that these modifications when training LSTMs are beneficial outside the transfer learning setting, and can potentially be used to improve performance for the broad range of problems that use LSTMs to model sentences.\nIn Table 6 we compare the various GRAN architectures under the same settings as the previous experiment. We find that the GRAN still has the best overall performance.\nWe also experiment with initializing the supervised models using our pretrained sentence model\nparameters, for the AVG model (no regularization), LSTMAVG (dropout, scrambling), and GATED RECURRENT AVERAGING NETWORK (dropout, scrambling) models from Table 2 and Table 3. We both initialize and then regularize back to these initial values, referring to this setting as \u201cuniversal\u201d.7\nThe results are shown in Table 8. Initializing and regularizing to the pretrained models significantly improves the performance for all three models, justifying our claim that these models serve a dual purpose: they can be used a black box semantic similarity function, and they possess rich knowledge that can be used to improve the performance of downstream tasks."}, {"heading": "5 Analysis", "text": ""}, {"heading": "5.1 Error Analysis", "text": "We analyze the predictions of AVG and the recurrent networks, represented by LSTMAVG, on the 20 STS datasets. We choose LSTMAVG as it correlates slightly less strongly with AVG than the GRAN on the results over all SemEval datasets used for evaluation. We scale the models\u2019 cosine similarities to lie within [0, 5], then compare the\n7In these experiments, we tuned \u03bbw over {10, 1, 10\u22121, 10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126, 10\u22127, 10\u22128, 0} and \u03bbc over {10, 1, 10 \u22121, 10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126, 0}.\npredicted similarities of LSTMAVG and AVG to the gold similarities. We analyzed instances in which each model would tend to overestimate or underestimate the gold similarity relative to the other. These are illustrated in Table 7.\nWe find that AVG tends to overestimate the semantic similarity of a sentence pair, relative to LSTMAVG, when the two sentences have a lot of word or synonym overlap, but have either important differences in key semantic roles or where one sentence has significantly more content than the other. These phenomena are shown in examples 1 and 2 in Table 7. Conversely, AVG tends to underestimate similarity when there are one-word-tomultiword paraphrases between the two sentences as shown in examples 3 and 4.\nLSTMAVG tends to overestimate similarity when the two inputs have similar sequences of syntactic categories, but the meanings of the sentences are different (examples 5, 6, and 7). Instances of LSTMAVG underestimating the similarity relative to AVG are relatively rare, and those that we found did not have any systematic patterns."}, {"heading": "5.2 GRAN Gate Analysis", "text": "We also investigate what is learned by the gating function of the GATED RECURRENT AVERAGING NETWORK. We are interested to see whether its estimates of importance correlate with those of traditional syntactic and (shallow) semantic analysis.\nWe use the oracle trained GATED RECURRENT AVERAGING NETWORK from Table 3 and calculate the L1 norm of the gate after embedding 10,000 sentences from English Wikipedia.8 We also automatically tag and parse these sentences using the Stanford dependency parser (Manning et al., 2014). We then compute\n8We selected only sentences of less than or equal to 15 tokens to ensure more accurate parsing.\nthe average gate L1 norms for particular part-ofspeech tags, dependency arc labels, and their conjunction.\nTable 9 shows the highest/lowest average norm tags and dependency labels. The network prefers nouns, especially proper nouns, as well as cardinal numbers, which is sensible as these are among the most discriminative features of a sentence.\nAnalyzing the dependency relations, we find that nouns in the object position tend to have higher weight than nouns in the subject position. This may relate to topic and focus; the object may be more likely to be the \u201cnew\u201d information related by the sentence, which would then make it more likely to be matched by the other sentence in the paraphrase pair.\nWe find that the weights of adjectives depend on their position in the sentence, as shown in Table 10. The highest norms appear when an adjective is an xcomp, acomp, or root; this typically means it is residing in an object-like position in its clause. Adjectives that modify a noun (amod) have medium weight, and those that modify another adjective or verb (advmod) have low weight.\nLastly, we analyze words tagged as VBG, a\nhighly ambiguous tag that can serve many syntactic roles in a sentence. As shown in Table 11, we find that when they are used to modify a noun (amod) or in the object position of a clause (xcomp, pcomp) they have high weight. Medium weight appears when used in verb phrases (root, vmod) and low weight when used as prepositions or auxiliary verbs (prep, auxpass)."}, {"heading": "6 Conclusion", "text": "We showed how to modify and regularize LSTMs to improve their performance for learning paraphrastic sentence embeddings in both transfer and supervised settings. We also introduced a new recurrent network, the GATED RECURRENT AVERAGING NETWORK, that improves upon both AVG and LSTMs for these tasks, and we release our code and trained models.\nFurthermore, we analyzed the different errors produced by AVG and the recurrent methods and found that the recurrent methods were learning composition that wasn\u2019t being captured by AVG. We also investigated the GRAN in order to better understand the compositional phenomena it was learning by analyzing the L1 norm of its gate over various inputs.\nFuture work will explore additional data sources, including from aligning different translations of novels (Barzilay and McKeown, 2001), aligning new articles of the same topic (Dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs. Our new techniques, combined with the promise of new data sources, offer a great deal of potential for improved universal paraphrastic sentence embeddings."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their valuable comments. This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DEAC02-06CH11357. We thank the developers of Theano (Theano Development Team, 2016) and NVIDIA Corporation for donating GPUs used in this research."}], "references": [{"title": "SemEval-2014 task 10: Multilingual semantic textual similarity", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2014", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of SemEval", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "SEM 2013 shared task: Semantic textual similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo."], "venue": "Second Joint", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "SemEval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."], "venue": "Proceedings", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Arora et al\\.,? 2017", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Extracting paraphrases from a parallel corpus", "author": ["Regina Barzilay", "Kathleen R McKeown."], "venue": "Pro-", "citeRegEx": "Barzilay and McKeown.,? 2001", "shortCiteRegEx": "Barzilay and McKeown.", "year": 2001}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Simple english wikipedia: a new text simplification task", "author": ["William Coster", "David Kauchak."], "venue": "In", "citeRegEx": "Coster and Kauchak.,? 2011", "shortCiteRegEx": "Coster and Kauchak.", "year": 2011}, {"title": "Unsupervised construction of large paraphrase corpora: Exploitingmassively parallel news sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett."], "venue": "In", "citeRegEx": "Dolan et al\\.,? 2004", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "The multilingual paraphrase database", "author": ["Juri Ganitkevitch", "Chris Callison-Burch."], "venue": "Proceedings of", "citeRegEx": "Ganitkevitch and Callison.Burch.,? 2014", "shortCiteRegEx": "Ganitkevitch and Callison.Burch.", "year": 2014}, {"title": "PPDB: The Paraphrase Database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["Felix A. Gers", "Nicol N. Schraudolph", "J\u00fcrgen Schmidhuber."], "venue": "The Journal of Machine Learning Research 3.", "citeRegEx": "Gers et al\\.,? 2003", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "KyunghyunCho", "Anna Korhonen."], "venue": "Proceedings of the 2016", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan \u0130rsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "\u0130rsoy and Cardie.,? 2014", "shortCiteRegEx": "\u0130rsoy and Cardie.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "Proceedings of the 53rd Annual Meeting of the", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "Proceedings of International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "TomasMikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning.", "citeRegEx": "Le and TomasMikolov.,? 2014", "shortCiteRegEx": "Le and TomasMikolov.", "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the 2015", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multi-timescale long shortterm memory neural network for modelling sentences and documents", "author": ["Pengfei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science 34(8).", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "author": ["Matteo Pagliardini", "Prakhar Gupta", "Martin Jaggi."], "venue": "arXiv preprint arXiv:1703.02507 .", "citeRegEx": "Pagliardini et al\\.,? 2017", "shortCiteRegEx": "Pagliardini et al\\.", "year": 2017}, {"title": "Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model", "author": ["Nghia The Pham", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Marco Baroni."], "venue": "Proceedings", "citeRegEx": "Pham et al\\.,? 2015", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "In", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research 15(1).", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meet-", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Charagram: Embedding words and sentences via character n-grams", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of", "citeRegEx": "Wieting et al\\.,? 2016a", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Wieting et al\\.,? 2016b", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu", "Dan Roth."], "venue": "Transactions of the ACL (TACL) .", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT)", "author": ["Wei Xu", "Chris Callison-Burch", "William B Dolan."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 36, "context": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion.", "startOffset": 113, "endOffset": 136}, {"referenceID": 26, "context": "Researchers have proposed a broad range of compositional functional architectures (Mitchell and Lapata, 2008; Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated", "startOffset": 82, "endOffset": 157}, {"referenceID": 30, "context": "Researchers have proposed a broad range of compositional functional architectures (Mitchell and Lapata, 2008; Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated", "startOffset": 82, "endOffset": 157}, {"referenceID": 17, "context": "Researchers have proposed a broad range of compositional functional architectures (Mitchell and Lapata, 2008; Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated", "startOffset": 82, "endOffset": 157}, {"referenceID": 3, "context": "Our goal is to learn a general-purpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) (Agirre et al., 2012) and can also serve as a useful initialization for downstream tasks.", "startOffset": 143, "endOffset": 164}, {"referenceID": 36, "context": "In particular, we focus on the setting of Wieting et al. (2016b), in which models are trained on noisy paraphrase pairs and evaluated on both STS and supervised semantic tasks.", "startOffset": 42, "endOffset": 65}, {"referenceID": 13, "context": "found that simple embedding functions\u2014those based on averaging word vectors\u2014outperform more powerful architectures based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 154, "endOffset": 188}, {"referenceID": 10, "context": "We first change data sources: rather than train on noisy phrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), we use noisy sentence pairs obtained automatically by aligning Simple English to standard English Wikipedia (Coster and Kauchak, 2011).", "startOffset": 99, "endOffset": 132}, {"referenceID": 7, "context": ", 2013), we use noisy sentence pairs obtained automatically by aligning Simple English to standard English Wikipedia (Coster and Kauchak, 2011).", "startOffset": 117, "endOffset": 143}, {"referenceID": 17, "context": "A comprehensive survey is beyond the scope of this paper, but we mention popular functional families: neural bag-ofwords models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al.", "startOffset": 128, "endOffset": 155}, {"referenceID": 16, "context": ", 2014), deep averaging networks (DANs) (Iyyer et al., 2015), recursive neural networks using syntactic parses (Socher et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 15, "context": ", 2015), recursive neural networks using syntactic parses (Socher et al., 2011, 2012, 2013; \u0130rsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al.", "startOffset": 58, "endOffset": 115}, {"referenceID": 17, "context": ", 2011, 2012, 2013; \u0130rsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 75, "endOffset": 130}, {"referenceID": 18, "context": ", 2011, 2012, 2013; \u0130rsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 75, "endOffset": 130}, {"referenceID": 14, "context": ", 2011, 2012, 2013; \u0130rsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al.", "startOffset": 75, "endOffset": 130}, {"referenceID": 34, "context": ", 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 22, "context": ", 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 23, "context": ", 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", "startOffset": 68, "endOffset": 123}, {"referenceID": 6, "context": "Simple operations based on vector addition and multiplication typically serve as strong baselines (Mitchell and Lapata, 2008, 2010; Blacoe and Lapata, 2012).", "startOffset": 98, "endOffset": 156}, {"referenceID": 30, "context": "Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for super-", "startOffset": 38, "endOffset": 185}, {"referenceID": 29, "context": "Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for super-", "startOffset": 38, "endOffset": 185}, {"referenceID": 20, "context": "Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for super-", "startOffset": 38, "endOffset": 185}, {"referenceID": 12, "context": "Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for super-", "startOffset": 38, "endOffset": 185}, {"referenceID": 4, "context": "Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for super-", "startOffset": 38, "endOffset": 185}, {"referenceID": 28, "context": "Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for super-", "startOffset": 38, "endOffset": 185}, {"referenceID": 36, "context": "An exception is the work of Wieting et al. (2016b). We closely follow their experimental setup and directly address some outstanding questions in their experimental results.", "startOffset": 28, "endOffset": 51}, {"referenceID": 36, "context": "This model was found by Wieting et al. (2016b) to perform very strongly for semantic similarity tasks.", "startOffset": 24, "endOffset": 47}, {"referenceID": 13, "context": "Our second model uses a long shortterm memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997) to embed s.", "startOffset": 78, "endOffset": 112}, {"referenceID": 11, "context": "We use the LSTM variant from Gers et al. (2003) including its \u201cpeephole\u201d connections.", "startOffset": 29, "endOffset": 48}, {"referenceID": 10, "context": "The training data consists of a set S of phrase or sentence pairs \u3008s1, s2\u3009 from either the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) or the aligned Wikipedia sentences (Coster and Kauchak, 2011) where s1 and s2 are assumed to be paraphrases.", "startOffset": 111, "endOffset": 144}, {"referenceID": 7, "context": ", 2013) or the aligned Wikipedia sentences (Coster and Kauchak, 2011) where s1 and s2 are assumed to be paraphrases.", "startOffset": 43, "endOffset": 69}, {"referenceID": 34, "context": "We follow the training procedure of Wieting et al. (2015) and Wieting et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 34, "context": "We follow the training procedure of Wieting et al. (2015) and Wieting et al. (2016b), described below.", "startOffset": 36, "endOffset": 85}, {"referenceID": 36, "context": "Our experiments are designed to address the empirical question posed by Wieting et al. (2016b): why do LSTMs underperform AVG for transfer learning? In Sections 4.", "startOffset": 72, "endOffset": 95}, {"referenceID": 39, "context": "We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al.", "startOffset": 50, "endOffset": 67}, {"referenceID": 25, "context": ", 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014).", "startOffset": 60, "endOffset": 82}, {"referenceID": 9, "context": "sions of PPDB have been released for many languages without the need for any manual annotation (Ganitkevitch and Callison-Burch, 2014).", "startOffset": 95, "endOffset": 134}, {"referenceID": 7, "context": "Table 1: Test results on SemEval semantic textual similarity datasets (Pearson\u2019s r\u00d7100) when training on different sources of data: phrase pairs from PPDB or simple-to-standard English Wikipedia sentence pairs from Coster and Kauchak (2011).", "startOffset": 215, "endOffset": 241}, {"referenceID": 7, "context": "The second source of data is a set of sentence pairs automatically extracted from Simple English Wikipedia and English Wikipedia articles by Coster and Kauchak (2011). This data was extracted for developing text simplification systems, where each instance pairs a simple and complex sentence representing approximately the same information.", "startOffset": 141, "endOffset": 167}, {"referenceID": 38, "context": "We use PARAGRAM-SL999 embeddings (Wieting et al., 2015) to initialize the word embedding matrix (Ww) for all models.", "startOffset": 33, "endOffset": 55}, {"referenceID": 19, "context": "For optimization we use Adam (Kingma and Ba, 2015) with a learning rate of 0.", "startOffset": 29, "endOffset": 50}, {"referenceID": 1, "context": "We use the 2016 STS tasks (Agirre et al., 2016) for model selection, where we average the Pearson\u2019s r over its 5 datasets.", "startOffset": 26, "endOffset": 47}, {"referenceID": 36, "context": "We first note that, when training on PPDB, we find the same result as Wieting et al. (2016b): AVG outperforms the LSTM bymore than 13 points.", "startOffset": 70, "endOffset": 93}, {"referenceID": 37, "context": "The latter consists of averaging the hidden vectors of the LSTM rather than using the final hidden vector as in prior work (Wieting et al., 2016b).", "startOffset": 123, "endOffset": 146}, {"referenceID": 36, "context": "Previous work (Wieting et al., 2016b,a) only used L2 regularization. Wieting et al. (2016b) also regularized the word embeddings back to their initial values.", "startOffset": 15, "endOffset": 92}, {"referenceID": 33, "context": "The first is just standard dropout (Srivastava et al., 2014) on the word embeddings.", "startOffset": 35, "endOffset": 60}, {"referenceID": 16, "context": "The second is \u201cword dropout\u201d, which drops out entire word embeddings with some probability (Iyyer et al., 2015).", "startOffset": 91, "endOffset": 111}, {"referenceID": 34, "context": "We minimize the loss function from Tai et al. (2015). Given a score for a sentence pair in the range [1,K], whereK is an integer, with sentence representations hL and hR, and model parameters \u03b8, they first compute:", "startOffset": 35, "endOffset": 53}, {"referenceID": 24, "context": "We also automatically tag and parse these sentences using the Stanford dependency parser (Manning et al., 2014).", "startOffset": 89, "endOffset": 111}, {"referenceID": 5, "context": "Future work will explore additional data sources, including from aligning different translations of novels (Barzilay and McKeown, 2001), aligning new articles of the same topic (Dolan et al.", "startOffset": 107, "endOffset": 135}, {"referenceID": 8, "context": "Future work will explore additional data sources, including from aligning different translations of novels (Barzilay and McKeown, 2001), aligning new articles of the same topic (Dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs.", "startOffset": 177, "endOffset": 197}], "year": 2017, "abstractText": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the GATED RECURRENT AVERAGING NETWORK, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations. 1", "creator": "LaTeX with hyperref package"}}}