{"id": "1705.00440", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Data Augmentation for Low-Resource Neural Machine Translation", "abstract": "occasionally the quality of a neural machine translation measurement system depends substantially on the availability of sizable parallel corpora. for low - resource language pairs this is not the case, resulting in poor translation quality. inspired by work applications in computer vision, however we propose a novel symbolic data augmentation approach that targets low - frequency words by generating new sentence path pairs containing rare words in new, synthetically created contexts. experimental results on simulated low - resource settings show that our graphical method improves translation vector quality by up to using 2. 9 bleu points over the baseline human and up to 3. 2 bleu over back - translation.", "histories": [["v1", "Mon, 1 May 2017 08:12:15 GMT  (174kb,D)", "http://arxiv.org/abs/1705.00440v1", "5 pages, 2 figures, Accepted at ACL 2017"]], "COMMENTS": "5 pages, 2 figures, Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marzieh fadaee", "arianna bisazza", "christof monz"], "accepted": true, "id": "1705.00440"}, "pdf": {"name": "1705.00440.pdf", "metadata": {"source": "CRF", "title": "Data Augmentation for Low-Resource Neural Machine Translation", "authors": ["Marzieh Fadaee", "Arianna Bisazza", "Christof Monz"], "emails": ["m.fadaee@uva.nl", "a.bisazza@uva.nl", "c.monz@uva.nl"], "sections": [{"heading": null, "text": "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation."}, {"heading": "1 Introduction", "text": "In computer vision, data augmentation techniques are widely used to increase robustness and improve learning of objects with a limited number of training examples. In image processing the training data is augmented by, for instance, horizontally flipping, random cropping, tilting, and altering the RGB channels of the original images (Krizhevsky et al., 2012; Chatfield et al., 2014). Since the content of the new image is still the same, the label of the original image is preserved (see top of Figure 1). While data augmentation has become a standard technique to train deep networks for image processing, it is not a common practice in training networks for NLP tasks such as Machine Translation.\nNeural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous\nLSTM hidden states and an attention mechanism, generates the target translation.\nTo train a model with reliable parameter estimations, these networks require numerous instances of sentence translation pairs with words occurring in diverse contexts, which is typically not available in low-resource language pairs. As a result NMT falls short of reaching state-of-the-art performances for these language pairs (Zoph et al., 2016). The solution is to either manually annotate more data or perform unsupervised data augmentation. Since manual annotation of data is timeconsuming, data augmentation for low-resource language pairs is a more viable approach. Recently Sennrich et al. (2016a) proposed a method to back-translate sentences from monolingual data and augment the bitext with the resulting pseudo parallel corpora.\nIn this paper, we propose a simple yet effective approach, translation data augmentation (TDA), that augments the training data by altering existing sentences in the parallel corpus, similar in spirit to the data augmentation approaches in computer vision (see Figure 1). In order for the augmentation process in this scenario to be label-preserving, any change to a sentence in one language must pre-\nar X\niv :1\n70 5.\n00 44\n0v 1\n[ cs\n.C L\n] 1\nM ay\n2 01\n7\nserve the meaning of the sentence, requiring sentential paraphrasing systems which are not available for many language pairs. Instead, we propose a weaker notion of label preservation that allows to alter both source and target sentences at the same time as long as they remain translations of each other.\nWhile our approach allows us to augment data in numerous ways, we focus on augmenting instances involving low-frequency words, because the parameter estimation of rare words is challenging, and further exacerbated in a lowresource setting. We simulate a low-resource setting as done in the literature (Marton et al., 2009; Duong et al., 2015) and obtain substantial improvements for translating English\u00d1German and German\u00d1English."}, {"heading": "2 Translation Data Augmentation", "text": "Given a source and target sentence pair (S,T), we want to alter it in a way that preserves the semantic equivalence between S and T while diversifying as much as possible the training examples. A number of ways to do this can be envisaged, as for example paraphrasing (parts of) S or T. Paraphrasing, however, is by itself a difficult task and is not guaranteed to bring useful new information into the training data. We choose instead to focus on a subset of the vocabulary that we know to be poorly modeled by our baseline NMT system, namely words that occur rarely in the parallel corpus. Thus, the goal of our data augmentation technique is to provide novel contexts for rare words. To achieve this we search for contexts where a common word can be replaced by a rare word and consequently replace its corresponding word in the other language by that rare word\u2019s translation:\noriginal pair augmented pair S : s1, ..., si, ..., sn S 1 : s1, ..., s 1 i, ..., sn T : t1, ..., tj , ..., tm T 1 : t1, ..., t 1 j , ..., tm\nwhere tj is a translation of si and word-aligned to si. Plausible substitutions are those that result in a fluent and grammatical sentence but do not necessarily maintain its semantic content. As an example, the rare word motorbike can be substituted in different contexts:\nSentence [original / substituted] Plausible My sister drives a [car / motorbike] yes My uncle sold his [house / motorbike] yes Alice waters the [plant / motorbike] no (semantics) John bought two [shirts / motorbike] no (syntax)\nImplausible substitutions need to be ruled out during data augmentation. To this end, rather than relying on linguistic resources which are not available for many languages, we rely on LSTM language models (LM) (Hochreiter and Schmidhuber, 1997; Jozefowicz et al., 2015) trained on large amounts of monolingual data in both forward and backward directions.\nOur data augmentation method involves the following steps:\nTargeted words selection: Following common practice, our NMT system limits its vocabulary V to the v most common words observed in the training corpus. We select the words in V that have fewer than R occurrences and use this as our targeted rare word list VR.\nRare word substitution: If the LM suggests a rare substitution in a particular context, we replace that word and add the new sentence to the training data. Formally, given a sentence pair pS, T q and a position i in S we compute the probability distribution over V by the forward and backward LMs and select rare word substitutions C as follows:\n\u00dd\u00d1C \u201c ts1i P VR : topKPForwardLMS ps 1 i | si\u00b411 qu \u00d0\u00ddC \u201c ts1i P VR : topKPBackwardLMS ps 1 i | si`1n qu C \u201c ts1i | s1i P \u00dd\u00d1C ^ s1i P \u00d0\u00ddC u\nwhere topK returns the K words with highest conditional probability according to the context. The selected substitutions s1i, are used to replace the original word and generate a new sentence.\nTranslation selection: Using automatic word alignments1 trained over the bitext, we replace the translation of word si in T by the translation of its substitution s1i. Following a common practice in statistical MT, the optimal translation t1j is chosen by multiplying direct and inverse lexical translation probabilities with the LM probability of the translation in context:\nt1j \u201c argmax tPtransps1iq P ps1i | tqP pt | s1iqPLMT pt | t j\u00b41 1 q\nIf no translation candidate is found because the word is unaligned or because the LM probability\n1We use fast-align (Dyer et al., 2013) to extract word alignments and a bilingual lexicon with lexical translation probabilities from the low-resource bitext.\nis less than a certain threshold, the augmented sentence is discarded. This reduces the risk of generating sentence pairs that are semantically or syntactically incorrect.\nSampling: We loop over the original parallel corpus multiple times, sampling substitution positions, i, in each sentence and making sure that each rare word gets augmented at most N times so that a large number of rare words can be affected. We stop when no new sentences are generated in one pass of the training data.\nTable 1 provides some examples resulting from our augmentation procedure. While using a large LM to substitute words with rare words mostly results in grammatical sentences, this does not mean that the meaning of the original sentence is preserved. Note that meaning preservation is not an objective of our approach.\nTwo translation data augmentation (TDA) setups are considered: only one word per sentence can be replaced (TDAr\u201c1), or multiple words per sentence can be replaced, with the condition that any two replaced words are at least five positions apart (TDAr\u011b1). The latter incurs a higher risk of introducing noisy sentences but has the potential to positively affect more rare words within the same amount of augmented data. We evaluate both setups in the following section."}, {"heading": "3 Evaluation", "text": "In this section we evaluate the utility of our approach in a simulated low-resource NMT scenario."}, {"heading": "3.1 Data and experimental setup", "text": "To simulate a low-resource setting we randomly sample 10% of the English\u00d8German WMT15 training data and report results on newstest 2014, 2015, and 2016 (Bojar et al., 2016). For reference we also provide the result of our baseline system on the full data.\nAs NMT system we use a 4-layer attentionbased encoder-decoder model as described in (Luong et al., 2015) trained with hidden dimension 1000, batch size 80 for 20 epochs. In all experiments the NMT vocabulary is limited to the most common 30K words in both languages. Note that data augmentation does not introduce new words to the vocabulary. In all experiments we preprocess source and target language data with Bytepair encoding (BPE) (Sennrich et al., 2016b) using 30K merge operations. In the augmentation experiments BPE is performed after data augmentation.\nFor the LMs needed for data augmentation, we train 2-layer LSTM networks in forward and backward directions on the monolingual data provided for the same task (3.5B and 0.9B tokens in English and German respectively) with embedding size 64 and hidden size 128. We set the rare word threshold R to 100, top K words to 1000 and maximum number N of augmentations per rare word to 500. In all experiments we use the English LM for the rare word substitutions, and the German LM to choose the optimal word translation in context. Since our approach is not label preserving we only perform augmentation during training and do not alter source sentences during testing.\nWe also compare our approach to Sennrich et al. (2016a) by back-translating monolingual data and adding it to the parallel training data. Specifically, we back-translate sentences from the target side of WMT\u201915 that are not included in our low-resource baseline with two settings: keeping a one-to-one ratio of back-translated versus original data (1 : 1) following the authors\u2019 suggestion, or using three times more back-translated data (3 : 1).\nWe measure translation quality by singlereference case-insensitive BLEU (Papineni et al., 2002) computed with the multi-bleu.perl script from Moses."}, {"heading": "3.2 Results", "text": "All translation results are displayed in Table 2. As expected, the low-resource baseline performs much worse than the full data system, re-iterating\nthe importance of sizable training data for NMT. Next we observe that both back-translation and our proposed TDA method significantly improve translation quality. However TDA obtains the best results overall and significantly outperforms backtranslation in all test sets. This is an important finding considering that our method involves only minor modifications to the original training sentences and does not involve any costly translation process. Improvements are consistent across both translation directions, regardless of whether rare word substitutions are first applied to the source or to the target side.\nWe also observe that altering multiple words in a sentence performs slightly better than altering only one. This indicates that addressing more rare words is preferable even though the augmented sentences are likely to be noisier.\nTo verify that the gains are actually due to the rare word substitutions and not just to the repetition of part of the training data, we perform a final experiment where each sentence pair selected for augmentation is added to the training data unchanged (Oversampling in Table 2). Surprisingly, we find that this simple form of sampled data replication outperforms both baseline and backtranslation systems,2 while TDAr\u011b1 remains the best performing system overall.\nWe also observe that the system trained on augmented data tends to generate longer translations. Averaging on all test sets, the length of translations generated by the baseline is 0.88 of the average reference length, while for TDAr\u201c1 and TDAr\u011b1 it is 0.95 and 0.94, respectively. We attribute this effect to the ability of the TDA-trained system to generate translations for rare words that were left\n2Note that this effect cannot be achieved by simply continuing the baseline training for up to 50 epochs.\nuntranslated by the baseline system."}, {"heading": "4 Analysis of the Results", "text": "A desired effect of our method is to increase the number of correct rare words generated by the NMT system at test time.\nTo examine the impact of augmenting the training data by creating contexts for rare words on the target side, Table 3 provides an example for German\u00d1English translation. We see that the baseline model is not able to generate the rare word centimetres as a correct translation of the German word zentimeter . However, this word is not rare in the training data of the TDAr\u011b1 model after augmentation and is generated during translation. Table 3 also provides several instances of augmented training sentences targeting the word centimetres. Note that even though some augmented sentences are nonsensical (e.g. the speed limit is five centimetres per hour), the NMT system still benefits from the new context for the rare word and is able to generate it during testing.\nFigure 2 demonstrates that this is indeed the case for many words: the number of rare words occurring in the reference translation (VR X Vref ) is three times larger in the TDA system output than in the baseline output. One can also see that this increase is a direct effect of TDA as most of the rare words are not \u2018rare\u2019 anymore in the augmented data, i.e., they were augmented sufficiently many times to occur more than 100 times (see hatched pattern in Figure 2). Note that during the experiments we did not use any information from the evaluation sets.\nTo gauge the impact of augmenting the contexts for rare words on the source side, we examine normalized attention scores of these words before and after augmentation. When translating\nEnglish\u00d1German with our TDA model, the attention scores for rare words on the source side are on average 8.8% higher than when translating with the baseline model. This suggests that having more accurate representations of rare words increases the model\u2019s confidence to attend to these words when encountered during test time.\nFinally Table 4 provides examples of cases where augmentation results in incorrect sentences. In the first example, the sentence is ungrammati-\ncal after substitution (of / yearly), which can be the result of choosing substitutions with low probabilities from the English LM topK suggestions.\nErrors can also occur during translation selection, as in the second example where betraut is an acceptable translation of entrusted but would require a rephrasing of the German sentence to be grammatically correct. Problems of this kind can be attributed to the German LM, but also to the lack of a more suitable translation in the lexicon extracted from the bitext. Interestingly, this noise seems to affect NMT only to a limited extent."}, {"heading": "5 Conclusion", "text": "We have proposed a simple but effective approach to augment the training data of Neural Machine Translation for low-resource language pairs. By leveraging language models trained on large amounts of monolingual data, we generate new sentence pairs containing rare words in new, synthetically created contexts. We show that this approach leads to generating more rare words during translation and, consequently, to higher translation quality. In particular we report substantial improvements in simulated lowresource English\u00d1German and German\u00d1English settings, outperforming another recently proposed data augmentation technique."}, {"heading": "Acknowledgments", "text": "This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 639.021.646, and a Google Faculty Research Award. We also thank NVIDIA for their hardware support, Ke Tran for providing the neural machine translation baseline system, and the anonymous reviewers for their helpful comments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["Ken Chatfield", "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman."], "venue": "Proceedings of the British Machine Vision Conference. BMVA Press.", "citeRegEx": "Chatfield et al\\.,? 2014", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "B van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A neural network model for lowresource universal dependency parsing", "author": ["Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Duong et al\\.,? 2015", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Process-", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Improved statistical machine translation using monolingually-derived paraphrases", "author": ["Yuval Marton", "Chris Callison-Burch", "Philip Resnik."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Marton et al\\.,? 2009", "shortCiteRegEx": "Marton et al\\.", "year": 2009}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "In image processing the training data is augmented by, for instance, horizontally flipping, random cropping, tilting, and altering the RGB channels of the original images (Krizhevsky et al., 2012; Chatfield et al., 2014).", "startOffset": 171, "endOffset": 220}, {"referenceID": 2, "context": "In image processing the training data is augmented by, for instance, horizontally flipping, random cropping, tilting, and altering the RGB channels of the original images (Krizhevsky et al., 2012; Chatfield et al., 2014).", "startOffset": 171, "endOffset": 220}, {"referenceID": 0, "context": "Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous A boy is holding a bat.", "startOffset": 33, "endOffset": 98}, {"referenceID": 14, "context": "Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous A boy is holding a bat.", "startOffset": 33, "endOffset": 98}, {"referenceID": 3, "context": "Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous A boy is holding a bat.", "startOffset": 33, "endOffset": 98}, {"referenceID": 15, "context": "As a result NMT falls short of reaching state-of-the-art performances for these language pairs (Zoph et al., 2016).", "startOffset": 95, "endOffset": 114}, {"referenceID": 12, "context": "Recently Sennrich et al. (2016a) proposed a method to back-translate sentences from monolingual data and augment the bitext with the resulting pseudo parallel corpora.", "startOffset": 9, "endOffset": 33}, {"referenceID": 10, "context": "We simulate a low-resource setting as done in the literature (Marton et al., 2009; Duong et al., 2015) and obtain substantial improvements for translating English\u00d1German and German\u00d1English.", "startOffset": 61, "endOffset": 102}, {"referenceID": 4, "context": "We simulate a low-resource setting as done in the literature (Marton et al., 2009; Duong et al., 2015) and obtain substantial improvements for translating English\u00d1German and German\u00d1English.", "startOffset": 61, "endOffset": 102}, {"referenceID": 6, "context": "To this end, rather than relying on linguistic resources which are not available for many languages, we rely on LSTM language models (LM) (Hochreiter and Schmidhuber, 1997; Jozefowicz et al., 2015) trained on large amounts of monolingual data in both forward and backward directions.", "startOffset": 138, "endOffset": 197}, {"referenceID": 7, "context": "To this end, rather than relying on linguistic resources which are not available for many languages, we rely on LSTM language models (LM) (Hochreiter and Schmidhuber, 1997; Jozefowicz et al., 2015) trained on large amounts of monolingual data in both forward and backward directions.", "startOffset": 138, "endOffset": 197}, {"referenceID": 5, "context": "We use fast-align (Dyer et al., 2013) to extract word alignments and a bilingual lexicon with lexical translation probabilities from the low-resource bitext.", "startOffset": 18, "endOffset": 37}, {"referenceID": 9, "context": "As NMT system we use a 4-layer attentionbased encoder-decoder model as described in (Luong et al., 2015) trained with hidden dimension 1000, batch size 80 for 20 epochs.", "startOffset": 84, "endOffset": 104}, {"referenceID": 13, "context": "In all experiments we preprocess source and target language data with Bytepair encoding (BPE) (Sennrich et al., 2016b) using 30K merge operations.", "startOffset": 94, "endOffset": 118}, {"referenceID": 12, "context": "We also compare our approach to Sennrich et al. (2016a) by back-translating monolingual data and adding it to the parallel training data.", "startOffset": 32, "endOffset": 56}, {"referenceID": 11, "context": "We measure translation quality by singlereference case-insensitive BLEU (Papineni et al., 2002) computed with the multi-bleu.", "startOffset": 72, "endOffset": 95}, {"referenceID": 12, "context": "Back-translation refers to the work of Sennrich et al. (2016a). Statistically significant improvements are marked IJ at the p \u0103 .", "startOffset": 39, "endOffset": 63}], "year": 2017, "abstractText": "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.", "creator": "LaTeX with hyperref package"}}}