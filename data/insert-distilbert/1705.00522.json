{"id": "1705.00522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Regularized Residual Quantization: a multi-layer sparse dictionary learning approach", "abstract": "the residual quantization ( rq ) framework is revisited where the quantization distortion is being successively reduced slowly in discrete multi - layers. generally inspired by the rigorous reverse - water - filling paradigm in rate - distortion theory, an efficient regularization on the variances of recovering the codewords is introduced which additionally allows to extend the rq for very large numbers of layers respectively and also for high dimensional raw data, without getting over - trained. the proposed regularized spectral residual quantization ( rrq ) results in multi - layer robust dictionaries which are additionally sparse, thanks to the soft - thresholding nature of the regularization when applied to variance - decaying data which can arise from de - correlating transformations applied to correlated data. furthermore, initially we also propose requesting a general - purpose sampling pre - processing for natural images which makes them suitable for such quantization. the rrq regression framework is first tested on synthetic variance - decaying data to show its efficiency factor in quantization of high - dimensional data. next, we use the rrq in augmented super - resolution of preparing a database of facial images where it frequently is shown indicates that low - resolution repetitive facial images from the test set quantized with codebooks trained on such high - achieving resolution face images from the training set show relevant random high - frequency content when reconstructed with only those codebooks.", "histories": [["v1", "Mon, 1 May 2017 13:59:04 GMT  (656kb,D)", "http://arxiv.org/abs/1705.00522v1", "To be presented at SPARS 2017, Lisbon, Portugal"]], "COMMENTS": "To be presented at SPARS 2017, Lisbon, Portugal", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["sohrab ferdowsi", "slava voloshynovskiy", "dimche kostadinov"], "accepted": false, "id": "1705.00522"}, "pdf": {"name": "1705.00522.pdf", "metadata": {"source": "CRF", "title": "Regularized Residual Quantization: a multi-layer sparse dictionary learning approach", "authors": ["Sohrab Ferdowsi", "Slava Voloshynovskiy", "Dimche Kostadinov"], "emails": ["Dimche.Kostadinov}@unige.ch"], "sections": [{"heading": null, "text": "Regularized Residual Quantization: a multi-layer sparse dictionary learning approach\nSohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov Department of Computer Science, University of Geneva, Switzerland\n{Sohrab.Ferdowsi, svolos, Dimche.Kostadinov}@unige.ch\nI. INTRODUCTION\nQuantizing the residual errors from a previous level of quantization has been considered in signal processing for different applications, e.g., image coding. This problem was extensively studied in the 80\u2019s and 90\u2019s (e.g., see [1] and [4]). However, due to strong over-fitting, its efficiency was limited for more modern applications with larger scales. In practice, it was not feasible to train more than a couple of layers. Particularly at high dimensions, the codebooks learned on a training set were not able to quantize a statistically similar test set.\nInspired by an insight from rate-distortion theory, we introduce an effective regularization for the framework of Residual Quantization (RQ), making it capable to learn multiple layers of codebooks with many stages. Moreover, the introduced framework effectively deals with high dimensions making it feasible to go beyond patch level processing and deals with entire images. The proposed regularization makes use of the problem of optimal rate allocation for asymptotic case of Gaussian independent sources, which is reviewed next."}, {"heading": "II. BACKGROUND: QUANTIZATION OF INDEPENDENT GAUSSIAN SOURCES", "text": "Given n independent Gaussian sources Xj\u2019s each with variance \u03c32j distributed as Xj\u223cN (0, \u03c32j ), the optimal rate allocation from the rate-distortion theory is derived for this setup as (Ch. 10 of [2]):\nDj = { \u03b3, if \u03c32j > \u03b3, \u03c32j , if \u03c3 2 j < \u03b3,\n(1)\nwhere \u03b3 should be chosen to guarantee that \u2211n j=1Dj = D. Hence, the optimal codeword variance \u03c32Cj is soft-thresholding of \u03c3 2 j :\n\u03c32Cj = ( \u03c32j \u2212 \u03b3 )+ = { \u03c32j \u2212 \u03b3, if \u03c32j > \u03b3, 0, if \u03c32j < \u03b3.\n(2)\nThis means that sources with variances less than \u03b3 should not be assigned any rate at all. We next incorporate this phenomenon for codebook learning and enforce it as a regularization for the codebook variances. This, in fact, will be an effective way to reduce the gap between the train and test distortion errors. Moreover, the inactivity of the dimensions with variances less than \u03b3 will also lead to a natural sparsity of codewords, which lowers the computational complexity."}, {"heading": "III. THE PROPOSED APPROACH: RRQ", "text": "Instead of the standard K-means used in RQ, we first propose its regularized version and then use it as the building-block for RRQ.\nA. VR-Kmeans algorithm\nAfter de-correlating the data points, e.g., using the pre-processing proposed in Fig. 2, and gathering them in in columns of X with \u03c32j at each dimension, define S , diag([\u03c32C1 , \u00b7 \u00b7 \u00b7 , \u03c3 2 Cn ]) from Eq. 2. For codebook C, to regularize only the diagonal elements of CCT , define\nPj with all elements as zeros except at P(j,j)=1. We formulate the variance-regularized K-means algorithm with parameter \u03bb as:\nminimize C,A\n1 2 ||X\u2212 CA||2F + 1 2 \u03bb|| n\u2211 j=1 PjCC TPj \u2212 S||2F ,\ns.t. ||\u03b1i||0 = ||\u03b1i||1 = 1. (3)\nLike the standard K-means algorithm, we iterate between fixing C and updating A, and then fixing A and updating C.\nFix C, update A: Exactly like the standard K-means. Fix A, update C: Eq. 3 can be re-written as:\nminimize Tr C\n[ \u2212XATCT + 1\n2 CAATCT\n+ 1\n2 \u03bb( n\u2211 j=1 PjCC TPj)(CC T \u2212 2S) ] .\n(4)\n\u2211n j=1 PjCC\nTPj , and due to its structure AAT , diag([a1, \u00b7 \u00b7 \u00b7 , ak]) are diagonal. Therefore Eq. 4 will reduce to minimizing independent sub-problems at each (active) dimension:\nminimize c(j)\n[ \u2212 z(j)T c(j) + 1\n2\n( a1c1(j) 2 + \u00b7 \u00b7 \u00b7+ akck(j)2 )\n+ 1\n2 \u03bb||c(j)||2(||c(j)||2 \u2212 2\u03c32Cj )\n] ,\n(5)\nwhere Z , XAT = [z(1), \u00b7 \u00b7 \u00b7 , z(n)]T . These independent problems can be solved easily using the Newton\u2019s algorithm, for which the derivation of the required gradient and Hessian is straightforward."}, {"heading": "B. Regularized Residual Quantization (RRQ) algorithm", "text": "For a fixed number of centroids K(l) at layer l and D(l\u22121)j the distortion of the previous stage of quantization for each dimension, the\nRRQ first specifies \u03b3\u2217 = argmin \u03b3\n( | log2K(l)\u2212 \u2211 j\u2208A\u03b3 1 2 log+2 D (l\u22121) j \u03b3 | )\nfollowed by calculation of an active set of dimensions A(l)\u03b3\u2217 = {j : 1 6 j 6 n|\u03c32j > \u03b3\u2217}. The algorithm then continues with quantizing the residual of stage l \u2212 1 with the VR-Kmeans algorithm described above, until a desired stage L which can be chosen based on distortion constraints or an overall rate budget allowed."}, {"heading": "IV. EXPERIMENTS", "text": "Fig. 1 and Table I compare the performance of VR-Kmeans with K-means in quantization of high-dimensional variance-decaying independent data. In fact, in many practical cases, the correlated data behaves similarly after an energy-compacting and de-correlating transform. As is seen in this figure, the VR-Kmeans regularizes the variance resulting in a reduced train-test distortion gap.\nFig. 3 demonstrates the performance of the RRQ in superresolution of similar images. It is clear from this figure that the highfrequency content lost in down-sampling can be reconstructed from a multi-layer codebook learned from face images with full resolution.\nar X\niv :1\n70 5.\n00 52\n2v 1\n[ cs\n.L G\n] 1\nM ay\n2 01\n7"}], "references": [{"title": "Advances in residual vector quantization: a review", "author": ["C.F. Barnes", "S.A. Rizvi", "N.M. Nasrabadi"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Elements of Information Theory 2nd Edition", "author": ["T. Cover", "J. Thomas"], "venue": "Wiley-Interscience, 2 edition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Image coding using vector quantization: a review", "author": ["N.M. Nasrabadi", "R.A. King"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}], "referenceMentions": [{"referenceID": 0, "context": ", see [1] and [4]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": ", see [1] and [4]).", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "10 of [2]):", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "3: Super-resolution using RRQ on the CroppedYale [3] set with L = 100 layers with K = 256 centroids each: After preprocessing as proposed in Fig.", "startOffset": 49, "endOffset": 52}], "year": 2017, "abstractText": "Quantizing the residual errors from a previous level of quantization has been considered in signal processing for different applications, e.g., image coding. This problem was extensively studied in the 80\u2019s and 90\u2019s (e.g., see [1] and [4]). However, due to strong over-fitting, its efficiency was limited for more modern applications with larger scales. In practice, it was not feasible to train more than a couple of layers. Particularly at high dimensions, the codebooks learned on a training set were not able to quantize a statistically similar test set. Inspired by an insight from rate-distortion theory, we introduce an effective regularization for the framework of Residual Quantization (RQ), making it capable to learn multiple layers of codebooks with many stages. Moreover, the introduced framework effectively deals with high dimensions making it feasible to go beyond patch level processing and deals with entire images. The proposed regularization makes use of the problem of optimal rate allocation for asymptotic case of Gaussian independent sources, which is reviewed next.", "creator": "LaTeX with hyperref package"}}}