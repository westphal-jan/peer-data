{"id": "1705.00581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Query-adaptive Video Summarization via Quality-aware Relevance Estimation", "abstract": "although the problem of automatic video summarization has recently received a lot of attention, the problem solution of creating a video summary that also highlights elements relevant to a consistent search query strategy has been less studied. we address this problem by posing query - relevant summarization as facilitating a video data frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, appropriately representative of the entire video, and relevant to a text query. we quantify relevance by measuring the distance lines between frames and queries in a common textual - visual semantic embedding space induced by a neural network. in addition, we extend the model to capture query - independent properties, such as frame quality. we compare our method against previous state of the art on textual - visual grammatical embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. furthermore, we introduce a new dataset, well annotated with diversity and query - specific relevance labels. on this dataset, we train and test our complete model for video summarization and could show that it outperforms standard baselines such as maximal marginal relevance.", "histories": [["v1", "Mon, 1 May 2017 16:28:18 GMT  (8913kb,D)", "http://arxiv.org/abs/1705.00581v1", "Submitted to ACM Multimedia 2017"], ["v2", "Thu, 28 Sep 2017 13:18:56 GMT  (3909kb,D)", "http://arxiv.org/abs/1705.00581v2", "ACM Multimedia 2017"]], "COMMENTS": "Submitted to ACM Multimedia 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["arun balajee vasudevan", "michael gygli", "anna volokitin", "luc van gool"], "accepted": false, "id": "1705.00581"}, "pdf": {"name": "1705.00581.pdf", "metadata": {"source": "META", "title": "Query-adaptive Video Summarization via Quality-aware Relevance Estimation-1.5ex", "authors": ["Arun Balajee Vasudevan", "Michael Gygli", "Anna Volokitin", "Luc Van Gool"], "emails": ["arunv@vision.ee.ethz.ch", "gygli@vision.ee.ethz.ch", "anna.volokitin@vision.ee.ethz.ch", "vangool@vision.ee.ethz.ch"], "sections": [{"heading": "1 INTRODUCTION", "text": "Video recording devices have become omnipresent. Most of the videos taken with smartphones, surveillance cameras and wearable cameras are recorded with a capture first, filter later mentality. However, most raw videos never end up getting curated and remain too long, shaky, redundant and boring to watch. This raises new challenges in searching both within and across videos.\nThe problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65]. In automatic tagging, the goal is to predict meta-data in form of tags, which makes videos searchable via text queries. Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].\nThis paper combines the goals of summarising videos and makes them searchable with text. Specifically, we propose a novel method that generates video summaries adapted to a text query (See Fig. 1). Our approach improves previous works in the area of textual-visual embeddings [25, 31] and proposes an extension of an existing video summarization method using submodular mixtures [14] for creating summaries that are query-adaptive.\nOur method for creating query-relevant summaries consists of two parts. We first develop a relevance model which allows us to rank frames of a video according to their relevance given a text query. Relevance is computed as the sum of the cosine similarity\n*Authors contributed equally\nbetween embeddings of frames and text queries in a learned visualsemantic embedding space and a query-independent term. While the embedding captures semantic similarity between video frames and text queries, the query-independent term predicts relevance based on the quality, composition and the interestingness of the content itself. We train this model on a large dataset of image search data [18] and our newly introduced Relevance and Diversity dataset (Section 5). The second part of the summarization system is a framework for optimising the selected set of frames not only for relevance, but also for representativeness and diversity using a submodular mixture of objectives. Figure 2 shows an overview of our complete pipeline. We make the following contributions:\n\u2022 Several improvements on learning a textual-visual embedding for thumbnail selection compared to previous work by Liu et al. [31]. These include better alignment of the learning objective to the task at test time and modelling the text queries by using LSTMs, leading to significant performance gains.\n\u2022 A way to model semantic similarity and quality aspects of frames jointly, leading to better performance compared to using the similarity to text queries only.\n\u2022 A novel method for diverse, query-adaptive video summaries based on submodular mixtures and ours frame-based relevance model.\nar X\niv :1\n70 5.\n00 58\n1v 1\n[ cs\n.C V\n] 1\nM ay\n2 01\n7\n\u2022 A new video thumbnail dataset providing query relevance and diversity labels*. As the judgements are subjective, we collect multiple annotations per video and analyse the consistency of the obtained labelling."}, {"heading": "2 RELATED WORK", "text": "The goal of video summarization is to select a subset of frames that gives a user an idea of the video\u2019s content at a glance [56]. To find informative frames for this task, two dominant approaches exist: (i) modelling generic frame interestingness [16, 28] or (ii) using additional information such as the video title or a text query to find relevant frames [30, 31, 52]. In this work we combine the two into one model and make several contributions for query-adaptive relevance prediction. Such models are related to automatic tagging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] . In the following we discuss approaches for video summarization, generic interestingness prediction models and previous works for obtaining embeddings.\nVideo summarization. Video summarization methods can be broadly classified into abstractive and extractive approaches. Abstractive or compositional approaches transform the initial video into a more compact and appealing representation, e.g. hyperlapses [26], montages [54] or video synopses [46]. The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video. Our method is extractive. Extractive methods need to optimise at least two properties of the summary: the quality of the selected frames and their diversity [13, 14, 48]. Sometimes, additional objectives such as temporal uniformity [14] and relevance [48] are also optimised. The simplest approach to obtain a representative and diverse summary is to cluster videos into events and select the best frame per event [7]. More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29]. Most related to our paper is the work of Sharghi et al. [48], who present an approach for query-adaptive video summarization using DPPs. Their method is however limited to a small, fixed set of concepts such as \u201ccar\u201d or \u201cflower\u201d. The authors leave handling of more complex, unconstrained queries, as in our approach, for future work. In this work, we formulate video summarization as a maximisation problem over a set of submodular functions, following [14].\nFrame quality/interestingness. Most methods that predict frame interestingness are based on supervised learning. The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61]. To simplify the task, some approaches assume the domain of the video given and train a model for each domain [45, 53, 61].\nAn alternative approach based on unsupervised learning, proposed by Xiong et al. [60], detects \u201csnap points\u201d by using a web image prior. Their model considers frames suitable as keyframes if the composition of the frames matches the composition of the web images, regardless of the frame content. Our approach is partially inspired by this work in that it predicts relevance even in the absence of a query, but relies on supervised learning.\n*We will make this data available upon publication.\nUnconstrained Textual-visual models. Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35]. These typically project both modalities into a joint embedding space [12], where semantic similarity can be compared using a measure like cosine similarity. Word2vec [38] and GloVe [44] are popular choices to obtain the embeddings of text. Deep image features are then mapped to the same space via a learned projection. Once both modalities are in the same space, they may be easily compared [12]. Liu et al. [31] applied this idea to video thumbnail selection. Our relevance model is based on Liu et al. [31], but we provide several important improvements. (i) Rather than keeping the word representation fixed, we jointly optimise the word and image projection. (ii) Instead of embedding each word separately, we train an LSTM model that combines a complete query into one single embedding vector, thus it even learns multi-word combinations such as visit to lake and Star Wars movie. (iii) In contrast to Liu et al. [31], we directly optimise the target objective. Our experiments show that these changes lead to significantly better performance in predicting relevant thumbnails."}, {"heading": "3 METHOD FOR RELEVANCE PREDICTION", "text": "The goal of this work is to introduce a method to automatically select a set of video thumbnails that are both relevant with respect to a query, but also diverse enough to represent the video. To later optimise relevance and diversity jointly, we first need a way to evaluate the relevance of frames.\nOur relevance model learns a projection of video framesv and text queries t into the same embedding space. We denote the projection of t and v as t and v, respectively. Once trained, the relevance of a frame v given a query t can be estimated via some similarity measure. As [12], we use the cosine similarity\ns(t, v) = t \u00b7 v\u2016t\u2016\u2016v\u2016 . (1)\nWhile this lets us assess the semantic relevance of a frame w.r.t. a query, it is also possible to make a prediction on the suitability as thumbnails a priori, based on the frame quality, composition, etc. [60]. Thus, we propose to extend above notion of relevance and model the quality aspects of thumbnails explicitly by computing the final relevance as the sum of the embedding similarity and the query-independent frame quality term, i.e.\nr (t ,v) = s(t, v) + qv , (2)\nwhere qv is a query-independent score determining the suitability of v as a thumbnail, based on the quality of a frame.\nIn the following, we investigate how to formulate the task of obtaining the embeddings t and v, as well as qv ."}, {"heading": "3.1 Training objective", "text": "Intuitively, our model should be able to answer \u201cWhat is the best thumbnail for this query?\u201d. Thus, the problem of picking the best thumbnail for a video is naturally formulated as a ranking problem. We desire that the embedding vectors of a query and frame that are a\ngood match are more similar than ones of the same query and a nonrelevant frame\u2020. Thus, our model should learn to satisfy the rank constraint that given a query t , the relevance score of the relevant frame v+ is higher than the relevance score of the irrelevant frame v\u2212:\nr (t ,v+) > r (t ,v\u2212). (3)\nAlternatively, we can train the model by requiring that both the similarity score and the quality score of the relevant frame are higher than for the irrelevant frame explicitly, rather than imposing a constraint only on their sum, as above. In this case we would be imposing the two following constraints:\ns(t, v+) > s(t, v\u2212) qv+ > qv\u2212 .\n(4)\nExperimentally, we find that training with these explicit constraints leads to slightly improved performance (See Tab. 1).\nIn order to impose these constraints and train the model, we define the loss as\nloss(t ,v+,v\u2212) = lp ( max ( 0,\u03b3 \u2212 s(t, v+) + s(t, v\u2212) ) ) + lp (max (0,\u03b3 \u2212 qv+ + qv\u2212 )) ,\n(5)\nwhere lp is a cost function and \u03b3 is a margin parameter. We follow [16] and use a Huber loss for lp , i.e. the robust version of an l2 loss. Next, we describe how to parametrize the t, v and qv , so that they can be learned."}, {"heading": "3.2 Text and Frame Representation", "text": "We use a convolutional neural network for predicting v and qv , while t is obtained via a recurrent neural network. To jointly learn the parameters of these networks, we use a Siamese ranking network, trained with triplets of (t ,v+,v\u2212). We provide the model architecture in supplementary material. We now describe the textual representation t and the image representations v and qv in more detail.\n\u2020Liu et al. [31] does the inverse. It poses the problem as learning to assign a higher similarity to corresponding frame and query than to the same frame and a random query. Thus, the model learns to answer the question \u201cwhat is a good query for this image?\u201d.\nTextual representation. As a feature representation t of the textual query t , we first project each word of the query into a 300- dimensional semantic space using the word2vec model [39], which is fine-tuned on unique queries from the Bing Clickture dataset [18]. Then, we encode the individual word representations into a single fixed-length embedding using an LSTM [17]. This allows us to emphasize visually informative words and handle phrases.\nImage representation. To represent the image, we leverage the feature representations of a pre-trained VGG-19 network [49]. We replace the softmax layer with a linear layer M with 301 dimensions. The first 300 dimensions are used as the embedding v, while the last dimension represents the quality score qv"}, {"heading": "4 SUMMARIZATION MODEL", "text": "We use the framework of submodular optimization to create summaries that take into account multiple objectives [29]. In this framework, summarization is posed as the problem of selecting a subset (in our case, of frames) y\u2217 that maximizes a linear combination of submodular objective functions f(xV , y) = [f1(xV , y), ..., fn (xV , y)]T . Specifically,\ny\u2217 = arg max y\u2208YV wTf(xV , y), (6)\nwhere YV denote the set of all possible solutions y and xV the features of videoV. In this work, we assume that the cardinality |y| is fixed to some value k (we use k = 5 in our experiments).\nFor non-negative weights w, the objective in Eq. (6) is submodular [27], meaning that it can be optimized near-optimally in an efficient way using a greedy algorithm with lazy evaluations [40, 43].\nObjective functions. We choose a small set of objective functions, each capturing different aspects of the summary.\n(1) Query similarity f(\u00b7, \u00b7) = \u2211v \u2208y s(t, v) where t is the query embedding, v is frame embedding and s(\u00b7, \u00b7) denotes the cosine similarity defined in Eq. (1).\n(2) Quality score f(\u00b7, \u00b7) = \u2211v \u2208y qv , where qv represents score that is based on the quality of v as a thumbnail. This model scores the image relevance in a query-independent manner based on properties such as contrast, composition, etc.\n(3) Diversity of the elements in the summary f(xV , y) = \u2211 i \u2208yminj<i\nDxV (i, j), according to some dissimilarity measure D. We use the Euclidean distance in of the FC2 features of the VGG-19 network for D\u2021.\n(4) Representativeness [14]. This objective favors selecting the medoid frames of a video, such that the visually frequent frames in the video are represented in the summary.\nWeight learning. To learn the weights w in Eq. (6), ground truth summaries for query-video pairs are required. Previous methods typically only optimized for relevance [31] or used small datasets with limited vocabularies [48]. Thus, to be able to train our model, we collected a new dataset with relevance and diversity annotations, which we introduce in the next Section.\nIf relevance and diversity labels are known, we can estimate the optimal mixing weights of the submodular functions through subgradient descent [29]. In order to directly optimize for the F1score used at test time, we use a locally modular approximation based on the procedure of [42] and optimize the weights using AdaGrad [9]."}, {"heading": "5 RELEVANCE AND DIVERSITY DATASET (RAD)", "text": "We collected a dataset with query relevance and diversity annotation to let us train and evaluate query-relevant summaries. Our dataset consists of 200 videos, each of which was retrieved given a different query.\nUsing Amazon Mechanical Turk (AMT) we first annotate the video frames with query relevance labels, and then partition the frames into clusters according to visual similarity. These kind of labels were used previously in the MediaEval diverse social images challenge [19] and enabled evaluation of the automatic methods for creating relevant and diverse summaries.\nTo select a representative sample of queries and videos for the dataset, we used the following procedure: We take the top YouTube queries between 2008 and 2016 from 22 different categories as seed queries\u00a7. These queries are typically rather short and generic concepts, so to obtain longer, more realistic queries we use YouTube auto-complete to suggest phrases. Using this approach we collect 200 queries. Some examples are brock lesnar vs big show, taylor swift out of the woods, etc. For each query, we take the top video result with a duration of 2 to 3 minutes.\nTo annotate the videos, we set up two consecutive tasks on Mechanical Turk. All videos are sampled at one frame per second. In the first task, a worker is asked to label each frame with its relevance w.r.t. the given query. Options for answers are \u201cVery Good\u201d,\u201cGood\u201d, \u201cNot good\u201d and \u201cTrash\u201d, where trash indicates that the frame is both irrelevant and low-quality (e.g. blurred, bad contrast, etc.). After annotating the relevance, the worker is asked to distribute the frames into clusters according to their visual similarity. We obtain one clustering per worker, where each clustering consists of mutually exclusive subsets of video frames as clusters. The number of clusters in the clustering is chosen by the worker. Each video is annotated by 5 different people and a total of 48 subjects participated in the\n\u2021Derivation of submodularity of this objective is provided in the suppl. \u00a7https://www.google.com/trends/explore\nannotation of the dataset. To ensure high-quality annotations, we defined a qualification task, where we check the results manually to ensure the workers provide good annotations. Only workers who passed this test were allowed to take further assignments."}, {"heading": "5.1 Analysis", "text": "We now analyse the two kinds of annotations obtained through this procedure and describe how we merge these annotations into one set of ground truth labels per video.\nLabel distributions. The distribution of relevance labels is \u201cVery Good\u201d: 17.55%, \u201cGood\u201d: 57.40%, \u201cNot good\u201d: 12.31% and \u201cTrash\u201d: 12.72%. The minimum, maximum and mean number of clusters per video are 4.9, 25.2 and 13.4 respectively over all videos of RAD.\nRelevance annotation consistency. Given the inherent subjectivity of the task, we want to know whether annotators agree with each other about the query relevance of frames. To do this, we follow previous work [15, 20, 58] and compute the Spearman rank correlation (\u03c1) between the relevance scores of different subjects, splitting five annotations of each video into two groups of two and three raters each. We take all split combination to find mean \u03c1 for a video.\nOur dataset has an average correlation of \u03c1 = 0.73 over all videos, where 1 is a perfect correlation while 0 would indicate no consistency in the scores. On the related task of event-specific image importance, using five annotators, consistency is only \u03c1 = 0.4 [58]. Thus, we can be confident that our relevance labels are of high quality.\nCluster consistency. To the best of our knowledge, we are the first to annotate multiple clusterings per video and look into the consistency of multiple annotators. MediaEval, for example, used multiple relevance labels but only one clustering [19]. Various ways of measuring the consistency of clusterings exist, e.g. Variation of Information, Normalised Mutual Information or the Rand index (See Wagner and Wagner [57] for an excellent overview). In the following we propose to use Normalised Mutual Information (NMI), an information theoretic measure [11] which is the ratio of the mutual information between two clusterings (I (C,C \u2032)) and the sum of entropies of the clusterings (H (C) + H (C \u2032)):\nNMI (C,C \u2032) = 2 \u00b7 I (C,C \u2032)\nH (C) + H (C \u2032) , (7)\n. We chose NMI over the more recently proposed Variation of Information (VI) [37], as NMI has a fixed range ([0, 1]) while still being closely related to VI (see supplementary material).\nOur dataset has a cluster consistency of 0.54 . Since NMI is 0 if two clusterings are independent and 1 iff they are identical, we see that our annotators have a high degree of agreement.\nGround truth For evaluation on the test videos, we create a single ground truth annotation for each video. We merge the five relevance annotations as well as the clustering of each query-video pair. For the final ground truth of relevance prediction, we require the labels be either positive or negative for each video frame. We map all \u201cVery Good\u201d labels to 1, \u201cGood\u201d labels to 0.5 and \u201cNot Good\u201d and \u201cTrash\u201d labels to 0. We compute the mean of the five relevance annotation labels and label the frame as positive if the mean is \u2265 0.5 and as negative otherwise.\nTo merge clustering annotations, we calculate NMI between all pairs of clustering and choose the clustering with the highest mean NMI, i.e. the most prototypical cluster. An example of relevance and clustering annotation is provided in Fig. 6."}, {"heading": "6 CONFIGURATION TESTING", "text": "Before comparing our proposed relevance model against state of the art, in Sec. 7, we first analyze the performance of using different objectives, cost functions and text representation. For evaluation we use the Query-dependent Thumbnail Selection Dataset (QTS) provided by Liu et al. [31]. The dataset contains 20 candidate thumbnails for each video, each of which is labeled one of five scores: Very Good (VG), Good (G), Fair (F), Bad (B), or Very Bad (VB). We evaluate on the available 749 query-video pairs. To transform the categorical labels to numerical values, we use the same mapping as [31].\nEvaluation metrics. As evaluation metrics, we are using HIT@1 and mean Average Precision (mAP) as reported and defined in Liu et al. [31], as well as the Spearman\u2019s Rank Correlation. HIT@1 is computed as the hit ratio for the highest ranked thumbnail.\nTraining dataset. For training, we use two datasets: (i) the Bing Clickture dataset [18] and (ii) the RAD dataset (Sec. 5). Clickture is a large dataset consisting of queries and retrieved images from Bing Image search. The annotation is in form of triplets (K ,Q,C) meaning that the image K was clicked C times in the search results of the query Q . This dataset is well suited for training our relevance model, since our task is the retrieval of relevant keyframes from a video. It is, however, from the image and not the video domain. Thus, we additionally fine-tune the models on the complete RAD dataset consisting of 200 query-video pairs. From each query-video pair, we sample an equal number of positive and negative frames to give equal weight to each video. In total, we use 0.5M triplets from the Clickture and 14K triplets from the RAD dataset for training.\nImplementation details. We preprocess the images before passing them into the network. We truncate the number of words in the query\nat 14, as a tradeoff between the mean and maximum query length in Clickture dataset(5 and 26 respectively) [41]. We set the margin parameter \u03b3 in the loss in Eq. (5) to 1 and the tradeoff parameter \u03b4 for the Huber loss to 1.5 as in [16]. The LSTM consists of a hidden layer with 512 units. We train the parameters of the LSTM and projection layer M using stochastic gradient descent with adaptive weight updates (AdaGrad) [9]. We add an l2 penalty on the weights, with a \u03bb of 10\u22123. We train for 20 epochs using minibatches of 128 triplets."}, {"heading": "6.1 Tested components", "text": "We test three important compontents of our model, which we discuss next.\nObjective. We compare our proposed training objective to that of Liu et al. [31]. Their model is trained to rank a positive query higher than a negative query given a fixed frame. In contrast, our method is trained to rank a positive frame higher than a negative frame given a fixed query.\nCost function. We also investigate the importance of modeling frame quality. In particular, we compare different cost functions. (i) We enforce two ranking constraints: one for the quality term and one for the embedding similarity, as in Eq.(4) (Qexpli ), (ii) We sum the quality and similarity term into one output score, for which we enforce the rank constraint, as in Eq.(3) (Qimpli ) or (iii) we don\u2019t model quality at all.\nText representation. As mentioned in Sec. 3.2, we represent the words of the query using a word vector. To combine the individual word representations into single vector, we investigate two approaches: (i) averaging the word embedding vectors and (ii) using an LSTM model that learns to combine the individual word embeddings."}, {"heading": "6.2 Results", "text": "We show the results of our detailed experiments in Tab. 1. They give insights on several important points.\nText representation. Modeling queries with an LSTM, rather than averaging the individual word representations, improves performance significantly. This is not surprising, as this model can learn to ignore stop words and words that are not visually informative (e.g. 2014).\nObjective and Cost function. The analysis shows that training with our objective leads to better performance compared to using the objective of Liu et al. [31]. This can be explained with the properties of videos, which typically contain many frames that are low-quality or not visually informative [51]. Thus, formulating the thumbnail task in a way that the model can learn about these quality aspects is beneficial. Using the appropriate triplets for training boosts performance substantially (correlation with the loss of Liu et al. [31] + LSTM: 0.270, ours Huber + LSTM 0.367). When including a quality term in the model, performance improves further, where an explicit loss performs slightly better (Ours: Huber + LSTM +Qexpli in Tab. 1).\nSomewhat surprisingly, modeling quality alone already outperforms Liu et al. [31] in terms of mAP, despite not using any textual information. Quality adds a significant boost to performance in the\nvideo domain. Interestingly, this is different in the image domain, due to the difference in quality statistics. Images returned by a search engine are mostly of good quality, thus explicitly accounting for it does not improve performance (see supplementary material).\nTo conclude, we see that the better alignment of the objective to the keyframe retrieval task, the addition of an LSTM and modeling quality of the thumbnails improves performance. Together, they provide an substantial improvement compared to Liu et al. \u2019s model. Our method achieves an absolute improvement of 6.01% in HIT@1, 4.04% in mAP, and an improvement in correlation from 0.186 to 0.376. These gains are even more significant when we consider the possible ranges of these metrics. e.g. for Spearman correlation, human agreement is at 0.73 on the RAD dataset (c.f. Sec. 5.1), thus providing an upper bound. Similarly, HIT@1 and mAP have small effective ranges given their high scores for a random model."}, {"heading": "7 EXPERIMENTS", "text": "In the previous section, we have determined that our objective, embedding queries with an LSTM and explicitly modelling quality performs best. We call this model QAR (Quality-Aware Relevance) in the following and compare against state-of-the-art models on the QTS and RAD datasets. We also evaluate the full summarization model on RAD. For these experiments, we split RAD into 100 videos for training, 50 for validation and 50 for testing.\nEvaluation metrics. For relevance we use the same metrics as in Sec. 6. To evaluate video summaries on RAD, we additionally use F1 scores. The F1 score is the harmonic mean of precision of relevance prediction and cluster recall [19]. It is high, if a method selects relevant frames from diverse clusters."}, {"heading": "7.1 Evaluating the Relevance Model", "text": "We evaluate our model (QAR) and compare it to Liu et al. [31] and Video2GIF [16].\nQuery-dependent Thumbnail Selection Dataset (QTS) [31] We compare against the state of the art on the QTS evaluation dataset in Tab. 2. We report the performance of Liu et al. [31] from their paper. Note, however, that the results are not directly comparable, as they use query-video pairs for predicting relevance, while only the titles are shared publicly. Thus, we use the titles instead, which is an important difference. Relevance is annotated with respect to the queries, which often differ from the video titles. We compare the re-implementation of [31] using titles in detail in Tab. 1.\nEncouragingly, our model performs well even when just using the titles and outperforms them on most metrics. It improves mAP by 4.22% over [31] and correlation by a margin of 0.254 (c.f. Table 2). Figure 3 shows the precision-recall curve for the experiment. As can be seen QAR outperforms [31] for all recall ratios. To better\nunderstand the effects of using titles or queries, we quantify the value of the two on the RAD dataset below.\nOur dataset (RAD) We also evaluate our model on the RAD test set (Tab. 3). QAR (ours) significantly outperforms the previous state of the art of [16, 31], even when augmenting Liu et al. [31] with an LSTM. QAR improves mAP by 2.9% when using TITLES and 3.9% when using QUERIES over our implementation of Liu et al. [31]+LSTM.\nWe also see that modeling quality leads to significant gains in terms of mAP when using TITLES or QUERIES (+1.7% in both cases). HIT@1 for query relevance, however, is lower when including quality. We believe that the reason for this is that when the query is given, the textual-visual similarity is a more reliable signal to determine the single best keyframe. While including quality improves the overall ranking on mAP, it is solely based on appearance and thus seems to inhibit the fine-grained ranking results at low recall(Fig. 4). However, when only the title is used, the frame quality becomes a stronger predictor for thumbnail selection and improves performance on all metrics.\nWe present some qualitative results of different methods for relevance prediction in Fig. 5."}, {"heading": "7.2 Evaluating the Summarization Model", "text": "As mentioned in Sec. 4, we use four objectives for our summarization model. Referring to Tab. 4, we use QAR model to get Similarity and Quality scores while Diversity and Representativeness scores are obtained as described in Sec. 4. We compare the performance of our full model with each individual objective, a baseline based on Maximal Marginal Relevance (MMR) [4] and Hecate [51]. MMR greedily builds a set that maximises the weighted sum of two terms: (i) The similarity of the selected elements to a query and (ii) The dissimilarity to previously selected elements. To estimate the similarity to the query we use our own model (QAR without Qexpli )\nand for dissimilarity the diversity as defined in Sec. 4. Finally, we compare it to Hecate, a recent method introduced in [51]. Hecate estimates frame quality using the stillness of the frame and selects representative and diverse thumbnails by clustering the video with k-means and selecting the highest quality frame from the k largest clusters.\nResults Quantitative results are shown in Tab. 4, while Fig. 6 shows qualitative results. As can be seen, combining all objectives with our model works best. It outperforms all single objectives, as well as the MMR [4] baseline, even though MMR also uses our wellperforming similarity estimation. Similarity alone has the highest precision, but tends to pick frames that are visually similar (c.f. Fig. 6), thus resulting in low cluster recall. Diversification objectives (diversity and representativeness) have a high cluster recall, but the frames are less relevant. Somewhat surprisingly, Hecate [51] is a relatively strong baseline. In particular, it performs well in terms of relevance, despite using a simple quality score. This further highlights the importance of quality for the thumbnail selection task. It also indicates that the used VGG-19 architecture might be\nsuboptimal for predicting quality. CNNs for classification use small input resolutions, thus making it difficult to predict quality aspects such as blur. Finding better architectures for that task is actively researched, e.g. [32, 34], and might be used to improve our method.\nWhen analysing the learned weights (c.f. Tab. 4) we find that the similarity prediction is the most important objective, which matches our expectations. Quality gets a lower, but non-zero weight, thus showing that it provides information that is complementary to querysimilarity. Thus, it helps predicting the relevance of a frame. The reader should however also be aware that differences in the variance of the objectives can affect the weights learned. Thus, they should be taken with a grain of salt and only be considered tendencies."}, {"heading": "8 CONCLUSION", "text": "We introduced a new method for query-adaptive video summarization. At its core lies a textual-visual embedding, which lets us select frames relevant to a query. In contrast to earlier works, such as [48, 63], this model allows us to handle unconstrained queries and even full sentences. We proposed and empirically evaluated\ndifferent improvements over [31], for learning a relevance model. Our empirical evaluation showed that a better training objective, a more sophisticated text model, and explicitly modelling quality leads to significant performance gains. In particular, we showed that quality plays an important role in the absence of high-quality relevance information, such as queries, i.e. when only the title can be used. Finally, we introduced a new dataset for thumbnail selection which comes with query-relevance labels and a grouping of the frames according to visual and semantic similarity. On this data, we tested our full summarization framework and showed that it compares favourably to strong baselines such as MMR [4] and [51]. We hope that our new dataset will spur further research in query adaptive video summarization."}, {"heading": "9 ACKNOWLEDGEMENTS", "text": "This work has been supported by Toyota via the project TRACEZurich. We also acknowledge the support by the CHIST-ERA project MUSTER. MG was supported by the European Research Council under the project VarCity (#273940)."}], "references": [{"title": "Automatic editing of footage from multiple social cameras", "author": ["I Arev", "HS Park", "Yaser Sheikh"], "venue": "ACM Transactions on Graphics (TOG)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A data-driven approach for tag refinement and localization in web videos", "author": ["Lamberto Ballan", "Marco Bertini", "Giuseppe Serra", "Alberto Del Bimbo"], "venue": "Computer Vision and Image Understanding", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein"], "venue": "In ACM SIGIR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Learning a Recurrent Visual Representation for Image Caption Generation", "author": ["Xinlei Chen", "C Lawrence Zitnick"], "venue": "Proceedings of CoRR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["Pradipto Das", "Chenliang Xu", "Richard F. Doell", "Jason J. Corso"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "VSUMM: A mechanism designed to produce static video summaries and a novel evaluation method", "author": ["Sandra Eliza Fontes de Avila", "Ana Paula Brand\u00e3o Lopes", "Antonio da Luz", "Arnaldo de Albuquerque Ara\u00fajo"], "venue": "Pattern Recognition Letters 32,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Robust Data Clustering", "author": ["Ana L.N. Fred", "Anil K. Jain"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Devise: A deep visualsemantic embedding model", "author": ["Andrea Frome", "Gs Corrado", "Jonathon Shlens"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Diverse sequential subset selection for supervised video summarization", "author": ["Boqing Gong", "Wei-Lun Chao", "Kristen Grauman", "Fei Sha"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Video Summarization by Learning Submodular Mixtures of Objectives", "author": ["Michael Gygli", "Helmut Grabner", "Luc Van Gool"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "The Interestingness of Images", "author": ["M Gygli", "H Grabner", "H Riemenschneider", "F Nater", "L Van Gool"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Video2GIF: Automatic Generation of Animated GIFs from Video", "author": ["Michael Gygli", "Yale Song", "Liangliang Cao"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Clickture: A large-scale real-world image dataset", "author": ["XS Hua", "L Yang", "M Ye", "K Wang", "Y Rui", "J Li"], "venue": "Technical Report. Microsoft Research Technical Report MSR-TR-2013-75", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Retrieving Diverse Social Images at MediaEval 2015:  Challenge, Dataset and Evaluation", "author": ["Bogdan Ionescu", "Adrian Popescu", "Mihai Lupu", "Alexandru Lucian Ginsca", "Henning M\u00fcller"], "venue": "MediaEval", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Understanding the intrinsic memorability of images", "author": ["Phillip Isola", "Devi Parikh", "Antonio Torralba", "Aude Oliva"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei Li"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei Fei Li"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Large-Scale Video Summarization Using Web-Image Priors", "author": ["Aditya Khosla", "Raffay Hamid", "CJ Lin", "Neel Sundaresan"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Joint summarization of large-scale collections of web images and videos for storyline reconstruction", "author": ["Gunhee Kim", "Leonid Sigal", "Eric P Xing"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "First-person hyper-lapse videos", "author": ["Johannes Kopf", "Michael F Cohen", "Richard Szeliski"], "venue": "ACM Transactions on Graphics", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Submodular function maximization", "author": ["Andreas Krause", "Daniel Golovin"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Discovering important people and objects for egocentric video summarization", "author": ["Yong Jae Lee", "Joydeep Ghosh", "Kristen Grauman"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["Hui Lin", "JA Bilmes"], "venue": "arXiv preprint arXiv:1210.4871", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Using Web Photos for Measuring Video Frame Interestingness", "author": ["Feng Liu", "Yuzhen Niu", "Michael Gleicher"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Multi-task deep visual-semantic embedding for video thumbnail selection", "author": ["Wu Liu", "Tao Mei", "Yongdong Zhang", "C Che", "Jiebo Luo"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Deep multi-patch aggregation network for image style, aesthetics, and quality estimation", "author": ["Xin Lu", "Zhe Lin", "Xiaohui Shen", "Radomir Mech", "James Z Wang"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Story-driven summarization for egocentric video", "author": ["Zheng Lu", "Kristen Grauman"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Composition-preserving deep photo aesthetics assessment", "author": ["Long Mai", "Hailin Jin", "Feng Liu"], "venue": "In CVPR", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "TagBook: A Semantic Video Representation without Supervision for Event Detection", "author": ["Masoud Mazloom", "Xirong Li", "Cees Snoek"], "venue": "IEEE Transactions on Multimedia", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Comparing clusterings by the variation of information. In Learning theory and kernel", "author": ["Marina Meil\u0103"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "J Dean"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Accelerated greedy algorithms for maximizing submodular set functions", "author": ["M Minoux"], "venue": "Optimization Techniques", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1978}, {"title": "Siamese Recurrent Architectures for Learning Sentence Similarity", "author": ["Jonas Mueller", "Aditya Thyagarajan"], "venue": "In AAAI", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "A submodular-supermodular procedure with applications to discriminative structure learning", "author": ["Mukund Narasimhan", "Jeff A Bilmes"], "venue": "arXiv preprint arXiv:1207.1404", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "An analysis of approximations for maximizing submodular set functions - I", "author": ["GL Nemhauser", "LA Wolsey", "ML Fisher"], "venue": "Mathematical Programming", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1978}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Category-specific video summarization", "author": ["Danila Potapov", "Matthijs Douze", "Zaid Harchaoui", "Cordelia Schmid"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Nonchronological video synopsis and indexing", "author": ["Yael Pritch", "Alex Rav-Acha", "Shmuel Peleg"], "venue": "IEEE transactions on pattern analysis and machine intelligence 30,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Query-Focused Extractive Video Summarization", "author": ["Aidean Sharghi", "Boqing Gong", "Mubarak Shah"], "venue": "CoRR abs/1607.05177", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": "Transactions of the Association for Computational Linguistics", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "To Click or Not To Click: Automatic Selection of Beautiful Thumbnails from Videos", "author": ["Yale Song", "Miriam Redi", "Jordi Vallmitjana", "Alejandro Jaimes"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "TV- SUM: Summarizing web videos using titles", "author": ["Yale Song", "Jordi Vallmitjana", "Amanda Stent", "Alejandro Jaimes"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Ranking Domain-Specific Highlights by Analyzing Edited Videos", "author": ["Min Sun", "Ali Farhadi", "Steve Seitz"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Salient Montages from Unconstrained Videos", "author": ["Min Sun", "Ali Farhadi", "Ben Taskar", "Steve Seitz"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Semantic Highlight Retrieval and Term Prediction", "author": ["Min Sun", "Kuo-Hao Zeng", "Yenchen Lin", "Farhadi Ali"], "venue": "IEEE Transactions on Image Processing", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2017}, {"title": "Comparing Clusterings - An Overview", "author": ["Silke Wagner", "Dorothea Wagner"], "venue": "Graph-Theoretic Concepts in Computer Science", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2007}, {"title": "Event-Specific Image Importance", "author": ["Yufei Wang", "Zhe Lin", "Xiaohui Shen", "Radomir Mech", "Gavin Miller", "Garrison W Cottrell"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2016}, {"title": "Key frame selection by motion analysis", "author": ["Wayne Wolf"], "venue": "Acoustics, Speech, and Signal Processing", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1996}, {"title": "Detecting snap points in egocentric video with a web photo prior", "author": ["Bo Xiong", "Kristen Grauman"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2014}, {"title": "Highlight detection with pairwise deep ranking for first-person video summarization", "author": ["Ting Yao", "Tao Mei", "Yong Rui"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2016}, {"title": "Mouse activity as an indicator of interestingness in video. In ICMR", "author": ["Gloria Zen", "Paloma de Juan", "Yale Song", "Alejandro Jaimes"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "Semantic highlight retrieval", "author": ["Kuo-Hao Zeng", "Yen-Chen Lin", "Ali Farhadi", "Min Sun"], "venue": "In ICIP", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2016}, {"title": "Video Summarization with Long Short-term Memory", "author": ["Ke Zhang", "Wei-Lun Chao", "Fei Sha", "Kristen Grauman"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2016}, {"title": "Quasi real-time summarization for consumer videos", "author": ["Bin Zhao", "Eric P Xing"], "venue": "In CVPR", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 95, "endOffset": 106}, {"referenceID": 33, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 95, "endOffset": 106}, {"referenceID": 0, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 11, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 20, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 21, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 25, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 30, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 42, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 49, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 60, "context": "The problem of making videos content more accessible has spurred research in automatic tagging [2, 36, 47] and video summarization [1, 14, 23, 24, 28, 33, 45, 53, 65].", "startOffset": 131, "endOffset": 166}, {"referenceID": 20, "context": "Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].", "startOffset": 142, "endOffset": 150}, {"referenceID": 25, "context": "Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].", "startOffset": 142, "endOffset": 150}, {"referenceID": 11, "context": "Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].", "startOffset": 160, "endOffset": 168}, {"referenceID": 48, "context": "Video summarization, on the other hand, aims at making videos more accessible by reducing them to a few interesting and representative frames [23, 28] or shots [14, 52].", "startOffset": 160, "endOffset": 168}, {"referenceID": 22, "context": "Our approach improves previous works in the area of textual-visual embeddings [25, 31] and proposes an extension of an existing video summarization method using submodular mixtures [14] for creating summaries that are query-adaptive.", "startOffset": 78, "endOffset": 86}, {"referenceID": 28, "context": "Our approach improves previous works in the area of textual-visual embeddings [25, 31] and proposes an extension of an existing video summarization method using submodular mixtures [14] for creating summaries that are query-adaptive.", "startOffset": 78, "endOffset": 86}, {"referenceID": 11, "context": "Our approach improves previous works in the area of textual-visual embeddings [25, 31] and proposes an extension of an existing video summarization method using submodular mixtures [14] for creating summaries that are query-adaptive.", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "We train this model on a large dataset of image search data [18] and our newly introduced Relevance and Diversity dataset (Section 5).", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "exist: (i) modelling generic frame interestingness [16, 28] or (ii) using additional information such as the video title or a text query to", "startOffset": 51, "endOffset": 59}, {"referenceID": 25, "context": "exist: (i) modelling generic frame interestingness [16, 28] or (ii) using additional information such as the video title or a text query to", "startOffset": 51, "endOffset": 59}, {"referenceID": 27, "context": "find relevant frames [30, 31, 52].", "startOffset": 21, "endOffset": 33}, {"referenceID": 28, "context": "find relevant frames [30, 31, 52].", "startOffset": 21, "endOffset": 33}, {"referenceID": 48, "context": "find relevant frames [30, 31, 52].", "startOffset": 21, "endOffset": 33}, {"referenceID": 1, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 5, "endOffset": 16}, {"referenceID": 33, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 5, "endOffset": 16}, {"referenceID": 9, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 44, "endOffset": 56}, {"referenceID": 28, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 44, "endOffset": 56}, {"referenceID": 46, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 44, "endOffset": 56}, {"referenceID": 3, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 4, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 6, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 18, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 19, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 32, "context": "ging [2, 36, 47], textual-visual embeddings [12, 31, 50] and image description [3, 5, 6, 8, 10, 21, 22, 35] .", "startOffset": 79, "endOffset": 107}, {"referenceID": 23, "context": "hyperlapses [26], montages [54] or video synopses [46].", "startOffset": 12, "endOffset": 16}, {"referenceID": 50, "context": "hyperlapses [26], montages [54] or video synopses [46].", "startOffset": 27, "endOffset": 31}, {"referenceID": 43, "context": "hyperlapses [26], montages [54] or video synopses [46].", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 87, "endOffset": 103}, {"referenceID": 21, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 87, "endOffset": 103}, {"referenceID": 25, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 87, "endOffset": 103}, {"referenceID": 54, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 87, "endOffset": 103}, {"referenceID": 11, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 122, "endOffset": 130}, {"referenceID": 30, "context": "The goal of extractive methods is instead to select an informative subset of keyframes [23, 24, 28, 59] or video segments [14, 33] from the initial video.", "startOffset": 122, "endOffset": 130}, {"referenceID": 10, "context": "Extractive methods need to optimise at least two properties of the summary: the quality of the selected frames and their diversity [13, 14, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 11, "context": "Extractive methods need to optimise at least two properties of the summary: the quality of the selected frames and their diversity [13, 14, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 44, "context": "Extractive methods need to optimise at least two properties of the summary: the quality of the selected frames and their diversity [13, 14, 48].", "startOffset": 131, "endOffset": 143}, {"referenceID": 11, "context": "Sometimes, additional objectives such as temporal uniformity [14] and relevance [48] are also optimised.", "startOffset": 61, "endOffset": 65}, {"referenceID": 44, "context": "Sometimes, additional objectives such as temporal uniformity [14] and relevance [48] are also optimised.", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "The simplest approach to obtain a representative and diverse summary is to cluster videos into events and select the best frame per event [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 10, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 120, "endOffset": 132}, {"referenceID": 44, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 120, "endOffset": 132}, {"referenceID": 59, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 120, "endOffset": 132}, {"referenceID": 11, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 156, "endOffset": 164}, {"referenceID": 26, "context": "More sophisticated approaches jointly optimise for importance and diversity by using determinantal point process (DPPs) [13, 48, 64] or submodular mixtures [14, 29].", "startOffset": 156, "endOffset": 164}, {"referenceID": 44, "context": "[48], who present an approach for query-adaptive video summarization using DPPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In this work, we formulate video summarization as a maximisation problem over a set of submodular functions, following [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 42, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 78, "endOffset": 86}, {"referenceID": 57, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 78, "endOffset": 86}, {"referenceID": 13, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 136, "endOffset": 152}, {"referenceID": 49, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 136, "endOffset": 152}, {"referenceID": 51, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 136, "endOffset": 152}, {"referenceID": 56, "context": "The prediction problem can be formulated as a classification [45], regression [28, 62], or, as is now most common, as a ranking problem [16, 53, 55, 61].", "startOffset": 136, "endOffset": 152}, {"referenceID": 42, "context": "To simplify the task, some approaches assume the domain of the video given and train a model for each domain [45, 53, 61].", "startOffset": 109, "endOffset": 121}, {"referenceID": 49, "context": "To simplify the task, some approaches assume the domain of the video given and train a model for each domain [45, 53, 61].", "startOffset": 109, "endOffset": 121}, {"referenceID": 56, "context": "To simplify the task, some approaches assume the domain of the video given and train a model for each domain [45, 53, 61].", "startOffset": 109, "endOffset": 121}, {"referenceID": 55, "context": "[60], detects \u201csnap points\u201d by using a web image prior.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 9, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 18, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 19, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 32, "context": "Several methods exist that can retrieve images given unconstrained text or vice versa [8, 10, 12, 21, 22, 35].", "startOffset": 86, "endOffset": 109}, {"referenceID": 9, "context": "These typically project both modalities into a joint embedding space [12], where semantic similarity can be compared using a measure like cosine similarity.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "Word2vec [38] and GloVe [44]", "startOffset": 9, "endOffset": 13}, {"referenceID": 41, "context": "Word2vec [38] and GloVe [44]", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "Once both modalities are in the same space, they may be easily compared [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "[31] applied this idea to video thumbnail", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31], but we provide several important improvements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31], we directly optimise the target objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "As [12], we use the cosine similarity", "startOffset": 3, "endOffset": 7}, {"referenceID": 55, "context": "[60].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We follow [16] and use a Huber loss for lp , i.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "[31] does the inverse.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "As a feature representation t of the textual query t , we first project each word of the query into a 300dimensional semantic space using the word2vec model [39], which is fine-tuned on unique queries from the Bing Clickture dataset [18].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "As a feature representation t of the textual query t , we first project each word of the query into a 300dimensional semantic space using the word2vec model [39], which is fine-tuned on unique queries from the Bing Clickture dataset [18].", "startOffset": 233, "endOffset": 237}, {"referenceID": 14, "context": "Then, we encode the individual word representations into a single fixed-length embedding using an LSTM [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 45, "context": "To represent the image, we leverage the feature representations of a pre-trained VGG-19 network [49].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "We use the framework of submodular optimization to create summaries that take into account multiple objectives [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "(6) is submodular [27], meaning that it can be optimized near-optimally in an efficient way using a greedy algorithm with lazy evaluations [40, 43].", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "(6) is submodular [27], meaning that it can be optimized near-optimally in an efficient way using a greedy algorithm with lazy evaluations [40, 43].", "startOffset": 139, "endOffset": 147}, {"referenceID": 40, "context": "(6) is submodular [27], meaning that it can be optimized near-optimally in an efficient way using a greedy algorithm with lazy evaluations [40, 43].", "startOffset": 139, "endOffset": 147}, {"referenceID": 11, "context": "(4) Representativeness [14].", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "Previous methods typically only optimized for relevance [31] or used small datasets with limited vocabularies [48].", "startOffset": 56, "endOffset": 60}, {"referenceID": 44, "context": "Previous methods typically only optimized for relevance [31] or used small datasets with limited vocabularies [48].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "If relevance and diversity labels are known, we can estimate the optimal mixing weights of the submodular functions through subgradient descent [29].", "startOffset": 144, "endOffset": 148}, {"referenceID": 39, "context": "In order to directly optimize for the F1score used at test time, we use a locally modular approximation based on the procedure of [42] and optimize the weights using AdaGrad [9].", "startOffset": 130, "endOffset": 134}, {"referenceID": 7, "context": "In order to directly optimize for the F1score used at test time, we use a locally modular approximation based on the procedure of [42] and optimize the weights using AdaGrad [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 16, "context": "These kind of labels were used previously in the MediaEval diverse social images challenge [19] and enabled evaluation of the automatic methods for creating relevant and diverse summaries.", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "To do this, we follow previous work [15, 20, 58] and compute the Spearman rank correlation (\u03c1) between the relevance scores of different subjects, splitting five annotations of each video into two groups of two and three raters each.", "startOffset": 36, "endOffset": 48}, {"referenceID": 17, "context": "To do this, we follow previous work [15, 20, 58] and compute the Spearman rank correlation (\u03c1) between the relevance scores of different subjects, splitting five annotations of each video into two groups of two and three raters each.", "startOffset": 36, "endOffset": 48}, {"referenceID": 53, "context": "To do this, we follow previous work [15, 20, 58] and compute the Spearman rank correlation (\u03c1) between the relevance scores of different subjects, splitting five annotations of each video into two groups of two and three raters each.", "startOffset": 36, "endOffset": 48}, {"referenceID": 53, "context": "4 [58].", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": "MediaEval, for example, used multiple relevance labels but only one clustering [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "Variation of Information, Normalised Mutual Information or the Rand index (See Wagner and Wagner [57] for an excellent overview).", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "In the following we propose to use Normalised Mutual Information (NMI), an information theoretic measure [11] which is the ratio of the mutual information between two clusterings (I (C,C \u2032)) and the sum of entropies of the clusterings (H (C) + H (C \u2032)):", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "We chose NMI over the more recently proposed Variation of Information (VI) [37], as NMI has a fixed range ([0, 1]) while still being closely related to VI (see supplementary material).", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "We chose NMI over the more recently proposed Variation of Information (VI) [37], as NMI has a fixed range ([0, 1]) while still being closely related to VI (see supplementary material).", "startOffset": 107, "endOffset": 113}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "To transform the categorical labels to numerical values, we use the same mapping as [31].", "startOffset": 84, "endOffset": 88}, {"referenceID": 28, "context": "[31], as well as the Spearman\u2019s Rank Correlation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For training, we use two datasets: (i) the Bing Clickture dataset [18] and (ii) the RAD dataset (Sec.", "startOffset": 66, "endOffset": 70}, {"referenceID": 38, "context": "We truncate the number of words in the query at 14, as a tradeoff between the mean and maximum query length in Clickture dataset(5 and 26 respectively) [41].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "5 as in [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "We train the parameters of the LSTM and projection layer M using stochastic gradient descent with adaptive weight updates (AdaGrad) [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] 40.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Note that [31] uses queries for their method which are not publicly available (see text).", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "This can be explained with the properties of videos, which typically contain many frames that are low-quality or not visually informative [51].", "startOffset": 138, "endOffset": 142}, {"referenceID": 28, "context": "[31] + LSTM: 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] in terms of mAP, despite not using any textual information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "674 Video2GIF [16] 67.", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "[31] +LSTM 70.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] +LSTM 72.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] Video2GIF [16] Ours", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[31] Video2GIF [16] Ours", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "The F1 score is the harmonic mean of precision of relevance prediction and cluster recall [19].", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "[31] and Video2GIF [16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[31] and Video2GIF [16].", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "Query-dependent Thumbnail Selection Dataset (QTS) [31] We compare against the state of the art on the QTS evaluation dataset in Tab.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "[31] from their paper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "We compare the re-implementation of [31] using titles in detail in Tab.", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "22% over [31] and correlation by a margin of 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "As can be seen QAR outperforms [31] for all recall ratios.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "MMR [4] X (33%) X (66%) \u2212 \u2212 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 47, "context": "Hecate [51] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "QAR (ours) significantly outperforms the previous state of the art of [16, 31], even when augmenting Liu et al.", "startOffset": 70, "endOffset": 78}, {"referenceID": 28, "context": "QAR (ours) significantly outperforms the previous state of the art of [16, 31], even when augmenting Liu et al.", "startOffset": 70, "endOffset": 78}, {"referenceID": 28, "context": "[31] with an LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31]+LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "We compare the performance of our full model with each individual objective, a baseline based on Maximal Marginal Relevance (MMR) [4] and Hecate [51].", "startOffset": 130, "endOffset": 133}, {"referenceID": 47, "context": "We compare the performance of our full model with each individual objective, a baseline based on Maximal Marginal Relevance (MMR) [4] and Hecate [51].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "H ec at e [5 1]", "startOffset": 10, "endOffset": 15}, {"referenceID": 0, "context": "H ec at e [5 1]", "startOffset": 10, "endOffset": 15}, {"referenceID": 2, "context": "M M R [4 ]", "startOffset": 6, "endOffset": 10}, {"referenceID": 47, "context": "Figure 6: We show video summaries created by Hecate [51], MMR [4], our similarity model and our full summarization approach.", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "Figure 6: We show video summaries created by Hecate [51], MMR [4], our similarity model and our full summarization approach.", "startOffset": 62, "endOffset": 65}, {"referenceID": 47, "context": "Finally, we compare it to Hecate, a recent method introduced in [51].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "It outperforms all single objectives, as well as the MMR [4] baseline, even though MMR also uses our wellperforming similarity estimation.", "startOffset": 57, "endOffset": 60}, {"referenceID": 47, "context": "Somewhat surprisingly, Hecate [51] is a relatively strong baseline.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "[32, 34], and might be used to improve our method.", "startOffset": 0, "endOffset": 8}, {"referenceID": 31, "context": "[32, 34], and might be used to improve our method.", "startOffset": 0, "endOffset": 8}, {"referenceID": 44, "context": "In contrast to earlier works, such as [48, 63], this model allows us to handle unconstrained queries and even full sentences.", "startOffset": 38, "endOffset": 46}, {"referenceID": 58, "context": "In contrast to earlier works, such as [48, 63], this model allows us to handle unconstrained queries and even full sentences.", "startOffset": 38, "endOffset": 46}, {"referenceID": 28, "context": "different improvements over [31], for learning a relevance model.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "data, we tested our full summarization framework and showed that it compares favourably to strong baselines such as MMR [4] and [51].", "startOffset": 120, "endOffset": 123}, {"referenceID": 47, "context": "data, we tested our full summarization framework and showed that it compares favourably to strong baselines such as MMR [4] and [51].", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture queryindependent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.", "creator": "LaTeX with hyperref package"}}}