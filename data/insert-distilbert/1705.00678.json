{"id": "1705.00678", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Twin Learning for Similarity and Clustering: A Unified Kernel Approach", "abstract": "many similarity - based clustering methods essentially work in two separate steps including similarity matrix method computation and subsequent spectral clustering. collectively however, similarity measurement is challenging because it is usually greatly impacted physically by many factors, e. g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. thus the learned overall similarity matrix is often not suitable, let alone optimal, for balancing the subsequent clustering. in addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by either most existing statistical methods. to tackle these obstacle two challenges, we propose seeking a model approach to simultaneously learn cluster, indicator matrix and similarity information in kernel spaces in more a principled way. we show favorable theoretical relationships favorable to kernel k - means, k - means, and spectral clustering methods. then, to address the practical issue of designing how hard to select successively the most suitable kernel for doing a particular clustering task, we further extend our model with a multiple kernel learning ability. with this joint model, we can automatically accomplish roughly three subtasks instances of finding the best cluster indicator matrix, the most accurate similarity relations and using the optimal combination profiles of multiple kernels. by leveraging the interactions between these estimated three subtasks in a joint framework, each subtask can be iteratively boosted by using the linear results of the others towards an overall optimal solution. already extensive experiments are performed to demonstrate the effectiveness of our method.", "histories": [["v1", "Mon, 1 May 2017 19:33:27 GMT  (39kb,D)", "http://arxiv.org/abs/1705.00678v1", "Published in AAAI 2017"], ["v2", "Wed, 3 May 2017 00:29:13 GMT  (39kb,D)", "http://arxiv.org/abs/1705.00678v2", "Published in AAAI 2017"]], "COMMENTS": "Published in AAAI 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["zhao kang", "chong peng", "qiang cheng"], "accepted": true, "id": "1705.00678"}, "pdf": {"name": "1705.00678.pdf", "metadata": {"source": "CRF", "title": "Twin Learning for Similarity and Clustering: A Unified Kernel Approach", "authors": ["Zhao Kang", "Chong Peng", "Qiang Cheng"], "emails": ["qcheng}@siu.edu"], "sections": [{"heading": "Introduction", "text": "Clustering is a fundamental topic in data mining and machine learning (Peng et al. 2016). It partitions data points into different groups, such that the objects within a group are similar to one another and different from those in other groups. Various methods have been proposed over the past decades. Some well-known algorithms include k-means clustering (MacQueen 1967), spectral clustering (Ng et al. 2002), and hierarchical clustering (Johnson 1967).\nThanks to the simplicity and the effectiveness, the kmeans algorithm is widely used. However, it fails to identify arbitrarily shaped clusters. Kernel k-means (Scho\u0308lkopf, Smola, and Mu\u0308ller 1998) has been developed to capture nonlinear structure information hidden in data sets. Kernelbased learning methods requires one to specify a kernel, which means one assumes a certain shape of the underlying\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ndata space. Thus the performance of kernel-based methods are largely affected by the choice of kernel.\nSpectral clustering does a low-dimensional embedding of the similarity matrix of the data before performing k-means clustering (Ng et al. 2002). The similarity between every pair of points, as an input, leverages the manifold information in this clustering model. Thus similarity-based clustering methods usually show better performance than k-means algorithm. However, the performance of this kind of methods is largely determined by the similarity matrix (Huang, Nie, and Huang 2015). Any variations during the similarity measurement, such as metric, neighborhood size, and data scale, may lead to suboptimal performance.\nRecently, self-expression has been successfully utilized in subspace recovery (Elhamifar and Vidal 2009; Luo et al. 2011), low rank representation (Kang, Peng, and Cheng 2015b; Kang, Peng, and Cheng 2015a), and recommender systems (Kang and Cheng 2016). It represents each data point in terms of the other points. By solving an optimization problem, the similarity information is automatically learned from the data. This approach can not only reveal low-dimensional structure, but also be robust to noise and data scale (Huang, Nie, and Huang 2015)."}, {"heading": "Contributions", "text": "In this paper, we perform clustering built upon the idea of using samples from the data to \u201cexpress itself\u201d. Rather than local structure learning (Nie, Wang, and Huang 2014), this approach extracts the global structure of data and can be extended to kernel spaces. Unlike existing clustering algorithms that work in two separate steps, we simultaneously learn similarity matrix and cluster indicators by imposing a rank constraint on the Laplacian matrix of the learned similarity matrix. By leveraging the intrinsic interactions between learning similarity and cluster indicators, our proposed model seamlessly integrates them into a joint framework, where the result of one task is used to improve the other one. To capture the nonlinear structure information inherent in many real world data sets, we directly develop our method in a kernel space, which is well known for its ability to explore the nonlinear relation. We design an efficient algorithm to find an optimal solution to our model, and show the theoretical analysis on the connections to kernel k-means, k-means, and spectral clustering methods. ar X iv :1\n70 5.\n00 67\n8v 1\n[ cs\n.L G\n] 1\nM ay\n2 01\n7\nWhile effective, the kernel in use often has enormous influence on the performance of any kernel method. Unfortunately, the most suitable kernel for a specific task is usually unknown in advance. Exhaustive search on a user-defined pool of kernels is time-consuming and impractical when the sizes of the pool and data become large (Zeng and Cheung 2011). Thus we further propose a multiple kernel algorithm for our model. Another benefit of applying multiple kernels is that we can fully utilize information from different sources equipped with heterogeneous features (Yu et al. 2012). To alleviate the effort for kernel construction and integrating complementary information, we learn an appropriate consensus kernel from a linear combination of multiple input kernels. As a result, our joint model can simultaneously learn the similarity information, cluster indicator matrix, and the optimal combination of multiple kernels. Extensive empirical results on real-world benchmark data sets show that our method consistently outperforms other stateof-the-art methods.\nNotations. In this paper, matrices are written as upper case letters and vectors are represented by boldface lowercase letters. The i-th column and the (i, j)-th element of matrix X are denoted by Xi and xij , respectively. The `2-norm of a vector x is defined as \u2016x\u20162 = xT \u00b7 x, where T means transpose. I denotes the identity matrix and 1 denotes a column vector with all the elements as one. Tr(\u00b7) is the trace operator. 0 \u2264 Z \u2264 1 means all elements of Z are in the range [0, 1]."}, {"heading": "Clustering with Single Kernel", "text": "According to the self-expressive property (Elhamifar and Vidal 2009),\nXi \u2248 \u2211 j Xjzij , s.t. Z T i 1 = 1, 0 \u2264 zij \u2264 1, (1)\nwhere zji is the weight for j-th sample. More similar data points should receive bigger weights and the weights should be smaller for less similar points. Thus Z is also called similarity matrix, which represents the global structure of data. Note that (1) is in a similar spirit of Locally Linear Embedding (LLE) (Roweis and Saul 2000), which assumes that the data points lie on a manifold and each data point can be expressed as a linear combination of its nearest neighbors. The difference from LLE lies in the fact that we specify no neighborhood, which is automatically determined by our method.\nTo obtain Z, we solve the following problem:\nmin Z \u2016X \u2212XZ\u20162F + \u03b1\u2016Z\u20162F ,\ns.t. ZT1 = 1, 0 \u2264 Z \u2264 1, (2)\nwhere the first term is to measure reconstruction error, the second term is imposed to avoid the trivial solution Z = I , and \u03b1 is a trade-off parameter.\nOne drawback of (2) is that it assumes linear relations between samples. To recover the nonlinear relations between the data points, we extend (2) to kernel spaces by deploying a general kernelization framework (Zhang, Nie, and Xiang 2010). Define \u03c6 : RD \u2192 H to be a kernel mapping the\ndata samples from the input space to a reproducing kernel Hilbert space H. For X = [X1, \u00b7 \u00b7 \u00b7 , Xn] containing n samples, the transformation is \u03c6(X) = [\u03c6(X1), \u00b7 \u00b7 \u00b7 , \u03c6(Xn)]. The kernel similarity between data samples Xi and Xj is defined through a predefined kernel as KXi,Xj =< \u03c6(Xi), \u03c6(Xj) >. It is easy to observe that all similarities can be computed exclusively using the kernel function and one does not need to know the transformation \u03c6. This is known as the kernel trick and it greatly simplifies the computations in the kernel space when the kernels are precomputed. Then (2) becomes\nmin Z Tr(K \u2212 2KZ + ZTKZ) + \u03b1\u2016Z\u20162F s.t. ZT1 = 1, 0 \u2264 Z \u2264 1. (3)\nBy solving above problem, we learn the linear sparse relations of \u03c6(X), and thus the nonlinear relations among X . Note that (3) goes back to (2) if a linear kernel is adopted.\nIdeally, we expect that the number of connected components in Z are exactly c if the given data set X consists of c clusters (that is, Z is block diagonal with proper permutations). However, the solution Z from (3) might not satisfy this desired property. Therefore, we will add another constraint based on the following theorem (Mohar et al. 1991). Theorem 1. The multiplicity c of the eigenvalue 0 of the Laplacian matrix L of Z is equal to the number of connected components in the graph with the similarity matrix Z.\nTheorem 1 means that rank(L) = n\u2212 c if the similarity matrix Z contains exactly c connected components. Thus our new clustering model is to solve:\nmin Z Tr(K \u2212 2KZ + ZTKZ) + \u03b1\u2016Z\u20162F s.t. ZT1 = 1, 0 \u2264 Z \u2264 1, rank(L) = n\u2212 c. (4)\nProblem (4) is not easy to tackle, since L := D \u2212 Z T+Z 2 , whereD \u2208 Rn\u00d7n is a diagonal matrix with the i-th diagonal element \u2211 j zij+zji\n2 , also depends on similarity matrix Z. Here L is positive semi-definite, thus \u03c3i(L) \u2265 0, where \u03c3i(L) is the i-th smallest eigenvalue of L. rank(L) = n\u2212 c is equivalent to \u2211c i=1 \u03c3i(L) = 0. It is not easy to enforce this constraint because the optimization problem with a rank constraint is known to be of combinatorial complexity. To mitigate the difficulty, (Wang et al. 2015; Nie et al. 2016) incorporates the rank constraint into the objective function as a regularizer. Motivated by this consideration, we relax the constraint and reformulate our model as\nmin Z Tr(K \u2212 2KZ + ZTKZ) + \u03b1\u2016Z\u20162F + \u03b2 c\u2211 i=1 \u03c3i(L)\ns.t. ZT1 = 1, 0 \u2264 Z \u2264 1. (5)\nThe minimization will make the regularizer \u2211c i=1 \u03c3i(L)\u2192 0 if \u03b2 is large enough. Then the constraint rank(L) = n\u2212 c will be satisfied.\nProblem (5) is still a challenging problem because of the last term. Fortunately, it can be solved by using Ky Fan\u2019s\nTheorem (Fan 1949), i.e.,\nc\u2211 i=1 \u03c3i(L) = min PTP=I Tr(PTLP ), (6)\nwhere P \u2208 Rn\u00d7c is the indicator matrix. The c elements of i-th row Pi,: \u2208 Rc\u00d71 are the measure of the membership of data point Xi belonging to the c clusters. Finally, our model of twin learning for similarity and clustering with a single kernel (SCSK) is formulated as\nmin Z,P\nTr(K \u2212 2KZ + ZTKZ) + \u03b1\u2016Z\u20162F + \u03b2Tr(PTLP ),\ns.t. ZT1 = 1, 0 \u2264 Z \u2264 1, PTP = I. (7)\nBy solving (7), we directly obtain the indicator matrix P ; therefore, we do not need to perform spectral clustering any more. By alternatively updating Z and P , they can improve each other and optimize (7)."}, {"heading": "Optimization Algorithm", "text": "We use an alternating optimization strategy for (7). When Z is fixed, (7) becomes\nmin PTP=I\nTr(PTLP ). (8)\nThe optimal solution P is obtained by the c eigenvectors of L corresponding to the c smallest eigenvalues.\nWhen P is fixed, (7) can be reformulated column-wisely as:\nmin Zi\nKii \u2212 2Ki,:Zi + ZTi KZi + \u03b1ZTi Zi + \u03b2\n2 dTi Zi,\ns.t. ZTi 1 = 1, 0 \u2264 zij \u2264 1, (9)\nwhere di \u2208 Rn\u00d71 is a vector with the j-th element dij being dij = \u2016Pi,: \u2212 Pj,:\u20162. To obtain (9), the important equation in spectral analysis\u2211\ni,j\n1 2 \u2016Pi,: \u2212 Pj,:\u20162zij = Tr(PTLP ) (10)\nis used. (9) can be further simplified as\nmin Zi ZTi (\u03b1I +K)Zi + ( \u03b2dTi 2 \u2212 2Ki,:)Zi,\ns.t. ZTi 1 = 1, 0 \u2264 zij \u2264 1. (11)\nThis problem can be solved by many existing quadratic programing packages. The complete algorithm is outlined in Algorithm 1."}, {"heading": "Theoretical Analysis of SCSK Model", "text": "In this section, we present a theoretical analysis of SCSK and its connections to kernel k-means, k-means, and SC.\nAlgorithm 1 The algorithm of SCSK Input: Kernel matrixK, parameters \u03b1 > 0, \u03b2 > 0. Initialize: Random matrix Z."}, {"heading": "REPEAT", "text": "1: Update P , which is formed by the c eigenvectors of L = D \u2212 ZT +Z2 corresponding to the c smallest eigenvalues. 2: For each i, update the i-th column of Z by solving problem (11). UNTIL stopping criterion is met."}, {"heading": "Connection to Kernel K-means and K-means", "text": "Here we introduce a theorem which states the equivalence of SCSK and kernel k-means, k-means under some condition.\nTheorem 2. When \u03b1\u2192\u221e, the problem (4) is equivalent to kernel k-means problem.\nProof. The constraint rank(L) = n \u2212 c in (4) makes the solution Z block diagonal. Let Zi \u2208 Rni\u00d7ni denote the similarity matrix of the i-th component of Z, where ni is the number of data samples in the component. Problem (4) is equivalent to solving the following problem for each i:\nmin Zi \u2016\u03c6(Xi)\u2212 \u03c6(Xi)Zi\u20162F + \u03b1\u2016Zi\u20162F s.t. (Zi)T1 = 1, 0 \u2264 Zi \u2264 1, (12)\nwhere Xi consists of the samples corresponding to the i-th component of Z. When \u03b1 \u2192 \u221e, the above problem becomes\nmin Zi\n\u03b1\u2016Zi\u20162F\ns.t. (Zi)T1 = 1, 0 \u2264 Zi \u2264 1. (13)\nThe optimal solution is that all elements of Zi are equal to 1 ni\n. Thus when \u03b1 \u2192 \u221e, the optimal solution Z to problem\n(4) is\nzij =\n{ 1 nk , if Xi and Xj are in the same k-th component\n0, otherwise (14)\nDenote the solution set of this form by C. It is easy to observe that \u2016Z\u20162F = c. Thus (4) becomes\nmin Zi\u2208C \u2211 i \u2016\u03c6(Xi)\u2212 \u03c6(X)Zi\u20162 (15)\nIt is easy to deduce that \u03c6(X)Zi is the mean of cluster ci in the kernel space. Therefore, (15) is exactly the kernel kmeans. Thus our proposed algorithm is to solve the kernel k-means problem when \u03b1\u2192\u221e.\nCorollary 1. When \u03b1\u2192\u221e and a linear kernel is adopted, the problem (4) is equivalent to k-means problem.\nProof. It is obvious when one does not use any transformations on X in (15)."}, {"heading": "Connection to Spectral Clustering", "text": "With a predefined similarity Z, spectral clustering is to solve the following problem:\nmin PTP=I\nTr(PTLP ). (16)\nThe optimal solution P is obtained by the c eigenvectors of L corresponding to the c smallest eigenvalues. Generally, P can not be directly used for clustering since Z does not have exactly c connected components. To obtain the final clustering results, k-means or some other discretization procedures must be performed on P (Huang, Nie, and Huang 2013).\nIn our proposed algorithm, the similarity matrix Z is not predefined as the existing spectral clustering methods in the literature. Also, the similarity matrix Z is learned by taking account of the clustering task at hand, as opposed to the existing subspace clustering methods in the literature which only focus on learning the similarity matrix Z without considering the effect of clustering on Z (Peng et al. 2015). In our method, the graph with the learned Z will be partitioned into c connected components by using P . The optimal solution P is formed by the c eigenvectors of L, which is defined by Z, corresponding to the c smallest eigenvalues. Therefore, the proposed algorithm learns the similarity matrix Z and the cluster indicator matrix P simultaneously in a coupled way, which leads to a better result in real applications than existing spectral methods as shown in our experiments, since it learns an adaptive graph for clustering."}, {"heading": "Clustering with Multiple Kernels", "text": "Although model (7) can automatically learn the similarity matrix and cluster indicator matrix, its performance will largely be determined by the choice of kernel. It is often impractical to exhaustively search for the most suitable kernel. Moreover, real world data sets are often generated from different sources along with heterogeneous features. Single kernel method may not be able to fully utilize such information. Multiple kernel learning is capable of integrating complementary information and identifying a suitable kernel for a given task. Here we present a way to learn an appropriate consensus kernel from a convex combination of several predefined kernel matrices.\nSuppose there are a total number of r different kernel functions {Ki}ri=1. Correspondingly, there would be r different kernel spaces denoted as {Hi}ri=1. An augmented Hilbert space, H\u0303 = \u2295r i=1Hi, can be constructed by concatenating all kernel spaces and by using the mapping of \u03c6\u0303(x) = [ \u221a w1\u03c61(x), \u221a w2\u03c62(x), ..., \u221a wr\u03c6r(x)]\nT with different weights \u221a wi(wi \u2265 0). Then the combined kernelKw can be represented as (Zeng and Cheung 2011)\nKw(x,y) =< \u03c6w(x), \u03c6w(y) >= r\u2211 i=1 wiK i(x,y). (17)\nNote that the convex combination of the positive semidefinite kernel matrices {Ki}ri=1 is still a positive semidefinite kernel matrix. Thus the combined kernel still satisfies Mercer\u2019s condition. Then we propose our joint similarity\nlearning and clustering with multiple kernel (SCMK) model which can be written as min Z,P,w Tr(Kw\u22122KwZ+ZTKwZ)+\u03b1\u2016Z\u20162F+\u03b2Tr(PTLP ),\ns.t. ZT1 = 1, 0 \u2264 Z \u2264 1, PTP = I,\nKw = r\u2211 i=1 wiK i, r\u2211 i=1 \u221a wi = 1, wi \u2265 0.\n(18)\nBy iteratively updating Z,P,w, each of them will be adaptively refined according to the results of the other two."}, {"heading": "Optimization", "text": "Problem (18) can be solved by alternatively updating Z, P , and w, while holding the other variables as constant.\n1) Optimizing with respect to Z and P when w is fixed: We can directly calculate Kw, and the optimization problem is exactly (7). Thus we just need to use Algorithm 1 with Kw as the input kernel matrix.\n2) Optimizing with respect to w when Z and P are fixed: Solving (18) with respect to w can be rewritten as (Cai et al. 2013)\nmin w r\u2211 i=1 wihi s.t. r\u2211 i=1 \u221a wi = 1, wi \u2265 0, (19)\nwhere hi = Tr(K\ni \u2212 2KiZ + ZTKiZ). (20) The Lagrange function of (19) is\nJ (w) = wTh+ \u03b3(1\u2212 r\u2211 i=1 \u221a wi). (21)\nBy utilizing the Karush-Kuhn-Tucker (KKT) condition with \u2202J (w) \u2202wi = 0 and the constraint r\u2211 i=1 \u221a wi = 1, we obtain the solution of w as follows:\nwi = (hi r\u2211 j=1 1 hj )\u22122. (22)\nIn Algorithm 2 we provide a complete algorithm for solving (18).\nAlgorithm 2 The algorithm of SCMK Input: A set of kernel matrices {Ki}ri=1, parameters \u03b1 > 0, \u03b2 > 0. Initialize: Random matrix Z, wi = 1/r. REPEAT\n1: CalculateKw by (17). 2: Update P with the c smallest eigenvectors of L = D \u2212 ZT +Z2 . 3: For each i, update the i-th column of Z by (11). 4: Calculate h by (20). 5: Update w by (22). UNTIL stopping criterion is met."}, {"heading": "Experiments", "text": "In this section, we demonstrate the effectiveness of the proposed method on real world benchmark data sets."}, {"heading": "Data Sets", "text": "There are altogether eight benchmark data sets used in our experiments. Table 1 summarizes the statistics of these data sets. Among them, five are image ones, and the other three are text corpora1. The five image data sets consist of four commonly used face databases (ORL2, YALE3, AR4 (Martinez and Benavente 2007) and JAFFE5), and a binary alpha digits data set BA6."}, {"heading": "Experiment Setup", "text": "To assess the effectiveness of multiple kernel learning, we design 12 kernels which include: seven Gaussian kernels of the form K(x,y) = exp(\u2212\u2016x \u2212 y\u201622/(td2max)), where dmax is the maximal distance between samples and t varies over the set {0.01, 0.05, 0.1, 1, 10, 50, 100}; a linear kernel K(x,y) = xTy; four polynomial kernels K(x,y) = (a + xTy)b with a = {0, 1} and b = {2, 4}. Furthermore, all kernels are rescaled to [0, 1] by dividing each element by the largest pair-wise squared distance.\nFor single kernel methods, we run kernel k-means (KKM) (Scho\u0308lkopf, Smola, and Mu\u0308ller 1998), spectral clustering (SC) (Ng et al. 2002), robust kernel k-means (RKKM) (Du et al. 2015), and our proposed SCSK7 on each kernel sepa-\n1http://www-users.cs.umn.edu/ han/data/tmdata.tar.gz 2http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html 3http://vision.ucsd.edu/content/yale-face-database 4http://www2.ece.ohio-state.edu/ aleix/ARdatabase.html 5http://www.kasrl.org/jaffe.html 6http://www.cs.nyu.edu/ roweis/data.html 7https://github.com/sckangz/AAAI17\nrately. The methods in comparison are downloaded from the their authors\u2019 websites. And we report both the best and the average results over all these kernels.\nFor multiple kernel methods, we implement the following algorithms on a combination of above kernels.\nMKKM8. MKKM (Huang, Chuang, and Chen 2012b) extends k-means in a multiple-kernel setting. However, it uses a different way to learn the kernel weight.\nAASC9. AASC (Huang, Chuang, and Chen 2012a) extends spectral clustering to the situation where multiple affinities exist.\nRMKKM10. RMKKM (Du et al. 2015) adopts `21 norm to measure the loss of k-means.\nSCMK. Our proposed method for joint similarity learning and clustering with multiple kernels.\nFor spectral clustering like SC and AASC, we run kmeans on spectral embedding to obtain the clustering results. To reduce the influence of initialization, we follow the strategy suggested in (Yang et al. 2010; Du et al. 2015), and we repeat clustering 20 times and present the results with the best objective values. We set the number of clusters to the true number of classes for all clustering algorithms.\nTo quantitatively evaluate the clustering performance, we adopt the three widely used metrics, accuracy (Acc), normalized mutual information (NMI) (Cai et al. 2009), and Purity."}, {"heading": "Clustering Result", "text": "Table 2 shows the clustering results in terms of accuracy, NMI and Purity on all the data sets. It can be seen that the proposed SCSK and SCMK produce promising results. Especially, our method can substantially improve the performance on JAFFE, AR, BA, TR11, and TR45 data sets. The big difference between best and average results confirms the fact that the choice of kernel has a huge influence on the performance of single kernel methods. This difference motivates the development of multiple kernel learning method. Besides, multiple kernel clustering approaches usually improve the results over single kernel clustering methods.\n8http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/mkfc/code 9http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/aasc/code\n10https://github.com/csliangdu/RMKKM"}, {"heading": "Parameter Selection", "text": "There are two parameters \u03b1 and \u03b2 in our models. We let \u03b1 vary over the range of {1e-5, 1e-4, 1e-3, 0.01, 0.1, 1, 10, 100}, and \u03b2 over {1e-6, 1e-5}. Figure 1 shows how the clustering results of SCMK in terms of Acc, NMI, and Purity vary with \u03b1 and \u03b2 on JAFFE and YALE data sets. We can observe that the performance of SCMK is very stable with respect to a large range of \u03b1 values and it is more sensitive to the value of \u03b2."}, {"heading": "Conclusion", "text": "In this paper, we first propose a clustering method to simultaneously perform similarity learning and the cluster indicator matrix construction. In our method, the similarity learning and the cluster indicator learning are integrated within one framework; the method can be easily extended to kernel\nspaces, so as to capture nonlinear structure information. The connections of the proposed method to kernel k-means, kmeans, and spectral clustering are also established. To avoid extensive search of the best kernel, we further incorporate multiple kernel learning into our model. Similarity learning, cluster indicator construction, and kernel weight learning can be boosted by using the results of the other two. Extensive experiments have been conducted on real-world benchmark data sets to demonstrate the superior performance of our method."}, {"heading": "Acknowledgements", "text": "This work is supported by US National Science Foundation Grants IIS 1218712. Q. Cheng is the corresponding author."}], "references": [{"title": "Locality preserving nonnegative matrix factorization", "author": ["Cai"], "venue": "In IJCAI,", "citeRegEx": "Cai,? \\Q2009\\E", "shortCiteRegEx": "Cai", "year": 2009}, {"title": "Heterogeneous image features integration via multimodal semi-supervised learning model", "author": ["Cai"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Cai,? \\Q2013\\E", "shortCiteRegEx": "Cai", "year": 2013}, {"title": "Robust multiple kernel k-means using 2; 1-norm", "author": ["Du"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Du,? \\Q2015\\E", "shortCiteRegEx": "Du", "year": 2015}, {"title": "R", "author": ["E. Elhamifar", "Vidal"], "venue": "2009. Sparse subspace clustering. In Computer Vision and Pattern Recognition, 2009. CVPR", "citeRegEx": "Elhamifar and Vidal 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "On a theorem of weyl concerning eigenvalues of linear transformations i", "author": ["K. Fan"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "citeRegEx": "Fan,? \\Q1949\\E", "shortCiteRegEx": "Fan", "year": 1949}, {"title": "Affinity aggregation for spectral clustering", "author": ["Chuang Huang", "H.-C. Chen 2012a] Huang", "Y.-Y. Chuang", "C.-S. Chen"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Multiple kernel fuzzy clustering", "author": ["Chuang Huang", "H.-C. Chen 2012b] Huang", "Y.-Y. Chuang", "C.-S. Chen"], "venue": "IEEE Transactions on Fuzzy Systems", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Spectral rotation versus k-means in spectral clustering", "author": ["Nie Huang", "J. Huang 2013] Huang", "F. Nie", "H. Huang"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "A new simplex sparse learning model to measure data similarity for clustering", "author": ["Nie Huang", "J. Huang 2015] Huang", "F. Nie", "H. Huang"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "S", "author": ["Johnson"], "venue": "C.", "citeRegEx": "Johnson 1967", "shortCiteRegEx": null, "year": 1967}, {"title": "and Cheng", "author": ["Z. Kang"], "venue": "Q.", "citeRegEx": "Kang and Cheng 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust subspace clustering via smoothed rank approximation", "author": ["Peng Kang", "Z. Cheng 2015a] Kang", "C. Peng", "Q. Cheng"], "venue": "IEEE Signal Processing Letters", "citeRegEx": "Kang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2015}, {"title": "Robust subspace clustering via tighter rank approximation", "author": ["Peng Kang", "Z. Cheng 2015b] Kang", "C. Peng", "Q. Cheng"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Kang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2015}, {"title": "Multi-subspace representation and discovery", "author": ["Luo"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Luo,? \\Q2011\\E", "shortCiteRegEx": "Luo", "year": 2011}, {"title": "R", "author": ["A. Martinez", "Benavente"], "venue": "2007. The ar face database,", "citeRegEx": "Martinez and Benavente 2007", "shortCiteRegEx": null, "year": 1998}, {"title": "The laplacian spectrum of graphs. Graph theory, combinatorics, and applications 2(871-898):12", "author": ["Mohar"], "venue": null, "citeRegEx": "Mohar,? \\Q1991\\E", "shortCiteRegEx": "Mohar", "year": 1991}, {"title": "M", "author": ["Ng, A.Y.", "Jordan"], "venue": "I.; Weiss, Y.; et al.", "citeRegEx": "Ng et al. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "M", "author": ["F. Nie", "X. Wang", "Jordan"], "venue": "I.; and Huang, H.", "citeRegEx": "Nie et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Clustering and projected clustering with adaptive neighbors", "author": ["Wang Nie", "F. Huang 2014] Nie", "X. Wang", "H. Huang"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Nie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2014}, {"title": "Subspace clustering using log-determinant rank approximation", "author": ["Peng"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Peng,? \\Q2015\\E", "shortCiteRegEx": "Peng", "year": 2015}, {"title": "Nonnegative matrix factorization with integrated graph and feature learning", "author": ["Peng"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "Peng,? \\Q2016\\E", "shortCiteRegEx": "Peng", "year": 2016}, {"title": "L", "author": ["S.T. Roweis", "Saul"], "venue": "K.", "citeRegEx": "Roweis and Saul 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem. Neural computation 10(5):1299\u20131319", "author": ["Smola Sch\u00f6lkopf", "B. M\u00fcller 1998] Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Discriminative unsupervised dimensionality reduction", "author": ["Wang"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Wang,? \\Q2015\\E", "shortCiteRegEx": "Wang", "year": 2015}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Yang"], "venue": "IEEE Transactions on Image Processing", "citeRegEx": "Yang,? \\Q2010\\E", "shortCiteRegEx": "Yang", "year": 2010}, {"title": "J", "author": ["S. Yu", "L. Tranchevent", "X. Liu", "W. Glanzel", "Suykens"], "venue": "A.; De Moor, B.; and Moreau, Y.", "citeRegEx": "Yu et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Cheung", "author": ["H. Zeng"], "venue": "Y.-m.", "citeRegEx": "Zeng and Cheung 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A general kernelization framework for learning algorithms based on kernel", "author": ["Nie Zhang", "C. Xiang 2010] Zhang", "F. Nie", "S. Xiang"], "venue": "pca. Neurocomputing", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [], "year": 2017, "abstractText": "Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method. Introduction Clustering is a fundamental topic in data mining and machine learning (Peng et al. 2016). It partitions data points into different groups, such that the objects within a group are similar to one another and different from those in other groups. Various methods have been proposed over the past decades. Some well-known algorithms include k-means clustering (MacQueen 1967), spectral clustering (Ng et al. 2002), and hierarchical clustering (Johnson 1967). Thanks to the simplicity and the effectiveness, the kmeans algorithm is widely used. However, it fails to identify arbitrarily shaped clusters. Kernel k-means (Sch\u00f6lkopf, Smola, and M\u00fcller 1998) has been developed to capture nonlinear structure information hidden in data sets. Kernelbased learning methods requires one to specify a kernel, which means one assumes a certain shape of the underlying Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. data space. Thus the performance of kernel-based methods are largely affected by the choice of kernel. Spectral clustering does a low-dimensional embedding of the similarity matrix of the data before performing k-means clustering (Ng et al. 2002). The similarity between every pair of points, as an input, leverages the manifold information in this clustering model. Thus similarity-based clustering methods usually show better performance than k-means algorithm. However, the performance of this kind of methods is largely determined by the similarity matrix (Huang, Nie, and Huang 2015). Any variations during the similarity measurement, such as metric, neighborhood size, and data scale, may lead to suboptimal performance. Recently, self-expression has been successfully utilized in subspace recovery (Elhamifar and Vidal 2009; Luo et al. 2011), low rank representation (Kang, Peng, and Cheng 2015b; Kang, Peng, and Cheng 2015a), and recommender systems (Kang and Cheng 2016). It represents each data point in terms of the other points. By solving an optimization problem, the similarity information is automatically learned from the data. This approach can not only reveal low-dimensional structure, but also be robust to noise and data scale (Huang, Nie, and Huang 2015).", "creator": "LaTeX with hyperref package"}}}