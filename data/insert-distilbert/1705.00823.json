{"id": "1705.00823", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset", "abstract": "in recent years, automatic generation of image descriptions ( captions ), that is, image captioning, has attracted a great deal of attention. in this paper, we particularly consider generating japanese captions for images. since most available obstacle caption datasets have been constructed for every english language, there remain are few datasets for japanese. to tackle this problem, we construct a large - scale japanese image caption dataset program based on images from stair ms - coco, information which is normally called stair captions. stair captions specifically consists of 820, 310 japanese captions for 164, 062 images. in playing the experiment, we show that a neural network trained using stair captions can generate dramatically more natural and better japanese captions, compared to those generated using english - japanese machine translation after generating english captions.", "histories": [["v1", "Tue, 2 May 2017 07:07:55 GMT  (1701kb,D)", "http://arxiv.org/abs/1705.00823v1", "Accepted as ACL2017 short paper. 5 pages"]], "COMMENTS": "Accepted as ACL2017 short paper. 5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["yuya yoshikawa", "yutaro shigeto", "akikazu takeuchi"], "accepted": true, "id": "1705.00823"}, "pdf": {"name": "1705.00823.pdf", "metadata": {"source": "META", "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset", "authors": ["Yuya Yoshikawa", "Yutaro Shigeto", "Akikazu Takeuchi"], "emails": ["yoshikawa@stair.center", "shigeto@stair.center", "takeuchi@stair.center"], "sections": [{"heading": "1 Introduction", "text": "Integrated processing of natural language and images has attracted attention in recent years. The Workshop on Vision and Language held in 2011 has since become an annual event1. In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) . Image captioning is to automatically generate a caption for a given image. By improving the quality of image captioning, image search using natural sentences and image recognition support for\n1In recent years it has been held as a joint workshop such as EMNLP and ACL; https://vision.cs.hacettepe. edu.tr/vl2017/\nvisually impaired people by outputting captions as sounds can be made available. Recognizing various images and generating appropriate captions for the images necessitates the compilation of a large number of image and caption pairs. In this study, we consider generating image captions in Japanese. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. A straightforward solution is to translate English captions into Japanese ones by using machine translation such as Google Translate. However, the translated captions may be literal and unnatural because image information cannot be reflected in the translation. Therefore, in this study, we construct a Japanese image caption dataset, and for given images, we aim to generate more natural Japanese captions than translating the generated English captions into the Japanese ones. The contributions of this paper are as follows:\n\u2022 We constructed a large-scale Japanese image caption dataset, STAIR Captions, which consists of Japanese captions for all the images in MS-COCO (Lin et al., 2014) (Section 3).\n\u2022 We confirmed that quantitatively and qualitatively better Japanese captions than the ones translated from English captions can be generated by applying a neural network-based image caption generation model learned on STAIR Captions (Section 5).\nSTAIR Captions is available for download from http://captions.stair.center."}, {"heading": "2 Related Work", "text": "Some English image caption datasets have been proposed (Krishna et al., 2016; Kuznetsova et al., 2013; Ordonez et al., 2011; Vedantam et al.,\nar X\niv :1\n70 5.\n00 82\n3v 1\n[ cs\n.C L\n] 2\nM ay\n2 01\n7\n2015; Isola et al., 2014). Representative examples are PASCAL (Rashtchian et al., 2010), Flickr3k (Rashtchian et al., 2010; Hodosh et al., 2013), Flickr30k (Young et al., 2014) \u2014an extension of Flickr3k\u2014, and MS-COCO (Microsoft Common Objects in Context) (Lin et al., 2014). As detailed in Section 3, we annotate Japanese captions for the images in MS-COCO. Note that when annotating the Japanese captions, we did not refer to the original English captions inMS-COCO. MS-COCO is a dataset constructed for research on image classification, object recognition, andEnglish caption generation. Since its release, MSCOCO has been used as a benchmark dataset for image classification and caption generation. In addition, many studies have extended MS-COCO by annotating additional information about the images in MS-COCO2. Recently, a few caption datasets in languages other than English have been constructed (Miyazaki and Shimizu, 2016; Grubinger et al., 2006; Elliott et al., 2016). In particular, the study of Miyazaki and Shimizu (2016) is closest to the present study. As in our study, they constructed a Japanese caption dataset called YJ Captions. The main difference between STAIR Captions and YJ Captions is that STAIR Captions provides Japanese captions for a greater number of images. In Section 3, we highlight this difference by comparing the statistics of STAIR Captions and YJ Captions."}, {"heading": "3 STAIR Captions", "text": ""}, {"heading": "3.1 Annotation Procedure", "text": "This section explains how we constructed STAIR Captions. We annotated all images (164,062 images) in the 2014 edition of MS-COCO. For each image, we provided five Japanese captions. Therefore, the total number of captions was 820,310. Following the rules for publishing datasets created based on MS-COCO, the Japanese captions we created for the test images are excluded from the public part of STAIR Captions. To annotate captions efficiently, we first developed a web system for caption annotation. Figure 1 shows the example of the annotation screen in the web system. Each annotator looks at the displayed image and writes the corresponding Japanese description in the text box under the image. By\n2http://mscoco.org/external/\npressing the send (\u9001\u4fe1) button, a single task is completed and the next task is started. To concurrently and inexpensively annotate captions by using the above web system, we asked part-time job workers and crowd-sourcing workers to perform the caption annotation. The workers annotated the images based on the following guidelines. (1) A caption must contain more than 15 letters. (2) A caption must follow the da/dearu style (one of writing styles in Japanese). (3) A caption must describe only what is happening in an image and the things displayed therein. (4) A caption must be a single sentence. (5) A caption must not include emotions or opinions about the image. To guarantee the quality of the captions created in this manner, we conducted sampling inspection of the annotated captions, and the captions not in line with the guidelines were removed. The entire annotation work was completed by about 2,100 workers in about half a year."}, {"heading": "3.2 Statistics", "text": "This section introduces the quantitative characteristics of STAIR Captions. In addition, we compare it to YJ Captions (Miyazaki and Shimizu, 2016), a dataset with Japanese captions for the images in MS-COCO like in STAIR Captions. Table 1 summarizes the statistics of the datasets. Compared with YJ Captions, overall, the numbers of Japanese captions and images in STAIR Captions are 6.23x and 6.19x, respectively. In the public part of STAIR Captions, the numbers of images and Japanese captions are 4.65x and 4.67x greater than those in YJ Captions, respectively. That the numbers of images and captions are large in STAIR Captions is an important point in image caption\ngeneration because it reduces the possibility of unknown scenes and objects appearing in the test images. The vocabulary of STAIR Captions is 2.69x larger than that of YJ Captions. Because of the large vocabulary of STAIR Captions, it is expected that the caption generation model can learn and generate a wide range of captions. The average numbers of characters per a sentence in STAIR Captions and in YJ Captions are almost the same."}, {"heading": "4 Image Caption Generation", "text": "In this section, we briefly review the caption generation method proposed by Karpathy and Fei-Fei (2015), which is used in our experiments (Section 5). This method consists of a convolutional neural network (CNN) and long short-term memory (LSTM)3. Specifically, CNN first extracts features from a given image, and then, LSTM generates a caption from the extracted features. Let I be an image, and the corresponding caption be Y = (y1, y2, \u00b7 \u00b7 \u00b7 , yn). Then, caption generation is defined as follows:\nx(im) = CNN(I), h0 = tanh ( W(im)x(im) + b(im) ) ,\nc0 = 0, ht, ct = LSTM (xt, ht\u22121, ct\u22121) (t \u2265 1),\nyt = softmax (Woht + bo) ,\nwhere CNN(\u00b7) is a function that outputs the image features extracted by CNN, that is, the final layer of CNN, and yt is the tth output word. The input xt at time t is substituted by a word embedding vector corresponding to the previous output, that is, yt\u22121. The generation process is repeated until LSTM outputs the symbol that indicates the end of sentence.\n3 Although their original paper used RNN, they reported in the appendix that LSTM performed better than RNN. Thus, we used LSTM.\nIn the training phase, given the training data, we train W(im), b(im),W\u2217, b\u2217, CNN, and LSTM parameters, where \u2217 represents wild card."}, {"heading": "5 Experiments", "text": "In this section, we perform an experiment which generates Japanese captions using STAIR Captions. The aim of this experiment is to show the necessity of a Japanese caption dataset. In particular, we evaluate quantitatively and qualitatively how fluent Japanese captions can be generated by using a neural network-based caption generation model trained on STAIR Captions."}, {"heading": "5.1 Experimental Setup", "text": ""}, {"heading": "5.1.1 Evaluation Measure", "text": "Following the literature (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015) as evaluation measures. Although BLEU and ROUGE were developed originally for evaluating machine translation and text summarization, we use them here because they are often used for measuring the quality of caption generation."}, {"heading": "5.1.2 Comparison Methods", "text": "In this experiment, we evaluate the following caption generation methods.\n\u2022 En-generator \u2192 MT: A pipeline method of English caption generation and EnglishJapanese machine translation. This method trains a neural network, which generates English captions, with MS-COCO. In the test phase, given an image, we first generate an English caption to the image by the trained neural network, and then translate the generated caption into Japanese one by machine translation. Here, we use Google translate4 for machine translation. This method is the baseline.\n\u2022 Ja-generator: This method trains a neural network using STAIR Captions. Unlike MSCOCO \u2192 MT, this method directly generate a Japanese caption from a given image .\nAs mentioned in Section 4, we used the method proposed by Karpathy and Fei-Fei (2015) as caption generation models for both En-generator \u2192 MT and Ja-generator.\n4https://translate.google.com/\nTable 3: Examples of generated image captions. En-generator denotes the caption generator trained with MS-COCO. En-generator \u2192 MT is the pipeline method: it first generates English caption and performs machine translation subsequently. Ja-generator was trained with Japanese captions.\nEn-generator: A double decker bus driving down a street. En-generator\u2192MT: \u30b9\u30c8\u30ea\u30fc\u30c8\u3092\u904b\u8ee2\u3059\u308b\u4e8c\u91cd\u30c7\u30c3\u30ab\u30fc\u30d0\u30b9\u3002 Ja-generator: \u4e8c\u968e\u5efa\u3066\u306e\u30d0\u30b9\u304c\u9053\u8def\u3092\u8d70\u3063\u3066\u3044\u308b\u3002\nEn-generator: A bunch of food that are on a table. En-generator\u2192MT: \u30c6\u30fc\u30d6\u30eb\u306e\u4e0a\u306b\u3042\u308b\u98df\u3079\u7269\u306e\u675f\u3002 Ja-generator: \u30c9\u30fc\u30ca\u30c4\u304c\u305f\u304f\u3055\u3093\u4e26\u3093\u3067\u3044\u308b\u3002\nIn both the methods, following Karpathy and Fei-Fei, we only trained LSTM parameters, while CNN parameters were fixed. We used VGG with 16 layers as CNN, where the VGG parameters were the pre-trained ones5. With the optimization of LSTM, we used mini-batch RMSProp, and the batch size was set to 20."}, {"heading": "5.1.3 Dataset Separation", "text": "Following the experimental setting in the previous studies (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we used 123,287 images included in the MS-COCO training and validation sets and their corresponding Japanese captions. We divided the dataset into three parts, i.e., 113,287 images for the training set, 5,000 images for the validation set, and 5,000 images for the test set. The hyper-parameters of the neural network were tuned based on CIDEr scores by using the validation set. As preprocessing, we applied morphological analysis to the Japanese captions using MeCab6.\n5http://www.robots.ox.ac.uk/~vgg/research/ very_deep/\n6http://taku910.github.io/mecab/"}, {"heading": "5.2 Results", "text": "Table 2 summarizes the experimental results. The results show that Ja-generator, that is, the approach in which Japanese captions were used as training data, outperformed En-generator \u2192 MT, which was trained without Japanese captions. Table 3 shows two examples where Ja-generator generated appropriate captions, whereas Engenerator \u2192 MT generated unnatural ones. In the example at the top in Table 3, En-generator first generated the term, \u201cA double decker bus.\u201d MT translated the term into as \u201c\u4e8c\u91cd\u30c7\u30c3\u30ab\u30fc\u30d0 \u30b9\u201d, but the translation is word-by-word and inappropriate as a Japanese term. By contrast, Jagenerator generated \u201c\u4e8c\u968e\u5efa\u3066\u306e\u30d0\u30b9 (two-story bus),\u201d which is appropriate as the Japanese translation of A double decker bus. In the example at the bottom of the table, En-generator \u2192 MT yielded the incorrect caption by translating \u201cA bunch of food\u201d as \u201c\u98df\u3079\u7269\u306e\u675f (A bundle of food).\u201d By contrast, Ja-generator correctly recognized that the food pictured in the image is a donut, and expressed it as \u201c\u30c9\u30fc\u30ca\u30c4\u304c\u305f\u304f\u3055\u3093 (A bunch of donuts).\u201d"}, {"heading": "6 Conclusion", "text": "In this paper, we constructed a new Japanese image caption dataset called STAIR Captions. In STAIR Captions, Japanese captions are provided for all the images of MS-COCO. The total number of Japanese captions is 820,310. To the best of our knowledge, STAIRCaptions is currently the largest Japanese image caption dataset. In our experiment, we compared the performance of Japanese caption generation by a neural network-based model with and without STAIR Captions to highlight the necessity of Japanese captions. As a result, we showed the necessity of STAIR Captions. In addition, we confirmed that Japanese captions can be generated simply by adapting the existing caption generation method. In future work, we will analyze the experimental results in greater detail. Moreover, by using both Japanese and English captions, we will develop multi-lingual caption generation models."}], "references": [{"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C. Lawrenc Zitnick."], "venue": "arXiv preprint 1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "Proceedings of the IEEE", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Multi30k: Multilingual english-german image descriptions", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": "In Workshop on Vision and Language", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "The IAPR Benchmark: A new evaluation resource for visual information systems", "author": ["Michael Grubinger", "Paul Clough", "Henning M\u00fcller", "Thomas Deselaers."], "venue": "Proceedings of the Fifth International Conference on Language Resources and Evaluation", "citeRegEx": "Grubinger et al\\.,? 2006", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "What makes a photograph memorable", "author": ["Phillip Isola", "Jianxiong Xiao", "Devi Parikh", "Antonio Torralba"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Isola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2014}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). pages 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Fei-Fei Li"], "venue": null, "citeRegEx": "Li.,? \\Q2016\\E", "shortCiteRegEx": "Li.", "year": 2016}, {"title": "Generalizing image captions for image-text parallel corpus", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Alexander Berg", "Tamara Berg", "Yejin Choi."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL). pages", "citeRegEx": "Kuznetsova et al\\.,? 2013", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2013}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Proceedings of the Workshop on Text Summarization Branches Out. pages 25\u201326.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Microsoft COCO: Common objects in context", "author": ["TY Lin", "M Maire", "S Belongie", "J Hays", "P Perona."], "venue": "European Conference on Computer Vision (ECCV). pages 740\u2013755.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (M-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Crosslingual image caption generation", "author": ["TakashiMiyazaki andNobuyuki Shimizu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). pages 1780\u20131790.", "citeRegEx": "Shimizu.,? 2016", "shortCiteRegEx": "Shimizu.", "year": 2016}, {"title": "Im2Text: Describing images using 1 million captioned dhotographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 1143\u2013 1151.", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL). pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Collecting image annotations using Amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier."], "venue": "NAACL HLT Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. pages 139\u2013", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["RamakrishnaVedantam", "C. Lawrence Zitnick", "andDevi Parikh"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "RamakrishnaVedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "RamakrishnaVedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics 2:67\u201378.", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) .", "startOffset": 156, "endOffset": 246}, {"referenceID": 1, "context": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) .", "startOffset": 156, "endOffset": 246}, {"referenceID": 17, "context": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) .", "startOffset": 156, "endOffset": 246}, {"referenceID": 11, "context": "In this research area, methods to automatically generate image descriptions (captions), that is, image captioning, have attracted a great deal of attention (Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Vinyals et al., 2015; Mao et al., 2015) .", "startOffset": 156, "endOffset": 246}, {"referenceID": 10, "context": "\u2022 We constructed a large-scale Japanese image caption dataset, STAIR Captions, which consists of Japanese captions for all the images in MS-COCO (Lin et al., 2014) (Section 3).", "startOffset": 145, "endOffset": 163}, {"referenceID": 15, "context": "Representative examples are PASCAL (Rashtchian et al., 2010), Flickr3k (Rashtchian et al.", "startOffset": 35, "endOffset": 60}, {"referenceID": 15, "context": ", 2010), Flickr3k (Rashtchian et al., 2010; Hodosh et al., 2013), Flickr30k (Young et al.", "startOffset": 18, "endOffset": 64}, {"referenceID": 4, "context": ", 2010), Flickr3k (Rashtchian et al., 2010; Hodosh et al., 2013), Flickr30k (Young et al.", "startOffset": 18, "endOffset": 64}, {"referenceID": 18, "context": ", 2013), Flickr30k (Young et al., 2014) \u2014an extension of Flickr3k\u2014, and MS-COCO (Microsoft Common Objects in Context) (Lin et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 10, "context": ", 2014) \u2014an extension of Flickr3k\u2014, and MS-COCO (Microsoft Common Objects in Context) (Lin et al., 2014).", "startOffset": 86, "endOffset": 104}, {"referenceID": 3, "context": "Recently, a few caption datasets in languages other than English have been constructed (Miyazaki and Shimizu, 2016; Grubinger et al., 2006; Elliott et al., 2016).", "startOffset": 87, "endOffset": 161}, {"referenceID": 2, "context": "Recently, a few caption datasets in languages other than English have been constructed (Miyazaki and Shimizu, 2016; Grubinger et al., 2006; Elliott et al., 2016).", "startOffset": 87, "endOffset": 161}, {"referenceID": 2, "context": ", 2006; Elliott et al., 2016). In particular, the study of Miyazaki and Shimizu (2016) is closest to the present study.", "startOffset": 8, "endOffset": 87}, {"referenceID": 6, "context": "In this section, we briefly review the caption generation method proposed by Karpathy and Fei-Fei (2015), which is used in our experiments (Section 5).", "startOffset": 77, "endOffset": 105}, {"referenceID": 0, "context": "Following the literature (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we use BLEU (Papineni et al.", "startOffset": 25, "endOffset": 72}, {"referenceID": 6, "context": "Following the literature (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we use BLEU (Papineni et al.", "startOffset": 25, "endOffset": 72}, {"referenceID": 14, "context": ", 2015; Karpathy and Fei-Fei, 2015), we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and CIDEr (Vedantam et al.", "startOffset": 49, "endOffset": 72}, {"referenceID": 9, "context": ", 2002), ROUGE (Lin, 2004), and CIDEr (Vedantam et al.", "startOffset": 15, "endOffset": 26}, {"referenceID": 6, "context": "As mentioned in Section 4, we used the method proposed by Karpathy and Fei-Fei (2015) as caption generation models for both En-generator \u2192 MT and Ja-generator.", "startOffset": 58, "endOffset": 86}, {"referenceID": 0, "context": "Following the experimental setting in the previous studies (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we used 123,287 images included in the MS-COCO training and validation sets and their corresponding Japanese captions.", "startOffset": 59, "endOffset": 106}, {"referenceID": 6, "context": "Following the experimental setting in the previous studies (Chen et al., 2015; Karpathy and Fei-Fei, 2015), we used 123,287 images included in the MS-COCO training and validation sets and their corresponding Japanese captions.", "startOffset": 59, "endOffset": 106}], "year": 2017, "abstractText": "In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIRCaptions. STAIRCaptions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.", "creator": "LaTeX with hyperref package"}}}