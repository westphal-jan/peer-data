{"id": "1705.02012", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2017", "title": "Machine Comprehension by Text-to-Text Neural Question Generation", "abstract": "we propose a recurrent neural model that generates natural - time language questions from documents, conditioned on answers. we show how to train the model best using a combination view of supervised and reinforcement learning. after teacher forcing for standard maximum likelihood training, we fine - tune the model instead using policy gradient techniques to maximize several rewards that measure question quality. most notably, one of these rewards is in the performance of a question - answering system. we motivate question generation systems as a common means available to improve the performance ratings of question answering systems. our model is externally trained and evaluated on the recent question - answering dataset squad.", "histories": [["v1", "Thu, 4 May 2017 20:58:06 GMT  (142kb)", "http://arxiv.org/abs/1705.02012v1", null], ["v2", "Mon, 15 May 2017 14:47:05 GMT  (51kb)", "http://arxiv.org/abs/1705.02012v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xingdi yuan", "tong wang", "caglar gulcehre", "alessandro sordoni", "philip bachman", "sandeep subramanian", "saizheng zhang", "adam trischler"], "accepted": false, "id": "1705.02012"}, "pdf": {"name": "1705.02012.pdf", "metadata": {"source": "CRF", "title": "Machine Comprehension by Text-to-Text Neural Question Generation", "authors": ["Xingdi Yuan", "Tong Wang", "Caglar Gulcehre", "Alessandro Sordoni", "Philip Bachman", "Sandeep Subramanian", "Saizheng Zhang", "Adam Trischler"], "emails": ["eric.yuan@microsoft.com", "tong.wang@microsoft.com", "alsordon@microsoft.com", "phbachma@microsoft.com", "adam.trischler@microsoft.com", "gulcehrc@iro.umontreal.ca", "sandeep.subramanian@gmail.com", "saizheng.zhang@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "People ask questions to improve their knowledge and understanding of the world. Questions can be used to access the knowledge of others or to direct one\u2019s own information-seeking behavior. Here we study the generation of natural-language questions by machines, based on text passages. This task is synergistic with machine comprehension (MC), which pursues the understanding of written language by machines at a near-human level. Because most human knowledge is recorded in text, this would enable transformative applications.\nMany machine comprehension datasets have been released recently. These generally comprise (document, question, answer) triples (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016), where the goal is to predict an answer, conditioned on a document and question. The availability of large labeled datasets has spurred development of increasingly advanced models for question answering (QA) from text (Kadlec et al., 2016; Seo et al., 2016; Wang et al., 2016; Shen et al., 2016).\nIn this paper we reframe the standard MC task: rather than answering questions about a document, we teach machines to ask questions. Our work has several motivations. First, we believe that posing appropriate questions is an important aspect of information acquisition in intelligent systems. Second, learning to ask questions may improve the ability to answer them. Singer and Donlan (1982) demonstrated that having students devise questions before reading can increase scores on subsequent comprehension tests. Third, answering the questions in most existing QA datasets is an extractive task \u2013 it requires selecting some span of text within the document \u2013 while question asking\n\u2020 These authors contributed equally. \u2021 These authors were supported by funding from Microsoft Maluuba.\nis comparatively abstractive \u2013 it requires generation of text that may not appear in the document. Fourth, asking good questions involves skills beyond those used to answer them. For instance, in existing QA datasets, a typical (document, question) pair specifies a unique answer. Conversely, a typical (document, answer) pair may be associated with multiple questions, since a valid question can be formed from any information or relations which uniquely specify the given answer. Finally, a mechanism to ask informative questions about documents (and eventually answer them) has many practical applications, e.g.: generating training data for question answering (Serban et al., 2016; Yang et al., 2017), synthesising frequently asked question (FAQ) documentation, and automatic tutoring systems (Lindberg et al., 2013).\nWe adapt the sequence-to-sequence approach of Cho et al. (2014) for generating questions, conditioned on a document and answer: first we encode the document and answer, then output question words sequentially with a decoder that conditions on the document and answer encodings. We augment the standard encoder-decoder approach with several modifications geared towards the question generation task. During training, in addition to maximum likelihood for predicting questions from (document, answer) tuples, we use policy gradient optimization to maximize several auxiliary rewards. These include a language-model-based score for fluency and the performance of a pretrained question-answering model on generated questions. We show quantitatively that policy gradient increases the rewards earned by generated questions at test time, and provide examples to illustrate the qualitative effects of different training schemes. To our knowledge, we present the first end-to-end, text-to-text model for question generation."}, {"heading": "2 Related Work", "text": "Recently, automatic question generation has received increased attention from the research community. It has been harnessed, for example, as a means to build automatic tutoring systems (Heilman and Smith, 2010; Ali et al., 2010; Lindberg et al., 2013; Labutov et al., 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al., 2011), and to enrich training data for question-answering systems (Serban et al., 2016; Yang et al., 2017).\nSeveral earlier works process documents as individual sentences using syntactic (Heilman and Smith, 2010; Ali et al., 2010; Kumar et al., 2015) or semantic-based parsing (Mannem et al., 2010; Lindberg et al., 2013), then reformulate questions using hand-crafted rules acting on parse trees. These traditional approaches generate questions with a high word overlap with the original text that pertain specifically to the given sentence by re-arranging the sentence parse tree. An alternative approach is to use generic question templates whose slots can be filled with entities from the document (Lindberg et al., 2013; Chali and Golestanirad, 2016). Labutov et al. (2015), for example, use ontology-derived templates to generate high-level questions related to larger portions of the document. These approaches comprise pipelines of independent components that are difficult to tune for final performance measures.\nMore recently, neural networks have enabled end-to-end training of question generation systems. Serban et al. (2016) train a neural system to convert knowledge base (KB) triples into natural-language questions. The head and the relation form a context for the question and the tail serves as the answer.\nSimilarly, we assume that the answer is known a priori, but we extend the context to encompass a span of unstructured text. Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system. The main goal of their question generation is data augmentation, thus questions themselves are not evaluated. In contrast, our work focuses primarily on developing a neural model for question generation that could be applied to a variety of downstream tasks that includes question answering.\nOur model shares similarities with recent end-to-end neural QA systems, e.g. Seo et al. (2016); Wang et al. (2016). I.e., we use an encoder-decoder structure, where the encoder processes answer and document (instead of question and document) and our decoder generates a question (instead of an answer). While existing question answering systems typically extract the answer from the document, our decoder is a fully generative model.\nFinally, we relate the recent body of works that apply reinforcement learning to natural language generation, such as Li et al. (2016); Ranzato et al. (2016); Kandasamy and Bachrach (2017); Zhang and Lapata (2017). We similarly apply a REINFORCE-style (Williams, 1992) algorithm to maximize various rewards earned by generated questions."}, {"heading": "3 Encoder-Decoder Model for Question Generation", "text": "We adapt the simple encoder-decoder architecture first outlined by Cho et al. (2014) to the question generation problem. In particular, we base our model on the attention mechanism of Bahdanau et al. (2015) and the pointer-softmax copying mechanism of Gulcehre et al. (2016). In question generation, we can condition our encoder on two different sources of information (compared to the single source in neural machine translation (NMT)): a document that the question should be about and an answer that should fit the generated question. Next, we describe how we adapt the encoder and decoder architectures in detail."}, {"heading": "3.1 Encoder", "text": "Our encoder is a neural model acting on two input sequences: the document, D = (d1, . . . , dn) and the answer, A = (a1, . . . , am). Sequence elements di, aj \u2208 R\nDe are given by embedding vectors (Bengio et al., 2001).\nIn the first stage of encoding, similar to current question answering systems, e.g. (Seo et al., 2016), we augment each document word embedding with a binary feature that indicates if the document word belongs to the answer. Then, we run a bidirectional long short-term memory (Hochreiter and Schmidhuber, 1997) (LSTM) network on the augmented document sequence, producing annotation vectors hd = (hd1, . . . ,h d n). Here, h d i \u2208 R Dh is the concatenation of the network\u2019s forward ( hdi ) and backward hidden states ( hdi ) for input token i, i.e., h d i = [ hdi ; hdi ]. 1\nOur model operates on QA datasets where the answer is extractive; thus, we encode the answer A using the annotation vectors corresponding to the answer word positions in the document. We assume that, without loss of generality, A consists of the sequence of words (ds, . . . , de) in the document, s.t. 1 \u2264 s \u2264 e \u2264 n. We concatenate the annotation sequence (hds , . . . ,h d e) with the corresponding answer word embeddings (as, . . . , ae), i.e., [h d j ; aj ], s \u2264 j \u2264 e, then apply a second bidirectional LSTM (biLSTM) over the resulting sequence of vectors to obtain the extractive condition encoding ha \u2208 RDh . We form ha by concatenating the final hidden states from each direction of the biLSTM.\nWe also compute an initial state s0 \u2208 R Ds for the decoder using the annotation vectors and the extractive condition encoding:\nr = Lha + 1\nn\n|D|\u2211\ni\nhdi , s0 = tanh (W0r+ b0) ,\nwhere L \u2208 RDh\u00d7Dh , W0 \u2208 R Ds\u00d7Dh , and b0 \u2208 R Ds are parameters.2\n1We use the notation [\u00b7; \u00b7] to denote concatenation of two vectors throughout the paper. 2Let |X| denote the length of sequence X ."}, {"heading": "3.2 Decoder", "text": "Our decoder is a neural model that generates outputs yt sequentially. At each time-step t, the decoder models a conditional distribution parametrized by \u03b8,\np\u03b8(yt|y<t, D,A), (1)\nwhere y<t represents the outputs at earlier time-steps. In question generation, output yt is a word sampled according to (1).\nWhen formulating questions based on documents, it is common to refer to phrases and entities that appear directly in the text. We therefore incorporate into our decoder a mechanism for copying relevant words from D. We use the pointer-softmax formulation (Gulcehre et al., 2016), which has two output layers: the shortlist softmax and the location softmax. The shortlist softmax induces a distribution over words in a predefined output vocabulary. The location softmax is a pointer network (Vinyals et al., 2015) that induces a distribution over document tokens to be copied. A source switching network enables the model to interpolate between these distributions.\nIn more detail, the decoder is a recurrent neural network. Its internal state, st \u2208 R Ds , updates according to the long short-term memory function (Hochreiter and Schmidhuber, 1997), i.e.,\nst = LSTM(st\u22121, yt\u22121,vt), (2)\nwhere vt is a the context vector computed from the document and answer encodings.\nAt every time-step t, the model computes a soft-alignment score over the document to decide which words are more relevant to the question being generated. As in a traditional NMT architecture, the decoder computes a relevance weight \u03b1tj for every jth word in the document when generating the tth word in the question. Alignment score vector \u03b1t \u2208 R |D| is computed with a single layer feedforward neural network f(\u00b7) using the tanh(\u00b7) activation function. The scores \u03b1t are also used as the location softmax distribution. The network defined by f(\u00b7) computes energies according to (3) for the alignments, and the normalized alignments \u03b1tj are computed as in (4):\netj = f(h d j , h a, yt, st\u22121), (3)\n\u03b1tj = exp(etj)\u2211T i=1 exp(eij) . (4)\nTo compute the context vector vt used in (2), we first construct context vector ct for the document and then concatenate it with ha:\nct =\n|D|\u2211\ni=1\n\u03b1tih d i , (5)\nvt = [ct;h a]. (6)\nWe use a deep output layer (Pascanu et al., 2013) at each time-step for the shortlist softmax vector ot. This layer fuses the information coming from st, vt and yt\u22121 through a simple MLP to predict the word logits for the softmax as in (7). Parameters of the softmax layer are denoted as Wo \u2208 R |V |\u00d7Dh and bo \u2208 R |V |, where |V | is the size of the shortlist vocabulary (2000 words herein).\net = g(st,vt, yt\u22121)\not = softmax(Woet + bo) (7)\nA source switching variable zt enables the model to interpolate between document copying and generation from shortlist. It is computed by an MLP with two hidden layers using tanh units (Gulcehre et al., 2016). Similarly to the computation of the shortlist softmax, the switching network takes st, vt and yt\u22121 as inputs. Its output layer generates the scalar zt through the logistic sigmoid activation function.\nFinally, p\u03b8(yt|y<t, D,A) is approximated by the full pointer-softmax pt \u2208 R |V |+|D| by concatenating ot and \u03b1t after both are weighted by zt:\npt = [ztot; (1 \u2212 zt)\u03b1t]. (8)\nAs is standard in NMT, during decoding we use a beam search (Graves, 2012) to maximize (approximately) the conditional probability of an output sequence. We discuss this in more detail in the following section."}, {"heading": "3.3 Training", "text": "The model is trained initially to minimize the negative log-likelihood of the training data under the model distribution,\nL = \u2212 \u2211\nt\nlog p\u03b8(yt|y<t, D,A), (9)\nwhere, in the decoder as defined in (2), the previous token yt\u22121 comes from the source sequence rather than the model output (this is called teacher forcing).\nBased on our knowledge of the task, we introduce additional training signals to aid the model\u2019s learning. First we encourage the model not to generate answer words in the question. We use the soft answer-suppression constraint given in (10) with the penalty hyperparameter \u03bbs; A\u0304 denotes the set of words that appear in the answer but not in the ground-truth question:\nLs = \u03bbs \u2211\nt\n\u2211\na\u0304\u2208A\u0304\np\u03b8(yt = a\u0304|y<t, D,A). (10)\nWe also encourage variety in the output words to counteract the degeneracy often observed in NLG systems towards common outputs (Sordoni et al., 2015). This is achieved with a loss term that maximizes entropy in the output softmax (8), i.e.,\nLe = \u03bbe \u2211\nt\npTt logpt. (11)"}, {"heading": "4 Policy Gradient Optimization", "text": "As described above, we use teacher forcing to train our model to generate text by maximizing ground-truth likelihood. Teacher forcing introduces critical differences between the training phase (in which the model is driven by ground-truth sequences) and the testing phase (in which the model is driven by its own outputs) (Bahdanau et al., 2016). Significantly, teacher forcing prevents the model from making and learning from mistakes during training. This is related to the observation that maximizing ground-truth likelihood does not teach the model how to distribute probability mass among examples other than the ground-truth, some of which may be valid questions and some of which may be completely incoherent. This is especially problematic in language, where there are often many ways to say the same thing. A reinforcement learning (RL) approach, by which a model is rewarded or penalized for its own actions, could mitigate these issues \u2013 though likely at the expense of reduced stability during training. A properly designed reward, maximized via RL, could provide a model with more information about how to distribute probability mass among sequences that do not occur in the training set (Norouzi et al., 2016).\nWe investigate the use of RL to fine-tune our question generation model. Specifically, we perform policy gradient optimization following a period of \u201cpretraining\u201d on maximum likelihood, using a combination of scalar rewards correlated to question quality. We detail this process below. To make clear that the model is acting freely without teacher forcing, we indicate model-generated tokens with\ny\u0302t and sequences with Y\u0302 ."}, {"heading": "4.1 Rewards", "text": "Question answering (QA) One obvious measure of a question\u2019s quality is whether it can be answered correctly given the context document D. We therefore feed model-generated questions into a pretrained question-answering system and use that system\u2019s accuracy as a reward. We use the recently proposed Multi-Perspective Context Matching (MPCM) (Wang et al., 2016) model as our reference QA system, sans character-level encoding. Broadly, that model takes in a generated\nquestion Y\u0302 and a document D, processes them through bidirectional recurrent neural networks, applies an attention mechanism, and points to the start and end tokens of the answer in D. After\ntraining a MPCM model on the SQuAD dataset, the reward RQA(Y\u0302 ) is given by MPCM\u2019s answer\naccuracy on Y\u0302 in terms of the F1 score, a token-based measure proposed by Rajpurkar et al. (2016) that accounts for partial word matches:\nRQA(Y\u0302 ) = F1(A\u0302, A), (12)\nwhere A\u0302 = MPCM(Y\u0302 ) is the answer to the generated question by the MPCM model. Optimizing the QA reward could lead to \u2018friendly\u2019 questions that are either overly simplistic or that somehow cheat by exploiting quirks in the MPCM model. One obvious way to cheat would be to inject answer words into the question. We prevented this by masking these out in the location softmax, a hard version of the answer suppression loss (10).\nFluency (PPL) Another measure of quality is a question\u2019s fluency \u2013 i.e., is it stated in proper, grammatical English? As simultaneously proposed in Zhang and Lapata (2017), we use a language model to measure and reward the fluency of generated questions. In particular, we use the perplexity\nassigned to Y\u0302 by an LSTM language model:\nRPPL(Y\u0302 ) = \u22122 \u2212\n1 T\n\u2211 T\nt=1 log 2 pLM(y\u0302t|y\u0302<t), (13)\nwhere the negation is to reward the model for minimizing perplexity. The language model is trained through maximum likelihood estimation on over 80, 000 human-generated questions from SQuAD (the training set).\nCombination For the total scalar reward earned by the word sequence Y\u0302 , we also test a weighted combination of the individual rewards:\nRPPL + QA(Y\u0302 ) = \u03bbQARQA(Y\u0302 ) + \u03bbPPLRPPL(Y\u0302 ),\nwhere \u03bbQA and \u03bbPPL are hyperparameters. The individual reward functions use neural models to tune the neural question generator. This is reminiscent of recent work on GANs (Goodfellow et al., 2014) and actor-critic methods (Bahdanau et al., 2016). We treat the reward models as black boxes, rather than attempting to optimize them jointly or backpropagate error signals through them. We leave these directions for future work.\nWe also experimented with several other rewards, most notably the BLEU score (Papineni et al.,\n2002) between Y\u0302 and the ground-truth question for the given document and answer, and a softer measure of similarity between output and ground-truth based on skip-thought vectors (Kiros et al., 2015). Empirically, we were unable to obtain consistent improvements on these rewards through training, though this may be an issue with hyperparameter settings."}, {"heading": "4.2 REINFORCE", "text": "We use the REINFORCE algorithm (Williams, 1992) to maximize the model\u2019s expected reward. For\neach generated question Y\u0302 , we define the loss\nLRL = \u2212EY\u0302\u223c\u03c0(Y\u0302 |D,A)[R(Y\u0302 )], (14)\nwhere \u03c0 is the policy to be trained. The policy is a distribution over discrete actions, i.e. words y\u0302t that make up the sequence Y\u0302 . It is the distribution induced at the output layer of the encoder-decoder model (8), initialized with the parameters determined through likelihood optimization.3\nREINFORCE approximates the expectation in (14) with independent samples from the policy distribution, yielding the policy gradient\n\u2207LRL \u2248 \u2211\nt=1\n\u2207 log \u03c0(y\u0302t|y\u0302<t, D,A) R(Y\u0302 )\u2212 \u00b5R\n\u03c3R , (15)\nwhere the optional \u00b5R and \u03c3R are the running mean and standard deviation of the reward, such that R(Y\u0302 ) has zero mean and unit variance. The resulting \u201cwhitening\u201d of the rewards is a simple version of PopArt (van Hasselt et al., 2016), and we found empirically that it stabilized learning.\nIt is straightforward to combine policy gradient with maximum likelihood, as both gradients can be computed by backpropagating through a properly reweighted sequence-level log-likelihood. The sequences for policy gradient are sampled from the model and weighted by a whitened reward, and the likelihood sequences are sampled from the training set and weighted by 1.\n3The policy also depends on the switch values but we omit these for brevity."}, {"heading": "4.3 Training Scheme", "text": "Instead of sampling from the model\u2019s output distribution, we use beam-search to generate questions from the model and approximate the expectation in Eq. 14. Empirically we found that rewards could not be improved through training without this approach. Randomly sampling from the model\u2019s distribution may not be as effective for estimating the modes of the generation policy and it may introduce more variance into the policy gradient.\nBeam search keeps a running set of candidates that expands and contracts adaptively. At each time-step t, k output words that maximize the probabilities of their respective paths are selected and added to the candidate sequences, where k is the beam size. The probabilities of these candidates are given by their accumulated log-likelihood up to t.4\nGiven a complete sample from the beam search and its accumulated log-likelihood, the gradient in (15) can be estimated as follows. After calculating the reward with a sequence generated by beam search, we use the sample to teacher-force the decoder so as to recreate exactly the model states from which the sequence was generated. The model can then be accurately updated by coupling the parameter-independent reward with the log-likelihood of the generated sequence. This approach adds a computational overhead but it significantly increases the initial reward values earned by the model and stabilizes policy gradient training.\nWe also further tune the likelihood during policy gradient optimization to prevent the model from overwriting its earlier training. We combine the policy gradient update to the model parameters, \u2207LRL, with an update from \u2207L based on teacher forcing on the ground-truth signal."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Dataset", "text": "We conducted our experiments on the SQuAD dataset for machine comprehension (Rajpurkar et al., 2016), a large-scale, human-generated corpus of (document, question, answer) triples. Documents are paragraphs from 536 high-PageRank Wikipedia articles covering a variety of subjects. Questions are posed by crowdworkers in natural language and answers are spans of text in the related paragraph highlighted by the same crowdworkers. There are 107,785 question-answer pairs in total, including 87,599 training instances and 10,570 development instances."}, {"heading": "5.2 Baseline Seq2Seq System", "text": "We build a simple baseline system, \u201cSeq2Seq,\u201d on the encoder-decoder architecture with attention and pointer-softmax outlined in Bahdanau et al. (2015) and Gulcehre et al. (2016). The baseline conditions question generation on the answer by setting ha to be the average of the document encodings corresponding to the answer positions in D."}, {"heading": "5.3 Quantitative Evaluation", "text": "We use several automatic evaluation metrics to judge the quality of generated questions with respect to the ground-truth questions from the dataset. We are undertaking a large-scale human evaluation to determine how these metrics align with human judgments. The first metric is BLEU (Papineni et al., 2002), a standard in machine translation, which computes {1,2,3,4}-gram matches between generated and ground-truth questions. Next we use F1, which focuses on unigram matches (Rajpurkar et al., 2016). We also report fluency and QA performance metrics used in our reward computation. Fluency is measured by the perplexity (PPL) of the generated question computed by the pretrained question\nlanguage model. The PPL score is proportional to the marginal probability p(Y\u0302 ) estimated from the corpus. The QA performance is measured by running the pretrained MPCM model on the generated questions and measuring F1 between the predicted answer and the conditioning answer.\n4We also experimented with a stochastic version of beam search by randomly sampling k words from top-2k predictions sorted by candidate sequence probability at each time step. No performance improvement was observed.\nText Passage\n...the court of justice accepted that a requirement to speak gaelic to teach in a dublin design college could be justified as part of the public policy of promoting the irish language."}, {"heading": "5.4 Results and qualitative analysis", "text": "Our results for automatic evaluation on SQuAD\u2019s development set are presented in Table 2. Implementation details for all models are given in the supplementary material. One striking feature is that BLEU scores are quite low for all systems tested, which relates to our earlier argument that a typical (document, answer) pair may be associated with multiple semantically-distinct questions. This seems to be born out by the result since most generated samples look reasonable despite low BLEU scores (see Tables 1, 3).\nOur system vs. Seq2Seq Comparing our model to the Seq2Seq baseline, we see that all metrics improve notably with the exception of PPL. Interestingly, our system performs worse in terms of PPL despite achieving lower negative log-likelihood. This, along with the improvements in BLEU, F1 and QA, suggests that our system learns a more powerful conditional model at the expense of accurately modelling the marginal distribution over questions. It is likely challenging for the model to allocate probability mass to rarer keywords that are helpful to recover the desired answer while also minimizing perplexity. We illustrate with samples from both models, specifically the first two samples in Table 3. The Seq2Seq baseline generated a well-formed English question, which is also quite vague \u2013 it is only weakly conditioned on the answer. On the other hand, our system\u2019s generated question is more specific, but still not correct given the context and perhaps less fluent given the repetition of the word language. We found that our proposed entropy regularization helped to avoid over-fitting and worked nicely in tandem with dropout: the training loss for our regularized model was 26.6 compared to 22.0 for the Seq2Seq baseline that used only dropout regularization.\nPolicy gradient (RPPL: \u03bbPPL = 0.1) Policy gradient training with the negative perplexity of the pretrained language model improves the generator\u2019s PPL score as desired, which approaches that of the baseline Seq2Seq model. However, QA, F1, and BLEU scores decrease. This aligns with the above observation that fluency and answerability (as measured by the automatic scores) may be in competition. As an example, the third sample in Table 3 is more fluent than the previous examples but does not refer to the desired answer.\nPolicy gradient (RQA: \u03bbQA = 1.0) Policy gradient is very effective at maximizing the QA reward, gaining 8.9% in accuracy over the improved Seq2Seq model and improving most other metrics as well. The fact that QA score is 3.7% higher than that obtained on the ground-truth questions suggests that the question generator may have learned to exploit MPCM\u2019s answering mechanism, and the higher reported perplexity suggests questions under this scheme may be less fluent. We explore this in more detail below. The fourth sample in Table 3, in contrast to the others, is clearly answered by the context word gaelic as desired.\nPolicy gradient (RPPL + QA: \u03bbPPL = 0.25,\u03bbQA = 0.5) We attempted to improve fluency and answerability in tandem by combining QA and PPL rewards. The PPL reward adds a prior towards questions that look natural. According to Table 2, this optimization scheme yields a good balance of performance, improving over the maximum-likelihood model by a large margin in terms of QA performance and gaining back some PPL. In the sample shown in Table 3, however, the question is specific to the answer but ends prematurely.\nIn Table 4 we provide additional generated samples from the different PG rewards. This table reveals one of the \u2018tricks\u2019 encouraged by the QA reward for improving MPCM performance: questions are often phrased with the interrogative \u2018wh\u2019 word at the end. This gives the language high perplexity, since such questions are rarer in the training data, but brings the question form closer to the form of the source text for answer matching."}, {"heading": "5.5 Discussion", "text": "Looking through examples revealed certain difficulties in the task and some pathologies in the model that should be rectified through future work.\nEntities and Verbs Similar entities and related verbs are often swapped, e.g., miami for jacksonville in a question about population. This issue could be mitigated by biasing the pointer softmax towards the document for certain word types.\nAbstraction We desire a system that generates interesting questions, which are not limited to reordering words from the context but exhibit some abstraction. Rewards from existing QA systems do not seem beneficial for this purpose. Questions generated through NLL training show more abstraction at the expense of decreased specificity.\nCommonsense and Reasoning Commonsense understanding appears critical for generating questions that are well-posed and show abstraction from the original text. Likewise, the ability to reason about and compose relations between entities could lead to more abstract and interesting questions. The existing model has no such capacity.\nEvaluation Due to the large number of possible questions given a predefined answer, it is challenging to evaluate the outputs using standard overlap-based metrics such as BLEU. This issue is made clear in the examples of Table 5. There, the second, model-generated generated question is valid given the context and refers clearly to the answer, but has low word overlap with the first, human-generated question. This suggests that question generation from text is similar to other tasks with large output spaces (Galley et al., 2015) and may benefit from corpora with multiple ground-truth questions associated to a quality rating (Mostafazadeh et al., 2016)."}, {"heading": "6 Conclusion", "text": "We proposed a recurrent neural model that generates natural-language questions conditioned on text passages and predefined answers. We showed how to train this model using a combination of maximum likelihood and policy gradient optimization, and demonstrated both quantitatively and qualitatively how several reward combinations affect the generated outputs. We are now undertaking a human evaluation to determine the correlation between rewards and human judgments, improving our model, and testing on additional datasets."}], "references": [{"title": "Automation of question generation from sentences", "author": ["Husam Ali", "Yllias Chali", "Sadid A Hasan."], "venue": "Proc. of QG2010: The Third Workshop on Question Generation .", "citeRegEx": "Ali et al\\.,? 2010", "shortCiteRegEx": "Ali et al\\.", "year": 2010}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1607.07086 .", "citeRegEx": "Bahdanau et al\\.,? 2016", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR) .", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent."], "venue": "Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, NIPS\u20192000. MIT Press, pages 932\u2013938.", "citeRegEx": "Bengio et al\\.,? 2001", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "Ranking automatically generated questions using common human queries", "author": ["Yllias Chali", "Sina Golestanirad."], "venue": "Proc. of INLG .", "citeRegEx": "Chali and Golestanirad.,? 2016", "shortCiteRegEx": "Chali and Golestanirad.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github.com/fchollet/keras. 10", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "deltableu: A discriminative metric for generation", "author": ["Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Aaron Courville", "Yoshua Bengio."], "venue": "Advances in neural", "citeRegEx": "Courville and Bengio.,? 2014", "shortCiteRegEx": "Courville and Bengio.", "year": 2014}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Good question! statistical ranking for question generation", "author": ["Michael Heilman", "Noah A Smith"], "venue": null, "citeRegEx": "Heilman and Smith.,? \\Q2010\\E", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Suleyman", "Phil Blunsom."], "venue": "Advances in", "citeRegEx": "Suleyman and Blunsom.,? 2015", "shortCiteRegEx": "Suleyman and Blunsom.", "year": 2015}, {"title": "The goldilocks principle", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "Neural Information Processing Systems", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["Sanja Fidler"], "venue": null, "citeRegEx": "Fidler.,? \\Q2015\\E", "shortCiteRegEx": "Fidler.", "year": 2015}, {"title": "reinforcement learning for dialogue generation", "author": ["David Lindberg", "Fred Popowich", "John Nesbit Phil Winne"], "venue": "Proc. of EMNLP", "citeRegEx": "Lindberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lindberg et al\\.", "year": 2013}, {"title": "Question generation from paragraphs", "author": ["ENLG . Prashanth Mannem", "Rashmi Prasad", "Aravind Joshi"], "venue": null, "citeRegEx": "Mannem et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mannem et al\\.", "year": 2010}, {"title": "Leveraging multiple views of text for automatic question", "author": ["Karen Mazidi", "Rodney D Nielsen"], "venue": null, "citeRegEx": "Mazidi and Nielsen.,? \\Q2015\\E", "shortCiteRegEx": "Mazidi and Nielsen.", "year": 2015}, {"title": "Generating natural questions about an image", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende."], "venue": "arXiv preprint arXiv:1603.06059 .", "citeRegEx": "Mostafazadeh et al\\.,? 2016", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Mohammad Norouzi", "Samy Bengio", "Zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans."], "venue": "Advances In Neural Information Processing Systems.", "citeRegEx": "Norouzi et al\\.,? 2016", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1312.6026 .", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "Proc. of ICLR ", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Iulian Vlad Serban", "Alberto Garc\u00eda-Dur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio."], "venue": "Proc. of ACL .", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."], "venue": "arXiv preprint arXiv:1609.05284 .", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Active comprehension: Problem-solving schema with question generation for comprehension of complex short stories", "author": ["Harry Singer", "Dan Donlan."], "venue": "Reading Research Quarterly pages 166\u2013186.", "citeRegEx": "Singer and Donlan.,? 1982", "shortCiteRegEx": "Singer and Donlan.", "year": 1982}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1506.06714 .", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "J. Mach. Learn. Res. 15(1):1929\u20131958. http://dl.acm.org/citation.cfm?id=2627435.2670313.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830 .", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Learning values across many orders of magnitude", "author": ["Hado P van Hasselt", "Arthur Guez", "Matteo Hessel", "Volodymyr Mnih", "David Silver."], "venue": "Advances in Neural Information Processing Systems. pages 4287\u20134295. 12", "citeRegEx": "Hasselt et al\\.,? 2016", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems. pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Semi-supervised qa with generative domain-adaptive nets", "author": ["Zhilin Yang", "Junjie Hu", "Ruslan Salakhutdinov", "William W Cohen."], "venue": "arXiv preprint arXiv:1702.02206 .", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Sentence simplification with deep reinforcement learning", "author": ["Xingxing Zhang", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1703.10931 .", "citeRegEx": "Zhang and Lapata.,? 2017", "shortCiteRegEx": "Zhang and Lapata.", "year": 2017}, {"title": "Automatically generating questions from queries for community-based question answering", "author": ["Shiqi Zhao", "Haifeng Wang", "Chao Li", "Ting Liu", "Yi Guan."], "venue": "Proc. of IJCNLP . 13", "citeRegEx": "Zhao et al\\.,? 2011", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "These generally comprise (document, question, answer) triples (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016), where the goal is to predict an answer, conditioned on a document and question.", "startOffset": 62, "endOffset": 172}, {"referenceID": 24, "context": "These generally comprise (document, question, answer) triples (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016), where the goal is to predict an answer, conditioned on a document and question.", "startOffset": 62, "endOffset": 172}, {"referenceID": 33, "context": "These generally comprise (document, question, answer) triples (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016), where the goal is to predict an answer, conditioned on a document and question.", "startOffset": 62, "endOffset": 172}, {"referenceID": 19, "context": "These generally comprise (document, question, answer) triples (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016), where the goal is to predict an answer, conditioned on a document and question.", "startOffset": 62, "endOffset": 172}, {"referenceID": 26, "context": "The availability of large labeled datasets has spurred development of increasingly advanced models for question answering (QA) from text (Kadlec et al., 2016; Seo et al., 2016; Wang et al., 2016; Shen et al., 2016).", "startOffset": 137, "endOffset": 214}, {"referenceID": 36, "context": "The availability of large labeled datasets has spurred development of increasingly advanced models for question answering (QA) from text (Kadlec et al., 2016; Seo et al., 2016; Wang et al., 2016; Shen et al., 2016).", "startOffset": 137, "endOffset": 214}, {"referenceID": 28, "context": "The availability of large labeled datasets has spurred development of increasingly advanced models for question answering (QA) from text (Kadlec et al., 2016; Seo et al., 2016; Wang et al., 2016; Shen et al., 2016).", "startOffset": 137, "endOffset": 214}, {"referenceID": 12, "context": ", 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016), where the goal is to predict an answer, conditioned on a document and question. The availability of large labeled datasets has spurred development of increasingly advanced models for question answering (QA) from text (Kadlec et al., 2016; Seo et al., 2016; Wang et al., 2016; Shen et al., 2016). In this paper we reframe the standard MC task: rather than answering questions about a document, we teach machines to ask questions. Our work has several motivations. First, we believe that posing appropriate questions is an important aspect of information acquisition in intelligent systems. Second, learning to ask questions may improve the ability to answer them. Singer and Donlan (1982) demonstrated that having students devise questions before reading can increase scores on subsequent comprehension tests.", "startOffset": 8, "endOffset": 785}, {"referenceID": 27, "context": ": generating training data for question answering (Serban et al., 2016; Yang et al., 2017), synthesising frequently asked question (FAQ) documentation, and automatic tutoring systems (Lindberg et al.", "startOffset": 50, "endOffset": 90}, {"referenceID": 38, "context": ": generating training data for question answering (Serban et al., 2016; Yang et al., 2017), synthesising frequently asked question (FAQ) documentation, and automatic tutoring systems (Lindberg et al.", "startOffset": 50, "endOffset": 90}, {"referenceID": 15, "context": ", 2017), synthesising frequently asked question (FAQ) documentation, and automatic tutoring systems (Lindberg et al., 2013).", "startOffset": 100, "endOffset": 123}, {"referenceID": 5, "context": "We adapt the sequence-to-sequence approach of Cho et al. (2014) for generating questions, conditioned on a document and answer: first we encode the document and answer, then output question words sequentially with a decoder that conditions on the document and answer encodings.", "startOffset": 46, "endOffset": 64}, {"referenceID": 10, "context": "It has been harnessed, for example, as a means to build automatic tutoring systems (Heilman and Smith, 2010; Ali et al., 2010; Lindberg et al., 2013; Labutov et al., 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al.", "startOffset": 83, "endOffset": 197}, {"referenceID": 0, "context": "It has been harnessed, for example, as a means to build automatic tutoring systems (Heilman and Smith, 2010; Ali et al., 2010; Lindberg et al., 2013; Labutov et al., 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al.", "startOffset": 83, "endOffset": 197}, {"referenceID": 15, "context": "It has been harnessed, for example, as a means to build automatic tutoring systems (Heilman and Smith, 2010; Ali et al., 2010; Lindberg et al., 2013; Labutov et al., 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al.", "startOffset": 83, "endOffset": 197}, {"referenceID": 17, "context": "It has been harnessed, for example, as a means to build automatic tutoring systems (Heilman and Smith, 2010; Ali et al., 2010; Lindberg et al., 2013; Labutov et al., 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al.", "startOffset": 83, "endOffset": 197}, {"referenceID": 40, "context": ", 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al., 2011), and to enrich training data for question-answering systems (Serban et al.", "startOffset": 94, "endOffset": 113}, {"referenceID": 27, "context": ", 2011), and to enrich training data for question-answering systems (Serban et al., 2016; Yang et al., 2017).", "startOffset": 68, "endOffset": 108}, {"referenceID": 38, "context": ", 2011), and to enrich training data for question-answering systems (Serban et al., 2016; Yang et al., 2017).", "startOffset": 68, "endOffset": 108}, {"referenceID": 10, "context": "Several earlier works process documents as individual sentences using syntactic (Heilman and Smith, 2010; Ali et al., 2010; Kumar et al., 2015) or semantic-based parsing (Mannem et al.", "startOffset": 80, "endOffset": 143}, {"referenceID": 0, "context": "Several earlier works process documents as individual sentences using syntactic (Heilman and Smith, 2010; Ali et al., 2010; Kumar et al., 2015) or semantic-based parsing (Mannem et al.", "startOffset": 80, "endOffset": 143}, {"referenceID": 16, "context": ", 2015) or semantic-based parsing (Mannem et al., 2010; Lindberg et al., 2013), then reformulate questions using hand-crafted rules acting on parse trees.", "startOffset": 34, "endOffset": 78}, {"referenceID": 15, "context": ", 2015) or semantic-based parsing (Mannem et al., 2010; Lindberg et al., 2013), then reformulate questions using hand-crafted rules acting on parse trees.", "startOffset": 34, "endOffset": 78}, {"referenceID": 15, "context": "An alternative approach is to use generic question templates whose slots can be filled with entities from the document (Lindberg et al., 2013; Chali and Golestanirad, 2016).", "startOffset": 119, "endOffset": 172}, {"referenceID": 4, "context": "An alternative approach is to use generic question templates whose slots can be filled with entities from the document (Lindberg et al., 2013; Chali and Golestanirad, 2016).", "startOffset": 119, "endOffset": 172}, {"referenceID": 0, "context": "It has been harnessed, for example, as a means to build automatic tutoring systems (Heilman and Smith, 2010; Ali et al., 2010; Lindberg et al., 2013; Labutov et al., 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al., 2011), and to enrich training data for question-answering systems (Serban et al., 2016; Yang et al., 2017). Several earlier works process documents as individual sentences using syntactic (Heilman and Smith, 2010; Ali et al., 2010; Kumar et al., 2015) or semantic-based parsing (Mannem et al., 2010; Lindberg et al., 2013), then reformulate questions using hand-crafted rules acting on parse trees. These traditional approaches generate questions with a high word overlap with the original text that pertain specifically to the given sentence by re-arranging the sentence parse tree. An alternative approach is to use generic question templates whose slots can be filled with entities from the document (Lindberg et al., 2013; Chali and Golestanirad, 2016). Labutov et al. (2015), for example, use ontology-derived templates to generate high-level questions related to larger portions of the document.", "startOffset": 109, "endOffset": 1052}, {"referenceID": 0, "context": "It has been harnessed, for example, as a means to build automatic tutoring systems (Heilman and Smith, 2010; Ali et al., 2010; Lindberg et al., 2013; Labutov et al., 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al., 2011), and to enrich training data for question-answering systems (Serban et al., 2016; Yang et al., 2017). Several earlier works process documents as individual sentences using syntactic (Heilman and Smith, 2010; Ali et al., 2010; Kumar et al., 2015) or semantic-based parsing (Mannem et al., 2010; Lindberg et al., 2013), then reformulate questions using hand-crafted rules acting on parse trees. These traditional approaches generate questions with a high word overlap with the original text that pertain specifically to the given sentence by re-arranging the sentence parse tree. An alternative approach is to use generic question templates whose slots can be filled with entities from the document (Lindberg et al., 2013; Chali and Golestanirad, 2016). Labutov et al. (2015), for example, use ontology-derived templates to generate high-level questions related to larger portions of the document. These approaches comprise pipelines of independent components that are difficult to tune for final performance measures. More recently, neural networks have enabled end-to-end training of question generation systems. Serban et al. (2016) train a neural system to convert knowledge base (KB) triples into natural-language questions.", "startOffset": 109, "endOffset": 1412}, {"referenceID": 37, "context": "We similarly apply a REINFORCE-style (Williams, 1992) algorithm to maximize various rewards earned by generated questions.", "startOffset": 37, "endOffset": 53}, {"referenceID": 18, "context": "Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text.", "startOffset": 0, "endOffset": 27}, {"referenceID": 18, "context": "Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system.", "startOffset": 0, "endOffset": 158}, {"referenceID": 18, "context": "Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system. The main goal of their question generation is data augmentation, thus questions themselves are not evaluated. In contrast, our work focuses primarily on developing a neural model for question generation that could be applied to a variety of downstream tasks that includes question answering. Our model shares similarities with recent end-to-end neural QA systems, e.g. Seo et al. (2016); Wang et al.", "startOffset": 0, "endOffset": 672}, {"referenceID": 18, "context": "Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system. The main goal of their question generation is data augmentation, thus questions themselves are not evaluated. In contrast, our work focuses primarily on developing a neural model for question generation that could be applied to a variety of downstream tasks that includes question answering. Our model shares similarities with recent end-to-end neural QA systems, e.g. Seo et al. (2016); Wang et al. (2016). I.", "startOffset": 0, "endOffset": 692}, {"referenceID": 18, "context": "Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system. The main goal of their question generation is data augmentation, thus questions themselves are not evaluated. In contrast, our work focuses primarily on developing a neural model for question generation that could be applied to a variety of downstream tasks that includes question answering. Our model shares similarities with recent end-to-end neural QA systems, e.g. Seo et al. (2016); Wang et al. (2016). I.e., we use an encoder-decoder structure, where the encoder processes answer and document (instead of question and document) and our decoder generates a question (instead of an answer). While existing question answering systems typically extract the answer from the document, our decoder is a fully generative model. Finally, we relate the recent body of works that apply reinforcement learning to natural language generation, such as Li et al. (2016); Ranzato et al.", "startOffset": 0, "endOffset": 1146}, {"referenceID": 18, "context": "Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system. The main goal of their question generation is data augmentation, thus questions themselves are not evaluated. In contrast, our work focuses primarily on developing a neural model for question generation that could be applied to a variety of downstream tasks that includes question answering. Our model shares similarities with recent end-to-end neural QA systems, e.g. Seo et al. (2016); Wang et al. (2016). I.e., we use an encoder-decoder structure, where the encoder processes answer and document (instead of question and document) and our decoder generates a question (instead of an answer). While existing question answering systems typically extract the answer from the document, our decoder is a fully generative model. Finally, we relate the recent body of works that apply reinforcement learning to natural language generation, such as Li et al. (2016); Ranzato et al. (2016); Kandasamy and Bachrach (2017); Zhang and Lapata (2017).", "startOffset": 0, "endOffset": 1169}, {"referenceID": 18, "context": "Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system. The main goal of their question generation is data augmentation, thus questions themselves are not evaluated. In contrast, our work focuses primarily on developing a neural model for question generation that could be applied to a variety of downstream tasks that includes question answering. Our model shares similarities with recent end-to-end neural QA systems, e.g. Seo et al. (2016); Wang et al. (2016). I.e., we use an encoder-decoder structure, where the encoder processes answer and document (instead of question and document) and our decoder generates a question (instead of an answer). While existing question answering systems typically extract the answer from the document, our decoder is a fully generative model. Finally, we relate the recent body of works that apply reinforcement learning to natural language generation, such as Li et al. (2016); Ranzato et al. (2016); Kandasamy and Bachrach (2017); Zhang and Lapata (2017).", "startOffset": 0, "endOffset": 1200}, {"referenceID": 18, "context": "Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system. The main goal of their question generation is data augmentation, thus questions themselves are not evaluated. In contrast, our work focuses primarily on developing a neural model for question generation that could be applied to a variety of downstream tasks that includes question answering. Our model shares similarities with recent end-to-end neural QA systems, e.g. Seo et al. (2016); Wang et al. (2016). I.e., we use an encoder-decoder structure, where the encoder processes answer and document (instead of question and document) and our decoder generates a question (instead of an answer). While existing question answering systems typically extract the answer from the document, our decoder is a fully generative model. Finally, we relate the recent body of works that apply reinforcement learning to natural language generation, such as Li et al. (2016); Ranzato et al. (2016); Kandasamy and Bachrach (2017); Zhang and Lapata (2017). We similarly apply a REINFORCE-style (Williams, 1992) algorithm to maximize various rewards earned by generated questions.", "startOffset": 0, "endOffset": 1225}, {"referenceID": 3, "context": "We adapt the simple encoder-decoder architecture first outlined by Cho et al. (2014) to the question generation problem.", "startOffset": 67, "endOffset": 85}, {"referenceID": 1, "context": "In particular, we base our model on the attention mechanism of Bahdanau et al. (2015) and the pointer-softmax copying mechanism of Gulcehre et al.", "startOffset": 63, "endOffset": 86}, {"referenceID": 1, "context": "In particular, we base our model on the attention mechanism of Bahdanau et al. (2015) and the pointer-softmax copying mechanism of Gulcehre et al. (2016). In question generation, we can condition our encoder on two different sources of information (compared to the single source in neural machine translation (NMT)): a document that the question should be about and an answer that should fit the generated question.", "startOffset": 63, "endOffset": 154}, {"referenceID": 3, "context": "Sequence elements di, aj \u2208 R De are given by embedding vectors (Bengio et al., 2001).", "startOffset": 63, "endOffset": 84}, {"referenceID": 26, "context": "(Seo et al., 2016), we augment each document word embedding with a binary feature that indicates if the document word belongs to the answer.", "startOffset": 0, "endOffset": 18}, {"referenceID": 13, "context": "Then, we run a bidirectional long short-term memory (Hochreiter and Schmidhuber, 1997) (LSTM) network on the augmented document sequence, producing annotation vectors h = (h1, .", "startOffset": 52, "endOffset": 86}, {"referenceID": 35, "context": "The location softmax is a pointer network (Vinyals et al., 2015) that induces a distribution over document tokens to be copied.", "startOffset": 42, "endOffset": 64}, {"referenceID": 13, "context": "Its internal state, st \u2208 R Ds , updates according to the long short-term memory function (Hochreiter and Schmidhuber, 1997), i.", "startOffset": 89, "endOffset": 123}, {"referenceID": 22, "context": "We use a deep output layer (Pascanu et al., 2013) at each time-step for the shortlist softmax vector ot.", "startOffset": 27, "endOffset": 49}, {"referenceID": 9, "context": "(8) As is standard in NMT, during decoding we use a beam search (Graves, 2012) to maximize (approximately) the conditional probability of an output sequence.", "startOffset": 64, "endOffset": 78}, {"referenceID": 30, "context": "We also encourage variety in the output words to counteract the degeneracy often observed in NLG systems towards common outputs (Sordoni et al., 2015).", "startOffset": 128, "endOffset": 150}, {"referenceID": 1, "context": "Teacher forcing introduces critical differences between the training phase (in which the model is driven by ground-truth sequences) and the testing phase (in which the model is driven by its own outputs) (Bahdanau et al., 2016).", "startOffset": 204, "endOffset": 227}, {"referenceID": 20, "context": "A properly designed reward, maximized via RL, could provide a model with more information about how to distribute probability mass among sequences that do not occur in the training set (Norouzi et al., 2016).", "startOffset": 185, "endOffset": 207}, {"referenceID": 36, "context": "We use the recently proposed Multi-Perspective Context Matching (MPCM) (Wang et al., 2016) model as our reference QA system, sans character-level encoding.", "startOffset": 71, "endOffset": 90}, {"referenceID": 24, "context": "After training a MPCM model on the SQuAD dataset, the reward RQA(\u0176 ) is given by MPCM\u2019s answer accuracy on \u0176 in terms of the F1 score, a token-based measure proposed by Rajpurkar et al. (2016) that accounts for partial word matches: RQA(\u0176 ) = F1(\u00c2, A), (12)", "startOffset": 169, "endOffset": 193}, {"referenceID": 39, "context": ", is it stated in proper, grammatical English? As simultaneously proposed in Zhang and Lapata (2017), we use a language model to measure and reward the fluency of generated questions.", "startOffset": 77, "endOffset": 101}, {"referenceID": 1, "context": ", 2014) and actor-critic methods (Bahdanau et al., 2016).", "startOffset": 33, "endOffset": 56}, {"referenceID": 21, "context": "We also experimented with several other rewards, most notably the BLEU score (Papineni et al., 2002) between \u0176 and the ground-truth question for the given document and answer, and a softer measure of similarity between output and ground-truth based on skip-thought vectors (Kiros et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 37, "context": "We use the REINFORCE algorithm (Williams, 1992) to maximize the model\u2019s expected reward.", "startOffset": 31, "endOffset": 47}, {"referenceID": 24, "context": "We conducted our experiments on the SQuAD dataset for machine comprehension (Rajpurkar et al., 2016), a large-scale, human-generated corpus of (document, question, answer) triples.", "startOffset": 76, "endOffset": 100}, {"referenceID": 1, "context": "We build a simple baseline system, \u201cSeq2Seq,\u201d on the encoder-decoder architecture with attention and pointer-softmax outlined in Bahdanau et al. (2015) and Gulcehre et al.", "startOffset": 129, "endOffset": 152}, {"referenceID": 1, "context": "We build a simple baseline system, \u201cSeq2Seq,\u201d on the encoder-decoder architecture with attention and pointer-softmax outlined in Bahdanau et al. (2015) and Gulcehre et al. (2016). The baseline conditions question generation on the answer by setting h to be the average of the document encodings corresponding to the answer positions in D.", "startOffset": 129, "endOffset": 179}, {"referenceID": 21, "context": "The first metric is BLEU (Papineni et al., 2002), a standard in machine translation, which computes {1,2,3,4}-gram matches between generated and ground-truth questions.", "startOffset": 25, "endOffset": 48}, {"referenceID": 24, "context": "Next we use F1, which focuses on unigram matches (Rajpurkar et al., 2016).", "startOffset": 49, "endOffset": 73}, {"referenceID": 18, "context": ", 2015) and may benefit from corpora with multiple ground-truth questions associated to a quality rating (Mostafazadeh et al., 2016).", "startOffset": 105, "endOffset": 132}], "year": 2017, "abstractText": "We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. Our model is trained and evaluated on the recent question-answering dataset SQuAD.", "creator": null}}}