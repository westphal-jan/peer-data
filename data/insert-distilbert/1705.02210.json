{"id": "1705.02210", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "SLDR-DL: A Framework for SLD-Resolution with Deep Learning", "abstract": "this paper introduces an sld - resolution analytical technique based on deep learning. this technique enables neural networks to learn from old and successful resolution processes and to use learnt experiences to guide new operational resolution processes. an implementation of this technique is named sldr - dl. it includes a prolog database library of important deep feedforward neural networks and some essential functions of resolution. in the sldr - dl representation framework, interested users can define logical representation rules in the abstract form of definite clauses and teach neural networks attempt to use the implied rules in reasoning processes.", "histories": [["v1", "Fri, 5 May 2017 13:32:54 GMT  (659kb,D)", "http://arxiv.org/abs/1705.02210v1", "12 pages, 5 figures"]], "COMMENTS": "12 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["cheng-hao cai"], "accepted": false, "id": "1705.02210"}, "pdf": {"name": "1705.02210.pdf", "metadata": {"source": "CRF", "title": "SLDR-DL: A Framework for SLD-Resolution with Deep Learning", "authors": ["Cheng-Hao Cai"], "emails": ["chenghao.cai@outlook.com"], "sections": [{"heading": null, "text": "Keywords: Automated Reasoning, Deep Learning, Logic Programming,\nResolution, Neural Networks."}, {"heading": "1 Introduction", "text": "SLDR-DL is a general purpose framework for SLD-resolution with deep learning. The name \u201cSLD-resolution\u201d is the abbreviation of SL-resolution for definite clauses [1, 2], while the name \u201cSL-resolution\u201d is the abbreviation of linear resolution with selection function [3]. In the SLDR-DL framework, computers can reason and learn to reason by using definite clauses [4] and deep neural networks [5]. The core concept of this framework is to train neural networks via successful resolution processes and to use the trained neural networks to guide new resolution processes heuristically.\nThe SLDR-DL framework has two aims: The first is to simulate the interaction between learning and reasoning: Systems are expected to learn from reasoning processes and to use learnt experiences to guide new reasoning processes. The second is to solve the problem of combinatorial explosion in automated reasoning [6]: When a problem becomes complex, its search tree of reasoning often grows rapidly. Many complex problems fail to be resolved because it is difficult to find true answers from huge search trees.\nThe SLDR-DL framework is implemented in SWI-Prolog [7]. Its source code can be downloaded from GitHub1, and it provides:\n1The source code can be downloaded from https://github.com/cchrewrite/SLDR-DL/\nar X\niv :1\n70 5.\n02 21\n0v 1\n[ cs\n.A I]\n5 M\n\u2022 A Prolog library of deep neural networks.\n\u2022 An implementation of SLD-resolution with deep learning.\n\u2022 Some worked examples.\nThe source code will be updated continuously. This paper consists of the following sections: Section 2 introduces related works briefly. Section 3 introduces the theory behind SLDR-DL. Section 4 provides a practical guide about how to use this framework. Section 5 provides a summary of this paper."}, {"heading": "2 Related Works", "text": "SLD-resolution [1] is a fundamental technique of automated reasoning. It has been used in many fields of artificial intelligence. For instance, Prolog is a programming language based on this technique [7, 8]. Mathematical reasoning processes, such as pattern matching, variable substitution and implication, can be simulated in Prolog [6]. Also, belief-desire-intention (BDI) agents can be developed with this technique [9, 10].\nRecently, many researchers have explored how to use deep learning to realise reasoning: For instance, Irving et al. [11] have developed DeepMath which uses deep neural networks to select possible premises in automated theorem proving processes. Also, Serafini and Garcez [12] have proposed Real Logic for the integration of learning and reasoning. In the field of reinforcement, Garnelo et al. [13] have tried to teach deep neural networks to generate symbols and build representations. In addition, Cai et al. [14] have explored the possibility of using deep feedforward neural networks to guide algebraic reasoning processes."}, {"heading": "3 SLD-Resolution with Deep Learning", "text": "SLD-resolution with deep learning is a fundamental technique of the SLDR-DL framework. It enables deep neural networks to guide new resolution processes after learning from old and successful resolution processes."}, {"heading": "3.1 SLD-Resolution", "text": "SLD-Resolution [1] is a process deciding whether a goal is satisfiable with a set of definite clauses. It is based on unification, definite clauses and resolution. In this section, we assume that readers have been familiar with these techniques, and only essential definitions and simple examples are carried out to aid the readability."}, {"heading": "3.1.1 Unification", "text": "Unification [15] is one of the core algorithms in logic programming. It can make two terms become equivalent ones by substitution:\nDefinition 1 (Term). A term is a constant, a variable or a functor followed by a sequence of terms. Formally, it is defined as:\nt := c | v | (f, t1, t2, \u00b7 \u00b7 \u00b7 , tm) (1)\nwhere c is a constant, v is a variable, f is a functor and t1, t2, \u00b7 \u00b7 \u00b7 , tm are terms. A term can be used to represent facts. For instance, if (Love, x, y) means \u201cx loves y\u201d, and (Know, p, q) means \u201cp knows q\u201d, then \u201cHaibara knows that Conan loves Ran\u201d can be represented as (Know,Haibara, (Love, Conan,Ran)).\nDefinition 2 (Unification). Unification is a process deciding whether or not two terms can be equivalent ones by substituting their variables for other variables or constants. The standard unification algorithm usually unifies two terms by computing the most general unifier (MGU). A unifier of two terms is a set of substitutions which can make the two terms to be equivalent ones, and the MGU of the two terms is the unifier which can be unified with all unifiers of the two terms. Formally, unification produces the MGU \u03c6 of two terms ta and tb such that:\nta[\u03c6] \u2261 tb[\u03c6] (2) For instance, (Know, p, (Love, x,Ran)) and (Know,Haibara, q) can be unified by applying the MGU {p/Haibara, q/(Love, x,Ran)}, where \u201c/\u201d is the substitution operation."}, {"heading": "3.1.2 Definite Clauses", "text": "Definite clauses [4] are used to represent relations between terms, especially their implication relations:\nDefinition 3 (Definite Clause). A definite clause is an implication relation between multiple premises and a single conclusion. Formally, it is defined as: p1 \u2227 p2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 pn =\u21d2 q (3) where p1, p2, \u00b7 \u00b7 \u00b7 , pn are premises, \u201c\u2227\u201d is the logical AND, \u201c =\u21d2 \u201d is the implication symbol, and q is a conclusion. All the premises and the conclusion are terms.\nDefinition 4 (Disjunction Form). The disjunction form of the definite clause p1 \u2227 p2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 pn =\u21d2 q is:\nq \u2228 \u00acp1 \u2228 \u00acp2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acpn (4)\nwhere \u201c\u2228\u201d is the logical OR, and \u201c\u00ac\u201d is the logical NOT. In this formula, q is called a positive literal, and \u00acp1,\u00acp2, \u00b7 \u00b7 \u00b7 ,\u00acpn are called negative literals. Formula (3) and Formula (4) can be proved to be logically equivalent [16].\nFor instance, \u201cif a is bigger than b, and b is bigger than c, then a is bigger than c\u201d can be represented as: (Bigger, a, b)\u2227 (Bigger, b, c) =\u21d2 (Bigger, a, c), and its disjunction form is (Bigger, a, c) \u2228 \u00ac(Bigger, a, b) \u2228 \u00ac(Bigger, b, c)."}, {"heading": "3.1.3 Resolution", "text": "The resolution algorithm [1] can decide whether or not a goal is satisfiable: Definition 5 (Goal). A goal is a definite clause with an empty conclusion g1 \u2227 g2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 gn =\u21d2 , and its disjunction form is \u00acg1 \u2228 \u00acg2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acgn. Definition 6 (Rule). A rule is a definite clause with a conclusion p1 \u2227 p2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 pn =\u21d2 q, and its disjunction form is q \u2228 \u00acp1 \u2228 \u00acp2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acpn. In particular, a rule is called an assertion if its premise is empty. In this case, it becomes =\u21d2 q, and its disjunction form is q.\nDefinition 7 (SLD-Resolution). SLD-resolution is a process analysing goals by applying rules: Assume that a goal is \u00acg1 \u2228\u00acg2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acgi \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acgn and a rule is q \u2228\u00acp1 \u2228\u00acp2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acpn. Firstly, a negative literal \u00acgi is selected from the goal. Secondly, unification is used to compute the MGU \u03c6 such that gi[\u03c6] \u2261 q[\u03c6]. Lastly, if the unification process is successful, then \u00acgi is replaced by \u00acp1 \u2228\u00acp2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acpn, and the goal becomes \u00acg1[\u03c6] \u2228\u00acg2[\u03c6]\u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acp1[\u03c6] \u2228 \u00acp2[\u03c6]\u2228\u00b7 \u00b7 \u00b7\u2228\u00acpn[\u03c6]\u2228\u00b7 \u00b7 \u00b7\u2228\u00acgn[\u03c6]. In particular, if the rule is an assertion, then the goal becomes \u00acg1[\u03c6] \u2228 \u00acg2[\u03c6] \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acgi\u22121[\u03c6] \u2228 \u00acgi+1[\u03c6] \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acgn[\u03c6], as \u00acgi is eliminated. The above process is run iteratively until the goal is empty, and backtracking is used to select new rules when unification fails.\nFor instance, given three rules:\n(Bigger, 4, 2) (5)\n(Bigger, 2, 1) (6)\n(Bigger, a, c) \u2228 \u00ac(Bigger, a, b) \u2228 \u00ac(Bigger, b, c) (7)\nand a goal: \u00ac(Bigger, x, y) (8)\nSLD-resolution can prove (Bigger, 4, 2), (Bigger, 2, 1) and (Bigger, 4, 1) by resolving the goal (\u201c \u201d is used to represent \u201cempty\u201d):\n\u00ac(Bigger, x, y) Rule (5)\u2212\u2212\u2212\u2212\u2212\u2192 [x/4, y/2] (9)\n\u00ac(Bigger, x, y) Rule (6)\u2212\u2212\u2212\u2212\u2212\u2192 [x/2, y/1] (10)\nor \u00ac(Bigger, x, y)\nRule (7)\u2212\u2212\u2212\u2212\u2212\u2192 \u00ac(Bigger, x, b)[a/x, c/y] \u2228 \u00ac(Bigger, b, y)[a/x, c/y] Rule (5)\u2212\u2212\u2212\u2212\u2212\u2192 \u00ac(Bigger, 2, y)[a/x, b/2, c/y, x/4, ] Rule (6)\u2212\u2212\u2212\u2212\u2212\u2192 [a/x, b/2, c/y, x/4, y/1]\n(11)"}, {"heading": "3.2 Deep Neural Networks", "text": "Deep neural networks are used to select rules during the process of SLDresolution. In this section, we assume that readers have been familiar with deep neural networks, and only essential definitions are carried out.\nDefinition 8 (Deep Feedforward Neural Network). A deep feedforward neural network (DFNN) [5] is a neural network satisfying: (1) It has 5 or more than 5 hidden layers. (2) Two neighbouring layers are fully connected. (3) It does not have any recurrent connections. A DFNN can map an input vector to an output vector.\nDefinition 9 (Back-Propagation). Back-propagation [17] is a supervised learning method of neural networks. Given an input vector, a feedforward neural network can map it to an output vector, compute an error between the output vector and a target vector and use back-propagation to transfer the error to different layers and update the neural network."}, {"heading": "3.3 The SLDR-DL Framework", "text": "The SLDR-DL framework is the combination of SLD-resolution and DFNNs. It enables the deep neural networks to guide and learn to guide resolution processes."}, {"heading": "3.3.1 The Framework Structure", "text": "The core part of the SLDR-DL framework is an implementation of SLDresolution with DFNNs.\nDefinition 10 (SLD-Resolution with DFNNs). SLD-resolution with DFNNs is adapted from the standard SLD-resolution (see [1] and Definition 7). When resolving a goal, the following strategy is used: Firstly, a goal literal is encoded to an input vector. Secondly, a trained neural network is used to maps the input vector to an output vector. Thirdly, the output vector is decoded to a ranking list of rules. Finally, rules are applied to the goal according to the ranking list. The methods of encoding and decoding will be discussed in 3.3.2.\nIn the above process, the neural network is used to predict the ranking list of rules for the given literal. Therefore, the neural network must learn to rank the rules before it is used for prediction.\nDefinition 11 (Learning by SLD-Resolution). Learning by SLDresolution is a technique which trains neural networks by using successful resolution processes. Before learning, a goal must be successfully resolved, and records of resolution must be produced. Each record consists of a selected literal and the name of a rule which has been applied to the literal. These records are used to train the neural network: Firstly, the selected literal is encoded to an input vector. Then the name of the rule is encoded to a target vector. Finally, the input vector and the target vector are used to train the neural network with the back-propagation algorithm [17]. The methods of encoding and decoding will be discussed in Section 3.3.2.\nBased on the resolution function and the learning function discussed above, an SLDR-DL system usually consists of:\n\u2022 A deep neural network.\n\u2022 A rule set for resolution and the encoding and decoding of rules.\n\u2022 A symbol set for the encoding of literals.\nDefinition 12 (Rule Set). A rule set contains logical rules with unique names and unique IDs. These rules are definite clauses written in disjunction form. Their IDs should be positive integers.\nFor instance, a rule set can be:\nID Name Rule\n1 Bigger42 (Bigger, 4, 2) 2 Bigger21 (Bigger, 2, 1) 3 BiggerABC (Bigger, a, c) \u2228 \u00ac(Bigger, a, b) \u2228 \u00ac(Bigger, b, c)\nDefinition 13 (Symbol Set). A symbol set contains symbols with unique IDs. The IDs should be positive integers.\nFor instance, a symbol set can be:\nID Symbol"}, {"heading": "1 V ble", "text": "2 Bigger 3 1 4 2 5 4"}, {"heading": "3.3.2 Encoding and Decoding", "text": "To enable neural networks to guide resolution processes, encoding and decoding are required, as discussed by Section 3.3.1: (1) Selected literals should be encoded to input vectors; (2) Rules should be encoded to target vectors; (3) Output vectors should be decoded to ranking lists of rules. In the SLDR-DL framework, we have implemented the following encoding or decoding methods:\n\u2022 Given a symbol set s, a predefined depth d and a predefined breadth b, a negative literal \u00acl is encoded to a vector via the following steps: Firstly, all variables of l are replaced by a notation \u201cV ble\u201d. Let lNV denote this new expression. Secondly, lNV is rewritten to a completed term lComp with the depth d and the breadth b. All positions exceed the depth and the breadth are omitted, and empty positions are filled by a notation \u201cEmpty\u201d. Thirdly, lComp is flatten to a list lList. Finally, lList is represented as a vector by using the one-hot encoding [18]. Activated positions of the one-hot encoding are decided by the IDs of symbols in s. In particular, \u201cEmpty\u201d is encoded to a zero block.\n\u2022 A rule is encoded to a vector via the one-hot encoding [18], according to its unique ID in a rule set.\n\u2022 An output vector (y1, y2, \u00b7 \u00b7 \u00b7 , ym) is decoded to a ranking list via the following steps: Firstly, IDs are attached to all elements, so that the vector becomes a list ([y1, id1], [y2, id2], \u00b7 \u00b7 \u00b7 , [ym, idm]). Secondly, the list is sorted by yi in descending order. Finally, the order of IDs is figured out from the sorted list, and the order decides a ranking list of rules."}, {"heading": "3.3.3 The Education of SLDR-DL Systems", "text": "We use the word \u201ceducation\u201d instead of \u201ctraining\u201d because the process of optimising an SLDR-system is usually from simple problems to complex problems and requires the interaction between learning and reasoning, and this process is similar to the process of educating a human. In other words, resolution in SLDR-DL is a heuristic search process which can optimise its search strategy via learning. Before learning, it can resolve simple goals, but the resolution of complex goals may fail, because the search space may be huge. After learning in proper ways, the search space can be reduced, so that the complex goals can be resolved successfully. Therefore, the education of an SLDR-DL system usually requires a schedule in which problems are sorted from simplest to hardest. By the schedule, the system tries to resolve simple problems at the beginning, works out resolution records and learns the records. Then the system proceeds to more complex problems and continues learning until all problems are resolved."}, {"heading": "3.3.4 A Prolog Library of Deep Neural Networks", "text": "The SLDR-DL framework also provides a Prolog library which supports essential neural network computations. Specifically, the library now supports:\n\u2022 Matrix addition and multiplication.\n\u2022 The back-propagation algorithm of feedforward neural networks.\n\u2022 The Softmax classifier.\nDetails of the above functions can be found from [19]. To expanded the use of the framework, more functions will be added to the library in the future."}, {"heading": "4 A Practical Guide", "text": "To build and use an SLDR-DL system, users need to define a rule set, a symbol set and a neural network. These definitions should be coded in Prolog (preferably SWI-Prolog) [7]."}, {"heading": "4.1 Defining a Rule Set", "text": "A rule set is defined as a list of rules (definite clauses) written in disjunction form with their unique IDs and names. A rule is defined in the following format:\n[\u2032Rule ID\u2032, \u2032Rule Name\u2032, \u2032Disjunction Form\u2032] (12)\nThe disjunction form of a rule p1 \u2227 p2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 pn =\u21d2 q is defined as:\n[\u2212p1,\u2212p2,\u2212p3, \u00b7 \u00b7 \u00b7 ,\u2212pn,+q] (13)\nwhere \u201c\u2212\u201d denotes a negative literal (premise), and \u201c+\u201d denotes a positive literal (conclusion). In particular, the number of negative literals can be zero, and the rule becomes an assertion [+q]. Figure 1 provides an example of a rule set, where MaxRuleID is the maximum ID of rules. It is important to note that when defining a rule set, we use the Prolog convention: A symbol is a constant if it is a number, or its first letter is in lower case. A symbol is a variable if its first letter is in lower case. For instance, \u201c[\u2212[child, Y,X],\u2212[male,X],+[father,X, Y ]]\u201d means that for any X and Y , if Y is a child of X, and X is a male, then X is the father of Y ."}, {"heading": "4.2 Defining a Symbol Set", "text": "A symbol set is defined as a list of symbols with their unique IDs. A symbol is defined in the following format:\n[\u2032Symbol ID\u2032, \u2032Symbol\u2032] (14)\nFigure 2 provides an example of a symbol set, where MaxSymbolID is the maximum ID of symbols."}, {"heading": "4.3 Defining a Neural Network", "text": "A neural network can be defined as a list of layers:\n[ \u2032Input Layer\u2032, \u2032Hidden Layer 1\u2032, \u2032Hidden Layer 2\u2032,\n\u00b7 \u00b7 \u00b7 , \u2032Hidden Layer N \u2032, \u2032Output Layer\u2032\n]\n(15)\nEach layer can be initialised via:\nlayer init( \u2032Layer Name\u2032, \u2032Input Dimension\u2032, \u2032Output Dimension\u2032, \u2032Activation Type\u2032, \u2032Scale of Randomisation\u2032,\n)\n(16)\nFigure 3 provides an example of the definition of a neural network."}, {"heading": "4.4 Learning and Reasoning", "text": "The framework provides a core function named \u201cdnn sl resolution\u201d:\ndnn sl resolution( \u2032Goal\u2032, \u2032Rule Set\u2032, \u2032Symbol Set\u2032, \u2032Neural Network\u2032, \u2032Method\u2032, \u2032SearchDepth\u2032, \u2032Result\u2032\n)\n(17)\nBoth learning and reasoning processes are based on the core function. Figure 4 provides an example about how to use the core function, where G1, G2, G3 and G4 are goals, \u201clearning(N,R)\u201d is used to define the number of learning epochs N and the learning rate R, \u201cinput(B,D)\u201d is used to define the breadth B and the depth D of encodings, and \u201coutput(Y )\u201d is used to define the dimension of decodings Y . When running the process, the neural network learns from the resolution processes of G1, G2 and G3 and then tries to resolve G4. Figure 5 shows a result of running, including a record of cross-entropy losses and a resolution process of G4."}, {"heading": "5 Summary", "text": "The SLDR-DL framework enables the interaction between resolution and deep learning. In the framework, users can define logical rules in the form of definite clauses, define neural networks and teach the neural networks to use the logical rules. The neural networks can learn from successful resolution processes and then use learnt experiences to guide new resolution processes. To expand the use of this framework, we will add more functions to it and refine it in the future."}], "references": [{"title": "Predicate logic as programming language", "author": ["Robert A. Kowalski"], "venue": "In IFIP Congress,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1974}, {"title": "Contributions to the theory of logic programming", "author": ["Krzysztof R. Apt", "Maarten H. van Emden"], "venue": "J. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1982}, {"title": "Linear resolution with selection function", "author": ["Robert A. Kowalski", "Donald Kuehner"], "venue": "Artif. Intell.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1971}, {"title": "On sentences which are true of direct unions of algebras", "author": ["Alfred Horn"], "venue": "J. Symb. Log.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1951}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7533):436\u2013444, May", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "The computer modelling of mathematical reasoning", "author": ["Alan Bundy"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1983}, {"title": "The birth of prolog", "author": ["Alain Colmerauer", "Philippe Roussel"], "venue": "In History of Programming Languages Conference (HOPL-II), Preprints,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "AgentSpeak(L): BDI agents speak out in a logical computable language. In Agents Breaking Away, 7th European Workshop on Modelling Autonomous Agents in a Multi-Agent World, Eindhoven", "author": ["Anand S. Rao"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Programming Multi-Agent Systems in AgentSpeak using Jason (Wiley Series in Agent Technology)", "author": ["Rafael H Bordini", "Jomi Fred Bner", "Michael Wooldridge"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Deepmath - deep sequence models for premise selection", "author": ["Geoffrey Irving", "Christian Szegedy", "Alexander A. Alemi", "Niklas E\u00e9n", "Fran\u00e7ois Chollet", "Josef Urban"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge", "author": ["Luciano Serafini", "Artur S. d\u2019Avila Garcez"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Towards deep symbolic reinforcement learning", "author": ["Marta Garnelo", "Kai Arulkumaran", "Murray Shanahan"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Learning of humanlike algebraic reasoning using deep feedforward neural networks", "author": ["Chenghao Cai", "Dengfeng Ke", "Yanyan Xu", "Kaile Su"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Unification theory. In Handbook of Automated Reasoning (in 2 volumes)", "author": ["Franz Baader", "Wayne Snyder"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Logic in computer science modelling and reasoning about systems (2", "author": ["Michael Huth", "Mark Dermot Ryan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Theory of the backpropagation neural network", "author": ["Robert Hecht-Nielsen"], "venue": "Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1988}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "ACL", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The name \u201cSLD-resolution\u201d is the abbreviation of SL-resolution for definite clauses [1, 2], while the name \u201cSL-resolution\u201d is the abbreviation of linear resolution with selection function [3].", "startOffset": 84, "endOffset": 90}, {"referenceID": 1, "context": "The name \u201cSLD-resolution\u201d is the abbreviation of SL-resolution for definite clauses [1, 2], while the name \u201cSL-resolution\u201d is the abbreviation of linear resolution with selection function [3].", "startOffset": 84, "endOffset": 90}, {"referenceID": 2, "context": "The name \u201cSLD-resolution\u201d is the abbreviation of SL-resolution for definite clauses [1, 2], while the name \u201cSL-resolution\u201d is the abbreviation of linear resolution with selection function [3].", "startOffset": 188, "endOffset": 191}, {"referenceID": 3, "context": "In the SLDR-DL framework, computers can reason and learn to reason by using definite clauses [4] and deep neural networks [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "In the SLDR-DL framework, computers can reason and learn to reason by using definite clauses [4] and deep neural networks [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "The second is to solve the problem of combinatorial explosion in automated reasoning [6]: When a problem becomes complex, its search tree of reasoning often grows rapidly.", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "SLD-resolution [1] is a fundamental technique of automated reasoning.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "For instance, Prolog is a programming language based on this technique [7, 8].", "startOffset": 71, "endOffset": 77}, {"referenceID": 5, "context": "Mathematical reasoning processes, such as pattern matching, variable substitution and implication, can be simulated in Prolog [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "Also, belief-desire-intention (BDI) agents can be developed with this technique [9, 10].", "startOffset": 80, "endOffset": 87}, {"referenceID": 8, "context": "Also, belief-desire-intention (BDI) agents can be developed with this technique [9, 10].", "startOffset": 80, "endOffset": 87}, {"referenceID": 9, "context": "[11] have developed DeepMath which uses deep neural networks to select possible premises in automated theorem proving processes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Also, Serafini and Garcez [12] have proposed Real Logic for the integration of learning and reasoning.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "[13] have tried to teach deep neural networks to generate symbols and build representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] have explored the possibility of using deep feedforward neural networks to guide algebraic reasoning processes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "1 SLD-Resolution SLD-Resolution [1] is a process deciding whether a goal is satisfiable with a set of definite clauses.", "startOffset": 32, "endOffset": 35}, {"referenceID": 13, "context": "1 Unification Unification [15] is one of the core algorithms in logic programming.", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": "2 Definite Clauses Definite clauses [4] are used to represent relations between terms, especially their implication relations: Definition 3 (Definite Clause).", "startOffset": 36, "endOffset": 39}, {"referenceID": 14, "context": "Formula (3) and Formula (4) can be proved to be logically equivalent [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "3 Resolution The resolution algorithm [1] can decide whether or not a goal is satisfiable: Definition 5 (Goal).", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "A deep feedforward neural network (DFNN) [5] is a neural network satisfying: (1) It has 5 or more than 5 hidden layers.", "startOffset": 41, "endOffset": 44}, {"referenceID": 15, "context": "Back-propagation [17] is a supervised learning method of neural networks.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "SLD-resolution with DFNNs is adapted from the standard SLD-resolution (see [1] and Definition 7).", "startOffset": 75, "endOffset": 78}, {"referenceID": 15, "context": "Finally, the input vector and the target vector are used to train the neural network with the back-propagation algorithm [17].", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "Finally, lList is represented as a vector by using the one-hot encoding [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "\u2022 A rule is encoded to a vector via the one-hot encoding [18], according to its unique ID in a rule set.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "Details of the above functions can be found from [19].", "startOffset": 49, "endOffset": 53}], "year": 2017, "abstractText": "This paper introduces an SLD-resolution technique based on deep learning. This technique enables neural networks to learn from old and successful resolution processes and to use learnt experiences to guide new resolution processes. An implementation of this technique is named SLDR-DL. It includes a Prolog library of deep feedforward neural networks and some essential functions of resolution. In the SLDR-DL framework, users can define logical rules in the form of definite clauses and teach neural networks to use the rules in reasoning processes.", "creator": "LaTeX with hyperref package"}}}