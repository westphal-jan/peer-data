{"id": "1705.03151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Phonetic Temporal Neural Model for Language Identification", "abstract": "deep neural models, particularly the lstm - rnn model, have shown great analytical potential in language identification ( lid ). however, the phonetic information has been subsequently largely overlooked by most of existing acoustic neural lid methods, lately although this information has been used in the conventional phonetic lid systems with a great success. we present a phonetic temporal neural model developed for lid, which is an lstm - rnn lid system but accepts phonetic features produced by a phone - discriminative dnn as the input, rather than raw acoustic features. this new model is a reminiscence of the old phonetic lid methods, but the phonetic knowledge here is much richer : it is at virtually the frame detection level and involves compacted information of nearly all phones. our experiments conducted on the babel database and the ap16 - olr database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic fuzzy neural models. instead it also outperforms the conventional i - vector approach on short utterances and in noisy conditions.", "histories": [["v1", "Tue, 9 May 2017 02:46:21 GMT  (1604kb,D)", "http://arxiv.org/abs/1705.03151v1", null], ["v2", "Mon, 22 May 2017 11:23:34 GMT  (1536kb,D)", "http://arxiv.org/abs/1705.03151v2", "Submitted to TASLP"], ["v3", "Fri, 25 Aug 2017 05:23:26 GMT  (1538kb,D)", "http://arxiv.org/abs/1705.03151v3", "Submitted to TASLP"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["zhiyuan tang", "dong wang", "yixiang chen", "lantian li", "andrew abel"], "accepted": false, "id": "1705.03151"}, "pdf": {"name": "1705.03151.pdf", "metadata": {"source": "CRF", "title": "Phonetic Temporal Neural Model for Language Identification", "authors": ["Zhiyuan Tang", "Dong Wang", "Yixiang Chen", "Lantian Li", "Andrew Abel"], "emails": ["dong99@mails.tsinghua.edu.cn)."], "sections": [{"heading": null, "text": "Keywords\u2014Language identification; Deep neural networks; Long short-term memory\nI. INTRODUCTION Language identification (LID) lends itself to a wide range of applications, e.g., mix-lingual (code-switching) speech recognition. Humans use many cues to discriminate languages, and more cues are used, better accuracy can be achieved. Based on different cues, various LID approaches have been developed."}, {"heading": "A. Cues for language identification", "text": "There are more than 5000 languages in the world, and each language holds its own distinct properties at different levels, from acoustic to semantics [1]\u2013[3]. A couple of studies have been conducted to investigate how our humans use these properties as cues to distinguish languages [4]. For example, Muthusamy [5] found that familiarity with a language is an important factor affecting the LID accuracy, and longer speech samples are easier to be identified. Moreover, people can easily tell what cues they use to make the identification, including phonemic inventory, word usage, and prosody. More thorough investigations were conducted by several researchers, by modifying the speech samples to promote one or several factors. For example, Mori et al. [6] found that people are able to identify Japanese and English fairly reliably even\nZ.T. is a collaborative PhD student from Chengdu Institute of Computer Applications, Chinese Academy of Sciences and CSLT, Tsinghua University. D.W., Y.C. and L.L. are with the Center for Speech and Language Technologies (CSLT) at Tsinghua University and Tsinghua National Laboratory for Information Science and Technology. A.A. is with Xi An Jiaotong Liverpool-University, Suzhou, China. D.W. is the corresponding author (wangdong99@mails.tsinghua.edu.cn).\nwhen phone information is reduced. They argued that other cues such as intensity and pitch has been used to make the judgments. Navratil [7] evaluated the importance of various types of knowledge, including lexical, phonotactic and prosodic, by humans asked to identify five languages including Chinese, English, French, German and Japanese. Subjects were presented unaltered speech samples, samples whose syllables were randomly ordered, and samples for which the vocaltract information was removed so that only F0 and amplitude were left. Navratil found that the speech samples with random syllables are more difficult to be identified compared to the original samples (73.9% vs 96%), and removing vocal-track information leads to significant performance reduction (73.9% vs 49.4%). This means that the lexical and phonotactic information is important when the human subjects make their decisions, at least with this 5-language LID task.\nThese human LID experiments mentioned above suggest that languages can be discriminated by multiple cues at different levels, and the cues used to differentiate different language pairs are different. In general, the cues can be categorized into three levels: feature level, token level and prosody level.\nAt the feature level, different languages have their own implementation of phones, and the transition between phones are also different. This acoustic speciality is a short-time property and can be discovered by certain spectral analysis and feature extraction of our auditory system. At the token level, the distribution and transition patterns of linguistic tokens at various levels are significantly different. The tokens can be phones/phonemes, syllables, words or even syntactic or semantic tags. At the prosody level, the duration, pitch and stress patterns in one language often differ from another. For example, patterns of stress can provide an important cue for discriminating between two stress languages. Duration can also be potentially useful. The tone patterns of syllables or words offer a clear cue to discriminate tonal languages."}, {"heading": "B. LID approaches", "text": "Based on the cues at different levels, multiple approaches have been proposed. Early work mostly focuses on featurelevel cues. These feature-based methods use strong statistical models built on raw acoustic features to make the LID decision. For instance, Cimarusti used LPC features [8], and Foil et al. [9] studied format features. Dynamic features that involve temporal information were also demonstrated to be effective [10]. The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17]. More recently, a\nar X\niv :1\n70 5.\n03 15\n1v 1\n[ cs\n.C L\n] 9\nM ay\n2 01\n7\n2 low-rank GMM model called \u2018i-vector model\u2019 was proposed and achieved significant success [18], [19]. This model constraints the mean vectors of the GMM components in a lowdimensional space to improve statistical strength for model training, and uses a task-oriented discriminative model (e.g., linear discriminative analysis, LDA) to improve the decision quality at run-time, leading to a superior LID performance. Due to the short-time property of the features, most of the feature-based methods model the distributional characters rather than the temporal characters of speech signals.\nThe token-based approach is based on the characters of high-level tokens. Since the dynamic properties of adjacent tokens is more stable than adjacent raw features, the temporal characters can be learned with the token-based approach, in additional to the distributional characters. A typical approach is to convert speech signals into phone sequences, and then build an n-gram language model (LM) for each target language to evaluate the confidence that the input speech is in that language. This is the famous phone recognition and language modeling (PRLM) approach. Multiple variants of the PRLM were proposed, for example, the parallel phone recognition followed by LM (PPRLM) [20], [21], and phone recognition on a multilingual phone set [22]. Other tokens were also studied, e.g., syllables [23] and words [24], [25].\nThe prosody-based approach utilizes patterns of duration, pitch, and stress to discriminate languages. For example, Foil et al. [9] studied format and prosodic features and found that formant features are more discriminative. Rouas et al. [26] studied a rhythmic model. Muthusamy [16] used pitch variation, duration and syllable rate. The duration and pitch patterns were also used by Hazen [22]. In most cases, the prosodic information is used as additional knowledge to improve the feature-based or token-based LID.\nMost of the above methods, no matter what information is used, heavily rely on probabilistic models to accumulate evidence from a long speech segment. For example, the PRLM method requires an n-gram probability of the phonetic sequence, and the GMM/i-vector method requires the distribution of the acoustic feature. Therefore, these approaches require long test utterances, leading to inevitable latency in LID decision. This latency is a serious problem for many practical applications, e.g., code-switching ASR. For a quick LID, frame-level decision is highly desirable, which, of course, can not rely on probabilistic models.\nThe recent emerging deep learning approach solves this problem by using various deep neural networks (DNNs) to produce frame-level LID decision. An early success of deep neural models was obtained by Lopez-Moreno et al. [27], who proposed an approach based on a feed-forward deep neural network (FFDNN), which accepts raw acoustic features and produces frame-level LID decisions. The utterance-based decision is calculated by averaging the scores of the frame-level decisions. This work was followed and extended by a number of researchers, by using various neural model structures, e.g., CNN [28], [29] and TDNN [30], [31]. These DNN models are certainly feature-based, but they consider a large context window, so can learn the feature\u2019s temporal information, which is not possible with the conventional feature-based models,\ne.g., the i-vector model, that learn only the distributional information. The temporal information can be better learned by recurrent neural networks (RNN), as proposed by GonzalezDominguez et al. [32]. Using an RNN structure based on longshort term memory unit (LSTM), the authors reported better performance with much less parameters. This RNN approach was followed by a number of researchers, e.g., [33], [34].\nIt should be noted that DNNs have been used in other ways in LID. For example, Song et al [35] used DNN to extract phonetic feature for the i-vector system, and Ferrer et al [36] proposed an DNN i-vector approach that uses posteriors produced by an phone-discriminative FFDNN to compute the Baum-Welch statistics. Tian et al [37] extended this study by using an RNN to produce the posteriors. All these methods use neural models as part of the system, but their basic framework is still probabilistic, so they share the same problem of latent decision. In this paper, we focus on the pure neural approach that uses neural models as the basic framework, so that short-time LID information can be learned by the framelevel discriminative training."}, {"heading": "C. Motivation of the paper", "text": "All the present neural LID methods are based on acoustic features, e.g., Mel filter banks (Fbanks) or Mel frequency cepstral coefficients (MFCCs), with phonetic information largely overlooked. This may have significantly hindered the performance of neural LID. Intuitively, it is a long-standing hypothesis that languages are discriminated by phonetic properties, either distributional or temporal; theoretically, phonetic features represent information at a higher level than that represented by acoustic features, and so are more invariant with respect to noise and channels; pragmatically, it has been demonstrated that phonetic information, either in the form of phone sequences, phone posteriors or phonetic bottleneck features, can significantly improve LID accuracy, in both the conventional PRLM approach [12] and the more modern ivector system [35]\u2013[37]. In this paper, we will study how to employ phonetic information to improve neural LID. The basic idea is to use a phone-discriminative model to produce frame-level phonetic features, and then use these features to enhance RNN LID systems that were originally built with raw acoustic features. The initial attempt follows the idea of feature combination where the phonetic feature is used as an auxiliary information to assist acoustic RNN LID, and further research discovers that a simpler model that uses the phonetic feature as the only input of the RNN LID provides even better performance. We call this RNN model based on phonetic features as the phonetic temporal neural LID approach, or PTN LID. Besides the simplicity in model structure, the PTN offers a deeper insight for the LID task, by rediscovering the value of the phonetic temporal property in language discrimination. This property was successfully applied in the historical token-based approach, e.g., PRLM [12], but has been largely overlooked since the popularity of the i-vector approach.\nTable I summarizes different systems that use deep neural models. The neural approach involves methods that use various types of DNNs as the decision architecture, while the\n3\nprobabilistic approach involves methods use DNNs as part of a probabilistic system, e.g., GMM or i-vector. Both approaches may use either acoustic features or phonetic features. The proposed PTN approach is on the right-bottom of the table."}, {"heading": "D. Paper organization", "text": "The rest of the paper is organized as follows: the model structures of the PTN approach will be presented in Section II, which is followed by the implementation details described in Section III. The experiments are reported in Section IV, and some conclusions and future work will be presented in Section V."}, {"heading": "II. PHONETIC NEURAL MODELING FOR LID", "text": "In this section, we present the models that employ phonetic information in the RNN LID. Although the phonetic aware approach treats phonetic information as auxiliary knowledge, the PTN approach is more aggressive and uses phonetic information as the only input of the RNN LID system. Both are depicted in Fig. 1.\ninput\nASR/LID labels LID output\ninput\nDNN DNN\ninput\nASR/LID labels\nLID output\ninput\nDNN DNN\n(a) (b)\nFig. 1: LID models employing phonetic information: (a) the phonetic aware acoustic RNN; (b) the PTN model. Both models consist a phonetic DNN to produce phonetic features. The LID model in our study is an LSTM-RNN."}, {"heading": "A. Phonetic aware acoustic neural model", "text": "The natural idea to utilize phonetic information in the acoustic RNN LID system is to treat it as an auxiliary knowledge, which we call a phonetic aware approach. Intuitively, this can be regarded as a knowledge-fusion method that uses both the phonetic and acoustic features to learn LID models. Fig. 1 (a) illustrates the diagram of this model. A phonetic DNN model (may be in any structure, such as FFDNN, RNN, TDNN) is used to produce frame-level phonetic feature. This feature can be read from any place of the phonetic DNN, e.g., the\noutput or the last hidden layer, and be propagated to the LID model, an LSTM-RNN in our study. This propagated phonetic information can be accepted by the LID model in different ways. For example, it can be as part of the input, or as an additional term of the gate or non-linear activation functions."}, {"heading": "B. Phonetic temporal neural model", "text": "The second model, which we call the PTN model, replaces the acoustic feature by the phonetic feature, thus entirely relies on the properties of the phonetic representation. Importantly, this learning is based on the RNN model, therefore the temporal patterns of the phonetic features can be learned. This PTN system is shown in Fig. 1 (b). Although the PTN model is a special, \u2018aggressive\u2019 case of the phonetic aware approach, the success of this model offers a deeper insight for the LID task as it rediscovers the importance of the temporal properties of phonetic representations."}, {"heading": "C. Understanding PTN", "text": "The rationality of the PTN approach can be understood in two perspectives: the phonetic perspective and the transfer learning perspective. The former relates to what information is important for LID, and the latter relates to how this information is learned.\nPhonetic perspective: The PTN approach adopts the longstanding hypothesis that languages should be discriminated by phonetic properties rather than spectral properties. This hypothesis is held by the conventional token-based approach, e.g., the PRLM model. However it has been largely overlooked since the success of the i-vector approach, which achieved pretty good performance using only raw acoustic features. Until recently, Song et al [35] rediscovered the value of phonetic features in the i-vector model. The PTN approach proposed here follows the same idea and rediscovers the value of phonetic features in the neural model. We conjecture that this value is more important for the neural model compared to the i-vector model, as its decision is based on only a few frames thus requires the feature involves more language-related information and less noise and uncertainties. The i-vector model, in contrast, can utilize more speech signals to make the decision, hence can discover language-related information from the distributional patterns even with raw acoustic features.\nBoth the PTN approach and the historical token-based approach share the same idea of utilizing phonetic information and model the temporal patterns, but they are fundamentally different. The first difference is that the phonetic information in the PTN approach is frame-level, while the information of the conventional token-based methods is unit-level. Therefore. the PTN approach can represent phonetic properties in a higher temporal resolution. The second difference is that conventional token-based methods represent phonetic information as a phone sequences derived from a phone recognition, while the PTN approach represents phonetic information as a feature vector that corresponds to a position of the frame in the phonetic space, hence more detailed phonetic information is represented. The final difference is that the back-end model\n4 of the conventional token-based approach is an n-gram LM based on discrete tokens, while the back-end model of the PTN approach is an RNN, which functions similarly as an RNN LM, but the model is based on continuous phonetic features, and it is trained with a task-oriented criterion that discriminates the target languages.\nTransfer learning perspective: The second perspective to understand the PTN approach is transfer learning [38]. It is well known that DNNs have the superiority in learning taskoriented features from raw data. This is the hypothesis behind the conventional acoustic RNN LID methods: if the neural model is successfully trained, it can learn any useful information from the raw acoustic features layer by layer, including the phonetic information. It therefore seems not necessary at all to design such a phonetic feature learning and modeling architecture, as the PTN approach. Our argument is that using the language labels alone to learn LID-related information from raw acoustic features is highly ineffective, because these labels are too coarse to provide sufficient supervision. By the PTN model, the feature extractor is trained on speech data labelled with phones or words. These labels are more fine-grained than the labels for languages, providing more informative supervision during the model training, leading to a strong DNN model for phonetic feature extraction. Fortunately, the phone discrimination and language identification are naturally correlated (according to our phonetic perspective), which means that the phonetic feature leaned with the goal of phone discrimination involves rich information for LID. This is a transfer learning that uses a related task (i.e., phone discrimination) to learn features for another task (LID).\nThe PTN approach also involves another two transfer learning schemes: cross languages and cross conditions (databases). This means that the phonetic DNN can be learned with any speech data in any languages. This property was known in the token-based LID [20], however it is more important for the phonetic neural models, as training the phonetic DNN often requires a large amount of speech data which is often not available for the target languages and the operating conditions that are under test. Moreover, it is also possible to train the phonetic DNN with multilingual, multi-conditional data [39], resulting in robust and reliable phonetic feature extraction.\nPutting them together, we highlight that the PTN approach utilizes a detailed phonetic representation (DNN phonetic feature) and a powerful temporal model (LSTM-RNN) to capture the phonetic temporal properties of a language with a high temporal resolution. Additionally, it utilizes three types of transfer learning to ensure that the phonetic feature is representative and robust. Therefore, the PTN approach is highly powerful and flexible. Most importantly, its success reconfirmed the belief of many LID researchers that the phonetic temporal information is the most valuable knowledge in language discrimination, not only for humans but also for machines."}, {"heading": "III. MODEL STRUCTURE", "text": "This section presents the details of the phonetic neural LID models, including both the phone aware acoustic RNN model\nand PTN model. The phonetic DNN can be implemented in various DNN structures, though we chose two typical models in our study, one is RNN and the other is TDNN. Both the two models can learn long-term phonetic patterns and have shown great advantage in speech recognition [40], [41].\nFor the LID neural model, we choose the LSTM-RNN. One reason for this choice is that LSTM-RNN has been demonstrated to perform well in both the pure neural LID approach [32] and the neural-probabilistic hybrid LID approach [37]. Another reason is that the RNN model can learn the temporal properties of speech signals, which is in accordance with our motivation to model the phonetic dynamics as in the conventional PRLM approach [21]. We first describe the LSTM-RNN structure used for LID, and then presents the model structures of the phonetic aware acoustic RNN model and PTN model."}, {"heading": "A. LSTM-RNN LID", "text": "The LSTM-RNN model used in this study is a one-layer RNN model, where the hidden units are LSTM. The structure proposed by Sak et al. [42] is used, as shown in Fig. 2.\nThe associated computation is given as follows:\nit = \u03c3(Wixxt +Wirrt\u22121 +Wicct\u22121 + bi)\nft = \u03c3(Wfxxt +Wfrrt\u22121 +Wfcct\u22121 + bf )\nct = ft ct\u22121 + it g(Wcxxt +Wcrrt\u22121 + bc) ot = \u03c3(Woxxt +Worrt\u22121 +Wocct + bo)\nmt = ot h(ct) rt = Wrmmt pt = Wpmmt yt = Wyrrt +Wyppt + by\nIn the above equations, the W terms denote weight matrices and those associated with the cells were constrained to be diagonal in our implementation. The b terms denote bias vectors. xt and yt are the input and output symbols respectively; it, ft, ot represent respectively the input, forget and output gates; ct is the cell and mt is the cell output. rt and pt are two output components derived from mt, where rt is recurrent and fed to\n5 the next time step, while pt is not recurrent and contributes to the present output only. \u03c3(\u00b7) is the logistic sigmoid function, and g(\u00b7) and h(\u00b7) are non-linear activation functions, chosen to be hyperbolic. denotes element-wise multiplication.\nIn this study, the LSTM layer consists of 1, 024 cells, and the dimensionality of both the recurrent and non-recurrent projections is set to 256. The natural stochastic gradient descent (NSGD) algorithm [43] is employed to train the model. During the training and decoding, the cells were reset for each 20 frames to ensure only short-time patterns are learned."}, {"heading": "B. Phonetic aware acoustic neural LID", "text": "In the phonetic aware model, the phonetic feature is read from the phonetic DNN and is propagated to the LID RNN as additional information to assist the acoustic neural LID. The phonetic feature can be read either from the output (phone posterior) or the last hidden layer (logits), and can be propagated to different components of the RNN LID model, e.g., the input/forget/output gates and/or the non-linear activations functions.\nFig. 3 (a) illustrates a simple configuration, where the phonetic DNN is a TDNN model, and the feature is read from the last hidden layer. The phonetic feature is propagated to the non-linear function g(\u00b7). With this configure, most computation of the LID RNN remains the same, except that the cell value should be updated as follows:\nct = ft ct\u22121 + it g(Wcxxt +Wcrrt\u22121 +W \u2032c\u03c6\u03c6t + bc)\nwhere \u03c6t is the phonetic feature obtained from the phonetic DNN."}, {"heading": "C. Phonetic temporal neural LID", "text": "The phonetic aware acoustic RNN model is still an acousticbased approach where the phonetic feature is used as auxiliary information. The PTN approach assumes that the phonetic temporal properties cover most of the information for language discrimination, so the acoustic feature is not important any more. Therefore, it removes all acoustic features and uses the phonetic feature as the only input of the LID RNN, as shown in Fig. 3 (b).\nIt is interesting to compare the PTN approach with other LID approaches. Firstly, it can be regarded as a new version of the conventional PRLM approach, particularly the recent PRLM implementation using RNN as the LM [44]. The major difference is that PTN approach uses frame-level phonetic features while the PRLM approach uses token-level phonetic sequence; another difference is that the phonetic information in the PTN approach is much richer than in PRLM, represented as a continuous phonetic vector rather than discrete phonetic symbols.\nThe PTN approach is also correlated to the neuralprobabilistic hybrid approach where the phonetic DNN is used to produce phonetic features, on which the GMM or i-vector model is constructed. The PTN approach uses the same phonetic feature, but employs an RNN model to describe\nt t-2\nt+2\nt-4\nt+4\nt\nin p\nu t\nin p\nu t\nLSTM memory block\nty\n'ty\nty\nt-2\nt+2\nt-4\nt+4\nt 'tyt\nt\nt\n(b)\n(a)\nLSTM memory block\ng\np h\no n es la n gu ag es\nla n\ngu ag\nes p\nh o\nn es\nFig. 3: The phonetic aware RNN LID system (above) and the PTN LID system (bottom). The phonetic feature is read from the last hidden layer of the phonetic DNN which is a TDNN. Then the phonetic feature is propagated to g function for the phonetic aware acoustic RNN LID system, and the only input for the PTN LID system.\nthe dynamic property of the feature, instead of modeling the distributional property using GMM or i-vector models. We will see in the next section that temporal modeling is predominantly important for the phonetic neural models.\nFinally, compared to the phonetic acoustic RNN LID model, the PTN model only differs in the usage of phonetic features. However, since the phonetic features can be learned with a large volume of speech database, they are much more robust against noise and uncertainties (e.g., speaker traits and channel distortions) than the raw acoustic features. This suggests that the PTN approach is more robust against noise than the conventional acoustic RNN approach."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Databases and configurations", "text": "The experiments were conducted on two databases: the Babel database and the AP16-OLR database. The Babel corpus\n6 was collected as part of the IARPA (Intelligence Advanced Research Projects Activity) Babel program, with the aim to develop speech technologies for low-resource languages. The sampling rate is 8 kHz and the sample size is 16 bits. In this paper, we chose speech data of seven languages from the Babel corpus to conduct the study: Assamese, Bengali, Cantonese, Georgian, Pashto Tagalog and Turkish. For each language, a training dataset and a development dataset were officially provided, The training datasets contain both conversational speech and scripted speech and the development datasets only contain conversational speech. We used the entire training set of each language for model training, but randomly selected 2, 000 utterances from the development set of each language to perform test.\nThe training data sets from the seven languages are as follows: Assamese1 75 hours, Bengali2 87 hours, Cantonese3 175 hours, Georgian4 64 hours, Pashto5 111 hours, Tagalog6 116 hours and Turkish7 107 hours. The average duration of the test utterances is 4.15 seconds, ranging from 0.19 seconds to 30.85 seconds.\nThe AP16-OL7 database was originally created by Speechocean Inc., targeting for various speech processing tasks (mainly speech recognition). The entire database involves seven datasets, each in a particular language. The seven languages are: Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean and Vietnamese. The data volume for each language is about 10 hours of speech signals recorded by 24 speakers (12 males and 12 females), and each speaker was recorded about 300 utterances in reading style by mobile phones, with a sampling rate of 16kHz and a sample size of 16 bits. Each dataset was split into a training set consisting of 18 speakers, and a test set consisting of 6 speakers. For Mandarin, Cantonese, Vietnamese and Indonesia, the recording was conducted in a quiet environment. As for Russian, Korean and Japanese, there are 2 recording conditions for each speaker, one is quiet and the other is noisy. The average duration of all 12, 939 test utterances of the seven languages is 4.74 seconds, ranging from 1.08 seconds to 18.06 seconds.\nWe experimented with two structures for the phonetic DNN, one is LSTM-RNN, and the other is TDNN. The LID model is based on the LSTM-RNN. The raw feature used for those models is 23-dimensional Fbanks, with a symmetric 2-frame window for RNN and a symmetric 4-frame window for TDNN to splice neighboring frames. All the experiments were conducted with Kaldi [45]. The default configurations of the Kaldi WSJ s5 nnet3 recipe were used to train the phonetic DNN and the LID RNN. We will start the experiments based on the Babel corpus, and then experiment with the AP16-OLR database.\n1Language collection release IARPA-babel102b-v0.5a. 2Language collection release IARPA-babel103b-v0.4b. 3Language collection release IARPA-babel101b-v0.4c. 4Language collection release IARPA-babel404b-v1.0a. 5Language collection release IARPA-babel104b-v0.4bY. 6Language collection release IARPA-babel106-v0.2g. 7Language collection release IARPA-babel105b-v0.5."}, {"heading": "B. Babel: baseline of bilingual LID", "text": "As the first step, we build three baseline LID systems, one is based on the i-vector model, and the other two are based on LSTM-RNN, using the speech data of two languages of Babel: Assamese and Georgian (AG).\nFor the i-vector baseline, the UBM involves 2, 048 Gaussian components and the dimensionality of the i-vectors is 400. The static acoustic features involves 12-dimensional MFCCs and the log energy. These static features are augmented by their first and second order derivatives, resulting in 39-dimensional feature vectors. In our experiment, we train an SVM for each language to determine the score that a test i-vector belongs to that language. The SVMs are trained on the i-vectors of all the training segments, following the one-verse-rest scheme.\nThe two RNN LID baselines are: a standard RNN LID system (AG-RNN-LID) that discriminates the two languages in its output, and a multi-task system (AG-RNN-MLT) that was trained to discriminate the two languages as well as the phones there. More precisely, the output units of the AG-RNN-MLT are separated into two groups: an LID group that involves two units corresponding to Assamese and Georgian respectively, and an ASR group that involves 3, 349 bilingual senones that are inherited from an HMM/GMM ASR system trained with the speech data of Assamese and Georgian, following the standard WSJ s5 HMM/GMM recipe of Kaldi. The WSJ s5 nnet3 recipe of Kaldi is then used to train the AG-RNN-LID and AG-RNN-MLT systems.\nThe LID task can be conducted by either AG-RNN-LID or AG-RNN-MLT (using the LID output group) at the framelevel, using the frame-level language posteriors they produce. To evaluate the utterance-level performance, the frame-level posteriors are averaged to form the utterance-level posterior, by which the language decision can be achieved.\nThe performance results with the three baseline systems, in terms of Cavg and equal error rate (EER), are shown in Table II. The results indicate that both the LID RNN and the multi-task LID RNN are capable of language discrimination, and the multi-task RNN significantly outperforms both the LID RNN and the i-vector baseline. This indicates that the phone information is very help for neural LID, even if it is simply used as an auxiliary objective in the model training. It also indicates that the phone targets can significantly improve the quality of the LID model, supporting our transfer learning perspective described in Section II.\nThe multi-task learning approach is an interesting way to involve phonetic information in LID. However, the shortcoming is that it requires the training data being labelled in both languages and words/phones. This is highly costly and not\n7 feasible in most scenarios. The phonetic neural models (the phonetic aware acoustic model and the PTN model) do not suffer from this problem."}, {"heading": "C. Babel: phonetic aware bilingual LID", "text": "The phonetic aware architecture uses phonetic features as an auxiliary information to improve the RNN LID. Due to the clear advantage of the AG-RNN-MLT model in language discrimination, we first choose this model to be the candidate of the phonetic DNN to produce phonetic features. The LID RNNs are trained to discriminate the two known languages: Assamese and Georgian. The results are shown in Table III, where four configurations of the \u2018receiver\u2019 of the phonetic feature are tested: the input gate, the forget gate, the output gate and the g function. Compared to the results with the baseline RNNs (Table II), it can be seen that introducing the phonetic feature clearly improves the RNN LID system, and the best configuration (g as receiver) outperforms both the baseline RNN LID and the multi-task RNN LID, on both the frame-level and the utterance-level, in terms of both Cavg and EER. This also suggests that although multi-task training is an effective way to leverage phone knowledge, using this knowledge to train a phonetic DNN and employing it produce phonetic features seems a better way.\nAs a further analysis, we experiment with another three phonetic DNNs. All of them are ASR-oriented RNNs, but are trained with the Assamese data (A-RNN-ASR), the Assamese and Georgian data (AG-RNN-ASR) and the Switchboard data (SWB-RNN-ASR), respectively. The ASR performance of SWB-RNN-ASR is 23.2% on Eval2000. According to the phonetic relevance of these models to our LID task, AGRNN-ASR is the most relevant, A-RNN-ASR is the second, and SWB-RNN-ASR is the least relevant. However, due to the transfer learning perspective, we expect all of them can produce reasonable phonetic features. The results shown in the second group of Table III indicate that more relevant the phonetic DNN is to the LID task, higher the LID performance. This is reasonable and is consistent with the human experiment that subjects who are familiar with the target languages perform better on LID tasks [16]. Nevertheless, it is still highly interesting to observe that clear benefit can be obtained by using phonetic features produced by the phonetic DNN (SWBRNN-ASR) that is trained with a totally irrelevant dataset, in both languages and recording conditions. This confirmed our transfer learning perspective and demonstrated that phonetic features are largely portable and the phonetic DNN can be trained with any data in any languages. This conclusion is particular interesting on LID tasks for low-resource languages, as the phonetic DNN can be trained with data of any richresource languages.\nWe also experiment with an LID-oriented RNN as the phonetic DNN (AG-RNN-LID), as shown in the third group of Table III. We can see it does not provide a powerful phonetic feature, which is not surprising as the information the feature involves is duplicated with the information learned by the LID RNN itself. However, the LID performance is still improved with this LID-oriented feature learning, when compared to\nthe LID RNN baseline (see Table II). This improvement is probably attributed to the deeper architecture that involves both the phonetic DNN and the LID RNN.\nWhen comparing the three training objectives: ASR, LID and MLT, it can be seen that the MLT criterion outperforms the other two. This indicates that the ASR and LID are two closely-related so the multi-task training can be beneficial; on the other hand, it also suggests that LID has some specialities different from ASR, and so LID-oriented tuning is helpful. Nevertheless, the difference between the ASR-oriented training and the MLT-oriented training is not significant, implying that ASR-oriented training is sufficient in many scenarios.\nAs a future study, the TDNN is also tested as the phonetic DNN, as shown in the fourth group of Table III. Two TDNN models are trained: the AG-TDNN-MLT model is a multitask model trained with the Assamese and Georgian data, and the SWB-TDNN-ASR model is an ASR model trained with the Switchboard data. The TDNN is composed of 6 time-delay layers, each is followed by a p-norm layer that reduces the dimensionality of the output from 2, 048 to 256, the same dimension as the recurrent layer of LSTM-RNN. The activations of last hidden layer in the TDNN are read out as the phonetic feature. The ASR performance (WER) of AG-TDNN-MLT is 66.4% and 64.2% for Assamese and Georgian respectively, both better than those with the RNN model AG-RNN-ASR, so is the LID performance. The ASR performance of SWB-TDNN-ASR is 20.8% on the Eval2000 dataset, which is also better than the RNN model SWB-RNNASR. The LID results with these TDNN models are showed in Table III, where we only test the configuration that uses the g function as the information receiver. The results show that TDNN-based phonetic feature exhibits better performance than the RNN-based feature. In the following experiments, we use the TDNN model as the phonetic DNN.\n8"}, {"heading": "D. Babel: PTN for bilingual LID", "text": "In the above phonetic aware experiments, the phonetic feature is used as an auxiliary information. In this section, we evaluate the PTN architecture where the phonetic feature entirely replaces the acoustic features (Fbanks). The experiment is conducted with two phonetic DNN models: AG-TDNN-MLT and SWB-TDNN-ASR.\nThe results are presented in Table IV. We fist observe that the PTN systems perform as well as the best phonetic aware system in Table III, and work even better in terms of the utterance-level EER. For better comparison, we also test the special case of the phonetic aware acoustic LID (Ph. Aware) where both the phonetic and acoustic features are used as the LID RNN input (Ph+Fb). This is the same as the PTN model, but involves additional acoustic feature. The results are shown in the second group of Table IV. It can be found that the feature combination does not provide valuable contribution. This means that the phonetic feature is sufficient to represent the speciality of each language, in accordance with our argument that the language characters are mostly phonetic.\nWe also tried to use the TDNN as the LID model (replacing the RNN) to learn the static (rather than temporal) patterns of the phonetic feature. We found that this model failed to converge. The same phenomenon was also observed in the AP16-OLR experiment (as shown shortly). This is an important observation and it suggests that, with the phonetic feature, only the temporal properties are informative for language discrimination."}, {"heading": "E. Babel: Phonetic knowledge or deep structure?", "text": "The performance with only the phonetic feature (as in PTN) is interesting. A question that one may ask is: how this performance advantage compared to the RNN LID baseline is obtained? We have discussed the phonetic perspective and the transfer learning perspective. These two perspectives jointly state that the main advantage of PTN is the phonetic knowledge learned through transfer learning. However, another possible reason is the deeper architecture consisting both the phonetic DNN and the LID RNN may help learn more abstract features. If the latter reason is more important, than a similar deep structure with only the LID labels can work similarly well. To answer this question, we design the following three experiments to test the contribution from phonetic information (transfer learning) and deep architecture (deep learning):\n\u2022 TDNN-LSTM. The phonetic DNN, TDNN in the experiment, is initialed randomly and trained together with\nthe LID RNN. This means that the TDNN is not trained with ASR labels, but as part of the LID neural model and is trained end-to-end. \u2022 Pretrained TDNN-LSTM. The same as TDNN-LSTM, except that the TDNN is initialized by AG-TDNN-MLT. \u2022 3-layer LSTM-RNN. The 1-layer LSTM-RNN LID model may be not strong enough to learn useful information from acoustic feature, hence leading to the suboptimal performance in Table II. We experiment with a 3-layer LSTM-RNN LID system to test if a simple deeper network can obtain the same performance as with the phonetic feature.\nThe results of these three deep models are shown in Table V. The TDNN-LSTM model totally fails. Using the phonetic TDNN as the initialization helps the training, but it is worse than using the phonetic model directly. This means that the phonetic feature is almost optimal, and it does not require any further LID-oriented end-to-end training. Finally, involving more LSTM layers (3-layer LSTM-RNN) does improve the performance a little when compared to the one-layer LSTM baseline (7.70 vs 9.20, ref. to Table II). This is consistent with our observation in Table III, where using LID-trained phonetic DNN (roughly equal to 2-layer LSTM layers but the two layers are trained layer-by-layer) gives a little help. This means that the improvement caused by the PTN architecture is mainly due to the phonetic information it has learned by the ASR-oriented training (sometimes by multi-task learning), rather than the deep network structure. In other words, it is the transfer learning instead of deep learning that promotes the LID performance with the PTN architecture."}, {"heading": "F. Babel: PTN on seven languages", "text": "We evaluate various LID models on the several languages of the Babel corpus. The i-vector baseline and LSTM-RNN LID baseline are presented firstly. For the i-vector system, a linear discriminative analysis (LDA) is employed to promote language-related information before training SVMs. The dimensionality of the LDA projection space is set to 6. For the phonetic aware RNN and the PTN systems, two phonetic DNNs are evaluated, AG-TDNN-MLT and SWB-TDNN-ASR. For the phonetic aware system, the g function of LSTM-RNN LID model is chosen as the receiver. The results are shown in Tabel VI. It can be seen that both the phonetic aware and the PTN systems outperform the i-vector baseline and the acoustic RNN LID baseline, and the PTN system with the AG-TDNN-MLT phonetic DNN performs the best. The SWBTDNN-ASR performs slightly worse than AG-TDNN-MLT, indicating that familiarity with the language and the channel\n9 condition is beneficial when discriminating languages. However, phonetic DNNs trained with data in foreign languages and in mismatched conditions still work pretty well."}, {"heading": "G. AP16-OLR: PTN on seven languages", "text": "In this section, we test the phonetic RNN LID approach on the AP16-OLR database. Compared to the Babel corpus, the speech signals in AP16-OLR are broad band (sampling rate of 16k Hz), and the acoustic environment is less noisy. Additionally, the speech data of each language is much more limited (10 hours per language), so we assume that training a phonetic DNN model is not feasible with the data of the target languages. We therefore have to resort to transfer learning, i.e., using phonetic DNNs trained on data in other languages.\nAll the test conditions are the same as with the Babel corpus when identifying seven languages. We trained two phonetic DNNs: one is a TDNN model with the same size as the AG-TDNN-ASR model in Section IV-C, but it is trained on the WSJ database, denoted by \u2018WSJ-TDNN-ASR\u2019. The other one is also a TDNN, but it is taken from an industry project, trained on a speech database involving 10, 000 hours of Chinese speech signals with 40 dimentinal Fbanks. The network involves 7 rectifier TDNN layers, each containing 1, 200 hidden units. This model is denoted by \u2018CH-TDNNASR\u2019. The weight matrix of the last hidden layer in CHTDNN-ASR is decomposed by SVD, where the dimensionality of the low-rank matrix is set to 400. This 400-dimensional activations are used as the phonetic feature.\nThe test results on the seven languages in the database are shown in Table VII. It can be seen that the phonetic RNN LID models, either the phonetic aware RNN or the PTN structure, significantly outperform the acoustic RNN baseline system. The PTN system seems much more effective, a bit different from the results on the Babel corpus. This may be attributed to the limited training data so the simpler PTN architecture is preferred. Comparing the WSJ-based phonetic DNN and the Chinese phonetic DNN, the Chinese model is better. This may be attributed to several reasons: (1) the Chinese database contains a larger volume of training data; (2) Chinese is one of the seven languages in AP16-OLR; (3) Chinese is more similar to the rest 6 target languages compared to English, as most of the languages in AP16-OLR are oriental languages.\nAnother observation is that the i-vector system outperforms the phonetic RNN systems in the AP16-OLR experiment, which is inconsistent with the observations in the Babel experiment, where the phonetic systems, both the phonetic aware system and the PTN system, perform much better than the i-vector system. This discrepancy can be attributed to the different data profiles of the two databases. Two factors may contribute the most: (1) the utterances of AP16-OLR are longer than Babel, so the i-vector system is more effective; (2) the speech signals of AP16-OLR are cleaner than those of Babel. The RNN system is more robust against noise, and this advantage is less prominent in clean data. We will examine the two conjectures in the following experiments."}, {"heading": "H. AP16-OLR: utterance duration effect", "text": "To show the relative advantage of the RNN system and the i-vector system on utterances of different length, we select the utterances at least 5 seconds from the test set of AP16OLR, and make up 10 test sets by cutting these utterances into small utterances in different durations, from 0.5 seconds to 5 seconds, with the step set to 0.5 seconds. Each group contains 5, 907 utterances, and each utterance in a group is a random segment excerpted from the original utterance.\nThe performance of the i-vector system and the PTN system on the 10 test sets are shown in Fig 4, in terms of Cavg and EER respectively. It is clear that the RNN system is more effective on short utterances, and if the utterance duration is more than 3 seconds, the i-vector system will win, especially in terms of EER.\nThe duration distribution of the test utterances of the Babel corpus and the AP16-OLR database are shown in Fig. 5. It is clear that the test utterances are generally longer in AP16-OLR than in Babel. This explains why the relative performance of the i-vector system and the RNN system is inconsistent with the two databases."}, {"heading": "I. AP16-OLR: noise robustness", "text": "We test the hypothesis that RNN system is more robust against noise. Firstly white noise is added to the test set of AP16-OLR with different SNR levels, and the noiseaugmented data are tested on two systems: the i-vector baseline and the best PTN system in Table VII, that is, the one with CH-TDNN-ASR as the phonetic DNN. The results of the above two systems with different levels of white noise are shown in Table VIII. It can be seen that the PTN system\n10\nis more noise-robust: with more noise corruptions, the gap between the i-vector system and the PTN system become less significant, and the PTN system is even better than the ivector system in terms of Cavg when the noise level is high (SNR=10). This can be observed more clearly in Fig 6, where the performance degradation rates compared to the noise-free condition are polted. From the figure, it can be found that when the noise gets stronger, the performance degradation with the PTN system is less significant. Since the Babel speech is much more noisy than the AP16-OLR speech, this noise robustness with the RNN approach partly explains why the relative performance is inconsistent with the two databases.\n11"}, {"heading": "V. CONCLUSIONS", "text": "We presented a phonetic temporal neural (PTN) approach for language identification. By this approach, acoustic features are substituted for phonetic features to build the RNN LID model. Our experiments conducted on the Babel database and the AP16-OLR database demonstrated that the PTN approach can provide dramatic performance improvement, even better than a phonetic aware approach that treats the phonetic feature as an auxiliary information. This demonstrated that phonetic temporal information is much more informative than raw acoustic information for discriminating languages. This is a long-standing belief of LID researchers in the PRLM era, but has been doubted since the popularity of the i-vector approach. In the future work, we will improve the performance of the neural LID approach on long sentences, by letting the LSTM-RNN to learn long-time patterns, e.g., by multi-scale RNNs [46]."}], "references": [{"title": "Spoken language characterization", "author": ["M. Harper", "M. Maxwell"], "venue": "Spring handbook of speech processing. Sringer, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Perceptual benchmarks for automatic language identification", "author": ["Y.K. Muthusamy", "N. Jain", "R.A. Cole"], "venue": "Acoustics, Speech, and Signal Processing, 1994. ICASSP-94., 1994 IEEE International Conference on, vol. 1. IEEE, 1994, pp. I\u2013333.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Spoken language recognition-a step toward multilinguality in speech processing", "author": ["J. Navratil"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 9, no. 6, pp. 678\u2013685, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Development of an automatic identification system of spoken languages: Phase i", "author": ["D. Cimarusti", "R. Ives"], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201982., vol. 7. IEEE, 1982, pp. 1661\u20131663.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1982}, {"title": "Language identification using noisy speech", "author": ["J. Foil"], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201986., vol. 11. IEEE, 1986, pp. 861\u2013864.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1986}, {"title": "Approaches to language identification using gaussian mixture models and shifted delta cepstral features.", "author": ["P.A. Torres-Carrasquillo", "E. Singer", "M.A. Kohler", "R.J. Greene", "D.A. Reynolds", "J.R. Deller Jr."], "venue": "Interspeech,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Automatic language identification using gaussian mixture and hidden markov models", "author": ["M.A. Zissman"], "venue": "Acoustics, Speech, and Signal Processing, 1993. ICASSP-93., 1993 IEEE International Conference on, vol. 2. IEEE, 1993, pp. 399\u2013402.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "Comparing gaussian mixture and neural network modelling approaches to automatic language identification of speech", "author": ["J. Willmore", "R. Price", "W. Roberts"], "venue": "Aust. Int. Conf. Speech Sci. & Tech, 2000, pp. 74\u201377.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic language identification using discrete hidden markov model.", "author": ["K. Wong", "M.-h. Siu"], "venue": "in INTERSPEECH. Citeseer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Speaker-independent, textindependent language identification by hmm.", "author": ["S. Nakagawa", "Y. Ueda", "T. Seino"], "venue": "in ICSLP, vol", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "Identifying language from speech: An example of high-level, statisticallybased feature extraction", "author": ["S.C. Kwasny", "B.L. Kalman", "W. Wu", "A.M. Engebretson"], "venue": "Proceedings 14th Annual Conference of the Cognitive Science Society, 1992.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "A segmental approach to automatic language identification", "author": ["Y.K. Muthusamy"], "venue": "Ph.D. dissertation, Jawaharlal Nehru Technological University, 1993.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Language recognition with support vector machines", "author": ["W.M. Campbell", "E. Singer", "P.A. Torres-Carrasquillo", "D.A. Reynolds"], "venue": "ODYSSEY04-The Speaker and Language Recognition Workshop, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Language recognition via i-vectors and dimensionality reduction", "author": ["N. Dehak", "A.-C. Pedro", "D. Reynolds", "R. Dehak"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2011, pp. 857\u2013860.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Language recognition in ivectors space", "author": ["D. Mart\u0131nez", "O. Plchot", "L. Burget", "O. Glembek", "P. Matejka"], "venue": "Proceedings of Interspeech, Firenze, Italy, pp. 861\u2013864, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparison of four approaches to automatic language identification of telephone speech", "author": ["M.A. Zissman"], "venue": "IEEE Transactions on speech and audio processing, vol. 4, no. 1, p. 31, 1996.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "Brno university of technology system for nist 2005 language recognition evaluation", "author": ["P. Matejka", "L. Burget", "P. Schwarz", "J. Cernocky"], "venue": "Speaker and Language Recognition Workshop, 2006. IEEE Odyssey 2006: The. IEEE, 2006, pp. 1\u20137.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Segment-based automatic language identification", "author": ["T.J. Hazen", "V.W. Zue"], "venue": "The Journal of the Acoustical Society of America, vol. 101, no. 4, pp. 2323\u20132331, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Different size multilingual phone inventories and context-dependent acoustic models for language identification.", "author": ["D. Zhu", "M. Adda-Decker", "F. Antoine"], "venue": "InterSpeech,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Lvcsr-based language identification", "author": ["T. Schultz", "I. Rogina", "A. Waibel"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP- 96. Conference Proceedings., 1996 IEEE International Conference on, vol. 2. IEEE, 1996, pp. 781\u2013784.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "Robust spoken language identification using large vocabulary speech recognition", "author": ["J.L. Hieronymus", "S. Kadambe"], "venue": "Acoustics, Speech, and Signal Processing, 1997. ICASSP-97., 1997 IEEE International Conference on, vol. 2. IEEE, 1997, pp. 1111\u20131114.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Modeling prosody for language identification on read and spontaneous speech", "author": ["J.-L. Rouas", "J. Farinas", "F. Pellegrino", "R. Andr\u00e9-Obrecht"], "venue": "Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903). 2003 IEEE International Conference on, vol. 6. IEEE, 2003, pp. I\u201340.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic language identification using deep neural networks", "author": ["I. Lopez-Moreno", "J. Gonzalez-Dominguez", "O. Plchot", "D. Martinez", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 5337\u20135341.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "An end-to-end approach to language identification in short utterances using convolutional neural networks", "author": ["A. Lozano-Diez", "R. Zazo Candil", "J. Gonz\u00e1lez Dom\u0131\u0301nguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH. International Speech and Communication Association, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "LIDsenone extraction via deep neural networks for end-to-end language identification", "author": ["M. Jin", "Y. Song", "I. Mcloughlin", "L.-R. Dai", "Z.-F. Ye"], "venue": "Proc. of Odyssey, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification using time delay neural network d-vector on short utterances", "author": ["M. Kotov", "M. Nastasenko"], "venue": "Speech and Computer: 18th International Conference, SPECOM 2016, Budapest, Hungary, August 23-27, 2016, Proceedings, vol. 9811. Springer, 2016, p. 443.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked long-term tdnn for spoken language recognition", "author": ["D. Garcia-Romero", "A. McCree"], "venue": "Interspeech 2016, pp. 3226\u20133230, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic language identification using  12 long short-term memory recurrent neural networks.", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez- Rodriguez", "P.J. Moreno"], "venue": "in Interspeech,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "A divide-andconquer approach for language identification based on recurrent neural networks", "author": ["G. Gelly", "J.-L. Gauvain", "V. Le", "A. Messaoudi"], "venue": "Interspeech 2016, pp. 3231\u20133235, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification in short utterances using long short-term memory (LSTM) recurrent neural networks", "author": ["R. Zazo", "A. Lozano-Diez", "J. Gonzalez-Dominguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "PloS one, vol. 11, no. 1, p. e0146917, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "I-vector representation based on bottleneck features for language identification", "author": ["Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.-R. Dai"], "venue": "Electronics Letters, vol. 49, no. 24, pp. 1569\u20131570, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Study of senone-based deep neural network approaches for spoken language recognition", "author": ["L. Ferrer", "Y. Lei", "M. McLaren", "N. Scheffer"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 24, no. 1, pp. 105\u2013116, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigation of senone-based longshort term memory rnns for spoken language recognition", "author": ["Y. Tian", "L. He", "Y. Liu", "J. Liu"], "venue": "Odyssey 2016, pp. 89\u201393, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Transfer learning for speech and language processing", "author": ["D. Wang", "T.F. Zheng"], "venue": "Proceedings of Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). IEEE, 2015, pp. 1225\u20131237.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers", "author": ["J.-T. Huang", "J. Li", "D. Yu", "L. Deng", "Y. Gong"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 7304\u20137308.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE transactions on acoustics, speech, and signal processing, vol. 37, no. 3, pp. 328\u2013339, 1989.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1989}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014, pp. 338\u2013342.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "On the use of phone-gram units in recurrent neural networks for language identification", "author": ["C. Salamea", "L.F. D\u2019Haro", "R. de C\u00f3rdoba", "R. San-Segundo"], "venue": "Odyssey 2016, pp. 117\u2013123, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "arXiv preprint arXiv:1609.01704, 2016.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "A couple of studies have been conducted to investigate how our humans use these properties as cues to distinguish languages [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "For example, Muthusamy [5] found that familiarity with a language is an important factor affecting the LID accuracy, and longer speech samples are easier to be identified.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "Navratil [7] evaluated the importance of various types of knowledge, including lexical, phonotactic and prosodic, by humans asked to identify five languages including Chinese, English, French, German and Japanese.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "For instance, Cimarusti used LPC features [8], and Foil et al.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "[9] studied format features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Dynamic features that involve temporal information were also demonstrated to be effective [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "The statistical model used include Gaussian mixture models (GMMs) [11], [12], hidden Markov models (HMMs) [13], [14], neural networks (NNs) [15], [16], and support vector machines (SVMs) [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "low-rank GMM model called \u2018i-vector model\u2019 was proposed and achieved significant success [18], [19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "low-rank GMM model called \u2018i-vector model\u2019 was proposed and achieved significant success [18], [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Multiple variants of the PRLM were proposed, for example, the parallel phone recognition followed by LM (PPRLM) [20], [21], and phone recognition on a multilingual phone set [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "Multiple variants of the PRLM were proposed, for example, the parallel phone recognition followed by LM (PPRLM) [20], [21], and phone recognition on a multilingual phone set [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "Multiple variants of the PRLM were proposed, for example, the parallel phone recognition followed by LM (PPRLM) [20], [21], and phone recognition on a multilingual phone set [22].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": ", syllables [23] and words [24], [25].", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": ", syllables [23] and words [24], [25].", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": ", syllables [23] and words [24], [25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "[9] studied format and prosodic features and found that formant features are more discriminative.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[26] studied a rhythmic model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Muthusamy [16] used pitch variation, duration and syllable rate.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "The duration and pitch patterns were also used by Hazen [22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "[27], who proposed an approach based on a feed-forward deep neural network (FFDNN), which accepts raw acoustic features and produces frame-level LID decisions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": ", CNN [28], [29] and TDNN [30], [31].", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": ", CNN [28], [29] and TDNN [30], [31].", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": ", CNN [28], [29] and TDNN [30], [31].", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": ", CNN [28], [29] and TDNN [30], [31].", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": ", [33], [34].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": ", [33], [34].", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "For example, Song et al [35] used DNN to extract phonetic feature for the i-vector system, and Ferrer et al [36] proposed an DNN i-vector approach that uses posteriors produced by an phone-discriminative FFDNN to compute the Baum-Welch statistics.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "For example, Song et al [35] used DNN to extract phonetic feature for the i-vector system, and Ferrer et al [36] proposed an DNN i-vector approach that uses posteriors produced by an phone-discriminative FFDNN to compute the Baum-Welch statistics.", "startOffset": 108, "endOffset": 112}, {"referenceID": 32, "context": "Tian et al [37] extended this study by using an RNN to produce the posteriors.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "Intuitively, it is a long-standing hypothesis that languages are discriminated by phonetic properties, either distributional or temporal; theoretically, phonetic features represent information at a higher level than that represented by acoustic features, and so are more invariant with respect to noise and channels; pragmatically, it has been demonstrated that phonetic information, either in the form of phone sequences, phone posteriors or phonetic bottleneck features, can significantly improve LID accuracy, in both the conventional PRLM approach [12] and the more modern ivector system [35]\u2013[37].", "startOffset": 552, "endOffset": 556}, {"referenceID": 30, "context": "Intuitively, it is a long-standing hypothesis that languages are discriminated by phonetic properties, either distributional or temporal; theoretically, phonetic features represent information at a higher level than that represented by acoustic features, and so are more invariant with respect to noise and channels; pragmatically, it has been demonstrated that phonetic information, either in the form of phone sequences, phone posteriors or phonetic bottleneck features, can significantly improve LID accuracy, in both the conventional PRLM approach [12] and the more modern ivector system [35]\u2013[37].", "startOffset": 592, "endOffset": 596}, {"referenceID": 32, "context": "Intuitively, it is a long-standing hypothesis that languages are discriminated by phonetic properties, either distributional or temporal; theoretically, phonetic features represent information at a higher level than that represented by acoustic features, and so are more invariant with respect to noise and channels; pragmatically, it has been demonstrated that phonetic information, either in the form of phone sequences, phone posteriors or phonetic bottleneck features, can significantly improve LID accuracy, in both the conventional PRLM approach [12] and the more modern ivector system [35]\u2013[37].", "startOffset": 597, "endOffset": 601}, {"referenceID": 7, "context": ", PRLM [12], but has been largely overlooked since the popularity of the i-vector approach.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "Acoustic DNN i-vector [36], [37] FFDNN [27],RNN [32]", "startOffset": 22, "endOffset": 26}, {"referenceID": 32, "context": "Acoustic DNN i-vector [36], [37] FFDNN [27],RNN [32]", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "Acoustic DNN i-vector [36], [37] FFDNN [27],RNN [32]", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "Acoustic DNN i-vector [36], [37] FFDNN [27],RNN [32]", "startOffset": 48, "endOffset": 52}, {"referenceID": 30, "context": "Phonetic DNN feature i-vector [35] PTN (proposed)", "startOffset": 30, "endOffset": 34}, {"referenceID": 30, "context": "Until recently, Song et al [35] rediscovered the value of phonetic features in the i-vector model.", "startOffset": 27, "endOffset": 31}, {"referenceID": 33, "context": "Transfer learning perspective: The second perspective to understand the PTN approach is transfer learning [38].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "This property was known in the token-based LID [20], however it is more important for the phonetic neural models, as training the phonetic DNN often requires a large amount of speech data which is often not available for the target languages and the operating conditions that are under test.", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "Moreover, it is also possible to train the phonetic DNN with multilingual, multi-conditional data [39], resulting in robust and reliable phonetic feature extraction.", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "Both the two models can learn long-term phonetic patterns and have shown great advantage in speech recognition [40], [41].", "startOffset": 111, "endOffset": 115}, {"referenceID": 36, "context": "Both the two models can learn long-term phonetic patterns and have shown great advantage in speech recognition [40], [41].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "One reason for this choice is that LSTM-RNN has been demonstrated to perform well in both the pure neural LID approach [32] and the neural-probabilistic hybrid LID approach [37].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "One reason for this choice is that LSTM-RNN has been demonstrated to perform well in both the pure neural LID approach [32] and the neural-probabilistic hybrid LID approach [37].", "startOffset": 173, "endOffset": 177}, {"referenceID": 16, "context": "Another reason is that the RNN model can learn the temporal properties of speech signals, which is in accordance with our motivation to model the phonetic dynamics as in the conventional PRLM approach [21].", "startOffset": 201, "endOffset": 205}, {"referenceID": 37, "context": "[42] is used, as shown in Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "The picture is reproduced from [42].", "startOffset": 31, "endOffset": 35}, {"referenceID": 38, "context": "The natural stochastic gradient descent (NSGD) algorithm [43] is employed to train the model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 39, "context": "Firstly, it can be regarded as a new version of the conventional PRLM approach, particularly the recent PRLM implementation using RNN as the LM [44].", "startOffset": 144, "endOffset": 148}, {"referenceID": 40, "context": "All the experiments were conducted with Kaldi [45].", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": "This is reasonable and is consistent with the human experiment that subjects who are familiar with the target languages perform better on LID tasks [16].", "startOffset": 148, "endOffset": 152}, {"referenceID": 41, "context": ", by multi-scale RNNs [46].", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "Deep neural models, particularly the LSTM-RNN model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID methods, although this information has been used in the conventional phonetic LID systems with a great success. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system but accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features. This new model is a reminiscence of the old phonetic LID methods, but the phonetic knowledge here is much richer: it is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional ivector approach on short utterances and in noisy conditions. Keywords\u2014Language identification; Deep neural networks; Long short-term memory", "creator": "LaTeX with hyperref package"}}}