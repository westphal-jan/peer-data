{"id": "1705.03613", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "An initialization method for the k-means using the concept of useful nearest centers", "abstract": "the aim of the k - means is to minimize squared sum of euclidean distance difference from the mean ( ssedm ) of each cluster. the k - means can effectively optimize this function, though but it is clearly too sensitive for initial centers ( seeds ). this paper actually proposed a method for initialization pricing of the k - means using the concept of no useful nearest center for each optimal data point.", "histories": [["v1", "Wed, 10 May 2017 06:19:04 GMT  (388kb)", "http://arxiv.org/abs/1705.03613v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hassan ismkhan"], "accepted": false, "id": "1705.03613"}, "pdf": {"name": "1705.03613.pdf", "metadata": {"source": "CRF", "title": "An initialization method for the k-means using the concept of useful nearest centers", "authors": ["Hassan Ismkhan"], "emails": ["h.ismkhan@bonabu.ac.ir,", "esmkhan@gmail.com"], "sections": [{"heading": null, "text": "The aim of the k-means is to minimize squared sum of Euclidean distance from the mean (SSEDM) of each cluster. The k-means can effectively optimize this function, but it is too sensitive for initial centers (seeds). This paper proposed a method for initialization of the k-means using the concept of useful nearest center for each data point.\nKeywords: k-means, initialization, k-means++, useful centers, useful nearest neighbor."}, {"heading": "1. Introduction", "text": "Let I = {x1, x2\u2026 xn} be an instance with n number of data points, and each data point is characterized by m variables: xi = {xi,1, xi,2, . . . xi,m}. The aim of the k-means [1] is to group these data points into the k disjoint subsets Si (0\u2264i\u2264k), such that minimize the sum of squared Euclidean distances to the mean of each subset (SSEDM) as its objective function. For number k, input instance I, and partitioning solution S, where S = S1 \u222a S2 \u222a . . . \u222a Sk, SSEDM can be defined as following equation:\n\ud835\udc46\ud835\udc46\ud835\udc38\ud835\udc37\ud835\udc40(\ud835\udc46) = \ud835\udc46\ud835\udc46\ud835\udc38\ud835\udc37\ud835\udc40 (\ud835\udc3c, \ud835\udc46, \ud835\udc58) = \u2211 \ud835\udc46\ud835\udc46\ud835\udc38\ud835\udc37\ud835\udc40(\ud835\udc46\ud835\udc56) \ud835\udc58 \ud835\udc56 =1 (1)\nWhere SSEDM(Si) is partial SSEDM of cluster Si and can be defined as following equation:\n\ud835\udc46\ud835\udc46\ud835\udc38\ud835\udc37\ud835\udc40(\ud835\udc46\ud835\udc56) = \u2211 dis(\ud835\udc43, \ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b(\ud835\udc46\ud835\udc56)) 2 \u2200\ud835\udc43\u2208\ud835\udc46\ud835\udc56 (2)\n1 Department of Computer Engineering, University of Bonab.\nEmail: h.ismkhan@bonabu.ac.ir, esmkhan@gmail.com\nAlthough exactly minimizing SSEDM can not guarantee the best quality for clustering solution [2], and exactly minimizing this function is an NP-Hard problem [3], not only this function is popular to measure quality of clustering solutions, it is also used as an objective function in the kmeans, which is one of the most influential data mining algorithms [4]. The k-means operates instructions as follows: (1) select k number of points as initial centers, (2) assign each data point to its nearest center, (3) updates location of centers, (4) repeat instructions 2 and 3 until convergence. The k-means can effectively minimize SSEDM for instances with linearly separable clusters [18], but it is too sensitive to initial centers. Therefore, many initialization methods are proposed in the literature. References [5] [6] propose an initialization method, namely Maxmin, which chooses the first center randomly, and in the rest, it selects a data point with largest distance to its nearest center as a new center.\nAmong famous initialization methods, k-means++ [7] is one of the most accurate algorithm and easy to be implemented. k-means++ chooses the first center, among data points of data set, randomly. It chooses a data point x \u2208 X as the i-th center with probability \ud835\udc37(\ud835\udc4b)2\n\u2211 \ud835\udc37(\ud835\udc4b)2\ud835\udc65\u2208\ud835\udc4b , where D(x)\ndenotes the shortest distance from a data point to the closest center we have already chosen. As the main aim of the k-means is to minimize SSEDM, this paper proposes a novel method. It is based on the concept of useful centers of each data point. Results of performed experiments shows how it can outperform k-means++ on real world datasets."}, {"heading": "2. The proposed initialization algorithm", "text": "Before stating the proposed initialization method, we need to state two simple definitions.\nDefinition 2. The concept of useful nearest center: a center C is useful for a data point P, if it is not a useless nearest center of P.\nExample 3.1. In Figure. 1, C1 and C2 are useful center for P. C3 is closer to P than C1, but C3 is not a useful center for P, because C2 causes that definition 2 does not meet for P and C3.\nThe proposed initialization method, selects the first center with the smallest value on the first axis,\nthen in each of next iterations, a data point P with the largest value of\naverage \u2200\ud835\udc50\u2208\ud835\udc48\ud835\udc41\ud835\udc36\ud835\udc43 (\ud835\udc51\ud835\udc56\ud835\udc60(\ud835\udc43,\ud835\udc50))\nmax \u2200\ud835\udc50\u2208\ud835\udc48\ud835\udc41\ud835\udc36\ud835\udc43\n(\ud835\udc51\ud835\udc56\ud835\udc60(\ud835\udc43,\ud835\udc50)) \u00d7\n\u2211 ln(\ud835\udc51\ud835\udc56\ud835\udc60(\ud835\udc43, \ud835\udc50))\u2200\ud835\udc50\u2208\ud835\udc48\ud835\udc41\ud835\udc36\ud835\udc43 is selected as the next center, where UNCP is the set of useful nearest centers of P. This process is continued until required number of centers are selected. It should be noted that in implementation, if a recently added center Ci-1 is useless for P, then it is ignored; otherwise, it is added to UNCP, and in this case, some data points may be dropped out from UNCP.\n3. Results of experiments\nincluding iris, human activity recognition (HAR), and shuttle which are available via UCI machine learning repository. The results are introduced in Table. 1. For iris and shuttle, UNC-KM wins with high gap of obtained SSEDM.\nTable. 1. Result of competitors on UCI datasets\nAverage SSEDM of 50 runs Average runtime (second) of 50 runs Iris HAR shuttle Iris HAR shuttle\nKM 9.95E+01 184867.5 8.13E+08 0.00E+00 1.70E+00 4.26E-01 MK++ 8.41E+01 184820.3 7.99E+08 0.00E+00 1.64E+00 4.34E-01 UNC-KM 7.89E+01 204329.6 5.44E+08 0.00E+00 2.80E+00 2.99E-01\nConclusion\nThis paper propose a novel and simple method to initialize centers of the k-means. The proposed method is based on the concept of useful nearest center which is defined for each data point. After each iteration, when a new center is added, the list of useful nearest center of each data point is updated. During each iteration, a data point with the largest amount of a value, which is depends on distance from its nearest center, is selected as the next center. In comparison to the random kmeans and k-means++, not only the proposed algorithm has acceptable runtime, it obtain better SSEDM with high gap from the random k-means and k-means++."}, {"heading": "Acknowledgment", "text": "This research is supported by University of Bonab, via author personal grant.\nReferences\n[1] J. B. MacQueen, \"Some methods for classification and analysis of multivariate observations,\" in Fifth Berkeley\nSymposium on Mathematical Statistics and Probability, Berkeley, California, 1967.\n[2] H. Ismkhan, \"A.1D-C: A novel fast automatic heuristic to handle large-scale one-dimensional clustering,\"\nApplied Soft Computing, vol. 52, p. 1200\u20131209, 2017.\n[3] P. Drineas, A. M. Frieze, R. Kannan, S. S. Vempala and V. Vinay, \"Clustering large graphs via the singular value\ndecomposition,\" Machine Learning, vol. 56, no. 1-3, pp. 9-33, 2004.\n[4] X. Wu, V. Kumar, J. Quinlan, J. Ghosh, Q. Yang, H. Motoda, G. J. McLachlan, A. Ng, B. Liu, P. S. Yu, Z.-H.\nZhou, M. Steinbach, D. J. Hand and D. Steinberg, \"Top 10 algorithms in data mining,\" Knowledge and Information Systems, vol. 14, no. 1, pp. 1-37, 2008. [5] T. F. Gonzalez, \"Clustering to minimize the maximum intercluster distance,\" Theoretical Computer Science, vol.\n38, no. 2\u20133, p. 293\u2013306, 1985.\n[6] I. Katsavounidis, C.-C. J. Kuo and Z. Zhang, \"A new initialization technique for generalized Lloyd iteration,\"\nIEEE Signal Processing Letters, vol. 1, no. 10, p. 144\u2013146, 1994.\n[7] D. Arthur and S. Vassilvitskii, \"k-means++ : The Advantages of Careful Seeding,\" in the eighteenth annual ACM-\nSIAM symposium on Discrete algorithms, New Orleans, Louisiana, 2007."}], "references": [{"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "Fifth Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, California, 1967.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1967}, {"title": "A.1D-C: A novel fast automatic heuristic to handle large-scale one-dimensional clustering", "author": ["H. Ismkhan"], "venue": "Applied Soft Computing, vol. 52, p. 1200\u20131209, 2017.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Clustering large graphs via the singular value decomposition", "author": ["P. Drineas", "A.M. Frieze", "R. Kannan", "S.S. Vempala", "V. Vinay"], "venue": "Machine Learning, vol. 56, no. 1-3, pp. 9-33, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "P.S. Yu", "Z.-H. Zhou", "M. Steinbach", "D.J. Hand", "D. Steinberg"], "venue": "Knowledge and Information Systems, vol. 14, no. 1, pp. 1-37, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["T.F. Gonzalez"], "venue": "Theoretical Computer Science, vol. 38, no. 2\u20133, p. 293\u2013306, 1985.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1985}, {"title": "A new initialization technique for generalized Lloyd iteration", "author": ["I. Katsavounidis", "C.-C.J. Kuo", "Z. Zhang"], "venue": "IEEE Signal Processing Letters, vol. 1, no. 10, p. 144\u2013146, 1994.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "k-means++ : The Advantages of Careful Seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "the eighteenth annual ACM- SIAM symposium on Discrete algorithms, New Orleans, Louisiana, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "The aim of the k-means [1] is to group these data points into the k disjoint subsets Si (0\u2264i\u2264k), such that minimize the sum of squared Euclidean distances to the mean of each subset (SSEDM) as its objective function.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "Although exactly minimizing SSEDM can not guarantee the best quality for clustering solution [2], and exactly minimizing this function is an NP-Hard problem [3], not only this function is popular to measure quality of clustering solutions, it is also used as an objective function in the kmeans, which is one of the most influential data mining algorithms [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "Although exactly minimizing SSEDM can not guarantee the best quality for clustering solution [2], and exactly minimizing this function is an NP-Hard problem [3], not only this function is popular to measure quality of clustering solutions, it is also used as an objective function in the kmeans, which is one of the most influential data mining algorithms [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Although exactly minimizing SSEDM can not guarantee the best quality for clustering solution [2], and exactly minimizing this function is an NP-Hard problem [3], not only this function is popular to measure quality of clustering solutions, it is also used as an objective function in the kmeans, which is one of the most influential data mining algorithms [4].", "startOffset": 356, "endOffset": 359}, {"referenceID": 4, "context": "References [5] [6] propose an initialization method, namely Maxmin, which chooses the first center randomly, and in the rest, it selects a data point with largest distance to its nearest center as a new center.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "References [5] [6] propose an initialization method, namely Maxmin, which chooses the first center randomly, and in the rest, it selects a data point with largest distance to its nearest center as a new center.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "Among famous initialization methods, k-means++ [7] is one of the most accurate algorithm and easy to be implemented.", "startOffset": 47, "endOffset": 50}], "year": 2017, "abstractText": "The aim of the k-means is to minimize squared sum of Euclidean distance from the mean (SSEDM) of each cluster. The k-means can effectively optimize this function, but it is too sensitive for initial centers (seeds). This paper proposed a method for initialization of the k-means using the concept of useful nearest center for each data point.", "creator": "Microsoft\u00ae Word 2013"}}}