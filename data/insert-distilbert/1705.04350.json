{"id": "1705.04350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Imagination improves Multimodal Translation", "abstract": "multimodal machine translation is the task of computer translating sentences in obtaining a visual context. we decompose this problem into making two sub - tasks : learning to locally translate and learning alongside visually grounded art representations. in a multitask learning framework, translations are learned in supporting an attention - based encoder - decoder, and grounded representations are learned through image representation prediction. our approach improves translation performance compared to the state of the art operations on the multi30k dataset. furthermore, it is equally effective if we train the image prediction synthesis task on both the external ms coco dataset, and we find improvements if we train the translation query model on the external mba news commentary parallel text.", "histories": [["v1", "Thu, 11 May 2017 18:49:17 GMT  (1023kb,D)", "http://arxiv.org/abs/1705.04350v1", "Under review"], ["v2", "Fri, 7 Jul 2017 09:18:55 GMT  (1023kb,D)", "http://arxiv.org/abs/1705.04350v2", "Clarified main contributions, minor correction to Equation 8, additional comparisons in Table 2, added more related work"]], "COMMENTS": "Under review", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["desmond elliott", "\\'akos k\\'ad\\'ar"], "accepted": false, "id": "1705.04350"}, "pdf": {"name": "1705.04350.pdf", "metadata": {"source": "CRF", "title": "Imagination improves Multimodal Translation", "authors": ["Desmond Elliott", "\u00c1kos K\u00e1d\u00e1r"], "emails": ["d.elliott@ed.ac.uk,", "a.kadar@uvt.nl"], "sections": [{"heading": "1 Introduction", "text": "Multimodal machine translation is the task of translating sentences in context, such as images paired with a parallel text (Specia et al., 2016). This is an emerging task in the area of multilingual multimodal natural language processing. Progress on this task may prove useful for translating the captions of the images illustrating online news articles, and for multilingual closed captioning in international television and cinema.\nInitial efforts have not convincingly demonstrated that visual context can improve translation quality. In the results of the First Multimodal Translation Shared Task, only three systems outperformed an off-the-shelf text-only phrase-based machine translation model, and the best performing system was equally effective with or without the visual features (Specia et al., 2016). There remains an open question about how translation models should take advantage of visual context.\nTranslation Decoder\nWe present a multitask learning model that decomposes multimodal translation into learning a translation model and learning visually grounded representations. This decomposition means that our model can be trained over external datasets of parallel text or described images, making it possible to take advantage of existing resources. Figure 1 presents an overview of our model, Imagination, in which source language representations are shared between tasks through the Shared Encoder. The translation decoder is an attention-based neural machine translation model (Bahdanau et al., 2015), and the image prediction decoder is trained to predict a global feature vector of an image that is associated with a sentence (Chrupa\u0142a et al., 2015, IMAGINET). This decomposition encourages grounded learning in the shared encoder because the IMAGINET decoder is trained to imagine ar X iv :1\n70 5.\n04 35\n0v 1\n[ cs\n.C L\n] 1\n1 M\nay 2\n01 7\nthe image associated with a sentence. It has been shown that grounded representations are qualitatively different from their text-only counterparts (Ka\u0301da\u0301r et al., 2016) and correlate better with human similarity judgements (Chrupa\u0142a et al., 2015). We assess the success of the grounded learning by evaluating the image prediction model on an image\u2013sentence ranking task to determine if the shared representations are useful for image retrieval (Hodosh et al., 2013). In contrast with most previous work, our model does not take images as input at translation time, rather it learns grounded representations in the shared encoder.\nWe evaluate Imagination on the Multi30K dataset (Elliott et al., 2016) using a combination of in-domain and out-of-domain data. In the indomain experiments, we find that multitasking translation with image prediction improves over existing approaches. Our model achieves 55.8 Meteor as a single model trained on multimodal in-domain data, and 57.6 Meteor as an ensemble.\nIn the experiments with out-of-domain resources, we find that the improvement in translation quality holds when training the IMAGINET decoder on the MS COCO dataset of described images (Chen et al., 2015). Furthermore, if we significantly improve our text-only baseline using out-of-domain parallel text from the News Commentary corpus (Tiedemann, 2012), we still find improvements in translation quality from the auxiliary image prediction task. Finally, we report a state-of-the-art result of 59.3 Meteor on the Multi30K corpus when ensembling models trained on in- and out-of-domain resources.1"}, {"heading": "2 Problem Formulation", "text": "Multimodal translation is the task of producing target language translation y, given the source language sentence x and additional context, such as an image v. Concretely, let x be a source language sentence consisting N tokens: x1, x2, . . ., xn and let y be a target language sentence consisting M tokens: y1, y2, . . ., ym. The training data consists of tuplesD \u2208 (x, y, v), where x is a description of image v, and y is a translation of x.\nMultimodal translation has previously been framed as minimising the negative log-likelihood of a translation model that is additionally conditioned on the image, i.e. J(\u03b8) = \u2212 \u2211\nj log p(yj |y<j , x, v). Here, we decompose 1The implementation will be available upon publication.\nthe problem into learning to translate and learning visually grounded representations. The decomposition is based on sharing parameters \u03b8 between these two tasks, and learning task-specific parameters \u03c6. We learn the parameters in a multitask model with shared parameters in the source language encoder. The translation model has taskspecific parameters \u03c6t in the attention-based decoder, which are optimized through the translation loss JT (\u03b8, \u03c6t). Grounded representations are learned through an image prediction model with task-specific parameters \u03c6g in the imageprediction decoder by minimizing JG(\u03b8, \u03c6g). The joint objective is given by mixing the translation and image prediction tasks with the parameter w:\nJ(\u03b8, \u03c6) = wJT (\u03b8, \u03c6 t) + (1\u2212 w)JG(\u03b8, \u03c6g) (1)\nOur decomposition of the problem makes it straightforward to optimise this objective without paired tuples, e.g. where we have an external dataset of described images Dimage \u2208 (x, v) or an external parallel corpus Dtext \u2208 (x, y).\nWe train our multitask model following the approach of Luong et al. (2016). We define a primary task and an auxiliary task, and a set of parameters \u03b8 to be shared between the tasks. A minibatch of updates is performed for the primary task with probabilityw, and for the auxiliary task with 1\u2212w. The primary task is trained until convergence and weight w determines the frequency of parameter updates for the auxiliary task."}, {"heading": "3 Imagination Model", "text": ""}, {"heading": "3.1 Shared Encoder", "text": "The encoder network of our model learns a representation of a sequence of N tokens x1...n in the source language with a bidirectional recurrent neural network (Schuster and Paliwal, 1997). This representation is shared between the different tasks. Each token is represented by a one-hot vector xi, which is mapped into an embedding ei through a learned matrix E:\nei = xi \u00b7E (2)\nA sentence is processed by a pair of recurrent neural networks, where one captures the sequence left-to-right (forward), and the other captures the sequence right-to-left (backward). The initial state\nof the encoder h\u22121 is a learned parameter:\n\u2212\u2192 hi = \u2212\u2212\u2192 RNN( \u2212\u2212\u2192 hi\u22121, ei) (3) \u2190\u2212 hi = \u2190\u2212\u2212 RNN( \u2190\u2212\u2212 hi\u22121, ei) (4)\nEach token in the source language input sequence is represented by a concatenation of the forward and backward hidden state vectors:\nhi = [ \u2212\u2192 hi; \u2190\u2212 hi] (5)"}, {"heading": "3.2 Neural Machine Translation Decoder", "text": "The translation model decoder is an attentionbased recurrent neural network (Bahdanau et al., 2015). Tokens in the decoder are represented by a one-hot vector yj, which is mapped into an embedding ej through a learned matrix Ey:\nej = yj \u00b7Ey (6)\nThe inputs to the decoder are the previously predicted token yj\u22121, the previous decoder state dj\u22121, and a timestep-dependent context vector cj calculated over the encoder hidden states:\ndj = RNN(dj\u22121,yj\u22121, ej) (7)\nThe initial state of the decoder d-1 is a nonlinear transform of the mean of the encoder states:\nd-1 = tanh( 1\nN N\u2211 i hi) (8)\nThe context vector cj is a weighted sum over the encoder hidden states, whereN denotes the length of the source sentence:\ncj = N\u2211 i=1 \u03b1jihi (9)\nThe \u03b1ji values are the proportion of which the encoder hidden state vectors h1...n contribute to the decoder hidden state when producing the jth token in the translation. They are computed by a feed-forward neural network, where va, Wa and Ua are learned parameters:\n\u03b1ji = exp(eji)\u2211N l=1 exp(eli)\n(10)\neji = va \u00b7 tanh(Wa \u00b7 dj\u22121 +Ua \u00b7 hi) (11)\nFrom the hidden state dj the network predicts the conditional distribution of the next token yj , given\na target language embedding ej\u22121 of the previous token, the current hidden state dj, and the calculated context vector cj . Note that at training time, yj\u22121 is the true observed token; whereas for unseen data we use the inferred token y\u0302j\u22121 sampled from the output of the softmax:\np(yj |y<j , c) = softmax(tanh(ej\u22121 + dj + cj)) (12)\nThe translation model is trained to minimise the negative log likelihood of predicting the the target language output:\nJNLL(\u03b8, \u03c6 t) = \u2212 \u2211 j log p(yj |y<j , x) (13)"}, {"heading": "3.3 Imaginet Decoder", "text": "The image prediction decoder is trained to predict the visual feature vector of the image associated with a sentence (Chrupa\u0142a et al., 2015). It encourages the shared encoder to learn grounded representations for the source language.\nWe represent a sentence by first producing a sequence of the concatenated hidden state vectors in the Encoder (Section 3.1). Then, the same way we initialise the hidden state of the decoder (Eqn. 8), we take the average of the encoder hidden states to represent the sentence as a single vector h = 1N \u2211N i hi. This is the input to a feed-forward neural network that predicts the visual feature vector v\u0302 of the sentence with parameters Wvis:\nv\u0302 = tanh(Wvis \u00b7 h) (14)\nThis decoder is trained to predict the true image vector v with a margin-based objective, parameterised by the minimum margin \u03b1, and the cosine distance d(\u00b7, \u00b7). The contrastive examples v\u2032 are drawn from the other instances in a minibatch:\nJMAR(\u03b8, \u03c6 t) = \u2211 v\u2032 6=v max{0, \u03b1\u2212 d(v\u0302,v)\n+ d(v\u0302,v\u2032)} (15)"}, {"heading": "4 Data", "text": "We evaluate our model using the benchmark Multi30K dataset (Elliott et al., 2016), which is the largest collection of images paired with sentences in multiple languages. This dataset contains 31,014 images paired with an English language sentence and a German language translation: 29,000 instances are reserved for training, 1,014 for development, and 1,000 for evaluation.2\n2Multi30K also contains 155K independently collected descriptions for German and English. We do not use this data.\nThe English and German sentences are preprocessed by normalising the punctuation, lowercasing and tokenizing the text using the Moses toolkit. We additionally decompound the German text using Zmorge (Sennrich and Kunz, 2014). This results in vocabulary sizes of 10,214 types for English and 16,022 for German.\nWe also use two external datasets to evaluate our model: the MS COCO dataset of English described images (Chen et al., 2015), and the English-German News Commentary parallel corpus (Tiedemann, 2012). When we perform experiments with the News Commentary corpus, we first calculate a 17,597 sub-word vocabulary using SentencePiece (Schuster and Nakajima, 2012) over the concatentation of the Multi30K and News Commentary datasets. This gives us a shared vocabulary for the external data that reduces the number of out-of-vocabulary tokens.\nImages are represented by 2048D vectors extracted from the \u2018pool5/7x7 s1\u2019 layer of the GoogLeNet v3 CNN (Szegedy et al., 2015)."}, {"heading": "5 Experiments", "text": "We evaluate our multitasking approach with inand out-of-domain resources. We start by reporting results of models trained using only the Multi30K dataset. We also report the results of training the IMAGINET decoder with the COCO dataset. Finally, we report results on incorporating the external News Commentary parallel text into our model. Throughout, we report performance of the En\u2192De translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) against lowercased tokenized references."}, {"heading": "5.1 Hyperparameters", "text": "The encoder is a 1000D Gated Recurrent Unit bidirectional recurrent neural network (Cho et al., 2014, GRU) with 620D embeddings. We share all of the encoder parameters between the primary and auxiliary task. The translation decoder is a 1000D GRU recurrent neural network, with a 2000D context vector over the encoder states, and 620D word embeddings (Sennrich et al., 2017). The Imaginet decoder is a single-layer feed-forward network, where we learn the parameters Wvis \u2208 R2048x2000 to predict the true image vector with \u03b1 = 0.1 for the Imaginet objective (Equation 15). The models are trained using the Adam optimiser with the default hyperparameters (Kingma and Ba, 2015) in minibatches of 80 instances. The translation task is defined as the primary task and convergence is reached when BLEU has not increased for five epochs on the validation data. Gradients are clipped when their norm exceeds 1.0. Dropout is set to 0.2 for the embeddings and the recurrent connections in both tasks (Gal and Ghahramani, 2016). Translations are decoded using beam search with 12 hypotheses."}, {"heading": "5.2 In-domain experiments", "text": "We start by presenting the results of our multitask model trained using only the Multi30K dataset. We compare against state-of-the-art approaches and text-only baselines. Moses is the phrase-based machine translation model (Koehn et al., 2007) reported in (Specia et al., 2016). NMT is a textonly neural machine translation model. Calixto\net al. (2017a) is a double-attention model over the source language and the image. Calixto et al. (2017b) is a multimodal translation model that conditions the decoder on semantic image vector extracted from the VGG-19 CNN. Hitschler et al. (2016) uses visual features in a target-side retrieval model for translation. Toyama et al. (2016) is most comparable to our approach: it is a multimodal variational NMT model that infers latent variables to represent the source language semantics from the image and linguistic data.\nTable 2 shows the results of this experiment. We can see that the combination of the attentionbased translation model and the image prediction model is a 1.8 Meteor point improvement over the NMT baseline, but it is 1.1 Meteor points worse than the strong Moses baseline. Our approach is competitive with previous approaches that use visual features as inputs to the decoder and the target-side reranking model. It also outperforms Toyama et al. (2016), which also only uses images for training. These results confirm that our multitasking approach uses the image prediction task to improve the encoder of the translation model."}, {"heading": "5.3 External described image data", "text": "Recall from Section 2 that we are interested in scenarios where x, y, and v are drawn from different sources. We now experiment with separating the translation data from the described image data using Dimage: MS COCO dataset of 83K described images3 and Dtext: Multi30K parallel text.\nTable 3 shows the results of this experiment. We find that there is no significant difference between training the IMAGINET decoder on in-domain (Multi30K) or out-of-domain data (COCO). This result confirms that we can separate the parallel text from the described images.\n3Due to differences in the vocabularies of the repsective datasets, we do not train on examples where more than 10% of the tokens are out-of-vocabulary in the Multi30K dataset."}, {"heading": "5.4 External parallel text data", "text": "We now experiment with training our model on a combination of the Multi30K and the News Commentary English-German data. In these experiments, we concatenate the Multi30K and News Commentary datasets into a single Dtext training dataset, similar to Freitag and Al-Onaizan (2016). We compare our model against Calixto et al. (2017a), who pre-train their model on the WMT\u201915 English-German parallel text and backtranslate (Sennrich et al., 2016) additional sentences from the bilingual independent descriptions in the Multi30K dataset (Footnote 2).\nTable 4 presents the results. The text-only NMT model using sub-words is 1.2 Meteor points lower than decompounding the German text. Nevertheless, the model trained over a concatentation of the parallel texts is a 2.7 Meteor point improvement over this baseline (+ NC) and matches the performance of our Multitasking model that uses only in-domain data (Section 5.2). We do not see an additive improvement for the multitasking model with the concatenated parallel text and the indomain data (+ Imagination) using a training objective interpolation of w = 0.89 (the ratio of the training dataset sizes). This may be because we are essentially learning a translation model and the updates from the IMAGINET decoder are forgotten. Therefore, we experiment with multitasking the concatenated parallel text and the COCO dataset (w = 0.5). We find that balancing the datasets improves over the concatenated text model by 0.4 Meteor (+ Imagination (COCO)). Our multitasking approach improves upon Calixto et al. by 0.3 Meteor points. Their model is pre-trained for 10 days on 4.3 million out-of-domain parallel sen-\ntences, whereas our model is trained in 48 hours using 240K parallel sentences and 414K described images from out-of-domain datasets."}, {"heading": "5.5 Ensemble results", "text": "Table 6 presents the results of ensembling different randomly initialised models. We achieve a start-of-the-art result of 57.6 Meteor for a model trained on only in-domain data. The improvements are more pronounced for the models trained using sub-words and out-of-domain data. An ensemble of baselines trained on sub-words is initially worse than an ensemble trained on Zmorge decompounded words. However, we always see an improvement from ensembling models trained on in- and out-of-domain data. Our best ensemble is trained on Multi30K parallel text, the News Com-\nmentary parallel text, and the COCO descriptions to set a new state-of-the-art result of 59.3 Meteor."}, {"heading": "5.6 Qualitative Examples", "text": "Table 5 shows examples of where the multitasking model improves or worsens translation performance compared to the baseline model4. The first example shows that the baseline model makes a significant error in translating the pose of the children, translating \u201con their stomachs\u201d as \u201con their faces\u201d). The middle example demonstrates that the baseline model translates the dog as walking (\u201cla\u0308uft\u201d) and then makes grammatical and sense errors after the clause marker. Both models neglect to translate the word \u201cdangling\u201d, which is a low-frequency word in the training data. There are instances where the baseline produces better translations than the multitask model: In the bottom example, our model translates a bird flying through the water (\u201cdurch\u201d) instead of \u201cover\u201d the water."}, {"heading": "6 Discussion", "text": ""}, {"heading": "6.1 Does the model learn grounded representations?", "text": "A natural question to ask if whether the multitask model is actually learning representations that are relevant for the images. We answer this question by evaluating the Imaginet decoder in an image\u2013 sentence ranking task. Here the input is a source\n4We used MT-ComparEval (Klejch et al., 2015)\nlanguage sentence, from which we predict its image vector v\u0302. The predicted vector v\u0302 can be compared against the true image vectors v in the evaluation data using the cosine distance to produce a ranked order of the images. Our model returns a median rank of 11.0 for the true image compared to the predicted image vector. Figure 2 shows examples of the nearest neighbours of the images predicted by our multitask model. We can see that the combination of the multitask source language representations and IMAGINET decoder leads to the prediction of relevant images. This confirms that the shared encoder is indeed learning visually grounded representations."}, {"heading": "6.2 The effect of visual feature vectors", "text": "We now study the effect of varying the Convolutional Neural Network used to extract the visual features used in the Imaginet decoder. It has previously been shown that the choice of visual features can affect the performance of vision and language models (Jabri et al., 2016; Kiela et al., 2016). We compare the effect of training the IMAGINET decoder to predict different types of image features, namely: 4096D features extracted from the \u2018fc7\u2018\u2019\nlayer of the VGG-19 model (Simonyan and Zisserman, 2015), 2048D features extracted from the \u2018pool5/7x7 s1\u2019 layer of InceptionNet V3 (Szegedy et al., 2015), and 2048D features extracted from \u2018avg pool\u2018 layer of ResNet-50 (He et al., 2016). Table 7 shows the results of this experiment. There is a clear difference between predicting the 2048D vectors (Inception-V3 and ResNet-50) compared to the 4096D vector from VGG-19). This difference is reflected in both the translation Meteor score and the Median rank of the images in the validation dataset. This is likely because it is easier to learn the parameters of the image prediction model that has fewer parameters (8.192 million for VGG19 vs. 4.096 million for Inception-V3 and ResNet50). However, it is not clear why there is such a pronounced difference between the Inception-V3 and ResNet-50 models5. Further work is needed to understand the difference in these results."}, {"heading": "7 Related work", "text": "Initial work on multimodal translation has focused on approaches that use either semantic or spatially-preserving image features as inputs to a translation model. Semantic image features are typically extracted from the final pooling layer of a pre-trained object recognition CNN, e.g. \u2018pool5/7x7 s1\u2019 in GoogLeNet (Szegedy et al., 2015). This type of feature vector has been used conditioning input to the encoder (Elliott et al., 2015; Huang et al., 2016), in the decoder (Libovicky\u0301 et al., 2016), or as additional features in a\n5We used the pre-trained models from Keras (Chollet, 2015) downloaded from https://github.com/ fchollet/deep-learning-models, which claim equal ILSVRC object recognition performance for both models: 7.8% top-5 error with a single-model and single-crop.\nphrase-based translation model (Shah et al., 2016; Hitschler et al., 2016). Spatially-preserving image features are extracted from deeper inside a CNN, where the position of a feature in the tensor is related to its position in the image. These features have been used in \u201cdouble-attention models\u201d, which calculate independent context vectors of the source language hidden states and a convolutional image feature map (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017a). Similar to most of these approaches, we use an attention-based translation model, but our multitask model does not use images for translation.\nMore related to our work are the recent papers by Toyama et al. (2016), Saha et al. (2016), and Nakayama and Nishida (2016). Toyama et al. (2016) extend the Variational Neural Machine Translation model (Zhang et al., 2016) by inferring latent variables to explicitly model the semantics of source sentences from both image and linguistic information. Similar to our approach, their model does not condition the translation model on images for translation. They report improvements on the Multi30K data set when using multimodal information, however, their model adds additional parameters in the form of \u201cneural inferrer\u201d modules. In our multitask model, the grounded semantics are represented implicitly in the hidden states of the shared encoder. Furthermore, Toyama et al. (2016) assumes Source-Target-Image tuples as training data; whereas our multitask framework achieves equally good results if we train on separate Source-Image and Source-Target datasets.\nSaha et al. (2016) study the problem of crosslingual image description where the task is to generate sentence in language L1 given the image, given only Image-L2 and L1-L2 parallel corpora. They propose a Correlational Encoder-Decoder Network to model the Image-L2 and L1-L2 parallel text data. Their model learns correlated representations for paired Image-L2 data and decodes L1 from this joint representation. Similarly to our setup the encoder is trained by minimizing two loss functions: the Image-L2 correlation loss, and the L1 decoding cross-entropy loss.\nNakayama and Nishida (2016) consider a zeroresource problem where the task is to translate from L1 to L2 but only Image-L1 and ImageL2 corpora are available. Their best performing model embeds the image, L1, and L2 in a joint multimodal space learned through minimizing a\nmulti-task ranking loss between both pairs of examples. The main difference between this approach and our models is that in our experiments we focus on enriching source language representations with visual information, rather than addressing the zero-resource issue.\nMultitask Learning improves the generalisability of a model by requiring it to be useful for more than one task (Caruana, 1997). This approach has seen a surge of attention and has recently been used to improve the performance of sentence compression using eye gaze as an auxiliary task (Klerke et al., 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016). These works hypothesise a relationship between specific biometric measurements and specific NLP tasks motivated by cognitive-linguistic theories. More recently, Bingel and S\u00f8gaard (2017) analysed the beneficial relationships between primary and auxiliary sequential prediction tasks. In our multitask framework we explore the benefits of the general notion of grounded learning in the specific case of multimodal translation. In the translation literature, multitask learning has been used to learn a oneto-many languages translation model (Dong et al., 2015), and as a subtask of multitask sequence-tosequence learning (Luong et al., 2016)."}, {"heading": "8 Conclusion", "text": "We decompose multimodal translation into two sub-problems: learning to translate and learning visually grounded representations. In a multitask learning framework, we show how these subproblems can be addressed by sharing an encoder between a translation model and an image prediction model. Our approach achieves state-of-theart results on the Multi30K dataset without using images for translation. We show that training on separate parallel text and described image datasets does not hurt performance, encouraging future research on multitasking with diverse sources of data. Furthermore, we still find improvements from image prediction when we improve our textonly baseline with the out-of-domain parallel text. Future work includes adapting our decomposition to other NLP tasks that may benefit from out-ofdomain resources, such as semantic role labelling, dependency parsing, and question-answering; exploring methods for inputting the (predicted) im-\nage into the translation model; experimenting with different image prediction architectures; multitasking different translation languages into a single shared encoder; and multitasking in both the encoder and decoder(s)."}, {"heading": "Acknowledgments", "text": "We thank Joost Bastings for sharing his multitasking Nematus model, Wilker Aziz for discussions about formulating the problem, Stella Frank for finding and explaining the qualitative examples to us, and Afra Alishahi, Grzegorz Chrupa\u0142a, and Philip Schulz for feedback on earlier drafts of the paper. DE acknowledges the support of NWO Vici grant nr. 277-89-002 awarded to K. Sima\u2019an, an Amazon Academic Research Award, and an NVIDIA Academic Hardware Grant."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Identifying beneficial task relations for multi-task learning in deep neural networks", "author": ["J. Bingel", "A. S\u00f8gaard."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. pages 164\u2013169.", "citeRegEx": "Bingel and S\u00f8gaard.,? 2017", "shortCiteRegEx": "Bingel and S\u00f8gaard.", "year": 2017}, {"title": "Multimodal attention for neural machine translation", "author": ["Ozan Caglayan", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": "CoRR abs/1609.03976", "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Dcu-uva multimodal mt system report", "author": ["Iacer Calixto", "Desmond Elliott", "Stella Frank."], "venue": "Proceedings of the First Conference on Machine Translation. pages 634\u2013638.", "citeRegEx": "Calixto et al\\.,? 2016", "shortCiteRegEx": "Calixto et al\\.", "year": 2016}, {"title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR abs/1702.01287.", "citeRegEx": "Calixto et al\\.,? 2017a", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Incorporating global visual features into attention-based neural machine translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR abs/1701.06521.", "citeRegEx": "Calixto et al\\.,? 2017b", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine Learning 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick."], "venue": "CoRR abs/1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Learning language through pictures", "author": ["Grzegorz Chrupa\u0142a", "\u00c1kos K\u00e1d\u00e1r", "Afra Alishahi."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Chrupa\u0142a et al\\.,? 2015", "shortCiteRegEx": "Chrupa\u0142a et al\\.", "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["D. Dong", "H. Wu", "W. He", "D. Yu", "H. Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR abs/1510.04709.", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil. Sima\u2019an", "Lucia Specia"], "venue": "In Proceedings of the 5th Workshop on Vision and Language", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Fast domain adaptation for neural machine translation", "author": ["Markus Freitag", "Yaser Al-Onaizan."], "venue": "CoRR abs/1612.06897.", "citeRegEx": "Freitag and Al.Onaizan.,? 2016", "shortCiteRegEx": "Freitag and Al.Onaizan.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29, pages 1019\u20131027.", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pages 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Multimodal Pivots for Image Caption Translation", "author": ["J. Hitschler", "S. Schamoni", "S. Riezler."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 2399\u2013 2409.", "citeRegEx": "Hitschler et al\\.,? 2016", "shortCiteRegEx": "Hitschler et al\\.", "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Attention-based multimodal neural machine translation", "author": ["P. Huang", "F. Liu", "S. Shiang", "J. Oh", "C. Dyer."], "venue": "Proceedings of the First Conference on Machine Translation. pages 639\u2013645.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten."], "venue": "CoRR abs/1606.08390.", "citeRegEx": "Jabri et al\\.,? 2016", "shortCiteRegEx": "Jabri et al\\.", "year": 2016}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["Akos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi."], "venue": "arXiv preprint arXiv:1602.08952 .", "citeRegEx": "K\u00e1d\u00e1r et al\\.,? 2016", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2016}, {"title": "Comparing Data Sources and Architectures for Deep Visual Representation Learning in Semantics", "author": ["D. Kiela", "A.L. Ver\u0151", "S. Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 447\u2013456.", "citeRegEx": "Kiela et al\\.,? 2016", "shortCiteRegEx": "Kiela et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "International Conference on Learning Representations .", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Mt-compareval: Graphical evaluation interface for machine translation development", "author": ["Ond\u0159ej Klejch", "Eleftherios Avramidis", "Aljoscha Burchardt", "Martin Popel."], "venue": "The Prague Bulletin of Mathematical Linguistics 104(1):63\u201374.", "citeRegEx": "Klejch et al\\.,? 2015", "shortCiteRegEx": "Klejch et al\\.", "year": 2015}, {"title": "Improving sentence compression by learning to predict gaze", "author": ["Sigrid Klerke", "Yoav Goldberg", "Anders S\u00f8gaard."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Klerke et al\\.,? 2016", "shortCiteRegEx": "Klerke et al\\.", "year": 2016}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens"], "venue": "In Proceedings of the 45th Annual meeting of Association", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Cuni system for wmt16 automatic post-editing and multimodal translation tasks", "author": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina."], "venue": "Proceedings of the First Conference on Machine Translation. pages 646\u2013654.", "citeRegEx": "Libovick\u00fd et al\\.,? 2016", "shortCiteRegEx": "Libovick\u00fd et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Zeroresource machine translation by multimodal encoder-decoder network with multimedia pivot", "author": ["Hideki Nakayama", "Noriki Nishida."], "venue": "CoRR abs/1611.04503.", "citeRegEx": "Nakayama and Nishida.,? 2016", "shortCiteRegEx": "Nakayama and Nishida.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Keystroke dynamics as signal for shallow syntactic parsing", "author": ["Barbara Plank."], "venue": "26th International Conference on Computational Linguistics. pages 609\u2013619.", "citeRegEx": "Plank.,? 2016", "shortCiteRegEx": "Plank.", "year": 2016}, {"title": "A correlational encoder decoder architecture for pivot based sequence generation", "author": ["Amrita Saha", "Mitesh M. Khapra", "Sarath Chandar", "Janarthanan Rajendran", "Kyunghyun Cho."], "venue": "26th International Conference on Computational Linguistics: Techni-", "citeRegEx": "Saha et al\\.,? 2016", "shortCiteRegEx": "Saha et al\\.", "year": 2016}, {"title": "Japanese and korean voice search", "author": ["Mike Schuster", "Kaisuke Nakajima."], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pages 5149\u20135152.", "citeRegEx": "Schuster and Nakajima.,? 2012", "shortCiteRegEx": "Schuster and Nakajima.", "year": 2012}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "IEEE Transactions on Signal Processing 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Nematus: a Toolkit for Neural Machine Translation", "author": ["R. Sennrich", "O. Firat", "K. Cho", "A. Birch", "B. Haddow", "J. Hitschler", "M. Junczys-Dowmunt", "S. L\u00e4ubli", "A. Valerio Miceli Barone", "J. Mokry", "M. N\u0103dejde."], "venue": "CoRR abs/1703.04357.", "citeRegEx": "Sennrich et al\\.,? 2017", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 86\u201396.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Zmorge: A german morphological lexicon extracted from wiktionary", "author": ["Rico Sennrich", "Beat Kunz."], "venue": "Language Resources and Evaluation Conference. pages 1063\u20131067.", "citeRegEx": "Sennrich and Kunz.,? 2014", "shortCiteRegEx": "Sennrich and Kunz.", "year": 2014}, {"title": "Shef-multimodal: Grounding machine translation on images", "author": ["Kashif Shah", "Josiah Wang", "Lucia Specia."], "venue": "Proceedings of the First Conference on Machine Translation. pages 660\u2013665.", "citeRegEx": "Shah et al\\.,? 2016", "shortCiteRegEx": "Shah et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna."], "venue": "CoRR abs/1512.00567.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann."], "venue": "Eight International Conference on Language Resources and Evaluation (LREC\u201912).", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Neural machine translation with latent semantic of image and text", "author": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo."], "venue": "CoRR abs/1611.08459.", "citeRegEx": "Toyama et al\\.,? 2016", "shortCiteRegEx": "Toyama et al\\.", "year": 2016}, {"title": "Variational neural machine translation", "author": ["B. Zhang", "D. Xiong", "J. Su", "H. Duan", "M. Zhang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pages 521\u2013530.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 41, "context": "Multimodal machine translation is the task of translating sentences in context, such as images paired with a parallel text (Specia et al., 2016).", "startOffset": 123, "endOffset": 144}, {"referenceID": 41, "context": "In the results of the First Multimodal Translation Shared Task, only three systems outperformed an off-the-shelf text-only phrase-based machine translation model, and the best performing system was equally effective with or without the visual features (Specia et al., 2016).", "startOffset": 252, "endOffset": 273}, {"referenceID": 0, "context": "The translation decoder is an attention-based neural machine translation model (Bahdanau et al., 2015), and the image prediction decoder is trained to predict a global feature vector of an image that is associated with a sentence (Chrupa\u0142a et al.", "startOffset": 79, "endOffset": 102}, {"referenceID": 22, "context": "It has been shown that grounded representations are qualitatively different from their text-only counterparts (K\u00e1d\u00e1r et al., 2016) and correlate better with human similarity judgements (Chrupa\u0142a et al.", "startOffset": 110, "endOffset": 130}, {"referenceID": 10, "context": ", 2016) and correlate better with human similarity judgements (Chrupa\u0142a et al., 2015).", "startOffset": 62, "endOffset": 85}, {"referenceID": 19, "context": "We assess the success of the grounded learning by evaluating the image prediction model on an image\u2013sentence ranking task to determine if the shared representations are useful for image retrieval (Hodosh et al., 2013).", "startOffset": 196, "endOffset": 217}, {"referenceID": 14, "context": "We evaluate Imagination on the Multi30K dataset (Elliott et al., 2016) using a combination of in-domain and out-of-domain data.", "startOffset": 48, "endOffset": 70}, {"referenceID": 7, "context": "In the experiments with out-of-domain resources, we find that the improvement in translation quality holds when training the IMAGINET decoder on the MS COCO dataset of described images (Chen et al., 2015).", "startOffset": 185, "endOffset": 204}, {"referenceID": 43, "context": "Furthermore, if we significantly improve our text-only baseline using out-of-domain parallel text from the News Commentary corpus (Tiedemann, 2012), we still find improvements in translation quality from the auxiliary image prediction task.", "startOffset": 130, "endOffset": 147}, {"referenceID": 29, "context": "We train our multitask model following the approach of Luong et al. (2016). We define a primary task and an auxiliary task, and a set of parameters \u03b8 to be shared between the tasks.", "startOffset": 55, "endOffset": 75}, {"referenceID": 35, "context": "n in the source language with a bidirectional recurrent neural network (Schuster and Paliwal, 1997).", "startOffset": 71, "endOffset": 99}, {"referenceID": 0, "context": "The translation model decoder is an attentionbased recurrent neural network (Bahdanau et al., 2015).", "startOffset": 76, "endOffset": 99}, {"referenceID": 10, "context": "The image prediction decoder is trained to predict the visual feature vector of the image associated with a sentence (Chrupa\u0142a et al., 2015).", "startOffset": 117, "endOffset": 140}, {"referenceID": 14, "context": "We evaluate our model using the benchmark Multi30K dataset (Elliott et al., 2016), which is the largest collection of images paired with sentences in multiple languages.", "startOffset": 59, "endOffset": 81}, {"referenceID": 38, "context": "We additionally decompound the German text using Zmorge (Sennrich and Kunz, 2014).", "startOffset": 56, "endOffset": 81}, {"referenceID": 7, "context": "We also use two external datasets to evaluate our model: the MS COCO dataset of English described images (Chen et al., 2015), and the English-German News Commentary parallel corpus (Tiedemann, 2012).", "startOffset": 105, "endOffset": 124}, {"referenceID": 43, "context": ", 2015), and the English-German News Commentary parallel corpus (Tiedemann, 2012).", "startOffset": 64, "endOffset": 81}, {"referenceID": 34, "context": "When we perform experiments with the News Commentary corpus, we first calculate a 17,597 sub-word vocabulary using SentencePiece (Schuster and Nakajima, 2012) over the concatentation of the Multi30K and News Commentary datasets.", "startOffset": 129, "endOffset": 158}, {"referenceID": 42, "context": "Images are represented by 2048D vectors extracted from the \u2018pool5/7x7 s1\u2019 layer of the GoogLeNet v3 CNN (Szegedy et al., 2015).", "startOffset": 104, "endOffset": 126}, {"referenceID": 11, "context": "Throughout, we report performance of the En\u2192De translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al.", "startOffset": 72, "endOffset": 99}, {"referenceID": 31, "context": "Throughout, we report performance of the En\u2192De translation using Meteor (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) against lowercased tokenized references.", "startOffset": 109, "endOffset": 132}, {"referenceID": 36, "context": "The translation decoder is a 1000D GRU recurrent neural network, with a 2000D context vector over the encoder states, and 620D word embeddings (Sennrich et al., 2017).", "startOffset": 143, "endOffset": 166}, {"referenceID": 24, "context": "The models are trained using the Adam optimiser with the default hyperparameters (Kingma and Ba, 2015) in minibatches of 80 instances.", "startOffset": 81, "endOffset": 102}, {"referenceID": 16, "context": "2 for the embeddings and the recurrent connections in both tasks (Gal and Ghahramani, 2016).", "startOffset": 65, "endOffset": 91}, {"referenceID": 27, "context": "Moses is the phrase-based machine translation model (Koehn et al., 2007) reported in (Specia et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 41, "context": ", 2007) reported in (Specia et al., 2016).", "startOffset": 20, "endOffset": 41}, {"referenceID": 3, "context": "Calixto et al. (2017b) is a multimodal translation model that conditions the decoder on semantic image vector extracted from the VGG-19 CNN.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Calixto et al. (2017b) is a multimodal translation model that conditions the decoder on semantic image vector extracted from the VGG-19 CNN. Hitschler et al. (2016) uses visual features in a target-side retrieval model for translation.", "startOffset": 0, "endOffset": 165}, {"referenceID": 3, "context": "Calixto et al. (2017b) is a multimodal translation model that conditions the decoder on semantic image vector extracted from the VGG-19 CNN. Hitschler et al. (2016) uses visual features in a target-side retrieval model for translation. Toyama et al. (2016) is most comparable to our approach: it is a multimodal variational NMT model that infers latent variables to represent the source language semantics from the image and linguistic data.", "startOffset": 0, "endOffset": 257}, {"referenceID": 44, "context": "It also outperforms Toyama et al. (2016), which also only uses images for training.", "startOffset": 20, "endOffset": 41}, {"referenceID": 37, "context": "(2017a), who pre-train their model on the WMT\u201915 English-German parallel text and backtranslate (Sennrich et al., 2016) additional sentences from the bilingual independent descriptions in the Multi30K dataset (Footnote 2).", "startOffset": 96, "endOffset": 119}, {"referenceID": 12, "context": "In these experiments, we concatenate the Multi30K and News Commentary datasets into a single Dtext training dataset, similar to Freitag and Al-Onaizan (2016). We compare our model against Calixto et al.", "startOffset": 128, "endOffset": 158}, {"referenceID": 3, "context": "We compare our model against Calixto et al. (2017a), who pre-train their model on the WMT\u201915 English-German parallel text and backtranslate (Sennrich et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 25, "context": "We used MT-ComparEval (Klejch et al., 2015)", "startOffset": 22, "endOffset": 43}, {"referenceID": 21, "context": "It has previously been shown that the choice of visual features can affect the performance of vision and language models (Jabri et al., 2016; Kiela et al., 2016).", "startOffset": 121, "endOffset": 161}, {"referenceID": 23, "context": "It has previously been shown that the choice of visual features can affect the performance of vision and language models (Jabri et al., 2016; Kiela et al., 2016).", "startOffset": 121, "endOffset": 161}, {"referenceID": 40, "context": "We compare the effect of training the IMAGINET decoder to predict different types of image features, namely: 4096D features extracted from the \u2018fc7\u2018\u2019 layer of the VGG-19 model (Simonyan and Zisserman, 2015), 2048D features extracted from the \u2018pool5/7x7 s1\u2019 layer of InceptionNet V3 (Szegedy et al.", "startOffset": 176, "endOffset": 206}, {"referenceID": 42, "context": "We compare the effect of training the IMAGINET decoder to predict different types of image features, namely: 4096D features extracted from the \u2018fc7\u2018\u2019 layer of the VGG-19 model (Simonyan and Zisserman, 2015), 2048D features extracted from the \u2018pool5/7x7 s1\u2019 layer of InceptionNet V3 (Szegedy et al., 2015), and 2048D features extracted from \u2018avg pool\u2018 layer of ResNet-50 (He et al.", "startOffset": 282, "endOffset": 304}, {"referenceID": 17, "context": ", 2015), and 2048D features extracted from \u2018avg pool\u2018 layer of ResNet-50 (He et al., 2016).", "startOffset": 73, "endOffset": 90}, {"referenceID": 42, "context": "\u2018pool5/7x7 s1\u2019 in GoogLeNet (Szegedy et al., 2015).", "startOffset": 28, "endOffset": 50}, {"referenceID": 13, "context": "This type of feature vector has been used conditioning input to the encoder (Elliott et al., 2015; Huang et al., 2016), in the decoder (Libovick\u00fd et al.", "startOffset": 76, "endOffset": 118}, {"referenceID": 20, "context": "This type of feature vector has been used conditioning input to the encoder (Elliott et al., 2015; Huang et al., 2016), in the decoder (Libovick\u00fd et al.", "startOffset": 76, "endOffset": 118}, {"referenceID": 28, "context": ", 2016), in the decoder (Libovick\u00fd et al., 2016), or as additional features in a", "startOffset": 24, "endOffset": 48}, {"referenceID": 9, "context": "We used the pre-trained models from Keras (Chollet, 2015) downloaded from https://github.", "startOffset": 42, "endOffset": 57}, {"referenceID": 39, "context": "phrase-based translation model (Shah et al., 2016; Hitschler et al., 2016).", "startOffset": 31, "endOffset": 74}, {"referenceID": 18, "context": "phrase-based translation model (Shah et al., 2016; Hitschler et al., 2016).", "startOffset": 31, "endOffset": 74}, {"referenceID": 3, "context": "These features have been used in \u201cdouble-attention models\u201d, which calculate independent context vectors of the source language hidden states and a convolutional image feature map (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017a).", "startOffset": 179, "endOffset": 247}, {"referenceID": 2, "context": "These features have been used in \u201cdouble-attention models\u201d, which calculate independent context vectors of the source language hidden states and a convolutional image feature map (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017a).", "startOffset": 179, "endOffset": 247}, {"referenceID": 4, "context": "These features have been used in \u201cdouble-attention models\u201d, which calculate independent context vectors of the source language hidden states and a convolutional image feature map (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017a).", "startOffset": 179, "endOffset": 247}, {"referenceID": 45, "context": "(2016) extend the Variational Neural Machine Translation model (Zhang et al., 2016) by inferring latent variables to explicitly model the semantics of source sentences from both image and linguistic information.", "startOffset": 63, "endOffset": 83}, {"referenceID": 42, "context": "More related to our work are the recent papers by Toyama et al. (2016), Saha et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 32, "context": "(2016), Saha et al. (2016), and Nakayama and Nishida (2016).", "startOffset": 8, "endOffset": 27}, {"referenceID": 30, "context": "(2016), and Nakayama and Nishida (2016). Toyama et al.", "startOffset": 12, "endOffset": 40}, {"referenceID": 30, "context": "(2016), and Nakayama and Nishida (2016). Toyama et al. (2016) extend the Variational Neural Machine Translation model (Zhang et al.", "startOffset": 12, "endOffset": 62}, {"referenceID": 30, "context": "(2016), and Nakayama and Nishida (2016). Toyama et al. (2016) extend the Variational Neural Machine Translation model (Zhang et al., 2016) by inferring latent variables to explicitly model the semantics of source sentences from both image and linguistic information. Similar to our approach, their model does not condition the translation model on images for translation. They report improvements on the Multi30K data set when using multimodal information, however, their model adds additional parameters in the form of \u201cneural inferrer\u201d modules. In our multitask model, the grounded semantics are represented implicitly in the hidden states of the shared encoder. Furthermore, Toyama et al. (2016) assumes Source-Target-Image tuples as training data; whereas our multitask framework achieves equally good results if we train on separate Source-Image and Source-Target datasets.", "startOffset": 12, "endOffset": 699}, {"referenceID": 6, "context": "Multitask Learning improves the generalisability of a model by requiring it to be useful for more than one task (Caruana, 1997).", "startOffset": 112, "endOffset": 127}, {"referenceID": 26, "context": "This approach has seen a surge of attention and has recently been used to improve the performance of sentence compression using eye gaze as an auxiliary task (Klerke et al., 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016).", "startOffset": 158, "endOffset": 179}, {"referenceID": 32, "context": ", 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016).", "startOffset": 128, "endOffset": 141}, {"referenceID": 12, "context": "In the translation literature, multitask learning has been used to learn a oneto-many languages translation model (Dong et al., 2015), and as a subtask of multitask sequence-tosequence learning (Luong et al.", "startOffset": 114, "endOffset": 133}, {"referenceID": 29, "context": ", 2015), and as a subtask of multitask sequence-tosequence learning (Luong et al., 2016).", "startOffset": 68, "endOffset": 88}, {"referenceID": 1, "context": "More recently, Bingel and S\u00f8gaard (2017) analysed the beneficial relationships between primary and auxiliary sequential prediction tasks.", "startOffset": 15, "endOffset": 41}], "year": 2017, "abstractText": "Multimodal machine translation is the task of translating sentences in a visual context. We decompose this problem into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoderdecoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.", "creator": "LaTeX with hyperref package"}}}