{"id": "1705.05020", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2017", "title": "Discrete-Continuous Splitting for Weakly Supervised Learning", "abstract": "all this paper proposes an approach for tackling an abstract formulation of weakly supervised learning, which is posed as a joint optimization problem in the continuous model parameters and discrete label variables. we devise a novel decomposition of the functional latter into two purely discrete and continuous operator subproblems within only the framework of the alternating direction method of multipliers ( admm ), which allows to efficiently compute a local minimum of the nonconvex graded objective function. our approach preserves integrality of the discrete label variables and admits a globally convergent kernel formulation. the resulting method implicitly alternates between a binary discrete and a continuous variable update, however, it is inherently different from simple alternating optimization ( called hard em ). in numerous experiments we illustrate that our method can learn obtaining a classifier from weak and abstract combinatorial linear supervision thereby being superior towards hard em.", "histories": [["v1", "Sun, 14 May 2017 19:32:50 GMT  (345kb,D)", "https://arxiv.org/abs/1705.05020v1", null], ["v2", "Mon, 19 Jun 2017 15:51:08 GMT  (190kb,D)", "http://arxiv.org/abs/1705.05020v2", null], ["v3", "Wed, 16 Aug 2017 15:18:11 GMT  (190kb,D)", "http://arxiv.org/abs/1705.05020v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emanuel laude", "jan-hendrik lange", "frank r schmidt", "bjoern", "res", "daniel cremers"], "accepted": false, "id": "1705.05020"}, "pdf": {"name": "1705.05020.pdf", "metadata": {"source": "CRF", "title": "Discrete-Continuous Splitting for Weakly Supervised Learning", "authors": ["Emanuel Laude", "Jan-Hendrik Lange", "Frank R. Schmidt", "Bjoern Andres", "Daniel Cremers"], "emails": ["emanuel.laude@in.tum.de,", "jlange@mpi-inf.mpg.de,", "f.schmidt@cs.tum.edu,", "andres@mpi-inf.mpg.de,", "cremers@tum.de"], "sections": [{"heading": "1 Introduction", "text": "One of the main challenges in machine learning is the exact annotation of training data, which has become a typical bottleneck of modern machine learning applications. For this reason, substantial research has been devoted to models and algorithms for weakly and semi-supervised learning that take advantage of weakly and sparsely labeled data.\nMoreover, the (human guided) annotation process may become more efficient, in case the training algorithm can exploit structured a-priori knowledge about labels instead of more expensive exact annotation. Such knowledge may take the form of hard or soft constraints on feasible labelings, or, more generally, of a supervision function which we refer to as a weak supervisor.\nIn this paper we consider weakly supervised classification problems that, given a set of training instances I := {1, . . . , N}, N \u2208 N, represented by their feature vectors xi \u2208 X \u2282 Rd, d \u2208 N, i \u2208 I, can be modeled in terms of a joint minimization problem in (a-priori unknown) discrete label variables Y and continuous model parameters W:\nminimize W,Y\n1\nN N\u2211 i=1 `(W,xi,yi) +R(W) + S(Y). (1)\nThe different terms in the objective correspond to:\n(i) An empirical risk defined as the sum of loss terms `(W,xi,yi) for individual training instances i \u2208 I . Each loss term depends on the continuous model parameters W and the discrete label variable\nar X\niv :1\n70 5.\n05 02\n0v 3\n[ cs\n.L G\n] 1\n6 A\nyi \u2208 Y given in one-hot encoding, where Y denotes the finite set of one-hot encodings for C distinct classes. Typical choices are, for instance, the Crammer and Singer multi-class SVM loss [8], or the softmax loss, in combination with a linear classifier, in which case W \u2208 Rd\u00d7C . The loss may also be given as a structured or latent variable SVM loss [33, 40] or it may incorporate a nonlinear classifier such as a neural network. However, in this work we focus on convex loss functions, as the individual loss terms can typically be optimized efficiently via duality.\n(ii) A regularizer R(W) on the model parameters. Typical choices are, for instance, the `1-norm or the squared Frobenius norm, that we denote by \u2016 \u00b7 \u20162. (iii) A weak supervisor function S : YN \u2192 R \u222a {\u221e}, that assigns a (possibly infinite) cost to any labeling Y \u2208 YN of the N instances. We define the feasible set of the supervisor as dom(S) := {Y : S(Y) <\u221e}. Problem (1) admits diverse forms of weak supervision, by modeling the supervisor in different ways. For instance, the supervisor can be defined in terms of the objective of a tree-structured Markov Random Field (MRF):\nS(Y) = \u2211 i\u2208I ui(yi) + \u2211 ij\u2208E pij(yi,yj), (2)\nwhere ui : Y \u2192 R\u222a{\u221e} and pij : Y\u00d7Y \u2192 R\u222a{\u221e} are the unary, respectively pairwise potentials, and E \u2282 (I 2 ) defines an undirected forest on I . The tree structure guarantees efficiency of our method. With the unary potentials we may model for a certain instance i the likelihood of it to belong to a particular class. We may also rule out specific class labels, which we refer to as negative labels. With the pairwise potentials we can model that certain label combinations assigned to pairs of instances are more likely than others. This enables the human annotation of similarities and dissimilarities of training instances without specifying any exact labels. For a supervisor with only pairwise terms and a quadratic distance loss, the model (1) specializes to constrained k-means clustering [34, 2].\nAlternatively, from a discrete optimization perspective, we may interpret the resulting model as the attempt to learn (additional) unary potentials u\u2032i : Y \u2192 R of an MRF in a joint fashion: Let (W\u2217,Y\u2217) be a minimizer of (1), then the u\u2032i are given via the loss function u \u2032 i(yi) = `(W\n\u2217,xi,yi). For instance in semantic segmentation, model (1) allows for learning a pixel-wise data term jointly with the segmentation using raw patch-features.\nThe supervisor can also be modeled in terms of a min cost flow objective [4] for an a-priori known flow network G with a-priori unknown costs [3, 2]. In [3, 2] this is used to efficiently model a constraint on the labeling Y, that balances the assignment of instances to different classes. More abstractly, we may interpret the resulting model as the attempt to learn an \u201coptimal\u201d cost matrix \u03b3 for G jointly with the flow. Here, the entries of the cost matrix have the form \u03b3ij = `(W\u2217,xi,yi), where (W\u2217,Y\u2217) is a minimizer of (1). This may have applications for instance in multi-object tracking, which is often modeled in terms of a min cost flow problem [42]."}, {"heading": "1.1 Related Work", "text": "A typical approach for tackling problems of the form (1) is via a coordinate descent scheme in the discrete and the continuous variables [3, 34, 2]. In the context of MAP-inference in latent variable models, this is referred to as hard Expectation-Maximization (EM). For instance, with a squared distance loss, and in the absence of any supervision, problem (1) specializes to k-means clustering. The latter is commonly tackled by Lloyd\u2019s algorithm [22] that employs alternating optimization to compute a local minimum. Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27]. A particular issue with Lloyd\u2019s algorithm is, that its performance is highly sensitive to the initialization. The efficient computation of provably good seedings is therefore studied in [1].\nThe k-means clustering problem can be extended so as to account for combinatorial soft and hard constraints on the clustering such as the discussed pairwise constraints [34] or a balancing constraint [3, 2]. This is commonly referred to as constrained clustering [2].\nFor loss functions that are more common in classification, such as the hinge loss, the alternating optimization scheme breaks down, because it converges early to a poor local optimum [41]. Therefore, specialized algorithms based on SDP relaxations [38, 17] or CCCP [43] have been devised for a\ngenerally unconstrained setting. An SDP relaxation approach for weakly supervised learning in the context of multinomial logistic regression is proposed by [17]. While convex SDP relaxation methods are robust regarding initialization, on the downside they generally do not scale well to larger problem instances: The approach by [17] has a per-iteration complexity of O(N3). Moreover, those methods typically need to eliminate or relax the discrete variables. This has two important drawbacks: (i) It is in general very difficult to strictly impose combinatorial constraints on the labeling and (ii) a post-processing (rounding) method is usually employed, which degrades the quality of the final solution compared to the optimal (relaxed) solution [17].\nIn order to overcome those issues, we present a different approach that is amenable to efficient optimization by the Alternating Direction Method of Multipliers (ADMM). ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9]. More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32]. In particular, the application of ADMM to quadratic mixed integer programs has recently been studied by [16, 32]. Typically, those approaches do not admit a convergence guarantee and the solution found by ADMM is generally not integral. Therefore, a rounding step may be integrated into the ADMM formulation [32]. However, as reported by the authors this may lead to an unstable algorithm.\nWhereas ADMM converges under very general assumptions for the convex case, the convergence in a more restrictive nonconvex setting has recently been proven [15, 21, 35]. In this case, however, the required assumptions are fairly strong."}, {"heading": "1.2 Contribution", "text": "This paper makes two contributions.\nFirstly, we propose a decomposition of the generally NP-hard weakly supervised learning problem (1) into purely discrete and purely continuous sub-problems. The discrete-continuous splitting we propose has three advantages: (a) it is amenable to optimization by ADMM, (b) it guarantees global convergence to a critical point of the objective function for classifiers parameterized by a kernel, and (c) it maintains integrality of the discrete labels (without postprocessing or rounding).\nSecondly, we solve challenging instances of Problem (1) experimentally, by means of ADMM, demonstrating the following advantages over alternative methods:\nCompared to alternating optimization by hard EM, which have been shown in [41] to be prone to bad local minima for classification loss functions, our algorithm is robust to initialization and systematically produces good local optima.\nCompared to the SDP relaxation proposed in [17] whose optimization has worst-case O(N3) for a kernelized classifier, our algorithm has worst-case time complexity O(N2) for a classifier parameterized by a kernel. In addition, it allows for more general forms of weak supervision and, on the task of semi-supervised learning produces more consistent results."}, {"heading": "2 Discrete-Continuous Splitting", "text": "The remainder of this section is organized as follows: In Section 2.1 we describe the consensus ADMM framework employed for a discrete-continuous decomposition of problem (1), and interpret our step size update as a form of graduated nonconvexity. In Section 2.2 we discuss a novel discretecontinuous proximal mapping, that preserves integrality of the discrete variable and therefore is considered the central part of our method. In Section 2.3 we further describe a kernelized variant of our formulation that admits a convergence guarantee and scales, for a separable supervisor, as the supervised training of a kernel SVM."}, {"heading": "2.1 Discrete-Continuous Consensus ADMM", "text": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16]. Most of the time the idea is to write an objective function F as a sum\nof N objective functions Fi that are easy to optimize. As a consequence, one is interested in\nminimize Y,{Yi}Ni=1\nN\u2211 i=1 Fi(Yi)\nsubject to Yi = Y, \u2200i.\n(3)\nThe idea of ADMM is to relax the couplings Yi = Y and successively strengthen these constraints in each iteration so that the \u201cindependent\u201d solutions Yi converge towards a consensus solution Y. Nonetheless, this approach does not always provide discrete solutions. Therefore, a post-processing method is usually employed and the final (rounded) solution need not be close to the optimal (relaxed) solution.\nProblem (1) is NP-hard in general as well, as it subsumes k-means clustering. We propose to reformulate the problem in a way that preserves integrality of the discrete variables.\nProposed Decomposition. In contrast to the discrete decomposition of (3) with respect to Y, we propose a decomposition with respect to the continuous variables W: We introduce continuous auxiliary variables Wi and linear consensus constraints Wi = W on a per-instance level, which (in case the constraints are absent) for fixed Y decouples the loss terms in W. This leads to the following equivalent constrained reformulation of problem (1):\nminimize Y,W,{Wi}Ni=1\n1\nN N\u2211 i=1 `(Wi,xi,yi) +R(Wi) + S(Y)\nsubject to Wi = W, \u2200i.\n(4)\nClearly, this formulation is different from the consensus setting in (3) for discrete NP-hard optimization, due to the presence of the discrete variables Y and the supervisor S(Y). Moreover, the discrete variable Y is another \u201ccomplicating variable\u201d, that also couples all summands via the supervisor.\nIn order to fit our formulation into the general consensus model (3) auxiliary variables also for the yi have to be introduced. As explained above, with this approach integrality of Y cannot be guaranteed. Therefore, we pursue a different and more efficient strategy: Instead of further decomposing the problem into even smaller subproblems, we do not let ADMM operate on the discrete variables and instead encapsulate the minimization over Y within a function L:\nL(W1, . . . ,WN ) := min Y\n1\nN N\u2211 i=1 `(Wi,xi,yi) +R(Wi) + S(Y). (5)\nAs an important consequence the function L has to be optimized efficiently within the algorithm. This becomes the central part of our method and therefore we devote Section 2.2 to this problem. Further, we note that in case the supervisor is the trivial zero function, our approach specializes to a decomposition that is typically used in supervised learning with ADMM [10, 5].\nUsing (5), problem (4) can be compactly written as\nmin W,{Wi}Ni=1 L(W1, . . . ,WN )\nsubject to Wi = W \u2200i. (6)\nFor a particular penalty parameter \u03c1, the ADMM iterates for the above problem are given as\n{Wt+1i } N i=1 = arg min\n{Wi}Ni=1 L(W1, . . . ,WN ) +\n\u03c1\n2 N\u2211 i=1 \u2016Wi \u2212Wt + \u03bbti/\u03c1\u20162\nWt+1 = 1\nN N\u2211 i=1 \u03c1Wt+1i \u2212 \u03bb t i\n\u03bbt+1i = \u03bb t i + \u03c1(W t+1 i \u2212W t+1) \u2200i.\n(7)\nGraduated Nonconvexity via Penalty Parameter Update. Next we focus on the choice of the penalty parameter \u03c1, as a carefully chosen parameter may drastically improve the quality of the solutions found by ADMM in nonconvex optimization [39]. In our method we choose \u03c1 adaptively: More precisely, we increase \u03c1 slowly during the iterations, starting from a small value in the order of 10\u22124. This, in practice, leads to much better local optima in terms of objective value and makes the algorithm robust towards initialization. In this section we provide an interpretation of this behavior in the context of graduated nonconvexity:\nIn [24] the authors observe that consensus ADMM approaches a projected subgradient descent on the convex, negative Lagrangian dual function, referred to as dual decompostion (DD) [19], if the penalty parameter \u03c1 is close to zero. This carries over also to our algorithm. Whereas in [24] a convex objective is optimized, and therefore the problems solved by DD and ADMM are equivalent, in our case DD solves a convex relaxation that we characterize in the following Observation:\nObservation 1. Let the supervisor S be separable, i.e. S(Y) = \u2211 i Si(yi). Then the Lagrangian dual problem of (4) is equivalent to the following convex relaxation\nmin W N\u2211 i=1 \u00af\u0300(\u00b7,xi)\u2217\u2217(W) (8)\nwhere \u00af\u0300(W,xi) = 1N minyi\u2208Y `(W,xi,yi) + Si(yi) + R(W) and \u00af\u0300(\u00b7,xi)\u2217\u2217 denotes the convex biconjugate of \u00af\u0300(\u00b7,xi), i.e. the largest lower semi-continuous convex underapproximation of \u00af\u0300(\u00b7,xi).\nThe above result can be obtained via a simple computation, starting from the definition of the Lagrangian dual problem using the definition of the Fenchel-Legendre convex conjugate and Fenchel duality. We provide a formal proof in the Appendix A.1.\nWe observe that, in the absence of the supervision function and the regularizer, for typical loss functions such as the hinge loss or the logistic loss this convex relaxation yields the trivial zero function. However, if weak supervision and a regularizer is present, the convex relaxation becomes meaningful. Clearly, if ` and R are convex and the supervisor is very certain about one particular labeling, the problem approaches a convex one. In that sense the convex relaxation becomes tighter as the supervisor is less uncertain about the labeling.\nIn the light of this observation, we may informally interpret a nondecreasing update schedule for \u03c1 as a form of graduated nonconvexity [37]. Continuation methods for graduated nonconvexity have for instance been applied for the optimization of semi-supervised versions of the SVM [6].\nLocal Convergence. Global convergence of Algorithm 1 can so far not be guaranteed by existing theory, since L is nonsmooth [35]. Clearly, L is continuous and piecewise convex with finitely many convex pieces, as it is given as a minimum over finitely many continuous convex functions. Therefore, there are only finitely many points, at which convex pieces join up so that they form a strict local maximum. This means that for any local optimum of the objective there exists a sufficiently small neighborhood on which the objective is convex. Local convergence of DC-ADMM, may therefore be deduced from existing convergence results for convex objectives [5].\nIn practice, we observe that the algorithm converges also globally, once the penalty parameter \u03c1 is large enough. A mathematical proof is an interesting open problem and subject to future research."}, {"heading": "2.2 Discrete-Continuous Proximal Mapping", "text": "In order to state the final Algorithm 1, which we refer to as discrete-continuous ADMM (DC-ADMM), it remains to discuss the solution of the subproblem: In this section we devise an efficient Algorithm 2, that computes the Wi update in (7) and, as a byproduct, outputs the corresponding optimal labeling Y.\nThis update may be interpreted as a proximal step on the nonconvex, nonsmooth function L, which is defined as the minimum over in general exponentially many functions. For some step size \u03c3 > 0 the proximal mapping P\u03c3F and the corresponding the Moreau envelope M\u03c3F of a proper, lower\nAlgorithm 1 DC-ADMM\nInitialize Y0, {W0i }Ni=1, {\u03bb 0 i }Ni=1,W0, \u03c10. repeat // Update Yt+1 and {Wt+1i } via Algorithm 2: (Yt+1, {Wt+1i }) = DC-Prox() Wt+1 = 1N \u2211N i=1 \u03c1tW t+1 i \u2212 \u03bb t i\nfor i = 1 to N do \u03bbt+1i = \u03bb t i + \u03c1t(W t+1 i \u2212Wt+1) end for // Update \u03c1t+1 by increasing \u03c1t.\nuntil convergence\nAlgorithm 2 DC-Prox\nInitialize B \u2208 RN\u00d7C\u00d7C , \u03b3 \u2208 RN\u00d7C . // Compute the lookup table via (11) for all i \u2208 {1, . . . , N} and yj \u2208 Y do Bij = arg minWi \u00af\u0300(Wi,xi,yj)\n\u03b3ij = \u00af\u0300(Bij ,xi,yj)\nend for Y\u2217 = arg minY\u3008\u03b3,Y\u3009+ S(Y) for i = 1 to N do W\u2217i = \u2211C j=1 y \u2217 ijBij // lookup minimizer end for Output: (Y\u2217, {W\u2217i })\nsemicontinuous function F at some point V are defined according to [26]:\nP\u03c3F (V) := arg min U\nF (U) + 1\n2\u03c3 \u2016U\u2212V\u20162,\nM\u03c3F (V) := min U\nF (U) + 1\n2\u03c3 \u2016U\u2212V\u20162.\n(9)\nClearly, for convex F the proximal mapping at some V is unique and therefore well-defined. To extend the definition to the nonconvex case one usually interprets the arg min as a set-valued operator, that returns the (possibly empty) set of all global minimizers of the problem (cf. [28, Definition 1.22]).\nThe proximal mapping that we like to solve for the Wi update is given as\narg min {Wi}Ni=1,Y N\u2211 i=1 \u00af\u0300(Wi,xi,yi) + S(Y), \u00af\u0300(Wi,xi,yi) := `(Wi,xi,yi) +R(Wi) + \u03c1\n2 \u2016Wi \u2212Wt + \u03bbti/\u03c1\u20162.\n(10)\nTo this end we assume that the minimization of both the discrete function S and the terms \u00af\u0300(\u00b7,xi,yi), each corresponding to an instance i, can be performed efficiently. We observe, that (10) could in principle be solved via minimization in the continuous variables for every feasible labeling Y \u2208 dom(S). Obviously, this is not a viable approach as it implies performing exhaustive search over the finite, but potentially exponential-size set dom(S). Instead, we pursue a more efficient strategy:\nDiscrete-Continuous Decoupling. We assume for now that the supervisor S is separable S(Y) =\u2211 i Si(yi). Since the objective is also separable w.r.t. Wi, the overall problem is decomposed into N independent problems of the form minyi,Wi \u00af\u0300(Wi,xi,yi) + Si(yi). Problem i can then be solved via optimization w.r.t. Wi for all possible labels yj \u2208 Y: For each 1 \u2264 i \u2264 N and each yj \u2208 Y we create an entry (\u03b3ij ,Bij) in a lookup-table (\u03b3,B), in which the minimizer Bij and the corresponding objective values \u03b3ij are stored:\nBij := arg min Wi\n\u00af\u0300(Wi,xi,yj) = P 1 \u03c1 (`(\u00b7,xi,yi)+R) (Wt \u2212 \u03bbti/\u03c1),\n\u03b3ij := \u00af\u0300(Bij ,xi,yj) = M 1 \u03c1 (`(\u00b7,xi,yi)+R) (Wt \u2212 \u03bbti/\u03c1).\n(11)\nClearly, the optimal value (y\u2217i ,W \u2217 i ) can be obtained via looking up the pair that attains the lowest value \u03b3ij + Si(yj).\nWe note, that Bij resp. \u03b3ij are in turn determined via the proximal mappings resp. the Moreau envelopes of the functions `(\u00b7, xi,yi) +R evaluated at the current iterates Wt \u2212 \u03bbti/\u03c1 for step size 1/\u03c1.\nFor the general case of a nonseparable supervisor S we may proceed similarly, as for fixed Y the objective is separable w.r.t. Wi: The difference is, that the optimal labeling Y\u2217 needs to be computed,\nbefore the optimal values are looked up. The optimal labeling Y\u2217 is given via the solution of\nY\u2217 = min Y \u3008\u03b3,Y\u3009+ S(Y), (12)\nwhich can for instance be obtained via belief propagation for the special case (2) or the solution of a min cost flow problem or, in a more general setting via integer linear programming. Here, the matrix \u03b3 specifies an additional linear cost on Y, which corresponds to an additional unary potential for the special case (2).\nIt may be interpreted as a message, that guides the supervisor to adapt the labeling towards a more \u201coptimal\u201d classifier. The latter is determined via a tradeoff between the distance to the current consensus variable Wt \u2212 \u03bbti/\u03c1 and low loss value.\nThe optimal values W\u2217i can be read off from the solution of (11) via W \u2217 i = \u2211C j=1 y \u2217 ijBij . We summarize this procedure in Algorithm 2.\nDistributed Optimization. Overall, the Wi update requires the solution of only N \u00b7 C instead of to NC many independent and small-scale continuous minimization problems and one additional discrete problem. As a positive side effect, this allows for the distributed solution of the subproblems for example on a GPU. Subsequently, the solution of the discrete problem and the update of the consensus variable is carried out by the \u201cmaster\u201d after gathering the solutions of the subproblems. Distributed optimization is considered one of the main advantages of ADMM in supervised learning [10, 5].\nExploit Duality. In case `(\u00b7,xi,yi) is convex and lower semicontinuous in the first argument, the independent subproblems (11) may be solved efficiently via duality: For the employed loss functions the dual problem scales linearly with the number of training samples (which is equal to one in our case), whereas the primal scales linearly with the feature dimension d which may be high. For the Crammer and Singer multiclass SVM loss [8] for instance, there exists an efficient variable fixing algorithm [18] for solving the dual. Via the Lambert-W function the dual problem for the softmax loss can be reduced to a one-dimensional nonlinear equation [20], that may be solved by Newton\u2019s or Halley\u2019s method. For simpler special cases such as the hinge loss, there exists a closed form solution."}, {"heading": "2.3 A Tractable and Convergent Kernel-Formulation", "text": "Finally, for a kernel setting, we consider a different ADMM formulation that we find more appealing than DC-ADMM in two ways: Firstly, for separable supervisors, the per-iteration complexity is O(N2) so that our method scales as fully supervised training of a kernel SVM. Secondly, it admits a convergence guarantee (under mild assumptions) by existing theory. We note that convergence of ADMM in nonconvex optimization cannot be taken for granted in general as it is a tool traditionally used in convex optimization.\nReparameterization for a Kernel Setting. For the kernel setting we assume that the loss `(\u00b7,yi) is given as a function of classifier scores \u03c6(xi)>W, where \u03c6 denotes a possibly infinite dimensional feature map, and the regularizer is given as R(W) = \u03bd\u2016W\u20162, with \u03bd > 0. Then, for a fixed labeling Y, the representer theorem [29] states that the parameters W = \u03a6(X)\u03b1 can be substituted via their representation \u03b1 in terms of the matrix features \u03a6(X). We use this to employ a change of representation, in our objective function. Let K denote the kernel matrix associated to \u03a6(X). Using the same trick as in Section 2.1, we may encapsulate the minimization over Y within a function L so that ADMM formally operates on the continuous variables only: Let L be given as\nL(\u03b2) = min Y\n1\nN N\u2211 i=1 `(\u03b2i,yi) + S(Y). (13)\nThen, we may reformulate problem (1) (in a constrained form) as\nminimize \u03b1,\u03b2\nL(\u03b2) + \u03bd\u3008\u03b1,K\u03b1\u3009\nsubject to K\u03b1 = \u03b2. (14)\nThe iterates of ADMM are by definition given via\n\u03b2t+1 = arg min \u03b2\nL(\u03b2) + \u03c1\n2 \u2016\u03b2 \u2212K\u03b1t + \u03bbt/\u03c1\u20162\n\u03b1t+1 = arg min \u03b1 \u03bd\u3008\u03b1,K\u03b1\u3009+ \u03c1 2 \u2016K\u03b1\u2212 \u03b2t+1 \u2212 \u03bbt/\u03c1\u20162\n\u03bbt+1 = \u03bbt + \u03c1(\u03b2t+1 \u2212K\u03b1t+1).\n(15)\nAgain, the update of the variable \u03b2, given via a proximal mapping can be performed using an algorithm similar to Algorithm 2. The update of the variable \u03b1, can be obtained via the solution of a normal equation using either a cached eigenvalue decomposition of the kernel matrix, or an iterative algorithm such as conjugate gradient. We observe that in practice only a small number of conjugate gradient iterations are necessary. Empirically, the number of required (outer) ADMM-iterations rather depends on the desired accuracy, the update schedule for the penalty parameter and the complexity of the model, than on the number of training examples. This observation is supported by the fact, that in convex optimization, ADMM may converge slowly to a high accuracy result (sublinear convergence). However, in practice, ADMM usually converges to modest accuracy\u2013sufficient in machine learning tasks\u2013within a few dozen iterations [5]. In this sense the method asymptotically scales as fully supervised training of a kernel SVM for separable supervisors.\nThe following proposition states that the explicit ADMM-scheme (15) for our problem produces a sequence that converges to a critical point of the objective function.\nProposition 1. Let the kernel matrix K be positive definite and let ` be either globally Lipschitz in \u03b2i or locally Lipschitz and lower bounded and let \u03bd > 0. Then there exists a penalty parameter \u03c1, large enough, so that the sequence produced by the ADMM scheme applied to (14), converges to a stationary point of the augmented Lagrangian of (14).\nThe proof can be obtained by verifying that the assumptions in [35] are met. A formal proof is provided in the Appendix A.2.\nDiscussion of the Assumptions. We note that most of the considered loss functions are Lipschitz continuous including the hinge loss and the logistic loss.\nFurther, note that in general the kernel matrix K is not positive definite but positive semidefinite. However, for the strictly positive definite radial basis function (RBF) kernel, K is positive definite. Although the matrix is ill-conditioned and its smallest eigenvalues may be numerically around zero this does not harm in practice: Due to the presence of the regularizer, the solution of the least squares problem for the \u03b1-update becomes unique.\nIn order to enforce theoretical convergence also for general kernels, one may add a small constant to the diagonal of the kernel matrix K := K + I without fundamentally changing the model. Further, for the hinge loss this is can be interpreted as a transition from an `1-norm soft margin to an `2-norm soft margin in the context of the SVM-model.\nIn all conducted experiments we have observed, that ADMM is stable and globally converges to a (verifiable) critical point of the objective, even if the kernel matrix is not strictly positive definite."}, {"heading": "3 Experiments", "text": "In this section, we provide illustrative experiments to demonstrate that our method is able to learn a model in diverse weakly supervised scenarios."}, {"heading": "3.1 Proof of Concept", "text": "As a proof of concept we conduct two small toy experiments with synthetic data sampled from 2D moon-shape distributions (600 samples, 150 per class). Instead of providing exact labels for training, we provide synthetic complex combinatorial supervision that is much weaker in the sense that it is highly ambiguous and yet involves complicated constraints.\nLearning from Negative Labels and Pairwise Constraints. In the first experiment we attach to each training example i a randomly chosen negative label y\u0304i 6= y\u2217i (where y\u2217i is the true label of the training example) that only indicates that the training example i does not belong to class y\u0304i. Additionally, we provide the algorithm with randomly sampled tree-structured pairwise should-link and should-not-link constraints, which indicate that certain pairs of training examples (i, j) should belong to the same class (yi = yj) or to distinct classes (yi 6= yj). Again, the pairwise constraints are in accordance with the true training labels. Mathematically, the supervisor is given in terms of a tree-structured MRF-objective (2), where the unary resp. pairwise potentials are given as\nui(yi) = { \u221e if yi = y\u0304i 0 otherwise,\npij(yi,yj) = { 0 if yi = yj (respectively yi 6= yj) T otherwise,\n(16)\nfor a fixed penalty T > 0. This form of supervision is depicted in Figure 1a. The negative labels are visually interpreted as follows: The colored marker that is not present within some moon, but all the other moons, represents the true class of this cluster (cf. Figure 1a). It can be seen that our method determines a reasonable classifier and a corresponding labeling that is in accordance with the supervisor. The resulting training error of our method is 1.3%.\nLearning from Balancing Constraints. In this experiment we sample 25 (possibly overlapping) mini batches Bk \u2282 I of cardinality 25 from the training set. The synthetic supervision in this experiment is given in terms of a set of balancing constraints on the batches. More precisely, it restricts the maximal deviation of the determined labeling Y from the true labeling to a given bound within each batch Bk. Mathematically, the supervisor S is given as\nS(Y) = { 0 if Lkj \u2264 |{i \u2208 Bk : Yij = 1}| \u2264 Ukj for all 1 \u2264 j \u2264 C and 1 \u2264 k \u2264 25 \u221e otherwise. (17)\nwhere the bounds Lkj and U k j are chosen such that the number of samples assigned to class j deviates by at most 3 from the true number within batch Bk.\nWe optimize the arising linear program (12) with the dual-simplex method and observe that the solution is integral in our experiments. This ensures optimality.\nWe compare our method to constrained kernel k-means and hard EM on this task (see Figure 2). Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.r.t. to (17). As a second baseline we solve the proposed model (1) with a discrete-continuous coordinate descent, which we refer to as hard EM. For both, hard EM and our method we choose an SVMmodel. It can be seen that both hard EM (Figure 2c) and constrained kernel k-means (Figure 2b) get stuck in a poor local minimum, that is mainly guided by the initial solution of the combinatorial supervision problem (initial E-step) depicted in Figure 2a. As it can be seen in Figure 2a, solving the combinatorial problem only does not suffice to determine the true labels. Even worse, it gives the wrong cues for the true clusters.\nAs the performance of hard EM is highly sensitive to initialization, it may be improved by providing the algorithm with an initialization, that is more consistent with the actual clusters and not only with the weak supervision. However, finding good initializations w.r.t. complex combinatorial supervision\nmay in general become as difficult as solving the original problem. For that reason, we can initialize hard EM only with random classifiers resp. cluster centers in our experiments.\nSince our proposed method, is robust regarding the initialization, we observe that it overcomes the chicken-and-egg problem of finding good initializations w.r.t. weak supervision. It can be seen in Figure 2d that our method is able to infer the true labels of most training instances and finds a reasonable classifier, even though initially the wrong cues are given by the supervisor and all variables are initialized with zero. The obtained training errors are 66.6% for constrained RBF-kernel k-means, 68.5% for hard EM and 2.5% for our method."}, {"heading": "3.2 Handwritten Digits Classification with Weak Supervision.", "text": "As another experiment we train a weakly supervised RBF-kernel SVM on the popular MNIST handwritten digits data set. We restrict ourselves to a balanced subset of 10000 images from the original training set (which consists of 60000 images from 10 different classes). Instead of using the true class labels from the training set, we randomly attach k \u2208 {1, . . . , 8} many negative labels to each training example, which only indicate classes that the training example does not belong to. In a second setup we additionally provide the algorithm with randomly sampled pairwise must-link and must-not-link constraints that are in accordance with the ground truth. As a baseline we use the same SVM-model trained in a fully supervised fashion, which achieves a test error of 3.37%. Figure 3 reports the increase of incorrectly predicted training labels and test error compared to the baseline model depending on the number of negative labels k. It can be seen that the absolute test error increases by up to roughly 4% for only 1 negative label and by 2% in case additional pairwise constraints are present. This suggests that it is possible to learn a reasonable classifier without using any true class labels."}, {"heading": "3.3 Comparison with SDP Relaxation.", "text": "Finally, we compare to an SDP relaxation method for weakly supervised multinomial logistic regression by [17] on the task of semi-supervised learning. We consider the standard SSL benchmark [7] for a comparison also used by [17]. The benchmark is a collection of several datasets, with varying feature dimensions and number of classes. Each dataset is provided with 12 splits into l = 10 or l = 100 labeled and N \u2212 l unlabeled samples. Whereas [17] incorporates an entropy prior on the labeling which favors an equal balance distribution, we adapt the supervisor S in a way that it restricts the solution to deviate at most b = 3 resp. b = 20 percent from the equal balance distribution. We use a MATLAB implementation, that is provided by the authors. For these experiments, we use the softmax loss and set the regularization parameter \u03bd = 0.0025, which corresponds to \u03bb = 0.01 in the model by [17]. All values are averaged over 12 different splits. It can be seen in Table 1 that our method mostly performs better in terms of higher training accuracy. For l = 10 our method consistently produces better accuracies. Moreover, it produces more consistent results over the splits in terms of lower variances, which suggests that our method is more robust towards noise and poorly labeled data."}, {"heading": "4 Conclusion", "text": "In this work we have presented a novel algorithm for mixed-integer problems in the context of weakly supervised learning. We devise a discrete-continuous decomposition that is amenable to optimization by ADMM. Like hard EM, this decouples the discrete and the continuous optimization which allows for solving complex discrete-continuous models. Yet, the approach is inherently different from a discrete-continuous coordinate descent scheme in the following sense: The discrete-continuous alternation is performed within a proximal mapping of a nonsmooth nonconvex function, and thus the discrete variable is hidden to ADMM. In diverse experiments we have demonstrated that our method can overcome the chicken-and-egg problem of finding good initializations w.r.t. combinatorial supervision (which hard EM relies on). This allows for learning a classifier from weak and abstract combinatorial supervision. Overall, we are optimistic that our method can open a door to more challenging mixed-integer optimization problems for weakly and semi-supervised learning."}, {"heading": "A Proofs", "text": "A.1 Proof of Observation 1 Proof. Let the supervisor S be separable, i.e. S(Y) = \u2211 i Si(yi) and let {\u03bbi}Ni=1 be the Lagrange multipliers corresponding to the linear coupling constraints. Let \u00af\u0300(\u00b7,xi)\u2217 denote the Fenchel-Legendre convex conjugate of \u00af\u0300(\u00b7,xi). Then the Lagrangian dual problem of (4) is given as\nmax {\u03bbi}Ni=1 min W,{Wi}Ni=1 N\u2211 i=1 \u00af\u0300(\u00b7,xi)(W) + \u3008\u03bbi,W \u2212Wi\u3009\n= max {\u03bbi}Ni=1 min W N\u2211 i=1 \u3008\u03bbi,W\u3009 \u2212max Wi \u3008\u03bbi,Wi\u3009 \u2212 \u00af\u0300(\u00b7,xi)(W)\n= max {\u03bbi}Ni=1 min W N\u2211 i=1 \u3008\u03bbi,W\u3009 \u2212 \u00af\u0300(\u00b7,xi)\u2217(\u03bbi)\n(18)\nClearly, strong duality holds so that we may exchange the order of minimum and maximum and as the objective is separable w.r.t. the \u03bbi further rewrite\nmin W N\u2211 i=1 max \u03bbi \u3008\u03bbi,W\u3009 \u2212 \u00af\u0300(\u00b7,xi)\u2217(\u03bbi), (19)\nwhich is by definition equal to the convex biconjugate of \u00af\u0300(\u00b7,xi), i.e. the largest lower semi-continuous convex underapproximation of \u00af\u0300.\nA.2 Proof of Proposition 1\nProof. For the proof we consider the vectorized formulation of problem (14) meaning that \u03b1,\u03b2 \u2208 RNC and the linear constraints are given as K\u03b1 \u2212 I\u03b2 = 0 where K is redefined as K = I \u2297K. Since K is positive definite, K has full rank implying im(K) = RNC . Clearly, the objective in (14) is coercive over the feasible set {(\u03b1,\u03b2) : K\u03b1 = \u03b2} as K is positive definite, \u03bd > 0 and the loss terms `(\u00b7, yi) are either locally Lipschitz and lower bounded or globally Lipschitz. W.l.o.g. we may assume that `(\u00b7, yi) is globally Lipschitz (cf. [35]). Then, also \u2211N i=1 `(\u03b2i,yi) Lipschitz in \u03b2, as it is separable. Since dom(S) = {Y \u2208 YN | S(Y) < \u221e} is finite, L is the pointwise minimum over finitely many Lipschitz continuous functions and thus Lipschitz. Clearly, \u03bd\u3008\u00b7,K\u00b7\u3009 is Lipschitz differentiable. As K is invertible, assumption A3 in [35] holds trivially. The global convergence to a stationary point of the augmented Lagrangian of (14) is a direct consequence of Theorem 1 in [35]."}], "references": [{"title": "Fast and provably good seedings for kmeans", "author": ["O. Bachem", "M. Lucic", "S.H. Hassani", "A. Krause"], "venue": "D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 55\u201363", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Constrained Clustering: Advances in Algorithms", "author": ["S. Basu", "I. Davidson", "K. Wagstaff"], "venue": "Theory, and Applications. Chapman & Hall/CRC, 1 edition", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained k-means clustering", "author": ["K. Bennett", "P. Bradley", "A. Demiriz"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Linear Network Optimization", "author": ["D.P. Bertsekas"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S.P. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, 3(1):1\u2013122", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "A continuation method for semi-supervised svms", "author": ["O. Chapelle", "M. Chi", "A. Zien"], "venue": "W. W. Cohen and A. Moore, editors, Machine Learning, Proceedings of the Twenty-Third International Conference ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "editors", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": "Semi-Supervised Learning. MIT Press, Cambridge, MA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research, 2:265\u2013292", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D.P. Bertsekas"], "venue": "Math. Program., 55:293\u2013318", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Consensus-based distributed support vector machines", "author": ["P.A. Forero", "A. Cano", "G.B. Giannakis"], "venue": "Journal of Machine Learning Research, 11:1663\u20131707", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Chapter ix applications of the method of multipliers to variational inequalities", "author": ["D. Gabay"], "venue": "Studies in mathematics and its applications, 15:299\u2013331", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1983}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications, 2(1):17\u201340", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1976}, {"title": "Robust k-means: a theoretical revisit", "author": ["A. Georgogiannis"], "venue": "D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2883\u20132891", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Sur l\u2019approximation", "author": ["R. Glowinski", "A. Marroco"], "venue": "par \u00e9l\u00e9ments finis d\u2019ordre un, et la r\u00e9solution, par p\u00e9nalisation-dualit\u00e9 d\u2019une classe de probl\u00e8mes de dirichlet non lin\u00e9aires. Revue fran\u00e7aise d\u2019automatique, informatique, recherche op\u00e9rationnelle. Analyse num\u00e9rique, 9(2):41\u201376", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1975}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "author": ["M. Hong", "Z. Luo", "M. Razaviyayn"], "venue": "SIAM Journal on Optimization, 26(1):337\u2013364", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Consensus-admm for general quadratically constrained quadratic programming", "author": ["K. Huang", "N.D. Sidiropoulos"], "venue": "IEEE Trans. Signal Processing, 64(20):5297\u20135310", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "A convex relaxation for weakly supervised classifiers", "author": ["A. Joulin", "F.R. Bach"], "venue": "Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Variable fixing algorithms for the continuous quadratic knapsack problem", "author": ["K.C. Kiwiel"], "venue": "Journal of Optimization Theory and Applications, 136(3):445\u2013458", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "MRF optimization via dual decomposition: Message-passing revisited", "author": ["N. Komodakis", "N. Paragios", "G. Tziritas"], "venue": "IEEE 11th International Conference on Computer Vision, ICCV 2007, Rio de Janeiro, Brazil, October 14-20, 2007, pages 1\u20138. IEEE Computer Society", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Analysis and optimization of loss functions for multiclass", "author": ["M. Lapin", "M. Hein", "B. Schiele"], "venue": "top-k, and multilabel classification. CoRR, abs/1612.03663", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Global convergence of splitting methods for nonconvex composite optimization", "author": ["G. Li", "T.K. Pong"], "venue": "SIAM Journal on Optimization, 25(4):2434\u20132460", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory, 28(2):129\u2013 136", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1982}, {"title": "A distributed approach for the optimal power-flow problem based on ADMM and sequential convex approximations", "author": ["S. Magn\u00fasson", "P.C. Weeraddana", "C. Fischione"], "venue": "IEEE Trans. Control of Network Systems, 2(3):238\u2013253", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "An augmented lagrangian approach to constrained MAP inference", "author": ["A.F.T. Martins", "M.A.T. Figueiredo", "P.M.Q. Aguiar", "N.A. Smith", "E.P. Xing"], "venue": "L. Getoor and T. Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pages 169\u2013176. Omnipress", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed non-convex admm-based inference in large-scale random fields", "author": ["O. Miksik", "V. Vineet", "P. P\u00e9rez", "P.H.S. Torr"], "venue": "M. F. Valstar, A. P. French, and T. P. Pridmore, editors, British Machine Vision Conference, BMVC 2014, Nottingham, UK, September 1-5, 2014. BMVA Press", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien", "author": ["J.-J. Moreau"], "venue": "Bulletin de la Soci\u00e9t\u00e9 math\u00e9matique de France, 93:273\u2013299", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1965}, {"title": "Nested mini-batch k-means", "author": ["J. Newling", "F. Fleuret"], "venue": "D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1352\u20131360", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Variational Analysis", "author": ["R. Rockafellar", "R.-B. Wets"], "venue": "Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "D. P. Helmbold and R. C. Williamson, editors, Computational Learning Theory, 14th Annual Conference on Computational Learning Theory, COLT 2001 and 5th European Conference on Computational Learning Theory, EuroCOLT 2001, Amsterdam, The Netherlands, July 16-19, 2001, Proceedings, volume 2111 of Lecture Notes in Computer Science, pages 416\u2013426. Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K. M\u00fcller"], "venue": "Neural Computation, 10(5):1299\u20131319", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Jump-sparse and sparse recovery using potts functionals", "author": ["M. Storath", "A. Weinmann", "L. Demaret"], "venue": "IEEE Trans. Signal Processing, 62(14):3654\u20133666", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple effective heuristic for embedded mixed-integer quadratic programming", "author": ["R. Takapoui", "N. Moehle", "S. Boyd", "A. Bemporad"], "venue": "International Journal of Control, pages 1\u201323", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, 6:1453\u20131484", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "C. E. Brodley and A. P. Danyluk, editors, Proceedings of the Eighteenth International Conference on Machine Learning ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "Global convergence of ADMM in nonconvex nonsmooth optimization", "author": ["Y. Wang", "W. Yin", "J. Zeng"], "venue": "CoRR, abs/1511.06324", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "A constant-factor bi-criteria approximation guarantee for k-means++", "author": ["D. Wei"], "venue": "D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 604\u2013612", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "The effective energy transformation scheme as a special continuation approach to global optimization with application to molecular conformation", "author": ["Z. Wu"], "venue": "SIAM Journal on Optimization, 6(3):748\u2013768", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1996}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pages 1537\u20131544", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "An empirical study of ADMM for nonconvex problems", "author": ["Z. Xu", "S. De", "M.A.T. Figueiredo", "C. Studer", "T. Goldstein"], "venue": "CoRR, abs/1612.03349", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning structural svms with latent variables", "author": ["C.J. Yu", "T. Joachims"], "venue": "A. P. Danyluk, L. Bottou, and M. L. Littman, editors, Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, volume 382 of ACM International Conference Proceeding Series, pages 1169\u20131176. ACM", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximum margin clustering made practical", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "Z. Ghahramani, editor, Machine Learning, Proceedings of the Twenty-Fourth International Conference ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Global data association for multi-object tracking using network flows", "author": ["L. Zhang", "Y. Li", "R. Nevatia"], "venue": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient maximum margin clustering via cutting plane algorithm", "author": ["B. Zhao", "F. Wang", "C. Zhang"], "venue": "Proceedings of the SIAM International Conference on Data Mining, SDM 2008, April 24-26, 2008, Atlanta, Georgia, USA, pages 751\u2013762. SIAM", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "Typical choices are, for instance, the Crammer and Singer multi-class SVM loss [8], or the softmax loss, in combination with a linear classifier, in which case W \u2208 Rd\u00d7C .", "startOffset": 79, "endOffset": 82}, {"referenceID": 32, "context": "The loss may also be given as a structured or latent variable SVM loss [33, 40] or it may incorporate a nonlinear classifier such as a neural network.", "startOffset": 71, "endOffset": 79}, {"referenceID": 39, "context": "The loss may also be given as a structured or latent variable SVM loss [33, 40] or it may incorporate a nonlinear classifier such as a neural network.", "startOffset": 71, "endOffset": 79}, {"referenceID": 33, "context": "For a supervisor with only pairwise terms and a quadratic distance loss, the model (1) specializes to constrained k-means clustering [34, 2].", "startOffset": 133, "endOffset": 140}, {"referenceID": 1, "context": "For a supervisor with only pairwise terms and a quadratic distance loss, the model (1) specializes to constrained k-means clustering [34, 2].", "startOffset": 133, "endOffset": 140}, {"referenceID": 3, "context": "The supervisor can also be modeled in terms of a min cost flow objective [4] for an a-priori known flow network G with a-priori unknown costs [3, 2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "The supervisor can also be modeled in terms of a min cost flow objective [4] for an a-priori known flow network G with a-priori unknown costs [3, 2].", "startOffset": 142, "endOffset": 148}, {"referenceID": 1, "context": "The supervisor can also be modeled in terms of a min cost flow objective [4] for an a-priori known flow network G with a-priori unknown costs [3, 2].", "startOffset": 142, "endOffset": 148}, {"referenceID": 2, "context": "In [3, 2] this is used to efficiently model a constraint on the labeling Y, that balances the assignment of instances to different classes.", "startOffset": 3, "endOffset": 9}, {"referenceID": 1, "context": "In [3, 2] this is used to efficiently model a constraint on the labeling Y, that balances the assignment of instances to different classes.", "startOffset": 3, "endOffset": 9}, {"referenceID": 41, "context": "This may have applications for instance in multi-object tracking, which is often modeled in terms of a min cost flow problem [42].", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "A typical approach for tackling problems of the form (1) is via a coordinate descent scheme in the discrete and the continuous variables [3, 34, 2].", "startOffset": 137, "endOffset": 147}, {"referenceID": 33, "context": "A typical approach for tackling problems of the form (1) is via a coordinate descent scheme in the discrete and the continuous variables [3, 34, 2].", "startOffset": 137, "endOffset": 147}, {"referenceID": 1, "context": "A typical approach for tackling problems of the form (1) is via a coordinate descent scheme in the discrete and the continuous variables [3, 34, 2].", "startOffset": 137, "endOffset": 147}, {"referenceID": 21, "context": "The latter is commonly tackled by Lloyd\u2019s algorithm [22] that employs alternating optimization to compute a local minimum.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27].", "startOffset": 128, "endOffset": 143}, {"referenceID": 12, "context": "Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27].", "startOffset": 128, "endOffset": 143}, {"referenceID": 35, "context": "Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27].", "startOffset": 128, "endOffset": 143}, {"referenceID": 26, "context": "Due to its simplicity and low complexity the method is still appealing and therefore recent research is devoted to improvements [1, 13, 36, 27].", "startOffset": 128, "endOffset": 143}, {"referenceID": 0, "context": "The efficient computation of provably good seedings is therefore studied in [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 33, "context": "The k-means clustering problem can be extended so as to account for combinatorial soft and hard constraints on the clustering such as the discussed pairwise constraints [34] or a balancing constraint [3, 2].", "startOffset": 169, "endOffset": 173}, {"referenceID": 2, "context": "The k-means clustering problem can be extended so as to account for combinatorial soft and hard constraints on the clustering such as the discussed pairwise constraints [34] or a balancing constraint [3, 2].", "startOffset": 200, "endOffset": 206}, {"referenceID": 1, "context": "The k-means clustering problem can be extended so as to account for combinatorial soft and hard constraints on the clustering such as the discussed pairwise constraints [34] or a balancing constraint [3, 2].", "startOffset": 200, "endOffset": 206}, {"referenceID": 1, "context": "This is commonly referred to as constrained clustering [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 40, "context": "For loss functions that are more common in classification, such as the hinge loss, the alternating optimization scheme breaks down, because it converges early to a poor local optimum [41].", "startOffset": 183, "endOffset": 187}, {"referenceID": 37, "context": "Therefore, specialized algorithms based on SDP relaxations [38, 17] or CCCP [43] have been devised for a", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "Therefore, specialized algorithms based on SDP relaxations [38, 17] or CCCP [43] have been devised for a", "startOffset": 59, "endOffset": 67}, {"referenceID": 42, "context": "Therefore, specialized algorithms based on SDP relaxations [38, 17] or CCCP [43] have been devised for a", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "An SDP relaxation approach for weakly supervised learning in the context of multinomial logistic regression is proposed by [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "While convex SDP relaxation methods are robust regarding initialization, on the downside they generally do not scale well to larger problem instances: The approach by [17] has a per-iteration complexity of O(N).", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "This has two important drawbacks: (i) It is in general very difficult to strictly impose combinatorial constraints on the labeling and (ii) a post-processing (rounding) method is usually employed, which degrades the quality of the final solution compared to the optimal (relaxed) solution [17].", "startOffset": 289, "endOffset": 293}, {"referenceID": 13, "context": "ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9].", "startOffset": 16, "endOffset": 24}, {"referenceID": 11, "context": "ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9].", "startOffset": 16, "endOffset": 24}, {"referenceID": 10, "context": "ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9].", "startOffset": 114, "endOffset": 121}, {"referenceID": 8, "context": "ADMM, coined by [14, 12] is traditionally applied in convex optimization where it converges under mild conditions [11, 9].", "startOffset": 114, "endOffset": 121}, {"referenceID": 30, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 24, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 22, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 15, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 31, "context": "More recently, it has been applied as a heuristic method to a variety of discrete and NP-hard problems as well [31, 25, 23, 16, 32].", "startOffset": 111, "endOffset": 131}, {"referenceID": 15, "context": "In particular, the application of ADMM to quadratic mixed integer programs has recently been studied by [16, 32].", "startOffset": 104, "endOffset": 112}, {"referenceID": 31, "context": "In particular, the application of ADMM to quadratic mixed integer programs has recently been studied by [16, 32].", "startOffset": 104, "endOffset": 112}, {"referenceID": 31, "context": "Therefore, a rounding step may be integrated into the ADMM formulation [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "Whereas ADMM converges under very general assumptions for the convex case, the convergence in a more restrictive nonconvex setting has recently been proven [15, 21, 35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 20, "context": "Whereas ADMM converges under very general assumptions for the convex case, the convergence in a more restrictive nonconvex setting has recently been proven [15, 21, 35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 34, "context": "Whereas ADMM converges under very general assumptions for the convex case, the convergence in a more restrictive nonconvex setting has recently been proven [15, 21, 35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 40, "context": "Secondly, we solve challenging instances of Problem (1) experimentally, by means of ADMM, demonstrating the following advantages over alternative methods: Compared to alternating optimization by hard EM, which have been shown in [41] to be prone to bad local minima for classification loss functions, our algorithm is robust to initialization and systematically produces good local optima.", "startOffset": 229, "endOffset": 233}, {"referenceID": 16, "context": "Compared to the SDP relaxation proposed in [17] whose optimization has worst-case O(N) for a kernelized classifier, our algorithm has worst-case time complexity O(N) for a classifier parameterized by a kernel.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16].", "startOffset": 110, "endOffset": 126}, {"referenceID": 24, "context": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16].", "startOffset": 110, "endOffset": 126}, {"referenceID": 22, "context": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16].", "startOffset": 110, "endOffset": 126}, {"referenceID": 15, "context": "Finding good feasible solutions of an NP-hard problem via an ADMM approach has become popular in recent years [31, 25, 23, 16].", "startOffset": 110, "endOffset": 126}, {"referenceID": 9, "context": "Further, we note that in case the supervisor is the trivial zero function, our approach specializes to a decomposition that is typically used in supervised learning with ADMM [10, 5].", "startOffset": 175, "endOffset": 182}, {"referenceID": 4, "context": "Further, we note that in case the supervisor is the trivial zero function, our approach specializes to a decomposition that is typically used in supervised learning with ADMM [10, 5].", "startOffset": 175, "endOffset": 182}, {"referenceID": 38, "context": "Next we focus on the choice of the penalty parameter \u03c1, as a carefully chosen parameter may drastically improve the quality of the solutions found by ADMM in nonconvex optimization [39].", "startOffset": 181, "endOffset": 185}, {"referenceID": 23, "context": "In this section we provide an interpretation of this behavior in the context of graduated nonconvexity: In [24] the authors observe that consensus ADMM approaches a projected subgradient descent on the convex, negative Lagrangian dual function, referred to as dual decompostion (DD) [19], if the penalty parameter \u03c1 is close to zero.", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "In this section we provide an interpretation of this behavior in the context of graduated nonconvexity: In [24] the authors observe that consensus ADMM approaches a projected subgradient descent on the convex, negative Lagrangian dual function, referred to as dual decompostion (DD) [19], if the penalty parameter \u03c1 is close to zero.", "startOffset": 283, "endOffset": 287}, {"referenceID": 23, "context": "Whereas in [24] a convex objective is optimized, and therefore the problems solved by DD and ADMM are equivalent, in our case DD solves a convex relaxation that we characterize in the following Observation: Observation 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "In the light of this observation, we may informally interpret a nondecreasing update schedule for \u03c1 as a form of graduated nonconvexity [37].", "startOffset": 136, "endOffset": 140}, {"referenceID": 5, "context": "Continuation methods for graduated nonconvexity have for instance been applied for the optimization of semi-supervised versions of the SVM [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 34, "context": "Global convergence of Algorithm 1 can so far not be guaranteed by existing theory, since L is nonsmooth [35].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "Local convergence of DC-ADMM, may therefore be deduced from existing convergence results for convex objectives [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 25, "context": "semicontinuous function F at some point V are defined according to [26]:", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "Distributed optimization is considered one of the main advantages of ADMM in supervised learning [10, 5].", "startOffset": 97, "endOffset": 104}, {"referenceID": 4, "context": "Distributed optimization is considered one of the main advantages of ADMM in supervised learning [10, 5].", "startOffset": 97, "endOffset": 104}, {"referenceID": 7, "context": "For the Crammer and Singer multiclass SVM loss [8] for instance, there exists an efficient variable fixing algorithm [18] for solving the dual.", "startOffset": 47, "endOffset": 50}, {"referenceID": 17, "context": "For the Crammer and Singer multiclass SVM loss [8] for instance, there exists an efficient variable fixing algorithm [18] for solving the dual.", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "Via the Lambert-W function the dual problem for the softmax loss can be reduced to a one-dimensional nonlinear equation [20], that may be solved by Newton\u2019s or Halley\u2019s method.", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "Then, for a fixed labeling Y, the representer theorem [29] states that the parameters W = \u03a6(X)\u03b1 can be substituted via their representation \u03b1 in terms of the matrix features \u03a6(X).", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "However, in practice, ADMM usually converges to modest accuracy\u2013sufficient in machine learning tasks\u2013within a few dozen iterations [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 34, "context": "The proof can be obtained by verifying that the assumptions in [35] are met.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.", "startOffset": 13, "endOffset": 23}, {"referenceID": 33, "context": "Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.", "startOffset": 13, "endOffset": 23}, {"referenceID": 1, "context": "Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.", "startOffset": 13, "endOffset": 23}, {"referenceID": 29, "context": "Similarly to [3, 34, 2], we apply k-means in the RBF-kernel space [30] and solve the E-step w.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Finally, we compare to an SDP relaxation method for weakly supervised multinomial logistic regression by [17] on the task of semi-supervised learning.", "startOffset": 105, "endOffset": 109}, {"referenceID": 6, "context": "We consider the standard SSL benchmark [7] for a comparison also used by [17].", "startOffset": 39, "endOffset": 42}, {"referenceID": 16, "context": "We consider the standard SSL benchmark [7] for a comparison also used by [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "Whereas [17] incorporates an entropy prior on the labeling which favors an equal balance distribution, we adapt the supervisor S in a way that it restricts the solution to deviate at most b = 3 resp.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "01 in the model by [17].", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "Table 1: Comparison with the method of [17] on the SSL benchmark [7].", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "Table 1: Comparison with the method of [17] on the SSL benchmark [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 34, "context": "[35]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "As K is invertible, assumption A3 in [35] holds trivially.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "The global convergence to a stationary point of the augmented Lagrangian of (14) is a direct consequence of Theorem 1 in [35].", "startOffset": 121, "endOffset": 125}], "year": 2017, "abstractText": "This paper introduces a novel algorithm for a class of weakly supervised learning tasks. The considered tasks are posed as joint optimization problems in the continuous model parameters and the (a-priori unknown) discrete label variables. In contrast to prior approaches such as convex relaxations, we decompose the nonconvex problem into purely discrete and purely continuous subproblems in a way that is amenable to distributed optimization by the Alternating Direction Method of Multipliers (ADMM). This approach preserves integrality of the discrete label variables and, for a reparameterized variant of the algorithm using kernels, guarantees global convergence to a critical point. The resulting method implicitly alternates between a discrete and a continuous variable update, however, it is inherently different from a discrete-continuous coordinate descent scheme (hard EM). In diverse experiments we show that our method can learn a classifier from weak supervision that takes the form of hard and soft constraints on the labeling and outperforms hard EM in this task.", "creator": "LaTeX with hyperref package"}}}