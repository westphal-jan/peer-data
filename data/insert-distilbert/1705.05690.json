{"id": "1705.05690", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "A Long Short-Term Memory Recurrent Neural Network Framework for Network Traffic Matrix Prediction", "abstract": "network traffic matrix ( tm ) prediction is defined as the problem of estimating future network potential traffic from the previous and achieved network traffic history data. lastly it is the widely used in application network planning, resource management and network security. long short - term memory ( lstm ) is a solution specific recurrent neural network ( rnn ) architecture that is well - suited to clients learn from experience to classify, process and predict time series with time lags of unknown size. lstms have been shown to model temporal sequences and their complex long - range dependencies more realistic accurately than conventional rnns. in this paper, we propose both a lstm adaptive rnn operating framework for predicting short and long term traffic matrix ( linear tm ) in large networks. by validating our resulting framework on real - world data outputs from geant network, we show that consistently our lstm models converge quickly and give state description of the art tm prediction computing performance for relatively small sized models.", "histories": [["v1", "Tue, 16 May 2017 12:57:24 GMT  (358kb,D)", "http://arxiv.org/abs/1705.05690v1", "Submitted for peer review"], ["v2", "Sat, 20 May 2017 12:33:10 GMT  (358kb,D)", "http://arxiv.org/abs/1705.05690v2", "Submitted for peer review"], ["v3", "Thu, 8 Jun 2017 12:31:20 GMT  (469kb,D)", "http://arxiv.org/abs/1705.05690v3", "Submitted for peer review. arXiv admin note: text overlap witharXiv:1402.1128by other authors"]], "COMMENTS": "Submitted for peer review", "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["abdelhadi azzouni", "guy pujolle"], "accepted": false, "id": "1705.05690"}, "pdf": {"name": "1705.05690.pdf", "metadata": {"source": "CRF", "title": "A Long Short-Term Memory Recurrent Neural Network Framework for Network Traffic Matrix Prediction", "authors": ["Abdelhadi Azzouni", "Guy Pujolle"], "emails": ["abdelhadi.azzouni@lip6.fr", "guy.pujolle@lip6.fr"], "sections": [{"heading": null, "text": "keywords - Traffic Matrix, Prediction, Neural Networks, Long Short-Term Mermory\nI. INTRODUCTION\nMost of the decisions that network operators make depend on how the traffic flows in their network. However, although it is very important to accurately estimate traffic parameters, current routers and network devices do not provide the possibility for real-time monitoring, hence network operators cannot react effectively to the traffic changes. To cope with this problem, prediction techniques have been applied to predict network parameters and therefore be able to react to network changes in near real-time.\nThe predictability of network traffic parameters is mainly determined by their statistical characteristics and the fact that they present a strong correlation between chronologically ordered values. Network traffic is characterized by: self-similarity, multiscalarity, long-range dependence and a highly nonlinear nature (insufficiently modeled by Poisson and Gaussian models) [2].\nA network TM presents the traffic volume between all pairs of origin and destination (OD) nodes of the network at a certain time t. The nodes in a traffic matrix can be Pointsof-Presence (PoPs), routers or links.\nHaving an accurate and timely network TM is essential for most network operation/management tasks such as traffic accounting, short-time traffic scheduling or re-routing, network design, long-term capacity planning, and network anomaly detection. For example, to detect DDoS attacks in their early stage, it is necessary to be able to detect high-volume traffic\nclusters in near real-time. Another example is, upon congestion occurrence in the network, traditional routing protocols cannot react immediately to adjust traffic distribution, resulting in high delay, packet loss and jitter. Thanks to the early warning, a proactive prediction-based approach will be faster, in terms of congestion identification and elimination, than reactive methods which detect congestion through measurements, only after it has significantly influenced the network operation.\nSeveral methods have been proposed in the literature for network traffic forecasting. These can be classified into two categories: linear prediction and nonlinear prediction. The most widely used traditional linear prediction methods are: a) the ARMA/ARIMA model [3], [6], [7] and b) the HoltWinters algorithm [3]. The most common nonlinear forecasting methods involve neural networks (NN) [3], [8], [9]. The experimental results from [13] show that nonlinear traffic prediction based on NNs outperforms linear forecasting models (e.g. ARMA, ARAR, HW) which cannot meet the accuracy requirements. Choosing a specific forecasting technique is based on a compromise between the complexity of the solution, characteristics of the data and the desired prediction accuracy. [13] suggests if we take into account both precision and complexity, the best results are obtained by the Feed Forward NN predictor with multiresolution learning approach.\nUnlike feed forward neural networks (FFNN), Recurrent Neural Network (RNNs) have cyclic connections over time. The activations from each time step are stored in the internal state of the network to provide a temporal memory. This capability makes RNNs better suited for sequence modeling tasks such as time series prediction and sequence labeling tasks.\nLong Short-Term Memory (LSTM) is a RNN architecture that was designed by Hochreiter and Schmidhuber [15] to address the vanishing and exploding gradient problems of conventional RNNs. RNNs and LSTMs have been successfully used for handwriting recognition [1], language modeling, phonetic labeling of acoustic frames [10].\nIn this paper, we present a LSTM based RNN framework which makes more effective use of model parameters to train prediction models for large scale TM prediction. We train and compare our LSTM models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art TM prediction performance\nar X\niv :1\n70 5.\n05 69\n0v 1\n[ cs\n.N I]\n1 6\nM ay\n2 01\n7\nfor relatively small sized models. Note that we do not address the problem of TM estimation in this paper and we suppose that historical TM data is already accurately obtained.\nThe remainder of this paper is organized as follows: Section II summarizes time-series prediction techniques. LSTM RNN architecture and equations are detailed in section III. We detail the process of feeding our LSTM architecture and predicting TM in section IV. The prediction evaluation and results are presented in section V. Related work is presented in section VI and the paper is concluded by section VII."}, {"heading": "II. TIME SERIES PREDICTION", "text": "In this section, we give a brief summary of various linear predictors based on traditional statistical techniques, such as ARMA (Autoregressive Moving Average), ARIMA (Autoregressive Integrated Moving Average), ARAR (Autoregressive Autoregressive) and HW (HoltWinters) algorithm. And nonlinear time series prediction with neural networks.\n1) Linear Prediction: a) ARMA model: The time series {Xt} is called an ARMA(p, q) process if {Xt} is stationary (i.e. its statistical properties do not change over time) and\nXt\u2212\u03c61Xt\u22121\u2212...\u2212\u03c6pXt\u2212p = Zt+\u03b81Zt\u22121+...+\u03b8qZt\u2212q (1)\nwhere {Zt} \u2248 WN(0, \u03c32) is white noise with zero mean and variance \u03c32 and the polynomials \u03c6(z) = 1\u2212 \u03c61z \u2212 ...\u2212 \u03c6pz\np and \u03b8(z) = 1+\u03b81z+ ...+\u03b8qzq have no common factors. The identification of a zero-mean ARMA model which describes a specific dataset involves the following steps [20]: a) order selection (p, q); b) estimation of the mean value of the series in order to subtract it from the data; c) determination of the coefficients {\u03c6i, i = 1, p} and {\u03b8i, i = 1, q}; d) estimation of the noise variance \u03c32. Predictions can be made recursively using:\nX\u0302n+1 =  \u2211n j=1 \u03b8nj(Xn+1\u2212j \u2212 X\u0302n+1\u2212j) if1 \u2264 n \u2264 m)\u2211q j=1 \u03b8nj(Xn+1\u2212j \u2212 X\u0302n+1\u2212j) +\u03c61Xn + ..+ \u03c6pXn+1\u2212p ifn \u2265 m where m = max(p, q) and \u03b8nj is determined using the innovations algorithm. b) ARIMA model: A ARIMA(p, q, d) process is described by:\n\u03c6(B)(1\u2212B)dXt = \u03b8(B)Zt (2)\nwhere \u03c6 and \u03b8 are polynomials of degree p and q respectively, (1\u2212B) represents the differencing operator, d indicates the level of differencing and B is the backward-shift operator, i.e. BjXt = Xt\u2212j\nc) ARAR algorithm: The ARAR algorithm applies memory-shortening transformations, followed by modeling the dataset as an AR(p) process: Xt = \u03c61Xt\u22121+..+\u03c6pXt\u2212p+Zt\nThe time series {Yt} of long-memory or moderately longmemory is processed until the transformed series can be declared to be short-memory and stationary:\nSt = \u03c8(B)Yt = Yt + \u03c81Yt\u22121 + ...+ \u03c8kYt\u2212k (3)\nThe autoregressive model fitted to the mean-corrected series Xt = St \u2212 S, t = k + 1, n, where S represents the sample mean for Sk+1, ..., Sn , is given by \u03c6(B)Xt = Zt , where \u03c6(B) = 1 \u2212 \u03c61B \u2212 \u03c6l1Bl1 \u2212 \u03c6l2Bl2 \u2212 \u03c6l3Bl3 , {Zt} \u2248 WN(0, \u03c32), while the coefficients \u03c6j and the variance \u03c32 are calculated using the YuleWalker equations described in [20]. We obtain the relationship:\n\u03be(B)Yt = \u03c6(1)S + Zt (4)\nwhere \u03be(B)Yt = \u03c8(B)\u03d5(B) = 1+ \u03be1B + ...+ \u03bek+l3B k+l3 From the following recursion relation we can determine the linear predictors\nPnYn+h = \u2212 k+l3\u2211 j=1 \u03bePnYn+h\u2212j + \u03c6(1)S h \u2265 1 (5)\nwith the initial condition PnYn+h = Yn+h for h \u2264 0.\nd) HoltWinters algorithm: The HoltWinters forecasting algorithm is an exponential smoothing method that uses recursions to predict the future value of series containing a trend. If the time series has a trend, then the forecast function is:\nY\u0302n+h = PnYn+h = a\u0302n + b\u0302nh (6)\nwhere a\u0302n and b\u0302n are the estimates of the level of the trend function and the slope respectively. These are calculated using the following recursive equations:{\na\u0302n+1 = \u03b1Yn+1 + (1\u2212 \u03b1)(a\u0302n + b\u0302n) b\u0302n+1 = \u03b2(a\u0302n+1 \u2212 a\u0302n) + (1\u2212 \u03b2)\u0302bn\n(7)\nWhere Y\u0302n+1 = PnYn+1 = a\u0302n + b\u0302n represents the onestep forecast. The initial conditions are: a\u03022 = Y2 and b\u03022 = Y2 \u2212 Y1. The smoothing parameters \u03b1 and \u03b2 can be chosen either randomly (between 0 and 1), or by minimizing the sum of squared one-step errors \u2211n i=3(Yi \u2212 Pi\u22121Yi)2 [20].\n2) Neural Networks for Time Series Prediction: Neural Networks (NN) are widely used for modeling and predicting network traffic because they can learn complex non-linear patterns thanks to their strong self-learning and self- adaptive capabilities. NNs are able to estimate almost any linear or non-linear function in an efficient and stable manner, when the underlying data relationships are unknown. The NN model is a nonlinear, adaptive modeling approach which, unlike the techniques presented above, relies on the observed data rather than on an analytical model. The architecture and the parameters of the NN are determined solely by the dataset. NNs are characterized by their generalization ability, robustness, fault tolerance, adaptability, parallel processing ability, etc [14].\nA neural network consists of interconnected nodes, called neurons. The interconnections are weighted and the weights are also called parameters. Neurons are organized in layers: a) an input layer, b) one or more hidden layers and c) an output layer. The most popular NN architecture is feed-forward in which the information goes through the network only in the\nforward direction, i.e. from the input layer towards the output layer, as illustrated in figure 1.\nPrediction using a NN involves two phases: a) the training phase and b) the test (prediction) phase. During the training phase, the NN is supervised to learn from the data by presenting the training data at the input layer and dynamically adjusting the parameters of the NN to achieve the desired output value for the input set. The most commonly used learning algorithm to train NNs is called the backpropagation algorithm. The idea of the backpropagation is to propagate of the error backward, from the output to the input, where the weights are changed continuously until the output error falls below a preset value. In this way, the NN learns correlated patterns between input sets and the corresponding target values. The prediction phase represents the testing of the NN. A new unseen input is presented to the NN and the output is calculated, thereby predicting the outcome of new input data."}, {"heading": "III. LONG SHORT TERM MEMORY NEURAL NETWORKS", "text": "FFNNs can provide only limited temporal modeling by operating on a fixed-size window of TM sequence. They can only model the data within the window and are unsuited to handle historical dependencies. By contrast, recurrent neural networks or deep recurrent neural networks (figure 2) contain cycles that feed back the network activations from a previous time step as inputs to influence predictions at the current time step (figure 3). These activations are stored in the internal states of the network as temporal contextual information [10].\nHowever, training conventional RNNs with the gradientbased back-propagation through time (BPTT) technique is\ndifficult due to the vanishing gradient and exploding gradient problems. The influence of a given input on the hidden layers, and therefore on the network output, either decays or blows up exponentially when cycling around the networks recurrent connections. These problems limit the capability of RNNs to model the long range context dependencies to 5-10 discrete time steps between relevant input signals and output [11].\nTo address these problems, an elegant RNN architecture Long Short-Term Memory (LSTM) has been designed [15]. LSTMs and conventional RNNs have been successfully applied to sequence prediction and sequence labeling tasks. LSTM models have been shown to perform better than RNNs on learning context- free and context-sensitive languages for example [5]."}, {"heading": "A. LSTM Architecture", "text": "The architecture of LSTMs is composed of units called memory blocks. Memory block contains memory cells with self-connections storing (remembering) the temporal state of the network in addition to special multiplicative units called gates to control the flow of information. Each memory block contains an input gate to control the flow of input activations into the memory cell, an output gate to control the output flow of cell activations into the rest of the network and a forget gate (figure 4).\nThe forget gate scales the internal state of the cell before adding it back to the cell as input through self recurrent connection, therefore adaptively forgetting or resetting the cells memory. The modern LSTM architecture also contains\npeephole connections from its internal cells to the gates in the same cell to learn precise timing of the outputs [4]."}, {"heading": "B. LSTM Equations", "text": "In this subsection we provide the equations for the activation (forward pass) and gradient calculation (backward pass) of an LSTM hidden layer within a recurrent neural network. The backpropagation through time algorithm with the exact error gradient is used to train the network. The LSTM equations are given for a single memory block only. For multiple blocks the calculations are simply repeated for each block, in any order [12].\na) Notations:\n\u2022 wij the weight of the connection from unit i to unit j \u2022 ati the network input to some unit j at time t \u2022 bti the value of the same unit after the activation function\nhas been applied \u2022 \u03b9 input gate, \u03c6 forget gate, \u03c9 output gate \u2022 C set of memory cells of the block \u2022 stc state of cell c at time t (i.e. the activation of the linear\ncell unit) \u2022 f the activation function of the gates, g cell input activa-\ntion functions, h cell output activation functions \u2022 I the number of inputs, K the number of outputs, H\nnumber of cells in the hidden layer\nNote that only the cell outputs btc are connected to the other blocks in the layer. The other LSTM activations, such as the states, the cell inputs, or the gate activations, are only visible within the block.\nWe use the index h to refer to cell outputs from other blocks in the hidden layer.\nAs with standard RNNs the forward pass is calculated for a length T input sequence x by starting at t = 1 and recursively applying the update equations while incrementing t, and the BPTT backward pass is calculated by starting at t = T , and recursively calculating the unit derivatives while decrementing t (see Section 3.2 for details). The final weight derivatives are found by summing over the derivatives at each timestep, as expressed in Eqn. (3.34). Recall that\n\u03b4tj = \u2202O\n\u2202atj (8)\nWhere O is the objective function used for training. The order in which the equations are calculated during the forward and backward passes is important, and should proceed as specified below. As with standard RNNs, all states and activations are set to zero at t = 0, and all \u03b4 terms are zero at t = T + 1.\nb) Forward Pass: Input Gates\nat\u03b9 = I\u2211 i=1 wi\u03b9x t i + H\u2211 h=1 wh\u03b9b t\u22121 h + C\u2211 c=1 wc\u03b9s t\u22121 c (9)\nbt\u03b9 = f(a t \u03b9) (10)\nForget Gates\nat\u03c6 = I\u2211 i=1 wi\u03c6x t i + H\u2211 h=1 wh\u03c6b t\u22121 h + C\u2211 c=1 wc\u03c6s t\u22121 c (11)\nbt\u03c6 = f(a t \u03c6) (12)\nCells\natc = I\u2211 i=1 wicx t i + H\u2211 h=1 whcb t\u22121 h (13)\nstc = b t \u03c6s t\u22121 c + b t \u03b9g(a t c) (14)\nOutput Gates\nat\u03c9 = I\u2211 i=1 wi\u03c9x t i + H\u2211 h=1 wh\u03c9b t\u22121 h + C\u2211 c=1 wc\u03c9s t\u22121 c (15)\nbt\u03c9 = f(a t \u03c9) (16)\nCell Outputs btc = b t \u03c9h(s t c) (17)\nc) Backward Pass:\ntc = \u2202O\n\u2202btc (18)\nts = \u2202O\n\u2202stc (19)\nCell Outputs\n\u03b4t\u03b9 = f \u2032(at\u03b9) C\u2211 c=1 g(atc) t s (20)\nOutput Gates\n\u03b4t\u03c6 = f \u2032(at\u03c6) C\u2211 c=1 st\u22121c t s (21)\nStates \u03b4tc = b t \u03b9g \u2032(atc) t s (22)\nCells\nts = b t \u03c9h \u2032(stc) t c + b t+1 \u03c6 t+1 s + wc\u03b9\u03b4 t+1 \u03b9 + wc\u03c6\u03b4 t+1 \u03c6 + wc\u03c9\u03b4 t+1 \u03c9\n(23) Forget Gates\n\u03b4t\u03c9 = f \u2032(at\u03c9) C\u2211 c=1 h(stc) t c (24)\nInput Gates\nts = K\u2211 k=1 wck\u03b4 t k + H\u2211 h=1 wch\u03b4 t+1 h (25)\nwhere f() (frequently noted as \u03c3(.)) is the standard logistic sigmoid function defined in Eq.(8), g() and h() are the transformations of function () whose range are [-2,2] and [- 1,1] respectively: \u03c3(x) = 11+e\u2212x , g(x) = 4 1+e\u2212x \u2212 2 and h(x) = 21+e\u2212x \u2212 1"}, {"heading": "IV. TRAFFIC MATRIX PREDICTION USING LSTM RNN", "text": "In this section we describe the use of a deep LSTM architecture with a deep learning method to extract the dynamic features of network traffic and predict the future TM. This architecture can deeply excavate mutual dependence among the traffic entries in various timeslots."}, {"heading": "A. Problem Statement", "text": "Let N be the number of nodes in the network. The N - by-N traffic matrix is denoted by Y such as an entry yij represents the traffic volume flowing from node i to node j. We add the time dimension to obtain a structure of N-by-N-by-T tensor (vector of matrices) S such as an entry stij represents the volume of traffic flowing from node i to node j at time t, and T is the total number of time-slots. The traffic matrix prediction problem is defined as solving the predictor of Y t (denoted by Y\u0302 t) via a series of historical and measured traffic data set (Y t\u22121, Y t\u22122, Y t\u22123, ..., Y t\u2212T ). The main challenge here is how to model the inherent relationships among the traffic data set so that one can exactly predict Y t."}, {"heading": "B. Feeding The LSTM RNN", "text": "To effectively feed the LSTM RNN, we transform each matrix Y t to a vector Xt (of size N\u00d7N ) by concatenating its N rows from top to bottom. Xt is called traffic vector (TV). Note that xn entries can be mapped to the original yij using the relation n = i\u00d7N + j. Now the traffic matrix prediction problem is defined as solving the predictor of Xt (denoted by X\u0302t) via a series of historical measured traffic vectors (Xt\u22121, Xt\u22122, Xt\u22123, ..., Xt\u2212T ).\nOne possible way to predict the traffic vector Xt is to predict one component xtn at a time by feeding the LSTM RNN one vector (xt0, x t 1, ..., x t N2) at a time. This is based on the assumption that each OD traffic is independent from all other ODs which was shown to be wrong by [21]. Hence, considering the previous traffic of all ODs is necessary to obtain a more accurate prediction of the traffic vector.\nContinuous Prediction Over Time: Real-time prediction of traffic matrix requires continuous feeding and learning. Over time, the total number of time-slots become too big resulting in high computational complexity. To cope with this problem, we introduce the notion of learning window (denoted by W ) which indicates a fixed number of previous time-slots to learn from in order to predict the current traffic vector Xt (Fig. 5).\nWe construct the W -by-N2 traffic-over-time matrix (that we denote by M ) by putting together W vectors (Xt\u22121, Xt\u22122, Xt\u22123, ..., Xt\u2212W ) ordered by time. Note that T \u2265 W (T being the total number of historical matrices) and the number of matrices M is equal to T/W ."}, {"heading": "C. Performance Metric", "text": "To quantitatively assess the overall performance of our LSTM model, Mean Square Error (MSE) is used to estimate the prediction accuracy. MSE is a scale dependent metric which quantifies the difference between the forecasted values and the actual values of the quantity being predicted by computing the average sum of squared errors:\nMSE = 1\nN N\u2211 i=1 (yi \u2212 y\u0302i)2 (26)\nwhere yi is the observed value, y\u0302i is the predicted value and N represents the total number of predictions."}, {"heading": "V. EXPERIMENTS AND EVALUATION", "text": "In this section, we will evaluate the prediction accuracy of our method using real traffic data from the GE\u0301ANT backbone networks [16]. GE\u0301ANT is the pan-European research network. GE\u0301ANT has a PoP in each European country and it carries research traffic from the European National Research and Education Networks (NRENs) connecting universities and research institutions. As of 2005, the GE\u0301ANT network was made up of 23 peer nodes interconnected using 38 links. In addition, GE\u0301ANT has 53 links with other domains.\n2004-timeslot traffic matrix data is sampled from the GE\u0301ANT network by 15-min interval [17] for several months. In our simulation, we also compare our prediction and estimation methods with a state-of-the-art method, that is, the PCA method introduced in the above section.\nTo evaluate our method on short term traffic matrix prediction, we consider a set of 309 traffic matrices measured between 01-01-2005 00am and 04-01-2005 5:15am. As detailed in section IV-B, we transform the matrices to vectors of size 529 each and we concatenate the vectors to obtain the trafficover-time matrix M of size 309\u00d7 529. We split M into two matrices, training matrix Mtrain and validation matrix Mtest of sizes 263 and 46 consecutively. Mtrain is used to train the LSTM RNN model and Mtest is used to evaluate and validate its accuracy. Finally, We normalize the data by dividing by the maximum value.\nWe use Keras library [18] to build and train our model. The training is done on a Intel core i7 machine with 16GB memory. Figures 6 and 7 show the variation of the prediction error when using different numbers of hidden units and hidden layers respectively. Finally, figure 8 compares the prediction error of the different prediction methods presented in this paper and shows the superiority of LSTM. Note that, the prediction results of the linear predictors and FFNN are obtained from [13] and they represent the error of predicting only one traffic value which is obviously an easier task than predicting the whole traffic matrix.\nFig. 6: MSE over size of hidden layer\nFig. 7: MSE over number of hidden layers (500 nodes each)"}, {"heading": "VI. RELATED WORK", "text": "Various methods have been proposed to predict traffic matrix. [13] evaluates and compares traditional linear prediction models (ARMA, ARAR, HW) and neural network based prediction with multi-resolution learning. The results show that NNs outperform traditional linear prediction methods which cannot meet the accuracy requirements. [21] proposes a FARIMA predictor based on an \u03b1-stable non-Gaussian selfsimilar traffic model. [19] compares three prediction methods: Independent Node Prediction (INP), Total Matrix Prediction with Key Element Correction (TMP-KEC) and Principle Component Prediction with Fluctuation Component Correction (PCP-FCC). INP method does not consider the correlations among the nodes, resulting in unsatisfying prediction error. TMP-KEC method reduces the forecasting error of key elements as well as that of the total matrix. PCP-FCC method improves the overall prediction error for most of the OD flows."}, {"heading": "VII. CONCLUSION", "text": "In this work, we have shown that LSTM RNN architectures are well suited for traffic matrix prediction. We have proposed a data pre-processing and RNN feeding technique that achieves high prediction accuracy in a few seconds of computation (approximately 60 seconds for one hidden layer of 300 nodes). The results of our evaluations show that LSTM RNNs outperforms traditional linear methods and feed forward neural networks by many orders of magnitude."}], "references": [{"title": "A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks.", "author": ["Liwicki", "Marcus"], "venue": "Proc. 9th Int. Conf. on Document Analysis and Recognition", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "On the self-similar nature of Ethernet traffic", "author": ["W. Leland", "M. Taqqu", "W. Willinger", "D. Wilson"], "venue": "In Proc. SIGCOMM 93,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "Internet Traffic Forecasting using Neural Networks, International", "author": ["P. Cortez", "M. Rio", "M. Rocha", "P. Sousa"], "venue": "Joint Conference on Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["Felix A. Gers", "Nicol N. Schraudolph", "Jurgen Schmidhuber"], "venue": "Journal of Machine Learning Research ,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "LSTM recurrent networks learn simple context free and context sensitive lan- guages", "author": ["Felix A. Gers", "Jurgen Schmidhuber"], "venue": "IEEE Transactions on Neural Networks ,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Study on Network Traffic Prediction Techniques, International Conference on Wireless Communications, Networking and Mobile Computing, pp. 10411044", "author": ["H. Feng", "Y. Shu"], "venue": "Wuhan, China,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "VBR MPEG Video Traffic Dynamic Prediction Based on the Modeling and Forecast of Time Series", "author": ["J. Dai", "J. Li"], "venue": "Fifth International Joint Conference on INC, IMS and IDC,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Gavade, An NN Approach for MPEG Video Traffic Prediction", "author": ["J.D.V.B. Dharmadhikari"], "venue": "2nd International Conference on Software Technology and Engineering, pp. V1-57V1-61. San Juan,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Evaluation of neural network architectures for MPEG-4 video traffic prediction", "author": ["A. Abdennour"], "venue": "IEEE Transactions on Broadcasting,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Long shortterm memory recurrent neural network architectures for large scale acoustic modeling.", "author": ["Sak", "Hasim", "Andrew W. Senior", "Franoise Beaufays"], "venue": "Interspeech", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Evaluation of network traffic prediction based on neural networks with multi-task learning and multiresolution decomposition.", "author": ["Barabas", "Melinda"], "venue": "Intelligent Computer Communication and Processing (ICCP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Study on Network Traffic Prediction Techniques, International Conference on Wireless Communications, Networking and Mobile Computing, pp. 10411044", "author": ["H. Feng", "Y. Shu"], "venue": "Wuhan, China,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Long short-term memory.", "author": ["Hochreiter", "Sepp", "Jrgen Schmidhuber"], "venue": "Neural computation", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Providing public intradomain traffic matrices to the research community.", "author": ["Uhlig", "Steve"], "venue": "ACM SIGCOMM Computer Communication Review", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Prediction and correction of traffic matrix in an IP backbone network.", "author": ["Liu", "Wei"], "venue": "Performance Computing and Communications Conference (IPCCC),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Introduction to Time Series and Forecasting, Second Edition", "author": ["P.J. Brockwell", "R.A. Davis"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Prediction for non-gaussian self-similar traffic with neural network.", "author": ["Wen", "Yong", "Guangxi Zhu"], "venue": "Intelligent Control and Automation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "Network traffic is characterized by: self-similarity, multiscalarity, long-range dependence and a highly nonlinear nature (insufficiently modeled by Poisson and Gaussian models) [2].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "The most widely used traditional linear prediction methods are: a) the ARMA/ARIMA model [3], [6], [7] and b) the HoltWinters algorithm [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "The most widely used traditional linear prediction methods are: a) the ARMA/ARIMA model [3], [6], [7] and b) the HoltWinters algorithm [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "The most widely used traditional linear prediction methods are: a) the ARMA/ARIMA model [3], [6], [7] and b) the HoltWinters algorithm [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "The most widely used traditional linear prediction methods are: a) the ARMA/ARIMA model [3], [6], [7] and b) the HoltWinters algorithm [3].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "The most common nonlinear forecasting methods involve neural networks (NN) [3], [8], [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "The most common nonlinear forecasting methods involve neural networks (NN) [3], [8], [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "The most common nonlinear forecasting methods involve neural networks (NN) [3], [8], [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "The experimental results from [13] show that nonlinear traffic prediction based on NNs outperforms linear forecasting models (e.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "[13] suggests if we take into account both precision and complexity, the best results are obtained by the Feed Forward NN predictor with multiresolution learning approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Long Short-Term Memory (LSTM) is a RNN architecture that was designed by Hochreiter and Schmidhuber [15] to address the vanishing and exploding gradient problems of conventional RNNs.", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "RNNs and LSTMs have been successfully used for handwriting recognition [1], language modeling, phonetic labeling of acoustic frames [10].", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "RNNs and LSTMs have been successfully used for handwriting recognition [1], language modeling, phonetic labeling of acoustic frames [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "The identification of a zero-mean ARMA model which describes a specific dataset involves the following steps [20]: a) order selection (p, q); b) estimation of the mean value of the series in order to subtract it from the data; c) determination of the coefficients {\u03c6i, i = 1, p} and {\u03b8i, i = 1, q}; d) estimation of the noise variance \u03c3.", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": ", Sn , is given by \u03c6(B)Xt = Zt , where \u03c6(B) = 1 \u2212 \u03c61B \u2212 \u03c6l1B \u2212 \u03c6l2B \u2212 \u03c6l3B , {Zt} \u2248 WN(0, \u03c3), while the coefficients \u03c6j and the variance \u03c3 are calculated using the YuleWalker equations described in [20].", "startOffset": 198, "endOffset": 202}, {"referenceID": 15, "context": "The smoothing parameters \u03b1 and \u03b2 can be chosen either randomly (between 0 and 1), or by minimizing the sum of squared one-step errors \u2211n i=3(Yi \u2212 Pi\u22121Yi) [20].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "NNs are characterized by their generalization ability, robustness, fault tolerance, adaptability, parallel processing ability, etc [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "These activations are stored in the internal states of the network as temporal contextual information [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "To address these problems, an elegant RNN architecture Long Short-Term Memory (LSTM) has been designed [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "LSTM models have been shown to perform better than RNNs on learning context- free and context-sensitive languages for example [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "peephole connections from its internal cells to the gates in the same cell to learn precise timing of the outputs [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "(8), g() and h() are the transformations of function () whose range are [-2,2] and [1,1] respectively: \u03c3(x) = 1 1+e\u2212x , g(x) = 4 1+e\u2212x \u2212 2 and h(x) = 2 1+e\u2212x \u2212 1", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "(8), g() and h() are the transformations of function () whose range are [-2,2] and [1,1] respectively: \u03c3(x) = 1 1+e\u2212x , g(x) = 4 1+e\u2212x \u2212 2 and h(x) = 2 1+e\u2212x \u2212 1", "startOffset": 83, "endOffset": 88}, {"referenceID": 0, "context": "(8), g() and h() are the transformations of function () whose range are [-2,2] and [1,1] respectively: \u03c3(x) = 1 1+e\u2212x , g(x) = 4 1+e\u2212x \u2212 2 and h(x) = 2 1+e\u2212x \u2212 1", "startOffset": 83, "endOffset": 88}, {"referenceID": 16, "context": "This is based on the assumption that each OD traffic is independent from all other ODs which was shown to be wrong by [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "2004-timeslot traffic matrix data is sampled from the G\u00c9ANT network by 15-min interval [17] for several months.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Note that, the prediction results of the linear predictors and FFNN are obtained from [13] and they represent the error of predicting only one traffic value which is obviously an easier task than predicting the whole traffic matrix.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "[13] evaluates and compares traditional linear prediction models (ARMA, ARAR, HW) and neural network based prediction with multi-resolution learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[21] proposes a FARIMA predictor based on an \u03b1-stable non-Gaussian selfsimilar traffic model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[19] compares three prediction methods: Independent Node Prediction (INP), Total Matrix Prediction with Key Element Correction (TMP-KEC) and Principle Component Prediction with Fluctuation Component Correction (PCP-FCC).", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Network Traffic Matrix (TM) prediction is defined as the problem of estimating future network traffic from the previous and achieved network traffic data. It is widely used in network planning, resource management and network security. Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that is well-suited to learn from experience to classify, process and predict time series with time lags of unknown size. LSTMs have been shown to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we propose a LSTM RNN framework for predicting Traffic Matrix (TM) in large networks. By validating our framework on real-world data from G\u00c9ANT network, we show that our LSTM models converge quickly and give state of the art TM prediction performance for relatively small sized models. keywords Traffic Matrix, Prediction, Neural Networks, Long Short-Term Mermory", "creator": "LaTeX with hyperref package"}}}