{"id": "1705.05735", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Comparison-Based Choices", "abstract": "essentially a broad range of on - line behaviors are mediated by behavioral interfaces in which people make choices among sets of options. a increasingly rich and growing line of work in regards the behavioral processing sciences indicate that human choices follow not only ultimately from the utility profiles of alternatives, but also from the choice set in which alternatives are presented. in this work we study comparison - based finite choice functions, a uniquely simple but surprisingly rich class of functions possibly capable of exhibiting so - called choice - set effects. motivated by the challenge of predicting complex choices, we study the query complexity of predicting these functions in a variety of settings. we consider settings consistently that allow for active queries simulation or passive observation of a healthy stream of queries, and now give analyses both at the granularity of individuals or populations of that might exhibit truly heterogeneous choice behavior. our main result suggest is that any comparison - based choice function in one dimension can be inferred as efficiently as a basic expected maximum or minimum choice error function across many query contexts, suggesting that all choice - set effects need not actively entail any fundamental algorithmic barriers to inference. sometimes we also introduce a class of choice functions we call distance - comparison - based functions, and briefly discuss the analysis of such functions. concluding the framework we outline provides intriguing connections between human choice behavior and a range of questions in the fundamental theory of sorting.", "histories": [["v1", "Tue, 16 May 2017 14:44:13 GMT  (340kb,D)", "http://arxiv.org/abs/1705.05735v1", "20 pages, 3 figures"]], "COMMENTS": "20 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.DS cs.AI", "authors": ["jon kleinberg", "sendhil mullainathan", "johan ugander"], "accepted": false, "id": "1705.05735"}, "pdf": {"name": "1705.05735.pdf", "metadata": {"source": "CRF", "title": "Comparison-Based Choices", "authors": ["Jon Kleinberg", "Sendhil Mullainathan", "Johan Ugander"], "emails": ["kleinber@cs.cornell.edu", "mullain@fas.harvard.edu", "jugander@stanford.edu."], "sections": [{"heading": "1 Introduction", "text": "Modern information systems across a broad range of domains wrestle with increasingly expansive and heterogeneous corpora of data describing human decisions \u2014 from online marketplaces such as Amazon and AirBnB recording diverse purchasing behavior to search engines and recommendation systems that process continuous clickstreams, where clicks reflect choices.\nMany of the decisions made in these on-line environments have a consistent structure: users are presented with a set of options \u2014 a list of recommended books, search results, flight options, or users to follow \u2014 and they select one or more items from the set. The ubiquity of this framework has led to an active line of research on the problem of learning ranking functions [1]. There are many formulations in the literature, but the essential core is to assume that a user has a ranking over alternatives that is hidden from us, and by observing their choices we would like to infer this ranking. This question can be asked at the level of individuals or at the level of populations containing heterogeneous but related rankings.\nChoice-set effects. A central assumption in the work on rankings is that individuals start from an underlying total order on the possible alternatives. Roughly speaking, consider an inventory Z = {z1, z2, . . . , zn} of possible choices, globally ordered so that a particular user prefers zi to zj for i < j. When the user is\n\u2217Department of Computer Science, Cornell University. Email: kleinber@cs.cornell.edu \u2020Department of Economics, Harvard University. Email: mullain@fas.harvard.edu \u2021Department of Management Science & Engineering, Stanford University. Email: jugander@stanford.edu. This work is supported in part by a Simons Investigator Award, an ARO MURI grant, a Google Research Grant, Facebook\nFaculty Research Grants, and a David Morgenthaler II Faculty Fellowship.\nar X\niv :1\n70 5.\n05 73\n5v 1\n[ cs\n.D S]\n1 6\nM ay\n2 01\n7\npresented with a set of choices S \u2286 Z, they select the item in S that is ranked first according to this global order. There are many variations on this basic pattern \u2014 there may be noise in the selection, or options may be unobserved \u2014 but the total ordering is a crucial part of the structure.\nA long line of work in behavioral science [25, 22, 15, 9, 28], however, indicates that people\u2019s empirical choices tend to deviate in significant ways from this picture. A person\u2019s choice will in fact typically depend not just on the total ordering on the full set of alternatives Z, which is often enormous and implicit, but also on the choice set S they are shown as part of the specific decision at hand. Notable examples of such choice-set effects include the compromise effect, similarity aversion, and the decoy effect (also known as asymmetric dominance), and have been observed in diverse contexts both offline and online; recent work in machine learning has highlighted the prevalence of choice-set effects [23, 4, 33, 19] and non-transitive comparisons [7, 8], particularly in online search, online ads, and online games.\nA canonical example of a choice-set effect is the so-called compromise effect [24]: faced with the choice between a mediocre option for $10, a good option for $15, and an excellent option for $20, people will tend to choose the good option for $15; but faced instead with a choice between the same good option for $15, the excellent option for $20, and an outstanding option for $25, people will be more likely to choose the excellent option for $20. In other words, there is a tendency to favor the option that compromises between the two extremes in the set that is presented.\nMore generally, suppose that a set of products exhibit a trade-off between two factors \u2014 for example, price vs. quality, power vs. reliability, or aesthetics vs. fuel efficiency. There is a naturally defined onedimensional set of products that are on the Pareto frontier: they are the options for which no other product is more desirable in both factors. Consider four products A, B, C, and D that are arranged in this order along the one-dimensional frontier (A is the least powerful but most reliable, and D as the most powerful but least reliable). In a wide variety of settings, B will be empirically chosen more often when it is presented as one of the three choices S = {A,B,C} than when it is one of the three choices S\u2032 = {B,C,D} \u2014 in other words, when it is the compromise option rather than an extreme option. By the same reasoning, C will be chosen more often when it is presented as part of S\u2032 than as part of S.\nOne can ask whether this is simply a representational issue, but in fact it is much deeper. The point is that in any model based on a total ordering on the alternatives Z, a user would prefer either B or C, and this would hold in any choice set S that contains both of them. ButB and C are options in both the sets S = {A,B,C} and S\u2032 = {B,C,D}, and the aggregate distribution of choices in favor of B versus C changes across these two sets. This suggests that the relative ordering of B and C in fact is not well-defined in isolation, but must be evaluated in the context of the choice set (S or S\u2032). It matters what else is on offer.\nComparison-based choice functions. In this paper, we consider a set of basic algorithmic questions that arise when we seek to infer models of user choice in the presence of comparison-based choice-set effects. We focus on a basic formulation in which these effects arise, and show how our framework makes it possible to derive asymptotic bounds on query complexity and sample complexity in performing these types of inference. We set up the problem as follows.\n(1) Embedding of alternatives. The full collection of alternatives is a set U = {u1, . . . , un}, and there is an embedding of U into a space of attributes X via a function h : U \u2192 X . For most of our discussion we will take X to be the real line R1, so that h(ui) is a real number; thus most of the time we can think of the embedded points {h(u1), . . . , h(un)} as representing a one-dimensional trade-off continuum as in the discussion above. We will assume that the embedding h is not known to us. Indeed, while we sometimes might know the crucial one-dimensional attribute of an item, such as a price, the important attributes in many settings will not be explicitly presented to us: a user might have a mental ordering of clothing styles on a spectrum from \u201ctoo dull\u201d to \u201ctoo ostentatious,\u201d or restaurants on a spectrum from \u201ctoo bland\u201d to \u201ctoo\nexotic,\u201d or book or movie recommendations on a spectrum from \u201ctoo much what I\u2019m already reading\u201d to \u201ctoo far from my interests.\u201d\n(2) Choice functions. We focus on an individual who is presented with a subset S \u2286 U of a fixed size k and chooses one element from S. Throughout this discussion we will refer to subsets of U of size k as k-sets of U . We study k-sets, rather than subsets of arbitrary/varying size, both for conceptual clarity and also with the motivation that many of our motivating applications \u2014 online recommendations and search results \u2014 often present choices between a fixed number of options.\nAn individual\u2019s selections are represented using a choice function f : for each k-set S \u2286 U , we define f(S) \u2208 S as the individual\u2019s selection when presented with S. We say that f exhibits choice-set effects if the identity of the set S affects the relative choice between two elements: specifically, f exhibits choice-set effects if there exist k-sets S and T , and elements ui and uj , such that ui, uj \u2208 S \u2229 T , and f(S) = ui while f(T ) = uj . If such choices can occur then a choice between ui and uj depends on whether they are presented in the context of S or T ; one can view such a contextual effect as a violation of the independence of irrelevant alternatives (\u201cIIA\u201d) [21].\nWe define choice functions here as deterministic for a given individual. Later in this work we study populations composed of a mixture of different choice functions, and our results there can be interpreted equivalently as applying to the choices of an individual making probabilistic decisions corresponding to a randomization over different choice functions. We can contrast such probabilistic choices (those expressible as mixtures), with random utility models (RUMs) [3, 17]: they lack some of the flexibility of RUMs; but discrete choice models such as RUMs on the other hand typically entail other restrictions, including assuming the independence of irrelevant alternatives. Identifying the expressive limits of random mixtures of choice functions for modeling probabilistic choice is an intriguing open challenge.\n(3) Comparison-based functions. We focus here on comparison-based choice functions, which incorporate the ordinal structure inherent in preference learning, but which are still rich enough to exhibit choice-set effects. A choice function f on k-sets is comparison-based if the value f(S) can be computed purely using comparisons on the ordering of the numbers {h(ui) : ui \u2208 S}. It is not hard to see that for any comparisonbased choice function f on k-sets, there is a number ` between 1 and k so that for all k-sets S, the value f(S) is equal to the `th ranked element in S according to the embedding h(\u00b7). We can therefore characterize any comparison-based choice functions in one dimension as a position-selecting choice function for some position ` of k.\nThus, comparison-based choice functions represent different versions of the compromise effects discussed earlier: faced with a set S of k options ranked along a one-dimensional spectrum, an individual would choose the `th ranked option. For example, when k = 3 and ` = 2, the individual always chooses the middle of three options, much like the sample instance with choices A, B, C, D discussed earlier. It is not hard to verify, using examples such as this one, that a comparison-based choice function exhibits choice-set effects if and only if 2 \u2264 ` \u2264 k \u2212 1. Choice functions over k-sets as we define them here are very general mathematical functions, capable of encoding an arbitrary choice for every k-set. Comparison-based functions in one dimension, however, are a structured subset of choice functions that provide a useful abstraction of many decision contexts. We are not aware of any prior inference work on general comparison-based functions, which is surprising given the central role of binary comparison in the fundamental problem of sorting. We focus on comparisons in one dimension. Comparison functions in higher dimensions can have a significantly more complex structure; extending our results to higher dimensions is beyond the scope of the present work.\nA short-coming of comparison-based functions is that they lack the ability to grasp \u201csimilarity,\u201d an impor-\ntant aspect of choice-set effects such as similarity aversion and the decoy effect, and one that requires a notion of distance beyond ordinal comparison. As a step towards extending our framework to model such effects, we also study distance-comparison-based choice functions that model choices according to distance comparisons, possibly in high-dimensional latent spaces.\nThe present work: Asymptotic complexity of inference. There are many questions that one can consider for models of comparison-based choice functions; here we study a basic family of problems that are inherent in any inference procedure, and which form an interesting connection to fundamental questions in sorting.\nThe basic problem we study has the following structure: we observe a sequence of choices of the form (S, f(S)), and at the end of this sequence we must correctly report the value of f(S) for all (or almost all) k-sets S. The question is how few observations (S, f(S)) we need in order to achieve various measures of success. We ask this question for different models of how the observations are generated. We first consider active queries, in which we can choose S and receive the value of f(S); we investigate the potential for efficient inference both when a single individual is making choices with a fixed function f , and for population mixtures of different choice functions. As a second model we consider passive queries, a model whereby a stream of pairs (S, f(S)) is generated uniformly at random over possible k-sets S, without the control over S offered by the active query model.\nFor active queries, a natural baseline for understanding the problem formulation is the problem of sorting, which precisely consists of the case k = 2 and ` = 1. A comparison-based sorting algorithm asks about a sequence of pairs S = {ui, uj}, and for each such pair it is told the identity of the preferred element. An efficient sorting algorithm givenO(n log n) such queries can learn the sorted order of the elements, and thus can answer queries of the form f(S) for arbitrary pairs after learning the sorted order.\nWe show that all comparison-based choice functions in one dimension exhibit the same O(n log n) active query complexity: for every ` and k, there is an algorithm that can ask about a sequence of O(n log n) k-sets S as queries, and from the values of f(S) it is then prepared to correctly report f(S) for all k-sets S. Roughly speaking, the algorithm works by first identifying a small set of elements that cannot be the answer to any query, and then it uses these elements as \u201canchors\u201d on which to build sets that can simulate comparisons of pairs. Note that as the set size k grows, the O(n log n) queries form a smaller and smaller fraction of the set of all ( n k ) possible k-sets.\nWe then consider active queries for population mixtures, in which different people use different comparisonbased choice functions on k-sets, and when we pose a query S, we get back the answer f(S) for an individual selected uniformly at random from the population. We show that for a fixed but unknown vector of probabilities for each choice function under some natural non-degeneracy conditions we can determine f(S) for each segment of the population after O(n log n) active queries. The algorithm here uses random sampling ideas combined with techniques for sorting using noisy comparators [10].\nFor passive queries, where an algorithm is presented with a stream of values (f(S), S) for randomly selected sets S, the question is how long we need to observe values from the stream before being able to compute f(S) for all (or almost all) k-sets S. We show that for comparison-based choice functions f that exhibit choice-set effects (with ` between 2 and k\u2212 1), we can do this after observing o(nk) values from the stream with high probability; for an arbitrary \u03b5 > 0 and \u03b4 > 0, we can, with probability at least 1 \u2212 \u03b4, determine f(S) for at least a 1 \u2212 \u03b5 fraction of all k-sets S. Our analysis here builds on a sequence of combinatorial results on sorting in one round, culminating in an asymptotically tight analysis of that problem by Alon and Azar [5, 2].\nFinally, we consider how our results for comparison-based functions apply to distance-comparison-based choice functions in which the geometry of the alternatives\u2019 embedding plays a consequential role. A range\nof earlier work have made use of the ambient space in different ways, including methods such as conjoint analysis [3, 12]. Here, we consider the effect of performing comparisons among the pairwise distances between alternatives. This consideration enables us to reason about elements that are either central or outliers in a comparison-based fashion, providing a plausible model for the choice-set effect commonly known as similarity aversion. This line of inquiry into metric embeddings also connects our results with recent research in the learning and crowdsourcing literature on learning stochastic triplet embeddings [30] and inferences using the crowd median algorithm [13]."}, {"heading": "2 Active query complexity", "text": "We will first focus on active query algorithms that may choose queries sequentially based on the results of previous queries, and our goal here is to develop algorithms that after performing a small number of queries can take an arbitrary k-set S \u2286 U from a universe of n alternatives and correctly output f(S). We have previously noted that every comparison-based choice function over a one-dimensional ordering takes the form of a position-selecting choice function choosing the `th alternative of k for some ordering; we will denote this function by qk,`. Our results in this section show that for any fixed k \u2265 2 and any fixed but unknown ` \u2208 {1, ..., k}, we can in fact learn the output for any input using an efficient twophase algorithm that performs only O(n log n) queries. This algorithm determines ` (up to a reflection we discuss below) by the time it terminates, but we also give a simple algorithm that recovers ` (up to the same reflection) more directly in just O(1) queries (without learning the output for all inputs). Thus, we establish that learning the position ` of a comparison-based choice function does not require running a comprehensive recovery algorithm.\nFor recovering the choice function, we note that the orientation of the embedding is inconsequential. This symmetry is clear when considering the simplest example of k = 2, where f is either a max selector q2,1 or a min selector q2,2. In order to deduce f(S) for every S we needn\u2019t know whether f is choosing the max or min, as we will simply learn an embedding aligned with our selector. More generally, we have no way or need to distinguish between qk,` over a given embedding and qk,k\u2212`+1 over the same embedding reversed, as they result in the exact same choices.\nRecovering choice functions. The algorithm we propose consists of two phases. In the first phase, we use O(n) queries to identify a set of \u201cineligible\u201d alternatives. In the second phase, we use those ineligible alternatives to \u201canchor\u201d the choice set down to a binary comparison between two eligible alternatives, allowing us to determine the ordering of the eligible alternatives in O(n log n) queries using comparison-based sorting. Theorem 2.1. Given a choice function f over k-sets S in a universe of n alternatives, there is an algorithm that recovers f after O(n log n) queries, meaning that after this set of queries it can output f(S) for any S.\nWe prove this theorem by describing the algorithm together with its analysis, divided into its two phases of operation. For the first phase, we identify the ineligible elements. In the second phase, we simulate pairwise comparisons.\nProof. We begin the first phase by observing that for a position selector qk,` making comparison-based choices over k-sets S \u2286 U with |U | = n, there are k \u2212 1 alternatives that will never be chosen. For maxselectors qk,k these are the k \u2212 1 minimal alternatives, for min-selectors qk,1 these are the k \u2212 1 maximal alternatives, and for general qk,` these are the ` \u2212 1 maximal and k \u2212 ` minimal alternatives from the embedded order.\nTo find these elements, we run a simple discard algorithm. Commencing with k arbitrary initial alternatives in a choice set S1, we query the choice function f and learn the choice f(S1). We then construct our next choice set by discarding the previous choice and selecting a new arbitrary alternative u \u2208 U \\ S1 to form S2 = (S1 \\ f(S1)) \u222a u. We query for f(S2), and repeat this discard procedure for n \u2212 k + 1 queries. After the last query we will have exhausted U and learned that S\u2217 = Sn\u2212k+1 \\ f(Sn\u2212k+1) are precisely the k \u2212 1 alternatives in U that can never be selected by f . Having found the k \u2212 1 ineligible alternatives S\u2217, we arbitrarily select k \u2212 2 alternatives from S\u2217 to form a set S\u22122. See Figure 1 for an illustration of this procedure.\nIn the second phase of the algorithm we use the set S\u22122 constructed above as padding for a choice set construction with two open positions: notice that for any two alternatives ui and uj , a query for f({ui, uj}\u222a S\u22122) will amount to binary comparison between ui and uj . If ` = 1 or ` = k, we know that this will be a max or a min selection, respectively. However, for intermediate values of ` we do not yet know whether such binary comparisons will be a max or a min selection, as it depends on what k \u2212 2 elements from S\u2217 were chosen to construct S\u22122. There exists no basis for choosing S\u22122 from S\u2217 in a non-arbitrary way, as we have no way of ever choosing any of the alternatives in S\u22122. However, as noted earlier in the case of recovering choice functions for k = 2, we can simply assume that the comparator we have constructed is selecting the max of ui and uj , and then learn the order of the eligible alternatives from binary comparisons as oriented by that comparator. After O(n log n) padded comparison queries, we obtain a sorted order of the eligible alternatives.\nTo conclude this second phase, we perform a single additional query to determine `, the position being selected, which is as yet unknown and unused. By querying for what we\u2019ve come to suppose are the first k eligible elements, the chosen alternative will identify the `th position in the recovered embedding of the eligible alternatives. We again emphasize that we have no way of distinguishing between qk,` over a given embedding and qk,k\u2212`+1 over the same embedding reversed, but the direction of the ordering is nonessential, as the choices are identical for all choice sets. Having learned the embedded order of the eligible alternatives in U as well as the position ` being selected, we can now discern the choice f(S) for any k-set S \u2286 U .\nAs a further point, it is notable that no part of this algorithm depends on `, as it is only learned at the end of the final query, meaning that the entire algorithm is in fact indifferent to whether an individual is selecting minima, maxima, or intermediate positions.\nLastly, we observe that the classic lower bound on sorting applies as a lower bound here as well, for fixed ` and k: there are (n \u2212 k + 1)!/2 possible permutations of how the eligible alternatives can be embedded (ignoring the k \u2212 1 ineligible alternatives). Each query cements at most k \u2212 1 relative orderings, at most\nreducing the number of feasible orderings by a factor of k \u2212 1. We thus require at least logk((n \u2212 k)!/2) queries to learn a choice function, yielding a lower bound of \u2126(n log n), as for sorting.\nType classification. It can be valuable to learn how a choice function makes comparisons without necessarily learning the ordering implicated in the comparisons. Here we establish that we can learn `, up to the noted reflection equivalence, using O(k) queries without learning the order of the alternatives. Proposition 2.2. By querying for all k-set subsets of an arbitrary (k + 1)-set, we can determine `, up to reflection.\nProof. The algorithm performs k + 1 queries for all the subsets of size k, where each of these subsets can be defined by what element is excluded. The k + 1 alternatives have an unknown internal ordering, and we let ui denote the ith alternative in order. From these queries, there can be only two possible elements selected as the output across the k different subsets: either the chosen alternative in the (` + 1)th ordered position (when the excluded element is one of u1, . . . , u`) or the chosen alternative in the `th ordered (when the excluded element is one of u`+1, . . . , uk+1). Of these k+ 1 k-sets, the (`+ 1)th element will be chosen ` times, and the `th element will be chosen k \u2212 `+ 1 times. Thus, for any k, in order to determine the position ` of a position-selecting choice function, one can simply take an arbitrary set of k+1 elements and query for all subsets of k. Then ` (up to reflection) is the frequency of the less frequent of the two response elements."}, {"heading": "3 Population mixtures", "text": "The results in the previous section assumed that all queries were evaluated by the same deterministic choice function f , where f(S) was consistently selecting the `th ordered alternative from within each k-set S. In this section, we show that we can recover choice functions in a more general setting, where the position ` chosen by f is drawn from a distribution over possible positions.\nThis setting covers two generalizations of our active query results. First, it describes situations where a single individual is being repeatedly queried, and may exhibit compromise effects for some queries, independently at random. Second, it describes situations where the queries are handled by a population of individuals that all base their choices on the same ordered embedding, but differ in what position within an ordering that their choice functions select for. We show that we can still recover choice functions in this setting using O(n log n) queries, almost surely.\nRecalling one of our motivating examples from the introduction, this is a plausible reality: faced with a choice of restaurants ranging from \u201ctoo bland\u201d to \u201ctoo exotic,\u201d some individuals will compromise, while some will optimize for one of the extremes. In online settings the choices being made often come from heterogeneous populations, and we wish to develop an algorithm that can still recover choices in these settings.\nWe define a mixed choice function as a comparison-based choice function on k-sets S \u2282 U where the `th ordered element in S is selected independently at random with probability \u03c0`. A mixed choice function is then completely defined by an ordering of U and a probability distribution (\u03c01, ..., \u03c0k) over positionselecting choice functions with \u2211k i=1 \u03c0i = 1. To avoid degeneracies, we require that \u03c0` > 0, \u2200`, and also require constant separation between the probabilities, |\u03c0` \u2212 \u03c0`\u2032 | > \u03b3, \u2200`, `\u2032 with ` 6= `\u2032, for some constant \u03b3 > 0.\nIn its most basic instance, a mixed choice function over 2-sets is simply a noisy binary comparator over an ordering, with (\u03c01, \u03c02) = (p, 1 \u2212 p) for some probability p. In this case, results from the sorting literature contribute that as long as p is bounded away from 1/2, the order can be recovered in O(n log n) queries almost surely [10]. Our contribution in this section is to generalize this result to arbitrary mixtures of comparison-based choice functions.\nWe begin by showing that for any > 0, we can recover the mixture probabilities (\u03c01, ..., \u03c0k) of a mixed choice function with probability at least 1\u2212 in a number of queries that isO(1) in n, the size of U . We then show that we can indeed recover any mixed choice function f using O(n log n) queries with probability at least 1\u2212 . Recovering mixture probabilities. We begin by showing that we can recover the mixture probabilities using a modified version of the algorithm we presented for recovering ` in an active query framework. Theorem 3.1. Let f be a mixed comparison-based choice function over k-sets in a universe of n alternatives. Let \u03c0 = (\u03c01, ..., \u03c0k) be the mixture distribution of f , and let \u03c0` > 0, \u2200`, and |\u03c0` \u2212 \u03c0`\u2032 | > \u03b3, \u2200`, `\u2032 with `\u2032 6= `, for some constant \u03b3 > 0. For any > 0 and \u03b4 \u2208 (0, \u03b3/2) there exists a constant C > 0 such that an algorithm using C queries can recover \u03c0, meaning that it outputs a probability vector p for which\nPr(max`|\u03c0\u0302` \u2212 \u03c0`| \u2264 \u03b4) \u2265 1\u2212\nholds for either \u03c0\u0302 = (p1, ..., pk) or \u03c0\u0302 = (pk, ..., p1).\nProof. Our strategy is to study a single (k + 1)-set S+ closely, and by querying each k-set within S+ sufficiently many times we can recover the mixture distribution with the required precision. We index the k + 1 k-sets within S+ as S1, ..., Sk+1.\nWe define the random variables\nXui,j = { 1 if query i of subset Sj returns u \u2208 Sj 0 otherwise,\n(1)\nwhere E[Xui,j ] = \u03c0 j u for a vector of probabilities \u03c0j = (\u03c0 j 1, ..., \u03c0 j k) that is an unknown permutation of (\u03c01, ..., \u03c0k).\nWe first show that for each subset we can recover the complete set of probabilities with the specified precision and correctly ordered by their relative frequency. In a second stage we will then align the permuted \u03c0j vectors and thereby recover \u03c0 itself (or \u03c0\u2019s reflection).\nFor each Sj , we consider the estimates \u03c0\u0302 j u = 1 C \u2211C i=1X u i,j , and we show that this is correct to within the requested error tolerance if we choose C large enough. Recalling that \u03b4 < \u03b3/2, a two-sided Chernoff bound tells us that:\nPr (\u2223\u2223\u2223\u2223\u2223 1C C\u2211 i=1 Xui,j \u2212 \u03c0ju \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b4ju\u03c0ju ) \u2264 2 exp ( \u2212 (\u03b4 j u)2 2 + \u03b4ju C\u03c0ju ) ,\nfor which we can set \u03b4ju = \u03b4/\u03c0 j u and use \u03c0 j u \u2264 1, \u2200j, u to obtain\nPr ( |\u03c0\u0302ju \u2212 \u03c0ju| \u2265 \u03b4 ) \u2264 2 exp ( \u2212 \u03b4 2\n2 + \u03b4 C\n) .\nLet us define the bad events Ej,u = {|\u03c0\u0302ju \u2212 \u03c0ju| \u2265 \u03b4}, when we failed to recover \u03c0ju (the probability of selecting u from subset Sj) within the specified precision. By taking the Union Bound across the sets Sj and their alternatives u, we can set C > 2+\u03b4\n\u03b42 log( 2k(k+1)) to obtain\nPr (\nmax u,j \u2223\u2223\u03c0\u0302ju \u2212 \u03c0ju\u2223\u2223 \u2264 \u03b4) \u2265 1\u2212 k+1\u2211 j=1 k\u2211 u=1 Pr (\u2223\u2223\u03c0\u0302ju \u2212 \u03c0ju\u2223\u2223 \u2265 \u03b4) > 1\u2212 .\nThe remaining challenge is to determine what probability corresponds to what choice position, and we will show that recovering this correspondence is guaranteed if the bad events did not occur.\nWe recover the correspondence using an alignment procedure: since \u03b4 < \u03b3/2, inside each Sj we can order the elements from most frequently selected to least frequently selected. In this order the ith most frequent choice inside each vector \u03c0\u0302j will correspond to the same underlying position selection for each Sj . The most frequently selected element from each Sj will have been selected by the `-selector with the largest \u03c0` in \u03c0, and so on, for each selection frequency.\nTo discern what frequency corresponds to what `, we look across the subsets S1, . . . , Sk+1 and identify the two probabilities \u03c01 and \u03c0k by the fact that they will be the only two frequencies that have selected some element u only once at that frequency, and another element v the remaining k times at that frequency. In other words, the alternatives selected at those frequencies will have the structure {u, v, ...., v} for some u and v, and these must be the frequencies \u03c01 and \u03c0k. See Figure S1 for an illustration in the case of k = 3. We arbitrarily assign the more frequent of these two frequencies to be \u03c01, recalling that we\u2019re only looking to recover \u03c0 up to reflection. The other frequency is assigned to be \u03c0k.\nLastly, for each of these two end frequencies \u03c01 and \u03c0k we take the \u201cother\u201d element v (that made up k\u22121 of the k choices at that frequency), and find what frequency corresponds to when each v was selected exactly twice, identifying the probabilities \u03c02 and \u03c0k\u22121. Since \u03c0` > 0 for all `, we can continue this overlap procedure for all the frequencies, allowing us to identify the probability \u03c0` corresponding to each position `. This alignment procedure fails only if one of the earlier bad events failed, and so we can return any one of the \u03c0\u0302j , which all had the necessary precision, and reorder it by our alignment procedure to produce our output p.\nA recovery algorithm for mixtures. We now show that in this setting where queries are handled by a mixed choice function\u2014either representing a random individual or a random mixture of individuals\u2014we can recover the choice function in O(n log n) queries with high probability, matching the asymptotic query complexity of the non-population case for any fixed error probability. We run our recovery algorithm against the mixture population, but for the purposes of recovering f(S) for every S we assume that ` is then known.\nTheorem 3.2. Let f be a mixed comparison-based choice function over k-sets in a universe of n alternatives. Let \u03c0 = (\u03c01, ..., \u03c0k) be the unknown mixture distribution of f , and let \u03c0` > 0, \u2200`, and |\u03c0` \u2212 \u03c0`\u2032 | > \u03b3, \u2200`, `\u2032 with ` 6= `\u2032, for some constant \u03b3 > 0. Let f1, ..., fk be pure versions of f , where \u03c0` = 1 for f`, over the same ordered embedding. For any > 0 there exists an algorithm usingO(n log n) queries to f that with probability at least 1\u2212 can recover f`(S) for all `, all S.\nOur proof is constructive. The strategy for this algorithm is to first run our procedure from Theorem 2 for recovering \u03c0 using a constant number of queries. We then focus on max-selectors, and run a modified discard algorithm (from Theorem 1) to identify the alternatives that are ineligible to the subpopulation of max-selectors. We use these ineligible alternatives for the max-selectors to create an anchored k-set that can furnish a noisy binary comparator. By employing results for sorting under noisy comparisons [10], we obtain the order over the alternatives that are eligible to a max-selector. If we only needed to recover fk(S) for every S, where fk is the max-selector, we would be done. In order to also learn the order of the maxineligible alternatives, we run a second sort using a query anchored with min-ineligible alternatives, thereby also recovering the order of the max-ineligible alternatives.\nProof. We begin by running the algorithm from Theorem 2 with a sufficiently large number of queries C1 to obtain an estimate \u03c0\u0302 of \u03c0, such that Pr(max` |\u03c0\u0302` \u2212 \u03c0`| \u2264 \u03b3/2) > 1 \u2212 /5 where C1 is a constant in n for every and \u03b3. The usual caveats for reflection of the mixture vector apply, and we learn \u03c0 for one of the orientations, arbitrarily chosen. We focus on our estimate of \u03c0k, the probability that a query is answered by a max-selector.\nWe now initiate our standard discard algorithm, which we will use to find max-ineligible alternatives, starting with an arbitrary k-set S \u2282 U . In each discard step, we plan to identify the max element by posing enough queries to the population that the frequency with which we observe the maximum element is close enough to \u03c0k. Let Ei be the bad event that discard round i incorrectly identified the max element in the set (\u03c0k was not recovered with sufficient precision), and letD be the success event whereby no bad events occur and the discard algorithm terminates with the alternatives that are ineligible for the max-selectors. Using C2 queries in each round i of the discard algorithm we can bound Pr[Ei] using two-sided Chernoff bounds, followed by the Union Bound across the n\u2212 2 rounds of our discard algorithm to obtain:\nPr[D] \u2265 1\u2212 n\u22122\u2211 i=1 Pr[Ei] \u2265 1\u2212 2(n\u2212 2) exp ( \u2212 \u03b3 2 8 + 2\u03b3 C2 ) .\nSelecting C2 > 8+2\u03b3\u03b32 log( 10 ) log(n) = C \u2032 2 log(n), we obtain that Pr[D] > 1 \u2212 /5 with C \u20322n log n queries across all the rounds of this discard algorithm, where C \u20322 is constant in n.\nConditional on succeeding thus far, we can now create a padded k-set from the max-ineligible alternatives S\u2217max that were never selected during our discard algorithm, forming a binary comparator for any u and v from a k-set of the form {u, v}\u222aS\u22122max for any (k\u22122)-set S\u22122max \u2282 S\u2217max. The max alternative will be selected with probability \u03c0k, and the second most maximal alternative will be selected with probability \u03c0k\u22121. Our goal is to hand this comparator off to the Feige et al. comparison-based query algorithm [10], but we note that the Feige et al. algorithm requires a binary comparator that returns each element with probabilities {p, 1\u2212p} for some p > 1/2. Our comparator, meanwhile, will \u201cfail\u201d with constant probability \u2206 = 1 \u2212 \u03c0k \u2212 \u03c0k\u22121, in the sense that some alternative u\u2217 \u2208 S\u22122max will be selected that is not one of the elements u or v being subjected to the noisy binary comparison. If one of \u03c0k or \u03c0k\u22121 exceed 1/2 then we can apply the Feige et al. procedure as-is, as we can simply bundle any choice u\u2217 \u2208 S\u22122max as a choice of the less frequent of the two\npositions, producing the necessary binary comparator. If the selection probabilities \u03c0k and \u03c0k\u22121 are both less than 1/2, we can modify our comparator as follows.\nFor each step s that the Feige et al. algorithm performs a query, we repeat the query until we first see one of k or k \u2212 1. This means that the outcome space is not {k, k \u2212 1, fail}, but just {k, k \u2212 1}, one of which will have a fixed probability p > 1/2. Let Cs be a random variable that takes a value equal to the number of queries needed before first seeing k or k \u2212 1. Seeing as Cs is geometrically distributed, we have E[Cs] = 1/\u2206, the inverse of the failure probability. But now the total number of queries being run throughout the Feige et al. algorithm is \u2211Dn logn s=1 Cs, for a constant D large enough to control the Feige\net al. error to at most /5, and so in expectation it takes E[ \u2211Dn logn\ns=1 Cs] = (1/\u2206)Dn log n queries. By Markov\u2019s inequality the probability that the number of queries exceeds its expectation by a multiplicative factor of 5/ is Pr( \u2211Dn logn s=1 Cs \u2265 5D \u2206n log n) \u2264 /5.\nThis modification of Feige et al. will then still only takeC3n log n queries, forC3 = 5D/( \u2206), to succeed in sorting the max-eligible alternatives, with two possible bad events: the failure of the Feige et al. algorithm, with probability at most /5, and the number of queries exceeding the query budget, with probability at most /5. This latter event is only a concern when both \u03c0k and \u03c0k\u22121 are less than 1/2.\nIf we only needed to recover the choices of a max-selector, we would be done. In order to recover all queries for any positions `, however, we also need to know the order of the max-ineligible alternatives. To recover such choices, we switch our focus from the max-selector subpopulation to the min-selector subpopulation. As a final step we repeat the above procedure using an anchoring set S\u2217min that contains the k \u2212 2 maximum alternatives of the ordering we just recovered. We then run the Feige et al. procedure to order the alternatives in Uscrap = un\u2212k\u22122\u222aS\u2217max. Here un\u2212k\u22122 is the last element of the order recovered above, included to orient the sort order with regard to the overall sorted order, and S\u2217max was the set of max-ineligible alternatives that we are trying to order. Since |Uscrap| = k \u2212 1, we can drive this error probability below 1 \u2212 /5 with a large constant C4 that does not depend on n, thereby also recovering the order over the max-ineligible alternatives.\nWe have outlined five separate bad events that would make the algorithm fail. By taking the Union Bound over the five bad event probabilities, each controlled to have an error \u2264 /5, the overall algorithm performs four sets of queries (where there are two bad events associated with the third set of queries) and succeeds with probability at least 1\u2212 after C1 +C \u20322n log n+C3n log n+C4 queries, asymptotically O(n log n) as specified."}, {"heading": "4 Passive query complexity", "text": "We now shift our attention to passive query algorithms, algorithms that pose all their queries at once without the sequential benefit of previous responses. This framework is motivated by a common scenario in the study of large datasets of recorded choices from online marketplaces, ranking algorithms, and recommendation systems. These systems are teeming with choices being made over sets of alternatives, but often one cannot adaptively guide the queries towards our goal of recovering choice functions. What if we simply had a large corpus of choices by an individual over random subsets? Could we still recover the choice function? If so, how few queries would we need?\nIn this section we analyze a model for this process, in which we are not able to select queries but instead have to watch them passively. We focus on the output of a single f ; handling population mixtures in a passive query model is an interesting open question.\nModel of passive choice streams. We analyze the following process for generating and analyzing a stream of random queries that arrive over time. For each possible k-set S, we define a Poisson process of rate \u03b1: we draw a length of time t from an exponentially distributed random variable of rate \u03b1 (with density \u03b1e\u2212\u03b1x), and after this time t elapses, we put the set S into the stream. We iterate this, repeatedly drawing a random length of time t and putting S into the stream when t is reached.\nWe run the process for each k-set S simultaneously, creating a merged stream of k-sets as they are selected and put into the stream. Now, suppose we observe the merged stream over the interval [0, T ]. For any fixed k-set S, the probability that it appears in the interval is 1 \u2212 e\u2212\u03b1T , a quantity that we\u2019ll refer to as pT . An algorithm is provided with the value f(S) for each k-set that appears in the stream during [0, T ]. After seeing these values it must correctly report the value of f(S) for almost every k-set. We want an algorithm that can achieve this goal while keeping T as small as possible.\nOur streaming model is arguably the simplest plausible model for generating collections of (choice set, choice) decisions that are independently drawn with replacement. Other seemingly simple models that query for a random set of k-sets without replacement exhibit an implausible slight dependence between decisions in the decision collection.\nA recovery algorithm for passive streams. Our main result for this streaming model of passive queries is that for 2 \u2264 ` \u2264 k \u2212 1 there is an algorithm able to provide the desired performance guarantee for a choice of T with pT \u2192 0, which is to say that the probability of seeing any fixed k-set S even once goes to zero. Equivalently, this result implies that we can recover f(S) for almost every k-set after only seeing a tiny fraction of all possible k-sets. Theorem 4.1. Let f be a choice function over k-sets in a universe of n alternatives that selects the `th ordered position, with k \u2265 3 and 2 \u2264 ` \u2264 k \u2212 1 known. For every \u03b5 > 0 and \u03b4 > 0, there is a constant \u03be > 0 and an algorithm that takes the stream over the interval [0, T ] with pT = \u03be log n log logn/n and with probability at least 1\u2212 \u03b4 will recover the correct value of f(S) for at least a 1\u2212 \u03b5 fraction of all k-sets.\nProof. The algorithm operates in two phases similarly to the algorithm for active queries, in that it first identifies the set S\u2217 of k\u22121 ineligible alternatives that cannot be selected in any query. Then it selects k\u22122 of these arbitrarily to form a set S\u22122 and analyzes sets of the form {ui, uj} \u222a S\u22122 to decide the relative ordering of pairs (ui, uj). The key difference from the case of active queries is that the algorithm needs to operate using only the k-sets that were handed to it initially at random, so it can\u2019t steer the queries to seek out the set S\u2217, or run an adaptive sorting algorithm on arbitrary pairs (ui, uj).\nBecause the algorithm has two phases, we split the time interval [0, T ] into two intervals [0, T1] and (T1, T1+ T2]. Note that for a given k-set S, the Union Bound implies that pT1 + pT2 is an upper bound on the probability seeing S during [0, T ], and hence pT1 + pT2 \u2265 pT . Given a time interval [0, T ] for generating the stream, for a sufficiently large constant b to be specified below, we choose T so that pT \u2265 b log n/n + b log n log log n/n. We split [0, T ] into [0, T1] and (T1, T1 + T2] so that pT1 \u2265 b log n/n and pT2 \u2265 b log n log logn/n. Let D[a, b] denote the k-sets observed in the stream during the interval [a, b]. We will use D1 = D[0, T1] for the first phase, and D2 = D(T1, T1 + T2] for the second phase. Note that the contents of D1 and D2 are independent of each other, an important facet of the streaming model.\nPhase 1: Finding ineligible elements. In the first phase, the algorithm will look at D1 and identify the set S\u2217\u2217 of elements v \u2208 U for which v is not the output f(S) for any S \u2208 D1. Clearly S\u2217 \u2286 S\u2217\u2217, since no element of S\u2217 can be the answer to any f(S). Now fix v 6\u2208 S\u2217; what is the probability that it belongs to S\u2217\u2217?\nEvery eligible alternative v will be the output f(S) for some S \u2208 D1 as long as there\u2019s a k-set in D1 containing exactly `\u22121 elements above v and exactly k\u2212` element below v. Note that since 2 \u2264 ` \u2264 k\u22121, this means that there are a non-zero number of elements above and below v in this case. Let\u2019s call such a query a \u201crevealing query\u201d for v. Out of ( n k ) possible queries there are at least n\u22122 such revealing queries for v, since in the worst case there\u2019s only one way to choose k\u2212 2 alternatives above and n\u2212 2 ways to choose one alternatives below, or vice versa. Each revealing query for v is chosen with probability p \u2265 b log n/n. The probability that none of these n\u2212 2 revealing queries is chosen is then\n(1\u2212 p)n\u22122 \u2264 (1\u2212 b(log n)/n)n\u22122 \u2264 (1\u2212 b(log n)/n)n/2 \u2264 e\u2212(b/2) logn = n\u2212b/2.\nNow taking the Union Bound over all n\u2212k eligible alternatives v says that the probability there exists a v for which no revealing query is chosen is bounded by n \u00b7n\u2212b/2 \u2264 n\u2212(b/2)+1. Let E1 denote the \u201cbad event\u201d that S\u2217 6= S\u2217\u2217, and let \u03b40 \u2264 \u03b4/4 be a constant. By choosing b large enough, we have Pr [E1] \u2264 n\u2212(b/2)+1 \u2264 \u03b40. Phase 2: Simulating pairwise comparisons. In phase 2 we now use k \u2212 2 of the ineligible elements as anchors for performing pairwise comparisons. Recall that because we have split the stream over two disjoint time intervals, the algorithm\u2019s behavior now is independent of its success or failure at finding S\u2217 in phase 1, so in our analysis we\u2019ll assume it has succeeded in finding S\u2217.\nFor this phase, we use p to denote pT2 , where T2 was chosen such that p \u2265 b log n log logn/n. To begin, we fix an arbitrary set S\u22122 of k \u2212 2 elements from S\u2217, and let U \u2032 = U \\ S\u22122. Let G be the undirected graph on U \u2032 in which (ui, uj) is an edge if and only if {ui, uj} \u222a S\u22122 is a k-set in D1. Note that G is a uniform sample from the distribution Gn\u2212k+2,p, the Erdo\u030bs-Re\u0301nyi random graph distribution. By a theorem of Alon and Azar [2], it follows that if we compare the pairs of elements (ui, uj) defined by the edges of G, then with probability at least 1\u2212 \u03b41 we will be able to infer the relative order of at least a 1\u2212 \u03b3 fraction of pairs of elements of U \u2032, where \u03b3 and \u03b41 depend on b. The remaining \u03b3 fraction of pairs are simply left with their choice uninferred. Let E2 denote the \u201cbad event\u201d that the Alon-Azar algorithm does not succeed, with Pr [E2] \u2264 \u03b41. We choose b large enough that \u03b3 \u2264 \u03b5/2k2 and \u03b41 \u2264 \u03b4/4. Let H be a directed acyclic graph on U \u2032 in which ui points to uj if the Alon-Azar procedure has inferred that ui is ranked ahead of uj , and there is no edge between ui and uj otherwise.\nNow we use the following procedure to answer queries of the form f(S). If S 6\u2286 U \u2032 then S contains an ineligible element and we answer arbitrarily. Otherwise, if S \u2286 U \u2032, we look at all the pairwise comparisons of elements in S according to H . If these pairwise comparisons form a complete acyclic digraph then we choose the `th element in order; otherwise we answer arbitrarily.\nFor how many k-sets do we get the correct answer? Let P denote the set of node pairs in U \u2032\u00d7U \u2032 for which there is no edge in H . If E2 does not happen, then |P | \u2264 \u03b3\n( n\u2212k+2\n2\n) . The number of k-sets that involve a pair\nfrom P is thus at most \u2211 (ui,uj)\u2208P ( n\u2212 k k \u2212 2 ) \u2264 \u03b3 ( n\u2212 k + 2 2 )( n\u2212 k k \u2212 2 ) \u2264 \u03b3k2 ( n k ) .\nMeanwhile, the number of k-sets that involve an element outside U \u2032 is at most\n(k \u2212 2) ( n\u2212 1 k \u2212 1 ) \u2264 (k \u2212 2) ( k n )( n k ) \u2264 \u03b3 ( n k ) ,\nprovided that n is large enough relative to k that k(k \u2212 2)/n \u2264 \u03b3. But if a k-set S satisfies S \u2286 U \u2032, and S \u00d7 S contains no pair of P , then all pairwise comparisons inside S have been determined and thus we can report the correct value of f(S). Hence if E2 does not occur, we report the correct value on all but at most \u03b3(1 + k2) ( n k ) \u2264 \u03b5 ( n k ) k-sets, as desired.\nThe full algorithm and its analysis. Now let\u2019s consider the full algorithm. It starts by identifying a set S\u2217\u2217 of ineligible elements in phase 1. If S\u2217\u2217 does not have k \u2212 1 elements, it terminates with no answer. Otherwise, it runs the second phase on the assumption that S\u2217 = S\u2217\u2217.\nIf the event E1 \u222a E2 does not occur, then S\u2217 = S\u2217\u2217 and the output of phase 2 satisfies the performance guarantee of Theorem 4.1. With probability at least 1 \u2212 (\u03b41 + \u03b42) \u2265 1 \u2212 \u03b4/2, this union E1 \u222a E2 does not occur, in which case the full algorithm satisfies the performance guarantee.\nThe above proof holds for all values of ` except for the two extremes of ` = 1 and ` = k, as stated. This distinction is intriguing since the two extreme values are the ones that don\u2019t exhibit choice-set effects: the individual making decisions is then selecting elements according to a fixed total ordering of U . The challenge with ` = 1 and ` = k is actually that the notion of \u201cineligible\u201d elements becomes more subtle: there are elements whose relative position in the embedding can be resolved, but only with a very large number of passive queries. Finding the right performance guarantee for this case is an interesting open question."}, {"heading": "5 Distance comparison", "text": "All of our results thus far focus on recovering choice functions that are comparison-based functions over ordered alternatives. As noted in the introduction, this class of functions is surprisingly rich, with the ability to capture compromise effects in choices along any one-dimensional frontier, a choice-set effect that cannot be exhibited by functions that merely maximize over an ordering. The preceding sections develop a theory of inference for such comparison-based functions.\nIn this section we shift our focus to recovering choice functions defined by a different structural relationship, namely distance comparisons in a metric embedding. A choice function f on k-sets is distance-comparisonbased if the value f(S) can be computed purely using comparisons on the pairwise distances in a embedding h with metric d(\u00b7, \u00b7), i.e. based on comparisons on the set of pairwise distances {d(h(ui), h(uj)) : ui, uj \u2208 S}. Distance comparisons form the basis of an additional important family of choice scenarios, namely choice queries answered with the \u201cmost medial\u201d or the \u201cmost distinctive\u201d alternative in a set, and in particular they model an important choice-set effect called similarity aversion [28]: given a choice between two dissimilar alternatives A and B and one of two alternatives A\u2032 and B\u2032 that are similar to A and B respectively, similarity aversion arises when f({A,B,B\u2032}) = A but f({A,B,A\u2032}) = B. Essentially: given two similar alternatives and one dissimilar alternative, the dissimilar option is chosen.\nIt is important to note that similarity aversion cannot be modeled by a choice function that is strictly comparison-based: such a function can only evaluate ordinal comparisons, and when considering two elements A and B embedded in one dimension, a comparison-based function cannot resolve whether a third element A\u2032 positioned between A and B is closer to A or closer to B.\nThe remainder of the section has the following format. First we show how similarity aversion can be modeled using distance-comparison-based choice functions. Next, we present two observations that suggest some of the difficulties that must be overcome to learn such functions, a seemingly more difficult inference context than comparison-based functions. We then connect our observations to a broader literature on learning metric embeddings, albeit without resolving our inference questions.\nSimilarity aversion from distance comparisons. We now describe two choice functions of principle importance, and their capacity for being formulated as distance-comparison-based functions: the median choice function and the outlier choice function. We restrict our definitions to choices over k-sets where k is odd. The outlier choice function for triplets (k = 3) will serve as our model of similarity aversion.\nThe median choice function selects the element of a k-set S in Rm with metric d(\u00b7, \u00b7) that minimizes the sum of distances to all other elements. In m = 1 dimension this choice is the traditional median element, and it can be selected according to the following distance-comparison procedure: repeatedly find the pair of alternatives x, y \u2208 S that are furthest apart and remove them from S. Since k is odd, after (k \u2212 1)/2 rounds there will be a single remaining element; return that element as the choice. Thus we see that the median choice function can be formulated as both a comparison-based function\u2014in the earlier sections, q(k\u22121)/2,k\u2014or as a distance-comparison-based function.\nFor m > 1 dimensions and sets of size k = 3 it is easy to verify that the above distance-comparison procedure will still return the element that minimizes the sum of distances. For m > 1 and k > 3, however, it is an open question whether a distance-comparison-based function can return the element that minimizes the sum of distances. Observe that this question amounts to solving a version of the Fermat-Weber problem [32] with a very restricted functional toolkit, and may be quite difficult. We are not aware of any prior work on the capabilities of distance-comparison-based functions. Here we are principally interested in the special case of k = 3, allowing us to skirt this difficulty.\nBuilding on the median choice function, the outlier choice function selects the element of a k-set S that is farthest (in Rm using d(\u00b7, \u00b7)) from the median element (the median element being the element that minimizes the sum of distances to all elements). The outlier choice function can clearly be written as a distancecomparison-based function whenever the median choice function can. For k = 3 elements it is precisely a model of similarity aversion on triplets.\nAs a general comment on the capabilities of distance-comparison-based functions, it is clear that they can make choices that ordinary comparison-based functions cannot, but it is important to highlight that the reverse is also true: the median choice function is the only position-selection function (in one dimension) that can be formulated as a distance-comparison-based function. A distance-comparison-based function cannot select a maximal or minimal element (or any off-center position), since it has no way of orienting a choice with regard to the direction of \u201cmore\u201d (in any dimension). The sets of comparison-based functions and distance-comparison-based functions are therefore not neatly nested.\nDistance comparison for triplets. In the particular case of k = 3, where choices are made over triplets S = {u, v, w}, we can think of a choice for an alternative u \u2208 S implicitly as a choice for the distance\nd(v, w) between the two remaining alternatives in S. For a triplet {u, v, w}, the effective comparison-based query is over the set of distances {d(u, v), d(u,w), d(v, w)}, where a choice of a pairwise distance maps to a choice of the complementary alternative. See Figure 3 for an illustration. There are N = ( n 2 ) distances, and so it may seem as though it would be possible to learn the ordering in O(N logN) queries in the active query framework, with similar results carried over for other query frameworks. Reality is more complicated.\nWe now give two observations on the difficulties of carrying over comparison-based results to distance comparisons. The first observation asks: when are there enough queries to learn the relative ordering of all the distances? The second observation pertains to the restricted nature of the distance triplets that can be queried.\nFirst, notice that each query for a 3-set furnishes 2 inequalities between distances. There are ( n 3 ) triplet\nqueries for n elements, which means that querying for every possible 3-set would produce 2 ( n 3 ) inequalities.\nMeanwhile there are ( n 2 ) pairwise distances, meaning that there are ( n 2 ) ! possible permutations of the distances, only one of which is the sorted order. A sufficient condition for inferring all choices is to know the sorted order of all the distances. Focusing on the outlier choice function (selecting the minimum distance that effectively returns the outlier element) in one does not need to know the relative order of the two largest distances. This tells us that it would take log2( ( n 2 ) !)\u2212 1 bisections of the set of permutations to identify the sorted order of all but the two largest distances.\nWe observe that 2 ( n 3 ) < log2( ( n 2 ) !) \u2212 1 for n \u2264 5, which tells us that for n \u2264 5 elements we will learn the choice for every query before we can possibly know the sorted order of the relevant pairwise distances. This tells us that a generic procedure that seeks to learn all the pairwise distances will not always succeed.\nWhile our earlier query complexity results focus on asymptotic complexity, the correctness of these algorithms holds for all n. The limiting observation presented here is specific to distance comparisons and does not apply to ordinary comparisons: in that context there are only n! permutations rather than ( n 2 ) !.\nSecond, observe that we cannot query for general triplets of distances {d(u, v), d(w, x), d(y, z)}, but are in fact restricted to queries in the special case of w = u, y = x, and z = v. It is unclear for what n it is possible to learn the relative ordering of all the distances from such restricted distance comparisons. While both of these initial observations are discouraging, an efficient inference algorithm for distance-comparisonbased functions would contribute a useful tool for learning in the presence of similarity aversion and other choice-set effects, and we deem it important to present distance-comparison-based functions as significant objects of study.\nAs an additional open direction for future work, we briefly mention the choice-set effect known as the decoy effect (or asymmetric dominance). The decoy effect describes when the presence of a similar but inferior alternative increases the desirability of an option. The difference between similarity aversion and the decoy effect is that the former hinges on similarity while the latter incorporates both similarity and inferiority. As such, modeling the decoy effect requires a composition of both comparison and distance-comparison. We leave the study of this alluring general class of choice functions, which contains comparison-based and distance-comparison-based choice functions as special cases, as future work.\nRelationship to metric embeddings. Beyond providing a model for similarity avoidance in the behavioral sciences, the ability to recover distance-comparison-based choice functions also speaks to a broad literature on learning metric embeddings of data from distance-comparison-based queries [20]. In that literature, the stochastic triplet embedding technique [30] has recently been introduced as a way to embed a generic dataset of elements through answers to choices of the form \u201cIs A more similar to B or to C?\u201d. This question is effectively a comparison-based query requesting a choice from the set {d(A,B), d(A,C)}. As an extension of that work, the crowd median algorithm [13] tries to learn embeddings from triplet queries with the request\n\u201cOut of three shown items pick one that appears to be different from the two others.\u201d The crowd median algorithm is employing precisely the outlier choice function for k = 3 described in this section.\nKnown work on the crowd median algorithm, however, leaves open the question of what embeddings it can learn. We observe that outside the behavioral modeling that drove our work, a slight modification of the crowd median algorithm to align with our results gives a method that can in fact learn embeddings efficiently. We define the generalized crowd median algorithm, which instead asks questions of the form \u201cof these k pairs, which pair is least similar?\u201d. Under this more flexible query framework, the ordering on the distance could then be inferred by the algorithms developed in this work: for n elements, it would then be possible in an active query framework to learn the ordering of the N = ( n 2 ) distances in O(N logN) queries, for large n.\nOur treatment of distance-comparison-based queries also speaks to a broad line or research in psychology on pair selection queries used to infer cognitive embeddings [26, 14, 27]. In these settings, sets of k elements (where k is often large) are presented to subjects with the question \u201cwhich two elements are most similar?\u201d. Such queries generalize the crowd-median algorithm. Lastly, very recent work has developed learning results for binary clustering (a simple form of embedding) using so-called triangle queries of the form \u201cwhich of these birds belong to the same species?\u201d [31], with possible relevance to learning from distancecomparison-based choices."}, {"heading": "6 Discussion", "text": "The prevalence of choice-set effects in human decision-making highlights a need for a principled inference model that can support learning such effects. In this work we\u2019ve proposed a framework aimed at helping with such an integration, focussing on comparison-based choices. A natural line of inquiry extending from choice-set effects quickly leads to rich issues related to the theory of sorting, including passive sorting (\u201csorting in one round\u201d) and sorting with noisy information.\nThere are clearly many directions for further work. We begin by mentioning a very concrete problem: while we studied passive queries and population mixtures, we did not attempt to combine these two aspects to infer a mixed collection of choice functions from a stream of passively observed queries. A challenge in trying to achieve such a combination can be seen in the technical ingredients that would need to be brought together; the mechanics of the Feige et al. procedure [10] and Alon-Azar procedure [2] are not obviously compatible. Even the basic question of binary sorting with passive queries in the presence of noisy information appears to be fairly open.\nMany questions remain regarding how to capture relevant population-level heterogeneities in large-scale choice corpora. Our framework for comparisons generally assumed a universally agreed upon ordered embedding, and it is very reasonable to suggest generalizations to a dispersed distribution over possible embeddings. A long line of work has brought distributions over rankings into the literature on learning preferences [18, 11, 6], including through comparisons [16], and it would be natural to explore analogous distributional modeling for the embedding that underlies comparison-based choice functions. Analyzing mixtures of comparison-based choice functions as a model of probabilistic discrete choice [3] is another closely related open research direction.\nOne can also consider generalizing the embedding that defines the structure of the alternatives and the choice sets. For example, we could think about the alternatives as being embedded not just in one dimension (or a one-dimensional Pareto frontier) but in multiple dimensions, and an individual could execute a sequence of comparison-based rules to select an item from a choice set. Such a generalization could provide a way to\nincorporate a number of other well-documented choice-set effects [28], and could form intriguing potential connections to the elimination by aspects [29] model of discrete choice.\nWe find it encouraging that in the face of choice behavior much more complex than ordered preferences, we are able to fully match most of the known query complexity results from the theory of sorting from binary comparisons. Continued work to develop a more complete theory of inference for other increasingly rich classes of choice functions has the potential to lay a foundation for a much needed unified theory for learning choice-set effects."}], "references": [{"title": "Learning to Rank (NIPS workshop)", "author": ["Shivani Agarwal", "Corinna Cortes", "Ralf Herbrich"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Sorting, approximate sorting, and searching in rounds", "author": ["Noga Alon", "Yossi Azar"], "venue": "SIAM J Discrete Math,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Random utility models in marketing research: a survey", "author": ["George Baltas", "Peter Doyle"], "venue": "J Bus Res,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "On the relevance of irrelevant alternatives", "author": ["Austin Benson", "Ravi Kumar", "Andrew Tomkins"], "venue": "In WWW,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Sorting in one round", "author": ["B\u00e9la Bollob\u00e1s", "Moshe Rosenfeld"], "venue": "Israel Journal of Mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1981}, {"title": "Sorting from noisy information", "author": ["Mark Braverman", "Elchanan Mossel"], "venue": "arXiv preprint arXiv:0910.1191,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Modeling intransitivity in matchup and comparison data", "author": ["Shuo Chen", "Thorsten Joachims"], "venue": "In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Predicting matchups and preferences in context", "author": ["Shuo Chen", "Thorsten Joachims"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "The money pump as a measure of revealed preference violations", "author": ["Federico Echenique", "Sangmok Lee", "Matthew Shum"], "venue": "Journal of Political Economy,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Computing with noisy information", "author": ["Uriel Feige", "Prabhakar Raghavan", "David Peleg", "Eli Upfal"], "venue": "SIAM J. Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Distance based ranking models", "author": ["Michael A Fligner", "Joseph S Verducci"], "venue": "J Roy Statist Soc Ser B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1986}, {"title": "Thirty years of conjoint analysis: Reflections and prospects", "author": ["Paul E. Green", "Abba M. Krieger", "Yoram Wind"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "The crowd-median algorithm", "author": ["Hannes Heikinheimo", "Antti Ukkonen"], "venue": "In HCOMP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Is it culture or is it language? examination of language effects in cross-cultural research on categorization", "author": ["Li-Jun Ji", "Zhiyong Zhang", "Richard E Nisbett"], "venue": "Journal of personality and social psychology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Contextual inference in markets: On the informational content of product lines", "author": ["Emir Kamenica"], "venue": "American Economic Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Learning mallows models with pairwise preferences", "author": ["Tyler Lu", "Craig Boutilier"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "The choice axiom after twenty years", "author": ["R Duncan Luce"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1977}, {"title": "Non-null ranking models", "author": ["Colin L Mallows"], "venue": "i. Biometrika,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1957}, {"title": "Pairwise choice markov chains", "author": ["Stephen Ragain", "Johan Ugander"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Learning a distance metric from relative comparisons", "author": ["Matthew Schultz", "Thorsten Joachims"], "venue": "NIPS, page", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Necessary and sufficient conditions for rational choice under majority decision", "author": ["Amartya Sen", "Prasanta K Pattanaik"], "venue": "Journal of Economic Theory,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1969}, {"title": "Predicting consumer behavior in commerce search", "author": ["Or Sheffet", "Nina Mishra", "Samuel Ieong"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Choice based on reasons: The case of attraction and compromise effects", "author": ["Itamar Simonson"], "venue": "Journal of Consumer Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1989}, {"title": "Choice in context: Tradeoff contrast and extremeness aversion", "author": ["Itamar Simonson", "Amos Tversky"], "venue": "Journal of Marketing Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1992}, {"title": "Conceptual preference for thematic or taxonomic relations: A nonmonotonic age trend from preschool to old age", "author": ["Sandra S Smiley", "Ann L Brown"], "venue": "Journal of Experimental Child Psychology,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1979}, {"title": "Liberals think more analytically (more \u201cweird\u201d) than conservatives", "author": ["Thomas Talhelm", "Jonathan Haidt", "Shigehiro Oishi", "Xuemin Zhang", "Felicity F Miao", "Shimin Chen"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Not just for consumers context effects are fundamental to decision making", "author": ["Jennifer S Trueblood", "Scott D Brown", "Andrew Heathcote", "Jerome R Busemeyer"], "venue": "Psychological science,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Elimination by aspects: A theory of choice", "author": ["Amos Tversky"], "venue": "Psychological Review,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1972}, {"title": "Stochastic triplet embedding", "author": ["Laurens Van Der Maaten", "Kilian Weinberger"], "venue": "In MLSP, pages", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Crowdsourced clustering: Querying edges vs triangles", "author": ["Ramya Korlakai Vinayak", "Babak Hassibi"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "The Weber problem: History and perspectives", "author": ["George O Wesolowsky"], "venue": "Computers & Operations Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}, {"title": "Estimating ad group performance in sponsored search", "author": ["Dawei Yin", "Bin Cao", "Jian-Tao Sun", "Brian D. Davison"], "venue": "In WSDM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The ubiquity of this framework has led to an active line of research on the problem of learning ranking functions [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 23, "context": "A long line of work in behavioral science [25, 22, 15, 9, 28], however, indicates that people\u2019s empirical choices tend to deviate in significant ways from this picture.", "startOffset": 42, "endOffset": 61}, {"referenceID": 14, "context": "A long line of work in behavioral science [25, 22, 15, 9, 28], however, indicates that people\u2019s empirical choices tend to deviate in significant ways from this picture.", "startOffset": 42, "endOffset": 61}, {"referenceID": 8, "context": "A long line of work in behavioral science [25, 22, 15, 9, 28], however, indicates that people\u2019s empirical choices tend to deviate in significant ways from this picture.", "startOffset": 42, "endOffset": 61}, {"referenceID": 26, "context": "A long line of work in behavioral science [25, 22, 15, 9, 28], however, indicates that people\u2019s empirical choices tend to deviate in significant ways from this picture.", "startOffset": 42, "endOffset": 61}, {"referenceID": 21, "context": "Notable examples of such choice-set effects include the compromise effect, similarity aversion, and the decoy effect (also known as asymmetric dominance), and have been observed in diverse contexts both offline and online; recent work in machine learning has highlighted the prevalence of choice-set effects [23, 4, 33, 19] and non-transitive comparisons [7, 8], particularly in online search, online ads, and online games.", "startOffset": 308, "endOffset": 323}, {"referenceID": 3, "context": "Notable examples of such choice-set effects include the compromise effect, similarity aversion, and the decoy effect (also known as asymmetric dominance), and have been observed in diverse contexts both offline and online; recent work in machine learning has highlighted the prevalence of choice-set effects [23, 4, 33, 19] and non-transitive comparisons [7, 8], particularly in online search, online ads, and online games.", "startOffset": 308, "endOffset": 323}, {"referenceID": 31, "context": "Notable examples of such choice-set effects include the compromise effect, similarity aversion, and the decoy effect (also known as asymmetric dominance), and have been observed in diverse contexts both offline and online; recent work in machine learning has highlighted the prevalence of choice-set effects [23, 4, 33, 19] and non-transitive comparisons [7, 8], particularly in online search, online ads, and online games.", "startOffset": 308, "endOffset": 323}, {"referenceID": 18, "context": "Notable examples of such choice-set effects include the compromise effect, similarity aversion, and the decoy effect (also known as asymmetric dominance), and have been observed in diverse contexts both offline and online; recent work in machine learning has highlighted the prevalence of choice-set effects [23, 4, 33, 19] and non-transitive comparisons [7, 8], particularly in online search, online ads, and online games.", "startOffset": 308, "endOffset": 323}, {"referenceID": 6, "context": "Notable examples of such choice-set effects include the compromise effect, similarity aversion, and the decoy effect (also known as asymmetric dominance), and have been observed in diverse contexts both offline and online; recent work in machine learning has highlighted the prevalence of choice-set effects [23, 4, 33, 19] and non-transitive comparisons [7, 8], particularly in online search, online ads, and online games.", "startOffset": 355, "endOffset": 361}, {"referenceID": 7, "context": "Notable examples of such choice-set effects include the compromise effect, similarity aversion, and the decoy effect (also known as asymmetric dominance), and have been observed in diverse contexts both offline and online; recent work in machine learning has highlighted the prevalence of choice-set effects [23, 4, 33, 19] and non-transitive comparisons [7, 8], particularly in online search, online ads, and online games.", "startOffset": 355, "endOffset": 361}, {"referenceID": 22, "context": "A canonical example of a choice-set effect is the so-called compromise effect [24]: faced with the choice between a mediocre option for $10, a good option for $15, and an excellent option for $20, people will tend to choose the good option for $15; but faced instead with a choice between the same good option for $15, the excellent option for $20, and an outstanding option for $25, people will be more likely to choose the excellent option for $20.", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "If such choices can occur then a choice between ui and uj depends on whether they are presented in the context of S or T ; one can view such a contextual effect as a violation of the independence of irrelevant alternatives (\u201cIIA\u201d) [21].", "startOffset": 231, "endOffset": 235}, {"referenceID": 2, "context": "We can contrast such probabilistic choices (those expressible as mixtures), with random utility models (RUMs) [3, 17]: they lack some of the flexibility of RUMs; but discrete choice models such as RUMs on the other hand typically entail other restrictions, including assuming the independence of irrelevant alternatives.", "startOffset": 110, "endOffset": 117}, {"referenceID": 16, "context": "We can contrast such probabilistic choices (those expressible as mixtures), with random utility models (RUMs) [3, 17]: they lack some of the flexibility of RUMs; but discrete choice models such as RUMs on the other hand typically entail other restrictions, including assuming the independence of irrelevant alternatives.", "startOffset": 110, "endOffset": 117}, {"referenceID": 9, "context": "The algorithm here uses random sampling ideas combined with techniques for sorting using noisy comparators [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Our analysis here builds on a sequence of combinatorial results on sorting in one round, culminating in an asymptotically tight analysis of that problem by Alon and Azar [5, 2].", "startOffset": 170, "endOffset": 176}, {"referenceID": 1, "context": "Our analysis here builds on a sequence of combinatorial results on sorting in one round, culminating in an asymptotically tight analysis of that problem by Alon and Azar [5, 2].", "startOffset": 170, "endOffset": 176}, {"referenceID": 2, "context": "of earlier work have made use of the ambient space in different ways, including methods such as conjoint analysis [3, 12].", "startOffset": 114, "endOffset": 121}, {"referenceID": 11, "context": "of earlier work have made use of the ambient space in different ways, including methods such as conjoint analysis [3, 12].", "startOffset": 114, "endOffset": 121}, {"referenceID": 28, "context": "This line of inquiry into metric embeddings also connects our results with recent research in the learning and crowdsourcing literature on learning stochastic triplet embeddings [30] and inferences using the crowd median algorithm [13].", "startOffset": 178, "endOffset": 182}, {"referenceID": 12, "context": "This line of inquiry into metric embeddings also connects our results with recent research in the learning and crowdsourcing literature on learning stochastic triplet embeddings [30] and inferences using the crowd median algorithm [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 9, "context": "In this case, results from the sorting literature contribute that as long as p is bounded away from 1/2, the order can be recovered in O(n log n) queries almost surely [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 9, "context": "By employing results for sorting under noisy comparisons [10], we obtain the order over the alternatives that are eligible to a max-selector.", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "comparison-based query algorithm [10], but we note that the Feige et al.", "startOffset": 33, "endOffset": 37}, {"referenceID": 1, "context": "By a theorem of Alon and Azar [2], it follows that if we compare the pairs of elements (ui, uj) defined by the edges of G, then with probability at least 1\u2212 \u03b41 we will be able to infer the relative order of at least a 1\u2212 \u03b3 fraction of pairs of elements of U \u2032, where \u03b3 and \u03b41 depend on b.", "startOffset": 30, "endOffset": 33}, {"referenceID": 26, "context": "Distance comparisons form the basis of an additional important family of choice scenarios, namely choice queries answered with the \u201cmost medial\u201d or the \u201cmost distinctive\u201d alternative in a set, and in particular they model an important choice-set effect called similarity aversion [28]: given a choice between two dissimilar alternatives A and B and one of two alternatives A\u2032 and B\u2032 that are similar to A and B respectively, similarity aversion arises when f({A,B,B\u2032}) = A but f({A,B,A\u2032}) = B.", "startOffset": 280, "endOffset": 284}, {"referenceID": 30, "context": "Observe that this question amounts to solving a version of the Fermat-Weber problem [32] with a very restricted functional toolkit, and may be quite difficult.", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "Beyond providing a model for similarity avoidance in the behavioral sciences, the ability to recover distance-comparison-based choice functions also speaks to a broad literature on learning metric embeddings of data from distance-comparison-based queries [20].", "startOffset": 255, "endOffset": 259}, {"referenceID": 28, "context": "In that literature, the stochastic triplet embedding technique [30] has recently been introduced as a way to embed a generic dataset of elements through answers to choices of the form \u201cIs A more similar to B or to C?\u201d.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "As an extension of that work, the crowd median algorithm [13] tries to learn embeddings from triplet queries with the request", "startOffset": 57, "endOffset": 61}, {"referenceID": 24, "context": "Our treatment of distance-comparison-based queries also speaks to a broad line or research in psychology on pair selection queries used to infer cognitive embeddings [26, 14, 27].", "startOffset": 166, "endOffset": 178}, {"referenceID": 13, "context": "Our treatment of distance-comparison-based queries also speaks to a broad line or research in psychology on pair selection queries used to infer cognitive embeddings [26, 14, 27].", "startOffset": 166, "endOffset": 178}, {"referenceID": 25, "context": "Our treatment of distance-comparison-based queries also speaks to a broad line or research in psychology on pair selection queries used to infer cognitive embeddings [26, 14, 27].", "startOffset": 166, "endOffset": 178}, {"referenceID": 29, "context": "Lastly, very recent work has developed learning results for binary clustering (a simple form of embedding) using so-called triangle queries of the form \u201cwhich of these birds belong to the same species?\u201d [31], with possible relevance to learning from distancecomparison-based choices.", "startOffset": 203, "endOffset": 207}, {"referenceID": 9, "context": "procedure [10] and Alon-Azar procedure [2] are not obviously compatible.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "procedure [10] and Alon-Azar procedure [2] are not obviously compatible.", "startOffset": 39, "endOffset": 42}, {"referenceID": 17, "context": "A long line of work has brought distributions over rankings into the literature on learning preferences [18, 11, 6], including through comparisons [16], and it would be natural to explore analogous distributional modeling for the embedding that underlies comparison-based choice functions.", "startOffset": 104, "endOffset": 115}, {"referenceID": 10, "context": "A long line of work has brought distributions over rankings into the literature on learning preferences [18, 11, 6], including through comparisons [16], and it would be natural to explore analogous distributional modeling for the embedding that underlies comparison-based choice functions.", "startOffset": 104, "endOffset": 115}, {"referenceID": 5, "context": "A long line of work has brought distributions over rankings into the literature on learning preferences [18, 11, 6], including through comparisons [16], and it would be natural to explore analogous distributional modeling for the embedding that underlies comparison-based choice functions.", "startOffset": 104, "endOffset": 115}, {"referenceID": 15, "context": "A long line of work has brought distributions over rankings into the literature on learning preferences [18, 11, 6], including through comparisons [16], and it would be natural to explore analogous distributional modeling for the embedding that underlies comparison-based choice functions.", "startOffset": 147, "endOffset": 151}, {"referenceID": 2, "context": "Analyzing mixtures of comparison-based choice functions as a model of probabilistic discrete choice [3] is another closely related open research direction.", "startOffset": 100, "endOffset": 103}, {"referenceID": 26, "context": "incorporate a number of other well-documented choice-set effects [28], and could form intriguing potential connections to the elimination by aspects [29] model of discrete choice.", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "incorporate a number of other well-documented choice-set effects [28], and could form intriguing potential connections to the elimination by aspects [29] model of discrete choice.", "startOffset": 149, "endOffset": 153}], "year": 2017, "abstractText": "A broad range of on-line behaviors are mediated by interfaces in which people make choices among sets of options. A rich and growing line of work in the behavioral sciences indicate that human choices follow not only from the utility of alternatives, but also from the choice set in which alternatives are presented. In this work we study comparison-based choice functions, a simple but surprisingly rich class of functions capable of exhibiting so-called choice-set effects. Motivated by the challenge of predicting complex choices, we study the query complexity of these functions in a variety of settings. We consider settings that allow for active queries or passive observation of a stream of queries, and give analyses both at the granularity of individuals or populations that might exhibit heterogeneous choice behavior. Our main result is that any comparison-based choice function in one dimension can be inferred as efficiently as a basic maximum or minimum choice function across many query contexts, suggesting that choice-set effects need not entail any fundamental algorithmic barriers to inference. We also introduce a class of choice functions we call distance-comparison-based functions, and briefly discuss the analysis of such functions. The framework we outline provides intriguing connections between human choice behavior and a range of questions in the theory of sorting.", "creator": "LaTeX with hyperref package"}}}