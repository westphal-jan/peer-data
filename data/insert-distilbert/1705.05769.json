{"id": "1705.05769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Multiobjective Programming for Type-2 Hierarchical Fuzzy Inference Trees", "abstract": "this paper proposes a design of hierarchical fuzzy inference tree ( hfit ). an hfit produces an optimum treelike structure, i. e., a natural hierarchical structure that accommodates simplicity by combining several low - dimensional fuzzy inference systems ( fiss ). such a natural hierarchical structure provides a high degree of approximation accuracy. the construction of hfit takes place in two phases. firstly, a generic nondominated sorting based multiobjective genetic programming ( mogp ) is applied to simply obtain a simple tree structure ( a low complexity genetic model ) with a high accuracy. secondly, the differential evolution algorithm is applied to optimize the obtained tree's parameters. in the derived tree, each node acquires a different input's combination, where the evolutionary process governs the input's combination. hence, hfit nodes are heterogeneous in nature, which leads to a high diversity among the constraints rules generated by the hfit. additionally, the hfit provides an automatic feature selection because it uses mogp for the tree's structural optimization that accepts inputs only relevant to the knowledge contained in data. the hfit was studied partly in the context of both type - 1 and type - 2 fiss, and its performance was evaluated through six key application problems. moreover, the proposed multiobjective hfit was compared both theoretically and empirically with recently proposed computational fiss methods from the literature, such as mcit2fis, tscit2fnn, sit2fnn, rit2fns - wb, et2fis, mrit2nfs, it2fnn - svr, etc. from the obtained results, it was found that the hfit provided one less complex and theoretically highly accurate models compared to how the models produced by the most of other methods. hence, the proposed hfit is an efficient and competitive alternative to the other fiss for function approximation and feature selection.", "histories": [["v1", "Tue, 16 May 2017 15:34:19 GMT  (1234kb,D)", "http://arxiv.org/abs/1705.05769v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["varun kumar ojha", "vaclav snasel", "ajith abraham"], "accepted": false, "id": "1705.05769"}, "pdf": {"name": "1705.05769.pdf", "metadata": {"source": "CRF", "title": "Multiobjective Programming for Type-2 Hierarchical Fuzzy Inference Trees", "authors": ["Varun Kumar Ojha", "Ajith Abraham"], "emails": ["ojha@arch.ethz.ch", "vaclav.snasel@vsb.cz", "ajith.abraham@ieee.org"], "sections": [{"heading": null, "text": "This paper proposes a design of hierarchical fuzzy inference tree (HFIT). An HFIT produces an optimum tree-like structure. Specifically, a natural hierarchical structure that accommodates simplicity by combining several low-dimensional fuzzy inference systems (FISs). Such a natural hierarchical structure provides a high degree of approximation accuracy. The construction of HFIT takes place in two phases. Firstly, a nondominated sorting based multiobjective genetic programming (MOGP) is applied to obtain a simple tree structure (low model\u2019s complexity) with a high accuracy. Secondly, the differential evolution algorithm is applied to optimize the obtained tree\u2019s parameters. In the obtained tree, each node has a different input\u2019s combination, where the evolutionary process governs the input\u2019s combination. Hence, HFIT nodes are heterogeneous in nature, which leads to a high diversity among the rules generated by the HFIT. Additionally, the HFIT provides an automatic feature selection because it uses MOGP for the tree\u2019s structural optimization that accept inputs only relevant to the knowledge contained in data. The HFIT was studied in the context of both type-1 and type-2 FISs, and its performance was evaluated through six application problems. Moreover, the proposed multiobjective HFIT was compared both theoretically and empirically with recently proposed FISs methods from the literature, such as McIT2FIS, TSCIT2FNN, SIT2FNN, RIT2FNS-WB, eT2FIS, MRIT2NFS, IT2FNN-SVR, etc. From the obtained results, it was found that the HFIT provided less complex and highly accurate models compared to the models produced by most of the other methods. Hence, the proposed HFIT is an efficient and competitive alternative to the other FISs for function approximation and feature selection.\nV K Ojha is with the Chair of Information Architecture, ETH Zurich, Zurich, Switzerland e-mail: ojha@arch.ethz.ch V Sna\u0301s\u030cel is with the Dept. of Computer Science, Technical University of Ostrava, Czech Republic, e-mail: vaclav.snasel@vsb.cz A Abraham is with Machine Intelligence Research Labs, Washington, USA, e-mail: ajith.abraham@ieee.org This work was supported by the IPROCOM Marie Curie initial training network, funded through the People Programme (Marie Curie Actions) of the European Union\u2019s Seventh Framework Programme FP7/2007-2013/ under REA Grant Agreement No. 316555.\nManuscript received Month xx, yyyy; revised Month xx, yyyy.\nar X\niv :1\n70 5.\n05 76\n9v 1\n[ cs\n.A I]\n1 6\nM ay\nIndex Terms\nHierarchical fuzzy inference system, multiobjective genetic programming, differential evolution,\napproximation, feature selection\nI. INTRODUCTION\nA fuzzy inference system (FIS)\u2014composed of a fuzzifier to fuzzify input information, an inference engine to infer information from a rule base (RB), and a defuzzifier to return crisp information\u2014solves a wide range of problems that are ambiguous, uncertain, inaccurate, and noisy. An RB of an FIS is a set of rules of the form IF-THEN, i.e., the antecedent and the consequent form. The Takagi\u2013Sugeno\u2013Kang (TSK) is a widely used FIS model [1]. It embraces the IF-THEN form, where the antecedent part consists of type-1 fuzzy sets (T1FS) and/or type-2 fuzzy sets (T2FS), and the consequent part consists of real values or a linear/nonlinear function.\nType-1 FIS (T1FIS) and type-2 FIS (T2FIS) differ when it comes to the representation of the antecedent part and the consequent part of a rule, and T1FS and T2FS differ in the definitions of their membership functions. Unlike the crisp output of a T1FS membership function (MF) [2], the output of a T2FS MF is fuzzy in nature [3]. Such nature of the T2FS MFs is advantageous in processing uncertain information more effectively than with T1FS MFs [4]. Hence, a T2FIS can overcome the inability of a T1FIS to fully handle or accommodate the linguistic and numerical uncertainties associated with a changing and dynamic environment [5].\nHowever, a T2FIS is computationally expensive because it has a larger number of parameters than a T1FIS, and it requires a type-reduction mechanism in its defuzzification part. The interval T2FIS (IT2FIS) reduces the computational cost by employing a simplified T2FS, known as interval T2FS (IT2FS) [4]. An IT2FS MF is bounded by a lower MF (LMF) and an upper MF (UMF), and the area between the LMF and UMF is called the footprint of uncertainty [4]. Then, a type-reducer reduces IT2FS to interval-valued T1FS. Subsequently, the output of IT2FIS is produced by averaging the intervals.\nThe construction and tuning of the rules are among the vital tasks in the optimization of an FIS, where the rule\u2019s construction is met by combining the fuzzy sets and the rule\u2019s tuning is met by adjusting the MF\u2019s parameters and the consequent part\u2019s parameters. Such a form of rule optimization is often achieved by mapping the rule\u2019s parameters onto a real-valued genetic vector, and it is known as the Michigan Approach [6]. Similarly, the construction/optimization of the RB is met by the genetic selection of the rules at the RB. Such a form of RB optimization\nis often achieved by mapping the rules onto a binary-valued genetic vector [7], and it is known as the Pittsburgh Approach [8].\nHowever, FIS optimization is not limited only to its mapping onto the genetic vector, but a structural/network-like implementation of FIS is often performed [9]. Additionally, TSK-based hierarchical self-organizing learning dynamics have also been proposed [10]. Moreover, several researchers have focused on the FIS and neural network (NN) integration and its parameter optimization using various learning methods including gradient-decent and the metaheuristic algorithms [11]\u2013[14]. The summaries of such optimization paradigms are described as follows:\nA self-constructing neural fuzzy inference network (SONFIN), proposed by Juang et al. [15], is a six layered network structure whose optimization begins with no rule and then rules are incrementally added during the learning process. SONFIN uses a clustering method to partition the input space that governs the number of rules extracted from the data, then the parameters (MF\u2019s arguments) of the determined SONFIN structure are tuned by the backpropagation algorithm. Later, in [16], SONFIN\u2019s concept was extended for the construction of T2FIS, where a self-evolving IT2FIS (SEIT2FNN) that implements a TSK-type FIS model was proposed, and the parameters of the evolved structure were tuned by using the Kalman-filtering algorithm. Additionally, a simplified type-reduction process for SEIT2FNN was proposed in [17]. Like SONFIN, in [18], a TSK-type FIS model, called a dynamic evolving neural-fuzzy inference system (DENFIS), was proposed, which evolved incrementally by choosing active rules from a set of rules and employed an evolving clustering method to partition the input space and the least-square estimator to optimize its parameters.\nTo overcome some limitations of the self-organizing fuzzy NN paradigm, Tung et al. [19] proposed a self-adaptive fuzzy inference network (SaFIN) that applied a categorical learning induced partitioning algorithm to eliminate two limitations: 1) the need for predefined numbers of fuzzy clusters and 2) the stability\u2013plasticity trade-off that addresses the difficulty in finding a balance between past knowledge and current knowledge during the learning process. SaFIN also employed a rule consistency checking mechanism to avoid inconsistent RB construction. Additionally, the Levenberg-Marquardt method was applied for RB\u2019s parameters tuning. In [20], to improve the efficiency of IT2FIS, a mutually recurrent interval type-2 neural fuzzy system (MRIT2NFS) was proposed which used weighted feedback loops in the antecedent parts of the formed rules and applied gradient-decent learning and a Kalman-filter algorithm to tune the recurrent weights and the rules\u2019 parameters, respectively. In [21], a self-evolving T2FIS model\nwas proposed that employed a compensatory operator in the type-2 inference mechanism and a variable-expansive Kalman-filter algorithm for parameter tuning.\nFurther, a simplified interval type-2 fuzzy NN with a simplified type-reduction process (SIT2FIS)\nwas proposed in [22], and a growing online self-learning IT2FIS that used the dynamics of a growing Gaussian mixture model was proposed in [23]. Recently, in [24], a meta-cognitive interval type-2 neuro FIS (McIT2FIS) was proposed, which employs a self-regulatory metacognitive system that extracts the knowledge contained in minimal samples by accepting or discarding data samples based on sample\u2019s contribution to knowledge. For the parameters tuning, McIT2FIS employed the Kalman-filtering algorithm.\nHowever, the self-organizing fuzzy NN paradigm discussed above has to employ a clustering method to partition the input space during the FIS structure\u2019s design. Contrary to this, a hierarchical FIS (HFIS) constructs an FIS by using a hierarchical arrangement of several low-dimensional fuzzy subsystems [25]. Initially, the input variables selection, the levels of hierarchy, and the number of parameters was fully up to the experts to determine. Moreover, HFIS design overcomes the curse of dimensionality [26], and it possesses a universal approximation ability [27]\u2013[30].\nTorra et al. [31] summarized the contributions where the expert\u2019s role in the HFIS design process was minimized/eliminated. For example, in [32], HFIS was realized as a feedforward network like structure in which the output of the previous layer\u2019s subsystem was only fed to the consequent part of the next layer, and so on. Similarly, in [33], a two-layered HFIS was developed, where, for each layer, the knowledge bases (KB) were generated by linguistics rule generation method and the KB rules were selected by genetic algorithm (GA). In [34], an adaptive fuzzy hierarchical sliding-mode control method was proposed, which was an arrangement of many subsystems, and the top layer accommodated all the subsystems\u2019 outputs. Moreover, in [35], to optimize the structure of a hierarchical arrangement of low-dimensional TSK-type FISs, probabilistic incremental program evolution [36] was employed. Similarly, the importance of the hierarchical arrangements of the low-dimensional T2FSs is explained in [5], [37].\nFor FIS models that have a structural representation (e.g., self-organizing fuzzy NN and HFIS models), multiobjective optimization is inherent since accuracy maximization and complexity minimization are two desirable objectives [38]. Hence, to make trade-offs between interpretability and accuracy, or, in other words, to make trade-offs between approximation error minimization and complexity minimization, a multiobjective orientation of FIS optimization can be used [39]\u2013 [41]. Complexity minimization can be defined in many ways, such as a reduced number of rules,\nreduced number of parameters, etc. [41], [42].\nSince a single solution may not satisfy both objectives simultaneously, a Pareto-based multiobjective optimization algorithm can be used in FIS optimization, the scope of which spans from the rule selection, to rule mining, rule learning, etc. [43]\u2013[46]. Similarly, in [47]\u2013[50], simultaneous learning of KB was proposed, which included feature selection, rule complexity minimization together with approximation error minimization, etc.\nMoreover, in [51], a co-evolutionary approach that aimed at combining a multiobjective approach with a single objective approach was presented where, at first, a multiobjective GA determined a Pareto-optimal solution by finding a trade-off between accuracy and rule complexity. Then, a single objective GA was applied to reduce training instances. Such a process was then repeated until a satisfactory solution was obtained. A summary of research works focused on multiobjective optimization of FIS is provided in [52].\nIn conclusion, the following are the necessary practices for an FIS model design: 1) input space partitioning; 2) rule formation; 3) rule tuning; 4) FIS structural representation; 5) improving accuracy and minimizing a model\u2019s complexity. Therefore, in this work, a multiobjective optimization of HFIS, called a hierarchical fuzzy inference tree (HFIT), was proposed.\nUnlike the self-organizing paradigm that has a network-like structure and uses a clustering algorithm for partitioning of input space, the proposed HFIT constructs a tree-like structure and uses the dynamics of the evolutionary algorithm for partitioning input space [53]. The HFIT is analogous to a multi-layered network and automatically partitions input space during the structure optimization phase, i.e., during the tree construction phase. The parameter tuning of the HFIT was performed by the differential evolution (DE) algorithm [54], which is a metaheuristic algorithm inspired by the dynamics of the evolutionary process. Metaheuristic algorithms, being independent of the problems, solve complex optimization problems. Hence, they are useful in finding the appropriate parameter values for an FIS [13].\nIn this work, the proposed HFIT implements a TSK-type FIS for both T1FIS and T2FIS, and HFIT was studied under both single objective and multiobjective optimization orientations. Hence, a total of four versions of HFIT algorithms were proposed: type-1 single objective HFIT (T1HFITS), type-1 multiobjective objective HFIT (T1HFITM), type-2 single objective HFIT (T2HFITS), and type-2 multiobjective objective HFIT (T2HFITM). In the construction of type-2 HFITs, the type-reduction algorithm of the Karnik-Mendel method described in [4] was used with an improvement in its termination criteria. In summary, the following are the main and\nnovel contributions of this work.\n1) The proposed hierarchical tree-like design (HFIT) forms a natural hierarchical structure\nby combining several low-dimensional fuzzy subsystems.\n2) MOGP driven optimization provided a trade-off between model\u2019s accuracy and complexity.\nMoreover, in the obtained tree, each node has a different input\u2019s combination, where the MOGP governs the input\u2019s combination. Hence, HFIT nodes are heterogeneous in nature, which leads to a high diversity among the rules generated by the HFIT. Such a diverse rule generation methods is a distinguished aspect of the proposed HFIT. 3) A comprehensive theoretical study of HFIT shows that when it comes to the partitioning\nof input space, membership function design, and even rule formation, it has advantages over network-like layered architecture models, which have to use clustering methods when they do input space partitioning. Clustering methods generate overlapping MFs in fuzzy sets, whereas HFIT\u2019s MOGP driven MFs selection avoid such a overlapping of MFs. 4) Unlike many models in the literature, HFIT performed an inclusive automatic feature\nselection, which led to the simplification of the RB in fuzzy subsystems and incorporated only relevant knowledge contained in the dataset into HFIT\u2019s structural representation. 5) A comprehensive performance comparison of the proposed four versions of the HFIT al-\ngorithms both in theoretical and empirical sense with the recently proposed FIS algorithms found in the literature suggests that HFIT design offers a high approximation ability with simple model complexity.\nThe structure of this article is as follows: Section II provides an introduction to T1FIS and T2FIS; Section III describes the proposed multiobjective strategy for developing HFIT and its parameter optimization; Section IV provides a comprehensive theoretical evaluation of HFIT; Section V provides a detailed description of parameter setting and a comprehensive empirical evaluation the proposed HFIT compared with the algorithms reported in the literature; finally, the obtained results are discussed in Section VI followed by a concise conclusion in Section VII."}, {"heading": "II. TSK FUZZY INFERENCE SYSTEMS", "text": ""}, {"heading": "A. Type-1 Fuzzy Inference Systems", "text": "A TSK-type FIS is governed by the IF\u2013THEN rules of the form [1]:\nRi : IF x1 is Ai1 AND . . . AND xdi is Aidi THEN y is Bi (1)\nwhere Ri is the i-th rule in an FIS, Ai1, . . . , Aidi are the T1FSs, Bi is a function of an input vector x = \u3008x1, x2, . . . , xdi\u3009 that returns a crisp output y, and di is the total number of the inputs presented to the i-th rule. Note that the number of inputs may vary from rule-to-rule. Hence, the dimension of inputs in a rule is denoted as di. In TSK, the function Bi is usually expressed as:\nBi = c 0 i + di\u2211 j=1 cjixj (2)\nwhere cji for j = 0 to d i is the free parameters in the consequent part of a rule. The defuzzified crisp output of FIS is computed as follows: First, the inference engine fires up the RB rules. The firing strength fi of the i-th rule is computed as:\nfi = di\u220f j=1 \u00b5Aij(xj) (3)\nwhere \u00b5Aij is the value of j-th T1FS MF at the i-th rule. Then, the defuzzified output y of an FIS is computed as:\ny = \u2211M i=1Bifi\u2211M i=1 fi\n(4)\nwhere M is the total rules in the RB. In this work, as shown in Fig. 1(a), the T1FS A was of the form:\n\u00b5A(x) = 1 1 + ( x\u2212m \u03c3 )2 (5) where m and \u03c3 are the center and the width of MF \u00b5A(x), respectively."}, {"heading": "B. Type-2 Fuzzy Inference Systems", "text": "A T2FS A\u0303 is characterized by a 3-dimensional (3-D) MF [55]. The three axes of T2FS are defined as follows. The x-axis is called the primary variable, the y-axis is called the secondary variable (or primary MF, which is denoted by u), and the z-axis is called the MF value (or secondary MF value), which is denoted by \u00b5. Hence, in a universal set X , a T2FS A\u0303 has the form:\nA\u0303 = {((x, u) , \u00b5A\u0303 (x, u)) |\u2200x \u2208 X, \u2200u \u2208 [0, 1]} (6)\nwhere the MF value \u00b5 has a 2-dimensional support called the footprint of uncertainty of A\u0303, which is bounded by an LMF \u00b5 A\u0303 (x) and a UMF \u00b5\u0304A\u0303(x) (Fig. 1(b)). A Gaussian function, with\nm em\nb er sh ip\nva lu e (\u00b5 )\nan uncertain mean within [m1,m2] and standard deviation \u03c3, is an IT2FS MF (Fig. 1(b)), which is written as:\n\u00b5A\u0303(x,m, \u03c3) = exp\n( \u22121\n2 ( x\u2212m \u03c3 )2) , m \u2208 [m1,m2]. (7)\nIn this work, the LMF was defined as [4]:\n\u00b5 A\u0303 (x) =  \u00b5A\u0303(x,m2, \u03c3), x \u2264 (m1 +m2)/2\u00b5A\u0303(x,m1, \u03c3), x > (m1 +m2)/2 (8) and the UMF was defined as [4]:\n\u00b5\u0304A\u0303(x) =  \u00b5A\u0303(x,m1, \u03c3), x < m1\n1, m1 \u2264 x \u2264 m2 \u00b5A\u0303(x,m2, \u03c3), x > m2 . (9)\nIn Fig. 1(b), the point xp along the x-axis of 3-D IT2FS MF cuts the LMF and UMF along the y-axis, and the value of the IT2FS is considered to be along the z-axis (not shown in Fig. 1(b)) are \u00b5\u0304A\u0303(x p) and \u00b5 A\u0303 (xp). Considering IT2FS MFs, i-th IF\u2013THEN rule of type-2 TSK-FIS for an input vector x = \u3008x1, x2, . . . , xdi\u3009 takes the following form:\nRi : IF x1 is A\u0303i1 AND . . . AND xdi is A\u0303idi THEN y is B\u0303i (10)\nwhere A\u0303i1, . . . , A\u0303idi are the T2FSs, B\u0303i is a function of x that returns a pair [bi, b\u0304i] called the left and right weights of the consequent part of the i-th rule. In TSK, B\u0303i is usually written as:\nB\u0303i = [c 0 i \u2212 s0i , c0i + s0i ] + di\u2211 j=1 [cji \u2212 s j i , c j i + s j i ]xj (11)\nwhere cji for j = 0 to d i is the free parameter in the consequent part of a rule and sij for j = 0 to di are the deviation factors of the free parameters. The firing strength of IT2FS Fi = [f i, f\u0304i] is computed as:\nf i = di\u220f j=1 \u00b5 A\u0303ij (xj) and f\u0304i = di\u220f j=1 \u00b5\u0304A\u0303ij(xj) (12)\nAt this stage, inference engine fires up the rule and the type-reducer reduces the IT2FS to\nT1FS. In this work, the center of set type-reducer ycos, prescribed in [4], was used:\nycos = \u22c3\nf i\u2208F i, bi\u2208B\u0303i\n\u2211M i=1 f\nibi\u2211M i=1 f i = [yl, yr] (13)\nwhere yl and yr are the left and the right end of the interval. For the ascending order of bi and b\u0304i , yl and yr are computed as:\nyl =\n\u2211L i=1 f\u0304 ibi + \u2211M\ni=L+1 f ibi\u2211L i=1 f\u0304 i + \u2211M i=L+1 f i\n(14)\nyr =\n\u2211R i=1 f ib\u0304i + \u2211M\ni=R+1 f\u0304 ib\u0304i\u2211R i=1 f i + \u2211M i=R+1 f\u0304 i\n(15)\nwhere L and R are the switch point, determined by\nbL \u2264 yl \u2264 bL+1 and b\u0304R \u2264 yr \u2264 b\u0304R+1,\nrespectively. The defuzzified crisp output is then computed as:\ny = yl + yr\n2 . (16)"}, {"heading": "III. MULTIOBJECTIVE OPTIMIZATION OF HIERARCHICAL FUZZY INFERENCE TREES", "text": ""}, {"heading": "A. Hierarchical Tree Formation", "text": "A hierarchical fuzzy inference tree (HFIT) is a tree-based system. Its hierarchical structure is analogous to a multilayer feedforward NN, where the nodes (the low-dimensional FISs) are connected using weighted links. The concept of forming a hierarchical fuzzy inference tree is inherited from the flexible neural tree proposed by Chen et al. [56], which has two\nlearning phases. First, in the tree construction phase, an evolutionary algorithm is employed to construct/optimize a tree-like structure. Second, in the parameter tuning phase, a genotype representing the underlying parameters of the tree structure is optimized by using parameter optimization algorithms.\nTo create an optimum tree based model, firstly, a population of randomly generated trees is formed. Once a satisfactory tree structure (a tree with a small approximation error and low complexity) is obtained using an evolutionary algorithm, the parameter tuning phase optimizes its parameters. The phases are repeated until a satisfactory solution is obtained. Fig. 2 is a clear representation of HFIT\u2019s two-phase construction approach."}, {"heading": "B. Tree Encoding", "text": "An HFIT G is a collection of nodes V and terminal nodes T :\nG = V \u222a T = {v2, v3, . . . , vtn} \u222a {x1, x2, . . . , xd} (17)\nwhere vj (j = 2, 3, . . . , tn) denotes non-leaf instruction and has 2 \u2264 j \u2264 tn arguments. The leaf node\u2019s instruction x1, x2, . . . , xd takes no argument and represents the input variable/instruction. A typical HFIT is shown in Fig. 3(a); whereas, Fig. 3(b) illustrates an HFIT\u2019s node Ni that takes ni inputs. The inputs zij \u2208 {x1, x2, . . . , xd} for j = 1 to ni to a node Ni is either from the input layer or from other nodes in HFIT. Each node in an HFIT receives a weighted input xiwi, where wi is the weight. In this work, however, the weights in HFIT were set to 1 because the objective of this work was also to reduce the complexity of the produced tree along with approximation error. Setting weights to 1 also allow raw input to be fed to the fuzzy sets."}, {"heading": "C. Rule Formation at the Nodes", "text": "1) Rules for Type-1 FIS Node: Each node in an HFIT is an FIS of either type-1 or type-2. Hence, the rules at a node were created as follows: Considering a reference to the node N1 from\noutput y1 of node N1 is computed as:\ny1 =\n\u22112 i=1 \u22112 j=1 f 1 ijB\n1 ij\u22112\ni=1 \u22112 j=1 f 1 ij\n(18)\nwhere the firing strength f 1ij is computed as:\nf 1ij = \u00b5A11i(x1)\u00b5A12j(x2), for i = 1, 2 and j = 1, 2. (19)\nSimilar to node N1, node N2 has two inputs and, if each input at node N2 is partitioned into two T1FSs, then the output y2 of node N2 is computed in a similar way to how the output of the node N1 is computed.\nThe output y3 of the HFIT shown in Fig. 3(a) is computed from node N3, which revives inputs y1 and y2 and x3, where y1 and y2 are the outputs of nodes N1 and N2, respectively. Therefore, the rules at node N3, considering each input y1, y2, and x3 has two T1FSs A311, A 3 12, A 3 21, A 3 22, and A331, A 3 32 respectively, is represented as:\nR3ijk : IF y1 is A 3 1i AND y2 is A 3 2j AND x3 is A 3 3k THEN y 3 ijk = c 0 ijk + c 1 ijky1 + c 2 ijky2 + c 3 ijkx3, for i = 1, 2 and j = 1, 2 and k = 1, 2.\nOutput y3 of node N3, which is also the output of the tree (Fig. 3(a)), is computed as:\ny3 =\n\u22112 i=1 \u22112 j=1 \u22112 k=1 f 3 ijkB\n3 ijk\u22112\ni=1 \u22112 j=1 \u22112 j=1 f 3 ijk\n(20)\nwhere the consequent part B3ij is computed using (2) and the firing strength f 3 ijk is computed as:\nf 3ij = \u00b5A31i(y1)\u00b5A32j(y2)\u00b5A33k(x3), for i = 1, 2 and j = 1, 2 and k = 1, 2. (21)\n2) Rules for Type-2 FIS Node: If the nodes of the HFIT in Fig. 3(a) are type-2 nodes, then,\nassuming that node N1 has two T2FSs A\u0303111, A\u0303 1 12 and A\u0303 1 21, A\u0303 1 22, respectively, the rules for T2FIS at node N1 are generated as:\nR1ij : IF x1 is A\u0303 1 1i AND x2 is A\u0303 1 2j THEN y 1 ij = [c 0 ij \u2212 s0ij] + [c1ij \u2212 s1ij]x1 + [c2ij \u2212 s2ij]x2, for i = 1, 2 and j = 1, 2\nand the lower and upper firing strengths f 1 ij and f\u0304 1ij at node N1 are computed as:\nf 1 ij = \u00b5A\u030311i(x1)\u00b5A\u030312j(x2), for i = 1, 2 and j = 1, 2 (22) f\u0304 1ij = \u00b5A\u030311i(x1)\u00b5A\u030312j(x2), for i = 1, 2 and j = 1, 2. (23)\nThen, the left and right weights b1ij and b\u0304 1 ij of the consequent part of the rules are produced by using (11). Thereafter, the type-reduction of the node is performed as described in [4], where the left and right intervals y1l and y 1 r are computed as per (14) and (15). During type-reduction [4], an early stopping mechanism was adopted to reduce computation time. Finally, output y1 of node N1 is computed as y1 = (y1l + y 1 r)/2.\nThe output computation at node N2 of the tree in Fig. 3(a) is similar to that of the output computation of node N1 because, at node N2, there are two inputs and each of these are partitioned into two T2FSs.\nThe output of the type-2 HFIT shown in Fig. 3(a) is computed from node N3, which receives inputs y1 and y2 and x3, where y1 and y2 are the outputs of nodes N1 and N2, respectively. Therefore, the rules at node N3, considering each input y1, y2, and x3 has two T2FSs A\u0303311, A\u0303 3 12, A\u0303321, A\u0303 3 22, and A\u0303 3 31, A\u0303 3 32 respectively, are represented as:\nR3ijk : IF y1 is A\u0303 3 1i AND y2 is A\u0303 3 2j AND x3 is A\u0303 3 3k THEN\ny3ijk = [c 0 ijk \u2212 s0ijk] + [c1ijk \u2212 s1ijk]y1 + [c2ijk \u2212 s2ijk]y2 + [c3ijk \u2212 s3ijk]x3,\nfor i = 1, 2 and j = 1, 2 and k = 1, 2.\nThe lower and upper firing strengths f 3 ijk and f\u0304 3ijk at node N3 are computed as:\nf 3 ijk = \u00b5A\u030331i(y1)\u00b5A\u030332j(y2)\u00b5A\u030333j(x3), for i = 1, 2 and j = 1, 2 and k = 1, 2 (24) f\u0304 3ijk = \u00b5A\u030331i(y1)\u00b5A\u030332j(y2)\u00b5A\u030333j(x3), for i = 1, 2 and j = 1, 2 and k = 1, 2. (25)\nAfter computing the firing strengths and the left and right weights b3ij and b\u0304 3 ij of the rules, the type-reduction at the node is performed by using (13), where the left and right intervals y3l and y3r are computed as per (14) and (15). Output y3 of node N3, which is also the output of the tree, is computed by averaging y3l and y 3 r as y3 = (y 3 l + y 3 r)/2."}, {"heading": "D. Structure Tuning (Pareto-based Multiobjective Optimization)", "text": "Usually, a learning algorithm owns a single objective (approximation error minimization) that\nis often achieved by minimizing the root mean squared error (RMSE) on the learning data:\nE = \u221a\u221a\u221a\u221a 1 N N\u2211 i=1 (di \u2212 yi)2 (26)\nwhere d and y are the desired and the model\u2019s outputs, respectively, and N is the number of data pairs in the training set. However, a single objective comes at the expense of a model\u2019s complexity or the generalization ability on unseen data. The generalization ability broadly depends on the model\u2019s complexity (e.g., the number of parameters c(w) in the model) [57]. The minimization of the approximation error E and the number of free parameters c(w) are conflicting objectives. Hence, a Pareto-based multiobjective optimization can be applied to obtained a Pareto set of nondominated solutions, in which no one objective function can be improved without a simultaneous detriment to at least one of the other objectives of the solution [58]\nTherefore, an HFIT that offers the lowest approximation error and simplest structure is the most desirable one. To obtain such a set of Pareto-optimal (nondominated) solutions, a nondominated sorting based MOGP was applied.\nThe proposed MOGP acquires the nondominated sorting algorithm [58] for computing Paretooptimal solutions from an initial population of fuzzy inference trees. The individuals in MOGP were sorted according to their dominance in population. Moreover, individuals were sorted according to the rank/Pareto-front/line. MOGP is an elitist algorithm that allows the best individuals to propagate into the next generation. Diversity in population was maintained by measuring the crowding distance among the individuals [58].\nA detailed description of MOGP algorithm is as follows: 1) Initial Population: Two fitness measures were considered: approximation error minimization and parameter count minimization. To simultaneously optimize these objectives during the structure-tuning phase, an initial population W0 of randomly generated HFITs was formed and sorted according to their nondominance.\n2) Selection: In selection operation, a mating pool Wp of size(W0)/2 was obtained using binary tournament selection that selects two candidates randomly at a time from a population Wt, and the best solution (according to its rank and crowding distance) is copied into the mating pool Wp. This process is continued until the mating pool becomes full.\n3) Generation: An offspring population Wc was generated using the individuals of the mating pool Wp. Two distinct individuals (parents) were randomly selected from the mating pool to create new individuals using the genetic operators crossover and mutation.\n4) Crossover: In crossover operation, randomly selected sub-trees of two parent trees are swapped (Fig. 4(a)). The swapping includes the exchange of nodes. A detailed description of the crossover operation in genetic programming is available in [59], [60]. The crossover operation is selected with the crossover probability pc. 5) Mutation: The mutation operators used in HFIT are as follows [59], [60]:\na) Replacing a randomly selected terminal xi \u2208 T with a newly generated terminal xj \u2208 T for j 6= i. b) Replacing all terminal nodes of an HFIT with a new set of terminal nodes derived from T . c) Replacing a randomly selected FIS node Ni \u2208 F with a newly generated FIS node Nj \u2208 F for j 6= i. d) Replacing a randomly selected terminal node xi \u2208 T with a newly created FIS node Ni \u2208 F . e) Deleting a randomly selected terminal node xi \u2208 T or deleting a randomly selected FIS node Ni \u2208 F . The mutation operation was selected with the probability pm, and the type of mutation operator\n(a or b or c or d or e) was chosen randomly during the mutation operation (Fig. 4(b)).\n6) Recombination: The offspring population Wc and the main population Wt were mixed\ntogether to make a combined population Wg.\n7) Elitism: In this work, elitism was decided according to the rank (based on both RMSE and the model\u2019s complexity) of the individuals (HFITs) in the population. Therefore, in this step, size(Wc) worst (poorer rank) individuals were weeded out from the combined population Wg. In other words, size(Wt) best individuals are propagated into the new generation t + 1 as the main population Wt+1."}, {"heading": "E. Parameter Tuning", "text": "In the structure tuning phase, an optimum phenotype (HFIT) was derived with the parameters being initially fixed by random guesswork. Hence, the obtained phenotype was further tuned in the parameter tuning phase by using a parameter optimization algorithm. To tune the parameters of the derived phenotype, its parameters were mapped onto a genotype, i.e., onto a real vector, called a solution vector. The selection of the best phenotype in a single objective training was solely based on a comparison of the RMSEs. However, selecting a solution in a multiobjective training is a difficult choice. In this work, after the multiobjective training of HFIT, the best solution for parameter tuning was picked from the Pareto front. Strictly, the solution that gave\nthe best RMSE among the solutions marked rank-one in the Pareto-optimal set was chosen. Fig. 5 is an illustration of the solutions that belong to the Pareto-front.\nThe genotype mapping of a T1FIS and a T2FIS differ only in regard to their number of parameters. In HFIT, a T1FIS uses the MF mentioned in (5), which has two arguments m and \u03c3 and each rule in T1FIS has di + 1 variables in the consequent part as referred to in (2), where di is the number of inputs to the i-th rule. On the other hand, a T2FIS uses IT2FSs, which are bounded by LMFs and UMFs (Fig. 1(b)) and have two Gaussian means m1 and m2 and a variance \u03c3 to be optimized. The Gaussian means m1 and m2 for type-2 Gaussian MF (7) were defined as:\nm1 = m+ \u03bb\u03c3 and m2 = m\u2212 \u03bb\u03c3,\nwhere \u03bb \u2208 [0, 1] is a random variable taken from uniform distribution and m is the center of the Gaussian means m1 and m2 taken from [0, 1]. Similarly, the variance \u03c3 of type-2 Gaussian MF (7) was taken from [0, 1]. The consequent part of the T2FIS was computed according to (11), which led to 2(di + 1) variables.\nAssume that an HFIT (a tree like Fig. 3(a)) has k many nodes and each node in the phenotype takes 2 \u2264 di \u2264 tn inputs, where each input is partitioned into two fuzzy sets (MFs). Then, the number of the fuzzy sets at a node is 2di. Since the number of inputs at a node is di and each input is partitioned into two fuzzy sets, the number of rules at a node is 2di . Hence, the number\nof parameters at a T1FIS node is [2(2di) + 2di(di+ 1)] and the number of parameters at a T2FIS node is [3(2di) + 2di(2(di + 1))]. Therefore, the total number of parameters in an HFIT is the summation of the number of parameters at all k nodes in the tree. For example, the number of parameters in the type-1 HFIT and type-2 HFIT shown in Fig. 3(a) are 84 and 154 respectively.\nAssuming n is the total number of parameters in a tree, the genotype or the solution vector\nw corresponding to the tree (phenotype) is expressed as:\nw = \u3008w1, w2, . . . , wn\u3009 (27)\nNow, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.\nIn this work, the differential evolution (DE) version \u201cDE/rand-to-best/1/bin\u201d [54] was used, which is a metaheuristic algorithm that uses a crossover operator inspired by the dynamics of \u201cnatural selection.\u201d The basic principle of the DE is as follows: First, an initial population matrix Wt = (w1,w2, . . . ,wP ) at the iteration t = 0 is randomly initialized. The population Wt contains P many solution vectors. A solution vector w in the population is an n-dimensional vector representing the free parameters of an HFIT. Secondly, the population Wt+1 is created using binomial trials. Hence, to create a new solution vector for the population Wt+1, three distinct solution vectors wa, wb, and wc and the best solution vector wg are selected from the population\nWt. Then, for a random index k \u2208 [1, n] and for the selected trial vector wa = \u3008wa1 , wa2 , . . . , wan\u3009, the j-th variable of the modified trial vector wa\u2032 are created as:\nwa \u2032\nj =  waj + F (w g j \u2212 waj ) + F (wbj \u2212 wcj), rj < cr \u2016 j = k\nwaj , rj \u2265 cr (28)\nwhere rj \u2208 [0, 1] is a uniform random sample, cr \u2208 [0, 1] is the crossover rate, and F \u2208 [0, 2] is the differential weight. Similarly, all the variables j = 1 to n of the trial vector wa is created using (28). After creation of the modified trial vector wa\u2032 , it is recombined as:\nwa =  wa \u2032 , E(wa\u2032) < E(wa)\nwa, E(wa\u2032) \u2265 E(wa) (29)\nwhere E(.) is the function that returns the fitness of a solution vector using (26). In DE , operators, such as selection, crossover, and recombination were repeated until a satisfactory solution vector w\u2217 was found or no improvement was observed compared to an obtained solution over a fixed period (100 DE iterations)."}, {"heading": "IV. THEORETICAL EVALUATION", "text": "Efficiency of the proposed HFIT comes from a combined influence of three basic operations involved in the model\u2019s development: tree construction through MOGP, combining several lowdimensional fuzzy systems in a hierarchical manner, and parameters tuning through differential evolution (DE). Hence, HFIT bears many distinguished properties that define its prediction efficiency compared to many models invoked from literature for comparison. Following are the HFIT\u2019s properties: 1) Convergence ability of the evolutionary class algorithms (EA) or for that matter MOGP. 2) Approximation ability of the evolved hierarchical fuzzy system (tree model). 3) Convergence ability of DE in tree\u2019s parameters tuning. Subsequent discussions theoretically analyze each of these properties one-by-one."}, {"heading": "A. Optimal tree structure through MOGP convergence", "text": "Evaluating the convergence of evolutionary class algorithms has been a challenging task because of their stochastic nature. Theoretical studies of EAs performed through various perspectives show that indeed an optimal solution is possible in a finite time. Initially, Goldberg and Sergret [67] showed convergence property of GA using a finite Markov chain analysis, where they considered GA with a finite population and recombination and mutation operators.\nA different viewpoint of MOGP convergence (EAs in general) can be referred to as by using Banach fixpoint theorem described in [68]. Banach fixpoint theorem [69] states that on a metric space a constructive mapping f has a unique fixpoint, i.e., for an element x, f(x) = x. Therefore, Banach fixpoint theorem can explain MOGP convergence with only assumption that there should be an improvement of the population (not necessarily of the optimal solution) from one generation to another. Banach fixpoint theorem also indicates that if MOGP semantics is to be considered as a transformation between one population to another and if it is possible to obtain a metric space in which transformation is constructive, then MOGP converges to a optimal population W\u2217, i.e., to a population containing optimal solution.\nA mapping f defined on elements of ordered pair set S is constructive if the distance between f(x) and f(y) is less than x and y for any x, y \u2208 S. Now, distance mapping \u03b4 : S \u00d7 S \u2192 R is a metric space iff for any x, y \u2208 S the following condition satisfy:\n\u2022 \u03b4(x, y) \u2265 0 and \u03b4(x, y) = 0 if x = y\n\u2022 \u03b4(x, y) = \u03b4(y, x) \u2022 \u03b4(x, y) + \u03b4(y, z) \u2265 \u03b4(x, z)\nLet \u3008S, \u03b4\u3009 be a metric space and f : S \u2192 S be a mapping, then f is constructive iff there is a constant \u2208 [0, 1) such that for all x, y \u2208 S\n\u03b4(f(x), f(y)) \u2264 \u03b4(x, y) (30)\nTherefore, for Banach theorem\u2019s formulation, the completeness of the metric space needs to be defined. Now, metric space elements p0, p1, . . . are a Cauchy sequence iff for any > 0, there exist k such that for all m,n > k, \u03b4(pm, pn) < . It also follows that, if such Cauchy sequence p0, p1, . . . has a limit p = limn\u2192\u221e pn, then metric space is complete.\nTheorem 1. For a complete metric space \u3008S, \u03b4\u3009 and constructive mapping f : S \u2192 S, mapping f has a unique fixpoint x \u2208 S such that for any x0 \u2208 S\nx = lim i\u2192\u221e\nf i(x0)\nwhere f 0(x0) = x0 and f i+1(x0) = f(f i(x0))\nProof. A proof of Banach theorem can be found in [70, p. 60] described as method of successive approximation.\nIn this article, it is necessary to show that if a metric space S for MOGP population can be obtained, then any constructive mapping f in MOGP will contain a unique fixpoint. The proposed MOGP has a fixed population size (say n), i.e., each population contain n individuals, and in each generation, the total fitness of the population is expected to increase. Let \u03b8 be a function that computes the fitness of a population, which is expressed as:\n\u03b8(W) = 1\nn \u2211 wi\u2208W \u2211 wi\u2208W E(wi) E(wi)\n(31)\nwhere function E(wi) evaluates RMSE of each wi. Now, distance mapping \u03b4 : S \u00d7 S \u2192 R, where S is a set of MOGP populations, can be defined as:\n\u03b4(W1,W2) =  0, W1 = W2|\u03b8(W1)|+ |\u03b8(W2)|, W1 6= W2 (32) It follows that\n\u2022 \u03b4(W1,W2) \u2265 0 and \u03b4(W1,W2) = 0 if W1 = W2 holds for any population W1 and W2 in\nMOGP.\n\u2022 \u03b4(W1,W2) = \u03b4(W2,W1) is obvious and \u2022 \u03b4(W1,W2) + \u03b4(W2,W3) = |\u03b8(W1)|+ |\u03b8(W2)|+ |\u03b8(W2)|+ |\u03b8(W3)| \u2265 |\u03b8(W1)|+ |\u03b8(W3)| =\n\u03b4(W1,W3)\nTherefore, MOGP has a metric space \u3008S, \u03b4\u3009. Now, it only remains to show that the MOGP follows a constructive mapping f : S \u2192 S, i.e., in each subsequent generation of MOGP, an improvement is possible. Altenberg [71] showed that by maintaining genetic operators, such as selection, crossover, and mutation, the evolvability of genetic programming can be increased. Additionally, Altenberg [71] analyzed the probability of a population containing fitter individuals than the previous population and offered the subsequent proof. It was observed that even for a random crossover operation, genetic programming evolvability can be ensured. It is then necessary to say that, indeed an MOGP can produce fitter population than the previous ones.\nLet\u2019s depart from MOGP operations descriptions to continue with Banach theorem since it is now known that MOGP offers constructive mapping f : S \u2192 S, for which t-th iteration population offers constructive mapping. In other words, \u03b8(Wt) < \u03b8(Wt+1), i.e., mapping f(Wt) = Wt+1 holds. It follows that\n\u03b4(f(Wt1), f(W t 2)) < \u03b4(W t 1,W t 2)\nMoreover, it satisfies Banach fixpoint theorem. Hence,\nW\u2217 = lim i\u2192\u221e f i(W0) (33)\nIt indicates that MOGP converges to a population W\u2217, which is a unique fixpoint in a population space.\nRemark 1. It is evident from MOGP operation that it produces an optimal tree structure from a population space. Although obtaining optimality in the tree design using MOGP is sufficient to claim the formation of a function that can approximate to a high degree of accuracy, it is necessary to investigate the approximation capability of the hierarchical fuzzy system developed in the form of a tree structure."}, {"heading": "B. Approximation ability of hierarchical fuzzy inference tree", "text": "This Section describes the approximation capability of an HFIT, which is a result of MOGP operation. Theoretical studies of special cases of the hierarchical fuzzy systems are provided in [27], [32]. Whereas, the proposed HFIT produces a general hierarchical fuzzy system. In HFIT, not only a cascaded hierarchy of fuzzy system (a fuzzy subsystem takes input only from its previous fuzzy subsystem [27]) can be produced, but a general hierarchical fuzzy system, in which a fuzzy subsystem can take inputs from any previous layer fuzzy subsystem, can be produced. A hierarchical fuzzy system described in [30] resembles the hierarchical fuzzy system produced by HFIT. To show the approximation capability of the proposed HFIT, it requires coming to the conclusion that the proposed HFIT is analogous to the hierarchical fuzzy system described by Zeng and Keane [30].\nLet\u2019s perform an analogy between the proposed HFIT and the concept of a natural hierarchical fuzzy system described by Zeng and Keane [30]. To show such an analogy, at first, it needs to establish the definition of the natural hierarchical structure of a continuous function, then it will be necessary to show that, for any such continuous function, a hierarchical fuzzy system exists.\nLet\u2019s take the example of the HFIT shown in Fig. 3(a), which can be represented as natural hierarchical structure of a continuous function. The tree in Fig. 3(a) gives the output y3 from node N3. Moreover, the tree in Fig. 3(a) gives the following functions:\ny3 = N3(y1, y2, x3) y1 = N1(x1, x2) y2 = N2(x4, x5)\nIt can also be expressed as:\nN(x1, x2, x3, x4, x5) = N3[N1(x1, x2), N2(x4, x5), x3] (34)\nIt follows that, for a given function y = N(x1, x2, x3, x4, x5), if there exist functions N3, N2, N1 such that function (34) can be obtained, then function N(x1, x2, x3, x4, x5) can be represented as hierarchical structure.\nFor simplicity\u2019s sake, let\u2019s take the case of a two-stage tree, where the top layer node is denoted by N1 and its output is by y. Similarly, second layer nodes are denoted by N2i and their outputs are by y2i , for 1 \u2265 i \u2264 m. Therefore, a natural hierarchical structure can be defined as:\nDefinition 1 (Natural Hierarchical Structure). Let y = N(x1, . . . , xn) be a multi-input-singleoutput continuous function with n input variables x = \u3008x1, . . . , xn\u3009 defined on input space\nU = \u220fn\ni=1 Ui \u2282 Rn and the output y defined on the output space V \u2282 R. If there exist m + 1\ncontinuous functions y = N1(y21, . . . , y 2 m, x 1 i , . . . , x\n1 di1 )\ny2j = N 2 j (x 2j i , . . . , x 2j\ndi2j )\n(35)\nand the functions have inputs x1 di1 and x2 di2j , where d1 < n and d2j < n are input dimensions at the top and second stage of hierarchy, respectively, such that\nN(x1, . . . , xn) = N 1[N21 (x 21 i , . . . , x 21 di21 ), . . . , N2m(x 2m i , . . . , x 2m di2m ), x1i , . . . , x 1 di1 ] (36)\nthen N(x1, . . . , xn) is a continuous function with natural hierarchal structure.\nSuch form of natural hierarchical structure also possesses separable or arbitrarily separable hierarchical structural property, i.e., the individual functions can be decomposed [30]. Now, from Kolmogorov\u2019s Theorem [72], the following can be stated: Any continuous function N(x1, . . . , xn)\non U = \u220fn\ni=1[\u03b11, \u03b2i] (\u03b1i and \u03b2i define the input range) can be represented as a sum of 2n + 1\ncontinuous functions with an arbitrarily separable hierarchical structure. This statement concludes to the following theorem. Theorem 2. Let N(x) be any continuous function on U = \u220fn\ni=1[\u03b1i, \u03b2i] and its hierarchical\nstructure representation be N(x) = N1[N2(x), . . . , Nm(x)], in which N j(x)(j = 1, . . . ,m) are continuous functions with natural hierarchical structure, then for any given > 0, there exists a hierarchical fuzzy system\nG(x) = G1[G2(x), . . . , Gm(x)]\nwhich has the same natural hierarchical structure as N(x) such that\n\u2016N \u2212G\u2016\u221e < (37)\n\u2016N i \u2212Gi\u2016\u221e < i = 0, 1, . . . ,m (38)\nand the same holds between the sub-functions of N i(x) and the fuzzy subsystems of Gi(x)(i = 0, 1, . . . ,m).\nProof. Proof of Theorem 2 can be found in [30].\nRemark 2. It is to note that Theorem 2 shows that hierarchical structure of fuzzy systems is universal approximators. Therefore, they can approximate any continuous function N(x) to any degree of accuracy as-well-as they can approximate each component of that function. Hence, the proposed HFIT that can form a natural hierarchical structure can achieve universal structure approximation.\nAnother property of the proposed HFIT is the parameter tuning, which is performed by a global optimizer (e.g., DE was applied in this research). Hence, it is required to investigate the convergence ability of the DE algorithm in parameter tuning of HFIT."}, {"heading": "C. Optimal parameter through differential evolution convergence", "text": "Convergence property and efficiency of DE is well studied [73], [74]. A probabilistic viewpoint of DE convergence followed by a description of global convergence condition for DE is described in [75]. They show that indeed DE converges to an optimal solution. Similarly, Zhang and Sanderson [74] studied the various property of DE, such as mutation, crossover and recombination operators that influence the DE convergence. DE follows a similar property as of EA class algorithms described in Section IV-A. Hence, its global convergence ability is not different than the one described for MOGP, and indeed it finds an optimal parameter vector for HFIT."}, {"heading": "D. Comparative study of HFIT with other models", "text": "The proposed HFIT learns knowledge contained in the training data through adaptation in its structure and the rules generated at its nodes. Such a process of learning/acquiring knowledge\nfrom data is somehow similar to the models having network-like layered architecture, i.e., ANFISlike approaches, which usually have 4 or 5 or 6 layered network structure. However, HFIT\u2019s strength comes from its adaptive structure formation, whereas most of the network-like models have fixed layer structure.\n1) Flexible structure formation: Specifically, the models depending on layered structure (e.g., HyFIS, DENFIS, D-FNN, EFuNN, FALCON, GNN, SaFIN, SONFIN, SuPFuNIS, eT2FIS, IT2FNN-SVR-N/F, McIT2FIS-UM/US, RIT2FNS-WB, SEIT2FNN, SIT2FNN, TSCIT2FNN, etc.) can only provide adaptation in the number of generated rules in their hidden layer by keeping the input (first) and output (last) layer fixed. Network-like model\u2019s fixed layered architecture in some sense limits their representational flexibility as compared to HFIT.\nIn Section IV-B, it was shown that HFIT has the capability of representing any continuous function in any natural and arbitrarily separable hierarchical form. Therefore, it can be said that the network-like models that grow rules only in one direction have a shortfall in structural representation compared to HFIT, which can grow in layer-wise as-well-as breadth-wise.\n2) Diverse fuzzy rules formation: Additionally, the interaction of one RB to another through the structural representation is what sets HFIT apart from the other models, which generate only a single RB and do not have the interaction as it is in HFIT. Moreover, nodes in HFIT take difference input\u2019s combination govern by MOGP. Therefore, HFIT nodes exhibit heterogeneity, which drives the formation diverse rules in the nodes of HFIT. Whereas, rules in network-like models use same combination inputs while adding rules in the hidden layer during their training process.\n3) Automatic fuzzy set selection: Adaptation in the most of the network-like models is due to the input space partitioning (usually for choosing the number of membership function at the second layer) in two or three fixed fuzzy sets or by using some clustering method, which directly influences the number of rules to form in the third layer (usually called rule layer). The necessity of predefining the number of clusters is basic disadvantage with the clustering based partitioning. Some of the practices in the clustering based partitioning, like the one in SaFIS, are devoted to improving the clustering algorithms to avoid the requirements of such predefinition. However, the overlapping of the membership function of the fuzzy sets is another common problem with clustering based input space partitioning [19]. In [76], authors pointed out four different cases of membership function\u2019s overlapping and proposed subsethood method to transmit the overlapping information to the rules layer.\nOn the other hand, HFIT does not use clustering to determine input space partitions. Instead, each input at each HFIT\u2019s node is partitioned into two fuzzy sets, which is eventually determined by MOGP through evolution. Section IV-A shows that MOGP finds an optimum solution through its iterations and the use of genetic operators. Hence, MOGP at it best avoids the overlapping of the membership function of the fuzzy sets and also eliminates the requirement of an external agent for input space partitioning.\n4) Minimal feature set selection: Feature selection is another important aspect of HFIT. The network-like models such as EFuNN and DENFIS also does feature selection externally (say by external agents). However, in a sense, feature selection in such kind of models do not have direct participation in the structural representation of knowledge contained in training data. Whereas, feature selection is an integral part HFIT\u2019s learning process. Hence, feature selection performed by HFIT incorporates knowledge contained in training data into its structural representation in an explicit way compared to other network-like models. Since an external agent performs feature selection in the network-like models and many other models do not even perform feature selection, they are disadvantageous compared to HFIT when in comes to solving highdimensional problems.\n5) Parameter tuning: Finally, most of the models such as HyFIS, DENFIS, SaFIN, SONFIN, SEIT2FNN, McIT2FIS-UM/US, etc. employ gradient-based methods (e.g., backpropagation) for the parameter tuning. The gradient-based techniques are known as local optimizers, which lacks exploration capability compared to global optimizers (e.g., DE) [77]. HFIT employs DE for its parameter optimization. When it comes to comparing models theoretically, it is not necessary to go deep in parameter tuning debate since such parameter tuning method like DE can also be applied to other models and backpropagation can be applied to HFIT. However, at present scenario, a combined effort of the proposed HFIT model, in this article, have an advantage compared to other models."}, {"heading": "V. EMPIRICAL EVALUATION", "text": "This section describes the evaluated results of the proposed algorithms T1HFITS, T1HFITM, T2HFITS, and T2HFITM on six example problems. Assume that the datasets in the examples are of the form: (X,d), where X = (x1, x2, . . . , xN) is the set of the input vectors and d = \u3008d1, d2, . . . , dN\u3009 is the desired output vector. Here, the dataset has N input\u2013output patterns (pairs) and if the vector y = \u3008y1, y2, . . . , yN\u3009 is the predicted output vector, then the performance of\nan algorithm for the dataset (X,d) can be measured using RMSE E as defined in (26) and correlation coefficient r between the desired output vector d and y as:\nr =\n\u2211N i=1 ( di \u2212 d\u0304 ) (yi \u2212 y\u0304)\u221a\u2211N\ni=1\n( di \u2212 d\u0304 )2\u2211N i=1 (yi \u2212 y\u0304) 2 (39)\nwhere d\u0304 and y\u0304 are the means of the vectors d and y. For simplicity\u2019s sake, the training and the test RMSEs were represented as En and Et, respectively. Similarly, the training and the test correlation coefficients were represented as rn and rt, respectively. Additionally, the model\u2019s complexity c(w) and training time (in minutes) were reported. The reported training time included the time taken to create a tree structure, tune the tree parameters, partition the dataset (file input\u2013output operations), write the developed model to a file, display the tree on a GUI, and compute RMSE and correlation coefficient. The parameter setting mentioned in Table I was used to train the proposed algorithms, which was developed as a software tool and is available at http://dap.vsb.cz/sw/hfit/. The experiments were conducted on a Windows Server R2 that had a 20 core and 700 GB RAM. Each run of experiments was conducted with the random seeds generated from the system. The proposed algorithms were compared with the algorithms collected from the literature (Table II)."}, {"heading": "A. Example 1\u2014System Identification", "text": "Online identification of the nonlinear system is a widely studied problem. The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90]. The nonlinear system identification of the plant is described by the following nonlinear difference equation:\nyp(k + 1) = yp(k)\n1 + yp(k)2 + u3(k) (40)\nwhere [u(k), yp(k)] is the input\u2013output pair of the single input and the single output plant at the time k and yp(k+1) is the one step ahead prediction. Hence, the objective is to predict yp(k+1) of the system based on the sinusoidal input u(k) = sin(2\u03c0k/100) and the current output yp(k). Let us assign the input x1 = u(k) and the input x2 = y(k).\nThe training patterns were generated with k = 1, . . . , 200 and yp(1) = 0. Similarly, the test patterns were generated for k = 201, . . . , 400 as mentioned in [17]. Therefore, for the training, the inputs were u(k) and yp(k), and the desired output was yp(k+ 1). The training and test were repeated ten times. Such repetitions were performed mainly for assessing an average performance of the proposed algorithms, which is shown in Table III(a). Since the experiments were repeated ten times, ten different models were obtained for each algorithm. The results of the best models (regarding their RMSEs) were compared with the best results available in the literature (Table III(b)).\nThe performance statistics, shown in Table III(a), are evidence of the efficiency of the proposed algorithms. They show that the mean correlation coefficients rn and rt of training and test sets are 1.00 and 1.00, respectively, which indicate that the algorithm consistently performed with a high accuracy. Moreover, such consistency of high accuracy is evident from the obtained small standard deviations (STD) of the training and test RMSEs and correlation coefficients (Table III(a)).\nInterestingly, the Pareto-based multiobjective training offered less complex models (the mean parameter count c(w) of T1HFITM was 34.4 compared to 57.2 of T1HFITS and c(w) of T2HFITM was 90.4 compared to 152.0 of T1HFITS) with high accuracies (Table III(a)). Additionally, the training time taken by T1HFITM and T2HFITM was much less than by T1HFITS and T2HFITS. Hence, the Pareto-based multiobjective was advantageous to use, which provided the option of choosing the best solution from a Pareto-front. An example of a Pareto-front is shown in Fig. 5.\nFor the performance comparisons, the SaFIN result was collected from [19], and FALCON and SONFIN from [16]. The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22]. Table II contains a detailed description of these algorithms.\nTwo parameters may be used for comparing the algorithms: 1) the training and test RMSEs and 2) the parameter count c(w). From the performance comparisons shown in Table III(b), it was found that the proposed algorithms T1HFITS and T1HFITM were better than the T1FIS algorithms FALCON, SaFIN, and SONFIN. SONFIN offered the test RMSE Et = 0.0085 with the smallest parameter count c(w) = 36; whereas, the proposed algorithm T1HFITM offered the better test RMSE Et = 0.0041 with a slightly larger parameter count c(w) = 40.\ncompared to the algorithms T2FLS (Singleton), T2FLS (TSK), TSCIT2FNN, T2TSKFNS, T2FNN, SIT2FNN, RIT2NFS-WB, MRI2NFS. The algorithm SEIT2FNN reported test RMSE Et = 0.0022 and the parameter count was 84; whereas, in comparison to SEIT2FNN, the algorithm T2HFITM offered a slightly higher test RMSE Et = 0.0028, but had a lower parameter count c(w), i.e., 72.\nThe time comparison, however, is limited since the training time depends on several factors: 1) the type of programming language used; 2) the platform and its configurations on which programs were executed; 3) the way data were fed for the training; 3) the status of the cache memory (in the case CPU time is observed); etc. It may be noted that, from the available training time reported in the literature, the MRI2NFS, RIT2NFS-W, T2FLS-G, SEIT2FNN approximately takes 0.15, 0.17, 2.41, and 2.24 minutes (CPU time only) respectively. This is in comparison\nou tp ut s\nto T1HFITM and T2HFITM, which take 1.52 and 3.23 minutes (including CPU time, other file operations, etc.) respectively. Since the training times taken by the algorithms were close to one another and the time comparison has limitations, it may be concluded that the proposed models performed efficiently. This is also evident from the performance statistics given in Table III(a) and the performance comparison is provided in Table III(b).\nThe best models obtained using the proposed algorithms are illustrated in Fig.6, which shows the hierarchical structure of the derived models and the selected inputs are indicated by xi in the models. The rectangular blocks in Fig 6 show the nodes (a T1FIS or T2FIS) of the tree (hierarchical structure). The target and predicted value plots of 200 samples are shown in Fig. 7.\nB. Example 2\u2014Noisy Chaotic Time Series Prediction\n1) Case\u2013Clean Set: A chaotic time series dataset, the Mackey-Glass chaotic time series, was\nused in this example, which was generated using the following delay differential equation:\ndx(k)\ndk = 0.2x(k \u2212 \u03c4) 1 + x10(k \u2212 \u03c4) \u2212 0.1x(k) (41)\nwhere delay constant \u03c4 > 17 and k is the time step. In this example, the objective was to predict x(k) using the past outputs of the time series as mentioned in [86]. Hence, the input\u2013output pattern was of the form:\n[x(k \u2212 24), x(k \u2212 18), x(k \u2212 12), x(k \u2212 6);x(k)] .\nLet us say that the inputs are x1 = x(k \u2212 24) , x2 = x(k \u2212 18), x3 = x(k \u2212 12), and x4 = x(k \u2212 6). For the training of the proposed algorithms, a total of 1000 patterns were generated from k = 124 to 1123, with the parameter \u03c4 being set to 30 and x(0) being set to 1.2 [86]. This set of training patterns were clean (no noise was added). From the generated clean patterns, as mentioned in [86], the first 500 patterns (clean training set) were used for training purposes and the second 500 patterns (clean test set) were used for test purposes. Aiming to assess the average performance of the proposed algorithms, ten repetitions of training and testing were performed using clean training and test sets, and the results were collected accordingly (Table IV(a)). Table IV(b) shows the comparison of results of the proposed algorithms (the best among ten models) with the best results reported by the algorithms listed in Table II.\nFor this example (clean set), the performance statistics are shown in Table IV(a). The obtained statistics illustrate that the proposed algorithms T1HFITS, T1HFITM, T2HFITS, and T2HFITM performed with high accuracies. It shows that the mean correlation coefficient rn of the training set of all algorithms is 1.00, and the mean correlation coefficient rt of the test set of the algorithms T1HFITS, T1HFITM, T2HFITS, and T2HFITM are 0.9858, 0.9864, 0.9783, and 0.9912 respectively. That is the test correlation coefficients are closer to 1.00 (a high positive correlation between target and predicted outputs). Such performance indicates that the algorithms consistently performed with a high accuracy, and the obtained low values of STDs are evidence of this fact (Table IV(a)).\nMoreover, the Pareto-based multiobjective training offered less complex models (the mean parameter count c(w) of T1HFITM was 57.6 compared to 71.6 of T1HFITS and the c(w) of T2HFITM was 129.5 compared to 203.4 of T1HFITS) with high accuracies (Table IV(a)). Hence, like in example 1, in this example also the Pareto-based multiobjective was advantageous to use,\nwhich provided the option to choose the best solution from a Pareto-front. Fig. 5 illustrates a Pareto-front created during the multiobjective training of HFIT.\nTable IV(b) describes the comparison between several algorithms on clean training and test set. The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].\nThe training and test RMSEs and the parameter count c(w) were used for comparing the algorithms, which is shown in Table IV(b). A training time comparison for this example cannot be performed because of the unavailability of the training time of other algorithms in the literature.\nIn T1FIS comparisons, it was found that the proposed algorithms T1HFITS and T1HFITM performed better than or were competitive with the algorithms NNT1FW, AFRS, IFRS, H-\nTS-FS, RBF-AFA, and HyFIS. The algorithms D-FNN and SuPFuNIS had better test RMESs Et = 0.008 and Et = 0.005, but their parameter counts were larger since the number of rules in each case was 10. Since each T1FS MF has at least two parameters and each rule has three free parameters at the consequent part, the number of parameter count for two input variables stands to at least 70 (this is an approximate calculation since D-FNN and SuPFuNIS may have other parameters that may increase the parameter count value). Whereas, the algorithms T1HFITS and T1HFITM had parameter counts equal to 60 and 40, respectively. Therefore, T1HFITS and T1HFITM are competitive with D-FNN and SuPFuNIS.\nIn T2FIS, the proposed algorithms clearly performed better than T2FLS (Singleton), T2FLS (TSK), and NNT2FW. Whereas, the performance of the proposed algorithms were competitive with SEIT2FNN1 (without fuzzy set reduction) and SEIT2FNN2 (with fuzzy set reduction) whose test RMSEs Et were 0.0034 and 0.0058, respectively. The algorithm SEIT2FNN1 had 28 fuzzy sets and SEIT2FNN2 had 16 fuzzy sets (reduced), and each of these had seven rules. Hence, the parameter count of these algorithms stands to at least 126 and 90, respectively. On the other hand, the proposed algorithm T2HFITS had a test RMSE of Et = 0.0086 (slightly larger than SEIT2FNN1 and SEIT2FNN2), but the parameter count was 108, which is smaller than SEIT2FNN1. Similarly, the proposed algorithm T2HFITM had a test RMSE of Et = 0.0058, which is close to SEIT2FNN2 and the parameter count was smaller than SEIT2FNN1 and closer to SEIT2FNN2. Therefore, in this case, the proposed T2HFITM is as efficient as SEIT2FNN1 and SEIT2FNN2 are. Fig.8 shows the hierarchical structure of the best models obtained by the proposed algorithms. Additionally, the target versus prediction plot of test data samples is illustrated in Fig. 9.\nou tp\nut s\n2) Case\u2013Noisy Set: The performances of the proposed algorithms were further evaluated for noisy patterns. Therefore, three training sets and three test sets were generated by adding Gaussian noise with a mean of 0 and STDs of 0.1, 0.2, and 0.3 to the original data x(k) as described in [86]. These noisy training sets (with STDs 0.1, 0.2, and 0.3) were presented for the training of the proposed algorithms. With each training set of STDs of 0.1, 0.2, and 0.3, three test sets were given for testing: clean, STD 0.1, and STD 0.3. The obtained results were compared with the results reported in the literature (Table V).\nTable V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22]. It is evident from the comparison of the results that the proposed algorithms performed efficiently over the noisy datasets and the obtained models were less complex than the other models listed in Table V. Particularly when T1FISs were compared. Moreover, for each noisy data (STD 0.1, STD 0.2, and STD 0.3), the proposed algorithms had a smaller parameter count and had a lower or competitive training RMSE En compared to other listed algorithms. In T1FIS comparisons, the SONFIN had a slightly better RMSE, but the number of parameters counts was larger than the proposed algorithms T1HFITS and T1HFITM. Similarly, in T2FIS comparison, the algorithm eT2FIS had slightly better RMSE than the other listed algorithms, but the models obtained using the proposed algorithms were less complex, i.e., had a smaller parameter count."}, {"heading": "C. Example 3\u2014Miles-Per-Gallon Prediction Problem", "text": "To evaluate the performance of the proposed algorithms, a real-world MPG problem was used. The objective of this example was to predict or estimate the city-cycle fuel consumption in MPG. The MPG dataset was collected from the UCI machine learning repository [91]. This dataset has 392 samples, each of which has six input variables, but in this example, as mentioned in [24], three variables (x1 = weight, x2 = acceleration, and x3 = model year) were selected. In the training process, 50% (196 patterns) of samples were randomly selected for training and the rest of the 50% (196 patterns) of samples were taken for testing. Such a process for the training set and test set selection was repeated ten times. Accordingly, the collected performance statistics are shown in Table VI(a).\nThe performances of the proposed algorithms were compared with the literature (Table VI(a)). However, the algorithms chosen from the literature were tested over fewer test samples. Therefore, the comparison shown in Table VI(a) is limited to the comparison of the training RMSE because all the mentioned algorithms were trained over the same number of training samples. For the comparisons, the T1FLS result was collected from [17] and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].\nThe comparisons of the models in this example were based on the mean training and test RMSEs En and Et obtained for the ten repetitions. However, the comparison on test RMSEs was limited since only 120 samples were used for testing by the algorithms considered from literature. Whereas, the algorithms proposed in this work used 196 samples for testing (Table VI(b)). It was observed that the proposed algorithms T2HFITS and T2HFITM outperformed all the other algorithms except for RIT2NFS-WB, which had a slightly better training RMSE En = 2.3685 in comparison to the training RMSEs En = 2.4699 and En = 2.4052 of T2HFITS and T2HFITM, respectively. Since the performance comparisons were based on the average value of ten repetitions, the model\u2019s hierarchical structures are not presented for this example.\nFrom the available training time reported in the literature, it may be noted that the algorithms McIT2FIS-UM, McIT2FIS-US, RIT2NFS-WB, and SEIT2FNN take 0.0025, 0.003, 0.16, and 0.33 minutes (CPU time only) compared to T2HFITM, which takes 8.23 minutes. However, it may be noted that T2HFITM is a two-phase population-based learning algorithm, whereas the\nother algorithms are single-solution based algorithms."}, {"heading": "D. Example 4\u2014Abalone Age Prediction", "text": "In this example, a prediction problem was taken in which a person\u2019s age was predicted based on their physical measurements. The Abalone dataset was collected from the UCI machine learning repository [91]. It has 4177 data samples, each of which has seven input variables (x1 = length, x2 = diameter, x3 = height, x4 = whole weight, x5 = shucked weight, x6 = viscera weight, and x7 = shell weight) and one output variable (rings). To train the proposed algorithms, 80% (3342 patterns) of samples were randomly taken for training and 20% (835 patterns) remaining samples were taken for testing. Additionally, in this work, to assess the average performance of proposed algorithms, training process was repeated ten times, and the collected results are summarized in Table VII(a).\nThe obtained results are compared with the results reported in the literature (Table VII(b)). For the comparisons, the results of General, HS, CCL, and Chen&Cheng were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24]. The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS. It is evident from the results in Table VII(b) that the proposed algorithms (both T1FIS and T2FIS) outperformed the algorithms considered for comparisons.\nHowever, when comparing the test RMSEs, McIT2FIS-US, McIT2FIS-UM, and RIT2NFSWB had a slight edge over T2HFITS and T2HFITM, but the parameter count of T2HFITM was smallest among all the algorithms, and it had the lowest training error. Hence, it may be concluded that T2HFITM is the best performing algorithm for example 4. T2HFITM performance falls behind only in training time comparison because T2HFITM, being a population based algorithm, takes longer training time than the other algorithms. The algorithms McIT2FIS-US, McIT2FIS-UM, RIT2NFS-WB, and SEIT2FNN take 1.81, 2.35, 5.48, and 17.33 minutes (CPU time only) compared to T2HFITM, which takes 65.02 minutes. It is important to note that the other algorithms are single solution based algorithms. The best-performing models of the proposed algorithms are illustrated in Fig. 10, where the selected input feature is indicated by xi."}, {"heading": "E. Example 5\u2014Box-Jenkins Gas Furnace Problem", "text": "In this example, the Box and Jenkins gas furnace dataset that was taken from [96], which has 296 data samples. The objective of this example was to predict the CO2 concentration from the gas-flow rate. The gas furnace system is modeled using a series, which is of the form: y(k) = f(y(k \u2212 1), u(k \u2212 4). For the training of the proposed models, as mentioned in [24],\n100% (296 patterns) of the samples were used. To show an average performance ability of the proposed algorithms, the training process was also repeated ten times, and the collected results are summarized in Table VIII(a). The performances of the proposed algorithms (the best results) were compared with the best performances of the algorithms reported in the literature (Table VIII(b)).\nTo compare the performance of the algorithms, the results of T1-NFS and GNN were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24]. As reported in Table VIII(b), the proposed algorithms clearly outperformed the algorithms T1-NFS, GNN1, and GNN2 in the case of T1FIS comparisons and algorithms SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US in the case of T2FIS comparisons.\nFor T2FIS, the proposed algorithm T2HFITM provided a training RMSE En = 0.284, which was slightly lower than the training RMSE En = 0.269 of SEIT2FNN. However, the parameter count of T2HFITM was 72 compared to 152 of SEIT2FNN. Additionally, despite being a population based algorithm, T2HFITM takes 6.31 minutes for the training, whereas SEIT2FNN takes 604.66 minutes for the training [17]. Therefore, it may be concluded that, for example 5, T2HFITM performed the best. The best-performing models are illustrated in Fig. 11."}, {"heading": "F. Example 6\u2014Poly (lactic-co-glycolic acid) (PLGA) micro- and nanoparticle dissolution rate", "text": "prediction\nThis example illustrates a pharmaceutical industry problem related to PLGA dissolution profile prediction, which is a complex problem since a vast number of factors governs its dissolution rate profile and it has a high noise and redundancy because the dataset was obtained from various experimental measurements and instruments. As per the dataset provided in [97], [98], this problem has 747 samples and a total of 300 input features, which influence the PLGA protein particle\u2019s dissolution rate [99]. The input features are categorized into five groups: protein descriptor, formulation characteristics, plasticizer, emulsifier, and time delay, which has 85, 17, 98, 99, and 1 features, respectively.\nThe description of each feature group is as follows: 1) The protein descriptors (85 features) describe the type of molecules and proteins used in the drug\u2019s manufacturing. 2) The formulation characteristics (17 features) describe the molecular properties, such as molecular weight, particle size, etc., of the molecules and proteins. 3) The plasticizer (98 features) describes properties, such as fluidity of the material used. 4) The emulsifier (99 features) describes the stabilizing properties of the material used in the drug\u2019s manufacturing. 5) The time delay (1 feature) represents the time taken to dissolve/dissolute a sample drug.\nThe PLGA dissolution profile prediction is a significant problem since it plays a crucial role in the medical application and toxicity evaluation of PLGA-based microparticles dosages [100]. Moreover, PLGA microparticles are important diluents, which are used for producing drugs in their correct dosage form. It is also used as a filler, as an excipient, and as an active pharmaceutical ingredient because it acts as a catalyst for drug absorption/dissolution/solubility [101].\nTherefore, PLGA dissolution is a widely studied research problem in pharmaceutical manufacturing and powder technology.\nUsing the parameter setting mentioned in Table I and using 10-fold cross-validation, the proposed algorithm T1HFITM was able to select seven input features and was able to approximate a test RMSE of Et = 18.66. The selected features were: phase polyvinyl alcohol Mw (x90), ASA (x122), pH 8 msdon (x192), aromatic bond count (x204), a(xx) (x218), pH 12 msacc (x281), time days (x299). Similarly, the proposed algorithm T2HFITM was able to approximate a test RMSE of Et = 15.259 with only four input features: aromatic atom count (x66), phase polyvinyl alcohol concentration inner phase (x88), pH 1 msdon (x285), time days (x299). Additionally, T2HFITM provided a simple model (i.e., c(w) = 108) compared to T2HFITM that had model complexity c(w) = 156. Moreover, T2HFITM takes 7.16 minutes of training time compared to the 45.7 minutes of T1HFITM. This difference in time is due to the difference between the number of input features being selected by T2HFITM and T1HFITM.\nFeature reduction is a significant task since it reduces drug\u2019s manufacturing cost. Table IX shows a comparison of the proposed T1HFITM and T2HFITM with algorithms such as multilayer perceptron (MLP), reduced error pruning tree (REP Tree), heterogeneous flexible neural tree (HFIT), and Gaussian process regression (GPR). It is evident from the results that the proposed algorithm approximates the PLGA dissolution profile with a lower number of features, and its approximation error was competitive with the performance of other algorithms. Fig. 12 illustrates the obtained models for the prediction of the PLGA dissolution profile."}, {"heading": "VI. DISCUSSION", "text": "The proposed HFIT algorithms T1HFITS, T1HFITM, T2HFITS, and T2HFITM were evaluated through six examples, including a real-world example from the pharmaceutical industry. Performance of the proposed algorithms was compared with algorithms that offer fuzzy system\u2019s structural optimization (e.g., SEIT2FNN, SONFIN, SaFIN, TSCIT2FNN, etc.), hierarchical fuzzy system design (e.g., IFRS, H-TS-FS, etc.), dynamic fuzzy system design (e.g., DENFIS, D-FNN, etc.), and so forth. The obtained results illustrate the efficiency of the proposed algorithms in comparison to the algorithms collected from the literature. Such performance was obtained by using the parameter setting mentioned in Table I. Moreover, a comparison using noisy data [example 2, case 2 (Section V-B1)] has proved the approximation efficiency of the proposed algorithms over other algorithms. The HFIT algorithms not only offer solutions with high accuracy (low approximation error), but they also provide the solutions with low complexity.\nThe number of clusters needs to be predetermined in the algorithms that use a cluster-based partitioning of the input space and to define fuzzy sets. On the contrary, the proposed HFIT uses a only two partitions for each inputs and automatically defines fuzzy sets by using the dynamics of the evolutionary process. Such ability is particularly significant for the predictive modeling of problems like example 6 (Section V-F) that has a large number of input features. It would be a difficult task for fuzzy-NN-based algorithms (e.g., SONFIN, SEIT2FNN, McIT2FIS, etc.) to design a network-like structure to solve a high-dimensional problem (e.g., example\u20136 that has 300 input features), whereas the proposed HFIT solves example\u20136 with satisfactory accuracy and low model\u2019s complexity. Section IV-D show that HFIT has several qualities that set it apart from many algorithms mentioned in this work.\nIn Section V, a comprehensive study of the comparative results of the proposed algorithms was presented. It was observed that the proposed HFIT-based algorithms gave better performance than the other algorithms collected from the literature. For example, in the case of example\u20131 T1HFITM provided better RMSE with a lower parameter count. Additionally, T2HFITM offered an RMSE (i.e., 0.0028) with a low complexity (i.e., 72) in comparison to SEIT2FNN that gave an RMSE of 0.0022 with a model complexity of 84.\nSimilarly, for example-2, T2HFITM offered a competitive RMSE (i.e., 0.0058) in comparison to SEIT2FNN2 that gave an RMSE of 0.0053. Additionally, in the noisy dataset comparison, the proposed T2HFITM provided better training RMSEs with lower model\u2019s complexities when compared to many of the recently proposed T2FIS algorithms, such as SEIT2FNN, IT2FNNSVR, and T2FLS-G. Moreover, the models developed by the proposed algorithm adapted its structure in each instance of noisy dataset experiments; whereas, the other models had a fixed structure in each instance of their experiments (Table V). Therefore, the proposed algorithm was able to accommodate the variance in noise more precisely than the other models.\nWith example\u20133, example\u20134, and example\u20135, the proposed type-1 HFIT surpassed all the other algorithms. Whereas, type-2 HFIT performed competitively with algorithms such as RIT2NFSWB, McIT2FIS, and SEIT2FNN. It was observed that the training RMSE of T2HFITM for example\u20133 was as per with RIT2NFS-WB, but the complexity of the proposed T2HFITM was much less than RIT2NFS-WB. For example\u20134, T2HFITM outperformed all its counterparts in both accuracy and complexity. For example\u20135, the training RMSE of T2HFITM was close to SEIT2FNN, but on model complexity and training time, T2HFITM outperformed SEIT2FNN by a comfortable margin. Therefore, it may be concluded that the proposed HFIT version performed efficiently against other algorithms found in the literature.\nThe proposed HFIT is a population-based algorithm. Therefore, it should naturally take more training time than a single solution based algorithm. In addition to that, the training time depends on several factors such as the programming language used, the type of platform, the hardware configuration of the machine, the method of data feeding during the training, etc. Therefore, training time comparison is limited. However, by comparing the training time of the proposed algorithms with the training times of some of other algorithms (training time of only a few algorithms is reported in the literature), the following was observed: 1) in the case of example\u20131, the proposed T2HFITM was found competitive with other algorithms, 2) in the case of example\u2013 5, T2HFITM outperformed SEIT2FNN, 3) in the case of example\u20136, which has 747 samples and\n300 input features, the proposed T2HFITM takes only about 7.16 minutes, which is remarkable.\nFor example\u20136, the proposed T2HFITM was more efficient than T1HFITM because T2HFITM was capable of accommodating noisy information more efficiently than T1HFITM. This is evident from the fact that the average RMSE of T2HFITM was 16.64, and the average RMSE of T1HFITM was 22.36. Hence, the proposed T2HFIT model, which is relied on interval type-2 MFs, is worth considering in such high-dimensional and noisy application problems.\nA comparison between single objective and multiobjective summarized in Table X suggests that the multiobjective approach has performance superiority over the single objective because multiobjective gives a competitively better approximation error with lower model complexity in both type-1 and type2 cases compared to single objective. Additionally, it can be observed that type-2 HFIT offers better approximation error against type-1 HFIT.\nSince HFIT algorithms were developed using the evolutionary process, the quality of their performance is subjected to carefully setting of the parameters mentioned in Table I. Hence, the results of the algorithms mentioned in this work may be further improved upon by choosing different sets of parameters; however, this is a trial-and-error process. For example, the feature selection, i.e., the number of inputs into a node (a fuzzy subsystem) is proportional to the setting of the maximum inputs into an node. Similarly, the hierarchy (number of layers) in an HFIT is proportional to the setting of the maximum depth of a tree. Therefore, HFIT\u2019s complexity can be controlled using these parameters. Additionally, the parameters of MOGP and DE, such as their\npopulation size, crossover probability, mutation probability, etc., influence HFIT\u2019s performance."}, {"heading": "VII. CONCLUSIONS", "text": "Using a fuzzy inference system (FIS) for data mining inherently requires a multiobjective solution and the proposed multiobjective design for a hierarchical fuzzy inference tree (HFIT) stands as a viable option that constructs a tree-like model whose nodes are low-dimensional FIS. The proposed HFIT was developed for both type-1 and type-2 FIS and each node in HFIT implements a Takagi\u2013Sugeno\u2013Kang model. Both type-1 and type-2 FIS were studied in the scope of single objective and multiobjective optimization using genetic programming. Hence, four versions of HFIT were studied: T1HFITS, T1HFITM, T2HFITS, and T2HFITM. The parameters of the membership functions and the consequent parts of the rules were optimized using a differential evolution algorithm. HFIT\u2019s optimization procedure was a two-phase evolutionary optimization approach, in which structure optimization and parameter optimization were applied one-by-one until a formidable solution was obtained. The approximation ability of the proposed HFIT was theoretically examined. As a result of that four distinguished quality of HFIT was discovered: adaptation in structure, diverse rule generation, automatic fuzzy set selection, minimal feature drive structure formation. A comprehensive performance comparison was performed for evaluating the efficiency of the proposed HFIT. The performance of the proposed HFIT algorithm was found to be efficient and competitive compared to the algorithms collected from the literature. HFIT provided competitive approximation compared to other algorithms and simultaneously it produced less complex models. Additionally, HFIT performs feature selection and automatic structure design, which is a necessary for solving high-dimensional problems."}], "references": [{"title": "Fuzzy identification of systems and its applications to modeling and control", "author": ["T. Takagi", "M. Sugeno"], "venue": "IEEE Trans. Syst. Man Cybern., vol. 15, no. 1, pp. 116\u2013132, 1985.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Fuzzy sets", "author": ["L. Zadeh"], "venue": "Inf. Control, vol. 8, no. 3, pp. 338 \u2013 353, 1965.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1965}, {"title": "The concept of a linguistic variable and its application to approximate reasoning", "author": ["L.A. Zadeh"], "venue": "Inf. Sci., vol. 8, no. 3, pp. 199\u2013249, 1975.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1975}, {"title": "Type-2 fuzzy logic systems", "author": ["N.N. Karnik", "J.M. Mendel", "Q. Liang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 7, no. 6, pp. 643\u2013658, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "A hierarchical type-2 fuzzy logic control architecture for autonomous mobile robots", "author": ["H.A. Hagras"], "venue": "IEEE Trans. Fuzzy Syst., vol. 12, no. 4, pp. 524\u2013539, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Intelligent behavior as an adaptation to the task environment", "author": ["L.B. Booker"], "venue": "Ph.D. dissertation, University of Michigan, Ann Arbor, MI, USA, 1982.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY  46", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1982}, {"title": "A hybrid fuzzy genetics-based machine learning algorithm: hybridization of Michigan approach and Pittsburgh approach", "author": ["H. Ishibuchi", "T. Nakashima", "T. Kuroda"], "venue": "1999 Int. Conf. Systems, Man, and Cybernetics, 1999. IEEE SMC\u201999 Conf. Proc., vol. 1, pp. 296\u2013301.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "A learning system based on genetic adaptive algorithms", "author": ["S.F. Smith"], "venue": "Ph.D. dissertation, University of Pittsburgh, Pittsburgh, PA, USA, 1980.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1980}, {"title": "ANFIS: adaptive-network-based fuzzy inference system", "author": ["J.-S.R. Jang"], "venue": "IEEE Trans. Syst. Man Cybern., vol. 23, no. 3, pp. 665\u2013685, 1993.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Dynamic fuzzy neural networks\u2013a novel approach to function approximation", "author": ["S. Wu", "M.J. Er"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 30, no. 2, pp. 358\u2013364, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "A hybrid approach for design of stable adaptive fuzzy controllers employing lyapunov theory and particle swarm optimization", "author": ["K.D. Sharma", "A. Chatterjee", "A. Rakshit"], "venue": "IEEE Trans. Fuzzy Syst., vol. 17, no. 2, pp. 329\u2013342, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimization of the carpool service problem via a fuzzy-controlled genetic algorithm", "author": ["S.-C. Huang", "M.-K. Jiau", "C.-H. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 23, no. 5, pp. 1698\u20131712, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimization of type-2 fuzzy systems based on bio-inspired methods: a concise review", "author": ["O. Castillo", "P. Melin"], "venue": "Inf. Sci., vol. 205, pp. 1\u201319, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Revisiting evolutionary fuzzy systems: Taxonomy, applications, new trends and challenges", "author": ["A. Fern\u00e1ndez", "V. L\u00f3pez", "M.J. del Jesus", "F. Herrera"], "venue": "Knowledge-Based Syst., vol. 80, pp. 109\u2013121, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "An online self-constructing neural fuzzy inference network and its applications", "author": ["C.-F. Juang", "C.-T. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 6, no. 1, pp. 12\u201332, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "A self-evolving interval type-2 fuzzy neural network with online structure and parameter learning", "author": ["C.-F. Juang", "Y.-W. Tsao"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 6, pp. 1411\u20131424, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Reduced interval type-2 neural fuzzy system using weighted bound-set boundary operation for computation speedup and chip implementation", "author": ["C.-F. Juang", "K.-J. Juang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 3, pp. 477\u2013491, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "DENFIS: dynamic evolving neural-fuzzy inference system and its application for time-series prediction", "author": ["N.K. Kasabov", "Q. Song"], "venue": "IEEE Trans. Fuzzy Syst., vol. 10, no. 2, pp. 144\u2013154, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "SaFIN: A self-adaptive fuzzy inference network", "author": ["S.W. Tung", "C. Quek", "C. Guan"], "venue": "IEEE Trans. Neural Netw, vol. 22, no. 12, pp. 1928\u20131940, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1928}, {"title": "A mutually recurrent interval type-2 neural fuzzy system (MRIT2NFS) with self-evolving structure and parameters", "author": ["Y.-Y. Lin", "J.-Y. Chang", "N.R. Pal", "C.-T. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 3, pp. 492\u2013509, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A TSK-type-based self-evolving compensatory interval type-2 fuzzy neural network (TSCIT2FNN) and its applications", "author": ["Y.-Y. Lin", "J.-Y. Chang", "C.-T. Lin"], "venue": "IEEE Trans. Ind. Electron., vol. 61, no. 1, pp. 447\u2013459, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Simplified interval type-2 fuzzy neural networks", "author": ["Y.-Y. Lin", "S.-H. Liao", "J.-Y. Chang", "C.-T. Lin"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 5, pp. 959\u2013969, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "GT2FC: an online growing interval type-2 self-learning fuzzy classifier", "author": ["A. Bouchachia", "C. Vanaret"], "venue": "IEEE Trans. Fuzzy Syst., vol. 22, no. 4, pp. 999\u20131018, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "An evolving interval type-2 neurofuzzy inference system and its metacognitive sequential learning algorithm", "author": ["A.K. Das", "K. Subramanian", "S. Sundaram"], "venue": "IEEE Trans. Fuzzy Syst., vol. 23, no. 6, pp. 2080\u20132093, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical fuzzy control", "author": ["G. Raju", "J. Zhou", "R.A. Kisner"], "venue": "Int. J. Control, vol. 54, no. 5, pp. 1201\u20131216, 1991.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1991}, {"title": "High dimensional neurofuzzy systems: overcoming the curse of dimensionality", "author": ["M. Brown", "K. Bossley", "D. Mills", "C. Harris"], "venue": "Proc. 1995 IEEE Int. Fuzzy Syst., 1995. Int. Jt. Conf. of the 4th Int. Conf. Fuzzy Syst. and The 2nd Int. Fuzzy Eng. Symp., vol. 4, pp. 2139\u20132146.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY  47", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Analysis and design of hierarchical fuzzy systems", "author": ["L.-X. Wang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 7, no. 5, pp. 617\u2013624, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Hierarchical genetic fuzzy systems", "author": ["M.R. Delgado", "F. Von Zuben", "F. Gomide"], "venue": "Inf. Sci., vol. 136, no. 1, pp. 29\u201352, 2001.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Modeling of hierarchical fuzzy systems", "author": ["M.-L. Lee", "H.-Y. Chung", "F.-M. Yu"], "venue": "Fuzzy Sets Syst., vol. 138, no. 2, pp. 343\u2013361, 2003.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Approximation capabilities of hierarchical fuzzy systems", "author": ["X.-J. Zeng", "J.A. Keane"], "venue": "IEEE Trans. Fuzzy Syst., vol. 13, no. 5, pp. 659\u2013672, 2005.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "A review of the construction of hierarchical fuzzy systems", "author": ["V. Torra"], "venue": "Int. J. Intell. Syst., vol. 17, no. 5, pp. 531\u2013543, 2002.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "A class of hierarchical fuzzy systems with constraints on the fuzzy rules", "author": ["M.G. Joo", "J.S. Lee"], "venue": "IEEE Trans. Fuzzy Syst., vol. 13, no. 2, pp. 194\u2013203, 2005.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Hierarchical fuzzy rule based classification systems with genetic rule selection for imbalanced data-sets", "author": ["A. Fern\u00e1ndez", "M.J. del Jesus", "F. Herrera"], "venue": "Int. J. Approximate Reasoning, vol. 50, no. 3, pp. 561\u2013577, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive fuzzy hierarchical sliding-mode control for the trajectory tracking of uncertain underactuated nonlinear dynamic systems", "author": ["C.-L. Hwang", "C.-C. Chiang", "Y.-W. Yeh"], "venue": "IEEE Trans. Fuzzy Syst., vol. 22, no. 2, pp. 286\u2013299, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic design of hierarchical takagi\u2013sugeno type fuzzy systems using evolutionary algorithms", "author": ["Y. Chen", "B. Yang", "A. Abraham", "L. Peng"], "venue": "IEEE Trans. Fuzzy Syst., vol. 15, no. 3, pp. 385\u2013397, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic incremental program evolution", "author": ["R. Salustowicz", "J. Schmidhuber"], "venue": "Evol. Comput., vol. 5, no. 2, pp. 123\u2013141, 1997.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "Two-mode indirect adaptive control approach for the synchronization of uncertain chaotic systems by the use of a hierarchical interval type-2 fuzzy neural network", "author": ["A. Mohammadzadeh", "O. Kaynak", "M. Teshnehlab"], "venue": "IEEE Trans. Fuzzy Syst., vol. 22, no. 5, pp. 1301\u20131312, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiobjective genetic fuzzy systems: review and future research directions", "author": ["H. Ishibuchi"], "venue": "IEEE Int. Fuzzy Syst. Conf., 2007. FUZZ-IEEE 2007, pp. 1\u20136.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Single-objective and two-objective genetic algorithms for selecting linguistic rules for pattern classification problems", "author": ["H. Ishibuchi", "T. Murata", "I. T\u00fcrk\u015fen"], "venue": "Fuzzy Sets Syst., vol. 89, no. 2, pp. 135\u2013150, 1997.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1997}, {"title": "A multi-objective genetic algorithm for tuning and rule selection to obtain accurate and compact linguistic fuzzy rule-based systems", "author": ["R. Alcal\u00e1", "M.J. Gacto", "F. Herrera", "J. Alcal\u00e1-Fdez"], "venue": "Int. J. Uncertainty Fuzziness Knowledge Based Syst., vol. 15, no. 05, pp. 539\u2013557, 2007.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "A historical review of evolutionary learning methods for Mamdani-type fuzzy rule-based systems: Designing interpretable genetic fuzzy systems", "author": ["O. Cord\u00f3n"], "venue": "Int. J. Approximate Reasoning, vol. 52, no. 6, pp. 894\u2013913, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Designing fuzzy inference systems from data: an interpretability-oriented review", "author": ["S. Guillaume"], "venue": "IEEE Trans. Fuzzy Syst., vol. 9, no. 3, pp. 426\u2013443, 2001.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "Analysis of interpretability-accuracy trade-off of fuzzy systems by multiobjective fuzzy genetics-based machine learning", "author": ["H. Ishibuchi", "Y. Nojima"], "venue": "Int. J. Approximate Reasoning, vol. 44, no. 1, pp. 4\u201331, 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptation and application of multi-objective evolutionary algorithms for rule reduction and parameter tuning of fuzzy rule-based systems", "author": ["M.J. Gacto", "R. Alcal\u00e1", "F. Herrera"], "venue": "Soft Comput., vol. 13, no. 5, pp. 419\u2013436, 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "NMEEF-SD: non-dominated multiobjective evolutionary algorithm for extracting fuzzy rules in subgroup discovery", "author": ["C.J. Carmona", "P. Gonz\u00e1lez", "M.J. del Jesus", "F. Herrera"], "venue": "IEEE Trans. Fuzzy Syst., vol. 18, no. 5, pp. 958\u2013970, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiobjective optimization and comparison of nonsingleton type-1 and singleton interval type-2 fuzzy logic systems", "author": ["A.B. Cara", "C. Wagner", "H. Hagras", "H. Pomares", "I. Rojas"], "venue": "IEEE Trans. Fuzzy syst., vol. 21, no. 3, pp. 459\u2013476, 2013.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY  48", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-objective hierarchical genetic algorithm for interpretable fuzzy rule-based knowledge extraction", "author": ["H. Wang", "S. Kwong", "Y. Jin", "W. Wei", "K.-F. Man"], "venue": "Fuzzy Sets Syst., vol. 149, no. 1, pp. 149\u2013186, 2005.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic tuning of a fuzzy visual system using evolutionary algorithms: single-objective versus multiobjective approaches", "author": ["R. Munoz-Salinas", "E. Aguirre", "O. Cord\u00f3n", "M. Garc\u0131\u0301a-Silvente"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 2, pp. 485\u2013501, 2008.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2008}, {"title": "A multiobjective evolutionary approach to concurrently learn rule and data bases of linguistic fuzzy-rule-based systems", "author": ["R. Alcal\u00e1", "P. Ducange", "F. Herrera", "B. Lazzerini", "F. Marcelloni"], "venue": "IEEE Trans. Fuzzy Syst., vol. 17, no. 5, pp. 1106\u20131122, 2009.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning knowledge bases of multi-objective evolutionary fuzzy systems by simultaneously optimizing accuracy, complexity and partition integrity", "author": ["M. Antonelli", "P. Ducange", "B. Lazzerini", "F. Marcelloni"], "venue": "Soft Comput., vol. 15, no. 12, pp. 2335\u20132354, 2011.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Genetic training instance selection in multiobjective evolutionary fuzzy systems: A coevolutionary approach", "author": ["M. Antonelli", "P. Ducange", "F. Marcelloni"], "venue": "IEEE Trans. Fuzzy Syst., vol. 20, no. 2, pp. 276\u2013290, 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "A review of the application of multiobjective evolutionary fuzzy systems: Current status and further directions", "author": ["M. Fazzolari", "R. Alcala", "Y. Nojima", "H. Ishibuchi", "F. Herrera"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 1, pp. 45\u201365, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "A field guide to genetic programming", "author": ["R. Poli", "W.B. Langdon", "N.F. McPhee", "J.R. Koza"], "venue": "Lulu. com,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "Differential evolution algorithm with strategy adaptation for global numerical optimization", "author": ["A.K. Qin", "V.L. Huang", "P.N. Suganthan"], "venue": "IEEE Trans. Evol. Comput., vol. 13, no. 2, pp. 398\u2013417, 2009.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "On KM algorithms for solving type-2 fuzzy set problems", "author": ["J.M. Mendel"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 3, pp. 426\u2013446, 2013.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Time-series forecasting using flexible neural tree model", "author": ["Y. Chen", "B. Yang", "J. Dong", "A. Abraham"], "venue": "Inf. Sci., vol. 174, no. 3, pp. 219\u2013235, 2005.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2005}, {"title": "Evolutionary multi-objective optimization for simultaneous generation of signal-type and symbol-type representations", "author": ["Y. Jin", "B. Sendhoff", "E. K\u00f6rner"], "venue": "Evolutionary Multi-Criterion Optimization, ser. Lecture Notes in Computer Science, vol. 3410. Springer, 2005, pp. 752\u2013766.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2005}, {"title": "A fast elitist non-dominated sorting genetic algorithm for multiobjective optimization: NSGA-II", "author": ["K. Deb", "S. Agrawal", "A. Pratap", "T. Meyarivan"], "venue": "Parallel Problem Solving from Nature PPSN VI, ser. Lecture Notes in Computer Science. Springer, 2000, vol. 1917, pp. 849\u2013858.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2000}, {"title": "Introduction to Evolutionary Computing, 2nd ed", "author": ["A.E. Eiben", "J.E. Smith"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2003}, {"title": "Ensemble of heterogeneous flexible neural trees using multiobjective genetic programming", "author": ["V.K. Ojha", "A. Abraham", "V. Sn\u00e1\u0161el"], "venue": "Appl. Soft Comput., vol. 52, pp. 909\u2013924, 2017.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2017}, {"title": "A powerful and efficient algorithm for numerical function optimization: Artificial bee colony (ABC) algorithm", "author": ["D. Karaboga", "B. Basturk"], "venue": "J. Global Optim., vol. 39, no. 3, pp. 459\u2013471, 2007.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2007}, {"title": "Particle swarm optimization", "author": ["R. Poli", "J. Kennedy", "T. Blackwell"], "venue": "Swarm Intell., vol. 1, no. 1, pp. 33\u201357, 2007.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2007}, {"title": "Recent advances in differential evolution\u2013an updated survey", "author": ["S. Das", "S.S. Mullick", "P. Suganthan"], "venue": "Swarm and Evolutionary Computation, vol. 27, pp. 1\u201330, 2016.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical mathematical optimization: an introduction to basic optimization theory and classical and new gradient-based algorithms", "author": ["J. Snyman"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2005}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proc. IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1990}, {"title": "Kalman filtering and Neural Networks", "author": ["S.S. Haykin"], "venue": "Wiley Online Library,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2001}, {"title": "Finite markov chain analysis of genetic algorithms", "author": ["D.E. Goldberg", "P. Segrest"], "venue": "Proc. of the 2nd Int. Conf. on Genetic Algorithms and Their Appl. Hillsdale, NJ, USA: L. Erlbaum Associates Inc., 1987, pp. 1\u20138.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1987}, {"title": "Contractive mapping genetic algorithms and their convergence", "author": ["A. Szalas", "Z. Michalewicz"], "venue": "Dept. of Computer Science, University of North Carolina at Charlotte, Tech. Rep. Technical Report 006-1993, 1993.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1993}, {"title": "Sur les op\u00e9rations dans les ensembles abstraits et leur application aux \u00e9quations int\u00e9grales (in French)", "author": ["S. Banach"], "venue": "Fund. Math., vol. 3, no. 1, pp. 133\u2013181, 1922.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 1922}, {"title": "The evolution of evolvability in genetic programming", "author": ["L. Altenberg"], "venue": "Advances in Genetic Programming, K. E. Kinnear Jr., Ed., vol. 3. MIT Press, 1994, pp. 47\u201374.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 1994}, {"title": "On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition", "author": ["A.N. Kolmogorov"], "venue": "Transl. Amer. Math. Soc., vol. 28, no. 2, pp. 55\u201359, 1963.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1963}, {"title": "Modeling and convergence analysis of a continuous multi-objective differential evolution algorithm", "author": ["F. Xue", "A.C. Sanderson", "R.J. Graves"], "venue": "Evolutionary Computation, 2005. The 2005 IEEE Congress on, vol. 1. IEEE, 2005, pp. 228\u2013235.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2005}, {"title": "Theoretical analysis of differential evolution", "author": ["J. Zhang", "A.C. Sanderson"], "venue": "Adaptive Differential Evolution. Springer, 2009, pp. 15\u201338.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2009}, {"title": "Sufficient conditions for global convergence of differential evolution algorithm", "author": ["Z. Hu", "S. Xiong", "Q. Su", "X. Zhang"], "venue": "J. Appl. Math., vol. 2013, 2013.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Subsethood-product fuzzy neural inference system (SuPFuNIS)", "author": ["S. Paul", "S. Kumar"], "venue": "IEEE Trans. Neural Netw, vol. 13, no. 3, pp. 578\u2013599, 2002.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2002}, {"title": "Differential evolution\u2013a simple and efficient heuristic for global optimization over continuous spaces", "author": ["R. Storn", "K. Price"], "venue": "J. Glob. Optim., vol. 11, no. 4, pp. 341\u2013359, Dec 1997.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1997}, {"title": "Evolving fuzzy neural networks for supervised/unsupervised online knowledge-based learning", "author": ["N. Kasabov"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 31, no. 6, pp. 902\u2013918, 2001.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2001}, {"title": "An ART-based fuzzy adaptive learning control network", "author": ["C.-J. Lin", "C.-T. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 5, no. 4, pp. 477\u2013496, 1997.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 1997}, {"title": "Granular neural networks with evolutionary interval learning", "author": ["Y.-Q. Zhang", "B. Jin", "Y. Tang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 2, pp. 309\u2013319, 2008.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2008}, {"title": "HyFIS: adaptive neuro-fuzzy inference systems and their application to nonlinear dynamical systems", "author": ["J. Kim", "N. Kasabov"], "venue": "Neural Netw., vol. 12, no. 9, pp. 1301\u20131319, 1999.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 1999}, {"title": "Multilevel fuzzy relational systems: structure and identification", "author": ["J.-C. Duan", "F.-L. Chung"], "venue": "Soft Comput., vol. 6, no. 2, pp. 71\u201386, 2002.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2002}, {"title": "Radial basis function based adaptive fuzzy systems and their applications to system identification and prediction", "author": ["K.B. Cho", "B.H. Wang"], "venue": "Fuzzy Sets Syst., vol. 83, no. 3, pp. 325\u2013339, 1996.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 1996}, {"title": "Support vector learning mechanism for fuzzy rule-based modeling: a new approach", "author": ["J.-H. Chiang", "P.-Y. Hao"], "venue": "IEEE Trans. Fuzzy Syst., vol. 12, no. 1, pp. 1\u201312, 2004.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2004}, {"title": "eT2FIS: an evolving type-2 neural fuzzy inference system", "author": ["S.W. Tung", "C. Quek", "C. Guan"], "venue": "Inf. Sci., vol. 220, pp. 124\u2013148, 2013.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2013}, {"title": "An interval type-2 fuzzy-neural network with support-vector regression for noisy regression problems", "author": ["C.-F. Juang", "R.-B. Huang", "W.-Y. Cheng"], "venue": "IEEE Trans. Fuzzy Syst., vol. 18, no. 4, pp. 686\u2013699, 2010.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2010}, {"title": "An approach to online identification of Takagi-Sugeno fuzzy models", "author": ["P.P. Angelov", "D.P. Filev"], "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern., vol. 34, no. 1, pp. 484\u2013498, 2004.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY  50", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2004}, {"title": "Uncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions", "author": ["J.M. Mendel"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2001}, {"title": "Computing derivatives in interval type-2 fuzzy logic systems", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Fuzzy Syst., vol. 12, no. 1, pp. 84\u201398, 2004.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2004}, {"title": "Identification and control of dynamical systems using neural networks", "author": ["K.S. Narendra", "K. Parthasarathy"], "venue": "IEEE Trans. Neural Netw, vol. 1, no. 1, pp. 4\u201327, 1990.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 1990}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013, accessed on: 01.05.2016. [Online]. Available: http: //archive.ics.uci.edu/ml", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2013}, {"title": "A generalized concept for fuzzy rule interpolation", "author": ["P. Baranyi", "L.T. K\u00f3czy", "T.T.D. Gedeon"], "venue": "IEEE Trans. Fuzzy Syst., vol. 12, no. 6, pp. 820\u2013837, 2004.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2004}, {"title": "Fuzzy interpolative reasoning for sparse fuzzy-rule-based systems based on the areas of fuzzy sets", "author": ["Y.-C. Chang", "S.-M. Chen", "C.-J. Liau"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 5, pp. 1285\u20131301, 2008.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2008}, {"title": "Fuzzy interpolation and extrapolation: A practical approach", "author": ["Z. Huang", "Q. Shen"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 1, pp. 13\u201328, 2008.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2008}, {"title": "Weighted fuzzy rule interpolation based on GA-based weight-learning techniques", "author": ["S.-M. Chen", "Y.-C. Chang"], "venue": "IEEE Trans. Fuzzy Syst., vol. 19, no. 4, pp. 729\u2013744, 2011.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2011}, {"title": "Time Series Analysis, Forecasting and Control", "author": ["G.E.P. Box", "G.M. Jenkins"], "venue": null, "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1976}, {"title": "Heuristic modeling of macromolecule release from PLGA microspheres", "author": ["J. Szlek", "A. Pac\u0142awski", "R. Lau", "R. Jachowicz", "A. Mendyk"], "venue": "Int. J. Nanomed., vol. 8, no. 1, pp. 4601\u20134611, 2013.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2013}, {"title": "Dimensionality reduction, and function approximation of poly (lactic-co-glycolic acid) micro-and nanoparticle dissolution rate", "author": ["V.K. Ojha", "K. Jackowski", "A. Abraham", "V. Sn\u00e1\u0161el"], "venue": "Int. J. Nanomed., vol. 10, pp. 1119 \u2013 1129, 2015.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2015}, {"title": "Synthesis and characterization of PLGA nanoparticles", "author": ["C.E. Astete", "C.M. Sabliov"], "venue": "J. Biomater. Sci., Polym. Ed., vol. 17, no. 3, pp. 247\u2013289, 2006.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2006}, {"title": "Designing materials for biology and medicine", "author": ["R. Langer", "D.A. Tirrell"], "venue": "Nature, vol. 428, no. 6982, pp. 487\u2013492, 2004.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2004}, {"title": "Poly lactic-co-glycolic acid (PLGA) as biodegradable controlled drug delivery carrier", "author": ["H.K. Makadia", "S.J. Siegel"], "venue": "Polymers (Basel), vol. 3, no. 3, pp. 1377\u20131397, 2011.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble of heterogeneous flexible neural tree for the approximation and feature-selection of poly (lactic-co-glycolic acid) micro-and nanoparticle", "author": ["V.K. Ojha", "A. Abraham", "V. Snasel"], "venue": "Proc. the 2nd Int. Afro-European Conf. for Ind. Adv. AECIA 2015, 2016, pp. 155\u2013165.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The Takagi\u2013Sugeno\u2013Kang (TSK) is a widely used FIS model [1].", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Unlike the crisp output of a T1FS membership function (MF) [2], the output of a T2FS MF is fuzzy in nature [3].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Unlike the crisp output of a T1FS membership function (MF) [2], the output of a T2FS MF is fuzzy in nature [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Such nature of the T2FS MFs is advantageous in processing uncertain information more effectively than with T1FS MFs [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Hence, a T2FIS can overcome the inability of a T1FIS to fully handle or accommodate the linguistic and numerical uncertainties associated with a changing and dynamic environment [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "The interval T2FIS (IT2FIS) reduces the computational cost by employing a simplified T2FS, known as interval T2FS (IT2FS) [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "An IT2FS MF is bounded by a lower MF (LMF) and an upper MF (UMF), and the area between the LMF and UMF is called the footprint of uncertainty [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "Such a form of rule optimization is often achieved by mapping the rule\u2019s parameters onto a real-valued genetic vector, and it is known as the Michigan Approach [6].", "startOffset": 160, "endOffset": 163}, {"referenceID": 6, "context": "is often achieved by mapping the rules onto a binary-valued genetic vector [7], and it is known as the Pittsburgh Approach [8].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "is often achieved by mapping the rules onto a binary-valued genetic vector [7], and it is known as the Pittsburgh Approach [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "However, FIS optimization is not limited only to its mapping onto the genetic vector, but a structural/network-like implementation of FIS is often performed [9].", "startOffset": 157, "endOffset": 160}, {"referenceID": 9, "context": "Additionally, TSK-based hierarchical self-organizing learning dynamics have also been proposed [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "Moreover, several researchers have focused on the FIS and neural network (NN) integration and its parameter optimization using various learning methods including gradient-decent and the metaheuristic algorithms [11]\u2013[14].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "Moreover, several researchers have focused on the FIS and neural network (NN) integration and its parameter optimization using various learning methods including gradient-decent and the metaheuristic algorithms [11]\u2013[14].", "startOffset": 216, "endOffset": 220}, {"referenceID": 14, "context": "[15], is a six layered network structure whose optimization begins with no rule and then rules are incrementally added during the learning process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Later, in [16], SONFIN\u2019s concept was extended for the construction of T2FIS, where a self-evolving IT2FIS (SEIT2FNN) that implements a TSK-type FIS model was proposed, and the parameters of the evolved structure were tuned by using the Kalman-filtering algorithm.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "Additionally, a simplified type-reduction process for SEIT2FNN was proposed in [17].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Like SONFIN, in [18], a TSK-type FIS model, called a dynamic evolving neural-fuzzy inference system (DENFIS), was proposed, which evolved incrementally by choosing active rules from a set of rules and employed an evolving clustering method to partition the input space and the least-square estimator to optimize its parameters.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "[19] proposed a self-adaptive fuzzy inference network (SaFIN) that applied a categorical learning induced partitioning algorithm to eliminate two limitations: 1) the need for predefined numbers of fuzzy clusters and 2) the stability\u2013plasticity trade-off that addresses the difficulty in finding a balance between past knowledge and current knowledge during the learning process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In [20], to improve the efficiency of IT2FIS, a mutually recurrent interval type-2 neural fuzzy system (MRIT2NFS) was proposed which used weighted feedback loops in the antecedent parts of the formed rules and applied gradient-decent learning and a Kalman-filter algorithm to tune the recurrent weights and the rules\u2019 parameters, respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [21], a self-evolving T2FIS model", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Further, a simplified interval type-2 fuzzy NN with a simplified type-reduction process (SIT2FIS) was proposed in [22], and a growing online self-learning IT2FIS that used the dynamics of a growing Gaussian mixture model was proposed in [23].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "Further, a simplified interval type-2 fuzzy NN with a simplified type-reduction process (SIT2FIS) was proposed in [22], and a growing online self-learning IT2FIS that used the dynamics of a growing Gaussian mixture model was proposed in [23].", "startOffset": 237, "endOffset": 241}, {"referenceID": 23, "context": "Recently, in [24], a meta-cognitive interval type-2 neuro FIS (McIT2FIS) was proposed, which employs a self-regulatory metacognitive system that extracts the knowledge contained in minimal samples by accepting or discarding data samples based on sample\u2019s contribution to knowledge.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "Contrary to this, a hierarchical FIS (HFIS) constructs an FIS by using a hierarchical arrangement of several low-dimensional fuzzy subsystems [25].", "startOffset": 142, "endOffset": 146}, {"referenceID": 25, "context": "Moreover, HFIS design overcomes the curse of dimensionality [26], and it possesses a universal approximation ability [27]\u2013[30].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "Moreover, HFIS design overcomes the curse of dimensionality [26], and it possesses a universal approximation ability [27]\u2013[30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 29, "context": "Moreover, HFIS design overcomes the curse of dimensionality [26], and it possesses a universal approximation ability [27]\u2013[30].", "startOffset": 122, "endOffset": 126}, {"referenceID": 30, "context": "[31] summarized the contributions where the expert\u2019s role in the HFIS design process was minimized/eliminated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "For example, in [32], HFIS was realized as a feedforward network like structure in which the output of the previous layer\u2019s subsystem was only fed to the consequent part of the next layer, and so on.", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "Similarly, in [33], a two-layered HFIS was developed, where, for each layer, the knowledge bases (KB) were generated by linguistics rule generation method and the KB rules were selected by genetic algorithm (GA).", "startOffset": 14, "endOffset": 18}, {"referenceID": 33, "context": "In [34], an adaptive fuzzy hierarchical sliding-mode control method was proposed, which was an arrangement of many subsystems, and the top layer accommodated all the subsystems\u2019 outputs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "Moreover, in [35], to optimize the structure of a hierarchical arrangement of low-dimensional TSK-type FISs, probabilistic incremental program evolution [36] was employed.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "Moreover, in [35], to optimize the structure of a hierarchical arrangement of low-dimensional TSK-type FISs, probabilistic incremental program evolution [36] was employed.", "startOffset": 153, "endOffset": 157}, {"referenceID": 4, "context": "Similarly, the importance of the hierarchical arrangements of the low-dimensional T2FSs is explained in [5], [37].", "startOffset": 104, "endOffset": 107}, {"referenceID": 36, "context": "Similarly, the importance of the hierarchical arrangements of the low-dimensional T2FSs is explained in [5], [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 37, "context": ", self-organizing fuzzy NN and HFIS models), multiobjective optimization is inherent since accuracy maximization and complexity minimization are two desirable objectives [38].", "startOffset": 170, "endOffset": 174}, {"referenceID": 38, "context": "Hence, to make trade-offs between interpretability and accuracy, or, in other words, to make trade-offs between approximation error minimization and complexity minimization, a multiobjective orientation of FIS optimization can be used [39]\u2013 [41].", "startOffset": 235, "endOffset": 239}, {"referenceID": 40, "context": "Hence, to make trade-offs between interpretability and accuracy, or, in other words, to make trade-offs between approximation error minimization and complexity minimization, a multiobjective orientation of FIS optimization can be used [39]\u2013 [41].", "startOffset": 241, "endOffset": 245}, {"referenceID": 40, "context": "[41], [42].", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[41], [42].", "startOffset": 6, "endOffset": 10}, {"referenceID": 42, "context": "[43]\u2013[46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[43]\u2013[46].", "startOffset": 5, "endOffset": 9}, {"referenceID": 46, "context": "Similarly, in [47]\u2013[50], simultaneous learning of KB was proposed, which included feature selection, rule complexity minimization together with approximation error minimization, etc.", "startOffset": 14, "endOffset": 18}, {"referenceID": 49, "context": "Similarly, in [47]\u2013[50], simultaneous learning of KB was proposed, which included feature selection, rule complexity minimization together with approximation error minimization, etc.", "startOffset": 19, "endOffset": 23}, {"referenceID": 50, "context": "Moreover, in [51], a co-evolutionary approach that aimed at combining a multiobjective approach with a single objective approach was presented where, at first, a multiobjective GA determined a Pareto-optimal solution by finding a trade-off between accuracy and rule complexity.", "startOffset": 13, "endOffset": 17}, {"referenceID": 51, "context": "A summary of research works focused on multiobjective optimization of FIS is provided in [52].", "startOffset": 89, "endOffset": 93}, {"referenceID": 52, "context": "Unlike the self-organizing paradigm that has a network-like structure and uses a clustering algorithm for partitioning of input space, the proposed HFIT constructs a tree-like structure and uses the dynamics of the evolutionary algorithm for partitioning input space [53].", "startOffset": 267, "endOffset": 271}, {"referenceID": 53, "context": "The parameter tuning of the HFIT was performed by the differential evolution (DE) algorithm [54], which is a metaheuristic algorithm inspired by the dynamics of the evolutionary process.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "Hence, they are useful in finding the appropriate parameter values for an FIS [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": "In the construction of type-2 HFITs, the type-reduction algorithm of the Karnik-Mendel method described in [4] was used with an improvement in its termination criteria.", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "A TSK-type FIS is governed by the IF\u2013THEN rules of the form [1]:", "startOffset": 60, "endOffset": 63}, {"referenceID": 54, "context": "A T2FS \u00c3 is characterized by a 3-dimensional (3-D) MF [55].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Hence, in a universal set X , a T2FS \u00c3 has the form: \u00c3 = {((x, u) , \u03bc\u00c3 (x, u)) |\u2200x \u2208 X, \u2200u \u2208 [0, 1]} (6)", "startOffset": 93, "endOffset": 99}, {"referenceID": 3, "context": "In this work, the LMF was defined as [4]:", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "and the UMF was defined as [4]:", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "In this work, the center of set type-reducer ycos, prescribed in [4], was used:", "startOffset": 65, "endOffset": 68}, {"referenceID": 55, "context": "[56], which has two", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Thereafter, the type-reduction of the node is performed as described in [4], where the left and right intervals y l and y 1 r are computed as per (14) and (15).", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "During type-reduction [4], an early stopping mechanism was adopted to reduce computation time.", "startOffset": 22, "endOffset": 25}, {"referenceID": 56, "context": ", the number of parameters c(w) in the model) [57].", "startOffset": 46, "endOffset": 50}, {"referenceID": 57, "context": "Hence, a Pareto-based multiobjective optimization can be applied to obtained a Pareto set of nondominated solutions, in which no one objective function can be improved without a simultaneous detriment to at least one of the other objectives of the solution [58] Therefore, an HFIT that offers the lowest approximation error and simplest structure is the most desirable one.", "startOffset": 257, "endOffset": 261}, {"referenceID": 57, "context": "The proposed MOGP acquires the nondominated sorting algorithm [58] for computing Paretooptimal solutions from an initial population of fuzzy inference trees.", "startOffset": 62, "endOffset": 66}, {"referenceID": 57, "context": "Diversity in population was maintained by measuring the crowding distance among the individuals [58].", "startOffset": 96, "endOffset": 100}, {"referenceID": 58, "context": "A detailed description of the crossover operation in genetic programming is available in [59], [60].", "startOffset": 89, "endOffset": 93}, {"referenceID": 59, "context": "A detailed description of the crossover operation in genetic programming is available in [59], [60].", "startOffset": 95, "endOffset": 99}, {"referenceID": 58, "context": "5) Mutation: The mutation operators used in HFIT are as follows [59], [60]: a) Replacing a randomly selected terminal xi \u2208 T with a newly generated terminal xj \u2208 T for j 6= i.", "startOffset": 64, "endOffset": 68}, {"referenceID": 59, "context": "5) Mutation: The mutation operators used in HFIT are as follows [59], [60]: a) Replacing a randomly selected terminal xi \u2208 T with a newly generated terminal xj \u2208 T for j 6= i.", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "The Gaussian means m1 and m2 for type-2 Gaussian MF (7) were defined as: m1 = m+ \u03bb\u03c3 and m2 = m\u2212 \u03bb\u03c3, where \u03bb \u2208 [0, 1] is a random variable taken from uniform distribution and m is the center of the Gaussian means m1 and m2 taken from [0, 1].", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "The Gaussian means m1 and m2 for type-2 Gaussian MF (7) were defined as: m1 = m+ \u03bb\u03c3 and m2 = m\u2212 \u03bb\u03c3, where \u03bb \u2208 [0, 1] is a random variable taken from uniform distribution and m is the center of the Gaussian means m1 and m2 taken from [0, 1].", "startOffset": 233, "endOffset": 239}, {"referenceID": 0, "context": "Similarly, the variance \u03c3 of type-2 Gaussian MF (7) was taken from [0, 1].", "startOffset": 67, "endOffset": 73}, {"referenceID": 58, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 91, "endOffset": 95}, {"referenceID": 58, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 116, "endOffset": 120}, {"referenceID": 60, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 144, "endOffset": 148}, {"referenceID": 61, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 154, "endOffset": 158}, {"referenceID": 62, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 163, "endOffset": 167}, {"referenceID": 63, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 64, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 217, "endOffset": 221}, {"referenceID": 65, "context": "Now, to optimize parameter vector w, a parameter optimizer can be used: genetic algorithms [59], evolution strategy [59], artificial bee colony [61], PSO [62], DE [63], gradient-based algorithms [64], backpropagation [65], Klaman-filter [66], etc.", "startOffset": 237, "endOffset": 241}, {"referenceID": 53, "context": "In this work, the differential evolution (DE) version \u201cDE/rand-to-best/1/bin\u201d [54] was used, which is a metaheuristic algorithm that uses a crossover operator inspired by the dynamics of \u201cnatural selection.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "where rj \u2208 [0, 1] is a uniform random sample, cr \u2208 [0, 1] is the crossover rate, and F \u2208 [0, 2] is the differential weight.", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "where rj \u2208 [0, 1] is a uniform random sample, cr \u2208 [0, 1] is the crossover rate, and F \u2208 [0, 2] is the differential weight.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "where rj \u2208 [0, 1] is a uniform random sample, cr \u2208 [0, 1] is the crossover rate, and F \u2208 [0, 2] is the differential weight.", "startOffset": 89, "endOffset": 95}, {"referenceID": 66, "context": "Initially, Goldberg and Sergret [67] showed convergence property of GA using a finite Markov chain analysis, where they considered GA with a finite population and recombination and mutation operators.", "startOffset": 32, "endOffset": 36}, {"referenceID": 67, "context": "A different viewpoint of MOGP convergence (EAs in general) can be referred to as by using Banach fixpoint theorem described in [68].", "startOffset": 127, "endOffset": 131}, {"referenceID": 68, "context": "Banach fixpoint theorem [69] states that on a metric space a constructive mapping f has a unique fixpoint, i.", "startOffset": 24, "endOffset": 28}, {"referenceID": 69, "context": "Altenberg [71] showed that by maintaining genetic operators, such as selection, crossover, and mutation, the evolvability of genetic programming can be increased.", "startOffset": 10, "endOffset": 14}, {"referenceID": 69, "context": "Additionally, Altenberg [71] analyzed the probability of a population containing fitter individuals than the previous population and offered the subsequent proof.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "Theoretical studies of special cases of the hierarchical fuzzy systems are provided in [27], [32].", "startOffset": 87, "endOffset": 91}, {"referenceID": 31, "context": "Theoretical studies of special cases of the hierarchical fuzzy systems are provided in [27], [32].", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "In HFIT, not only a cascaded hierarchy of fuzzy system (a fuzzy subsystem takes input only from its previous fuzzy subsystem [27]) can be produced, but a general hierarchical fuzzy system, in which a fuzzy subsystem can take inputs from any previous layer fuzzy subsystem, can be produced.", "startOffset": 125, "endOffset": 129}, {"referenceID": 29, "context": "A hierarchical fuzzy system described in [30] resembles the hierarchical fuzzy system produced by HFIT.", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "To show the approximation capability of the proposed HFIT, it requires coming to the conclusion that the proposed HFIT is analogous to the hierarchical fuzzy system described by Zeng and Keane [30].", "startOffset": 193, "endOffset": 197}, {"referenceID": 29, "context": "Let\u2019s perform an analogy between the proposed HFIT and the concept of a natural hierarchical fuzzy system described by Zeng and Keane [30].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": ", the individual functions can be decomposed [30].", "startOffset": 45, "endOffset": 49}, {"referenceID": 70, "context": "Now, from Kolmogorov\u2019s Theorem [72], the following can be stated: Any continuous function N(x1, .", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "Proof of Theorem 2 can be found in [30].", "startOffset": 35, "endOffset": 39}, {"referenceID": 71, "context": "Convergence property and efficiency of DE is well studied [73], [74].", "startOffset": 58, "endOffset": 62}, {"referenceID": 72, "context": "Convergence property and efficiency of DE is well studied [73], [74].", "startOffset": 64, "endOffset": 68}, {"referenceID": 73, "context": "A probabilistic viewpoint of DE convergence followed by a description of global convergence condition for DE is described in [75].", "startOffset": 125, "endOffset": 129}, {"referenceID": 72, "context": "Similarly, Zhang and Sanderson [74] studied the various property of DE, such as mutation, crossover and recombination operators that influence the DE convergence.", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "However, the overlapping of the membership function of the fuzzy sets is another common problem with clustering based input space partitioning [19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 74, "context": "In [76], authors pointed out four different cases of membership function\u2019s overlapping and proposed subsethood method to transmit the overlapping information to the rules layer.", "startOffset": 3, "endOffset": 7}, {"referenceID": 75, "context": ", DE) [77].", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "Algorithm training parameter Value Maximum depth (layers) of a tree 4 Maximum inputs to an FIS node 4 Membership function search range [0,1] GP population 50 CP mutation probability pm 0.", "startOffset": 135, "endOffset": 140}, {"referenceID": 17, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 101, "endOffset": 105}, {"referenceID": 76, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 172, "endOffset": 176}, {"referenceID": 77, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 248, "endOffset": 252}, {"referenceID": 78, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 336, "endOffset": 340}, {"referenceID": 34, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 395, "endOffset": 399}, {"referenceID": 79, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 474, "endOffset": 478}, {"referenceID": 80, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 559, "endOffset": 563}, {"referenceID": 81, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 658, "endOffset": 662}, {"referenceID": 18, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 750, "endOffset": 754}, {"referenceID": 14, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 835, "endOffset": 839}, {"referenceID": 74, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 928, "endOffset": 932}, {"referenceID": 82, "context": "Ty pe \u20131 DENFIS [18] Dynamic evolving neural-fuzzy inference system TSK Least-square estimator D-FNN [10] Dynamic fuzzy neural networks TSK Backpropagation algorithm EFuNN [78] Evolving fuzzy neural networks Mamdani Widrow\u2013Hoff least square FALCON [79] ART-based fuzzy adaptive learning control network \u2212\u2212 Backpropagation algorithm GNN [80] Granular neural networks \u2212\u2212 Genetic algorithm H-TS-FS [35] Hierarchical Tukagi\u2013Sugno fuzzy system TSK Evolutionary programming HyFIS [81] Hybrid neural fuzzy inference system \u2212\u2212 Gradient descent learning IFRS and AFRS [82] Incremental and aggregated fuzzy relational systems Mamdani Backpropagation algorithm RBF-AFA [83] Radial basis function based adaptive fuzzy systems TSK Gradient descent learning SaFIN [19] Self-adaptive fuzzy inference network Mamdani Levenberg-Marquardt method SONFIN [15] Self-constructing neural fuzzy inference network TSK Backpropagation algorithm SuPFuNIS [76] Subsethood-product fuzzy neural inference system \u2212\u2212 Gradient descent learning SVR-FM [84] Support-vector regression fuzzy model TSK Support vector regression", "startOffset": 1018, "endOffset": 1022}, {"referenceID": 83, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 16, "endOffset": 20}, {"referenceID": 84, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 222, "endOffset": 226}, {"referenceID": 85, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 334, "endOffset": 338}, {"referenceID": 16, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 436, "endOffset": 440}, {"referenceID": 16, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 514, "endOffset": 518}, {"referenceID": 15, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 596, "endOffset": 600}, {"referenceID": 21, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 661, "endOffset": 665}, {"referenceID": 86, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 751, "endOffset": 755}, {"referenceID": 87, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 826, "endOffset": 830}, {"referenceID": 20, "context": "Ty pe \u20132 eT2FIS [85] Evolving type-2 neural fuzzy inference system Mamdani Gradient descent learning IT2FNN-SVR-N/F [86] IT2fuzzy-NN-support-vector regression-fuzzy and numeric TSK Support vector regression McIT2FIS-UM/US [24] Metacognitive interval type-2 neuro-fuzzy inference system TSK Gradient descent learning NNT1FW and NNT2FW [87] Type-1 and type-2 fuzzy backpropagation neural networks TSK Backpropagation algorithm RIT2FNS-WB [17] Reduced IT2NFS-weighted bound-set TSK Gradient descent learning MRIT2NFS [17] Reduced IT2NFS-weighted bound-set Mamdani Gradient descent learning SEIT2FNN [16] Self-evolving IT2FIS TSK Kalman filtering algorithm SIT2FNN [22] Simplified Interval Type-2 Fuzzy Neural Networks TSK gradient descent learning T2FLS [88] Interval type-2 fuzzy logic system (TSK and singleton) TSK \u2212\u2212 T2FLS-G [89] Gradient-descent based IT2FIS tuning TSK Derivation-based learning TSCIT2FNN [21] Compensatory interval type-2 fuzzy neural network TSK Kalman filter algorithm", "startOffset": 908, "endOffset": 912}, {"referenceID": 15, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 128, "endOffset": 132}, {"referenceID": 20, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 140, "endOffset": 144}, {"referenceID": 84, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 146, "endOffset": 150}, {"referenceID": 88, "context": "The significance of this problem is evident from its usage in the literature for the validation of the approximation algorithms [16], [21], [24], [86], [90].", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": ", 400 as mentioned in [17].", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "For the performance comparisons, the SaFIN result was collected from [19], and FALCON and SONFIN from [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "For the performance comparisons, the SaFIN result was collected from [19], and FALCON and SONFIN from [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22].", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "The results of T2FLS (singleton) and T2FLS (TSK) were obtained from [16]; FT2FNN, TSCIT2FNN, T2TSKFNS, and T2FNN from [21]; SEIT2FNN, MRI2NFS, RIT2NFS-WB, and T2FLS-G from [17]; and SIT2FNN from [22].", "startOffset": 195, "endOffset": 199}, {"referenceID": 84, "context": "In this example, the objective was to predict x(k) using the past outputs of the time series as mentioned in [86].", "startOffset": 109, "endOffset": 113}, {"referenceID": 84, "context": "2 [86].", "startOffset": 2, "endOffset": 6}, {"referenceID": 84, "context": "From the generated clean patterns, as mentioned in [86], the first 500 patterns (clean training set) were used for training purposes and the second 500 patterns (clean test set) were used for test purposes.", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 85, "context": "The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "The results of IFRS, AFRS, H-TS-FS1, and H-TS-FS2 were collected from [35]; RBF-AFA, HyFIS, D-FNN, and SuPFuNIS from [16]; NNT1FW and NNT2FW from [87]; and T2FLS (Singleton), T2FLS (TSK), and SEIT2FN from [16].", "startOffset": 205, "endOffset": 209}, {"referenceID": 84, "context": "3 to the original data x(k) as described in [86].", "startOffset": 44, "endOffset": 48}, {"referenceID": 84, "context": "Table V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 83, "context": "Table V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22].", "startOffset": 160, "endOffset": 164}, {"referenceID": 84, "context": "Table V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22].", "startOffset": 219, "endOffset": 223}, {"referenceID": 21, "context": "Table V describes the comparisons between the results of the algorithms, where the results of SONFIN and SVR-FM were collected from [86], DENFIS and EFuNN from [85], SEIT2FNN, T2FLS-G, IT2FNN-SVR(N), IT2FNN-SVR(F) from [86], and eT2FIS from [22].", "startOffset": 241, "endOffset": 245}, {"referenceID": 89, "context": "The MPG dataset was collected from the UCI machine learning repository [91].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "This dataset has 392 samples, each of which has six input variables, but in this example, as mentioned in [24], three variables (x1 = weight, x2 = acceleration, and x3 = model year) were selected.", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "For the comparisons, the T1FLS result was collected from [17] and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 57, "endOffset": 61}, {"referenceID": 23, "context": "For the comparisons, the T1FLS result was collected from [17] and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 152, "endOffset": 156}, {"referenceID": 89, "context": "The Abalone dataset was collected from the UCI machine learning repository [91].", "startOffset": 75, "endOffset": 79}, {"referenceID": 16, "context": "For the comparisons, the results of General, HS, CCL, and Chen&Cheng were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "For the comparisons, the results of General, HS, CCL, and Chen&Cheng were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 185, "endOffset": 189}, {"referenceID": 90, "context": "The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS.", "startOffset": 23, "endOffset": 27}, {"referenceID": 91, "context": "The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS.", "startOffset": 33, "endOffset": 37}, {"referenceID": 92, "context": "The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS.", "startOffset": 42, "endOffset": 46}, {"referenceID": 93, "context": "The algorithms General [92], CCL [93], HS [94], and WFRI-GA [95] were fuzzy interpolate reasoning methods, where WFRI-GA was based on the genetic algorithm and the algorithm \u2018General\u2019 implemented the Mamdani type FIS.", "startOffset": 60, "endOffset": 64}, {"referenceID": 94, "context": "In this example, the Box and Jenkins gas furnace dataset that was taken from [96], which has 296 data samples.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "For the training of the proposed models, as mentioned in [24],", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "To compare the performance of the algorithms, the results of T1-NFS and GNN were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "To compare the performance of the algorithms, the results of T1-NFS and GNN were collected from [17], and the results of SEIT2FNN, RIT2NFS-WB, McIT2FIS-UM, and McIT2FIS-US were collected from [24].", "startOffset": 192, "endOffset": 196}, {"referenceID": 16, "context": "66 minutes for the training [17].", "startOffset": 28, "endOffset": 32}, {"referenceID": 95, "context": "As per the dataset provided in [97], [98], this problem has 747 samples and a total of 300 input features, which influence the PLGA protein particle\u2019s dissolution rate [99].", "startOffset": 31, "endOffset": 35}, {"referenceID": 96, "context": "As per the dataset provided in [97], [98], this problem has 747 samples and a total of 300 input features, which influence the PLGA protein particle\u2019s dissolution rate [99].", "startOffset": 37, "endOffset": 41}, {"referenceID": 97, "context": "As per the dataset provided in [97], [98], this problem has 747 samples and a total of 300 input features, which influence the PLGA protein particle\u2019s dissolution rate [99].", "startOffset": 168, "endOffset": 172}, {"referenceID": 98, "context": "The PLGA dissolution profile prediction is a significant problem since it plays a crucial role in the medical application and toxicity evaluation of PLGA-based microparticles dosages [100].", "startOffset": 183, "endOffset": 188}, {"referenceID": 99, "context": "It is also used as a filler, as an excipient, and as an active pharmaceutical ingredient because it acts as a catalyst for drug absorption/dissolution/solubility [101].", "startOffset": 162, "endOffset": 167}, {"referenceID": 95, "context": "of features MLP [97] 14.", "startOffset": 16, "endOffset": 20}, {"referenceID": 100, "context": "3 17 HFIT [102] 13.", "startOffset": 10, "endOffset": 15}, {"referenceID": 96, "context": "2 15 REP Tree [98] 13.", "startOffset": 14, "endOffset": 18}, {"referenceID": 96, "context": "3 15 GPR [98] 14.", "startOffset": 9, "endOffset": 13}, {"referenceID": 96, "context": "9 15 MLP [98] 15.", "startOffset": 9, "endOffset": 13}, {"referenceID": 95, "context": "2 15 MLP [97] 15.", "startOffset": 9, "endOffset": 13}], "year": 2017, "abstractText": "This paper proposes a design of hierarchical fuzzy inference tree (HFIT). An HFIT produces an optimum tree-like structure. Specifically, a natural hierarchical structure that accommodates simplicity by combining several low-dimensional fuzzy inference systems (FISs). Such a natural hierarchical structure provides a high degree of approximation accuracy. The construction of HFIT takes place in two phases. Firstly, a nondominated sorting based multiobjective genetic programming (MOGP) is applied to obtain a simple tree structure (low model\u2019s complexity) with a high accuracy. Secondly, the differential evolution algorithm is applied to optimize the obtained tree\u2019s parameters. In the obtained tree, each node has a different input\u2019s combination, where the evolutionary process governs the input\u2019s combination. Hence, HFIT nodes are heterogeneous in nature, which leads to a high diversity among the rules generated by the HFIT. Additionally, the HFIT provides an automatic feature selection because it uses MOGP for the tree\u2019s structural optimization that accept inputs only relevant to the knowledge contained in data. The HFIT was studied in the context of both type-1 and type-2 FISs, and its performance was evaluated through six application problems. Moreover, the proposed multiobjective HFIT was compared both theoretically and empirically with recently proposed FISs methods from the literature, such as McIT2FIS, TSCIT2FNN, SIT2FNN, RIT2FNS-WB, eT2FIS, MRIT2NFS, IT2FNN-SVR, etc. From the obtained results, it was found that the HFIT provided less complex and highly accurate models compared to the models produced by most of the other methods. Hence, the proposed HFIT is an efficient and competitive alternative to the other FISs for function approximation and feature selection. V K Ojha is with the Chair of Information Architecture, ETH Zurich, Zurich, Switzerland e-mail: ojha@arch.ethz.ch V Sn\u00e1\u0161el is with the Dept. of Computer Science, Technical University of Ostrava, Czech Republic, e-mail: vaclav.snasel@vsb.cz A Abraham is with Machine Intelligence Research Labs, Washington, USA, e-mail: ajith.abraham@ieee.org This work was supported by the IPROCOM Marie Curie initial training network, funded through the People Programme (Marie Curie Actions) of the European Union\u2019s Seventh Framework Programme FP7/2007-2013/ under REA Grant Agreement No. 316555. Manuscript received Month xx, yyyy; revised Month xx, yyyy. ar X iv :1 70 5. 05 76 9v 1 [ cs .A I] 1 6 M ay 2 01 7 JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, MONTH YYYY 2", "creator": "LaTeX with hyperref package"}}}