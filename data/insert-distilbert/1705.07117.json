{"id": "1705.07117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Deep Learning as a Tool to Predict Flow Patterns in Two-Phase Flow", "abstract": "in order to better model complex theoretical real - motion world data such as multiphase flow, one approach is to develop pattern motif recognition techniques and robust features approaches that capture the relevant information. so in this paper, we use deep blended learning methods, and in particular employ the multilayer perceptron, to build an algorithm that can predict flow pattern in differential twophase flow from fluid properties and nonlinear pipe conditions. the preliminary results show nearly excellent performance when compared with classical methods of flow pattern prediction.", "histories": [["v1", "Fri, 19 May 2017 03:11:30 GMT  (13kb)", "http://arxiv.org/abs/1705.07117v1", "Part of DM4OG 2017 proceedings (arXiv:1705.03451)"]], "COMMENTS": "Part of DM4OG 2017 proceedings (arXiv:1705.03451)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammadmehdi ezzatabadipour", "parth singh", "melvin d robinson", "pablo guillen-rondon", "carlos torres"], "accepted": false, "id": "1705.07117"}, "pdf": {"name": "1705.07117.pdf", "metadata": {"source": "CRF", "title": "Deep Learning as a Tool to Predict Flow Patterns in Two-Phase Flow", "authors": ["Mohammadmehdi Ezzatabadipour", "Parth Singh"], "emails": ["mezzatab@central.uh.edu", "psingh@central.uh.edu", "mrobinson@uttyler.edu", "pgrondon@uh.edu", "ctorres@ula.ve"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n07 11\n7v 1\n[ cs\n.L G\n] 1\n9 M"}, {"heading": "1 Introduction", "text": "The term flow pattern refers to the spatial distribution of the phases, which occur during gas-liquid two-phase flow in pipes. When gases and liquids flow simultaneously in a pipe, the two phases can distribute themselves in a variety of flow configurations. The flow configurations differ from each other in the interface distribution, resulting in different flow characteristics.\nDetermination of flow patterns is a fundamental problem in two-phase flow analysis. Indeed all the design variables, namely, phase velocity, pressure drop, liquid holdup, heat and mass transfer coefficients, residence time distribution, and rate of chemical reaction, are all strongly dependent on the existing flow pattern. Thus, knowledge of the existing flow pattern can help the industry carry out a better design of two-phase flow systems [PTM+12].\nThere is not agreement in the number of flow patterns in two-phase flow due to overlapping and characterization subjectivity, especially at the transition zones. Shoham [Sho06] attempted to summarize the main flow patterns for all inclination angles as Dispersed bubble, Bubble, Slug, Churn, Annular and Stratified (smooth and wavy). The flow patterns depend on parameters such as pipe inclination and diameter, physical properties of the phases, and their superficial velocities. There are many models and approaches used to predict the two-phase flow patterns in pipes based on mechanistic modeling or dimensionless analysis [SB12].\nCopyright c\u00a9 2017 by the paper\u2019s authors. This volume is published and copyrighted by its editors.\nIn: A. Jorge, G. Larrazabal, P. Guillen, R.L. Lopes (eds.): Proceedings of the Workshop on Data Mining for Oil and Gas (DM4OG), Houston, Texas, USA, April 29th, 2017 (arXiv:1705.03451).\nHidden layer Input layer\nOutput layer\nArtificial neural networks have been used to identify and predict flow patterns [ANEAS16]. Because of the increase in computing power researchers have developed deep learning techniques which originated from artificial neural networks. Multilayer perceptron with many hidden layers is a good example of the models with deep architectures [LBH15]. Deep learning techniques have been applied to a wide variety of problems in recent years [LKL14, YD11, Yus15]. In many of these applications, algorithms based on deep learning have surpassed the previous state-of-art performance. At the heart of all deep learning algorithms is the domain independent idea of using hierarchical layers of learned abstraction to efficiently accomplish a high-level task. Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.\nThe rest of this paper is organized as follows: Section 2 explains the concept of deep learning, followed by a description of the two-phase flow patterns data base under study, and the strategy of supervised classification used in machine learning. Section 3 present and discusses the results of the deep learning model developed in this work and finally Section 4 presents the conclusions."}, {"heading": "2 Materials and Methods", "text": "The concept of deep learning originated from artificial neural network research. Unlike the neural networks of the past, modern deep learning methods have cracked the code for training stability, generalization and scale on big data. They are often the algorithm of choice for highest predictive accuracy, as they perform well in a number of diverse problems. There are several types of learning machines for deep learning. In our research we use a feedforward neural network known as a multilayer perceptron (MLP), a feed-forward neural network consisting of an input layer, one or more hidden layers and an output layer. Each layer is comprised of small units known as neurons. Neurons in the input layer receive the input signals X and distribute them forward to the rest of the network. In subsequent layers, each neuron receives a signal, which is a weighted sum of the outputs of the nodes in the previous layer and a constant term called a bias. Inside each neuron, a nonlinear activation function transforms this input and passes it to the next layer. The advantage of such a network is that we can more accurately represent a richer set of data due to the non-linear mapping from an input vector to the output vector.\nThe first step in using a multilayer perceptron is determining an appropriate architecture. This means that we must determine the number of hidden units, the number of hidden layers, the type of activation function, as well as the number of input and output variables. Some of this information can be determined by the structure of the training set, and some must be determined experimentally.\nThe second step involves determining a training algorithm to estimate the weights and biases of the MLP. This involves iteratively solving an optimization problem to estimate the network\u2019s weights and biases so that the network\u2019s output is as close to a desired output as possible. The structure of a MLP network is shown in Figure 1."}, {"heading": "2.1 Dataset", "text": "A flow pattern experimental data base was collected [PTM+12], which consists of the most relevant studies developed in the area. Specifically for this study, the data set from Shoham [Sho] was selected among the available sets due to its large number of data points (5676), range in inclination angle (\u221290\u25e6 to 90\u25e6), two pipe diameters (ID=1in and 2in), and the wide range of flow patterns observed for all pipe inclination angles. The flow patterns considered in this study are: Annular (A), Bubble (B), Dispersed bubble (DB), Intermittent (I), Stratified smooth (SS) and Stratified wavy (SW). The Intermittent flow pattern considers Slug (SL) and Churn (CH) flow pattern combined [PTM+12]. In order to analyze the performance of the algorithm, three tests are proposed: Test 1 considers all the flow patterns proposed; Test 2 combines the SS and SW data points into stratified flow ST (ST = SS + SW); finally Test 3 combines the segregated flow patterns (ST + A) and the dispersed flow patterns (DB + B)."}, {"heading": "2.2 Supervised Classification of Flow Pattern Using Machine Learning", "text": "In supervised learning we assume each element of study is represented as an n-component vector-valued random variable, (X1, X2, . . . , Xn) where each Xi represents an attribute or feature; the space of all possible feature vectors is called the input space X . We also consider a set {y1, y2, . . . , yk} corresponding to the k possible classes; this forms the output space Y . A classifier or learning algorithm typically receives a set of training examples from a source domain T = (x,y), where x = (x1, x2, . . . , xn) is a vector in the input space, and y \u2208 R k is a value in the (discrete) output space. We assume the training or source sample T consists of independently and identically distributed (i.i.d.) examples obtained according to a fixed but unknown joint probability distribution, P (x,y), in the input-output space. The output of the classifier is a hypothesis or function f(x) mapping the input space to the output space, f : X \u2192 Y . In supervised learning, this mapping is the result of optimizing a loss function where our ultimate goal is to minimize number of misclassifications. For this research we use the Python bindings of the H2O library [ACL+15] as our deep learning platform."}, {"heading": "3 Results", "text": "In our approach, we trained a MLP on a set of randomly selected samples, approximately 60% of the entire dataset was used for training, approximately 20% was used for validation, and approximately 20% was used as the testing set for the 3 different tests. Using the multilayer perceptron we can train using simple stochastic gradient descent [Bot10]. Table 1 shows our chosen multilayer perceptron architecture and parameters used in our experiments. Other design considerations are the choice of ReLU as our non-linear activation function, the \u21131 and \u21132 regularization penalties to avoid overfitting, the number of epochs, and nfolds, the number of cross-validation folds.\nIn evaluating the effectiveness of our deep learning methodology, the confusion matrix is an important measure. Table 2 shows the confusion matrix for the training data set for Test 1, predicting classes A, B, DB, I, SS, and SW. We can readily see the strong diagonal components. This means that our classifier is achieving little classification error. The testing set is used to predict the variable Flow Pattern, which contains labels for each class (A, B, DB, I, SS, and SW), and a predictive accuracy of 83.87% for the different classes is obtained, the details of\nwhich are shown in Table 3. Off diagonal elements of a confusion matrix show misclassification with other flow patterns. The confusion matrix\u2019s columns represent the output patterns predicted by Deep Learning while the rows represent the true class which is denoted here by each flow pattern.\nTable 4 shows the confusion matrix on train data for Test 2, predicting classes A, B, DB, I and ST. Similar to Test 1 we can see the strong diagonal components, and the classifier has small classification error. The testing set is used to predict the variable Flow Pattern, which contains labels for each class (A, B, DB, I, and ST), and we achieve a predictive accuracy of 83.34%. The details are shown in Table 5.\nTable 6 shows the confusion matrix for the training data set for Test 3, predicting the classes Intermittent, Dispersed, and Segregate. We can readily see the strong diagonal components with the classifier achieving little classification error. The testing set is used to predict the variable flow pattern, which contains labels for each class (Intermittent, Dispersed, and Segregate) with a predictive accuracy of 85.97%, the details of which are shown in Table 7\nA comparison between the predicted flow pattern and the experimental database considering the three data sets under study show low error and high classification accuracy. Results for Test 1 and Test 2 are very similar. Most of the failed predictions between the flow patterns can be attributed to the different criteria used by the different experimentalists to classify the flow patterns and their relationships [Sho]. Finally an improvement is obtained for Test 3 by combining the segregated flow patterns (ST + A) and the dispersed flow patterns (DB + B). The prediction accuracy for this case increases to 85.97%. This is improvement is due to the clear and straightforward distinction between the two combined flow patterns [PTM+12, Sho]. The results for the deep learning approach for classification of two-phase flow pattern are encouraging."}, {"heading": "4 Conclusions", "text": "In this paper we proposed three types of data sets as input features and investigated the use of deep learning for the classification and prediction of two-phase flow, based on experimental data, obtained from [PTM+12, Sho].\nWe proposed six types of input features, and a corresponding architecture to precisely predict flow patterns. First, we showed that the network can learn surprisingly well as using our chosen architecture and parameters allowed us to achieve high classification accuracy. Second, we showed that the network can classify the different flow patterns with high efficiency. Finally, we achieved high precision predicting different combinations of classes.\nOur experiments indicate that a deep learning approach, has the potential to capture flow patterns, which may boost the classification performance. These investigations could be further improved in future studies by carrying out more exhaustive searches for the parameters in the architectures. The result would be improved overall performance of these systems.\nFinally, deep learning can be used to predict flow patterns using pipe characteristics, fluid properties and superficial velocities of the two-phase flows. It outperforms results from previous studies.[PTM+12]"}], "references": [{"title": "and Viraj Parmar", "author": ["Anisha Arora", "Arno Candel", "Jessica Lanford", "Erin LeDell"], "venue": "Deep Learning with H2O,", "citeRegEx": "ACL15", "shortCiteRegEx": null, "year": 2015}, {"title": "Artificial neural network application for multiphase flow patterns detection: A new approach", "author": ["Mustafa Al-Naser", "Moustafa Elshafei", "Abdelsalam Al-Sarkhi"], "venue": "Journal of Petroleum Science and Engineering, 145:548\u2013564,", "citeRegEx": "ANEAS16", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of COMPSTAT\u20192010", "author": ["L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent"], "venue": "pages 177\u2013186. Springer,", "citeRegEx": "Bot10", "shortCiteRegEx": null, "year": 2010}, {"title": "Nature", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton. Deep learning"], "venue": "521(7553):436\u2013444,", "citeRegEx": "LBH15", "shortCiteRegEx": null, "year": 2015}, {"title": "Pattern Recognition Letters", "author": ["Martin L\u00e4ngkvist", "Lars Karlsson", "Amy Loutfi. A review of unsupervised feature learning", "deep learning for time-series modeling"], "venue": "42:11\u201324,", "citeRegEx": "LKL14", "shortCiteRegEx": null, "year": 2014}, {"title": "A methodology and database to quantify the confidence level of methods for gas\u2013liquid two-phase flow pattern prediction", "author": ["E Pereyra", "C Torres", "R Mohan", "L Gomez", "G Kouba", "O Shoham"], "venue": "Chemical Engineering Research and Design, 90(4):507\u2013513", "citeRegEx": "PTM12", "shortCiteRegEx": null, "year": 2012}, {"title": "Steady-State Multiphase Flow\u2013Past", "author": ["Mack Shippen", "William J Bailey"], "venue": "Present, and Future, with a Perspective on Flow Assurance. Energy & Fuels, 26(7):4145\u20134157,", "citeRegEx": "SB12", "shortCiteRegEx": null, "year": 2012}, {"title": "Flow Pattern Transition and Characterization in Gas-Liquid Two-Phase Flow in Inclined Pipes", "author": ["Sho] O Shoham"], "venue": "PhD Dissertation", "citeRegEx": "Shoham.,? \\Q1982\\E", "shortCiteRegEx": "Shoham.", "year": 1982}, {"title": "Richardson", "author": ["Ovadia Shoham. Mechanistic modeling of gas-liquid two-phase flow in pipes"], "venue": "TX: Society of Petroleum Engineers,", "citeRegEx": "Sho06", "shortCiteRegEx": null, "year": 2006}, {"title": "Nature Reviews Neuroscience", "author": ["Rafael Yuste. From the neuron doctrine to neural networks"], "venue": "16(8):487\u2013497,", "citeRegEx": "Yus15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Thus, knowledge of the existing flow pattern can help the industry carry out a better design of two-phase flow systems [PTM12].", "startOffset": 119, "endOffset": 126}, {"referenceID": 8, "context": "Shoham [Sho06] attempted to summarize the main flow patterns for all inclination angles as Dispersed bubble, Bubble, Slug, Churn, Annular and Stratified (smooth and wavy).", "startOffset": 7, "endOffset": 14}, {"referenceID": 6, "context": "There are many models and approaches used to predict the two-phase flow patterns in pipes based on mechanistic modeling or dimensionless analysis [SB12].", "startOffset": 146, "endOffset": 152}, {"referenceID": 1, "context": "Artificial neural networks have been used to identify and predict flow patterns [ANEAS16].", "startOffset": 80, "endOffset": 89}, {"referenceID": 3, "context": "Multilayer perceptron with many hidden layers is a good example of the models with deep architectures [LBH15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 5, "context": "A flow pattern experimental data base was collected [PTM12], which consists of the most relevant studies developed in the area.", "startOffset": 52, "endOffset": 59}, {"referenceID": 5, "context": "The Intermittent flow pattern considers Slug (SL) and Churn (CH) flow pattern combined [PTM12].", "startOffset": 87, "endOffset": 94}, {"referenceID": 0, "context": "For this research we use the Python bindings of the H2O library [ACL15] as our deep learning platform.", "startOffset": 64, "endOffset": 71}, {"referenceID": 2, "context": "Using the multilayer perceptron we can train using simple stochastic gradient descent [Bot10].", "startOffset": 86, "endOffset": 93}, {"referenceID": 5, "context": "[PTM12]", "startOffset": 0, "endOffset": 7}], "year": 2017, "abstractText": "In order to better model complex real-world data such as multiphase flow, one approach is to develop pattern recognition techniques and robust features that capture the relevant information. In this paper, we use deep learning methods, and in particular employ the multilayer perceptron, to build an algorithm that can predict flow pattern in twophase flow from fluid properties and pipe conditions. The preliminary results show excellent performance when compared with classical methods of flow pattern prediction.", "creator": "LaTeX with hyperref package"}}}