{"id": "1705.07171", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Nestrov's Acceleration For Second Order Method", "abstract": "optimization plays a strategically key role in machine learning. recently, stochastic second - order methods suggested have attracted unexpectedly much attention due to their low computational cost in each iteration. however, designing these algorithms undoubtedly might perform poorly especially if it is hard to approximate the hessian well critically and efficiently. as far as we know, again there is no effective way to handle this problem. in this paper, instead we resort to nestrov's acceleration technique to improve the convergence performance sensitivity of a class of second - order methods called approximate newton. we give a theoretical analysis that nestrov's acceleration technique can improve the convergence performance for solving approximate matched newton algorithms just like for similar first - finite order methods. we accordingly propose selecting an accelerated regularized sub - - sampled newton. our accelerated algorithm performs much better than the original regularized sub - sampled newton in experiments, which validates our theory empirically. besides, here the accelerated regularized sub - sampled newton has exactly good performance comparable to or even better software than state - of - art algorithms.", "histories": [["v1", "Fri, 19 May 2017 20:17:56 GMT  (1077kb,D)", "http://arxiv.org/abs/1705.07171v1", null], ["v2", "Tue, 17 Oct 2017 06:13:24 GMT  (0kb,I)", "http://arxiv.org/abs/1705.07171v2", "Have an important typo in title. Superseded byarXiv:1710.08496"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haishan ye", "zhihua zhang"], "accepted": false, "id": "1705.07171"}, "pdf": {"name": "1705.07171.pdf", "metadata": {"source": "CRF", "title": "Nestrov\u2019s Acceleration For Second Order Method", "authors": ["Haishan Ye", "Zhihua Zhang"], "emails": ["yhs12354123@gmail.com", "zhzhang@math.pku.edu.cn"], "sections": [{"heading": null, "text": "1. Introduction\nOptimization has become an increasingly popular issue in machine learning. Many machine learning models can be reformulated as the following optimization problems:\nmin x\u2208Rd\nF (x) = 1\nn n\u2211 i=1 fi(x). (1)\nwhere each fi is the loss with respect to (w.r.t.) the i-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.\nIn the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the computational cost [4, 10, 14]. However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD [8, 16, 17, 20].\nFor the first-order methods which only make use of the gradient information, Nestrov\u2019s acceleration technique is a very useful tool [11]. It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.\nar X\niv :1\n70 5.\n07 17\n1v 1\n[ cs\n.L G\nRecently, second-order methods have also received great attention due to their high convergence rate. However, conventional second-order methods are very costly because they take heavy computational cost to obtain the Hessian matrices. To conquer this weakness, one proposed a sub-sampled Newton which only samples a subset of functions fi randomly to construct a sub-sampled Hessian [15, 3, 18] . Pilanci and Wainwright [13] applied the sketching technique to alleviate the computational burden of computing Hessian and brought up sketch Newton. Regularized sub-sampled Newton methods were also devised to deal with the ill-condition problem [5, 15].\nIn the latest work, Ye et al. [19] cast these stochastic second-order procedures into a so-called approximate Newton framework. They showed that if approximate Hessian H(t) satisfies\n[1\u2212 (1\u2212 \u03c0)]\u22072F (x(t)) H(t) [1 + (1\u2212 \u03c0)]\u22072F (x(t)), (2)\nwhere 0 < \u03c0 < 1, then approximate Newton converges with rate 1\u2212\u03c0. If H(t) is a poor approximation like \u03c0 = 1/\u03ba, where \u03ba is the condition number of object function F (x), then approximate Newton has the same convergence rate with gradient descent.\nSince approximate Newton converges with a linear rate, it is natural to ask whether approximate Newton can be accelerated just like gradient descent. If it can be accelerated, can the convergence rate be promoted to 1\u2212 \u221a \u03c0 compared to original 1\u2212\u03c0? In this paper, we aim to introduce Nestrov\u2019s acceleration technique to promote the performance of second-order methods, specifically approximate Newton.\nWe summarize our work and contribution as follows:\n\u2022 First, we introduce Nestrov\u2019s acceleration technique to improve the convergence rate of the stochastic second-order methods (approximate Newton). This acceleration is very important especially when n and d are close to each other and object function in question is ill-conditioned. In these cases, it is very hard to construct a good approximate Hessian with low cost.\n\u2022 Our theoretical analysis shows that by Nestrov\u2019s acceleration, the convergence rate of approximate Newton can be improved to 1 \u2212 \u221a \u03c0 from original rate 1 \u2212 \u03c0 where 0 < \u03c0 < 1 when\nthe object function is quadratic. For general smooth convex functions, we also show that the similar acceleration also holds when the initial point is close to the optimal point.\n\u2022 We empirically validate our theory about accelerated second-order algorithms. Our experimental study shows that Nestrov\u2019s acceleration technique can improve approximate Newton methods effectively. Our experiments also reveal a fact that adding curvature information properly can always improve the algorithm\u2019s convergence performance.\n\u2022 We propose an accelerated regularized sub-sampled Newton. Compared with state-of-art stochastic first-order methods, our algorithm shows competitive or even better performance. This demonstrates the efficiency of the accelerated second-order method.\nThe remainder of this paper is organized as follows. Section 2 defines notation and introduces preliminaries will be used in this paper. We describe and analyze accelerated second-order methods in detail in Section 3. In Section 4, we propose accelerated regularized sub-sampled Newton and validate our theory empirically. Finally, we conclude our work in Section 5. All proofs are in Appendix.\n2. Notation and Preliminaries\nWe first introduce notation that will be used in this paper. Then, we give some properties of object function that will be used.\nAlgorithm 1 Accelerated Second Order Method. 1: Input: x(0) and x(1) are initial points sufficient close to x\u2217. \u03b8 is the momentum parameter with 0 < \u03b8 < 1. H is an approximate Hessian matrix of \u22072F (x\u2217) with \u2016I \u2212 [\u22072F (x\u2217)] 1 2H\u22121[\u22072F (x\u2217)] 1 2 \u2016 = 1\u2212 \u03c0 with\n0 < \u03c0 < 1. 2: for t = 1, . . . until termination do 3: y(t+1) = (1 + \u03b8)x(t) \u2212 \u03b8x(t\u22121); 4: x(t+1) = y(t+1) \u2212H\u22121\u2207F (y(t+1)). 5: end for\nAlgorithm 2 Extended Accelerated Second Order Method. 1: Input: x(0) and x(1) are initial points sufficient close to x\u2217. \u03b8 is the momentum parameter with 0 < \u03b8 < 1.\n2: for t = 1, . . . until termination do 3: y(t+1) = (1 + \u03b8)x(t) \u2212 \u03b8x(t\u22121). 4: Construct H(t+1) as an approximation of \u22072F (y(t+1)) satisfying Eqn. (5). 5: x(t+1) = y(t+1) \u2212 [ H(t+1) ]\u22121 \u2207F (y(t+1)).\n6: end for\n2.1 Notation\nGiven a matrix A = [aij ] \u2208 Rm\u00d7n of rank ` and a positive integer k \u2264 `, its SVD is given as A = U\u03a3V T = Uk\u03a3kV T k +U\\k\u03a3\\kV T \\k, where Uk and U\\k contain the left singular vectors of A, Vk and V\\k contain the right singular vectors of A, and \u03a3 = diag(\u03c31, . . . , \u03c3`) with \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3` > 0 are the nonzero singular values of A. Additionally, \u2016A\u2016 , \u03c31 is the spectral norm. If A is positive semidefinite, then U = V and the eigenvalue decomposition of A is the same to singular value decomposition. It also holds that \u03bbi(A) = \u03c3i(A), where \u03bbi(A) is the i-th largest eigenvalue of A. Let \u03bbmax(A) and \u03bbmin(A) denote the largest and smallest eigenvalue of A, respectively.\n2.2 Assumptions\nIn this paper, we focus on the problem described in Eqn. (1). Moreover, we will make the following two assumptions.\nAssumption 1 The objective function F is \u00b5-strongly convex, that is,\nF (y) \u2265 F (x) + [\u2207F (x)]T (y \u2212 x) + \u00b5 2 \u2016y \u2212 x\u20162, for \u00b5 > 0.\nAssumption 2 \u2207F (x) is L-Lipschitz continuous, that is,\n\u2016\u2207F (x)\u2212\u2207F (y)\u2016 \u2264 L\u2016y \u2212 x\u2016, for L > 0.\nBy Assumptions 1 and 2, we define the condition number of function F (x) as: \u03ba , L\u00b5 . Besides, we will also use the nation of Lipschitz continuity of \u22072F (x) in this paper. We say \u22072F (x) is L\u0302-Lipschitz continuous if\n\u2016\u22072F (x)\u2212\u22072F (y)\u2016 \u2264 L\u0302\u2016y \u2212 x\u2016, for L\u0302 > 0.\n3. Accelerated Second-Order Methods\nIn this section, we apply Nesterov\u2019s acceleration technique to second-order methods and present two accelerated second-order methods in Algorithms 1 and 2. Just like conventional second-order\nmethods, we assume the initial points x(0) and x(1) are sufficient close to the optimal point x\u2217 in our algorithms.\nIn Algorithm 1, H is a fixed approximate Hessian such that\n\u2016I \u2212 [\u22072F (x\u2217)] 12H\u22121[\u22072F (x\u2217)] 12 \u2016 = 1\u2212 \u03c0,\nwhere 0 < \u03c0 < 1. And we update sequence x(t) as follows,{ y(t+1) = (1 + \u03b8)x(t) \u2212 \u03b8x(t\u22121), x(t+1) = y(t+1) \u2212H\u22121\u2207F (y(t+1)),\n(3)\nwhere \u03b8 is chosen in terms of the value of \u03c0. We can see that the iteration (3) is much like the update procedure of Nestrov\u2019s accelerated gradient descent but replacing step size with H\u22121.\nRather fixing approximate Hessian H, we can also construct an approximate Hessian H(t) for each t. If H(t) does not vary heavily, then Nesterov\u2019s acceleration technique still works fine. In this case, we update sequence x(t) as follows y (t+1) = (1 + \u03b8)x(t) \u2212 \u03b8x(t\u22121), x(t+1) = y(t+1) \u2212 [ H(t+1) ]\u22121 \u2207F (y(t+1)), (4)\nwhere H(t+1) is an approximation of \u22072F (y(t+1)) such that\n(1\u2212 (1\u2212 \u03c0))H(t+1) \u22072F (y(t+1)) (1 + (1\u2212 \u03c0))H(t+1). (5)\nThe detailed description is depicted in Algorithm 2. If we set \u03b8 = 0, the above iteration will reduce to x(t+1) = x(t)\u2212 [H(t)]\u22121\u2207F (x(t)), where H(t) is an approximation of \u22072F (x(t)). Then Algorithm 2 will become the approximate Newton method defined in [19].\nIn fact, we can regard Algorithm 2 as an extension of Algorithm 1. If H(t) varies very slowly, then the convergence properties of Algorithms 2 and 1 are close.\n3.1 Theoretical Analysis\nWe now discuss the convergence properties of Algorithm 1 and Algorithm 2. We will prove the theoretical convergence properties of Algorithm 1 applied to a quadratic object function, i.e., the least square regression. For a general smooth convex object function, it can be approximated by quadratic functions in a region close to optimal point. Hence, it can result in a similar convergence property with the quadratic objective function.\nBefore convergence analysis, we give an important lemma which is closely related to the convergence behavior of the accelerated second-order methods.\nLemma 1 Let Assumptions 1 and 2 hold. Suppose that \u22072F (x) exists and is continuous in a neighborhood of a minimizer x\u2217. Let H(t) be an approximation of \u22072F (y(t)). Consider the iteration (4). If x(t) is sufficient close to x\u2217 then we have the following result\n[\u22072F (x\u2217)]\u2212 12\u2207F (x(t+1)) = ( I \u2212 [\u22072F (x\u2217)] 12 [H(t+1)]\u22121[\u22072F (x\u2217)] 12 ) \u00d7 ( (1 + \u03b8) [ \u22072F (x\u2217)\n]\u2212 12 \u2207F (x(t))\u2212 \u03b8 [\u22072F (x\u2217)]\u2212 12 \u2207F (x(t\u22121))) + o(\u2207F (x(t))) + o(\u2207F (x(t\u22121))).\nAlgorithm 3 Accelerated Regularized Subsample Newton. 1: Input: x(0), 0 < \u03b4 < 1, regularizer parameter \u03b1, sample size |S|, acceleration parameter \u03b8(t) ; Let y(0) = x(0)\n2: for t = 0, . . . until termination do 3: Select a sample set S, of size |S| and H(t) = 1|S| \u2211 j\u2208S \u2207 2fj(x (t)) + \u03b1I; 4: Update y(t+1) = x(t) \u2212 [H(t)]\u22121\u2207F (x(t)); 5: Update x(t+1) = y(t+1) + \u03b8(t)(y(t+1) \u2212 y(t)) 6: end for\nFurthermore, if \u22072F (x) is L\u0302-Lipschitz continuous, then the above result holds whenever x(t) satisfies\n\u2016x(t) \u2212 x\u2217\u2016 \u2264 o(1) L\u0302 .\nFrom Lemma 1, we can see that the convergence property of the second-order methods are mainly determined by I \u2212 [\u22072F (x\u2217)] 12 [H(t+1)]\u22121[\u22072F (x\u2217)] 12 and \u03b8.\nLemma 1 describes such a fact that if x(t\u22121) and x(t) are sufficient close to x\u2217, that is, o(F (x(t))) and o(F (x(t\u22121))) are very small, then the convex function can be well approximated by a quadratic function. Therefore, we will demonstrate the convergence analysis of Algorithm 1 on the least square regression problem. The other quadratic functions can be analyzed in the same way and the convergence rate is also the same.\nThe least square regression is defined as follows\nF (x) = \u2016Ax\u2212 b\u20162, (6)\nwhere A \u2208 Rn\u00d7d is of full column rank. Because the Hessian of F (x) is ATA, the Lipschitz constant of \u22072F (x) is zero. Hence, the result of Lemma 1 degenerates to\n[\u22072F (x\u2217)]\u2212 1 2\u2207F (x(t+1)) = ( I \u2212 [\u22072F (x\u2217)] 1 2 [H(t+1)]\u22121[\u22072F (x\u2217)] 1 2 ) \u00d7 ( (1 + \u03b8) [ \u22072F (x\u2217) ]\u2212 1 2 \u2207F (x(t))\u2212 \u03b8 [ \u22072F (x\u2217) ]\u2212 1 2 \u2207F (x(t\u22121)) ) .\nThis equation describes a linear dynamic system which contains the convergence property of Algorithm 1 applied to the least square regression problem. That is,\nTheorem 2 For the least square regression problem (6), we solve it by Algorithm 1 with \u03b8 = 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 .\nThen after t iterations, we have \u2016\u2207F (x(t))\u2016 \u2264 \u221a \u03bbmax(\u22072F (x\u2217))(1\u2212 \u221a \u03c0)t |\u03be + \u03d5t| ,\nwhere \u03be and \u03d5 are constants determined by initial points x(0) and x(1).\nRemark 3 Theorem 2 assumes that the Hessian matrix ATA is positive definite, i.e., A is of full column rank. If A is not full column rank or even if d > n, we can alternatively consider the ridge regression problem:\nmin x \u2016Ax\u2212 b\u20162 + \u03b3\u2016x\u20162, for \u03b3 > 0.\nRight now the Hessian matrix ATA+ \u03b3I is positive definite and is constant. Thus, Theorem 2 still holds.\nRemark 4 In real applications, it is hard to get the exact value of \u03c0 in Theorem 2. However, from the proof of Theorem 2, we can see that if \u03b8 is close to 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 , then the convergence rate is also close\nto 1\u2212 \u221a \u03c0.\nFrom Theorem 2, we can see that if we choose \u03b8 = 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 , then the convergence rate of Algorithm 1\nis 1 \u2212 \u221a \u03c0 in contrast to 1 \u2212 \u03c0 of the conventional approximate Newton method. If we choose H = (1/L)I and \u03b8 = \u221a L\u2212\u221a\u00b5\u221a L+ \u221a \u00b5 , where L = \u03bbmax(ATA) and \u00b5 = \u03bbmin(ATA), then Theorem 2 shows\nthat the convergence rate of Nestrov\u2019s acceleration for the least square regression problem is 1\u2212 \u221a \u00b5/L.\nSince a smooth convex function can be well approximated by a quadratic function once x(t) gets into the region close enough to the optimal point, in this case the convergence behavior of Algorithm 1 is also similar to that in Theorem 2.\nAlgorithm 2 is almost the same with Algorithm 1 except the approximate Hessian H(t) varying as t. Hence, Algorithm 2 will bring in more perturbation. If matrix H(t) does not vary heavily, then Nesterov\u2019s acceleration technique still works fine. And Algorithm 2 has the similar convergence behavior with Algorithm 1.\nIn Appendix E, we will analysis the influence of perturbation to convergence properties of Algorithms 1 and 2. We will show that if the perturbation is small, then the convergence properties of Algorithms 1 and 2 are close to the analysis in Theorem 2.\n3.2 Accelerated Regularized Sub-sampled Newton\nIn Section 3.1, we have proposed the theoretical analysis for accelerated second-order methods. Based on the theoretical analysis, we now devise a concrete accelerated second-order method that we call accelerated regularized sub-sampled Newton.\nRegularized sub-sampled Newton (RegSN, Algorithm 4 in Appendix) is an effective alternative to reduce the sample size of sub-sampled Newton when the objective function is ill-conditioned. From Theorem 5 in Appendix B, we can see that RegSN achieves a linear convergence rate if sample size |S| and regularizer \u03b1 are properly chosen. However, RegSN has its own weakness. We can also observe that it needs a large sample size |S| to achieve a fast convergence rate when K/\u03c3 is large, where K and \u03c3 are defined in Appendix B.\nTherefore, we apply Nestrov\u2019s acceleration technique to RegSN, giving the accelerated regularized sub-sampled Newton (AccRegSN) in Algorithm 3. We can see that Algorithm 3 provides a concrete construction of the approximate Hessian. Hence, AccRegSN is an implementation of Algorithm 2. Our theoretical analysis shows that AccRegSN has better performance than RegSN if these two algorithms share the same sample size |S| and regularizer \u03b1. Hence, we can construct a poor approximate Hessian very efficiently when K/\u03c3 is large. At the same time, AccRegSN still has a fast convergence rate.\nBesides, since AccRegSN makes use of more curvature information than Nestrov\u2019s accelerated gradient descent (AGD) method [11], AccRegSN should converge faster than AGD theoretically.\n4. Experiments\nIn Section 3.1, we have shown that Nestrov\u2019s acceleration technique can improve the performance of approximate Newton method theoretically. In this section, we will validate our theory empirically. In particular, we first compare AccRegSN with RegSN on the least square regression to validate the theoretical analysis in Section 3.1. Then we conduct more experiments on a popular machine learning problem called Ridge Logistic Regression, and compare AccRegSN with other state-of-art algorithms.\nThe least square regression is defined in Eqn. (6). In our experiments, A is a 3000\u00d7 1000 random matrix with the i.i.d. entries from U(0, 1), where U(0, 1) means uniform distribution on [0, 1]. And b is a 3000\u00d7 1 random Gaussian vector of the i.i.d. entries from N(0, 1). The condition number of A is 128.855.\nIn experiments, we set the sample size |S| to be 0.5%n, 5%n and 10%n. The regularizer \u03b1 is properly chosen according to |S|. AccRegSN and RegSN share the same |S| and \u03b1. And \u03b8 in AccRegSN is fixed and appropriately selected. We report the experiments result in Figure 1.\nFrom Figure 1, we can see that AccRegSN and RegSN have significant difference in convergence rate and AccRegSN is much faster. This validates the analysis in Section 3. Besides, we can also observe that AccRegSN runs faster as sample size |S| increases. When |S| = 10%n, AccRegSN takes only less than 4000 iterations to achieve an e\u221230 error while it needs 5000 iterations to get an e\u221225 error if |S| = 0.5n%.\n4.2 Experiments on the Ridge Logistic Regression\nWe conduct experiments on the Ridge Logistic Regression problem whose objective is\nF (x) = 1\nn n\u2211 i=1 log[1 + exp(\u2212bi\u3008ai, x\u3009)] + \u03bb 2 \u2016x\u20162, (7)\nwhere ai \u2208 Rd is the i-th input vector, and bi \u2208 {\u22121, 1} is the corresponding label. We choose \u03bb = 1n in our experiments.\nWe conduct our experiments on six datasets: \u2018gisette\u2019, \u2018protein\u2019, \u2018svhn\u2019, \u2018rcv1\u2019, \u2018sido0\u2019, and \u2018real-sim\u2019. The first three datasets are dense and the last three ones are sparse. We give the detailed description of the datasets in Table 1. Notice that the size and dimension of dataset are close to each other, so the sketch Newton method [13, 18] can not be used. We compare Algorithm 3 (AccRegSN) with RegSN (Algorithm 4), AGD and SVRG.\nIn our experiments, the sample size |S| and regularizer \u03b1 of RegSN and AccRegSN are chosen according to Theorem 5. For a fixed |S|, a proper \u03b1 can be found after several tries. In our experiments, AccRegSN and RegSN pick samples uniformly.\nThe current sub-sampled Hessian H(t) constructed in Algorithm 3 can be written as\nH(t) = A\u0303T A\u0303+ (\u03b1+ \u03bb)I,\nwhere A\u0303 \u2208 R`\u00d7d, where ` < n. Notice that if ` < d, we can resort to Woodbury\u2019 identity to compute the inverse of H(t) efficiently. Furthermore, if A\u0303 is sparse, we can use conjugate gradient (Algorithm 5 in Appendix) to obtain an approximation of [H(t)]\u22121\u2207F (x(t)) which exploits the sparsity of A\u0303. In our experiments on sparse datasets, we set tol = 0.01\u2016\u2207F (x(t))\u2016 for conjugate gradient (Algorithm 5 ).\nFor the momentum parameter \u03b8, it is hard to get the best value for AccRegSN just like AGD. However, our theoretical analysis implies that for large sample size |S|, a small \u03b8 should chosen. In our experiments, we set \u03b8(t) = tt+16 in Algorithm 3 for the dense datasets and \u03b8\n(t) = tt+30 for the sparse datasets. We set x(0) = 0 for all the datasets and all the algorithms.\nWe report our result in Figure 2. The flops are computed as Appendix A. We can see that AccRegSN converges much faster than RegSN when these two algorithms have the same sample size. This shows Nestrov\u2019s acceleration technique can promote the performance of regularized sub-sampled Newton effectively. We can also observe that AccRegSN outperforms AGD significantly even when the sample size S is 1%n or even less. This validates the fact that adding curvature information is an effective way to improve the ability of accelerated gradient descent.\nCompared with SVRG, we can see that AccRegSN also has better performance on most of the datasets. Specifically, AccRegSN performs much better than SVRG on \u2018svhn\u2019. This means that AccRegSN is an efficient algorithm. Furthermore, we can observe that AccRegSN is very robust. It works well in different sample sizes.\nThe experiments also reveal that AccRegSN outperforms the other algorithms especially on datasets that RegSN performs very poor. In fact, poor performance of RegSN means the problem is ill-conditioned. This shows that AccRegSN has advantages when the problem is ill-conditioned. In summary, AccRegSN is good choice in practice.\n4.3 Conclusion of Empirical Study\nThe above experiments show that Nestrov\u2019s acceleration is an effective way to promote the convergence rate of approximate Newton methods. The experiments also show that adding some curvature information always help AGD to obtain a faster convergence rate. Compared with SVRG, AccRegSN still has its own advantages even AccRegSN just picks the training samples uniformly in constructing the approximate Hessian. Therefore, we can conclude that the accelerated second-order method is efficient for a smooth convex object function. In fact, AccRegSN is just a simple demonstration of accelerated second order methods. Obviously, a better sampling strategy in constructing the approximate Hessian can further improve the performance of AccRegSN.\n5. Conclusion\nIn this paper, we have exploited Nestrov\u2019s acceleration technique to promote the performance of second-order methods, specifically approximate Newton. We have presented the theoretical analysis on the convergence properties of accelerated second-order methods, showing that accelerated approximate Newton has higher convergence rate, especially when the approximate Hessian is not a good approximation. Based on our theory, we have developed AccRegSN. Our experiments have shown that our AccRegSN performs much better than the conventional RegSN, which meets our theory well. AccRegSN also has several advantages over other state-of-art algorithms, demonstrating the efficiency of accelerated second-order methods.\nReferences\n[1] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. arXiv preprint arXiv:1603.05953, 2016.\n[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.\n[3] Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian information in optimization methods for machine learning. SIAM Journal on Optimization, 21 (3):977\u2013995, 2011.\n[4] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. In Advances in neural information processing systems, pages 1647\u20131655, 2011.\n[5] Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In Advances in Neural Information Processing Systems, pages 3034\u20133042, 2015.\n[6] Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.\n[7] I. Guyon. Sido: A phamacology dataset. URL http://www.causality.inf.ethz.ch/data/ SIDO.html.\n[8] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013323, 2013.\n[9] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv preprint arXiv:1507.02000, 2015.\n[10] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efficient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 661\u2013670. ACM, 2014.\n[11] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372\u2013376, 1983.\n[12] Yurii Nesterov et al. Gradient methods for minimizing composite objective function, 2007.\n[13] Mert Pilanci and Martin J. Wainwright. Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205\u2013245, 2017.\n[14] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.\n[15] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods ii: Local convergence rates. arXiv preprint arXiv:1601.04738, 2016.\n[16] Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential convergence _rate for finite training sets. In Advances in Neural Information Processing Systems, pages 2663\u20132671, 2012.\n[17] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.\n[18] Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher R\u00e9, and Michael W Mahoney. Sub-sampled newton methods with non-uniform sampling. In Advances in Neural Information Processing Systems, pages 3000\u20133008, 2016.\n[19] Haishan Ye, Luo Luo, and Zhihua Zhang. A unifying framework for convergence analysis of approximate newton methods. arXiv preprint arXiv:1702.08124, 2017.\n[20] Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence with condition number independent access of full gradients. In Advance in Neural Information Processing Systems 26 (NIPS), pages 980\u2013988, 2013.\nAppendix A. Computation cost of matrix operations\nWe will give the computation cost of basic matrix operations, the result can be found in Matrix Computation [6], or can be calculated easily.\nFor matrix multiplication, given dense matrices B \u2208 Rm\u00d7n and C \u2208 Rn\u00d7k, the basic cost of the matrix product B \u00d7 C is 2mnk flops. It costs 2k \u00b7 nnz(B) flops for the matrix product B \u00d7 C when B is sparse, where nnz(B) denotes the number of nonzero entries of B.\nA linear equation with positive matrix can be solved efficiently by Cholesky decomposition and back substitution. Cholesky decomposition of a positive-definite matrix A \u2208 Rn\u00d7n costs n3/3 flops. To get a solution of n\u00d7 n triangular system, it needs n2 flops.\nAppendix B. Regularized Sub-sampled Newton\nWe assume that each fi(x) and F (x) in (1) have the following properties:\nmax 1\u2264i\u2264n\n\u2016\u22072fi(x)\u2016 \u2264 K <\u221e, (8)\n\u03bbmin(\u22072F (x)) \u2265 \u03c3 > 0. (9)\nThe regularized Sub-sampled Newton method is depicted in Algorithm 4, and we now give its local convergence properties in the following theorem [19].\nTheorem 5 Let F (x) satisfy Assumption 1 and 2. Assume Eqns. (8) and (9) hold, and let 0 < \u03b4 < 1, 0 \u2264 1 < 1 and 0 < \u03b1 be given. Assume \u03b2 is a constant such that 0 < \u03b2 < \u03b1 + \u03c32 , the subsampled size |S| satisfies |S| \u2265 16K\n2 log(2d/\u03b4) \u03b22 , and H (t) is constructed as in Algorithm 4. Define\n0 = max\n( \u03b2 \u2212 \u03b1\n\u03c3 + \u03b1\u2212 \u03b2 ,\n\u03b1+ \u03b2\n\u03c3 + \u03b1+ \u03b2\n) ,\nwhich implies that 0 < 0 < 1. And we define \u2016x\u2016M\u2217 = \u2016[M\u2217]\u2212 1 2x\u2016. Then Algorithm 4 has the following convergence properties:\n1. There exists a sufficient small value \u03b3, 0 < \u03bd(t) < 1, and 0 < \u03b7(t) < 1 such that when \u2016x(t) \u2212 x\u2217\u2016 \u2264 \u03b3, each iteration satisfies\n\u2016\u2207F (x(t+1))\u2016M\u2217 \u2264 ( 0 + 2\u03b7(t)\n1\u2212 0\n) 1 + \u03bd(t)\n1\u2212 \u03bd(t) \u2016\u2207F (x(t))\u2016M\u2217 .\nBesides, \u03bd(t) and \u03b7(t) will go to 0 as x(t) goes to x\u2217.\n2. If \u22072F (x(t)) is also Lipschitz continuous with parameter L\u0302 and x(t) satisfies\n\u2016x(t) \u2212 x\u2217\u2016 \u2264 \u00b5 L\u0302\u03ba \u03bd(t),\nwhere 0 < \u03bd(t) < 1, then it holds that\n\u2016\u2207F (x(t+1))\u2016M\u2217 \u2264 0 1 + \u03bd(t)\n1\u2212 \u03bd(t) \u2016\u2207F (x(t))\u2016M\u2217 +\n2 (1\u2212 0)2 L\u0302\u03ba \u00b5 \u221a \u00b5\n(1 + \u03bd(t))2\n1\u2212 \u03bd(t) \u2016\u2207F (x(t))\u20162M\u2217 .\nAlgorithm 4 Regularized Subsample Newton. 1: Input: x(0), 0 < \u03b4 < 1, regularizer parameter \u03b1, sample size |S| ; 2: for t = 0, 1, . . . until termination do 3: Select a sample set S, of size |S| and H(t) = 1|S| \u2211 j\u2208S \u2207 2fj(x (t)) + \u03b1I;\n4: Update x(t+1) = x(t) \u2212 [ H(t) ]\u22121 \u2207F (x(t));\n5: end for\nAppendix C. Proof of Lemma 1\nProof By Taylor\u2019s theorem, we have\n\u2207F (x(t+1)) =\u2207F (y(t+1)) +\u22072F (y(t+1))(\u2212p(t+1)) + o(p(t+1)) =\u2207F (y(t+1))\u2212\u22072F (y(t+1))[H(t+1)]\u22121\u2207F (y(t+1)) + o(p(t+1)) =\u2207F (y(t+1))\u2212\u22072F (x\u2217)[H(t+1)]\u22121\u2207F (y(t+1))\u2212 (\u22072F (x\u2217)\u2212\u22072F (y(t+1)))[H(t+1)]\u22121\u2207F (y(t+1)) + o(p(t+1))\n= [ \u22072F (x\u2217) ] 1 2 ( I \u2212 [\u22072F (x\u2217)] 12 [H(t+1)]\u22121[\u22072F (x\u2217)] 12 ) [ \u22072F (x\u2217) ]\u2212 12 \u2207F (y(t+1)) + (\u22072F (y(t+1))\u2212\u22072F (x\u2217))[H(t+1)]\u22121\u2207F (y(t+1)) + o(p(t+1)).\nFor \u2207F (y(t+1)), we have\n\u2207F (y(t+1)) =\u2207F (x(t) + \u03b8s(t)) =\u2207F (x(t)) + \u03b8\u22072F (x(t))(s(t)) + o(s(t)) =\u2207F (x(t)) + \u03b8(\u2207F (x(t))\u2212\u2207F (x(t\u22121))) + o(s(t)) =(1 + \u03b8)\u2207F (x(t))\u2212 \u03b8\u2207F (x(t\u22121)) + o(s(t)).\nBesides, we have\no(s(t)) =o(x(t) \u2212 x(t\u22121)) = o(x(t) \u2212 x\u2217 \u2212 (x(t\u22121) \u2212 x\u2217)) =o(\u2207F (x(t))) + o(\u2207F (x(t\u22121))),\nwhere the last equality is because \u2207F (x) is L-Lipschitz continuous. Hence, it holds that\n\u2207F (y(t+1)) = (1 + \u03b8)\u2207F (x(t))\u2212 \u03b8\u2207F (x(t\u22121)) + o(\u2207F (x(t)) +\u2207F (x(t\u22121))) (10)\nFor (\u22072F (y(t+1)) \u2212 \u22072F (x\u2217))[H(t+1)]\u22121\u2207F (y(t+1)), we show that it is of order o(\u2207F (y(t+1))) as follows. First, if \u22072F (x) is not Lipschitz continuous, then there exists a \u03b3 such that when \u2016y \u2212 x\u2217\u2016 \u2264 \u03b3, it holds that\n\u2016\u22072F (y)\u2212\u22072F (x\u2217)\u2016 = o(1).\nSuch \u03b3 exists because \u22072F (x) is continuous near optimal point x\u2217. Hence, (\u22072F (y(t+1)) \u2212 \u22072F (x\u2217))[H(t+1)]\u22121\u2207F (y(t+1)) is of order o(\u2207F (y(t+1))) when y(t+1) is sufficient close to x\u2217.\nIf \u22072F (x) is L\u0302-Lipschitz continuous and F (x) is \u00b5-strongly convex, then we have\n\u2016\u22072F (y(t+1))\u2212\u22072F (x\u2217)\u2016 \u2264 L\u0302\u2016y(t+1) \u2212 x\u2217\u2016 \u2264 L\u0302 \u00b5 \u2016\u2207F (y(t+1))\u2016.\nThen, it holds that\n\u2016(\u22072F (y(t+1))\u2212\u22072F (x\u2217))[H(t+1)]\u22121\u2207F (y(t+1))\u2016 = O(\u2016\u2207F (y(t+1))\u20162).\nAlgorithm 5 Conjugate Gradient Descent Method. 1: Input: A, b,x0, and tol; 2: Set r0 = Ax0 \u2212 b, p0 = \u2212r0, k = 0; 3: while \u2016rk\u2016 > tol do 4: Calculate \u03b1k =\nrTk rk pT k Apk\n; 5: Calculate xk+1 = xk + \u03b1kpk and rk+1 = rk + \u03b1kApk; 6: Calculate \u03b2k+1 = rTk+1rk+1\nrT k rk and pk+1 = \u2212rk+1 + \u03b2k+1pk; 7: k = k + 1; 8: end while 9: Output: xk.\nBesides, because of p(t+1) = [H(t+1)]\u22121\u2207F (y(t+1)), we have\no(p(t+1)) = o(\u2207F (y(t+1))).\nCombining Eqn. (10), we have\n(\u22072F (y(t+1))\u2212\u22072F (x\u2217))[H(t+1)]\u22121\u2207F (y(t+1)) = o(\u2207F (x(t))) + o(\u2207F (x(t\u22121)))\nand\no(p(t+1)) = o(\u2207F (x(t))) + o(\u2207F (x(t\u22121))).\nHence, we have the following result\n[\u22072F (x\u2217)]\u2212 12\u2207F (x(t+1)) = ( I \u2212 [\u22072F (x\u2217)] 12 [H(t+1)]\u22121[\u22072F (x\u2217)] 12 )( (1 + \u03b8) [ \u22072F (x\u2217) ]\u2212 12 \u2207F (x(t))\u2212 \u03b8 [\u22072F (x\u2217)]\u2212 12 \u2207F (x(t\u22121))) + o(\u2207F (x(t))) + o(\u2207F (x(t\u22121))).\nAppendix D. Proof of Theorem 2\nWe first give the following lemma which describes the solution form of the second-order difference equation.\nLemma 6 For the following homogeneous second-order difference equation\nz(t+2) + az(t+1) + bz(t) = 0,\nwe have 1. If a2 > 4b, and \u03b1 = \u2212(1/2)a+ \u221a (1/4)a2 \u2212 b and \u03b2 = \u2212(1/2)a\u2212 \u221a\n(1/4)a2 \u2212 b are two solution of equation x2 + ax+ b = 0, then the solution of above difference equation is of the form\nz(t) = \u03be\u03b1t + \u03d5\u03b2t\n2. If a2 = 4b, let \u03b1 = \u2212(1/2)a, then the solution of above difference equation is of the form\nz(t) = (\u03be + \u03d5t)\u03b1t.\n3. If a2 < 4b, then the solution of above difference equation is of the form\nz(t) = \u03be\u03b1t cos(\u03c9t+ \u03d5),\nwhere \u03b1 = \u221a b and \u03c9 = arccos(\u2212a/(2 \u221a b)).\n\u03be and \u03d5 are two coefficients determined by initial value z(0) and z(1).\nProof of Theorem 2 First, we have\n[\u22072F (x\u2217)]\u2212 12\u2207F (x(t+1)) = ( I \u2212 [\u22072F (x\u2217)] 12H\u22121[\u22072F (x\u2217)] 12 )( (1 + \u03b8) [ \u22072F (x\u2217) ]\u2212 12 \u2207F (x(t))\u2212 \u03b8 [\u22072F (x\u2217)]\u2212 12 \u2207F (x(t\u22121))) . Let U\u039bUT be the svd decomposition of I\u2212 [\u22072F (x\u2217)] 12H\u22121[\u22072F (x\u2217)] 12 , then we have \u039b(1, 1) = 1\u2212\u03c0. We denote Z(t+1) = UT [\u22072F (x\u2217)]\u2212 12\u2207F (x(t+1)), then we have the following difference equations\nZ(t+1) = (1 + \u03b8)\u039bZ(t) \u2212 \u03b8\u039bZ(t\u22121).\nSince \u039b is a diagonal matrix, we have\nz (t+1) i = \u03bbi(1 + \u03b8)z (t) i \u2212 \u03bbi\u03b8z (t\u22121) i , (11)\nwhere z(t)i is the i-th entry of Z (t) and \u03bbi = \u039b(i, i). Equation (11) is a second-order difference equation. And its solution depends on \u03b8 and \u03bbi. We first consider case i = 1 with \u039b(1, 1) = 1\u2212 \u03c0. By Lemma 6, we have\n1. If \u03b8 < 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 , then we have\nz (t) 1 = \u03be\u03b1 t + \u03d5\u03b2t,\nwhere\n\u03b1 = (1\u2212 \u03c0)(1 + \u03b8) +\n\u221a (1\u2212 \u03c0)2(1 + \u03b8)2 \u2212 4(1\u2212 \u03c0)\u03b8\n2 (12)\nand\n\u03b2 = (1\u2212 \u03c0)(1 + \u03b8)\u2212\n\u221a (1\u2212 \u03c0)2(1 + \u03b8)2 \u2212 4(1\u2212 \u03c0)\u03b8\n2 . (13)\n2. If \u03b8 = 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 , then we have\nz (t) 1 = (\u03be + \u03d5t)\u03b1 t\nwith \u03b1 = 1\u2212 \u221a \u03c0.\n3. If \u03b8 > 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 , then we have\nz (t) 1 = \u03be\u03b1 t cos(\u03c9t+ \u03d5), with \u03b1 = \u221a (1\u2212 \u03c0)\u03b8 and \u03c9 = arccos((1\u2212 \u03c0)(1 + \u03b8)/(2 \u221a (1\u2212 \u03c0)\u03b8)).\n\u03be and \u03d5 are coefficients determined by z(0)1 and z (1) 1 .\nNow, we consider the i 6= 1 cases. We denote \u03b3i = \u039b(i, i) with i = 2, ..., d. We have \u03b3i \u2264 1\u2212 \u03c0 because 1 \u2212 \u03c0 is the largest singular value of I \u2212 [\u22072F (x\u2217)] 12H\u22121[\u22072F (x\u2217)] 12 . Similar to above analysis, we can calculate the dominating convergence rate \u03b1i for z (t) i with fixed \u03b8. It is easy to check that \u03b1i \u2264 \u03b1 when \u03b8 is set. Hence, the convergence property of Z(t) is mainly decided by sequence z (t) 1 .\nBy choosing \u03b8 = 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 , after t iterations, we have\nz (t) 1 = (\u03be + \u03d5t)(1\u2212\n\u221a \u03c0)t.\nHence, it holds that \u2016Z(t)\u2016 \u2264 |\u03be + \u03d5t|(1\u2212 \u221a \u03c0)t. By the definition of Z(t), we obtain the result that\n\u2016\u2207F (x(t))\u2016 \u2264 \u221a \u03bbmax(\u22072F (x\u2217))|\u03be + \u03d5t|(1\u2212 \u221a \u03c0)t,\nwhere \u03be and \u03d5 are determined by the initial points.\nAppendix E. Convergence Property of Accelerated Second Order Method\nWe will discuss the convergence properties of accelerated second-order methods applying to general smooth convex functions. The idea behind this section is that general smooth convex function can be represent by a quadratic function plus a small perturbation when x(t) is close enough to the optimal point. Algorithm 2 can be regarded as a disturbed version of Algorithm 1.\nTherefore, the convergence behavior of general smooth convex functions can be described by a disturbed second-order difference equation formulated as\nz(t+1) \u2212 (1\u2212 \u03c0)(1 + \u03b8(t+1))z(t) + \u03b8(t+1)(1 + \u03b7(t))(1\u2212 \u03c0)z(t\u22121) = 0. (14)\nIn Eqn. (14), \u03c0 is fixed and \u03b8(t) and \u03b7(t) vary as t. In fact, a perturbation on \u03c0 can reduce to a perturbation on \u03b8 and \u03b7. Hence, we will analyze the influence of the perturbation on \u03b8 and \u03b7.\nE.1 Intuition\nFor concise representation, we represent Eqn. (14) as\nz(t) \u2212 (1\u2212 \u03c0)(1 + \u03b8)z(t\u22121) + \u03b8(1 + \u03b7)(1\u2212 \u03c0)z(t\u22122) = 0, (15)\nand the sequence Z\u0302(t) satisfies Eqn. (15). Then \u03b8 and \u03b7 are disturbed by and \u03b4 relatively, the difference equation is formalized as\nz(t+1) \u2212 (1\u2212 \u03c0)(1 + (1 + )\u03b8)z(t) + (1 + \u03b7)(1 + \u03b4)(1 + )\u03b8(1\u2212 \u03c0)z(t\u22121) = 0, (16)\nand the sequence Z(t) which satisfies Eqn. (16). Without loss of generality, we assume z(t\u22121) and z(t) are the second and third term of the sequence Z\u0302(t), that is Z\u0302(1) = z(t\u22121) and Z\u0302(2) = z(t). z(t\u22121) and z(t) are also the first two terms of Z(t), that is Z(0) = z(t\u22121) and Z(1) = z(t).\nWithout perturbations, Z(2) and Z\u0302(3) are equal. We will show that Z(2) = (1 + O( + \u03b4))Z\u0302(3). This shows that the convergence rate is only slightly perturbed if the perturbation is small.\nTherefore, if x(0) and x(1) are close to x\u2217 and H(t) in Algorithm 2 does not vary severely, then Algorithm 1 and Algorithm 2 converge with rate close to 1\u2212 \u221a \u03c0 if we choose \u03b8 close to 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 .\nE.2 Proof of Convergence Property\nNow we begin to prove that Z(2) = (1 +O( + \u03b4))Z\u0302(3).\nBy Lemma 6, sequence Z\u0302(t) and Z(t) are of different solution forms determined by \u03c0, \u03b8, and \u03b7. We will analyze the value of Z(2) case by case. However, we will not consider Z(2) and Z\u0302(3) of the form in the case 2 in Lemma 6. These cases will not happen in real application almost sure.\nBesides, we assume that parameter \u03b8 in Eqn. (15) and (16) are close to 1\u2212 \u221a \u03c0\n1+ \u221a \u03c0 . The parameter \u03b7\nis close to 0. Now, we begin to analyze case by case.\ncase (a): We consider the case that Z\u0302(1) = \u03be\u0302\u03b1\u0302 cos(\u03c9\u0302 + \u03d5\u0302) and Z\u0302(2) = \u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302), which are the first two terms of sequence Z(t) = \u03be\u03b1t cos(\u03c9t+ \u03d5), that is{\n\u03be\u0302\u03b1\u0302 cos(\u03c9\u0302 + \u03d5\u0302) = \u03be cos(\u03d5)\n\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302) = \u03be\u03b1 cos(\u03c9 + \u03d5)\nWe also have\n\u03b1\u0302 = \u221a (1\u2212 \u03c0)(1 + \u03b7)\u03b8\n\u03b1 = \u221a (1\u2212 \u03c0)(1 + )(1 + \u03b7)(1 + \u03b4)\u03b8\n\u03b1 cos(\u03c9) = (1\u2212 \u03c0)(1 + (1 + )\u03b8)\n2\n\u03b1\u0302 cos(\u03c9\u0302) = (1\u2212 \u03c0)(1 + \u03b8)\n2 .\nAnd z(t+1) = \u03be\u03b12 cos(2\u03c9 + \u03d5) satisfies\n\u03be\u03b12 cos(2\u03c9 + \u03d5) =\u03be\u03b12 cos(\u03c9 + \u03d5) cos(\u03c9)\u2212 \u03be\u03b12 sin(\u03c9 + \u03d5) sin(\u03c9) =\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302)\u03b1 cos(\u03c9)\u2212 \u03b1 \u221a \u03be2\u03b12 \u2212 (\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302))2 sin(\u03c9).\nBesides, it holds that\n\u03be2\u03b12 sin2(\u03c9) =\u03be\u03022\u03b1\u03022\u03b12 cos2(\u03c9\u0302 + \u03d5\u0302) sin2(\u03c9) + ( \u03be\u0302\u03b1\u0302\u03b1 cos(\u03c9\u0302 + \u03d5\u0302) cos(\u03c9)\u2212 \u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302) )2 =\u03be\u03022\u03b1\u03022 ( \u03b12 cos2(\u03c9\u0302 + \u03d5\u0302) sin2(\u03c9) + \u03b12 cos2(\u03c9\u0302 + \u03d5\u0302) cos2(\u03c9) + \u03b1\u03022 cos2(2\u03c9\u0302 + \u03d5\u0302) ) +\n\u2212 \u03be\u03022\u03b1\u03022 (2\u03b1\u03b1\u0302 cos(\u03c9\u0302 + \u03d5\u0302) cos(\u03c9) cos(2\u03c9\u0302 + \u03d5\u0302)) =\u03be\u03022\u03b1\u03022 ( \u03b12 cos2(\u03c9\u0302 + \u03d5\u0302) + \u03b1\u03022 cos2(2\u03c9\u0302 + \u03d5\u0302)\u2212 2\u03b1\u03b1\u0302 cos(\u03c9\u0302 + \u03d5\u0302) cos(\u03c9) cos(2\u03c9\u0302 + \u03d5\u0302) ) .\nHence, we have\n\u03be2\u03b12 sin2(\u03c9)\u2212 (\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302))2 sin2(\u03c9) =\u03be\u03022\u03b1\u03022 ( \u03b12 cos2(\u03c9\u0302 + \u03d5\u0302) + \u03b1\u03022 cos2(2\u03c9\u0302 + \u03d5\u0302)\u2212 2\u03b1\u03b1\u0302 cos(\u03c9\u0302 + \u03d5\u0302) cos(\u03c9) cos(2\u03c9\u0302 + \u03d5\u0302) ) \u2212\u03be\u03022\u03b1\u03022 ( \u03b1\u03022 cos2(2\u03c9\u0302 + \u03d5\u0302) sin2(\u03c9)\n) =\u03be\u03022\u03b1\u03022 ( \u03b12 cos2(\u03c9\u0302 + \u03d5\u0302) + \u03b1\u03022 cos2(2\u03c9\u0302 + \u03d5\u0302) cos2(\u03c9)\u2212 2\u03b1\u03b1\u0302 cos(\u03c9\u0302 + \u03d5\u0302) cos(\u03c9) cos(2\u03c9\u0302 + \u03d5\u0302)\n) =\u03be\u03022\u03b1\u03022 (\u03b1 cos(\u03c9\u0302 + \u03d5\u0302)\u2212 \u03b1\u0302 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9))2 .\nWe obtain\n\u03be\u03b12 cos(2\u03c9 + \u03d5) =\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302)\u03b1 cos(\u03c9) + \u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302)\u03b1 cos(\u03c9)\u2212 \u03be\u0302\u03b1\u0302\u03b12 cos(\u03c9\u0302 + \u03d5\u0302).\nFurthermore, we have\n\u03be\u0302\u03b1\u03023 cos(3\u03c9\u0302 + \u03d5\u0302)\u2212 \u03be\u03b12 cos(2\u03c9 + \u03d5)\n=\u03be\u0302\u03b1\u03023 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302)\u2212 \u03be\u0302\u03b1\u03023 sin(2\u03c9\u0302 + \u03d5\u0302) sin(\u03c9\u0302)\u2212 \u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302)\u03b1 cos(\u03c9)\n\u2212 (\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302)\u03b1 cos(\u03c9)\u2212 \u03be\u0302\u03b1\u0302\u03b12 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302)\u2212 \u03be\u0302\u03b1\u0302\u03b12 sin(2\u03c9\u0302 + \u03d5\u0302) sin(\u03c9\u0302))\n=\u03be\u0302\u03b1\u03023 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302)\u2212 \u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302)\u03b1 cos(\u03c9)\n\u2212 (\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302)\u03b1 cos(\u03c9)\u2212 \u03be\u0302\u03b1\u0302\u03b12 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302)\n\u2212 (\u03be\u0302\u03b1\u03023 sin(2\u03c9\u0302 + \u03d5\u0302) sin(\u03c9\u0302)\u2212 \u03be\u0302\u03b1\u0302\u03b12 sin(2\u03c9\u0302 + \u03d5\u0302) sin(\u03c9\u0302)) =\u2212 ( \u03b8\n1 + \u03b8\n) \u03be\u0302\u03b1\u03023 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302)\u2212 ( \u03b8\n1 + \u03b8 \u2212 \u2212 \u03b4 \u2212 \u03b4\n) \u03be\u0302\u03b1\u03023 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302)\n\u2212 (\u2212 \u2212 \u03b4 \u2212 \u03b4)\u03be\u0302\u03b1\u03023 sin(2\u03c9\u0302 + \u03d5\u0302) sin(\u03c9\u0302)\n=\u2212 2\u03b8 1 + \u03b8 \u03be\u0302\u03b1\u03023 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302) + 2( + \u03b4 + \u03b4)\u03be\u0302\u03b1\u03023 sin(2\u03c9\u0302 + \u03d5\u0302) sin(\u03c9\u0302).\nTherefore, we have \u03be\u03b12 cos(\u03c9 + \u03d5) = (1 +O( + \u03b4))\u03be\u0302\u03b1\u03023 cos(\u03c9\u0302 + \u03d5\u0302).\nThis means Z(2) = (1 +O( + \u03b4))Z\u0302(3).\ncase (b): Now we consider the case that Z\u0302(1) = \u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302 and Z\u0302(2) = \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022, which are the first two terms of sequence Z(t) = \u03be\u03b1t + \u03d5\u03b2t, that is{\n\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302 = \u03be + \u03d5\n\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 = \u03be\u03b1+ \u03d5\u03b2\nwhere\n\u03b1\u0302 = (1\u2212 \u03c0)(1 + \u03b8)\n2 +\n\u221a 1\n4 (1\u2212 \u03c0)2(1 + \u03b8)2 \u2212 (1 + \u03b7)(1\u2212 \u03c0)\u03b8\n\u03b2\u0302 = (1\u2212 \u03c0)(1 + \u03b8) 2 \u2212 \u221a 1 4 (1\u2212 \u03c0)2(1 + \u03b8)2 \u2212 (1 + \u03b7)(1\u2212 \u03c0)\u03b8\n\u03b1 = (1\u2212 \u03c0)(1 + (1 + )\u03b8)\n2 +\n\u221a 1\n4 (1\u2212 \u03c0)2(1 + (1 + )\u03b8)2 \u2212 (1\u2212 \u03c0)(1 + \u03b7)(1 + \u03b4)(1 + )\u03b8\n\u03b2 = (1\u2212 \u03c0)(1 + (1 + )\u03b8) 2 \u2212 \u221a 1 4 (1\u2212 \u03c0)2(1 + (1 + )\u03b8)2 \u2212 (1\u2212 \u03c0)(1 + \u03b7)(1 + \u03b4)(1 + )\u03b8.\nWe have\n\u03be\u03b12 + \u03d5\u03b22 =\u03be\u03b12 + \u03d5\u03b2\u03b1\u2212 \u03d5\u03b2\u03b1+B\u03b22\n=\u03b1(\u03be\u03b1+ \u03d5\u03b2) + \u03d5\u03b2(\u03b2 \u2212 \u03b1)\n=(\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1\u2212 \u03d5\u03b2(\u03b1\u2212 \u03b2).\nWe also have\n\u03d5(\u03b1\u2212 \u03b2) =\u03be\u0302\u03b1\u0302\u03b1+ \u03d5\u0302\u03b2\u0302\u03b1\u2212 \u03be\u0302\u03b1\u03022 \u2212 \u03d5\u0302\u03b2\u03022.\nHence, we obtain\n\u03be\u03b12 + \u03d5\u03b22 =(\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1\u2212 \u03b2(\u03be\u0302\u03b1\u0302\u03b1+ \u03d5\u0302\u03b2\u0302\u03b1\u2212 \u03be\u0302\u03b1\u03022 \u2212 \u03d5\u0302\u03b2\u03022) =\u03be\u0302\u03b1\u03022 ( \u03b1+ \u03b2 \u2212 \u03b1\u03b2\n\u03b1\u0302\u03b2\u0302 \u03b2\u0302\n) + \u03d5\u0302\u03b2\u03022 ( \u03b1+ \u03b2 \u2212 \u03b1\u03b2\n\u03b1\u0302\u03b2\u0302 \u03b1\u0302\n) .\nThere exists \u03b1\u03b2\n\u03b1\u0302\u03b2\u0302 = (1\u2212 \u03c0)(1 + \u03b7)(1 + \u03b4)(1 + )\u03b8 (1 + \u03b7)(1\u2212 \u03c0)\u03b8 = (1 + \u03b4)(1 + ).\nHence, we have\n\u03b1+ \u03b2 \u2212 \u03b1\u03b2 \u03b1\u0302\u03b2\u0302 \u03b2\u0302 = 2(1\u2212 \u03c0)(1 + (1 + )\u03b8) 2 \u2212 (1 + \u03b4)(1 + )(1\u2212 \u03c0)(1 + \u03b8) 2\n+ (1 + \u03b4)(1 + )\n\u221a 1\n4 (1\u2212 \u03c0)2(1 + \u03b8)2 \u2212 (1 + \u03b7)(1\u2212 \u03c0)\u03b8\n= (1\u2212 \u03c0)(1\u2212 \u2212 \u03b4 \u2212 \u03b4 + (1 + \u2212 \u03b4 \u2212 \u03b4)\u03b8)\n2\n+ (1 + \u03b4)(1 + )\n\u221a 1\n4 (1\u2212 \u03c0)2(1 + \u03b8)2 \u2212 (1 + \u03b7)(1\u2212 \u03c0)\u03b8\n=(1 +O( + \u03b4))\u03b1\u0302.\nSimilarly, we have\n\u03b1+ \u03b2 \u2212 \u03b1\u03b2 \u03b1\u0302\u03b2\u0302 \u03b1\u0302 = (1 +O( + \u03b4))\u03b2\u0302.\nTherefore, we obtain\n\u03be\u03b12 + \u03d5\u03b22 = (1 +O( + \u03b4))(\u03be\u0302\u03b1\u03023 + \u03d5\u0302\u03b2\u03023)\nThis means Z(2) = (1 +O( + \u03b4))Z\u0302(3).\ncase (c): We consider the case that Z\u0302(1) = \u03be\u0302\u03b1\u0302 cos(\u03c9\u0302+ \u03d5\u0302) and Z\u0302(2) = \u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302+ \u03d5\u0302), which is the first two terms of sequence Z(t) = \u03be\u03b1t + \u03d5\u03b2t, that is{\n\u03be\u0302\u03b1\u0302 cos(\u03c9\u0302 + \u03d5\u0302) = \u03be + \u03d5\n\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302) = \u03be\u03b1+ \u03d5\u03b2\nwhere \u03b1\u0302 = \u221a (1\u2212 \u03c0)(1 + \u03b7)\u03b8\n\u03b1\u0302 cos(\u03c9\u0302) = (1\u2212 \u03c0)(1 + \u03b8)\n2\n\u03b1 = (1\u2212 \u03c0)(1 + (1 + )\u03b8)\n2 +\n\u221a 1\n4 (1\u2212 \u03c0)2(1 + (1 + )\u03b8)2 \u2212 (1\u2212 \u03c0)(1 + \u03b7)(1 + \u03b4)(1 + )\u03b8\n\u03b2 = (1\u2212 \u03c0)(1 + (1 + )\u03b8) 2 \u2212 \u221a 1 4 (1\u2212 \u03c0)2(1 + (1 + )\u03b8)2 \u2212 (1\u2212 \u03c0)(1 + \u03b7)(1 + \u03b4)(1 + )\u03b8.\nWe have\n\u03be\u03b12 + \u03d5\u03b22\n=\u03be\u03b12 + \u03d5\u03b2\u03b1\u2212 \u03d5\u03b2\u03b1+ \u03d5\u03b22\n=\u03b1(\u03be\u03b1+ \u03d5\u03b2) + \u03d5\u03b2(\u03b2 \u2212 \u03b1)\n=\u03be\u0302\u03b1\u03022\u03b1 cos(2\u03c9\u0302 + \u03d5\u0302) + \u03b2\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302)\u2212 \u03be\u0302\u03b1\u0302\u03b1\u03b2 cos(\u03c9\u0302 + \u03d5\u0302)\n=\u03be\u0302\u03b1\u03022(1\u2212 \u03c0)(1 + (1 + )\u03b8) cos(2\u03c9\u0302 + \u03d5\u0302)\u2212 (1\u2212 \u03c0)(1 + \u03b7)(1 + \u03b4)(1 + )\u03b8\u03be\u0302\u03b1\u0302 cos(\u03c9\u0302 + \u03d5\u0302)\n=2\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302) ( 1 + \u03b8\n1 + \u03b8\n) \u03b1\u0302 cos(\u03c9\u0302)\u2212 (1 + \u03b4)(1 + )\u03be\u0302\u03b1\u03023 cos(\u03c9\u0302 + \u03d5\u0302).\nWe also have\n\u03be\u0302\u03b1\u03023 cos(3\u03c9\u0302 + \u03d5\u0302)\u2212 (\u03be\u03b12 + \u03d5\u03b22)\n=\u03be\u0302\u03b1\u03023 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302)\u2212 \u03be\u0302\u03b1\u03023 sin(2\u03c9\u0302 + \u03d5\u0302) sin(\u03c9\u0302) \u2212 2\u03be\u0302\u03b1\u03022 cos(2\u03c9\u0302 + \u03d5\u0302) ( 1 + \u03b8\n1 + \u03b8\n) \u03b1\u0302 cos(\u03c9\u0302) + (1 + \u03b4)(1 + )\u03be\u0302\u03b1\u03023 cos(\u03c9\u0302 + \u03d5\u0302)\n=\u2212 2\u03b8 1 + \u03b8 \u03be\u0302\u03b1\u03023 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302)\u2212 \u03be\u0302\u03b1\u03023 cos(\u03c9\u0302 + \u03d5\u0302) + (1 + \u03b4)(1 + )\u03be\u0302\u03b1\u03023 cos(\u03c9\u0302 + \u03d5\u0302)\n=\u2212 2\u03b8 1 + \u03b8 \u03be\u0302\u03b1\u03023 cos(2\u03c9\u0302 + \u03d5\u0302) cos(\u03c9\u0302) + ( + \u03b4 + \u03b4)\u03be\u0302\u03b1\u03023 cos(\u03c9\u0302 + \u03d5\u0302)\n=O( + \u03b4)\u03be\u0302\u03b1\u03023 cos(3\u03c9\u0302 + \u03d5\u0302).\nTherefore, we have \u03be\u03b12 + \u03d5\u03b22 = (1 +O( + \u03b4))\u03be\u0302\u03b1\u03023 cos(3\u03c9\u0302 + \u03d5\u0302)\nThis means Z(2) = (1 +O( + \u03b4))Z\u0302(3).\ncase (d): Now we consider the case that Z\u0302(1) = \u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302 and Z\u0302(2) = \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022, which are the first two terms of sequence Z(t) = \u03be\u03b1t cos(\u03c9t+ \u03d5), that is{\n\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302 = \u03be cos(\u03d5)\n\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 = \u03be\u03b1 cos(\u03c9 + \u03d5)\nwhere\n\u03b1\u0302 = (1\u2212 \u03c0)(1 + \u03b8)\n2 +\n\u221a 1\n4 (1\u2212 \u03c0)2(1 + \u03b8)2 \u2212 (1\u2212 \u03c0)(1 + \u03b7)\u03b8\n\u03b2\u0302 = (1\u2212 \u03c0)(1 + \u03b8) 2 \u2212 \u221a 1 4 (1\u2212 \u03c0)2(1 + \u03b8)2 \u2212 (1\u2212 \u03c0)(1 + \u03b7)\u03b8\n\u03b1 = \u221a (1\u2212 \u03c0)(1 + )(1 + \u03b7)(1 + \u03b4)\u03b8\n\u03b1 cos(\u03c9) = (1\u2212 \u03c0)(1 + (1 + )\u03b8)\n2 .\nFirst, we can derive that\n\u03be\u03b1 cos(\u03c9) cos(\u03d5)\u2212 \u03be\u03b1 sin(\u03c9) sin(\u03d5) = \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 \u21d2\u03be\u03b1 sin(\u03c9) sin(\u03d5) = (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b1 cos(\u03c9)\u2212 ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 ) \u21d2\u03be2\u03b12 sin2(\u03c9)(1\u2212 cos2(\u03d5)) = ( (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b1 cos(\u03c9)\u2212 ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022\n))2 \u21d2\u03be2\u03b12 sin2(\u03c9) = ( (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b1 cos(\u03c9)\u2212 ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 ))2 + (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)2\u03b12 sin2(\u03c9).\nHence, we have \u03be2\u03b12 sin2(\u03c9) = ( (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b1 cos(\u03c9)\u2212 ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 ))2 + (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)2\u03b12 sin2(\u03c9)\n=(\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)2\u03b12 cos2(\u03c9) + ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 )2 \u2212 2 ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 ) (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b1 cos(\u03c9)\n+ (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)2\u03b12 sin2(\u03c9) =(\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)2\u03b12 + ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 )2 \u2212 2 ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 ) (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b1 cos(\u03c9).\nWe have\n\u03be\u03b12 cos(2\u03c9 + \u03d5)\n=\u03be\u03b12 cos(\u03c9 + \u03d5) cos(\u03c9)\u2212 \u03be\u03b12 sin(\u03c9 + \u03d5) sin(\u03c9) =(\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1 cos(\u03c9)\u2212 \u03b1 \u221a \u03be2\u03b12 sin2(\u03c9)\u2212 (\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)2 sin2(\u03c9)\n=(\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1 cos(\u03c9) \u2212 \u03b1 \u221a (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)2\u03b12 + ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 )2 \u2212 2 ( \u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022 ) (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b1 cos(\u03c9)\u2212 (\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)2 sin2(\u03c9)\n=(\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1 cos(\u03c9) + (\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1 cos(\u03c9)\u2212 (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b12.\nWe also have\n\u03be\u0302\u03b1\u03023 + \u03d5\u0302\u03b2\u03023 \u2212 \u03be\u03b12 cos(2\u03c9 + \u03d5) =\u03be\u0302\u03b1\u03023 + \u03d5\u0302\u03b2\u03023 \u2212 (\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1 cos(\u03c9)\u2212 ( (\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1 cos(\u03c9)\u2212 (\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b12 )\n=\u03be\u0302\u03b1\u03022(\u03b1\u0302\u2212 \u03b1 cos(\u03c9)) + \u03d5\u0302\u03b2\u03022(\u03b2\u0302 \u2212 \u03b1 cos(\u03c9))\u2212 ( (\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022)\u03b1 cos(\u03c9)\u2212 (1 + )(1 + \u03b4)(\u03be\u0302\u03b1\u0302+ \u03d5\u0302\u03b2\u0302)\u03b1\u0302\u03b2\u0302 )\n=\u03be\u0302\u03b1\u03022(\u03b1\u0302\u2212 \u03b1 cos(\u03c9)) + \u03d5\u0302\u03b2\u03022(\u03b2\u0302 \u2212 \u03b1 cos(\u03c9)) \u2212 ( (\u03be\u0302\u03b1\u03022(\u03b1 cos(\u03c9)\u2212 (1 + )(1 + \u03b4)\u03b2\u0302) + \u03d5\u0302\u03b2\u03022(\u03b1 cos(\u03c9)\u2212 (1 + )(1 + \u03b4)\u03b1\u0302) )\n=\u03be\u0302\u03b1\u03022(\u03b1\u0302+ \u03b2\u0302 \u2212 2\u03b1 cos(\u03c9)) + \u03d5\u0302\u03b2\u03022(\u03b1\u0302+ \u03b2\u0302 \u2212 2\u03b1 cos(\u03c9)) + ( + \u03b4 + \u03b4)(\u03be\u0302\u03b1\u03022\u03b2\u0302 + \u03d5\u0302\u03b2\u03022\u03b1\u0302)\n= (1\u2212 \u03c0)\u03b8(\u03be\u0302\u03b1\u03022 + \u03d5\u0302\u03b2\u03022) + ( + \u03b4 + \u03b4)(\u03be\u0302\u03b1\u03022\u03b2\u0302 + \u03d5\u0302\u03b2\u03022\u03b1\u0302)\n= ( \u03be\u0302\u03b1\u03023 \u03b2\u0302\n1 + \u03b7 + \u03d5\u0302\u03b2\u03023\n\u03b1\u0302\n1 + \u03b7\n) + ( + \u03b4 + \u03b4)(\u03be\u0302\u03b1\u03022\u03b2\u0302 + \u03d5\u0302\u03b2\u03022\u03b1\u0302).\nBy the assumption, \u03b1\u0302 and \u03b2\u0302 are close to each other. Hence, it holds that\n\u03be\u0302\u03b1\u03023 + \u03d5\u0302\u03b2\u03023 \u2212 \u03be\u03b12 cos(2\u03c9 + \u03d5) = O( + \u03b4)(\u03be\u0302\u03b1\u03023 + \u03d5\u0302\u03b2\u03023).\nThis means Z(2) = (1 +O( + \u03b4))Z\u0302(3)."}], "references": [{"title": "Katyusha: The first direct acceleration of stochastic gradient methods", "author": ["Zeyuan Allen-Zhu"], "venue": "arXiv preprint arXiv:1603.05953,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM journal on imaging sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "On the use of stochastic hessian information in optimization methods for machine learning", "author": ["Richard H Byrd", "Gillian M Chin", "Will Neveitt", "Jorge Nocedal"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["Murat A Erdogdu", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "An optimal randomized incremental gradient method", "author": ["Guanghui Lan", "Yi Zhou"], "venue": "arXiv preprint arXiv:1507.02000,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J Smola"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Yurii Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1983}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence", "author": ["Mert Pilanci", "Martin J. Wainwright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The annals of mathematical statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1951}, {"title": "Sub-sampled newton methods ii: Local convergence rates", "author": ["Farbod Roosta-Khorasani", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1601.04738,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "A stochastic gradient method with an exponential convergence _rate for finite training sets", "author": ["Nicolas L Roux", "Mark Schmidt", "Francis R Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Sub-sampled newton methods with non-uniform sampling", "author": ["Peng Xu", "Jiyan Yang", "Farbod Roosta-Khorasani", "Christopher R\u00e9", "Michael W Mahoney"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A unifying framework for convergence analysis of approximate newton methods", "author": ["Haishan Ye", "Luo Luo", "Zhihua Zhang"], "venue": "arXiv preprint arXiv:1702.08124,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}], "referenceMentions": [{"referenceID": 3, "context": "The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the computational cost [4, 10, 14].", "startOffset": 106, "endOffset": 117}, {"referenceID": 7, "context": "The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the computational cost [4, 10, 14].", "startOffset": 106, "endOffset": 117}, {"referenceID": 11, "context": "The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the computational cost [4, 10, 14].", "startOffset": 106, "endOffset": 117}, {"referenceID": 5, "context": "Hence, many variants have been proposed to improve the convergence rate of SGD [8, 16, 17, 20].", "startOffset": 79, "endOffset": 94}, {"referenceID": 13, "context": "Hence, many variants have been proposed to improve the convergence rate of SGD [8, 16, 17, 20].", "startOffset": 79, "endOffset": 94}, {"referenceID": 14, "context": "Hence, many variants have been proposed to improve the convergence rate of SGD [8, 16, 17, 20].", "startOffset": 79, "endOffset": 94}, {"referenceID": 8, "context": "For the first-order methods which only make use of the gradient information, Nestrov\u2019s acceleration technique is a very useful tool [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.", "startOffset": 88, "endOffset": 95}, {"referenceID": 9, "context": "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.", "startOffset": 88, "endOffset": 95}, {"referenceID": 0, "context": "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.", "startOffset": 145, "endOffset": 151}, {"referenceID": 6, "context": "It greatly improves the convergence of gradient descent [11], proximal gradient descent [2, 12], and stochastic gradient with variance reduction [1, 9], etc.", "startOffset": 145, "endOffset": 151}, {"referenceID": 12, "context": "To conquer this weakness, one proposed a sub-sampled Newton which only samples a subset of functions fi randomly to construct a sub-sampled Hessian [15, 3, 18] .", "startOffset": 148, "endOffset": 159}, {"referenceID": 2, "context": "To conquer this weakness, one proposed a sub-sampled Newton which only samples a subset of functions fi randomly to construct a sub-sampled Hessian [15, 3, 18] .", "startOffset": 148, "endOffset": 159}, {"referenceID": 15, "context": "To conquer this weakness, one proposed a sub-sampled Newton which only samples a subset of functions fi randomly to construct a sub-sampled Hessian [15, 3, 18] .", "startOffset": 148, "endOffset": 159}, {"referenceID": 10, "context": "Pilanci and Wainwright [13] applied the sketching technique to alleviate the computational burden of computing Hessian and brought up sketch Newton.", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "Regularized sub-sampled Newton methods were also devised to deal with the ill-condition problem [5, 15].", "startOffset": 96, "endOffset": 103}, {"referenceID": 12, "context": "Regularized sub-sampled Newton methods were also devised to deal with the ill-condition problem [5, 15].", "startOffset": 96, "endOffset": 103}, {"referenceID": 16, "context": "[19] cast these stochastic second-order procedures into a so-called approximate Newton framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Then Algorithm 2 will become the approximate Newton method defined in [19].", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "Besides, since AccRegSN makes use of more curvature information than Nestrov\u2019s accelerated gradient descent (AGD) method [11], AccRegSN should converge faster than AGD theoretically.", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "entries from U(0, 1), where U(0, 1) means uniform distribution on [0, 1].", "startOffset": 66, "endOffset": 72}, {"referenceID": 10, "context": "Notice that the size and dimension of dataset are close to each other, so the sketch Newton method [13, 18] can not be used.", "startOffset": 99, "endOffset": 107}, {"referenceID": 15, "context": "Notice that the size and dimension of dataset are close to each other, so the sketch Newton method [13, 18] can not be used.", "startOffset": 99, "endOffset": 107}], "year": 2017, "abstractText": "Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted much attention due to their low computational cost in each iteration. However, these algorithms might perform poorly especially if it is hard to approximate the Hessian well and efficiently. As far as we know, there is no effective way to handle this problem. In this paper, we resort to Nestrov\u2019s acceleration technique to improve the convergence performance of a class of second-order methods called approximate Newton. We give a theoretical analysis that Nestrov\u2019s acceleration technique can improve the convergence performance for approximate Newton just like for first-order methods. We accordingly propose an accelerated regularized sub-sampled Newton. Our accelerated algorithm performs much better than the original regularized sub-sampled Newton in experiments, which validates our theory empirically. Besides, the accelerated regularized subsampled Newton has good performance comparable to or even better than state-of-art algorithms.", "creator": "LaTeX with hyperref package"}}}