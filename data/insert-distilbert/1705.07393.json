{"id": "1705.07393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "Recurrent Additive Networks", "abstract": "notably we introduce recurrent additive networks ( rans ), a new gated rnn which is distinguished uniquely by the use parameter of purely additive latent quantum state updates. at every time step, creating the new state is computed as a gated component - wise sum of the input and unlike the previous state, without any of the non - linearities commonly used in rnn transition dynamics. we formally show that ran states together are weighted sums of the input vectors, and that the gates only contribute to computing the weights of these sums. despite this relatively simple functional form, experiments demonstrate finding that rans outperform both symmetric lstms and grus on different benchmark modeling language modeling problems. this result shows that many of the non - linear computations in lstms itself and related networks model are not essential, too at least for the problems so we consider, shows and suggests that identifying the gates are doing more of the computational work than previously understood.", "histories": [["v1", "Sun, 21 May 2017 04:42:11 GMT  (87kb,D)", "http://arxiv.org/abs/1705.07393v1", null], ["v2", "Thu, 29 Jun 2017 14:37:32 GMT  (103kb)", "http://arxiv.org/abs/1705.07393v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kenton lee", "omer levy", "luke zettlemoyer"], "accepted": false, "id": "1705.07393"}, "pdf": {"name": "1705.07393.pdf", "metadata": {"source": "CRF", "title": "Recurrent Additive Networks", "authors": ["Kenton Lee", "Omer Levy", "Luke Zettlemoyer"], "emails": ["kentonl@cs.washington.edu", "omerlevy@cs.washington.edu", "lsz@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "Gated recurrent neural networks (GRNNs), such as long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRUs) (Cho et al., 2014), have become ubiquitous in natural language processing (NLP). GRNN\u2019s widespread popularity is at least in part due to their ability to model crucial language phenomena such as word order (Adi et al., 2017), syntactic structure (Linzen et al., 2016), and even long-range semantic dependencies (He et al., 2017). Like simple recurrent neural networks (S-RNNs) (Elman, 1990), they are able to learn non-linear functions of arbitrary-length input sequences, while at the same time alleviating the problem of vanishing gradients (Bengio et al., 1994) by including gated additive state updates. While GRNNs work well in practice for a wide range of tasks, it is often difficult to interpret what function they have learned.\nIn this paper, we introduce a new GRNN architecture that is much simpler than existing approaches (e.g. fewer parameters and fewer non-linearities), produces highly interpretable outputs, and improves performance significantly on benchmark language modeling tasks. More specifically, we propose recurrent additive networks (RANs), which are distinguished by their use of purely additive latent state updates. At every time step, the new state is computed as a gated component-wise sum of the input and the previous state. Unlike almost all existing RNNs, there are no non-linearities in the transition dynamics.\nOne advantage of simplifying the dynamics this way is that we can formally characterize the space of functions RANs compute. It is easy to show that the internal state of a RAN at each time step is simply a component-wise weighted sum of the input vectors up to that time. The only non-linearities come from the gates, which specify the weights for this sum. Because all computations are component-wise, RANs can directly select which part of each input element to retain at each time step, and in which proportions, leading to a highly expressive yet interpretable model.\n\u2217The first two authors contributed equally to this paper.\nar X\niv :1\n70 5.\n07 39\n3v 1\n[ cs\n.C L\n] 2\n1 M\nay 2\nDespite their relatively simplicity, RANs significantly outperform both LSTMs and GRUs on two language modeling benchmarks and yield comparable results on a third character-based language modeling task with far fewer parameters. To better understand this result, we show that it is possible to derive the RAN updates from LSTM equations by sequentially ablating two recurrent non-linearities. Experiments show that both simplifications improve performance, and suggest that additive connections, rather than the non-linear transition dynamics, are the driving force behind LSTM\u2019s success."}, {"heading": "2 Recurrent Additive Networks", "text": "In this section, we first formally define the RAN model and then show that it represents a relatively simple class of additive functions over the input vectors."}, {"heading": "2.1 Model Definition", "text": "We assume a sequence of input vectors {x1, . . . ,xn}, and define a network that produces a sequence of output vectors {h1, . . . ,hn}. All recurrences over time are mediated by a sequence of state vectors {c1, . . . , cn}, computed as follows for each time step t:2\nc\u0303t = Wcxxt it = \u03c3(Wicct\u22121 +Wixxt)\nft = \u03c3(Wfcct\u22121 +Wfxxt)\nct = it \u25e6 c\u0303t + ft \u25e6 ct\u22121 ht = g(ct)\n(1)\nThe new state ct is simply a weighted sum where two gates, it (input) and ft (forget), control the mixing of the content layer c\u0303t and the previous state ct\u22121. The content layer c\u0303t is a linear transformation Wx over the input xt, which is useful when the number of input dimensions di is different from the number of hidden dimensions dh (e.g. embedding one-hot vectors). We also include a non-recurrent output layer ht computed with a function g of the internal state ct. In our experiments, we use both g = tanh and the identity function. The weight matrices W\u2217 are free trainable parameters.\nThe content layer c\u0303t and the output function g are presented above to be consistent with other GRNN notation. However, the content layer c\u0303t in RANs is very simple and only serves to allow different input vector and state vector dimensions. Similarly, the output function g is not recurrent and should be considered external to the central recurrent computations. When these are not used, the RAN can be reduced to an even simpler form:\nit = \u03c3(Wicct\u22121 +Wixxt)\nft = \u03c3(Wfcct\u22121 +Wfxxt) ct = it \u25e6 xt + ft \u25e6 ct\u22121 (2)\nUnlike LSTMs and GRUs, RANs only use additive connections to update the latent state ct. RANs also use relatively fewer parameters. For example, the number of parameters (omitting biases) in an LSTM are 4d2h + 4dhdi, but only 2d 2 h + 3dhdi in a RAN. In Section 4, we explore their relationships more closely, showing that RANs are a significantly simplified variation of both LSTMs and GRUs. These simplifications, perhaps surprisingly, improve performance for language modeling (Section 3). They also lead to a highly interpretable model that can be careful analyzed, as we present in more detail in the rest of this section."}, {"heading": "2.2 Analysis", "text": "Another advantage of the relative simplicity of RANs is that we can formally characterize the space of functions that are used to compute the hidden states ct. In particular, each state is a component-wise\n2Throughout this paper, we omit bias terms for brevity. However, bias terms are always present. Operations such as Wxxt should always be interpreted as Wxxt + bx.\nweighted sum of the input with the form:\nct = it \u25e6 xt + ft \u25e6 ct\u22121\n= t\u2211 j=1 ( ij \u25e6 t\u220f k=j+1 fk ) \u25e6 xt\n= t\u2211 j=1 wtj \u25e6 xt\n(3)\nwhen considering the simpler RAN described in equation set (2).3 Each weight wtj is a product of the input gate ij (when its respective input xj was read) and every subsequent forget gate fk. An interesting property of these weights is that, like the gates, they are also soft component-wise binary filters. This also produces a highly interpretable model, where each component of each state can be directly traced back to the inputs that contributed the most to its sum."}, {"heading": "3 Language Modeling Experiments", "text": "We compare the RANs\u2019 performance to LSTMs, GRUs, and S-RNNs on three benchmark language modeling tasks.\nBenchmarks We use three language modeling benchmarks: Penn Treebank (PTB) (Marcus et al., 1993), Google\u2019s billion-word benchmark (BWB) (Chelba et al., 2014), and the Text8 character-based language modeling benchmark.4 For PTB and BWB, our vocabulary contained the top 10,000 words in each training set, while other tokens were marked as <unk>. Text8 uses 27 character types as input (lowercase a\u2013z and space).\nModels We compare various recurrent architectures: simple RNN (S-RNN), GRU, LSTM, and RAN. For RANs, we test two variants: a linear RAN with an identity output function (identity), and a non-linear RAN with a tanh output function (tanh).\nTo control for external variables, we implement a very lightweight language model around each of the five different RNNs. We represent each input token wt (word or character) as a 256-dimension embedding xt (di = 256). These word embeddings xt are provided to the RNN to produce the final outputs ht, which always have 1024 dimensions (dh = 1024). Finally, we project ht onto a vocabulary-sized softmax layer to predict the next token wt+1.\nTraining We applied 50% dropout (Srivastava et al., 2014) immediately before, after, and inside each of the RNNs (using variational dropout (Gal and Ghahramani, 2016)). We optimized the cross-entropy loss using Adam (Kingma and Ba, 2014), using a batch size of 512. We ran 100 epochs on PTB, 1 epoch on BWB (due to its size), and 20 epochs on Text8.\nResults Table 1 shows the performance for different architectures. In both word-based benchmarks (PTB and BWB), non-linear RAN substantially outperforms LSTM and GRU, yielding approximately 10%-20% relative perplexity reduction. On the character-based task, the relative difference between the best model (LSTM) to non-linear RAN is less than 1%; however, RAN reaches its result with about half as many parameters as LSTM. Overall, it appears that RANs are \u201cdoing more for less\u201d, and that there is a real advantage in using them for language modeling tasks.\nPerhaps an even more remarkable result is the fact that linear RANs \u2013 which use the identity as the output function g and whose only non-linearities are in the gates \u2013 are still performing comparably or even better than LSTMs and GRUs. This observation begs the question: how is it possible that a simple weighted sum outperforms LSTMs? In Section 4, we show that LSTMs (and similarly GRUs) are implicitly computing some form of component-wise weighted sum as well, and that this computation is key to their success.\n3The state is also a linear function of the inputs in the more general form (equation set (1)). However, it is a weighted sum of linearly transformed inputs, instead of a weighted sum of the input vectors themselves.\n4http://mattmahoney.net/dc/textdata"}, {"heading": "4 The Role of Recurrent Additive Networks in Long Short-Term Memory", "text": "The need for LSTMs is typically motivated by the fact that they can ease the vanishing gradient problem found in simple RNNs (S-RNNs) (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997). By introducing cell states that are controlled by a set of gates, LSTMs enable shortcuts through which gradients can flow easily when learning with backpropagation. This mechanism enables learning of long-distance dependencies while preserving the expressive power of recurrent non-linearities provided by S-RNNs.\nRather than viewing the gating mechanism as simply an auxiliary mechanism to address a learning problem, we present an alternate of view of LSTMs that emphasizes the modeling strengths of the gates. We argue that it is possible to reinterpret LSTMs as a hybrid of two other recurrent architectures: (1) S-RNNs and (2) RANs. More specifically, LSTMs can be seen as computing a content layer using an S-RNN, which is then aggregated into a weighted sum using a RAN.\nWe show empirically that the recurrent non-linearity provided by S-RNN is not required for language modeling, and that the RAN is in fact performing the heavy lifting. To better understand this result, we derive the RAN using two modifications of the LSTMs: a simplification of the content layer (Section 4.2), and a simplification of the output layer (Section 4.3). Figures 1(a)-1(d) illustrate the derivation process, on which we will elaborate more formally below. We also provide experimental evidence, shown in Table 2, that these simplifications maintain or improve model performance."}, {"heading": "4.1 LSTM as a Hybrid of S-RNN and RAN", "text": "We first demonstrate the two LSTM sub-components of LSTM by dissecting its definition: c\u0303t = tanh(Wchht\u22121 +Wcxxt)\nit = \u03c3(Wihht\u22121 +Wixxt)\nft = \u03c3(Wfhht\u22121 +Wfxxt)\not = \u03c3(Wohht\u22121 +Woxxt)\nct = it \u25e6 c\u0303t + ft \u25e6 ct\u22121 ht = ot \u25e6 tanh(ct)\n(4)\nWe refer to c\u0303t as the content layer, which like S-RNNs is a non-linear recurrent layer. The cell state ct behaves like a RAN, using input and forget gates to compute a weighted sum of the current content layer c\u0303t and the previous cell state ct\u22121. In fact, just as in RANs, we can express each cell state as a component-wise weighted sum of the all previous content layers:\nct = it \u25e6 c\u0303t + ft \u25e6 ct\u22121\n= t\u2211 j=0 ( ij \u25e6 t\u220f k=j+1 fk ) \u25e6 c\u0303j\n= t\u2211 j=0 wtj \u25e6 c\u0303j\nHowever, unlike RANs, we can no longer express the cell state as a weighted sum of the input vectors, due to the S-RNN inspired non-linear recurrence."}, {"heading": "4.2 Simplifying the Content Layer", "text": "We first examine the necessity of using an embedded S-RNN to compute the content layer. In this step of the derivation, we remove the non-linearity of the content layer c\u0303t and its dependence on the previous output vector ht\u22121, and replace it with a simple linear projection of the input vector c\u0303t = Wxxt:\nc\u0303t = Wcxxt it = \u03c3(Wihht\u22121 +Wixxt)\nft = \u03c3(Wfhht\u22121 +Wfxxt)\not = \u03c3(Wohht\u22121 +Woxxt)\nct = it \u25e6 c\u0303t + ft \u25e6 ct\u22121 ht = ot \u25e6 tanh(ct)\n(5)\nThis modification results in a content layer c\u0303t that is identical to the content layer in a RAN. We can also consider this step as the removal of the S-RNN from the LSTM.\nIn our experiments, we observe that this simplification (Simplified Content Layer in Table 2) produces a large improvement in word-based language modeling and near-identical performance in characterbased language modeling. This result suggests that the embedded S-RNN is not only unnecessary\u2014it might also be an impediment."}, {"heading": "4.3 Simplifying the Output Layer", "text": "In the second step, which brings us directly to RANs, we simplify two aspects of the output layer. First, we remove the output gate. Second, we extract the remaining tanh non-linearity from the recurrent process, making it an effectively external component (like g in RANs). We do so by rewiring the recurrent connection of the gates: instead of depending on the previous output vector ht\u22121, the gates take the previous cell state ct\u22121 as the recurrent argument, which does not pass through the tanh non-linearity. These two modifications result in the RAN, which we present here again for\ncompleteness:\nc\u0303t = Wcxxt it = \u03c3(Wicct\u22121 +Wixxt)\nft = \u03c3(Wfcct\u22121 +Wfxxt)\nct = it \u25e6 c\u0303t + ft \u25e6 ct\u22121 ht = tanh(ct)\nThis simplification of the output layer results in further significant improvements on word-based language modeling, while producing near-identical results for character-based language modeling (RAN (tanh) in Table 2).\nThe final result of this derivation is that both tanh non-linearities present in LSTMs are either removed or applied in a non-recurrent, modular way. The sparing use of non-linearities allows us to better reason about the computations of the architecture. More importantly, these results suggest that the success of LSTMs is better attributed to the internal structure that is shares with RANs rather than the recurrent non-linearities associated with its internal S-RNN."}, {"heading": "4.4 Deriving RAN from GRU", "text": "A similar analysis can be applied to other gated RNNs such as GRUs, which are typically defined as follows:\nzt = \u03c3(Wzhht\u22121 +Wzxxt)\nrt = \u03c3(Wrhht\u22121 +Wrxxt)\nh\u0303t = tanh(Whh(r \u25e6 ht\u22121) +Whxxt)\nht = zt \u25e6 h\u0303t + (1\u2212 zt) \u25e6 ht\u22121\n(6)\nFor ease of discussion, we present the following non-standard but equivalent form of GRUs, which aligns better with the notation from LSTMs and RANs:\nc\u0303t = tanh(Wchht\u22121 +Wcxxt)\nit = \u03c3(Wicct\u22121 +Wixxt)\not = \u03c3(Wocct\u22121 +Woxxt)\nct = it \u25e6 c\u0303t + (1\u2212 it) \u25e6 ct\u22121 ht = ot \u25e6 ct\n(7)\nIn this form, the content layer c\u0303t is equivalent to h\u0303t. The input gate it is equivalent to zt, and the output gate ot is equivalent to the reset gate rt. A subtle detail that enables this transformation is that GRUs can be seen as maintaining separate cell and output states, similar to LSTMs. However, GRUs compute gates with respect to the previous cell state ct\u22121 rather than the previous output ht\u22121, similar to RANs.\nIn our alternate form, it is clear that GRUs also accumulate weighted summations ct of previous content layers c\u0303t. The derivation of RAN from here involves only two straightforward steps: (1) the non-linear recurrence of the content layer c\u0303t is replaced by a simple linear projection c\u0303t = Wcxxt,\nand (2) the output gate ot is used instead as a forget gate by applying it to the previous cell state ct\u22121 rather than the current cell state ct:\nct = it \u25e6 c\u0303t + ot \u25e6 ct\u22121 The result is a RAN with an identity output function.\nAn interesting effect of the coupled gates in GRU is that these weights are normalized, i.e. the weights over all previous time steps sum to 1. As a result, GRUs (and our simplifications of them) are computing weighted averages rather than weighted sums. In development experiments, we found that this normalization hurt performance if added to the RAN architecture. However, it is closely related to another architecture that computes weighted averages: attention (Bahdanau et al., 2015). In fact, we can consider a GRU as a component-wise attention mechanism over its previous content layers and an RAN to be an unnormalized component-wise generalization of attention, which can point to many different possible inputs independently."}, {"heading": "5 Weight Visualization", "text": "Given the relative interpretability of the RAN predictions, we can trace the importance of each component in each of the previous elements explicitly, as shown in equation set (3). In language modeling, we can also compute which previous word had the overall strongest influence in predicting the next word at time step t:\nvt = t\u22121\nargmax j=1 ( dh max m=1 (wtj(m)) )\n(8)\nIn Figure 2, we visualize this computation by drawing arrows from each word t to its most influential predecessor (i.e. the previous word with the highest weighted component). In these four example sentences, we see that RANs can recover long distance dependencies that make intuitive sense given the syntactic and semantic structure of the text. For example, verbs are related to their arguments, even when coordinated, and nouns in relative clauses depend on the noun they are modifying. This illustration provides only a glimpse into what the model is capturing, and perhaps future, more detailed visualizations that take the individual components into account can provide further insight into what RANs are learning in practice."}, {"heading": "6 Related Work", "text": "Many variants of LSTMs (Hochreiter and Schmidhuber, 1997) and alternative designs for more general GRNNs have been proposed. One such variant adds peephole connections between the cell state and the gates (Gers and Schmidhuber, 2000). GRUs (Cho et al., 2014) decrease the number of parameters by coupling the input and forget gates and rewiring the gate computations (see Section 4.4 for an alternative formulation). Greff et al. (2016) conducted an LSTM ablation study that probed the importance of each component independently, while J\u00f3zefowicz et al. (2015) took an automatic approach to the task of architecture design, and found additional variants of GRNNs. While we demonstrate that RANs can be seen as an ablation of LSTMs or GRUs, they are vastly simpler than the variants explored in the aforementioned studies.\nSeveral approaches represent sequences as weighted sums of their elements. Perhaps the most popular mechanism in NLP is attention (Bahdanau et al., 2015), which assigns a normalized scalar weight to each element (typically a word vector) as a function of its compatibility with an external element. The ability to inspect attention weights has driven the use of more interpretable neural models. Selfattention (Cheng et al., 2016; Parikh et al., 2016) extends this notion by computing intra-sequence attention. Recently, Arora et al. (2017) proposed a theory-driven approach to assign scalar weights to elements in a bag of words. RANs, on the other hand, implicitly compute a component-wise weighted sum as a byproduct of a simple RNN state scheme."}, {"heading": "7 Conclusion", "text": "We introduced recurrent additive networks (RANs), a type of gated RNNs that performs purely additive updates to its latent state. While RANs do not include the non-linear recurrent connections that are typically considered to be crucial for RNNs, they have remarkably strong performance on several language modeling benchmarks compared to other popular recurrent architectures, such as LSTMs and GRUs. RANs are also considerably more transparent than other RNNs; their limited use of non-linearities enables the state vector to be expressed as a component-wise weighted sum of previous input vectors. This also allows the individual impact of the sequence inputs on the state to be recovered explicitly, directly pointing to which factors are most influencing each part of the current state.\nThis work also sheds light on the inner workings of existing, more opaque models, primarily LSTMs and GRUs. We demonstrate that RAN-like components exist within LSTMs and GRUs. Furthermore, we provide empirical evidence that the use of recurrent non-linearities within those architectures is not only unhelpful, but that it can become a liability in language modeling.\nWhile we demonstrate the robustness of RANs on several language modeling benchmarks, it is an open question whether these findings will generalize across a wider variety of tasks. However, the results do suggest that it may be possible to develop related, relatively simple, additive gated RNNs that are better building blocks for a wide range of different neural architectures. We hope that our findings prove helpful in the design of future recurrent neural networks."}, {"heading": "Acknowledgements", "text": "The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS-1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award. We also thank Yoav Goldberg, Luheng He, Aaron Jaech, Ariel Holtzman, and the UW NLP group for helpful conversations and comments on the work."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction", "author": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "venue": null, "citeRegEx": "Adi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Adi et al\\.", "year": 2017}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "venue": "In ICLR,", "citeRegEx": "Arora et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Y. Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn"], "venue": "In INTERSPEECH,", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gal and Ghahramani.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Recurrent nets that time and count", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber"], "venue": "In IJCNN,", "citeRegEx": "Gers and Schmidhuber.,? \\Q2000\\E", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Deep semantic role labeling: What works and what\u2019s next", "author": ["Luheng He", "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "He et al\\.,? \\Q2017\\E", "shortCiteRegEx": "He et al\\.", "year": 2017}, {"title": "Long Short-term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In ICML,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Assessing the ability of lstms to learn syntaxsensitive", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg"], "venue": "dependencies. TACL,", "citeRegEx": "Linzen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Parikh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Gated recurrent neural networks (GRNNs), such as long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRUs) (Cho et al.", "startOffset": 82, "endOffset": 116}, {"referenceID": 6, "context": "Gated recurrent neural networks (GRNNs), such as long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRUs) (Cho et al., 2014), have become ubiquitous in natural language processing (NLP).", "startOffset": 150, "endOffset": 168}, {"referenceID": 0, "context": "GRNN\u2019s widespread popularity is at least in part due to their ability to model crucial language phenomena such as word order (Adi et al., 2017), syntactic structure (Linzen et al.", "startOffset": 125, "endOffset": 143}, {"referenceID": 15, "context": ", 2017), syntactic structure (Linzen et al., 2016), and even long-range semantic dependencies (He et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 11, "context": ", 2016), and even long-range semantic dependencies (He et al., 2017).", "startOffset": 51, "endOffset": 68}, {"referenceID": 7, "context": "Like simple recurrent neural networks (S-RNNs) (Elman, 1990), they are able to learn non-linear functions of arbitrary-length input sequences, while at the same time alleviating the problem of vanishing gradients (Bengio et al.", "startOffset": 47, "endOffset": 60}, {"referenceID": 3, "context": "Like simple recurrent neural networks (S-RNNs) (Elman, 1990), they are able to learn non-linear functions of arbitrary-length input sequences, while at the same time alleviating the problem of vanishing gradients (Bengio et al., 1994) by including gated additive state updates.", "startOffset": 213, "endOffset": 234}, {"referenceID": 16, "context": "Benchmarks We use three language modeling benchmarks: Penn Treebank (PTB) (Marcus et al., 1993), Google\u2019s billion-word benchmark (BWB) (Chelba et al.", "startOffset": 74, "endOffset": 95}, {"referenceID": 4, "context": ", 1993), Google\u2019s billion-word benchmark (BWB) (Chelba et al., 2014), and the Text8 character-based language modeling benchmark.", "startOffset": 47, "endOffset": 68}, {"referenceID": 18, "context": "Training We applied 50% dropout (Srivastava et al., 2014) immediately before, after, and inside each of the RNNs (using variational dropout (Gal and Ghahramani, 2016)).", "startOffset": 32, "endOffset": 57}, {"referenceID": 8, "context": ", 2014) immediately before, after, and inside each of the RNNs (using variational dropout (Gal and Ghahramani, 2016)).", "startOffset": 90, "endOffset": 116}, {"referenceID": 14, "context": "We optimized the cross-entropy loss using Adam (Kingma and Ba, 2014), using a batch size of 512.", "startOffset": 47, "endOffset": 68}, {"referenceID": 3, "context": "The need for LSTMs is typically motivated by the fact that they can ease the vanishing gradient problem found in simple RNNs (S-RNNs) (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997).", "startOffset": 134, "endOffset": 189}, {"referenceID": 12, "context": "The need for LSTMs is typically motivated by the fact that they can ease the vanishing gradient problem found in simple RNNs (S-RNNs) (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997).", "startOffset": 134, "endOffset": 189}, {"referenceID": 2, "context": "However, it is closely related to another architecture that computes weighted averages: attention (Bahdanau et al., 2015).", "startOffset": 98, "endOffset": 121}, {"referenceID": 12, "context": "Many variants of LSTMs (Hochreiter and Schmidhuber, 1997) and alternative designs for more general GRNNs have been proposed.", "startOffset": 23, "endOffset": 57}, {"referenceID": 9, "context": "One such variant adds peephole connections between the cell state and the gates (Gers and Schmidhuber, 2000).", "startOffset": 80, "endOffset": 108}, {"referenceID": 6, "context": "GRUs (Cho et al., 2014) decrease the number of parameters by coupling the input and forget gates and rewiring the gate computations (see Section 4.", "startOffset": 5, "endOffset": 23}, {"referenceID": 2, "context": "Perhaps the most popular mechanism in NLP is attention (Bahdanau et al., 2015), which assigns a normalized scalar weight to each element (typically a word vector) as a function of its compatibility with an external element.", "startOffset": 55, "endOffset": 78}, {"referenceID": 5, "context": "Selfattention (Cheng et al., 2016; Parikh et al., 2016) extends this notion by computing intra-sequence attention.", "startOffset": 14, "endOffset": 55}, {"referenceID": 17, "context": "Selfattention (Cheng et al., 2016; Parikh et al., 2016) extends this notion by computing intra-sequence attention.", "startOffset": 14, "endOffset": 55}, {"referenceID": 3, "context": "GRUs (Cho et al., 2014) decrease the number of parameters by coupling the input and forget gates and rewiring the gate computations (see Section 4.4 for an alternative formulation). Greff et al. (2016) conducted an LSTM ablation study that probed the importance of each component independently, while J\u00f3zefowicz et al.", "startOffset": 6, "endOffset": 202}, {"referenceID": 3, "context": "GRUs (Cho et al., 2014) decrease the number of parameters by coupling the input and forget gates and rewiring the gate computations (see Section 4.4 for an alternative formulation). Greff et al. (2016) conducted an LSTM ablation study that probed the importance of each component independently, while J\u00f3zefowicz et al. (2015) took an automatic approach to the task of architecture design, and found additional variants of GRNNs.", "startOffset": 6, "endOffset": 326}, {"referenceID": 1, "context": "Recently, Arora et al. (2017) proposed a theory-driven approach to assign scalar weights to elements in a bag of words.", "startOffset": 10, "endOffset": 30}], "year": 2017, "abstractText": "We introduce recurrent additive networks (RANs), a new gated RNN which is distinguished by the use of purely additive latent state updates. At every time step, the new state is computed as a gated component-wise sum of the input and the previous state, without any of the non-linearities commonly used in RNN transition dynamics. We formally show that RAN states are weighted sums of the input vectors, and that the gates only contribute to computing the weights of these sums. Despite this relatively simple functional form, experiments demonstrate that RANs outperform both LSTMs and GRUs on benchmark language modeling problems. This result shows that many of the non-linear computations in LSTMs and related networks are not essential, at least for the problems we consider, and suggests that the gates are doing more of the computational work than previously understood.", "creator": "LaTeX with hyperref package"}}}