{"id": "1705.07485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "Shake-Shake regularization", "abstract": "the method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. the idea is to replace, in a multi - branch network, the standard summation functions of parallel branches with all a stochastic affine combination. applied to 3 - branch residual networks, automatic shake - shake regularization improves on the best single shot published results on cifar - 10 and cifar - 100 by reaching test errors of 2. 86 % and 15. 85 %. beta experiments on architectures without skip connections maintenance or batch normalization show encouraging results and helps open the door mechanisms to access a large set suite of applications. code is made available at", "histories": [["v1", "Sun, 21 May 2017 18:51:27 GMT  (1504kb,D)", "http://arxiv.org/abs/1705.07485v1", null], ["v2", "Tue, 23 May 2017 13:36:46 GMT  (1504kb,D)", "http://arxiv.org/abs/1705.07485v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["xavier gastaldi"], "accepted": false, "id": "1705.07485"}, "pdf": {"name": "1705.07485.pdf", "metadata": {"source": "CRF", "title": "Shake-Shake regularization", "authors": ["Xavier Gastaldi"], "emails": ["xgastaldi.mba2011@london.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep residual nets (He et al., 2016a) were first introduced in the ILSVRC & COCO 2015 competitions (Russakovsky et al., 2015; Lin et al., 2014), where they won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. Since then, significant effort has been put into trying to improve their performance. Scientists have investigated the impact of pushing depth (He et al., 2016b; Huang et al., 2016a), width (Zagoruyko & Komodakis, 2016) and cardinality (Xie et al., 2016; Szegedy et al., 2016; Abdi & Nahavandi, 2016).\nWhile residual networks are powerful models, they still overfit on small datasets. A large number of techniques have been proposed to tackle this problem, including weight decay (Nowlan & Hinton, 1992), early stopping, and dropout (Srivastava et al., 2014). While not directly presented as a regularization method, Batch Normalization (Ioffe & Szegedy, 2015) regularizes the network by computing statistics that fluctuate with each mini-batch. Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)).\nPre-2015, most computer vision classification architectures used dropout to combat overfit but the introduction of Batch Normalization reduced its effectiveness (see Ioffe & Szegedy (2015); Zagoruyko & Komodakis (2016); Huang et al. (2016b)). Searching for other regularization methods, researchers started to look at the possibilities specifically offered by multi-branch networks. Some of them noticed that, given the right conditions, it was possible to randomly drop some of the information paths during training (Huang et al., 2016b; Larsson et al., 2016).\nLike these last 2 works, the method proposed in this document aims at improving the generalization ability of multi-branch networks by replacing the standard summation of parallel branches with a stochastic affine combination."}, {"heading": "1.1 Motivation", "text": "Data augmentation techniques have traditionally been applied to input images only. However, for a computer, there is no real difference between an input image and an intermediate representation. As a consequence, it might be possible to apply data augmentation techniques to internal representations.\nar X\niv :1\n70 5.\n07 48\n5v 1\n[ cs\n.L G\n] 2\n1 M\nay 2\n01 7\nShake-Shake regularization was created as an attempt to produce this sort of effect by stochastically \"blending\" 2 viable tensors."}, {"heading": "1.2 Model description on 3-branch ResNets", "text": "Let xi denote the tensor of inputs into residual block i. W(1)i andW (2) i are sets of weights associated with the 2 residual units. F denotes the residual function, e.g. a stack of two 3x3 convolutional layers. xi+1 denotes the tensor of outputs from residual block i.\nA typical pre-activation ResNet with 2 residual branches would follow this equation:\nxi+1 = xi + F(xi,W(1)i ) + F(xi,W (2) i ) (1)\nProposed modification: If \u03b1i is a random variable following a uniform distribution between 0 and 1, then during training:\nxi+1 = xi + \u03b1iF(xi,W(1)i ) + (1\u2212 \u03b1i)F(xi,W (2) i ) (2)\nFollowing the same logic as for dropout, all \u03b1i are set to the expected value of 0.5 at test time.\nThis method can be seen as a form of drop-path (Larsson et al., 2016) where residual branches are scaled-down instead of being completely dropped (i.e. multiplied by 0).\nReplacing binary variables with enhancement or reduction coefficients is also explored in dropout variants like shakeout (Kang et al., 2016) and whiteout (Yinan et al., 2016). However, where these methods perform an element-wise multiplication between an input tensor and a noise tensor, shake-shake regularization multiplies the whole image tensor with just one scalar \u03b1i (or 1\u2212 \u03b1i)."}, {"heading": "1.3 Training procedure", "text": "As shown in Figure 1, all scaling coefficients are overwritten with new random numbers before each forward pass. The key to making this work is to repeat this coefficient update operation before each backward pass. This results in a stochastic blend of forward and backward flows during training.\nRelated to this idea are the works of An (1996) and Neelakantan et al. (2015). These authors showed that adding noise to the gradient during training helps training and generalization of complicated neural networks. Shake-Shake regularization can be seen as an extension of this concept where gradient noise is replaced by a form of gradient augmentation."}, {"heading": "2 Improving on the best single shot published results on CIFAR", "text": ""}, {"heading": "2.1 CIFAR-10", "text": ""}, {"heading": "2.1.1 Implementation details", "text": "The Shake-Shake code is based on fb.resnet.torch1 and is available at https://github.com/ xgastaldi/shake-shake. The first layer is a 3x3 Conv with 16 filters, followed by 3 stages each having 4 residual blocks. The feature map size is 32, 16 and 8 for each stage. Width is doubled when downsampling. The network ends with a 8x8 average pooling and a fully connected layer (total 26 layers deep). Residual paths have the following structure: ReLU-Conv3x3-BN-ReLU-Conv3x3-BN-Mul. The skip connections represent the identity function except during downsampling where a slightly customized structure consisting of 2 concatenated flows is used. Each of the 2 flows has the following components: 1x1 average pooling with step 2 followed by a 1x1 convolution. The input of one of the two flows is shifted by 1 pixel right and 1 pixel down to make the average pooling sample from a different position. The concatenation of the two flows doubles the width. Models were trained on the CIFAR-10 (Krizhevsky, 2009) 50k training set and evaluated on the 10k test set. Standard translation and flipping data augmentation is applied on the 32x32 input image. Due to the introduced stochasticity, all models were trained for 1800 epochs. Training starts with a learning rate of 0.2 and is annealed using a Cosine function without restart (see Loshchilov & Hutter (2016)). All models were trained on 2 GPUs with a mini-batch size of 128. Other implementation details are as in fb.resnet.torch."}, {"heading": "2.1.2 Influence of Forward and Backward training procedures", "text": "The base network is a 26 2x32d ResNet (i.e. the network has a depth of 26, 2 residual branches and the first residual block has a width of 32). \"Shake\" means that all scaling coefficients are overwritten with new random numbers before the pass. \"Even\" means that all scaling coefficients are set to 0.5 before the pass. \"Keep\" means that we keep, for the backward pass, the scaling coefficients used during the forward pass. \"Batch\" means that, for each residual block i, we apply the same scaling coefficient for all the images in the mini-batch. \"Image\" means that, for each residual block i, we apply a different scaling coefficient for each image in the mini-batch (see Image level update procedure below).\nImage level update procedure: Let x0 denote the original input mini-batch tensor of dimensions 128x3x32x32. The first dimension \u00ab stacks \u00bb 128 images of dimensions 3x32x32. Inside the second stage of a 26 2x32d model, this tensor is transformed into a mini-batch tensor xi of dimensions 128x64x16x16. Applying Shake-Shake regularization at the Image level means slicing this tensor along the first dimension and, for each of the 128 slices, multiplying the jth slice (of dimensions 64x16x16) with a scalar \u03b1i.j (or 1\u2212 \u03b1i.j). The numbers in Table 1 represent the average of 3 runs except for the 96d models which were run 5 times. What can be observed in Table 1 and Figure 2 is that \"Shake-Keep\" or \"S-K\" models (i.e. \"Shake\" Forward \"Keep\" Backward) do not have a particularly strong effect on the error rate. The network seems to be able to see through the perturbations when the weight update is done with the same ratios as during the forward pass. \"Even-Shake\" only works when applied at the \"Image\" level. \"Shake-Even\" and \"Shake-Shake\" models all produce strong results at 32d but the better training curves of \"Shake-Shake\" models start to make a difference when the number of filters of the first residual block is increased to 64d. Applying coefficients at the \"Image\" level seems to improve regularization."}, {"heading": "2.2 CIFAR-100", "text": "The network architecture chosen for CIFAR-100 is a ResNeXt without pre-activation (this model gives slightly better results on CIFAR-100 than the model used for CIFAR-10). Hyperparameters are the same as in Xie et al. (2016) except for the learning rate which is annealed using a Cosine function and the number of epochs which is increased to 1800. The network in Table 2 is a ResNeXt-29 2x4x64d (2 residual branches with 4 grouped convolutions, each with 64 channels). Due to the\n1https://github.com/facebook/fb.resnet.torch\ncombination of the larger model (34.4M parameters) and the long training time, fewer tests were performed than on CIFAR-10.\nInterestingly, a key hyperparameter on CIFAR-100 is the batch size which, compared to CIFAR-10, has to be reduced from 128 to 32 if using 2 GPUs.2 Without this reduction, the E-E-B network does not produce competitive results. As shown in Table 2, the increased regularization produced by the smaller batch size impacts the training procedure selection and makes S-E-I a slightly better choice.\n2As per notes in https://github.com/facebookresearch/ResNeXt"}, {"heading": "2.3 Comparisons with state-of-the-art results", "text": "At the time of writing, the best single shot model on CIFAR-10 is a DenseNet-BC k=40 (3.46% error rate) with 25.6M parameters. The second best model is a ResNeXt-29, 16x64d (3.58% error rate) with 68.1M parameters. A small 26 2x32d \"Shake-Even-Image\" model with 2.9M parameters obtains approximately the same error rate. This is roughly 9 times less parameters than the DenseNet model and 23 times less parameters than the ResNeXt model. A 26 2x96d \"Shake-Shake-Image\" ResNet with 26.2M parameters, reaches a test error of 2.86% (Average of 5 runs - Median 2.87%, Min = 2.72%, Max = 2.95%).\nOn CIFAR-100, a few hyperparameter modifications of a standard ResNeXt-29 8x64d (batchsize, no pre-activation, longer training time and cosine annealing) lead to a test error of 16.34%. Adding shake-even regularization reduces the test error to 15.85% (Average of 3 runs - Median 15.85%, Min = 15.66%, Max = 16.04%)."}, {"heading": "3 Correlation between residual branches", "text": "To check whether the correlation between the 2 residual branches is increased or decreased by the regularization, the following test was performed:\nFor each residual block:\n1. Forward a mini-batch tensor xi through the residual branch 1 (ReLU-Conv3x3-BN-ReLUConv3x3-BN-Mul(0.5)) and store the output tensor in y(1)i . Do the same for residual branch 2 and store the output in y(2)i .\n2. Flatten these 2 tensors into vectors flat(1)i and flat (2) i . Calculate the covariance between\neach corresponding item in the 2 vectors using an online version of the covariance algorithm.\n3. Calculate the variances of flat(1)i and flat (2) i using an online variance algorithm.\n4. Repeat until all the images in the test set have been forwarded. Use the resulting covariance and variances to calculate the correlation.\nThis algorithm was run on CIFAR-10 for 3 EEB models and 3 S-S-I models both 26 2x32d. The results are presented in Figure 3. The correlation between the output tensors of the 2 residual branches seems to be reduced by the regularization. This would support the assumption that the regularization forces the branches to learn something different.\nOne problem to be mindful of is the issue of alignment (see Li et al. (2016)). The method above assumes that the summation at the end of the residual blocks forces an alignment of the layers on the left and right residual branches. This can be verified by calculating the layer wise correlation for each configuration of the first 3 layers of each block.\nThe results are presented in Figure 4. L1R3 for residual block i means the correlation between the activations of the first layer in y(1)i (left branch) and the third layer in y (2) i (right branch). Figure 4 shows that the correlation between the same layers on the left and right branches (i.e. L1R1, L2R2, etc..) is higher than in the other configurations, which is consistent with the assumption that the summation forces alignment."}, {"heading": "4 Regularization strength", "text": "This section looks at what would happen if we give, during the backward pass, a large weight to a branch that received a small weight in the forward pass (and vice-versa).\nLet \u03b1i.j be the coefficient used during the forward pass for image j in residual block i. Let \u03b2i.j be the coefficient used during the backward pass for the same image at the same position in the network.\nThe first test (method 1) is to set \u03b2i.j = 1 - \u03b1i.j . All the tests in this section were performed on CIFAR-10 using 26 2x32d models at the Image level. These models are compared to a 26 2x32d Shake-Keep-Image model. The results of M1 can be seen on the left part of Figure 5 (blue curve). The effect is quite drastic and the training error stays really high.\nTests M2 to M5 in Table 4 were designed to understand why Method 1 (M1) has such a strong effect. The right part of Figure 5 illustrates Table 4 graphically.\nWhat can be seen is that:\n1. The regularization effect seems to be linked to the relative position of \u03b2i.j compared to \u03b1i.j\n2. The further away \u03b2i.j is from \u03b1i.j , the stronger the regularization effect\n3. There seems to be a jump in strength when 0.5 is crossed\nThese insights could be useful when trying to control with more accuracy the strength of the regularization."}, {"heading": "5 Removing skip connections / Removing Batch Normalization", "text": "One interesting question is whether the skip connection plays a role. A lot of deep learning systems don\u2019t use ResNets and making this type of regularization work without skip connections could extend the number of potential applications.\nTable 5 and Figure 6 present the results of removing the skip connection. The first variant (A) is exactly like the 26 2x32d used on CIFAR-10 but without the skip connection (i.e. 2 branches with the following components ReLU-Conv3x3-BN-ReLU-Conv3x3-BN-Mul). The second variant (B) is the same as A but with only 1 convolutional layer per branch (ReLU-Conv3x3-BN-Mul) and twice the number of blocks. Models using architecture A were tested once and models using architecture B were tested twice.\nThe results of architecture A clearly show that shake-shake regularization can work even without a skip connection. On that particular architecture and on a 26 2x32d model, S-S-I is too strong and the model underfits. The softer effect of S-E-I works better but this could change if the capacity is increased (e.g. 64d or 96d).\nThe results of architecture B are actually the most surprising. The first point to notice is that the regularization no longer works. This, in itself, would indicate that the regularization happens thanks to the interaction between the 2 convolutions in each branch. The second point is that the train and test curves of the S-E-I and E-E-B models are absolutely identical. This would indicate that, for architecture B, the shake operation of the forward pass has no effect on the cost function. The third point is that even with a really different training curve, the test curve of the S-S-I model is nearly identical to the test curves of the E-E-B and S-E-I models (albeit with a smaller variance).\nFinally, it would be interesting to see whether this method works without Batch Normalization. While batchnorm is commonly used on computer vision datasets, it is not necessarily the case for other types of problems (e.g. NLP, etc ..). Architecture C is the same as architecture A but without Batch Normalization (i.e. no skip, 2 branches with the following structure ReLU-Conv3x3-ReLU-Conv3x3-Mul). To allow the E-E-B model to converge the depth was reduced from 26 to 14 and the initial learning rate was set to 0.05 after a warm start at 0.025 for 1 epoch. The absence of Batch Normalization makes the model a lot more sensitive and applying the same methods as before makes the model diverge. To soften the effect a S-E-I model was chosen and the interval covered by \u03b1i.j was reduced from [0,1] to [0.4,0.6]. Models using architecture C and different intervals were tested once on CIFAR-10. As shown in Table 5 and Figure 6, this method works quite well but it is also really easy to make the model diverge (see model 14 2x32d S-E-I v3)."}, {"heading": "6 Conclusion", "text": "A series of experiments seem to indicate an ability to combat overfit by decorrelating the branches of multi-branch networks. This method leads to state of the art results on CIFAR datasets and could potentially improve the accuracy of architectures that do not use ResNets or Batch Normalization. While these results are encouraging, questions remain on the exact dynamics at play. Understanding these dynamics could help expand the application field to a wider variety of complex architectures."}], "references": [{"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["Guozhong An"], "venue": "Neural Comput.,", "citeRegEx": "An.,? \\Q1996\\E", "shortCiteRegEx": "An.", "year": 1996}, {"title": "Online algorithms and stochastic approximations. In Online Learning and Neural Networks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ECCV,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Q. Weinberger"], "venue": "In ECCV,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Shakeout: A new regularized deep neural network training scheme", "author": ["Guoliang Kang", "Jun Li", "Dacheng Tao"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Kang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2016}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "venue": "In International Conference on Learning Representation (ICLR", "citeRegEx": "Keskar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Tech Report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Convergent learning: Do different neural networks learn the same representations", "author": ["Yixuan Li", "Jason Yosinski", "Jeff Clune", "Hod Lipson", "John Hopcroft"], "venue": "In International Conference on Learning Representation (ICLR", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Microsoft COCO: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "Lubomir D. Bourdev", "Ross B. Girshick", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Sgdr: stochastic gradient descent with restarts", "author": ["Ilya Loshchilov", "Frank Hutter"], "venue": "arXiv preprint arXiv:1608.03983,", "citeRegEx": "Loshchilov and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Loshchilov and Hutter.", "year": 2016}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Steven J. Nowlan", "Geoffrey E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Nowlan and Hinton.,? \\Q1992\\E", "shortCiteRegEx": "Nowlan and Hinton.", "year": 1992}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alex A. Alemi"], "venue": "In ICLR 2016 Workshop,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Aggregated residual transformations for deep neural networks", "author": ["Saining Xie", "Ross Girshick", "Piotr Doll\u00e1r", "Zhuowen Tu", "Kaiming He"], "venue": "arXiv preprint arXiv:1611.05431,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Whiteout: Gaussian adaptive regularization noise in deep neural networks", "author": ["Li Yinan", "Xu Ruoyi", "Liu Fang"], "venue": "arXiv preprint arXiv:1612.01490v2,", "citeRegEx": "Yinan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yinan et al\\.", "year": 2016}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "In BMVC,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": ", 2016a) were first introduced in the ILSVRC & COCO 2015 competitions (Russakovsky et al., 2015; Lin et al., 2014), where they won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "startOffset": 70, "endOffset": 114}, {"referenceID": 19, "context": ", 2016a), width (Zagoruyko & Komodakis, 2016) and cardinality (Xie et al., 2016; Szegedy et al., 2016; Abdi & Nahavandi, 2016).", "startOffset": 62, "endOffset": 126}, {"referenceID": 18, "context": ", 2016a), width (Zagoruyko & Komodakis, 2016) and cardinality (Xie et al., 2016; Szegedy et al., 2016; Abdi & Nahavandi, 2016).", "startOffset": 62, "endOffset": 126}, {"referenceID": 16, "context": "A large number of techniques have been proposed to tackle this problem, including weight decay (Nowlan & Hinton, 1992), early stopping, and dropout (Srivastava et al., 2014).", "startOffset": 148, "endOffset": 173}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al.", "startOffset": 45, "endOffset": 83}, {"referenceID": 17, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al.", "startOffset": 45, "endOffset": 83}, {"referenceID": 10, "context": "Some of them noticed that, given the right conditions, it was possible to randomly drop some of the information paths during training (Huang et al., 2016b; Larsson et al., 2016).", "startOffset": 134, "endOffset": 177}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)).", "startOffset": 46, "endOffset": 274}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)). Pre-2015, most computer vision classification architectures used dropout to combat overfit but the introduction of Batch Normalization reduced its effectiveness (see Ioffe & Szegedy (2015); Zagoruyko & Komodakis (2016); Huang et al.", "startOffset": 46, "endOffset": 465}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)). Pre-2015, most computer vision classification architectures used dropout to combat overfit but the introduction of Batch Normalization reduced its effectiveness (see Ioffe & Szegedy (2015); Zagoruyko & Komodakis (2016); Huang et al.", "startOffset": 46, "endOffset": 495}, {"referenceID": 1, "context": "Similarly, Stochastic Gradient Descent (SGD) (Bottou, 1998; Sutskever et al., 2013) can also be interpreted as Gradient Descent using noisy gradients and the generalization performance of neural networks often depends on the size of the mini-batch (see Keskar et al. (2017)). Pre-2015, most computer vision classification architectures used dropout to combat overfit but the introduction of Batch Normalization reduced its effectiveness (see Ioffe & Szegedy (2015); Zagoruyko & Komodakis (2016); Huang et al. (2016b)).", "startOffset": 46, "endOffset": 517}, {"referenceID": 10, "context": "This method can be seen as a form of drop-path (Larsson et al., 2016) where residual branches are scaled-down instead of being completely dropped (i.", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "Replacing binary variables with enhancement or reduction coefficients is also explored in dropout variants like shakeout (Kang et al., 2016) and whiteout (Yinan et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 20, "context": ", 2016) and whiteout (Yinan et al., 2016).", "startOffset": 21, "endOffset": 41}, {"referenceID": 0, "context": "Related to this idea are the works of An (1996) and Neelakantan et al.", "startOffset": 38, "endOffset": 48}, {"referenceID": 0, "context": "Related to this idea are the works of An (1996) and Neelakantan et al. (2015). These authors showed that adding noise to the gradient during training helps training and generalization of complicated neural networks.", "startOffset": 38, "endOffset": 78}, {"referenceID": 9, "context": "Models were trained on the CIFAR-10 (Krizhevsky, 2009) 50k training set and evaluated on the 10k test set.", "startOffset": 36, "endOffset": 54}, {"referenceID": 9, "context": "Models were trained on the CIFAR-10 (Krizhevsky, 2009) 50k training set and evaluated on the 10k test set. Standard translation and flipping data augmentation is applied on the 32x32 input image. Due to the introduced stochasticity, all models were trained for 1800 epochs. Training starts with a learning rate of 0.2 and is annealed using a Cosine function without restart (see Loshchilov & Hutter (2016)).", "startOffset": 37, "endOffset": 406}, {"referenceID": 19, "context": "Hyperparameters are the same as in Xie et al. (2016) except for the learning rate which is annealed using a Cosine function and the number of epochs which is increased to 1800.", "startOffset": 35, "endOffset": 53}, {"referenceID": 11, "context": "One problem to be mindful of is the issue of alignment (see Li et al. (2016)).", "startOffset": 60, "endOffset": 77}], "year": 2017, "abstractText": "The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of applications. Code is available at https://github.com/xgastaldi/shake-shake.", "creator": "LaTeX with hyperref package"}}}