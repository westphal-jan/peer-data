{"id": "1705.07853", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Nonparametric Online Regression while Learning the Metric", "abstract": "we study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. then our algorithm learns the mahalanobis metric based on the polynomial gradient outer product matrix $ \\ boldsymbol { g } $ of the regression function ( presumably automatically adapting to perform the given effective rank of this estimate matrix ), while simultaneously bounding the regret - - - on the same data sequence - - - in terms of the spectrum of $ \\ variable boldsymbol { g } $. as a preliminary step in our analysis, maybe we generalize a nonparametric online learning algorithm by hazan and rene megiddo by enabling it to compete individually against functions whose lipschitzness is measured with respect to an arbitrary mahalanobis metric.", "histories": [["v1", "Mon, 22 May 2017 16:58:13 GMT  (20kb,D)", "http://arxiv.org/abs/1705.07853v1", null], ["v2", "Mon, 23 Oct 2017 15:51:33 GMT  (166kb,D)", "http://arxiv.org/abs/1705.07853v2", "To appear in NIPS 2017"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ilja kuzborskij", "nicol\\`o cesa-bianchi"], "accepted": true, "id": "1705.07853"}, "pdf": {"name": "1705.07853.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Online Regression while Learning the Metric", "authors": ["Ilja Kuzborskij"], "emails": ["ilja.kuzborskij@idiap.ch", "nicolo.cesa-bianchi@unimi.it"], "sections": [{"heading": "1 Introduction", "text": "An online learner is an agent interacting with an unknown and arbitrary environment over a sequence of rounds. At each round t, the learner observes a data point (or instance) xt \u2208 X \u2282 Rd, outputs a prediction y\u0302t for the label yt \u2208 R associated with that instance, and incurs some loss `t(y\u0302t), which in this paper is the square loss (y\u0302t \u2212 yt)2. At the end of the round, the label yt is given to the learner, which he can use to reduce his loss in subsequent rounds. The performance of an online learner is typically measured using the regret. This is defined as the amount by which the learner\u2019s cumulative loss exceeds the cumulative loss (on the same sequence of instances and labels) of any function f in a given reference class F of functions,\nRT (f) = T\u2211 t=1 ( `t(y\u0302t)\u2212 `t ( f(xt) )) \u2200f \u2208 F . (1)\nNote that typical regret bounds apply to all f \u2208 F and to all individual data sequences. However, the bounds are allowed to scale with parameters arising from the interplay between f and the data sequence.\nIn order to capture complex environments, the reference class of functions should be large. In this work we focus on nonparametric classes F , containing all differentiable functions that are smooth with respect to some metric on X . Our approach builds on the simple and versatile algorithm for nonparametric online learning introduced in [7]. This algorithm has a bound on the regret RT (f) of order (ignoring logarithmic factors) 1 +\n\u221a\u221a\u221a\u221a d\u2211 i=1 \u2016\u2202if\u20162\u221e T d1+d \u2200f \u2208 F . (2) Here \u2016\u2202if\u2016\u221e is the value of the partial derivative \u2202f(x) / \u2202xi maximized over x \u2208 X . The square root term is the Lipschitz constant of f , measuring smoothness with respect to the Euclidean metric. However, in some directions f may be smoother than in others. Therefore, if we knew in advance the set of directions along which the best performing reference functions f are smooth, we could use this information to control regret better. In this paper we extend the algorithm from [7] and make it adaptive\nar X\niv :1\n70 5.\n07 85\n3v 1\n[ cs\n.L G\n] 2\n2 M\nay 2\n01 7\nto the Mahalanobis distance defined through an arbitrary positive definite matrix M with spectrum{ (ui, \u03bbi) }d i=1\nand unit spectral radius (\u03bb1 = 1). We prove a bound on the regret RT (f) of order (ignoring logarithmic factors) \u221adet\u03ba(M) +\n\u221a\u221a\u221a\u221a d\u2211 i=1 \u2016\u2207uif\u2016 2 \u221e \u03bbi T \u03c1T1+\u03c1T \u2200f \u2208 F . (3) Here \u03c1T \u2264 d is, roughly, the number of eigenvalues of M larger than a threshold shrinking polynomially in T , and det\u03ba(M) \u2264 1 is the determinant of M truncated at \u03bb\u03ba (with \u03ba \u2264 \u03c1T ). The quantity \u2016\u2207uif\u2016 2 \u221e is defined like \u2016\u2202if\u2016\u221e, but with the directional derivative\u2207f(x)>u instead of the partial derivative. When the spectrum of M is light-tailed (so that \u03c1T d and, simultaneously, det\u03ba(M) 1), with the smaller eigenvalues \u03bbi corresponding to eigenvectors in which f is smoother (so that the ratios \u2016\u2207uif\u2016 2 \u221e / \u03bbi remain controlled), then our bound improves on (2). On the other hand, when no preliminary knowledge about good f is available, we may run the algorithm with M equal to the identity matrix and recover exactly the bound (2).\nGiven that the regret can be improved by informed choices of M , it is natural to ask whether some kind of improvement is still possible when M is learned online, from the same data sequence on which the regret is being measured. Of course, this question makes sense if the data tell us something about the smoothness of the f against which we are measuring the regret. In the second part of the paper we implement this idea by considering a scenario where instances are drawn i.i.d. from some unknown distribution, labels are stochastically generated by some unknown regression function f0, and we have no preliminary knowledge about the directions along which f0 is smoother.\nIn this stochastic scenario, the expected gradient outer product matrix G = E [ \u2207f0(X)\u2207f0(X)> ] provides a natural choice for the matrix M in our algorithm. Indeed, E [( \u2207f0(X)>ui )2] = \u00b5i where u1, . . . ,ud are the eigenvectors of G while \u00b51, . . . , \u00b5d are the corresponding eigenvalues. However, as G is unknown, we run our algorithm in phases using a recently proposed estimator G\u0302 of G. The estimator is trained on the same data sequence and is fed to the algorithm at the beginning of each phase. Under mild assumptions on f0, the noise in the labels, and the instance distribution, we prove a high probability bound on the regret RT (f0) of order (ignoring logarithmic factors)1 +\n\u221a\u221a\u221a\u221a d\u2211 j=1 ( \u2225\u2225\u2207ujf0\u2225\u2225\u221e + \u2225\u2225\u2207V f0\u2225\u2225\u221e)2 \u00b5j/\u00b51 T \u03c1\u0303T1+\u03c1\u0303T . (4) Observe that the rate at which the regret grows is the same as the one in (3), though now the effective dimension parameter \u03c1\u0303T is larger than \u03c1T by an amount related to the rate of convergence of the eigenvalues of G\u0302 to those of G. The square root term is also similar to (3), but for the extra quantity\n\u2225\u2225\u2207V f0\u2225\u2225\u221e, which accounts for the error in approximating the eigenvectors of G. More precisely,\n\u2225\u2225\u2207V f0\u2225\u2225\u221e is \u2016\u2207vf\u2016\u221e maximized over directions v in the span of V , where V contains those eigenvectors of G that cannot be identified because their eigenvalues are too close to each other (we come back to this issue shortly). Finally, we lose the dependence on the truncated determinant, which is replaced here by its trivial upper bound 1.\nThe proof of (2) in [7] is based on the sequential construction of a sphere packing of X , where the spheres are centered on adaptively chosen instances xt, and have radii shrinking polynomially with time. Each sphere hosts an online learner, and each new incoming instance is predicted using the learner hosted in the nearest sphere. Our variant of that algorithm uses an ellipsoid packing, and computes distances using the Mahalanobis distance \u2016\u00b7\u2016M . The main new ingredient in the analysis leading to (3) is our notion of effective dimension \u03c1T (we call it the effective rank of M ), which measures how fast the spectrum of M vanishes. The proof also uses an ellipsoid packing bound and a lemma relating the Lipschitz constant to the Mahalanobis distance.\nThe proof of (4) is more intricate because G is only known up to a certain approximation. We use an estimator G\u0302, recently proposed in [14], which is consistent under mild distributional assumptions when f0\nis continuously differentiable. The first source of difficulty is adjusting the notion of effective rank (which the algorithm needs to compute) to compensate for the uncertainty in the knowledge of the eigenvalues of G. A further problematic issue arises because we want to measure the smoothness of f0 along the eigendirections of G, and so we need to control the convergence of the eigenvectors, given that G\u0302 converges to G in spectral norm. However, when two eigenvalues of G are close, then the corresponding eigenvectors in the estimated matrix G\u0302 are strongly affected by the stochastic perturbation (a phenomenon known as hybridization or spectral leaking in matrix perturbation theory, see [1, Section 2]). Hence, in our analysis we need to separate out the eigenvectors that correspond to well spaced eigenvalues from the others. This lack of discrimination causes the appearance in the regret of the extra term\n\u2225\u2225\u2207V f0\u2225\u2225\u221e."}, {"heading": "2 Related works", "text": "Nonparametric estimation problems have been a long-standing topic of study in statistics, where one is concerned with the recovery of an optimal function from a rich class under appropriate probabilistic assumptions. In online learning, the nonparametric approach was investigated in [15, 16, 17] by Vovk, who considered regression problems in large spaces and proved bounds on the regret. Minimax rates for the regret were later derived in [13] using a non-constructive approach. The first explicit online nonparametric algorithms for regression with minimax rates were obtained in [5].\nThe nonparametric online algorithm of [7] is known to have a suboptimal regret bound for Lipschitz classes of functions. However, it is a simple and efficient algorithm, well suited to the design of extensions that exhibit different forms of adaptivity to the data sequence. For example, the paper [10] derived a variant that automatically adapts to the intrinsic dimension of the data manifold. Our work explores an alternative direction of adaptivity, mainly aimed at taming the effect of the curse of dimensionality in nonparametric prediction through the learning of an appropriate Mahalanobis distance on the instance space. There is a rich literature on metric learning \u2014see the survey [2], where the Mahalanobis metric \u2016\u00b7\u2016M is typically learned through minimization of the pairwise loss function of the form `(M ,x,x\u2032). This loss is high whenever dissimilar pairs of x and x\u2032 are close in the Mahalanobis metric, and whenever similar ones are far apart in the same metric \u2014see, e.g. [20]. The works [8, 19, 6] analyzed generalization and consistency properties of online learning algorithms employing pairwise losses.\nIn this work we are primarily interested in using a metric \u2016\u00b7\u2016M where M is close to the gradient outer product matrix of the best model in the reference class of functions. As we are not aware whether pairwise loss functions can indeed consistently recover such metrics, we directly estimate the gradient outer product matrix. This approach to metric learning was mostly explored in statistics \u2014e.g., by locallylinear Empirical Risk Minimization on RKHS [12, 11], and through Stochastic Gradient Descent [4]. Our learning approach combines \u2014in a phased manner\u2014 a Mahalanobis metric extension of the algorithm by [7] with the estimator of [14]. Our work is also similar in spirit to the \u201cgradient weights\u201d approach of [9], which learns a distance based on a simpler diagonal matrix.\nPreliminaries and notation. Let B(z, r) \u2282 Rd be the ball of center z and radius r > 0 and let B(r) = B(0, r). We assume instances x belong to X \u2261 B(1) and labels y belong to Y \u2261 [0, 1].\nWe consider the following online learning protocol with oblivious adversary. Given an unknown sequence (x1, y1), (x2, y2), \u00b7 \u00b7 \u00b7 \u2208 X \u00d7 Y of instances and labels, for every round t = 1, 2, . . .\n1. the environment reveals instance xt \u2208 X ; 2. the learner selects an action y\u0302t \u2208 Y and incurs the square loss `t ( y\u0302t ) = ( y\u0302t \u2212 yt )2; 3. the learner observes yt.\nGiven a positive definite d \u00d7 d matrix M , the norm \u2016x\u2212 z\u2016M induced by M (a.k.a. Mahalanobis distance) is defined by \u221a (x\u2212 z)>M(x\u2212 z). The \u03b5-covering and \u03b5-packing numbers of a set S \u2282 Rd according to a metric \u03c1 are respectively denoted by N (S, \u03b5, \u03c1) andM(B, \u03b5, \u03c1). It is well known that M(S, 2\u03b5, \u03c1) \u2264 N (S, \u03b5, \u03c1) \u2264 M(S, \u03b5, \u03c1). For all differentiable f : X \u2192 Y and for any orthonormal\nAlgorithm 1 Nonparametric online regression Input: Positive definite d\u00d7 d matrix M .\n1: S \u2190 \u2205 . Centers 2: for t = 1, 2, . . . do 3: \u03b5t \u2190 t \u2212 1 1+\u03c1t . Update radius 4: Observe xt 5: if S \u2261 \u2205 then 6: S \u2190 {t}, Tt \u2190 \u2205 . Create initial ball 7: end if 8: s\u2190 arg min\ns\u2208S \u2016xt \u2212 xs\u2016M . Find active center\n9: if Ts \u2261 \u2205 then 10: yt = 1 2 11: else 12: y\u0302t \u2190 1\n|Ts| \u2211 t\u2032\u2208Ts yt\u2032 . Predict using active center\n13: end if 14: Observe yt 15: if \u2016xt \u2212 xs\u2016M \u2264 \u03b5t then 16: Ts \u2190 Ts \u222a {t} . Update list for active center 17: else 18: S \u2190 S \u222a {s}, Ts \u2190 \u2205 . Create new center 19: end if 20: end for\nbasis V \u2261 {u1, . . . ,uk} with k \u2264 d we define\n\u2016\u2207V f\u2016\u221e = max v \u2208 span(V ) \u2016v\u2016 = 1 sup x\u2208X \u2207f(x)>v .\nIf V = {u} we simply write \u2016\u2207uf\u2016\u221e. In the following, M is a positive definite d \u00d7 d matrix with eigenvalues \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd > 0 and corresponding eigenvectors u1, . . . ,ud. For each k = 1, . . . , d the truncated determinant is detk(M) = \u03bb1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u03bbk. The kappa function for the matrix M is defined by\n\u03ba(r, t) = max { m : \u03bbm \u2265 t\u2212 2 1+r , m = 1, . . . , d } t \u2265 1 and r = 1, . . . , d. (5)\nNote that \u03ba(r + 1, t) \u2264 \u03ba(r, t). Now define the effective rank of M at horizon t by\n\u03c1t = min {r : \u03ba(r, t) \u2264 r, r = 1, . . . , d} . (6)\nSince \u03ba(d, t) \u2264 d for all t \u2265 1, this is a well defined quantity. Note that \u03c11 \u2264 \u03c12 \u2264 \u00b7 \u00b7 \u00b7 \u2264 d. Also, \u03c1t = d for all t \u2265 1 when M is the d\u00d7 d identity matrix. Note that the effective rank \u03c1t measures the number of eigenvalues that are larger than a threshold that shrinks with t. Hence matrices M with extremely light-tailed spectra cause \u03c1t to remain small even when t grows large.\nThroughout the paper, we use f O= (g) and f O\u0303= (g) to denote, respectively, f = O(g) and f = O\u0303(g)."}, {"heading": "3 Online nonparametric learning with ellipsoid packing", "text": "In this section we present a variant (Algorithm 1) of the online nonparametric regression algorithm introduced in [7]. Since our analysis is invariant to rescalings of the matrix M , without loss of generality we assume M has unit spectral radius (i.e., \u03bb1 = 1). Algorithm 1 sequentially constructs a packing of\nX using M -ellipsoids centered on a subset of the past observed instances. At each step t, the label of the current instance xt is predicted using the average y\u0302t of the labels of past instances that fell inside the ellipsoid whose center xs is closest to xt in the Mahalanobis metric. At the end of the step, if xt was outside the closest ellipsoid, then a new ellipsoid is created with center xt. The radii \u03b5t of all ellipsoids are shrunk at rate t\u22121/(1+\u03c1t). The regret contribution of each ellipsoid is logarithmic in the number of predictions made. Since each instance is predicted by a single ellipsoid, the overall regret contribution of the ellipsoids is bounded by their number times log T . An additional regret term is due to the fact that \u2014at any point of time\u2014 the prediction of the algorithm is constant within the Voronoi cells of X induced by the current centers (recall that we predict with nearest neighbor). Hence, we pay an extra term equal to the radius of the ellipsoids times the Lipschitz constant with respect to the Mahalanobis metric.\nTheorem 1 (Regret with Fixed Metric). Suppose Algorithm 1 is run with a positive definite matrix M with eigenbasis u1, . . . ,ud and eigenvalues 1 = \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd > 0. Then, for any differentiable f : X \u2192 Y we have that\nRT (f) O\u0303 = \u221adet\u03ba(M) + \u221a\u221a\u221a\u221a d\u2211\ni=1\n\u2016\u2207uif\u2016 2 \u221e\n\u03bbi T \u03c1T1+\u03c1T where \u03ba = \u03ba(\u03c1T , T ) \u2264 \u03c1T \u2264 d.\nThe core idea of the proof (deferred to the supplementary material) is to maintain the trade-off between the overall regret contribution of the ellipsoids and an additional regret term arising due to approximation of f by the Voronoi partitioning. Since the former term is bounded by the number of ellipsoids times log T , it is essentially controlled by the packing number of an ellipsoid induced by M . The later term relies on the directional Lipschitzness of f with respect to the eigenbasis of M . We first prove two technical lemmas about packings of ellipsoids.\nLemma 1 (Volumetric packing bound). Consider a pair of norms \u2016 \u00b7 \u2016 , \u2016 \u00b7 \u2016\u2032 and let B,B\u2032 \u2282 Rd be the corresponding unit balls. Then\nM(B, \u03b5, \u2016 \u00b7 \u2016\u2032) \u2264 vol ( B + \u03b52B \u2032) vol ( \u03b5 2B \u2032 ) .\nLemma 2 (Ellipsoid packing bound). If B is the unit Euclidean ball then\nM ( B, \u03b5, \u2016 \u00b7 \u2016M ) \u2264\n( 8 \u221a 2\n\u03b5 )s s\u220f i=1 \u221a \u03bbi where s = max { i : \u221a \u03bbi \u2265 \u03b5, i = 1, . . . , d } .\nThe following lemma states that whenever f has bounded partial derivatives with respect to the eigenbase of M , then f is Lipschitz with respect to \u2016 \u00b7 \u2016M .\nLemma 3 (Bounded derivatives imply Lipschitzness in M -metric). Let f : X \u2192 R be everywhere differentiable. Then for any x,x\u2032 \u2208 X ,\n\u2223\u2223f(x)\u2212 f(x\u2032)\u2223\u2223 \u2264 \u2225\u2225x\u2212 x\u2032\u2225\u2225 M \u221a\u221a\u221a\u221a d\u2211 i=1 \u2016\u2207uif\u2016 2 \u221e \u03bbi ."}, {"heading": "4 Learning while learning the metric", "text": "In this section, we assume instances xt are realizations of i.i.d. random variables Xt drawn according to some fixed and unknown distribution \u00b5 which has a continuous density on its support X . We also assume labels yt are generated according to the noise model yt = f0(xt) + \u03bd(xt), where f0 is some unknown\nregression function and \u03bd(x) is a subgaussian zero-mean random variable for all x \u2208 X . We then simply write RT to denote the regret RT (f0). Note that RT is now a random variable which we bound with high probability.\nWe now show how the nonparametric online learning algorithm (Algorithm 1) of Section 3 can be combined with an algorithm that learns an estimate\nG\u0302n = 1\nn n\u2211 t=1 \u2207\u0302f0(xt)\u2207\u0302f0(xt)> (7)\nof the expected outer product gradient matrix G = E [ \u2207f0(X)\u2207f0(X)> ] . The algorithm (described in the supplementary material) is consistent under the following assumptions. Let X (\u03c4) be X blown up by a factor of 1 + \u03c4 .\nAssumption 1. 1. There exists \u03c40 > 0 such that f0 is continuously differentiable on X (\u03c40). 2. There exists G > 0 such that max\nx\u2208X (\u03c40) \u2016\u2207f0(x)\u2016 \u2264 G.\n3. The distribution \u00b5 is full-dimensional: there exists C\u00b5 > 0 such that for all x \u2208 X and \u03b5 > 0, \u00b5 ( B(x, \u03b5) ) \u2265 C\u00b5\u03b5d.\nOur algorithm works in phases i = 1, 2, . . . where phase i has length n(i) = 2i. Let T (i) = 2i+1 \u2212 2 be the index of the last time step in phase i. The algorithm uses a nonincreasing regularization sequence \u03b30 \u2265 \u03b31 \u2265 \u00b7 \u00b7 \u00b7 > 0. Let M\u0302(0) = \u03b30I . During each phase i, the algorithm predicts the data points by running Algorithm 1 with M = M\u0302(i\u2212 1)\n/\u2225\u2225M\u0302(i\u2212 1)\u20162 (where \u2016 \u00b7 \u20162 denotes the spectral norm). Simultaneously, the gradient outer product estimate (7) is trained over the same data points. At the end of phase i, the current gradient outer product estimate G\u0302(i) = G\u0302T (i) is used to form a new matrix M\u0302(i) = G\u0302(i) + \u03b3T (i)I . Algorithm 1 is then restarted in phase i+ 1 with M = M\u0302(i)\n/\u2225\u2225M\u0302(i)\u20162. Let \u00b51 \u2265 \u00b52 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u00b5d be the eigenvalues and u1, . . . ,ud be the eigenvectors of G. We define the\nj-th eigenvalue separation \u2206j by \u2206j = min\nk 6=j \u2223\u2223\u00b5j \u2212 \u00b5k\u2223\u2223 . For any \u2206 > 0 define also V\u2206 \u2261 { uj : |\u00b5j \u2212 \u00b5k| \u2265 \u2206, k 6= j } and V \u22a5\u2206 = {u1, . . . ,ud} \\ V\u2206. Our results are expressed in terms of the effective rank (6) of G at horizon T . However, in order to account for the error in estimating the eigenvalues of G, we define the effective rank \u03c1\u0303t with respect to the following slight variant of the function kappa,\n\u03ba\u0303(r, t) = max { m : \u00b5m + 2\u03b3t \u2265 \u00b51t \u2212 2 1+r , m = 1, . . . , d } t \u2265 1 and r = 1, . . . , d.\nLet M\u0302(i) be the estimated gradient outer product constructed at the end of phase i, and let \u00b5\u03021(i) +\u03b3(i) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u00b5\u0302d(i) + \u03b3(i) and u\u03021(i), . . . , u\u0302d(i) be the eigenvalues and eigenvectors of M\u0302(i), where we also write \u03b3(i) to denote \u03b3T (i). We use \u03ba\u0302 to denote the kappa function with estimated eigenvalues and \u03c1\u0302 to denote the effective rank defined through \u03ba\u0302.\nTheorem 2. Suppose Assumption 1 holds. If the algorithm is ran with a regularization sequence \u03b30 = 1 and \u03b3t = t \u2212\u03b1 for some \u03b1 > 0 such that \u03b3t \u2265 \u03b3t for all t \u2265 ( d+ 1 / 2\u00b5d )1/\u03b1 and for \u03b31 \u2265 \u03b32 \u2265 \u00b7 \u00b7 \u00b7 > 0 satisfying Lemma 4, then for any given \u2206 > 0\nRT O\u0303 = 1 + \u221a\u221a\u221a\u221a d\u2211\nj=1\n( \u2225\u2225\u2207ujf0\u2225\u2225\u221e + \u2225\u2225\u2207V \u22a5\u2206 f0\u2225\u2225\u221e)2 \u00b5j/\u00b51 T \u03c1\u0303T1+\u03c1\u0303T with high probability with respect to the random draw of X1, . . . ,XT .\nNote that the asymptotic notation is hiding terms that depend on 1/\u2206, hence we can not zero out the term \u2225\u2225\u2207V \u22a5\u2206 f0\u2225\u2225\u221e in the bound by taking \u2206 arbitrarily small.\nProof. Pick the smallest i0 such that\nT (i0) \u2265 ( d+ 1\n2\u00b5d\n)1/\u03b1 (8)\n(we need this condition in the proof). The total regret in phases 1, 2, . . . , i0 is bounded by ( d+ 1 / 2\u00b5d )1/\u03b1\n= O(1). Let the value \u03c1\u0302T (i) at the end of phase i be denoted by \u03c1\u0302(i). By Theorem 1, the regret RT (i+ 1) of Algorithm 1 in each phase i+ 1 > i0 is deterministically upper bounded by\nRT (i+ 1) \u2264 8 ln (e2i+1)(8\u221a2)\u03c1\u0302(i+1) + 4 \u221a\u221a\u221a\u221a\u221a d\u2211\nj=1\n\u2225\u2225\u2225\u2207u\u0302j(i)f0\u2225\u2225\u22252\u221e \u03bbj(i) / \u03bb1(i)  2(i+1) \u03c1\u0302(i+1)1+\u03c1\u0302(i+1) (9) where \u03bbj(i) = \u00b5\u0302j(i) + \u03b3(i). Here we used the trivial upper bound det\u03ba ( M\u0302(i)\n/\u2225\u2225M\u0302(i)\u20162) \u2264 1 for all \u03ba = 1, . . . , d. Now assume that \u00b5\u03021(i) + \u03b3(i) \u2264 ( \u00b5\u0302m(i) + \u03b3(i) ) t 2 1+r for some m, r \u2208 {1, . . . , d} and for some t in phase i+ 1. Hence, using Lemma 4 and \u03b3t \u2264 \u03b3t, we have that\nmax j=1,...,d \u2223\u2223\u00b5\u0302j(i)\u2212 \u00b5j\u2223\u2223 \u2264 \u2225\u2225G\u0302(i)\u2212G\u2225\u22252 \u2264 \u03b3(i) with high probability. (10) where the first inequality is straightforward. Hence we may write\n\u00b51 \u2264 \u00b51 \u2212 \u03b3(i) + \u03b3(i) \u2264 \u00b5\u03021(i) + \u03b3(i) \u2264 ( \u00b5\u0302m(i) + \u03b3(i) ) t 2 1+r\n\u2264 ( \u00b5m + \u03b3(i) + \u03b3(i) ) t 2 1+r (using Lemma 4)\n\u2264 ( \u00b5m + 2\u03b3(i) ) t 2 1+r .\nRecall \u03b3(i) = T (i)\u2212\u03b1. Using Lemma 5, the derivative of F (t) = ( \u00b5m + 2 ( T (i) + t )\u2212\u03b1) t 2 1+r is positive for all t \u2265 1 when\nT (i) \u2265 ( r + 1\n2\u00b5d\n)1/\u03b1 \u2265 ( r + 1\n2\u00b5m )1/\u03b1 which is guaranteed by our choice (8). Hence, ( \u00b5m + 2\u03b3(i) ) t 2 1+r \u2264 ( \u00b5m + 2\u03b3T ) ) T 2 1+r and so\n\u00b5\u03021(i) + \u03b3(i) \u00b5\u0302m(i) + \u03b3(i) \u2264 t 2 1+r implies\n\u00b51 \u00b5m + 2\u03b3T \u2264 T 2 1+r .\nRecalling the definitions of \u03ba\u0303 and \u03ba\u0302, this in turn implies \u03ba\u0302(r, t) \u2264 \u03ba\u0303(r, T ), which also gives \u03c1\u0302t \u2264 \u03c1\u0303T for all t \u2264 T . Next, we bound the approximation error in each individual eigenvalue of G. By (10) we obtain, for any phase i and for any j = 1, . . . , d,\n\u00b5j + 2\u03b3(i) \u2265 \u00b5j + \u03b3(i) + \u03b3(i) \u2265 \u00b5\u0302j(i) + \u03b3(i) \u2265 \u00b5j \u2212 \u03b3(i) + \u03b3(i) \u2265 \u00b5j .\nHence, bound (9) implies\nRT (i+ 1) \u2264 8 ln (e2i+1)12\u03c1\u0303T + 4 \u221a\u221a\u221a\u221a(\u00b51 + 2\u03b3(i)) d\u2211\nj=1\n\u2225\u2225\u2207u\u0302jf0\u2225\u22252\u221e \u00b5j  2(i+1) \u03c1\u0303T1+\u03c1\u0303T . (11) The error in approximating the eigenvectors of G is controlled via the following first-order eigenvector approximation result from matrix perturbation theory [21, equation (10.2)], for any vector v of constant norm,\nv> ( u\u0302j(i)\u2212 uj ) = \u2211 k 6=j\nu>k ( M\u0302(i)\u2212G ) uj\n\u00b5j \u2212 \u00b5k v>uk + o\n(\u2225\u2225M\u0302(i)\u2212G\u2225\u22252 2 ) \u2264 \u2211 k 6=j 2\u03b3(i) \u00b5j \u2212 \u00b5k v>uk + o ( \u03b3(i)2 ) (12)\nwhere we used u>k ( M\u0302(i) \u2212G ) uj \u2264 \u2225\u2225M\u0302(i) \u2212G\u2225\u2225 2 \u2264 \u03b3(i) + \u03b3(i) \u2264 2\u03b3(i). Then for all j such that uj \u2208 V\u2206,\n\u2207f0(x)> ( u\u0302j(i)\u2212 uj ) = \u2211 k 6=j 2\u03b3(i) \u00b5j \u2212 \u00b5k \u2207f0(x)>uk + o ( \u03b3(i)2 ) \u2264 2\u03b3(i) \u2206 \u221a d \u2016\u2207f0(x)\u20162 + o ( \u03b3(i)2 ) .\nNote that the coefficients \u03b1k = u>k ( M\u0302(i)\u2212G ) uj \u00b5j \u2212 \u00b5k + o ( \u03b3(i)2 ) k 6= j\nare a subset of coordinate values of vector u\u0302j(i)\u2212 uj with respect to the orthonormal basis u1, . . . ,ud. Then, by Parseval\u2019s identity,\n4 \u2265 \u2016u\u0302j(i)\u2212 uj\u201622 \u2265 \u2211 k 6=j \u03b12k .\nTherefore, it must be that\nmax k 6=j \u2223\u2223\u2223\u2223\u2223u>k ( M\u0302(i)\u2212G ) uj\n\u00b5j \u2212 \u00b5k \u2223\u2223\u2223\u2223\u2223 \u2264 2 + o(\u03b3(i)2) . For any j such that uj \u2208 V \u22a5\u2206 , since \u00b5j \u2212 \u00b5k \u2265 \u2206 for all uk \u2208 V\u2206, we may write \u2207f0(x)> ( u\u0302j(i)\u2212 uj ) \u2264 2\u03b3(i)\n\u2206 \u2211 uk\u2208V\u2206 \u2207f0(x)>uk + ( 2 + o ( \u03b3(i)2 )) \u2211 uk\u2208V \u22a5\u2206 \u2207f0(x)>uk + o ( \u03b3(i)2 ) \u2264 2\u03b3(i)\n\u2206\n\u221a d \u2016P V\u2206 \u2207f0(x)\u20162 + ( 2 + o ( \u03b3(i)2 ))\u221a d \u2225\u2225P V \u22a5\u2206\u2207f0(x)\u2225\u22252 + o(\u03b3(i)2)\nwhere P V\u2206 and P V \u22a5\u2206 are the orthogonal projections onto, respectively, V\u2206 and V \u22a5 \u2206 . Therefore, we have that\u2225\u2225\u2207u\u0302jf0\u2225\u2225\u221e = sup\nx\u2208X \u2207f0(x)>u\u0302j(i) = sup x\u2208X \u2207f0(x)>\n( u\u0302j(i)\u2212 uj + uj ) \u2264 sup\nx\u2208X \u2207f0(x)>uj + sup x\u2208X \u2207f0(x)>\n( u\u0302j(i)\u2212 uj ) \u2264 \u2225\u2225\u2207ujf0\u2225\u2225\u221e + 2\u03b3(i)\u2206 \u221ad \u2016\u2207V\u2206f0\u2016\u221e + (2 + o(\u03b3(i)2))\u221ad\u2225\u2225\u2207V \u22a5\u2206 f0\u2225\u2225\u221e + o(\u03b3(i)2) .\n(13)\nLetting \u03b1\u2206(i) = 2\u03b3(i)\n\u2206 \u221a d \u2016\u2207V\u2206f0\u2016\u221e + ( 2 + o ( \u03b3(i)2 ))\u221a d \u2225\u2225\u2207V \u22a5\u2206 f0\u2225\u2225\u221e + o(\u03b3(i)2) we can upper\nbound (11) as follows\nRT (i+ 1) \u2264 8 ln (e2i+1)12\u03c1\u0303T+ 4 \u221a\u221a\u221a\u221a(\u00b51 + 2\u03b3(i)) d\u2211\nj=1\n( \u2225\u2225\u2207ujf0\u2225\u2225\u221e+ \u03b1\u2206(i))2 \u00b5j  2(i+1) \u03c1\u0303T1+\u03c1\u0303T . Recall that, due to (10), the above holds at the end of each phase i+ 1 with high probability. Now observe that \u03b3(i) = O ( 2\u2212\u03b1i ) and so \u03b1\u2206(i) O = (\u2016\u2207V\u2206f0\u2016\u221e /\u2206 + \u2225\u2225\u2207V \u22a5\u2206 f0\u2225\u2225\u221e). Hence, by summing over phases\ni = 1, . . . , \u2308 log2 T \u2309 and applying the union bound,\nRT = dlog2 T e\u2211 i=1 RT (i)\n\u2264 8 ln (eT )12d+ 4 \u221a\u221a\u221a\u221a(\u00b51 + 2\u03b3(i\u2212 1)) d\u2211\nj=1\n( \u2225\u2225\u2207ujf0\u2225\u2225\u221e+ \u03b1\u2206(i\u2212 1))2 \u00b5j (2 \u03c1\u0303T1+\u03c1\u0303T )i\nO\u0303 = 1 + d\u2211 j=1 (\u2225\u2225\u2207ujf0\u2225\u2225\u221e + \u2225\u2225\u2207V \u22a5\u2206 f0\u2225\u2225\u221e)2 \u00b5j / \u00b51 T \u03c1\u0303T1+\u03c1\u0303T concludes the proof."}, {"heading": "5 Conclusions and future work", "text": "We presented an efficient algorithm for online nonparametric regression which adapts to the directions along which the regression function f0 is smoother. It does so by learning the Mahalanobis metric through the estimation of the gradient outer product matrix E[\u2207f0(X)\u2207f0(X)>]. As a preliminary result, we analyzed the regret of a generalized version of the algorithm from [7], capturing situations where one competes against functions with directional Lipschitzness with respect to an arbitrary Mahalanobis metric. Our main result is then obtained through a phased algorithm that estimates the gradient outer product matrix while running online nonparametric regression on the same sequence. Both algorithms automatically adapt to the effective rank of the metric.\nThis work could be extended by investigating a variant of Algorithm 1 for classification, in which ball radii shrink at a nonuniform rate, depending on the mistakes accumulated within each ball rather than on time. This could lead to the ability of competing against functions f that are only locally Lipschitz. In addition, it is conceivable that under appropriate assumptions, a fraction of the balls could stop shrinking at a certain point when no more mistakes are made. This might yield better asymptotic bounds than those implied by Theorem 1, because \u03c1T would never attain the ambient dimension d."}, {"heading": "A Nonparametric gradient learning", "text": "In this section we describe a nonparametric gradient learning algorithm introduced in [14]. Throughout this section, we assume instances xt are realizations of i.i.d. random variables Xt drawn according to some fixed and unknown distribution \u00b5 which has a continuous density on its support X . Labels yt are generated according to the noise model yt = f(xt) + \u03bd(xt), where \u03bd(x) is a subgaussian zero-mean random variable for all x \u2208 X . The algorithm computes a sequence of estimates f\u03021, f\u03022, . . . of the regression function f through kernel regression. Let Xn \u2261 { x1, . . . ,xn } \u2282 X be the data observed so far and let y1, . . . , yn their corresponding labels. Let K : R+ \u2192 R+ be a nonincreasing kernel, strictly positive on [0, 1), and such that K(1) = 0. Then the estimate at time n is defined by\nf\u0302n(x) = n\u2211 t=1 yt \u03c9t(x) where \u03c9t(x) =  K ( \u2016x\u2212 xt\u2016 / \u03b5n )\u2211n s=1K ( \u2016x\u2212 xs\u2016 / \u03b5n ) if B(x, \u03b5n) \u2229 Xn 6= \u2205,\n1/n otherwise\nwhere \u03b5n > 0 is the kernel scaling parameter. We then approximate the gradient of f\u0302 at any given point through the finite difference method\n\u2206i(x) = 1\n2\u03c4n\n( f\u0302(x + \u03c4nei)\u2212 f\u0302(x\u2212 \u03c4nei) ) for i = 1, . . . , d\nwhere \u03c4n > 0 is a parameter. Let further Ai(x) = I {\nmin b\u2208{\u2212\u03c4n,\u03c4n}\n\u00b5n ( B(x + bei, \u03b5/2) ) \u2265 2d\nn (ln 2n)\n} for i = 1, . . . , d\nwhere \u00b5n is the empirical distribution of \u00b5 after observing Xn, and define the gradient estimate \u2207\u0302f(xt) = ( \u22061(xt)A1(xt), . . . ,\u2206d(xt)Ad(xt) ) .\nThe algorithm outputs at time n the gradient outer product estimate\nG\u0302n = 1\nn n\u2211 t=1 \u2207\u0302f(xt)\u2207\u0302f(xt)>\nLet G = E [ \u2207f(X)\u2207f(X)> ] be the expected gradient outer product, where X has law \u00b5. The next\nlemma states that, under Assumption 1, G\u0302n is a consistent estimate of G.\nLemma 4 ([14, Theorem 1]). If Assumption 1 holds, then there exists a nonnegative and nonincreasing sequence {\u03b3n}n\u22651 such that for all n, the estimated gradient outerproduct (7) computed with parameters \u03b5n > 0, and 0 < \u03c4n < \u03c40 satisfies \u2225\u2225G\u0302n \u2212G\u2225\u22252 \u2264 \u03b3n with high probability with respect do the random draw of X1, . . . ,Xn. Moreover, if \u03c4n = \u0398 ( \u03b5 1/4 n ) , \u03b5n = \u2126 (( lnn ) 2 dn\u2212 1 d ) , and \u03b5n = O ( n \u2212 1 2(d+1) ) then \u03b3n \u2192 0 as n\u2192\u221e.\nThe actual rate of convergence depends, in a complicated way, on parameters related to the distribution \u00b5 and the regression function f . In our application of Lemma 4 we assume \u03b3n \u2264 n\u2212\u03b1 for all n large enough and for some \u03b1 > 0. Note also that the convergence of G\u0302n to G holds in probability with respect to the random draw of X1, . . . ,Xn. Hence there is a confidence parameter \u03b4 which is not shown here. However, the dependence of the convergence rate on 1\u03b4 is only polylogarithmic and therefore not problematic for our applications."}, {"heading": "B Proofs from Section 3", "text": "Lemma (Volumetric packing bound). Consider a pair of norms \u2016 \u00b7 \u2016 , \u2016 \u00b7 \u2016\u2032 and let B,B\u2032 \u2282 Rd be the corresponding unit balls. Then\nM(B, \u03b5, \u2016 \u00b7 \u2016\u2032) \u2264 vol ( B + \u03b52B \u2032) vol ( \u03b5 2B \u2032 ) .\nProof. Let {x1, . . . ,xM} be a maximal \u03b5-packing of B according to \u2016 \u00b7 \u2016\u2032. Since we have a packing, the \u2016 \u00b7 \u2016\u2032-balls of radius \u03b5/2 and centers x1, . . . ,xM are disjoint, and their union is contained in B + \u03b52B\n\u2032. Thus,\nMvol (\u03b5 2 B\u2032 ) \u2264 vol ( B + \u03b5 2 B\u2032 )\nwhich concludes the proof.\nLemma (Ellipsoid packing bound). If B is the unit Euclidean ball then\nM ( B, \u03b5, \u2016 \u00b7 \u2016M ) \u2264\n( 8 \u221a 2\n\u03b5 )s s\u220f i=1 \u221a \u03bbi where s = max { i : \u221a \u03bbi \u2265 \u03b5, i = 1, . . . , d } .\nProof. The change of variable x\u2032 = M1/2x implies \u2016x\u20162 = \u2016x\u2032\u2016M\u22121 and \u2016x\u2016M = \u2016x\u2032\u20162. Therefore M ( B, \u03b5, \u2016 \u00b7 \u2016M ) =M ( E, \u03b5, \u2016 \u00b7 \u20162 ) where E \u2261 { x \u2208 Rd : \u2016x\u2016M\u22121 \u2264 1 } is the unit ball in the norm \u2016 \u00b7 \u2016M\u22121 . Next, we write the coordinates (x1, . . . , xd) of any point x \u2208 Rd using the orthonormal basis\nu1, . . . ,ud. Consider the truncated ellipsoid E\u0303 \u2261 {x \u2208 E : xi = 0, i = s+ 1, . . . , d}. By adapting an argument from [18], we prove that any \u03b5-cover of E\u0303 according to \u2016 \u00b7 \u20162 is also a ( \u03b5 \u221a 2 ) -cover of E according to the same norm. Indeed, let S\u0303 \u2282 E\u0303 be a \u03b5-cover of E\u0303. Fix any x \u2208 E and let\nmin x\u0303\u2208S\u0303 \u2016x\u2212 x\u0303\u201622 = min x\u0303\u2208S\u0303 s\u2211 j=1 ( xj \u2212 x\u0303j )2 + d\u2211 j=s+1 x2j\n\u2264 \u03b52 + d\u2211\nj=s+1\nx2j (since S\u0303 is a \u03b5-covering of E\u0303)\n\u2264 \u03b52 + \u03bbs+1 d\u2211\nj=s+1\nx2j \u03bbj\n(since \u03bbs+1/\u03bbj \u2265 1 for j = s+ 1, . . . , d)\n\u2264 2 \u03b52\nwhere the last inequality holds since \u03bbs+1 \u2264 \u03b52 and since \u2016x\u20162M\u22121 = \u2211d i=1 x 2 i /\u03bbi \u2264 1 for any x \u2208 E, where xi = u>i x for all i = 1, . . . , d. Let B\u2032 \u2282 Rd be the unit Euclidean ball, and let B\u0303\u2032 \u2261 {x \u2208 B\u2032 : xi = 0, i = s+ 1, . . . , d} be its truncated version. Since \u03bbi \u2265 \u03b52 for i = 1, . . . , s we have that for all x \u2208 \u03b5B\u0303\u2032, x21 + \u00b7 \u00b7 \u00b7+ x2s \u2264 \u03b52 and so\n\u2016x\u20162M\u22121 = s\u2211 i=1 x2i \u03bbi \u2264 s\u2211 i=1 \u03b52 \u03bbi \u2264 1 .\nTherefore \u03b5B\u0303\u2032 \u2286 E\u0303 which implies vol ( E\u0303 + \u03b52B\u0303 \u2032) \u2264 vol(2E\u0303). M ( E, 2\u03b5 \u221a 2, \u2016 \u00b7 \u20162 ) \u2264 N ( E, \u03b5 \u221a 2, \u2016 \u00b7 \u20162 )\n\u2264 N ( E\u0303, \u03b5, \u2016 \u00b7 \u20162 ) \u2264M ( E\u0303, \u03b5, \u2016 \u00b7 \u20162\n) \u2264 vol ( E\u0303 + \u03b52B\u0303 \u2032 )\nvol ( \u03b5 2B\u0303 \u2032 ) (by Lemma 1)\n\u2264 vol ( 2E\u0303 ) vol ( \u03b5 2B\u0303 \u2032 ) = (4 \u03b5 )s vol(E\u0303) vol ( B\u0303\u2032 )\nNow, using the standard formula for the volume of an ellipsoid,\nvol ( E\u0303 ) = vol ( B\u0303\u2032 ) s\u220f i=1 \u221a \u03bbi .\nThis concludes the proof.\nThe following lemma states that whenever f has bounded partial derivatives with respect to the eigenbase of M , then f is Lipschitz with respect to \u2016 \u00b7 \u2016M .\nLemma (Bounded derivatives imply Lipschitzness in M -metric). Let f : X \u2192 R be everywhere differentiable. Then for any x,x\u2032 \u2208 X ,\n\u2223\u2223f(x)\u2212 f(x\u2032)\u2223\u2223 \u2264 \u2225\u2225x\u2212 x\u2032\u2225\u2225 M \u221a\u221a\u221a\u221a d\u2211 i=1 \u2016\u2207uif\u2016 2 \u221e \u03bbi .\nProof. By the mean value theorem, there exists a z on the segment joining x and y such that f(x) \u2212 f(y) = \u2207f(z)> (x\u2212 y). Hence\nf(x)\u2212 f(y) = \u2207f(z)> (x\u2212 y)\n= d\u2211 i=1 \u2207f(z)>uiu>i (x\u2212 y)\n\u2264 d\u2211 i=1 ( sup z\u2032\u2208X \u2207f(z\u2032)>ui ) u>i (x\u2212 y)\n= d\u2211 i=1 \u2016\u2207uif\u2016\u221e\u221a \u03bbi (\u221a \u03bbiu > i (x\u2212 y) )\n\u2264 \u221a\u221a\u221a\u221a d\u2211 i=1 \u2016\u2207uif\u20162\u221e \u03bbi \u221a\u221a\u221a\u221a d\u2211 i=1 \u03bbi ( u>i (x\u2212 y)\n)2 (by the Cauchy-Schwarz inequality) = \u2225\u2225x\u2212 y\u2225\u2225\nM \u221a\u221a\u221a\u221a d\u2211 i=1 \u2016\u2207uif\u20162\u221e \u03bbi .\nBy symmetry, we can upper bound f(y)\u2212 f(x) with the same quantity.\nNow we are ready to prove the regret bound.\nTheorem (Regret with Fixed Metric). Suppose Algorithm 1 is run with a positive definite matrix M with eigenbasis u1, . . . ,ud and eigenvalues 1 = \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd > 0. Then, for any differentiable f : X \u2192 Y we have that\nRT (f) O\u0303 = \u221adet\u03ba(M) + \u221a\u221a\u221a\u221a d\u2211\ni=1\n\u2016\u2207uif\u2016 2 \u221e\n\u03bbi T \u03c1T1+\u03c1T where \u03ba = \u03ba(\u03c1T , T ) \u2264 \u03c1T \u2264 d.\nProof. Let St be the value of the variable S at the end of time t. Hence S0 = \u2205. The functions \u03c0t : X \u2192 {1, . . . , t} for t = 1, 2, . . . map each data point x to its closest (in norm \u2016 \u00b7 \u2016M ) center in St\u22121,\n\u03c0t(x) =\n{ arg min s\u2208St\u22121 \u2016x\u2212 xs\u2016M if St\u22121 6\u2261 \u2205\nt otherwise.\nThe set Ts contain all data points xt that at time t belonged to the ball with center xs and radius \u03b5t,\nTs \u2261 {t : \u2016xt \u2212 xs\u2016M \u2264 \u03b5t, t = s, . . . , T} .\nFinally, y?s is the best fixed prediction for all examples (xt, yt) such that t \u2208 Ts,\ny?s = arg min y\u2208Y \u2211 t\u2208Ts `t(y) = 1 |Ts| \u2211 t\u2208Ts yt . (14)\nWe proceed by decomposing the regret into a local (estimation) and a global (approximation) term,\nRT (f) = T\u2211 t=1 ( `t(y\u0302t)\u2212 `t ( f(xt) )) = T\u2211 t=1 ( `t(y\u0302t)\u2212 `t ( y?\u03c0t(xt) )) + T\u2211 t=1 ( `t ( y?\u03c0t(xt) ) \u2212 `t ( f(xt) )) .\nThe estimation term is bounded as\nT\u2211 t=1 ( `t(y\u0302t)\u2212 `t ( y?\u03c0t(xt) )) = \u2211 s\u2208ST \u2211 t\u2208Ts ( `t(y\u0302t)\u2212 `t(y?s) ) \u2264 8 \u2211 s\u2208ST ln(e|Ns|) \u2264 8 ln(eT )|ST | .\nThe first inequality is a known bound on the regret under square loss [3, page 43]. We upper bound the size of the final packing ST using Lemma 2,\n|ST | \u2264 M ( B, \u03b5T , \u2016 \u00b7 \u2016M ) \u2264\n( 8 \u221a 2\n\u03b5T )\u03ba \u03ba\u220f i=1 \u221a \u03bbi \u2264 ( 8 \u221a 2 )\u03ba\u221a det\u03ba(M)T \u03ba 1+\u03c1T\nwhere \u03ba = \u03ba(\u03c1T , T ). Therefore, since \u03c1T \u2265 \u03ba(\u03c1T , T ),\nT\u2211 t=1 ( `t(y\u0302t)\u2212 `t ( y?\u03c0t(xt) )) \u2264 8 ln(eT ) ( 8 \u221a 2 )\u03c1T\u221adet\u03ba(M)T \u03c1T1+\u03c1T . (15)\nNext, we bound the approximation term. Using (14) we have\nT\u2211 t=1 ( `t ( y?\u03c0t(xt) ) \u2212 `t ( f(xt) )) \u2264 T\u2211 t=1 ( `t ( f(x\u03c0t(xt)) ) \u2212 `t ( f(xt) )) .\nNote that `t is 2-Lipschitz because yt, y\u0302t \u2208 [0, 1]. Hence, using Lemma 3, `t ( f(x\u03c0t(xt)) ) \u2212 `t ( f(xt) ) \u2264 2 \u2223\u2223f(x\u03c0t(xt))\u2212 f(xt)\u2223\u2223\n\u2264 2 \u2225\u2225xt \u2212 x\u03c0t(xt)\u2225\u2225M \u221a\u221a\u221a\u221a d\u2211 i=1 \u2016\u2207uif\u2016 2 \u221e \u03bbi\n\u2264 2\u03b5t \u221a\u221a\u221a\u221a d\u2211 i=1 \u2016\u2207uif\u2016 2 \u221e \u03bbi .\nRecalling \u03b5t = t \u2212 1 1+\u03c1t where \u03c1t \u2264 \u03c1t+1, we write\nT\u2211 t=1 t \u2212 1 1+\u03c1t \u2264 T\u2211 t=1 t \u2212 1 1+\u03c1T \u2264 \u222b T 0 \u03c4 \u2212 1 1+\u03c1T d\u03c4 = ( 1 + 1 \u03c1T ) T \u03c1T 1+\u03c1T \u2264 2T \u03c1T 1+\u03c1T .\nThus we may write\nT\u2211 t=1 ( `t ( y?\u03c0t(xt) ) \u2212 `t ( f(xt) )) \u2264 4\n \u221a\u221a\u221a\u221a d\u2211\ni=1\n\u2016\u2207uif\u2016 2 \u221e\n\u03bbi T \u03c1T1+\u03c1T . The proof is concluded by combining the above with (15)."}, {"heading": "C Proofs from Section 4", "text": "Lemma 5. Let \u00b5d, \u03b1 > 0 and d \u2265 1. Then the derivative of F (t) = ( \u00b5d + 2 ( T0 + t )\u2212\u03b1) t 2 1+d\nis positive for all t \u2265 1 when\nT0 \u2265 ( d+ 1\n2\u00b5d\n)1/\u03b1 .\nProof. We have that F \u2032(t) \u2265 0 if and only if\nt \u2264 2(T0 + t) \u03b1(d+ 1)\n( 1 + (T0 + t) \u03b1\u00b5d ) This is implied by\nt \u2264 2\u00b5d(T0 + t) 1+\u03b1\n\u03b1(d+ 1) or, equivalently, T0 \u2265 A1/(1+\u03b1)t1/(1+\u03b1) \u2212 t\nwhere A = \u03b1(d+ 1)/(2\u00b5d). The right-hand side A1/(1+\u03b1)t1/(1+\u03b1) \u2212 t is a concave function of t. Hence the maximum is found at the value of t where the derivative is zero, this value satisfies\nA1/(1+\u03b1)\n1 + \u03b1 t\u2212\u03b1/(1+\u03b1) = 1 which solved for t gives t = A1/\u03b1(1 + \u03b1)\u2212(1+\u03b1)/\u03b1 .\nSubstituting this value of t in A1/(1+\u03b1)t1/(1+\u03b1) \u2212 t gives the condition T0 \u2265 A1/\u03b1\u03b1(1 + \u03b1)\u2212(1+\u03b1)/\u03b1 which is satisfied when T0 \u2265 A1/\u03b1 ."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We study algorithms for online nonparametric regression that learn the directions along which the<lb>regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient<lb>outer product matrix G of the regression function (automatically adapting to the effective rank of this<lb>matrix), while simultaneously bounding the regret \u2014on the same data sequence\u2014 in terms of the<lb>spectrum of G. As a preliminary step in our analysis, we generalize a nonparametric online learning<lb>algorithm by Hazan and Megiddo by enabling it to compete against functions whose Lipschitzness is<lb>measured with respect to an arbitrary Mahalanobis metric.", "creator": "LaTeX with hyperref package"}}}