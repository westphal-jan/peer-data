{"id": "1705.08480", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit", "abstract": "recurrent neural networks architectures vastly excel at processing sequences by modelling dependencies continually over different timescales. the recently firmly introduced recurrent weighted average ( rwa ) unit captures long term linear dependencies making far by better than an lstm on several challenging tasks. the rwa achieves this by applying attention to each input and computing a weighted average posterior over the full history of its computations. unfortunately, today the rwa cannot change the attention it merely has assigned to previous timesteps, and so struggles with carrying out continually consecutive tasks or tasks involved with changing privacy requirements. we present the appropriate recurrent discounted attention ( rda ) unit that builds on the rwa by additionally allowing the discounting of the past.", "histories": [["v1", "Tue, 23 May 2017 18:57:50 GMT  (375kb)", "https://arxiv.org/abs/1705.08480v1", null], ["v2", "Mon, 19 Jun 2017 08:53:04 GMT  (375kb)", "http://arxiv.org/abs/1705.08480v2", "Updated results of RDA-exp-tanh unit for the wikipedia char prediction task"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["brendan maginnis", "pierre h richemond"], "accepted": false, "id": "1705.08480"}, "pdf": {"name": "1705.08480.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["b.maginnis@imperial.ac.uk", "pierre.richemond@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n08 48\n0v 2\n[ cs\n.L G\n] 1\n9 Ju\nWe empirically compare our model to RWA, LSTM and GRU units on several challenging tasks. On tasks with a single output the RWA, RDA and GRU units learn much quicker than the LSTM and with better performance. On the multiple sequence copy task our RDA unit learns the task three times as quickly as the LSTM or GRU units while the RWA fails to learn at all. On the Wikipedia character prediction task the LSTM performs best but it followed closely by our RDA unit. Overall our RDA unit performs well and is sample efficient on a large variety of sequence tasks."}, {"heading": "1 Introduction", "text": "Many types of information such as language, music and video can be represented as sequential data. Sequential data often contains related information separated by many timesteps, for instance a poem may start and end with the same line, a scenario which we call long term dependencies. Long term dependencies are difficult to model as we must retain information from the whole sequence and this increases the complexity of the model.\nA class of model capable of capturing long term dependencies are Recurrent Neural Networks (RNNs). A specific RNN architecture, known as Long Short-Term Memory (LSTM) [14], is the benchmark against which other RNNs are compared. LSTMs have been shown to learn many difficult sequential tasks effectively. They store information from the past within a hidden state that is combined with the latest input at each timestep. This hidden state can carry information right from the beginning of the input sequence, which allows long term dependencies to be captured. However, the hidden state tends to focus on the more recent past and while this mostly works well, in tasks requiring equal weighting between old and new information LSTMs can fail to learn.\nA technique for accessing information from anywhere in the input sequence is known as attention. The attention mechanism was introduced to RNNs by Bahdanau et al. [3] for neural machine translation. The text to translate is first encoded by a bidirectional-RNN producing a new sequence of encoded state. Different locations within the encoded state are focused on by multiplying each of\nthem by an attention matrix and calculating the weighted average. This attention is calculated for each translated word. Computing the attention matrix for each encoded state and translated word combination provides a great deal of flexibility in choosing where in the sequence to attend to, but the cost of computing these matrices grows as a square of the number of words to translate. This cost limits this method to short sequences, typically only single sentences are processed at a time.\nThe Recurrent Weighted Average (RWA) unit, recently introduced by Ostmeyer and Cowell [18], can apply attention to sequences of any length. It does this by only computing the attention for each input once and computing the weighted average by maintaining a running average up to the current timestep. Their experiments show that the RWA performs very well on tasks where information is needed from any point in the input sequence. Unfortunately, as it cannot change the attention it assigns to previous timesteps, it performs poorly when asked to carry out multiple tasks within the same sequence, or when asked to predict the next character in a sample of text, a task in which new information is more important than old.\nWe introduce the Recurrent Discounted Attention (RDA) unit, which extends the RWA by allowing it to discount the attention applied to previous timesteps. As this adjustment is applied to all previous timesteps at once, it continues to be efficient. It performs very well both at tasks requiring equal weighting over all information seen and at tasks in which new information is more important than old.\nThe main contributions of this paper are as follows:\n1. We analyse the Recurrent Weighted Average unit and show that it cannot output certain simple sequences.\n2. We propose the Recurrent Discounted Attention unit that extends the Recurrent Weighted Average by allowing it to discount the past.\n3. We run extensive experiments on the RWA, RDA, LSTM and GRU units and show that the RWA, RDA and GRU units are well suited to tasks with a single output, the RDA performs best on the multiple sequence copy task while the LSTM unit performs better on the Hutter Prize Wikipedia dataset.\nOur paper is setout as follows: we present the analysis of the RWA (sections 3 and 4) and propose the RDA (section 5). The experimental results (section 6), discussion (section 7) and conclusion follow (section 8)."}, {"heading": "2 Related Work", "text": "Recently many people have worked on using RNNs to predict the next character in a corpus of text. Sutskever et al. [20] first attempted this on the Hutter Prize Wikipedia datasets using the MRNN archtecture. Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.\nMany of the above architectures are very complex, and so the Gated Recurrent Unit (GRU) is a much simpler design that achieves similar performance to the LSTM. Our experiments confirm previous literature [4] that reports it performing very well.\nAttention mechanisms have been used in neural machine translation by Bahdanau et al. [3]. Xu et al. [21] experimented with hard-attention on image where a single location is selected from a multinomial distribution. Xu et al. [21] introduced the global and local attention to refer to attention applied to the whole input and hard attention applied to a local subset of the input.\nAn idea related to attention is the notion of providing additional computation time for difficult inputs. Graves [10] shows that this yields insight into the distribution of information in the input data itself.\nSeveral RNN architectures have attempted to deal with long term dependencies by storing information in an external memory [11, 12]."}, {"heading": "3 Recurrent Weighted Average", "text": "At each timestep the Recurrent Weighted Average model uses its current hidden state ht\u22121 and the input xt to calculate two quantities:\n1. The features zt of the current input:\nut = Wu \u00b7 xt + bu gt = Wg \u00b7 [xt, ht\u22121] + bg\nzt = ut \u2299 tanh gt\nwhere ut is an unbounded vector dependent only on the input xt, and tanh gt is a bounded vector dependent on the input xt and the hidden state ht\u22121. Notation: W are weights, b are biases, (\u00b7) is matrix multiplication, and \u2299 is the elementwise product.\n2. The attention at to pay to the features zt:\nat = e Wa\u00b7[xt,ht\u22121]+ba\nThe hidden state ht is then the average of of the features zt, weighted by the attention at, and squashed through the hyperbolic tangent function:\nht = tanh\n(\n\u2211t i=1 zi \u2299 ai \u2211t\ni=1 ai\n)\nThis is implemented efficiently as a running average:\nnt = nt\u22121 + zt \u2299 at\ndt = dt\u22121 + at\nht = tanh\n(\nnt\ndt\n)\nwhere nt is the numerator and dt is the denominator of the average."}, {"heading": "4 Properties of the Recurrent Weighted Average", "text": "The RWA shows superior experimental results compared to the LSTM on the following tasks:\n1. Classifying whether a sequence is longer than 500 elements.\n2. Memorising short random sequences of symbols and recalling them at any point in the subsequent 1000 timesteps.\n3. Adding two numbers spaced randomly apart on a sequence of length 1000.\n4. Classifying MNIST images pixel by pixel.\nAll of these tasks require combining the full sequence of inputs into a single output. It makes perfect sense that an average over all timesteps would perform well in these tasks.\nOn the other hand, we can imagine tasks where an average over all timesteps would not work effectively:\n1. Copying many input sequences from input to output. It will need to forget sequences once they have been output.\n2. Predicting the next character in a body of text. Typically, the next character depends much more on recent characters than on those from the beginning of the text.\n3. Outputting the parity of an input sequence\nht = \u22121 tc for 0 < c < 1\n.\nAll of these follow from the property that dt is monotonically increasing in t, which can be seen from at > 0 and dt = dt\u22121+at. As dt becomes larger, the magnitude of at must increase to change the value of ht. This means that it becomes harder and harder to change the value of ht to the point where it almost becomes fixed. In the specific case of outputting the sequence ht = \u22121\ntc we can show that at must grow geometrically with time.\nLemma 1 Let the task be to output the sequence ht = \u22121 tc for 0 < c < 1. Let ht be defined by the equations of the Recurrent Weighted Average, and let zt be bounded and fh be a continuous, monotonically increasing surjection from R \u2192 (\u22121, 1). Then, at grows geometrically with increasing t.\nProof. Provided in Appendix A.\nCorollary 2 If at is also bounded then it cannot grow geometrically for all time and so the RWA cannot output the sequence ht = \u22121 tc.\nCorollary 2 suggests that the Recurrent Weighted Average may not actually be Turing Complete.\nOverall, these properties suggest the the RWA is a good choice for tasks with a single result, but not for sequences with multiple results or tasks that require forgetting."}, {"heading": "5 The Recurrent Discounted Attention unit", "text": "The RDA uses its current hidden state ht\u22121 and the input xt to calculate three quantities:\n1. The features zt of the current input are calculated identically to the RWA:\nut = Wu \u00b7 xt + bu\ngt = Wg \u00b7 [xt, ht\u22121] + bg\nzt = ut \u2299 tanh gt\n2. The attention at to pay to the features: zt\nat = fa(Wa \u00b7 [xt, ht\u22121] + ba)\nHere we generalize attention to allow any function fa which is non-negative and monotonically increasing. If we choose fa = exp, then we recover the RWA attention function.\n3. The discount factor \u03b3t to apply to the previous values in the average\n\u03b3t = \u03c3(W\u03b3 \u00b7 [xt, ht\u22121] + b\u03b3)\nwhere \u03c3 is the sigmoid/logistic function defined as \u03c3(x) = 11+e\u2212x .\nWe use these values to calculate a discounted moving average. This discounting mechanism is crucial in remediating the RWA\u2019s inability to forget the past\nnt = nt\u22121 \u2299 \u03b3t + zt \u2299 at\ndt = dt\u22121 \u2299 \u03b3t + at\nht = fh\n(\nnt\ndt\n)\nHere we generalize RWA further by allowing fh to be any function, and we also introduce a final transformation to the hidden state ht to produce the output\not = fo(ht)\n5.1 Choices for the attention function fa, hidden state function fh and output function fo\nThe attention function fa(x) is a non-negative monotonically increasing function of x. There are several possible choices:\n\u2022 fa(x) = e x - This is used in the RWA.\n\u2022 fa(x) = max(0, x) - Using a ReLU allows the complete ignoring of some timesteps with linearly increasing attention for others.\n\u2022 fa(x) = ln(1 + e x) - The softplus function is a smooth approximation to the ReLU.\n\u2022 fa(x) = \u03c3(x) - Using the sigmoid limits the maximum attention an input can be given.\nThe domain of the hidden activation function fh is the average nt dt . This average is bounded by the minimum and maximum values of zt. Possible choices of fh include:\n\u2022 fh( nt dt ) = tanh(nt dt ) - This is used in the RWA. We observed that the range of nt dt mostly\nremained in the linear domain of tanh centred around 0, suggesting that using this was unneccessary.\n\u2022 fh( nt dt ) = nt dt - The identity is our choice for fh in the RDA.\nPossible choices for the output function fo are:\n\u2022 fo(ht) = ht - The RWA uses the identity as its hidden state has already been transformed by tanh.\n\u2022 fo(ht) = tanh(ht) - The output can be squashed between [\u22121, 1] using tanh."}, {"heading": "6 Experiments", "text": "We ran experiments to investigate the following questions:\n1. Which form of the RDA works best? (Section 6.2)\n2. The RWA unit works remarkably well for sequences with a single task. Does the RDA unit retain this strength? (Section 6.3)\n3. We expect the RWA unit to struggle with consecutive independent tasks. Does this happen in practice and does the RDA solve this problem? (Section 6.4)\n4. How does the RDA unit scale up to very long sequences? We test character prediction on the Hutter Prize Wikipedia dataset. (Section 6.5)\n5. How does the RDA unit compare to RWA, LSTM and GRU units? Are some units more suited to certain types of tasks than others? (Section 7)\nWe provide plots of the training process in Appendix B."}, {"heading": "6.1 Implementation details", "text": "For all tasks except the Wikipedia character prediction task, we use 250 recurrent units. Weights are initialized using Xavier initialization [8] and biases are initialized to 0, except for forget gates and discount gates which are initialized to 1 [7]. We use mini-batches of 100 examples and backpropagate over the full sequence length. We train the models using Adam [16] with a learning rate of 0.001. Gradients are clipped between -1 and 1.\nFor the Wikipedia task, we use a character embedding of 64 dimensions, followed by a single layer of 1800 recurrent units, and a final softmax layer for the character prediction. We apply truncated backpropagation every 250 timesteps, and use the last hidden state of the sequence as the initial hidden state of the next sequence to approximate full backpropagation.\nAll of our experiments are implemented in TensorFlow [1]."}, {"heading": "6.2 Empirical evaluation of RDA activation functions", "text": "We ran our experiments with different combinations of fa and fo and found the following:\n\u2022 Using a ReLU for the attention function fa almost always fails to train. Using a Softplus for fa is much more stable than a ReLU. However, it doesn\u2019t perform as well as using sigmoid or exponential attention.\nClassify Length\nModel Steps until accuracy = 1.0.\nGRU 71 LSTM 776 RDA-exp-tanh 164 RDA-sigmoid-id 414\nRWA 133\nTable 2: Classify: steps until accuracy = 1.0.\nMNIST\nModel Test Set Accuracy\nGRU 0.985 LSTM 0.114 RDA-exp-tanh 0.985 RDA-sigmoid-id 0.987\nRWA 0.979\nTable 3: MNIST test set accuracy.\nMNIST permuted\nModel Permuted Test Set Accuracy\nGRU 0.944 LSTM 0.915 RDA-exp-tanh 0.905 RDA-sigmoid-id 0.913\nRWA 0.899\nTable 4: MNIST permuted test set accuracy.\n\u2022 Exponential attention performs well in all tasks, and works best with the tanh output function fo(ht) = tanh(ht). We refer to this as RDA-exp-tanh.\n\u2022 Sigmoid attention performs well in all tasks, and works best with the identity output function fo(ht) = ht. We refer to this as RDA-sigmoid-id.\n\u2022 It is difficult to choose between RDA-exp-tanh and RDA-sigmoid-id. RDA-exp-tanh often trains faster, but RDA-sigmoid-id tends to have better loss. We include results for both of them."}, {"heading": "6.3 Single task sequences", "text": "Here we investigate whether sequences with a single task can be performed as well with the RDA as with the RWA.\nEach of the four tasks detailed below require the RNN to save some or all of the input sequence before outputting a single result many steps later.\n1. Addition - The input consists of two sequences. The first is a sequence of numbers each uniformly sampled from [0, 1], and the second consists of all zeros except for two ones which indicate the two numbers of the first sequence to be added together. (Table 1)\n2. Classify length - A sequence of length between 1 and 1000 is input. The goal is to classify whether the sequence is longer than 500.\nAll RNN architectures could learn their initial hidden state for this task, which improved performance for all of them. (Table 2)\n3. MNIST - The task is supervised classification of MNIST digits. We flatten the 28x28 pixel arrays into a single 784 element sequence and use RNNs to predict the digit class labels. This task challenges networks\u2019 ability to learn long-range dependencies, as crucial pixels are present at the beginning, middle and end of the sequence. We implement two variants of this task:\n(a) Sequential - the pixels are fed in from the top left to the bottom right of the image. (Table 3)\n(b) Permuted - the pixels of the image are randomly permuted before the image is fed in. The same permutation is applied to all images. (Table 4)\n4. Copy - The input sequence starts with randomly sampled symbols. The rest of the input is blanks except for a single recall symbol. The goal is to memorize the starting symbols and\nCopy\nModel Steps until accuracy > 0.999\nGRU 5329 LSTM > 20000\nRDA-exp-tanh 11831 RDA-sigmoid-id 9840\nRWA 5660\nTable 5: Copy: steps until accuracy > 0.999.\nMulticopy\nModel Steps until accuracy > 0.99\nGRU 3984 LSTM 4048 RDA-exp-tanh 1114 RDA-sigmoid-id 1316\nRWA > 10000\nTable 6: Multicopy: steps until accuracy > 0.99.\noutput them when prompted by the recall symbol. All other output symbols must be blank. (Table 5)"}, {"heading": "6.4 Multiple sequence copy task", "text": "Here we investigate whether the different RNN units can cope with doing the same task repeatedly.\nThe tasks consists of multiple copying tasks all within the same sequence. Instead of having the recall symbol randomly placed over the whole sequence it always appears a couple of steps after the sequence being memorized. This gives room for 50 consecutive copying tasks in a length 1000 input sequence. (Table 6)"}, {"heading": "6.5 Wikipedia character prediction task", "text": "The standard test for RNN models is character-level languagemodelling. We evaluate our models on the Hutter PrizeWikipedia dataset enwik8, which contains 100M characters of 205 different symbols including XML markup and special characters. We split the data into the first 90M characters for the training set, the next 5M for validation, and the final 5M for the test set. (Table 7)"}, {"heading": "7 Discussion", "text": "We start our discussion by describing the performance of each individual unit.\nOur analysis of the RWA unit showed that it should only work well on the single task sequences and we confirm this experimentally. It learns the single sequence tasks quickly but is unable to learn the multiple sequence copy task and Wikipedia character prediction task.\nOur experiments show that the RDA unit is a consistent performer on all types of tasks. As expected, it learns single task sequences slower than the RWA but it actually achieves better generalization on the MNIST test sets. We speculate that the cause of this improvement is because the ability to forget effectively allows it to compress the information it has previously processed, or perhaps discounting the past should be considered as changing the attention on the past and the RDA is able to vary its attention on previous inputs based on later inputs. On the multiple sequence copy task the RDA unit was far superior to all other units learning three times as fast as the LSTM and GRU units. On the Wikipedia character prediction task the RDA unit performed respectably, achieving a better compression rate than the GRU but worse than the LSTM.\nThe LSTM unit learns the single task sequences slower than all the other units and often fails to learn at all. This is surprising as it is often used on these tasks as a baseline against which other archtectures are compared. On the multiple sequence copy task it learns slowly compared to the RDA units but solves the task. The Wikipedia character prediction task is where it performs best, learning much faster and achieving better compression than the other units.\nThe GRU unit works very well on single task sequences often learning the fastest and achieving excellent generalization on the MNIST test sets. On the multiple sequence copy task it has equal performance to the LSTM. On the Wikipedia character prediction task it performs worse than the LSTM and RDA units but still achieves a good performance.\nWe now look at how our results show that different neural network architectures are suited for different tasks.\nFor our single output tasks the RWA, RDA and GRU units work best. Thus for similar real work applications such as encoding a molecule into a latent representation, classification of genomic sequences, answering questions or language translation, these units should be considered before LSTM units. However, our results are yet to be verified in these domains.\nFor sequences that contain an unknown number of independent tasks the RDA unit should be used.\nFor the Wikipedia character prediction task the LSTM performs best. Therefore we can\u2019t recommend RWA, RDA or GRU units on this or similar tasks."}, {"heading": "8 Conclusion", "text": "We analysed the RecurrentWeighted Average (RWA) unit and identified its weakness as the inability to forget the past. By adding this ability to forget the past we arrived at the Recurrent Discounted Attention (RDA). We implemented several varieties of the RDA and compared them to the RWA, LSTM and GRU units on several different tasks. We showed that in almost all cases the RDA should be used in preference to the RWA and is a flexible RNN unit that can perform well on all types of tasks.\nWe also determined which types of tasks were more suited to each different RNN unit. For tasks involving a single output the RWA, RDA and GRU units performed best, for the multiple sequence copy task the RDA performed best, while on the Wikipedia character prediction task the LSTM unit performed best. We recommend taking these results into account when choosing a unit for real world applications."}, {"heading": "9 Acknowledgements", "text": "Thanks to Elisabeth Adlington, Neil Maginnis and Nick Thapen for many helpful comments and suggestions. Thanks also to Jared Ostmeyer, the author of the RWA unit, who got in contact to discuss our work and pointed out a bug in the implementation of the RMA-exp-tanh unit."}, {"heading": "A Mathematical Proofs", "text": "Lemma 1 Let the task be to output the sequence ht = \u22121 tc for 0 < c < 1. Let ht be defined by the equations of the Recurrent Weighted Average, and let zt be bounded and fh be a continuous, monotonically increasing surjection from R \u2192 (\u22121, 1). Then, at grows geometrically with increasing t.\nProof\nGiven that the activation fh is a continuous, monotonically increasing surjection from R to (\u22121, 1), we know that there are two values x+ and x\u2212 such that f(x+) = c and f(x\u2212) = \u2212c. Define x+ \u2212 x\u2212 = \u03b4.\nThen for every even integer i, we have ni di = x+ and ni+1 di+1 = x\u2212.\nFrom the definitions of nt and dt we have\nni+1 di+1 = ni + zi+1ai+1 di + ai+1 = x\u2212\nSubstituting ni = dix+ and rearranging yields\nai+1 = di(x\u2212 \u2212 x+)\nzi+1 \u2212 x\u2212 \u2265\ndi|\u03b4|\n|z|max + |x|max\nwhere |z|max = max{|z|} and |x|max = max{|x+|, |x\u2212|}.\nSubstituting this into di+1 = di + ai+1 gives us\ndi+1 \u2265 di\n(\n1 + |\u03b4|\n|z|max + |x|max\n)\nBy a similar argument, we have the same growth for odd integers di+2 \u2265 di+1 ( 1 + |\u03b4||z|max+|x|max ) and we get geometric growth of dt.\nFrom the definition of dt we have\nai = di+1 \u2212 di \u2265 di |\u03b4|\n|z|max + |x|max\nAs dt grows geometrically, then so does at.\nCorollary 2 If at is also bounded then it cannot grow geometrically for all time and so the RWA cannot output the sequence ht = \u22121 tc\nProof Assume the RWA can output the sequence ht = \u22121 tc. As at grows geometrically, it is unbounded, but this is a contradiction.\nB Illustrated empirical results\nWe include figures of the loss function learning curves during training. In the case of MNIST, we report on validation accuracy instead. These experiments provide evidence that the two flavours of the RDA unit consistently perform close to the best across a broad range of tasks. Figures are best viewed in colour.\n0 500 1,000 1,500 2,000 2,500\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nSteps\nL o ss\nGRU\nLSTM\nRDA-exp-tanh\nRDA-sigmoid-id\nRWA\n(a) Addition task.\n0 200 400 600 800 1,000\n0\n0.2\n0.4\n0.6\n0.8\nSteps\nL o ss\n(b) Classification task.\n0 0.5 1 1.5 2\n\u00b7104\n10\u22121\n100\n101\n102\n103\n104\nSteps\nL o ss\n(l o g sc al e)\n(c) Single copy task.\n0 1,000 2,000 3,000 4,000 5,000\n10\u22121\n100\n101\n102\n103\n104\n105\nSteps\nL o ss\n(l o g sc al e)\n(d) Multiple copy task.\n0 1 2 3 4\n\u00b7104\n0\n0.2\n0.4\n0.6\n0.8\n1\nSteps\nV al id at io n ac cu ra cy\n(e) MNIST, pixel per pixel classification.\n0 0.5 1 1.5 2 2.5\n\u00b7104\n0.4\n0.6\n0.8\n1\nSteps\nV al id at io n ac cu ra cy\n(f) MNIST permuted classification."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Gregory S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian J. Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal J\u00f3zefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Gordon Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul A. Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "CoRR, abs/1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "CoRR, abs/1609.01704,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1603.08983,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska-Barwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "CoRR, abs/1507.01526,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Machine learning on sequential data using a recurrent weighted average", "author": ["Jared Ostmeyer", "Lindsay Cowell"], "venue": "arXiv preprint arXiv:1703.01253,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Recurrent memory array structures", "author": ["Kamil Rocki"], "venue": "CoRR, abs/1607.03085,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "A specific RNN architecture, known as Long Short-Term Memory (LSTM) [14], is the benchmark against which other RNNs are compared.", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "[3] for neural machine translation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "The Recurrent Weighted Average (RWA) unit, recently introduced by Ostmeyer and Cowell [18], can apply attention to sequences of any length.", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "[20] first attempted this on the Hutter Prize Wikipedia datasets using the MRNN archtecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 3, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 12, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 16, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 4, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 30, "endOffset": 55}, {"referenceID": 14, "context": "Since then many architectures [9, 5, 15, 19, 22, 13, 6] and regularization techniques [2, 17] have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.", "startOffset": 86, "endOffset": 93}, {"referenceID": 2, "context": "Our experiments confirm previous literature [4] that reports it performing very well.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[21] experimented with hard-attention on image where a single location is selected from a multinomial distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] introduced the global and local attention to refer to attention applied to the whole input and hard attention applied to a local subset of the input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Graves [10] shows that this yields insight into the distribution of information in the input data itself.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "Several RNN architectures have attempted to deal with long term dependencies by storing information in an external memory [11, 12].", "startOffset": 122, "endOffset": 130}, {"referenceID": 10, "context": "Several RNN architectures have attempted to deal with long term dependencies by storing information in an external memory [11, 12].", "startOffset": 122, "endOffset": 130}, {"referenceID": 6, "context": "Weights are initialized using Xavier initialization [8] and biases are initialized to 0, except for forget gates and discount gates which are initialized to 1 [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Weights are initialized using Xavier initialization [8] and biases are initialized to 0, except for forget gates and discount gates which are initialized to 1 [7].", "startOffset": 159, "endOffset": 162}, {"referenceID": 13, "context": "We train the models using Adam [16] with a learning rate of 0.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "All of our experiments are implemented in TensorFlow [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "The first is a sequence of numbers each uniformly sampled from [0, 1], and the second consists of all zeros except for two ones which indicate the two numbers of the first sequence to be added together.", "startOffset": 63, "endOffset": 69}], "year": 2017, "abstractText": "Recurrent Neural Networks architectures excel at processing sequences by modelling dependencies over different timescales. The recently introduced Recurrent Weighted Average (RWA) unit captures long term dependencies far better than an LSTM on several challenging tasks. The RWA achieves this by applying attention to each input and computing a weighted average over the full history of its computations. Unfortunately, the RWA cannot change the attention it has assigned to previous timesteps, and so struggles with carrying out consecutive tasks or tasks with changing requirements. We present the Recurrent Discounted Attention (RDA) unit that builds on the RWA by additionally allowing the discounting of the past. We empirically compare our model to RWA, LSTM and GRU units on several challenging tasks. On tasks with a single output the RWA, RDA and GRU units learn much quicker than the LSTM and with better performance. On the multiple sequence copy task our RDA unit learns the task three times as quickly as the LSTM or GRU units while the RWA fails to learn at all. On the Wikipedia character prediction task the LSTM performs best but it followed closely by our RDA unit. Overall our RDA unit performs well and is sample efficient on a large variety of sequence tasks.", "creator": "LaTeX with hyperref package"}}}