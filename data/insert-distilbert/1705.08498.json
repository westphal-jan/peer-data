{"id": "1705.08498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Clinical Intervention Prediction and Understanding using Deep Networks", "abstract": "real - time prediction of clinical interventions remains mostly a challenge within intensive care units ( icus ). obtaining this task is complicated by data sources that are noisy, sparse, heterogeneous and outcomes that are imbalanced. in this paper, we integrate data from all available icu sources ( vitals, analytical labs, notes, disease demographics ) and focus adequately on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. thus in particular, we compare both long short - term sequential memory networks ( lstm ) and convolutional neural networks ( cnn ) for prediction of five intervention tasks : full invasive ventilation, non - conventional invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. our predictions also are done in a forward - facing manner to enable \" real - given time \" performance, adjustments and predictions are easily made with a six hour gap time to enable support rapid clinically actionable planning. we never achieve state - of - them the - art systematic results on our predictive tasks using deep temporal architectures. yet we critically explore the analytic use of feature occlusion tools to interpret lstm models, and compare this to confirm the interpretability gained from examining inputs that maximally activate cnn outputs. now we show that our models are able to significantly outperform normal baselines in intervention prediction, and provide insight into model learning, which is crucial for the adoption of such models generally in practice.", "histories": [["v1", "Tue, 23 May 2017 19:42:20 GMT  (760kb,D)", "http://arxiv.org/abs/1705.08498v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harini suresh", "nathan hunt", "alistair johnson", "leo anthony celi", "peter szolovits", "marzyeh ghassemi"], "accepted": false, "id": "1705.08498"}, "pdf": {"name": "1705.08498.pdf", "metadata": {"source": "CRF", "title": "Clinical Intervention Prediction and Understanding using Deep Networks", "authors": ["Harini Suresh", "Nathan Hunt", "Alistair Johnson", "Leo Anthony Celi"], "emails": ["HSURESH@MIT.EDU", "NHUNT@MIT.EDU", "AEWJ@MIT.EDU", "LCELI@MIT.EDU", "PSZ@MIT.EDU", "MGHASSEM@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "As Intensive Care Units (ICUs) play an increasing role in acute healthcare delivery (Vincent, 2013), clinicians must anticipate patient care needs in a fast-paced, data-overloaded setting. The secondary analysis of healthcare data is a critical step toward improving modern healthcare, as it affords the study of care in the real care settings and patient populations. The widespread availability of electronic healthcare data (Charles et al., 2013; Jamoom E and E, 2016) allows new investigations into evidence-based decision support, where we can learn when patients need a given intervention. Continuous, forward-facing event prediction is particularly applicable in the ICU setting where we want to account for evolving clinical needs and information throughout the patient\u2019s stay.\nIn this work, we focus on predicting the onset and weaning of interventions. The efficacy of interventions can vary drastically from patient to patient, and unnecessarily administering an in-\nar X\niv :1\n70 5.\n08 49\n8v 1\n[ cs\n.L G\ntervention can be harmful and expensive. Any treatments come with inherent risks, and we target interventions that span a wide severity of needs in critical care\u2014 specifically, invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Mechanical ventilation is commonly used for breathing assistance, but has many potential complications (Yang and Tobin) and small changes in ventilation settings can have large impact in patient outcomes (Tobin, 2006). Vasopressors are a common ICU medication, but there is no robust evidence of improved outcomes from their use (Mu\u0308llner et al., 2004), and some evidence they may be harmful (DAragon et al., 2015). Fluid boluses are used to improve cardiovascular function and organ perfusion. There are two bolus types: crystalloid and colloid. Both are often considered as less aggressive alternatives to vasopressors, but there are no multi-center trials studying whether fluid bolus therapy should be given to critically ill patients, only studies trying to distinguish which type of fluid should be given (Malbrain et al., 2014).\nCapturing complex relationships across many disparate data types is key for predictive performance in our tasks. To this end, we take advantage of the success of deep learning models to capture rich representations of data with little hand-engineering by domain experts. We use long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), which have been shown to effectively model complicated dependencies in timeseries data (Bengio et al., 1994). Previously, LSTMs have achieved state-of-the-art results in many different applications, such as machine translation (Hermann et al., 2015), dialogue systems (Chorowski et al., 2015) and image captioning (Xu et al., 2015). They are well-suited to our modeling tasks because clinical conditions may be spread apart over several hours. We compare the LSTM models to a convolutional neural network (CNN) architecture that has previously been explored for longitudinal laboratory data (Razavian et al., 2016). All models predict outcomes in a continuous manner given any patient record over vitals, labs, demographic, and notes. In doing so, we:\n1. Achieve state-of-the-art prediction results in our forward-facing, hourly prediction of clinical interventions (onset, weaning, and continuity) that could be used at the time of care. 2. Demonstrate that different data modalities and features are most important for different types of predictive tasks in our LSTM using feature occlusion. This is an important step in making models more interpretable by physicians. 3. Highlight patient trajectories that lead to the most and least confident predictions in our CNN across outcomes and features, also aiding in interpretability."}, {"heading": "2. Background and Related Work", "text": "Clinical decision-making often happens in settings of limited knowledge and high uncertainty; for example, only 10 of the 72 ICU interventions evaluated in randomized controlled trials (RCTs) are not associated with improved outcomes (Ospina-Tasco\u0301n et al., 2008). Our goal is to gain insight from healthcare data previously collected for the primary purpose of facilitating patient care.\nRecent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al., 2015). Razavian et al. (2016) compared CNNs to LSTMs for longitudinal outcome prediction on billing codes using lab tests. With regard to interpretability, Choi et al. (2016) used temporal attention to identify important features in early diagnostic prediction of chronic diseases from time-ordered billing codes. Others have focused on using representations of clinical notes (Ghassemi et al., 2014) or patient physiological signals to predict mortality (Ghassemi et al., 2015).\nPrevious work on interventions in ICU populations have often either focused on a single outcome or used data from specialized cohorts. Such models with vasopressors as a predictive target have achieved AUCs of 0.79 in patients receiving fluid resuscitation (Fialho et al., 2013), 0.85 in septic shock patients (Salgado et al., 2016), and 0.88 for onset after a 4 hour gap and 0.71 for weaning, only trained on patients who did receive a vasopressor (Wu et al., 2016). However, we train our models on general ICU populations in order to make them more applicable. In the most recent prior work on interventions, also on a general ICU population, the best AUC performances were 0.67 (ventilation), 0.78 (vasopressor) for vasopressor onset prediction after a 4 hour gap (Ghassemi et al., 2017). These were lowered to 0.66 and 0.74 with a longer gap time of 8 hours."}, {"heading": "3. Data and Preprocessing", "text": "See Figure 1 for an overall description of data flow."}, {"heading": "3.1 Data Source", "text": "We use data from the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC-III v1.4) database (Johnson et al., 2016). MIMIC is publicly available, and contains over 58,000 hospital admissions from approximately 38,600 adults. We consider patients 15 and older who had ICU stays from 12 to 240 hours and consider each patient\u2019s first ICU stay only. This yields 34,148 unique ICU stays."}, {"heading": "3.2 Data Extraction and Preprocessing", "text": "For each patient, we extract: 1. 5 static variables such as gender and age 2. 29 time-varying vitals and labs such as oxygen saturation and blood urea nitrogen 3. All available, de-identified clinical notes for each patient as timeseries across their entire stay (See Table 3 for a complete listing of variables) Static variables were replicated across all timesteps for each patient. Vital and lab measurements are given timestamps that are rounded to the nearest hour. If an hour has multiple measurements for a signal, those measurements are averaged."}, {"heading": "3.3 Representation of Notes and Vitals", "text": "Clinical narrative notes were processed to create a 50-dimensional vector of topic proportions for each note using Latent Dirichlet Allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). These vectors are replicated forward and aggregated through time (Ghassemi et al., 2014). For example, if a patient had a note A recorded at hour 3 and a note B at hour 7, hours 3\u20136 would contain the topic distribution from A, while hours 7 onward would contain the aggregated topic distribution from A and B combined.\nWe compare raw physiological data to physiological words, where we categorize the vitals data by first converting each value into a z-score based on the population mean and standard deviation for that variable, and then rounding this score to the nearest integer and capping it to be between -4 and 4. Each z-score value then becomes its own column, which explicitly allows for a representation of missingness (e.g., all columns for a particular variable zeroed) that does not require imputation (Figure 7 in Appendix B) (Wu et al., 2016).\nThe physiological variables, topic distribution, and static variables for each patient are concatenated into a single feature vector per patient per hour (Esteban et al., 2016). The intervention state\nof each patient (a binary value indicating whether or not they are on the intervention of interest at each timestep) and the time of day for each timestep (an integer from 0 to 23 representing the hour) are also added to this feature vector. Using the time of day as a feature makes it easier for the model to capture circadian rhythms that may be present in, e.g., the vitals data."}, {"heading": "3.4 Prediction Task", "text": "We split each patients record into 6 hour chunks using a sliding window and make a prediction for a window of 4 hours after a gap time of 6 hours (Figure 2). When predicting ventilation, non-invasive ventilation, or vasopressors, the model classifies the prediction window as one of four possible outcomes: 1) Onset, 2) Wean, 3) Staying on intervention, 4) Staying off intervention. A prediction window is an onset if there is a transition from a label of 0 to 1 for the patient during that window; weaning is the opposite: a transition from 1 to 0. A window is classified as \u201dstay on\u201d if the label for the entire window is 1 or \u201dstay off\u201d if 0. When predicting colloid or crystalloid boluses, we classify the prediction window into one of two classes: 1) Onset, or 2) No Onset, since these interventions are not administered for on-going durations of time. After splitting the patient records into fixedlength chunks, we end up with 1,154,101 examples. Table 1 lists the proportions of each class for each intervention."}, {"heading": "4. Methods", "text": ""}, {"heading": "4.1 Long Short-Term Memory Network (LSTM)", "text": "We use long short-term memory networks (LSTM) as our first model. Having seen the input sequence x1 . . . xt of a given example, we predict y\u0302t, a probability distribution over the outcomes, with target outcome yt:\nh1 . . . ht = LSTM(x1 . . . xt) (1) y\u0302t = softmax(Wyht + by) (2)\nwhere xi \u2208 RV ,Wy \u2208 RNC\u00d7L2 , ht \u2208 RL2 , by \u2208 RNC where V is the dimensionality of the input (number of variables), NC is the number of classes we predict, and L2 is the second hidden layer size. For a model schematic, see Figure 3a, and for more details on model implementation, see the Appendix."}, {"heading": "4.2 Convolution Neural Network (CNN)", "text": "We employ a similar CNN architecture to Razavian et al. (2016), except that we do not initially convolve the features into an intermediate representation. We represent features as channels and perform 1D temporal convolutions, rather than treating the input as a 2D image. Our architecture consists of temporal convolutions at three different temporal granularities with 64 filters each. The dimensions of the filters are 1\u00d7 i, where i \u2208 {3, 4, 5}.\nWe pad the inputs such that the outputs from the convolutional layers are the same size, and we use a stride of 1. Each convolution is followed by a max pooling layer with a pooling size of 3. The outputs from all three temporal granularities are concatenated and flattened, and followed by 2 fully connected layers with dropout in between and a softmax over the output (Figure 3b)."}, {"heading": "4.3 Experimental Settings", "text": "We use a train/validation/test split of 70/10/20 and stratify the splits based on outcome. For the LSTM, we use dropout with a keep probability of 0.8 during training (only on stacked layers), and L2 regularization with lambda = 0.0001. We use 2 hidden LSTM layers of 512 nodes each. For the CNN, we use dropout between fully-connected layers with a keep probability of 0.5. We use a weighted loss function during optimization to account for class imbalances. All parameters were determined using cross-validation with the validation set. We implemented all models in TensorFlow version 1.0.1 using the Adam optimizer on mini-batches of 128 examples. We determine when to stop training with early stopping based on the AUC on the validation set."}, {"heading": "4.4 Evaluation", "text": "We evaluate our results based on per-class AUCs as well as aggregated macro AUCs. If there are K classes each with a per-class AUC of AUCk then the macro AUC is defined as the average of the per-class AUCS, AUCmacro = 1K \u2211 k AUCk. We use the macro AUC as an aggregate score because it weights the AUCs of all classes equally, regardless of class size (Manning et al., 2008). This is important because of the large class imbalance present in the data.\nWe use L2 regularized logistic regression (LR) as a baseline for comparison with the neural networks (Pedregosa et al., 2011). The same input is used as for the numerical LSTM and CNN (imputed 6 hour chunks of data)."}, {"heading": "4.5 Interpretibility", "text": ""}, {"heading": "4.5.1 LSTM FEATURE-LEVEL OCCLUSIONS", "text": "Because of the additional time dependencies of recurrent neural networks, getting feature-level interpretability from LSTMs is notoriously difficult. To achieve this, we borrow an idea from image recognition to help understand how the LSTM uses different features of the patients. Zeiler and Fergus (2013) use occlusion to understand how models process images: they remove a region of the image (by setting all values in that region to 0) and compare the model\u2019s prediction of this occluded image with the original prediction. A large shift in the prediction implies that the occluded region contains important information for the correct prediction. With our LSTM model, we remove features one by one from the patients (by replacing the given feature with noise drawn from a uniform distribution in [0,1)). We then compare the predictive ability of the model with and without each feature; when this difference is large, then the model was relying heavily on that feature to make the prediction."}, {"heading": "4.5.2 CNN FILTER/ACTIVATION VISUALIZATION", "text": "We get interpretability from the CNN models in two ways. First, in order to understand how the CNN is using the patient data to predict certain tasks, we find and compare the top 10 real examples that our model predicts are most and least likely to have a specific outcome. As our gap time is 6 hours, this means that the model predicts high probability of onset of the given task 6 hours after the end of the identified trajectories.\nSecond, we generate \u201challucinations\u201d from the model which maximize the predicted probability for a given task (Erhan et al., 2009). This is done by creating an objective function that maximizes the activation of a specific output node, and backpropagating gradients back to the input image, adjusting the image so that it maximally activates the output node."}, {"heading": "5. Results", "text": "We found deep architectures achieved state-of-the-art prediction results for our intervention tasks. The AUCs for each of our five intervention types and 4 prediction tasks are shown for all models in Table 2. All models use 6 hour chucks of \u201craw\u201d data which have either been transformed to a 0-1 range (normalized and mean imputed), or discretized into physiological words (Section 3.3)."}, {"heading": "5.1 Physiological Words Improve Predictive Task Performance With High Class Imbalance", "text": "We observed a significantly increased AUC for some interventions when we used physiological words \u2014 specifically for ventilation onset (from 0.61 to 0.75) and colloid bolus onset (from 0.52 to 0.72), which have the lowest proportion of onset examples (Table 1). This may be because physiological words have a smoothing effect. Since we round the z-score for each value to the nearest integer, if a patient has a heart rate of 87 at one hour and then 89 at the next, those will probably be represented as the same word. This may make the model invariant to small fluctuations in the patient\u2019s data and more resilient to overfitting small classes. In addition, the physiological word representation has an explicit encoding for missing data. This is in contrast to the raw data that has been forward-filled and mean-imputed, introducing noise and making it difficult for the model to know how confident to be in the measurements it is given (Che et al., 2016)."}, {"heading": "5.2 Feature-Level Occlusions Identify Important Per-Class Features", "text": "We are able to interpret the LSTM\u2019s predictions using feature occlusion (Section 4.5.1). We note that vitals, labs, topics and static data are important for different interventions (Figure 4). Table 5 has a complete listing of the most probable words for each topic mentioned.\nFor mechanical ventilation, the top five important features are consistent for weaning and onset (pH, sodium, lactate, hemoglobin, and potassium). This is sensible, because all are important lab values used to assess a patient\u2019s physiological stability, and ventilation is an aggressive intervention. However, ventilation onset additionally places importance on a patient\u2019s Glasgow Coma Score (GCS) and Topic 4 (assessing patient consciousness), likely because patient sedation is a critical part of mechanical ventilation. We also note that the scale of AUC difference between ventilation onset and weaning is the largest observed (up to 0.30 for weaning and 0.12 for onset).\nIn vasopressor onset prediction, physiological variables such as potassium and hematocrit are consistently important, which agrees with clinical assessment of cardiovascular state (Bassi et al., 2013). Similarly, Topic 3 (noting many physiological values) is also important for both onset and weaning. Note that the overall difference in AUC for onset ranges up to 0.16, but there is no significant decrease in AUC for weaning (< 0.02). This is consistent with previous work that demonstrated weaning to be a more difficult task in general for vasopressors (Wu et al., 2016). We also note that weaning prediction places importance on time of day. As noted by Wu et al. (2016), this could be a side-effect of patients being left on interventions longer than necessary.\nFor non-invasive ventilation onset and weaning the learned topics are more important than physiological variables. This may mean that the need for less severe interventions can only be detected\nfrom clinical insights derived in notes. Similarly to vasopressors, we note that onset AUCs vary more than weaning AUCs (0.14 vs 0.01), and that time of day is important for weaning.\nFor crystalloid and colloid bolus onsets, topics are all but one of the five most important features for detection. Colloid boluses in general have more AUC variance for the topic features (0.14 vs. 0.05), which is likely due to the larger class imbalance compared to crystalloids."}, {"heading": "5.3 Convolutional Filters Target Short-term Trajectories", "text": "We are able to understand the CNN by examining maximally activating patient trajectories (Section 4.5.2). Figure 5 shows the mean with standard deviation error bars for four of the most differentiated features of the 10 real patient trajectories that are the highest and lowest activating for each task. The trends suggest that patients who will require ventilation in the future have higher diastolic blood pressure, respiratory rate, and heart rate, and lower oxygen saturation \u2014 possibly corresponding to patients who are experiencing hyperventilation. For vasopressor onsets, we see a decreased systolic blood pressure, heart rate and oxygen saturation rate. These could either indicate altered peripheral perfusion or stress hyperglycemia. Topic 3, which was important for vasopressor onset using occlusion 4, is also increased.\nIn the less invasive tasks, we saw decreased creatinine, phosphate, oxygen saturation and blood urea nitrogen for non-invasive ventilation, potentially indicating neuromuscular respiratory failure.\nFor colloid and crystalloid boluses we note general indicators of physiological decline, as boluses are given for a wide range of conditions.\n\u201cHallucinations\u201d for vasopressor and ventilation onset are shown in Figure 6. While our model was not trained with any physiological knowledge or priors, we note that it identifies blood pressure drops as being maximally activating for vasopressor onset, and respiratory rate decreasing for ventilation onset. This suggests that it is still able to independently learn physiological factors that are important for intervention prediction. We note that these hallucinations give us more insight into underlying properties of the network and what it is looking for. However, since these trajectories are made to maximize the output of the model, they do not necessarily correspond to physiologically plausible trajectories."}, {"heading": "6. Conclusion", "text": "In this work, we targeted forward-facing prediction of ICU interventions covering multiple physiological organ systems. To our knowledge, our model is the first to use deep neural networks to predict both onset and weaning of interventions using all available modalities of ICU data. In our tasks, deep learning methods beat state-of-the-art AUCs reported in prior work for intervention prediction tasks \u2014 this is sensible given that prior works have focused on single targets with smaller datasets (Wu et al., 2016) or unsupervised representations prior to supervised training (Ghassemi et al., 2017). We also note that the LSTM over physiological words significantly improved perfor-\nmance in the two intervention tasks with the lowest incidence rate \u2014 possibly because this representation encodes important information about what is \u201cnormal\u201d for each physiological value, or is more robust to missingness in the physiological data.\nImportantly, we were able to gain interpretability in both models. In the LSTMs, we examined feature importance using occlusion, and found that physiological data were important in more invasive tasks, while clinical note topics were more important for less invasive interventions. This could indicate that there is more clinical discretion at play for less invasive tasks. We also found that all weaning tasks save ventilation had less AUC variance, which could indicate that these decisions are also made with a large amount of clinical judgment.\nThe temporal convolutions in our CNN filters over the multi-channel input learnt interesting and clinically-relevant trends in real patient trajectories, and these were further mimicked in the hallucinations generated by the network. As in prior work, we found that RNNs often have similar or improved performance as compared to CNNs Razavian et al. (2016). However, it is possible that more complex models would perform better as they uncover more long and short-term dependencies.\nOur results are an interesting start to extracting interpretability from neural networks on patient data, and future work to expand this will enable these models to be adopted in real clinical settings.\nAcknowledgments\nThis research was funded in part by the Intel Science and Technology Center for Big Data and the National Library of Medicine Biomedical Informatics Research Training grant (NIH/NLM 2T15 LM007092-22)."}, {"heading": "B. Physiological Word Generation", "text": "See Figure 7."}, {"heading": "C. LSTM Model Details", "text": "LSTM performs the following update equations for a single layer, given its previous hidden state and the new input:\nft = \u03c3(Wf [ht\u22121, xt] + bf ) (3) it = \u03c3(Wi[ht\u22121, xt] + bi) (4)\nc\u0303t = tanh(Wc[ht\u22121, xt] + bc) (5) ct = ft ct\u22121 + it c\u0303t (6)\not = \u03c3(Wo[ht\u22121, xt] + bo) (7) ht = ot tanh(ct) (8)\nwhere Wf ,Wi,Wc,Wo \u2208 RL1\u00d7(L1+V ), bf , bi, bc, bo \u2208 RL1 are learned parameters, and ft, it, c\u0303t, ct, ot, ht \u2208 RL1 . In these equations, \u03c3 stands for an element-wise application of the sigmoid (logistic) function, and is an element-wise product. This is generalized to multiple layers by providing ht from the previous layer in place of the input.\nWe calculate classification loss using categorical cross-entropy, which sets the loss for predictions for N examples over M classes as:\nL(y\u03021 . . . y\u0302N ) = \u2212 1\nN N\u2211 i=1 M\u2211 j=1 yij log y\u0302ij\nwhere y\u0302ij is the probability our model predicts for example i being in class j, and yij is the true value.\nD. Generated Topics"}], "references": [{"title": "Therapeutic strategies for highdose vasopressor-dependent shock", "author": ["Estev\u00e3o Bassi", "Marcelo Park", "Luciano Cesar Pontes Azevedo"], "venue": "Critical care research and practice,", "citeRegEx": "Bassi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bassi et al\\.", "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Adoption of electronic health record systems among us non-federal acute care hospitals: 2008-2012", "author": ["Dustin Charles", "Meghan Gabriel", "Michael F Furukawa"], "venue": "ONC data brief,", "citeRegEx": "Charles et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Charles et al\\.", "year": 2013}, {"title": "Recurrent neural networks for multivariate time series with missing values", "author": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "venue": "arXiv preprint arXiv:1606.01865,", "citeRegEx": "Che et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Che et al\\.", "year": 2016}, {"title": "Doctor AI: predicting clinical events via recurrent neural networks", "author": ["Edward Choi", "Mohammad Taha Bahadori", "Jimeng Sun"], "venue": "CoRR, abs/1511.05942,", "citeRegEx": "Choi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2015}, {"title": "Retain: An interpretable predictive model for healthcare using reverse time attention mechanism", "author": ["Edward Choi", "Mohammad Taha Bahadori", "Jimeng Sun", "Joshua Kulas", "Andy Schuetz", "Walter Stewart"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Choi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Blood pressure targets for vasopressor therapy: A systematic review", "author": ["Frederick DAragon", "Emilie P Belley-Cote", "Maureen O Meade", "Fran\u00e7ois Lauzier", "Neill KJ Adhikari", "Matthias Briel", "Manoj Lalu", "Salmaan Kanji", "Pierre Asfar", "Alexis F Turgeon"], "venue": "Shock, 43(6):530\u2013539,", "citeRegEx": "DAragon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "DAragon et al\\.", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Technical report, University of Montreal,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Predicting clinical events by combining static and dynamic information using recurrent neural networks", "author": ["Crist\u00f3bal Esteban", "Oliver Staeck", "Stephan Baier", "Yinchong Yang", "Volker Tresp"], "venue": "In Healthcare Informatics (ICHI),", "citeRegEx": "Esteban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Esteban et al\\.", "year": 2016}, {"title": "Diseasebased modeling to predict fluid response in intensive care units", "author": ["AS Fialho", "LA Celi", "F Cismondi", "SM Vieira", "SR Reti", "JM Sousa", "SN Finkelstein"], "venue": "Methods Inf Med,", "citeRegEx": "Fialho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fialho et al\\.", "year": 2013}, {"title": "A multivariate timeseries modeling approach to severity of illness assessment and forecasting in icu with sparse, heterogeneous clinical data", "author": ["Marzyeh Ghassemi", "Marco AF Pimentel", "Tristan Naumann", "Thomas Brennan", "David A Clifton", "Peter Szolovits", "Mengling Feng"], "venue": "In Proc. Twenty-Ninth AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Ghassemi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghassemi et al\\.", "year": 2015}, {"title": "Predicting intervention onset in the icu with switching state space models", "author": ["Marzyeh Ghassemi", "Mike Wu", "Michael Hughes", "Finale Doshi-Velez"], "venue": "In Proceedings of the AMIA Summit on Clinical Research Informatics (CRI),", "citeRegEx": "Ghassemi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ghassemi et al\\.", "year": 2017}, {"title": "Finding scientific topics", "author": ["T. Griffiths", "M. Steyvers"], "venue": "In PNAS,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Office-based physician electronic health record adoption", "author": ["E Yang N Jamoom", "E. Hing"], "venue": "Office of the National Coordinator for Health Information Technology,", "citeRegEx": "Jamoom and Hing,? \\Q2016\\E", "shortCiteRegEx": "Jamoom and Hing", "year": 2016}, {"title": "MIMIC-III, a freely accessible critical care database", "author": ["Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "Li-wei H Lehman", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark"], "venue": "Scientific data,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Deep kalman filters", "author": ["Rahul G Krishnan", "Uri Shalit", "David Sontag"], "venue": "arXiv preprint arXiv:1511.05121,", "citeRegEx": "Krishnan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2015}, {"title": "Learning to diagnose with lstm recurrent neural networks", "author": ["Zachary C Lipton", "David C Kale", "Charles Elkan", "Randall Wetzell"], "venue": "arXiv preprint arXiv:1511.03677,", "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Fluid overload, de-resuscitation, and outcomes in critically ill or injured patients: a systematic review with suggestions for clinical practice", "author": ["ML Malbrain", "Paul E Marik", "Ine Witters", "Colin Cordemans", "Andrew W Kirkpatrick", "Derek J Roberts", "Niels Van Regenmortel"], "venue": "Anaesthesiol Intensive Ther,", "citeRegEx": "Malbrain et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malbrain et al\\.", "year": 2014}, {"title": "Introduction to Information Retrieval", "author": ["Christopher Manning", "Prabhakar Raghavan", "Hinrich Schtze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Vasopressors for shock", "author": ["Marcus M\u00fcllner", "Bernhard Urbanek", "Christof Havel", "Heidrun Losert", "Gunnar Gamper", "Harald Herkner"], "venue": "The Cochrane Library,", "citeRegEx": "M\u00fcllner et al\\.,? \\Q2004\\E", "shortCiteRegEx": "M\u00fcllner et al\\.", "year": 2004}, {"title": "Multicenter, randomized, controlled trials evaluating mortality in intensive care: Doomed to fail", "author": ["Gustavo A Ospina-Tasc\u00f3n", "Gustavo Luiz B\u00fcchele", "Jean-Louis Vincent"], "venue": "Critical care medicine,", "citeRegEx": "Ospina.Tasc\u00f3n et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ospina.Tasc\u00f3n et al\\.", "year": 2008}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Multi-task prediction of disease onsets from longitudinal lab tests", "author": ["Narges Razavian", "Jake Marcus", "David Sontag"], "venue": "In JMLR (Journal of Machine Learning Research): MLHC Conference Proceedings,", "citeRegEx": "Razavian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2016}, {"title": "Ensemble fuzzy models in personalized medicine: Application to vasopressors administration", "author": ["C\u00e1tia M Salgado", "Susana M Vieira", "Lu\u0131\u0301s F Mendon\u00e7a", "Stan Finkelstein", "Jo\u00e3o MC Sousa"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "Salgado et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salgado et al\\.", "year": 2016}, {"title": "Principles and practice of mechanical ventilation", "author": ["Martin J Tobin"], "venue": null, "citeRegEx": "Tobin.,? \\Q2006\\E", "shortCiteRegEx": "Tobin.", "year": 2006}, {"title": "Critical care-where have we been and where are we going", "author": ["Jean-Louis Vincent"], "venue": "Critical Care, 17 (Suppl 1):S2,", "citeRegEx": "Vincent.,? \\Q2013\\E", "shortCiteRegEx": "Vincent.", "year": 2013}, {"title": "Understanding vasopressor intervention and weaning: Risk prediction in a public heterogeneous clinical time series database", "author": ["Mike Wu", "Marzyeh Ghassemi", "Mengling Feng", "Leo A Celi", "Peter Szolovits", "Finale Doshi-Velez"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "CoRR, abs/1311.2901,", "citeRegEx": "Zeiler and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "As Intensive Care Units (ICUs) play an increasing role in acute healthcare delivery (Vincent, 2013), clinicians must anticipate patient care needs in a fast-paced, data-overloaded setting.", "startOffset": 84, "endOffset": 99}, {"referenceID": 2, "context": "The widespread availability of electronic healthcare data (Charles et al., 2013; Jamoom E and E, 2016) allows new investigations into evidence-based decision support, where we can learn when patients need a given intervention.", "startOffset": 58, "endOffset": 102}, {"referenceID": 27, "context": "Mechanical ventilation is commonly used for breathing assistance, but has many potential complications (Yang and Tobin) and small changes in ventilation settings can have large impact in patient outcomes (Tobin, 2006).", "startOffset": 204, "endOffset": 217}, {"referenceID": 22, "context": "Vasopressors are a common ICU medication, but there is no robust evidence of improved outcomes from their use (M\u00fcllner et al., 2004), and some evidence they may be harmful (DAragon et al.", "startOffset": 110, "endOffset": 132}, {"referenceID": 7, "context": ", 2004), and some evidence they may be harmful (DAragon et al., 2015).", "startOffset": 47, "endOffset": 69}, {"referenceID": 20, "context": "Both are often considered as less aggressive alternatives to vasopressors, but there are no multi-center trials studying whether fluid bolus therapy should be given to critically ill patients, only studies trying to distinguish which type of fluid should be given (Malbrain et al., 2014).", "startOffset": 264, "endOffset": 287}, {"referenceID": 15, "context": "We use long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), which have been shown to effectively model complicated dependencies in timeseries data (Bengio et al.", "startOffset": 46, "endOffset": 80}, {"referenceID": 1, "context": "We use long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), which have been shown to effectively model complicated dependencies in timeseries data (Bengio et al., 1994).", "startOffset": 169, "endOffset": 190}, {"referenceID": 14, "context": "Previously, LSTMs have achieved state-of-the-art results in many different applications, such as machine translation (Hermann et al., 2015), dialogue systems (Chorowski et al.", "startOffset": 117, "endOffset": 139}, {"referenceID": 6, "context": ", 2015), dialogue systems (Chorowski et al., 2015) and image captioning (Xu et al.", "startOffset": 26, "endOffset": 50}, {"referenceID": 30, "context": ", 2015) and image captioning (Xu et al., 2015).", "startOffset": 29, "endOffset": 46}, {"referenceID": 25, "context": "We compare the LSTM models to a convolutional neural network (CNN) architecture that has previously been explored for longitudinal laboratory data (Razavian et al., 2016).", "startOffset": 147, "endOffset": 170}, {"referenceID": 23, "context": "Clinical decision-making often happens in settings of limited knowledge and high uncertainty; for example, only 10 of the 72 ICU interventions evaluated in randomized controlled trials (RCTs) are not associated with improved outcomes (Ospina-Tasc\u00f3n et al., 2008).", "startOffset": 234, "endOffset": 262}, {"referenceID": 3, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al.", "startOffset": 137, "endOffset": 195}, {"referenceID": 19, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al.", "startOffset": 137, "endOffset": 195}, {"referenceID": 4, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al.", "startOffset": 137, "endOffset": 195}, {"referenceID": 18, "context": ", 2015), to identify the impact of different drugs for diabetes (Krishnan et al., 2015).", "startOffset": 64, "endOffset": 87}, {"referenceID": 11, "context": ", 2014) or patient physiological signals to predict mortality (Ghassemi et al., 2015).", "startOffset": 62, "endOffset": 85}, {"referenceID": 3, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al., 2015). Razavian et al. (2016) compared CNNs to LSTMs for longitudinal outcome prediction on billing codes using lab tests.", "startOffset": 138, "endOffset": 300}, {"referenceID": 3, "context": "Recent studies have applied recurrent neural networks (RNNs) to modeling sequential EHR data to tag ICU signals with billing code labels (Che et al., 2016; Lipton et al., 2015; Choi et al., 2015), to identify the impact of different drugs for diabetes (Krishnan et al., 2015). Razavian et al. (2016) compared CNNs to LSTMs for longitudinal outcome prediction on billing codes using lab tests. With regard to interpretability, Choi et al. (2016) used temporal attention to identify important features in early diagnostic prediction of chronic diseases from time-ordered billing codes.", "startOffset": 138, "endOffset": 445}, {"referenceID": 10, "context": "79 in patients receiving fluid resuscitation (Fialho et al., 2013), 0.", "startOffset": 45, "endOffset": 66}, {"referenceID": 26, "context": "85 in septic shock patients (Salgado et al., 2016), and 0.", "startOffset": 28, "endOffset": 50}, {"referenceID": 29, "context": "71 for weaning, only trained on patients who did receive a vasopressor (Wu et al., 2016).", "startOffset": 71, "endOffset": 88}, {"referenceID": 12, "context": "78 (vasopressor) for vasopressor onset prediction after a 4 hour gap (Ghassemi et al., 2017).", "startOffset": 69, "endOffset": 92}, {"referenceID": 17, "context": "4) database (Johnson et al., 2016).", "startOffset": 12, "endOffset": 34}, {"referenceID": 13, "context": "3 Representation of Notes and Vitals Clinical narrative notes were processed to create a 50-dimensional vector of topic proportions for each note using Latent Dirichlet Allocation (Blei et al., 2003; Griffiths and Steyvers, 2004).", "startOffset": 180, "endOffset": 229}, {"referenceID": 29, "context": ", all columns for a particular variable zeroed) that does not require imputation (Figure 7 in Appendix B) (Wu et al., 2016).", "startOffset": 106, "endOffset": 123}, {"referenceID": 9, "context": "The physiological variables, topic distribution, and static variables for each patient are concatenated into a single feature vector per patient per hour (Esteban et al., 2016).", "startOffset": 154, "endOffset": 176}, {"referenceID": 25, "context": "2 Convolution Neural Network (CNN) We employ a similar CNN architecture to Razavian et al. (2016), except that we do not initially convolve the features into an intermediate representation.", "startOffset": 75, "endOffset": 98}, {"referenceID": 21, "context": "We use the macro AUC as an aggregate score because it weights the AUCs of all classes equally, regardless of class size (Manning et al., 2008).", "startOffset": 120, "endOffset": 142}, {"referenceID": 24, "context": "We use L2 regularized logistic regression (LR) as a baseline for comparison with the neural networks (Pedregosa et al., 2011).", "startOffset": 101, "endOffset": 125}, {"referenceID": 31, "context": "Zeiler and Fergus (2013) use occlusion to understand how models process images: they remove a region of the image (by setting all values in that region to 0) and compare the model\u2019s prediction of this occluded image with the original prediction.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Second, we generate \u201challucinations\u201d from the model which maximize the predicted probability for a given task (Erhan et al., 2009).", "startOffset": 110, "endOffset": 130}, {"referenceID": 3, "context": "This is in contrast to the raw data that has been forward-filled and mean-imputed, introducing noise and making it difficult for the model to know how confident to be in the measurements it is given (Che et al., 2016).", "startOffset": 199, "endOffset": 217}, {"referenceID": 0, "context": "In vasopressor onset prediction, physiological variables such as potassium and hematocrit are consistently important, which agrees with clinical assessment of cardiovascular state (Bassi et al., 2013).", "startOffset": 180, "endOffset": 200}, {"referenceID": 29, "context": "This is consistent with previous work that demonstrated weaning to be a more difficult task in general for vasopressors (Wu et al., 2016).", "startOffset": 120, "endOffset": 137}, {"referenceID": 0, "context": "In vasopressor onset prediction, physiological variables such as potassium and hematocrit are consistently important, which agrees with clinical assessment of cardiovascular state (Bassi et al., 2013). Similarly, Topic 3 (noting many physiological values) is also important for both onset and weaning. Note that the overall difference in AUC for onset ranges up to 0.16, but there is no significant decrease in AUC for weaning (< 0.02). This is consistent with previous work that demonstrated weaning to be a more difficult task in general for vasopressors (Wu et al., 2016). We also note that weaning prediction places importance on time of day. As noted by Wu et al. (2016), this could be a side-effect of patients being left on interventions longer than necessary.", "startOffset": 181, "endOffset": 676}, {"referenceID": 29, "context": "In our tasks, deep learning methods beat state-of-the-art AUCs reported in prior work for intervention prediction tasks \u2014 this is sensible given that prior works have focused on single targets with smaller datasets (Wu et al., 2016) or unsupervised representations prior to supervised training (Ghassemi et al.", "startOffset": 215, "endOffset": 232}, {"referenceID": 12, "context": ", 2016) or unsupervised representations prior to supervised training (Ghassemi et al., 2017).", "startOffset": 69, "endOffset": 92}, {"referenceID": 25, "context": "As in prior work, we found that RNNs often have similar or improved performance as compared to CNNs Razavian et al. (2016). However, it is possible that more complex models would perform better as they uncover more long and short-term dependencies.", "startOffset": 100, "endOffset": 123}], "year": 2017, "abstractText": "Real-time prediction of clinical interventions remains a challenge within intensive care units (ICUs). This task is complicated by data sources that are noisy, sparse, heterogeneous and outcomes that are imbalanced. In this paper, we integrate data from all available ICU sources (vitals, labs, notes, demographics) and focus on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. In particular, we compare both long short-term memory networks (LSTM) and convolutional neural networks (CNN) for prediction of five intervention tasks: invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Our predictions are done in a forward-facing manner to enable \u201creal-time\u201d performance, and predictions are made with a six hour gap time to support clinically actionable planning. We achieve state-of-the-art results on our predictive tasks using deep architectures. We explore the use of feature occlusion to interpret LSTM models, and compare this to the interpretability gained from examining inputs that maximally activate CNN outputs. We show that our models are able to significantly outperform baselines in intervention prediction, and provide insight into model learning, which is crucial for the adoption of such models in practice.", "creator": "LaTeX with hyperref package"}}}