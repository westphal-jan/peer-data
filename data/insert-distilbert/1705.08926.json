{"id": "1705.08926", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Counterfactual Multi-Agent Policy Gradients", "abstract": "cooperative multi - response agent systems can be just naturally used to model many real world problems, respectively such as network packet routing and the coordination performance of autonomous vehicles. there is a great need not for new reinforcement learning methods that can eventually efficiently learn decentralised policies for such systems. to this end, we propose a new multi - function agent actor - critic method called strategic counterfactual multi - interaction agent ( coma ) policy integration gradients. coma uses a centralised critic to estimate the q - function and decentralised actors to optimise the local agents'policies. in addition, to address the potential challenges required of multi - agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's avoidance action, while keeping the variables other agents'actions fixed. coma also uses a critic representation that allows the counterfactual baseline model to be computed efficiently in generating a single forward pass. we evaluate kernel coma in the testbed of another starcraft unit micromanagement, using a generalized decentralised variant with significant partial observability. coma significantly improves average performance outcome over other multi - agent actor - critic methods in this setting, and the best performing agents are competitive citizens with emerging state - of - the - art centralised controllers configurations that get access to the full state.", "histories": [["v1", "Wed, 24 May 2017 18:52:17 GMT  (301kb,D)", "http://arxiv.org/abs/1705.08926v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.MA", "authors": ["jakob foerster", "gregory farquhar", "triantafyllos afouras", "nantas nardelli", "shimon whiteson"], "accepted": false, "id": "1705.08926"}, "pdf": {"name": "1705.08926.pdf", "metadata": {"source": "CRF", "title": "Counterfactual Multi-Agent Policy Gradients", "authors": ["Jakob N. Foerster", "Gregory Farquhar", "Triantafyllos Afouras", "Nantas Nardelli", "Shimon Whiteson"], "emails": ["jakob.foerster@cs.ox.ac.uk", "gregory.farquhar@cs.ox.ac.uk", "afourast@robots.ox.ac.uk", "nantas@robots.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Many complex reinforcement learning (RL) problems such as the coordination of autonomous vehicles (Cao et al., 2013), network packet delivery (Ye et al., 2015), and distributed logistics (Ying & Dayong, 2005) are naturally modelled as cooperative multi-agent systems. However, RL methods designed for single agents typically fare poorly on such tasks, since the joint action space of the agents grows exponentially with the number of agents.\nTo cope with such complexity, it is often necessary to resort to decentralised policies, in which each agent selects its own action conditioned only on its local action-observation history. Furthermore, partial observability and communication constraints during execution may necessitate the use of decentralised policies even when the joint action space is not prohibitively large.\nHence, there is a great need for new RL methods that can efficiently learn decentralised policies. In some settings, the learning itself may also need to be decentralised. However, in many cases, learning can take place in a simulator or a laboratory in which extra state information is available and agents can communicate freely. This centralised training of decentralised policies is a standard paradigm for multi-agent planning (Oliehoek et al., 2008; Kraemer & Banerjee, 2016) and has recently been picked up by the deep RL community (Foerster et al., 2016; Jorge et al., 2016). However, the question of how best to exploit the opportunity for centralised learning remains open.\nAnother crucial challenge is multi-agent credit assignment (Chang et al., 2003): in cooperative settings, joint actions typically generate only global rewards, making it difficult for each agent to\n\u2020These authors contributed equally to this work.\nar X\niv :1\n70 5.\n08 92\n6v 1\n[ cs\n.A I]\n2 4\nM ay\ndeduce its own contribution to the team\u2019s success. Sometimes it is possible to design individual reward functions for each agent. However, these rewards are not generally available in cooperative settings and often fail to encourage individual agents to sacrifice for the greater good.\nIn this paper, we propose a new multi-agent RL method called counterfactual multi-agent (COMA) policy gradients, in order to address these issues. COMA takes an actor-critic (Konda & Tsitsiklis, 1999) approach, in which the actor, i.e., the policy, is trained by following a gradient estimated by a critic. COMA is based on three main ideas.\nFirst, COMA uses a centralised critic. The critic is only used during learning, while only the actor is needed during execution. Since learning is centralised, we can therefore use a centralised critic that conditions on the joint action and all available state information, while each agent\u2019s policy conditions only on its own action-observation history.\nSecond, COMA uses a counterfactual baseline. The idea is inspired by difference rewards (Tumer & Agogino, 2007), in which each agent learns from a shaped reward that compares the global reward to the reward received when that agent\u2019s action is replaced with a default action. While difference rewards are a powerful way to perform multi-agent credit assignment, they typically require access to a simulator and, in many applications, it is unclear how to choose the default action. COMA addresses this by using the centralised critic to compute an advantage function that compares the value for the current action to a counterfactual baseline that marginalises out a single agent\u2019s action, while keeping the other agents\u2019 actions fixed. Hence, instead of relying on extra simulations, COMA computes a separate baseline for each agent that relies on the centralised critic to reason about counterfactuals in which only that agent\u2019s action changes.\nThird, COMA uses a critic representation that allows the counterfactual baseline to be computed efficiently. In a single forward pass, it computes the Q-values for all the different actions of a given agent, conditioned on the actions of all the other agents.\nWe evaluate COMA in the testbed of StarCraft unit micromanagement1, which has recently emerged as a challenging RL benchmark task with high stochasticity, a large state-action space, and delayed rewards. Previous works (Usunier et al., 2016; Peng et al., 2017) have made use of a centralised control policy that conditions on the entire state and can use powerful macro-actions, using StarCraft\u2019s built-in planner, that combine movement and attack actions. To produce a meaningfully decentralised benchmark that proves challenging for scenarios with even relatively few agents, we propose a variant that massively reduces each agent\u2019s field-of-view and removes access to these macro-actions.\nOur empirical results on this new benchmark show that COMA can significantly improve performance over other multi-agent actor-critic methods, as well as ablated versions of COMA itself. In addition, COMA\u2019s best agents are competitive with state-of-the-art centralised controllers which are given access to full state information and macro-actions."}, {"heading": "2 Related Work", "text": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments. One exception is recent work in deep multi-agent RL, which can scale to high dimensional input and action spaces. Tampuu et al. (2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014) to learn how to play two-player pong. More recently the same method has been used by Leibo et al. (2017) to study the emergence of collaboration and defection in sequential social dilemmas.\nAlso related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016). In this line of work, passing gradients between agents during training and sharing parameters are two common ways to take advantage of centralised training. However, these methods do not allow for extra state information to be used during learning and do not address the multi-agent credit assignment problem.\nGupta et al. (2017) investigate actor-critic methods for decentralised execution with centralised training. However, in their methods both the actors and the critic condition on local, per-agent, obser-\n1StarCraft and its expansion StarCraft: Brood War are trademarks of Blizzard EntertainmentTM.\nvations and actions, and multi-agent credit assignment is addressed only with hand-crafted local rewards.\nMost previous applications of RL to StarCraft micromanagement use a centralised controller, with access to the full state, and control of all units, although the architecture of the controllers exploits the multi-agent nature of the problem. Usunier et al. (2016) use a greedy MDP, which at each timestep sequentially chooses actions for agents given all previous actions, in combination with zero-order optimisation, while Peng et al. (2017) use an actor-critic method that relies on RNNs to exchange information between the agents. The closest to our problem setting is that of Foerster et al. (2017), who also use a multi-agent representation and decentralised policies. However, they focus on stabilising experience replay while using DQN and do not make full use of the centralised training regime. As they do not report on absolute win-rates we do not compare performance directly. However, Usunier et al. (2016) address similar scenarios to our experiments and implement a DQN baseline in a fully observable setting. In section 6 we therefore report our competitive performance against these state-of-the-art baselines, while maintaining decentralised control. Omidshafiei et al. (2017) also address the stability of experience replay in multi-agent settings, but assume a fully decentralised training regime."}, {"heading": "3 Background", "text": "We consider a fully cooperative multi-agent task that can be described as a stochastic game G, defined by a tuple G = \u3008S,U, P, r, Z,O, n, \u03b3\u3009, in which n agents identified by a \u2208 A \u2261 {1, ..., n} choose sequential actions. The environment has a true state s \u2208 S. At each time step, each agent takes an action ua \u2208 U , forming a joint action u \u2208 U \u2261 Un which induces a transition in the environment according to the state transition function P (s\u2032|s,u) : S \u00d7U\u00d7 S \u2192 [0, 1]. The agents all share the same reward function r(s,u) : S \u00d7U\u2192 R and \u03b3 \u2208 [0, 1) is a discount factor. We consider a partially observable setting, in which agents draw observations z \u2208 Z according to the observation function O(s, a) : S \u00d7 A \u2192 Z. Each agent has an action-observation history \u03c4a \u2208 T \u2261 (Z \u00d7 U)\u2217, on which it conditions a stochastic policy \u03c0a(ua|\u03c4a) : T \u00d7 U \u2192 [0, 1]. We denote joint quantities over agents in bold, and joint quantities over agents other than a given agent a with the superscript \u2212a. The discounted return is Rt = \u2211\u221e l=0 \u03b3\nlrt+l. The agents\u2019 joint policy induces a value function, i.e., an expectation over Rt, V \u03c0(st) = Est+1:\u221e,ut:\u221e [Rt|st], and an action-value function Q\u03c0(st,ut) = Est+1:\u221e,ut+1:\u221e [Rt|st,ut]. The advantage function is given by A\u03c0(st,ut) = Q\u03c0(st,ut)\u2212 V \u03c0(st). Following previous work (Oliehoek et al., 2008; Kraemer & Banerjee, 2016; Foerster et al., 2016; Jorge et al., 2016), our problem setting allows centralised training but requires decentralised execution. This is a natural paradigm for a large set of multi-agent problems where training is carried out using a simulator with additional state information, but the agents must rely on local actionobservation histories during execution. To condition on this full history, a deep RL agent may make use of a recurrent neural network (Hausknecht & Stone, 2015), typically making use of a gated model such as LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Chung et al., 2014).\nIn Section 4, we develop a new multi-agent policy gradient method for tackling this setting. In the remainder of this section, we provide some background on single-agent policy gradient methods (Sutton et al., 1999). Such methods optimise a single agent\u2019s policy, parameterised by \u03b8\u03c0 , by performing gradient ascent on an estimate of the expected discounted total reward E\u03c0 [Rt]. Perhaps the simplest form of policy gradient is REINFORCE (Williams, 1992), in which the gradient is:\ng = Es0:\u221e,u0:\u221e [ T\u2211 t=0 Rt\u2207\u03b8\u03c0 log \u03c0(ut|st) ] . (1)\nIn actor-critic approaches (Kimura et al., 2000; Schulman et al., 2015; Wang et al., 2016; Hafner & Riedmiller, 2011), the actor, i.e., the policy, is trained by following a gradient that depends on a critic, which usually estimates a value function. In particular, Rt is replaced by any expression equivalent to Q(st, ut) \u2212 b(st), where b(st) is a baseline designed to reduce variance (Weaver & Tao, 2001). A common choice is b(st) = V (st), in which case Rt is replaced by A(st, ut). Another option is to replace Rt with the temporal difference (TD) error rt + \u03b3V (st+1) \u2212 V (s), which is an unbiased estimate of A(st, ut). In practice, the gradient must be estimated from trajectories sampled from the environment, and the (action-)value functions must be estimated with function\napproximators. Consequently, the bias and variance of the gradient estimate depends strongly on the exact choice of estimator (Konda & Tsitsiklis, 1999).\nIn this paper, we train critics f c(\u00b7, \u03b8c) using a variant of TD(\u03bb) (Sutton, 1988) adapted for use with deep neural networks. TD(\u03bb) uses a mixture of n-step returns G(n)t = \u2211n l=1 \u03b3\nl\u22121rt+l + \u03b3nf(\u00b7t+n, \u03b8c). In particular, the critic parameters \u03b8c are updated to minimise the following loss:\nLt(\u03b8c) = (y(\u03bb) \u2212 f c(\u00b7t, \u03b8c))2, (2) where y(\u03bb) = (1 \u2212 \u03bb) \u2211\u221e n=1 \u03bb n\u22121G (n) t , and the n-step returns G (n) t are calculated using a target network (Mnih et al., 2015) with parameters copied periodically from \u03b8c."}, {"heading": "4 Methods", "text": "In this section, we describe approaches for extending policy gradients to our multi-agent setting."}, {"heading": "4.1 Independent Actor-Critic", "text": "The simplest way to apply policy gradients to multiple agents is to have each agent learn independently, with its own actor and critic, from its own action-observation history. This is essentially the idea behind independent Q-learning (Tan, 1993), which is perhaps the most popular multi-agent learning algorithm, but with actor-critic in place of Q-learning. Hence, we call this approach independent actor-critic (IAC).\nIn our implementation of IAC, we speed learning by sharing parameters among the agents, i.e., we learn only one actor and one critic, which are used by all agents. The agents can still behave differently because they receive different observations, including an agent-specific ID, and thus evolve different hidden states. Learning remains independent in the sense that each agent\u2019s critic estimates only a local value function, i.e., one that conditions on ua, not u. Though we are not aware of previous applications of this specific algorithm, we do not consider it a significant contribution but instead merely a baseline algorithm.\nWe consider two variants of IAC. In the first, each agent\u2019s critic estimates V (\u03c4a) and follows a gradient based on the TD error, as described in Section 3. In the second, each agent\u2019s critic estimates Q(\u03c4a, ua) and follows a gradient based on the advantage: A(\u03c4a, ua) = Q(\u03c4a, ua)\u2212 V (\u03c4a), where V (\u03c4a) = \u2211 ua \u03c0(u\na|\u03c4a)Q(\u03c4a, ua). Independent learning is straightforward, but the lack of information sharing at training time makes it difficult to learn coordinated strategies that depend on interactions between multiple agents, or for an individual agent to estimate the contribution of its actions to the team\u2019s reward."}, {"heading": "4.2 Counterfactual Multi-Agent Policy Gradients", "text": "The difficulties discussed above arise because, beyond parameter sharing, IAC fails to exploit the fact that learning is centralised in our setting. In this section, we propose counterfactual multi-agent (COMA) policy gradients, which overcome this limitation. Three main ideas underly COMA: 1) centralisation of the critic, 2) use of a counterfactual baseline, and 3) use of a critic representation that allows efficient evaluation of the baseline. The remainder of this section describes these ideas.\nFirst, COMA uses a centralised critic. Note that in IAC, each actor \u03c0(ua|\u03c4a) and each critic Q(\u03c4a, ua) or V (\u03c4a) conditions only on the agent\u2019s own action-observation history \u03c4a. However, the critic is used only during learning and only the actor is needed during execution. Since learning is centralised, we can therefore use a centralised critic that conditions on the true global state s, if it is available, or the joint action-observation histories \u03c4 otherwise. Each actor conditions on its own action-observation histories \u03c4a, with parameter sharing, as in IAC. Figure 1a illustrates this setup.\nA naive way to use this centralised critic would be for each actor to follow a gradient based on the TD error estimated from this critic:\n\u2207\u03b8\u03c0 = \u2202 \u2202\u03b8\u03c0 log \u03c0(u|\u03c4at ) (r + \u03b3V (st+1)\u2212 V (st)) . (3)\nHowever, such an approach fails to address a key credit assignment problem. Because the TD error considers only global rewards, the gradient computed for each actor does not explicitly reason about\nhow that particular agent\u2019s actions contribute to that global reward. Since the other agents may be exploring, the gradient for that agent becomes very noisy, particularly when there are many agents.\nTherefore, COMA uses a counterfactual baseline. The idea is inspired by difference rewards (Tumer & Agogino, 2007), in which each agent learns from a shaped rewardDa = r(s,u)\u2212r(s, (u\u2212a, ca)) that compares the global reward to the reward received when the action of agent a is replaced with a default action ca. Any action by agent a that improves Da also improves the true global reward r(s,u), because r(s, (u\u2212a, ca)) does not depend on agent a\u2019s actions.\nDifference rewards are a powerful way to perform multi-agent credit assignment. However, they typically require access to a simulator in order to estimate r(s, (u\u2212a, ca)). When a simulator is already being used for learning, difference rewards increase the number of simulations that must be conducted, since each agent\u2019s difference reward requires a separate counterfactual simulation. In addition, in many applications it is unclear how to choose ca.\nA key insight underlying COMA is that a centralised critic can be used to implement difference rewards in a way that avoids both these problems. COMA learns a centralised critic, Q(s,u) that estimates Q-values for the joint action u conditioned on the central state s. For each agent a we can then compute an advantage function that compares the Q-value for the current action ua to a counterfactual baseline that marginalises out ua, while keeping the other agents\u2019 actions u\u2212a fixed:\nAa(s, ua) = Q(s,u)\u2212 \u2211 ua \u03c0a(ua|\u03c4a)Q(s, (u\u2212a, ua)). (4)\nHence, instead of relying on extra simulations, Aa(s, ua) computes a separate baseline for each agent that uses the centralised critic to reason about counterfactuals in which only a\u2019s action changes.\nHowever, while this advantage function replaces extra simulations with evaluations of the critic, those evaluations may themselves be expensive if the critic is a deep neural network. Furthermore, in a typical representation, the number of output nodes of such a network would equal |U |n, the size of the joint action space, making it impractical to train. To address both these issues, COMA uses a critic representation that allows for efficient evaluation of the baseline. In particular, the actions of the other agents, u\u2212at , are part of the input to the network, which outputs aQ-value for each of agent a\u2019s actions, as shown in Figure 1c. Consequently, the counterfactual advantage can be calculated efficiently by a single forward pass of the actor and critic, for each agent. Furthermore, the number of outputs is only |U | instead of (|U |n). While the network has a large input space that scales linearly in the number of agents and actions, deep neural networks can generalise well across such spaces."}, {"heading": "5 Experimental Setup", "text": "In this section, we describe the StarCraft problem to which we apply COMA, as well as details of the state features, network architectures, training regimes, and ablations.\nDecentralised StarCraft Micromanagement. StarCraft is a rich environment with stochastic dynamics that cannot be easily emulated. Many simpler multi-agent settings, such as Predator-Prey (Tan, 1993) or Packet World (Weyns et al., 2005), by contrast, have full simulators with controlled randomness that can be freely set to any state in order to perfectly replay experiences. This makes it possible, though computationally expensive, to compute difference rewards via extra simulations. In StarCraft, as in the real world, this is not possible.\nIn this paper, we focus on the problem of micromanagement in StarCraft, which refers to the lowlevel control of individual units\u2019 positioning and attack commands as they fight enemies. This task is naturally represented as a multi-agent system, where each StarCraft unit is replaced by a decentralised controller. We consider several scenarios with symmetric teams, formed of: 3 marines (3m), 5 marines (5m), 5 wraiths (5w), or 2 dragoons with 3 zealots (2d 3z). The enemy team is controlled by the StarCraft AI, which uses a set of reasonable but suboptimal hand-crafted heuristics.\nWe allow the agents to choose from a set of discrete actions: move[direction], attack[enemy id], stop, and noop. In the StarCraft game, when a unit selects an attack action, it first moves into attack range before firing, using the game\u2019s built-in pathfinding to choose a route. These powerful attack-move macro-actions make the control problem considerably easier.\nTo create a more challenging benchmark that is meaningfully decentralised, we impose a restricted field of view on the agents, equal to the firing range of the weapons of the ranged units, shown in Figure 2. This departure from the standard setup for centralised StarCraft control has three effects.\nFirst, it introduces significant partial observability. Second, it means units can only attack when they are in range of enemies, removing access to the StarCraft macro-actions. Third, agents cannot distinguish between enemies who are dead and those who are out of range and so can issue invalid attack commands at such enemies, which results in no action being taken. This substantially increases the average size of the action space, which in turn increases the difficulty of both exploration and control.\nUnder these difficult conditions, scenarios with even relatively small numbers of units become much harder to solve. As seen in Table 1, we compare against a simple hand-coded heuristic that instructs the agents to run forwards into range and then focus their fire, attacking each enemy in turn until it dies. This heuristic achieves a 98% win rate on m5v5 with a full field of view, but only 66% in our setting. To perform well in this task, the agents must learn to cooperate by positioning properly and focussing their fire, while remembering which enemy and ally units are alive or out of view.\nAll agents receive the same global reward at each time step, equal to the sum of damage inflicted on the opponent units minus half the damage taken. Killing an opponent generates a reward of 10 points, and winning the game generates a reward equal to the team\u2019s remaining total health plus 200. This damage-based reward signal is comparable to that used by Usunier et al. (2016). Unlike Peng et al. (2017), our approach does not require estimating local rewards.\nState Features. The actor and critic receive different input features, corresponding to local observations and global state, respectively. Both include features for allies and enemies. Units can be either allies or enemies, while agents are the decentralised controllers that command ally units.\nThe local observations for every agent are drawn only from a circular subset of the map centred on the unit it controls and include for each unit within this field of view: distance, relative x, relative y, unit type and shield.2 All features are normalized by their maximum values. We do not include any information about the units\u2019 current target.\nThe global state representation consists of similar features, but for all units on the map regardless of fields of view. Absolute distance is not included, and x-y locations are given relative to the centre of the map rather than to a particular agent. The global state also includes health points and cooldown for all agents. The representation fed to the centralised Q-function critic is the concatenation of the global state representation with the local observation of the agent whose actions\n2After firing, a unit\u2019s cooldown is reset, and it must drop before firing again. Shields absorb damage until they break, after which units start losing health. Dragoons and zealots have shields but marines do not.\nare being evaluated. Our centralised critic that estimates V (s), and is therefore agent-agnostic, receives the global state concatenated with all agents\u2019 observations. The observations contain no new information but include the egocentric distances relative to that agent.\nArchitecture & Training. The actor consists of 128-bit gated recurrent units (GRUs) (Cho et al., 2014) that use fully connected layers both to process the input and to produce the output values from the hidden state, hat . The IAC critics use extra output heads appended to the last layer of the actor network. Action probabilities are produced from the final layer, z, via a bounded softmax distribution that lower-bounds the probability of any given action by /|U |: P (u) = (1\u2212 )softmax(z)u+ /|U |). We anneal linearly from 0.5 to 0.02 across 750 training episodes. The centralised critic is a feedforward network with multiple ReLU layers combined with fully connected layers. Hyperparameters were coarsely tuned on the m5v5 scenario and then used for all other maps. We found that the most sensitive parameter was TD(\u03bb), but settled on \u03bb = 0.8 which worked best for both COMA and our baselines. Our implementation uses TorchCraft (Synnaeve et al., 2016) and Torch 7 (Collobert et al., 2011). Further details on the training procedure, and pseudocode are in the supplementary material.\nAblations. We perform ablation experiments to validate three key elements of COMA. First, we test the importance of centralising the critic by comparing against two IAC variants, IAC-Q and IAC-V . These take the same decentralised input as the actor, and share the actor network parameters up to the final layer. IAC-Q then outputs |U | Q-values, one for each action, while IAC-V outputs a single state-value. Second, we test the significance of learning Q instead of V . The method central-V still uses a central state for the critic, but learns V (s), and uses the TD error to estimate the advantage for policy gradient updates. Third, we test the utility of our counterfactual baseline. The method central-QV learns both Q and V simultaneously and estimates the advantage as Q \u2212 V , replacing our counterfactual baseline with V . All methods use the same architecture and training scheme for the actors, and all critics are trained with TD(\u03bb)."}, {"heading": "6 Results", "text": "Figure 3 shows average win rates as a function of episode for each method and each StarCraft scenario. For each method, we conducted 35 independent trials and froze learning every 100 training episodes to evaluate the learned policies across 200 episodes per method, plotting the average across episodes and trials. Also shown is one standard deviation in performance.\nThe results show that COMA is superior to the IAC baselines in all scenarios. Interestingly, the IAC methods also eventually learn reasonable policies in m5v5, although they need substantially more episodes to do so. This may seem counter-intuitive since in the IAC methods, the actor and critic networks share parameters in their early layers (see Section 5). This could be expected to speed learning, but these results suggest that the improved accuracy of policy evaluation made possible by conditioning on the global state outweighs the overhead of training a separate network.\nFurthermore, COMA strictly dominates central-QV , both in training speed and in final performance across all settings. This is a strong indicator that our counterfactual baseline is crucial when using a central Q-critic to train decentralised policies.\n35w DQN and GMEZO benchmark performances are of a policy trained on a larger map and tested on 5w\nLearning a state-value function has the obvious advantage of not conditioning on the joint action. Still, we find that COMA outperforms our baseline central-V in final performance. Further, COMA typically achieves good policies faster, which is expected as COMA provides a shaped training signal. Training is also more stable than central-V , which is a consequence of the COMA gradient tending to zero as the policy becomes greedy. Overall, COMA is the best performing and most consistent method.\nUsunier et al. (2016) report the performance of their best agents trained with their state-of-the-art centralised controller labelled GMEZO (greedy-MDP with episodic zero-order optimisation), and for a centralised DQN controller, both given a full field of view and access to attack-move macroactions. These results are compared in Table 1 against the best agents trained with COMA for each map. Clearly, in most settings these agents achieve performances comparable to the best published win rates despite being restricted to decentralised policies and a local field of view."}, {"heading": "7 Conclusions & Future Work", "text": "This paper presented COMA policy gradients, a method that uses a centralised critic in order to estimate a counterfactual advantage for decentralised policies in mutli-agent RL. COMA addresses the challenges of multi-agent credit assignment by using a counterfactual baseline that marginalises out a single agent\u2019s action, while keeping the other agents\u2019 actions fixed. Our results in a decentralised StarCraft unit micromanagement benchmark show that COMA significantly improves final performance and training speed over other multi-agent actor-critic methods and remains competitive with state-of-the-art centralised controllers under best-performance reporting. Future work will extend COMA to tackle scenarios with large numbers of agents, where centralised critics are more difficult to train and exploration is harder to coordinate. We also aim to develop more sample-efficient variants that are practical for real-world applications such as self-driving cars."}, {"heading": "Acknowledgements", "text": "This work was supported by the Oxford-Google DeepMind Graduate Scholarship, the UK EPSRC CDT in Autonomous Intelligent Machines and Systems, and a generous grant from Microsoft for their Azure cloud computing services. We would like to thank Nando de Freitas, Yannis Assael, and Brendan Shillingford for helpful comments and discussion. We also thank Gabriel Synnaeve, Zeming Lin, and the rest of the TorchCraft team at FAIR for their work on the interface."}, {"heading": "A Training Details and Hyperparameters", "text": "Training is performed in batch mode, with a batch size of 30. Due to parameter sharing, all agents can be processed in parallel, with each agent for each episode and time step occupying one batch entry. The training cycle progresses in three steps (completion of all three steps constitutes as one episode in our graphs): 1) collect data: collect 30n episodes; 2) train critic: for each time step, apply a gradient step to the feed-forward critic, starting at the end of the episode; and 3) train actor: fully unroll the recurrent part of the actor, aggregate gradients in the backward pass across all time steps, and apply a gradient update.\nWe use a target network for the critic, which updates every 150 training steps for the feed-forward centralised critics and every 50 steps for the recurrent IAC critics. The feed-forward critic receives more learning steps, since it performs a parameter update for each timestep. Both the actor and the critic networks are trained using RMS-prop with learning rate 0.0005 and alpha 0.99, without weight decay. We set gamma to 0.99 for all maps.\nAlthough tuning the skip-frame in StarCraft can improve absolute performance (Peng et al., 2017), we use a default value of 7, since the main focus is a relative evaluation between COMA and the baselines."}, {"heading": "B Algorithm", "text": "Algorithm 1 Counterfactual Multi-Agent (COMA) Policy Gradients\nInitialise \u03b8c1, \u03b8\u0302 c 1, \u03b8 \u03c0 for each training episode e do Empty buffer for ec = 1 to BatchSizen do s1 = initial state, t = 0, ha0 = 0 for each agent a while st 6= terminal and t < T do t = t+ 1 for each agent a do hat = Actor ( oat , h a t\u22121, u a t\u22121, a, u; \u03b8i\n) Sample uat from \u03c0(h a t , (e))\nGet reward rt and next state st+1 Add episode to buffer\nCollate episodes in buffer into single batch for t = 1 to T do // from now processing all agents in parallel via single batch\nBatch unroll RNN using states, actions and rewards Calculate TD(\u03bb) targets yat using \u03b8\u0302 c i\nfor t = T down to 1 do \u2206Qat = y a t \u2212Q ( saj ,u ) \u2207\u03b8c = \u2207\u03b8c + \u2202\u2202\u03b8c (\u2206Q a t )\n2 // calculate critic gradient \u03b8ci+1 = \u03b8 c i + \u03b1\u2207\u03b8c // update critic weights Every C steps reset \u03b8\u0302ci = \u03b8 c i\nfor t = T down to 1 do Aa(sat ,u) = Q(s a t ,u)\u2212 \u2211 uQ(s a t , u,u\n\u2212a)\u03c0(u|hat ) // calculate COMA \u2207\u03b8\u03c0 = \u2207\u03b8\u03c0 + \u2202\u2202\u03b8\u03c0 log \u03c0(u|h a t )A\na(sat ,u) // accumulate actor gradients \u03b8\u03c0i+1 = \u03b8 \u03c0 i + \u03b1\u2207\u03b8\u03c0 // update actor weights"}], "references": [{"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Busoniu", "Lucian", "Babuska", "Robert", "De Schutter", "Bart"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "An overview of recent progress in the study of distributed multi-agent coordination", "author": ["Cao", "Yongcan", "Yu", "Wenwu", "Ren", "Wei", "Chen", "Guanrong"], "venue": "IEEE Transactions on Industrial informatics,", "citeRegEx": "Cao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "All learning is local: Multi-agent learning in global reward games", "author": ["Chang", "Yu-Han", "Ho", "Tracey", "Kaelbling", "Leslie Pack"], "venue": "In NIPS,", "citeRegEx": "Chang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2003}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Das", "Abhishek", "Kottur", "Satwik", "Moura", "Jos\u00e9 MF", "Lee", "Stefan", "Batra", "Dhruv"], "venue": "arXiv preprint arXiv:1703.06585,", "citeRegEx": "Das et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob", "Assael", "Yannis M", "de Freitas", "Nando", "Whiteson", "Shimon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Stabilising experience replay for deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob", "Nardelli", "Nantas", "Farquhar", "Gregory", "Torr", "Philip", "Kohli", "Pushmeet", "Whiteson", "Shimon"], "venue": "In Proceedings of The 34th International Conference on Machine Learning,", "citeRegEx": "Foerster et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2017}, {"title": "Cooperative multi-agent control using deep reinforcement learning", "author": ["Gupta", "Jayesh K", "Egorov", "Maxim", "Kochenderfer", "Mykel"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2017}, {"title": "Reinforcement learning in feedback control", "author": ["Hafner", "Roland", "Riedmiller", "Martin"], "venue": "Machine learning,", "citeRegEx": "Hafner et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hafner et al\\.", "year": 2011}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning to play guess who? and inventing a grounded language as a consequence", "author": ["Jorge", "Emilio", "K\u00e5geb\u00e4ck", "Mikael", "Gustavsson", "Emil"], "venue": "arXiv preprint arXiv:1611.03218,", "citeRegEx": "Jorge et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jorge et al\\.", "year": 2016}, {"title": "An analysis of actor-critic algorithms using eligibility traces: reinforcement learning with imperfect value functions", "author": ["Kimura", "Hajime", "Kobayashi", "Shigenobu"], "venue": "Journal of Japanese Society for Artificial Intelligence,", "citeRegEx": "Kimura et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kimura et al\\.", "year": 2000}, {"title": "Multi-agent reinforcement learning as a rehearsal for decentralized planning", "author": ["Kraemer", "Landon", "Banerjee", "Bikramjit"], "venue": null, "citeRegEx": "Kraemer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kraemer et al\\.", "year": 2016}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["Lazaridou", "Angeliki", "Peysakhovich", "Alexander", "Baroni", "Marco"], "venue": "arXiv preprint arXiv:1612.07182,", "citeRegEx": "Lazaridou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2016}, {"title": "Multi-agent reinforcement learning in sequential social dilemmas", "author": ["Leibo", "Joel Z", "Zambaldi", "Vinicius", "Lanctot", "Marc", "Marecki", "Janusz", "Graepel", "Thore"], "venue": "arXiv preprint arXiv:1702.03037,", "citeRegEx": "Leibo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Leibo et al\\.", "year": 2017}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Emergence of grounded compositional language in multi-agent populations", "author": ["Mordatch", "Igor", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1703.04908,", "citeRegEx": "Mordatch et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2017}, {"title": "Optimal and approximate Q-value functions for decentralized POMDPs", "author": ["Oliehoek", "Frans A", "Spaan", "Matthijs T. J", "Vlassis", "Nikos"], "venue": null, "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "Deep decentralized multi-task multi-agent rl under partial observability", "author": ["Omidshafiei", "Shayegan", "Pazis", "Jason", "Amato", "Christopher", "How", "Jonathan P", "Vian", "John"], "venue": "arXiv preprint arXiv:1703.06182,", "citeRegEx": "Omidshafiei et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Omidshafiei et al\\.", "year": 2017}, {"title": "Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games", "author": ["Peng", "Yuan", "Quan", "Wen", "Ying", "Yang", "Yaodong", "Tang", "Zhenkun", "Long", "Haitao", "Wang", "Jun"], "venue": "arXiv preprint arXiv:1703.10069,", "citeRegEx": "Peng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "CoRR, abs/1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown", "year": 2009}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sukhbaatar", "Sainbayar", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Torchcraft: a library for machine learning research on real-time strategy games", "author": ["Synnaeve", "Gabriel", "Nardelli", "Nantas", "Auvolat", "Alex", "Chintala", "Soumith", "Lacroix", "Timoth\u00e9e", "Lin", "Zeming", "Richoux", "Florian", "Usunier", "Nicolas"], "venue": "arXiv preprint arXiv:1611.00625,", "citeRegEx": "Synnaeve et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Synnaeve et al\\.", "year": 2016}, {"title": "Multiagent cooperation and competition with deep reinforcement learning", "author": ["Tampuu", "Ardi", "Matiisen", "Tambet", "Kodelja", "Dorian", "Kuzovkin", "Ilya", "Korjus", "Kristjan", "Aru", "Juhan", "Jaan", "Vicente", "Raul"], "venue": "arXiv preprint arXiv:1511.08779,", "citeRegEx": "Tampuu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tampuu et al\\.", "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Tan", "Ming"], "venue": "In Proceedings of the tenth international conference on machine learning,", "citeRegEx": "Tan and Ming.,? \\Q1993\\E", "shortCiteRegEx": "Tan and Ming.", "year": 1993}, {"title": "Distributed agent-based air traffic flow management", "author": ["Tumer", "Kagan", "Agogino", "Adrian"], "venue": "In Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Tumer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tumer et al\\.", "year": 2007}, {"title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks", "author": ["Usunier", "Nicolas", "Synnaeve", "Gabriel", "Lin", "Zeming", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1609.02993,", "citeRegEx": "Usunier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Usunier et al\\.", "year": 2016}, {"title": "Sample efficient actor-critic with experience", "author": ["Wang", "Ziyu", "Bapst", "Victor", "Heess", "Nicolas", "Mnih", "Volodymyr", "Munos", "Remi", "Kavukcuoglu", "Koray", "de Freitas", "Nando"], "venue": "replay. arXiv preprint arXiv:1611.01224,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "The optimal reward baseline for gradient-based reinforcement learning", "author": ["Weaver", "Lex", "Tao", "Nigel"], "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Weaver et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Weaver et al\\.", "year": 2001}, {"title": "The packet-world: A test bed for investigating situated multi-agent systems. In Software Agent-Based Applications, Platforms and Development Kits", "author": ["Weyns", "Danny", "Helleboogh", "Alexander", "Holvoet", "Tom"], "venue": null, "citeRegEx": "Weyns et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weyns et al\\.", "year": 2005}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Multiagent reinforcement learning for multi-robot systems: A survey", "author": ["Yang", "Erfu", "Gu", "Dongbing"], "venue": "Technical report, tech. rep,", "citeRegEx": "Yang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2004}, {"title": "A multi-agent framework for packet routing in wireless sensor", "author": ["Ye", "Dayong", "Zhang", "Minjie", "Yang", "Yun"], "venue": "networks. sensors,", "citeRegEx": "Ye et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2015}, {"title": "Multi-agent framework for third party logistics in e-commerce", "author": ["Ying", "Wang", "Dayong", "Sang"], "venue": "Expert Systems with Applications,", "citeRegEx": "Ying et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2005}, {"title": "Empirically evaluating multiagent learning algorithms", "author": ["E. Zawadzki", "A. Lipson", "K. Leyton-Brown"], "venue": "arXiv preprint 1401.8074,", "citeRegEx": "Zawadzki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zawadzki et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Many complex reinforcement learning (RL) problems such as the coordination of autonomous vehicles (Cao et al., 2013), network packet delivery (Ye et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 38, "context": ", 2013), network packet delivery (Ye et al., 2015), and distributed logistics (Ying & Dayong, 2005) are naturally modelled as cooperative multi-agent systems.", "startOffset": 33, "endOffset": 50}, {"referenceID": 20, "context": "This centralised training of decentralised policies is a standard paradigm for multi-agent planning (Oliehoek et al., 2008; Kraemer & Banerjee, 2016) and has recently been picked up by the deep RL community (Foerster et al.", "startOffset": 100, "endOffset": 149}, {"referenceID": 7, "context": ", 2008; Kraemer & Banerjee, 2016) and has recently been picked up by the deep RL community (Foerster et al., 2016; Jorge et al., 2016).", "startOffset": 91, "endOffset": 134}, {"referenceID": 13, "context": ", 2008; Kraemer & Banerjee, 2016) and has recently been picked up by the deep RL community (Foerster et al., 2016; Jorge et al., 2016).", "startOffset": 91, "endOffset": 134}, {"referenceID": 2, "context": "Another crucial challenge is multi-agent credit assignment (Chang et al., 2003): in cooperative settings, joint actions typically generate only global rewards, making it difficult for each agent to", "startOffset": 59, "endOffset": 79}, {"referenceID": 32, "context": "Previous works (Usunier et al., 2016; Peng et al., 2017) have made use of a centralised control policy that conditions on the entire state and can use powerful macro-actions, using StarCraft\u2019s built-in planner, that combine movement and attack actions.", "startOffset": 15, "endOffset": 56}, {"referenceID": 22, "context": "Previous works (Usunier et al., 2016; Peng et al., 2017) have made use of a centralised control policy that conditions on the entire state and can use powerful macro-actions, using StarCraft\u2019s built-in planner, that combine movement and attack actions.", "startOffset": 15, "endOffset": 56}, {"referenceID": 0, "context": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments.", "startOffset": 66, "endOffset": 105}, {"referenceID": 40, "context": "(2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014) to learn how to play two-player pong.", "startOffset": 60, "endOffset": 123}, {"referenceID": 6, "context": "Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 99, "endOffset": 214}, {"referenceID": 16, "context": "Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 99, "endOffset": 214}, {"referenceID": 7, "context": "Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 99, "endOffset": 214}, {"referenceID": 25, "context": "Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016).", "startOffset": 99, "endOffset": 214}, {"referenceID": 0, "context": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments. One exception is recent work in deep multi-agent RL, which can scale to high dimensional input and action spaces. Tampuu et al. (2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al.", "startOffset": 67, "endOffset": 315}, {"referenceID": 0, "context": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments. One exception is recent work in deep multi-agent RL, which can scale to high dimensional input and action spaces. Tampuu et al. (2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014) to learn how to play two-player pong. More recently the same method has been used by Leibo et al. (2017) to study the emergence of collaboration and defection in sequential social dilemmas.", "startOffset": 67, "endOffset": 537}, {"referenceID": 0, "context": "Although multi-agent RL has been applied in a variety of settings (Busoniu et al., 2008; Yang & Gu, 2004), it has often been restricted to tabular methods and simple environments. One exception is recent work in deep multi-agent RL, which can scale to high dimensional input and action spaces. Tampuu et al. (2015) use a combination of DQN with independent Q-learning (Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014) to learn how to play two-player pong. More recently the same method has been used by Leibo et al. (2017) to study the emergence of collaboration and defection in sequential social dilemmas. Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al., 2017; Mordatch & Abbeel, 2017; Lazaridou et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016). In this line of work, passing gradients between agents during training and sharing parameters are two common ways to take advantage of centralised training. However, these methods do not allow for extra state information to be used during learning and do not address the multi-agent credit assignment problem. Gupta et al. (2017) investigate actor-critic methods for decentralised execution with centralised training.", "startOffset": 67, "endOffset": 1168}, {"referenceID": 28, "context": "Usunier et al. (2016) use a greedy MDP, which at each timestep sequentially chooses actions for agents given all previous actions, in combination with zero-order optimisation, while Peng et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "(2016) use a greedy MDP, which at each timestep sequentially chooses actions for agents given all previous actions, in combination with zero-order optimisation, while Peng et al. (2017) use an actor-critic method that relies on RNNs to exchange information between the agents.", "startOffset": 167, "endOffset": 186}, {"referenceID": 7, "context": "The closest to our problem setting is that of Foerster et al. (2017), who also use a multi-agent representation and decentralised policies.", "startOffset": 46, "endOffset": 69}, {"referenceID": 7, "context": "The closest to our problem setting is that of Foerster et al. (2017), who also use a multi-agent representation and decentralised policies. However, they focus on stabilising experience replay while using DQN and do not make full use of the centralised training regime. As they do not report on absolute win-rates we do not compare performance directly. However, Usunier et al. (2016) address similar scenarios to our experiments and implement a DQN baseline in a fully observable setting.", "startOffset": 46, "endOffset": 385}, {"referenceID": 7, "context": "The closest to our problem setting is that of Foerster et al. (2017), who also use a multi-agent representation and decentralised policies. However, they focus on stabilising experience replay while using DQN and do not make full use of the centralised training regime. As they do not report on absolute win-rates we do not compare performance directly. However, Usunier et al. (2016) address similar scenarios to our experiments and implement a DQN baseline in a fully observable setting. In section 6 we therefore report our competitive performance against these state-of-the-art baselines, while maintaining decentralised control. Omidshafiei et al. (2017) also address the stability of experience replay in multi-agent settings, but assume a fully decentralised training regime.", "startOffset": 46, "endOffset": 660}, {"referenceID": 20, "context": "Following previous work (Oliehoek et al., 2008; Kraemer & Banerjee, 2016; Foerster et al., 2016; Jorge et al., 2016), our problem setting allows centralised training but requires decentralised execution.", "startOffset": 24, "endOffset": 116}, {"referenceID": 7, "context": "Following previous work (Oliehoek et al., 2008; Kraemer & Banerjee, 2016; Foerster et al., 2016; Jorge et al., 2016), our problem setting allows centralised training but requires decentralised execution.", "startOffset": 24, "endOffset": 116}, {"referenceID": 13, "context": "Following previous work (Oliehoek et al., 2008; Kraemer & Banerjee, 2016; Foerster et al., 2016; Jorge et al., 2016), our problem setting allows centralised training but requires decentralised execution.", "startOffset": 24, "endOffset": 116}, {"referenceID": 4, "context": "To condition on this full history, a deep RL agent may make use of a recurrent neural network (Hausknecht & Stone, 2015), typically making use of a gated model such as LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Chung et al., 2014).", "startOffset": 213, "endOffset": 233}, {"referenceID": 27, "context": "In the remainder of this section, we provide some background on single-agent policy gradient methods (Sutton et al., 1999).", "startOffset": 101, "endOffset": 122}, {"referenceID": 14, "context": "In actor-critic approaches (Kimura et al., 2000; Schulman et al., 2015; Wang et al., 2016; Hafner & Riedmiller, 2011), the actor, i.", "startOffset": 27, "endOffset": 117}, {"referenceID": 23, "context": "In actor-critic approaches (Kimura et al., 2000; Schulman et al., 2015; Wang et al., 2016; Hafner & Riedmiller, 2011), the actor, i.", "startOffset": 27, "endOffset": 117}, {"referenceID": 33, "context": "In actor-critic approaches (Kimura et al., 2000; Schulman et al., 2015; Wang et al., 2016; Hafner & Riedmiller, 2011), the actor, i.", "startOffset": 27, "endOffset": 117}, {"referenceID": 18, "context": "where y = (1 \u2212 \u03bb) \u2211\u221e n=1 \u03bb n\u22121G (n) t , and the n-step returns G (n) t are calculated using a target network (Mnih et al., 2015) with parameters copied periodically from \u03b8.", "startOffset": 109, "endOffset": 128}, {"referenceID": 35, "context": "Many simpler multi-agent settings, such as Predator-Prey (Tan, 1993) or Packet World (Weyns et al., 2005), by contrast, have full simulators with controlled randomness that can be freely set to any state in order to perfectly replay experiences.", "startOffset": 85, "endOffset": 105}, {"referenceID": 31, "context": "This damage-based reward signal is comparable to that used by Usunier et al. (2016). Unlike Peng et al.", "startOffset": 62, "endOffset": 84}, {"referenceID": 22, "context": "Unlike Peng et al. (2017), our approach does not require estimating local rewards.", "startOffset": 7, "endOffset": 26}, {"referenceID": 3, "context": "The actor consists of 128-bit gated recurrent units (GRUs) (Cho et al., 2014) that use fully connected layers both to process the input and to produce the output values from the hidden state, ht .", "startOffset": 59, "endOffset": 77}, {"referenceID": 28, "context": "Our implementation uses TorchCraft (Synnaeve et al., 2016) and Torch 7 (Collobert et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 5, "context": ", 2016) and Torch 7 (Collobert et al., 2011).", "startOffset": 20, "endOffset": 44}, {"referenceID": 32, "context": "Usunier et al. (2016) report the performance of their best agents trained with their state-of-the-art centralised controller labelled GMEZO (greedy-MDP with episodic zero-order optimisation), and for a centralised DQN controller, both given a full field of view and access to attack-move macroactions.", "startOffset": 0, "endOffset": 22}], "year": 2017, "abstractText": "Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing or the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents\u2019 policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent\u2019s action, while keeping the other agents\u2019 actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.", "creator": "LaTeX with hyperref package"}}}