{"id": "1705.08942", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Joint PoS Tagging and Stemming for Agglutinative Languages", "abstract": "the number of word forms in agglutinative languages is theoretically infinite and this variety in word forms introduces sparsity in many natural language processing tasks. part - of - speech semantics tagging ( pos tagging ) is one of these tasks that often suffers from sparsity. in this paper, we present an unsupervised bayesian model using hidden markov models ( aka hmms ) semantics for joint pos logic tagging and stemming for agglutinative languages. we then use stemming to reduce sparsity in pos tagging. two tasks are jointly performed to provide a mutual benefit in both tracking tasks. our results models show that joint pos tagging and stemming improves of pos tagging scores. we present results encoding for turkish and finnish subjects as agglutinative languages and english as a morphologically poor language.", "histories": [["v1", "Wed, 24 May 2017 19:44:35 GMT  (314kb,D)", "http://arxiv.org/abs/1705.08942v1", "12 pages with 3 figures, accepted and presented at the CICLING 2017 - 18th International Conference on Intelligent Text Processing and Computational Linguistics"]], "COMMENTS": "12 pages with 3 figures, accepted and presented at the CICLING 2017 - 18th International Conference on Intelligent Text Processing and Computational Linguistics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["necva b\\\"ol\\\"uc\\\"u", "burcu can"], "accepted": false, "id": "1705.08942"}, "pdf": {"name": "1705.08942.pdf", "metadata": {"source": "CRF", "title": "Joint PoS Tagging and Stemming for Agglutinative Languages", "authors": ["Burcu Can"], "emails": ["necva@cs.hacettepe.edu.tr", "burcu@cs.hacettepe.edu.tr"], "sections": [{"heading": null, "text": "theoretically infinite and this variety in word forms introduces sparsity\nin many natural language processing tasks. Part-of-speech tagging (PoS\ntagging) is one of these tasks that often suffers from sparsity. In this paper,\nwe present an unsupervised Bayesian model using Hidden Markov Models\n(HMMs) for joint PoS tagging and stemming for agglutinative languages.\nWe use stemming to reduce sparsity in PoS tagging. Two tasks are jointly\nperformed to provide a mutual benefit in both tasks. Our results show\nthat joint POS tagging and stemming improves PoS tagging scores. We\npresent results for Turkish and Finnish as agglutinative languages and\nEnglish as a morphologically poor language.\nKeywords: unsupervised learning, part-of-speech tagging (PoS tagging),\nstemming, Bayesian learning, hidden Markov models (HMMs)"}, {"heading": "1 Introduction", "text": "Part-of-speech (PoS) tagging is one of the essential tasks in many natural language processing (NLP) applications, such as machine translation, sentiment analysis, question answering etc. The task is especially crucial for the disambiguation of a word. For example, the word saw can correspond to either a noun or a verb. The meaning is ambiguous unless its syntactic category is known. Once its syntactic category is assigned a noun, it becomes clear that the word corresponds to the tool, saw.\nAgglutinative languages introduce the sparsity problem in NLP tasks due to their rich morphology. Hankamer [12] claims that the number of various word forms in an agglutinative language like Turkish is theoretically infinite. The sparsity emerges with out-of-vocabulary (OOV) problem and is often a bottleneck in PoS tagging. Therefore, PoS tagging in agglutinative languages becomes even more challenging compared to other languages with a poorer morphology.\nIn this paper, we tackle the sparsity problem by combining PoS tagging with stemming in the same framework by reducing the number of distinct word forms to distinct stem types. Stemming is the process of finding the stem of the word by removing its suffixes. In stemming, normally inflectional suffixes are stripped off, whereas the derivational suffixes are kept because the stem refers to a different\nar X\niv :1\n70 5.\n08 94\n2v 1\n[ cs\n.C L\n] 2\n4 M\nay 2\nword type (i.e. lemma). For example, the stem of bookings is booking since -s is an inflectional suffix, whereas -ing is a derivational suffix. Moreover, booking exists in dictionary as a word itself.\nMany PoS tagging models ignore the morphological structure of the agglutinative languages. In this paper, we present an unsupervised model for PoS tagging that jointly finds stems and PoS tags. We propose different approaches to the same model, where all of them learn the tags and stems from a given raw text in a fully unsupervised setting. Different approaches show that using stems rather than words in learning PoS tagging improves PoS tagging performance, which also helps in learning stems cooperatively. Our model is based on a Bayesian hidden Markov Model (HMM) with a second order Markov chain for the tag transitions. We test with different emission types and the results show that emitting stems rather than words improves PoS tagging accuracy.\nThe paper is organized as follows: Section 2 addresses the related work on unsupervised POS tagging and stemming, section 3 describes our Bayesian HMM model and the different settings of the same Bayesian model applied for joint learning of PoS tags and stems, section 4 explains the inference algorithm to learn the model, section 5 presents the experimental results obtained from different datasets for English, Turkish and Finnish languages along with a discussion on the results, and finally section 6 concludes the paper with the future goals.\n2 Related Work\n2.1 PoS Tagging\nVarious methods have been applied for PoS tagging. Some of them have seen PoS tagging as a clustering/classification problem. Brown et al. [4] introduce a class-based n-gram model that learns either syntactic or semantic classes of words depending on the adopted language model; Sch\u00fctze [28] classifies the vector representation of words using neural networks to learn syntactic categories; Clark [6] proposes a probabilistic context distributional clustering to cluster words occurring in similar contexts, thereby having similar syntactic features. Bienmann [2] introduces a graph clustering algorithm as a PoS tagger. The graph based tagger involves two stages: In the first stage, words are clustered based on their contextual statistics; in the second stage, less frequent words are clustered using their similarity scores.\nSome other approaches have tackled PoS tagging as a sequential learning problem. For that purpose, hidden Markov models (HMMs) are commonly used for PoS tagging. HMM-based PoS tagging models go back to Merialdo [18]. Merialdo uses a trigram HMM model with maximum likelihood (ML) estimation. Trigrams\u2019n\u2019Tags (TnT) [3] is another statistical PoS tagger that uses a second order Markov Model with also maximum likelihood estimation.\nTwo tasks are jointly performed to provide a mutual benefit in bJohnson [13] compares the estimators used in HMM PoS taggers. He discovers that ExpectationMaximization (EM) is not good at estimation in HMM-based PoS taggers. Gao\net al. [7] also compare different Bayesian estimators for HMM PoS taggers. Gao et al. state that Gibbs sampler performs better on small datasets with few tags, but variational Bayes performs better on larger datasets.\nBayesian methods have also been used in PoS tagging. Goldwater and Griffiths [10] adopt Bayesian learning in HMMs. HMM parameters are modeled as a Multinomial-Dirichlet distribution. In this paper, we also use their model as a baseline to our joint model.\nVan Gael et al. [30] and Synder et al. [29] introduce infinite HMMs that are non-parametric Bayesian where the number of states is not set and the states can grow with data.\n2.2 Stemming\nStemmers are mainly based on three approaches: rule-based, statistical, and hybrid.\nRule based stemmers, as the name implies, extract the base forms by using manually defined rules. The oldest stemmers are rule-based [14,25,26].\nOne of the earliest statistical stemmers is developed by Xu and Croft [31]. Their method makes use of co-occurences of words to deal with words grouped in equivalence classes that are built by aggressive stemming. Mayfield and McNamee [16] propose a language independent n-gram stemmer. In their approach, stems are induced using n-gram letter statistics obtained from a corpus. Melucci et al. [17] implement a HMM-based stemmer using ML estimate to select the most likely stem and suffix based on the substring frequencies obtained from a corpus.\nLinguistica [9], although being an unsupervised morphological segmentation system, is also used as a stemmer. The method is based on Minimum Description Length (MDL) model that aims to minimize the size of the lexicon by segmenting words into its segments.\nGRAph-based Stemmer (GRAS) is introduced by Paik et al. [21] that groups words to find suffix pairs. These suffix pairs are used to build an undirected graph. Another unsupervised stemmer of the same authors [22] use co-occurence statistics of words to find the common prefixes.\nHPS [5] is one of the recent unsupervised stemmers that exploits lexical and semantic information to prepare large-scale training data in the first state, and use a maximum entropy classifier in the second stage by using the training data obtained from the first stage.\nHybrid stemmers involve different methods in a single model. Popat et al. [24] propose a hybrid stemmer for Gujarati that combines statistical and rule-based models. MAULIK [19] introduce another hybrid stemmer that combines the rule-based stemmers. A word is searched in the lexicon. If not found, suffix stripping rules are used to detect the stem. Adam et al. [1] apply PoS tagging and then use a rule-based stemmer to strip off the suffix from the word based on its tag in a pipelineframework."}, {"heading": "3 Model & Algorithm", "text": "We define a joint PoS tagger and stemmer that extends the fully Bayesian PoS tagger by Goldwater and Griffiths [10]. By joining PoS tagging and stemming, we aim to reduce the sparsity in PoS tagging for agglutinative languages while also improving the stemming accuracy using the tag information.\n3.1 Word-based Bayesian HMM Model\nThe word-based Bayesian HMM model (Goldwater and Griffiths [10]) for PoS tagging is defined as follows (see Fig. 1):\n\ud835\udc61\ud835\udc56|\ud835\udc61\ud835\udc56\u22121 = \ud835\udc61, \ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 ) \u221d \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 )) (1)\n\ud835\udc64\ud835\udc56|\ud835\udc61\ud835\udc56 = \ud835\udc61, \ud835\udf14(\ud835\udc61) \u221d \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf14(\ud835\udc61)) (2)\n\ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 )|\ud835\udefc \u221d \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefc) (3)\n\ud835\udf14(\ud835\udc61)|\ud835\udefd \u221d \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefd) (4)\nwhere \ud835\udc64\ud835\udc56 denotes the ith word in the corpus and \ud835\udc61\ud835\udc56 is its tag. \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf14\ud835\udc61) is the emission distribution in the form of a Multinomial distribution with parameters \ud835\udf14(\ud835\udc61) that are generated by \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefd) with hyperparameters \ud835\udefd. Analogously, \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 )) is the transition distribution with parameters \ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 ) that are generated by \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefc) with hyperpameters \ud835\udefc. The conditional distribution of \ud835\udc61\ud835\udc56 under this model is:\n\ud835\udc43 (\ud835\udc61\ud835\udc56|\ud835\udc61\u2212\ud835\udc56, \ud835\udc64, \ud835\udefc, \ud835\udefd) = \ud835\udc5b(\ud835\udc61\ud835\udc56,\ud835\udc64\ud835\udc56) + \ud835\udefd \ud835\udc5b\ud835\udc61\ud835\udc56 + \ud835\udc4a\ud835\udc61\ud835\udc56\ud835\udefd \u00b7 \ud835\udc5b(\ud835\udc61\ud835\udc56\u22121,\ud835\udc61\ud835\udc56) + \ud835\udefc \ud835\udc5b\ud835\udc61\ud835\udc56\u22121 + \ud835\udc47\ud835\udefc\n(5)\n\u00b7 \ud835\udc5b(\ud835\udc61\ud835\udc56,\ud835\udc61\ud835\udc56+1)+\ud835\udc3c(\ud835\udc61\ud835\udc56\u22121=\ud835\udc61\ud835\udc56=\ud835\udc61\ud835\udc56+1) + \ud835\udefc\n\ud835\udc5b\ud835\udc61\ud835\udc56 + \ud835\udc3c(\ud835\udc61\ud835\udc56\u22121 = \ud835\udc61\ud835\udc56) + \ud835\udc47\ud835\udefc\nwhere \ud835\udc4a\ud835\udc61\ud835\udc56 is the number of word types in the corpus, \ud835\udc47 is the size of the tag set, \ud835\udc5b\ud835\udc61\ud835\udc56 is the number of words tagged with \ud835\udc61\ud835\udc56, \ud835\udc5b(\ud835\udc61\ud835\udc56\u22121,\ud835\udc61\ud835\udc56) is the frequency of tag bigram < \ud835\udc61\ud835\udc56\u22121, \ud835\udc61\ud835\udc56 >. \ud835\udc3c(.) is a function that gives 1 if its argument is true, and otherwise 0.\n3.2 Stem-based Bayesian HMM Model\nWe extend the basic HMM model for PoS tagging introduced by Goldwater and Griffiths [10] by replacing the word emissions with stem emissions in order to reduce the emission sparsity, thereby mitigating the size of the out-of-vocabulary words. Therefore, we obtain a joint PoS tagger and stemmer with this model.\nThe stem-based model is defined as follows (see Fig. 2):\n\ud835\udc61\ud835\udc56|\ud835\udc61\ud835\udc56\u22121 = \ud835\udc61, \ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 ) \u221d \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 )) (6)\n\ud835\udc60\ud835\udc56|\ud835\udc61\ud835\udc56 = \ud835\udc61, \ud835\udf14(\ud835\udc61) \u221d \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf14(\ud835\udc61)) (7)\n\ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 )|\ud835\udefc \u221d \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefc) (8)\n\ud835\udf14(\ud835\udc61)|\ud835\udefd \u221d \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefd) (9)\nHere, \ud835\udc61\ud835\udc56 and \ud835\udc60\ud835\udc56 are the ith tag and stem, where \ud835\udc64\ud835\udc56 = \ud835\udc60\ud835\udc56 + \ud835\udc5a\ud835\udc56, \ud835\udc5a\ud835\udc56 being the suffix of \ud835\udc64\ud835\udc56.\nUnder this model, the conditional distribution of \ud835\udc61\ud835\udc56 becomes as follows:\n\ud835\udc43 (\ud835\udc61\ud835\udc56|\ud835\udc61\u2212\ud835\udc56, \ud835\udc60\ud835\udc56, \ud835\udefc, \ud835\udefd) = \ud835\udc5b(\ud835\udc61\ud835\udc56,\ud835\udc60\ud835\udc56) + \ud835\udefd \ud835\udc5b\ud835\udc61\ud835\udc56 + \ud835\udc46\ud835\udc61\ud835\udefd \u00b7 \ud835\udc5b(\ud835\udc61\ud835\udc56\u22121,\ud835\udc61\ud835\udc56) + \ud835\udefc \ud835\udc5b\ud835\udc61\ud835\udc56\u22121 + \ud835\udc47\ud835\udefc\n(10)\n\u00b7 \ud835\udc5b(\ud835\udc61\ud835\udc56,\ud835\udc61\ud835\udc56+1)+\ud835\udc3c(\ud835\udc61\ud835\udc56\u22121=\ud835\udc61\ud835\udc56=\ud835\udc61\ud835\udc56+1) + \ud835\udefc\n\ud835\udc5b\ud835\udc61\ud835\udc56 + \ud835\udc3c(\ud835\udc61\ud835\udc56\u22121 = \ud835\udc61\ud835\udc56) + \ud835\udc47\ud835\udefc\nwhere \ud835\udc46\ud835\udc61 is the number of stem types in the corpus. When compared to the word-based model, the number of word types reduces to stem types. Therefore, sparsity also decreases.\n3.3 Stem/Suffix-based Bayesian HMM Model\nWords belonging to the same syntactic category take also similar suffixes. For example, words ending with ly are usually adverbs, whereas words ending with ness are usually nouns. We include suffixes in the emissions in addition to the stems (see Fig. 3):\n\ud835\udc61\ud835\udc56|\ud835\udc61\ud835\udc56\u22121 = \ud835\udc61, \ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 ) \u221d \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 )) (11)\n\ud835\udc60\ud835\udc56|\ud835\udc61\ud835\udc56 = \ud835\udc61, \ud835\udf14(\ud835\udc61) \u221d \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf14(\ud835\udc61)) (12)\n\ud835\udc5a\ud835\udc56|\ud835\udc61\ud835\udc56 = \ud835\udc61, \ud835\udf13(\ud835\udc61) \u221d \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf13(\ud835\udc61)) (13)\n\ud835\udf0f (\ud835\udc61,\ud835\udc61 \u2032 )|\ud835\udefc \u221d \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefc) (14)\n\ud835\udf14\ud835\udc61|\ud835\udefd \u221d \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefd) (15) \ud835\udf13(\ud835\udc61)|\ud835\udefe \u221d \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefe) (16)\nwhere \ud835\udc5a\ud835\udc56 is the suffix of \ud835\udc64\ud835\udc56 = \ud835\udc60\ud835\udc56 + \ud835\udc5a\ud835\udc56 which is generated by \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61(\ud835\udf13(\ud835\udc61)) with parameters drawn from \ud835\udc37\ud835\udc56\ud835\udc5f\ud835\udc56\ud835\udc50\u210e\ud835\udc59\ud835\udc52\ud835\udc61(\ud835\udefe) with hyperparameters \ud835\udefe.\nThe new conditional distribution of \ud835\udc61\ud835\udc56 becomes:\n\ud835\udc43 (\ud835\udc61\ud835\udc56|\ud835\udc61\u2212\ud835\udc56, \ud835\udc60\ud835\udc56, \ud835\udc5a\ud835\udc56, \ud835\udefc, \ud835\udefd, \ud835\udefe) = \ud835\udc5b(\ud835\udc61\ud835\udc56,\ud835\udc60\ud835\udc56) + \ud835\udefd \ud835\udc5b\ud835\udc61\ud835\udc56 + \ud835\udc46\ud835\udc61\ud835\udefd \u00b7 \ud835\udc5b(\ud835\udc61\ud835\udc56,\ud835\udc5a\ud835\udc56) + \ud835\udefe \ud835\udc5b\ud835\udc61\ud835\udc56 + \ud835\udc40\ud835\udc61\ud835\udefe \u00b7 \ud835\udc5b(\ud835\udc61\ud835\udc56\u22121,\ud835\udc61\ud835\udc56) + \ud835\udefc \ud835\udc5b\ud835\udc61\ud835\udc56\u22121 + \ud835\udc47\ud835\udefc\n\u00b7 \ud835\udc5b(\ud835\udc61\ud835\udc56,\ud835\udc61\ud835\udc56+1)+\ud835\udc3c(\ud835\udc61\ud835\udc56\u22121=\ud835\udc61\ud835\udc56=\ud835\udc61\ud835\udc56+1) + \ud835\udefc\n\ud835\udc5b\ud835\udc61\ud835\udc56 + \ud835\udc3c(\ud835\udc61\ud835\udc56\u22121 = \ud835\udc61\ud835\udc56) + \ud835\udc47\ud835\udefc (17)\nwhere \ud835\udc40\ud835\udc61 is the number of suffix types in the corpus."}, {"heading": "4 Inference", "text": "We use Gibbs sampling [8] for the inference of the model. t are drawn from the posterior distribution \ud835\udc43 (t|w, \ud835\udefc, \ud835\udefd) \u221d \ud835\udc43 (w|t, \ud835\udefd)\ud835\udc43 (t|\ud835\udefc) in the word-based Bayesian HMM model, \ud835\udc43 (t|s, \ud835\udefc, \ud835\udefd) \u221d \ud835\udc43 (s|t, \ud835\udefd)\ud835\udc43 (t|\ud835\udefc) in the stem-based model, and \ud835\udc43 (t|s,m, \ud835\udefc, \ud835\udefd, \ud835\udefe) \u221d \ud835\udc43 (s|t, \ud835\udefd)\ud835\udc43 (m|t, \ud835\udefe)\ud835\udc43 (t|\ud835\udefc) in the stem/suffix-based model.\nIn the word-based Bayesian HMM model, all tags are randomly initialized at the beginning of the inference. Then each word\u2019s tag is sampled from the model\u2019s posterior distribution given in Equation 5. This process is repeated until the system converges.\nIn the stem-based and stem/suffix-based model, all tags are randomly initialized and all words are split into two segments randomly. In each iteration of the algorithm, a tag and a stem are sampled for each word from the posterior distribution given in Equation 10 and Equation 17 respectively."}, {"heading": "5 Experiments & Results", "text": "Data: We used three datasets for the experiments and evaluation:\n\u2013 Turkish: METU-Sabanc\u0131 Turkish Treebank [20] that consists of 53751 word tokens. \u2013 English: The first 12K and 24K words from the WSJ Penn Treebank [15]. \u2013 Finnish: The first 12k and 24k words from FinnTreeBank corpus that is a\nrevised version of the original FTB11.\nThere are 41 tags in both Penn Treebank and METU-Sabanc\u0131 Turkish Treebank, and 12 tags in FinnTreeBank. We mapped the three tagsets to the Universal tagset [23] that involves 12 categories. The new tagsets for Turkish, English and Finnish are given in Table 1 and Table 2. Therefore, the size of the tagset is 12 in all experiments for three languages.\nWe ran each model with four settings of parameters. In the first setting, we assigned \ud835\udefc =0.001, \ud835\udefd=0.1, and \ud835\udefe=0.001 (indicated as setting 1 in the tables); in the second setting, \ud835\udefc =0.003, \ud835\udefd=1, and \ud835\udefe=0.003 (indicated as setting 2 in the tables); in the third setting, \ud835\udefc =0.001, \ud835\udefd=0.1, and \ud835\udefe=0.001 (indicated as setting 3 in the tables); and in the fourth setting we assigned \ud835\udefc =0.003, \ud835\udefd=1, and \ud835\udefe=0.003 (indicated as setting 4 in the tables).\nThe stemming results are evaluated based on the accuracy measure. We compare our stemming results obtained from the stem-based Bayesian HMM (Bayesian S-HMM) and stem/suffix-based HMM (Bayesian SM-HMM) with HPS [5] and FlatCat [11]. The results for Turkish and Finnish obtained from the Metu-Sabanc\u0131 Turkish Treebank and FinnTreeBank respectively are given in Table 3. Although our stemming results are far behind the results of the HPS algorithm for Turkish, our Finnish results are on a par with HPS and Morfessor\n1 Available at http://www.ling.helsinki.fi/kieliteknologia/tutkimus/treebank/sources/\nFlatCat. The results show that using suffixes does not help in stemming. Using stem emissions alone gives the best accuracy for stemming in the joint task.\nSince the English stems are not covered in Penn TreeBank, we were not able to evaluate the English stemming results.\nExamples to correct and incorrect stems in all languages are given in Table 4. The results show that our joint model can find the common endings, such as s, ed, ted, er, d, e, ing in English. However, since we do not exploit any semantic features in the model, words such as filter can be stemmed as filt+er. This is also one of the main problems in morphological segmentation models that rely only on the orthographic features. Our stemming results are promising, but it shows that it is not sufficient to reduce the sparsity based on the common segments and it requires more features.\nWe evaluate PoS tagging results with many-to-one accuracy and variation of information (VI) measure [27]. Turkish, English and Finnish results are given in Table 5, Table 6, and Table 7 respectively. The overall results show that using stems rather than words leads to better results in three languages. Therefore, the Bayesian S-HMM model outperforms other two models in three languages in general. Although English has got a poor morphology when compared to Turkish and Finnish, the Bayesian S-HMM model still outperforms other two models. Using suffixes also does not help in PoS tagging and its scores are generally behind the Bayesian S-HMM model. However, in some parameter settings Bayesian SM-HMM model outperforms other two Bayesian models.\nThe overall PoS tagging results show that our stem-based and stem/suffixbased Bayesian models outperform both Brown Clustering [4] and word-based Bayesian HMM model [10] for three languages according to both many-to-one measure and VI measure.\n2 Morfessor FlatCat: https://github.com/aalto-speech/flatcat 3 HPS: http://liks.fav.zcu.cz/HPS/ 4 Brown Clustering: http://www.cs.berkeley.edu/~pliang/software/brown-cluster-1.2.\nzip(Percy Liang)"}, {"heading": "6 Conclusion & Future Work", "text": "In this paper, we extend the Bayesian HMM model [10] for joint learning of PoS tags and stems in a fully unsupervised framework. Our model reduces the sparsity by using stems and suffixes instead of words in a HMM model. The results show that using stems and suffixes rather than words outperforms a simple word-based Bayesian HMM model for especially agglutinative languages such as Turkish and Finnish. Although English has got a poor morphology, the English PoS tagging results are also better when the stems are used instead of words.\nAlthough our Turkish stemming results are far behind the other compared models, our Finnish stemming results are on par with other models.\nWe aim to use other features (such as semantic features) in our model to capture the semantic similarity between the stems and their derived forms, which is left as a future work.\nOur model does not deal with irregular word forms. We also leave this as a future work."}, {"heading": "Acknowledgments", "text": "This research is supported by the Scientific and Technological Research Council of Turkey (TUBITAK) with the project number EEEAG-115E464."}], "references": [{"title": "An efficient mechanism for stemming and tagging: the case of Greek language", "author": ["G. Adam", "K. Asimakis", "C. Bouras", "V. Poulopoulos"], "venue": "International Conference on Knowledge-Based and Intelligent Information and Engineering Systems. pp. 389\u2013397. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised part-of-speech tagging employing efficient graph clustering", "author": ["C. Biemann"], "venue": "Proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics: student research workshop. pp. 7\u201312. Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Tnt: a statistical part-of-speech tagger", "author": ["T. Brants"], "venue": "Proceedings of the sixth conference on Applied natural language processing. pp. 224\u2013231. Association for Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics 18(4), 467\u2013479", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Hps: High precision stemmer", "author": ["T. Brychc\u00edn", "M. Konop\u00edk"], "venue": "Information Processing & Management 51(1), 68\u201391", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Inducing syntactic categories by context distribution clustering", "author": ["A. Clark"], "venue": "Proceedings of the 2nd workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning-Volume 7. pp. 91\u201394. Association for Computational Linguistics", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparison of Bayesian estimators for unsupervised hidden Markov model PoS taggers", "author": ["J. Gao", "M. Johnson"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 344\u2013352. Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (6), 721\u2013741", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1984}, {"title": "Unsupervised learning of the morphology of a natural language", "author": ["J. Goldsmith"], "venue": "Computational linguistics 27(2), 153\u2013198", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "A fully Bayesian approach to unsupervised part-ofspeech tagging", "author": ["S. Goldwater", "T. Griffiths"], "venue": "Annual meeting-association for computational linguistics. vol. 45, p. 744. Citeseer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Morfessor FlatCat: An HMMbased method for unsupervised and semi-supervised learning of morphology", "author": ["S.A. Gr\u00f6nroos", "S. Virpioja", "P. Smit", "M. Kurimo"], "venue": "COLING. pp. 1177\u20131185", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Finite state morphology and left to right phonology", "author": ["J. Hankamer"], "venue": "Proceedings of the West Coast Conference on Formal Linguistics. vol. 5, pp. 41\u201352", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1986}, {"title": "Why doesn\u2019t EM find good HMM PoS-taggers? In: EMNLP-CoNLL", "author": ["M. Johnson"], "venue": "pp. 296\u2013305", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Development of a stemming algorithm", "author": ["J.B. Lovins"], "venue": "Mechanical Translation and Computational Linguistics 11", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1968}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics 19(2), 313\u2013330", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "Single n-gram stemming", "author": ["J. Mayfield", "P. McNamee"], "venue": "Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval. pp. 415\u2013416. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "A novel method for stemmer generation based on hidden Markov models", "author": ["M. Melucci", "N. Orio"], "venue": "Proceedings of the Twelfth International Conference on Information and Knowledge Management. pp. 131\u2013138. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Tagging English text with a probabilistic model", "author": ["B. Merialdo"], "venue": "Computational Linguistics 20(2), 155\u2013171", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "Maulik: An effective stemmer for Hindi language", "author": ["U. Mishra", "C. Prakash"], "venue": "International Journal on Computer Science and Engineering 4(5), 711", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Building a Turkish treebank", "author": ["K. Oflazer", "B. Say", "D.Z. Hakkani-T\u00fcr", "G. T\u00fcr"], "venue": "Treebanks, pp. 261\u2013277. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Gras: An effective and efficient stemming algorithm for information retrieval", "author": ["J.H. Paik", "M. Mitra", "S.K. Parui", "K. J\u00e4rvelin"], "venue": "ACM Transactions on Information Systems (TOIS) 29(4), 19", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A novel corpus-based stemming algorithm using co-occurrence statistics", "author": ["J.H. Paik", "D. Pal", "S.K. Parui"], "venue": "Proceedings of the 34th international ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 863\u2013872. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "arXiv preprint arXiv:1104.2086", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Hybrid stemmer for Gujarati", "author": ["P.P.K. Popat", "P. Bhattacharyya"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics. pp. 51\u201355", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program 14(3), 130\u2013137", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1980}, {"title": "Snowball: A language for stemming algorithms", "author": ["M.F. Porter"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. vol. 7, pp. 410\u2013420", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Part-of-speech induction from scratch", "author": ["H. Sch\u00fctze"], "venue": "Proceedings of the 31st Annual Meeting on Association for Computational Linguistics. pp. 251\u2013258. Association for Computational Linguistics", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1993}, {"title": "Unsupervised multilingual learning for PoS tagging", "author": ["B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 1041\u20131050. Association for Computational Linguistics", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "The infinite HMM for unsupervised PoS tagging", "author": ["J. Van Gael", "A. Vlachos", "Z. Ghahramani"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2. pp. 678\u2013687. Association for Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Corpus-based stemming using cooccurrence of word variants", "author": ["J. Xu", "W.B. Croft"], "venue": "ACM Transactions on Information Systems (TOIS) 16(1), 61\u201381", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 11, "context": "Hankamer [12] claims that the number of various word forms in an agglutinative language like Turkish is theoretically infinite.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "[4] introduce a class-based n-gram model that learns either syntactic or semantic classes of words depending on the adopted language model; Sch\u00fctze [28] classifies the vector representation of words using neural networks to learn syntactic categories; Clark [6] proposes a probabilistic context distributional clustering to cluster words occurring in similar contexts, thereby having similar syntactic features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[4] introduce a class-based n-gram model that learns either syntactic or semantic classes of words depending on the adopted language model; Sch\u00fctze [28] classifies the vector representation of words using neural networks to learn syntactic categories; Clark [6] proposes a probabilistic context distributional clustering to cluster words occurring in similar contexts, thereby having similar syntactic features.", "startOffset": 148, "endOffset": 152}, {"referenceID": 5, "context": "[4] introduce a class-based n-gram model that learns either syntactic or semantic classes of words depending on the adopted language model; Sch\u00fctze [28] classifies the vector representation of words using neural networks to learn syntactic categories; Clark [6] proposes a probabilistic context distributional clustering to cluster words occurring in similar contexts, thereby having similar syntactic features.", "startOffset": 258, "endOffset": 261}, {"referenceID": 1, "context": "Bienmann [2] introduces a graph clustering algorithm as a PoS tagger.", "startOffset": 9, "endOffset": 12}, {"referenceID": 17, "context": "HMM-based PoS tagging models go back to Merialdo [18].", "startOffset": 49, "endOffset": 53}, {"referenceID": 2, "context": "Trigrams\u2019n\u2019Tags (TnT) [3] is another statistical PoS tagger that uses a second order Markov Model with also maximum likelihood estimation.", "startOffset": 22, "endOffset": 25}, {"referenceID": 12, "context": "Two tasks are jointly performed to provide a mutual benefit in bJohnson [13] compares the estimators used in HMM PoS taggers.", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "[7] also compare different Bayesian estimators for HMM PoS taggers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Goldwater and Griffiths [10] adopt Bayesian learning in HMMs.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "[30] and Synder et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] introduce infinite HMMs that are non-parametric Bayesian where the number of states is not set and the states can grow with data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The oldest stemmers are rule-based [14,25,26].", "startOffset": 35, "endOffset": 45}, {"referenceID": 24, "context": "The oldest stemmers are rule-based [14,25,26].", "startOffset": 35, "endOffset": 45}, {"referenceID": 25, "context": "The oldest stemmers are rule-based [14,25,26].", "startOffset": 35, "endOffset": 45}, {"referenceID": 30, "context": "One of the earliest statistical stemmers is developed by Xu and Croft [31].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Mayfield and McNamee [16] propose a language independent n-gram stemmer.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "[17] implement a HMM-based stemmer using ML estimate to select the most likely stem and suffix based on the substring frequencies obtained from a corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Linguistica [9], although being an unsupervised morphological segmentation system, is also used as a stemmer.", "startOffset": 12, "endOffset": 15}, {"referenceID": 20, "context": "[21] that groups words to find suffix pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Another unsupervised stemmer of the same authors [22] use co-occurence statistics of words to find the common prefixes.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "HPS [5] is one of the recent unsupervised stemmers that exploits lexical and semantic information to prepare large-scale training data in the first state, and use a maximum entropy classifier in the second stage by using the training data obtained from the first stage.", "startOffset": 4, "endOffset": 7}, {"referenceID": 23, "context": "[24] propose a hybrid stemmer for Gujarati that combines statistical and rule-based models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "MAULIK [19] introduce another hybrid stemmer that combines the rule-based stemmers.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "[1] apply PoS tagging and then use a rule-based stemmer to strip off the suffix from the word based on its tag in a pipelineframework.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "We define a joint PoS tagger and stemmer that extends the fully Bayesian PoS tagger by Goldwater and Griffiths [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "The word-based Bayesian HMM model (Goldwater and Griffiths [10]) for PoS tagging is defined as follows (see Fig.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "We extend the basic HMM model for PoS tagging introduced by Goldwater and Griffiths [10] by replacing the word emissions with stem emissions in order to reduce the emission sparsity, thereby mitigating the size of the out-of-vocabulary words.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "We use Gibbs sampling [8] for the inference of the model.", "startOffset": 22, "endOffset": 25}, {"referenceID": 19, "context": "\u2013 Turkish: METU-Sabanc\u0131 Turkish Treebank [20] that consists of 53751 word tokens.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "\u2013 English: The first 12K and 24K words from the WSJ Penn Treebank [15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "We mapped the three tagsets to the Universal tagset [23] that involves 12 categories.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "We compare our stemming results obtained from the stem-based Bayesian HMM (Bayesian S-HMM) and stem/suffix-based HMM (Bayesian SM-HMM) with HPS [5] and FlatCat [11].", "startOffset": 144, "endOffset": 147}, {"referenceID": 10, "context": "We compare our stemming results obtained from the stem-based Bayesian HMM (Bayesian S-HMM) and stem/suffix-based HMM (Bayesian SM-HMM) with HPS [5] and FlatCat [11].", "startOffset": 160, "endOffset": 164}, {"referenceID": 4, "context": "Model Metu Finn 12K Finn 24K HPS 3 [5] 53.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "04 Morfessor FlatCat [11] 52.", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "58 Brown Clustering 4 [4] 54.", "startOffset": 22, "endOffset": 25}, {"referenceID": 26, "context": "We evaluate PoS tagging results with many-to-one accuracy and variation of information (VI) measure [27].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The overall PoS tagging results show that our stem-based and stem/suffixbased Bayesian models outperform both Brown Clustering [4] and word-based Bayesian HMM model [10] for three languages according to both many-to-one measure and VI measure.", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "The overall PoS tagging results show that our stem-based and stem/suffixbased Bayesian models outperform both Brown Clustering [4] and word-based Bayesian HMM model [10] for three languages according to both many-to-one measure and VI measure.", "startOffset": 165, "endOffset": 169}, {"referenceID": 3, "context": "17 Brown Clustering 4 [4] 53.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "14 Brown Clustering 4 [4] 44.", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "In this paper, we extend the Bayesian HMM model [10] for joint learning of PoS tags and stems in a fully unsupervised framework.", "startOffset": 48, "endOffset": 52}], "year": 2017, "abstractText": "The number of word forms in agglutinative languages is theoretically infinite and this variety in word forms introduces sparsity in many natural language processing tasks. Part-of-speech tagging (PoS tagging) is one of these tasks that often suffers from sparsity. In this paper, we present an unsupervised Bayesian model using Hidden Markov Models (HMMs) for joint PoS tagging and stemming for agglutinative languages. We use stemming to reduce sparsity in PoS tagging. Two tasks are jointly performed to provide a mutual benefit in both tasks. Our results show that joint POS tagging and stemming improves PoS tagging scores. We present results for Turkish and Finnish as agglutinative languages and English as a morphologically poor language.", "creator": "LaTeX with hyperref package"}}}