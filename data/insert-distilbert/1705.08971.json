{"id": "1705.08971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Optimal Cooperative Inference", "abstract": "cooperative transmission of data eventually fosters rapid accumulation of knowledge by efficiently combining new experience across learners. although well studied in human learning, there has been less attention to cooperative transmission of data in machine learning, education and we consequently lack strong formal frameworks through which we may reason about the benefits and limitations of cooperative inference. we present such a framework. we consequently introduce a novel index for measuring the effectiveness of probabilistic information transmission, and cooperative information transmission work specifically. we relate our cooperative index to previous measures of teaching in deterministic instruction settings. we prove conditions broadly under which optimal cooperative inference can be achieved, including a representation theorem which constrains the form determination of inductive biases for more learners optimized for cooperative inference. we conclude by demonstrating strongly how these principles may inform the design of machine learning algorithms and discuss implications for human learning, machine learning, and human - machine learning systems.", "histories": [["v1", "Wed, 24 May 2017 21:42:00 GMT  (356kb,D)", "http://arxiv.org/abs/1705.08971v1", "14 pages (5 pages of Supplementary Material), 1 figure"]], "COMMENTS": "14 pages (5 pages of Supplementary Material), 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["scott cheng-hsin yang", "yue yu", "arash givchi", "pei wang", "wai keen vong", "patrick shafto"], "accepted": false, "id": "1705.08971"}, "pdf": {"name": "1705.08971.pdf", "metadata": {"source": "CRF", "title": "Optimal Cooperative Inference", "authors": ["Scott Cheng-Hsin Yang", "Yue Yu", "Arash Givchi", "Pei Wang", "Wai Keen Vong"], "emails": ["scott.cheng.hsin.yang@gmail.com", "patrick.shafto@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Human learning is characterized by the cooperative transmission of data [21]. In addition to direct observations and taking actions in one\u2019s own environment, humans also engage in purposeful selection of data whose goal is conveying knowledge about the world to less knowledgeable agents. Moreover, less knowledgeable agents assume purposeful, cooperative selection and leverage cooperation to augment learning. The cooperative selection of data, and learning from such data, plays a central role in theories of cognition [1], cognitive development [11], and cultural evolution [22]. Indeed, this cooperative inference is argued to be the feature that drives accumulation of knowledge over generations [23, 6].\nSuch communication through cooperative inference has received relatively limited attention in machine learning. The principles that appear to drive human cultural knowledge accumulation may also be leveraged in machines to achieve similar ends. A recent effort in this direction is machine teaching [25, 10], which formalizes how to translate and communicate model inferences using data. While this development begins to address how communication between learning models could occur, it is mute on the effectiveness of communication between models, and does not formalize cooperative inference by both the teacher and the learner.\nIn this paper, we address this lack by introducing a measure of communication effectiveness in the cooperative setting. The role that this measure plays in cooperative knowledge accumulation is analogous to the role that training and test errors play in traditional machine learning. We also use the measure to extend the teaching dimension [12, 26]\u2014a classical measure of communication efficiency1\u2014from deterministic to probabilistic settings. We show how analyzing this measure reveals the conditions, in terms of constraints on learning model\u2019s inductive biases, under which cooperation may produce optimal communication.\nThe paper is organized as follows: In Section 2, we first introduce a Transmission Index that quantifies communication effectiveness for any pair of probabilistic inference and data selection processes. In Section 3,\n\u2217scott.cheng.hsin.yang@gmail.com \u2020patrick.shafto@gmail.com 1Effectiveness is a measure of the quality of communication; efficiency is the size of the data necessary to reach a particularly\neffectiveness.\nar X\niv :1\n70 5.\n08 97\n1v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\nwe make connection between this index and the Average Teaching Dimension, thereby connecting our measure of effectiveness with previous measures of efficiency. In Sections 4, we introduce cooperative inference based on previous research in human social learning [15, 17], present a Cooperative Index by extend the Transmission index to the cooperative setting, and identify the condition that must be satisfied to achieve optimal communication. In Section 5, we conclude with connections to human, machine, and human-machine learning."}, {"heading": "2 The Transmission Index", "text": "In this section, we define Transmission Index to quantify the communication effectiveness. Communication occurs between two agents, which we call a teacher and a learner. Here the teacher represents the process of selecting data to convey a particular concept, and the learner represents the inference process of interpreting the received data. In a probabilistic setting, the effectiveness of communication is related to the probability that the learner\u2019s interpretation matches the teacher\u2019s intended concept.\nDefinition 2.1. Let h be a concept in a finite concept space H. A data set space, D, is a collection of subsets of a given finite set of data points. D \u2208 D is called a data set. Further, let PT(D|h) be the teacher\u2019s probability of selecting a data set D for communicating a given concept h and PL(h|D) be the learner\u2019s posterior for h given data set D. We denote the size of H and D by |H| and |D|, respectively.\nBecause H and D are both discrete, in matrix notation, we can form the row-stochastic learner\u2019s inference matrix, L \u2208 [0, 1]|D|\u00d7|H|, having elements PL(h|D), and the column-stochastic teacher\u2019s selection matrix, T \u2208 [0, 1]|D|\u00d7|H|, having elements PT(D|h).\nDefinition 2.2. The Transmission Index (TI) is defined as\nTI(L,T) = 1\n|H| |H|\u2211 j=1 |D|\u2211 i=1 Li,jTi,j .\nIn connection to information theory, the Transmission Index can be understood as a measure of the effectiveness of information through coding information, sending the code, and then decoding. In our context, the channel is not arbitrary but is restricted to be the data, i.e., information is transmitted by passing data. Similarly, the decoding and coding are not arbitrary but correspond to the inference process and data selection process, and does not need to be agreed upon beforehand.\nNow we give a few examples to show that TI captures how well on average a concept in a given concept space can be communicated with a given data set space. Also, note that in the case where H and D are clear from the context, we represent TI(L,T) simply by TI.\nExample 2.3. Let |D| = |H| = 2. Consider this teacher\u2019s selection matrix, T = (\n1 0 0 1\n) , and these three\nlearner\u2019s inference matrices, L(a) = ( 1 0 0 1 ) , L(b) = ( 0 1 1 0 ) , and L(c) = ( 1/2 1/2 1/2 1/2 ) .\nIn the first case (a), TI(L(a),T) = 1, because the concept that the teacher intends to teach through a certain data set matches perfectly what the learner would infer given that data set. In the second case (b), TI(L(b),T) = 0, because the concept that the teacher intends to teach through a certain data set leads the learner to infer the other concept with certainty. In the last case (c), TI(L(c),T) = 12 . Here the learner\u2019s inference is ambiguous, and TI captures that. In summary, TI captures the expected probability that the learner will interpret the teacher\u2019s intention correctly.\nProposition 2.4. The range of the Transmission Index is 0 \u2264 TI \u2264 1, and TI = 1 if and only if two conditions hold: (i) Li,j = 1 if Ti,j > 0 for all i, j, and (ii) there is no zero column in L and T. Also, TI = 1 implies that |D| \u2265 |H|, with equality achieved when L and T are the same permutation matrix.\nProof. TI \u2265 0 because T and L are stochastic matrices, and TI = 0 if and only if for any i, j, either Li,j = 0 or Ti,j = 0.\nWe show TI \u2264 1:\nTI(L,T) = 1\n|H| |H|\u2211 j=1 |D|\u2211 i=1 Li,jTi,j \u2264 (a) 1 |H| |H|\u2211 j=1  |D|\u2211 i=1 Ti,j  \u2264 (b) 1 |H| |H|\u2211 j=1 1 = 1. (1)\nInequality (a) in (1) becomes an equality if and only if condition (i) is satisfied. This is because in order for Li,jTi,j = Ti,j , we need (Li,j \u2212 1)Ti,j = 0, and this implies that Li,j = 1 or Ti,j = 0, for any i, j. Inequality (b) in (1) follows from T being a column-stochastic matrix, and it becomes an equality if and only if condition (ii) is satisfied.\nGiven that L is a row-stochastic matrix, if Li,j = 1, then there is no other non-zero elements in row i. This means that there are at most |D| elements with value one in L; hence, by condition (i) the number of non-zero elements in T is at most |D|. Also, condition (ii) requires that the number of non-zero elements in T be at least |H|. Therefore, |D| \u2265 |H|, with equality achieved if and only if T has only one positive element for each column. Together with condition (i), this implies that L has at least one element with value one in each column. Because L is row-stochastic, this implies L is a permutation matrix. Condition (i) also implies that if Li,j < 1, then Ti,j = 0. Together with condition (ii), T is the same permutation matrix.\nRemark 2.5. TI is invariant under joint row and column permutations of L and T. When |H| = |D| and TI = 1, row and column exchangeability implies that L and T can always be arranged into an identity matrix of order |H|.\nRemark 2.6. TI is well-defined even when one of H and D is countably infinite; that is, the definition of TI can be modified to\nTI(L,T) := lim n\u2192|H| lim m\u2192|D|\n1\nn n\u2211 j=1 m\u2211 i=1 Li,jTi,j ,\nand the limit exists in those two cases. For the case where |D| \u2192 \u221e and |H| is finite, one can show that the above limit exists because TI increases with m and has an upper bound. For the case where |H| \u2192 \u221e and |D| is finite, TI(L,T) = 0 because there is always infinitely many untaught concepts given finitely many examples. However, when both H and D are countably infinite, TI is not generally well-defined. In particular, it is possible to construct a pair of L and T that causes the limit of TI to bounce in the interval [0, 1]. See Supplementary Material for full detail."}, {"heading": "3 Connection to Average Teaching Dimension", "text": "In this section, we make the connection between the Transmission Index and the Average Teaching Dimension. The Average Teaching Dimension is a variant of Teaching Dimension, a classic measure for quantifying the efficiency of teaching. The Teaching Dimension is well-studied; it has formal connections with the VC Dimension [12] and has been analyzed for certain models in continuous concept space [14] and in cooperative settings [26, 8]. However, Teaching Dimension and these analyses assume a deterministic the learning model and focus on efficiency rather than effectiveness. To make connection to the analysis of Teaching Dimension, we first extend the Transmission Index, a measure of effectiveness, to the Expected Teaching Dimension, a measure of efficiency. Then we show that the Expected Teaching Dimension, which is well-defined for probabilistic knowledge transmission, is the same as the Average Teaching Dimension as the knowledge transmission becomes deterministic.\nThe analyses of Teaching Dimension are typically couched in the concept learning framework. In this framework, a concept, h, is a function that maps an instance, x, to a label, y. By observing examples, pairs of (x, y), the learner can rule out concepts that are not consistent with the examples. With this notation, we can define the Average Teaching Dimension:\nDefinition 3.1 (Average Teaching Dimension). A concept h \u2208 H is consistent with a data set D if and only if for every data point (x, y) \u2208 D, h(x) = y. D \u2208 D is a teaching set for concept h \u2208 H if h, but no other concept in H, is consistent with D. Let D\u2217(h) \u2282 D be the collection of teaching sets in D for concept h. The classical version of Average Teaching Dimension [8] is defined as follows: First, for any h \u2208 H, let\nTD(h) = { \u221e if D\u2217(h) is empty minD\u2208D\u2217(h)|D| otherwise ,\nwhere |D| is the size of the data set D. Then, the Average Teaching Dimension (ATD) for the concept space H is\nATD(H) = 1 |H| \u2211 h\u2208H TD(h).\nExpected Teaching Dimension extends the Transmission Index to incorporate data set size as follows:\nDefinition 3.2. The Expected Teaching Dimension (ETD) is defined as ETD(H) = \u2211 h\u2208H \u2211 D\u2208D |D|PL(h|D)PT(D|h)\u2211\nh\u2208H \u2211 D\u2208D PL(h|D)PT(D|h) .\nDefinition 3.3. Let M \u2208 [0, 1]|D|\u00d7|H| be a matrix, where the element M i,j represents the probability that hi is consistent with Dj . We define C \u2208 {0, 1}|D|\u00d7|H| to be a consistency matrix, where Ci,j = 1 if hi is consistent with Dj and Ci,j = 0 otherwise. C can be sampled from M by treating Ci,j as the outcome of a Bernoulli trial with parameter M i,j .\nProposition 3.4. Let |H| = |D| = N , and C be a consistency matrix of size N \u00d7N . Let L and T be the the row-normalized and column-normalized matrices of C, respectively. Then, ATD(H) is finite if and only if TI(L,T) = 1.\nProof. ATD(H) is finite if and only if TD(h) is finite for all h \u2208 H. Finite TD(h) means that there is at least one teaching set D \u2208 D for h. Let \u03b1i \u2282 {1, 2, . . . , N} be the index set for the teaching sets of hi. Because every D can only belong to at most one D\u2217(hi), so \u03b1i \u2282 {1, \u00b7 \u00b7 \u00b7 , N}\\ \u222aj 6=i \u03b1j for every i \u2208 {1, 2, . . . , N}. Further because |D| = |H|, this construction of \u03b1i implies that if |\u03b1i| > 1 for some i, then there must exist at least one j 6= i with the property that |\u03b1j | = 0. However, because TD(hi) is finite, \u03b1i cannot be an empty set for any i. Hence, |\u03b1i| = 1 for all i. In particular, this implies that C is a permutation matrix. Thus, ATD(H) is finite if and only if C is a permutation matrix. C being a permutation matrix implies that C = L = T, which by Proposition 2.4 is equivalent to TI(L,T) = 1.\nExample 3.5. If L = T and is a permutation matrix, C = T. As we proved in Proposition 3.4, ETD is the same as ATD.\nExample 3.6. We give an example when ETD is finite but ATD is infinite in the probabilistic setting. Let |H| = |D| = 2, M = (\n1 1/2 0 1/2\n) . There are four possible consistency matrices that can be sampled from\nM : C(a) = ( 1 0 0 0 ) , C(b) = ( 1 0 0 1 ) , C(c) = ( 1 1 0 0 ) , C(d) = ( 1 1 0 1 ) . For C(a), C(c) and C(d), the corresponding ATD(H) is \u221e, and for C(b) it is |D1|+|D2|2 . Let L (\u2217) and T(\u2217) be the row-normalized and columnnormalized matrices of C(\u2217), respectively, for \u2217 \u2208 {a, b, c, d}. Then, TI(L(a),T(a)) = 12 , TI(L (b),T(b)) = 1, TI(L(c),T(c)) = 12 , and TI(L (d),T(d)) = 58 , with ETD(H) = |D1|, |D1|+|D2| 2 , |D1|, 3|D1|+2|D2| 5 , respectively. Thus, ETD can be seen as an generalization of ATD from scenarios of perfect transmission (TI = 1) to those of imperfect transmission (0 \u2264 TI \u2264 1) as well.\nIn Definition 3.3, probabilistic transmission is expressed as the sampling of consistency matrices. This corresponds to the view that there is a distribution of learning scenarios and the learner can be uncertain which scenario is selected. Another way probabilistic transmission can enter is that M represents the degree of consistency between data and hypotheses. In this case, the learner would need to make a decision on\nwhat the underlying true consistency matrix is. Consider M = ( 1 1/2 0 1/2 ) again. A simple decision rule is to round up M i,j to 1 if it exceeds a threshold and down to 0 otherwise. This decision rule would result in either C(a) or C(d), both of which correspond to ATD =\u221e."}, {"heading": "4 Optimal cooperative inference", "text": "The Transmission Index introduced in Section 2 assumes that the learner and teacher, or more abstractly, the inference process and the data selection process, are independent. However, communication for the transmission of knowledge is often cooperative (e.g., in pedagogy [9, 10] and conversations [13]). Here, cooperation implies that the teacher\u2019s selection of data depends on what the learner is likely to infer and vice versa. In this section, we formalize cooperative inference, which captures this inter-dependency between the two processes of inference and selection [15, 17]. It can be seen as a way to map a common convention to another one that is more effective at transmitting knowledge without a priori agreement on the encoding of data-concept pairs [26]. We define Cooperative Index as a measure of communication effectiveness in the cooperative setting by applying the Transmission Index to cooperative inference. Finally, we provide proofs regarding the form of the shared likelihood matrix required to maximize the cooperative index and optimize cooperative inference.\nDefinition 4.1. [Cooperative inference] Let D \u2208 D and h \u2208 H, we define cooperative inference as a system of two equations:\nPL(h|D) = PT(D|h)PL0(h)\nPL(D) (2a)\nPT(D|h) = PL(h|D)PT0(D)\nPT(h) , (2b)\nwhere PL(h|D) and PT(D|h) are defined in Definition 2.1, PL0(h) is the learner\u2019s prior of h, PT0(D) is the teacher\u2019s prior of selecting D, PL(D) = \u2211 h\u2208H PT(D|h)PL0(h) is the normalizing constant for PL(h|D), and\nPT(h) = \u2211 D\u2208D PL(h|D)PT0(D) is normalizing constant for PT(D|h).\nThe cooperative inference equations in (2) can be solved using fixed-point iteration [15, 17]: Define an initial likelihood, or common convention, PT(D|h) = P0(D|h), for the first evaluation of (2a). Then, given PL0(h) and PT0(D), one can evaluate (2a), use the resulting PL(h|D) to evaluate (2b), and iterate until convergence. By symmetry, the iteration can also begin with (2b). This symmetry implies that the initial likelihood matrix, M \u2208 [0, 1]|D|\u00d7|H| with elements P0(D|h), can be an arbitrary non-negative matrix because it always gets appropriately normalized in the first iteration.\nFor the remainder of the paper, we assume that PL0 and PT0 are uniform distributions over H and D, respectively. In this case, the the fixed-point iteration of (2) depends only on M and is simply the repetition of column and row normalization on M . Without loss of generality, we also assume that the iteration begins with (2a).\nDefinition 4.2. Let L(k) and T(k) be the matrices with elements PL(h|D) and PT(D|h), respectively, at the kth iteration of (2). If the iteration of (2) converges, we define L(\u221e) := limk\u2192\u221e L\n(k) and T(\u221e) := limk\u2192\u221eT (k).\nDefinition 4.3 (Cooperative Index, CI). Given M and assuming that the fixed-point iteration of (2) converges, we define the cooperative index as\nCI(M ) = TI(L(\u221e),T(\u221e)) = 1\n|H| |H|\u2211 j=1 |D|\u2211 i=1 L (\u221e) i,j T (\u221e) i,j .\nWe further assume that M is a square matrix unless otherwise stated. Then, the iteration of (2) becomes the well-known Sinkhorn-Knopp algorithm, which provably converges under certain conditions by Sinkhorn\u2019s theorem [19]. Below, we state a simpler version of Sinkhorn\u2019s theorem.\nDefinition 4.4 (Positive diagonal). If M is a n\u00d7n square matrix and \u03c3 is a permutation of {1, \u00b7 \u00b7 \u00b7 , n}, then a sequence of positive elements {M i,\u03c3(i)}ni=1 is called a positive diagonal. If \u03c3 is the identity permutation, the diagonal is called the main diagonal.\nTheorem 4.5 (A simpler version of Sinkhorn\u2019s theorem [19]). Given any nonnegative square matrix M with at least one positive diagonal, L(k) and T(k) in the fixed-point iteration of (2) converges to the same doubly stochastic matrix, M(\u221e), as k \u2192\u221e.\nProof. Here we provide a sketch of the proof (see Supplementary Material for full detail). We pick one positive diagonal. First we show the product of all elements on that diagonal is positive and upper-bounded by 1 throughout the fixed-point iteration of (2). Given uniform priors on both hypothesis and data set space, we then use inequality of arithmetic and geometric means to prove that the product either stay the same or increase throughout the iteration. Finally, monotone convergence theorem of real numbers guarantees that the product will converge to its supremum, at which point L and T must have converged to the same doubly stochastic matrix.\nAs is for TI, If M is clear from the context, we denote CI(M ) simply by CI for brevity. Now, we give two simple examples: The first demonstrates the fixed-point iteration of (2); the second compares cooperative inference with machine teaching [25].\nExample 4.6. Let M = ( 1 1 0 1 ) , then L(k) = ( 1\u2212 12k 1 2k 0 1 ) and T(k) = ( 1 12k+1 0 1\u2212 12k+1 ) . Notice that zero elements remain zero throughout the iteration process, but non-zero elements may converge to zero. Since\nL(\u221e) and T(\u221e) are both the identity matrix, CI = 1. In contrast, after one iteration of (2), L(1) = ( 1/2 1/2 0 1 ) ,\nT(1) = ( 1 1/3 0 2/3 ) , and TI(L(1),T(1)) is only 23 . Similarly, for any k, TI(L (k),T(k)) < 1. Thus, cooperative inference increases the effectiveness of communication.\nExample 4.7. We apply cooperative index to examples previously used in human teaching [4] and machine teaching [14], and compare the effectiveness of cooperative inference and other communication protocols.\nThe first example [4, 5, 16] tested children\u2019s learning of hidden functions of a novel toy after observing data presented by an experimenter. Empirical results [4, 5] have shown that when the experimenter activated one function accidentally (accidental demonstration), children inferred the toy to afford at least one function, and they also explored possible additional functions. However, when the experimenter activated one function pedagogically (pedagogical demonstration), children inferred that the toy afford one and only one function, which restrained their exploration of additional functions.\nThese results can be explained as two different communication protocols having different effectiveness. In the case of accidental demonstration (A), the learner assigns equal likelihood to all hypotheses that are consistent with the observed data, and assumes the experimenter to randomly choose a random subset of functions to demonstrate. The experiment considered three concepts (the novel toy has zero, one, or two functions) and three data sets (zero, one, or two functions demonstrated). Given this order for the concepts\nand data sets, L(A) = 1/3 1/3 1/30 1/2 1/2 0 0 1  and T(A) = 1 1/2 1/30 1/2 1/3 0 0 1/3 . This communication protocol entails much uncertainty for the correspondence between data and hypothesis, which is captured by a low TI: TI(L(A),T(A)) = 0.453. In contrary, under pedagogical demonstration (P), both parties apply cooperative inference on the selection and interpretation of data [16], which leads to a perfectly effective communication protocol, TI(L(P ),T(P )) = CI(M = L(A)) = 1. This explains the empirical finding that children inferred the toy to afford exactly the number of functions demonstrated by the experimenter.\nThe second example [14] considers a version-space learner who is trying to learn a threshold classifier h\u03b8, \u03b8 \u2208 {1 . . . 3}. For x \u2208 {0 . . . 3}, h\u03b8 returns y = \u2212 if x < \u03b8 and y = + if x \u2265 \u03b8. Assume a teacher provides training sets D = {(x1, y1), (x2, y2)}, and the learner assigns the same likelihood to all hypotheses that are consistent with the data. Following [14], machine teaching chooses data that maximize the likelihood for the learner to infer the correct hypothesis:\nL = {x1, y1, x2, y2}\\h\u03b8 h1 h2 h3 {0,\u2212, 1,+} 1 0 0 {0,\u2212, 2,+} 1/2 1/2 0 {0,\u2212, 3,+} 1/3 1/3 1/3 {1,\u2212, 2,+} 0 1 0 {1,\u2212, 3,+} 0 1/2 1/2 {2,\u2212, 3,+} 0 0 1 , T(mt) = {x1, y1, x2, y2}\\h\u03b8 h1 h2 h3 {0,\u2212, 1,+} 1 0 0 {0,\u2212, 2,+} 0 0 0 {0,\u2212, 3,+} 0 0 0 {1,\u2212, 2,+} 0 1 0 {1,\u2212, 3,+} 0 0 0 {2,\u2212, 3,+} 0 0 1 .\nTI of L and T(mt) can be understood as the effectiveness of machine teaching as a communication protocol. When all data sets are considered (as shown above), the effectiveness of machine teaching is perfect, TI(L,T(mt)) = 1, which is higher than the effectiveness of cooperative inference, CI(M = L) = 0.734. However when data sets are restricted, such as when only considering the first three rows of the data above, the effectiveness of cooperative inference can be higher than that of machine teaching CI(L \u2032 ) = 1. TI(L,T(mt) \u2032 ) = 0.611.\nGiven the cooperative index, which quantifies the effectiveness of transmission for cooperative inference, we can prove conditions under which M maximizes CI:\nDefinition 4.8. A square matrix is triangular if it has a positive main diagonal, and has only zeros below (upper-triangular) or above (lower-triangular) the main diagonal.\nTheorem 4.9 (Representation theorem for cooperative inference). Let M be a nonnegative square matrix with at least one positive diagonal, then the following are equivalent.\n(a) The corresponding cooperative index is optimal, i.e. CI(M ) = 1;\n(b) M has exactly one positive diagonal;\n(c) M is a permutation of an upper-triangular matrix.\nProof. From Proposition 2.4 we know that CI(M ) = TI(M(\u221e),M(\u221e)) = 1 if and only if M(\u221e) is a permutation matrix. Since elements of M that lie in a positive diagonal do not tend to zero during cooperative inference [19] (i.e. if M i,j 6= 0 lies in a positive diagonal, then M(\u221e)i,j 6= 0), M(\u221e) is a permutation matrix if and only if M has exactly one positive diagonal. So we have (a) \u21d0\u21d2 (b). (b) \u21d0\u21d2 (c) is a fact of linear algebra which can be proved by induction on the dimension n of M (see Supplementary Material for full detail).\nTheorem 4.9 shows that in order to achieve optimal cooperative inference and thereby effective knowledge accumulation, the shared inductive bias should be one that constraints the form of M to be upper triangular (or permutation thereof). This in turn constraints the learner\u2019s likelihood function such that it applies zero probability to particular data-concept relationships. Below, we show an example of using CI to investigate the form of the likelihood that leads to optimal transmission effectiveness.\nExample 4.10. Consider polynomial regression. In order to have a triangular M , the likelihood must have finite support. We explore the behavior of CI under different likelihood functions, ranging from fat-tailed to compact. In particular, we explore the conditions under which the different distributions lead to optimal CI.\nLet {xi}6i=1 = {\u22121,\u22121, 0, 0, 1, 1} and {yi}6i=1 = {a,\u2212a,\u2206 + a,\u2206 \u2212 a, a,\u2212a}. \u2206/a can be viewed as a signal-to-noise ratio for a second-order polynomial. Let D = {D1, D2}, where D1 = {x1, . . . , x4, y1, . . . , y4} and D2 = {x1, . . . , x6, y1, . . . , y6}. Let H = {h1, h2}, where hi is a polynomial of order i with a likelihood function that defines the assumed noise distribution. The likelihood function is a q-Gaussian Nq(z;\u00b5) set to have unit variance. We construct the M via maximum likelihood as a function of \u2206 and a for q = (0, 1, 1.5). Briefly, we find the maximum-likelihood estimate of hi to Dj , then assign M i,j the likelihood produced by that estimate (see Supplementary Material for more details). Having obtained these M matrices, we iterate them according to (2) to explore the behavior of CI.\nIn Figure 1 we show the phase diagrams of CI for the three q-Gaussian distributions, which correspond to a compact (q = 0), normal (q = 1), and fat-tailed (q = 1.5) distributions respectively. This result shows that when the error likelihood is a compact distribution, there exists at least one setting of a such that CI = 1 for all \u2206 > 0. This is not the case when the error likelihood has infinite support, i.e. q = 1 or q = 1.5. As suggested by Theorem 4.9, modeling choices that yield M s that are closer to triangular, such as compact likelihood functions, can produce optimal cooperative inference."}, {"heading": "5 Discussion", "text": "Cooperative inference is central to accounts of human learning, language, and cultural evolution, but has not been deeply investigated in machine learning (but see [7]). We have presented a Transmission Index to\nquantify the effectiveness of knowledge transmission via data, connected to previous approaches focusing on efficiency, presented a Cooperative Index to quantify effectiveness of cooperative transmission, proven bidirectional conditions that relate inductive biases to optimal cooperative inference, and provided a simulation that illustrates how to modify classic learning models to improve cooperative transmission of knowledge.\nOur results inform theory across fields. For fields interested in human learning, our representation theorem for cooperative inference (Theorem 4.9) provides the first predictions about how cognition would be structured if it were optimized to facilitate transmission of knowledge in over generations. For fields interested in machine learning, there has been much interest in whether machine learning is interpretable or explainable (e.g., banking [20], medicine [18], self-driving cars [3]). We have provided a theoretical framework for evaluating the degree to which an algorithm is explainable, and presented one case where simple modifications may yield improved explanation by examples."}, {"heading": "6 Supplementary Material", "text": ""}, {"heading": "6.1 Details of Remark 2.6", "text": "Case 1. |D| \u2192 \u221e and |H| is finite. TI(L,T) is well-defined since\nlim m\u2192|D| m\u2211 i=1 Li,jTi,j \u2264 |D|\u2211 i=1 Ti,j = 1\nimplies that lim m\u2192|D| m\u2211 i=1 Li,jTi,j exists (limits exist for increasing sequences with upper bounds).\nCase 2. |H| \u2192 \u221e and |D| is finite. Similarly we have lim n\u2192|H| n\u2211 j=1 Li,jTi,j \u2264 |H|\u2211 j=1 Li,j = 1. Then\n0 \u2264 TI(L,T) = lim n\u2192|H|\n1\nn |D|\u2211 i=1 n\u2211 j=1 Li,jTi,j \u2264 lim n\u2192|H| 1 n |D|\u2211 i=1 1 \u2264 lim n\u2192|H| |D| n \u2192 0.\nSo TI(L,T) = 0 when |H| \u2192 \u221e. This is reasonable because in this case there is always infinitely untaught concepts given finitely many examples.\nCase 3. |H|, |D| \u2192 \u221e. TI may not exist. For example, let L = I\u221e be the identity matrix of infinite dimension and T = Diag(I2, J2, I4, J4, . . . Ink , Jnk , Ink+1 , Jnk+1 , . . . ) be a block diagonal matrix where Ini is the identity matrix of dimension ni, Jni is the matrix with 1\u2019s on its skew-diagonal and 0\u2019s elsewhere\nand n1 = 2, nk+1 = 2 k\u2211 i=1 ni. Denote Sn = 1 n n\u2211 j=1 |D|\u2211 i=1 Li,jTi,j . Then if TI exists, TI = lim n\u2192\u221e Sn. However, S2nk = 1\n2 , S2nk+nk+1 =\n3 4 for any k. Therefore, TI does not exist."}, {"heading": "6.2 Proof of Theorem 4.5", "text": "For convenience, we first write the fixed-point iteration of (2) explicitly in vector form. We denote the matrix with elements PL(h|D) by L \u2208 [0, 1]|D|\u00d7|H|, the matrix with elements PT(D|h) by T \u2208 [0, 1]|D|\u00d7|H|, and the matrix with elements P0(D|h) by M \u2208 [0, 1]|D|\u00d7|H|. Further, denote the vectors consisting of PL0(h) and PT(h) by a,d \u2208 [0, 1]|H|\u00d71, vectors consisting of PT0(D) and PL(D) by b, c \u2208 [0, 1]|D|\u00d71, respectively. Given a, b, and M , the fixed-point iteration of the cooperative inference equations can be expressed as:\nPL1(h|D) = P0(D|h)PL0(h)\nPL1(D) \u21d0\u21d2 L(1) = Diag\n( 1\nM a\n) M Diag(a) (3a)\nPTk+1(D|h) = PLk+1(h|D)PT0(D)\nPTk+1(h) \u21d0\u21d2 T(k+1) = Diag(b) L(k+1)Diag\n( 1\nd(k+1)\n) (3b)\nPTk+1(h) = \u2211 D\u2208D PLk(h|D)PT0(D) \u21d0\u21d2 d(k+1) = (L(k+1))T b (3c)\nPLk+1(h|D) = PTk(D|h)PL0(h)\nPLk+1(D) \u21d0\u21d2 L(k+1) = Diag\n( 1\nc(k+1)\n) T(k)Diag(a) (3d)\nPLk+1(D) = \u2211 h\u2208H PTk(D|h)PL0(h) \u21d0\u21d2 c(k+1) = T(k)a, (3e)\nwhere, k denotes the iteration step, and Diag(z) denotes the diagonal matrix with elements of the vector z on its diagonal, and 1z denotes element-wise inverse for any vector z.\nNote that (3b) and (3c) are the operations to column normalize Diag(b) L(k), and (3d) and (3e) are the operations to row normalize T(k)Diag(a). Zero rows in L(k) and zero columns in T(k) are fixed throughout the iteration of (3). This is equivalent to removing the zero rows and zero columns of M for (3) and inserting them back at convergence or when the iteration is stopped.\nNow we provide a version of the proof using the notations introduced in the paper. The original proof can be found in [19]. Remember that a and b are assumed to be uniform.\nProof. Let \u03c3 be a permutation of {1, \u00b7 \u00b7 \u00b7 , n} that makes {M i,\u03c3(i)}ni=1 a positive diagonal. Define\ne(k) := n\u220f i=1 L (k) i,\u03c3(i); f (k) := n\u220f i=1 T (k) i,\u03c3(i).\nApplying (3a), L(1) is a row-stochastic matrix, and {L(1)i,\u03c3(i)} n i=1 is a positive diagonal, hence e (1) is positive. Also, by applying (3b),\nf (1) = n\u220f i=1 T (1) i,\u03c3(i) = n\u220f i=1 biL(1)i,\u03c3(i) d (1) \u03c3(i)  = e(1) nn \u220fn i=1 d (1) \u03c3(i) = e(1) nn \u220fn i=1 d (1) i . (4)\nBy the inequality of arithmetic and geometric means, (\u220fn\ni=1 d (1) i ) 1 n \u2264 1n \u2211n i=1 d (1) i . Also, L (1) is a row-\nstochastic matrix and we assumed uniform prior on data set space, hence by (3c),\nnn n\u220f j=1 d (1) j \u2264  n\u2211 j=1 d (1) j n =  n\u2211 i=1 n\u2211 j=1 bjL (1) i,j n =  1 n n\u2211 i=1 n\u2211 j=1 L (1) i,j n = 1 (5) The Equality in (5) is achieved if and only if d = ( 1 n , . . . , 1 n ) , or equivalently, L(1) being a doubly stochastic matrix. Because f (1) is the product of n values between 0 and 1,\n0 < e(1) \u2264 (a) f (1) \u2264 (b) 1, (6)\nwith equality in (a) if and only if L(1) is a doubly stochastic matrix, and equality in (b) if and only if L(1) is a permutation matrix. By the same logic applied to equations (3d) and (3e),\n0 < f (1) \u2264 (c) e(2) \u2264 (d) 1,\nwith equality in (c) if and only if T(1) is a doubly stochastic matrix, and equality in (d) if and only if T(1) is a permutation matrix. Repeating this argument, we get the increasing sequence\n0 < e(1) \u2264 f (1) \u2264 e(2) \u2264 f (2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 1.\nMonotone convergence theorem of real numbers guarantees that this sequence converges to its supremum\nlim k\u2192\u221e e(k) = lim k\u2192\u221e f (k) = sup{e, f}.\ne(k) = f (k) = e(k+1) asymptotically, therefore L(k) and T(k) are both doubly stochastic matrices. Because doubly stochastic matrices are stable under row and column normalization, L and T converge to the same doubly stochastic matrix\nM(\u221e) := lim k\u2192\u221e L(k) = lim k\u2192\u221e T(k)."}, {"heading": "6.3 Proof of Theorem 4.9", "text": "Proof. (1) (a) \u21d0\u21d2 (b): We first prove that (a) CI(M ) = 1, and (b) M has exactly one positive diagonal, are equivalent. Since M is an n \u00d7 n nonnegative matrix with at least one positive diagonal, Theorem 4.5 guarantees that the iteration of equation set (3) converges to a doubly stochastic matrix, M(\u221e). According to Birkhoff\u2013von Neumann theorem [2, 24], there exist \u03b81, . . . , \u03b8k \u2208 (0, 1] with \u2211 i \u03b8i = 1 and distinct permutation\nmatrices P1, . . . , Pk such that M (\u221e) = \u03b81P1 + \u00b7 \u00b7 \u00b7+ \u03b8kPk. To simplify, we adopt the inner product notation between matrices: A \u00b7 B = \u2211 i,j Ai,jBi,j , for any two n \u00d7 n square matrices A and B. Then the following holds.\nCI = TI(M(\u221e),M(\u221e)) = (I)\n1 n M(\u221e) \u00b7 M(\u221e) = (II) 1 n ( \u2211 i \u03b8iPi) \u00b7 ( \u2211 j \u03b8jPj) = (III) 1 n \u2211 i,j \u03b8i\u03b8jPi \u00b7 Pj\nEquality (I) comes from rewriting TI in the inner product notation. Equality (II) comes from substituting M(\u221e) by its Birkhoff\u2013von Neumann decomposition. Equality (III) comes from distribution.\nFurther as permutation matrices, Pi \u00b7 Pj \u2264 n, and the equality holds if and only if Pi = Pj . So we have that,\nCI(M ) = 1\nn \u2211 i,j \u03b8i\u03b8jPi \u00b7 Pj \u2264 (IV ) 1 n \u2211 i,j \u03b8i\u03b8jn = \u2211 i,j \u03b8i\u03b8j = ( \u2211 i \u03b8i)\u00d7 ( \u2211 j \u03b8j) = 1\nThe equality in (IV) holds if and only if Pi = Pj for any i, j. Note that P1, . . . , Pk are distinct, i.e. Pi 6= Pj when i 6= j. So the equality in (IV) is achieved precisely when k = 1 and M(\u221e) = P1. Hence, CI(M ) is maximized if and only if M(\u221e) is a permutation matrix.\nWe then prove that M(\u221e) is a permutation matrix if and only if M has exactly one positive diagonal. This follows the claim (1): elements of M that lie in a positive diagonal do not tend to zero during the cooperative inference iteration [19] (i.e. if M i,j 6= 0 lies in a positive diagonal, then M(\u221e)i,j 6= 0). Claim (1) implies that M(\u221e) and M have the same number of positive diagonals. Further note that a doubly stochastic matrix has exactly one diagonal if and only it is a permutation matrix. So as a doubly stochastic matrix, M(\u221e) is a permutation matrix if and only if M has exactly one positive diagonal. Thus, CI is maximized if and only if M has exactly one positive diagonal.\nTo complete the proof for (a) \u21d0\u21d2 (b), we only need to justify Claim (1). Note that the product of any positive diagonal converges to a positive number sup{e, f} (shown in the proof for Theorem 4.5) and all elements on the positive diagonal is upper-bounded by 1 and lower-bounded by sup{e, f}. Therefore elements on a diagonal of M cannot converge to 0.\n(2) (b)\u21d0\u21d2 (c): This follows immediately from a slightly more general claim below. Claim (2): Let A be an n\u00d7n-square matrix. Then A has exactly one non-zero diagonal (i.e. a diagonal with no zero element) if and only if A is a permutation of an upper-triangular matrix. We now prove Claim (2). The if direction is clear since an upper-triangular matrix always has exactly one non-zero diagonal, which is its main diagonal. The only if direction is proved by induction on the dimension n of A.\nStep 1 - Induction basis: When n = 2, it is easy to check that any 2 \u00d7 2 matrix with exactly one\ndiagonal is either of the form ( a b 0 c ) or ( a 0 b c ) , where a, c 6= 0. So it is a permutation of an upper-triangular matrix.\nStep 2 - Inductive step: Suppose that the claim - an n\u00d7 n-square matrix A has exactly one non-zero diagonal if and only if it is a permutation of an upper-triangular matrix - holds for any n < N . We need to show that the claim also holds when n = N .\nThe following notation will be used. Let A be an n\u00d7 n-square matrix. Ai,j denotes the element of A at row i and column j. A\u0303i,j denotes the (n \u2212 1) \u00d7 (n \u2212 1) sub-matrix obtained from A by crossing out row i and column j.\nFirst, we will prove three handy observations. Observation 1 If A has exactly one non-zero diagonal and Ai,j 6= 0, then A\u0303i,j has at most one non-zero\ndiagonal. In particular, if Ai,j is on that non-zero diagonal, then A\u0303i,j has exactly one non-zero diagonal.\nProof of Observation 1 : Suppose that A\u0303i,j has more than one diagonal. Then these diagonals for A\u0303i,j along with Ai,j form different diagonals for A, contradiction.\nObservation 2 If A has exactly one non-zero diagonal and A has a row or a column with exactly one non-zero element, then A is a permutation of an upper-triangular matrix.\nProof of Observation 2 : Suppose that A has a column with exactly one non-zero element. Then by permutation, we may assume that it is the first column of A and the only non-zero element is A1,1. A1,1 must be on the non-zero diagonal of A. Hence according to observation 1, A\u03031,1 is a (N \u2212 1\u00d7N \u2212 1)-square matrix has exactly one non-zero diagonal. Then by the inductive assumption, we may permute A\u03031,1 into an upper-triangular matrix. Note that each permutation of A\u03031,1 induces a permutation of A. So there exist permutations that convert A into A\u2032 such that A\u2032i,j = 0 when j > 1 and i > j. Moreover, permutations that convert A to A\u2032 never switch column 1 (row 1) of A with any other columns (rows). So A\u2032i,1 = 0 for i 6= 1 as A1,1 is the only non-zero element in the first column of A. Thus, we have A \u2032 i,j = 0 when i > j which implies that A\u2032 is an upper-triangular matrix. If A has a row with exactly one non-zero element. Then up to permutation, we may assume it is the last row of A and the only non-zero element is AN,N . Following similar argument as above, we may show that A\u0303N,N can be arranged into an upper-triangular matrix by permutations. And corresponding permutations of A will also convert A into an upper triangular matrix. So observation 2 holds.\nObservation 3 If the main diagonal ofA is the only non-zero diagonal ofA, thenAt1,t2At2,t3 \u00b7 \u00b7 \u00b7Atk\u22121,tkAtk,t1 = 0 for any distinct t1, t2, . . . , tk.\nProof of Observation 3 : Suppose that At1,t2At2,t3 \u00b7 \u00b7 \u00b7Atk\u22121,tkAtk,t1 6= 0. Then a different non-zero diagonal for A other than the main diagonal is form by {Ai,i|i 6= t1, . . . , tk} and At1,t2 , At2,t3 , \u00b7 \u00b7 \u00b7 , Atk\u22121,tk , Atk,t1 .\nNow back to the inductive step. Suppose that A is an N \u00d7N -square matrix with exactly one non-zero diagonal. By permutation, we may assume that the main diagonal of A is the only non-zero diagonal. In particular, A1,1 6= 0. According to Observation 1, A\u03031,1 has exactly one non-zero diagonal and so can be arranged into an upper-triangular matrix by permutations. The corresponding permutations convert A into a new form, denoted by A1 with the property that A1i,j = 0 when j > 1 and i > j. In particular, A 1 Nj = 0 when j 6= 1 and j 6= N . A\u030311,1 is an upper-triangular matrix implies that A1N,N 6= 0. If A1N,1 = 0, then the last row of A1 contains only one non-zero element A1N,N . So we are done by Observation 2.\nOtherwise, according to Observation 1, A\u03031N,N can be arranged into an upper-triangular matrix by permutation. Hence, after the corresponding permutations, we may convert A1 into a new form, denoted by A2 with the property that A2i,j = 0 when i > j and i 6= N . Moreover, permutations that convert A1 to A2 never switch row N (column N) of A1 with any other rows (columns). So only one of {A2N,j |j 6= N} is not zero. If A2N,1 = 0, along with A 2 i,1 = 0 for N > i > 1, we have that first column of A 2 contains exactly one non-zero element A21,1. So by Observation 2, we are done. Otherwise, A2N,1 6= 0. According to Observation 3, A2N,1A21,kA2k,N = 0, for k = 2, . . . , N \u2212 1. So we have that A21,kA 2 k,N = 0, for k = 2, . . . , N \u2212 1. We will proceed by analyzing cases from k = 2 to k = N \u2212 1\nWhen k = 2, if A21,2 = 0, then column 2 of A 2 contains only one non-zero element A22,2 and we are done by Observation 2. Otherwise, we may assume that A21,2 6= 0 and A22,N = 0. When k = 3, if A23,N 6= 0, then A21,3 = 0. According to Observation 3, A2N,1A21,2A22,3A23,N = 0 and this implies that A22,3 = 0. Hence column 3 of A 2 contains only one non-zero element A23,3 and again we are done by Observation 2. Otherwise, we may assume that A23,N = 0 and one of {A21,3, A22,3} is not zero. When k = k, if A24,N 6= 0, then A21,4 = 0. Similarly as in the case where k = 3 (by Observation 3), A2N,1A 2 1,2A 2 2,4A 2 3,N = 0 and this implies that A 2 2,4 = 0. One of {A21,3, A22,3} is not zero =\u21d2 either A2N,1A 2 1,3A 2 3,4A 2 3,N = 0 or A 2 N,1A 2 1,2A 2 2,3A 2 3,4A 2 3,N = 0 =\u21d2 A23,4 = 0. Hence column 4 of A2 contains only one non-zero element A24,4 and again we are done by Observation 2. Otherwise, we may assume that A 2 4,N = 0 and at least one of {A21,4, A22,4, A23,4} is not zero. Inductively, either one of column k\u2019s of A2 contains only one non-zero element or A2k,N = 0 for all k = 2, . . . , N \u2212 1. Note the latter case implies that the column N of A2 contains only one non-zero element A2N,N as A 2 N,1 6= 0 =\u21d2 A21,N = 0. Either way, the proof is then completed by Observation 2."}, {"heading": "6.4 Details to Example 4.10", "text": "To construct M , first notice that if maximum likelihood is achieved, M 1,1 = M 1,2 under all settings of \u2206, a, and q. This is because a first- and second-order polynomial give the same fit to D1.\nFor M 2,1, by symmetry arguments we know that the maximum-likelihood fit of a first-order polynomial to D2 is a horizontal line (f(x) = b). We can find this value of b through a grid search. Given this b,\nM 2,1 = Nq(a; b) 2Nq(\u2212a; b)2Nq(\u2206 + a; b)Nq(\u2206\u2212 a; b),\nwhere\nNq(z; b) =\n\u221a \u03b2 Cq eq(\u2212\u03b2(xi \u2212 \u00b5)2).\nHere, \u03b2 = 15\u22123q so that the variance is 1. eq(x) is the q-exponential function defined by [1 + (1\u2212 q)x] 1 1\u2212q when q 6= 1, and exp(x) when q = 1. The normalizing constant Cq is given by:\nCq =  2 \u221a \u03c0\u0393( 11\u2212q ) (3\u2212q) \u221a 1\u2212q\u0393( 3\u2212q 2(1\u2212q) ) for \u2212\u221e < q < 1 \u221a \u03c0 for q = 1 \u221a \u03c0\u0393( 3\u2212q\n2(q\u22121)\u221a q\u22121\u0393 1q\u22121\nfor 1 < q < 3.\nFor M 2,2, again by symmetry arguments we know that the maximum-likelihood fit of a second order polynomial to D2 is a parabola that passes through the middle of each of the three pairs of data points. Thus, M 2,2 = Nq(a; 0) 6."}], "references": [{"title": "Evolution of the social brain", "author": ["Robert A Barton", "Robin IM Dunbar"], "venue": "Machiavellian intelligence II: Extensions and evaluations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Three observations on linear algebra", "author": ["Garrett Birkhoff"], "venue": "Univ. Nac. Tucuma\u0301n. Revista A,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1946}, {"title": "End to end learning for self-driving cars", "author": ["Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The double-edged sword of pedagogy: Modeling the effect of pedagogical contexts on preschoolers\u2019 exploratory play", "author": ["Elizabeth Bonawitz", "Patrick Shafto", "Hyowon Gweon", "Isabel Chang", "Sydney Katz", "Laura Schulz"], "venue": "In Proceedings of the 31th annual conference of the Cognitive Science Society,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery", "author": ["Elizabeth Bonawitz", "Patrick Shafto", "Hyowon Gweon", "Noah D Goodman", "Elizabeth Spelke", "Laura Schulz"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "The cultural niche: Why social learning is essential for human adaptation", "author": ["Robert Boyd", "Peter J Richerson", "Joseph Henrich"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Cooperating with machines", "author": ["Jacob W Crandall", "Mayada Oudah", "Fatimah Ishowo-Oloko", "Sherief Abdallah", "Jean-Fran\u00e7ois Bonnefon", "Manuel Cebrian", "Azim Shariff", "Michael A Goodrich", "Iyad Rahwan"], "venue": "arXiv preprint arXiv:1703.06207,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Recursive teaching dimension, VC-dimension and sample compression", "author": ["Thorsten Doliwa", "Gaojian Fan", "Hans Ulrich Simon", "Sandra Zilles"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Epistemic trust and education: Effects of informant reliability on student learning of decimal concepts", "author": ["Kelley Durkin", "Patrick Shafto"], "venue": "Child Development,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Parameterizing developmental changes in epistemic trust", "author": ["Baxter S Eaves", "Patrick Shafto"], "venue": "Psychonomic Bulletin & Review,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Prosocial development", "author": ["Nancy Eisenberg", "Richard A Fabes", "Tracy L Spinrad"], "venue": "Wiley Online Library,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "On the complexity of teaching", "author": ["Sally A Goldman", "Michael J Kearns"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Nonliteral understanding of number words", "author": ["Justine T Kao", "Jean Y Wu", "Leon Bergen", "Noah D Goodman"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The teaching dimension of linear learners", "author": ["Ji Liu", "Xiaojin Zhu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Teaching games: Statistical sampling assumptions for learning in pedagogical situations", "author": ["Patrick Shafto", "Noah Goodman"], "venue": "In Proceedings of the 30th annual conference of the Cognitive Science Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Learning from others: The consequences of psychological reasoning for human learning", "author": ["Patrick Shafto", "Noah D Goodman", "Michael C Frank"], "venue": "Perspectives on Psychological Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A rational account of pedagogical reasoning: Teaching by, and learning from, examples", "author": ["Patrick Shafto", "Noah D Goodman", "Thomas L Griffiths"], "venue": "Cognitive Psychology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Deep learning in medical image analysis", "author": ["Dinggang Shen", "Guorong Wu", "Heung-Il Suk"], "venue": "Annual Review of Biomedical Engineering,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Concerning nonnegative matrices and doubly stochastic matrices", "author": ["Richard Sinkhorn", "Paul Knopp"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1967}, {"title": "Deep learning for mortgage risk", "author": ["Justin Sirignano", "Apaar Sadhwani", "Kay Giesecke"], "venue": "SSRN: https://ssrn.com/abstract=2799443,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "The cultural origins of human cognition", "author": ["M. Tomasello"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Why we cooperate", "author": ["Michael Tomasello"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Origins of human communication", "author": ["Michael Tomasello"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A certain zero-sum two-person game equivalent to the optimal assignment problem", "author": ["John Von Neumann"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1953}, {"title": "Machine teaching: An inverse problem to machine learning and an approach toward optimal education", "author": ["Xiaojin Zhu"], "venue": "In AAAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Human learning is characterized by the cooperative transmission of data [21].", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "The cooperative selection of data, and learning from such data, plays a central role in theories of cognition [1], cognitive development [11], and cultural evolution [22].", "startOffset": 110, "endOffset": 113}, {"referenceID": 10, "context": "The cooperative selection of data, and learning from such data, plays a central role in theories of cognition [1], cognitive development [11], and cultural evolution [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "The cooperative selection of data, and learning from such data, plays a central role in theories of cognition [1], cognitive development [11], and cultural evolution [22].", "startOffset": 166, "endOffset": 170}, {"referenceID": 22, "context": "Indeed, this cooperative inference is argued to be the feature that drives accumulation of knowledge over generations [23, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 5, "context": "Indeed, this cooperative inference is argued to be the feature that drives accumulation of knowledge over generations [23, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 24, "context": "A recent effort in this direction is machine teaching [25, 10], which formalizes how to translate and communicate model inferences using data.", "startOffset": 54, "endOffset": 62}, {"referenceID": 9, "context": "A recent effort in this direction is machine teaching [25, 10], which formalizes how to translate and communicate model inferences using data.", "startOffset": 54, "endOffset": 62}, {"referenceID": 11, "context": "We also use the measure to extend the teaching dimension [12, 26]\u2014a classical measure of communication efficiency\u2014from deterministic to probabilistic settings.", "startOffset": 57, "endOffset": 65}, {"referenceID": 14, "context": "In Sections 4, we introduce cooperative inference based on previous research in human social learning [15, 17], present a Cooperative Index by extend the Transmission index to the cooperative setting, and identify the condition that must be satisfied to achieve optimal communication.", "startOffset": 102, "endOffset": 110}, {"referenceID": 16, "context": "In Sections 4, we introduce cooperative inference based on previous research in human social learning [15, 17], present a Cooperative Index by extend the Transmission index to the cooperative setting, and identify the condition that must be satisfied to achieve optimal communication.", "startOffset": 102, "endOffset": 110}, {"referenceID": 0, "context": "Because H and D are both discrete, in matrix notation, we can form the row-stochastic learner\u2019s inference matrix, L \u2208 [0, 1]|D|\u00d7|H|, having elements PL(h|D), and the column-stochastic teacher\u2019s selection matrix, T \u2208 [0, 1]|D|\u00d7|H|, having elements PT(D|h).", "startOffset": 118, "endOffset": 124}, {"referenceID": 0, "context": "Because H and D are both discrete, in matrix notation, we can form the row-stochastic learner\u2019s inference matrix, L \u2208 [0, 1]|D|\u00d7|H|, having elements PL(h|D), and the column-stochastic teacher\u2019s selection matrix, T \u2208 [0, 1]|D|\u00d7|H|, having elements PT(D|h).", "startOffset": 216, "endOffset": 222}, {"referenceID": 0, "context": "In particular, it is possible to construct a pair of L and T that causes the limit of TI to bounce in the interval [0, 1].", "startOffset": 115, "endOffset": 121}, {"referenceID": 11, "context": "The Teaching Dimension is well-studied; it has formal connections with the VC Dimension [12] and has been analyzed for certain models in continuous concept space [14] and in cooperative settings [26, 8].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "The Teaching Dimension is well-studied; it has formal connections with the VC Dimension [12] and has been analyzed for certain models in continuous concept space [14] and in cooperative settings [26, 8].", "startOffset": 162, "endOffset": 166}, {"referenceID": 7, "context": "The Teaching Dimension is well-studied; it has formal connections with the VC Dimension [12] and has been analyzed for certain models in continuous concept space [14] and in cooperative settings [26, 8].", "startOffset": 195, "endOffset": 202}, {"referenceID": 7, "context": "The classical version of Average Teaching Dimension [8] is defined as follows: First, for any h \u2208 H, let", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "Let M \u2208 [0, 1]|D|\u00d7|H| be a matrix, where the element M i,j represents the probability that hi is consistent with Dj .", "startOffset": 8, "endOffset": 14}, {"referenceID": 8, "context": ", in pedagogy [9, 10] and conversations [13]).", "startOffset": 14, "endOffset": 21}, {"referenceID": 9, "context": ", in pedagogy [9, 10] and conversations [13]).", "startOffset": 14, "endOffset": 21}, {"referenceID": 12, "context": ", in pedagogy [9, 10] and conversations [13]).", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "In this section, we formalize cooperative inference, which captures this inter-dependency between the two processes of inference and selection [15, 17].", "startOffset": 143, "endOffset": 151}, {"referenceID": 16, "context": "In this section, we formalize cooperative inference, which captures this inter-dependency between the two processes of inference and selection [15, 17].", "startOffset": 143, "endOffset": 151}, {"referenceID": 14, "context": "The cooperative inference equations in (2) can be solved using fixed-point iteration [15, 17]: Define an initial likelihood, or common convention, PT(D|h) = P0(D|h), for the first evaluation of (2a).", "startOffset": 85, "endOffset": 93}, {"referenceID": 16, "context": "The cooperative inference equations in (2) can be solved using fixed-point iteration [15, 17]: Define an initial likelihood, or common convention, PT(D|h) = P0(D|h), for the first evaluation of (2a).", "startOffset": 85, "endOffset": 93}, {"referenceID": 0, "context": "This symmetry implies that the initial likelihood matrix, M \u2208 [0, 1]|D|\u00d7|H| with elements P0(D|h), can be an arbitrary non-negative matrix because it always gets appropriately normalized in the first iteration.", "startOffset": 62, "endOffset": 68}, {"referenceID": 18, "context": "Then, the iteration of (2) becomes the well-known Sinkhorn-Knopp algorithm, which provably converges under certain conditions by Sinkhorn\u2019s theorem [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "5 (A simpler version of Sinkhorn\u2019s theorem [19]).", "startOffset": 43, "endOffset": 47}, {"referenceID": 24, "context": "Now, we give two simple examples: The first demonstrates the fixed-point iteration of (2); the second compares cooperative inference with machine teaching [25].", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "We apply cooperative index to examples previously used in human teaching [4] and machine teaching [14], and compare the effectiveness of cooperative inference and other communication protocols.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "We apply cooperative index to examples previously used in human teaching [4] and machine teaching [14], and compare the effectiveness of cooperative inference and other communication protocols.", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "The first example [4, 5, 16] tested children\u2019s learning of hidden functions of a novel toy after observing data presented by an experimenter.", "startOffset": 18, "endOffset": 28}, {"referenceID": 4, "context": "The first example [4, 5, 16] tested children\u2019s learning of hidden functions of a novel toy after observing data presented by an experimenter.", "startOffset": 18, "endOffset": 28}, {"referenceID": 15, "context": "The first example [4, 5, 16] tested children\u2019s learning of hidden functions of a novel toy after observing data presented by an experimenter.", "startOffset": 18, "endOffset": 28}, {"referenceID": 3, "context": "Empirical results [4, 5] have shown that when the experimenter activated one function accidentally (accidental demonstration), children inferred the toy to afford at least one function, and they also explored possible additional functions.", "startOffset": 18, "endOffset": 24}, {"referenceID": 4, "context": "Empirical results [4, 5] have shown that when the experimenter activated one function accidentally (accidental demonstration), children inferred the toy to afford at least one function, and they also explored possible additional functions.", "startOffset": 18, "endOffset": 24}, {"referenceID": 15, "context": "In contrary, under pedagogical demonstration (P), both parties apply cooperative inference on the selection and interpretation of data [16], which leads to a perfectly effective communication protocol, TI(L ,T ) = CI(M = L) = 1.", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "The second example [14] considers a version-space learner who is trying to learn a threshold classifier h\u03b8, \u03b8 \u2208 {1 .", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Following [14], machine teaching chooses data that maximize the likelihood for the learner to infer the correct hypothesis:", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "Since elements of M that lie in a positive diagonal do not tend to zero during cooperative inference [19] (i.", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "Cooperative inference is central to accounts of human learning, language, and cultural evolution, but has not been deeply investigated in machine learning (but see [7]).", "startOffset": 164, "endOffset": 167}, {"referenceID": 19, "context": ", banking [20], medicine [18], self-driving cars [3]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": ", banking [20], medicine [18], self-driving cars [3]).", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": ", banking [20], medicine [18], self-driving cars [3]).", "startOffset": 49, "endOffset": 52}], "year": 2017, "abstractText": "Cooperative transmission of data fosters rapid accumulation of knowledge by efficiently combining experience across learners. Although well studied in human learning, there has been less attention to cooperative transmission of data in machine learning, and we consequently lack strong formal frameworks through which we may reason about the benefits and limitations of cooperative inference. We present such a framework. We introduce a novel index for measuring the effectiveness of probabilistic information transmission, and cooperative information transmission specifically. We relate our cooperative index to previous measures of teaching in deterministic settings. We prove conditions under which optimal cooperative inference can be achieved, including a representation theorem which constrains the form of inductive biases for learners optimized for cooperative inference. We conclude by demonstrating how these principles may inform the design of machine learning algorithms and discuss implications for human learning, machine learning, and human-machine learning systems.", "creator": "LaTeX with hyperref package"}}}