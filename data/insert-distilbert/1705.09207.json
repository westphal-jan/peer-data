{"id": "1705.09207", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Learning Structured Text Representations", "abstract": "in this paper, we focus on learning structure - aware document comparing representations from data without recourse to consult a discourse parser or additional presentation annotations. drawing inspiration from recent efforts to further empower neural networks with a structural bias, we propose a model that can encode primarily a document while automatically inducing configuration rich structural dependencies. specifically, we embed a differentiable non - projective parsing algorithm into a neural model structure and use attention recovery mechanisms to incorporate the structural biases. experimental evaluation across different tasks and datasets shows that the proposed encoding model achieves state - of - the - art results on document modeling tasks while inducing intermediate structures on which are both interpretable and meaningful.", "histories": [["v1", "Thu, 25 May 2017 14:54:07 GMT  (162kb,D)", "http://arxiv.org/abs/1705.09207v1", null], ["v2", "Thu, 20 Jul 2017 23:14:58 GMT  (176kb,D)", "http://arxiv.org/abs/1705.09207v2", "Comments: 10 pages; typos corrected"], ["v3", "Thu, 14 Sep 2017 20:42:25 GMT  (208kb,D)", "http://arxiv.org/abs/1705.09207v3", "Accepted by TACL"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yang liu", "mirella lapata"], "accepted": false, "id": "1705.09207"}, "pdf": {"name": "1705.09207.pdf", "metadata": {"source": "CRF", "title": "Learning Structured Text Representations", "authors": ["Yang Liu"], "emails": ["Yang.Liu2@ed.ac.uk,", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013).\nRecent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016). Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al.,\n2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Ji and Eisenstein, 2014), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content.\nLinguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard NLP tools (e.g., tokenizers, pos-taggers, syntactic parsers). Unfortunately, the reliance on labeled data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread use of discourse structure for document modeling across different languages and text genres. And although discourse treebanks exist nowadays in languages other than English (e.g., Spanish, German, Basque, Dutch, and Brazilian Portuguese), they tend to be smaller than their English equivalents and of limited value for modeling purposes (Braud et al., 2017). Moreover, despite recent advances in discourse processing, the use of an external parser often leads to pipeline-style architectures where errors propagate to later processing stages, affecting model performance.\nIt is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations. The main idea is to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mecha-\nar X\niv :1\n70 5.\n09 20\n7v 1\n[ cs\n.C L\n] 2\n5 M\nay 2\n01 7\nnism (Bahdanau et al., 2015). In other words, it is acknowledged that sentences are differentially important in different contexts and the model learns to pay more or less attention to individual sentences when constructing the representation of the document.\nOur work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees. Specifically, they take into account the dependency structure of a sentence by viewing the attention mechanism as a graphical model over latent variables. They first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the forward-backward algorithm to normalize the scores with the marginal probabilities of a dependency tree. Without recourse to an external parser, their model learns meaningful taskspecific dependency structures, achieving competitive results in several sentence-level tasks. For document modeling, this approach has two drawbacks. Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006). As illustrated in Figure 1, the tree structure of a document can be flexible and the dependency edges may cross. Secondly, the forwardbackward pass in Kim et al. (2017) has complexity O(n3) with n being the number of text units, and without an efficient parallelizable implementation it is impractical for modeling documents.1\nIn this paper, we propose a new model for representing documents while automatically learning richer structural dependencies. Using a variant of Kirchhoff\u2019s Matrix-Tree Theorem (Tutte, 1984), our model implicitly considers non-projective dependency tree structures. We keep each step of the learning process differentiable, so the model can be trained in an end-to-end fashion and induce discourse information that is helpful to specific tasks without an external parser. Importantly, major operations can be efficiently parallelized on GPU com-\n1In our experiments, adding the forward-backward pass increases training time by a factor of 10.\nputing hardware. Although our primary focus is on document modeling, there is nothing inherent in our model that prevents its application to individual sentences. Advantageously, it can induce nonprojective structures which are required for representing languages with free or flexible word order (McDonald and Satta, 2007).\nOur contributions in this work are threefold: a model for learning document representations whilst taking structural information into account; an efficient training procedure which allows to compute document level representations of arbitrary length; and a large scale evaluation study showing that the proposed model performs competitively against strong baselines while inducing intermediate structures which are both interpretable and meaningful."}, {"heading": "2 Background", "text": "In this section, we describe how previous work uses the attention mechanism for representing individual sentences. The key idea is to capture the interaction between tokens within a sentence, generating a context representation for each word with weak structural information. This type of intra-sentence attention encodes relationships between words within each sentence and differs from inter-sentence attention which has been widely applied to sequence transduction tasks like machine translation (Bahdanau et al., 2015) and learns the latent alignment between source and target sequences.\nFigure 2 provides a schematic view of the intrasentential attention mechanism. Given a sentence represented as a sequence of n word vectors [u0,u1, \u00b7 \u00b7 \u00b7 ,un], for each word pair \u3008ui,uj\u3009, the\nattention score aij is estimated as:\nfij = F (ui,uj) (1)\naij = exp(fij)\u2211n k=1 exp(fik)\n(2)\nwhere F () is a function for computing the unnormalized score fij which is then normalized by calculating a probability distribution aij . Individual words collect information from their context based on aij and obtain a context representation:\nri = n\u2211\ni=0\naijuj (3)\nwhere attention score aij indicates the (dependency) relation between the i-th and the j-th-words and how information from uj should be fed into ui.\nDespite successful application of the above attention mechanism in sentiment analysis (Cheng et al., 2016) and entailment recognition (Parikh et al., 2016), the structural information under consideration is shallow, limited to word-word dependencies. Since attention is computed as a simple probability distribution, it cannot capture structural dependencies such as trees (or graphs). Kim et al. (2017) induce richer internal structure by imposing structural constraints on the probability distribution computed by the attention mechanism. Specifically, they normalize fij with a projective dependency tree using the forward-backward pass in the inside-outside al-\ngorithm (Baker, 1979):\nfij = F (ui,uj) (4) a = forward-backward(f) (5)\nri = n\u2211\ni=0\naijuj (6)\nThis process is differentiable, so the model can be trained end-to-end and learn structural information without relying on a parser. However, efficiency is a major issue, since the forward-backward algorithm has time complexity O(n3) (where n represents the number of tokens) and does not lend itself to easy parallelization. The high order complexity renders the approach impractical for real-world applications."}, {"heading": "3 Encoding Text Representations", "text": "In this section we present our document representation model. We follow previous work (Tang et al., 2015a; Yang et al., 2016) in modeling documents hierarchically by first obtaining representations for sentences and then composing those into a document representation. Structural information is taken into account while learning representations for both sentences and documents and an attention mechanism is applied on both words within a sentence and sentences within a document. The general idea is to force pair-wise attention between text units to form a non-projective dependency tree, and automatically induce this tree for different natural language processing tasks in a differentiable way. In the following, we first describe how the attention mechanism is applied to sentences, and then move on to present our document-level model."}, {"heading": "3.1 Sentence Model", "text": "Let T = [u0,u1, \u00b7 \u00b7 \u00b7 ,un] denote a sentence containing a sequence of words, each represented by a vector u, which can be pre-trained on a large corpus. Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition Graves et al. (2013), and image caption generation (Xu et al., 2015). In this paper we use bidirectional LSTMs as a way of representing elements in a sequence (i.e., words or sentences) together with their contexts, capturing the element and an \u201cinfinite\u201d\nwindow around it. Specifically, we run a bidirectional LSTM over sentence T , and take the output vectors [h0,h1, \u00b7 \u00b7 \u00b7 ,hn] as the representations of words in T , where ht \u2208 Rk is the output vector for word ut based on its context.\nWe then exploit the structure of T which we induce based on an attention mechanism detailed below to obtain more precise representations. Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSMT output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts:\n[et,dt] = ht (7)\nwhere et \u2208 Rkt , the semantic vector, encodes semantic information for specific tasks, and dt \u2208 Rks , the structure vector, is used to calculate structured attention.\nWe use a series of operations based on the MatrixTree Theorem (Tutte, 1984) to incorporate the structural bias of non-projective dependency trees into the attention weights. We constrain the probability distributions aij (see Equation (2)) to be the posterior marginals of a dependency tree structure. We then use the normalized structured attention, to build a context vector for updating the semantic vector of each word, obtaining new representations [r0, r1, \u00b7 \u00b7 \u00b7 , rn]. An overview of the model is pre-\nsented in Figure 3. We describe the attention mechanism in detail in the following section."}, {"heading": "3.2 Structured Attention Mechanism", "text": "Dependency representations of natural language are a simple yet flexible mechanism for encoding words and their syntactic relations through directed graphs. Much work in descriptive linguistics (Melc\u0306uk, 1988; Tesnie\u0301re, 1959) has advocated their suitability for representing syntactic structure across languages. A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions arising from long distance dependencies or free word order through nonprojective dependency edges.\nMore formally, building a dependency tree amounts to finding latent variables zij for all i 6= j, where word i is the parent node of word j, under some global constraints, amongst which the singlehead constraint is the most important, since it forces the structure to be a rooted tree. We use a variant of Kirchhoff\u2019s Matrix-Tree Theorem (Koo et al., 2007; Tutte, 1984) to calculate the marginal probability of each dependency edge P (zij = 1) of a non-projective dependency tree, and this probability is used as the attention weight that decides how much information is collected from child unit j to the parent unit i.\nWe first calculate unnormalized attention scores fij with structure vector d (see Equation (7) via a bilinear function:\ntp = tanh(Wpdi) (8) tc = tanh(Wcdj) (9) fij = tpWatc (10)\nwhere Wp \u2208 Rks\u2217ks and Wc \u2208 Rks\u2217ks are the weights for building the representation of parent and child nodes. Wa \u2208 Rks\u2217ks is the weight for the bilinear transformation. f can be viewed as a weighted adjacency matrix for a graph G whose nodes correspond to the words in a sentence. We also calculate the root score f ri , indicating the unnormalized possibility of a node being the root:\nf ri = Wrdi (11)\nwhere Wr \u2208 R1\u2217ks . We calculate P (zij = 1), the marginal probability of the dependency edge, fol-\nlowing Koo et al. (2007):\nLij =\n{\u2211n i\u2032=1 exp(fi\u2032j) if i = j\n\u2212exp(fij) otherwise (12)\nL\u0304ij =\n{ exp(f ri ) i = 0\nLij i > 0 (13)\nP (zij = 1) = (1\u2212 \u03b40,j)fij [L\u0304\u22121]jj \u2212 (1\u2212 \u03b4i,0)fij [L\u0304\u22121]ji (14)\nwhere L \u2208 Rn\u2217n is the Laplacian matrix for graphG and L\u0304 is a variant of L that takes the root node into consideration, and \u03b4 is the Kronecker delta. The key for the calculation to hold is for Lii, the minor of the Laplacian matrix L with respect to row i and column i, to be equal to the sum of the weights of all directed spanning trees of G which are rooted at i. Details of the proof can be found in (Koo et al., 2007).\nWe denote the marginal probabilities P (zij = 1) as aij . This can be interpreted as attention scores which are constrained to converge to a structured object, a non-projective dependency tree, in our case. We then update the semantic vector ei of each word with structured attention:\npi = n\u2211\nk=1\nakiei (15)\nci = n\u2211 k=1 aikei (16)\nri = tanh(Wr[ei,pi, ci]) (17)\nwhere pi \u2208 Rke is the context vector gathered from possible parents of ui and ci \u2208 Rke the context vector gathered from possible children. The context vectors are concatenated with ei and transformed with weights Wr \u2208 Rke\u22173ke to obtain the updated semantic vector ri \u2208 Rke with rich structural information (see Figure 3)."}, {"heading": "3.3 Document Model", "text": "We build document representations hierarchically: sentences are composed of words and documents are composed of sentences. Composition on the document level also makes use of structured attention in the form of a dependency graph. Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016;\nLi et al., 2014; Muller et al., 2012) and in applications such as summarization (Hirao et al., 2013).\nAs illustrated in Figure 4, given a document with n sentences [s0, s1, \u00b7 \u00b7 \u00b7 , sn], for each sentence si, the input is a sequence of word embeddings [ui0,ui1, \u00b7 \u00b7 \u00b7 ,uim], where m is the number of tokens in si. By feeding the embeddings into a sentence-level bi-LSTM and applying the proposed structured attention mechanism, we obtain the updated semantic vector [ri0, ri1, \u00b7 \u00b7 \u00b7 , rim]. Then a pooling operation produces a fixed-length vector vi for each sentence. Analogously, we view the document as a sequence of sentence vectors [v0,v1, \u00b7 \u00b7 \u00b7 ,vn] whose embeddings are fed to a document-level bi-LSTM. Application of the structured attention mechanism creates new semantic vectors [qi0, \u00b7 \u00b7 \u00b7 , qim] and another pooling operation yields the final document representation y."}, {"heading": "3.4 End-to-End Training", "text": "Our model can be trained in an end-to-end fashion since all operations required for computing structured attention and using it to update the semantic vectors are differentiable. In contrast to in Kim et al. (2017), training can be done efficiently. The major complexity of our model lies in the computation of the gradients of the the inverse matrix. Let A denote a matrix depending on a real parameter x; assuming all component functions in A are differentiable, and A is invertible for all possible values, the gradient\nof A with respect respect to x is:\ndA\u22121\ndx = \u2212A\u22121dA dx A\u22121 (18)\nMultiplication of the three matrices and matrix inversion can be computed efficiently on modern parallel hardware architectures such as GPUs. In our experiments, computation of structured attention takes only 1/10 of the training time."}, {"heading": "4 Experiments", "text": "In this section we present our experiments for evaluating the performance of our model. Since sentence representations constitute the basic building blocks of our document model, we first evaluate the performance of structured attention on a sentencelevel task, namely natural language inference. We then assess the document-level representations obtained by our model on a variety of classification tasks representing documents of different length, subject matter, and language. Our code is available at https://github.com/xxx/xxx."}, {"heading": "4.1 Natural Language Inference", "text": "The ability to reason about the semantic relationship between two sentences is an integral part of text understanding. We therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we obtained 549,367 pairs for training, 9,842 for development and 9,824 for testing.\nSentence-level representations obtained by our model (with structured attention) were used to encode the premise and hypothesis by modifying the model of Parikh et al. (2016) as follows. Let [xp1, \u00b7 \u00b7 \u00b7 ,x p n] and [xh1 , \u00b7 \u00b7 \u00b7 ,xhm] be the input vectors for the premise and hypothesis, respectively. Application of structured attention yields new vector representations [rp1, \u00b7 \u00b7 \u00b7 , r p n] and [rh1 , \u00b7 \u00b7 \u00b7 , rhm]. Then we combine these two vectors with inter-sentential\nattention, and apply an average pooling operation:\noij = MLP (r p i ) TMLP (rhj ) (19)\nr\u0304pi = [r p i , m\u2211 j=1 exp(oij)\u2211m k=1 exp(oi,k) ] (20)\nr\u0304hi = [r h i , m\u2211 i=1 exp(oij)\u2211m k=1 exp(ok,j) ] (21)\nrp = n\u2211\ni=0\ng(r\u0304pi ), r h = m\u2211 i=0 g(r\u0304hi ) (22)\nwhere MLP () is a two-layer perceptron with a ReLU activation function. The new representations rp, rh are then concatenated and fed into another two-layer perceptron with a softmax layer to obtain the predicted distribution over the labels.\nThe hidden size of the LSTM was set to 150. The dimensions of the semantic vector were 100 and the dimensions of structure vector were 50. We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the learning rate was set to 0.05. The hidden size of the two-layer perceptron was set to 200 and dropout was used with ratio 0.2. The mini-batch size was 32.\nWe compared our model (and variants thereof) against several related systems. Results (in terms of 3-class accuracy) are shown in Table 1. Most previous systems employ LSTMs and do not incorporate a structured attention component. Exceptions include Cheng et al. (2016) and Parikh et al. (2016) whose models include intra-attention encoding relationships between words within each sentence (see Equation (2)). It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2016b). The second block of Table 1 presents a version of our model without an intra-sentential attention mechanism as well as three variants with attention, assuming the structure of word-to-word relations and dependency trees. In the latter case we compare our matrix inversion based model against Kim et al.\u2019s (2017) forward-backward attention model. Consistent with previous work (Cheng et al., 2016; Parikh et al., 2016), we observe that simple attention brings performance improvements over no attention. Structured attention further enhances performance. Our\nown model with tree matrix inversion slightly outperforms the forward-backward model of Kim et al. (2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2016b; Cheng et al., 2016; Parikh et al., 2016).\nTable 2 compares the running speed of the models shown in the second block of Table 1. As can be seen matrix inversion does not increase running speed over the simpler attention mechanism and is considerably faster compared to forward-backward. The latter is 10\u201320 times slower than our model on the same platform."}, {"heading": "4.2 Document Classification", "text": "In this section, we evaluate our document-level model on a variety of classification tasks. We selected four datasets which we describe below. Table 3 summarizes some statistics for each dataset.\nYelp reviews were obtained from the 2013 Yelp Dataset Challenge. This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we followed the preprocessing introduced in (Tang et al., 2015a) and report experiments on their training, development, and testing partitions (80/10/10).\nIMDB reviews were obtained from Diao et al. (2014), who randomly crawled reviews for 50K movies. Each review is associated with user ratings ranging from 1 to 10.\nCzech reviews were obtained from Brychc\u0131n and Habernal (2013). The dataset contains reviews from the Czech Movie Database2 each labeled as positive, neutral, or negative. Experiments on this dataset perform 10-fold cross-validation following previous work (Brychc\u0131n and Habernal, 2013).\n2http://www.csfd.cz/\nCongressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.S. floor debates in the House of Representatives for the year 2005. Each debate consists of a series of speech segments, each labeled by the vote (\u201cyea\u201d or \u201cnay\u201d) cast for the proposed bill by the the speaker of each segment. We used the pre-processed corpus from Yogatama and Smith (2014).3\nFollowing previous work (Yang et al., 2016), we only retained words appearing more than five times in building the vocabulary and replaced words with lesser frequencies with a special UNK token. Word embeddings were initialized by training word2vec (Mikolov et al., 2013) on the training and validation splits of each dataset. In our experiments, we set the word embedding dimension to be 200 and the hidden size for the sentence-level and documentlevel LSTMs to 100 (the dimensions of the semantic and structure vectors were set to 75 and 25, respectively). We used a mini-batch size of 32 during training and documents of similar length were grouped in one batch. Parameters were optimized with Adagrad (Duchi et al., 2011), the learning rate was set to 0.05. We used L2 regularization for all parameters except word embeddings with regularization constant set to 1e\u22124. Dropout was applied on the input and output layers with dropout rate 0.3.\n3http://www.cs.cornell.edu/\u02dcainur/data. html\nOur results are summarized in Table 4. We compared our model against several related models covering a wide spectrum of representations including word-based ones (e.g., paragraph vector and CNN models) as well as hierarchically composed ones (e.g., a CNN or LSTM provides a sentence vector and then a recurrent neural network combines the sentence vectors to form a document level representation for classification). Previous state-of-the-art results on the three review datasets were achieved by the hierarchical attention network of Yang et al. (2016), which models the document hierarchically with two GRUs and uses an attention mechanism to weigh the importance of each word and sentence. On the debates corpus, Ji and Smith (2017) obtained best results with a recursive neural network model operating on the output of an RST parser. Table 4 presents three variants4 of our model, one with structured attention on the sentence level, another one with structured attention on the document level and a third model which employs attention on both levels. As can be seen, the combination is beneficial achieving best results on three out of four datasets. Furthermore, structured attention is superior to the simpler word-to-word attention mechanism, and both types of attention bring improvements over no attention. Also, the structured attention approach is still very\n4We do not report comparisons with the forward-backward approach on document classification tasks due to its prohibitive computation cost leading to 5 hours of training for one epoch.\nefficient, taking only 20 minutes for one training epoch on the largest dataset."}, {"heading": "4.3 Analysis of Induced Structures", "text": "To gain further insight on structured attention, we inspected the dependency trees it produces. Specifically, we used the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to extract the maximum spanning tree from the attention scores. We report various statistics on the characteristics of the induced trees across different tasks and datasets. We also provide examples of tree output, in an attempt to explain how our model uses dependency structures to model text.\nSentence Trees We compared the dependency trees obtained from our model with those produced by a state-of-the-art dependency parser trained on the English Penn Treebank. Table 5 presents various statistics on the depth of the trees produced by our model on the SNLI test set and the Stanford dependency parser (Manning et al., 2014). As can be seen, the induced dependency structures are simpler compared to those obtained from the Stanford parser. The trees are generally less deep (their height is 5.78\ncompared to 8.99 for the Stanford parser), with the majority being of depth 2\u20134. Almost half of the induced trees have a projective structure, although there is nothing in the model to enforce this constraint. We also calculated the percentage of headdependency edges that are identical between the two sets of trees. Although our model is not exposed to annotated trees during training, a large number of edges agree with the output of the Stanford parser.\nFigure 5 shows examples of dependency trees induced on the SNLI dataset. Although the model is trained without ever being exposed to a parse tree, it is able to learn plausible dependency structures via the attention mechanism. Overall we observe that the induced trees differ from linguistically motivated ones in the types of dependencies they create which tend to be of shorter length. The dependencies obtained from structured attention are more direct as shown in the first premise sentence in Figure 5 where words at and bar are directly connected to the verb drink. This is perhaps to be expected since the attention mechanism uses the dependency structures to collect information from other words, and the direct links will be more effective.\nDocument Trees We also used the Chu-LiuEdmonds algorithms to obtain document-level dependency trees. Table 6 summarizes various characteristics of these trees. For most datasets, document-\nlevel trees are not very deep, they mostly contain up to nodes of depth 3. This is not surprising as the documents are relatively short (see Table 3) with the exception of debates which are longer and the induced trees more complex. The fact that most documents exhibit simple discourse structures is fur-\nther corroborated by the large number (over 70%) of projective trees induced on Yelp, IMBD, and CZ Movies datasets. Unfortunately, our trees cannot be directly compared with the output of a discourse parser which typically involves a segmentation process splitting sentences into smaller units. Our trees are constructed over entire sentences, and there is no mechanism currently in the model to split sentences into discourse units.\nFigure 6 shows examples of document-level trees taken from Yelp and the Czech Movie dataset. In the first tree, most edges are examples of the \u201celaboration\u201d discourse relation, i.e., the child presents additional information about the parent. The second tree is non-projective, the edges connecting sentences 1 and 4 and 3 and 5 cross. The third review, perhaps due to its colloquial nature, is not entirely coherent. However, the model manages to link sentences 1 and 3 to sentence 2, i.e., the movie being discussed; it also relates sentence 6 to 4, both of which express highly positive sentiment."}, {"heading": "5 Conclusions", "text": "In this paper we proposed a new model for representing documents while automatically learning rich structural dependencies. Our model normalizes intra-attention scores with the marginal probabilities of a non-projective dependency tree based on a matrix inversion process. Each operation in this process is differentiable and the model can be trained efficiently end-to-end, while inducing structural information. We applied this approach to model documents hierarchically, incorporating both sentenceand document-level structure. Experiments on sentence and document modeling tasks show that the representations learned by our model achieve competitive performance against strong comparison systems. Analysis of the induced tree structures revealed that they are meaningful, albeit different from linguistics ones, without ever exposing the model to linguistic annotations or an external parser.\nDirections for future work are many and varied. Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism. We also plan to explore how document-level trees can be usefully employed in summarization, e.g., as a means to represent or event extract important content."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the ICLR Conference. San Diego, California.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Trainable grammars for speech recognition", "author": ["James K Baker."], "venue": "The Journal of the Acoustical Society of America 65(1):132\u2013132.", "citeRegEx": "Baker.,? 1979", "shortCiteRegEx": "Baker.", "year": 1979}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Computational Linguistics 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Better document-level sentiment analysis from RST discourse parsing", "author": ["Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Por-", "citeRegEx": "Bhatia et al\\.,? 2015", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "Proceedings of the 54th Annual Meeting of the As-", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Cross-lingual RST discourse parsing", "author": ["Chlo\u00e9 Braud", "Maximin Coavoux", "Anders S\u00f8gaard."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Vancouver, Canada. To appear.", "citeRegEx": "Braud et al\\.,? 2017", "shortCiteRegEx": "Braud et al\\.", "year": 2017}, {"title": "Unsupervised improving of sentiment analysis using global target context", "author": ["Tom\u00e1\u0161 Brychc\u0131n", "Ivan Habernal."], "venue": "Proceedings of the Recent Advances in Natural Language Processing Conference. Hissar, Bulgaria, pages 122\u2013128.", "citeRegEx": "Brychc\u0131n and Habernal.,? 2013", "shortCiteRegEx": "Brychc\u0131n and Habernal.", "year": 2013}, {"title": "Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory", "author": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski."], "venue": "Current and new directions in discourse and dialogue, Springer, pages 85\u2013112.", "citeRegEx": "Carlson et al\\.,? 2003", "shortCiteRegEx": "Carlson et al\\.", "year": 2003}, {"title": "Distraction-based neural networks for modeling documents", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang."], "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16). New York City,", "citeRegEx": "Chen et al\\.,? 2016a", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Enhancing and combining sequential and tree LSTM for natural language inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang."], "venue": "arXiv preprint arXiv:1609.06038 .", "citeRegEx": "Chen et al\\.,? 2016b", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 551\u2013561.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On shortest arborescence of a directed graph", "author": ["Yoeng-Jin Chu", "Tseng-Hong Liu."], "venue": "Scientia Sinica 14(10):1396.", "citeRegEx": "Chu and Liu.,? 1965", "shortCiteRegEx": "Chu and Liu.", "year": 1965}, {"title": "Frustratingly short attention spans in neural language modeling", "author": ["Micha\u0142 Daniluk", "Tim Rockt\u00e4schel", "Johannes Welbl", "Sebastian Riedel."], "venue": "In Proceedings of the ICLR Conference. Toulon, France.", "citeRegEx": "Daniluk et al\\.,? 2017", "shortCiteRegEx": "Daniluk et al\\.", "year": 2017}, {"title": "Jointly modeling aspects, ratings and sentiments for movie recommendation", "author": ["Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J Smola", "Jing Jiang", "Chong Wang."], "venue": "Proceedings of the 20th ACM SIGKDD international con-", "citeRegEx": "Diao et al\\.,? 2014", "shortCiteRegEx": "Diao et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Journal of Research of the national Bureau of Standards B 71(4):233\u2013240.", "citeRegEx": "Edmonds.,? 1967", "shortCiteRegEx": "Edmonds.", "year": 1967}, {"title": "Textlevel discourse parsing with rich linguistic features", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Jeju Island, Korea,", "citeRegEx": "Feng and Hirst.,? 2012", "shortCiteRegEx": "Feng and Hirst.", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "Proceedings of the", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Empirical comparison of dependency conversions for RST discourse trees", "author": ["Katsuhiko Hayashi", "Tsutomu Hirao", "Masaaki Nagata."], "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dia-", "citeRegEx": "Hayashi et al\\.,? 2016", "shortCiteRegEx": "Hayashi et al\\.", "year": 2016}, {"title": "Single-document summarization as a tree knapsack problem", "author": ["Tsutomu Hirao", "Yasuhisa Yoshida", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural", "citeRegEx": "Hirao et al\\.,? 2013", "shortCiteRegEx": "Hirao et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland,", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "Neural discourse structure for text categorization", "author": ["Yangfeng Ji", "Noah Smith."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Vancouver, Canada. To appear.", "citeRegEx": "Ji and Smith.,? 2017", "shortCiteRegEx": "Ji and Smith.", "year": 2017}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M Rush."], "venue": "Proceedings of the ICLR Conference. Toulon, France.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Structured prediction models via the matrix-tree theorem", "author": ["Terry Koo", "Amir Globerson", "Xavier Carreras P\u00e9rez", "Michael Collins."], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural", "citeRegEx": "Koo et al\\.,? 2007", "shortCiteRegEx": "Koo et al\\.", "year": 2007}, {"title": "Complexity of dependencies in discourse: Are dependencies in discourse more complex than in syntax", "author": ["Alan Lee", "Rashmi Prasad", "Aravind Joshi", "Nikhil Dinesh", "Bonnie Webber."], "venue": "Proceedings of the 5th International Workshop", "citeRegEx": "Lee et al\\.,? 2006", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Text-level discourse dependency parsing", "author": ["Sujian Li", "Liang Wang", "Ziqiang Cao", "Wenjie Li."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland,", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Automatically evaluating text coherence using discourse relations", "author": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Lin et al\\.,? 2011", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:521\u2013535.", "citeRegEx": "Linzen et al\\.,? 2016", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William C Mann", "Sandra A Thompson."], "venue": "Text-Interdisciplinary Journal for the Study of Discourse 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "On the complexity of non-projective data-driven dependency parsing", "author": ["Ryan McDonald", "Giorgio Satta."], "venue": "Proceedings of the 10th International Conference on Parsing Technologies. Prague, Czech Republic, pages 121\u2013132.", "citeRegEx": "McDonald and Satta.,? 2007", "shortCiteRegEx": "McDonald and Satta.", "year": 2007}, {"title": "Dependency Syntax: Theory and Practice", "author": ["Igor A. Melc\u0306uk"], "venue": null, "citeRegEx": "Melc\u0306uk.,? \\Q1988\\E", "shortCiteRegEx": "Melc\u0306uk.", "year": 1988}, {"title": "Graphbased coherence modeling for assessing readability", "author": ["Mohsen Mesgar", "Michael Strube."], "venue": "Proceedings of the 4th Joint Conference on Lexical and Computational Semantics. Denver, Colorado, pages 309\u2013318.", "citeRegEx": "Mesgar and Strube.,? 2015", "shortCiteRegEx": "Mesgar and Strube.", "year": 2015}, {"title": "Implicitation of discourse connectives in (machine) translation", "author": ["Thomas Meyer", "Bonnie Webber."], "venue": "Proceedings of the Workshop on Discourse in Machine Translation. Sofia, Bulgaria, pages 19\u201326.", "citeRegEx": "Meyer and Webber.,? 2013", "shortCiteRegEx": "Meyer and Webber.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositiona lity", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S a nd Dean Jeff Corrado."], "venue": "Advances in Neural Information Processing Systems 26, Curran Associates, Inc.,", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Nat-", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Constrained decoding for text-level discourse parsing", "author": ["Philippe Muller", "Stergos Afantenos", "Pascal Denis", "Nicholas Asher."], "venue": "Proceedings of COLING 2012. Mumbai, India, pages 1883\u2013 1900.", "citeRegEx": "Muller et al\\.,? 2012", "shortCiteRegEx": "Muller et al\\.", "year": 2012}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Parikh et al\\.,? 2016", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar,", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The penn discourse treebank 2.0", "author": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind Joshi", "Bonnie Webber"], "venue": "In Proceedings of the 6th International Conference on Language Resources and Evalua-", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "Proceedings of the ICLR Conference. San Juan, Puerto Rico.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2016", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal,", "citeRegEx": "Tang et al\\.,? 2015a", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Learning semantic representations of users and products for document level sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and", "citeRegEx": "Tang et al\\.,? 2015b", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "\u00c9l\u00e9ments de Syntaxe Structurale", "author": ["Louis Tesni\u00e9re."], "venue": "Editions Klincksieck.", "citeRegEx": "Tesni\u00e9re.,? 1959", "shortCiteRegEx": "Tesni\u00e9re.", "year": 1959}, {"title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts", "author": ["Matt Thomas", "Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Syd-", "citeRegEx": "Thomas et al\\.,? 2006", "shortCiteRegEx": "Thomas et al\\.", "year": 2006}, {"title": "Discourse-based answering of why-questions", "author": ["Suzan Verberne", "Lou Boves", "Nelleke Oostdijk", "Peter-Arno Coppen."], "venue": "Traitement Automatique des Language, Discours et Document: traitements automatics 47(2):21\u201341.", "citeRegEx": "Verberne et al\\.,? 2007", "shortCiteRegEx": "Verberne et al\\.", "year": 2007}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1512.08849 .", "citeRegEx": "Wang and Jiang.,? 2015", "shortCiteRegEx": "Wang and Jiang.", "year": 2015}, {"title": "Coherence in Natural Language: Data Structures and Applications", "author": ["Florian Wolf", "Edward Gibson."], "venue": "The MIT Press.", "citeRegEx": "Wolf and Gibson.,? 2006", "shortCiteRegEx": "Wolf and Gibson.", "year": 2006}, {"title": "Integrating document clustering and topic modeling", "author": ["Pengtao Xie", "Eric P. Xing."], "venue": "Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence. Bellevue, Washington, pages 694\u2013703.", "citeRegEx": "Xie and Xing.,? 2013", "shortCiteRegEx": "Xie and Xing.", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with vi sual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aa ron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proceedings of the 32nd", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Linguistic structured sparsity in text categorization", "author": ["Dani Yogatama", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland,", "citeRegEx": "Yogatama and Smith.,? 2014", "shortCiteRegEx": "Yogatama and Smith.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies.", "startOffset": 90, "endOffset": 128}, {"referenceID": 24, "context": "Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies.", "startOffset": 90, "endOffset": 128}, {"referenceID": 50, "context": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al.", "startOffset": 138, "endOffset": 158}, {"referenceID": 9, "context": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al.", "startOffset": 174, "endOffset": 217}, {"referenceID": 49, "context": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al.", "startOffset": 174, "endOffset": 217}, {"referenceID": 3, "context": ", 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al.", "startOffset": 52, "endOffset": 73}, {"referenceID": 47, "context": ", 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013).", "startOffset": 28, "endOffset": 51}, {"referenceID": 35, "context": ", 2007), and machine translation (Meyer and Webber, 2013).", "startOffset": 33, "endOffset": 57}, {"referenceID": 3, "context": "Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016).", "startOffset": 128, "endOffset": 188}, {"referenceID": 23, "context": "Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016).", "startOffset": 128, "endOffset": 188}, {"referenceID": 52, "context": "Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016).", "startOffset": 128, "endOffset": 188}, {"referenceID": 30, "context": "Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al.", "startOffset": 187, "endOffset": 217}, {"referenceID": 28, "context": "Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al.", "startOffset": 226, "endOffset": 267}, {"referenceID": 49, "context": "Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al.", "startOffset": 226, "endOffset": 267}, {"referenceID": 2, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al.", "startOffset": 51, "endOffset": 78}, {"referenceID": 28, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015).", "startOffset": 104, "endOffset": 147}, {"referenceID": 34, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015).", "startOffset": 104, "endOffset": 147}, {"referenceID": 8, "context": "The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.", "startOffset": 48, "endOffset": 91}, {"referenceID": 41, "context": "The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.", "startOffset": 48, "endOffset": 91}, {"referenceID": 22, "context": ", 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Ji and Eisenstein, 2014), and the common use of trees as representations of document structure.", "startOffset": 70, "endOffset": 123}, {"referenceID": 2, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Ji and Eisenstein, 2014), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content.", "startOffset": 52, "endOffset": 462}, {"referenceID": 2, "context": ", 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Ji and Eisenstein, 2014), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content.", "startOffset": 52, "endOffset": 595}, {"referenceID": 6, "context": ", Spanish, German, Basque, Dutch, and Brazilian Portuguese), they tend to be smaller than their English equivalents and of limited value for modeling purposes (Braud et al., 2017).", "startOffset": 159, "endOffset": 179}, {"referenceID": 43, "context": "The main idea is to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechaar X iv :1 70 5.", "startOffset": 163, "endOffset": 205}, {"referenceID": 0, "context": "nism (Bahdanau et al., 2015).", "startOffset": 5, "endOffset": 28}, {"referenceID": 11, "context": "Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016).", "startOffset": 165, "endOffset": 185}, {"referenceID": 19, "context": "Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006).", "startOffset": 121, "endOffset": 161}, {"referenceID": 26, "context": "Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006).", "startOffset": 121, "endOffset": 161}, {"referenceID": 11, "context": "Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees.", "startOffset": 166, "endOffset": 205}, {"referenceID": 11, "context": "Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees. Specifically, they take into account the dependency structure of a sentence by viewing the attention mechanism as a graphical model over latent variables. They first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the forward-backward algorithm to normalize the scores with the marginal probabilities of a dependency tree. Without recourse to an external parser, their model learns meaningful taskspecific dependency structures, achieving competitive results in several sentence-level tasks. For document modeling, this approach has two drawbacks. Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006). As illustrated in Figure 1, the tree structure of a document can be flexible and the dependency edges may cross. Secondly, the forwardbackward pass in Kim et al. (2017) has complexity O(n3) with n being the number of text units, and without an efficient parallelizable implementation it is impractical for modeling documents.", "startOffset": 166, "endOffset": 1327}, {"referenceID": 30, "context": "Figure 1: The document is analyzed in the style of Rhetorical Structure Theory (Mann and Thompson, 1988), and represented as dependency tree following the conversion algorithm of Hayashi et al.", "startOffset": 79, "endOffset": 104}, {"referenceID": 19, "context": "Figure 1: The document is analyzed in the style of Rhetorical Structure Theory (Mann and Thompson, 1988), and represented as dependency tree following the conversion algorithm of Hayashi et al. (2016).", "startOffset": 179, "endOffset": 201}, {"referenceID": 32, "context": "Advantageously, it can induce nonprojective structures which are required for representing languages with free or flexible word order (McDonald and Satta, 2007).", "startOffset": 134, "endOffset": 160}, {"referenceID": 0, "context": "This type of intra-sentence attention encodes relationships between words within each sentence and differs from inter-sentence attention which has been widely applied to sequence transduction tasks like machine translation (Bahdanau et al., 2015) and learns the latent alignment between source and target sequences.", "startOffset": 223, "endOffset": 246}, {"referenceID": 11, "context": "Despite successful application of the above attention mechanism in sentiment analysis (Cheng et al., 2016) and entailment recognition (Parikh et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 39, "context": ", 2016) and entailment recognition (Parikh et al., 2016), the structural information under consideration is shallow, limited to word-word dependencies.", "startOffset": 35, "endOffset": 56}, {"referenceID": 1, "context": "Specifically, they normalize fij with a projective dependency tree using the forward-backward pass in the inside-outside algorithm (Baker, 1979):", "startOffset": 131, "endOffset": 144}, {"referenceID": 10, "context": "Despite successful application of the above attention mechanism in sentiment analysis (Cheng et al., 2016) and entailment recognition (Parikh et al., 2016), the structural information under consideration is shallow, limited to word-word dependencies. Since attention is computed as a simple probability distribution, it cannot capture structural dependencies such as trees (or graphs). Kim et al. (2017) induce richer internal structure by imposing structural constraints on the probability distribution computed by the attention mechanism.", "startOffset": 87, "endOffset": 404}, {"referenceID": 43, "context": "We follow previous work (Tang et al., 2015a; Yang et al., 2016) in modeling documents hierarchically by first obtaining representations for sentences and then composing those into a document representation.", "startOffset": 24, "endOffset": 63}, {"referenceID": 52, "context": "We follow previous work (Tang et al., 2015a; Yang et al., 2016) in modeling documents hierarchically by first obtaining representations for sentences and then composing those into a document representation.", "startOffset": 24, "endOffset": 63}, {"referenceID": 21, "context": "Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al.", "startOffset": 39, "endOffset": 80}, {"referenceID": 0, "context": "Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition Graves et al.", "startOffset": 180, "endOffset": 203}, {"referenceID": 51, "context": "(2013), and image caption generation (Xu et al., 2015).", "startOffset": 37, "endOffset": 54}, {"referenceID": 0, "context": "Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition Graves et al. (2013), and image caption generation (Xu et al.", "startOffset": 181, "endOffset": 248}, {"referenceID": 13, "context": "Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSMT output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts:", "startOffset": 24, "endOffset": 67}, {"referenceID": 37, "context": "Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSMT output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts:", "startOffset": 24, "endOffset": 67}, {"referenceID": 33, "context": "Much work in descriptive linguistics (Melc\u0306uk, 1988; Tesni\u00e9re, 1959) has advocated their suitability for representing syntactic structure across languages.", "startOffset": 37, "endOffset": 68}, {"referenceID": 45, "context": "Much work in descriptive linguistics (Melc\u0306uk, 1988; Tesni\u00e9re, 1959) has advocated their suitability for representing syntactic structure across languages.", "startOffset": 37, "endOffset": 68}, {"referenceID": 25, "context": "We use a variant of Kirchhoff\u2019s Matrix-Tree Theorem (Koo et al., 2007; Tutte, 1984) to calculate the marginal probability of each dependency edge P (zij = 1) of a non-projective dependency tree, and this probability is used as the attention weight that decides how much information is collected from child unit j to the parent unit i.", "startOffset": 52, "endOffset": 83}, {"referenceID": 25, "context": "lowing Koo et al. (2007):", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": "Details of the proof can be found in (Koo et al., 2007).", "startOffset": 37, "endOffset": 55}, {"referenceID": 20, "context": ", 2012) and in applications such as summarization (Hirao et al., 2013).", "startOffset": 50, "endOffset": 70}, {"referenceID": 24, "context": "In contrast to in Kim et al. (2017), training can be done efficiently.", "startOffset": 18, "endOffset": 36}, {"referenceID": 4, "context": "For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation.", "startOffset": 77, "endOffset": 98}, {"referenceID": 39, "context": "Sentence-level representations obtained by our model (with structured attention) were used to encode the premise and hypothesis by modifying the model of Parikh et al. (2016) as follows.", "startOffset": 154, "endOffset": 175}, {"referenceID": 40, "context": "We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings.", "startOffset": 36, "endOffset": 61}, {"referenceID": 15, "context": "All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the learning rate was set to 0.", "startOffset": 69, "endOffset": 89}, {"referenceID": 5, "context": "It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2016b).", "startOffset": 109, "endOffset": 150}, {"referenceID": 10, "context": "It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2016b).", "startOffset": 109, "endOffset": 150}, {"referenceID": 11, "context": "Consistent with previous work (Cheng et al., 2016; Parikh et al., 2016), we observe that simple attention brings performance improvements over no attention.", "startOffset": 30, "endOffset": 71}, {"referenceID": 39, "context": "Consistent with previous work (Cheng et al., 2016; Parikh et al., 2016), we observe that simple attention brings performance improvements over no attention.", "startOffset": 30, "endOffset": 71}, {"referenceID": 7, "context": "Exceptions include Cheng et al. (2016) and Parikh et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 7, "context": "Exceptions include Cheng et al. (2016) and Parikh et al. (2016) whose models include intra-attention encoding relationships between words within each sentence (see Equation (2)).", "startOffset": 19, "endOffset": 64}, {"referenceID": 4, "context": "It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2016b). The second block of Table 1 presents a version of our model without an intra-sentential attention mechanism as well as three variants with attention, assuming the structure of word-to-word relations and dependency trees. In the latter case we compare our matrix inversion based model against Kim et al.\u2019s (2017) forward-backward attention model.", "startOffset": 110, "endOffset": 464}, {"referenceID": 4, "context": "Classifier with handcrafted features (Bowman et al., 2015) 78.", "startOffset": 37, "endOffset": 58}, {"referenceID": 4, "context": "2 \u2014 300D LSTM encoders (Bowman et al., 2015) 80.", "startOffset": 23, "endOffset": 44}, {"referenceID": 5, "context": "0M 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 83.", "startOffset": 54, "endOffset": 75}, {"referenceID": 42, "context": "7M 100D LSTM with inter-attention (Rockt\u00e4schel et al., 2016) 83.", "startOffset": 34, "endOffset": 60}, {"referenceID": 48, "context": "5 252K 200D Matching LSTMs (Wang and Jiang, 2015) 86.", "startOffset": 27, "endOffset": 49}, {"referenceID": 11, "context": "9M 450D LSTMN with deep attention fusion (Cheng et al., 2016) 86.", "startOffset": 41, "endOffset": 61}, {"referenceID": 39, "context": "4M 200D Decomposable Attention over word embeddings (Parikh et al., 2016) 86.", "startOffset": 52, "endOffset": 73}, {"referenceID": 10, "context": "8 582K Enhanced BiLSTM Inference Model (Chen et al., 2016b) 88.", "startOffset": 39, "endOffset": 59}, {"referenceID": 10, "context": "(2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2016b; Cheng et al., 2016; Parikh et al., 2016).", "startOffset": 86, "endOffset": 147}, {"referenceID": 11, "context": "(2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2016b; Cheng et al., 2016; Parikh et al., 2016).", "startOffset": 86, "endOffset": 147}, {"referenceID": 39, "context": "(2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2016b; Cheng et al., 2016; Parikh et al., 2016).", "startOffset": 86, "endOffset": 147}, {"referenceID": 21, "context": "own model with tree matrix inversion slightly outperforms the forward-backward model of Kim et al. (2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 43, "context": "This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we followed the preprocessing introduced in (Tang et al., 2015a) and report experiments on their training, development, and testing partitions (80/10/10).", "startOffset": 225, "endOffset": 245}, {"referenceID": 14, "context": "IMDB reviews were obtained from Diao et al. (2014), who randomly crawled reviews for 50K movies.", "startOffset": 32, "endOffset": 51}, {"referenceID": 7, "context": "Experiments on this dataset perform 10-fold cross-validation following previous work (Brychc\u0131n and Habernal, 2013).", "startOffset": 85, "endOffset": 114}, {"referenceID": 7, "context": "Czech reviews were obtained from Brychc\u0131n and Habernal (2013). The dataset contains reviews from the Czech Movie Database2 each labeled as positive, neutral, or negative.", "startOffset": 33, "endOffset": 62}, {"referenceID": 43, "context": "0 \u2014 Paragraph vector (Tang et al., 2015a) 57.", "startOffset": 21, "endOffset": 41}, {"referenceID": 43, "context": "1 \u2014 \u2014- \u2014 Convolutional neural network (Tang et al., 2015a) 59.", "startOffset": 38, "endOffset": 58}, {"referenceID": 43, "context": "7 \u2014 \u2014 \u2014 \u2014 Convolutional gated RNN (Tang et al., 2015a) 63.", "startOffset": 34, "endOffset": 54}, {"referenceID": 43, "context": "5 \u2014 \u2014 \u2014 LSTM gated RNN (Tang et al., 2015a) 65.", "startOffset": 23, "endOffset": 43}, {"referenceID": 23, "context": "3 \u2014 \u2014 \u2014 RST-based recursive neural network (Ji and Smith, 2017) \u2014 \u2014 \u2014 75.", "startOffset": 43, "endOffset": 63}, {"referenceID": 52, "context": "7 \u2014 75D Hierarchical attention networks (Yang et al., 2016) 68.", "startOffset": 40, "endOffset": 59}, {"referenceID": 42, "context": "Regarding feature-based classification methods, results on Yelp and IBDM are taken from Tang et al. (2015a), on CZ movies from Brychc\u0131n and Habernal (2013), and Debates from Yogatama and Smith (2014).", "startOffset": 88, "endOffset": 108}, {"referenceID": 7, "context": "(2015a), on CZ movies from Brychc\u0131n and Habernal (2013), and Debates from Yogatama and Smith (2014).", "startOffset": 27, "endOffset": 56}, {"referenceID": 7, "context": "(2015a), on CZ movies from Brychc\u0131n and Habernal (2013), and Debates from Yogatama and Smith (2014). Wherever available we also provide the size of the hidden unit.", "startOffset": 27, "endOffset": 100}, {"referenceID": 46, "context": "Congressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.", "startOffset": 78, "endOffset": 99}, {"referenceID": 46, "context": "Congressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.S. floor debates in the House of Representatives for the year 2005. Each debate consists of a series of speech segments, each labeled by the vote (\u201cyea\u201d or \u201cnay\u201d) cast for the proposed bill by the the speaker of each segment. We used the pre-processed corpus from Yogatama and Smith (2014).3", "startOffset": 78, "endOffset": 421}, {"referenceID": 52, "context": "Following previous work (Yang et al., 2016), we only retained words appearing more than five times in building the vocabulary and replaced words with lesser frequencies with a special UNK token.", "startOffset": 24, "endOffset": 43}, {"referenceID": 36, "context": "Word embeddings were initialized by training word2vec (Mikolov et al., 2013) on the training and validation splits of each dataset.", "startOffset": 54, "endOffset": 76}, {"referenceID": 15, "context": "Parameters were optimized with Adagrad (Duchi et al., 2011), the learning rate was set to 0.", "startOffset": 39, "endOffset": 59}, {"referenceID": 51, "context": "Previous state-of-the-art results on the three review datasets were achieved by the hierarchical attention network of Yang et al. (2016), which models the document hierarchically with two GRUs and uses an attention mechanism to weigh the importance of each word and sentence.", "startOffset": 118, "endOffset": 137}, {"referenceID": 23, "context": "On the debates corpus, Ji and Smith (2017) obtained best results with a recursive neural network model operating on the output of an RST parser.", "startOffset": 23, "endOffset": 43}, {"referenceID": 31, "context": "Table 5: Descriptive statistics for dependency trees produced by our model and the Stanford parser (Manning et al., 2014) on the SNLI test set.", "startOffset": 99, "endOffset": 121}, {"referenceID": 12, "context": "Specifically, we used the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to extract the maximum spanning tree from the attention scores.", "startOffset": 52, "endOffset": 86}, {"referenceID": 16, "context": "Specifically, we used the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to extract the maximum spanning tree from the attention scores.", "startOffset": 52, "endOffset": 86}, {"referenceID": 31, "context": "Table 5 presents various statistics on the depth of the trees produced by our model on the SNLI test set and the Stanford dependency parser (Manning et al., 2014).", "startOffset": 140, "endOffset": 162}, {"referenceID": 29, "context": "Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism.", "startOffset": 38, "endOffset": 59}], "year": 2017, "abstractText": "In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.", "creator": "LaTeX with hyperref package"}}}