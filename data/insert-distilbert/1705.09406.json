{"id": "1705.09406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Multimodal Machine Learning: A Survey and Taxonomy", "abstract": "our experience of the world is multimodal - we see objects, hear distinct sounds, feel texture, smell visual odors, and taste flavors. modality recognition refers to the way in which something significant happens or is experienced and a research problem is characterized as multimodal when it includes analyzing multiple such modalities. in order for artificial intelligence to somehow make progress in understanding the world reality around us, it needs to be able to interpret such multimodal signals even together. multimodal machine learning aims to build working models that institutions can process and relate information from multiple modalities. it is a vibrant multi - disciplinary field of increasing importance and with extraordinary potential. expanding instead of focusing properly on specific multimodal applications, this paper surveys the recent advances discussed in functional multimodal machine learning itself and presents them in a common taxonomy. we simply go beyond the typical early and late fusion categorization concerns and identify broader challenges that are traditionally faced by multimodal machine learning, a namely : representation, role translation, alignment, fusion, and co - use learning. this new taxonomy will enable researchers to better understand the state of the field and identify future directions for future research.", "histories": [["v1", "Fri, 26 May 2017 01:35:31 GMT  (2070kb,D)", "http://arxiv.org/abs/1705.09406v1", null], ["v2", "Tue, 1 Aug 2017 17:39:39 GMT  (2070kb,D)", "http://arxiv.org/abs/1705.09406v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tadas baltru\\v{s}aitis", "chaitanya ahuja", "louis-philippe morency"], "accepted": false, "id": "1705.09406"}, "pdf": {"name": "1705.09406.pdf", "metadata": {"source": "CRF", "title": "Multimodal Machine Learning: A Survey and Taxonomy", "authors": ["Tadas Baltru\u0161aitis", "Chaitanya Ahuja", "Louis-Philippe Morency"], "emails": ["morency@cs.cmu.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014Multimodal, machine learning, introductory, survey.\nF"}, {"heading": "1 INTRODUCTION", "text": "THE world surrounding us involves multiple modalities\u2014 we see objects, hear sounds, feel texture, smell odors, and so on. In general terms, a modality refers to the way in which something happens or is experienced. Most people associate the word modality with the sensory modalities which represent our primary channels of communication and sensation, such as vision or touch. A research problem or dataset is therefore characterized as multimodal when it includes multiple such modalities. In this paper we focus primarily, but not exclusively, on three modalities: natural language which can be both written or spoken; visual signals which are often represented with images or videos; and vocal signals which encode sounds and para-verbal information such as prosody and vocal expressions.\nIn order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret and reason about multimodal messages. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. From early research on audio-visual speech recognition to the recent explosion of interest in language and vision models, multimodal machine learning is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential.\nThe research field of Multimodal Machine Learning brings some unique challenges for computational researchers given the heterogeneity of the data. Learning from multimodal sources offers the possibility of capturing correspondences between modalities and gaining an in-depth understanding of natural phenomena. In this paper we identify and explore five core technical challenges (and related sub-challenges) surrounding multimodal machine learning. They are central to the multimodal setting and need to be\n\u2022 T. Baltrus\u030caitis, C. Ahuja and L-P. Morency are with the Language Technologies Institute, at Carnegie Mellon University, Pittsburgh, Pennsylvania E-mail: tbaltrus, cahuja, morency@cs.cmu.edu\ntackled in order to progress the field. Our taxonomy goes beyond the typical early and late fusion split, and consists of the five following challenges: 1) Representation A first fundamental challenge is learning\nhow to represent and summarize multimodal data in a way that exploits the complementarity and redundancy of multiple modalities. The heterogeneity of multimodal data makes it challenging to construct such representations. For example, language is often symbolic while audio and visual modalities will be represented as signals. 2) Translation A second challenge addresses how to translate (map) data from one modality to another. Not only is the data heterogeneous, but the relationship between modalities is often open-ended or subjective. For example, there exist a number of correct ways to describe an image and and one perfect translation may not exist. 3) Alignment A third challenge is to identify the direct relations between (sub)elements from two or more different modalities. For example, we may want to align the steps in a recipe to a video showing the dish being made. To tackle this challenge we need to measure similarity between different modalities and deal with possible longrange dependencies and ambiguities. 4) Fusion A fourth challenge is to join information from two or more modalities to perform a prediction. For example, for audio-visual speech recognition, the visual description of the lip motion is fused with the speech signal to predict spoken words. The information coming from different modalities may have varying predictive power and noise topology, with possibly missing data in at least one of the modalities. 5) Co-learning A fifth challenge is to transfer knowledge between modalities, their representation, and their predictive models. This is exemplified by algorithms of cotraining, conceptual grounding, and zero shot learning. Co-learning explores how knowledge learning from one modality can help a computational model trained on a ar X iv :1 70 5. 09 40 6v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 7\n2\ndifferent modality. This challenge is particularly relevant when one of the modalities has limited resources (e.g., annotated data). For each of these five challenges, we defines taxonomic classes and sub-classes to help structure the recent work in this emerging research field of multimodal machine learning. We start with a discussion of main applications of multimodal machine learning (Section 2) followed by a discussion on the recent developments on all of the five core technical challenges facing multimodal machine learning: representation (Section 3), translation (Section 4), alignment (Section 5), fusion (Section 6), and co-learning (Section 7). We conclude with a discussion in Section 8."}, {"heading": "2 APPLICATIONS: A HISTORICAL PERSPECTIVE", "text": "Multimodal machine learning enables a wide range of applications: from audio-visual speech recognition to image captioning. In this section we present a brief history of multimodal applications, from its beginnings in audiovisual speech recognition to a recently renewed interest in language and vision applications.\nOne of the earliest examples of multimodal research is audio-visual speech recognition (AVSR) [243]. It was motivated by the McGurk effect [138] \u2014 an interaction between hearing and vision during speech perception. When human subjects heard the syllable /ba-ba/ while watching the lips of a person saying /ga-ga/, they perceived a third sound: /da-da/. These results motivated many researchers from the speech community to extend their approaches with visual information. Given the prominence of hidden Markov models (HMMs) in the speech community at the time [95], it is without surprise that many of the early models for AVSR were based on various HMM extensions [23], [24]. While research into AVSR is not as common these days, it has seen renewed interest from the deep learning community [151].\nWhile the original vision of AVSR was to improve speech recognition performance (e.g., word error rate) in all contexts, the experimental results showed that the main advantage of visual information was when the speech signal was noisy (i.e., low signal-to-noise ratio) [75], [151], [243]. In\nother words, the captured interactions between modalities were supplementary rather than complementary. The same information was captured in both, improving the robustness of the multimodal models but not improving the speech recognition performance in noiseless scenarios.\nA second important category of multimodal applications comes from the field of multimedia content indexing and retrieval [11], [188]. With the advance of personal computers and the internet, the quantity of digitized multimedia content has increased dramatically [2]. While earlier approaches for indexing and searching these multimedia videos were keyword-based [188], new research problems emerged when trying to search the visual and multimodal content directly. This led to new research topics in multimedia content analysis such as automatic shot-boundary detection [123] and video summarization [52]. These research projects were supported by the TrecVid initiative from the National Institute of Standards and Technologies which introduced many high-quality datasets, including the multimedia event detection (MED) tasks started in 2011 [1].\nA third category of applications was established in the early 2000s around the emerging field of multimodal interaction with the goal of understanding human multimodal behaviors during social interactions. One of the first landmark datasets collected in this field is the AMI Meeting Corpus which contains more than 100 hours of video recordings of meetings, all fully transcribed and annotated [32]. Another important dataset is the SEMAINE corpus which allowed to study interpersonal dynamics between speakers and listeners [139]. This dataset formed the basis of the first audio-visual emotion challenge (AVEC) organized in 2011 [179]. The fields of emotion recognition and affective computing bloomed in the early 2010s thanks to strong technical advances in automatic face detection, facial landmark detection, and facial expression recognition [45]. The AVEC challenge continued annually afterward with the later instantiation including healthcare applications such as automatic assessment of depression and anxiety [208]. A great summary of recent progress in multimodal affect recognition was published by D\u2019Mello et al. [49]. Their metaanalysis revealed that a majority of recent work on mul-\n3 timodal affect recognition show improvement when using more than one modality, but this improvement is reduced when recognizing naturally-occurring emotions.\nMost recently, a new category of multimodal applications emerged with an emphasis on language and vision: media description. One of the most representative applications is image captioning where the task is to generate a text description of the input image [83]. This is motivated by the ability of such systems to help the visually impaired in their daily tasks [19]. The main challenges media description is evaluation: how to evaluate the quality of the predicted descriptions. The task of visual question-answering (VQA) was recently proposed to address some of the evaluation challenges [9], where the goal is to answer a specific question about the image.\nIn order to bring some of the mentioned applications to the real world we need to address a number of technical challenges facing multimodal machine learning. We summarize the relevant technical challenges for the above mentioned application areas in Table 1. One of the most important challenges is multimodal representation, the focus of our next section."}, {"heading": "3 MULTIMODAL REPRESENTATIONS", "text": "Representing raw data in a format that a computational model can work with has always been a big challenge in machine learning. Following the work of Bengio et al. [17] we use the term feature and representation interchangeably, with each referring to a vector or tensor representation of an entity, be it an image, audio sample, individual word, or a sentence. A multimodal representation is a representation of data using information from multiple such entities. Representing multiple modalities poses many difficulties: how to combine the data from heterogeneous sources; how to deal with different levels of noise; and how to deal with missing data. The ability to represent data in a meaningful way is crucial to multimodal problems, and forms the backbone of any model.\nGood representations are important for the performance of machine learning models, as evidenced behind the recent leaps in performance of speech recognition [79] and visual object classification [109] systems. Bengio et al. [17] identify a number of properties for good representations: smoothness, temporal and spatial coherence, sparsity, and natural clustering amongst others. Srivastava and Salakhutdinov [198] identify additional desirable properties for multimodal representations: similarity in the representation space should reflect the similarity of the corresponding concepts, the representation should be easy to obtain even in the absence of some modalities, and finally, it should be possible to fill-in missing modalities given the observed ones.\nThe development of unimodal representations has been extensively studied [5], [17], [122]. In the past decade there has been a shift from hand-designed for specific applications to data-driven. For example, one of the most famous image descriptors in the early 2000s, the scale invariant feature transform (SIFT) was hand designed [127], but currently most visual descriptions are learned from data using neural architectures such as convolutional neural networks (CNN) [109]. Similarly, in the audio domain, acoustic features\nsuch as Mel-frequency cepstral coefficients (MFCC) have been superseded by data-driven deep neural networks in speech recognition [79] and recurrent neural networks for para-linguistic analysis [207]. In natural language processing, the textual features initially relied on counting word occurrences in documents, but have been replaced datadriven word embeddings that exploit the word context [141]. While there has been a huge amount of work on unimodal representation, up until recently most multimodal representations involved simple concatenation of unimodal ones [49], but this has been rapidly changing.\nTo help understand the breadth of work, we propose two categories of multimodal representation: joint and coordinated. Joint representations combine the unimodal signals into the same representation space, while coordinated representations process unimodal signals separately, but enforce certain similarity constraints on them to bring them to what we term a coordinated space. An illustration of different multimodal representation types can be seen in Figure 1.\nMathematically, the joint representation is expressed as:\nxm = f(x1, . . . ,xn), (1)\nwhere the multimodal representation xm is computed using function f (e.g., a deep neural network, restricted Boltzmann machine, or a recurrent neural network) that relies on unimodal representations x1, . . .xn. While coordinated representation is as follows:\nf(x1) \u223c g(x2), (2)\nwhere each modality has a corresponding projection function (f and g above) that maps it into a coordinated multimodal space. While the projection into the multimodal space is independent for each modality, but the resulting space is coordinated between them (indicated as \u223c). Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces."}, {"heading": "3.1 Joint Representations", "text": "We start our discussion with joint representations that project unimodal representations together into a multimodal space (Equation 1). Joint representations are mostly (but not exclusively) used in tasks where multimodal data is present both during training and inference steps. The simplest example of a joint representation is a concatenation of individual modality features (also referred to as early fusion [49]). In this section we discuss more advanced methods for creating joint representations starting with neural networks, followed by graphical models and recurrent neural networks (representative works can be seen in Table 2). Neural networks have become a very popular method for unimodal data representation [17]. They are used to represent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217]. In this section we describe how neural networks can be used to construct a joint multimodal representation, how to train them, and what advantages they offer.\nIn general, neural networks are made up of successive building blocks of inner products followed by non-linear activation functions. In order to use a neural network as\n4 (a) Joint representation (b) Coordinated representations\nFigure 1: Structure of joint and coordinated representations. Joint representations are projected to the same space using all of the modalities as input. Coordinated representations, on the other hand, exist in their own space, but are coordinated through a similarity (e.g. Euclidean distance) or structure constraint (e.g. partial order).\na way to represent data, it is first trained to perform a specific task (e.g., recognizing objects in images). Due to the multilayer nature of deep neural networks each successive layer is hypothesized to represent the data in a more abstract way [17], hence it is common to use the final or penultimate neural layers as a form of data representation. To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227]. The joint multimodal representation is then be passed through multiple hidden layers itself or used directly for prediction. Such models can be trained end-to-end \u2014 learning both to represent the data and to perform a particular task. This results in a close relationship between multimodal representation learning and multimodal fusion when using neural networks.\nAs neural networks require a lot of labeled training data, it is common to pre-train such representations using an autoencoder on unsupervised data [80]. The model proposed by Ngiam et al. [151] extended the idea of using autoencoders to the multimodal domain. They used stacked denoising autoencoders to represent each modality individually and then fused them into a multimodal representation using another autoencoder layer. Similarly, Silberer and Lapata [184] proposed to use a multimodal autoencoder for the task of semantic concept grounding (see Section 7.2). In addition to using a reconstruction loss to train the representation they introduce a term into the loss function that uses the representation to predict object labels. It is also common to fine-tune the resulting representation on a particular task at hand as the representation constructed using an autoencoder is generic and not necessarily optimal for a specific task [217].\nThe major advantage of neural network based joint representations comes from their often superior performance and the ability to pre-train the representations in an unsupervised manner. The performance gain is, however, dependent on the amount of data available for training. One of the disadvantages comes from the model not being able to handle missing data naturally \u2014 although there are ways to alleviate this issue [151], [217]. Finally, deep networks are often difficult to train [69], but the field is making progress in better training techniques [196]. Probabilistic graphical models are another popular way to\nconstruct representations through the use of latent random variables [17]. In this section we describe how probabilistic graphical models are used to represent unimodal and multimodal data.\nThe most popular approaches for graphical-model based representation are deep Boltzmann machines (DBM) [176], that stack restricted Boltzmann machines (RBM) [81] as building blocks. Similar to neural networks, each successive layer of a DBM is expected to represent the data at a higher level of abstraction. The appeal of DBMs comes from the fact that they do not need supervised data for training [176]. As they are graphical models the representation of data is probabilistic, however it is possible to convert them to a deterministic neural network \u2014 but this loses the generative aspect of the model [176].\nWork by Srivastava and Salakhutdinov [197] introduced multimodal deep belief networks as a multimodal representation. Kim et al. [104] used a deep belief network for each modality and then combined them into joint representation for audiovisual emotion recognition. Huang and Kingsbury [86] used a similar model for AVSR, and Wu et al. [225] for audio and skeleton joint based gesture recognition.\nMultimodal deep belief networks have been extended to multimodal DBMs by Srivastava and Salakhutdinov [198]. Multimodal DBMs are capable of learning joint representations from multiple modalities by merging two or more undirected graphs using a binary layer of hidden units on top of them. They allow for the low level representations of each modality to influence each other after the joint training due to the undirected nature of the model.\nOuyang et al. [156] explore the use of multimodal DBMs for the task of human pose estimation from multi-view data. They demonstrate that integrating the data at a later stage \u2014 after unimodal data underwent nonlinear transformations \u2014 was beneficial for the model. Similarly, Suk et al. [199] use multimodal DBM representation to perform Alzheimer\u2019s disease classification from positron emission tomography and magnetic resonance imaging data.\nOne of the big advantages of using multimodal DBMs for learning multimodal representations is their generative nature, which allows for an easy way to deal with missing data \u2014 even if a whole modality is missing, the model has a natural way to cope. It can also be used to generate samples of one modality in the presence of the other one, or\n5\nboth modalities from the representation. Similar to autoencoders the representation can be trained in an unsupervised manner enabling the use of unlabeled data. The major disadvantage of DBMs is the difficulty of training them \u2014 high computational cost, and the need to use approximate variational training methods [198]. Sequential Representation. So far we have discussed models that can represent fixed length data, however, we often need to represent varying length sequences such as sentences, videos, or audio streams. In this section we describe models that can be used to represent such sequences.\nRecurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213]. So far RNNs have mostly been used to represent unimodal sequences of words, audio, or images, with most success in the language domain. Similar to traditional neural networks, the hidden state of an RNN can be seen as a representation of the data, i.e., the hidden state of RNN at timestep t can be seen as the summarization of the sequence up to that timestep. This is especially apparent in RNN encoderdecoder frameworks where the task of an encoder is to represent a sequence in the hidden state of an RNN in such a way that a decoder could reconstruct it [12].\nThe use of RNN representations has not been limited to the unimodal domain. An early use of constructing a multimodal representation using RNNs comes from work by Cosi et al. [42] on AVSR. They have also been used for representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166]."}, {"heading": "3.2 Coordinated Representations", "text": "An alternative to a joint multimodal representation is a coordinated representation. Instead of projecting the modalities together into a joint space, we learn separate representations for each modality but coordinate them through a constraint. We start our discussion with coordinated representations that enforce similarity between representations, moving on to coordinated representations that enforce more structure on the resulting space (representative works of different coordinated representations can be seen in Table 2).\nSimilarity models minimize the distance between modalities in the coordinated space. For example such models encourage the representation of the word dog and an image of a dog to have a smaller distance between them than distance between the word dog and an image of a car [61]. One of the earliest examples of such a representation comes from the work by Weston et al. [221], [222] on the WSABIE (web scale annotation by image embedding) model, where a coordinated space was constructed for images and their annotations. WSABIE constructs a simple linear map from image and textual features such that corresponding annotation and image representation would have a higher inner product (smaller cosine distance) between them than noncorresponding ones.\nMore recently, neural networks have become a popular way to construct coordinated representations, due to their ability to learn representations. Their advantage lies in the fact that they can jointly learn coordinated representations in an end-to-end manner. An example of such coordinated representation is DeViSE \u2014 a deep visual-semantic embedding [60]. DeViSE uses a similar inner product and ranking loss function to WSABIE but uses more complex image and word embeddings. Kiros et al. [105] extended this to sentence and image coordinated representation by using an LSTM model and a pairwise ranking loss to coordinate the feature space. Socher et al. [191] tackle the same task, but extend the language model to a dependency tree RNN to incorporate compositional semantics. A similar model was also proposed by Pan et al. [159], but using videos instead of images. Xu et al. [231] also constructed a coordinated space between videos and sentences using a \u3008subject, verb, object\u3009 compositional language model and a deep video model. This representation was then used for the task of cross-modal retrieval and video description.\nWhile the above models enforced similarity between representations, structured coordinated space models go beyond that and enforce additional constraints between the modality representations. The type of structure enforced is often based on the application, with different constraints for hashing, cross-modal retrieval, and image captioning.\nStructured coordinated spaces are commonly used in cross-modal hashing \u2014 compression of high dimensional data into compact binary codes with similar binary codes for similar objects [218]. The idea of cross-modal hashing is to create such codes for cross-modal retrieval [26], [93], [113]. Hashing enforces certain constraints on the resulting multimodal space: 1) it has to be an N -dimensional Hamming space \u2014 a binary representation with controllable number of bits; 2) the same object from different modalities has to have a similar hash code; 3) the space has to be similarity-preserving. Learning how to represent the data as a hash function attempts to enforce all of these three requirements [26], [113]. For example, Jiang and Li [92] introduced a method to learn such common binary space between sentence descriptions and corresponding images using end-to-end trainable deep learning techniques. While Cao et al. [31] extended the approach with a more complex LSTM sentence representation and introduced an outlier insensitive bit-wise margin loss and a relevance feedback based semantic similarity constraint. Similarly, Wang et al. [219] constructed a coordinated space in which images (and\n6\nsentences) with similar meanings are closer to each other. Another example of a structured coordinated representation comes from order-embeddings of images and language [212], [249]. The model proposed by Vendrov et al. [212] enforces a dissimilarity metric that is asymmetric and implements the notion of partial order in the multimodal space. The idea is to capture a partial order of the language and image representations \u2014 enforcing a hierarchy on the space; for example image of \u201ca woman walking her dog\u201c\u2192 text \u201cwoman walking her dog\u201d \u2192 text \u201cwoman walking\u201d. A similar model using denotation graphs was also proposed by Young et al. [238] where denotation graphs are used to induce a partial ordering. Lastly, Zhang et al. present how exploiting structured representations of text and images can create concept taxonomies in an unsupervised manner [249].\nA special case of a structured coordinated space is one based on canonical correlation analysis (CCA) [84]. CCA computes a linear projection which maximizes the correlation between two random variables (in our case modalities) and enforces orthogonality of the new space. CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187]. Extensions to CCA attempt to construct a correlation maximizing nonlinear projection [7], [116]. Kernel canonical correlation analysis (KCCA) [116] uses reproducing kernel Hilbert spaces for projection. However, as the approach is nonparametric it scales poorly with the size of the training set and has issues with very large real-world datasets. Deep canonical correlation analysis (DCCA) [7] was introduced as an alternative to KCCA and addresses the scalability issue, it was also shown to lead to better correlated representation space. Similar correspondence autoencoder [57] and deep correspondence RBMs [56] have also been proposed for cross-modal retrieval.\nCCA, KCCA, and DCCA are unsupervised techniques and only optimize the correlation over the representations, thus mostly capturing what is shared across the modalities. Deep canonically correlated autoencoders [220] also include an autoencoder based data reconstruction term. This encourages the representation to also capture modality specific information. Semantic correlation maximization method [248] also encourages semantic relevance, while retaining correlation maximization and orthogonality of the resulting space \u2014 this leads to a combination of CCA and cross-modal hashing techniques."}, {"heading": "3.3 Discussion", "text": "In this section we identified two major types of multimodal representations \u2014 joint and coordinated. Joint representations project multimodal data into a common space and are best suited for situations when all of the modalities are present during inference. They have been extensively used for AVSR, affect, and multimodal gesture recognition. Coordinated representations, on the other hand, project each modality into a separate but coordinated space, making them suitable for applications where only one modality is present at test time, such as: multimodal retrieval and translation (Section 4), grounding (Section 7.2), and zero shot learning (Section 7.2). Finally, while joint representations have been used in situations to construct representations of\nmore than two modalities, coordinated spaces have, so far, been mostly limited to two modalities."}, {"heading": "4 TRANSLATION", "text": "A big part of multimodal machine learning is concerned with translating (mapping) from one modality to another. Given an entity in one modality the task is to generate the same entity in a different modality. For example given an image we might want to generate a sentence describing it or given a textual description generate an image matching it. Multimodal translation is a long studied problem, with early work in speech synthesis [88], visual speech generation [136] video description [107], and cross-modal retrieval [169].\nMore recently, multimodal translation has seen renewed interest due to combined efforts of the computer vision and natural language processing (NLP) communities [18] and recent availability of large multimodal datasets [37], [205]. A particularly popular problem is visual scene description, also known as image [214] and video captioning [213], which acts as a great test bed for a number of computer vision and NLP problems. To solve it, we not only need to fully understand the visual scene and to identify its salient parts, but also to produce grammatically correct and comprehensive yet concise sentences describing it.\nWhile the approaches to multimodal translation are very broad and are often modality specific, they share a number of unifying factors. We categorize them into two types \u2014 example-based, and generative. Example-based models use a dictionary when translating between the modalities. Generative models, on the other hand, construct a model that is able to produce a translation. This distinction is similar to the one between non-parametric and parametric machine learning approaches and is illustrated in Figure 2, with representative examples summarized in Table 3.\nGenerative models are arguably more challenging to build as they require the ability to generate signals or sequences of symbols (e.g., sentences). This is difficult for any modality \u2014 visual, acoustic, or verbal, especially when temporally and structurally consistent sequences need to be generated. This led to many of the early multimodal translation systems relying on example-based translation. However,\n7 (a) Example-based (b) Generative\nFigure 2: Overview of example-based and generative multimodal translation. The former retrieves the best translation from a dictionary, while the latter first trains a translation model on the dictionary and then uses that model for translation.\nthis has been changing with the advent of deep learning models that are capable of generating images [171], [210], sounds [157], [209], and text [12]."}, {"heading": "4.1 Example-based", "text": "Example-based algorithms are restricted by their training data \u2014 dictionary (see Figure 2a). We identify two types of such algorithms: retrieval based, and combination based. Retrieval-based models directly use the retrieved translation without modifying it, while combination-based models rely on more complex rules to create translations based on a number of retrieved instances. Retrieval-based models are arguably the simplest form of multimodal translation. They rely on finding the closest sample in the dictionary and using that as the translated result. The retrieval can be done in unimodal space or intermediate semantic space.\nGiven a source modality instance to be translated, unimodal retrieval finds the closest instances in the dictionary in the space of the source \u2014 for example, visual feature space for images. Such approaches have been used for visual speech synthesis, by retrieving the closest matching visual example of the desired phoneme [25]. They have also been used in concatenative text-to-speech systems [88]. More recently, Ordonez et al. [155] used unimodal retrieval to generate image descriptions by using global image features to retrieve caption candidates [155]. Yagcioglu et al. [232] used a CNN-based image representation to retrieve visually similar images using adaptive neighborhood selection. Devlin et al. [48] demonstrated that a simple k-nearest neighbor retrieval with consensus caption selection achieves competitive translation results when compared to more complex generative approaches. The advantage of such unimodal retrieval approaches is that they only require the representation of a single modality through which we are performing retrieval. However, they often require an extra processing step such as re-ranking of retrieved translations [135], [155], [232]. This indicates a major problem with this approach \u2014 similarity in unimodal space does not always imply a good translation.\nAn alternative is to use an intermediate semantic space for similarity comparison during retrieval. An early example of a hand crafted semantic space is one used by\nFarhadi et al. [55]. They map both sentences and images to a space of \u3008object, action, scene\u3009, retrieval of relevant caption to an image is then performed in that space. In contrast to hand-crafting a representation, Socher et al. [191] learn a coordinated representation of sentences and CNN visual features (see Section 3.2 for description of coordinated spaces). They use the model for both translating from text to images and from images to text. Similarly, Xu et al. [231] used a coordinated space of videos and their descriptions for cross-modal retrieval. Jiang and Li [93] and Cao et al. [31] use cross-modal hashing to perform multimodal translation from images to sentences and back, while Hodosh et al. [83] use a multimodal KCCA space for imagesentence retrieval. Instead of aligning images and sentences globally in a common space, Karpathy et al. [99] propose a multimodal similarity metric that internally aligns image fragments (visual objects) together with sentence fragments (dependency tree relations).\nRetrieval approaches in semantic space tend to perform better than their unimodal counterparts as they are retrieving examples in a more meaningful space that reflects both modalities and that is often optimized for retrieval. Furthermore, they allow for bi-directional translation, which is not straightforward with unimodal methods. However, they require manual construction or learning of such a semantic space, which often relies on the existence of large training dictionaries (datasets of paired samples). Combination-based models take the retrieval based approaches one step further. Instead of just retrieving examples from the dictionary, they combine them in a meaningful way to construct a better translation. Combination based media description approaches are motivated by the fact that sentence descriptions of images share a common and simple structure that could be exploited. Most often the rules for combinations are hand crafted or based on heuristics.\nKuznetsova et al. [114] first retrieve phrases that describe visually similar images and then combine them to generate novel descriptions of the query image by using Integer Linear Programming with a number of hand crafted rules. Gupta et al. [74] first find k images most similar to the source image, and then use the phrases extracted from their captions to generate a target sentence. Lebret et al. [119] use a CNN-based image representation to infer phrases that describe it. The predicted phrases are then combined using\n8 a trigram constrained language model. A big problem facing example-based approaches for translation is that the model is the entire dictionary \u2014 making the model large and inference slow (although, optimizations such as hashing alleviate this problem). Another issue facing example-based translation is that it is unrealistic to expect that a single comprehensive and accurate translation relevant to the source example will always exist in the dictionary \u2014 unless the task is simple or the dictionary is very large. This is partly addressed by combination models that are able to construct more complex structures. However, they are only able to perform translation in one direction, while semantic space retrieval-based models are able to perform it both ways."}, {"heading": "4.2 Generative approaches", "text": "Generative approaches to multimodal translation construct models that can perform multimodal translation given a unimodal source instance. It is a challenging problem as it requires the ability to both understand the source modality and to generate the target sequence or signal. As discussed in the following section, this also makes such methods much more difficult to evaluate, due to large space of possible correct answers.\nIn this survey we focus on the generation of three modalities: language, vision, and sound. Language generation has been explored for a long time [170], with a lot of recent attention for tasks such as image and video description [18]. Speech and sound generation has also seen a lot of work with a number of historical [88] and modern approaches [157], [209]. Photo-realistic image generation has been less explored, and is still in early stages [132], [171], however, there have been a number of attempts at generating abstract scenes [253], computer graphics [44], and talking heads [6].\nWe identify three broad categories of generative models: grammar-based, encoder-decoder, and continuous generation models. Grammar based models simplify the task by restricting the target domain by using a grammar, e.g., by generating restricted sentences based on a \u3008subject, object, verb\u3009 template. Encoder-decoder models first encode the source modality to a latent representation which is then used by a decoder to generate the target modality. Continuous generation models generate the target modality continuously based on a stream of source modality inputs and are most suited for translating between temporal sequences \u2014 such as text-to-speech. Grammar-based models rely on a pre-defined grammar for generating a particular modality. They start by detecting high level concepts from the source modality, such as objects in images and actions from videos. These detections are then incorporated together with a generation procedure based on a pre-defined grammar to result in a target modality.\nKojima et al. [107] proposed a system to describe human behavior in a video using the detected position of the person\u2019s head and hands and rule based natural language generation that incorporates a hierarchy of concepts and actions. Barbu et al. [14] proposed a video description model that generates sentences of the form: who did what to whom and where and how they did it. The system was based on handcrafted object and event classifiers and used\na restricted grammar suitable for the task. Guadarrama et al. [73] predict \u3008subject, verb, object\u3009 triplets describing a video using semantic hierarchies that use more general words in case of uncertainty. Together with a language model their approach allows for translation of verbs and nouns not seen in the dictionary.\nTo describe images, Yao et al. [235] propose to use an and-or graph-based model together with domain-specific lexicalized grammar rules, targeted visual representation scheme, and a hierarchical knowledge ontology. Li et al. [121] first detect objects, visual attributes, and spatial relationships between objects. They then use an n-gram language model on the visually extracted phrases to generate \u3008subject, preposition, object\u3009 style sentences. Mitchell et al. [142] use a more sophisticated tree-based language model to generate syntactic trees instead of filling in templates, leading to more diverse descriptions. A majority of approaches represent the whole image jointly as a bag of visual objects without capturing their spatial and semantic relationships. To address this, Elliott et al. [50] propose to explicitly model proximity relationships of objects for image description generation.\nSome grammar-based approaches rely on graphical models to generate the target modality. An example includes BabyTalk [112], which given an image generates \u3008object, preposition, object\u3009 triplets, that are used together with a conditional random field to construct the sentences. Yang et al. [233] predict a set of \u3008noun, verb, scene, preposition\u3009 candidates using visual features extracted from an image and combine them into a sentence using a statistical language model and hidden Markov model style inference. A similar approach has been proposed by Thomason et al. [204], where a factor graph model is used for video description of the form \u3008subject, verb, object, place\u3009. The factor model exploits language statistics to deal with noisy visual representations. Going the other way Zitnick et al. [253] propose to use conditional random fields to generate abstract visual scenes based on language triplets extracted from sentences.\nAn advantage of grammar-based methods is that they are more likely to generate syntactically (in case of language) or logically correct target instances as they use predefined templates and restricted grammars. However, this limits them to producing formulaic rather than creative translations. Furthermore, grammar-based methods rely on complex pipelines for concept detection, with each concept requiring a separate model and a separate training dataset. Encoder-decoder models based on end-to-end trained neural networks are currently some of the most popular techniques for multimodal translation. The main idea behind the model is to first encode a source modality into a vectorial representation and then to use a decoder module to generate the target modality, all this in a single pass pipeline. Although, first used for machine translation [97], such models have been successfully used for image captioning [134], [214], and video description [174], [213]. So far, encoderdecoder models have been mostly used to generate text, but they can also be used to generate images [132], [171], and continuos generation of speech and sound [157], [209].\nThe first step of the encoder-decoder model is to encode the source object, this is done in modality specific way.\n9 Popular models to encode acoustic signals include RNNs [34] and DBNs [79]. Most of the work on encoding words sentences uses distributional semantics [141] and variants of RNNs [12]. Images are most often encoded using convolutional neural networks (CNN) [109], [185]. While learned CNN representations are common for encoding images, this is not the case for videos where hand-crafted features are still commonly used [174], [204]. While it is possible to use unimodal representations to encode the source modality, it has been shown that using a coordinated space (see Section 3.2) leads to better results [105], [159], [231].\nDecoding is most often performed by an RNN or an LSTM using the encoded representation as the initial hidden state [53], [132], [214], [215]. A number of extensions have been proposed to traditional LSTM models to aid in the task of translation. A guide vector could be used to tightly couple the solutions in the image input [91]. Venugopalan et al. [213] demonstrate that it is beneficial to pre-train a decoder LSTM for image captioning before fine-tuning it to video description. Rohrbach et al. [174] explore the use of various LSTM architectures (single layer, multilayer, factored) and a number of training and regularization techniques for the task of video description.\nA problem facing translation generation using an RNN is that the model has to generate a description from a single vectorial representation of the image, sentence, or video. This becomes especially difficult when generating long sequences as these models tend to forget the initial input. This has been partly addressed by neural attention models (see Section 5.2) that allow the network to focus on certain parts of an image [230], sentence [12], or video [236] during generation.\nGenerative attention-based RNNs have also been used for the task of generating images from sentences [132], while the results are still far from photo-realistic they show a lot of promise. More recently, a large amount of progress has been made in generating images using generative adversarial networks [71], which have been used as an alternative to RNNs for image generation from text [171].\nWhile neural network based encoder-decoder systems have been very successful they still face a number of issues. Devlin et al. [48] suggest that it is possible that the network is memorizing the training data rather than learning how to understand the visual scene and generate it. This is based on the observation that k-nearest neighbor models perform very similarly to those based on generation. Furthermore, such models often require large quantities of data for training. Continuous generation models are intended for sequence translation and produce outputs at every timestep in an online manner. These models are useful when translating from a sequence to a sequence such as text to speech, speech to text, and video to text. A number of different techniques have been proposed for such modeling \u2014 graphical models, continuous encoder-decoder approaches, and various other regression or classification techniques. The extra difficulty that needs to be tackled by these models is the requirement of temporal consistency between modalities.\nA lot of early work on sequence to sequence translation used graphical or latent variable models. Deena and Galata [46] proposed to use a shared Gaussian process latent\nvariable model for audio-based visual speech synthesis. The model creates a shared latent space between audio and visual features that can be used to generate one space from the other, while enforcing temporal consistency of visual speech at different timesteps. Hidden Markov models (HMM) have also been used for visual speech generation [203] and textto-speech [245] tasks. They have also been extended to use cluster adaptive training to allow for training on multiple speakers, languages, and emotions allowing for more control when generating speech signal [244] or visual speech parameters [6].\nEncoder-decoder models have recently become popular for sequence to sequence modeling. Owens et al. [157] used an LSTM to generate sounds resulting from drumsticks based on video. While their model is capable of generating sounds by predicting a cochleogram from CNN visual features, they found that retrieving a closest audio sample based on the predicted cochleogram led to best results. Directly modeling the raw audio signal for speech and music generation has been proposed by van den Oord et al. [209]. The authors propose using hierarchical fully convolutional neural networks, which show a large improvement over previous state-of-the-art for the task of speech synthesis. RNNs have also been used for speech to text translation (speech recognition) [72]. More recently encoder-decoder based continuous approach was shown to be good at predicting letters from a speech signal represented as a filter bank spectra [34] \u2014 allowing for more accurate recognition of rare and out of vocabulary words. Collobert et al. [41] demonstrate how to use a raw audio signal directly for speech recognition, eliminating the need for audio features.\nA lot of earlier work used graphical models for multimodal translation between continuous signals. However, these methods are being replaced by neural network encoder-decoder based techniques. Especially as they have recently been shown to be able to represent and generate complex visual and acoustic signals."}, {"heading": "4.3 Model evaluation and discussion", "text": "A major challenge facing multimodal translation methods is that they are very difficult to evaluate. While some tasks such as speech recognition have a single correct translation, tasks such as speech synthesis and media description do not. Sometimes, as in language translation, multiple answers are correct and deciding which translation is better is often subjective. Fortunately, there are a number of approximate automatic metrics that aid in model evaluation.\nOften the ideal way to evaluate a subjective task is through human judgment. That is by having a group of people evaluating each translation. This can be done on a Likert scale where each translation is evaluated on a certain dimension: naturalness and mean opinion score for speech synthesis [209], [244], realism for visual speech synthesis [6], [203], and grammatical and semantic correctness, relevance, order, and detail for media description [37], [112], [142], [213]. Another option is to perform preference studies where two (or more) translations are presented to the participant for preference comparison [203], [244]. However, while user studies will result in evaluation closest to human judgments they are time consuming and costly. Furthermore, they re-\n10\nquire care when constructing and conducting them to avoid fluency, age, gender and culture biases.\nWhile human studies are a gold standard for evaluation, a number of automatic alternatives have been proposed for the task of media description: BLEU [160], ROUGE [124], Meteor [47], and CIDEr [211]. These metrics are directly taken from (or are based on) work in machine translation and compute a score that measures the similarity between the generated and ground truth text. However, the use of them has faced a lot of criticism. Elliott and Keller [51] showed that sentence-level unigram BLEU is only weakly correlated with human judgments. Huang et al. [87] demonstrated that the correlation between human judgments and BLEU and Meteor is very low for visual story telling task. Furthermore, the ordering of approaches based on human judgments did not match that of the ordering using automatic metrics on the MS COCO challenge [37] \u2014 with a large number of algorithms outperforming humans on all the metrics. Finally, the metrics only work well when a number of reference translations is high [211], which is often unavailable, especially for current video description datasets [205]\nThese criticisms have led to Hodosh et al. [83] proposing to use retrieval as a proxy for image captioning evaluation, which they argue better reflects human judgments. Instead of generating captions, a retrieval based system ranks the available captions based on their fit to the image, and is then evaluated by assessing if the correct captions are given a high rank. As a number of caption generation models are generative they can be used directly to assess the likelihood of a caption given an image and are being adapted by image captioning community [99], [105]. Such retrieval based evaluation metrics have also been adopted by the video captioning community [175].\nVisual question-answering (VQA) [130] task was proposed partly due to the issues facing evaluation of image captioning. VQA is a task where given an image and a question about its content the system has to answer it. Evaluating such systems is easier due to the presence of a correct answer. However, it still faces issues such as ambiguity of certain questions and answers and question bias.\nWe believe that addressing the evaluation issue will be crucial for further success of multimodal translation systems. This will allow not only for better comparison between approaches, but also for better objectives to optimize."}, {"heading": "5 ALIGNMENT", "text": "We define multimodal alignment as finding relationships and correspondences between sub-components of instances from two or more modalities. For example, given an image and a caption we want to find the areas of the image corresponding to the caption\u2019s words or phrases [98]. Another example is, given a movie, aligning it to the script or the book chapters it was based on [252].\nWe categorize multimodal alignment into two types \u2013 implicit and explicit. In explicit alignment, we are explicitly interested in aligning sub-components between modalities, e.g., aligning recipe steps with the corresponding instructional video [131]. Implicit alignment is used as an intermediate (often latent) step for another task, e.g., image retrieval\nbased on text description can include an alignment step between words and image regions [99]. An overview of such approaches can be seen in Table 4 and is presented in more detail in the following sections."}, {"heading": "5.1 Explicit alignment", "text": "We categorize papers as performing explicit alignment if their main modeling objective is alignment between subcomponents of instances from two or more modalities. A very important part of explicit alignment is the similarity metric. Most approaches rely on measuring similarity between sub-components in different modalities as a basic building block. These similarities can be defined manually or learned from data.\nWe identify two types of algorithms that tackle explicit alignment \u2014 unsupervised and (weakly) supervised. The first type operates with no direct alignment labels (i.e., labeled correspondences) between instances from the different modalities. The second type has access to such (sometimes weak) labels. Unsupervised multimodal alignment tackles modality alignment without requiring any direct alignment labels. Most of the approaches are inspired from early work on alignment for statistical machine translation [27] and genome sequences [3], [111]. To make the task easier the approaches assume certain constrains on alignment, such as temporal ordering of sequence or an existence of a similarity metric between the modalities.\nDynamic time warping (DTW) [3], [111] is a dynamic programming approach that has been extensively used to align multi-view time series. DTW measures the similarity between two sequences and finds an optimal match between them by time warping (inserting frames). It requires the timesteps in the two sequences to be comparable and requires a similarity measure between them. DTW can be used directly for multimodal alignment by hand-crafting similarity metrics between modalities; for example Anguera et al. [8] use a manually defined similarity between graphemes and phonemes; and Tapaswi et al. [201] define a similarity between visual scenes and sentences based on appearance of same characters [201] to align TV shows and plot synopses. DTW-like dynamic programming approaches have also been used for multimodal alignment of text to speech [77] and video [202].\nAs the original DTW formulation requires a pre-defined similarity metric between modalities, it was extended using\n11\ncanonical correlation analysis (CCA) to map the modalities to a coordinated space. This allows for both aligning (through DTW) and learning the mapping (through CCA) between different modality streams jointly and in an unsupervised manner [180], [250], [251]. While CCA based DTW models are able to find multimodal data alignment under a linear transformation, they are not able to model nonlinear relationships. This has been addressed by the deep canonical time warping approach [206], which can be seen as a generalization of deep CCA and DTW.\nVarious graphical models have also been popular for multimodal sequence alignment in an unsupervised manner. Early work by Yu and Ballard [239] used a generative graphical model to align visual objects in images with spoken words. A similar approach was taken by Cour et al. [43] to align movie shots and scenes to the corresponding screenplay. Malmaud et al. [131] used a factored HMM to align recipes to cooking videos, while Noulas et al. [154] used a dynamic Bayesian network to align speakers to videos. Naim et al. [147] matched sentences with corresponding video frames using a hierarchical HMM model to align sentences with frames and a modified IBM [27] algorithm for word and object alignment [15]. This model was then extended to use latent conditional random fields for alignments [146] and to incorporate verb alignment to actions in addition to nouns and objects [195].\nBoth DTW and graphical model approaches for alignment allow for restrictions on alignment, e.g. temporal consistency, no large jumps in time, and monotonicity. While DTW extensions allow for learning both the similarity metric and alignment jointly, graphical model based approaches require expert knowledge for construction [43], [239]. Supervised alignment methods rely on labeled aligned instances. They are used to train similarity measures that are used for aligning modalities.\nA number of supervised sequence alignment techniques take inspiration from unsupervised ones. Bojanowski et al. [21], [22] proposed a method similar to canonical time warping, but have also extended it to take advantage of existing (weak) supervisory alignment data for model training. Plummer et al. [161] used CCA to find a coordinated space between image regions and phrases for alignment. Gebru et al. [65] trained a Gaussian mixture model and performed semi-supervised clustering together with an unsupervised latent-variable graphical model to align speakers in an audio channel with their locations in a video. Kong et al. [108] trained a Markov random field to align objects in 3D scenes to nouns and pronouns in text descriptions.\nDeep learning based approaches are becoming popular for explicit alignment (specifically for measuring similarity) due to very recent availability of aligned datasets in the language and vision communities [133], [161]. Zhu et al. [252] aligned books with their corresponding movies/scripts by training a CNN to measure similarities between scenes and text. Mao et al. [133] used an LSTM language model and a CNN visual one to evaluate the quality of a match between a referring expression and an object in an image. Yu et al. [242] extended this model to include relative appearance and context information that allows to better disambiguate between objects of the same type. Finally, Hu et al. [85] used an LSTM based scoring function to find similarities between\nimage regions and their descriptions."}, {"heading": "5.2 Implicit alignment", "text": "In contrast to explicit alignment, implicit alignment is used as an intermediate (often latent) step for another task. This allows for better performance in a number of tasks including speech recognition, machine translation, media description, and visual question-answering. Such models do not explicitly align data and do not rely on supervised alignment examples, but learn how to latently align the data during model training. We identify two types of implicit alignment models: earlier work based on graphical models, and more modern neural network methods. Graphical models have seen some early work used to better align words between languages for machine translation [216] and alignment of speech phonemes with their transcriptions [186]. However, they require manual construction of a mapping between the modalities, for example a generative phone model that maps phonemes to acoustic features [186]. Constructing such models requires training data or human expertise to define them manually. Neural networks Translation (Section 4) is an example of a modeling task that can often be improved if alignment is performed as a latent intermediate step. As we mentioned before, neural networks are popular ways to address this translation problem, using either an encoder-decoder model or through cross-modal retrieval. When translation is performed without implicit alignment, it ends up putting a lot of weight on the encoder module to be able to properly summarize the whole image, sentence or a video with a single vectorial representation.\nA very popular way to address this is through attention [12], which allows the decoder to focus on sub-components of the source instance. This is in contrast with encoding all source sub-components together, as is performed in a conventional encoder-decoder model. An attention module will tell the decoder to look more at targeted sub-components of the source to be translated \u2014 areas of an image [230], words of a sentence [12], segments of an audio sequence [34], [38], frames and regions in a video [236], [241], and even parts of an instruction [140]. For example, in image captioning instead of encoding an entire image using a CNN, an attention mechanism will allow the decoder (typically an RNN) to focus on particular parts of the image when generating each successive word [230]. The attention module which learns what part of the image to focus on is typically a shallow neural network and is trained end-to-end together with a target task (e.g., translation).\nAttention models have also been successfully applied to question answering tasks, as they allow for aligning the words in a question with sub-components of an information source such as a piece of text [228], an image [62], or a video sequence [246]. This both allows for better performance in question answering and leads to better model interpretability [4]. In particular, different types of attention models have been proposed to address this problem, including hierarchical [128], stacked [234], and episodic memory attention [228].\nAnother neural alternative for aligning images with captions for cross-modal retrieval was proposed by Karpathy\n12\net al. [98], [99]. Their proposed model aligns sentence fragments to image regions by using a dot product similarity measure between image region and word representations. While it does not use attention, it extracts a latent alignment between modalities through a similarity measure that is learned indirectly by training a retrieval model."}, {"heading": "5.3 Discussion", "text": "Multimodal alignment faces a number of difficulties: 1) there are few datasets with explicitly annotated alignments; 2) it is difficult to design similarity metrics between modalities; 3) there may exist multiple possible alignments and not all elements in one modality have correspondences in another. Earlier work on multimodal alignment focused on aligning multimodal sequences in an unsupervised manner using graphical models and dynamic programming techniques. It relied on hand-defined measures of similarity between the modalities or learnt them in an unsupervised manner. With recent availability of labeled training data supervised learning of similarities between modalities has become possible. However, unsupervised techniques of learning to jointly align and translate or fuse data have also become popular."}, {"heading": "6 FUSION", "text": "Multimodal fusion is one of the original topics in multimodal machine learning, with previous surveys emphasizing early, late and hybrid fusion approaches [49], [247]. In technical terms, multimodal fusion is the concept of integrating information from multiple modalities with the goal of predicting an outcome measure: a class (e.g., happy vs. sad) through classification, or a continuous value (e.g., positivity of sentiment) through regression. It is one of the most researched aspects of multimodal machine learning with work dating to 25 years ago [243].\nThe interest in multimodal fusion arises from three main benefits it can provide. First, having access to multiple modalities that observe the same phenomenon may allow for more robust predictions. This has been especially explored and exploited by the AVSR community [163]. Second, having access to multiple modalities might allow us to capture complementary information \u2014 something that is not visible in individual modalities on their own. Third, a multimodal system can still operate when one of the modalities is missing, for example recognizing emotions from the visual signal when the person is not speaking [49].\nMultimodal fusion has a very broad range of applications, including audio-visual speech recognition (AVSR) [163], multimodal emotion recognition [192], medical image analysis [89], and multimedia event detection [117]. There are a number of reviews on the subject [11], [163], [188], [247]. Most of them concentrate on multimodal fusion for a particular task, such as multimedia analysis, information retrieval or emotion recognition. In contrast, we concentrate on the machine learning approaches themselves and the technical challenges associated with these approaches.\nWhile some prior work used the term multimodal fusion to include all multimodal algorithms, in this survey paper we classify approaches as fusion category when the multimodal integration is performed at the later prediction\nstages, with the goal of predicting outcome measures. In recent work, the line between multimodal representation and fusion has been blurred for models such as deep neural networks where representation learning is interlaced with classification or regression objectives. As we will describe in this section, this line is clearer for other approaches such as graphical models and kernel-based methods.\nWe classify multimodal fusion into two main categories: model-agnostic approaches (Section 6.1) that are not directly dependent on a specific machine learning method; and model-based (Section 6.2) approaches that explicitly address fusion in their construction \u2014 such as kernel-based approaches, graphical models, and neural networks. An overview of such approaches can be seen in Table 5."}, {"heading": "6.1 Model-agnostic approaches", "text": "Historically, the vast majority of multimodal fusion has been done using model-agnostic approaches [49]. Such approaches can be split into early (i.e., feature-based), late (i.e., decision-based) and hybrid fusion [11]. Early fusion integrates features immediately after they are extracted (often by simply concatenating their representations). Late fusion on the other hand performs integration after each of the modalities has made a decision (e.g., classification or regression). Finally, hybrid fusion combines outputs from early fusion and individual unimodal predictors. An advantage of model agnostic approaches is that they can be implemented using almost any unimodal classifiers or regressors.\nEarly fusion could be seen as an initial attempt by multimodal researchers to perform multimodal representation learning \u2014 as it can learn to exploit the correlation and interactions between low level features of each modality. Furthermore it only requires the training of a single model, making the training pipeline easier compared to late and hybrid fusion.\nIn contrast, late fusion uses unimodal decision values and fuses them using a fusion mechanism such as averaging [181], voting schemes [144], weighting based on channel noise [163] and signal variance [52], or a learned model [68], [168]. It allows for the use of different models for each modality as different predictors can model each individual modality better, allowing for more flexibility. Furthermore, it makes it easier to make predictions when one or more of\n13\nthe modalities is missing and even allows for training when no parallel data is available. However, late fusion ignores the low level interaction between the modalities.\nHybrid fusion attempts to exploit the advantages of both of the above described methods in a common framework. It has been used successfully for multimodal speaker identification [226] and multimedia event detection (MED) [117]."}, {"heading": "6.2 Model-based approaches", "text": "While model-agnostic approaches are easy to implement using unimodal machine learning methods, they end up using techniques that are not designed to cope with multimodal data. In this section we describe three categories of approaches that are designed to perform multimodal fusion: kernel-based methods, graphical models, and neural networks. Multiple kernel learning (MKL) methods are an extension to kernel support vector machines (SVM) that allow for the use of different kernels for different modalities/views of the data [70]. As kernels can be seen as similarity functions between data points, modality-specific kernels in MKL allows for better fusion of heterogeneous data.\nMKL approaches have been an especially popular method for fusing visual descriptors for object detection [30], [66] and only recently have been overtaken by deep learning methods for the task [109]. They have also seen use for multimodal affect recognition [35], [90], [182], multimodal sentiment analysis [162], and multimedia event detection (MED) [237]. Furthermore, McFee and Lanckriet [137] proposed to use MKL to perform musical artist similarity ranking from acoustic, semantic and social view data. Finally, Liu et al. [125] used MKL for multimodal fusion in Alzheimer\u2019s disease classification. Their broad applicability demonstrates the strength of such approaches in various domains and across different modalities.\nBesides flexibility in kernel selection, an advantage of MKL is the fact that the loss function is convex, allowing for model training using standard optimization packages and global optimum solutions [70]. Furthermore, MKL can be used to both perform regression and classification. One of the main disadvantages of MKL is the reliance on training data (support vectors) during test time, leading to slow inference and a large memory footprint. Graphical models are another family of popular methods for multimodal fusion. In this section we overview work done on multimodal fusion using shallow graphical models. A description of deep graphical models such as deep belief networks can be found in Section 3.1.\nMajority of graphical models can be classified into two main categories: generative \u2014 modeling joint probability; or discriminative \u2014 modeling conditional probability [200]. Some of the earliest approaches to use graphical models for multimodal fusion include generative models such as coupled [149] and factorial hidden Markov models [67] alongside dynamic Bayesian networks [64]. A more recentlyproposed multi-stream HMM method proposes dynamic weighting of modalities for AVSR [75].\nArguably, generative models lost popularity to discriminative ones such as conditional random fields (CRF) [115] which sacrifice the modeling of joint probability for predictive power. A CRF model was used to better segment\nimages by combining visual and textual information of image description [59]. CRF models have been extended to model latent states using hidden conditional random fields [165] and have been applied to multimodal meeting segmentation [173]. Other multimodal uses of latent variable discriminative graphical models include multi-view hidden CRF [194] and latent variable models [193]. More recently Jiang et al. [93] have shown the benefits of multimodal hidden conditional random fields for the task of multimedia classification. While most graphical models are aimed at classification, CRF models have been extended to a continuous version for regression [164] and applied in multimodal settings [13] for audio visual emotion recognition.\nThe benefit of graphical models is their ability to easily exploit spatial and temporal structure of the data, making them especially popular for temporal modeling tasks, such as AVSR and multimodal affect recognition. They also allow to build in human expert knowledge into the models. and often lead to interpretable models. Neural Networks have been used extensively for the task of multimodal fusion [151]. The earliest examples of using neural networks for multi-modal fusion come from work on AVSR [163]. Nowadays they are being used to fuse information for visual and media question answering [63], [130], [229], gesture recognition [150], affect analysis [96], [153], and video description generation [94]. While the modalities used, architectures, and optimization techniques might differ, the general idea of fusing information in joint hidden layer of a neural network remains the same.\nNeural networks have also been used for fusing temporal multimodal information through the use of RNNs and LSTMs. One of the earlier such applications used a bidirectional LSTM was used to perform audio-visual emotion classification [224]. More recently, Wo\u0308llmer et al. [223] used LSTM models for continuous multimodal emotion recognition, demonstrating its advantage over graphical models and SVMs. Similarly, Nicolaou et al. [152] used LSTMs for continuous emotion prediction. Their proposed method used an LSTM to fuse the results from a modality specific (audio and facial expression) LSTMs.\nApproaching modality fusion through recurrent neural networks has been used in various image captioning tasks, example models include: neural image captioning [214] where a CNN image representation is decoded using an LSTM language model, gLSTM [91] which incorporates the image data together with sentence decoding at every time step fusing the visual and sentence data in a joint representation. A more recent example is the multi-view LSTM (MV-LSTM) model proposed by Rajagopalan et al. [166]. MV-LSTM model allows for flexible fusion of modalities in the LSTM framework by explicitly modeling the modalityspecific and cross-modality interactions over time.\nA big advantage of deep neural network approaches in data fusion is their capacity to learn from large amount of data. Secondly, recent neural architectures allow for end-toend training of both the multimodal representation component and the fusion component. Finally, they show good performance when compared to non neural network based system and are able to learn complex decision boundaries that other approaches struggle with.\nThe major disadvantage of neural network approaches\n14\nis their lack of interpretability. It is difficult to tell what the prediction relies on, and which modalities or features play an important role. Furthermore, neural networks require large training datasets to be successful."}, {"heading": "6.3 Discussion", "text": "Multimodal fusion has been a widely researched topic with a large number of approaches proposed to tackle it, including model agnostic methods, graphical models, multiple kernel learning, and various types of neural networks. Each approach has its own strengths and weaknesses, with some more suited for smaller datasets and others performing better in noisy environments. Most recently, neural networks have become a very popular way to tackle multimodal fusion, however graphical models and multiple kernel learning are still being used, especially in tasks with limited training data or where model interpretability is important.\nDespite these advances multimodal fusion still faces the following challenges: 1) signals might not be temporally aligned (possibly dense continuous signal and a sparse event); 2) it is difficult to build models that exploit supplementary and not only complementary information; 3) each modality might exhibit different types and different levels of noise at different points in time."}, {"heading": "7 CO-LEARNING", "text": "The final multimodal challenge in our taxonomy is colearning \u2014 aiding the modeling of a (resource poor) modality by exploiting knowledge from another (resource rich) modality. It is particularly relevant when one of the modalities has limited resources \u2014 lack of annotated data, noisy input, and unreliable labels. We call this challenge colearning as most often the helper modality is used only during model training and is not used during test time. We identify three types of co-learning approaches based on their training resources: parallel, non-parallel, and hybrid. Parallel-data approaches require training datasets where the observations from one modality are directly linked to the observations from other modalities. In other words, when the multimodal observations are from the same instances, such as in an audio-visual speech dataset where the video and speech samples are from the same speaker. In contrast, nonparallel data approaches do not require direct links between observations from different modalities. These approaches usually achieve co-learning by using overlap in terms of categories. For example, in zero shot learning when the conventional visual object recognition dataset is expanded with a second text-only dataset from Wikipedia to improve the generalization of visual object recognition. In the hybrid data setting the modalities are bridged through a shared modality or a dataset. An overview of methods in co-learning can be seen in Table 6 and summary of data parallelism in Figure 3."}, {"heading": "7.1 Parallel data", "text": "In parallel data co-learning both modalities share a set of instances \u2014 audio recordings with the corresponding videos, images and their sentence descriptions. This allows for two types of algorithms to exploit that data to better model the modalities: co-training and representation learning.\nCo-training is the process of creating more labeled training samples when we have few labeled samples in a multimodal problem [20]. The basic algorithm builds weak classifiers in each modality to bootstrap each other with labels for the unlabeled data. It has been shown to discover more training samples for web-page classification based on the web-page itself and hyper-links leading in the seminal work of Blum and Mitchell [20]. By definition this task requires parallel data as it relies on the overlap of multimodal samples.\nCo-training has been used for statistical parsing [178] to build better visual detectors [120] and for audio-visual speech recognition [39]. It has also been extended to deal with disagreement between modalities, by filtering out unreliable samples [40]. While co-training is a powerful method for generating more labeled data, it can also lead to biased training samples resulting in overfitting. Transfer learning is another way to exploit co-learning with parallel data. Multimodal representation learning (Section 3.1) approaches such as multimodal deep Boltzmann machines [198] and multimodal autoencoders [151] transfer information from representation of one modality to that of another. This not only leads to multimodal representations, but also to better unimodal ones, with only one modality being used during test time [151] .\nMoon et al. [143] show how to transfer information from a speech recognition neural network (based on audio) to a lip-reading one (based on images), leading to a better visual representation, and a model that can be used for lip-reading without need for audio information during test time. Similarly, Arora and Livescu [10] build better acoustic features using CCA on acoustic and articulatory (location of lips, tongue and jaw) data. They use articulatory data only during CCA construction and use only the resulting acoustic (unimodal) representation during test time."}, {"heading": "7.2 Non-parallel data", "text": "Methods that rely on non-parallel data do not require the modalities to have shared instances, but only shared categories or concepts. Non-parallel co-learning approaches can help when learning representations, allow for better semantic concept understanding and even perform unseen object recognition.\n15\nTransfer learning is also possible on non-parallel data and allows to learn better representations through transferring information from a representation built using a data rich or clean modality to a data scarce or noisy modality. This type of trasnfer learning is often achieved by using coordinated multimodal representations (see Section 3.2). For example, Frome et al. [61] used text to improve visual representations for image classification by coordinating CNN visual features with word2vec textual ones [141] trained on separate large datasets. Visual representations trained in such a way result in more meaningful errors \u2014 mistaking objects for ones of similar category [61]. Mahasseni and Todorovic [129] demonstrated how to regularize a color video based LSTM using an autoencoder LSTM trained on 3D skeleton data by enforcing similarities between their hidden states. Such an approach is able to improve the original LSTM and lead to state-of-the-art performance in action recognition. Conceptual grounding refers to learning semantic meanings or concepts not purely based on language but also on additional modalities such as vision, sound, or even smell. While the majority of concept learning approaches are purely language-based, representations of meaning in humans are not merely a product of our linguistic exposure, but are also grounded through our sensorimotor experience and perceptual system [16], [126]. Human semantic knowledge relies heavily on perceptual information [126] and many concepts are grounded in the perceptual system and are not purely symbolic [16]. This implies that learning semantic meaning purely from textual information might not be optimal, and motivates the use of visual or acoustic cues to ground our linguistic representations.\nStarting from work by Feng and Lapata [58], grounding is usually performed by finding a common latent space between the representations [58], [183] (in case of parallel datasets) or by learning unimodal representations separately and then concatenating them to lead to a multimodal one [28], [101], [172], [181] (in case of non-parallel data). Once a multimodal representation is constructed it can be used on purely linguistic tasks. Shutova et al. [181] and Bruni et al. [28] used grounded representations for better classification of metaphors and literal language. Such repre-\nsentations have also been useful for measuring conceptual similarity and relatedness \u2014 identifying how semantically or conceptually related two words are [29], [101], [183] or actions [172]. Furthermore, concepts can be grounded not only using visual signals, but also acoustic ones, leading to better performance especially on words with auditory associations [103], or even olfactory signals [102] for words with smell associations. Finally, there is a lot of overlap between multimodal alignment and conceptual grounding, as aligning visual scenes to their descriptions leads to better textual or visual representations [108], [161], [172], [240].\nConceptual grounding has been found to be an effective way to improve performance on a number of tasks. It also shows that language and vision (or audio) are complementary sources of information and combining them in multimodal models often improves performance. However, one has to be careful as grounding does not always lead to better performance [102], [103], and only makes sense when grounding has relevance for the task \u2014 such as grounding using images for visually-related concepts. Zero shot learning (ZSL) refers to recognizing a concept without having explicitly seen any examples of it. For example classifying a cat in an image without ever having seen (labeled) images of cats. This is an important problem to address as in a number of tasks such as visual object classification: it is prohibitively expensive to provide training examples for every imaginable object of interest.\nThere are two main types of ZSL \u2014 unimodal and multimodal. The unimodal ZSL looks at component parts or attributes of the object, such as phonemes to recognize an unheard word or visual attributes such as color, size, and shape to predict an unseen visual class [54]. The multimodal ZSL recognizes the objects in the primary modality through the help of the secondary one \u2014 in which the object has been seen. The multimodal version of ZSL is a problem facing non-parallel data by definition as the overlap of seen classes is different between the modalities.\nSocher et al. [190] map image features to a conceptual word space and are able to classify between seen and unseen concepts. The unseen concepts can be then assigned to a word that is close to the visual representation \u2014 this is enabled by the semantic space being trained on a separate dataset that has seen more concepts. Instead of learning a mapping from visual to concept space Frome et al. [61] learn a coordinated multimodal representation between concepts and images that allows for ZSL. Palatucci et al. [158] perform prediction of words people are thinking of based on functional magnetic resonance images, they show how it is possible to predict unseen words through the use of an intermediate semantic space. Lazaridou et al. [118] present a fast mapping method for ZSL by mapping extracted visual feature vectors to text-based vectors through a neural network."}, {"heading": "7.3 Hybrid data", "text": "In the hybrid data setting two non-parallel modalities are bridged by a shared modality or a dataset (see Figure 3c). The most notable example is the Bridge Correlational Neural Network [167], which uses a pivot modality to learn coordinated multimodal representations in presence of nonparallel data. For example, in the case of multilingual image\n16\ncaptioning, the image modality would always be paired with at least one caption in any language. Such methods have also been used to bridge languages that might not have parallel corpora but have access to a shared pivot language, such as for machine translation [148], [167] and document transliteration [100].\nInstead of using a separate modality for bridging, some methods rely on existence of large datasets from a similar or related task to lead to better performance in a task that only contains limited annotated data. Socher and Fei-Fei [189] use the existence of large text corpora in order to guide image segmentation. While Hendricks et al. [78] use separately trained visual model and a language model to lead to a better image and video description system, for which only limited data is available."}, {"heading": "7.4 Discussion", "text": "Multimodal co-learning allows for one modality to influence the training of another, exploiting the complementary information across modalities. It is important to note that co-learning is task independent and could be used to create better fusion, translation, and alignment models. This challenge is exemplified by algorithms such as co-training, multimodal representation learning, conceptual grounding, and zero shot learning (ZSL) and has found many applications in visual classification, action recognition, audio-visual speech recognition, and semantic similarity estimation."}, {"heading": "8 CONCLUSION", "text": "As part of this survey, we introduced a taxonomy of multimodal machine learning: representation, translation, fusion, alignment, and co-learning. Some of them such as fusion have been studied for a long time, but more recent interest in representation and translation have led to a large number of new multimodal algorithms and exciting multimodal applications.\nWe believe that our taxonomy will help to catalog future research papers and also better understand the remaining unresolved problems facing multimodal machine learning."}], "references": [{"title": "Analyzing the Behavior of Visual Question Answering Models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "EMNLP, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011", "author": ["C.N. Anagnostopoulos", "T. Iliou", "I. Giannoukos"], "venue": "Artificial Intelligence Review, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Expressive visual text-to-speech using active appearance models", "author": ["R. Anderson", "B. Stenger", "V. Wan", "R. Cipolla"], "venue": "CVPR, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu"], "venue": "ICML, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Audio-to-text alignment for speech recognition with very limited resources.", "author": ["X. Anguera", "J. Luque", "C. Gracia"], "venue": "in INTER- SPEECH,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["R. Arora", "K. Livescu"], "venue": "ICASSP, pp. 7135\u20137139, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal fusion for multimedia analysis: A survey", "author": ["P.K. Atrey", "M.A. Hossain", "A. El Saddik", "M.S. Kankanhalli"], "venue": "2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural Machine Translation By Jointly Learning To Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Dimensional Affect Recognition using Continuous Conditional Random Fields", "author": ["T. Baltru\u0161aitis", "N. Banda", "P. Robinson"], "venue": "IEEE FG, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Video In Sentences Out", "author": ["A. Barbu", "A. Bridge", "Z. Burchill", "D. Coroian", "S. Dickinson", "S. Fidler", "A. Michaux", "S. Mussman", "S. Narayanaswamy", "D. Salvi", "L. Schmidt", "J. Shangguan", "J.M. Siskind", "J. Waggoner", "S. Wang", "J. Wei", "Y. Yin", "Z. Zhang"], "venue": "Proc. of the Conference on Uncertainty in Artificial Intelligence, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Matching Words and Pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. de Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "JMLR, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Grounded cognition", "author": ["L.W. Barsalou"], "venue": "Annual review of psychology, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "TPAMI, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler- Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "JAIR, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "VizWiz: Nearly Real-Time Answers to Vvisual Questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "UIST, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Computational learning theory, 1998.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["P. Bojanowski", "R. Lajugie", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "ECCV, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly-Supervised Alignment of Video With Text", "author": ["P. Bojanowski", "R. Lajugie", "E. Grave", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid"], "venue": "ICCV, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A mew ASR approach based on independent processing and recombination of partial frequency bands", "author": ["H. Bourlard", "S. Dupont"], "venue": "International Conference on Spoken Language, 1996.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Coupled hidden Markov models for complex action recognition", "author": ["M. Brand", "N. Oliver", "A. Pentland"], "venue": "CVPR, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Video rewrite: Driving visual speech with audio", "author": ["C. Bregler", "M. Covell", "M. Slaney"], "venue": "SIGGRAPH, 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Data Fusion through Cross-modality Metric Learning using Similarity-Sensitive Hashing", "author": ["M.M. Bronstein", "A.M. Bronstein", "F. Michel", "N. Paragios"], "venue": "CVPR, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics, pp. 263\u2013311, 1993.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1993}, {"title": "Distributional Semantics in Technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N.-K. Tran"], "venue": "ACL, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Distributional Semantics", "author": ["E. Bruni", "N.K. Tran", "M. Baroni"], "venue": "JAIR, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple Kernel Learning for Visual Object Recognition: A Review", "author": ["S.S. Bucak", "R. Jin", "A.K. Jain"], "venue": "TPAMI, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Visual- Semantic Hashing for Cross-Modal Retrieval", "author": ["Y. Cao", "M. Long", "J. Wang", "Q. Yang", "P.S. Yu"], "venue": "KDD, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "The AMI Meeting Corpus: A Pre-Announcement", "author": ["J. Carletta", "S. Ashby", "S. Bourban", "M. Flynn", "M. Guillemot", "T. Hain", "J. Kadlec", "V. Karaiskos", "W. Kraaij", "M. Kronenthal", "G. Lathoud", "M. Lincoln", "A. Lisowska", "I. McCowan", "W. Post", "D. Reidsma", "P. Wellner"], "venue": "Int. Conf. on Methods and Techniques in Behavioral Research, 2005.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Emotion recognition through multiple modalities: Face, body gesture, speech", "author": ["G. Castellano", "L. Kessous", "G. Caridakis"], "venue": "LNCS, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Listen, Attend, and Spell: a Neural Network for Large Vocabulary Conversational Speech Recognition", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "ICASSP, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Emotion Recognition in the Wild with Feature Fusion and Multiple Kernel Learning", "author": ["J. Chen", "Z. Chen", "Z. Chi", "H. Fu"], "venue": "ICMI, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-modal Dimensional Emotion Recognition Using Recurrent Neural Networks", "author": ["S. Chen", "Q. Jin"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, 2015.  17", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft COCO Captions: Data Collection and Evaluation Server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Dollar", "L. Zitnick"], "venue": "2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "NIPS, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Co-Adaptation of audio-visual speech and gesture classifiers", "author": ["C.M. Christoudias", "K. Saenko", "L.-P. Morency", "T. Darrell"], "venue": "ICMI, 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-view learning in the presence of view disagreement", "author": ["C.M. Christoudias", "R. Urtasun", "T. Darrell"], "venue": "UAI, 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Wav2Letter: an Endto-End ConvNet-based Speech Recognition System", "author": ["R. Collobert", "C. Puhrsch", "G. Synnaeve"], "venue": "2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Bimodal recognition experiments with recurrent neural networks", "author": ["P. Cosi", "E. Caldognetto", "K. Vagges", "G. Mian", "M. Contolini", "C. per Le Ricerche", "C. di Fonetica"], "venue": "ICASSP, 1994.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1994}, {"title": "Movie / Script : Alignment and Parsing of Video and Text Transcription", "author": ["T. Cour", "C. Jordan", "E. Miltsakaki", "B. Taskar"], "venue": "ECCV, 2008, pp. 1\u201314.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "WordsEye: an automatic text-to-scene conversion system", "author": ["B. Coyne", "R. Sproat"], "venue": "SIGGRAPH, 2001.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Facial Expression Analysis", "author": ["F. De la Torre", "J.F. Cohn"], "venue": "Guide to Visual Analysis of Humans: Looking at People, 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech-Driven Facial Animation Using a Shared Gaussian Process Latent Variable Model", "author": ["S. Deena", "A. Galata"], "venue": "Advances in Visual Computing, 2009.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["M. Denkowski", "A. Lavie"], "venue": "EACL, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Language Models for Image Captioning: The Quirks and What Works", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "ACL, 2015.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "A Review and Meta-Analysis of Multimodal Affect Detection Systems", "author": ["S.K. D\u2019mello", "J. Kory"], "venue": "ACM Computing Surveys, 2015.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Image Description using Visual Dependency Representations", "author": ["D. Elliott", "F. Keller"], "venue": "EMNLP, no. October, 2013.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparing Automatic Evaluation Measures for Image Description", "author": ["\u2014\u2014"], "venue": "ACL, 2014.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention", "author": ["G. Evangelopoulos", "A. Zlatintsi", "A. Potamianos", "P. Maragos", "K. Rapantzikos", "G. Skoumas", "Y. Avrithis"], "venue": "IEEE Trans. Multimedia, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "TTS synthesis with bidirectional LSTM based Recurrent Neural Networks", "author": ["Y. Fan", "Y. Qian", "F. Xie", "F.K. Soong"], "venue": "IN- TERSPEECH, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR, 2009.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "LNCS, 2010.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep correspondence restricted Boltzmann machine for cross-modal retrieval", "author": ["F. Feng", "R. Li", "X. Wang"], "venue": "Neurocomputing, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-modal Retrieval with Correspondence Autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "ACMMM, 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual Information in Semantic Representation", "author": ["Y. Feng", "M. Lapata"], "venue": "NAACL, 2010.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2010}, {"title": "A Sentence is Worth a Thousand Pixels Holistic CRF model", "author": ["S. Fidler", "A. Sharma", "R. Urtasun"], "venue": "CVPR, 2013.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2013}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "NIPS, 2013.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "DeViSE: A deep visualsemantic embedding model", "author": ["A. Frome", "G. Corrado", "J. Shlens"], "venue": "NIPS, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "EMNLP, 2016.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2016}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS, 2015.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "Boosted learning in dynamic bayesian networks for multimodal speaker detection", "author": ["A. Garg", "V. Pavlovic", "J.M. Rehg"], "venue": "Proceedings of the IEEE, 2003.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2003}, {"title": "Audio-visual speaker diarization based on spatiotemporal bayesian fusion", "author": ["I.D. Gebru", "S. Ba", "X. Li", "R. Horaud"], "venue": "TPAMI, 2017.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2017}, {"title": "On Feature Combination for Multiclass Object Classification", "author": ["P. Gehler", "S. Nowozin"], "venue": "ICCV, 2009.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "Factorial hidden Markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine Learning, 1997.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple classifier systems for the classification of audio-visual emotional states", "author": ["M. Glodek", "S. Tschechne", "G. Layher", "M. Schels", "T. Brosch", "S. Scherer", "M. K\u00e4chele", "M. Schmidt", "H. Neumann", "G. Palm", "F. Schwenker"], "venue": "LNCS, 2011.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiple Kernel Learning Algorithms", "author": ["M. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "JMLR, 2011.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2011}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde- Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, 2014.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "ICASSP, 2013.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2013}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV, 2013.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2013}, {"title": "Choosing Linguistics over Vision to Describe Images", "author": ["A. Gupta", "Y. Verma", "C.V. Jawahar"], "venue": "AAAI, 2012.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic Modality Weighting for Multi-stream HMMs in Audio-Visual Speech Recognition", "author": ["M. Gurban", "J.-P. Thiran", "T. Drugman", "T. Dutoit"], "venue": "ICMI, 2008.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2008}, {"title": "Canonical correlation analysis; An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-taylor"], "venue": "Tech. Rep., 2003.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2003}, {"title": "Alignment of speech to highly imperfect text transcriptions", "author": ["A. Haubold", "J.R. Kender"], "venue": "ICME, 2007.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, 2012.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Autoencoders, minimum description length and Helmoltz free energy", "author": ["G. Hinton", "R.S. Zemel"], "venue": "NIPS, 1993.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 1993}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, 2006.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 1997.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "JAIR, 2013.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2013}, {"title": "Relations Between Two Sets of Variates", "author": ["H. Hotelling"], "venue": "Biometrika, 1936.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 1936}, {"title": "Natural Language Object Retrieval", "author": ["R. Hu", "H. Xu", "M. Rohrbach", "J. Feng", "K. Saenko", "T. Darrell"], "venue": "CVPR, 2016.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio-Visual Deep Learning for Noise Robust Speech Recognition", "author": ["J. Huang", "B. Kingsbury"], "venue": "ICASSP, 2013.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2013}, {"title": "Unit selection in a concatenative speech synthesis system using a large speech database", "author": ["A. Hunt", "A.W. Black"], "venue": "ICASSP, 1996.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 1996}, {"title": "Medical image fusion : A survey of the state of the art", "author": ["A.P. James", "B.V. Dasarathy"], "venue": "Information Fusion, vol. 19, 2014.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-task , Multi- Kernel Learning for Estimating Individual Wellbeing", "author": ["N. Jaques", "S. Taylor", "A. Sano", "R. Picard"], "venue": "Multimodal Machine Learning Workshop in conjunction with NIPS, 2015.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Guiding the Long-Short Term Memory Model for Image Caption Generation", "author": ["X. Jia", "E. Gavves", "B. Fernando", "T. Tuytelaars"], "venue": "ICCV, 2015.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Cross-Modal Hashing", "author": ["Q.-y. Jiang", "W.-j. Li"], "venue": "CVPR, 2017.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2017}, {"title": "The classification of multi-modal data with hidden conditional random field", "author": ["X. Jiang", "F. Wu", "Y. Zhang", "S. Tang", "W. Lu", "Y. Zhuang"], "venue": "Pattern Recognition Letters, 2015.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2015}, {"title": "Video Description Generation using Audio and Visual Cues", "author": ["Q. Jin", "J. Liang"], "venue": "ICMR, 2016.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2016}, {"title": "Hidden Markov Models for Speech Recognition", "author": ["B.H. Juang", "L.R. Rabiner"], "venue": "Technometrics, 1991.  18", "citeRegEx": "95", "shortCiteRegEx": null, "year": 1991}, {"title": "EmoNets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. Kahou", "X. Bouthillier", "P. Lamblin", "C. Gulchere", "V. Michalski", "K. Konda", "J. Sebastien", "P. Froumenty", "Y. Dauphin", "N. Boulanger- Lewandowski", "R.C. Ferrari", "M. Mirza", "D. Warde-Farley", "A. Courville", "P. Vincent", "R. Memisevic", "C. Pal", "Y. Bengio"], "venue": "Journal on Multimodal User Interfaces, 2015.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent Continuous Translation Models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP, 2013.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, 2015.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "NIPS, 2014.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2014}, {"title": "Everybody loves a rich cousin: An empirical study of transliteration through bridge languages", "author": ["M.M. Khapra", "A. Kumaran", "P. Bhattacharyya"], "venue": "NAACL, 2010.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics", "author": ["D. Kiela", "L. Bottou"], "venue": "EMNLP, 2014.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounding Semantics in Olfactory Perception", "author": ["D. Kiela", "L. Bulat", "S. Clark"], "venue": "ACL, 2015.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi- and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception", "author": ["D. Kiela", "S. Clark"], "venue": "EMNLP, 2015.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Learning for Robust Feature Generation in Audiovisual Emotion Recognition", "author": ["Y. Kim", "H. Lee", "E.M. Provost"], "venue": "ICASSP, 2013.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2013}, {"title": "Unifying Visual- Semantic Embeddings with Multimodal Neural Language Models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "2014.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2014}, {"title": "Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for Image Annotation", "author": ["B. Klein", "G. Lev", "G. Sadeh", "L. Wolf"], "venue": "CVPR, 2015.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language description of human activities from video images based on concept hierarchy of actions", "author": ["A. Kojima", "T. Tamura", "K. Fukunaga"], "venue": "IJCV, 2002.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2002}, {"title": "What are you talking about? Text-to-Image Coreference", "author": ["C. Kong", "D. Lin", "M. Bansal", "R. Urtasun", "S. Fidler"], "venue": "CVPR, 2014.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-relational learning, text mining, and semi-supervised learning for functional genomics", "author": ["M.A. Krogel", "T. Scheffer"], "venue": "Machine Learning, 2004.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2004}, {"title": "An Overview of Sequence Comparison: Time Warps, String Edits, and Macromolecules", "author": ["J.B. Kruskal"], "venue": "Society for Industrial and Applied Mathematics Review, vol. 25, no. 2, pp. 201\u2013237, 1983.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 1983}, {"title": "BabyTalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "TPAMI, 2013.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hash functions for cross-view similarity search", "author": ["S. Kumar", "R. Udupa"], "venue": "IJCAI, 2011.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "ACL, 2012.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2012}, {"title": "Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "ICML, 2001.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2001}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "International Journal of Neural Systems, 2000.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimedia classification and event detection using double fusion", "author": ["Z.Z. Lan", "L. Bao", "S.I. Yu", "W. Liu", "A.G. Hauptmann"], "venue": "Multimedia Tools and Applications, 2014.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2014}, {"title": "Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world", "author": ["A. Lazaridou", "E. Bruni", "M. Baroni"], "venue": "ACL, 2014.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2014}, {"title": "Phrase-based Image Captioning", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "ICML, 2015.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised improvement of visual detectors using cotraining", "author": ["A. Levin", "P. Viola", "Y. Freund"], "venue": "ICCV, 2003.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2003}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T. Berg", "A. Berg", "Y. Choi"], "venue": "CoNLL, 2011.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey of recent advances in visual feature detection", "author": ["Y. Li", "S. Wang", "Q. Tian", "X. Ding"], "venue": "Neurocomputing, 2015.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparison of automatic shot boundary detection algorithms", "author": ["R.W. Lienhart"], "venue": "Proceedings of SPIE, 1998.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 1998}, {"title": "Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics", "author": ["C.-Y. Lin", "E. Hovy"], "venue": "NAACL, 2003.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple kernel learning in the primal for multimodal Alzheimer\u2019s disease classification", "author": ["F. Liu", "L. Zhou", "C. Shen", "J. Yin"], "venue": "IEEE Journal of Biomedical and Health Informatics, 2014.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2014}, {"title": "Symbol interdependency in symbolic and embodied cognition", "author": ["M.M. Louwerse"], "venue": "Topics in Cognitive Science, 2011.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2011}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "IJCV, 2004.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical Co-Attention for Visual Question Answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS, 2016.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2016}, {"title": "Regularizing Long Short Term Memory with 3D Human-Skeleton Sequences for Action Recognition", "author": ["B. Mahasseni", "S. Todorovic"], "venue": "CVPR, 2016.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV, 2015.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2015}, {"title": "What\u2019s cookin\u2019? interpreting cooking videos using text, speech and vision", "author": ["J. Malmaud", "J. Huang", "V. Rathod", "N. Johnston", "A. Rabinovich", "K. Murphy"], "venue": "NAACL, 2015.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating Images from Captions with Attention", "author": ["E. Mansimov", "E. Parisotto", "J.L. Ba", "R. Salakhutdinov"], "venue": "ICLR, 2016.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2016}, {"title": "Generation and Comprehension of Unambiguous Object Descriptions", "author": ["J. Mao", "J. Huang", "A. Toshev", "O. Camburu", "A. Yuille", "K. Murphy"], "venue": "CVPR, 2016.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR, 2015.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonparametric Method for Datadriven Image Captioning", "author": ["R. Mason", "E. Charniak"], "venue": "ACL, 2014.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2014}, {"title": "Text-to-Visual Speech Synthesis Based on Parameter Generation from HMM", "author": ["T. Masuko", "T. Kobayashi", "M. Tamura", "J. Masubuchi", "K. Tokuda"], "venue": "ICASSP, 1998.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning Multi-modal Similarity", "author": ["B. McFee", "G.R.G. Lanckriet"], "venue": "JMLR, 2011.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2011}, {"title": "The SE- MAINE corpus of emotionally coloured character interactions", "author": ["G. McKeown", "M.F. Valstar", "R. Cowie", "M. Pantic"], "venue": "IEEE International Conference on Multimedia and Expo, 2010.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2010}, {"title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "author": ["H. Mei", "M. Bansal", "M.R. Walter"], "venue": "AAAI, 2016.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 2013.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2013}, {"title": "Midge: Generating Image Descriptions From Computer Vision Detections", "author": ["M. Mitchell", "J. Dodge", "A. Goyal", "K. Yamaguchi", "K. Stratos", "A. Mensch", "A. Berg", "X. Han", "T. Berg", "O. Health"], "venue": "EACL, 2012.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Transfer Deep Learning for Audio-Visual Recognition", "author": ["S. Moon", "S. Kim", "H. Wang"], "venue": "NIPS Workshops, 2015.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2015}, {"title": "Majority vote of diverse classifiers for late fusion", "author": ["E. Morvant", "A. Habrard", "S. Ayache"], "venue": "LNCS, 2014.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multimodal learning for Audio-Visual Speech Recognition", "author": ["Y. Mroueh", "E. Marcheret", "V. Goel"], "venue": "ICASSP, 2015.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminative unsupervised alignment of natural language instructions with corresponding video segments", "author": ["I. Naim", "Y. Song", "Q. Liu", "L. Huang", "H. Kautz", "J. Luo", "D. Gildea"], "venue": "NAACL, 2015.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised Alignment of Natural Language Instructions with Video Segments", "author": ["I. Naim", "Y.C. Song", "Q. Liu", "H. Kautz", "J. Luo", "D. Gildea"], "venue": "AAAI, 2014.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving statistical machine translation for a resource-poor language using related resource-rich languages", "author": ["P. Nakov", "H.T. Ng"], "venue": "JAIR, 2012.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2012}, {"title": "A coupled HMM for audio-visual speech recognition", "author": ["A.V. Nefian", "L. Liang", "X. Pi", "L. Xiaoxiang", "C. Mao", "K. Murphy"], "venue": "Interspeech, vol. 2, 2002.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2002}, {"title": "ModDrop: Adaptive multi-modal gesture recognition", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "IEEE TPAMI, 2016.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal Deep Learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML, 2011.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2011}, {"title": "Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence \u2013 Arousal Space", "author": ["M.A. Nicolaou", "H. Gunes", "M. Pantic"], "venue": "IEEE TAC, 2011.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep multimodal fusion for persuasiveness prediction", "author": ["B. Nojavanasghari", "D. Gopinath", "J. Koushik", "T. Baltru\u0161aitis", "L.-P. Morency"], "venue": "ICMI, 2016.  19", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal Speaker diarization", "author": ["A. Noulas", "G. Englebienne", "B.J. Kr\u00f6se"], "venue": "IEEE TPAMI, 2012.", "citeRegEx": "154", "shortCiteRegEx": null, "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS, 2011.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-source Deep Learning for Human Pose Estimation", "author": ["W. Ouyang", "X. Chu", "X. Wang"], "venue": "CVPR, 2014.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 2014}, {"title": "Visually Indicated Sounds", "author": ["A. Owens", "P. Isola", "J. McDermott", "A. Torralba", "E.H. Adelson", "W.T. Freeman"], "venue": "CVPR, 2016.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2016}, {"title": "Zero-Shot Learning with Semantic Output Codes", "author": ["M. Palatucci", "G.E. Hinton", "D. Pomerleau", "T.M. Mitchell"], "venue": "NIPS, 2009.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2009}, {"title": "Jointly Modeling Embedding and Translation to Bridge Video and Language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "CVPR, 2016.", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2016}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-j. Zhu"], "venue": "ACL, 2002.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2002}, {"title": "Flickr30k Entities: Collecting Regionto-Phrase Correspondences for Richer Image-to-Sentence Models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "ICCV, 2015.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-level Multimodal Sentiment Analysis", "author": ["S. Poria", "E. Cambria", "A. Gelbukh"], "venue": "EMNLP, 2015.", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2015}, {"title": "Recent advances in the automatic recognition of audio-visual speech", "author": ["G. Potamianos", "C. Neti", "G. Gravier", "A. Garg", "A.W. Senior"], "venue": "Proceedings of the IEEE, 2003.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 2003}, {"title": "Global Ranking Using Continuous Conditional Random Fields", "author": ["T. Qin", "T.-y. Liu", "X.-d. Zhang", "D.-s. Wang", "H. Li"], "venue": "NIPS, 2008.", "citeRegEx": "164", "shortCiteRegEx": null, "year": 2008}, {"title": "Hidden conditional random fields.", "author": ["A. Quattoni", "S. Wang", "L.-P. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE TPAMI, vol", "citeRegEx": "165", "shortCiteRegEx": "165", "year": 2007}, {"title": "Extending Long Short-Term Memory for Multi-View Structured Learning", "author": ["S.S. Rajagopalan", "L.-P. Morency", "T. Baltru\u0161aitis", "R. Goecke"], "venue": "ECCV, 2016.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2016}, {"title": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning", "author": ["J. Rajendran", "M.M. Khapra", "S. Chandar", "B. Ravindran"], "venue": "NAACL, 2015.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling Latent Discriminative Dynamic of Multi-Dimensional Affective Signals", "author": ["G.A. Ramirez", "T. Baltru\u0161aitis", "L.-P. Morency"], "venue": "ACII workshops, 2011.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 2011}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACMMM, 2010.", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2010}, {"title": "Trainable methods for surface natural language generation", "author": ["A. Ratnaparkhi"], "venue": "NAACL, 2000.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2000}, {"title": "Generative Adversarial Text to Image Synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "H. Lee", "B. Schiele"], "venue": "ICML, 2016.", "citeRegEx": "171", "shortCiteRegEx": null, "year": 2016}, {"title": "Grounding Action Descriptions in Videos", "author": ["M. Regneri", "M. Rohrbach", "D. Wetzel", "S. Thater", "B. Schiele", "M. Pinkal"], "venue": "TACL, 2013.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 2013}, {"title": "Hidden Conditional Random Fields for Meeting Segmentation", "author": ["S. Reiter", "B. Schuller", "G. Rigoll"], "venue": "ICME, 2007.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2007}, {"title": "The long-short story of movie description", "author": ["A. Rohrbach", "M. Rohrbach", "B. Schiele"], "venue": "Pattern Recognition, 2015.", "citeRegEx": "174", "shortCiteRegEx": null, "year": 2015}, {"title": "Movie description", "author": ["A. Rohrbach", "A. Torabi", "M. Rohrbach", "N. Tandon", "C. Pal", "H. Larochelle", "A. Courville", "B. Schiele"], "venue": "International Journal of Computer Vision, 2017.", "citeRegEx": "175", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Boltzmann Machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International conference on artificial intelligence and statistics, 2009.", "citeRegEx": "176", "shortCiteRegEx": null, "year": 2009}, {"title": "Audiovisual synchronization and fusion using canonical correlation analysis", "author": ["M.E. Sargin", "Y. Yemez", "E. Erzin", "A.M. Tekalp"], "venue": "IEEE Trans. Multimedia, 2007.", "citeRegEx": "177", "shortCiteRegEx": null, "year": 2007}, {"title": "Applying Co-Training methods to statistical parsing", "author": ["A. Sarkar"], "venue": "ACL, 2001.", "citeRegEx": "178", "shortCiteRegEx": null, "year": 2001}, {"title": "AVEC 2011 \u2013 The First International Audio / Visual Emotion Challenge", "author": ["B. Schuller", "M.F. Valstar", "F. Eyben", "G. McKeown", "R. Cowie", "M. Pantic"], "venue": "ACII, 2011.", "citeRegEx": "179", "shortCiteRegEx": null, "year": 2011}, {"title": "Isotonic CCA for sequence alignment and activity recognition", "author": ["S. Shariat", "V. Pavlovic"], "venue": "ICCV, 2011.", "citeRegEx": "180", "shortCiteRegEx": null, "year": 2011}, {"title": "Black Holes and White Rabbits : Metaphor Identification with Visual Features", "author": ["E. Shutova", "D. Kelia", "J. Maillard"], "venue": "NAACL, 2016.", "citeRegEx": "181", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiple Kernel Learning for Emotion Recognition in the Wild", "author": ["K. Sikka", "K. Dykstra", "S. Sathyanarayana", "G. Littlewort", "M. Bartlett"], "venue": "ICMI, 2013.", "citeRegEx": "182", "shortCiteRegEx": null, "year": 2013}, {"title": "Grounded Models of Semantic Representation", "author": ["C. Silberer", "M. Lapata"], "venue": "EMNLP, 2012.", "citeRegEx": "183", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Grounded Meaning Representations with Autoencoders", "author": ["\u2014\u2014"], "venue": "ACL, 2014.", "citeRegEx": "184", "shortCiteRegEx": null, "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "185", "shortCiteRegEx": null, "year": 2015}, {"title": "An HMM-based system for automatic segmentation and alignment of speech", "author": ["K. Sj\u00f6lander"], "venue": "Proceedings of Fonetik, 2003.", "citeRegEx": "186", "shortCiteRegEx": null, "year": 2003}, {"title": "FaceSync: A linear operator for measuring synchronization of video facial images and audio tracks", "author": ["M. Slaney", "M. Covell"], "venue": "NIPS, 2000.", "citeRegEx": "187", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimodal video indexing: A review of the state-of-the-art", "author": ["C.G.M. Snoek", "M. Worring"], "venue": "Multimedia Tools and Applications, 2005.", "citeRegEx": "188", "shortCiteRegEx": null, "year": 2005}, {"title": "Connecting modalities: Semisupervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": "CVPR, 2010.", "citeRegEx": "189", "shortCiteRegEx": null, "year": 2010}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS, 2013.", "citeRegEx": "190", "shortCiteRegEx": null, "year": 2013}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "TACL, 2014.", "citeRegEx": "191", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal emotion recognition in response to videos", "author": ["M. Soleymani", "M. Pantic", "T. Pun"], "venue": "TAC, 2012.", "citeRegEx": "192", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-view latent variable discriminative models for action recognition", "author": ["Y. Song", "L.-P. Morency", "R. Davis"], "venue": "CVPR, 2012.", "citeRegEx": "193", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Human Behavior Analysis: Learning Correlation and Interaction Across Modalities", "author": ["\u2014\u2014"], "venue": "ICMI, 2012.", "citeRegEx": "194", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised Alignment of Actions in Video with Text Descriptions", "author": ["Y.C. Song", "I. Naim", "A.A. Mamun", "K. Kulkarni", "P. Singla", "J. Luo", "D. Gildea", "H. Kautz"], "venue": "IJCAI, 2016.", "citeRegEx": "195", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout : A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR, 2014.", "citeRegEx": "196", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Representations for Multimodal Data with Deep Belief Nets", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "ICML, 2012.", "citeRegEx": "197", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Learning with Deep Boltzmann Machines", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "NIPS, 2012.", "citeRegEx": "198", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis", "author": ["H.I. Suk", "S.-W. Lee", "D. Shen"], "venue": "NeuroImage, 2014.", "citeRegEx": "199", "shortCiteRegEx": null, "year": 2014}, {"title": "Introduction to Conditional Random Fields for Relational Learning", "author": ["C. Sutton", "A. McCallum"], "venue": "Introduction to Statistical Relational Learning. MIT Press, 2006.", "citeRegEx": "200", "shortCiteRegEx": null, "year": 2006}, {"title": "Aligning plot synopses to videos for story-based retrieval", "author": ["M. Tapaswi", "M. B\u00e4uml", "R. Stiefelhagen"], "venue": "IJMIR, 2015.", "citeRegEx": "201", "shortCiteRegEx": null, "year": 2015}, {"title": "Book2Movie: Aligning video scenes with book chapters", "author": ["\u2014\u2014"], "venue": "CVPR, 2015.", "citeRegEx": "202", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic units of visual speech", "author": ["S.L. Taylor", "M. Mahler", "B.-j. Theobald", "I. Matthews"], "venue": "SIGGRAPH, 2012.", "citeRegEx": "203", "shortCiteRegEx": null, "year": 2012}, {"title": "Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild", "author": ["J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R. Mooney"], "venue": "COLING, 2014.", "citeRegEx": "204", "shortCiteRegEx": null, "year": 2014}, {"title": "Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research", "author": ["A. Torabi", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "2015.", "citeRegEx": "205", "shortCiteRegEx": null, "year": 2015}, {"title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["G. Trigeorgis", "F. Ringeval", "R. Brueckner", "E. Marchi", "M.A. Nicolaou", "B. Schuller", "S. Zafeiriou"], "venue": "ICASSP, 2016.", "citeRegEx": "207", "shortCiteRegEx": null, "year": 2016}, {"title": "AVEC 2013 \u2013 The Continuous Audio / Visual Emotion and Depression Recognition Challenge", "author": ["M. Valstar", "B. Schuller", "K. Smith", "F. Eyben", "B. Jiang", "S. Bilakhia", "S. Schnieder", "R. Cowie", "M. Pantic"], "venue": "ACM International Workshop on Audio/Visual Emotion Challenge, 2013.", "citeRegEx": "208", "shortCiteRegEx": null, "year": 2013}, {"title": "WaveNet: A Generative Model for Raw Audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "2016.", "citeRegEx": "209", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixel Recurrent Neural Networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "ICML, 2016.", "citeRegEx": "210", "shortCiteRegEx": null, "year": 2016}, {"title": "CIDEr: Consensusbased Image Description Evaluation Ramakrishna Vedantam", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "CVPR, 2015.  20", "citeRegEx": "211", "shortCiteRegEx": null, "year": 2015}, {"title": "Order- Embeddings of Images and Language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "ICLR, 2016.", "citeRegEx": "212", "shortCiteRegEx": null, "year": 2016}, {"title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "NAACL, 2015.", "citeRegEx": "213", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "ICML, 2014.", "citeRegEx": "214", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["\u2014\u2014"], "venue": "CVPR, 2015.", "citeRegEx": "215", "shortCiteRegEx": null, "year": 2015}, {"title": "HMM-based word alignment in statistical translation", "author": ["S. Vogel", "H. Ney", "C. Tillmann"], "venue": "Computational Linguistics, 1996.", "citeRegEx": "216", "shortCiteRegEx": null, "year": 1996}, {"title": "Deep Multimodal Hashing with Orthogonal Regularization", "author": ["D. Wang", "P. Cui", "M. Ou", "W. Zhu"], "venue": "IJCAI, 2015.", "citeRegEx": "217", "shortCiteRegEx": null, "year": 2015}, {"title": "Hashing for Similarity Search: A Survey", "author": ["J. Wang", "H.T. Shen", "J. Song", "J. Ji"], "venue": "2014.", "citeRegEx": "218", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Deep Structure- Preserving Image-Text Embeddings", "author": ["L. Wang", "Y. Li", "S. Lazebnik"], "venue": "CVPR, 2016.", "citeRegEx": "219", "shortCiteRegEx": null, "year": 2016}, {"title": "On deep multiview representation learning", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "ICML, 2015.", "citeRegEx": "220", "shortCiteRegEx": null, "year": 2015}, {"title": "Web Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings Image Annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "ECML, 2010.", "citeRegEx": "221", "shortCiteRegEx": null, "year": 2010}, {"title": "WSABIE: Scaling up to large vocabulary image annotation", "author": ["\u2014\u2014"], "venue": "IJCAI, 2011.", "citeRegEx": "222", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework", "author": ["M. W\u00f6llmer", "M. Kaiser", "F. Eyben", "B. Schuller", "G. Rigoll"], "venue": "IMAVIS, 2013.", "citeRegEx": "223", "shortCiteRegEx": null, "year": 2013}, {"title": "Context-Sensitive Multimodal Emotion Recognition from Speech and Facial Expression using Bidirectional LSTM Modeling", "author": ["M. W\u00f6llmer", "A. Metallinou", "F. Eyben", "B. Schuller", "S. Narayanan"], "venue": "INTERSPEECH, 2010.", "citeRegEx": "224", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal Dynamic Networks for Gesture Recognition", "author": ["D. Wu", "L. Shao"], "venue": "ACMMM, 2014.", "citeRegEx": "225", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-level Fusion of Audio and Visual Features for Speaker Identification", "author": ["Z. Wu", "L. Cai", "H. Meng"], "venue": "Advances in Biometrics, 2005.", "citeRegEx": "226", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploring Interfeature and Inter-class Relationships with Deep Neural Networks for Video Classification", "author": ["Z. Wu", "Y.-G. Jiang", "J. Wang", "J. Pu", "X. Xue"], "venue": "ACMMM, 2014.", "citeRegEx": "227", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "ICML, 2016.", "citeRegEx": "228", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "ECCV, 2016.", "citeRegEx": "229", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, 2015.", "citeRegEx": "230", "shortCiteRegEx": null, "year": 2015}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso"], "venue": "AAAI, 2015.", "citeRegEx": "231", "shortCiteRegEx": null, "year": 2015}, {"title": "A Distributed Representation Based Query Expansion Approach for Image Captioning", "author": ["S. Yagcioglu", "E. Erdem", "A. Erdem", "R. Cakici"], "venue": "ACL, 2015.", "citeRegEx": "232", "shortCiteRegEx": null, "year": 2015}, {"title": "Corpus-Guided Sentence Generation of Natural Images", "author": ["Y. Yang", "C.L. Teo", "H. Daume", "Y. Aloimonos"], "venue": "EMNLP, 2011.", "citeRegEx": "233", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "CVPR, 2016.", "citeRegEx": "234", "shortCiteRegEx": null, "year": 2016}, {"title": "I2T: Image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proceedings of the IEEE, 2010.", "citeRegEx": "235", "shortCiteRegEx": null, "year": 2010}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "CVPR, 2015.", "citeRegEx": "236", "shortCiteRegEx": null, "year": 2015}, {"title": "A Novel Multiple Kernel Learning Framework for Heterogeneous Feature Fusion and Variable Selection", "author": ["Y.-r. Yeh", "T.-c. Lin", "Y.-y. Chung", "Y.-c. F. Wang"], "venue": "IEEE Trans. Multimedia, 2012.", "citeRegEx": "237", "shortCiteRegEx": null, "year": 2012}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["M.H.P. Young", "A. Lai", "J. Hockenmaier"], "venue": "TACL, 2014.", "citeRegEx": "238", "shortCiteRegEx": null, "year": 2014}, {"title": "On the Integration of Grounding Language and Lear ning Objects", "author": ["C. Yu", "D. Ballard"], "venue": "AAAI, 2004.", "citeRegEx": "239", "shortCiteRegEx": null, "year": 2004}, {"title": "Grounded Language Learning from Video Described with Sentences", "author": ["H. Yu", "J.M. Siskind"], "venue": "ACL, 2013.", "citeRegEx": "240", "shortCiteRegEx": null, "year": 2013}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "CVPR, 2016.", "citeRegEx": "241", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling Context in Referring Expressions", "author": ["L. Yu", "P. Poirson", "S. Yang", "A.C. Berg", "T.L. Berg"], "venue": "ECCV, 2016.", "citeRegEx": "242", "shortCiteRegEx": null, "year": 2016}, {"title": "Integration of Acoustic and Visual Speech Signals Using Neural Networks", "author": ["B.P. Yuhas", "M.H. Goldstein", "T.J. Sejnowski"], "venue": "IEEE Communications Magazine, 1989.", "citeRegEx": "243", "shortCiteRegEx": null, "year": 1989}, {"title": "Statistical Parametric Speech Synthesis Based on Speaker and Language Factorization", "author": ["H. Zen", "N. Braunschweiler", "S. Buchholz", "M.J.F. Gales", "S. Krstulovi", "J. Latorre"], "venue": "IEEE Transactions on Audio, Speech & Language Processing, 2012.", "citeRegEx": "244", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication, vol. 51, 2009.", "citeRegEx": "245", "shortCiteRegEx": null, "year": 2009}, {"title": "Leveraging Video Descriptions to Learn Video Question Answering", "author": ["K.-H. Zeng", "T.-H. Chen", "C.-Y. Chuang", "Y.-H. Liao", "J.C. Niebles", "M. Sun"], "venue": "AAAI, 2017.", "citeRegEx": "246", "shortCiteRegEx": null, "year": 2017}, {"title": "A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE TPAMI, 2009.", "citeRegEx": "247", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization", "author": ["D. Zhang", "W.-J. Li"], "venue": "AAAI, 2014.", "citeRegEx": "248", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Concept Taxonomies from Multi-modal Data", "author": ["H. Zhang", "Z. Hu", "Y. Deng", "M. Sachan", "Z. Yan", "E.P. Xing"], "venue": "ACL, 2016.", "citeRegEx": "249", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalized time warping for multimodal alignment of human motion", "author": ["F. Zhou", "F. De la Torre"], "venue": "CVPR, 2012.", "citeRegEx": "250", "shortCiteRegEx": null, "year": 2012}, {"title": "Canonical time warping for alignment of human behavior", "author": ["F. Zhou", "F. Torre"], "venue": "NIPS, 2009.", "citeRegEx": "251", "shortCiteRegEx": null, "year": 2009}, {"title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "ICCV, 2015.", "citeRegEx": "252", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 235, "context": "One of the earliest examples of multimodal research is audio-visual speech recognition (AVSR) [243].", "startOffset": 94, "endOffset": 99}, {"referenceID": 89, "context": "Given the prominence of hidden Markov models (HMMs) in the speech community at the time [95], it is", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "without surprise that many of the early models for AVSR were based on various HMM extensions [23], [24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 20, "context": "without surprise that many of the early models for AVSR were based on various HMM extensions [23], [24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 144, "context": "renewed interest from the deep learning community [151].", "startOffset": 50, "endOffset": 55}, {"referenceID": 71, "context": ", low signal-to-noise ratio) [75], [151], [243].", "startOffset": 29, "endOffset": 33}, {"referenceID": 144, "context": ", low signal-to-noise ratio) [75], [151], [243].", "startOffset": 35, "endOffset": 40}, {"referenceID": 235, "context": ", low signal-to-noise ratio) [75], [151], [243].", "startOffset": 42, "endOffset": 47}, {"referenceID": 7, "context": "A second important category of multimodal applications comes from the field of multimedia content indexing and retrieval [11], [188].", "startOffset": 121, "endOffset": 125}, {"referenceID": 181, "context": "A second important category of multimodal applications comes from the field of multimedia content indexing and retrieval [11], [188].", "startOffset": 127, "endOffset": 132}, {"referenceID": 181, "context": "While earlier approaches for indexing and searching these multimedia videos were keyword-based [188], new research problems emerged when trying to search the visual and multimodal content directly.", "startOffset": 95, "endOffset": 100}, {"referenceID": 117, "context": "This led to new research topics in multimedia content analysis such as automatic shot-boundary detection [123] and video summarization [52].", "startOffset": 105, "endOffset": 110}, {"referenceID": 48, "context": "This led to new research topics in multimedia content analysis such as automatic shot-boundary detection [123] and video summarization [52].", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "recordings of meetings, all fully transcribed and annotated [32].", "startOffset": 60, "endOffset": 64}, {"referenceID": 132, "context": "speakers and listeners [139].", "startOffset": 23, "endOffset": 28}, {"referenceID": 172, "context": "This dataset formed the basis of the first audio-visual emotion challenge (AVEC) organized in 2011 [179].", "startOffset": 99, "endOffset": 104}, {"referenceID": 41, "context": "landmark detection, and facial expression recognition [45].", "startOffset": 54, "endOffset": 58}, {"referenceID": 200, "context": "later instantiation including healthcare applications such as automatic assessment of depression and anxiety [208].", "startOffset": 109, "endOffset": 114}, {"referenceID": 45, "context": "[49].", "startOffset": 0, "endOffset": 4}, {"referenceID": 78, "context": "One of the most representative applications is image captioning where the task is to generate a text description of the input image [83].", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "This is motivated by the ability of such systems to help the visually impaired in their daily tasks [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "The task of visual question-answering (VQA) was recently proposed to address some of the evaluation challenges [9], where the goal is to answer a specific question about the image.", "startOffset": 111, "endOffset": 114}, {"referenceID": 13, "context": "[17] we use the term feature and representation interchangeably, with each referring to a vector or tensor representation of an entity, be it an image, audio sample, individual word, or a sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 74, "context": "Good representations are important for the performance of machine learning models, as evidenced behind the recent leaps in performance of speech recognition [79] and visual", "startOffset": 157, "endOffset": 161}, {"referenceID": 103, "context": "object classification [109] systems.", "startOffset": 22, "endOffset": 27}, {"referenceID": 13, "context": "[17] identify a number of properties for good representations: smoothness, temporal and spatial coherence, sparsity, and natural", "startOffset": 0, "endOffset": 4}, {"referenceID": 191, "context": "Srivastava and Salakhutdinov [198] identify additional desirable properties for multimodal representations: similarity in the representation space should reflect the similarity of the corresponding concepts, the representation should be easy to obtain even in the", "startOffset": 29, "endOffset": 34}, {"referenceID": 1, "context": "extensively studied [5], [17], [122].", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "extensively studied [5], [17], [122].", "startOffset": 25, "endOffset": 29}, {"referenceID": 116, "context": "extensively studied [5], [17], [122].", "startOffset": 31, "endOffset": 36}, {"referenceID": 121, "context": "descriptors in the early 2000s, the scale invariant feature transform (SIFT) was hand designed [127], but currently", "startOffset": 95, "endOffset": 100}, {"referenceID": 103, "context": "most visual descriptions are learned from data using neural architectures such as convolutional neural networks (CNN) [109].", "startOffset": 118, "endOffset": 123}, {"referenceID": 74, "context": "Similarly, in the audio domain, acoustic features such as Mel-frequency cepstral coefficients (MFCC) have been superseded by data-driven deep neural networks in speech recognition [79] and recurrent neural networks for para-linguistic analysis [207].", "startOffset": 180, "endOffset": 184}, {"referenceID": 199, "context": "Similarly, in the audio domain, acoustic features such as Mel-frequency cepstral coefficients (MFCC) have been superseded by data-driven deep neural networks in speech recognition [79] and recurrent neural networks for para-linguistic analysis [207].", "startOffset": 244, "endOffset": 249}, {"referenceID": 134, "context": "In natural language processing, the textual features initially relied on counting word occurrences in documents, but have been replaced datadriven word embeddings that exploit the word context [141].", "startOffset": 193, "endOffset": 198}, {"referenceID": 45, "context": "While there has been a huge amount of work on unimodal representation, up until recently most multimodal representations involved simple concatenation of unimodal ones [49], but this has been rapidly changing.", "startOffset": 168, "endOffset": 172}, {"referenceID": 57, "context": "Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces.", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces.", "startOffset": 94, "endOffset": 97}, {"referenceID": 204, "context": "Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces.", "startOffset": 129, "endOffset": 134}, {"referenceID": 45, "context": "plest example of a joint representation is a concatenation of individual modality features (also referred to as early fusion [49]).", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "Neural networks have become a very popular method for unimodal data representation [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 144, "context": "sent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217].", "startOffset": 92, "endOffset": 97}, {"referenceID": 149, "context": "sent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217].", "startOffset": 99, "endOffset": 104}, {"referenceID": 209, "context": "sent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217].", "startOffset": 106, "endOffset": 111}, {"referenceID": 13, "context": "Due to the multilayer nature of deep neural networks each successive layer is hypothesized to represent the data in a more abstract way [17], hence it is common to use the final or penultimate neural layers as a form of data representation.", "startOffset": 136, "endOffset": 140}, {"referenceID": 5, "context": "To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].", "startOffset": 197, "endOffset": 200}, {"referenceID": 138, "context": "To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].", "startOffset": 202, "endOffset": 207}, {"referenceID": 149, "context": "To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].", "startOffset": 209, "endOffset": 214}, {"referenceID": 219, "context": "To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].", "startOffset": 216, "endOffset": 221}, {"referenceID": 75, "context": "As neural networks require a lot of labeled training data, it is common to pre-train such representations using an autoencoder on unsupervised data [80].", "startOffset": 148, "endOffset": 152}, {"referenceID": 144, "context": "[151] extended the idea of using autoencoders to the multimodal domain.", "startOffset": 0, "endOffset": 5}, {"referenceID": 177, "context": "Similarly, Silberer and Lapata [184] proposed to use a multimodal autoencoder", "startOffset": 31, "endOffset": 36}, {"referenceID": 209, "context": "It is also common to fine-tune the resulting representation on a particular task at hand as the representation constructed using an autoencoder is generic and not necessarily optimal for a specific task [217].", "startOffset": 203, "endOffset": 208}, {"referenceID": 144, "context": "the disadvantages comes from the model not being able to handle missing data naturally \u2014 although there are ways to alleviate this issue [151], [217].", "startOffset": 137, "endOffset": 142}, {"referenceID": 209, "context": "the disadvantages comes from the model not being able to handle missing data naturally \u2014 although there are ways to alleviate this issue [151], [217].", "startOffset": 144, "endOffset": 149}, {"referenceID": 65, "context": "often difficult to train [69], but the field is making progress in better training techniques [196].", "startOffset": 25, "endOffset": 29}, {"referenceID": 189, "context": "often difficult to train [69], but the field is making progress in better training techniques [196].", "startOffset": 94, "endOffset": 99}, {"referenceID": 13, "context": "Probabilistic graphical models are another popular way to construct representations through the use of latent random variables [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 169, "context": "The most popular approaches for graphical-model based representation are deep Boltzmann machines (DBM) [176], that stack restricted Boltzmann machines (RBM) [81] as building blocks.", "startOffset": 103, "endOffset": 108}, {"referenceID": 76, "context": "The most popular approaches for graphical-model based representation are deep Boltzmann machines (DBM) [176], that stack restricted Boltzmann machines (RBM) [81] as building blocks.", "startOffset": 157, "endOffset": 161}, {"referenceID": 169, "context": "The appeal of DBMs comes from the fact that they do not need supervised data for training [176].", "startOffset": 90, "endOffset": 95}, {"referenceID": 169, "context": "As they are graphical models the representation of data is probabilistic, however it is possible to convert them to a deterministic neural network \u2014 but this loses the generative aspect of the model [176].", "startOffset": 199, "endOffset": 204}, {"referenceID": 190, "context": "Work by Srivastava and Salakhutdinov [197] introduced multimodal deep belief networks as a multimodal representation.", "startOffset": 37, "endOffset": 42}, {"referenceID": 98, "context": "[104] used a deep belief network for each modality and then combined them into joint representation for audiovisual emotion recognition.", "startOffset": 0, "endOffset": 5}, {"referenceID": 81, "context": "Huang and Kingsbury [86] used a similar model for AVSR, and Wu et al.", "startOffset": 20, "endOffset": 24}, {"referenceID": 217, "context": "[225] for audio and skeleton joint based gesture recognition.", "startOffset": 0, "endOffset": 5}, {"referenceID": 191, "context": "Multimodal deep belief networks have been extended to multimodal DBMs by Srivastava and Salakhutdinov [198].", "startOffset": 102, "endOffset": 107}, {"referenceID": 149, "context": "[156] explore the use of multimodal DBMs for the task of human pose estimation from multi-view data.", "startOffset": 0, "endOffset": 5}, {"referenceID": 192, "context": "[199] use multimodal DBM representation to perform Alzheimer\u2019s", "startOffset": 0, "endOffset": 5}, {"referenceID": 138, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 37, "endOffset": 42}, {"referenceID": 144, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 44, "endOffset": 49}, {"referenceID": 219, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 51, "endOffset": 56}, {"referenceID": 177, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 71, "endOffset": 76}, {"referenceID": 191, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 108, "endOffset": 113}, {"referenceID": 98, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 129, "endOffset": 134}, {"referenceID": 90, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 160, "endOffset": 164}, {"referenceID": 145, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 166, "endOffset": 171}, {"referenceID": 159, "context": "Joint Neural networks Images + Audio [145], [151], [227] Images + Text [184] Graphical models Images + Text [198] Images + Audio [104] Sequential Audio + Video [96], [152] Images + Text [166]", "startOffset": 186, "endOffset": 191}, {"referenceID": 56, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 37, "endOffset": 41}, {"referenceID": 99, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 43, "endOffset": 48}, {"referenceID": 152, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 62, "endOffset": 67}, {"referenceID": 223, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 69, "endOffset": 74}, {"referenceID": 27, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 100, "endOffset": 104}, {"referenceID": 204, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 106, "endOffset": 111}, {"referenceID": 240, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 113, "endOffset": 118}, {"referenceID": 212, "context": "Coordinated Similarity Images + Text [60], [105] Video + Text [159], [231] Structured Images + Text [31], [212], [248] Audio + Articulatory [220]", "startOffset": 140, "endOffset": 145}, {"referenceID": 191, "context": "The major disadvantage of DBMs is the difficulty of training them \u2014 high computational cost, and the need to use approximate variational training methods [198].", "startOffset": 154, "endOffset": 159}, {"referenceID": 77, "context": "Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213].", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213].", "startOffset": 202, "endOffset": 206}, {"referenceID": 205, "context": "Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213].", "startOffset": 208, "endOffset": 213}, {"referenceID": 8, "context": "a way that a decoder could reconstruct it [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 38, "context": "[42] on AVSR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166].", "startOffset": 54, "endOffset": 58}, {"referenceID": 145, "context": "representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166].", "startOffset": 60, "endOffset": 65}, {"referenceID": 159, "context": "representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166].", "startOffset": 157, "endOffset": 162}, {"referenceID": 57, "context": "For example such models encourage the representation of the word dog and an image of a dog to have a smaller distance between them than distance between the word dog and an image of a car [61].", "startOffset": 188, "endOffset": 192}, {"referenceID": 213, "context": "[221], [222] on the WSABIE (web scale annotation by image embedding) model, where a coordinated space was constructed for images and their annotations.", "startOffset": 0, "endOffset": 5}, {"referenceID": 214, "context": "[221], [222] on the WSABIE (web scale annotation by image embedding) model, where a coordinated space was constructed for images and their annotations.", "startOffset": 7, "endOffset": 12}, {"referenceID": 56, "context": "An example of such coordinated representation is DeViSE \u2014 a deep visual-semantic embedding [60].", "startOffset": 91, "endOffset": 95}, {"referenceID": 99, "context": "[105] extended this to sentence and image coordinated representation by using an LSTM model and a pairwise ranking loss to coordinate the feature space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 184, "context": "[191] tackle the same task, but extend the language model to a dependency tree RNN to incorporate compositional semantics.", "startOffset": 0, "endOffset": 5}, {"referenceID": 152, "context": "[159], but using videos instead of images.", "startOffset": 0, "endOffset": 5}, {"referenceID": 223, "context": "[231] also constructed a coordinated space between videos and sentences using a \u3008subject, verb, object\u3009 compositional language model and a deep video model.", "startOffset": 0, "endOffset": 5}, {"referenceID": 210, "context": "for similar objects [218].", "startOffset": 20, "endOffset": 25}, {"referenceID": 22, "context": "The idea of cross-modal hashing is to create such codes for cross-modal retrieval [26], [93], [113].", "startOffset": 82, "endOffset": 86}, {"referenceID": 87, "context": "The idea of cross-modal hashing is to create such codes for cross-modal retrieval [26], [93], [113].", "startOffset": 88, "endOffset": 92}, {"referenceID": 107, "context": "The idea of cross-modal hashing is to create such codes for cross-modal retrieval [26], [93], [113].", "startOffset": 94, "endOffset": 99}, {"referenceID": 22, "context": "as a hash function attempts to enforce all of these three requirements [26], [113].", "startOffset": 71, "endOffset": 75}, {"referenceID": 107, "context": "as a hash function attempts to enforce all of these three requirements [26], [113].", "startOffset": 77, "endOffset": 82}, {"referenceID": 86, "context": "For example, Jiang and Li [92] introduced a method to learn such common binary space between sentence descriptions and corresponding images", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "[31] extended the approach with a more complex", "startOffset": 0, "endOffset": 4}, {"referenceID": 211, "context": "[219] constructed a coordinated space in which images (and", "startOffset": 0, "endOffset": 5}, {"referenceID": 204, "context": "Another example of a structured coordinated representation comes from order-embeddings of images and language [212], [249].", "startOffset": 110, "endOffset": 115}, {"referenceID": 241, "context": "Another example of a structured coordinated representation comes from order-embeddings of images and language [212], [249].", "startOffset": 117, "endOffset": 122}, {"referenceID": 204, "context": "[212] enforces a dissimilarity metric that is asymmetric and implements the notion of partial order in the multimodal space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 230, "context": "[238] where denotation graphs are used to induce a partial ordering.", "startOffset": 0, "endOffset": 5}, {"referenceID": 241, "context": "present how exploiting structured representations of text and images can create concept taxonomies in an unsupervised manner [249].", "startOffset": 125, "endOffset": 130}, {"referenceID": 79, "context": "A special case of a structured coordinated space is one based on canonical correlation analysis (CCA) [84].", "startOffset": 102, "endOffset": 106}, {"referenceID": 72, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 64, "endOffset": 68}, {"referenceID": 100, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 70, "endOffset": 75}, {"referenceID": 162, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 77, "endOffset": 82}, {"referenceID": 170, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 115, "endOffset": 120}, {"referenceID": 180, "context": "CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].", "startOffset": 122, "endOffset": 127}, {"referenceID": 3, "context": "Extensions to CCA attempt to construct a correlation maximizing nonlinear projection [7], [116].", "startOffset": 85, "endOffset": 88}, {"referenceID": 110, "context": "Extensions to CCA attempt to construct a correlation maximizing nonlinear projection [7], [116].", "startOffset": 90, "endOffset": 95}, {"referenceID": 110, "context": "Kernel canonical correlation analysis (KCCA) [116] uses reproducing kernel Hilbert spaces for projection.", "startOffset": 45, "endOffset": 50}, {"referenceID": 3, "context": "Deep canonical correlation analysis (DCCA) [7] was introduced as an alternative to KCCA and addresses the scalability issue, it was also shown to lead to better correlated representation space.", "startOffset": 43, "endOffset": 46}, {"referenceID": 53, "context": "Similar correspondence autoencoder [57] and deep correspondence RBMs [56] have also been proposed for cross-modal retrieval.", "startOffset": 35, "endOffset": 39}, {"referenceID": 52, "context": "Similar correspondence autoencoder [57] and deep correspondence RBMs [56] have also been proposed for cross-modal retrieval.", "startOffset": 69, "endOffset": 73}, {"referenceID": 212, "context": "Deep canonically correlated autoencoders [220] also include an autoencoder based data reconstruction term.", "startOffset": 41, "endOffset": 46}, {"referenceID": 240, "context": "method [248] also encourages semantic relevance, while retaining correlation maximization and orthogonality of the resulting space \u2014 this leads to a combination of CCA and cross-modal hashing techniques.", "startOffset": 7, "endOffset": 12}], "year": 2017, "abstractText": "Our experience of the world is multimodal we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.", "creator": "LaTeX with hyperref package"}}}