{"id": "1705.09476", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Learning Robust Features with Incremental Auto-Encoders", "abstract": "automatically learning features, especially robust features, especially has attracted much attention in the machine learning community. in this paper, we propose a new instruction method \" to learn non - linear, robust features by taking advantage of the data manifold structure. we first follow the commonly used trick of the trade, that is learning robust features with artificially corrupted data, which are training samples with manually injected noise. following the idea of the auto - encoder, we first assume features should contain much information to well reconstruct the input from its corrupted copies. however, merely reconstructing clean input from its noisy copies could make data manifold in the feature space noisy. to address this problem, we propose a new method, called incremental auto - encoders, to iteratively denoise the extracted features. we assume the noisy manifold structure is caused by a diffusion process. consequently, we reverse this specific diffusion process to further contract this noisy model manifold, which readily results in an incremental optimization matrix of model parameters. furthermore, soon we show these learned non - linear features can even be stacked into a hierarchy of features. experimental investigation results on real - world datasets demonstrate the proposed method can achieve better classification performances.", "histories": [["v1", "Fri, 26 May 2017 08:30:41 GMT  (1840kb,D)", "http://arxiv.org/abs/1705.09476v1", "This work was completed in Feb, 2015"]], "COMMENTS": "This work was completed in Feb, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yanan li", "donghui wang"], "accepted": false, "id": "1705.09476"}, "pdf": {"name": "1705.09476.pdf", "metadata": {"source": "CRF", "title": "Learning Robust Features with Incremental Auto-Encoders", "authors": ["Yanan Li", "Donghui Wang"], "emails": ["ynli}@zju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Feature extraction, transforming the original input features to new feature space, has attracted much attention in machine learning community, especially when data are represented by high dimensional feature vectors. Many linear (e.g. PCA, LDA, etc.) and non-linear feature learning methods (e.g. sparse coding, dictionary learning, etc.) have been proposed to address this problem during the past few years [Elad, 2010; Bengio et al., 2013]. Recent years, learning robust features is getting more and more attention from researchers in various areas, especially in the deep learning community [Wan et al., 2013; Farabet et al., 2013].\nIn general, considering the types of training sets, robust feature learning methods can be roughly classified into two\ngroups. Algorithms in one group learn features from natural noisy datasets. Whereas, methods in the other group are given clean training datasets. In order to extract robust features, they learn with artificially corrupted data, which are training samples with manually injected noise. For example, to learn robust features, handwritten digits dataset are manually injected with various noises, such as random binary background noise or image background noise [Larochelle et al., 2007]. In the deep learning literature, DAE (Denoising Auto-encoder) [Vincent et al., 2010], composed of an encoding and a decoding function, is one of the best known building blocks for constructing a hierarchy of non-linear features. Features are made robust by reconstructing the clean input from its artificially corrupted copies via a decoding function. To make features invulnerable to different noises, AMC-SSDA combines multiple DAEs by a set of weights [Agostinelli et al., 2013]. Although the performances of these methods are prominent in many cases, their efficiency can also be improved since from the view of manifold learning, the high dimensional data are nearly lying on a low dimensional manifold. These methods have not taken fully considerations about the manifold structure.\nTo leverage the manifold structure, some methods have been proposed. Typical linear feature learning methods are: LPP [Niyogi, 2004], LLE [Roweis and Saul, 2000] and Isomap [Tenenbaum et al., 2000]. They learn linear features by preserving the local relationships within the data set and uncovering its essential manifold structure. There are also some other non-linear feature learning algorithms, such as SNE [Hinton and Roweis, 2002], t-SNE [Maaten, 2009], etc. Commonly, these methods use various neighborhood graphs to characterize the manifold structure. Differently, in the deep learning community, CAE (Contractive Auto-encoder) uses a contractive penalty term to force the learned features to capture the local direction of the nonlinear manifold [Rifai et al., 2011b]. Compared with linear feature learning approaches, non-linear methods have proven to perform better in many cases. In addition, extracting robust features with the consideration of non-linear manifolds structures has also attracted much attention [Hein and Maier, 2006; Wang and Tu, 2013]. However, these works learn the same dimensional features as the high dimensional input. In practice, not all features are relevant and important to the learning task, many of which are often redundant. ar X iv :1 70 5.\n09 47\n6v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 7\nIn light of these works, in this paper we propose a new method which can learn non-linear and robust features from manifold-embedded datasets. Similar to the above work, we first follow the well-known trick of the trade to learn with artificially corrupted data for extracting robust features. We assume extracted features should contain much information. Thus they can well reconstruct the clean input from its corresponding corrupted samples via a decoding function. To get more reliable features, we then using a denoising method based on the following assumption: local structures of data in different features space should be consistent. From the view of manifold learning, artificially corrupting data makes the embedded manifold of input noisy. Merely minimizing the reconstruction error can not guarantee manifold in the new feature space being noiseless. Thus manifolds are inconsistent with that of the input. To address this problem, we iteratively refine the learned features using a Laplacian-based method. We assume the noisy manifold is formed by a diffusion process on the Laplacian graph of data. We then reverse this diffusion process to denoise hidden features. Each step, representations are denoised towards the manifold. Step by step, the manifold structure of data becomes more and more refined. We further show that these non-linear features can be stacked to yield multiple levels of representations. Experimental results on several real world datasets illustrate that the proposed method can achieve better performance.\nThe remainder of the paper is organized as follows. We introduce the related work in Section 2. Then the proposed method is presented in Section 3, followed by the optimization of the proposed method in Section 4. Following the experimental results in Section 5, we conclude the paper in Section 6."}, {"heading": "2 Related Work", "text": "In recent years, automatically feature learning has received increasing attention from machine learning community, especially in the deep learning community, due to its wide applications in practice. There are a rich body of work on feature learning in the literature. We provide a review to the most related methods in this section.\nAuto-encoder. The auto-encoder is one of the most popular methods for learning informative non-linear features. It assumes these extracted features should contain as much information of input as possible and well reconstruct its input. To extract non-linear features, it exploits a direct parameterized function f(x), called encoder, to output hidden representations, defined as follows.\nh = f(x) = se(W1x + b1) (1) where W1 \u2208 RK\u00d7D is the weight matrix and b1 is the bias vector. h is the K-dimensional feature vector.\nIn the meanwhile, another function g(h), called decoder, is defined to map from feature space back into the input space, producing a reconstruction x\u0302. It is parameterized as:\nx\u0302 = g(h) = sd(W2h + b2) (2) where W2 \u2208 RK\u00d7D and b2 \u2208 RD are the weight matrix and bias vector respectively. se and sd are the activation functions, whose typical choices are sigmoid, tanh, rectified linear.\nThe set of parameters \u03b8 = {W1,W2,b1,b2} are learned simultaneously on the task of minimizing the reconstruction error over the whole training dataset X = {x1,x2, ...,xn} \u2208 RD\u00d7n, which correspond to the following optimization function:\n\u03b8? = arg min \u03b8\n1\nn n\u2211 i=1 l(xi, g(f(xi))), (3)\nwhere l is the reconstruction loss, whose typical choices are cross-entropy loss and the squared error loss.\nTraditionally, auto-encoder is used as a dimensionality reduction technique, which can learn equivalent or more useful features than what are obtained with simple linear PCA. Recently, a more successful use of auto-encoder is to learn overcomplete features, yielding more rich hidden representations. However, this renders the problem that the basic auto-encoder can learn an identity mapping with perfectly reconstructing its input and without extracting more meaningful features. To tackle this problem, various methods with different criteria have been proposed, such as sparse auto-encoder [Ngiam et al., 2011], RBM [Hinton et al., 2006] and so on. Among all the various constraints, robustness of features is most favored.\nDenoising Auto-encoder. One popular method to impose the robustness constraint is denoising auto-encoder (DAE). Except for remaining much information of input, it assumes good hidden features should well reconstruct its clean input from the corrupted copies, which avoids the uninteresting solutions of auto-encoder. From the geometric structure of input, which assumes high dimensional data are concentrated on a low dimensional manifoldM, DAE maps far away corrupted data to small regions close to the intrinsic data manifold. Formally, it is trained by the following function:\n\u03b8? = arg min \u03b8\n1\nnm n\u2211 i=1 m\u2211 j=1 l(xi, g(f(x\u0303ij ))), (4)\nwhere each sample xi is reconstructed from its m corrupted copies x\u0303ij = \u03c1(xi). Typical choices for the corrupting function \u03c1 are additive isotropic Gaussian noise, salt and pepper noise and masking noise. Comparing to the traditional autoencoder, these learned features are qualitatively better in classification performance. Exploiting DAE as a building block, several other methods have been proposed, such as AMCSDAE [Agostinelli et al., 2013], spDAE [Cho, 2013], mDAE [Chen et al., 2014] and so on.\nHowever, DAE still subjects to some drawbacks. Based on the manifold hypothesis, hidden representations correspond to an intrinsic coordinate system on the manifold structure. Variations in the input should be reflected in the learned representation. Whereas, since DAE makes the whole mapping robust instead of h, this assumption is not guaranteed. In addition, just mapping back corrupted samples to a nearby region makes the intrinsic manifold structure divergent. It fails to maintain the local structure when multiple manifolds exists in training data, which is often the case.\nContractive Auto-encoder. Another method to learn robust features is contractive auto-encoder (CAE). From a different perspective, it assumes features should be contractive along the orthogonal direction to the manifold. Its goal is\nachieved by adding a contractive penalty term directly on the hidden features to the basis auto-encoder. Hidden features are made insensitive to small changes of input by the Frobenius norm of the encoder\u2019s Jocabian. It is trained by minimizing the following objective function:\n\u03b8? = arg min \u03b8 n\u2211 i=1 l(xi, g(f(xi))) + \u03bb||J(xi)||2F (5)\nwhere J \u2208 RK\u00d7D is the encoder\u2019s Jacobian matrix and \u03bb is the trade-off parameter.\nComparing with DAE, CAE captures the local changes of the data manifold in the hidden representation. However, the contractive penalty term merely encourages robustness to infinitesimal changes of input. Thus, when data is corrupted by a large noise, it could fail. This problem is further considered by [Rifai et al., 2011a], which penalizes all higher order derivatives."}, {"heading": "3 InAE: Incremental Auto-Encoders", "text": ""}, {"heading": "3.1 Problem modeling", "text": "From the manifold hypothesis, hidden representations correspond to an intrinsic coordinate system on the embedded manifoldM. Variations along the manifold in the input space should be well captured or reflected in the learned representations. However, merely reconstructing clean input from itself or its noisy copies could make manifold M in the feature space noisy. As a result, intrinsic manifolds between original space and the hidden feature space are not consistent. To converge the noisy manifold, DAE uses a denoising criterion while CAE proposes a contractive penalty. Differing from them, we refine the manifold structure by reversing a diffusion process, which results in an incremental optimization of the model parameter."}, {"heading": "3.2 Reverse the diffusion process to contract the noisy manifold", "text": "The data manifold in the extracted feature space is noisy, which is obtained by learning with artificially corrupted data, i.e. training samples with manually injected noise. We assume the divergent manifold is caused by a diffusion process from the intrinsic manifoldM. Consequently, we propose to reverse the specific diffusion process to refine the manifold structure .\nFormally, given the noisy hidden features H = {h1,h2, ...,hnm} , we reverse the diffusion process iteratively by the following equation:\n\u2202tH = \u2212\u03b3LH, (6) where \u03b3 is the diffusion constant and t indicates the t-th iteration. L = (D \u2212 S) \u2208 RN\u00d7N is the Laplacian matrix of G, where S is the similarity matrix of X and Dii = \u2211N j=1 Sij . Along with the increase in the number of iteration, H is inching closer to the data manifoldM.\nUsing an implicit Euler-scheme, H is updated by the following function, Ht+1 = (I + \u03b4t\u03b3L)\u22121Ht, (7) where \u03b4t is the time-step and can be chosen arbitrarily. We assume step by step, h goes closer toM."}, {"heading": "3.3 Adaptively construct the neighboring graph", "text": "During the process of reversing the diffusion process, its generator, i.e. the graph Laplacian of the neighborhood graph is a key factor. Similarly, it does matter how the neighborhood graph is constructed. A good neighborhood graph can potentially preserve the locality of data manifold. Here we use two alternative strategies to construct the neighboring graph G.\nThe first one is the popular method k-nn, which chooses k nearest neighbors for data xi. k-nn performs pretty well in most cases. However, several problems still arises with k-nn, especially when data are concentrated on a non-linear manifold. (1) k-nn assumes data are distributed over a Gaussian distribution. This is often violated by real world data which are concentrated over a complex non-linear manifold. Most Euclidean nearest neighbors are chosen from different data manifolds. (2) When clusters have unbalanced number of training samples, it is not proper to set the same value of k for different clusters. For each data, it would be better to choose its nearest neighbors automatically according to the intrinsic manifold.\nThus, an alternative method is proposed. First, we utilize k-nn to choose relatively large number of neighbors for each data point x. These neighbors, denoted as XG(x), come from not only the same manifold as x, but also different manifolds of other classes. Then we explore a sparse subspace learning method [Elhamifar and Vidal, 2012] to further select these neighbors. The sparse subspace clustering method has turned out to be very effective for discovering data manifold in high dimensional space. It assumes each data x is a linear combination of its neighbors within the same cluster. By optimizing a `1 minimization problem, samples with non-zero coefficients are adaptively selected as neighbors. Thus, the nearest neighbors are finally selected by optimizing the following function:\narg min c ||x\u2212XG(x)c||+ \u03bb||c||1, (8)\nwhere c \u2208 Rk is the corresponding sparse coefficient. Points with the non-zeros coefficients, are treated as the neighbors of x, denoted as XL(x). Combining these two steps can not only select effective neighbors from the same manifold, but remove the neighbors lying in the different manifolds, as the experimental results demonstrate.\nThus, the similarity matrix S can be constructed as follows:\nSij = { d(xi,xj) if xj \u2208 XL(xi) 0 else (9)\nwhere d(xi,xj) measures the similarity between xi and xj , which can be chosen as Gaussian kernel exp\u2212 ||xi\u2212xj || 2\n2\u03c32 or cosine distance x T i xj\n||xi||||xj || [Yan et al., 2007]."}, {"heading": "3.4 Impose insensitivity to input noise", "text": "To further learn robust features, we follow the same idea in DAE, i.e. features should well reconstruct clean input from its corrupted copies . In each step, we assume the hidden feature h should: (1) contain much information of the input and well reconstruct x from its corrupted versions x\u0303. (2) approach\nthe intrinsic manifold gradually, i.e. manifold of H is being gradually denoised.\nThus, we formulate the proposed method, incremental auto-encoder (InAE) is obtained by combing Eq.4 with Eq.7. In each step, features are learned by optimizing the following objective function.\n\u03b8?t+1 = arg min \u03b8t+1\n1\nnm n\u2211 i=1 m\u2211 j=1 l(xi, g(f(x\u0303ij ))),\ns.t. Ht+1 = (I + \u03b4t\u03b3L)\u22121Ht,\n(10)\nwhere x\u0303ij is the j-th noisy copy of xi and H t = f(Wt1X\u0303 + bt1). From the objective function Eq.10, we see: (1) the proposed method explicitly constrains the extracted features, which are prompted to capture the variations of the input; (2) Differing from CAE-like methods, the noise magnitude is not confined to infinitesimal. Thus robustness of features is guaranteed."}, {"heading": "4 Optimization of the objective function", "text": "To train this model, we rewrite Eq.10 as a general regularized function. Following the idea in [Scherzer and Weickert, 2000], Eq.7 is equivalent to the solution of the minimization of the following regularization problem:\n\u03a6(Ht+1) = ||Ht+1\u2212Ht||2F +(\u03b4t)tr(Ht+1LHt+1 T ), (11)\ntr(\u00b7) computes the trace value. Thus, the objective function in each step becomes:\n\u03b8?t+1 = arg min \u03b8t+1\n1\nnm n\u2211 i=1 m\u2211 j=1 l(xi, g(f(x\u0303ij ))) + \u03b1\u03a6(H t+1),\n(12) where \u03b1 is the trade-off parameter between the reconstruction error and the process of reversing diffusion process. \u03b8t+1 = {Wt+11 ,W t+1 2 ,b t+1 1 ,b t+1 2 } contains all the parameters in (t+ 1)-th iteration. By analyzing the two penalty terms in Eq.11, we see (1) two consecutive updates of H are forced to change smoothly. In other words, h comes gradually closer to the manifoldM, which evades the oscillation phenomenon when optimizing the objective function. (2) close-by points in the original space is rendered to be close in the new feature space. Since tr(Ht+1LHt+1 T ) = \u2211N i,j ||h t+1 i \u2212 h t+1 j ||2Sij , if xi and xj are close, i.e. Sij is large, hi and hj should be close as well. Specifically, the local structure in the data can be maintained.\nDifferent from the traditional auto-encoder, the proposed method is trained by an incremental optimization procedure, resulting in a series of parameter updates:\nH0 \u2192 \u03b81 \u2192 H1 \u2192 \u03b82 . . .HT\u22121 \u2192 \u03b8T , (13)\nwhere T denotes the number of update. Each parameter \u03b8t+1 is better than the last update \u03b8t.\nTo obtain each parameter \u03b8, Eq.12 is optimized by stochastic gradient descent. Here, we just give a simple description of the first derivative of the last penalty. For clarity, we omit\nthe subscript t+ 1 and denote the whole number of H as N . First, we compute the derivative w.r.t. each element Wij1 .\n\u2202tr(HLHT ) \u2202Wij1 = \u2202 \u2211N m,n ||hm \u2212 hn||2Smn \u2202Wij1\n= \u2202 \u2211N m,n Smn(h T mhm + h T nhn \u2212 2hTmhn)\n\u2202Wij1 = 2 \u2211N mDmm\u2202h T mhm\n\u2202Wij1 \u2212\n2 \u2211N m,n Smn\u2202h T mhn\n\u2202Wij1\n(14)\nAs hm is a non-linear function of zm = W1xm + b1, using the chain rule, Eq.(14) is written as:\n4 N\u2211 m (Dmmhmj \u2212 N\u2211 n Wmnhnj ) \u2202hmj \u2202zmj xmi , (15)\nwhere hmj is the j-th element in hm. Therefore, we get the derivative w.r.t. W1 as follows:\n\u2202tr(HLHT )\n\u2202W1\n= 4 N\u2211 m {xm[(Dmmhm \u2212 N\u2211 n Wmnhn) T \u25e6 s\u2032(zm)]T }\n(16)\nwhere \u25e6 is the Hadamard product. In this paper, we use the sigmoid function in the encoder, where s\u2032(x) = s(x)(1 \u2212 s(x)).\nIn summary, the whole training algorithm is described in Algorithm. 1\nAlgorithm 1 Training the Incremental Auto-Encoder Require: Training data X, parameter k and \u03c3 in constructing\nW, number of iterations T ; Ensure: Model parameters \u03b8 = {W1, W2, b1 , b2};\n1: Generate the noisy training dataset X\u0303 from X; 2: Construct an adaptive neighborhood graph G on training\ndata; 3: Compute similarity matrix S on G by Eq.9; 4: Compute the Laplacian matrix L = D\u2212 S; 5: Initialize H0; 6: for each iteration t do 7: Update \u03b8t by stochastic gradient descent; 8: Obtain Ht = f(Wt1X\u0303 + b t 1);\n9: end for 10: \u03b8 = \u03b8T ;"}, {"heading": "4.1 Multiple levels of representation", "text": "Similar to the auto-encoder, we treat the proposed method as a building block of forming the deep architecture. Stacking several layers to initialize a deep network works in much the same way as stacking auto-encoders. We stack multiple layers by feeding the output h of lth layer as input into the (l + 1)th layer. Once one layer is trained, the encoding function f is used to generate uncorrupted input for the next layer.\nOnce a deep architecture has thus been built, its highest level output representation can be used as input to a stand-alone supervised learning algorithm. Experimental results show that the high level representations achieve better performance."}, {"heading": "5 Experimental Results", "text": "In this section, we separate the experiments into model validation on synthetic data and performance comparison on benchmark datasets, showing the prominent locality preserving and discriminative performance of our proposed method."}, {"heading": "5.1 Model validation on synthetic data", "text": "Dataset. To validate our proposed method, we generate a moon-like dataset consisting of 2 clusters, each of which is generated from a 2-D function and embedded into a 9-D space with an isotropic Gaussian noise \u223c N(0, \u03c3I).\nEvaluation metric. We evaluate our method on the locality preserving ability in two indicatorsNratio and Cratio, and on discriminative power in classification error.\nSince the number of neighbours is adaptive, we choose the first k neighbours corresponding to the number in k-nn algorithm. For each data xi, we introduce Nratio as the ratio of the number of selected neighbours on same manifold of xi to k and Cratio indicates the percentage of sum of the coefficients of selected neighbours on same manifold. And\nlarge values mean good locality preserving ability. The error is compared when k is 30 and iterations number t is 5.\nExperimental results. In order to show the performance of the proposed InAE, we give simple illustrations in Fig.1.\nFig.1 (a) and (b) illustrate results on indicators Nratio and Cratio of the two strategy for constructing neighboring graph, as described in Section 3.3. We see in (a) when k is small, knn and our method perform almost equally well, since the data is relatively large. The difference in Nratio grows larger as the number of neighbours is larger. Similar result of Cratio in (b) further convinces the locality preserving ability of our method.\nFig.1 (c) and (d) show the errors respect to dimensionality and noise scale on several numbers of iterations t. We notice from (c) that higher level representation is more discriminative, and our model is more suitable when the dimension is higher. And (d) shows when Gaussian noise scale \u03c3 is small, higher level representations are already able to obtain high classification performance. However, large \u03c3 destroys data severely, causing high classification error.\nFig.2 gives an intuitive denoised results H, with t increases, H become cleaner and preserve their original manifold at the same time. From Fig.1 and Fig.2, we see just 2 or 3 iterations is sufficient to greatly improve the classification accuracy of hidden features. The discrimination power is\nstrengthened at the cost of small computational complexity."}, {"heading": "5.2 Performance on benchmark datasets", "text": "Dataset. After the validation of our model on synthetic data, we compare our method with state-of-the-art algorithms on several popular benchmark datasets, i.e. the handwritten digits MNIST dataset and its variants, and CIFAR-10 dataset. The variants of MNIST are generated by imposing various challenging factors [Larochelle et al., 2007]. The CIFAR-10 dataset consists of 6000 32\u00d7 32 color images in 10 classes.\nEvaluation metric. Besides the traditional classification error metric on synthetic data, we employ another measure eig(S\u22121w Sb), which denotes the maximum eigenvalue of S\u22121w Sb, where Sw and Sb are the with-in-class and betweenclass variance of h, respectively. This measure is inspired from Fisher LDA, which assumes that a discriminative feature should make data in different classes far away, while data in same class close to each other. Large value indirectly indicates the discriminative power of the hidden representations, and better classification performance is expected.\nExperimental results. We compare our method against the following algorithms on feature extraction: AE (traditional auto-encoder), DAE-b (denosing auto-encoders with masking-out noise), CAE (contractive auto-encoders) and RBM (restricted Boltzmann machine). We use a linear SVM on the raw image pixels as baseline. For all methods but RBM, we use untied weights (i.e. W1 6= W2) in each layer and train them using Stochastic Gradient Descent. RBM is trained using Contrastive Divergence, of which the hyperparameter are chosen by a grid search on a validation set.\nWe represent the classification error and eig(S\u22121w Sb) on MNIST and CIFAR-10 datasets in Tab.2, and error rate on MNIST variant datasets in Tab.1. Our proposed method achieves best performance in almost all datasets, and Tab.2 proves that the metric eig(S\u22121w Sb) is positively correlated with classification performance. We draw a conclusion that distance between samples provides features valuable information for subsequent tasks. Since the manifold structure of data is important for good performance, you may try to well utilize it before we have a chance. Moreover, Tab.1 demonstrates that stacking multiple layers significantly improves performance."}, {"heading": "6 Conclusions", "text": "In this paper, we proposed a novel robust feature learning method by utilizing the Laplacian structure of training data. To learn robust features, it follows the well known trick in machine learning and learns with artificially corrupted data, which are training samples with manually injected noise. First, we assume features should contain much information and well reconstruct the clean input. However, since the data manifold is injected by some noise, this structure can not be consistent in the new feature space, if features are learned merely based on minimization of the reconstruction error. To address this problem, we model the noisy manifold is the result of a diffusion process on the Laplacian graph of training data. Then we reverse this specific diffusion process to denoise the manifold. Each time the diffusion process is reversed, the manifold is refined. This results in an incremental optimization of model parameters. In addition, a new strategy of constructing the neighboring graph of data is introduced. We find that the Incremental Auto-Encoder is capable of contracting the noisy manifold in the feature space. Experimental results on real-world datasets suggest that the Incremental Auto-Encoder performs better than other comparing methods."}], "references": [{"title": "In Advances in Neural Information Processing Systems", "author": ["Forest Agostinelli", "Michael R Anderson", "Honglak Lee. Adaptive multi-column deep neural networks with application to robust image denoising"], "venue": "pages 1493\u20131501,", "citeRegEx": "Agostinelli et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Representation learning: A review", "author": ["Bengio et al", "2013] Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14)", "author": ["Minmin Chen", "Kilian Q Weinberger", "Fei Sha", "Yoshua Bengio. Marginalized denoising autoencoders for nonlinear representations"], "venue": "pages 1476\u20131484,", "citeRegEx": "Chen et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple sparsification improves sparse denoising autoencoders in denoising highly noisy images", "author": ["KyungHyun Cho"], "venue": "ICML,", "citeRegEx": "Cho. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse and redundant representations: from theory to applications in signal and image processing", "author": ["Michael Elad"], "venue": "Springer Science & Business Media,", "citeRegEx": "Elad. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse subspace clustering: Algorithm", "author": ["Ehsan Elhamifar", "Ren\u00e9 Vidal"], "venue": "theory, and applications. CoRR, abs/1203.1005,", "citeRegEx": "Elhamifar and Vidal. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "IEEE Transactions on", "author": ["Clement Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun. Learning hierarchical features for scene labeling. Pattern Analysis", "Machine Intelligence"], "venue": "35(8):1915\u2013 1929,", "citeRegEx": "Farabet et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Manifold denoising. In Advances in Neural Information Processing Systems", "author": ["Hein", "Maier", "2006] Matthias Hein", "Markus Maier"], "venue": "Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems,", "citeRegEx": "Hein et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hein et al\\.", "year": 2006}, {"title": "In Advances in neural information processing systems", "author": ["Geoffrey E Hinton", "Sam T Roweis. Stochastic neighbor embedding"], "venue": "pages 833\u2013840,", "citeRegEx": "Hinton and Roweis. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural computation", "author": ["Geoffrey Hinton", "Simon Osindero", "Yee-Whye Teh. A fast learning algorithm for deep belief nets"], "venue": "18(7):1527\u20131554,", "citeRegEx": "Hinton et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["Larochelle et al", "2007] Hugo Larochelle", "Dumitru Erhan", "Aaron C. Courville", "James Bergstra", "Yoshua Bengio"], "venue": "In Machine Learning, Proceedings of the Twenty-Fourth International Conference", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "In International Conference on Artificial Intelligence and Statistics", "author": ["Laurens Maaten. Learning a parametric embedding by preserving local structure"], "venue": "pages 384\u2013391,", "citeRegEx": "Maaten. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proceedings of the 28th International Conference on Machine Learning (ICML-11)", "author": ["Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Quoc V Le", "Andrew Y Ng. On optimization methods for deep learning"], "venue": "pages 265\u2013272,", "citeRegEx": "Ngiam et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Locality preserving projections", "author": ["X Niyogi"], "venue": "Neural information processing systems, volume 16, page 153", "citeRegEx": "Niyogi. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Higher order contractive autoencoder", "author": ["Xavier Glorot"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Glorot.,? \\Q2011\\E", "shortCiteRegEx": "Glorot.", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai et al", "2011b] Salah Rifai", "Pascal Vincent", "Xavier Muller", "Xavier Glorot", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Science", "author": ["Sam T Roweis", "Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding"], "venue": "290(5500):2323\u20132326,", "citeRegEx": "Roweis and Saul. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Journal of Mathematical Imaging and Vision", "author": ["Otmar Scherzer", "Joachim Weickert. Relations between regularization", "diffusion filtering"], "venue": "12(1):43\u201363,", "citeRegEx": "Scherzer and Weickert. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Science", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford. A global geometric framework for nonlinear dimensionality reduction"], "venue": "290(5500):2319\u20132323,", "citeRegEx": "Tenenbaum et al.. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent et al", "2010] Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "In Proceedings of the 30th International Conference on Machine Learning (ICML-13)", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus. Regularization of neural networks using dropconnect"], "venue": "pages 1058\u20131066,", "citeRegEx": "Wan et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In 2013 IEEE Conference on Computer Vision and Pattern Recognition", "author": ["Bo Wang", "Zhuowen Tu. Sparse subspace denoising for image manifolds"], "venue": "Portland, OR, USA, June 23-28, 2013, pages 468\u2013475,", "citeRegEx": "Wang and Tu. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph embedding and extensions: a general framework for dimensionality reduction", "author": ["Shuicheng Yan", "Dong Xu", "Benyu Zhang", "Hong-Jiang Zhang", "Qiang Yang", "Stephen Lin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(1):40\u201351,", "citeRegEx": "Yan et al.. 2007", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": ") have been proposed to address this problem during the past few years [Elad, 2010; Bengio et al., 2013].", "startOffset": 71, "endOffset": 104}, {"referenceID": 20, "context": "Recent years, learning robust features is getting more and more attention from researchers in various areas, especially in the deep learning community [Wan et al., 2013; Farabet et al., 2013].", "startOffset": 151, "endOffset": 191}, {"referenceID": 6, "context": "Recent years, learning robust features is getting more and more attention from researchers in various areas, especially in the deep learning community [Wan et al., 2013; Farabet et al., 2013].", "startOffset": 151, "endOffset": 191}, {"referenceID": 0, "context": "To make features invulnerable to different noises, AMC-SSDA combines multiple DAEs by a set of weights [Agostinelli et al., 2013].", "startOffset": 103, "endOffset": 129}, {"referenceID": 13, "context": "Typical linear feature learning methods are: LPP [Niyogi, 2004], LLE [Roweis and Saul, 2000] and Isomap [Tenenbaum et al.", "startOffset": 49, "endOffset": 63}, {"referenceID": 16, "context": "Typical linear feature learning methods are: LPP [Niyogi, 2004], LLE [Roweis and Saul, 2000] and Isomap [Tenenbaum et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 18, "context": "Typical linear feature learning methods are: LPP [Niyogi, 2004], LLE [Roweis and Saul, 2000] and Isomap [Tenenbaum et al., 2000].", "startOffset": 104, "endOffset": 128}, {"referenceID": 8, "context": "There are also some other non-linear feature learning algorithms, such as SNE [Hinton and Roweis, 2002], t-SNE [Maaten, 2009], etc.", "startOffset": 78, "endOffset": 103}, {"referenceID": 11, "context": "There are also some other non-linear feature learning algorithms, such as SNE [Hinton and Roweis, 2002], t-SNE [Maaten, 2009], etc.", "startOffset": 111, "endOffset": 125}, {"referenceID": 21, "context": "In addition, extracting robust features with the consideration of non-linear manifolds structures has also attracted much attention [Hein and Maier, 2006; Wang and Tu, 2013].", "startOffset": 132, "endOffset": 173}, {"referenceID": 12, "context": "To tackle this problem, various methods with different criteria have been proposed, such as sparse auto-encoder [Ngiam et al., 2011], RBM [Hinton et al.", "startOffset": 112, "endOffset": 132}, {"referenceID": 9, "context": ", 2011], RBM [Hinton et al., 2006] and so on.", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "Exploiting DAE as a building block, several other methods have been proposed, such as AMCSDAE [Agostinelli et al., 2013], spDAE [Cho, 2013], mDAE [Chen et al.", "startOffset": 94, "endOffset": 120}, {"referenceID": 3, "context": ", 2013], spDAE [Cho, 2013], mDAE [Chen et al.", "startOffset": 15, "endOffset": 26}, {"referenceID": 2, "context": ", 2013], spDAE [Cho, 2013], mDAE [Chen et al., 2014] and so on.", "startOffset": 33, "endOffset": 52}, {"referenceID": 5, "context": "Then we explore a sparse subspace learning method [Elhamifar and Vidal, 2012] to further select these neighbors.", "startOffset": 50, "endOffset": 77}, {"referenceID": 22, "context": "sine distance x T i xj ||xi||||xj || [Yan et al., 2007].", "startOffset": 37, "endOffset": 55}, {"referenceID": 17, "context": "Following the idea in [Scherzer and Weickert, 2000], Eq.", "startOffset": 22, "endOffset": 51}], "year": 2017, "abstractText": "Automatically learning features, especially robust features, has attracted much attention in the machine learning community. In this paper, we propose a new method to learn non-linear robust features by taking advantage of the data manifold structure. We first follow the commonly used trick of the trade, that is learning robust features with artificially corrupted data, which are training samples with manually injected noise. Following the idea of the auto-encoder, we first assume features should contain much information to well reconstruct the input from its corrupted copies. However, merely reconstructing clean input from its noisy copies could make data manifold in the feature space noisy. To address this problem, we propose a new method, called Incremental Auto-Encoders, to iteratively denoise the extracted features. We assume the noisy manifold structure is caused by a diffusion process. Consequently, we reverse this specific diffusion process to further contract this noisy manifold, which results in an incremental optimization of model parameters . Furthermore, we show these learned non-linear features can be stacked into a hierarchy of features. Experimental results on real-world datasets demonstrate the proposed method can achieve better classification performances.", "creator": "LaTeX with hyperref package"}}}