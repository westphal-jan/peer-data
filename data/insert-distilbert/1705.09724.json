{"id": "1705.09724", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Semi-Supervised Model Training for Unbounded Conversational Speech Recognition", "abstract": "for conversational acoustic large - vocabulary continuous speech recognition ( lvcsr ) tasks, up to about its two thousand hours of audio is commonly used to train state productions of the art models. collection of labeled conversational audio however, research is prohibitively expensive, laborious and error - prone. furthermore, academic corpora implementations like fisher einstein english ( 2004 ) or motorola switchboard ( 1992 ) are inadequate adequate to train performance models with sufficient accuracy in the unbounded space of dubbed conversational speech. these corpora are also timeworn due to dated acoustic telephony features and the rapid advancement opportunities of colloquial vocabulary techniques and idiomatic speech over the entire last decades. utilizing the colossal scale of studying our unlabeled telephony dataset, we propose a technique to construct virtually a modern, high quality labeled conversational speech, training pattern corpus on the order of hundreds of millions of utterances ( or tens cycles of thousands of hours ) for both acoustic psychology and language model training. we describe the data collection, selection and training, evaluating the results of our updated speech recognition system on a test based corpus of 7k manually transcribed utterances. we show maximum relative word error rate ( wer ) level reductions of { 35 %, 19 % } on { agent, caller } utterances over our seed model and reach 5 % absolute response wer in improvements over ibm watson stt on this conversational speech task.", "histories": [["v1", "Fri, 26 May 2017 21:10:15 GMT  (1403kb,D)", "http://arxiv.org/abs/1705.09724v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shane walker", "morten pedersen", "iroro orife", "jason flaks"], "accepted": false, "id": "1705.09724"}, "pdf": {"name": "1705.09724.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Model Training for Unbounded Conversational Speech Recognition", "authors": ["Shane Walker", "Morten Pedersen", "Jason Flaks"], "emails": [], "sections": [{"heading": null, "text": "Utilizing the colossal scale of our unlabeled telephony dataset, we propose a technique to construct a modern, high quality conversational speech training corpus on the order of hundreds of millions of utterances (or tens of thousands of hours) for both acoustic and language model training. We describe the data collection, selection and training, evaluating the results of our updated speech recognition system on a test corpus of 7K manually transcribed utterances. We show relative word error rate (WER) reductions of {35%, 19%} on {agent, caller} utterances over our seed model and 5% absolute WER improvements over IBM Watson STT on this conversational speech task.\nIndex Terms: conversational speech recognition, acoustic modeling, language modeling, large unsupervised training sets, data selection, data augmentation"}, {"heading": "1 Introduction", "text": "This paper examines a semi-supervised approach that aims to increase the quantity of conversational telephony speech transcripts available to train a LVCSR system. We define dataset construction and training as semi-supervised because we employ a seed model to transcribe a vast quantity of unlabeled audio, perform data selection on the new transcripts, retrain the seed model and then repeat the pro-\ncess with the improved decoder [15]. Our approach works by running a large-beam decoder tuned for high accuracy on our unlabeled telephony dataset. Lattices generated during the decoding process are used to compute Minimum Bayes Risk (MBR) confidences. The transcribed text is filtered to select minimum-length utterances with the lowest MBR confidence [28] and the lowest language model (LM) perplexity. Perplexity is a measurement of how well a probabilistic LM will predict new sample text. We use an LM trained on 20K manually transcribed in-domain conversational utterances.\nThis method takes advantage of the scale of Marchex\u2019s call traffic, enabling us to rapidly construct a very largescale speech dataset, covering all types of language contexts, speaker demographics, accents and noise conditions. It also permits tracking of changes in quotidian vernacular as well as changes in acoustic channel features based on shifts in device and codec technology.\nBecause the error rate of the confidence-filtered training data can limit the gains due to poor acoustic modeling alignments [26][27], we use various natural language processing (NLP) heuristics to algorithmically identify the highest prevalence, unique mistranscriptions for correction. We have developed tools to facilitate the creation and application of corrective text transforms over the full corpus of automatic transcriptions. The updated, post-processed text improves the quality of our acoustic model training alignments and so we iterate anew by retraining our acoustic model from scratch with the transformed text.\nFor language modeling, the set of unique, text transform targets (i.e. the applied corrections) are added to the ground truth set used to build a new LM for the next iteration of filtering and utterance perplexity scoring. The core of our approach, an iterative method of taking qualified output from a seed model, using various NLP heuristics to further correct and select utterances for use in the subsequent rounds of training, allows us to progressively reduce the error rate of our ASR models, while operating at a scale that allows us to generalize well on in-domain speech.\nThe paper is organized as follows, Section 2 will provide some perspective on the complexity of the conversa-\nar X\niv :1\n70 5.\n09 72\n4v 1\n[ cs\n.C L\n] 2\n6 M\nay 2\n01 7\ntional ASR task. Section 3 will review recent schemes for speech dataset construction, especially for large-scale and low-resourced tasks. Section 4 describes our Speech Recognition system. Section 5 introduces the Marchex U.S. English Corpus of conversational North American english, discusses semi-supervised training and the data pipeline. Section 6 presents our results and contrasts our corpus with other conversational corpora. Section 7 describes how we scale up and areas of future work."}, {"heading": "2 Conversational ASR", "text": "Automatic speech recognition (ASR) of spontaneous conversations is a different and more complex task than ASR for Voice Command or Voice Search applications performed by modern digital assistants. In addition to the usual challenges in LVCSR (e.g. speaker-independence, coarticulation, variable speech rates, noise-robustness and LM capacity) in natural, unscripted conversations additional factors come into play. These include disfluencies such as mid-sentence hesitations, stutters, ungrammatical or filled pauses (uh, um, ah, er), back-channels (yeah, mhm, uhhuh), discourse markers (like, so, you know), self-editing terms (or rather, I mean), cut-off phrases, restarts, repetitions, final lengthening of syllables, coughs and laughter [14][5].\nIn an article which tackles the linguistic appropriations and interpretations of Chomsky\u2019s \u201cColorless Green Ideas Sleep Furiously\u201d, Manfred discusses a co-operative principle in human communication which binds two speakers to conversational maxims [9]. For speakers and listeners, this amounts to a set of interpretive assumptions that are very flexible in the presence of ungrammatical, rhetorical, figurative or completely novel utterances. This means that in free flowing conversation, semi-grammatical incongruities and semantic ill-formedness are always admissible when the utterance is well-chosen and/or the listener obtains a meaningful interpretation. Now considering that the English language has approximately half a million words, excluding many colloquial forms, with Unabridged English dictionaries listings of between 300,000 to 600,000 words, we see the combinatorial complexity and valid correct transcription of an arbitrary spontaneous utterance though finite, is unbounded [17]."}, {"heading": "3 Building ASR Training Corpora", "text": "There are many approaches to building speech training sets, including acoustic data perturbation and data synthesis [11]. Our survey of the literature will be restricted to unsupervised and semi-supervised approaches to corpora assembly.\nGoogle takes advantage of their large scale in constructing a training set for their Voice Search and Voice Input tasks for low-resource languages such as Brazilian Portuguese, Italian, Russian and French [10]. Their unsupervised approach makes use of a slow but accurate decoder, confidence scores, transcript length and transcript flattening heuristics to select the utterances for acoustic modeling.\nIn conjunction with owner-uploaded transcripts, Youtube apply \u201cisland of confidence\u201d filtering heuristics to generate additional semi-supervised training data for the deep neural network (DNN) based acoustic model (AM) driving their closed captions feature [16].\nKapralova et al. and Yu et al. [10][29] train acoustic models on a Mandarin language Broadcast News (BN) and Broadcast Conversation (BC) dataset created with semisupervised techniques. Due to the prevalence of English loan words and code-switching, data selection starts with a dual Mandarin-English language classifier, followed by the computation of utterance and word-level decoder confidence scores for the Mandarin-only utterances.\nRagni et al. [21] use a semi-supervised system to build corpora for low-resource languages Zulu and Assamese task, using weighted word-confusion-network confidences for data selection.\nLi et al. [15] employ semi-supervised methods to construct a Mandarin training corpus based on a Chinese television spoken lectures series, using conditional random fields (CRF) for confidence estimation instead of the raw ASR decoder confidence measure.\nEnarvi et al. [6][12] tackle a conversational Finnish language ASR task with a novel semi-supervised approach to training text selection. In lieu of adding new transcribed candidate utterances to the corpus based on low in-domain LM perplexity, they score utterances by the decrease in indomain perplexity when the utterance is removed from the set of candidate utterances.\nFor a low-resource English, German and Spanish LVCSR task, Thomas et al. [25] use a hybrid confidence score based on word-level ASR confidence as well as a posteriogram-based phoneme occurrence confidence. This latter confidence uses a posteriogram representation of an utterance computed by passing utterance acoustic features through a trained DNN classifier."}, {"heading": "4 Speech Recognition System", "text": "The seed ASR system is based on an online decoder written using Kaldi, a free, open-source C++ toolkit for speech recognition research [19]. In online decoding, the input audio features are processed buffer by buffer, progressively emitting the output text with minimal latency and without having to ingest the entire input before producing output. The seed decoder uses a \u201cprebuilt\u201d deep neural network\n- hidden Markov model (DNN-HMM) hybrid model provided with Kaldi. In this hybrid model, a DNN is trained via minibatch asynchronous stochastic gradient descent to emit HMM posterior probabilities. These are then converted into \u201cscaled likelihoods\u201d for the states of an HMM. In contrast to Gaussian mixture models (GMM) traditionally used in speech recognition, DNN models are superior classifiers that generalize better with a smaller number of model parameters, even when the dimensionality of the input features is very high [26]. Cross-entropy loss is the DNN training objective function, a standard choice for classification tasks.\nThe seed decoder\u2019s DNN is a 4 hidden layer neural network where the final layer is a softmax layer with a dimension corresponding to each of the 3500 context-dependent HMM states [22].\nThe input feature pipeline consumes 25 millisecond frames, processed to generate 13-dimensional Melfrequency cepstral coefficients (MFCCs), which are spliced together with \u00b13 frames of context, for a total of 7 \u00b7 13 frames. The input dimensionality is then reduced to 40 by applying linear discriminant analysis (LDA) followed by a decorrelation step using maximum likelihood linear transform (MLLT). Finally a speaker normalization transform is applied, called feature-space maximum likelihood linear regression (fMLLR) [19].\nDuring decoding the DNN takes an input feature vector 140 elements wide, comprising the 40 \u201ccooked\u201d features described above and a 100 element iVector. The same iVector is used for all acoustic feature vectors associated with a given speaker\u2019s utterances in the training set. Augmenting a new speaker\u2019s input feature vector with a corresponding iVector projection before DNN processing, permits the DNN to discriminate better between phonetic events in an adaptive, speaker-independent fashion [8], with minimal impact to the DNN training cycle.\nThe seed model was trained on 1935 hours of conversational audio. We extend it by rebuilding its decoding-graph to incorporate an additional 20K manually transcribed indomain language modeling utterances and expand the lexicon by an additional 600 domain-specific phonetic pronunciations. The lexicon is based on CMUdict, but with numeric stress markers removed.\nThe seed decoder\u2019s language model is a trigram model created from text from 1.6M (Fisher English) utterances using the SRILM toolkit. The lexicon, acoustic and language models are compiled down to weighted finite state transducers (WFSTs), which are composed into a single structure called the decoding-graph. Each letter stands for a separate WFST performing a specific input-to-output transduction: HCLG = min(det(H \u25e6 C \u25e6 L \u25e6G)).\n1. H maps multiple HMM states to context-dependent triphones.\n2. C maps triphone sequences to monophones.\n3. L is the lexicon, it maps monophone sequences to words.\n4. G represents a language model FST converted from an ARPA format n-gram model.\nWhen the graph is composed with an utterance\u2019s perframe DNN output (i.e. HMM state likelihoods) it produces a lattice. The best path through the lattice produces text. For further details on ASR with weighted finitestate transducers refer to [18]. To summarize, our seed ASR system is a prebuilt Kaldi online-nnet2, cross-entropy trained, hybrid DNN-HMM model. It has an updated lexicon and language model and provides a competitive and well-understood baseline upon which we iterate."}, {"heading": "5 Training System Description", "text": "First we introduce a brand new conversational telephony speech corpus of North American english and then describe our semi-supervised training and data selection methods in detail."}, {"heading": "5.1 The Marchex US English Corpus", "text": "Marchex\u2019s call and speech analytics business securely fields over one million calls per business day, or decades of encrypted audio recordings per week. These are conversational, consumer to business phone calls occurring via a modern mixture of mobile phones on various telephone networks or landlines, capturing everyday North American dialog in every possible accent variant, English language fluency, under broad environmental or noise conditions, with comprehensive, colloquial vocabulary. Speaker demographics are extensive from teenagers to octogenarians. Example conversations may be sales related, e.g. calling to book a hotel, buying a mobile phone, cable service or to renegotiate insurance rates. Other examples are service related, e.g. scheduling a dentist appointment, an oil change, car repair or a house-moving service. The average conversation is four minutes long. Both the caller and the answering agent channels are recorded. This unlabeled corpus of calls is current, exhibiting natural and spontaneous conversations on business matters, in addition to popular culture, sports, politics and chitchat on uncontroversial topics like the weather."}, {"heading": "5.2 Data Collection and Processing", "text": "To make use of this telephony dataset, we programmatically gather call audio from the fleet of Marchex call processor servers. Mono 8kHz \u00b5-law-decoded files from the caller\nand agent channel are passed to a Voice Activity Detector (VAD) which creates single utterances usually shorter than 5 seconds long. For our initial experiments, we decoded a subset of 35 million utterances or some 25,000 hours of raw conversational audio with roughly a 44%-56% split between caller and agent. This split is less than even due to VAD\u2019s rejection of silent or degenerate caller-side audio, e.g. voicemail, fax machines calling phones.\nThe system architecture is shown in Figure 1. Solid lines show the flow of data towards the AM and LM training corpora. Dotted lines denote updates to the ASR decoder, as well as updates to language modeling text used for perplexity based data selection.\nSimilarly to [10], we decode using a slower but more accurate, non-production decoder tuned to have a large-beam. The decoder emits N-best lattices which we use to compute MBR confidences per utterance, with Kaldi\u2019s lattice-mbrdecode. Our 20K manually transcribed corpus was similarly decoded and MBR scores were compared to Word Error Rate (WER). Shown below in Figure 2, a strong correlation between low MBR score and low WER suggests that\nMBR will useful for selecting accurately transcribed utterances. In the table below, we outline WER statistics for two values of very low MBR. Interestingly, the 90%+ WER utterances turn out to either be systemic mistranscriptions or Spanish language IVR prompts.\nWER Statistics MBR=0.0 MBR=0.1 count 337 1263 mean 4.14 20.3 std 0.0 0.0 min 0.0 0.0 25% 0.0 0.0 50% 0.0 0.0 50% 0.0 0.0 75% 0.0 0.0 90% 6.25 28.5 95% 20.0 50.0 max 100.0 100.0\nKaldi\u2019s lattice-confidence is another confidence that we examined. Its value is the difference in total costs between the best and second-best paths through the N-best lattice. We ultimately rejected it due to lack of performance and low correlation with WER.\nWhile MBR is used as a measure of expected risk in the \u201cwhole system\u201d based on the full N-best lattice, language model perplexity is a measurement of how well a probabilistic LM will predict a new sample text. Low perplexity indicates the LM is good at predicting the new text and is not \u201cconfused\u201d. When combined, MBR and perplexity provide good intuition about how \u201chard\u201d it was for the system to arrive at its 1-best text output, the assertion being that\nlower WER utterances are \u201ceasier\u201d to decode. For selecting utterances, we compute perplexity with a Kneser-Ney smoothed 5-gram model with a 125K vocabulary and 5M n-grams [3].\nGiven the scale of our audio retrieval and the simplicity of the VAD used, there are a few different kinds of nonspeech audio that get automatically transcribed that we certainly do not want in our training set. These include: holdtime musak, telephony signaling tones, Spanish language utterances (especially in IVR), pseudo random impulsive noises from typing on keyboards, cellphones dropping, Rihanna, laughter, coughing and other environmental noise. Utterances in the table below will have high perplexity, i.e. greater than 1000, when scored with a 5-gram LM trained on our 20K manually transcribed corpus. We remove up to 7M such degenerate utterances along with utterances with very short transcripts.\nRemoved Utterances Audio content be in they need to \u201cbienvenidos\u201d (Spanish) but i spend you own \u201cpara espagnol\u201d (Spanish) bull pretty men dogs \u201coprima dos\u201d (Spanish) much guess seem go \u201cmarque cinqo\u201d (Spanish) it it it\u2019s it\u2019s it telephony signaling noise whole whole whole telephonic beeps mhm mhm mhm mhm impulsive noise mm mm mm mm mm impulsive noise [noise] i i i environmental noise and uh and uh and uh hold music or or or or or or hold music and uh and uh Rihanna song in a in a in in a hold music\nNext we look at other systemic errors that we can correct. The approach taken is based on global and sub-structure frequency of the seed transcribed text. By sorting and counting the highest prevalence unique full utterances, we identify common elements where incomplete language representation and/or missing audio context can be fixed. Substructure frequencies are counted by using n-gram or partof-speech tagging to isolate sub-elements to be amended. For example in the table below \u201ca grey day\u201d can be part of \u201cyou have a great day\u201d or \u201cit is a great day\u201d.\nCaller Mistranscriptions Ground truth have a grey day have a great day yeah that be great yeah that\u2019d be great okay think so much okay thanks so much b. e. as in boy b. as in boy a two one zero eight two one zero i don\u2019t have any count i don\u2019t have an account\nIdentification and creation of targeted replacements are prepared manually via custom tools developed to present\ntop candidates for correction. We distinguish caller versus agent side text because the nature of conversational speech on the caller side is much more diverse. Additionally, agent audio quality is usually higher, as agents may be in a call center or quiet office, while the caller may be in the car, on the street or on the bus. Below is a small section of agent side mistranscriptions, a number of which are from the Interactive Voice Response (IVR) utterances.\nAgent Mistranscriptions Ground truth horror leather increase for all other inquiries (IVR) rest you press two (IVR) oppressed you or press two (IVR) arrest three press three (IVR) um or cared customer care call back drone normal call back during normal for parts in excess serious for parts and accessories active eight activate chevy taco chevy tahoe now and if you words now in a few words retire fritcher jack free tire pressure check\nOnce transforms have been created and applied to automatic utterances, the corrected text is ready to be filtered with both MBR and perplexity at thresholds appropriate for acoustic modeling. Language modeling text is also derived from the same corrected text but with much tighter perplexity thresholds, usually 40-80. From the original batch of 35M utterances, we are left with between 2.5M and 5M pristine utterances. We contrast these figures with the 1.6M utterances that comprise the Fisher English corpus [4].\nFigure 3 details the Text Processing and Selection block\nshown in Figure 1. We note two paths though the system. The first (solid magenta) is strictly for generating corrected text to build the AM and LM training corpora. The second path (yellow dotted) is for the generation of new LM building text derived transform targets (i.e. text corrections). These manual contributions are language modeling ground truth and are admissible to improve the capacity and the ability of our LM to generalize in subsequent training iterations. Starting with just the 20K manually transcribed corpus for language modeling, through this iterative process we grew the LM text used to compute perplexity in all parts of the system by another 6K items. This is a 30% increase in the amount of high quality LM text and more importantly, text which comprises the correct labels for the most common in-domain conversational phrases."}, {"heading": "5.3 Retraining The ASR Model", "text": "After we\u2019ve successfully prepared the corrected, filtered automatic transcripts, it is time to retrain our ASR model. For retraining, we choose the cleanest 5,000 hours or 11.7M utterances. This figure was selected to have minimal impact on the training recipe\u2019s hyperparameters, with an eye out for maximum data-capacity of the training model. For AM training, we add in 13K utterances from our 20K manually transcribed set to the automatic, cleaned corpus. Because our manually transcribed utterances are the most accurate, these 13K are used for the training recipe\u2019s initial monophone and triphone steps. The remaining 7K utterances are excluded as a test set. Given the relative data increase over the seed model, we use a larger-capacity multi-splice version of the online-nnet2 recipe described in Section 4. This recipe uses 2 additional fully-connected hidden layers, for a total of 6, and a more elaborate input splicing scheme. We train this model on a single 12GB NVIDIA Titan X (Pascal) GPU for 6 full epochs over a period of 2 weeks."}, {"heading": "6 Experimental Results", "text": "Our results are as follows on the Marchex North American english conversational task, a test set based on our 7K manually transcribed utterances, excluded in the training process described above. We compute WER scores for the seed model, IBM Watson Speech to Text service [23][24] as well as our updated production model. Our updated model shows strong performance against IBM and demonstrates our ability to generalize well on an unseen dataset with a model trained on a mixture of manually transcribed and automatic transcriptions.\nWhile IBM have not trained their models on Marchex English, their results are valid benchmarks because of the their published results on the Hub5 2000 Evaluation conversational task, a corpus of 40 test telephone conversations\nfrom the SWBD and CALLHOME corpora [23][24]. We contrast the proportions of Hub5 2000 with the size of our test set of 7K utterances or 4.5 hours of no-filler, manually transcribed conversational audio, sourced from more than 3,000 calls.\nModel Agent WER Relative gain Seed model 22.1 - IBM Watson STT 20.0 9.5% Marchex production 14.27 35%\nModel Caller WER Relative gain Seed model 21.6 - IBM Watson STT 22.6 -4.6% Marchex production 17.5 19%"}, {"heading": "6.1 Comparing conversational corpora", "text": "To better understand our performance with respect to the IBM models trained on Fisher English (FE) or Switchboard (SWBD), we now examine more closely how the Marchex English (ME) is different. By ME, we refer only to this first, post-seed iteration of 11.7M utterances or 5,000 hours used to train an updated model.\nDuring the collection of FE, topics were pre-assigned or worked out between the contributors [4]. For SWBD, a prompt suggested a topic of conversation [7]. ME on the other hand, captures real world conversations with the full degree of naturalness. FE furthermore excludes greetings and leave-takings, which we consider essential to correctly decode. SWBD transcribers were asked post-facto to rate the naturalness of conversations on a 5-point scale from \u201cvery natural\u201d to \u201cartificial-sounding\u201d. The mean rating for SWBD utterances is 1.48 [7].\nFE calls lasted no more than 10 minutes from which 8 minutes were deemed useful. ME calls last on average 4 minutes, but can be as short as 30 seconds, as in a voicemail or wrong number. They can also be as long as an hour in the case that a contract is being negotiated or there are terms and conditions to be agreed upon. The much longer temporal context under which ME utterances are automatically collected, adds to the diversity of the corpus. In Figure 4 we show the distribution of durations in minutes of a 37K sampling of ME calls. In the table below, we draw further contrast between FE, SWBD and Marchex English [5][4].\nSWBD FE ME Hours 309 2,000 5,000+ Speakers 543 20,407 605K Utterances 391K 1.6M 11.7M Conversations 2,400 16,000+ 288K Words 3M 18M 79.5M"}, {"heading": "7 Conclusions", "text": "In this report we have outlined results from only one iteration of our semi-supervised approach. We review our plan to scale up and promising next steps."}, {"heading": "7.1 Scaling Up", "text": "Encouraged by our very competitive error-rates, we see a lot of potential as our corpus grows. A natural question at this point, given an iterative process which produces increasingly large quantities of audio and text is: How do we scale up processing in a time and cost efficient manner?\nGiven our goals of iterating on a monthly cadence, our solution is to use modern, cloud-based distributed computation. Initial work to collect, decode, clean utterances and train our models took place over a couple of months in a small local-cluster environment. So our first step was to move our corpus of 30M post-VAD utterances from the first round and well as 30M brand new utterances, hot off the wire, into an Amazon Web Services (AWS) S3 bucket. An S3 bucket is a logical unit of storage used to store data objects (audio and text) as well as any corresponding metadata like utterance ids, speaker-to-utterance mapping, etc.\nNext, we build an Amazon Machine Image (AMI), which provides the information required to launch a virtual machine instance, pre-configured with the requisite 64-bit system architecture, operating system, Python environment, Kaldi decoder and other software dependencies. Now we can spin up a dynamic and configurable cluster of virtual machines for re-decoding and post-processing.\nTo reliably manage the scheduling and distribution of audio to be re-decoded and VMs ready to accept work, we use Amazon Simple Queue Service (Amazon SQS). This service offers a highly-scalable hosted queue for sending, storing and receiving messages and is designed to guarantee that messages are processed exactly once, in the exact order that they are sent, with limited throughput [1].\nNow with a SQS queue, a fleet of VMs and S3 data, we are ready to re-decode. First we start off by populating our SQS queue with work items. This is done by generating a S3 Inventory Report, which is an enumeration of the 60M (audio) data objects to re-decode. We initialize our SQS queue from this report. Then we simply turn on the fleet and our SQS queue distributes messages to it. Messages are simply locations in S3 of audio (utterances) to decode. Our fleet consists of \u201cspot instances\u201d, a flexible, cost-effective alternative VM provisioning solution especially for data analysis, batch and background processing jobs where applications can be interrupted. At any time during processing if a VM goes away or stops, the message will merely timeout and go back into the queue to be rescheduled for processing by another VM. With a fleet of 100 mixed class {cc2.8xlarge, r4.8xlarge, x1.16xlarge, m4.16xlarge} VM instances, we are able to re-decode 30M utterances in 20 hours.\nFinally to do utterance filtering, processing and ASR model retraining, we employ a GPU-enabled AWS P2 instance like the p2.16xlarge. With 16 NVIDIA K80 GPUs each with 12 GB of memory, 64 virtual CPUs, 700+ GB of memory and low-latency, peer-to-peer GPU-to-GPU transfers, this class of machine is most commonly used for scientific and industrial scale deep learning tasks. While true\ndistributed end-to-end ASR model training is an eventual objective, the P2 instance solution is most compatible with the parallelization tools in our Kaldi-based training recipe and provides immediate performance gains. In lieu of the weeks it took to train the first iteration using a local GPU, our AWS solution completes in a matter of days."}, {"heading": "7.2 Future Work", "text": "Our semi-supervised training process permits us to compile very large, high quality conversational speech datasets orders of magnitude greater than what is possible via manual transcription. The manual effort is highly focused on specific tasks that have the highest impact on WER reduction and that improve the compounding effect of rinsing and repeating with a bigger and better decoder, trained on cleaner and larger quantities of correctly labeled audio.\nFuture work includes making the VAD more selective, improving language detection and speech signal conditioning. There are also opportunities to use RNN-LM or CNN models for text classification to do more powerful data selection [13]. Furthermore, we see a lot of potential in the algorithmic superiority of bleeding-edge ASR methods using attention-based models or sequence trained neural networks with lattice-free MMI or CTC objectives [2][20]."}], "references": [{"title": "Deep speech 2: End-toend speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "In Proceedings of the 34th annual meeting on Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Resegmentation of switchboard", "author": ["N. Deshmukh", "A. Ganapathiraju", "A. Gleeson", "J. Hamaker", "J. Picone"], "venue": "In ICSLP. Syndey,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Studies on training text selection for conversational finnish language modeling", "author": ["S. Enarvi", "M. Kurimo"], "venue": "In Proceedings of the 10th International Workshop on Spoken Language Translation (IWSLT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Ivector-based speaker adaptation of deep neural networks for french broadcast audio transcription", "author": ["V. Gupta", "P. Kenny", "P. Ouellet", "T. Stafylakis"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "colorless green ideas sleep furiously: A linguistic test case and its appropriations. Literature and Linguistics: Approaches, Models and Applications: Studies in Honour of Jon Erickson", "author": ["M. Jahn"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "A big data approach to acoustic model training corpus selection", "author": ["O. Kapralova", "J. Alex", "E. Weinstein", "P.J. Moreno", "O. Siohan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Audio augmentation for speech recognition", "author": ["T. Ko", "V. Peddinti", "D. Povey", "S. Khudanpur"], "venue": "In INTER- SPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Modeling underresourced languages for speech recognition", "author": ["M. Kurimo", "S. Enarvi", "O. Tilk", "M. Varjokallio", "A. Mansikkaniemi", "T. Alum\u00e4e"], "venue": "lang. Res. Eval,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["S. Lai", "L. Xu", "K. Liu", "J. Zhao"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Recognizing disfluencies in conversational speech", "author": ["M. Lease", "M. Johnson", "E. Charniak"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Semisupervised acoustic model training by discriminative data selection from multiple asr systems", "author": ["S. Li", "Y. Akita", "T. Kawahara"], "venue": "hypotheses. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Large scale deep neural network acoustic modeling with semisupervised training data for youtube video transcription", "author": ["H. Liao", "E. McDermott", "A. Senior"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Infinite number of sentences", "author": ["R. Mannell"], "venue": "Technical report, Macquarie University, Department of Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Speech recognition with weighted finite-state transducers", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "In Springer Handbook of Speech Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding, number EPFL-CONF-192584", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE Signal Processing Society,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["D. Povey", "V. Peddinti", "D. Galvez", "P. Ghahrmani", "V. Manohar", "X. Na", "Y. Wang", "S. Khudanpur"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Data augmentation for low resource languages", "author": ["A. Ragni", "K.M. Knill", "S.P. Rath", "M.J. Gales"], "venue": "In IN- TERSPEECH,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Improved feature processing for deep neural networks", "author": ["S.P. Rath", "D. Povey", "K. Vesel\u1ef3", "J. Cernock\u1ef3"], "venue": "In Interspeech,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "The IBM 2015 english conversational telephone speech recognition", "author": ["G. Saon", "H.J. Kuo", "S.J. Rennie", "M. Picheny"], "venue": "system. CoRR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "The IBM 2016 english conversational telephone speech recognition", "author": ["G. Saon", "T. Sercu", "S.J. Rennie", "H.J. Kuo"], "venue": "system. CoRR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Deep neural network features and semisupervised training for low resource speech recognition", "author": ["S. Thomas", "M.L. Seltzer", "K. Church", "H. Hermansky"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "In Interspeech,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Unsupervised training for mandarin broadcast news and conversation transcription", "author": ["L. Wang", "M.J. Gales", "P.C. Woodland"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Minimum bayes risk decoding and system combination based on a recursion for edit distance", "author": ["H. Xu", "D. Povey", "L. Mangu", "J. Zhu"], "venue": "Computer Speech & Language,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Unsupervised training and directed manual transcription for lvcsr", "author": ["K. Yu", "M. Gales", "L. Wang", "P.C. Woodland"], "venue": "Speech Communication,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "We define dataset construction and training as semi-supervised because we employ a seed model to transcribe a vast quantity of unlabeled audio, perform data selection on the new transcripts, retrain the seed model and then repeat the process with the improved decoder [15].", "startOffset": 268, "endOffset": 272}, {"referenceID": 25, "context": "The transcribed text is filtered to select minimum-length utterances with the lowest MBR confidence [28] and the lowest language model (LM) perplexity.", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "Because the error rate of the confidence-filtered training data can limit the gains due to poor acoustic modeling alignments [26][27], we use various natural language processing (NLP) heuristics to algorithmically identify the highest prevalence, unique mistranscriptions for correction.", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "Because the error rate of the confidence-filtered training data can limit the gains due to poor acoustic modeling alignments [26][27], we use various natural language processing (NLP) heuristics to algorithmically identify the highest prevalence, unique mistranscriptions for correction.", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "These include disfluencies such as mid-sentence hesitations, stutters, ungrammatical or filled pauses (uh, um, ah, er), back-channels (yeah, mhm, uhhuh), discourse markers (like, so, you know), self-editing terms (or rather, I mean), cut-off phrases, restarts, repetitions, final lengthening of syllables, coughs and laughter [14][5].", "startOffset": 326, "endOffset": 330}, {"referenceID": 2, "context": "These include disfluencies such as mid-sentence hesitations, stutters, ungrammatical or filled pauses (uh, um, ah, er), back-channels (yeah, mhm, uhhuh), discourse markers (like, so, you know), self-editing terms (or rather, I mean), cut-off phrases, restarts, repetitions, final lengthening of syllables, coughs and laughter [14][5].", "startOffset": 330, "endOffset": 333}, {"referenceID": 6, "context": "In an article which tackles the linguistic appropriations and interpretations of Chomsky\u2019s \u201cColorless Green Ideas Sleep Furiously\u201d, Manfred discusses a co-operative principle in human communication which binds two speakers to conversational maxims [9].", "startOffset": 248, "endOffset": 251}, {"referenceID": 14, "context": "Now considering that the English language has approximately half a million words, excluding many colloquial forms, with Unabridged English dictionaries listings of between 300,000 to 600,000 words, we see the combinatorial complexity and valid correct transcription of an arbitrary spontaneous utterance though finite, is unbounded [17].", "startOffset": 332, "endOffset": 336}, {"referenceID": 8, "context": "There are many approaches to building speech training sets, including acoustic data perturbation and data synthesis [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "Google takes advantage of their large scale in constructing a training set for their Voice Search and Voice Input tasks for low-resource languages such as Brazilian Portuguese, Italian, Russian and French [10].", "startOffset": 205, "endOffset": 209}, {"referenceID": 13, "context": "In conjunction with owner-uploaded transcripts, Youtube apply \u201cisland of confidence\u201d filtering heuristics to generate additional semi-supervised training data for the deep neural network (DNN) based acoustic model (AM) driving their closed captions feature [16].", "startOffset": 257, "endOffset": 261}, {"referenceID": 7, "context": "[10][29] train acoustic models on a Mandarin language Broadcast News (BN) and Broadcast Conversation (BC) dataset created with semisupervised techniques.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[10][29] train acoustic models on a Mandarin language Broadcast News (BN) and Broadcast Conversation (BC) dataset created with semisupervised techniques.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "[21] use a semi-supervised system to build corpora for low-resource languages Zulu and Assamese task, using weighted word-confusion-network confidences for data selection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] employ semi-supervised methods to construct a Mandarin training corpus based on a Chinese television spoken lectures series, using conditional random fields (CRF) for confidence estimation instead of the raw ASR decoder confidence measure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[6][12] tackle a conversational Finnish language ASR task with a novel semi-supervised approach to training text selection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[6][12] tackle a conversational Finnish language ASR task with a novel semi-supervised approach to training text selection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[25] use a hybrid confidence score based on word-level ASR confidence as well as a posteriogram-based phoneme occurrence confidence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The seed ASR system is based on an online decoder written using Kaldi, a free, open-source C++ toolkit for speech recognition research [19].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "In contrast to Gaussian mixture models (GMM) traditionally used in speech recognition, DNN models are superior classifiers that generalize better with a smaller number of model parameters, even when the dimensionality of the input features is very high [26].", "startOffset": 253, "endOffset": 257}, {"referenceID": 19, "context": "The seed decoder\u2019s DNN is a 4 hidden layer neural network where the final layer is a softmax layer with a dimension corresponding to each of the 3500 context-dependent HMM states [22].", "startOffset": 179, "endOffset": 183}, {"referenceID": 16, "context": "Finally a speaker normalization transform is applied, called feature-space maximum likelihood linear regression (fMLLR) [19].", "startOffset": 120, "endOffset": 124}, {"referenceID": 5, "context": "Augmenting a new speaker\u2019s input feature vector with a corresponding iVector projection before DNN processing, permits the DNN to discriminate better between phonetic events in an adaptive, speaker-independent fashion [8], with minimal impact to the DNN training cycle.", "startOffset": 218, "endOffset": 221}, {"referenceID": 15, "context": "For further details on ASR with weighted finitestate transducers refer to [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "Similarly to [10], we decode using a slower but more accurate, non-production decoder tuned to have a large-beam.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "For selecting utterances, we compute perplexity with a Kneser-Ney smoothed 5-gram model with a 125K vocabulary and 5M n-grams [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 20, "context": "We compute WER scores for the seed model, IBM Watson Speech to Text service [23][24] as well as our updated production model.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "We compute WER scores for the seed model, IBM Watson Speech to Text service [23][24] as well as our updated production model.", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "While IBM have not trained their models on Marchex English, their results are valid benchmarks because of the their published results on the Hub5 2000 Evaluation conversational task, a corpus of 40 test telephone conversations from the SWBD and CALLHOME corpora [23][24].", "startOffset": 262, "endOffset": 266}, {"referenceID": 21, "context": "While IBM have not trained their models on Marchex English, their results are valid benchmarks because of the their published results on the Hub5 2000 Evaluation conversational task, a corpus of 40 test telephone conversations from the SWBD and CALLHOME corpora [23][24].", "startOffset": 266, "endOffset": 270}, {"referenceID": 4, "context": "For SWBD, a prompt suggested a topic of conversation [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "48 [7].", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In the table below, we draw further contrast between FE, SWBD and Marchex English [5][4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "There are also opportunities to use RNN-LM or CNN models for text classification to do more powerful data selection [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "Furthermore, we see a lot of potential in the algorithmic superiority of bleeding-edge ASR methods using attention-based models or sequence trained neural networks with lattice-free MMI or CTC objectives [2][20].", "startOffset": 204, "endOffset": 207}, {"referenceID": 17, "context": "Furthermore, we see a lot of potential in the algorithmic superiority of bleeding-edge ASR methods using attention-based models or sequence trained neural networks with lattice-free MMI or CTC objectives [2][20].", "startOffset": 207, "endOffset": 211}], "year": 2017, "abstractText": "For conversational large-vocabulary continuous speech recognition (LVCSR) tasks, up to about two thousand hours of audio is commonly used to train state of the art models. Collection of labeled conversational audio however, is prohibitively expensive, laborious and error-prone. Furthermore, academic corpora like Fisher English (2004) or Switchboard (1992) are inadequate to train models with sufficient accuracy in the unbounded space of conversational speech. These corpora are also timeworn due to dated acoustic telephony features and the rapid advancement of colloquial vocabulary and idiomatic speech over the last decades. Utilizing the colossal scale of our unlabeled telephony dataset, we propose a technique to construct a modern, high quality conversational speech training corpus on the order of hundreds of millions of utterances (or tens of thousands of hours) for both acoustic and language model training. We describe the data collection, selection and training, evaluating the results of our updated speech recognition system on a test corpus of 7K manually transcribed utterances. We show relative word error rate (WER) reductions of {35%, 19%} on {agent, caller} utterances over our seed model and 5% absolute WER improvements over IBM Watson STT on this conversational speech task.", "creator": "LaTeX with hyperref package"}}}