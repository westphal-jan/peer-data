{"id": "1705.09899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Understanding Abuse: A Typology of Abusive Language Detection Subtasks", "abstract": "as the body of research reflecting on abusive language detection strategies and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been widely grouped under this national label. based on theoretical work on hate speech, cyberbullying, blogs and online email abuse we accordingly propose a typology that captures central similarities and differences between subtasks and targeting we discuss its implications for data annotation and feature construction. we emphasize the practical actions that can be taken by researchers to typically best approach their abusive language detection subtask of interest.", "histories": [["v1", "Sun, 28 May 2017 06:59:07 GMT  (35kb)", "https://arxiv.org/abs/1705.09899v1", "To appear in the proceedings of the 1st Workshop on Abusive Language Online. Please cite that version"], ["v2", "Tue, 30 May 2017 11:07:51 GMT  (35kb)", "http://arxiv.org/abs/1705.09899v2", "To appear in the proceedings of the 1st Workshop on Abusive Language Online. Please cite that version"]], "COMMENTS": "To appear in the proceedings of the 1st Workshop on Abusive Language Online. Please cite that version", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zeerak waseem", "thomas davidson", "dana warmsley", "ingmar weber"], "accepted": false, "id": "1705.09899"}, "pdf": {"name": "1705.09899.pdf", "metadata": {"source": "CRF", "title": "Understanding Abuse: A Typology of Abusive Language Detection Subtasks", "authors": ["Zeerak Waseem", "Thomas Davidson", "Dana Warmsley"], "emails": ["z.w.butt@shef.ac.uk,", "trd54@cornell.edu,", "dw457@cornell.edu,", "iweber@hbku.edu.qa"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n09 89\n9v 2\n[ cs\n.C L\n] 3\n0 M\nay 2\n01 7\nAs the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and we discuss its implications for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.\n1"}, {"heading": "1 Introduction", "text": "There has been a surge in interest in the detection of abusive language, hate speech, cyberbullying, and trolling in the past several years (Schmidt and Wiegand, 2017). Social media sites have also come under increasing pressure to tackle these issues. Similarities between these subtasks have led scholars to group them together under the umbrella terms of \u201cabusive language\u201d, \u201charmful speech\u201d, and \u201chate speech\u201d (Nobata et al., 2016; Faris et al., 2016; Schmidt and Wiegand, 2017) but little work has been done to examine the relationship between them. As each of these subtasks seeks to address a specific yet partially overlapping phenomenon, we believe that there is much to gain by studying how they are related.\nThe overlap between subtasks is illustrated by the variety of labels used in prior work. For example, in annotating for cyberbullying events,\n1This paper has been accepted at the 1st Workshop on Abusive Language Online. Please be sure to cite that version.\nVan Hee et al. (2015b) identifies discriminative remarks (racist, sexist) as a subset of \u201cinsults\u201d, whereas Nobata et al. (2016) classifies similar remarks as \u201chate speech\u201d or \u201cderogatory language\u201d. Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines - some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al. (2017).\nTo help to bring together these literatures and to avoid these contradictions, we propose a typology that synthesizes these different subtasks. We argue that the differences between subtasks within abusive language can be reduced to two primary factors:\n1. Is the language directed towards a specific\nindividual or entity or is it directed towards a generalized group?\n2. Is the abusive content explicit or implicit?\nEach of the different subtasks related to abusive language occupies one or more segments of this typology. Our aim is to clarify the similarities and differences between subtasks in abusive language detection to help researchers select appropriate strategies for data annotation and modeling."}, {"heading": "2 A typology of abusive language", "text": "Much of the work on abusive language subtasks can be synthesized in a two-fold typology that con-\nsiders whether (i) the abuse is directed at a specific target, and (ii) the degree to which it is explicit.\nStarting with the targets, abuse can either be directed towards a specific individual or entity, or it can be used towards a generalized Other, for example people with a certain ethnicity or sexual orientation. This is an important sociological distinction as the latter references a whole category of people rather than a specific individual, group, or organization (see Brubaker 2004, Wimmer 2013) and, as we discuss below, entails a linguistic distinction that can be productively used by researchers. To better illustrate this, the first row of Table 1 shows examples from the literature of directed abuse, where someone is either mentioned by name, tagged by a username, or referenced by a pronoun.2 Cyberbullying and trolling are instances of directed abuse, aimed at individuals and online communities respectively. The second row shows cases with abusive expressions towards generalized groups such as racial categories and sexual orientations. Previous work has identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al. (2016) come closest to making a distinction between directed and generalized hate.\nThe other dimension is the extent to which abusive language is explicit or implicit. This is roughly analogous to the distinction in linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by Barthes (1957). Explicit abusive lan-\n2All punctuation is as reported in original papers. We have added all the * symbols.\nguage is that which is unambiguous in its potential to be abusive, for example language that contains racial or homophobic slurs. Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012; Davidson et al., 2017), with abusive terms being used in a colloquial manner or by people who are victims of abuse. Implicit abusive language is that which does not immediately imply or denote abuse. Here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches (Dinakar et al., 2011; Dadvar et al., 2013; Justo et al., 2014). Social scientists and activists have recently been paying more attention to implicit, and even unconscious, instances of abuse that have been termed \u201cmicroaggressions\u201d (Sue et al., 2007). As the examples show, such language may nonetheless have extremely abusive connotations. The first column of Table 1 shows instances of explicit abuse, where it should be apparent to the reader that the content is abusive. The messages in the second column are implicit and it is harder to determine whether they are abusive without knowing the context. For example, the word \u201cthem\u201d in the first two examples in the generalized and implicit cell refers to an ethnic group, and the words \u201cskypes\u201d and \u201cGoogle\u201d are used as euphemisms for slurs about Jews and African-Americans respectively. Abuse using sarcasm can be even more elusive for detection systems, for instance the seemingly harmless comment praising someone\u2019s intelligence was a sarcastic response to a beauty pageant contestants unsatisfactory answer to a question (Dinakar et al.,\n2011)."}, {"heading": "3 Implications for future research", "text": "In the following section we outline the implications of this typology, highlighting where the existing literatures indicate how we can understand, measure, and model each subtype of abuse."}, {"heading": "3.1 Implications for annotation", "text": "In the task of annotating documents that contain bullying, it appears that there is a common understanding of what cyberbullying entails: an intentionally harmful electronic attack by an individual or group against a victim, usually repetitive in nature (Dadvar et al., 2013). This consensus allows for a relatively consistent set of annotation guidelines across studies, most of which simply ask annotators to determine if a post contains bullying or harassment (Dadvar et al., 2014; Kontostathis et al., 2013; Bretschneider et al., 2014). High interannotator agreement on cyberbullying tasks (93%) (Dadvar et al., 2013) further indicates a general consensus around the features of cyberbullying (Van Hee et al., 2015b). After bullying has been identified annotators are typically asked more detailed questions about the extremity of the bullying, the identification of phrases that indicate bullying, and the roles of users as bully/victim (Dadvar et al., 2014; Van Hee et al., 2015b; Kontostathis et al., 2013).\nWe expect that consensus may be due to the directed nature of the phenomenon. Cyberbullying involves a victim whom annotators can identify and relatively easily discern whether statements directed towards the victim should be considered abusive. In contrast, in work on annotating harassment, offensive language, and hate speech there appears to be little consensus on definitions and lower inter-annotator agreement (\u03ba \u2248 0.60\u22120.80) (Ross et al., 2016; Waseem, 2016a; Tulkens et al., 2016; Bretschneider and Peters, 2017) are obtained. Given that these tasks are often broadly defined and the target is often generalized, all else being equal, it is more difficult for annotators to determine whether statements should be considered abusive. Future work in these subtasks should aim to have annotators distinguish between targeted and generalized abuse so that each subtype can be modeled more effectively.\nAnnotation (via crowd-sourcing and other\nmethods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011). The connotations of language can be difficult to classify without domainspecific knowledge. Furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions (Davidson et al., 2017), others find that they do not improve the reliability of non-expert classifications (Ross et al., 2016). In such cases, expert annotators with domain specific knowledge are preferred as they tend to produce more accurate classifications (Waseem, 2016a).\nUltimately, the nature of abusive language can be extremely subjective, and researchers must endeavor to take this into account when using human annotators. Davidson et al. (2017), for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism. As such, it is important that researchers consider the social biases that may lead people to disregard certain types of abuse.\nThe type of abuse that researchers are seeking to identify should guide the annotation strategy. Where subtasks occupy multiple cells in our typology, annotators should be allowed to make nuanced distinctions that differentiate between different types of abuse. In highlighting the major differences between different abusive language detection subtasks, our typology indicates that different annotation strategies are appropriate depending on the type of abuse."}, {"heading": "3.2 Implications for modeling", "text": "Existing research on abusive language online has used a diverse set of features. Moving forward, it is important that researchers clarify which features are most useful for which subtasks and which subtasks present the greatest challenges. We do not attempt to review all the features used (see Schmidt and Wiegand 2017 for a detailed review) but make suggestions for which features could be most helpful for the different subtasks. For each aspect of the typology, we suggest features that have been shown to be successful predictors in prior work. Many features occur in more than one form of abuse. As such, we do not propose that particular features are necessarily unique to each\nphenomenon, rather that they provide different insights and should be employed depending on what the researcher is attempting to measure.\nDirected abuse. Features that help to identify the target of abuse are crucial to directed abuse detection. Mentions, proper nouns, named entities, and co-reference resolution can all be used in different contexts to identify targets. Bretschneider and Peters (2017) use a multi-tiered system, first identifying offensive statements, then their severity, and finally the target. Syntactical features have also proven to be successful in identifying abusive language. A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015; Davidson et al., 2017). Typed dependencies offer a more sophisticated way to capture the relationship between terms (Burnap and Williams, 2015). Overall, there are many tools that researchers can use to model the relationship between abusive language and targets, although many of these require high-quality annotations to use as training data.\nGeneralized abuse. Generalized abuse online tends to target people belonging to a small set of categories, primarily racial, religious, and sexual minorities (Silva et al., 2016). Researchers should consider identifying forms of abuse unique to each target group addressed, as vocabularies may depend on the groups targeted. For example, the language used to abuse trans-people and that used against Latin American people are likely to differ, both in the nouns used to denote the target group and the other terms associated with them. In some cases a lexical method may therefore be an appropriate strategy. Further research is necessary to determine if there are underlying syntactic structures associated with generalized abusive language.\nExplicit abuse Explicit abuse, whether directed or generalized, is often indicated by specific keywords. Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al., 2017). Negative polarity and sentiment of the text are also likely indicators of explicit abuse that can be leveraged by researchers (Gitari et al., 2015).\nImplicit abuse. Building a specific lexicon may prove impractical, as in the case of the appropriation of the term \u201cskype\u201d in some forums (Magu et al., 2017). Still, even partial lexicons may be used as seeds to inductively discover other keywords by use of a semi-supervised method proposed by King et al. (2017). Additionally, character n-grams have been shown to be apt for abusive language tasks due to their ability to capture variation of words associated with abuse (Nobata et al., 2016; Waseem, 2016a). Word embeddings are also promising ways to capture terms associated with abuse (Djuric et al., 2015; Badjatiya et al., 2017), although they may still be insufficient for cases like 4Chan\u2019s connotation of \u201cskype\u201d where a word has a dominant meaning and a more subversive one. Furthermore, as some of the above examples show, implicit abuse often takes on complex linguistic forms like sarcasm, metonymy, and humor. Without high quality labeled data to learn these representations, it may be difficult for researchers to come up with models of syntactic structure that can help to identify implicit abuse. To overcome these limitations researchers may find it prudent to incorporate features beyond just textual analysis, including the characteristics of the individuals involved (Dadvar et al., 2013) and other extra-textual features."}, {"heading": "4 Discussion", "text": "This typology has a number of implications for future work in the area.\nFirst, we want to encourage researchers working on these subtasks to learn from advances in other areas. Researchers working on purportedly distinct subtasks are often working on the same problems in parallel. For example, the field of hate speech detection can be strengthened by interactions with work on cyberbullying, and vice versa, since a large part of both subtasks consists of identifying targeted abuse.\nSecond, we aim to highlight the important distinctions within subtasks that have hitherto been ignored. For example, in much hate speech research, diverse types of abuse have been lumped together under a single label, forcing models to account for a large amount of within-class variation. We suggest that fine-grained distinctions along the axes allows for more focused systems that may be more effective at identifying particular types of abuse.\nThird, we call for closer consideration of how annotation guidelines are related to the phenomenon of interest. The type of annotation and even the choice of annotators should be motivated by the nature of the abuse. Further, we welcome discussion of annotation guidelines and the annotation process in published work. Many existing studies only tangentially mention these, sometimes never explaining how the data were annotated.\nFourth, we encourage researchers to consider which features are most appropriate for each subtask. Prior work has found a diverse array of features to be useful in understanding and identifying abuse, but we argue that different feature sets will be relevant to different subtasks. Future work should aim to build a more robust understanding of when to use which types of features.\nFifth, it is important to emphasize that not all abuse is equal, both in terms of its effects and its detection. We expect that social media and website operators will be more interested in identifying and dealing with explicit abuse, while activists, campaigners, and journalists may have more incentive to also identify implicit abuse. Targeted abuse such as cyberbullying may be more likely to be reported by victims and thus acted upon than generalized abuse. We also expect that implicit abuse will be more difficult to detect and model, although methodological advances may make such tasks more feasible."}, {"heading": "5 Conclusion", "text": "We have presented a typology that synthesizes the different subtasks in abusive language detection. Our aim is to bring together findings in these different areas and to clarify the key aspects of abusive language detection. There are important analytical distinctions that have been largely overlooked in prior work and through acknowledging these and their implications we hope to improve abuse detection systems and our understanding of abusive language.\nRather than attempting to resolve the \u201cdefinitional quagmire\u201d (Faris et al., 2016) involved in neatly bounding and defining each subtask we encourage researchers to think carefully about the phenomena they want to measure and the appropriate research design. We intend for our typology to be used both at the stage of data collection and annotation and the stage of feature creation\nand modeling. We hope that future work will be more transparent in discussing the annotation and modeling strategies used, and will closely examine the similarities and differences between these subtasks through empirical analyses."}], "references": [{"title": "Deep learning for hate speech detection in tweets", "author": ["Pinkesh Badjatiya", "Shashank Gupta", "Manish Gupta", "Vasudeva Varma."], "venue": "Proceedings of the 26th International Conference on World Wide Web Companion. pages 759\u2013760.", "citeRegEx": "Badjatiya et al\\.,? 2017", "shortCiteRegEx": "Badjatiya et al\\.", "year": 2017}, {"title": "Mythologies", "author": ["Roland Barthes."], "venue": "Seuil.", "citeRegEx": "Barthes.,? 1957", "shortCiteRegEx": "Barthes.", "year": 1957}, {"title": "Detecting offensive statements towards foreigners in social media", "author": ["Uwe Bretschneider", "Ralf Peters."], "venue": "Proceedings of the 50th Hawaii International Conference on System Sciences.", "citeRegEx": "Bretschneider and Peters.,? 2017", "shortCiteRegEx": "Bretschneider and Peters.", "year": 2017}, {"title": "Detecting online harassment in social networks", "author": ["Uwe Bretschneider", "Thomas Whner", "Ralf Peters."], "venue": "ICIS 2014 Proceedings: Conference Theme Track: Building a Better World through IS.", "citeRegEx": "Bretschneider et al\\.,? 2014", "shortCiteRegEx": "Bretschneider et al\\.", "year": 2014}, {"title": "Ethnicity without groups", "author": ["Rogers Brubaker."], "venue": "Harvard University Press.", "citeRegEx": "Brubaker.,? 2004", "shortCiteRegEx": "Brubaker.", "year": 2004}, {"title": "Cyber hate speech on twitter: An application of machine classification and statistical modeling for policy and decision making", "author": ["Pete Burnap", "Matthew L Williams."], "venue": "Policy & Internet 7(2):223\u2013242.", "citeRegEx": "Burnap and Williams.,? 2015", "shortCiteRegEx": "Burnap and Williams.", "year": 2015}, {"title": "Experts and machines against bullies: a hybrid approach to detect cyberbullies", "author": ["Maral Dadvar", "Dolf Trieschnigg", "Franciska de Jong."], "venue": "Conference on Artificial Intelligence. Springer International Publishing.", "citeRegEx": "Dadvar et al\\.,? 2014", "shortCiteRegEx": "Dadvar et al\\.", "year": 2014}, {"title": "Improving cyberbullying detection with user context", "author": ["Maral Dadvar", "Dolf Trieschnigg", "Roeland Ordelman", "Franciska de Jong."], "venue": "European Conference on Information Retrieval. Springer, pages 693\u2013696.", "citeRegEx": "Dadvar et al\\.,? 2013", "shortCiteRegEx": "Dadvar et al\\.", "year": 2013}, {"title": "Automated hate speech detection and the problem of offensive language", "author": ["Thomas Davidson", "Dana Warmsley", "Micheel Macy", "Ingmar Weber."], "venue": "Proceedings of the Eleventh International Conference on Web and Social Media. Montreal, Canada,", "citeRegEx": "Davidson et al\\.,? 2017", "shortCiteRegEx": "Davidson et al\\.", "year": 2017}, {"title": "Common sense reasoning for detection, prevention, and mitigation of cyberbullying", "author": ["Karthik Dinakar", "Birago Jones", "Catherine Havasi", "Henry Lieberman", "Rosalind Picard."], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS) 2(3):18.", "citeRegEx": "Dinakar et al\\.,? 2012", "shortCiteRegEx": "Dinakar et al\\.", "year": 2012}, {"title": "Modeling the detection of textual cyberbullying", "author": ["Karthik Dinakar", "Roi Reichart", "Henry Lieberman."], "venue": "The Social Mobile Web 11(02).", "citeRegEx": "Dinakar et al\\.,? 2011", "shortCiteRegEx": "Dinakar et al\\.", "year": 2011}, {"title": "Hate speech detection with comment embeddings", "author": ["Nemanja Djuric", "Jing Zhou", "Robin Morris", "Mihajlo Grbovic", "Vladan Radosavljevic", "Narayan Bhamidipati."], "venue": "Proceedings of the 24th International Conference on World Wide Web. ACM, pages", "citeRegEx": "Djuric et al\\.,? 2015", "shortCiteRegEx": "Djuric et al\\.", "year": 2015}, {"title": "Understanding harmful speech online", "author": ["Robert Faris", "Amar Ashar", "Urs Gasser", "Daisy Joo."], "venue": "Berkman Klein Center Research Publication 21.", "citeRegEx": "Faris et al\\.,? 2016", "shortCiteRegEx": "Faris et al\\.", "year": 2016}, {"title": "A lexicon-based approach for hate speech detection", "author": ["Njagi Dennis Gitari", "Zhang Zuping", "Hanyurwimfura Damien", "Jun Long."], "venue": "International Journal of Multimedia and Ubiquitous Engineering 10(4):215\u2013230.", "citeRegEx": "Gitari et al\\.,? 2015", "shortCiteRegEx": "Gitari et al\\.", "year": 2015}, {"title": "A longitudinal measurement study of 4chan\u2019s politically incorrect forum and its", "author": ["Gabriel Emile Hine", "Jeremiah Onaolapo", "Emiliano De Cristofaro", "Nicolas Kourtellis", "Ilias Leontiadis", "Riginos Samaras", "Gianluca Stringhini", "Jeremy Blackburn"], "venue": null, "citeRegEx": "Hine et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hine et al\\.", "year": 2017}, {"title": "Extracting relevant knowledge for the detection of sarcasm and nastiness in the social web", "author": ["Raquel Justo", "Thomas Corcoran", "Stephanie M. Lukin", "Marilyn Walker", "M. Ins Torres."], "venue": "Knowledge-Based Systems 69:124 \u2013 133.", "citeRegEx": "Justo et al\\.,? 2014", "shortCiteRegEx": "Justo et al\\.", "year": 2014}, {"title": "Computer-assisted keyword and document set discovery from unstructured text", "author": ["Gary King", "Patrick Lam", "Margaret E Roberts."], "venue": "American Journal of Political Science .", "citeRegEx": "King et al\\.,? 2017", "shortCiteRegEx": "King et al\\.", "year": 2017}, {"title": "Detecting cyberbullying: Query terms and techniques", "author": ["April Kontostathis", "Kelly Reynolds", "Andy Garron", "Lynne Edwards."], "venue": "Proceedings of the 5th Annual ACM Web Science Conference. ACM, New York, NY, USA, WebSci \u201913, pages 195\u2013204.", "citeRegEx": "Kontostathis et al\\.,? 2013", "shortCiteRegEx": "Kontostathis et al\\.", "year": 2013}, {"title": "Locate the hate: Detecting tweets against blacks", "author": ["Irene Kwok", "Yuzhou Wang."], "venue": "Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence. AAAI Press, AAAI\u201913, pages 1621\u2013 1622.", "citeRegEx": "Kwok and Wang.,? 2013", "shortCiteRegEx": "Kwok and Wang.", "year": 2013}, {"title": "Detecting the hate code on social media", "author": ["Rijul Magu", "Kshitij Joshi", "Jiebo Luo."], "venue": "Proceedings of the Eleventh International Conference on Web and Social Media. Montreal, Canada, pages 608\u2013612.", "citeRegEx": "Magu et al\\.,? 2017", "shortCiteRegEx": "Magu et al\\.", "year": 2017}, {"title": "Abusive language detection in online user content", "author": ["Chikashi Nobata", "Joel Tetreault", "Achint Thomas", "Yashar Mehdad", "Yi Chang."], "venue": "Proceedings of the 25th International Conference on World Wide Web. pages 145\u2013153.", "citeRegEx": "Nobata et al\\.,? 2016", "shortCiteRegEx": "Nobata et al\\.", "year": 2016}, {"title": "Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "author": ["Bj\u00f6rn Ross", "Michael Rist", "Guillermo Carbonell", "Benjamin Cabrera", "Nils Kurowsky", "Michael Wojatzki."], "venue": "Proceedings of NLP4CMC III:", "citeRegEx": "Ross et al\\.,? 2016", "shortCiteRegEx": "Ross et al\\.", "year": 2016}, {"title": "A survey on hate speech detection using natural language processing", "author": ["Anna Schmidt", "Michael Wiegand."], "venue": "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media. Association for Computational Linguis-", "citeRegEx": "Schmidt and Wiegand.,? 2017", "shortCiteRegEx": "Schmidt and Wiegand.", "year": 2017}, {"title": "Analyzing the targets of hate in online social media", "author": ["Leandro Ara\u00fajo Silva", "Mainack Mondal", "Denzil Correa", "Fabr\u0131\u0301cio Benevenuto", "Ingmar Weber"], "venue": "In Proceedings of the Tenth International Conference on Web and Social Media. Cologne, Germany,", "citeRegEx": "Silva et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2016}, {"title": "Racial microaggressions in everyday life: implications for clinical practice", "author": ["Derald Wing Sue", "Christina M Capodilupo", "Gina C Torino", "Jennifer M Bucceri", "Aisha Holder", "Kevin L Nadal", "Marta Esquilin."], "venue": "American Psychologist 62(4):271\u2013286.", "citeRegEx": "Sue et al\\.,? 2007", "shortCiteRegEx": "Sue et al\\.", "year": 2007}, {"title": "The automated detection of racist discourse in dutch social media", "author": ["St\u00e9phan Tulkens", "Lisa Hilte", "Elise Lodewyckx", "Ben Verhoeven", "Walter Daelemans."], "venue": "CLIN Journal 6:3\u201320.", "citeRegEx": "Tulkens et al\\.,? 2016", "shortCiteRegEx": "Tulkens et al\\.", "year": 2016}, {"title": "Detection and fine-grained classification of cyberbullying events", "author": ["Cynthia Van Hee", "Els Lefever", "Ben Verhoeven", "Julie Mennes", "Bart Desmet", "Guy De Pauw", "Walter Daelemans", "Veronique Hoste."], "venue": "Proceedings of the International Conference Re-", "citeRegEx": "Hee et al\\.,? 2015a", "shortCiteRegEx": "Hee et al\\.", "year": 2015}, {"title": "Guidelines for the fine-grained analysis of cyberbullying", "author": ["Cynthia Van Hee", "Ben Verhoeven", "Els Lefever", "Guy De Pauw", "V\u00e9ronique Hoste", "Walter Daelemans."], "venue": "Technical report, LT3, Ghent University, Belgium.", "citeRegEx": "Hee et al\\.,? 2015b", "shortCiteRegEx": "Hee et al\\.", "year": 2015}, {"title": "Detecting hate speech on the world wide web", "author": ["William Warner", "Julia Hirschberg."], "venue": "Proceedings of the Second Workshop on Language in Social Media. Association for Computational Linguistics, LSM \u201912, pages 19\u201326.", "citeRegEx": "Warner and Hirschberg.,? 2012", "shortCiteRegEx": "Warner and Hirschberg.", "year": 2012}, {"title": "Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter", "author": ["ZeerakWaseem."], "venue": "Proceedings of the First Workshop on NLP andComputational Social Science. Association for Computational Linguistics, Austin, Texas, pages", "citeRegEx": "ZeerakWaseem.,? 2016a", "shortCiteRegEx": "ZeerakWaseem.", "year": 2016}, {"title": "Automatic Hate Speech Detection", "author": ["Zeerak Waseem."], "venue": "Master\u2019s thesis, University of Copenhagen.", "citeRegEx": "Waseem.,? 2016b", "shortCiteRegEx": "Waseem.", "year": 2016}, {"title": "Hateful symbols or hateful people? predictive features for hate speech detection on twitter", "author": ["Zeerak Waseem", "Dirk Hovy."], "venue": "Proceedings of the NAACL Student ResearchWorkshop. Association for Computational Linguistics, San Diego, California,", "citeRegEx": "Waseem and Hovy.,? 2016", "shortCiteRegEx": "Waseem and Hovy.", "year": 2016}, {"title": "Ethnic boundary making: Institutions, power, networks", "author": ["Andreas Wimmer."], "venue": "Oxford University Press.", "citeRegEx": "Wimmer.,? 2013", "shortCiteRegEx": "Wimmer.", "year": 2013}, {"title": "Ex machina: Personal attacks seen at scale", "author": ["Ellery Wulczyn", "Nithum Thain", "Lucas Dixon."], "venue": "Proceedings of the 26th International Conference on World Wide Web.", "citeRegEx": "Wulczyn et al\\.,? 2017", "shortCiteRegEx": "Wulczyn et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 22, "context": "There has been a surge in interest in the detection of abusive language, hate speech, cyberbullying, and trolling in the past several years (Schmidt and Wiegand, 2017).", "startOffset": 140, "endOffset": 167}, {"referenceID": 20, "context": "Similarities between these subtasks have led scholars to group them together under the umbrella terms of \u201cabusive language\u201d, \u201charmful speech\u201d, and \u201chate speech\u201d (Nobata et al., 2016; Faris et al., 2016; Schmidt and Wiegand, 2017) but little work has been done to examine the relationship between them.", "startOffset": 161, "endOffset": 229}, {"referenceID": 12, "context": "Similarities between these subtasks have led scholars to group them together under the umbrella terms of \u201cabusive language\u201d, \u201charmful speech\u201d, and \u201chate speech\u201d (Nobata et al., 2016; Faris et al., 2016; Schmidt and Wiegand, 2017) but little work has been done to examine the relationship between them.", "startOffset": 161, "endOffset": 229}, {"referenceID": 22, "context": "Similarities between these subtasks have led scholars to group them together under the umbrella terms of \u201cabusive language\u201d, \u201charmful speech\u201d, and \u201chate speech\u201d (Nobata et al., 2016; Faris et al., 2016; Schmidt and Wiegand, 2017) but little work has been done to examine the relationship between them.", "startOffset": 161, "endOffset": 229}, {"referenceID": 24, "context": "Van Hee et al. (2015b) identifies discriminative remarks (racist, sexist) as a subset of \u201cinsults\u201d, whereas Nobata et al.", "startOffset": 4, "endOffset": 23}, {"referenceID": 19, "context": "(2015b) identifies discriminative remarks (racist, sexist) as a subset of \u201cinsults\u201d, whereas Nobata et al. (2016) classifies similar remarks as \u201chate speech\u201d or \u201cderogatory language\u201d.", "startOffset": 93, "endOffset": 114}, {"referenceID": 19, "context": "(2015b) identifies discriminative remarks (racist, sexist) as a subset of \u201cinsults\u201d, whereas Nobata et al. (2016) classifies similar remarks as \u201chate speech\u201d or \u201cderogatory language\u201d. Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al.", "startOffset": 93, "endOffset": 207}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language.", "startOffset": 144, "endOffset": 167}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language.", "startOffset": 144, "endOffset": 248}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines - some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al.", "startOffset": 144, "endOffset": 511}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines - some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al.", "startOffset": 144, "endOffset": 580}, {"referenceID": 8, "context": "Waseem and Hovy (2016) only consider \u201chate speech\u201d without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language. Wulczyn et al. (2017) annotates for personal attacks, which likely encompasses identifying cyberbullying, hate speech, and offensive language. The lack of consensus has resulted in contradictory annotation guidelines - some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al. (2017). To help to bring together these literatures and to avoid these contradictions, we propose a typology that synthesizes these different subtasks.", "startOffset": 144, "endOffset": 607}, {"referenceID": 8, "context": "the border little n*gga\u201d (Davidson et al., 2017),", "startOffset": 25, "endOffset": 48}, {"referenceID": 17, "context": "(Kontostathis et al., 2013).", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "visit?\u201d (Dinakar et al., 2012),", "startOffset": 8, "endOffset": 30}, {"referenceID": 14, "context": "Google balls? #Dumbgoogles\u201d (Hine et al., 2017),", "startOffset": 28, "endOffset": 47}, {"referenceID": 10, "context": "\u201cyou\u2019re intelligence is so breathtaking!!!!!!\u201d (Dinakar et al., 2011)", "startOffset": 47, "endOffset": 69}, {"referenceID": 20, "context": "Kill all the g*ys there!\u201d (Nobata et al., 2016),", "startOffset": 26, "endOffset": 47}, {"referenceID": 18, "context": "another n*gger off the streets!!\u201d (Kwok and Wang, 2013).", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": "\u201d (Burnap and Williams, 2015),", "startOffset": 2, "endOffset": 29}, {"referenceID": 10, "context": "(Dinakar et al., 2011),", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "\u201cGas the skypes\u201d (Magu et al., 2017)", "startOffset": 17, "endOffset": 36}, {"referenceID": 5, "context": "identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al.", "startOffset": 75, "endOffset": 148}, {"referenceID": 31, "context": "identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al.", "startOffset": 75, "endOffset": 148}, {"referenceID": 8, "context": "identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al.", "startOffset": 75, "endOffset": 148}, {"referenceID": 5, "context": "identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017), although Nobata et al. (2016) come closest", "startOffset": 76, "endOffset": 180}, {"referenceID": 1, "context": "This is roughly analogous to the distinction in linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by Barthes (1957). Explicit abusive lan-", "startOffset": 208, "endOffset": 223}, {"referenceID": 28, "context": "Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012; Davidson et al., 2017), with abusive terms being used in a colloquial manner or by people who are victims of abuse.", "startOffset": 79, "endOffset": 131}, {"referenceID": 8, "context": "Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012; Davidson et al., 2017), with abusive terms being used in a colloquial manner or by people who are victims of abuse.", "startOffset": 79, "endOffset": 131}, {"referenceID": 10, "context": "Here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches (Dinakar et al., 2011; Dadvar et al., 2013; Justo et al., 2014).", "startOffset": 229, "endOffset": 292}, {"referenceID": 7, "context": "Here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches (Dinakar et al., 2011; Dadvar et al., 2013; Justo et al., 2014).", "startOffset": 229, "endOffset": 292}, {"referenceID": 15, "context": "Here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches (Dinakar et al., 2011; Dadvar et al., 2013; Justo et al., 2014).", "startOffset": 229, "endOffset": 292}, {"referenceID": 24, "context": "Social scientists and activists have recently been paying more attention to implicit, and even unconscious, instances of abuse that have been termed \u201cmicroaggressions\u201d (Sue et al., 2007).", "startOffset": 168, "endOffset": 186}, {"referenceID": 7, "context": "In the task of annotating documents that contain bullying, it appears that there is a common understanding of what cyberbullying entails: an intentionally harmful electronic attack by an individual or group against a victim, usually repetitive in nature (Dadvar et al., 2013).", "startOffset": 254, "endOffset": 275}, {"referenceID": 6, "context": "This consensus allows for a relatively consistent set of annotation guidelines across studies, most of which simply ask annotators to determine if a post contains bullying or harassment (Dadvar et al., 2014; Kontostathis et al., 2013; Bretschneider et al., 2014).", "startOffset": 186, "endOffset": 262}, {"referenceID": 17, "context": "This consensus allows for a relatively consistent set of annotation guidelines across studies, most of which simply ask annotators to determine if a post contains bullying or harassment (Dadvar et al., 2014; Kontostathis et al., 2013; Bretschneider et al., 2014).", "startOffset": 186, "endOffset": 262}, {"referenceID": 3, "context": "This consensus allows for a relatively consistent set of annotation guidelines across studies, most of which simply ask annotators to determine if a post contains bullying or harassment (Dadvar et al., 2014; Kontostathis et al., 2013; Bretschneider et al., 2014).", "startOffset": 186, "endOffset": 262}, {"referenceID": 7, "context": "High interannotator agreement on cyberbullying tasks (93%) (Dadvar et al., 2013) further indicates a general consensus around the features of cyberbullying (Van Hee et al.", "startOffset": 59, "endOffset": 80}, {"referenceID": 6, "context": "After bullying has been identified annotators are typically asked more detailed questions about the extremity of the bullying, the identification of phrases that indicate bullying, and the roles of users as bully/victim (Dadvar et al., 2014; Van Hee et al., 2015b; Kontostathis et al., 2013).", "startOffset": 220, "endOffset": 291}, {"referenceID": 17, "context": "After bullying has been identified annotators are typically asked more detailed questions about the extremity of the bullying, the identification of phrases that indicate bullying, and the roles of users as bully/victim (Dadvar et al., 2014; Van Hee et al., 2015b; Kontostathis et al., 2013).", "startOffset": 220, "endOffset": 291}, {"referenceID": 21, "context": "80) (Ross et al., 2016; Waseem, 2016a; Tulkens et al., 2016; Bretschneider and Peters, 2017) are obtained.", "startOffset": 4, "endOffset": 92}, {"referenceID": 25, "context": "80) (Ross et al., 2016; Waseem, 2016a; Tulkens et al., 2016; Bretschneider and Peters, 2017) are obtained.", "startOffset": 4, "endOffset": 92}, {"referenceID": 2, "context": "80) (Ross et al., 2016; Waseem, 2016a; Tulkens et al., 2016; Bretschneider and Peters, 2017) are obtained.", "startOffset": 4, "endOffset": 92}, {"referenceID": 30, "context": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al.", "startOffset": 161, "endOffset": 176}, {"referenceID": 7, "context": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011).", "startOffset": 247, "endOffset": 310}, {"referenceID": 15, "context": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011).", "startOffset": 247, "endOffset": 310}, {"referenceID": 10, "context": "Annotation (via crowd-sourcing and other methods) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon (Waseem, 2016b), but is considerably more difficult when implicit abuse is considered (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011).", "startOffset": 247, "endOffset": 310}, {"referenceID": 8, "context": "Furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions (Davidson et al., 2017), others find that they do not improve the reliability of non-expert classifications (Ross et al.", "startOffset": 108, "endOffset": 131}, {"referenceID": 21, "context": ", 2017), others find that they do not improve the reliability of non-expert classifications (Ross et al., 2016).", "startOffset": 92, "endOffset": 111}, {"referenceID": 8, "context": "Davidson et al. (2017), for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism.", "startOffset": 0, "endOffset": 23}, {"referenceID": 28, "context": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015; Davidson et al., 2017).", "startOffset": 98, "endOffset": 171}, {"referenceID": 13, "context": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015; Davidson et al., 2017).", "startOffset": 98, "endOffset": 171}, {"referenceID": 8, "context": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015; Davidson et al., 2017).", "startOffset": 98, "endOffset": 171}, {"referenceID": 5, "context": "Typed dependencies offer a more sophisticated way to capture the relationship between terms (Burnap and Williams, 2015).", "startOffset": 92, "endOffset": 119}, {"referenceID": 2, "context": "Bretschneider and Peters (2017) use a multi-tiered system, first identifying offensive statements, then their severity, and finally the target.", "startOffset": 0, "endOffset": 32}, {"referenceID": 23, "context": "Generalized abuse online tends to target people belonging to a small set of categories, primarily racial, religious, and sexual minorities (Silva et al., 2016).", "startOffset": 139, "endOffset": 159}, {"referenceID": 28, "context": "Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al.", "startOffset": 85, "endOffset": 135}, {"referenceID": 20, "context": "Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al.", "startOffset": 85, "endOffset": 135}, {"referenceID": 18, "context": ", 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al., 2017).", "startOffset": 156, "endOffset": 200}, {"referenceID": 8, "context": ", 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al., 2017).", "startOffset": 156, "endOffset": 200}, {"referenceID": 13, "context": "Negative polarity and sentiment of the text are also likely indicators of explicit abuse that can be leveraged by researchers (Gitari et al., 2015).", "startOffset": 126, "endOffset": 147}, {"referenceID": 19, "context": "Building a specific lexicon may prove impractical, as in the case of the appropriation of the term \u201cskype\u201d in some forums (Magu et al., 2017).", "startOffset": 122, "endOffset": 141}, {"referenceID": 20, "context": "Additionally, character n-grams have been shown to be apt for abusive language tasks due to their ability to capture variation of words associated with abuse (Nobata et al., 2016; Waseem, 2016a).", "startOffset": 158, "endOffset": 194}, {"referenceID": 11, "context": "Word embeddings are also promising ways to capture terms associated with abuse (Djuric et al., 2015; Badjatiya et al., 2017), although they may still be insufficient for cases like 4Chan\u2019s connotation of \u201cskype\u201d where a word has a dominant meaning and a more subversive one.", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": "Word embeddings are also promising ways to capture terms associated with abuse (Djuric et al., 2015; Badjatiya et al., 2017), although they may still be insufficient for cases like 4Chan\u2019s connotation of \u201cskype\u201d where a word has a dominant meaning and a more subversive one.", "startOffset": 79, "endOffset": 124}, {"referenceID": 7, "context": "To overcome these limitations researchers may find it prudent to incorporate features beyond just textual analysis, including the characteristics of the individuals involved (Dadvar et al., 2013) and other extra-textual features.", "startOffset": 174, "endOffset": 195}, {"referenceID": 5, "context": ", 2016), although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al., 2017). Negative polarity and sentiment of the text are also likely indicators of explicit abuse that can be leveraged by researchers (Gitari et al., 2015). Implicit abuse. Building a specific lexicon may prove impractical, as in the case of the appropriation of the term \u201cskype\u201d in some forums (Magu et al., 2017). Still, even partial lexicons may be used as seeds to inductively discover other keywords by use of a semi-supervised method proposed by King et al. (2017). Additionally, character n-grams have been shown to be apt for abusive language tasks due to their ability to capture variation of words associated with abuse (Nobata et al.", "startOffset": 178, "endOffset": 665}, {"referenceID": 12, "context": "Rather than attempting to resolve the \u201cdefinitional quagmire\u201d (Faris et al., 2016) involved in neatly bounding and defining each subtask we encourage researchers to think carefully about the phenomena they want to measure and the appro-", "startOffset": 62, "endOffset": 82}], "year": 2017, "abstractText": "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and we discuss its implications for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.", "creator": "LaTeX with hyperref package"}}}