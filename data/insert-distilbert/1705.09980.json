{"id": "1705.09980", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations", "abstract": "we evaluate the character - level human translation method for neural semantic parsing on relatively a large corpus of sentences annotated with abstract - meaning representations ( amrs ). using a seq2seq model, and some pretty trivial preprocessing and postprocessing of amrs, we obtain a baseline accuracy of 53. 1 ( f - score on amr - triples ). we examine four different approaches to improve this baseline result : ( i ) reordering amr branches to match the word order of the input sentence increases performance to 58. 3 ; ( ii ii ) adding part - of - speech tags ( automatically produced ) to the input shows gradual improvement as well ( 57. 2 ) ; ( iii ) so does the introduction of super characters ( conflating frequent sequences of characters to a single character ), reaching 57. 4 ; ( iv ) adding advanced silver - standard training sentence data obtained by an off - the - shelf parser yields the biggest improvement, resulting in an f - score of 64. 0. combining all four techniques leads to an f - score of 69. 0, which is supposedly state - of - the - art in amr parsing. this algorithm is remarkable because of the relatively simplicity of the approach : the only explicit linguistic knowledge that we use are part - of - speech tags.", "histories": [["v1", "Sun, 28 May 2017 19:41:09 GMT  (59kb,D)", "http://arxiv.org/abs/1705.09980v1", "In review for CLIN Journal"], ["v2", "Mon, 9 Oct 2017 08:30:33 GMT  (30kb)", "http://arxiv.org/abs/1705.09980v2", "Camera ready for CLIN 2017 journal"]], "COMMENTS": "In review for CLIN Journal", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rik van noord", "johan bos"], "accepted": false, "id": "1705.09980"}, "pdf": {"name": "1705.09980.pdf", "metadata": {"source": "CRF", "title": "Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations", "authors": ["Rik van Noord", "Johan Bos"], "emails": ["r.i.k.van.noord@rug.nl", "johan.bos@rug.nl"], "sections": [{"heading": "1. Introduction", "text": "Various approaches to open-domain semantic parsing have been proposed in the last years. What we now could refer to as \u201dtraditional\u201d approaches are semantic parsers that use supervised learning to create a syntactic analysis on which the meaning representations are constructed, usually in a compositional way. Research in this area comprises Bos et al. (2004), Bos (2015), Butler (2010), and Artzi et al. (2015). But recently there have been interesting attempts to view semantic parsing as a translation task, mapping English expressions to some logical form, under supervision of some deep learning method. Dong and Lapata (2016) used sequence-to-sequence (seq2seq) and sequence-totree (seq2tree) neural translation models to produce logical forms from sentences for four different datasets (but not AMRs). Barzdins and Gosko (2016) used a similar method to produce Abstract Meaning Representations (AMRs) in the context of the shared task on semantic parsing at SemEval2016 (May 2016). They get decent results, but the performance of their neural parser is still far below the state-of-the-art, and only got an overall improvement on AMR parsing by including it in an ensemble.\nWhat all these attempts have in common, and why they are fascinating, is that they completely avoid complex models of the syntactic and semantic parsing process. Relatively little is known about the performance and fine-tuning of such parsers, and whether they can reach performance of traditional semantic parsers, or whether they could contribute to performance in an ensemble setting. The aim of this article is to find out how far we can push neural semantic parsing: can we reach accuracy scores comparable with traditional approaches to semantic parsing? We will do this by concentrating on a particular task, namely AMR parsing. More specifically, our objectives are (1) try to reproduce the results of Barzdins and Gosko (2016), who used character-level models for neural semantic parsing; (2) improve on their results by employing several novel techniques; and (3) investigate whether injecting linguistic knowledge can improve neural semantic parsing.\nar X\niv :1\n70 5.\n09 98\n0v 1\n[ cs\n.C L\n] 2\n8 M\nay 2\n01 7\nWe make three main contributions. First, we introduce three novel techniques to improve neural AMR parsing. Second, we show that linguistic knowledge can contribute to neural semantic parsing. Third, we show that adding silver standard to the training data makes a considerable (positive) difference in terms of performance."}, {"heading": "2. Method and Data", "text": "We first give a bit of background on AMRs. Then we outline the basic ideas of the character-based translation model with English sentences as input and AMRs as output. We then establish a baseline system with the aim to improve it in the next section."}, {"heading": "2.1 Abstract Meaning Representations", "text": "In our experiments utilizing neural semantic parsing we will focus on parsing Abstract Meaning Representations (AMRs). AMRs were introduced by Banarescu et al. (2013) and are acyclic, directed graphs that represent the meaning of a sentence. There are, in fact, three ways to display an AMR: as a graph, as a set of triples, or as a tree. An example of an AMR is shown in Figure 1, here displayed as a tree, the format that is used in the annotated corpora.\n(a / affect-01\nAn AMR consists of concepts that are linked to variable names with a slash. In the example above we have that a is an instance of the concept affect-01, and p is an instance of the concept person (note that the names of the variables are not important). Concepts can be related to each other by using two-place predicates, which are indicated by a colon. So, the first :ARG0 is an ordered relation between a and w. Inverse relations are denoted by the suffix -of. Note that, if one concept relates to more than one other concept (for instance, in the example above, the node a is related to w via :ARG0, and to p via :ARG1), the order of these relations within the AMR is not important.\nAMRs also allow for a re-occurrence of variables: the concept person with variable p stands in a relation with affect-01 as well as with hunger-01. The brackets are important, because they signal which relations belong to which concepts (the spacing used in Figure 1 is optional and is only used to increase readability). Some of the concepts have a number as suffix that indicate a specific word sense. AMRs also include proper name reference resolution by including a link to a wikipedia entry (wikification).\nFor evaluation purposes, AMRs are converted into triples. For the AMR in Figure 1 the triples would be inst(a,affect-01), ARG0(a,w), ARG1(a,p), inst(p,person), ARG0(s,p), and so on. The accuracy of an AMR parser is computed by precision and recall on matching triples between gold standard AMRs and system-produced AMRs, using the SMATCH system (Cai and Knight 2013).\nFor the evaluation of our experiments we use the sentences annotated with AMRs from LDC release LDC2016E25, consisting of 36,521 training AMRs, 1,368 development AMRs and 1,371 test AMRs. This release also includes the PropBank frameset and comes with pre-aligned AMRs and sentences. In all results shown in this article, the models are trained on the training data. As\ndevelopment and test data we use the designated dev and test set from LDC2016E25, which are the exact same sets that are used in LDC2015E89."}, {"heading": "2.2 The Basic Translation Model", "text": "To create our sequence-to-sequence translation model, we use the OpenNMT system (Klein et al. 2017). In contrast to Peng et al. (2017) and Konstas et al. (2017), who use word-level input, we use character-level input, because we believe it generalizes better and it is therefore one of the promising trends in machine translation (Chung et al. 2016). We train a model with bidirectional encoding and general attention (Luong et al. 2015). Since training a full model takes two to three days on a GPU, we perform a heuristic parameter search instead of an exhaustive one. Parameter settings are optimized based on the development set and the final values are shown in Table 1. All our described models in this paper are trained with these settings. Training is stopped 3 epochs after there is no improvement in validation perplexity on the development set anymore. The best performing model on the development set is then used to decode the test set. HTML-tags are removed from the input sentences, but URLs are kept in.\nFollowing Barzdins and Gosko (2016), we do not want our model to learn the arbitrary characters that are used to represent variables. Therefore we just remove all variables and simply duplicate co-referring nodes from the input. An example of such a preprocessed AMR is shown in Figure 2. Any wikification relation present in AMRs in the training set are also removed. Newlines present in an AMR are replaced by spaces, and multiple spaces are squeezed into single ones (so the input AMR is represented on a single line).\n(m / material\n(material"}, {"heading": "2.3 Postprocessing and Restoring Information", "text": "The output of the seq2seq model is, of course, an AMR without variables, without wiki-links, and without co-occurrent variables. Furthermore, because of the character-based seq2seq model, it could\nwell be that there are brackets in the output that do not match, or that some nodes representing concepts are incomplete. This, obviously, needs to be fixed.\nFirst, the variables in the AMRs are restored. We also try to fix invalidly produced AMRs by applying a few heuristics, such as inserting parentheses and quotes, or simply removing unfinished nodes. This is done by using a slightly modified version of the restoring script from Barzdins and Gosko (2016). Then, we apply four methods to increase the quality of the AMRs. They are described below."}, {"heading": "2.3.1 Pruning", "text": "A problem with many deep learning approaches is that the decoder does not keep track of what it has already produced. As a consequence, we sometimes end up with duplicated, redundant material in our generation AMRs. This hurts precision. Therefore, we remove duplicated branches if they are related to the same relation-concept pair. Duplicate material that occurs in different AMR branches is removed after the second time it is encountered. This process is a trade-off: usually duplicates are correctly recognized as redundant and can be removed, but sometimes duplicates we erroneously remove should remain. Two example AMRs whose branches are pruned are shown in Figure 3.\n(material\n(material"}, {"heading": "2.3.2 Sense Validation", "text": "We have no separate word-sense disambiguation system. This means that our general seq2seq model needs to learn the appropriate sense per verb. However, as a consequence, the model sometimes outputs nodes that are impossible, either by producing a sense that is not in the PropBank frameset that is used by AMRs, or by producing a combination of verb sense and arguments that is not allowed by that frameset. For example, the verb break-out can have sense 06, 07 and 10. If we encounter a different sense for break-out, we know that the model produced an incorrect node and try to replace it by a possibly correct one. All these senses can have different sets of arguments they are allowed to occur with, (e.g. break-out-06 with :ARG0 and :ARG1, but break-out-07 only with :ARG1), making it a complicated matter. Our method of changing senses and arguments is presented in Figure 4."}, {"heading": "2.3.3 Wikification", "text": "Since we removed wikification relations in preprocessing, our model will never output such a link. We restore wiki links in the output AMR by using an off-the-shelf system (Daiber et al. 2013), following the method presented by Bjerva et al. (2016). They look at the :name relations in an\nAMR and try to find this name on Wikipedia. If it has a page, the corresponding link gets added; otherwise the AMR remains unaltered."}, {"heading": "2.3.4 Restoring co-referring nodes", "text": "Our system also tries to restore co-referring nodes. If we output a duplicate node (a node already output for this AMR), it replaces the node by the variable name of the node encountered first. This can only happen once per unique node, since the third instance of such a node is already removed in the pruning phase."}, {"heading": "2.4 Baseline Results", "text": "Our first objective was to reproduce the results obtained by Barzdins and Gosko (2016). We did so, arriving at an F-score of 53.0 (see Table 2). Compared to the F-score of 43.0 by Barzdins and Gosko (2016), our score is significantly higher. This is probably due to the higher amount of training data and the use of different seq2seq software.\nAs is shown in Table 2, concept pruning, restoring co-reference variables, and wikification all increase the F-score by about a percentage point each. This small gain of performance is what one\ncould expect as each single operation has only a small impact on the overall contents of an AMR. Perhaps surprisingly, sense validation did not help to get a better performance."}, {"heading": "3. Improving the Basic Translation Model", "text": "In the previous section we outlined our basic method of producing AMRs using a seq2seq model based on characters. In this section, we look at four different techniques to move beyond the F-score that we obtain with our basic method, that we will consider in this section as baseline."}, {"heading": "3.1 AMR Reshuffling", "text": "Although AMRs are unordered by definition, in our textual representation of the AMRs there is an order of the branches. However, these branches do not necessarily follow the word order in the corresponding English sentence. It has been shown that for (statistical) machine translation reordering improves translation quality (Collins et al. 2005). We use the provided alignments to permute the AMR in such a way that it best matches the word order. We do this both on sub-tree level and on individual node level. An example of an AMR with a branch order best matching the input sentence is shown in Figure 1.\n(material\n(material\nWe are also able to use this approach to augment the training data, since each reordering of the AMR provides us with a new AMR-sentence pair. Due to the exponential increase, large AMRs often have thousands of possible orders. We performed a number of experiments to find out how we could best exploit this surplus of data. Ultimately, we found that it is most beneficial to \u201cdouble\u201d the training data by adding the best matching AMR to the existing data set.1"}, {"heading": "3.2 Introducing Super Characters", "text": "We are not necessarily restricted to only using characters as input. For example, we can view the AMR relations (e.g. :ARG0, :name) as atomic instead of a set of characters. This way, we create a hybrid model that is a combination of word and character level input. An example of the AMR level input using super characters is shown in Figure 6.\nWe also extend the set of super characters outside of just the relations, by automatically finding AMR concepts that rarely align with a word in the English sentence. This set includes chunks such as government-organization, date-entity and temporal-quantity. We conduct two experiments, one with only the relations as super characters and one with the extended super character set.2\n1. Instead of ordering the AMR nodes reflected by the word order of sentence, we also tried two different experiments based on consistency. The first experiment simply ordered the nodes alphabetically, without any other influence. This decreased the result of our baseline model by 2.0. Our second experiment was focused on fixing irregularities: if two nodes occur in a different order than they usually do (based on the full training set), we simply switch them around. This method did not change the order as considerably as the alphabetical ordering, but the result of the baseline model still decreased by 1.0. Hence we discarded both reordering techniques. 2. We also tried various ways to explicitly encode the tree structure by using super characters. In our basic model, the parentheses \u2019(\u2019 and \u2019)\u2019 are simply characters. This means that the model cannot differentiate between a"}, {"heading": "3.3 Adding Part-of-Speech Information", "text": "We might still be able to benefit from syntactic information, even though we use a character-level neural semantic parser. To show this, we parse the sentences with the POS-tagger of the C&C tools (Clark et al. 2003), employing the Penn POS tagset. Each tag is represented as a single character and placed after the last character representation of the word that matches the tag (see Figure 6). Put differently, we create a new super character for each unique tag and add this to the input sentence. On the one hand, this will increase the size of the input. On the other hand, just a single character will add a lot of general, potentially useful, information."}, {"heading": "3.4 Adding Silver Standard Data", "text": "A problem with neural parsing approaches is data sparsity, since a lot of manual effort is required to create gold standard data. Peng et al. (2017) tried to overcome this by extensive generalization of the training data, but did not get near state-of-the-art results. Konstas et al. (2017) used the GigaWord corpus to self-train their system. They use their own pre-trained parser to parse the previously unseen sentences and add those to the training data in a series of iterations. Ultimately, their system is trained on 20 million additional data AMR-sentence pairs and obtains an F-score of 62.1. Without this additional data, they only score 55.5, which is better than Peng et al. (2017), but not close to state-of-the-art performance.\nOur method differs from Konstas et al. (2017) in three ways: (i) we directly add new data to the training data instead of using it to pre-train the model; (ii) we use two off-the-shelf parsers to create the training data instead of self-training; (iii) we employ a method to exclude lower-quality AMRs instead of using all available data. We therefore refer to this data as \u201dsilver standard\u201d data, by which we mean something in between unchecked automatically produced data and gold standard data.\nInstead of self-training our parser, we use the off-the-shelf AMR parsers CAMR (Wang et al. 2015) and JAMR (Flanigan et al. 2014) to create silver standard data for our system. Both systems are trained on the LDC2015E86 AMR corpus, which contains 16,833 training instances. We parse 1,303,419 sentences from the Groningen Meaning Bank (Basile et al. 2012), which mainly consists of newswire text. AMRs that are either invalid or include null-tag or null-edge (this is what the CAMR parser outputs when it is not able to find a suitable candidate parse) are removed.\nparenthesis that opens the full AMR and a parenthesis that opens, say, the fifth subtree of the AMR. One would expect it would help the model if it has this information explicitly encoded in the input. In an experiment we replaced each parenthesis in the structure by a super character that also provides the subtree information (e.g., an opening parenthesis on the fifth level becomes *5*(, while a closing bracket on the third level becomes *3*). However this resulted in an F-score lower than the baseline and we discarded the technique.\nWe do not simply add the other AMRs to our data set. To ensure that the AMRs are at least of decent quality, we compare the produced AMRs with each other using Smatch (Cai and Knight 2013). If their pairwise Smatch score exceeds 55.0, we add the AMR produced by CAMR to our data set. We do not add both, since CAMR produces higher quality AMRs in general (64.0 vs 55.0 on the test set). This method is designed to filter out AMRs that would only hurt the training process, but should still include a large variety of AMRs. This method left us with 530,450 instances, of which we randomly selected 20k, 50k, 75k and 100k instances for our experiments."}, {"heading": "4. Results and Discussion", "text": "Table 3 shows the results of our improvement methods in isolation, meaning that only that individual method is added to our baseline model. Re-ordering has a clear positive effect, both for using the best re-ordering (+2.0) and adding that re-ordering to the existing data set (+5.2). Constructing super characters and adding POS-tags both lead to a similar increase in performance. However, the biggest improvement comes from adding silver standard data to our training set. Adding more silver standard data even leads to a higher F-score, with 64.0 (+10.9) as our best score.\nSince the previous experiments were all in isolation, we now test whether a combination of our methods still increases performance. The tested combinations are shown in Table 4. Even after adding the silver data, the addition of POS-tags and super characters still increased the performance, albeit by a smaller margin. Adding POS-tags after the addition of super characters resulted in an improvement of only 0.1 on the dev set, but it resulted in an increase of 0.5 on the test set. Interestingly, the best result (69.0) was not obtained by combining all improvement methods, since re-ordering the AMRs does not show an increase anymore after adding POS-tags and super characters. The best model without using any silver data obtains an F-score of 62.0, which is considerably higher than the AMR-only score (55.5) of Konstas et al. (2017).\nTable 5 shows the results of the most notable previous AMR parsing systems. Our best model outperforms all these previous parsers and reaches state-of-the-art results. However, we are also the first approach that uses the LDC2016E25 data set, which contains slightly more than double the number of gold standard training instances compared to the LDC2015E86 data set.3 Therefore, we also trained the best performing model in Table 4 on the LDC2015E86 data set, while still applying all our improvement methods. This model still beats all previous approaches to AMR parsing, although this time by a smaller margin, obtaining an F-score of 67.3.\n3. The differences in size between these two datasets are 36,521 versus 16,833 instances of sentence-AMR pairs.\nDamonte et al. (2017) presented a way to evaluate system output in a more detailed way, by focussing on various aspects that are present in an AMR: the role labelling, word senses disambiguation, named entity recognition, wikification, detecting negation, and so on. These detailed results of our best system are shown in Table 6, in which the results of the other parsers are taken from Damonte et al. (2017). As the table shows, our system scores higher than the other parsers on five of the eight metrics other than Smatch. In general, our system is quite conservative, obtaining a higher precision than recall for each metric. Given the results in Table 6, you would think that detecting negation and re-entrancy would be ways to get an improvement in accuracy, note that the other parsers score also relatively bad at these metrics. Compared to the other systems, our system scores worse on concepts, named entities, and wikification. A possible method to increase performance in the first two of those metrics is to adopt an anonymization or generalization approach for named entities and concepts, similar to Peng et al. (2017) or Konstas et al. (2017)."}, {"heading": "5. Conclusion and Future Work", "text": "Applying re-ordering of AMR branches, introducing super characters, and adding POS-tags are techniques that substantially improve neural AMR parsing using a character-based seq2seq model. However, the biggest increase of performance is triggered by adding a large quantity of silver standard AMRs produced by existing (traditional) parsers. This is in line with the findings of Konstas et al. (2017), who used the Gigaword corpus to get extra training data, although their training method is different from ours.\nThe obtained results are promising. Our best model, with an F-score of 69.0, outperformed any known previously published result on AMR parsing. This is remarkable, for traditional approaches are often based on extensive, manually crafted lexicons using linguistic knowledge. It should be noted, of course, that we use some linguistic knowledge in the form of POS-tags in our best models. This can only mean that we can expect further improvements in neural semantic parsing when adding further linguistic knowledge.\nOne could consider the use of silver standard AMR data as a disadvantage, as there is still a need of an existing high-quality AMR parser to get the silver data in the first place. In our approach we rely even on two different off-the-shelf parsers. It would therefore be interesting to explore other opportunities, such as self-learning, as proposed by Konstas et al. (2017).\nWe have the feeling that there are still a lot of techniques that one could try to increase the performance of neural AMR parsing. From a more esthetical perspective, it would be nice if one could\neliminate the AMR repair strategies that are used to resolve unbalanced brackets. An interesting candidate that could master this problem would be the seq2tree model presented by Dong and Lapata (2016). Similarly, a more principled approach to deal with co-occurring variables would be desirable.\nAnother possible next step in semantic parsing is to change the target meaning representation. AMRs are unscoped meaning representations, and have no quantifiers. It would be challenging to transfer the techniques of neural semantic parsing to scoped meaning representations, such as those used in the Groningen Meaning Bank (Basile et al. 2012) or the Parallel Meaning Bank (Abzianidze et al. 2017)."}, {"heading": "Acknowledgements", "text": "First of all we would like to thank Antonio Toral and Lasha Abzianidze for helpful discussion on neural AMR parsing and machine translation. We would also like to thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine high performance computing cluster. We also used a Tesla K40 GPU, which was kindly donated to us by the NVIDIA Corporation. This work was funded by the NWO-VICI grant Lost in Translation Found in Meaning (288-89-003)."}], "references": [{"title": "The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning representations", "author": ["Abzianidze", "Lasha", "Johannes Bjerva", "Kilian Evang", "Hessel Haagsma", "Rik van Noord", "Pierre Ludmann", "Duc-Duy Nguyen", "Johan Bos"], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume", "citeRegEx": "Abzianidze et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Abzianidze et al\\.", "year": 2017}, {"title": "Broad-coverage ccg semantic parsing with amr", "author": ["Artzi", "Yoav", "Kenton Lee", "Luke Zettlemoyer"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Artzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, Association for Computational Linguistics, Sofia, Bulgaria", "author": ["Banarescu", "Laura", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": null, "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Developing a large semantically annotated corpus", "author": ["Basile", "Valerio", "Johan Bos", "Kilian Evang", "Noortje"], "venue": "Venhuizen", "citeRegEx": "Basile et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Basile et al\\.", "year": 2012}, {"title": "The meaning factory at semeval-2016 task 8: Producing amrs with boxer", "author": ["Bjerva", "Johannes", "Johan Bos", "Hessel Haagsma"], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016),", "citeRegEx": "Bjerva et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bjerva et al\\.", "year": 2016}, {"title": "Open-domain semantic parsing with boxer", "author": ["Bos", "Johan"], "venue": "Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA", "citeRegEx": "Bos and Johan,? \\Q2015\\E", "shortCiteRegEx": "Bos and Johan", "year": 2015}, {"title": "Wide-coverage semantic representations from a ccg parser", "author": ["Bos", "Johan", "Stephen Clark", "Mark Steedman", "James R. Curran", "Julia Hockenmaier"], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING \u201904),", "citeRegEx": "Bos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Smatch: an evaluation metric for semantic feature structures, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "author": ["Cai", "Shu", "Kevin Knight"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Nyu-mila neural machine translation systems for wmt16", "author": ["Chung", "Junyoung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Bootstrapping pos taggers using unlabelled data, Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, Association for Computational Linguistics", "author": ["Clark", "Stephen", "James R Curran", "Miles Osborne"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2003}, {"title": "Clause restructuring for statistical machine translation, Proceedings of the 43rd annual meeting on association for computational linguistics", "author": ["Collins", "Michael", "Philipp Koehn", "Ivona Ku\u010derov\u00e1"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Collins et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Daiber", "Joachim", "Max Jakob", "Chris Hokamp", "Pablo N. Mendes"], "venue": "Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)", "citeRegEx": "Daiber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "An incremental parser for abstract meaning representation, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, Association for Computational Linguistics", "author": ["Damonte", "Marco", "Shay B. Cohen", "Giorgio Satta"], "venue": null, "citeRegEx": "Damonte et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Damonte et al\\.", "year": 2017}, {"title": "Language to logical form with neural attention, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin", "author": ["Dong", "Li", "Mirella Lapata"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2016}, {"title": "Cmu at semeval-2016 task 8: Graph-based amr parsing with infinite ramp loss", "author": ["Flanigan", "Jeffrey", "Chris Dyer", "Noah A Smith", "Jaime Carbonell"], "venue": "Proceedings of SemEval pp", "citeRegEx": "Flanigan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "A discriminative graph-based parser for the abstract meaning representation, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Flanigan", "Jeffrey", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Flanigan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Opennmt: Open-source toolkit for neural machine translation, arXiv preprint arXiv:1701.02810", "author": ["Klein", "Guillaume", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M Rush"], "venue": null, "citeRegEx": "Klein et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Neural amr: Sequence-to-sequence models for parsing and generation, arXiv preprint (accepted in ACL-2017) arXiv:1704.08381", "author": ["Konstas", "Ioannis", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer"], "venue": null, "citeRegEx": "Konstas et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Konstas et al\\.", "year": 2017}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Luong", "Thang", "Hieu Pham", "Christopher D. Manning"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Semeval-2016 task 8: Meaning representation parsing", "author": ["May", "Jonathan"], "venue": "Proceedings of SemEval pp", "citeRegEx": "May and Jonathan,? \\Q2016\\E", "shortCiteRegEx": "May and Jonathan", "year": 2016}, {"title": "Addressing the data sparsity issue in neural amr parsing, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, Association for Computational Linguistics", "author": ["Peng", "Xiaochang", "Chuan Wang", "Daniel Gildea", "Nianwen Xue"], "venue": null, "citeRegEx": "Peng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "A transition-based algorithm for amr parsing, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association", "author": ["Wang", "Chuan", "Nianwen Xue", "Sameer Pradhan"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Research in this area comprises Bos et al. (2004), Bos (2015), Butler (2010), and Artzi et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 5, "context": "Research in this area comprises Bos et al. (2004), Bos (2015), Butler (2010), and Artzi et al.", "startOffset": 32, "endOffset": 62}, {"referenceID": 5, "context": "Research in this area comprises Bos et al. (2004), Bos (2015), Butler (2010), and Artzi et al.", "startOffset": 32, "endOffset": 77}, {"referenceID": 1, "context": "(2004), Bos (2015), Butler (2010), and Artzi et al. (2015). But recently there have been interesting attempts to view semantic parsing as a translation task, mapping English expressions to some logical form, under supervision of some deep learning method.", "startOffset": 39, "endOffset": 59}, {"referenceID": 1, "context": "(2004), Bos (2015), Butler (2010), and Artzi et al. (2015). But recently there have been interesting attempts to view semantic parsing as a translation task, mapping English expressions to some logical form, under supervision of some deep learning method. Dong and Lapata (2016) used sequence-to-sequence (seq2seq) and sequence-totree (seq2tree) neural translation models to produce logical forms from sentences for four different datasets (but not AMRs).", "startOffset": 39, "endOffset": 279}, {"referenceID": 1, "context": "(2004), Bos (2015), Butler (2010), and Artzi et al. (2015). But recently there have been interesting attempts to view semantic parsing as a translation task, mapping English expressions to some logical form, under supervision of some deep learning method. Dong and Lapata (2016) used sequence-to-sequence (seq2seq) and sequence-totree (seq2tree) neural translation models to produce logical forms from sentences for four different datasets (but not AMRs). Barzdins and Gosko (2016) used a similar method to produce Abstract Meaning Representations (AMRs) in the context of the shared task on semantic parsing at SemEval2016 (May 2016).", "startOffset": 39, "endOffset": 482}, {"referenceID": 2, "context": "AMRs were introduced by Banarescu et al. (2013) and are acyclic, directed graphs that represent the meaning of a sentence.", "startOffset": 24, "endOffset": 48}, {"referenceID": 16, "context": "To create our sequence-to-sequence translation model, we use the OpenNMT system (Klein et al. 2017).", "startOffset": 80, "endOffset": 99}, {"referenceID": 8, "context": "(2017), who use word-level input, we use character-level input, because we believe it generalizes better and it is therefore one of the promising trends in machine translation (Chung et al. 2016).", "startOffset": 176, "endOffset": 195}, {"referenceID": 18, "context": "We train a model with bidirectional encoding and general attention (Luong et al. 2015).", "startOffset": 67, "endOffset": 86}, {"referenceID": 15, "context": "To create our sequence-to-sequence translation model, we use the OpenNMT system (Klein et al. 2017). In contrast to Peng et al. (2017) and Konstas et al.", "startOffset": 81, "endOffset": 135}, {"referenceID": 15, "context": "To create our sequence-to-sequence translation model, we use the OpenNMT system (Klein et al. 2017). In contrast to Peng et al. (2017) and Konstas et al. (2017), who use word-level input, we use character-level input, because we believe it generalizes better and it is therefore one of the promising trends in machine translation (Chung et al.", "startOffset": 81, "endOffset": 161}, {"referenceID": 11, "context": "We restore wiki links in the output AMR by using an off-the-shelf system (Daiber et al. 2013), following the method presented by Bjerva et al.", "startOffset": 73, "endOffset": 93}, {"referenceID": 4, "context": "2013), following the method presented by Bjerva et al. (2016). They look at the :name relations in an", "startOffset": 41, "endOffset": 62}, {"referenceID": 10, "context": "It has been shown that for (statistical) machine translation reordering improves translation quality (Collins et al. 2005).", "startOffset": 101, "endOffset": 122}, {"referenceID": 9, "context": "To show this, we parse the sentences with the POS-tagger of the C&C tools (Clark et al. 2003), employing the Penn POS tagset.", "startOffset": 74, "endOffset": 93}, {"referenceID": 21, "context": "Instead of self-training our parser, we use the off-the-shelf AMR parsers CAMR (Wang et al. 2015) and JAMR (Flanigan et al.", "startOffset": 79, "endOffset": 97}, {"referenceID": 15, "context": "2015) and JAMR (Flanigan et al. 2014) to create silver standard data for our system.", "startOffset": 15, "endOffset": 37}, {"referenceID": 3, "context": "We parse 1,303,419 sentences from the Groningen Meaning Bank (Basile et al. 2012), which mainly consists of newswire text.", "startOffset": 61, "endOffset": 81}, {"referenceID": 16, "context": "Peng et al. (2017) tried to overcome this by extensive generalization of the training data, but did not get near state-of-the-art results.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Konstas et al. (2017) used the GigaWord corpus to self-train their system.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Konstas et al. (2017) used the GigaWord corpus to self-train their system. They use their own pre-trained parser to parse the previously unseen sentences and add those to the training data in a series of iterations. Ultimately, their system is trained on 20 million additional data AMR-sentence pairs and obtains an F-score of 62.1. Without this additional data, they only score 55.5, which is better than Peng et al. (2017), but not close to state-of-the-art performance.", "startOffset": 0, "endOffset": 425}, {"referenceID": 14, "context": "Konstas et al. (2017) used the GigaWord corpus to self-train their system. They use their own pre-trained parser to parse the previously unseen sentences and add those to the training data in a series of iterations. Ultimately, their system is trained on 20 million additional data AMR-sentence pairs and obtains an F-score of 62.1. Without this additional data, they only score 55.5, which is better than Peng et al. (2017), but not close to state-of-the-art performance. Our method differs from Konstas et al. (2017) in three ways: (i) we directly add new data to the training data instead of using it to pre-train the model; (ii) we use two off-the-shelf parsers to create the training data instead of self-training; (iii) we employ a method to exclude lower-quality AMRs instead of using all available data.", "startOffset": 0, "endOffset": 519}, {"referenceID": 17, "context": "5) of Konstas et al. (2017). Table 5 shows the results of the most notable previous AMR parsing systems.", "startOffset": 6, "endOffset": 28}, {"referenceID": 11, "context": "0 Damonte et al. (2017) AMR-eager LDC2015E86 64.", "startOffset": 2, "endOffset": 24}, {"referenceID": 1, "context": "0 Artzi et al. (2015) CCG parsing LDC2014T12 66.", "startOffset": 2, "endOffset": 22}, {"referenceID": 1, "context": "0 Artzi et al. (2015) CCG parsing LDC2014T12 66.3 Wang et al. (2015) CAMR LDC2015E86 66.", "startOffset": 2, "endOffset": 69}, {"referenceID": 1, "context": "0 Artzi et al. (2015) CCG parsing LDC2014T12 66.3 Wang et al. (2015) CAMR LDC2015E86 66.5 Flanigan et al. (2016) JAMR-16 LDC2015E86 67.", "startOffset": 2, "endOffset": 113}, {"referenceID": 1, "context": "0 Artzi et al. (2015) CCG parsing LDC2014T12 66.3 Wang et al. (2015) CAMR LDC2015E86 66.5 Flanigan et al. (2016) JAMR-16 LDC2015E86 67.0 Pust et al. (2015) SBMT LDC2015E86 67.", "startOffset": 2, "endOffset": 156}, {"referenceID": 19, "context": "0 Peng et al. (2017) word-based seq2seq LDC2015E86 52.", "startOffset": 2, "endOffset": 21}, {"referenceID": 17, "context": "0 Konstas et al. (2017) word-based seq2seq LDC2015E86 55.", "startOffset": 2, "endOffset": 24}, {"referenceID": 17, "context": "0 Konstas et al. (2017) word-based seq2seq LDC2015E86 55.5 Konstas et al. (2017) word-based seq2seq + giga LDC2015E86 62.", "startOffset": 2, "endOffset": 81}, {"referenceID": 12, "context": "Table 6: Comparison with previous parsers using the evaluation script of Damonte et al. (2017). We also included precision and recall scores for our system.", "startOffset": 73, "endOffset": 95}, {"referenceID": 17, "context": "This is in line with the findings of Konstas et al. (2017), who used the Gigaword corpus to get extra training data, although their training method is different from ours.", "startOffset": 37, "endOffset": 59}, {"referenceID": 17, "context": "This is in line with the findings of Konstas et al. (2017), who used the Gigaword corpus to get extra training data, although their training method is different from ours. The obtained results are promising. Our best model, with an F-score of 69.0, outperformed any known previously published result on AMR parsing. This is remarkable, for traditional approaches are often based on extensive, manually crafted lexicons using linguistic knowledge. It should be noted, of course, that we use some linguistic knowledge in the form of POS-tags in our best models. This can only mean that we can expect further improvements in neural semantic parsing when adding further linguistic knowledge. One could consider the use of silver standard AMR data as a disadvantage, as there is still a need of an existing high-quality AMR parser to get the silver data in the first place. In our approach we rely even on two different off-the-shelf parsers. It would therefore be interesting to explore other opportunities, such as self-learning, as proposed by Konstas et al. (2017). We have the feeling that there are still a lot of techniques that one could try to increase the performance of neural AMR parsing.", "startOffset": 37, "endOffset": 1064}, {"referenceID": 3, "context": "It would be challenging to transfer the techniques of neural semantic parsing to scoped meaning representations, such as those used in the Groningen Meaning Bank (Basile et al. 2012) or the Parallel Meaning Bank (Abzianidze et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 0, "context": "2012) or the Parallel Meaning Bank (Abzianidze et al. 2017).", "startOffset": 35, "endOffset": 59}], "year": 2017, "abstractText": "We evaluate the character-level translation method for neural semantic parsing on a large corpus of sentences annotated with Abstract Meaning Representations (AMRs). Using a seq2seq model, and some trivial preprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1 (F-score on AMR-triples). We examine four different approaches to improve this baseline result: (i) reordering AMR branches to match the word order of the input sentence increases performance to 58.3; (ii) adding part-of-speech tags (automatically produced) to the input shows improvement as well (57.2); (iii) So does the introduction of super characters (conflating frequent sequences of characters to a single character), reaching 57.4; (iv) adding silver-standard training data obtained by an off-the-shelf parser yields the biggest improvement, resulting in an F-score of 64.0. Combining all four techniques leads to an F-score of 69.0, which is state-of-the-art in AMR parsing. This is remarkable because of the relatively simplicity of the approach: the only explicit linguistic knowledge that we use are part-of-speech tags.", "creator": "LaTeX with hyperref package"}}}