{"id": "1705.10092", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Role Playing Learning for Socially Concomitant Mobile Robot Navigation", "abstract": "in this paper, we present the comprehensive role playing learning ( rpl ) scheme for a mobile robot to navigate socially voluntarily with its human companion relative in populated environments. neural networks ( nn ) are constructed to parameterize a stochastic statistical policy that directly maps sensory data collected by the robot to its velocity outputs, while respecting achieving a set of social norms. an efficient simulative learning environment is always built substantially with maps and pedestrians gps trajectories collected periodically from a number of real - world crowd data sets. in each learning iteration, a robot equipped with the efficient nn policy is created virtually in the learning environment to play itself as a freely companied household pedestrian and navigate towards a goal in a socially concomitant manner. thus, we call'this generation process role playing learning, which is formulated under \" a developmental reinforcement learning ( rl ) framework. the nn policy is optimized while end - generation to - end using adaptive trust region policy optimization ( trpo ), with consideration of noticing the imperfectness of robot's sensor measurements. simulative and experimental assessment results are provided to demonstrate the efficacy and superiority of our method.", "histories": [["v1", "Mon, 29 May 2017 09:42:36 GMT  (2902kb)", "http://arxiv.org/abs/1705.10092v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["mingming li", "rui jiang", "shuzhi sam ge", "tong heng lee"], "accepted": false, "id": "1705.10092"}, "pdf": {"name": "1705.10092.pdf", "metadata": {"source": "CRF", "title": "Role Playing Learning for Socially Concomitant Mobile Robot Navigation", "authors": ["Mingming Li", "Rui Jiang", "Shuzhi Sam Ge"], "emails": ["mingming@u.nus.edu;", "jiang@u.nus.edu;", "samge@nus.edu.sg;", "eleleeth@nus.edu.sg)."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 09\n2v 1\n[ cs\n.R O\n] 2\n9 M\nay 2\n01 7\nl Index Terms\u2014Socially concomitant navigation, mobile robot, neural network, reinforcement learning\nI. INTRODUCTION\nThe capability to navigate in densely populated and dynamic environments is one of the most important features that enable the deployment of mobile robots in unstructured environment, such as schools, shopping malls and transportation hubs. The key difference between the problem of navigating among humans and the traditional path planning and obstacle avoidance problems is that humans tend to smoothly evade each other interactively and cooperatively, rather than remaining static or maintaining an indifferent trajectory dynamics. In other words, there are social norms that need to be understood and complied to achieve maximum comfort of all involved pedestrians during navigation. We refer to this as the problem of social navigation, which aims to model such social norms and develop a robotic navigation policy that is socially acceptable to the pedestrians around.\nFor social navigation, the traditional approaches based on Dynamic Window Approach (DWA) [1] or potential fields [2], [3] are usually of limited efficacy as pedestrians are simply regarded as uncooperative obstacles. An illustrative example is the freezing robot problem (FRP) [4], [5], where a mobile robot will be stuck in a narrow corridor when facing a crowd of people if it lacks the ability to predict the joint collision avoidance behaviors of human pedestrians. To this\nM. Li, R. Jiang, S. S. Ge and T. H. Lee are with the Department of Electrical and Computer Engineering, and the Social Robotics Lab, Smart System Institute (SSI), National University of Singapore, Singapore 117576 (e-mail: li mingming@u.nus.edu; rui jiang@u.nus.edu; samge@nus.edu.sg; eleleeth@nus.edu.sg).\nend, researches have been done to understand the principles of humans\u2019 joint collision avoidance strategies and one of the pioneering works are the social force model (SFM) [6], [7]. Other joint collision avoidance model such as reciprocal velocity obstacles (RVO) have been proposed in [8], [9], [10], with an underlying assumption that all involved agents adopt the same collision avoidance strategies. These ideas are also applied to visual tracking of pedestrians [11], [12]. More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].\nIn this paper, we propose to augment the dimensions of human-robot interaction in social navigation by further endowing robot with appropriate group behaviors when it is travelling with a human companion. This capability is highly desirable for assistive mobile robots [16], [17], [18], which serve as assistants and companions and are expected to travel along with theirx human partners in not only home environment but also possibly crowded public areas. In other words, apart from understanding the collision avoidance behaviors of pedestrians, the robot also needs to consider the motion of its companion so as to maintain a sense of affinity when they are travelling together towards a certain goal. We call this socially concomitant navigation (SCN) and it is more challenging than the aforementioned social navigation problem, where the robot is assumed to travel alone with a simpler pursuit of reaching a specific goal while being free of collision.\nTo address the problem of SCN, we develop a new learning scheme called Role Playing Learning (RPL). Particularly, we formulate such problem under the framework of Partially Observable Markov Decision Process (POMDP) and reinforcement learning (RL). A neural network (NN) is used to parameterize the navigation policy of the robot, which is optimized to gives proper steering commands for the next time instance based on the robot\u2019s current and previous observations to its surroundings. To facilitate the RL process, we create a simulative navigation environment by mirroring a collections of real world pedestrians data sets and develop an on-policy optimization method called Partially Observable Trust Region Policy Optimization (PO-TRPO). In each run in an optimization iteration, the robot will attempt to play itself as a companion of a randomly chosen pedestrian by executing the NN navigation policy. The NN policy is then optimized using PO-TRPO based on a batch of collected trajectories. Compared to the existing analytically derived or data-driven approaches, our RPL scheme has the following advantages:\n2 1) RPL scheme is less restrictive. It does not rely on the\nassumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14]. 2) The formulation of RPL scheme is more generaliz-\nable and flexible. Our formulation contain no manuallydefined feature and domain knowledge (e.g., statistics of pedestrians\u2019 behaviors). It is not hardware-specific and can be easily modified to incorporate kinematics of different mobile robot platforms, sensor specifications and navigation objectives. In addition, unlike [15], [14], the learned navigation policy operates without assess to the global map of the environment. Therefore, it is not environment-specific and is well generalizable to unmet real-world scenarios. 3) We explicitly consider the noise and limitation of the\nrobot\u2019s sensor measurements. Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14]. On the contrary, our RPL schemes is rooted from the situation where the robot can only perceive those lie within its sensor\u2019s Field of View (FoV), with the existence of measurement noise. 4) As a RL-based approach, RPL is efficient. Although\nRPL aims at solving tasks that involve interaction among robot, humans and physical environment, it does not require participation of human in both data collection and learning, which is known to be tedious and timeconsuming. Instead, the learning process is safely automated in a simulative yet realistic environment with no human intervention.\nWe evaluate the performance of our approach in both simulations and real-world experiments, by comparing it with a baseline planner based on RVO [8] and humans, repectively. We also show that, with some tricks, the learned navigation policy can still be effective when the navigation scenario is reduced to the aforementioned social navigation, which means the robot is travelling without human companion.\nThe remainder of this paper is organized as follows. Related work is first reviewed in Section II. In Section III, the problem of SCN is formulated as a POMDP and associated definitions are given. RPL scheme and PO-TRPO algorithm are described in Section IV. Sections V and VI provide extensive results of simulation and experiment, followed by some concluding remarks in Section VII."}, {"heading": "II. RELATED WORK", "text": "The problems of robot navigation in populated and dynamic environment can be addressed from a number of angles, which can be largely classified into two groups as in the following subsections.\nA. Interactive Behaviors Models\nMany researches have been proposed to describe the interactive navigation behaviors of humans by fitting a computational\nmodel to the observed pedestrians trajectories [19]. In this way, the robot\u2019s path planner is able to understand pedestrians\u2019 intention during joint collision avoidance and actively calculate an optimal route towards its goal.\nIn the field of robotics, a majority of work in this direction is done via inverse reinforcement learning (IRL) [20], which learns a cost function that explains the observed behaviors. For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning. However, discrete representation is less desirable when modeling trajectories, which are in nature continuous and has higher order dynamics, such as velocities and acceleration. Instead, [15] adopts Maximum-A-Posteriori Bayesian IRL [27] to learn appropriate navigation behavior of a specific mobile robot from a set of demonstration trajectories. Note that, the demonstration data in [15] is specific to configurations of the robot and its sensor and has to be collected via human operation, which could be time-consuming. On the other hand, [13], [14] learns probabilistic models of composite trajectories of pedestrians from video data by maximum entropy learning and IRL. To better capture the characteristics of observed trajectories, they propose to develop their models based on a set of features that are hand-crafted according to the domain knowledge from psychological studies. In addition, those features contain velocities and accelerations of pedestrians, which, in practice, are hard to precisely measure. Besides, interacting Gaussian process (IGP) is derived in [5] to model the joint trajectories of pedestrian while explicitly considering the effects of observation noise. Nevertheless, the design of IGP also requires several hand-crafted kernels that are formulated based on the priori information in a specific application scenario.\nOther than researchers in robotics, the community of computer vision also possess great interest in pedestrian modeling. One of the important topics is trajectory prediction in video space. In [11], Linear Trajectory Avoidance (LTA) is developed as a dynamic model for pedestrians in video space for shortterm trajectory prediction and it is integrated into visual tracking system. Gaussian process is adopted in [28] to learn the motion pattern of pedestrians. Recently, Social LSTM is proposed in [29] for human trajectory prediction in crowd space. Similarly, the feature of social sensitivity is developed in [30] to analyze trajectories of pedestrians and bicyclists. While the above methods can effectively predict the navigation intention of pedestrians in videos, it is still unclear how to apply these model to navigation of robot in real scenarios."}, {"heading": "B. Steering Models", "text": "In contrast to learning behavior models of pedestrians, a more direct perspective is to develop a steering model that outputs the immediate navigation actions given the robot\u2019s current observation to the environment. One of the pioneering work in this direction is the social force model (SFM) [6], which uses energy/potential functions to encode the social status of pedestrian. Then, the navigation motivation of a pedestrian can be derived by taking the gradients of these energy functions. Following this idea, subsequent work [31], [32], [33], [12]\n3 propose to infer the optimal parameters of the energy function by fitting them to video data. However, they are likely to produce suboptimal results if the demonstration data from humans are imperfect. In [34], the authors integrate a people tracker and an iterative A\u2217 planner, with which the robot actively follows the pedestrian travelling in a similar direction to navigate through crowded environment. [35] follow the same idea and formulate the choice of a pedestrian to follow as a Multi-Policy Decision Making process. On the other hands, [36] develops a hierarchical POMDP for predictive navigation in dynamic environment. The idea is to predict the motion of pedestrians and generate a environment-specific cost map for path planning and obstacle avoidance.\nOther than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10]. The common idea of these methods is to treat pedestrians as moving obstacles and reactively update the planner every short periods to achieve collision avoidance. As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].\nOur proposed navigation policy belongs to the steering models. It takes an observation vector as input and outputs the navigation action through a stochastic neural networks. During RPL, our policy is optimized by the PO-TRPO algorithm, which is derived based on the recent advances in deep reinforcement learning (DRL) [39], [40]. DRL exploits the massive representation power of deep neural networks (DNN) [41] to build a complex yet sophisticated decision model, with which an agent can directly learn from raw signals instead of carefully crafted feature and tends to act more intelligently. Recently, there are several attempts in using DNN and DRL for robot navigation. For example, an end-to-end motion planner is learned in [42] to map raw sensor data of a laser range finder onto steering commands of a mobile robot. In [43], a decentralized multi-agent collision avoidance policy is learned via DRL, which can be thought as a DRL version of the original RVO approach [8]. Finally, a target-driven visual navigation policy for home environment is learned in [44] via DRL. They create a set 3D virtual home environments for effective and efficient training of the agent, which shares a similar idea with our proposed RPL scheme."}, {"heading": "III. PROBLEM FORMULATION", "text": "To formulate the problem of socially concomitant naviga-\ntion, we gives the following rules of SCN:\n1) The robot should reach its goal as fast as possible; 2) The robot should not collide with any of the pedestrians\nor its companion, or run into any obstacle;\n3) The robot should not run too far away from its compan-\nion.\nThe above rules serve as a generic description of the robot\u2019s desired performance during navigation. To give concrete definitions, consider the navigation process as an infinite-horizon\ndiscounted POMDP in discrete time, defined by the tuple (S,A, F,O, p0, r, \u03b3). S is a finite set of states s reflecting the navigation status of the robot. A is a finite set of actions a. In this paper, it is defined as a twosome of the translational and rotational velocities of a synchro-drive mobile robot, i.e., a = [vT , vR]. F : S \u00d7 A \u2192 S is state-transition mapping, which is characterized by the dynamics of the robot, the other humans and the environment. Without loss of generality, we assume deterministic state transition, i.e., si+1 = F (si, ai), where si, ai are the state and action taken at time ti. O is the set of the robot\u2019s observation o to the state s and \u03b2(o|s) denotes the conditional observation probability distribution. Note that, in practice, the robot\u2019s observation has only incomplete access to s or is subject to certain measurement noise, which implies o 6= s. p0 : S \u2192 R is the initial state distribution, i.e., s0 \u223c p0. r : S \u2192 R is a scalar reward given to the robot and \u03b3 \u2208 (0, 1] is the reward discount factor. Robot motion dynamics: In this paper, synchro-drive mobile robots are considered, whose motion equation can be approximated by assuming the robot\u2019s velocities to be constant within a certain short time period [ti, ti+1] [1] with length \u2206t = ti+1 \u2212 ti. Particularly, let \u03c6r(ti) and \u03c1r(ti) = [xr(ti), yr(ti)] denote the robot\u2019s heading and its positions in a 2D Cartesian space at time ti, respectively. vT (ti) \u2208 [0, v\u0304T ] and vR(ti) \u2208 [\u2212v\u0304R, v\u0304R] represent the robot\u2019s translational and rotational velocities. Define \u2206xr = xr(ti+1) \u2212 xr(ti) and \u2206yr = yr(ti+1) \u2212 yr(ti). When the robot has nonzero rotational velocity, i.e., vR(ti) 6= 0, we have\n\u2206xr = \u2212 vT (ti)(sin\u03c6r(ti)\u2212 sin(\u03c6r(ti) + vR(ti)\u2206t))\nvR(ti) (1)\n\u2206yr = vT (ti)(cos\u03c6r(ti)\u2212 cos(\u03c6r(ti) + vR(ti)\u2206t))\nvR(ti) (2)\nOtherwise, when vR(ti) = 0,\n\u2206xr = vT (ti) cos\u03c6r(ti) (3)\n\u2206yr = vT (ti) sin\u03c6r(ti) (4)\nWith the above formulations, our goal is optimizing a stochastic navigation policy P\u03b8 : O \u00d7 A \u2192 [0, 1] with parameters \u03b8 in order to maximize the expected discounted reward:\n\u03b7(P\u03b8) = E\u03c4 [ \u221e \u2211\ni=0\n\u03b3ir(si, ai)] (5)\nwhere \u03c4 = (s0, o0, a0, s1, o1, a1, \u00b7 \u00b7 \u00b7 ) denotes the whole trajectory and ai \u223c P\u03b8(ai|oi). The specific definitions of the above ingredients for SCN will be elaborated as follows:\nState: Given \u03c1r and \u03c6r, define the distance d and direction \u03c6 of a point \u03c1 = [x, y] to the robot as follows:\nd(\u03c1) = \u221a (x\u2212 xr)2 + (y \u2212 yr)2 (6) \u03c6(\u03c1) = arctan( y \u2212 yr x\u2212 xr )\u2212 \u03c6r (7)\nThen, the robot\u2019s distance to the goal located at \u03c1g = [xg, yg] are computed as dg = d(\u03c1g) and \u03c6g = \u03c6(\u03c1g) denotes the offset angle between the robot\u2019s current heading \u03c6r and its goal. Similarly, we can define the twosomes (djped, \u03c6 j ped),\n4 (djcom, \u03c6 j com) or (d j obs, \u03c6 j obs) to describe the relative position of a pedestrians \u03c1jped, a companion \u03c1 j com or an obstacle \u03c1 j obs to the robot. With such definitions, the state s is defined to incorporate the information related to the robot\u2019s navigation status as follows:\ns = [dg, \u03c6g, a, pped, pcom, pobs] (8)\nwhere a is the current action vector and\npped=[d 1 ped, \u03c6 1 ped, \u00b7 \u00b7 \u00b7 , d nped ped , \u03c6 nped ped ] (9) pcom=[dcom, \u03c6com] (10)\npobs=[d F obs, d\nL\u2212 obs , \u03c6 L\u2212 obs , d R\u2212 obs , \u03c6 R\u2212 obs , d L+ obs, \u03c6 L+ obs , d R+ obs , \u03c6 R+ obs ](11)\nThe vector pped includes the distances and directions of nped closest pedestrians while pcom includes those of the robot\u2019s companion.\nThe vector pobs is a compact description to the robot\u2019s perception of the surrounding environment. Particularly, the boundaries of the occupied space (obstacles) in the environment are represented as a finite point set Z = {\u03c11obs, \u03c1 2 obs, \u00b7 \u00b7 \u00b7 , \u03c1 j obs, \u00b7 \u00b7 \u00b7 }. Then, the 9 variables in pobs are defined based on the following assumption\nAssumption 1: An obstacle \u03c1obs \u2208 Z has no effect on the robot\u2019s navigation decision if it satisfies d(\u03c1obs) > d\u0304obs, where d\u0304obs is a predefined finite constant. By Assumption 1, it is sufficient to consider only obstacles in Z that are closed enough to the robot, whose distances are less than d\u0304obs. In practice, this limit may correspond to the robot\u2019s perception range. Let\nZ\u0304 = {\u03c1|\u03c1 \u2208 Z, d\u03c1 \u2264 d\u0304obs} (12)\nThe components in vector pobs are described as follows: The distance to the nearest obstacle located at heading of the robot, i.e.,\ndFobs = min \u03c1\u2208Z\u0304 and |\u03c6(\u03c1)|\u2264\u01eb\u03c1 d(\u03c1) (13)\nwhere \u01eb\u03c1 is a small constant. For dLobs, \u03c6 L obs, d R obs, \u03c6 R obs, they represent the distance and direction of the closest and farthest obstacles on the robot\u2019s left (\u03c1Lobs) and right side (\u03c1 R obs), respectively, which are defined mathematically as follows:\n\u03c1L \u2212\nobs = argmin \u03c1\u2208Z\u0304 and \u03c6(\u03c1)>\u01eb\u03c1 d(\u03c1) (14)\n\u03c1R \u2212\nobs = argmin \u03c1\u2208Z\u0304 and \u03c6(\u03c1)<\u2212\u01eb\u03c1 d(\u03c1) (15)\n\u03c1L +\nobs = argmax \u03c1\u2208Z\u0304 and \u03c6(\u03c1)>\u01eb\u03c1 d(\u03c1) (16)\n\u03c1R +\nobs = argmax \u03c1\u2208Z\u0304 and \u03c6(\u03c1)<\u2212\u01eb\u03c1 d(\u03c1) (17)\nThen, the variables in pobs can be simply determined as the distance and directions of the above points according to Eqs. (6) and (7). Figure. 1 provides a comprehensive illustration of the state variables pped, pcom and pobs. Observation: As discussed in the previous sections, sensors mounted on the robot are always subject to various kinds of limitation and measurement noise, which must be taken into\nFig. 1: Illustration of the state variables in Eq. (8). The blue, yellow and green circles represent the robot, its companion (Com.) and the pedestrians (Ped.) respectively. The red dashed circle with a radii d\u0304obs represents the boundary of the set Z\u0304 in Eq. 12. The black arrow shows the current heading of the robot. Considering the robot\u2019s current position as the origin, the polar coordinates of the pedestrians, the companion, the closest( and the farthest) obstacles in each direction are compactly represented as vectors pped, pcom, pobs.\naccount in order to develop a robust and practical navigation system. To this end, we define o as the robot\u2019s observation to the true state s as follows:\no = [dg, \u03c6g, a, p\u0302ped, p\u0302com, p\u0302obs] (18)\nBy Eq. (18), we assume that the robot has accurate information about the goal position and its current taken action (i.e., the velocity commands output to the robot\u2019s motor) while its observations to pped, pcom, pobs may be imperfect. Particularly, consider the Field of Views (FoVs) for the robot\u2019s pedestrian and obstacle detectors illustrated as Fig. 2.\nMathematically, let finite point sets Fped and Fobs denote the current FoVs of pedestrian and obstacle detectors, characterized by threesomes (\u03c6+ped, \u03c6 \u2212 ped, d + ped) and (\u03c6 + obs, \u03c6 \u2212 obs, d + obs), respectively. The robot\u2019s observations to the pedestrians\u2019 relative positions are obtained as\np\u0302ped = [d\u0302 1 ped, \u03c6\u0302 1 ped, \u00b7 \u00b7 \u00b7 , d\u0302 nped ped , \u03c6\u0302 nped ped ] (19)\nwhere\nd\u0302jped =\n{\ndjped + d\u0303ped, if \u03c1 j ped \u2208 Fped d+ped, else (20)\nand\n\u03c6\u0302jped =\n{\n\u03c6jped, if \u03c1 j ped \u2208 Fped \u03c0, if else (21)\nfor j = 1, \u00b7 \u00b7 \u00b7 , nped, with d\u0303ped being the measurement noise/error.\n5\nSimilarly, define\np\u0302obs = [d\u0302 F obs, d\u0302\nL\u2212 obs , \u03c6\u0302 L\u2212 obs , d\u0302 R\u2212 obs , \u03c6\u0302 R\u2212 obs , d\u0302 L+ obs , \u03c6\u0302 L+ obs , d\u0302 R+ obs , \u03c6\u0302 R+\nobs ] (22)\nCompared to the states in (13) to (17), only the obstacles within Fobs are observable. Thus, d\u0302 F obs is formulated as\nd\u0302Fobs = min \u03c1\u2208Z\u0304\u2229Fobs and |\u03c6(\u03c1)|\u2264\u01eb\u03c1 d(\u03c1) + d\u0303obs (23)\nwhere d\u0303obs is the measurement noise/error for obstacle detection. The closest observed obstacles on the robot\u2019s left and right sides are defined in a similar way as:\n\u03c1\u0302L \u2212\nobs = argmin \u03c1\u2208Z\u0304\u2229Fobs and \u03c6(\u03c1)>\u01eb\u03c1 d(\u03c1) + d\u0303obs (24)\n\u03c1\u0302R \u2212\nobs = argmin \u03c1\u2208Z\u0304\u2229Fobs and \u03c6(\u03c1)<\u2212\u01eb\u03c1 d(\u03c1) + d\u0303obs (25)\n\u03c1\u0302L +\nobs = argmax \u03c1\u2208Z\u0304\u2229Fobs and \u03c6(\u03c1)>\u01eb\u03c1 d(\u03c1) + d\u0303obs (26)\n\u03c1\u0302R +\nobs = argmax \u03c1\u2208Z\u0304\u2229Fobs and \u03c6(\u03c1)<\u2212\u01eb\u03c1 d(\u03c1) + d\u0303obs (27)\nThen, their distance and directions to the robot are calculated using Eqs. (6) and (7). For observation to the robot\u2019s companions, we rely on the following assumptions.\nAssumption 2: The companions \u03c11com, \u00b7 \u00b7 \u00b7 , \u03c1 ncom com are always\nobservable to the robot.\nThen, p\u0302com = [d\u0302com, \u03c6com], where\nd\u0302com = dcom + d\u0303com (28)\nRemark 1: By Eqs. (20) and (23) to (28), it is implied that the observation/measurement noises d\u0303ped, d\u0303obs and d\u0303com are additive and independent in different observations. A typical example of such noise is the Additive Gaussian White Noise (AGWN).\nRemark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard\nsensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.\nRemark 3: The mathematical definitions of the variables in observations p\u0302ped, p\u0302com, p\u0302obs are given for better understanding and are required only in the simulative RPL process. In practice, it is clear that these values can be directly measured via the robot\u2019s onboard sensors without accessing the actual 2-D Cartesian coordinates [x, y] of the considered point sets (e.g., Z, Fped and Fobs). For example, consider a robot equipped with a laser range finder. These distances and offset angles can be easily obtained from the returned ranges array[50].\nReward function: A scalar reward will be given to the robot as an award of reaching the goal or a penalty of colliding with obstacles/pedestrians/companions or losing its companions. Particularly, at time ti, the process of SCN will be terminated if any of the following three termination conditions is true.\n1) Goal Reaching Condition\ndg(ti) \u2264 0.8 (29)\n2) Collision Conditions\nmin j\ndjped(ti) \u2264 0.4 (30)\ndcom(ti) \u2264 0.4 (31)\nmin(dFobs(ti), d L\u2212 obs (ti), d R\u2212 obs (ti)) \u2264 0.2 (32)\n3) Stray Condition\ndcom(ti) \u2265 2 (33)\nBased on the above three terminal conditions, a reward r will be given to the robot as follows:\nr =\n\n \n  \n10000, if (29) \u221210000, if (30) or (31) or\n((32) or (33)\n\u221210|vR|, else\n(34)\nClearly, a positive reward will be given to the robot if it reaches its goal and it will receive a large negative reward if it collides with anything or be stray from its companion. Otherwise, the robot will receive an intermediate reward \u221210|vR|, which penalizes the robot for its rotational velocity to encourage a smoother trajectory with less turning behaviors."}, {"heading": "IV. ROLE PLAYING LEARNING", "text": "In this section, we described the RPL scheme to learn an effective navigation policy P\u03b8(a|o) for SCN in an efficient data-driven manner. The core idea is to transform the crowd trajectories data collected from real-world into a simulative and dynamic navigation environment, where the robot can play itself as a virtual pedestrian and iteratively improve the performance of P\u03b8(a|o) via Partially Observable Trusted Region Policy Optimization (PO-TRPO).\nConsider a set of simulative navigation environment E = {E1, \u00b7 \u00b7 \u00b7 , Ej, \u00b7 \u00b7 \u00b7 }. Each environment Ej = (Tj ,Mj) contains a set of pedestrian trajectories Tj = {\u03c1\nk 0:Tk } and a binary map Mj that annotates the 2-D Cartesian coordinates of\n6 obstacles/occupied space in the environment. With E, the abstract process of RPL is described by the following pseudo codes in Algorithm 1.\nAlgorithm 1 Role Playing Learning\nInitialize navigation policy P\u03b8 for Iter = 0, 1, \u00b7 \u00b7 \u00b7 ,MaxIter do while Number of collected sample time steps \u2264 Batch size do\nRandomly choose an environment Ej from E and then a trajectory \u03c1k0:Tk from Tj . Initialize the robot\u2019s position at \u03c1k0 and initial velocities [vT , vR] = [0, 0]. Set \u03c1g = \u03c1\nk Tk . Choose the robot\u2019s\nheading such that \u03c6g = 0. Choose SCN Mode with probability 0.5 if SCN Mode then\nAssign \u03c1kT0:Tk as the trajectory of the robot\u2019s companion, where T0 = argminT \u2032 \u2016\u03c1 k T \u2032 \u2212 \u03c1 k Tk \u2016 \u2265 0.6\nelse\nCreate a synthesized companion that moves along the robot\nend if Assign all other trajectories in Tj as pedestrians. while None of the termination conditions in (29) to (33) is satisfied do\nUpdate the states and observations of the robot according to Eqs. (8) and (18). Let the robot execute its policy P\u03b8. Update the robot\u2019s position according to dynamics (1) to (4) Calculate the current reward from Eq. (34) Update the positions of the companion and pedestrians according to the trajectories in Tj .\nend while\nend while\nUpdate P\u03b8 using PO-TRPO. end for\nCompanion Synthesization in non-SCN mode: As described in Algorithm 1, RPL actually incorporates two different navigation scenarios: the SCN proposed in this paper and the traditional social navigation scenario, where the robot has no human companion. This helps develop a navigation policy adaptable to both situations, with no restrictive assumption on the existence of companion. Particularly, the companion position vector pcom and its observation p\u0302com are synthesized, with dcom = d\u0302com = 0.8 for every time step while \u03c6com = \u03c6g . It is clear that the synthesized pcom is equivalent to the situation where the companion is travelling non-distractively along the robot with a constant distance and guarantee that termination conditions (31) and (33) are always false.\nOn the other hand, in SCN mode, the companion is assigned with a truncated trajectory \u03c1kT0:Tk such that the initial robotcompanion distance is sufficiently large.\nIn this paper, we construct a deep policy neural network to parameterize the navigation policy P\u03b8 , whose structure is shown in Fig. 3. The policy network P\u03b8 is to be trained with the Trust Region Policy Optimization (TRPO) [40] method.\nFig. 3: Structure of the deep policy network P\u03b8. At time ti, the observation vector oi is input to the feature network, which is a feedforward multi-layer perceptron (MLP). The output of the feature network is then fed to a LSTM network [51], a recurrent network for aggregation of the information collected through the navigation process. The LSTM network\u2019s outputs are assigned as the mean vector \u00b5 \u2208 R2 of the diagonal Gaussian unit N (\u00b5,\u03a3) on the right. The covariance matrix \u03a3 = \u03c32I \u2208 R2\u00d72, however, is independent of oi amd it is designed to be gradually decreasing during training and fixed during tests and experiments. Finally, the actions ai = [vT , VR] are drawn according to N (\u00b5,\u03a3).\nHowever, the original TRPO method is derived based on fully observable MDP, which can not be directly applied to our problem due to the imperfect observation in our formulation and practice. Thus, we proposed to extend the original TRPO algorithm as PO-TRPO, which will be described in the following subsections."}, {"heading": "A. Trusted Region Policy Optimization", "text": "The TRPO [40] algorithm is an effective on-policy optimization method for large nonlinear policies and tends to give monotonic improvement during the iterative optimization process. To be specific, a fully observable MDP is considered by TRPO and therefore the policy to be optimized is formulated as P \u2217\u03be (a|s), where \u03be is the parameter vector of the policy P \u2217. Note that, P \u2217\u03be (a|s) determines the action a directly from the true state s, which differs from our observation-based policy P\u03b8(a|o). Let us consider the following standard definitions of the state-action value function Q\u03be(si, ai), the value function V\u03be(si) and the advantage function A\u03be(si, ai):\nQ\u03be(si, ai) = Esi+1,ai+1,\u00b7\u00b7\u00b7[\n\u221e \u2211\nl=0\n\u03b3lr(si+l)], (35)\nV\u03be(si) = Eai,si+1,\u00b7\u00b7\u00b7[\n\u221e \u2211\nl=0\n\u03b3lr(si+l)], (36)\nA\u03be(si, ai) = Q\u03be(si, ai)\u2212 V\u03be(si) (37)\nwhere\nai \u223c P \u2217 \u03be (a|s), si+1 = F (si, ai) (38)\nIn addition, define \u03bd\u03be as the discounted visitation frequencies\n\u03bd\u03be(s) = p(s0 = s) + \u03b3p(s1 = s) + \u03b3 2p(s2 = s) + \u00b7 \u00b7 \u00b7 (39)\nwhere s0 \u223c p0, ai and si\u22651 are generated according to P \u2217\u03be and F . Let \u03be\u2212 denote the old parameters in last iteration. TRPO proposes to optimize the parameters \u03be iteratively regarding the following objective function:\n7 maximize Es\u223c\u03bd \u03be\u2212 ,a\u223cq\u2217 [ P\u2217\u03be (a|s) q(a|s) A\u03be\u2212(a|s)] (40) subject to Es\u223c\u03bd \u03be\u2212 [DKL(P \u2217 \u03be\u2212 (\u00b7|s) \u2016 P \u2217\u03be (\u00b7|s))] \u2264 \u01eb (41)\nwhere q\u2217(a|s) is the importance sampling distribution and DKL(P \u2217 \u03be\u2212\n\u2016 P \u2217\u03be ) is the Kullback-Leibler divergence between the old and current policies."}, {"heading": "B. Partially Observable TRPO", "text": "As mentioned, our navigation problem is considered as a POMDP. The policy P\u03b8(ai|oi) depends on the observation oi instead of the true state. Therefore, we write the objective function (40) and the constraint (41) as\nmaximize Es\u223c\u03bd \u03b8\u2212\n,a,o\u223cq[ \u2211 o \u03b2(o|s)P\u03b8(a|o)\nq(a,o|s) A\u03b8\u2212(a|s)] (42)\nsubject to Es\u223c\u03bd \u03b8\u2212 [DKL(P\u03b8\u2212(\u00b7|o) \u2016 P\u03b8(\u00b7|o))] \u2264 \u01eb (43)\nFor PO-TRPO, samples are collected by executing the old policy P\u03b8\u2212(a|o) to generate a set of trajectories, such as s0, o0, a0, s1, o1, a1, \u00b7 \u00b7 \u00b7 , sT\u22121, oT\u22121, aT\u22121, sT . Therefore,\nq(ai, oi|si) = \u03b2(oi|si)P\u03b8\u2212(ai|oi) (44)\nwhere i = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1. Next, for a trajectory s0:T , we use the generalized advantage estimation (GAE) [39] to construct an empirical estimation A\u0302 of the advantage function A\u03b8\u2212(ai|si) as the following:\nA\u0302i =\nT\u2212i \u2211\nl=0\n(\u03b3\u03bb)l\u03b4Vi+l (45)\nwhere\n\u03b4Vi = ri + \u03b3V\u0302\u03b6(si+1)\u2212 V\u0302\u03b6(si) (46)\nand V\u0302\u03b6(si) is the estimation of the value function (36) with parameters \u03b6 (and \u03b6\u2212 being the old parameters). By collecting a set of K trajectories {sk0:Tk , o k 0:Tk , ak0:Tk} K k=1, V\u0302\u03b6 is obtained by solving the following constrained regression problem [39]:\nminimize J1\u03b6 = \u2211K\nk=1 \u2211Tk i=0\u2016V\u0302\u03b6(s k i )\u2212 \u2211Tk\u2212i l=0 \u03b3 lrki+l\u2016 2 (47)\nsubject to \u2211K\nk=1 \u2211Tk i=0\n\u2016V\u03b6(s k i )\u2212V\u03b6\u2212 (s k i )\u2016\n2J 1\u03b6\u2212\n\u2264 \u01eb1 (48)\nFinally, as the conditional observation probability distribution \u03b2(o|s) is independent of parameters \u03b8 and time, we obtain an estimation of the objective function (42) and the constraints (43) by replacing the expectations with sample averages as:\nmaximize J\u03b8 = 1\u2211\nK k=1 Tk\n\u2211K\nk=1 \u2211Tk i=0\nP\u03b8(a k i |o k i )\nP \u03b8\u2212 (aki |o k i ) A\u0302ki (49)\nsubject to D\u0304\u03b8 \u2212\nKL (P\u03b8\u2212 , P\u03b8) \u2264 \u01eb (50)\nwhere\nD\u0304\u03b8 \u2212 KL (P\u03b8\u2212 , P\u03b8) = 1\n\u2211K k=1 Tk\nK \u2211\nk=1\nTk \u2211\ni=0\nDKL(P\u03b8\u2212(\u00b7|o k i ) \u2016 P\u03b8(\u00b7|o k i ))]\n(51)\nwhich has the same form as the one obtained in [39], except that the policy P\u03b8(a|o) is conditioned on observation o instead. Finally, the constrained optimization problem described in (49) and (50) is solved by conjugate gradient algorithm [52] algorithm. To summarize, the pseudo code for PO-TRPO update in Algorithm 1 is given as below:\nAlgorithm 2 PO-TRPO\nCompute the estimated advantages A\u0302i for all time steps using GAE with the estimated value function V\u0302\u03b6 . Update \u03b8 with objective function (49) and constraints (50) Update \u03b6 with objective function (47) and constraints (48)"}, {"heading": "V. SIMULATION", "text": "As a data-driven approach, our deep neural network policy requires a massive amount of data to learn the socially concomitant navigation behavior. In this section, we describe how to construct a simulative environment according to the proposed RPL scheme. Particularly, the environments, the deep neural network policy and the PO-TRPO algorithm (Algorithm 2) are developed under the framework of RLLAB [53]. We make use of trajectories of interacting pedestrians collected from five different data sets, which includes the ETH and Hotel video clips from the ETH Walking Pedestrians (EWAP) [11], the motion capture (MC) data set from [14], as well as the Zara and UCY video clips from [32]. Note that, the Zara and UCY data sets have multiple subsets: Zara01, Zara02, Zara03, UCY01 and UCY03. Thus, there are totally 8 different RPL environments, i.e., E = {E1, \u00b7 \u00b7 \u00b7 , E8}. The details of these 8 environments are summarized in Tab. I.\nEach trajectory in these environment provides the ID and a sequences of 2-D Cartesian positions of a pedestrian with a sampling period \u2206t = 0.1 second. In addition, eight binary grid mapsM1, \u00b7 \u00b7 \u00b7 ,M8 representing the occupied space/static obstacles are given. However, these maps are kept unknown to the robot throughout training and evaluation. They are only used to simulate the robot\u2019s perception to the environment as the state pobs and observation p\u0302obs. Without loss of generality, we use the ETH data set as the evaluation environment and all other data sets in the Tab. I as training environments. In other words, the learned policy\u2019s performance will be assessed in an RPL environment that is excluded during training, which reflects whether it can properly generalize to uncovered situations.\nAs some of the trajectories in these environments are of people who were wandering or remained approximately stationary, they are excluded from the candidates of the robot\u2019s companion but will still be considered as pedestrians when the robot is navigating in the same environment.\nWe use a feed-forward neural network with 2 hidden layers as the feature network in our NN policy, containing 256 and 64 Tanh units, respectively. Its output is then fed to a LSTM network with 64 units. The variance of of the Gaussian output unit \u03c3 is chosen to be linearly decaying from 0.5 to 0.05 in 100 training iterations, which effectively encourages exploration during the early stage of learning and ensures convergence of the navigation policy. For GAE, a 3 layer feed-forward\n8 network with 256,64,16 Tanh units are used, with \u03b3 = 0.995 and \u03bb = 0.96. The update step size for policy network is adaptively chosen as \u01eb = 0.01/\u03c3. For GAE update, a fixed step size \u01eb1 = 0.1 is used. The update batch size (Batch size in Algorithm 1) is 50000.\nIn RPL, we consider at most 3 pedestrians (i.e., np = 3). Thus, the state pped and observation p\u0302ped will only describe the 3 closest pedestrians and omit the others. For the situation where less that 3 pedestrians are perceived, dummy static pedestrians will be created in the remote corner of the environment so as to maintain the dimensions of pped and p\u0302ped. Considering a Kobuki Turtlebot 2 with a Hokuyo URG04LX laser range finder [50] mounted on its top, we specify the sensor limitation of the robot in simulation as follows:\n\u03c6+ped = \u03c6 + obs =\n2\u03c0\n3 (52)\n\u03c6\u2212ped = \u03c6 \u2212 obs = \u2212\n2\u03c0\n3 (53)\nThe measurement noises d\u0303ped, d\u0303com and d\u0303obs are modeled by zero-mean Gaussian N (0, \u03c32ped),N (0, \u03c3 2 com) and N (0, \u03c3 2 obs) with their variances specified as follows:\n\u03c3ped = 0.01d j ped (54)\n\u03c3com = 0.01d j com (55)\n\u03c3obs = 0.01d j obs (56)\nFinally, the maximum translational and rotational velocities are assigned as 0.7m/s and \u03c03 rad/s, i.e., 0 \u2264 vT \u2264 0.7 and |vR| \u2264 \u03c0 3 and \u2206t = 0.1. An example of our RPL environment constructed from the ETH data set is illustrated in Fig. 4"}, {"heading": "A. Results", "text": "We trained our deep policy network for 1200 iterations with the data from RPL environments except for the held-out ETH environment. The curve of average discounted return obtained from each batch of trajectories is visualized in Fig. 5\nWe compare the performance of our policy with a planner based on RVO [8], where the robot, its companion and the surrounding pedestrians are treated agents. In every time steps, the positions and velocities of all agents are given to the planner. Note that, for fair comparison, the agents\u2019 positions are subject to noise described in (54) and (55). For observations to obstacles, we assume the planner has full and perfect knowledge as required in the original RVO algorithm. With this protocol, we update the robot\u2019s position according to the planner\u2019s output and update the positions of the other agents according to their own trajectories in the RPL environments. The same termination conditions in Section III are applied to the robot directed by the RVO-based planner to determine whether the robot has conducted an successful navigation. For both of our policy and the RVO-based planner, we conduct 300 trials in the evaluation environment and compute the rates (in percentages) of different terminal conditions (RG: the robot reaches the goal successfully; HC/HP/HO: the robot hits a companion/pedestrian/obstacle; and LC: the robot loses its companion). The performance statistics of our policy and the RVO-based planner in SCN scenarios are listed in Tab. II.\nIt can bee seen from Tab. II that our policy performs much better than the RVO-based planner in SCN. The RVO-based planner has a much lower success rate (29.7%) while its rate of LC is 47%, suggesting that it frequently losses its companion in SCN. Clearly, this is due to the fact that RVO is in nature a collision avoidance algorithm. Thus, it simply takes the robot\u2019s\n9 companion as another normal agent and the robot tends to stay far behind its companion to avoid collision instead of actively following it. On the contrary, our policy achieves a much higher success rate (77%). This indicates that it learns to effectively balance the objectives of SCN so that the robot is able to reach the prescribed goal while maintaining its distance to its companion and avoiding collision with other agents in the environment.\nIn addition to SCN, the scenarios without companion are also tested, which, as analyzed in the previous sections, reduces to the traditional social navigation scenarios. The comparative results are shown in Tab. III.\nFor situations without companion, our policy still outperforms the RVO-based planner with higher success rate (84% to 80%) and lower HP rate (13.7% to 18%).\nFinally, it is worth noting that the RVO-based planner requires velocities of the companion/pedestrians and an accurate global map of the static obstacles. Conversely, our policy depends only on position measurements that are directly accessible from the robot\u2019s onboard sensors, which is therefore much simpler and more practical."}, {"heading": "VI. EXPERIMENTS", "text": "In experiments, we assess the performance of our developed navigation policy by comparing it with humans in the same scenarios. Particularly, a robot and a human are to repeat each specific navigation scenario for 10 times, respectively. Then, the the following two metrics are calculated:\n1) Average minimum distance to the pedestrians (D\u0304ped): the average of the minimum distance between the\nrobot/compared human to other pedestrians throughout a trajectory . 2) Average maximum distance to the companion (D\u0304com): the average of the maximum distance between the\nrobot/compared human to its/his companion throughout a trajectory.\nWe use the same mobile platform (a synchron-drive Turtlebot 2 with a Kobuki base) and the same laser range finder (Hokuyo URG-04LX) simulated in last section. For pedestrian detection and localization, we adopt the ROS-compatible leg tracker in [54]. We use an ultra wideband (UWB) indoor positioning system to localize the companion and the navigation goal, which can then be easily mapped to the observations p\u0302com and dg, \u03c6g based on the odometry of the robot. Finally, a laptop is placed onboard as the processing unit and the policy is operated with a period of 0.1 second. The experiments are conducted in a narrow corridor with width of 1.56 meters as shown in Fig. 6, which is a typical scenario that requires pedestrians to navigate cooperatively."}, {"heading": "A. Scenario 1: Traditional Social Navigation", "text": "In this subsection, we examine our method\u2019s performance in traditional social navigation scenario. Particularly, the robot is required to pass the corridor with two oncoming pedestrians and arrive at a goal that is 7 meters ahead. In addition, a control experiment of 3 humans (one as the compared human and the other two as pedestrians) is conducted in the same space. The metric D\u0304ped is computed. Example trajectories of the robot and the human control are shown in Fig. 7. In the robotic experiments, the trajectories of pedestrians are obtained from the robot\u2019s laser range finder while the robot\u2019s trajectory is based on its own odometry sensor. On the other hand, all trajectories in the human control experiments are captured using the UWB localization system.\nFrom Fig. 7, it is clear that the robot with our policy is\n10\nable understand human\u2019s cooperative behavior for collision avoidance and navigate in an appropriate manner such that both itself and the other two pedestrians can successfully pass through the corridor. Specifically, when observing the two pedestrians (blue and purple) 4 meters ahead. The robot started to approach the wall on its left side so as to create free space on the right for the pedestrians to smoothly walk through. By comparing both figures in Fig. 7, we can see that the robot is as proactive as human since both black trajectories in Fig. 7(a) and Fig. 7(b) started to make space for the oncoming pedestrians at the early stage of cooperative avoidance process. As for the performance metrics, the average minimal distance to pedestrians for our robot is D\u0304ped = 0.35m. Although it is smaller than that of the human control experiments (D\u0304ped=0.56m), this value still indicates a safe and decent navigation behavior of our robot as its radius is only 0.17m."}, {"heading": "B. Scenario 2: Socially Concomitant Navigation", "text": "In this subsection, the scenario of SCN is studied. A human companion initially standing in front of the robot will start to walk through the same corridor while another pedestrian is passing from the other end. As described in the previous sections, the robot with our policy should closely navigate with its companion and avoid the oncoming pedestrian cooperatively. An additional metric D\u0304com is used to evaluate the performance of our policy by comparing with the statistics obtained from another 10 human control experiments. Example trajectories are shown in Fig. 8 and the performance metrics D\u0304ped and D\u0304com are summarized in Tab. IV.\nAs shown in Fig. 8 and Tab. IV, the robot is able to achieve both objectives of SCN. On one hand, it is effectively engaged into the joint collision avoidance process. The resulted behavior is similar to that observed in the last subsection and the robot even has a slightly larger D\u0304ped. On the other hand, the average maximum distance D\u0304com is 1.05m, which is within the limit (2m) we specified in the learning process and nearly the same as that of the compared human, showing that the robot can actively navigate along with its companion instead of deviating to other areas or lagging itself behind. This shows that the robot driven by our policy is able to understand the pace of its companion and achieve a similar sense of companionship in terms of distance.\nIn sum, the above results demonstrate the practical efficacy of our methods for both the traditional social navigation and the more complicated SCN scenarios. It proves that the policy learned from our RPL simulative environment is transferable to uncovered real-world situations."}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, the problem of socially concomitant navigation (SCN) has been investigated and formulated under a POMDP\n(a) Trajectories of the robot and its companion (moving from left to right) and a pedestrian (moving from right to left).\n(b) Human control experiment in a similar SCN. The black (compared human) and orange (companion) trajectories are from left to right and the blue (pedestrian) trajectory is from right to left.\nFig. 8: Comparison between the robot with our policy and human control experiment in a SCN scenario\nframework, with explicit considerations of the limitation and inaccuracy of mobile robots\u2019 onboard sensors. The Partially Observable TRPO (PO-TRPO) algorithm has been proposed for optimization of navigation policies. The Role Playing Learning (RPL) scheme has been developed to enable efficient and safe reinforcement learning of navigation policies by mirroring a large amount of real-world pedestrian trajectories into simulative environments. Comparative simulation and experiment studies have demonstrated the efficacy and superiority of our policy in both SCN and traditional social navigation scenarios."}], "references": [{"title": "The dynamic window approach to collision avoidance", "author": ["D.F.W.B.S. Thrun", "D. Fox", "W. Burgard"], "venue": "IEEE Transactions on Robotics and Automation, vol. 4, p. 1, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A potential field approach to path planning", "author": ["Y.K. Hwang", "N. Ahuja"], "venue": "IEEE Transactions on Robotics and Automation, vol. 8, no. 1, pp. 23\u201332, 1992.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "New potential functions for mobile robot path planning", "author": ["S.S. Ge", "Y.J. Cui"], "venue": "IEEE Transactions on robotics and automation, vol. 16, no. 5, pp. 615\u2013620, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Unfreezing the robot: Navigation in dense, interacting crowds", "author": ["P. Trautman", "A. Krause"], "venue": "Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on, 2010, pp. 797\u2013803.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Robot navigation in dense human crowds: Statistical models and experimental studies of human\u2013robot cooperation", "author": ["P. Trautman", "J. Ma", "R.M. Murray", "A. Krause"], "venue": "The International Journal of Robotics Research, vol. 34, no. 3, pp. 335\u2013356, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Social force model for pedestrian dynamics", "author": ["D. Helbing", "P. Molnar"], "venue": "Physical review E, vol. 51, no. 5, p. 4282, 1995.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Simulating dynamical features of escape panic", "author": ["D. Helbing", "I. Farkas", "T. Vicsek"], "venue": "Nature, vol. 407, no. 6803, pp. 487\u2013490, 2000.  11", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Reciprocal nbody collision avoidance", "author": ["J. Van Den Berg", "S.J. Guy", "M. Lin", "D. Manocha"], "venue": "Robotics research, 2011, pp. 3\u201319.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information", "author": ["J. Van Den Berg", "P. Abbeel", "K. Goldberg"], "venue": "The International Journal of Robotics Research, vol. 30, no. 7, pp. 895\u2013913, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Reciprocal velocity obstacles for real-time multi-agent navigation", "author": ["J. Van den Berg", "M. Lin", "D. Manocha"], "venue": "Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on, 2008, pp. 1928\u20131935.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "You\u2019ll never walk alone: Modeling social behavior for multi-target tracking", "author": ["S. Pellegrini", "A. Ess", "K. Schindler", "L. Van Gool"], "venue": "2009 IEEE 12th International Conference on Computer Vision, 2009, pp. 261\u2013268.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Who are you with and where are you going?", "author": ["K. Yamaguchi", "A.C. Berg", "L.E. Ortiz", "T.L. Berg"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Feature-based prediction of trajectories for socially compliant navigation.", "author": ["M. Kuderer", "H. Kretzschmar", "C. Sprunk", "W. Burgard"], "venue": "in Robotics: science and systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Socially compliant mobile robot navigation via inverse reinforcement learning", "author": ["H. Kretzschmar", "M. Spies", "C. Sprunk", "W. Burgard"], "venue": "The International Journal of Robotics Research, p. 0278364915619772, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Socially adaptive path planning in human environments using inverse reinforcement learning", "author": ["B. Kim", "J. Pineau"], "venue": "International Journal of Social Robotics, vol. 8, no. 1, pp. 51\u201366, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards a society of robots", "author": ["A. Bicchi", "A. Fagiolini", "L. Pallottino"], "venue": "IEEE Robotics & Automation Magazine, vol. 17, no. 4, pp. 26\u201336, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Progress in developing a socially assistive mobile home robot companion for the elderly with mild cognitive impairment", "author": ["H.-M. Gross", "C. Schroeter", "S. Mueller", "M. Volkhardt", "E. Einhorn", "A. Bley", "C. Martin", "T. Langner", "M. Merten"], "venue": "Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on, 2011, pp. 2430\u20132437.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive shared control for a novel mobile assistive robot", "author": ["H. Wang", "X.P. Liu"], "venue": "IEEE/ASME Transactions on Mechatronics, vol. 19, no. 6, pp. 1725\u20131736, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems, vol. 57, no. 5, pp. 469\u2013483, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning, 2004, p. 1.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Maximum entropy inverse reinforcement learning.", "author": ["B.D. Ziebart", "A.L. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "in AAAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Maximum margin planning", "author": ["N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "Proceedings of the 23rd international conference on Machine learning, 2006, pp. 729\u2013736.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Planning-based prediction for pedestrians", "author": ["B.D. Ziebart", "N. Ratliff", "G. Gallagher", "C. Mertz", "K. Peterson", "J.A. Bagnell", "M. Hebert", "A.K. Dey", "S. Srinivasa"], "venue": "Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on, 2009, pp. 3931\u2013 3936.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to navigate through crowded environments", "author": ["P. Henry", "C. Vollmer", "B. Ferris", "D. Fox"], "venue": "Robotics and Automation (ICRA), 2010 IEEE International Conference on, 2010, pp. 981\u2013986.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions", "author": ["P. Vernaza", "D. Bagnell"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 575\u2013583.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Activity forecasting", "author": ["K.M. Kitani", "B.D. Ziebart", "J.A. Bagnell", "M. Hebert"], "venue": "European Conference on Computer Vision. Springer, 2012, pp. 201\u2013214.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Map inference for bayesian inverse reinforcement learning", "author": ["J. Choi", "K.-E. Kim"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 1989\u20131997.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian process regression flow for analysis of motion trajectories", "author": ["K. Kim", "D. Lee", "I. Essa"], "venue": "Computer vision (ICCV), 2011 IEEE international conference on, 2011, pp. 1164\u20131171.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Social lstm: Human trajectory prediction in crowded spaces", "author": ["A. Alahi", "K. Goel", "V. Ramanathan", "A. Robicquet", "L. Fei-Fei", "S. Savarese"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 961\u2013971.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning social etiquette: Human trajectory understanding in crowded scenes", "author": ["A. Robicquet", "A. Sadeghian", "A. Alahi", "S. Savarese"], "venue": "European Conference on Computer Vision, 2016, pp. 549\u2013565.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Specification of the social force pedestrian model by evolutionary adjustment to video tracking  data", "author": ["A. Johansson", "D. Helbing", "P.K. Shukla"], "venue": "Advances in complex systems, vol. 10, no. supp02, pp. 271\u2013288, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Crowds by example", "author": ["A. Lerner", "Y. Chrysanthou", "D. Lischinski"], "venue": "Computer Graphics Forum, vol. 26, no. 3. Wiley Online Library, 2007, pp. 655\u2013664.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Pedestrian, crowd and evacuation dynamics", "author": ["D. Helbing", "A. Johansson"], "venue": "Encyclopedia of Complexity and Systems Science, 2009, pp. 6476\u20136495.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Socially inspired motion planning for mobile robots in populated environments", "author": ["J. M\u00fcller", "C. Stachniss", "K. Arras", "W. Burgard"], "venue": "Proc. of International Conference on Cognitive Systems, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Autonomous navigation in dynamic social environments using multi-policy decision making", "author": ["D. Mehta", "G. Ferrer", "E. Olson"], "venue": "Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, 2016, pp. 1190\u20131197.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic autonomous robot navigation in dynamic environments with human motion prediction", "author": ["A.F. Foka", "P.E. Trahanias"], "venue": "International Journal of Social Robotics, vol. 2, no. 1, pp. 79\u201394, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic window based approach to mobile robot motion control in the presence of moving obstacles", "author": ["M. Seder", "I. Petrovic"], "venue": "Robotics and Automation, 2007 IEEE International Conference on, 2007, pp. 1986\u20131991.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Motion planning in dynamic environments using velocity obstacles", "author": ["P. Fiorini", "Z. Shiller"], "venue": "The International Journal of Robotics Research, vol. 17, no. 7, pp. 760\u2013772, 1998.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "From perception to decision: A data-driven approach to end-toend motion planning for autonomous ground robots", "author": ["M. Pfeiffer", "M. Schaeuble", "J. Nieto", "R. Siegwart", "C. Cadena"], "venue": "arXiv preprint arXiv:1609.07910, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Decentralized noncommunicating multiagent collision avoidance with deep reinforcement learning", "author": ["Y.F. Chen", "M. Liu", "M. Everett", "J.P. How"], "venue": "arXiv preprint arXiv:1609.07845, 2016.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Extrinsic calibration of 2-d lidars using two orthogonal planes", "author": ["D.-G. Choi", "Y. Bok", "J.-S. Kim", "I.S. Kweon"], "venue": "IEEE Transactions on Robotics, vol. 32, no. 1, pp. 83\u201398, 2016.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimal planning for target localization and coverage using range sensing", "author": ["L.M. Miller", "T.D. Murphey"], "venue": "Automation Science and Engineering (CASE), 2015 IEEE International Conference on, 2015, pp. 501\u2013508.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "3-d mapping with an rgb-d camera", "author": ["F. Endres", "J. Hess", "J. Sturm", "D. Cremers", "W. Burgard"], "venue": "IEEE Transactions on Robotics, vol. 30, no. 1, pp. 177\u2013187, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Object modeling using a tof camera under an uncertainty reduction approach", "author": ["S. Foix", "G. Alenya", "J. Andrade-Cetto", "C. Torras"], "venue": "Robotics and Automation (ICRA), 2010 IEEE International Conference on, 2010, pp. 1306\u20131312.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Topological mapping and scene recognition with lightweight color descriptors for an omnidirectional camera", "author": ["M. Liu", "R. Siegwart"], "venue": "IEEE Transactions on Robotics, vol. 30, no. 2, pp. 310\u2013324, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Characterization of the compact hokuyo urg-04lx 2d laser range scanner", "author": ["L. Kneip", "F. T\u00e2che", "G. Caprari", "R. Siegwart"], "venue": "Robotics and Automation, 2009. ICRA\u201909. IEEE International Conference on, 2009, pp. 1447\u20131454.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1997}, {"title": "Conjugate gradient methods", "author": ["J. Nocedal", "S.J. Wright"], "venue": "Numerical optimization, pp. 101\u2013134, 2006.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2006}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778, 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Person tracking and following with 2d laser scanners", "author": ["A. Leigh", "J. Pineau", "N. Olmedo", "H. Zhang"], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on, 2015, pp. 726\u2013733.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "For social navigation, the traditional approaches based on Dynamic Window Approach (DWA) [1] or potential fields [2], [3] are usually of limited efficacy as pedestrians are simply regarded as uncooperative obstacles.", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "For social navigation, the traditional approaches based on Dynamic Window Approach (DWA) [1] or potential fields [2], [3] are usually of limited efficacy as pedestrians are simply regarded as uncooperative obstacles.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "For social navigation, the traditional approaches based on Dynamic Window Approach (DWA) [1] or potential fields [2], [3] are usually of limited efficacy as pedestrians are simply regarded as uncooperative obstacles.", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "An illustrative example is the freezing robot problem (FRP) [4], [5], where a mobile robot will be stuck in a narrow corridor when facing a crowd of people if it lacks the ability to predict the joint collision avoidance behaviors of human pedestrians.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "An illustrative example is the freezing robot problem (FRP) [4], [5], where a mobile robot will be stuck in a narrow corridor when facing a crowd of people if it lacks the ability to predict the joint collision avoidance behaviors of human pedestrians.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "end, researches have been done to understand the principles of humans\u2019 joint collision avoidance strategies and one of the pioneering works are the social force model (SFM) [6], [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 6, "context": "end, researches have been done to understand the principles of humans\u2019 joint collision avoidance strategies and one of the pioneering works are the social force model (SFM) [6], [7].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "Other joint collision avoidance model such as reciprocal velocity obstacles (RVO) have been proposed in [8], [9], [10], with an underlying assumption that all involved agents adopt the same collision avoidance strategies.", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "Other joint collision avoidance model such as reciprocal velocity obstacles (RVO) have been proposed in [8], [9], [10], with an underlying assumption that all involved agents adopt the same collision avoidance strategies.", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "Other joint collision avoidance model such as reciprocal velocity obstacles (RVO) have been proposed in [8], [9], [10], with an underlying assumption that all involved agents adopt the same collision avoidance strategies.", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "These ideas are also applied to visual tracking of pedestrians [11], [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "These ideas are also applied to visual tracking of pedestrians [11], [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].", "startOffset": 271, "endOffset": 275}, {"referenceID": 4, "context": "More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].", "startOffset": 277, "endOffset": 280}, {"referenceID": 13, "context": "More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].", "startOffset": 282, "endOffset": 286}, {"referenceID": 14, "context": "More recently, several attempts are made to learn probabilistic models of pedestrians\u2019 trajectories during joint collision avoidance, based on which the robot\u2019s navigation decision is generated such that it is able to behave naturally and correctly in similar situations [13], [5], [14], [15].", "startOffset": 288, "endOffset": 292}, {"referenceID": 15, "context": "This capability is highly desirable for assistive mobile robots [16], [17], [18], which serve as assistants and companions and are expected to travel along with theirx human partners in not only home environment but also possibly crowded public areas.", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "This capability is highly desirable for assistive mobile robots [16], [17], [18], which serve as assistants and companions and are expected to travel along with theirx human partners in not only home environment but also possibly crowded public areas.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "This capability is highly desirable for assistive mobile robots [16], [17], [18], which serve as assistants and companions and are expected to travel along with theirx human partners in not only home environment but also possibly crowded public areas.", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 135, "endOffset": 138}, {"referenceID": 13, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 140, "endOffset": 144}, {"referenceID": 4, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 199, "endOffset": 202}, {"referenceID": 13, "context": "It does not rely on the assumption that the robot and other agents (pedestrians) share the same decision-making models [8], [9], [10], [5], [14] or that the navigation goals of pedestrians are known [5], [14].", "startOffset": 204, "endOffset": 208}, {"referenceID": 14, "context": "In addition, unlike [15], [14], the learned navigation policy operates without assess to the global map of the environment.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "In addition, unlike [15], [14], the learned navigation policy operates without assess to the global map of the environment.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14].", "startOffset": 176, "endOffset": 179}, {"referenceID": 8, "context": "Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14].", "startOffset": 181, "endOffset": 184}, {"referenceID": 9, "context": "Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": "Most approaches for social navigation assume that the robot has full and accurate knowledge of interested variables, such as positions or distance of pedestrians and obstacles [8], [9], [10], [14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 7, "context": "We evaluate the performance of our approach in both simulations and real-world experiments, by comparing it with a baseline planner based on RVO [8] and humans, repectively.", "startOffset": 145, "endOffset": 148}, {"referenceID": 18, "context": "Many researches have been proposed to describe the interactive navigation behaviors of humans by fitting a computational model to the observed pedestrians trajectories [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "In the field of robotics, a majority of work in this direction is done via inverse reinforcement learning (IRL) [20], which learns a cost function that explains the observed behaviors.", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "For example, maximum entropy IRL [21] is adopted in a number works [22], [23], [24], [25], [26] for discrete human behavior prediction and route planning.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "Instead, [15] adopts Maximum-A-Posteriori Bayesian IRL [27] to learn appropriate navigation behavior of a specific mobile robot from a set of demonstration trajectories.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "Instead, [15] adopts Maximum-A-Posteriori Bayesian IRL [27] to learn appropriate navigation behavior of a specific mobile robot from a set of demonstration trajectories.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Note that, the demonstration data in [15] is specific to configurations of the robot and its sensor and has to be collected via human operation, which could be time-consuming.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "On the other hand, [13], [14] learns probabilistic models of composite trajectories of pedestrians from video data by maximum entropy learning and IRL.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "On the other hand, [13], [14] learns probabilistic models of composite trajectories of pedestrians from video data by maximum entropy learning and IRL.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "Besides, interacting Gaussian process (IGP) is derived in [5] to model the joint trajectories of pedestrian while explicitly considering the effects of observation noise.", "startOffset": 58, "endOffset": 61}, {"referenceID": 10, "context": "In [11], Linear Trajectory Avoidance (LTA) is developed as a dynamic model for pedestrians in video space for shortterm trajectory prediction and it is integrated into visual tracking system.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "Gaussian process is adopted in [28] to learn the motion pattern of pedestrians.", "startOffset": 31, "endOffset": 35}, {"referenceID": 28, "context": "Recently, Social LSTM is proposed in [29] for human trajectory prediction in crowd space.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "Similarly, the feature of social sensitivity is developed in [30] to analyze trajectories of pedestrians and bicyclists.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "One of the pioneering work in this direction is the social force model (SFM) [6], which uses energy/potential functions to encode the social status of pedestrian.", "startOffset": 77, "endOffset": 80}, {"referenceID": 30, "context": "Following this idea, subsequent work [31], [32], [33], [12]", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "Following this idea, subsequent work [31], [32], [33], [12]", "startOffset": 43, "endOffset": 47}, {"referenceID": 32, "context": "Following this idea, subsequent work [31], [32], [33], [12]", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "Following this idea, subsequent work [31], [32], [33], [12]", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "In [34], the authors integrate a people tracker and an iterative A planner, with which the robot actively follows the pedestrian travelling in a similar direction to navigate through crowded environment.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "[35] follow the same idea and formulate the choice of a pedestrian to follow as a Multi-Policy Decision Making process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "On the other hands, [36] develops a hierarchical POMDP for predictive navigation in dynamic environment.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 138, "endOffset": 141}, {"referenceID": 36, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 37, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 213, "endOffset": 216}, {"referenceID": 8, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 218, "endOffset": 221}, {"referenceID": 9, "context": "Other than navigating in a pedestrian-aware manner, several reactive collision avoidance techniques have also been developed, such as DWA [1], [37], velocity obstacles [38] and reciprocal velocity obstacles (RVO) [8], [9], [10].", "startOffset": 223, "endOffset": 227}, {"referenceID": 36, "context": "As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].", "startOffset": 219, "endOffset": 223}, {"referenceID": 7, "context": "As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].", "startOffset": 293, "endOffset": 296}, {"referenceID": 8, "context": "As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].", "startOffset": 298, "endOffset": 301}, {"referenceID": 9, "context": "As mentioned in Section I, these methods are less effective for social navigation as they lack predictive abilities and are based on some restrictive assumptions, such as accurate knowledge of moving agents\u2019 velocities [37] and that all agents adopt the identical collision avoidance strategy [8], [9], [10].", "startOffset": 303, "endOffset": 307}, {"referenceID": 38, "context": "During RPL, our policy is optimized by the PO-TRPO algorithm, which is derived based on the recent advances in deep reinforcement learning (DRL) [39], [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 39, "context": "During RPL, our policy is optimized by the PO-TRPO algorithm, which is derived based on the recent advances in deep reinforcement learning (DRL) [39], [40].", "startOffset": 151, "endOffset": 155}, {"referenceID": 40, "context": "DRL exploits the massive representation power of deep neural networks (DNN) [41] to build a complex yet sophisticated decision model, with which an agent can directly learn from raw signals instead of carefully crafted feature and tends to act more intelligently.", "startOffset": 76, "endOffset": 80}, {"referenceID": 41, "context": "For example, an end-to-end motion planner is learned in [42] to map raw sensor data of a", "startOffset": 56, "endOffset": 60}, {"referenceID": 42, "context": "In [43], a decentralized multi-agent collision avoidance policy is learned via DRL, which can be thought as a DRL version of the original RVO approach [8].", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [43], a decentralized multi-agent collision avoidance policy is learned via DRL, which can be thought as a DRL version of the original RVO approach [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 43, "context": "Finally, a target-driven visual navigation policy for home environment is learned in [44] via DRL.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Robot motion dynamics: In this paper, synchro-drive mobile robots are considered, whose motion equation can be approximated by assuming the robot\u2019s velocities to be constant within a certain short time period [ti, ti+1] [1] with length \u2206t = ti+1 \u2212 ti.", "startOffset": 220, "endOffset": 223}, {"referenceID": 0, "context": "With the above formulations, our goal is optimizing a stochastic navigation policy P\u03b8 : O \u00d7 A \u2192 [0, 1] with parameters \u03b8 in order to maximize the expected discounted reward:", "startOffset": 96, "endOffset": 102}, {"referenceID": 44, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 145, "endOffset": 149}, {"referenceID": 45, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 151, "endOffset": 155}, {"referenceID": 46, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 163, "endOffset": 167}, {"referenceID": 47, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 189, "endOffset": 193}, {"referenceID": 48, "context": "Remark 2: Our general formulations of states (8) and observations (18) are applicable to various types of onboard sensors, such as range sensors [45], [46], RGB-D [47], Timeof-Flight (ToF) [48] and omnidirectional cameras [49], as long as the interested positions can be extracted/estimated from the sensor\u2019s raw measurements.", "startOffset": 222, "endOffset": 226}, {"referenceID": 49, "context": "These distances and offset angles can be easily obtained from the returned ranges array[50].", "startOffset": 87, "endOffset": 91}, {"referenceID": 39, "context": "The policy network P\u03b8 is to be trained with the Trust Region Policy Optimization (TRPO) [40] method.", "startOffset": 88, "endOffset": 92}, {"referenceID": 50, "context": "The output of the feature network is then fed to a LSTM network [51], a recurrent network for aggregation of the information collected through the navigation process.", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "The TRPO [40] algorithm is an effective on-policy optimization method for large nonlinear policies and tends to give monotonic improvement during the iterative optimization process.", "startOffset": 9, "endOffset": 13}, {"referenceID": 38, "context": "Next, for a trajectory s0:T , we use the generalized advantage estimation (GAE) [39] to construct an empirical estimation \u00c2 of the advantage function A\u03b8\u2212(ai|si) as the following:", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "By collecting a set of K trajectories {sk0:Tk , o k 0:Tk , ak0:Tk} K k=1, V\u0302\u03b6 is obtained by solving the following constrained regression problem [39]:", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "(51) which has the same form as the one obtained in [39], except that the policy P\u03b8(a|o) is conditioned on observation o instead.", "startOffset": 52, "endOffset": 56}, {"referenceID": 51, "context": "Finally, the constrained optimization problem described in (49) and (50) is solved by conjugate gradient algorithm [52] algorithm.", "startOffset": 115, "endOffset": 119}, {"referenceID": 52, "context": "Particularly, the environments, the deep neural network policy and the PO-TRPO algorithm (Algorithm 2) are developed under the framework of RLLAB [53].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "We make use of trajectories of interacting pedestrians collected from five different data sets, which includes the ETH and Hotel video clips from the ETH Walking Pedestrians (EWAP) [11], the motion capture (MC) data set from [14], as well as the Zara and UCY video clips from [32].", "startOffset": 181, "endOffset": 185}, {"referenceID": 13, "context": "We make use of trajectories of interacting pedestrians collected from five different data sets, which includes the ETH and Hotel video clips from the ETH Walking Pedestrians (EWAP) [11], the motion capture (MC) data set from [14], as well as the Zara and UCY video clips from [32].", "startOffset": 225, "endOffset": 229}, {"referenceID": 31, "context": "We make use of trajectories of interacting pedestrians collected from five different data sets, which includes the ETH and Hotel video clips from the ETH Walking Pedestrians (EWAP) [11], the motion capture (MC) data set from [14], as well as the Zara and UCY video clips from [32].", "startOffset": 276, "endOffset": 280}, {"referenceID": 49, "context": "Considering a Kobuki Turtlebot 2 with a Hokuyo URG04LX laser range finder [50] mounted on its top, we specify the sensor limitation of the robot in simulation as follows:", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "We compare the performance of our policy with a planner based on RVO [8], where the robot, its companion and the surrounding pedestrians are treated agents.", "startOffset": 69, "endOffset": 72}, {"referenceID": 53, "context": "For pedestrian detection and localization, we adopt the ROS-compatible leg tracker in [54].", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "In this paper, we present the Role Playing Learning (RPL) scheme for a mobile robot to navigate socially with its human companion in populated environments. Neural networks (NN) are constructed to parameterize a stochastic policy that directly maps sensory data collected by the robot to its velocity outputs, while respecting a set of social norms. An efficient simulative learning environment is built with maps and pedestrians trajectories collected from a number of real-world crowd data sets. In each learning iteration, a robot equipped with the NN policy is created virtually in the learning environment to play itself as a companied pedestrian and navigate towards a goal in a socially concomitant manner. Thus, we call this process Role Playing Learning, which is formulated under a reinforcement learning (RL) framework. The NN policy is optimized end-toend using Trust Region Policy Optimization (TRPO), with consideration of the imperfectness of robot\u2019s sensor measurements. Simulative and experimental results are provided to demonstrate the efficacy and superiority of our method. l", "creator": "LaTeX with hyperref package"}}}