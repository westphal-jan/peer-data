{"id": "1705.10209", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "On Multilingual Training of Neural Dependency Parsers", "abstract": "we notably show that a recently proposed neural dependency parser can be improved by joint training on multiple languages from nearly the same family. the parser is implemented as having a deep neural network whose seemingly only input is orthographic representations of words. in order one to successfully parse, the network has to discover how linguistically relevant concepts can be inferred from word spellings. we analyze the different representations of characters and words that are learned further by placing the auditory network to establish which properties of languages were accounted for. in particular together we show that the parser has approximately learned to associate latin characters with their cyrillic counterparts there and that it can group polish and russian words that have evolved a similar respective grammatical function. finally, we evaluate the parser on selected languages from the universal dependencies / dataset and show that it is technically competitive with other recently proposed state - of - the counter art methods, while simply having a simple structure.", "histories": [["v1", "Mon, 29 May 2017 14:24:08 GMT  (317kb,D)", "http://arxiv.org/abs/1705.10209v1", "preprint accepted into the TSD2017"]], "COMMENTS": "preprint accepted into the TSD2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["micha{\\l} zapotoczny", "pawe{\\l} rychlikowski", "jan chorowski"], "accepted": false, "id": "1705.10209"}, "pdf": {"name": "1705.10209.pdf", "metadata": {"source": "CRF", "title": "On Multilingual Training of Neural Dependency Parsers", "authors": ["Micha\u0142 Zapotoczny", "Jan Chorowski"], "emails": ["mzapotoczny@gmail.com", "pawel.rychlikowski@cs.uni.wroc.pl", "jan.chorowski@cs.uni.wroc.pl"], "sections": [{"heading": null, "text": "Keywords: Dependency Parsing, Recurrent Neural Networks, Multitask Training"}, {"heading": "1 Introduction", "text": "Parsing text is an important part of many natural language processing applications. Recent state-of-the-art results were obtained with parsers implemented using deep neural networks [3]. Neural networks are flexible learners able to express complicated inputoutput relationships. However, as more powerful machine learning techniques are used, the quality of results will not be limited by the capacity of the model, but by the amount of the available training data. In this contribution we examine the possibility of increasing the training set by using treebanks from similar languages.\nFor example, in the upcoming Universal Dependencies (UD) 2.0 treebank collection [28] there are 863 annotated Ukrainian sentences, 333 Belarusian, but nearly 60k Russian ones (divided into two sets: a default one of 4.4k sentences and SynTagRus with 55.4k sentences). Similarly, there are 7k Polish sentences and a little over 100k Czech ones1. Since these languages belong to the same Slavic language family, performance on the low resource languages should improve by joint training the model also on a better annotated language [6]. In this paper, we demonstrate this improvement. Starting with a parser competitive with the current state-of-the-art, we are able to further improve the results for tested languages from the Slavic family. We train the model on pairs of languages through simple parameter sharing in an end-to-end fashion, retaining the structure and qualities of the base model.\n1 However, experiments use UD 1.3 dataset which does not include Belarusian and Ukrainian.\nar X\niv :1\n70 5.\n10 20\n9v 1\n[ cs\n.C L\n] 2\n9 M\nay 2\n01 7"}, {"heading": "2 Background and Related Work", "text": "Dependency parsers represent sentences as trees in which every word is connected to its head with a directed edge (called a dependency) labeled with the dependency\u2019s type. Parsers often contain parts that are learned on a corpus. In example, transition-based dependency parsers use the learned component to guide their actions, while graphbased dependency parser learn a scoring that measures the quality of inserting a (head, dependency) edge into the tree.\nHistorically, the learning algorithms were relatively simple ones, e.g. transitionbased parsers used linear SVMs [27,26]. Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3]. This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].\nNeural networks have enough capacity to directly solve the parsing task. For example a constituency parser can be implemented using a sequence-to-sequence network originally developed for translation [34]. Similarly, a graph-based dependency parser can be implemented by solving two supervised tasks: head selection and dependency labeling. Both are easily solved using neural networks [22,37,13,12]. Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].\nAnother particularly nice property of neural models is that all internal computations use distributed representations of input data that are embedded in highly dimensional vector spaces [19]. These internal representation can be easily shared between tasks [8]. Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2]. Creation of multilingual parsers is further facilitated by the introduction of standardized treebanks, such as the Universal Dependencies [28]."}, {"heading": "3 Model", "text": "Our multilingual parser can be seen as n identical neural dependency parsers for n languages, which share parameters. When all parameters are shared a single parser is obtained for all n languages. When only a subset of parameters is shared the model can be seen as a parser for a main language that is partially regularized using data for other languages.\nEach of the n parsers is a single neural network that directly reads a sequence of characters and finds dependency edges along with their labels [12]. We can functionally describe four basic parts: Reader, Tagger, Labeler/Scorer, and an optional POS Tag Predictor (Figure 1).\nThe reader is tasked with transforming the orthographic representation of a single word w into a vector Ew \u2208 REdim, also called the word w\u2019s embedding. First, we represent each word as a sequence of characters fenced with start-of-word and end-ofword tokens. We find low dimensional characters embeddings and concatenate them to form a matrix Cw. Next we convolve this matrix with a learned filterbank F\nRw,i = max(Cw \u2217 Fi), (1)\nwhere Fi is the i-th filter and \u2217 denotes convolution over the length of the word. Thanks to the start- and end-of-word tokens the filters can selectively target infixes, prefixes and suffixes of words. Finally, we max-pool the filter activations over the word length and apply a small feedforward network to obtain final word embedding Ew = MLP(Rw).\nThe tagger processes complete sentences and puts individual word embeddings Ew into their contexts. We use a multi-layer bidirectional GRU Recurrent Neural Network (BiRNN) [10,29]. The output of the tagger is a sequence of the BiRNN\u2019s hidden states H0, H1, . . . ,Hn with Hi \u2208 RHdim, where H0 corresponds to a prepended ROOT word and n is the length of the sentence. Please observe that while the embedding Ei of the i-th word only depends on the word\u2019s spelling, the corresponding hidden state Hi depends on the whole sentence.\nWe have also added an auxiliary network to predict POS tags based on hidden states Hi. It serves two purposes: first, it can provide extra supervision on POS tags known during training. Second, it helps to attribute errors to various parts of the network (c.f. Sec. 4.4). The POS tag predictor is optional: its output is not used during inference because the tagger communicates all information to the scorer and labeler through the hidden states Hi.\nFinally, the network produces the dependency tree by solving two supervised learning tasks: using a scorer to find the head word, then using a labeler to find the edge label .\nThe scorer determines whether each pair of hidden vectors (Hw, Hh) forms a dependency. We employ per-word normalization of scores: for a given word locationw \u2208 1, 2, . . . , n scores are SoftMax-normalzied over all head locations h \u2208 0, 1, 2, . . . , n.\nThe labeler reads a pair of hidden vectors (Hw, Hh) and predicts the label of this dependency edge. During training we use the ground-truth head location, while during inference we use the location predicted using the scorer.\nWe employ the following training criterion:\nL = \u03b1hLh + \u03b1lLl + \u03b1tLt\n, where Lh, Ll, Lt are negative log-likelihood losses of the scorer, the labeler and POS tag predictor, respectively."}, {"heading": "4 Experiment Details and Results", "text": ""}, {"heading": "4.1 Model Hyperparameters", "text": "We have decided to use the same set of hyperparameters for all languages and multilingual parsers, which were a compromise in model capacity for languages that had small and large treebanks. The reported size of recurrent layers is slightly too big for low-resources single-language parser, but we have determined that it is optimal for languages with large treebanks and for multilingual training.\nThe reader embeds each character into vector of size 15, and contains 1050 filters (50\u00b7k filters of length k for k = 1, 2,. . . , 6) whose outputs are projected into 512- dimensional vector transformed by a 3 equally sized layers of feedforward neural network with ReLU activation. Unlike [21,12] we decided to remove Highway layers [31] from the reader. Their usage introduced a marginal accuracy gain, while nearly doubling the computational burden. The tagger contains 2 BiRNN layers of GRU units with 548 hidden states for both forward and backward passes which are later aggregated using addition [12]. Therefore the hidden states of the tagger are also 548-dimensional. The POS tag predictor consists of a single affine transformation followed by a SoftMax predictor for each POS category. The scorer uses a single layer of 384 tanh for head word scoring while the labeller uses 256 Maxout units (each using 2 pieces) to classify the relation label [17]. The training cost used the constants \u03b1h = 0.6, \u03b1l = 0.4, \u03b1t = 1.0.\nWe regularize the models using Dropout [30] applied to the reader output (20%), between the BiRNN layers of the tagger (70%) and to the labeller (50%). Moreover we apply mild weight decay of 0.95.\nWe have trained all models using the Adadelta [36] learning rule with epsilon annealed from 1e-8 to 1e-12 and adaptive gradient clipping [11]. Experiments are earlystopped on validation set Unlabeled Attachment Score (UAS) score. Unfortunately, due to limited computational resources we are only able to present the results for a subset of the UD treebanks that are shown in Table 1.\nMultilingual models use the same architecture. We unify the inputs and outputs of all models by taking the union of all possible token categories (characters, POS categories, dependency labels). If some category does not exist within a particular language we use a special UNK token. All parsers are trained in parallel minimizing a sum of their individual training costs. We use early-stopping on the main (first) language UAS score. We equalize training mini-batches such that each contains the same number of sentences from all languages. We determined the optimal amount of parameter sharing and show it in Table 2. Moreover, we never share the start-of-word and end-of-word tokens to indicate to the network which language is parsed."}, {"heading": "4.2 Main Results", "text": "Our results on single language training are presented in Table 1. Our models reach better scores than the highly tuned SyntaxNet transition-based parser [3] and are competitive with the DRAGNN based ParseySaurus which also uses character-based input [1].\nMultilingual training (Table 2) improves the performance on low-resource languages. We observe that the optimal amount of parameter sharing depends on the similarity\nbetween languages and corpus size \u2013 while it is beneficial to share all parameters of the PL-CZ and RU-CZ parser, the PL-RU parser works best if the reader subnetworks are separated. We attribute this to the quality of Czech treebank which has several times more examples than Polish and Russian datasets combined."}, {"heading": "4.3 Analysis of Language Similarities Identified by the Network", "text": "We have first analyzed whether a PL-RU parser can learn the correspondence between Latin and Cyrillic scripts2. We have inspected the reader subnetworks of a PL-RU parser that shared all parameters. As described in Section 3, the model begins processing a word by finding the embedding of each character. For the analysis we have extracted the embeddings associated with all Polish and Russian characters. We have paired Polish and Russian letters which have similar pronunciations. We note that the pairing omits\n2 Conveniently, the Unicode has separate codes for Latin and Cyrillic letters.\nAdapting the famous equation king \u2212 man + woman \u2248 queen [24] we inspected to what extent our network was able to deduce Latin-Cyrillic correspondences. For all distinct pairs (p1 \u2212 r1, p2 \u2212 r2) of letter correspondences we computed the vector C(p2)\u2212 C(p1) + C(r1), where C stands for char embedding, and found Russian letter which had the closest embedding vector. In 48.3% cases we choose the right vector. We found it quite striking given that the two languages have separated from their common root (Proto-Slavic) more than 1000 years ago. Moreover, relations between Polish and Russian letters are side effects, not the main objective of the neural network.\nWe have also examined word representations Ew computed for Polish and Russian by the shared reader subnetwork. As one could expect, the network was able to realize that in these languages morphology is suffix based. However, the network was also able to learn that words built from different letters can behave in similar way. We can observe it in both monolingual or multilingual context. Table 3 shows some Polish adjectives and the top-7 Russian words with the closest embedding. All Russian words which are not italics have the same morphological tags as the Polish word. In the first row we can observe 2 suffixes -sko (skoy) and -nno (nnoy) quite distant from polish -owej (ovey). In the second row we see that the model was able to correctly alias the Polish 3-letter suffix -ych with the Russian 2 letter suffix -yh which are pronounced the same way. The relation found by the network is purely syntactical \u2013 there is no easy-to-find connection between semantics of these words."}, {"heading": "4.4 Common Error Analysis", "text": "We have investigated two possible sources of errors produced by the parser. First, we verified if using a more advanced tree-building algorithm was better than using a greedy one. We have observed that the scorer produces very sharp probability distributions that can be transformed into trees using a greedy algorithm that simply selects for each word the highest scored head [12,13]. Counterintuitively, the Chu-Liu-Edmonds (CLE) maximum spanning tree algorithm [16] often makes the decoding results slightly worse. We have established that the network is so confident in its predictions that non-top scores\ndo not reflect alternatives but are only noise. Therefore when the greedy decoding creates a cycle the CLE usually breaks it in a wrong place introducing another pointer error.\nWe have used the POS predictor to pinpoint which parts of the network (reader/tagger or labeler/scorer) were responsible for errors. Tests showed that if the predicted tag was wrong, the scorer and labeler will nearly always produce erroneous results too."}, {"heading": "5 Conclusions and Future Works", "text": "We have demonstrated a graph-based dependency parser implemented as a single deep neural network that directly produces parse trees from characters and does not require other NLP tools such as a POS tagger. The proposed parser can be easily used in a multilingual setup, in which parsers for many languages that share parameters are jointly trained. We have established that the degree of sharing depends on language similarity and corpus size: the best PL-CZ parser and RU-CZ shared all parameters (essentially creating a single parser for both languages), while the best PL-RU parser had separate morphological feature detectors (i.e. readers). We have also determined that the network can extract meaningful relations between languages, such as approximately learning a mapping from Latin to Cyrillic characters or associate Polish and Russian words that have a similar grammatical function. While this contribution focused on improving the performance on a low-resource language using data from another languages, similar parameter sharing techniques could be used to create one universal parser [2].\nWe have performed qualitative error analysis and have determined to regions for possible future improvements. First, the network does not indicate alternatives to the produced parse tree. Second, errors in word interpretation are often impossible to correct by the upper layers of the network. In the future we plan to investigate training a better POS tagging subnetwork possibly using other sources of data.\nAcknowledgments. The experiments used Theano [7], Blocks and Fuel [23] libraries. The authors would like to acknowledge the support of the following agencies for research funding and computing support: National Science Center (Poland) grant Sonata 8 2014/15/D/ST6/04402, National Center for Research and Development (Poland) grant Audioscope (Applied Research Program, 3rd contest, submission no. 245755)."}], "references": [{"title": "SyntaxNet Models for the CoNLL 2017 Shared Task", "author": ["C Alberti"], "venue": "arXiv:1703.04929", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Many Languages, One Parser", "author": ["W Ammar"], "venue": "Transactions of the Association for Computational Linguistics 4(0), 431\u2013444", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Globally Normalized Transition-Based Neural Networks", "author": ["D. Andor", "C. Alberti", "D. Weiss", "A. Severyn", "A. Presta", "K. Ganchev", "S. Petrov", "M. Collins"], "venue": "arXiv:1603.06042 [cs]", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv:1409.0473 [cs, stat]", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["M. Ballesteros", "C. Dyer", "N.A. Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "On achieving and evaluating language-independence in nlp", "author": ["E.M. Bender"], "venue": "Linguistic Issues in Language Technology 6(3), 1\u201326", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J Bergstra"], "venue": "Proc. SciPy", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning 28(1), 41\u201375", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["D. Chen", "C.D. Manning"], "venue": "EMNLP. pp. 740\u2013750", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K Cho"], "venue": "CoRR abs/1406.1078", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv:1412.1602 [cs, stat]", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Read, tag, and parse all at once, or fullyneural dependency parsing", "author": ["J. Chorowski", "M. Zapotoczny", "P. Rychlikowski"], "venue": "CoRR abs/1609.03441", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["T. Dozat", "C.D. Manning"], "venue": "CoRR abs/1611.01734", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "A neural network model for low-resource universal dependency parsing", "author": ["L. Duong", "T. Cohn", "S. Bird", "P. Cook"], "venue": "EMNLP. pp. 339\u2013348. Citeseer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["C. Dyer", "M. Ballesteros", "W. Ling", "A. Matthews", "N.A. Smith"], "venue": "arXiv preprint arXiv:1505.08075", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimim Branchings", "author": ["J. Edmonds"], "venue": "JOURNAL OF RESEARCH of the National Bureau of Standards - B. 71B(4), 233\u2013240", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1966}, {"title": "Maxout Networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML. pp. 1319\u20131327", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["J. Guo", "W. Che", "D. Yarowsky", "H. Wang", "T. Liu"], "venue": "ACL (1). pp. 1234\u20131244", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Paralell Distributed Processing: Explorations in the microstructure of cognition", "author": ["G.E. Hinton", "J.L. McClelland", "D.E. Rumelhart"], "venue": "Volume 1: Foundations. MIT Press/Bradford Books", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1986}, {"title": "Exploring the Limits of Language Modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "arXiv:1602.02410 [cs]", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations", "author": ["E. Kiperwasser", "Y. Goldberg"], "venue": "arXiv:1603.04351 [cs]", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["B van Merri\u00ebnboer"], "venue": "arXiv:1506.00619 [cs, stat]", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS. pp. 3111\u20133119", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "Makuhari, Chiba, Japan", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithms for Deterministic Incremental Dependency Parsing", "author": ["J. Nivre"], "venue": "Comput. Linguist. 34(4), 513\u2013553", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "MaltParser: A language-independent system for data-driven dependency parsing", "author": ["J Nivre"], "venue": "Natural Language Engineering p. 1", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal Dependencies 1.2. http://universaldependencies.github.io/docs/ (Nov 2015", "author": ["J Nivre"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing 45(11), 2673\u20132681", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR 15, 1929\u20131958", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway Networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv:1505.00387 [cs]", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "arXiv preprint arXiv:1409.3215", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "A latent variable model for generative dependency parsing", "author": ["I. Titov", "J. Henderson"], "venue": "In Proceedings of IWPT", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Grammar as a Foreign Language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "arXiv:1412.7449 [cs, stat]", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["Y Wu"], "venue": "arXiv: 1609.08144", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv:1212.5701", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Dependency parsing as head selection", "author": ["X. Zhang", "J. Cheng", "M. Lapata"], "venue": "CoRR abs/1606.01280", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Recent state-of-the-art results were obtained with parsers implemented using deep neural networks [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 27, "context": "0 treebank collection [28] there are 863 annotated Ukrainian sentences, 333 Belarusian, but nearly 60k Russian ones (divided into two sets: a default one of 4.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "Since these languages belong to the same Slavic language family, performance on the low resource languages should improve by joint training the model also on a better annotated language [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 26, "context": "transitionbased parsers used linear SVMs [27,26].", "startOffset": 41, "endOffset": 48}, {"referenceID": 25, "context": "transitionbased parsers used linear SVMs [27,26].", "startOffset": 41, "endOffset": 48}, {"referenceID": 32, "context": "Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3].", "startOffset": 90, "endOffset": 101}, {"referenceID": 8, "context": "Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3].", "startOffset": 90, "endOffset": 101}, {"referenceID": 14, "context": "Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3].", "startOffset": 90, "endOffset": 101}, {"referenceID": 2, "context": "Recently, those simple learning models were successfully replaced by deep neural networks [33,9,15,3].", "startOffset": 90, "endOffset": 101}, {"referenceID": 24, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 98, "endOffset": 105}, {"referenceID": 19, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 98, "endOffset": 105}, {"referenceID": 3, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 122, "endOffset": 131}, {"referenceID": 31, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 122, "endOffset": 131}, {"referenceID": 34, "context": "This trend coincides with successes of those models on other NLP tasks, such as language modeling [25,20] and translation [4,32,35].", "startOffset": 122, "endOffset": 131}, {"referenceID": 33, "context": "For example a constituency parser can be implemented using a sequence-to-sequence network originally developed for translation [34].", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "Both are easily solved using neural networks [22,37,13,12].", "startOffset": 45, "endOffset": 58}, {"referenceID": 36, "context": "Both are easily solved using neural networks [22,37,13,12].", "startOffset": 45, "endOffset": 58}, {"referenceID": 12, "context": "Both are easily solved using neural networks [22,37,13,12].", "startOffset": 45, "endOffset": 58}, {"referenceID": 11, "context": "Both are easily solved using neural networks [22,37,13,12].", "startOffset": 45, "endOffset": 58}, {"referenceID": 23, "context": "Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].", "startOffset": 165, "endOffset": 169}, {"referenceID": 20, "context": "Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].", "startOffset": 217, "endOffset": 226}, {"referenceID": 4, "context": "Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].", "startOffset": 217, "endOffset": 226}, {"referenceID": 11, "context": "Moreover, neural networks can extract meaningful features from the data, which may augment or replace manually designed ones, as it is the case with word embeddings [24] or features derived from the spelling of words [21,5,12].", "startOffset": 217, "endOffset": 226}, {"referenceID": 18, "context": "Another particularly nice property of neural models is that all internal computations use distributed representations of input data that are embedded in highly dimensional vector spaces [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 7, "context": "These internal representation can be easily shared between tasks [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2].", "startOffset": 102, "endOffset": 113}, {"referenceID": 17, "context": "Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2].", "startOffset": 102, "endOffset": 113}, {"referenceID": 13, "context": "Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2].", "startOffset": 102, "endOffset": 113}, {"referenceID": 1, "context": "Likewise, neural parsers can share some of their parameters to harness similarities between languages [6,18,14,2].", "startOffset": 102, "endOffset": 113}, {"referenceID": 27, "context": "Creation of multilingual parsers is further facilitated by the introduction of standardized treebanks, such as the Universal Dependencies [28].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "Each of the n parsers is a single neural network that directly reads a sequence of characters and finds dependency edges along with their labels [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "We use a multi-layer bidirectional GRU Recurrent Neural Network (BiRNN) [10,29].", "startOffset": 72, "endOffset": 79}, {"referenceID": 28, "context": "We use a multi-layer bidirectional GRU Recurrent Neural Network (BiRNN) [10,29].", "startOffset": 72, "endOffset": 79}, {"referenceID": 20, "context": "Unlike [21,12] we decided to remove Highway layers [31] from the reader.", "startOffset": 7, "endOffset": 14}, {"referenceID": 11, "context": "Unlike [21,12] we decided to remove Highway layers [31] from the reader.", "startOffset": 7, "endOffset": 14}, {"referenceID": 30, "context": "Unlike [21,12] we decided to remove Highway layers [31] from the reader.", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "The tagger contains 2 BiRNN layers of GRU units with 548 hidden states for both forward and backward passes which are later aggregated using addition [12].", "startOffset": 150, "endOffset": 154}, {"referenceID": 16, "context": "The scorer uses a single layer of 384 tanh for head word scoring while the labeller uses 256 Maxout units (each using 2 pieces) to classify the relation label [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "We regularize the models using Dropout [30] applied to the reader output (20%), between the BiRNN layers of the tagger (70%) and to the labeller (50%).", "startOffset": 39, "endOffset": 43}, {"referenceID": 35, "context": "We have trained all models using the Adadelta [36] learning rule with epsilon annealed from 1e-8 to 1e-12 and adaptive gradient clipping [11].", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "We have trained all models using the Adadelta [36] learning rule with epsilon annealed from 1e-8 to 1e-12 and adaptive gradient clipping [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 2, "context": "Our models reach better scores than the highly tuned SyntaxNet transition-based parser [3] and are competitive with the DRAGNN based ParseySaurus which also uses character-based input [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "Our models reach better scores than the highly tuned SyntaxNet transition-based parser [3] and are competitive with the DRAGNN based ParseySaurus which also uses character-based input [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 1, "context": "[2] uses version 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "SyntaxNet[3,1] works on predicted POS tags, while ParseySaurus[1] uses word spellings.", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "SyntaxNet[3,1] works on predicted POS tags, while ParseySaurus[1] uses word spellings.", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "SyntaxNet[3,1] works on predicted POS tags, while ParseySaurus[1] uses word spellings.", "startOffset": 62, "endOffset": 65}, {"referenceID": 23, "context": "Adapting the famous equation king \u2212 man + woman \u2248 queen [24] we inspected to what extent our network was able to deduce Latin-Cyrillic correspondences.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "We have observed that the scorer produces very sharp probability distributions that can be transformed into trees using a greedy algorithm that simply selects for each word the highest scored head [12,13].", "startOffset": 197, "endOffset": 204}, {"referenceID": 12, "context": "We have observed that the scorer produces very sharp probability distributions that can be transformed into trees using a greedy algorithm that simply selects for each word the highest scored head [12,13].", "startOffset": 197, "endOffset": 204}, {"referenceID": 15, "context": "Counterintuitively, the Chu-Liu-Edmonds (CLE) maximum spanning tree algorithm [16] often makes the decoding results slightly worse.", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "While this contribution focused on improving the performance on a low-resource language using data from another languages, similar parameter sharing techniques could be used to create one universal parser [2].", "startOffset": 205, "endOffset": 208}, {"referenceID": 6, "context": "The experiments used Theano [7], Blocks and Fuel [23] libraries.", "startOffset": 28, "endOffset": 31}, {"referenceID": 22, "context": "The experiments used Theano [7], Blocks and Fuel [23] libraries.", "startOffset": 49, "endOffset": 53}], "year": 2017, "abstractText": "We show that a recently proposed neural dependency parser can be improved by joint training on multiple languages from the same family. The parser is implemented as a deep neural network whose only input is orthographic representations of words. In order to successfully parse, the network has to discover how linguistically relevant concepts can be inferred from word spellings. We analyze the representations of characters and words that are learned by the network to establish which properties of languages were accounted for. In particular we show that the parser has approximately learned to associate Latin characters with their Cyrillic counterparts and that it can group Polish and Russian words that have a similar grammatical function. Finally, we evaluate the parser on selected languages from the Universal Dependencies dataset and show that it is competitive with other recently proposed state-of-the art methods, while having a simple structure.", "creator": "LaTeX with hyperref package"}}}