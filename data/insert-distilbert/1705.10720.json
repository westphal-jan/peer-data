{"id": "1705.10720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Low Impact Artificial Intelligences", "abstract": "there are many goals for an ai that alone could become dangerous if the ai becomes superintelligent or otherwise powerful. much work on the ai control problem has been focused on constructing ai goals that are safe even removed for such ais. this paper looks further at an alternative approach : defining a general concept of ` low impact '. the aim is to usually ensure that administering a powerful ai which implements low impact will not essentially modify the world extensively, even if it is given a simple or dangerous goal. the paper proposes various ways indicative of defining and grounding low impact, and discusses methods for ensuring that the ai can still be allowed to always have a ( desired ) sustained impact despite the restriction. here the end of the paper addresses known issues with this approach and avenues for future research.", "histories": [["v1", "Tue, 30 May 2017 16:15:16 GMT  (1174kb,D)", "http://arxiv.org/abs/1705.10720v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["stuart armstrong", "benjamin levinstein"], "accepted": false, "id": "1705.10720"}, "pdf": {"name": "1705.10720.pdf", "metadata": {"source": "CRF", "title": "Low Impact Artificial Intelligences", "authors": ["Stuart Armstrong", "Benjamin Levinstein"], "emails": ["stuart.armstrong@philosophy.ox.ac.uk;", "balevinstein@gmail.com"], "sections": [{"heading": null, "text": "Keywords: low impact, AI, motivation, value, control"}, {"heading": "1 Introduction", "text": "Imagine an artificial intelligence that has been given a goal such as \u2018make paperclips\u2019, \u2018filter spam in this account\u2019, or \u2018cure this person\u2019s cancer\u2019. If this AI is not very powerful, it is likely to attempt to achieve its goals in the ways we intend: improving industrial production, analysing and selectively filtering incoming messages, or looking for compounds able to differentially attack cancer cells.\nIf the AI becomes very powerful, however, these goals all become problematic [Bos14]. The goal \u2018make paperclips\u2019 is perfectly compatible with a world in which the AI expands across the Earth, taking control of its resources to start an intense mass production of paperclips, while starting to launch colonisation projects for the other planets to use their resources for the same purposes, and so on. In fact, a naive version of the goal \u2018make paperclips\u2019 mandates such actions. Similarly, \u2018filter spam\u2019 is compatible with shutting down the internet entirely,\n\u2217Email: stuart.armstrong@philosophy.ox.ac.uk; Corresponding author \u2020Email: balevinstein@gmail.com\nar X\niv :1\n70 5.\n10 72\n0v 1\n[ cs\n.A I]\n3 0\nand \u2018cure this person\u2019s cancer\u2019 is compatible with killing her and destroying all the cells in her body.\nThere are several proposed approaches to combat this issue. The most standard is to add something to the goal, fleshing it out so that it includes safety components (\u2018and don\u2019t kill anyone, or inadvertently cause their deaths, or...\u2019). As the AI\u2019s power increases, its potential influence over the world increases as well, and the safety components need to be fleshed out ever more (\u2018...and don\u2019t imprison people, or cause a loss of happiness or perceived liberty or free will, or...\u2019). The \u2018Friendly AI\u2019 approach aims roughly to specify these safety components in as much specific detail as possible [Yud08]. Other approaches aim to instil the these components via implicit or explicit learning and feedback [Dew11, Arm15].\nThis paper takes a different tack. Instead of specifying the safety components, it aims to ensure AI has a low impact on the world. Given this low impact, many otherwise unsafe goals become safe even with a very powerful AI. Such an AI would manufacture a few more paperclips/filter a few more messages/kill a few cancer cells, but would otherwise not take any disruptive action.\nThe first challenge is, of course, to actually define low impact. Any action (or inaction) has repercussions that percolate through the future light-cone, changing things subtly but irreversibly. It is hard to capture the intuitive human idea of \u2018a small change\u2019.\nThere are a few intuitive ways in which an action can have a low impact, though, which we examine in some depth in Section 3. For example, if we can describe the universe in terms of a huge number of disparate but well-chosen variables and the action has little impact on their values, then it was not of high impact. We can also assess whether knowing the action is particularly \u2018important\u2019 in terms of predicting the future, or whether we can see if the actions are likely to be detectable at a later date. If the action is such that any difference to the universe is lost in entropy or absorbed into a chaotic and unpredictable process, it certainly has a low impact. Finally, we can also abstractly compare the features of probability distributions of future worlds given the action or not.\nThe second challenge, tackled in Section 4, is to figure out how to ensure that the AI\u2019s impact is not too low \u2013 that we can still get useful work out of the AI, without risking a larger or negative impact. Although low impact seems to preclude any action of significance on the part of the AI, there are a number of ways around this limitation. Unlike the bad AI impacts that we are trying to prevent, we can often have a much clearer idea of the positive impact we are attempting to accomplish. This allows us to construct various specific setups and targeted loopholes to allow specific high impact from otherwise low impact AIs.\nThe final, brief, section looks at some of the problems and unresolved issues with the current setup \u2013 and hence the directions for future research."}, {"heading": "2 The General Framework", "text": ""}, {"heading": "2.1 The penalty function", "text": "Although determining what exactly counts as \u2018impact\u2019 will be a thorny issue, we can nonetheless characterise the approach abstractly. The basic idea is that\nthe AI has some active goal, such as cure cancer or filter spam, but it wants to pursue this goal without changing the world in any important way. We can then describe its utility function as follows:\nU = u\u2212 \u00b5R. (1)\nThe function u is a standard utility function that gives the AI its active goal. The function R is the penalty function, penalising the AI for having a large impact. The number \u00b5 is some scaling factor, setting the importance of low impact relative to the AI\u2019s active goal u.\nIn order to prevent the AI accepting a large R penalty in exchange for a large u gain, we will want to define a bounded u, such that performance close to the maximum bound is not too difficult to obtain. There is no such bound on R, of course: the more impact the AI has, the more it gets penalised.1"}, {"heading": "2.2 Defining the alternatives", "text": "To define low impact, we first will need a baseline for comparison. What is a low impact, as opposed to a non-low one? The most natural alternative, and the one we\u2019ll use, is the world in which the AI is never successfully turned on in the first place, or, to be more precise: some prior probability distribution P over the set of worlds W conditional on the AI not having been turned on. An AI that was never turned on is assumed to have very low impact; the behaviour of the active AI is compared with this baseline.\nFor such a distribution to make sense, we\u2019ll assume the turning on of the AI does not occur with probability 1. For instance, we can make it depend on a signal going through a specific wire, which goes through some noisy environment \u2013 say, an unstable gas \u2013 that has a tiny chance of interrupting the signal. If the signal is interrupted, the AI is not turned on.\nDesignate the event of the signal passing through the wire as X (which then turns on the AI), and the signal being blocked as \u00acX (which then leaves the AI off). We then want the AI to have a low impact on the world, with a low impact being informally defined as the world, conditional on X, being \u2018not too dissimilar\u2019 from the world, conditional on \u00acX. The purpose of this paper is to make this definition rigorous2.\nSince conditional probability does not depend on the probability of the event being conditioned on, we would want the probability of X to be almost 1 \u2013 the AI is almost certain to be turned on. This is consistent with the fact that, having built an AI, the designers would certainly want to make use of it3.\n1Note that equation (1), if it works as intended, makes the AI relatively transparent to humans. The active goal u can be simple, while the intent of low impact is clear.\n2Instead of comparing the world conditional on X with the world conditional on \u00acX, it may be desirable to consider various kinds of subjunctive or counterfactual suppositions instead if we\u2019re especially interested in, say, X\u2019s expected causal consequences and not in its evidential impact. For the sake of simplicity, however, we\u2019ll restrict attention to the standard notion of conditional probability instead in this report.\n3There are many subtleties here, the main ones of being that if \u00acX happens, humans are likely to question their probability estimates, and will likely just try turning it on again. Humans concluding that something is wrong can be dealt with via methods similar to http://lesswrong.com/r/discussion/lw/lxl/humans_get_different_counterfactuals/. Preventing the AI from taking into account future human attempts to turn it on again could be addressed by adapting the methods proposed in http://lesswrong.com/r/discussion/lw/"}, {"heading": "2.3 Boxing the AI", "text": "One useful precaution is to \u2018box\u2019 the AI, confining it to a sealed environment somewhat isolated from the outside world. Unlike other boxing suggestions [ASB12, Bos14, Yam12], the aim here is not to prevent the AI from having an impact by confining it. Rather, it is to minimise the difference between worlds with X and \u00acX. The box is not to prevent the AI from acting (the definition of low impact will ensure that), but rather to prevent us from seeing that the AI is active. Thus any visible impact of the AI will come only from via the AI\u2019s decisions, rather than from it\u2019s mere existence."}, {"heading": "3 Different measures of low impact", "text": ""}, {"heading": "3.1 The fundamental challenge", "text": "The fundamental challenge of developing the low impact approach is that, from a physical point of view, everything done or not done has an impact (a probabilistic one, at least), across all possible futures. Therefore the world, given X, must be fundamentally different from the world, given \u00acX, simply because of the difference between X and \u00acX. Particles will be arranged differently in these different scenarios, and this effect will compound the further we get from the event.\nWhat we\u2019re really interested in \u2018no important impact\u2019. It matters little if the atoms that define the US president are a little to the left of where they would be otherwise. It matters a lot that the AI doesn\u2019t escape from its box, eliminate all humans and optimise the universe in the service of goal u.\nA second component of this challenge is that the AI may end up creating an importantly different world, simply because some processes such as the weather are chaotic and X versus \u00acX change the initial conditions. Yet in these cases, its impact should nonetheless count as low. Suppose, for instance, the upcoming election would be very close. The AI makes breakfast on election day for a capricious and unpredictable voter who ends up voting for Alice instead of Bob in part because he had apricots instead of biscuits to eat. As a result, Alice is elected instead of Bob. Although the effect of the breakfast decision is large, it ought not be considered \u2018high impact\u2019, since if an election was this close, it could be swung by all sorts of minor effects. We\u2019ll therefore investigate probabilistic approaches: what sorts of changes can be predicted ex ante if X is true?"}, {"heading": "3.2 Coarse graining: Twenty billion questions", "text": "One way to solve the fundamental challenge is first to find a way of \u2018coarsegraining\u2019 the set of worlds. That is, we partition the set of worlds into small cells, and any two elements of the same cell count as equivalent for our purposes. Generally these cells will be determined by the values of certain variables or characteristics. We can then measure impact in terms of the AI\u2019s expected effects over this partition. Thus if A is any element of this partition, we typically\nluy/acaucal_trade_barriers/. But a detailed discussion of this is beyond the scope of this current report.\nexpect the probabilities P (A|X) and P (A|\u00acX) to be close.4 The purpose of coarse graining is to define the world in sufficient detail that the AI cannot have a large impact without disrupting most of those details. The characteristics used must be as broad and as diverse as possible, making it impossible for the AI to grain great power without disrupting some of them.5 For instance, we could use the air pressure in Dhaka, the average night-time luminosity at the South Pole, the rotational speed of Io, and the closing numbers of the Shanghai stock exchange. To be confident that we can sufficiently constrain a super-intelligent AI, we\u2019ll need millions if not billions of these variables, separating the universes into uncountable numbers of different cells6.\nOf course, we have to take care in deciding which characteristics to use. This procedure doesn\u2019t work if some variables are too directly connected with X or \u00acX. The electricity consumption of the AI, for example, cannot be a variable, but the electricity consumption of the entire county could be if the AI\u2019s usage is likely to be lost in the noise. But we can hope that for \u201clarge scale\u201d issues, that the universe is \u201croughly\u201d unchanged given X and \u00acX.\nFor any world w, we can define a world vector Vw which is the values taken by the billions of chosen variables7. We can then make use of vectors to coarse grain the space of worlds, defining an equivalence relation:\nv \u223c= w iff Vv = Vw\nThe cells are the equivalence classes of this relation. This allows us to make statements like \u2018the probability of w is equal whether or not the AI was turned on\u2019. Without the course graining, P (w|X) = 0 or P (w|\u00acX) = 0, depending on whether or not w contained the AI. But with the coarse graining, the statement becomes:\nP (Vw|X) = P (Vw|\u00acX)\nAnd, as long as those variable values are possible given X and given \u00acX, the above formula makes sense.\nThen we can formulate low impact as some sort of measure of the difference between the expected worlds given X and \u00acX. The l\u221e norm, for instance, could work:\nR = max w |P (Vw|X)\u2212 P (Vw|\u00acX)| (2)\nThe \u2018box\u2019 defined by this norm is illustrated in figure 1. \u2018Softer\u2019 versions of this maximum norm could work as well.\n4Note that the AI\u2019s limitations make this approach easier. If the AI has difficulty predicting A precisely (maybe because A is chaotic or because of the fundamental entropy limitations on information), then those two probabilities are likely to be closer than otherwise. This helps to distinguish small inadvertent impacts the AI might have from the larger deliberate ones we want to avoid.\n5It\u2019s important to note that an extremely powerful AI could potentially gain control of the world and set the characteristics to whatever values it wants. Therefore, the purpose of the coarse graining approach is to catch the AI before it initiates sweeping changes to the world.\n6Though not uncountable in the sense of an uncountable infinity, the number of cells is exponential in the number of characteristics/variables, and is thus easily larger than the number of particles in the reachable universe. Therefore, as we add variables, the difficulty for the AI to outsmart us and find an unintended high-impact way of maximising u should grow fairly rapidly, since all constraints must be met simultaneously.\n7Continuous variables will need a sensible binning.\nAn important feature of the approach is that we have no need to measure these billion variables precisely. They are simply used to define the coarse graining, with the AI then trying to get its best estimate of R. Nor do they need to perfectly define the variables: the closing numbers of the Shanghai stock exchange could be defined as a specific output along a specific wire. In worlds where the stock exchange didn\u2019t exist, that input would correspond to something else entirely \u2013 maybe blank signals, maybe noise. But this shouldn\u2019t matter, as it still coarse grains \u2018worlds with stock exchanges emitting along this wire, given X\u2019 together in the same cell with \u2018worlds with stock exchanges emitting along this wire, given \u00acX\u2019 and similarly for worlds without8."}, {"heading": "3.3 The importance of knowing X", "text": "A related option is to measure specifically the informational value of the AI on things we might care about. The intuitive idea is that if the AI is high impact, then knowing whether or not the AI was turned on tells you a lot more about the future of the world than another generic piece of information. We can then set the penalty R to measure the importance of X/\u00acX with respect to other pieces of evidence. If the AI has truly low impact, then if presented with a large number of facts about the world, X will not stand out as being more informative than other facts.\nHow would we measure importance? One approach is to look at a large set U of different utility functions, and check that knowing X makes little expected difference for any of them9.\n8Note that this is in contrast with more traditional AI approaches, where the terms have to be correctly defined [Yud08] or grounded [Har90]. The purpose is not to have the meaning of each variable defined precisely, but to capture so many variables that they limit the AI\u2019s ability to have large impacts.\n9One method for choosing U is to have the AI or some other superintelligence select a class of utility functions that it thinks is likely to contain the set of actual or potential human\nWhen measuring importance, we could either check the expected difference of knowing X, or the expected importance of other facts, conditional on knowing X. More formally, let F be a large set of facts compatible with both X and \u00acX, and define:\nR = max u\u2032\u2208U,S\u2282F\n|E(u\u2032|S \u222aX)\u2212 E(u\u2032|S \u222a \u00acX)|"}, {"heading": "3.4 Undetectable means unimpactful?", "text": "This sections looks at another way of defining low impact: undetectability. If, during the 19th century, there was an inhabitant of London, and there now remains no record whatsoever of their existence, it is likely that they had a very low impact. Presume that we are given a partial description of them, and asked, from the point of view of today and all the evidence we have, whether they existed or not. If we really cannot tell either way, this seems a definitional example of low impact: their potential impact is literally lost in the noise of the world10.\nHow could this be formalised? Suppose that the AI uses a probability estimator P \u2032. This estimator is bounded by practical concerns, as any real-world agent can only have bounded rationality. Its unbounded equivalent is P . And these estimators have some level of reflection [CYHB13], so P \u2032 can express some estimates over the outcome of P \u2032 (self-reflection) and P (modelling of a superior rational agent). Let b designate background information about the world (prior to X), and let G designate the set of events in a slice S of event X/\u00acX\u2019s future light-cone \u2013 maybe the slice 50 (time-like) years in the future.\nThen what we would want is that P (g|X, b) = P (g|\u00acX, b) for g \u2208 G \u2013 that the likely future is identical whether or not the AI is turned on or not (as assessed by the unbounded version of the agent, with access to all of S). If we designate the expectation estimator derived from P \u2032 as E\u2032, this reduces to wanting:\nR = E\u2032(|P (g|X, b)\u2212 P (g|\u00acX, b)|) = 0.\nThis expectation is both over P (the AI\u2019s estimate of its unbounded estimator) and over the likely g \u2208 G (the likely future; this also depends on the AI\u2019s policy choices).\nThis works if the impact of the AI is really lost through entropy, such that even P cannot detect it. However, an unbounded agent with full access to the slice G, might be able to always detect the presence of an AI, no matter how low impact it attempts to have.\nutility functions, or some near enough approximation. That is, we want U more or less to contain representations of everything humans may really care about. If the AI does not, in expectation, affect the elements of U that much, then it counts as low impact in the desired sense of the term. Just as in the coarse graining case, we\u2019d want to add a variety of other utilities to U , in case there are vulnerabilities that have not occurred to us. We of course will need to exclude utility functions that care directly about whether X for this approach to work.\n10Of course, some care must still be taken when we decide how to describe this person. For instance, we can\u2019t tell now whether there really was somebody called \u2018Jack the Ripper\u2019 who was a serial killer and had exactly 100,000 hairs on his head at midnight on 1 January 1888. The more formal development of this approach below will eliminate this issue.\nIn that situation, we can consider the change in P as it\u2019s fed information from G, one bit at a time. When P has all of G, it will detect the presence of the AI, but before that point, it will have doubts. An AI that optimises the world is likely to leave many traces all over G; one that changes little will leave corresponding few choices. For 0 \u2264 \u03c1 \u2264 1, designate by g\u03c1 a random sampling of G where we have randomly chosen a proportion \u03c1 of all the data in G. We can designate a new measure of R:\nR = min \u03c1\n{ E\u2032 ( P (g\u03c1|X, b) P (g\u03c1|\u00acX, b) ) > 10, or E\u2032 ( P (g\u03c1|\u00acX, b) P (g\u03c1|X, b) ) > 10 } (3)\nHere R is a measure of how much information P is likely to need before detecting the difference between X and \u00acX (\u2018difference\u2019 being somewhat arbitrarily defined as the expected ratio of probabilities, or inverse ratios, being higher than 10)."}, {"heading": "3.5 Comparing distributions abstractly", "text": "To avoid some of the issues and choices needed for the options discussed so far, we can attempt to compare the two conditional probability distributions P (\u2212|X) and P (\u2212|\u00acX) abstractly. The intuition behind this is that if we have a low impact AI or no AI at all, the probability distribution over future worlds is likely spread far and wide across many different options, but if there is a powerful unconstrained AI, the future is likely to be highly optimised causing P (\u2212|X) to take on a different shape. We can therefore compare the conditional distributions by looking at various descriptors they might have \u2013 means, variances, peaks, and so on. Such descriptors depend on the underlying metric on the set of worlds W, and so ideally these descriptors will be very similar for a wide variety of natural metrics we could choose.\nThe weakness of this approach is that we have an extremely reduced understanding of what is going on in the comparison, so we don\u2019t know if the AI can produce a distribution P (W|X) that is formally similar to P (W|\u00acX), but in practice is a very concentrated, u-optimised world. However, perhaps with a sufficiently robust statistical characterisation of the relevant distributions, this won\u2019t be an issue."}, {"heading": "3.5.1 Generalised Cross-Entropy", "text": "One natural abstract way of measuring the expected impact of X is to compare the divergence between P (W|X) in terms of P (W|\u00acX). If the two distributions are relatively close, then X likely does not have an especially large impact.\nUnfortunately, it\u2019s not obvious what particular measure of divergence we ought to use. Kullback-Leibler divergence\u2014the standard measure\u2014won\u2019t work in this case. Let PX = P (W|X) and P\u00acX = P (W|\u00acX). PX(X) = 1 and P\u00acX(X) = 0, so DKL(P\u00acX ||PX) =\u221e.\nThere are, however, other measures of generalised entropy and divergence that are bounded and may be able to do the job. Bounded Bregman-divergences, for instance, are often used to quantify the amount of generalised information needed to move from one probability function to another.11 Whether such an approach will work for our purposes remains to be seen.\n11The precise details of generalised measures of entropy and Bregman divergences and their"}, {"heading": "4 High impact from low impact", "text": "All the preceding methods aim to reduce the impact of the AI. Of course, we don\u2019t actually want a low impact overall \u2013 we want a low negative impact. The problem is that we cannot successfully define ahead of time what these negative impacts are.\nSo how can we ensure that we actually get some level of positive impact from using such AIs?"}, {"heading": "4.1 Calibrating the penalty function", "text": "The most obvious option is to \u2018tune the dial\u2019 in equation (1) by changing the value of \u00b5. We can start with a very large \u00b5 that ensures no impact at all \u2013 the AI will do nothing. We can then gradually reduce \u00b5 until we get an action that actually increases u.\nThis does not seem especially safe, however. The first issue is that we have little understanding of the correct value for \u00b5, so little understanding of the correct rate to reduce \u00b5 at. It is conceivable that we spend a million steps reducing \u00b5 through the \u2018do nothing\u2019 range, and that the next step moves over the \u2018safe increase of u\u2019, straight to the \u2018dangerous impact\u2019 area. In other words, there may be a precipitous jump from the level at which \u00b5R dominates u, to the level at which u becomes sufficiently unconstrained by \u00b5R to lead to dangerous behaviour. See figure 2 for illustration.\nThe central failure, however, is that in many cases it is not clear that low impact is compatible with any increase in u. In particular, when it\u2019s clear that the AI has done something, low impact might be impossible. Even the simple fact that the AI had done anything might get reported, passed on, commented upon. It might affect the whole future development of AI, economic policy, philosophy, and so on. This might disrupt any effect of low impact (e.g. any action the AI takes might have an impact the AI can predict), meaning that there is no safe range for \u00b5: the AI must either do nothing, or have a large impact.\nHowever, though we cannot successfully define the negative impacts of the AI we wish to avoid, we are on much firmer grounds when defining the positive\nrelationship to information theory are involved and not worth expounding in detail here. For extended discussions, see [GR07] and [GD04].\naim we are looking for. This suggests other ways of producing higher impact: by specifically allowing what we want to allow."}, {"heading": "4.2 Unsafe output channel", "text": "The easiest way to allow for higher impact, is to specifically exclude the AI\u2019s output from the low impact requirements. For instance, assume the AI is going to send out message O. To ensure that P (O|\u00acX) 6= 0, we set up an alternative system that will produce a random message.\nThen we exclude the contents of O from the low impact considerations. For instance, we can modify the coarse-graining approaching developed above by replacing equation (2) with R = maxw |P (Vw|X,O) \u2212 P (Vw|\u00acX,O)|. Alternatively, we can abstractly compare P (W|X,O) with P (W|\u00acX,O), or replace equation (3) with:\nR = min \u03c1\n{ E\u2032 ( P (g\u03c1|X, b,O) P (g\u03c1|\u00acX, b,O) ) > 10, or E\u2032 ( P (g\u03c1|\u00acX, b,O) P (g\u03c1|X, b,O) ) > 10 } .\nAll of these conditionings reflect the fact that though a coherent O is likely to present very strong evidence for X over \u00acX, we can control for any further evidence \u2013 such as the evidence provided by an AI expanding and optimising the world. A diagram of the setup for the \u2018undetectable\u2019 approach of section 3.4 is given in figure 3.\nThis type of low impact keeps the AI from having an impact, outside of the content of the message O. It does not make the message O itself safe \u2013 O is excluded from any low impact requirements. Therefore, the use of this method is to block other routes for AI influence, and confine the risk to the output itself, which can still be very dangerous [Bos14]."}, {"heading": "4.3 WBE, chatbots, and conditioning on results", "text": "As we just observed, one of the problems of low impact is the content of any message the AI sends out. Indeed, unless we condition away that message as in the previous subsection, the very existence of a coherent message is a potentially massive indication of impact: it means the AI was turned on and did something.\nSo one way of dealing with this is to have something else in the box that could output a coherent message. Maybe a real human12, a whole brain emulation [SB08], another less intelligent artificial agent, a chatbot, or maybe a random selection of internet messages. If the AI is not turned on \u2013 if \u00acX \u2013 then the other agent will output some message on the subject. So the emission of a coherent message per se is no longer surprising, nor direct evidence for the AI being turned on.\nIn that situation, the low impact setting now motivates the AI to do a perfect imitation of a chatbot/human/WBE \u2013 and nothing more. But we can get around this restriction by conditioning on something other than the actual content of the message.\nSuppose that we want the AI to make us rich \u2013 maybe with stock market advice. Then, in a year\u2019s time, if we have, say, multiplied our fortune by ten,\n12About to die, so never able to talk about this experience, or sworn to eternal silence?\nwe will announce \u201cthe AI\u2019s advice was good\u201d. Call this announcement A. Then we use A to condition on as in Section 4.2, instead of O.\nWhat does this mean in practice? It means that the AI is committed to low impact, given that we multiply our fortune by ten. So, the world in which we get a random lucky message O and then get rich must resemble the world in which the AI gives us advice O and we get rich13. To make this safe, we would want P (A|\u00acX) to not be too low \u2013 we have a non-insignificant chance of getting rich even if the AI was actually not present. So a world with A and \u00acX is not particularly unusual; therefore, the world with X that is of low impact compared with it is not particularly unusual either. We might even be\n13Note that we condition on the announcement A (which is easy to define) not on the concept of getting rich (or whatever else our goal is) as that might be harder. This would be very dangerous if we told the AI \u2018ensure A happens\u2019, as A is only loosely connected with our true goal. But here we are using A as a sufficient measure of success in a world in which the AI doesn\u2019t exist. In that world, it is a safe measure of success. Hence, since we are extending that world via low impact to situations where the AI exists, the difference between A and our true goals is not an issue.\nable to specify P (A|\u00acX) precisely \u2013 maybe there is a selection of a thousand stocks, and the AI recommends one; otherwise a random one is selected. Then A is the announcement that that stock outperformed all the others, making P (A|\u00acX) = 1/1000.\nEssentially, the AI operates here as a probability pump, pushing us in the direction we might have been able to achieve anyway, but now increasing its probability. It gives us advice that that a human/WBE/chatbot/random selector could have given us, but gives us the best version of that advice."}, {"heading": "4.4 High impact from multiple conditional low impact", "text": "Take the following hypothetical. Assume that an asteroid is heading to Earth, and that we wish to aim a laser to deflect it. If we ask a single low impact AI to do so, low impact will prevent it from doing so.\nBut now imagine there are two AIs, Alice and Bob, and that they are activated by events X and Y respectively. Each is indifferent or nearly indifferent to how to world ends up given that the other is turned on. However, conditional on Bob being turned off, Alice is low impact but desires to output the correct x-coordinate for aiming the laser (this defines uAlice). Likewise, conditional on Alice being turned off, Bob is low impact, but desires to output the correct y-coordinate.\nSo, because Alice only cares about worlds in which \u00acY , even if she\u2019s highly confident that Y , she\u2019ll still output the correct x-coordinate. Furthermore, since in the \u00acY -worlds, the laser will miss the asteroid for want of a correct y-coordinate, she can still count as conditionally low impact. And the same, mutatis mutandis, goes for Bob.\nSo, both Alice and Bob will output the correct coordinates, and we get a large impact \u2013 as long as we set the probabilities of \u00acX and \u00acY low enough. But all negative AI impacts \u2013 such as sneaking something dangerous into the coordinates, or escaping and optimising the world \u2013 can still be precluded. This is illustrated by figure 4.\nThis doesn not work if the two AIs are robots that can move around and observe that the other is, in fact, active. In that case, they have to act as if the other is not active while observing that they are, thus causing them to come up with complicated explanations as to why this is so. Their behaviour is thus likely to become undefined. In particular, they are probably not going to act as low impact laser aimers, see figure 5."}, {"heading": "4.5 Extension of category concepts", "text": "The previous \u2018aiming laser\u2019 example does not allow the AIs to have a (desired and targeted) high impact in general situations. It does not, for instance, allow the AIs to walk around and notice that the other AI is actually turned on \u2013 it only works because they don\u2019t know the other AI was activated.\nHow might we make this approach more broadly applicable? One potential idea is for the AI to derive general concepts of low impact conditional on \u00acX \u2013 concepts such as \u2018don\u2019t fill the universe with self-replicating probes\u2019 or \u2018don\u2019t take over the world\u2019 conditional on the other AI not being activated. Then we could hope that these concepts can be generalised to cover the X situation as well. Specifically, we might imagine that the AI creates the code for a subagent,\nusing these concepts, and that the subagent goes out and does things in the world, maybe interacting with the other subagents, and physically aiming the laser.\nThere are two fundamental challenges here. The first is that we don\u2019t want the subagent to generalise low impact to include \u2018not outputting the correct y-coordinates\u2019. This could be avoided either by requiring that the categories used be roughly independent of X vs \u00acX, or by using some of the methods of section 4.3 (a successful deflection of the asteroid seems a pretty clear outcome that could be conditioned on).\nThe second issue is more problematic. Consider the concept C: \u201cif \u00acX, nothing happens, if X, a nuclear war starts\u201d. This concept can describe low impact, conditional on \u00acX, but is obviously terrible at extending low impact to the X case.\nNow C looks disjunctive and artificial, and we\u2019d like to rule concepts like this out. But it turns out to be hard; there are no easy ways to distinguish unnatural disjunctive categories from natural ones (see the issues with Grue and Bleen versus Blue and Green for a very relevant example of this kind of problem [Goo83]). Research in this possibility is ongoing14."}, {"heading": "5 Known issues", "text": "There are a couple of difficulties with the whole low impact approach. The general one, common to the Friendly AI approach as well, is that the AI may think of a loophole that we cannot; the risk of this is low the more analysis we do and the better we understand the situation.\nBut there are more specific issues. The R in equation (1) is not a utility function; instead it is a penalty function that the AI itself calculates, using its own probability modules P \u2032 (and in one case it uses this to estimate the output of an idealised probability module P \u2013 see Section 3.4).\nWhat would happen if the AI self-modifies and changes P \u2032? There is a meta-argument that this shouldn\u2019t matter \u2013 the AI is committed to low impact, and therefore it will ensure that its future copies also have at least as much low impact. This argument does not feel fully reassuring, however, and it is very possible that some bad programming would be disastrous. For instance, we want P \u2032 to be properly abstractly defined, not labeled as (the equivalent of) \u2018the output of that box over there\u2019, as \u2018that box over there\u2019 can always be modified physically. But it might not always be clear how the agent is formally defining P \u2032; this is especially the case if there is some implicit probability estimate happening elsewhere in the AI. For instance, what if the pre-processing of inputs to P \u2032 was very important, and R was defined sloppily enough that changing the pre-processing could change its definition?\nThe more general issue is that any goal that is not a utility function is unstable [Omo08], in that an agent with one will seek to change it if they can15.\nThe corresponding author intends to analyse the issue in subsequent papers: what do unstable goals tend to if the agent can self-modify? This would both be useful to preserve the needed parts of unstable goals (such as the low impact) and might also allow us to express things like low impact in a clear, and we hope instructive, utility function format.\n14See the corresponding author\u2019s work at http://lesswrong.com/lw/mbq/the_president_ didnt_die_failures_at_extending_ai/ , http://lesswrong.com/lw/mbp/green_emeralds_ grue_diamonds/ , http://lesswrong.com/r/discussion/lw/mbr/grue_bleen_and_natural_ categories/ , and http://lesswrong.com/r/discussion/lw/mfq/presidents_asteroids_ natural_categories_and/.\n15Utility functions are generally seen as stable, but even there there are subtleties. Because utility functions form a kind of affine space, any utility function being unstable means almost all of them are. To see why, note that a stable utility function mixed with or added to an unstable one will be unstable. It still remains the case, though, that nearly all utility functions we could naturally think of, are stable.\nThere is also the risk that a series of low impact AIs, through their individual decisions, end up having a large impact even if no specific AI does so. That particular problem can be addressed by making the AIs indifferent to the existence/outputs of the other AIs16. However, this is a patch for a particular issue, rather than a principled declaration that there are no further issues. Such a declaration or proof would be of great use, as repeated patching of an idea does not end when the idea is safe, but when we can no longer think of reasons it is unsafe."}], "references": [{"title": "Motivated value selection for artificial agents", "author": ["Stuart Armstrong"], "venue": "presented at the 1st International Workshop on AI and Ethics,", "citeRegEx": "Armstrong.,? \\Q2015\\E", "shortCiteRegEx": "Armstrong.", "year": 2015}, {"title": "Thinking inside the box: Controlling and using an oracle ai", "author": ["Stuart Armstrong", "Anders Sandberg", "Nick Bostrom"], "venue": "Minds and Machines,", "citeRegEx": "Armstrong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Armstrong et al\\.", "year": 2012}, {"title": "Superintelligence: Paths, dangers, strategies", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "Bostrom.,? \\Q2014\\E", "shortCiteRegEx": "Bostrom.", "year": 2014}, {"title": "Definability of truth in probabilistic logic", "author": ["Paul Christiano", "Eliezer Yudkowsky", "Marcello Herreshoff", "Mihaly Barasz"], "venue": "MIRI Early Draft,", "citeRegEx": "Christiano et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Christiano et al\\.", "year": 2013}, {"title": "Learning what to value. In Artificial General Intelligence, pages 309\u2013314", "author": ["Daniel Dewey"], "venue": null, "citeRegEx": "Dewey.,? \\Q2011\\E", "shortCiteRegEx": "Dewey.", "year": 2011}, {"title": "Game theory, maximum entropy, minimum discrepancy, and robust bayesian decision theory", "author": ["P.D. Gr\u00fcnwald", "A.P. Dawid"], "venue": "Annals of Statistics,", "citeRegEx": "Gr\u00fcnwald and Dawid.,? \\Q2004\\E", "shortCiteRegEx": "Gr\u00fcnwald and Dawid.", "year": 2004}, {"title": "Fact, fiction, and forecast", "author": ["Nelson Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q1983\\E", "shortCiteRegEx": "Goodman.", "year": 1983}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["Tilmann Gneiting", "Adrian E. Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gneiting and Raftery.,? \\Q2007\\E", "shortCiteRegEx": "Gneiting and Raftery.", "year": 2007}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Basic ai drives", "author": ["S. Omohundro"], "venue": "In Proceedings of the First AGI Conference,", "citeRegEx": "Omohundro.,? \\Q2008\\E", "shortCiteRegEx": "Omohundro.", "year": 2008}, {"title": "Whole brain emulation. a roadmap", "author": ["Anders Sandberg", "Nick Bostrom"], "venue": "Future of Humanity Institute. Technical Report,", "citeRegEx": "Sandberg and Bostrom.,? \\Q2008\\E", "shortCiteRegEx": "Sandberg and Bostrom.", "year": 2008}, {"title": "Leakproofing the singularity: artificial intelligence confinement problem", "author": ["Roman V. Yampolskiy"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Yampolskiy.,? \\Q2012\\E", "shortCiteRegEx": "Yampolskiy.", "year": 2012}, {"title": "Artificial intelligence as a positive and negative factor in global risk", "author": ["Eliezer Yudkowsky"], "venue": "Global catastrophic risks,", "citeRegEx": "Yudkowsky.,? \\Q2008\\E", "shortCiteRegEx": "Yudkowsky.", "year": 2008}], "referenceMentions": [], "year": 2017, "abstractText": "There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of \u2018low impact\u2019. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.", "creator": "LaTeX with hyperref package"}}}