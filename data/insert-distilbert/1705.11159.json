{"id": "1705.11159", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Reinforcement Learning for Learning Rate Control", "abstract": "stochastic gradient descent ( sgd ), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. it is observed that the models trained heavily by sgd are sensitive to learning rates and good learning rates are problem topics specific. we propose an algorithm to automatically learn learning processing rates using neural network based actor - critic methods from deep reinforcement learning ( rl ). in particular, we train a dedicated policy network called actor to decide accurately the learning rate at each step during training, and a value network called critic to generally give feedback about quality of the decision ( e. g., the goodness of the learning rate outputted by the actor ) that the actor made. the introduction of auxiliary actor and critic action networks helps the main network achieve better performance. experiments on different organizational datasets configurations and network architectures show that our numerical approach leads to better convergence of sgd than human - designed competitors.", "histories": [["v1", "Wed, 31 May 2017 15:58:35 GMT  (4040kb,D)", "http://arxiv.org/abs/1705.11159v1", "7 pages, 9 figures"]], "COMMENTS": "7 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chang xu", "tao qin", "gang wang", "tie-yan liu"], "accepted": false, "id": "1705.11159"}, "pdf": {"name": "1705.11159.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning for Learning Rate Control", "authors": ["Chang Xu", "Tao", "Qin", "Gang Wang", "Tie-Yan"], "emails": ["wgzwp}@nbjl.nankai.edu.cn,", "Liu}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "While facing large scale of training data, stochastic learning such as stochastic gradient descent (SGD) is usually much faster than batch learning and often results in better models. An observation for SGD methods is that their performances are highly sensitive to the choice of learning rate [LeCun et al., 2012]. Clearly, setting a static learning rate for the whole training process is insufficient, since intuitively the learning rate should decrease when the model becomes more and more close to a (local) optimum as the training goes on over time [Maclaurin et al., 2015]. Although there are some empirical suggestions to guide how to adjust the learning rate over time in training, it is still a difficult task to find a good policy to adjust the learning rate, given that good policies are problem specific and depend on implementation details of a machine learning algorithm. One usually needs to try many times and adjust the learning rate manually to accumulate knowledge about the problem. However, human involvement often needs domain knowledge about the target problems, which is inefficient and difficult to scale up to different problems. Thus,\na natural question arises: can we learn to adjust the learning rate? This is exactly the focus of this work and we aim to learn learning rates for SGD based machine learning (ML) algorithms without human-designed rules or hand-crafted features.\nBy examining the current practice of learning rate control/adjustment, we have two observations. First, learning rate control is a sequential decision process: We set an initial learning rate at the beginning, and then at each step we decide whether to change the learning rate and how to change it, based on the current model and loss, training data at hand, and maybe history of the training process. As suggested in [Orr and Mu\u0308ller, 2003], one well-principled method for estimating the ideal learning rate that is to decrease the learning rate when the weight vector oscillates, and increase it when the weight vector follows a relatively steady direction. Second, although at each step some immediate reward (e.g., the loss decrement) can be obtained by taking actions, we care more about the performance of the final model found by the ML algorithm. Consider two different learning rate control policies: the first one leads to fast loss decrease at the beginning but gets saturated and stuck in a local minimum quickly, while the second one starts with slower loss decrease but results in much smaller final loss. Obviously, the second policy is better. That is, we prefer long-term rewards over short-term rewards.\nCombining the two observations, it is not difficult to see that the problem of finding a good policy to control/adjust learning rate falls into the scope of reinforcement learning (RL) [Sutton and Barto, 1998]. Inspired by the recent success of RL for sequential decision problems, in this work, we leverage RL techniques and try to learn learning rate for SGD based methods.\nWe propose an algorithm to learn learning rate within the actor-critic framework [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014], which is widely used in RL. An actor network is trained to take an action that decides the learning rate for current step, and a critic network is trained to give feedback to the actor network about long-term performance and help the actor network to adjust itself so as to perform better in the future. To reduce oscillation during training, we take gradient disagreement among training samples into account. By feeding different training samples to the actor network and the critic network, learning rate is en-\nar X\niv :1\n70 5.\n11 15\n9v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\n01 7\ncouraged to be small when gradients oscillate, which is consistent with the suggestion for ideal learning rate strategy in [Orr and Mu\u0308ller, 2003]. A series of experiments on different datasets and network architectures validate the effectiveness of our proposed algorithm for learning rate control.\nThe main contributions of this paper include:\n\u2022 We propose to use actor-critic based auxiliary networks to learn and control the learning rate for ML algorithms, so that the ML algorithm can achieve better convergence. \u2022 Long-term rewards are exploited in our approach rather\nthan only immediate rewards (e.g., the decrease of loss for one step). The expected total decrease of loss in future steps is modeled by the critic network, so that the actor can make far-sighted decision for learning rate."}, {"heading": "2 Related Work", "text": "We review some related work in this section."}, {"heading": "2.1 Improved Gradient Methods", "text": "Our focus is to improve gradient based ML algorithm through automatic learning of learning rate. Different approaches have been proposed to improve gradient methods, especially for deep neural networks.\nSince SGD solely rely on a given example (or a mini-batch of examples) to compare gradient, its model update at each step tends to be unstable and it takes many steps to converge. To solve this problem, momentum SGD [Jacobs, 1988] is proposed to accelerate SGD by using recent gradients. RMSprop [Tieleman and Hinton, 2012] utilizes the magnitude of recent gradients to normalize the gradients. It always keeps a moving average over the root mean squared gradients, by which it divides the current gradient. Adagrad [Duchi et al., 2011] adapts component-wise learning rates, and performs larger updates for infrequent and smaller updates for frequent parameters. Adadelta [Zeiler, 2012] extends Adagrad by reducing its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size. Adam [Kingma and Ba, 2014] computes component-wise learning rates using the estimates of first and second moments of the gradients, which combines the advantages of AdaGrad and RMSProp.\n[Senior et al., 2013; Sutton, 1992; Darken and Moody, 1990] focus on predefining update rules to adjust learning rates during training. A limitation of these methods is that they have additional free parameters which need to be set manually. [Schaul et al., 2013] proposes a method to choose good learning rate for SGD, which relies on the square norm of the expectation of the gradient, and the expectation of the square norm of the gradient. The method is much more constrained than ours and several assumption should be met. Another recent work [Daniel et al., 2016] investigates several hand-tuned features and uses the Relative Entropy Policy Search method as controller to select step size for SGD and RMSprop."}, {"heading": "2.2 Reinforcement Learning", "text": "Since our proposed algorithm is based on RL techniques, here we give a very brief introduction to RL, which will ease the description of our algorithm in next section.\nReinforcement learning [Sutton, 1988] is concerned with how an agent acts in a stochastic environment by sequentially choosing actions over a sequence of time steps, in order to maximize a cumulative reward. In RL, a state st encodes the agent\u2019s observation about the environment at a time step t, and a policy function \u03c0(st) determines how the agent behaves (e.g., which action to take) at state st. An action-value function (or, Q function) Q\u03c0(st, at) is usually used to denote the cumulative reward of taking action at at state st and then following policy \u03c0 afterwards.\nMany RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014] can be described under the actor-critic framework. An actor-critic algorithm learns the policy function and the value function simultaneously and interactively. The policy structure is known as the actor, and is used to select actions; the estimated value function is known as the critic, and it criticizes the actions made by the actor.\nRecently, deep reinforcement learning, which uses deep neural networks to approximate/represent the policy function and/or the value function, have shown promise in various domains, including Atari games [Mnih et al., 2015], Go [Silver et al., 2016], machine translation [Bahdanau et al., 2016], image recognition [Xu et al., 2015], etc."}, {"heading": "3 Method", "text": "In this section, we present an actor-critic algorithm that can automate the learning rate control for SGD based machine learning algorithms.\nMany machine learning tasks need to train a model with parameters \u03c9 by minimizing a loss function f defined over a set X of training examples:\n\u03c9\u2217 = arg min \u03c9 f\u03c9(X). (1)\nA standard approach for the loss function minimization is gradient descent, which sequentially updates the parameters using gradients step by step:\n\u03c9t+1 = \u03c9t \u2212 at\u2207f t, (2) where at is the learning rate at step t, and\u2207f t is the local gradient of f at \u03c9t. Here one step can be the whole batch of all the training data, a mini batch of tens/hundreds of examples, or a random sample.\nIt is observed that the performance of SGD based methods is quite sensitive to the choice of at for non-convex loss function f . Unfortunately, f is usually non-convex with respect to the parameters w in many ML algorithms, especially for deep neural networks. We aim to learn a learning rate controller using RL techniques that can automatically control at.\nFigure 1 illustrates our automatic learning rate controller, which adopts the actor-critic framework in RL. The basic idea is that at each step, given the current model \u03c9t and training sample x, an actor network is used to take an action (the learning rate at, and it will be used to update the model \u03c9t), and a critic network is used to estimate the goodness of the action. The actor network will be updated using the estimated goodness of at, and the critic network will be updated by minimizing temporal difference (TD) [Sutton and Barto, 1990] error.\nWe describe the details of our algorithm in the following subsections."}, {"heading": "3.1 Actor Network", "text": "The actor network, which is called policy network in RL, plays the key role in our algorithm: it determines the learning rate control policy for the primary ML algorithm1 based on the current model, training data, and maybe historical information during the training process.\nNote that \u03c9t could be of huge dimensions, e.g., one widely used image recognition model VGGNet [Simonyan and Zisserman, 2014] has more than 140 million parameters. If the actor network takes all of those parameters as the inputs, its computational complexity would dominate the complexity of the primary algorithm, which is unfordable. Therefore, we propose to use a function \u03c7(\u00b7) to process and yield a compact vector st as the input of the actor network. Following the practice in RL, we call \u03c7(\u00b7) the state function, which takes \u03c9t and the training data x as inputs:\nst = \u03c7(\u03c9t, X). (3) Then the actor network \u03c0\u03b8(\u00b7) parameterized by \u03b8 yields an action at:\n\u03c0\u03b8(s t) = at, (4)\nwhere the action at \u2208 R is a continuous value. When at is determined, we update the model of the primary algorithm by Equation 2.\nNote that the actor network has its own parameters and we need to learn them to output a good action. To learn the actor network, we need to know how to evaluate the goodness of an actor network. The critic network exactly plays this role.\n1Here we have two learning algorithms. We call the one with learning rate to adjust as the primary ML algorithm, and the other one which optimizes the learning rate of the primary one as the secondary ML algorithm.\nAlgorithm 1 Actor-Critic Algorithm for Learning Rate Learning\nInputs: Training steps T ; training set X; loss function f ; state function \u03c7; discount factor: \u03b3 ; mini-batch size m\u03b8 of actor network; mini-batch size m\u03d5 of critic network; reset frequency of the model e\n1: Initialize model parameters \u03c9 as \u03c90, policy parameters \u03b8 of the actor network as \u03b80, and value parameters \u03d5 of the critic network as \u03d50. 2: for t = 1, ..., T do 3: Sample xi \u2208 X, i \u2208 1, ..., N. 4: Extract state vector: sti = \u03c7(\u03c9\nt, xi). 5: //Actor network selects an action. 6: Computes learning rate ati = \u03c0\u03b8(s t i). 7: //Update model parameters \u03c9. 8: Compute\u2207f t(xi). 9: Update \u03c9: \u03c9t+1 = \u03c9t \u2212 ati\u2207f t(xi).\n10: //Update critic network by minimizing square error between estimation and label. 11: rt = f t(xi)\u2212 f t+1(xi) 12: Extract state vector: st+1i = \u03c7(\u03c9 t+1, xi) 13: Compute Q\u03d5(st+1i , \u03c0\u03b8(s t+1 i )), Q\u03d5(s t i, a t i) 14: Compute \u03b4t according to Equation 7: \u03b4t = rt + \u03b3Q\u03d5(s t+1 i , \u03c0\u03b8(s t+1 i ))\u2212Q\u03d5(sti, ati) 15: Compute the gradients of critic network according to Equation 8 : \u2207\u03d5t = \u03b4t\u2207\u03d5Q\u03d5(sti, ati) 16: if t mod m\u03d5 = 0 then 17: Update \u03d5 by \u2207\u03d5 = 1m\u03d5 \u2211m\u03d5\u22121 u=0 \u2207\u03d5t\u2212u 18: end if 19: // Update actor network 20: Sample xj \u2208 X, j \u2208 1, ..., N, j 6= i. 21: Extract state vector: st+1j = \u03c7(\u03c9\nt+1, xj). 22: Compute at+1j = \u03c0\u03b8(s t+1 j ). 23: Compute the gradients of actor network according to Equation 9:\n\u2207\u03b8t = \u2207\u03b8\u03c0\u03b8(st+1j )\u2207aQ\u03d5(s t+1 j , a t+1 j )|a=\u03c0\u03b8(s)\n24: if t mod m\u03b8 = 0 then 25: Update \u03b8 by\u2207\u03b8 = 1m\u03b8 \u2211m\u03b8\u22121 u=0 \u2207\u03b8t\u2212u 26: end if 27: if t mod e = 0 then set \u03c9t+1 = \u03c90 end if 28: end for 29: return \u03c9, \u03b8, \u03d5;"}, {"heading": "3.2 Critic Network", "text": "Recall that our goal is to find a good policy for learning rate control to ensure that a good model can be learnt eventually by the primary ML algorithm. For this purpose, the actor network needs to output a good action at at state st so that finally a low training loss f(\u00b7) can be achieved. In RL, the Q function Q\u03c0(s, a) is often used to denote the long term reward of the state-action pair s, a while following the policy \u03c0 to take future actions. In our problem, Q\u03c0(st, at) indicates the accumulative decrement of training loss starting from step t. We define the immediate reward at step t as the one step\nloss decrement: rt = f t \u2212 f t+1. (5)\nThe accumulative value Rt\u03c0 of policy \u03c0 at step t is the total discounted reward from step t:\nRt\u03c0 = \u03a3 T k=t\u03b3 k\u2212tr(sk, ak),\nwhere \u03b3 \u2208 (0, 1] is the discount factor. Considering that both the states and actions are uncountable in our problem, the critic network uses a parametric function Q\u03d5(s, a) with parameters \u03d5 to approximate the Q value function Q\u03c0(s, a)."}, {"heading": "3.3 Training of Actor and Critic Networks", "text": "The critic network has its own parameters \u03d5, which is updated at each step using TD learning. More precisely, the critic is trained by minimizing the square error between the estimation Q\u03d5(st, at) and the target yt:\nyt = rt + \u03b3Q\u03d5(s t+1, at+1). (6)\nThe TD error is defined as:\n\u03b4t = yt \u2212Q\u03d5(st, at) = rt + \u03b3Q\u03d5(s t+1, \u03c0\u03b8(s t+1))\u2212Q\u03d5(st, at)\n(7)\nThe weight update rule follows the on-policy deterministic actor-critic algorithm. The gradients of critic network are:\n\u2207\u03d5 = \u03b4t\u2207\u03d5Q\u03d5(st, at), (8)\nThe policy parameters \u03b8 of the actor network is updated by ensuring that it can output the action with the largest Q value at state st, i.e., a\u2217 = arg maxaQ\u03d5(st, a). Mathematically,\n\u2207\u03b8 = \u2207\u03b8\u03c0\u03b8(st+1)\u2207aQ\u03d5(st+1, at+1)|a=\u03c0\u03b8(s). (9)"}, {"heading": "3.4 The Algorithm", "text": "The overall algorithm for learning rate learning is shown in Algorithm 1. In each step, we sample an example (Line 3), extract the current state vector (Line 4), compute the learning rate using the actor network (Line 6), update the model (Lines 8-9), compute TD error (Lines 11-15), update the critic network (Line 16-18), and sample another example (Line 20) to update the actor network (Line 21-26). We would like to make some discussions about the algorithm.\nFirst, in the current algorithm, for simplicity, we consider using only one example for model update. It is easy to generalize to a mini batch of random examples.\nSecond, one may notice that we use one example (e.g., xi) for model and the critic network update, but a different example (e.g., xj) for the actor network update.\nDoing so we can reduce oscillation during training. Suppose that the gradient direction of current example (or mini batch of examples) is quite different from others in this stage of training process. Intuitively at this step the model will be changed a lot to fit the example, consequently resulting in oscillation of the training, as shown in our experiments. As aforementioned, one principle for ideal learning rate control is to decrease it when the gradient vector oscillates, and increase it when the gradient vector follows a relatively steady direction. Therefore, we try to alleviate the problem by controlling learning rate according to gradient disagreement.\nBy feeding different examples to the actor and critic networks, it is very likely the critic network will find that the gradient direction of the example fed into the actor network is inconsistent with its own training example and thus criticize the large learning rate suggested by the actor network. More precisely, the update of \u03c9 is based on xi and the learning rate suggested by the actor network, while the training target of the actor network is to maximize the output of the critic network on xj . If there is big gradient disagreement between xi and xj , the update of \u03c9, which is affected by actor\u2019s decision, would cause the critic\u2019s output on xj to be small. To compensate this effect, the actor network is forced to predict a small learning rate for big gradient disagreement in this situation."}, {"heading": "4 Experiments", "text": "We conducted a set of experiments to test the performance of our learning rate learning algorithm and compared with several baseline methods. We report the experimental results in this section."}, {"heading": "4.1 Experimental Setup", "text": "We specified our actor-critic algorithm in experiments as follows. Given that stochastic mini-batch training is a common practice in deep learning, the actor-critic algorithm also operated on mini-batches, i.e., each step is a mini batch in our\nexperiments. The state st = \u03c7(\u03c9t, Xi) is defined as the average loss of learning model \u03c9t on the input mini batch Xi.The actor network is specified as a two-layer long shortterm memory (LSTM) network with 20 units in each layer, considering that a good learning rate for step t depends on and correlates with the learning rates at previous steps while LSTM is well suited to model sequences with long-distance dependence. The critic network is specified as a simple neural network with one hidden layer and 10 hidden units. Adam with the default setting in toolbox is used to train the learning rate learner in all the experiments.\nWe compared our method with several mainstream SGD algorithms, including SGD, Adam [Kingma and Ba, 2014], Adagrad [Duchi et al., 2011] and RMSprop [Tieleman and Hinton, 2012]. We also compare our method with a recent work by [Daniel et al., 2016]2. This work identifies several hand-designed features and use an RL method (Relative Entropy Policy Search) to learn a learning rate controller. Another baseline method is \u201cvSGD\u201d [Schaul et al., 2013]2, which automatically adjusts learning rates to minimize the expected error. It tries to compute learning rate at each update by optimizing the expected loss after the next update according to (1) the square norm of the expectation of the gradient and (2) the expectation of the square norm of the gradient."}, {"heading": "4.2 Experimental Results", "text": "To verify the effectiveness of our method on different datasets and model structures, experiments are conducted on two widely used image classification datasets: MNIST [LeCun et al., 1998] and CIFAR-10 [Krizhevsky and Hinton, 2009]. For simplicity, the primary ML algorithm adopted the CNN models and settings from tensorflow [Abadi et al., 2015] tutorial, whose source code can be found at [TensorflowExamples, ] For each of these algorithms and each dataset, we tried the following learning rates 10\u22124, 10\u22123, ..., 100. We report the best performance of these algorithms over those learning rates. If an algorithm needs some other parameters to set, such as decay coefficients for Adam, we used the default setting in the toolbox. For each benchmark and our proposed method, five independent runs are averaged and reported in all of the following experiments. We trained all the baseline models until convergence.\n2Thank the authors for providing the source code.\nResults on MNIST MNIST is a dataset for handwritten digit classification task. Each example in the dataset is a 28 \u00d7 28 black and white image containing a digit in {0, 1, \u00b7 \u00b7 \u00b7 , 9}. The CNN model used in the primary ML algorithm is consist of two convolutional layers, each followed by a pooling layer, and finally a fully connected layer. There are 60,000 training images and 10,000 test images in this dataset. We scaled the pixel values to the [0,1] range before inputting to all the algorithms. Each mini batch contains 50 randomly sampled images.\nFigure 2 shows the results of our actor-critic algorithm and the baseline methods, including the curves of training loss and test loss. We have the following observations.\n\u2022 Although the loss of our algorithm does not decrease very fast at the beginning, our algorithm achieves the best performance at the end. One may expect that our algorithm should have significantly faster convergence speed from the beginning considering that our algorithm learns both the learning rate and the CNN model, while most of the baseline methods only learn the CNN model and choose the learning rate per some predefine rules. However, this is not the case. Since our method targets at future long-term rewards rather than immediate rewards, it can make far-sighted decision and lead to better performance in long term.\n\u2022 The loss curves of our approach is more smooth and stable than others. That is because we carefully design the algorithm and feed different samples to the actor network and critic network. As discussed in Section3.4, doing so we can reduce oscillation during training.\nResults on CIFAR-10 CIFAR-10 is a dataset consisting of 60000 natural 32 \u00d7 32 RGB images in 10 classes: 50,000 images for training and 10,000 for test. We used a CNN with 2 convolutional layers (each followed by max-pooling layer) and 2 fully connected layers for this task.Before inputting an image to the CNN, we subtracted the per-pixel mean computed over the training set from each image.\nFigure 3 shows the results of all the algorithms on CIFAR10, including the curves of training loss and test loss. While the convergence speed of our method is similar to that of the\n0 5 10 15 20 25 30\nTraining Step\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nLo ss\nTest Loss of Our Method Test Loss of SGD Train Loss of Our Method Train Loss of SGD\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nG ra\nd ie\nn t\nD is\na g re\ne m\ne n t\nGradient Disagreement of Our Method Gradient Disagreement of SGD\nFigure 5: Gradient disagreement, training loss and test loss of SGD and our method on a two-dimensional regression problem.\nbaselines, the final performance of our method is the best among all the compared algorithms."}, {"heading": "4.3 Further Analysis", "text": "In order to verify our intuitive explanation that by considering gradient disagreement, our method can make the learning process of the primary ML algorithm stable, here we conducted another experiment. In this experiment, we investigate the relationship between gradient disagreement and loss in the training process of a simple two-dimensional regression problem. We quantify gradient disagreement by using Euclidean distance between gradient on current batch of data and the overall gradient.\nFigure 5 shows the gradient disagreement, training loss and test loss of SGD and our method. We can observe the correlation among them from the figure. As discussed in Section 3.4, by feeding different samples to actor and critic networks, our\nmethod would encourage the learning rate to be small when gradient disagreement is large, so that the oscillation of the training process would be relieved.\nIt is easy to see from the figure that test loss of our method is stable when there is big gradient disagreement, while the loss of SGD oscillates along with gradient disagreement, leading to slow speed of convergence. The test loss of SGD may increase when gradient disagreement increases, while in overall, our test loss decline in monotonous in the figure. Therefore, we need to feed different training data to the actor network and the critic network to ensure the performance of the algorithm.\nTo get deeper insight, we visualized the optimization process of our method. From Figure 4, we can find that our method get to convergence with fewer steps and the optimization trajectory is relatively smooth compared to other methods."}, {"heading": "5 Conclusions and Future Work", "text": "In this work, we have studied how to control learning rates for gradient based machine learning methods and proposed an actor-critic algorithm, to help the main network achieve better performance. The experiments on two image classification tasks have shown that our method (1) can successfully adjust learning rate for different datasets and CNN model structures, leading to better convergence, and 2) can reduce oscillation during training.\nFor the future work, we will explore the following directions. In this work, we have applied our algorithm to control the learning rates of SGD. We will apply to other variants of SGD methods. We have focused on learning a learning rate for all the model parameters. We will study how to learn an individual learning rate for each parameter. We have considered learning learning rates using RL techniques. We will consider learning other hyperparameters such as step-\ndependent dropout rates for deep neural networks."}], "references": [{"title": "et al", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham"], "venue": "Tensorflow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow. org, 1,", "citeRegEx": "Abadi et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "An actorcritic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1607.07086,", "citeRegEx": "Bahdanau et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "man", "author": ["Andrew G Barto", "Richard S Sutton", "Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems"], "venue": "and cybernetics, (5):834\u2013846,", "citeRegEx": "Barto et al.. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "Learning step size controllers for robust neural network training", "author": ["Christian Daniel", "Jonathan Taylor", "Sebastian Nowozin"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Daniel et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast adaptive k-means clustering: some empirical results", "author": ["Christian Darken", "John Moody"], "venue": "Neural Networks, 1990., 1990 IJCNN International Joint Conference on, pages 233\u2013238. IEEE,", "citeRegEx": "Darken and Moody. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12(Jul):2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural networks", "author": ["Robert A Jacobs. Increased rates of convergence through learning rate adaptation"], "venue": "1(4):295\u2013307,", "citeRegEx": "Jacobs. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Hinton", "2009] Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Proceedings of the IEEE", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner. Gradient-based learning applied to document recognition"], "venue": "86(11):2278\u20132324,", "citeRegEx": "LeCun et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In Neural networks: Tricks of the trade", "author": ["Yann A LeCun", "L\u00e9on Bottou", "Genevieve B Orr", "Klaus-Robert M\u00fcller. Efficient backprop"], "venue": "pages 9\u201348. Springer,", "citeRegEx": "LeCun et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["Dougal Maclaurin", "David Duvenaud", "Ryan P Adams"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Maclaurin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural networks: tricks of the trade", "author": ["Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": "Springer,", "citeRegEx": "Orr and M\u00fcller. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "ICML (3)", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun. No more pesky learning rates"], "venue": "28:343\u2013 351,", "citeRegEx": "Schaul et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["Andrew Senior", "Georg Heigold", "Ke Yang"], "venue": "An empirical study of learning rates in deep neural networks for speech recognition. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6724\u20136728. IEEE,", "citeRegEx": "Senior et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Nicolas Heess", "author": ["David Silver", "Guy Lever"], "venue": "Deterministic policy gradient algorithms.", "citeRegEx": "Silver et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["David Silver", "Aja Huang"], "venue": "Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489,", "citeRegEx": "Silver et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556", "citeRegEx": "Simonyan and Zisserman. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Time-derivative models of pavlovian reinforcement", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "pages 497\u2013537,", "citeRegEx": "Sutton and Barto. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "volume 1. MIT press Cambridge,", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "et al", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pages 1057\u20131063,", "citeRegEx": "Sutton et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Machine learning", "author": ["Richard S Sutton. Learning to predict by the methods of temporal differences"], "venue": "3(1):9\u201344,", "citeRegEx": "Sutton. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Adapting bias by gradient descent: An incremental version of delta-bar-delta", "author": ["Richard S Sutton"], "venue": "AAAI, pages 171\u2013176,", "citeRegEx": "Sutton. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Hinton", "2012] Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Machine learning", "author": ["Christopher JCH Watkins", "Peter Dayan. Q-learning"], "venue": "8(3-4):279\u2013 292,", "citeRegEx": "Watkins and Dayan. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "et al", "author": ["Kelvin Xu", "Jimmy Ba"], "venue": "Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2(3):5,", "citeRegEx": "Xu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "An observation for SGD methods is that their performances are highly sensitive to the choice of learning rate [LeCun et al., 2012].", "startOffset": 110, "endOffset": 130}, {"referenceID": 11, "context": "Clearly, setting a static learning rate for the whole training process is insufficient, since intuitively the learning rate should decrease when the model becomes more and more close to a (local) optimum as the training goes on over time [Maclaurin et al., 2015].", "startOffset": 238, "endOffset": 262}, {"referenceID": 13, "context": "As suggested in [Orr and M\u00fcller, 2003], one well-principled method for estimating the ideal learning rate that is to decrease the learning rate when the weight vector oscillates, and increase it when the weight vector follows a relatively steady direction.", "startOffset": 16, "endOffset": 38}, {"referenceID": 20, "context": "Combining the two observations, it is not difficult to see that the problem of finding a good policy to control/adjust learning rate falls into the scope of reinforcement learning (RL) [Sutton and Barto, 1998].", "startOffset": 185, "endOffset": 209}, {"referenceID": 21, "context": "We propose an algorithm to learn learning rate within the actor-critic framework [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014], which is widely used in RL.", "startOffset": 81, "endOffset": 157}, {"referenceID": 2, "context": "We propose an algorithm to learn learning rate within the actor-critic framework [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014], which is widely used in RL.", "startOffset": 81, "endOffset": 157}, {"referenceID": 16, "context": "We propose an algorithm to learn learning rate within the actor-critic framework [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014], which is widely used in RL.", "startOffset": 81, "endOffset": 157}, {"referenceID": 13, "context": "couraged to be small when gradients oscillate, which is consistent with the suggestion for ideal learning rate strategy in [Orr and M\u00fcller, 2003].", "startOffset": 123, "endOffset": 145}, {"referenceID": 6, "context": "To solve this problem, momentum SGD [Jacobs, 1988] is proposed to accelerate SGD by using recent gradients.", "startOffset": 36, "endOffset": 50}, {"referenceID": 5, "context": "Adagrad [Duchi et al., 2011] adapts component-wise learning rates, and performs larger updates for infrequent and smaller updates for frequent parameters.", "startOffset": 8, "endOffset": 28}, {"referenceID": 27, "context": "Adadelta [Zeiler, 2012] extends Adagrad by reducing its aggressive, monotonically decreasing learning rate.", "startOffset": 9, "endOffset": 23}, {"referenceID": 7, "context": "Adam [Kingma and Ba, 2014] computes component-wise learning rates using the estimates of first and second moments of the gradients, which combines the advantages of AdaGrad and RMSProp.", "startOffset": 5, "endOffset": 26}, {"referenceID": 15, "context": "[Senior et al., 2013; Sutton, 1992; Darken and Moody, 1990] focus on predefining update rules to adjust learning rates during training.", "startOffset": 0, "endOffset": 59}, {"referenceID": 23, "context": "[Senior et al., 2013; Sutton, 1992; Darken and Moody, 1990] focus on predefining update rules to adjust learning rates during training.", "startOffset": 0, "endOffset": 59}, {"referenceID": 4, "context": "[Senior et al., 2013; Sutton, 1992; Darken and Moody, 1990] focus on predefining update rules to adjust learning rates during training.", "startOffset": 0, "endOffset": 59}, {"referenceID": 14, "context": "[Schaul et al., 2013] proposes a method to choose good learning rate for SGD, which relies on the square norm of the expectation of the gradient, and the expectation of the square norm of the gradient.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Another recent work [Daniel et al., 2016] investigates several hand-tuned features and uses the Relative Entropy Policy Search method as controller to select step size for SGD and RMSprop.", "startOffset": 20, "endOffset": 41}, {"referenceID": 22, "context": "Reinforcement learning [Sutton, 1988] is concerned with how an agent acts in a stochastic environment by sequentially choosing actions over a sequence of time steps, in order to maximize a cumulative reward.", "startOffset": 23, "endOffset": 37}, {"referenceID": 20, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al.", "startOffset": 38, "endOffset": 87}, {"referenceID": 25, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al.", "startOffset": 38, "endOffset": 87}, {"referenceID": 21, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014] can be described under the actor-critic framework.", "startOffset": 112, "endOffset": 188}, {"referenceID": 2, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014] can be described under the actor-critic framework.", "startOffset": 112, "endOffset": 188}, {"referenceID": 16, "context": "Many RL algorithms have been proposed [Sutton and Barto, 1998; Watkins and Dayan, 1992], and many RL algorithms [Sutton, 1984; Sutton et al., 1999; Barto et al., 1983; Silver et al., 2014] can be described under the actor-critic framework.", "startOffset": 112, "endOffset": 188}, {"referenceID": 12, "context": "Recently, deep reinforcement learning, which uses deep neural networks to approximate/represent the policy function and/or the value function, have shown promise in various domains, including Atari games [Mnih et al., 2015], Go [Silver et al.", "startOffset": 204, "endOffset": 223}, {"referenceID": 17, "context": ", 2015], Go [Silver et al., 2016], machine translation [Bahdanau et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 1, "context": ", 2016], machine translation [Bahdanau et al., 2016], image recognition [Xu et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 26, "context": ", 2016], image recognition [Xu et al., 2015], etc.", "startOffset": 27, "endOffset": 44}, {"referenceID": 19, "context": "The actor network will be updated using the estimated goodness of a, and the critic network will be updated by minimizing temporal difference (TD) [Sutton and Barto, 1990] error.", "startOffset": 147, "endOffset": 171}, {"referenceID": 18, "context": ", one widely used image recognition model VGGNet [Simonyan and Zisserman, 2014] has more than 140 million parameters.", "startOffset": 49, "endOffset": 79}, {"referenceID": 7, "context": "We compared our method with several mainstream SGD algorithms, including SGD, Adam [Kingma and Ba, 2014], Adagrad [Duchi et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 5, "context": "We compared our method with several mainstream SGD algorithms, including SGD, Adam [Kingma and Ba, 2014], Adagrad [Duchi et al., 2011] and RMSprop [Tieleman and Hinton, 2012].", "startOffset": 114, "endOffset": 134}, {"referenceID": 3, "context": "We also compare our method with a recent work by [Daniel et al., 2016]2.", "startOffset": 49, "endOffset": 70}, {"referenceID": 14, "context": "Another baseline method is \u201cvSGD\u201d [Schaul et al., 2013]2, which automatically adjusts learning rates to minimize the expected error.", "startOffset": 34, "endOffset": 55}, {"referenceID": 9, "context": "To verify the effectiveness of our method on different datasets and model structures, experiments are conducted on two widely used image classification datasets: MNIST [LeCun et al., 1998] and CIFAR-10 [Krizhevsky and Hinton, 2009].", "startOffset": 168, "endOffset": 188}, {"referenceID": 0, "context": "For simplicity, the primary ML algorithm adopted the CNN models and settings from tensorflow [Abadi et al., 2015] tutorial, whose source code can be found at [TensorflowExamples, ] For each of these algorithms and each dataset, we tried the following learning rates 10\u22124, 10\u22123, .", "startOffset": 93, "endOffset": 113}], "year": 2017, "abstractText": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. We propose an algorithm to automatically learn learning rates using neural network based actor-critic methods from deep reinforcement learning (RL). In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. The introduction of auxiliary actor and critic networks helps the main network achieve better performance. Experiments on different datasets and network architectures show that our approach leads to better convergence of SGD than humandesigned competitors.", "creator": "LaTeX with hyperref package"}}}