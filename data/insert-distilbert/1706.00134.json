{"id": "1706.00134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Semantic Refinement GRU-based Neural Language Generation for Spoken Dialogue Systems", "abstract": "natural language generation ( nlg ) plays a critical role in spoken dialogue systems.... this paper presents a new approach to nlg by using recurrent neural networks ( rnn ), typically in which a gating mechanism is applied before rnn computation. this allows the proposed model to generate appropriate sentences. the rnn - based computational generator model can be learned from purely unaligned temporal data processed by jointly training sentence tree planning and surface realization to then produce natural language responses. the model was extensively evaluated on four different nlg domains. the results show that the proposed generator achieved considerable better performance on all representing the sampled nlg domains compared to previous generators.", "histories": [["v1", "Thu, 1 Jun 2017 00:37:46 GMT  (450kb)", "http://arxiv.org/abs/1706.00134v1", "has been accepted to appear at PACLING 2017"], ["v2", "Sat, 3 Jun 2017 00:45:11 GMT  (436kb)", "http://arxiv.org/abs/1706.00134v2", "To be appear at PACLING 2017"], ["v3", "Thu, 6 Jul 2017 15:32:59 GMT  (83kb)", "http://arxiv.org/abs/1706.00134v3", "To be appear at PACLING 2017"], ["v4", "Tue, 11 Jul 2017 14:40:56 GMT  (83kb)", "http://arxiv.org/abs/1706.00134v4", "To be appear at PACLING 2017"]], "COMMENTS": "has been accepted to appear at PACLING 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["van-khanh tran", "le-minh nguyen"], "accepted": false, "id": "1706.00134"}, "pdf": {"name": "1706.00134.pdf", "metadata": {"source": "CRF", "title": "Semantic Refinement GRU-based Neural Language Generation for Spoken Dialogue Systems", "authors": ["Van-Khanh Tran", "Le-Minh Nguyen"], "emails": ["nguyenml}@jaist.ac.jp", "tvkhanh@ictu.edu.vn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n00 13\n4v 1\n[ cs\n.C L\n] 1\nJ un\nI. INTRODUCTION\nNatural Language Generation is a critical component in a Spoken Dialogue System (SDS), and its task is to convert a meaning representation produced by the dialogue manager into natural language utterances. Conventional approaches to NLG follow a pipeline. This typically breaks down the task into sentence planning and surface realization. Sentence planning decides the order and structure of sentence, which is followed by a surface realization which converts the sentence structure into the final utterance. Previous approaches to NLG still rely on extensive hand-tuning templates and rules that require expert knowledge of linguistic representation. There are some common and widely approaches to solve NLG problems, including rule-based [1], corpus-based n-gram models [2], and a trainable generator [3]. Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].\nRecently, approaches based on recurrent neural networks have shown advantages in solving the NLG tasks. RNNbased models have been used for NLG as a joint training model [6], [7] and an end-to-end training model [8]. A recurring problem in such systems is requiring datasets annotated for specific dialogue acts1 (DA). Moreover, previous works may lack ability to handle cases such as the binary slots (i.e., yes and no) and slots that take don\u2019t care value\n1A combination of an action type and a list of slot-value pairs. e.g. inform(name=\u2018Frances\u2019; area=\u2018City Center\u2019)\nwhich cannot be directly delexicalized [6], or to generalize to unseen domains [7]. Furthermore, a problem of the current generators is that the generators produce the next token based on the information from the forward context, whereas some sentences may depend on the backward context. As a result, the models tend to generate nonsensical utterances.\nWe propose a statistical NLG based on a gating mechanism on a GRU model, in which the gating mechanism is applied before RNN computation. The proposed model can learn from unaligned data by jointly training the sentence planning and surface realization to generate sentences. We found that the proposed model can produce sentences in a more correct oder than the existing models. The results showed that our proposed method outperforms the previous state-of-the-art methods in terms of BLEU [9] and slot error rate ERR [7] scores. Overall, we make three contributions, in which we: (i) propose two semantic refinement based RNN models, in which a gating mechanism is applied before RNN computation to refine the original inputs, (ii) conduct extensively experiments on four NLG domains, and (iii) analyze the effectiveness of the proposed models on ability to handle the undelixicalized tokens, and to generalize to unseen domain with limited amount of in-domain data used."}, {"heading": "II. RELATED WORK", "text": "Conventional approaches to NLG traditionally split the task into two subtasks: sentence planning and surface realization. Sentence planning deals with mapping of the input semantic symbols onto a linguistic structure, e.g., a tree-like or a template structure. The surface realization then converts the structure into an appropriate sentence [3]. Despite their success and wide use in solving NLG problems, these traditional methods still rely on the handcrafted rule-based generators or rerankers. [2] proposed a class-based n-gram language model (LM) generator which can learn to generate the sentences for a given DA and then select the best sentences using a rule-based reranker. [10] addressed some of the limitation of the class-based LMs by proposing a method based on a syntactic dependency tree. A phrase-\nbased generator based on factored LMs was introduced in [11], which can learn from a semantically aligned corpus.\nRecently, RNNs-based approaches have shown promising performance in the NLG domain. [12], [13] used RNNs in a multi-modal setting to generate captions for images. [14] also proposed a generator using RNNs to create Chinese poetry. [15] encoded an unstructured textual knowledge source along with previous responses and context to produce a response for technical support queries. For task-oriented dialogue systems, [6] combined a forward RNN generator, a CNN reranker, and a backward RNN reranker to generate utterances. A semantically conditioned-based Long ShortTerm Memory (LSTM) generator was proposed in [7], which introduced a control \u201creading\u201d gate to the LSTM cell and can learn the gating mechanism and language model jointly. A recurring problem in such systems is the lack of sufficient domain-specific annotated data."}, {"heading": "III. RECURRENT NEURAL LANGUAGE GENERATOR", "text": "The recurrent language generator proposed in this paper is based on a RNN language model [16], which consists of three layers: an input layer, a hidden layer and an output layer. The input to the network at each time step t is a 1- hot encoding wt of a token 2 wt which is conditioned on a recurrent hidden layer ht. The output layer yt represents the probability distribution of the next token given previous token wt and hidden ht. We can sample from this conditional distribution to obtain the next token in a generated string, and feed it as the next input to the generator. This process finishes when a stop sign is generated [12], or some constraint is reached [14]. The network can generate a sequence of tokens which can be lexicalized3 to form the required utterance. Moreover, in order to ensure that the generated utterance represents the intended meaning of the given DA, the generator is further conditioned on a vector z, a 1-hot vector representation of DA. Inspired by work in [17], we propose an intuition: Gating before computation, in which we add gating mechanism before the RNN computation to semantically refine the input tokens. The following sections present two proposed Semantic Refinement (SR) gating based RNN generators."}, {"heading": "A. SRGRU-BASE", "text": "In this model, instead of feeding an input token wt to the RNN model at each time step t, the input token is filtered by a semantic gate which is computed as follows:\ndt = \u03c3(Wdzz) xt = dt \u2299 wt (1)\nwhere: Wdz is a trained matrix to project the given DA representation into the word embedding space, and xt is\n2Input texts are delexicalized in which slot values are replaced by its corresponding slot tokens.\n3The process in which slot token is replaced by its value.\nnew input. Here Wdz plays a role of sentence planning since it can directly capture which DA features are useful during the generation to encode the input information. The \u2299 element-wise multiplication plays a part in word-level matching which learns not only the vector similarity, but also preserve information about the two vectors. dt is called a refinement gate since the input tokens are refined by the DA information. As a result, we can represent the whole input sentence based on these refined inputs using RNN model.\nIn this study, we use GRU, which was recently proposed in [18], instead of LSTM as building computational block for RNN, which is formulated as follows:\nrt = \u03c3(Wrxxt +Wrhht\u22121) (2)\nut = \u03c3(Wuxxt +Wuhht\u22121) (3)\nh\u0303t = tanh(Whxxt + rt \u2299Whhht\u22121) (4)\nht = ut \u2299 ht\u22121 + (1\u2212 ut)\u2299 h\u0303t (5)\nwhere: Wrx,Wrh,Wux,Wuh,Whx,Whh are weight matrices; rt, ut are reset and update gate, respectively, and \u2299 denotes for element-wise product. The Semantic Refinement GRU (SRGRU)-Base architecture is shown in Figure 1.\nFinally, the output distribution of each token is defined by\napplying a softmax function g as follows:\nP (wt+1 | wt, wt\u22121, ...w0, z) = g(Whoht) (6)\nwhere: Who is learned linear projection matrix. At training time, we use the ground truth token for the previous time step in place of the predicted output. At test time, we\nimplement a simple beam search to over-generate several candidate responses."}, {"heading": "B. SRGRU-CONTEXT", "text": "SRGRU-Base uses only the DA information to gate the input sequence token by token. As a results, this gating mechanism may not capture the relationship between multiple words. In order to import context information into the gating mechanism, the Equation 1 is modified as follows:\ndt = \u03c3(Wdzz+Wdhht\u22121) xt = dt \u2299 wt (7)\nwhere: Wdz and Wdh are weight matrices. Wdh acts like a key phrase detector that learns to capture the pattern of generation tokens or the relationship between multiple tokens. In other words, the new input xt consists of information of the original input token wt, the dialogue act z, and the hidden context ht\u22121. dt is called the refinement gate because the input tokens are refined by a combination gating information of the dialogue act z and the previous hidden state ht\u22121. By taking advantage of gating mechanism from the LSTM model [19] in which the gating mechanism is employed to solve the gradient vanishing and exploding problem, we propose to apply the refinement gate deeper into the GRU cell. Firstly, the GRU reset and update gates can be further influenced on the given dialogue act z and the refined input xt. The Equations (2) and (3) are modified as follows:\nrt = \u03c3(Wrxxt +Wrhht\u22121 +Wrzz) (8)\nut = \u03c3(Wuxxt +Wuhht\u22121 +Wuzz) (9)\nwhere: Wrz and Wuz act like background detectors that learn to control the style of the generating sentence. Secondly, Equation (4) is modified so that the candidate activation h\u0303t also depends on the refinement gate,\nh\u0303t = tanh(Whxxt + rt \u2299Whhht\u22121)\n+ tanh(Wdcdt) (10)\nBy this way, the reset and update gates learn not only the long-term dependency but also the gating information from the dialogue act and the previous hidden state. We call the resulting architecture Semantic Refinement GRU (SRGRU)Context which is shown in Figure 1."}, {"heading": "C. Training", "text": "The objective function was the negative log-likelihood and\ncomputed by:\nF (\u03b8) = \u2212\nT\u2211\nt=1\ny\u22a4t log pt (11)\nwhere yt is the ground truth word distribution, pt is the predicted word distribution, T is length of the input sequence. The generators were trained by treating each sentence as a mini-batch. The l2 regularization was added to the objective\nfunction for every 10 training examples. The models were initialized with pre-trained word vectors [20] and optimized by using stochastic gradient descent and back propagation through time. To prevent over-fitting, early stopping was implemented using a validation set as suggested in [16]."}, {"heading": "D. Decoding", "text": "The decoding phase consists of two tasks: (i) overgeneration, and (ii) reranking. In the over-generation phase, the forward generator conditioned on the given DA uses a beam search algorithm to generate candidate utterances. In the reranking phase, the cost of forward generator Ffw(\u03b8) is computed to form the reranking score R as follow:\nR = Ffw(\u03b8) + \u03bbERR (12)\nwhere \u03bb is a trade off constant which is set to a large value in order to severely penalize nonsensical outputs. The slot error rate ERR, which is the number of generated slots that are either redundant or missing, is computed by:\nERR = p+ q\nN (13)\nwhere N is the total number of slots in DA, and p, q is the number of missing and redundant slots, respectively. Note that the ERR reranking criteria cannot handle arbitrary slotvalue pairs such as binary slots or slots that take dont care value because they cannot be delexicalized and matched."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Datasets", "text": "We conducted experiments using four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. The Restaurant and Hotel domains were collected in [7] which contain system dialogue acts, shared slots, and specific domain slots. The Laptop and TV datasets have been released in [21] with about 13K distinct DAs in the Laptop and 7K distinct DAs in the TV. These two datasets have a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs. The number of dialogue act types and slots of datasets is also larger than in Restaurant and Hotel datasets. As a result, the NLG tasks for the Laptop and TV datasets become much harder."}, {"heading": "B. Experimental Setups", "text": "The generators were implemented using the TensorFlow library [22] and trained by partitioning each of the datasets into training, validation and testing set in the ratio 3:1:1. The hidden layer size was set to be 80, and the generators were trained with a 70% of dropout rate. We perform 5 runs with different random initialization of the network and the training is terminated by using early stopping as described in Section III-C. We select model that yields the highest\nBLEU score on the validation set. The decoder procedure used beam search with a beam width of 10. We set \u03bb to 1000 to severely discourage the reranker from selecting utterances which contain either redundant or missing slots. For each DA, we over-generated 20 candidate utterances and selected the top 5 realizations after reranking. Because the proposed models work stochastically, except the results reported in Table I, all the results shown were averaged over 5 randomly initialized networks.\nSince some sentences may depend on both the past and the future during generation, we train another backward SRGRU-Context to utilizing the flexibility of the refinement gate dt, in which we tie its weight matrices such Wdz and Wdh (Equation 7) for both. We found that by tying matrix Wdz for both forward and backward RNNs, the proposed generator seems to produce more correct and grammatical utterances than those having the only forward RNN. This model called Tying Backward SRGRU-Context (TB-SRGRU).\nIn order to better understand the effectiveness of our proposed model, we conduct more experiments to compare the SRGRU-Context with the state-of-the-art SCLSTM in a variety of setups on proportion of training corpus, beam size, and top-k best results. Firstly, the Restaurant and TV datasets were chosen, in which the SCLSTM model obtained the best performances and the NLG task comes from a limited domain to a more diverse domain as described in Section IV-A. In this setup, the models were run with different size of training corpus. Secondly, we examined the stability of SRGRU-Context model on different setups of beam size and top-k best results."}, {"heading": "C. Evaluation Metrics and Baselines", "text": "The generator performance was assessed by using two objective evaluation metrics, the BLEU score and the slot error rate ERR. Note that the slot error rate ERR was computed as an auxiliary metric alongside the BLEU score and calculated by averaging slot errors over each of the top 5 realizations in the entire corpus. Both metrics were computed by adopting code from an open source benchmark toolkit for Natural Language Generation3.\nWe compared our proposed models against with the general GRU (GRU-Base) and three strong baselines released from the NLG toolkit:\n\u2022 ENCDEC proposed in [23] which applies the attention\nmechanism to an RNN encoder-decoder.\n\u2022 HLSTM proposed in [6] which uses a heuristic gate to\nensure that all of the attribute-value information was accurately captured when generating. \u2022 SCLSTM proposed in [7] which can learn the gating\nsignal and language model jointly."}, {"heading": "V. RESULTS AND ANALYSIS", "text": "Overall, the proposed models SRGRUs consistently achieve better performance in term of the BLEU score in all domains. Especially, on the Hotel and TV datasets, the proposed models outperform the previous methods in both evaluation metrics. Moreover, our models also outperform the GRU basic model (GRU-Base) in all cases. However, the proposed models get worse on the Restaurant and Hotel datasets in terms of the error rate ERR score in comparison with SCLSTM. This indicates the advantage of the proposed\n3https://github.com/shawnwun/RNNLG\nrefinement gate. A comparison of the two proposed generators is shown in Table II: Without the backward RNN reranker, the generator tends to make semantic errors since it gains the higher slot error rate ERR. However, using the backward SRGRU reranker can improve the results in both evaluation metrics. This reranker provides benefit to the generator on producing higher-quality utterances.\nFigure 2 compares two generators trained with different proportion of data evaluated on two metrics. As can be seen in Figure 2a, the SCLSTM model achieves better results than SRGRU-Context model on both of BLEU and ERR scores since a small amount of training data was provided. However, the SRGRU-Context obtains the higher BLEU score and slightly higher ERR score as more training data was fed. On the other hand, in a more diverse dataset TV, the SRGRU-Context model consistently outperforms the SCLSTM on both evaluation metrics no matter how much training data is (Figure 2b). This is mainly due to the ability of refinement gate which feeds to the GRU model a new input xt conveying useful information filtered from the original input and the gating mechanism. Moreover, this gate also keeps the pattern of the generated utterance during generation. As a result, it can have a better realization of unseen slot-value pairs. Figure 3a shows an effect of beam\nsize on the SRGRU-Context model evaluated on Restaurant and TV datasets. As can be seen that, the model performs worse in terms of degrading the BLEU score and upgrading the slot error rate ERR when the beam size increases. The model seems to perform best with beam size less than 100. Figure 3b presents an effect of top-k best results in which we fixed the beam size at 100 and top-k best results varied as k = 1, 5, 10 and 20. In each case, the BLEU and the error rate ERR scores were computed on Restaurant and TV datasets. The results are consistent with Figure 3a in which the BLEU and ERR scores get worse as more top-k best utterances were chosen. Table III shows comparison of top responses generated by different models for given DAs. Firstly, both models SCLSTM and SRGRU-Context seem to produce the same kind of error, for example, grammar mistakes or missing information, partly because of using the same idea about gating mechanism. However, TB-SRGRU, with tying the refinement gate, has ability to fix this problem and produce the correct utterances (row 1 of Table III). Secondly, as noted earlier, one problem of the previous methods is the ability to handle the binary slot and slots that take don\u2019t care value. Both SCLSTM and the proposed models are able to handle this problem (row 2 of Table III). Finally, the TV dataset is more diverse and much\nharder than the others because the order of slot-value pairs should be considered during generation. For example, to generate a comparison sentence of 2 items Triton 52 and Hades 76 for given dialogue act compare(name=Triton 52; ecorating=A+; family=L7; name=Hades 76; ecorating=C; family=L9), the generator should consider that A+ and L7 values belong to the former item while C and L9 values to the latter. The HLSTM tends to make the repeated information error while the ENCDEC and SCLSTM seem to misplace the slot value during generation. Take L7 value, for instance, which should be generated follow the Triton 52 instead of Hades 76 as in row 3 of Table III. We found that both proposed models SRGRU-Context and TB-SRGRU can deal with this problem to generate appropriate utterances."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "We propose a gating mechanism GRU-based generator, in which we introduced a refinement gate to semantically refine the original input tokens. The refined inputs conveying meaningful information are then fed into the GRU cell. The proposed models can learn from the unaligned data to produce natural language responses conditioned on the given DA. We extensively evaluated our model on four NLG datasets and compared against the state-of-the-art generators. The results show that the proposed models obtain better performance than the existing generators on all of four NLG domains in terms of the BLEU and ERR metrics. In the future, we plan to further investigate the gating mechanism to multi-domain NLG since the refinement gate shows its ability to handle the unseen slot-value pairs."}], "references": [{"title": "Method and apparatus for building an intelligent automated assistant", "author": ["A. Cheyer", "D. Guzzoni"], "venue": "Mar. 18 2014, uS Patent 8,677,377.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["A.H. Oh", "A.I. Rudnicky"], "venue": "Proc. NAACL. ACL, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Trainable sentence planning for complex information presentation in spoken dialog systems", "author": ["A. Stent", "R. Prasad", "M. Walker"], "venue": "Proc. ACL. ACL, 2004, p. 79.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimising information presentation for spoken dialogue systems", "author": ["V. Rieser", "O. Lemon", "X. Liu"], "venue": "Proc. ACL. ACL, 2010, pp. 1009\u20131018.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequence-to-sequence generation for spoken dialogue via deep syntax trees and strings", "author": ["O. Du\u0161ek", "F. Jur\u010d\u0131\u0301\u010dek"], "venue": "arXiv preprint arXiv:1606.05491, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking", "author": ["T.-H. Wen", "M. Ga\u0161i\u0107", "D. Kim", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "Proc. SIGDIAL. ACL, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Ga\u0161i\u0107", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "Proc. EMNLP. ACL, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["T.-H. Wen", "D. Vandyke", "N. Mrksic", "M. Gasic", "L.M. Rojas- Barahona", "P.-H. Su", "S. Ultes", "S. Young"], "venue": "arXiv preprint arXiv:1604.04562, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proc. ACL. ACL, 2002, pp. 311\u2013318.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Trainable methods for surface natural language generation", "author": ["A. Ratnaparkhi"], "venue": "Proc. NAACL. ACL, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Stochastic language generation in dialogue using factored language models", "author": ["F. Mairesse", "S. Young"], "venue": "CL, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proc. CVPR, 2015, pp. 3128\u20133137.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Chinese poetry generation with recurrent neural networks.", "author": ["X. Zhang", "M. Lapata"], "venue": "in EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Incorporating unstructured textual knowledge sources into neural dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "L. Charlin", "J. Pineau"], "venue": "NIPS Workshop MLNLU, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Inner attention based recurrent neural networks for answer selection", "author": ["B. Wang", "K. Liu", "J. Zhao"], "venue": "2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Multi-domain neural network language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrksic", "L.M. Rojas-Barahona", "P.- H. Su", "D. Vandyke", "S. Young"], "venue": "arXiv preprint arXiv:1603.01232, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Toward multi-domain language generation using recurrent neural networks", "author": ["T.-H. Wen", "M. Ga\u0161ic", "N. Mrk\u0161ic", "L.M. Rojas-Barahona", "P.- H. Su", "D. Vandyke", "S. Young"], "venue": "2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "There are some common and widely approaches to solve NLG problems, including rule-based [1], corpus-based n-gram models [2], and a trainable generator [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "There are some common and widely approaches to solve NLG problems, including rule-based [1], corpus-based n-gram models [2], and a trainable generator [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "There are some common and widely approaches to solve NLG problems, including rule-based [1], corpus-based n-gram models [2], and a trainable generator [3].", "startOffset": 151, "endOffset": 154}, {"referenceID": 3, "context": "Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "Joint based generators use a two-step pipeline [4], [5]; or applying a joint model for both tasks [6], [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "RNNbased models have been used for NLG as a joint training model [6], [7] and an end-to-end training model [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "RNNbased models have been used for NLG as a joint training model [6], [7] and an end-to-end training model [8].", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "RNNbased models have been used for NLG as a joint training model [6], [7] and an end-to-end training model [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "inform(name=\u2018Frances\u2019; area=\u2018City Center\u2019) which cannot be directly delexicalized [6], or to generalize to unseen domains [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "inform(name=\u2018Frances\u2019; area=\u2018City Center\u2019) which cannot be directly delexicalized [6], or to generalize to unseen domains [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "The results showed that our proposed method outperforms the previous state-of-the-art methods in terms of BLEU [9] and slot error rate ERR [7] scores.", "startOffset": 111, "endOffset": 114}, {"referenceID": 6, "context": "The results showed that our proposed method outperforms the previous state-of-the-art methods in terms of BLEU [9] and slot error rate ERR [7] scores.", "startOffset": 139, "endOffset": 142}, {"referenceID": 2, "context": "The surface realization then converts the structure into an appropriate sentence [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "[2] proposed a class-based n-gram language model (LM) generator which can learn to generate the sentences for a given DA and then select the best sentences using a rule-based reranker.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] addressed some of the limitation of the class-based LMs by proposing a method based on a syntactic dependency tree.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "based generator based on factored LMs was introduced in [11], which can learn from a semantically aligned corpus.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "[12], [13] used RNNs in a multi-modal setting to generate captions for images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12], [13] used RNNs in a multi-modal setting to generate captions for images.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "[14] also proposed a generator using RNNs to create Chinese poetry.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] encoded an unstructured textual knowledge source along with previous responses and context to produce a response for technical support queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "For task-oriented dialogue systems, [6] combined a forward RNN generator, a CNN reranker, and a backward RNN reranker to generate utterances.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "A semantically conditioned-based Long ShortTerm Memory (LSTM) generator was proposed in [7], which introduced a control \u201creading\u201d gate to the LSTM cell and can learn the gating mechanism and language model jointly.", "startOffset": 88, "endOffset": 91}, {"referenceID": 15, "context": "The recurrent language generator proposed in this paper is based on a RNN language model [16], which consists of three layers: an input layer, a hidden layer and an output layer.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "This process finishes when a stop sign is generated [12], or some constraint is reached [14].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "This process finishes when a stop sign is generated [12], or some constraint is reached [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "Inspired by work in [17], we propose an intuition: Gating before computation, in which we add gating mechanism before the RNN computation to semantically refine the input tokens.", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "In this study, we use GRU, which was recently proposed in [18], instead of LSTM as building computational block for RNN, which is formulated as follows:", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "By taking advantage of gating mechanism from the LSTM model [19] in which the gating mechanism is employed to solve the gradient vanishing and exploding problem, we propose to apply the refinement gate deeper into the GRU cell.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "The models were initialized with pre-trained word vectors [20] and optimized by using stochastic gradient descent and back propagation through time.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "To prevent over-fitting, early stopping was implemented using a validation set as suggested in [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "The Restaurant and Hotel domains were collected in [7] which contain system dialogue acts, shared slots, and specific domain slots.", "startOffset": 51, "endOffset": 54}, {"referenceID": 20, "context": "The Laptop and TV datasets have been released in [21] with about 13K distinct DAs in the Laptop and 7K distinct DAs in the TV.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "The generators were implemented using the TensorFlow library [22] and trained by partitioning each of the datasets into training, validation and testing set in the ratio 3:1:1.", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "\u266e reported in [7].", "startOffset": 14, "endOffset": 17}, {"referenceID": 22, "context": "\u2022 ENCDEC proposed in [23] which applies the attention mechanism to an RNN encoder-decoder.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "\u2022 HLSTM proposed in [6] which uses a heuristic gate to ensure that all of the attribute-value information was accurately captured when generating.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "\u2022 SCLSTM proposed in [7] which can learn the gating signal and language model jointly.", "startOffset": 21, "endOffset": 24}], "year": 2017, "abstractText": "Natural language generation (NLG) plays a critical role in spoken dialogue systems. This paper presents a new approach to NLG by using recurrent neural networks (RNN), in which a gating mechanism is applied before RNN computation. This allows the proposed model to generate appropriate sentences. The RNN-based generator can be learned from unaligned data by jointly training sentence planning and surface realization to produce natural language responses. The model was extensively evaluated on four different NLG domains. The results show that the proposed generator achieved better performance on all the NLG domains compared to previous generators.", "creator": "LaTeX with hyperref package"}}}