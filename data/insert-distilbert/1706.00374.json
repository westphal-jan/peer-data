{"id": "1706.00374", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints", "abstract": "we present attract - repel, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. attract - repel effectively facilitates the use of constraints from mono - and cross - lingual resources, yielding semantically specialised cross - lingual vector spaces. our evaluation shows that the method can make creative use of existing cross - lingual lexicons to construct high - quality vector mesh spaces for a plethora of different constraint languages, facilitating semantic transfer from high - to lower - resource ones. the effectiveness of our approach is demonstrated with state - of - the - art results on semantic similarity datasets in six languages. we next tools show that attract - directed repel - specialised vectors boost performance in the downstream task of dialogue state tracking ( dst ) function across genetically multiple languages. recently finally, we show that cross - lingual vector spaces produced by our algorithm facilitate the training of multilingual dst models, which likely brings further performance improvements.", "histories": [["v1", "Thu, 1 Jun 2017 16:29:47 GMT  (71kb,D)", "http://arxiv.org/abs/1706.00374v1", "Accepted for publication at TACL (to be presented at EMNLP 2017)"]], "COMMENTS": "Accepted for publication at TACL (to be presented at EMNLP 2017)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["nikola mrk\\v{s}i\\'c", "ivan vuli\\'c", "diarmuid \\'o s\\'eaghdha", "ira leviant", "roi reichart", "milica ga\\v{s}i\\'c", "anna korhonen", "steve young"], "accepted": true, "id": "1706.00374"}, "pdf": {"name": "1706.00374.pdf", "metadata": {"source": "CRF", "title": "Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints", "authors": ["Nikola Mrk\u0161i\u0107", "Ivan Vuli\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Ira Leviant", "Roi Reichart", "Milica Ga\u0161i\u0107", "Anna Korhonen", "Steve Young"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; \u00d3 S\u00e9aghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods which go beyond stand-alone unsupervised learning have gained increased popularity.\nThese models typically build on distributional ones by using human- or automatically-constructed knowledge bases to enrich the semantic content of existing word vector collections. Often this is done as a postprocessing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrk\u0161ic\u0301 et al., 2016). We term this approach semantic specialisation.\nIn this paper we advance the semantic specialisation paradigm in a number of ways. We introduce a new algorithm, ATTRACT-REPEL, that uses synonymy and antonymy constraints drawn from lexical resources to tune word vector spaces using linguistic information that is difficult to capture with conventional distributional training. Our evaluation shows that ATTRACT-REPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016).\nWe then deploy the ATTRACT-REPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to improve the word representations of lower-resource ones. Table 1 illustrates the effects of cross-lingual ATTRACT-REPEL specialisation by showing the nearest neighbours for three English words across three cross-lingual spaces.\nar X\niv :1\n70 6.\n00 37\n4v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\nIn each case, the vast majority of each words\u2019 neighbours are meaningful synonyms/translations.1\nWhile there is a considerable amount of prior research on joint learning of cross-lingual vector spaces (see Sect. 2.2), to the best of our knowledge we are the first to apply semantic specialisation to this problem.2 We demonstrate its efficacy with state-of-theart results on the four languages in the Multilingual SimLex-999 dataset (Leviant and Reichart, 2015). To show that our approach yields semantically informative vectors for lower-resource languages, we collect intrinsic evaluation datasets for Hebrew and Croatian and show that cross-lingual specialisation significantly improves word vector quality in these two (comparatively) low-resource languages.\nIn the second part of the paper, we explore the use of ATTRACT-REPEL-specialised vectors in a downstream application. One important motivation for training word vectors is to improve the lexical coverage of supervised models for language understanding tasks, e.g. question answering (Iyyer et al., 2014) or textual entailment (Rockt\u00e4schel et al., 2016). In\n1Some residual (negative) effects of the distributional hypothesis do persist. For example, nl_krieken, which is Dutch for cherries, is (presumably) identified as a synonym for en_morning due to a song called \u2018a Morning Wish\u2019 by Emile Van Krieken.\n2Our approach is not suited for languages for which no lexical resources exist. However, many languages have some coverage in cross-lingual lexicons. For instance, BabelNet 3.7 automatically aligns WordNet to Wikipedia, providing accurate cross-lingual mappings between 271 languages. In our evaluation, we demonstrate substantial gains for Hebrew and Croatian, both of which are spoken by less than 10 million people worldwide.\nthis work, we use the task of dialogue state tracking (DST) for extrinsic evaluation. This task, which arises in the construction of statistical dialogue systems (Young et al., 2013), involves understanding the goals expressed by the user and updating the system\u2019s distribution over such goals as the conversation progresses and new information becomes available.\nWe show that incorporating our specialised vectors into a state-of-the-art neural-network model for DST improves performance on English dialogues. In the multilingual spirit of this paper, we produce new Italian and German DST datasets and show that using ATTRACT-REPEL-specialised vectors leads to even stronger gains in these two languages. Finally, we show that our cross-lingual vectors can be used to train a single model that performs DST in all three languages, in each case outperforming the monolingual model. To the best of our knowledge, this is the first work on multilingual training of any component of a statistical dialogue system. Our results indicate that multilingual training holds great promise for bootstrapping language understanding models for other languages, especially for dialogue domains where data collection is very resource-intensive.\nAll resources relating to this paper are available at www.github.com/nmrksic/ attract-repel. These include: 1) the ATTRACTREPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Semantic Specialisation", "text": "The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialisation for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrk\u0161ic\u0301 et al., 2016; Mrk\u0161ic\u0301 et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vulic\u0301 et al., 2016).\nSemantic specialisation methods (broadly) fall into two categories: a) those which train distributed representations \u2018from scratch\u2019 by combining distributional knowledge and lexical information; and b) those which inject lexical information into pre-trained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Databases (PPDB) (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014; Pavlick et al., 2015).\nLearning from Scratch: some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other ones modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasise word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in\ntheir pattern-based vector space. Ono et al. (2015) combine both approaches, using thesauri and distributional data to train embeddings specialised for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks.\nIn theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3\nFine-Tuning Pre-trained Vectors: Rothe and Sch\u00fctze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. Faruqui et al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrk\u0161ic\u0301 et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles of rich concept dictionaries to further improve a combined collection of semantically specialised word vectors.\nATTRACT-REPEL is an instance of the second family of models, providing a portable, light-weight approach for incorporating external knowledge into arbitrary vector spaces. In our experiments, we show that ATTRACT-REPEL outperforms previously proposed post-processors, setting the new state-of-art performance on the widely used SimLex-999 word similarity dataset. Moreover, we show that starting from distributional vectors allows our method to use existing cross-lingual resources to tie distributional vector spaces of different languages into a unified vector space which benefits from positive semantic transfer between its constituent languages.\n3The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly."}, {"heading": "2.2 Cross-Lingual Word Representations", "text": "Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentencealigned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other ones require documentaligned data (S\u00f8gaard et al., 2015; Vulic\u0301 and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vulic\u0301 and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vulic\u0301 and Korhonen (2016b) for an overview of cross-lingual word embedding work.\nThe inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vulic\u0301 and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (S\u00f8gaard et al., 2015; Guo et al., 2015).\nHowever, prior work on cross-lingual word embedding has tended not to exploit pre-existing linguistic resources such as BabelNet. In this work, we make use of cross-lingual constraints derived from such repositories to induce high-quality cross-lingual vector spaces by facilitating semantic transfer from highto lower-resource languages. In our experiments, we show that cross-lingual vector spaces produced by ATTRACT-REPEL consistently outperform a representative selection of five strong cross-lingual word embedding models in both intrinsic and extrinsic evaluation across several languages."}, {"heading": "3 The ATTRACT-REPEL Model", "text": "In this section, we propose a new algorithm for producing semantically specialised word vectors by in-\njecting similarity and antonymy constraints into distributional vector spaces. This procedure, which we term ATTRACT-REPEL, builds on the Paragram (Wieting et al., 2015) and counter-fitting procedures (Mrk\u0161ic\u0301 et al., 2016), both of which inject linguistic constraints into existing vector spaces to improve their ability to capture semantic similarity.\nLet V be the vocabulary, S the set of synonymous word pairs (e.g. intelligent and brilliant), and A the set of antonymous word pairs (e.g. vacant and occupied). For ease of notation, let each word pair (xl, xr) in these two sets correspond to a vector pair (xl,xr). The optimisation procedure operates over mini-batches B, where each of these consists of a set of synonymy pairs BS (of size k1) and a set of antonymy pairs BA (of size k2). Let TS(BS) = [(t1l , t1r), . . . , (t k1 l , t k1 r )] and TA(BA) = [(t1l , t 1 r), . . . , (t k2 l , t k2 r )] be the pairs of negative examples for each synonymy and antonymy example pair. These negative examples are chosen from the 2(k1 + k2) word vectors present in BS \u222a BA:\n\u2022 For each synonymy pair (xl,xr), the negative example pair (tl, tr) is chosen from the remaining in-batch vectors so that tl is the one closest (cosine similarity) to xl and tr is closest to xr.\n\u2022 For each antonymy pair (xl,xr), the negative example pair (tl, tr) is chosen from the remaining in-batch vectors so that tl is the one furthest away from xl and tr is the one furthest from xr.\nThese negative examples are used to: a) force synonymous pairs to be closer to each other than to their respective negative examples; and b) to force antonymous pairs to be further away from each other than from their negative examples. The first term of the cost function pulls synonymous words together:\nS(BS) = \u2211\n(xl,xr)\u2208BS\n[ \u03c4 (\u03b4syn + xltl \u2212 xlxr)\n+ \u03c4 (\u03b4syn + xrtr \u2212 xlxr) ]\nwhere \u03c4(x) = max(0, x) is the hinge loss function and \u03b4syn is the similarity margin which determines how much closer synonymous vectors should be to each other than to their respective negative examples. The second part of the cost function pushes antony-\nmous word pairs away from each other: A(BA) = \u2211\n(xl,xr)\u2208BA\n[ \u03c4 (\u03b4ant + xlxr \u2212 xltl)\n+ \u03c4 (\u03b4ant + xlxr \u2212 xrtr) ]\nIn addition to these two terms, we include an additional regularisation term which aims to preserve the abundance of high-quality semantic content present in the initial (distributional) vector space, as long as this information does not contradict the injected linguistic constraints. If V (B) is the set of all word vectors present in the given mini-batch, then:\nR(BS ,BA) = \u2211\nxi\u2208V (BS\u222aBA)\n\u03bbreg \u2016x\u0302i \u2212 xi\u20162\nwhere \u03bbreg is the L2 regularisation constant and x\u0302i denotes the original (distributional) word vector for word xi. The final cost function of the ATTRACTREPEL algorithm can then be expressed as:\nC(BS ,BA) = S(BS) +A(BA) +R(BS ,BA)\nComparison to Prior Work ATTRACT-REPEL draws inspiration from three methods: 1) retrofitting (Faruqui et al., 2015); 2) PARAGRAM (Wieting et al., 2015); and 3) counter-fitting (Mrk\u0161ic\u0301 et al., 2016). Whereas retrofitting and PARAGRAM do not consider antonymy, counter-fitting models both synonymy and antonymy. ATTRACT-REPEL differs from this method in two important ways:\n1. Context-Sensitive Updates: Counter-fitting uses attract and repel terms which pull synonyms together and push antonyms apart without considering their relation to other word vectors. For example, its \u2018attract term\u2019 is given by:\nAttract(S) = \u2211\n(xl,xr)\u2208S \u03c4(\u03b4syn \u2212 xlxr)\nwhere S is the set of synonymy constraints and \u03b4syn is the (minimum) similarity enforced between synonyms. Conversely, ATTRACT-REPEL fine-tunes vector spaces by operating over minibatches of example pairs, updating word vectors only if the position of their negative example implies a stronger semantic relation than that expressed by the position of its target example. Importantly, ATTRACT-REPEL makes finegrained updates to both the example pair and\nthe negative examples, rather than updating the example word pair but ignoring how this affects its relation to all other word vectors.\n2. Regularisation: Counter-fitting preserves distances between pairs of word vectors in the initial vector space, trying to \u2018pull\u2019 the words\u2019 neighbourhoods with them as they move to incorporate external knowledge. The radius of this initial neighbourhood introduces an opaque hyperparameter to the procedure. Conversely, ATTRACT-REPEL implements standard L2 regularisation, which \u2018pulls\u2019 each vector towards its distributional vector representation.\nIn our intrinsic evaluation (Sect. 5), we perform an exhaustive comparison of these models, showing that ATTRACT-REPEL significantly outperforms counterfitting in both mono- and cross-lingual setups.\nOptimisation Following Wieting et al. (2015), we use the AdaGrad algorithm (Duchi et al., 2011) to train the word embeddings for five epochs, which suffices for the magnitude of the parameter updates to converge. Similar to Faruqui et al. (2015), Wieting et al. (2015) and Mrk\u0161ic\u0301 et al. (2016), we do not use early stopping. By not relying on language-specific validation sets, the ATTRACT-REPEL procedure can induce semantically specialised word vectors for languages with no intrinsic evaluation datasets.4\nHyperparameter Tuning We use Spearman\u2019s correlation of the final word vectors with the Multilingual WordSim-353 gold-standard association dataset (Finkelstein et al., 2002; Leviant and Reichart, 2015). The ATTRACT-REPEL procedure has six hyperparameters: the regularization constant \u03bbreg, the similarity and antonymy margins \u03b4sim and \u03b4ant, mini-batch sizes k1 and k2, and the size of the PPDB constraint set used for each language (larger sizes include more constraints, but also a larger proportion of false synonyms). We ran a grid search over these for the four SimLex languages, choosing the hyperparameters which achieved the best WordSim-353 score.5\n4Many languages are present in semi-automatically constructed lexicons such as BabelNet or PPDB (see the discussion in Sect. 4.2.). However, intrinsic evaluation datasets such as SimLex-999 exist for very few languages, as they require expert translators and skilled annotators.\n5We ran the grid search over \u03bbreg \u2208 [10\u22123, . . . , 10\u221210], \u03b4sim, \u03b4ant \u2208 [0, 0.1, . . . , 1.0], k1, k2 \u2208 [10, 25, 50, 100, 200]"}, {"heading": "4 Experimental Setup", "text": ""}, {"heading": "4.1 Distributional Vectors", "text": "We first present our sixteen experimental languages: English (EN), German (DE), Italian (IT), Russian (RU), Dutch (NL), Swedish (SV), French (FR), Spanish (ES), Portuguese (PT), Polish (PL), Bulgarian (BG), Croatian (HR), Irish (GA), Persian (FA) and Vietnamese (VI). The first four languages are those of the Multilingual SimLex-999 dataset.\nFor the four SimLex languages, we employ four well-known, high-quality word vector collections: a) The Common Crawl GloVe English vectors from Pennington et al. (2014); b) German vectors from Vulic\u0301 and Korhonen (2016a); c) Italian vectors from Dinu et al. (2015); and d) Russian vectors from Kutuzov and Andreev (2015). In addition, for each of the 16 languages we also train the skip-gram with negative sampling variant of the word2vec model (Mikolov et al., 2013b), on the latest Wikipedia dump of each language, to induce 300-dimensional word vectors.6"}, {"heading": "4.2 Linguistic Constraints", "text": "Table 2 shows the number of monolingual and crosslingual constraints for the four SimLex languages.\nMonolingual Similarity We employ the Multilingual Paraphrase Database (Ganitkevitch and CallisonBurch, 2014). This resource contains paraphrases automatically extracted from parallel-aligned corpora\nand over the six PPDB sizes for the four SimLex languages. \u03bbreg = 10\n\u22129, \u03b4sim = 0.6, \u03b4ant = 0.0 and k1 = k2 \u2208 [10, 25, 50] consistently achieved the best performance (we use k1 = k2 = 50 in all experiments for consistency). The PPDB constraint set size XL was best for English, German and Italian, and M achieved the best performance for Russian.\n6The frequency cut-off was set to 50: words that occurred less frequently were removed from the vocabularies. Other word2vec parameters were set to the standard values (Vulic\u0301 and Korhonen, 2016a): 15 epochs, 15 negative samples, global (decreasing) learning rate: 0.025, subsampling rate: 1e\u2212 4.\nfor ten of our sixteen languages. In our experiments, the remaining six languages (HE, HR, SV, GA, VI, FA) serve as examples of lower-resource languages, as they have no monolingual synonymy constraints.\nCross-Lingual Similarity We employ BabelNet, a multilingual semantic network automatically constructed by linking Wikipedia to WordNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014). BabelNet groups words from different languages into Babel synsets. We consider two words from any (distinct) language pair to be synonymous if they belong to (at least) one set of synonymous Babel synsets. We made use of all BabelNet word senses tagged as conceptual but ignored the ones tagged as Named Entities.\nGiven a large collection of cross-lingual semantic constraints (e.g. the translation pair en_sweet and it_dolce), ATTRACT-REPEL can use them to bring the vector spaces of different languages together into a shared cross-lingual space. Ideally, sharing information across languages should lead to improved semantic content for each language, especially for those with limited monolingual resources.\nAntonymy BabelNet is also used to extract both monolingual and cross-lingual antonymy constraints. Following Faruqui et al. (2015), who found PPDB constraints more beneficial than the WordNet ones, we do not use BabelNet for monolingual synonymy.\nAvailability of Resources Both PPDB and BabelNet are created automatically. However, PPDB relies on large, high-quality parallel corpora such as Europarl (Koehn, 2005). In total, Multilingual PPDB provides collections of paraphrases for 22 languages. On the other hand, BabelNet uses Wikipedia\u2019s interlanguage links and statistical machine translation (Google Translate) to provide cross-lingual mappings for 271 languages. In our evaluation, we show that PPDB and BabelNet can be used jointly to improve word representations for lower-resource languages by tying them into bilingual spaces with high-resource ones. We validate this claim on Hebrew and Croatian, which act as \u2018lower-resource\u2019 languages because of their lack of any PPDB resource and their relatively small Wikipedia sizes.7\n7Hebrew and Croatian Wikipedias (which are used to induce their BabelNet constraints) currently consist of 203,867 / 172,824 articles, ranking them 40th / 42nd by size."}, {"heading": "5 Intrinsic Evaluation", "text": ""}, {"heading": "5.1 Datasets", "text": "Spearman\u2019s rank correlation with the SimLex-999 dataset (Hill et al., 2015) is used as the intrinsic evaluation metric throughout the experiments. Unlike other gold standard resources such as WordSim-353 (Finkelstein et al., 2002) or MEN (Bruni et al., 2014), SimLex-999 consists of word pairs scored by annotators instructed to discern between semantic similarity and conceptual association, so that related but nonsimilar words (e.g. book and read) have a low rating.\nLeviant and Reichart (2015) translated SimLex999 to German, Italian and Russian, crowd-sourcing the similarity scores from native speakers of these languages. We use this resource for multilingual intrinsic evaluation.8 To investigate the portability of our approach to lower-resource languages, we used the same experimental setup to collect SimLex999 datasets for Hebrew and Croatian.9 For English vectors, we also report Spearman\u2019s correlation with SimVerb-3500 (Gerz et al., 2016), a semantic similarity dataset that focuses on verb pair similarity."}, {"heading": "5.2 Experiments", "text": "Monolingual and Cross-Lingual Specialisation We start from distributional vectors for the SimLex languages: English, German, Italian and Russian. For each language, we first perform semantic specialisation of these spaces using: a) monolingual synonyms; b) monolingual antonyms; and c) the combination of both. We then add cross-lingual synonyms and antonyms to these constraints and train a shared fourlingual vector space for these languages.\nComparison to Baseline Methods Both monoand cross-lingual specialisation was performed using ATTRACT-REPEL and counter-fitting, in order to conclusively determine which of the two methods exhibited superior performance. Retrofitting and PARAGRAM methods only inject synonymy, and their cost functions can be expressed using sub-components of\n8Leviant and Reichart (2015) also re-scored the original English SimLex. We report results on their version, but also provide numbers for the original dataset for comparability.\n9The 999 word pairs and annotator instructions were translated by native speakers and scored by 10 annotators. The interannotator agreement scores (Spearman\u2019s \u03c1) were 0.77 (pairwise) and 0.87 (mean) for Croatian, and 0.59 / 0.71 for Hebrew.\ncounter-fitting and ATTRACT-REPEL cost functions. As such, the performance of the two investigated methods when they make use of similarity (but not antonymy) constraints illustrates the performance range of the two preceding models.\nImportance of Initial Vectors We use three different sets of initial vectors: a) well-known distributional word vector collections (Sect. 4.1); b) distributional vectors trained on the latest Wikipedia dumps; and c) word vectors randomly initialised using the XAVIER initialisation (Glorot and Bengio, 2010).\nSpecialisation for Lower-Resource Languages In this experiment, we first construct bilingual spaces which combine: a) one of the four SimLex languages; with b) each of the other twelve languages.10 Since each pair contains at least one SimLex language, we can analyse the improvement over monolingual specialisation to understand how robust the performance gains are across different language pairs. We next use the newly collected SimLex datasets for Hebrew and Croatian to evaluate the extent to which bilingual semantic specialisation using ATTRACT-REPEL and BabelNet constraints can improve word representations for lower-resource languages.\nComparison to State-of-the-Art Bilingual Spaces The English-Italian and English-German bilingual spaces induced by ATTRACT-REPEL were compared to five state-of-the-art methods for constructing bilingual vector spaces: 1. (Mikolov et al., 2013a), retrained using the constraints used by our model; and 2.-5. (Hermann and Blunsom, 2014a; Gouws et al., 2015; Vulic\u0301 and Korhonen, 2016a; Vulic\u0301 and Moens, 2016). The latter models use various sources of supervision (word-, sentence- and document-aligned corpora), which means they cannot be trained using our sets of constraints. For these models, we use competitive setups proposed in (Vulic\u0301 and Korhonen, 2016a). The goal of this experiment is to show that vector spaces induced by ATTRACT-REPEL exhibit better intrinsic and extrinsic performance when deployed in language understanding tasks.\n10Hyperparameters: we used \u03b4sim = 0.6, \u03b4ant = 0.0 and \u03bbreg = 10\n\u22129, which achieved the best performance when tuned for the original SimLex languages. The largest available PPDB size was used for the six languages with available PPDB (French, Spanish, Portuguese, Polish, Bulgarian and Dutch)."}, {"heading": "5.3 Results and Discussion", "text": "Table 3 shows the effects of monolingual and crosslingual semantic specialisation of four well-known distributional vector spaces for the SimLex languages. Monolingual specialisation leads to very strong improvements in the SimLex performance across all languages. Cross-lingual specialisation brings further improvements, with all languages benefiting from sharing the cross-lingual vector space. Italian in particular shows strong evidence of effective transfer, with Italian vectors\u2019 performance coming close to the top-performing English ones.\nComparison to Baselines Table 3 gives an exhaustive comparison of ATTRACT-REPEL to counterfitting: ATTRACT-REPEL achieved substantially stronger performance in all experiments. We believe these results conclusively show that the finegrained updates and L2 regularisation employed by ATTRACT-REPEL present a better alternative to the context-insensitive attract/repel terms and pair-wise regularisation employed by counter-fitting.\nState-of-the-Art Wieting et al. (2016) note that the hyperparameters of the widely used Paragram-SL999 vectors (Wieting et al., 2015) are tuned on SimLex999, and as such are not comparable to methods which holdout the dataset. This implies that further work which uses these vectors (e.g., (Mrk\u0161ic\u0301 et al., 2016; Recski et al., 2016)) as starting point does not yield meaningful high scores either. Our reported English score of 0.71 on the Multilingual SimLex-999 corresponds to 0.751 on the original SimLex-999: it outperforms the 0.706 score reported by Wieting et al. (2016) and sets a new high score for this dataset. Similarly, the SimVerb-3500 score of these vectors is 0.674, outperforming the current state-of-the-art score of 0.628 reported by Gerz et al. (2016).\nStarting Distributional Spaces Table 4 repeats the previous experiment with two different sets of initial vector spaces: a) randomly initialised word vectors;11 and b) skip-gram with negative sampling vectors trained on the latest Wikipedia dumps. The randomly initialised vectors serve to decouple the impact of injecting external knowledge from the information embedded in the distributional vectors. The random vectors benefit from both mono- and crosslingual specialisation: the English performance is surprisingly strong, with other languages suffering more from the lack of initialisation.\n11The XAVIER initialisation populates the values for each word vector by uniformly sampling from the interval [\u2212\n\u221a 6\u221a d ,+ \u221a 6\u221a d ],\nwhere d is the vector dimensionality. This is a typical init method in neural nets research (Goldberg, 2015; Bengio et al., 2013).\nWhen comparing distributional vectors trained on Wikipedia to the high-quality word vector collections used in Table 3, the Italian and Russian vectors in particular start from substantially weaker SimLex scores. The difference in performance is largely mitigated through semantic specialisation. However, all vector spaces still exhibit weaker performance compared to those in Table 3. We believe this shows that the quality of the initial distributional vector spaces is important, but can in large part be compensated for through semantic specialisation.\nBilingual Specialisation Table 5 shows the effect of combining the four original SimLex languages with each other and with twelve other languages (Sect. 4.1). Bilingual specialisation substantially improves over monolingual specialisation for all language pairs. This indicates that our improvements are language independent to a large extent.\nInterestingly, even though we use no monolingual synonymy constraints for the six right-most languages, combining them with the SimLex languages still improved word vector quality for these four highresource languages. The reason why even resourcedeprived languages such as Irish help improve vector space quality of high-resource ones such as English or Italian is that they provide implicit indicators of\nsemantic similarity. English words which map to the same Irish word are likely to be synonyms, even if those English pairs are not present in the PPDB datasets (Faruqui and Dyer, 2014).12\nLower-Resource Languages The previous experiment indicates that bilingual specialisation further improves the (already) high-quality estimates for highresource languages. However, it does little to show how much (or if) the word vectors of lower-resource languages improve during such specialisation. Table 6 investigates this proposition using the newly collected SimLex datasets for Hebrew and Croatian.\nTying the distributional vectors for these languages (which have no monolingual constraints) into crosslingual spaces with high-resource ones (which do, in our case from PPDB) leads to substantial improvements. Table 6 also shows how the distributional vectors of the four SimLex languages improve when tied to other languages (in each row, we use monolingual constraints only for the \u2018added\u2019 language). Hebrew and Croatian exhibit similar trends to the original SimLex languages: tying to English and Italian leads to stronger gains than tying to the morphologically sophisticated German and Russian. Indeed, tying to English consistently lead to strongest performance. We believe this shows that bilingual ATTRACT-REPEL specialisation with English promises to produce highquality vector spaces for many lower-resource languages which have coverage among the 271 BabelNet languages (but are not available in PPDB).\nExisting Bilingual Spaces Table 7 compares the intrinsic (i.e. SimLex-999) performance of bilingual English-Italian and English-German vectors produced by ATTRACT-REPEL to five previously proposed approaches for constructing bilingual vector\n12We release bilingual vector spaces for EN + 51 other languages: the 16 presented here and another 35 languages (all available at www.github.com/nmrksic/attract-repel).\nspaces. For both languages in both language pairs, ATTRACT-REPEL achieves substantial gains over all of these methods. In the next section, we show that these differences in intrinsic performance lead to substantial gains in downstream evaluation."}, {"heading": "6 Downstream Task Evaluation", "text": ""}, {"heading": "6.1 Dialogue State Tracking", "text": "Task-oriented dialogue systems help users achieve goals such as making travel reservations or finding restaurants. In slot-based systems, application domains are defined by ontologies which enumerate the goals that users can express (Young, 2010). The goals are expressed by slot-value pairs such as [price: cheap] or [food: Thai]. For modular task-based systems, the Dialogue State Tracking (DST) component is in charge of maintaining the belief state, which is the system\u2019s internal distribution over the possible states of the dialogue. Figure 1 shows the correct dialogue state for each turn of an example dialogue.\nUnseen Data/Labels As dialogue ontologies can be very large, many of the possible class labels (i.e., the various food types or street names) will not occur in the training set. To overcome this problem, delexicalisation-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrk\u0161ic\u0301 et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values. This is done through exact matching supplemented with semantic lexicons which encode rephrasings, morphology and other linguistic variation. For instance, such lexicons would be required to deal with the underlined non-exact matches in Figure 1.\nExact Matching as a Bottleneck Semantic lexicons can be hand-crafted for small dialogue domains. Mrk\u0161ic\u0301 et al. (2016) showed that semantically specialised vector spaces can be used to automatically induce such lexicons for simple dialogue domains. However, as domains grow more sophisticated, the reliance on (manually- or automatically-constructed) semantic dictionaries which list potential rephrasings for ontology values becomes a bottleneck for deploying dialogue systems. Ambiguous rephrasings are just one problematic instance of this approach: a user asking about Iceland could be referring to the country or the supermarket chain, and someone asking for songs by Train is not interested in train timetables. More importantly, the use of English as the principal language in most dialogue systems research understates the challenges that complex linguistic phenomena present in other languages. In this work, we investigate the extent to which semantic specialisation can empower DST models which do not rely on such dictionaries.\nNeural Belief Tracker (NBT) The NBT is a novel DST model which operates purely over distributed representations of words, learning to compose utterance and context representations which it then uses to decide which of the potentially many ontologydefined intents (goals) have been expressed by the user (Mrk\u0161ic\u0301 et al., 2017). To overcome the data sparsity problem, the NBT uses label embedding to decompose this multi-class classification problem into many binary classification ones: for each slot, the\nmodel iterates over slot values defined by the ontology, deciding whether each of them was expressed in the current utterance and its surrounding context. The first NBT layer consists of neural networks which produce distributed representations of the user utterance,13 the preceding system output and the embedded label of the candidate slot-value pair. These representations are then passed to the downstream semantic decoding and context modelling networks, which subsequently make the binary decision regarding the current slot-value candidate. When contradicting goals are detected (i.e. cheap and expensive), the model chooses the more probable one.\nThe NBT training procedure keeps the initial word vectors fixed: that way, at test time, unseen words semantically related to familiar slot values (i.e. affordable or cheaper to cheap) are recognised purely by their position in the original vector space. Thus, it is essential that deployed word vectors are specialised for semantic similarity, as distributional effects which keep antonymous words\u2019 vectors together can be very detrimental to DST performance (e.g., by matching northern to south or inexpensive to expensive).\nThe Multilingual WOZ 2.0 Dataset Our DST evaluation is based on the WOZ 2.0 dataset introduced by Wen et al. (2017) and Mrk\u0161ic\u0301 et al. (2017). This dataset is based on the ontology used for the 2nd DST Challenge (DSTC2) (Henderson et al., 2014a). It consists of 1,200 Wizard-of-Oz (Fraser and Gilbert, 1991) dialogues in which Amazon Mechanical Turk users assumed the role of the dialogue system or the caller looking for restaurants in Cambridge, UK. Since users typed instead of using speech and interacted with intelligent assistants, the language they used was more sophisticated than in case of DSTC2, where users would quickly adapt to the system\u2019s inability to cope with complex queries. For our experiments, the ontology and 1,200 dialogues were translated to Italian and German through gengo.com, a web-based human translation platform."}, {"heading": "6.2 DST Experiments", "text": "The principal evaluation metric in our DST experiments is the joint goal accuracy, which represents\n13There are two variants of the NBT model: NBT-DNN and NBT-CNN. In this work, we limit our investigation to the latter one, as it achieved consistently stronger DST performance.\nthe proportion of test set dialogue turns where all the search constraints expressed up to that point in the conversation were decoded correctly. Our DST experiments investigate two propositions:\n1. Intrinsic vs. Downstream Evaluation If mono- and cross-lingual semantic specialisation improves the semantic content of word vector collections according to intrinsic evaluation, we would expect the NBT model to perform higherquality belief tracking when such improved vectors are deployed. We investigate the difference in DST performance for English, German and Italian when the NBT model employs the following word vector collections: 1) distributional word vectors; 2) monolingual semantically specialised vectors; and 3) monolingual subspaces of the cross-lingual semantically specialised EN-DE-IT-RU vectors. For each language, we also compare to the NBT performance achieved using the five state-of-the-art bilingual vector spaces we compared to in Sect. 5.3.\n2. Training a Multilingual DST Model The values expressed by the domain ontology (e.g., cheap, north, Thai, etc.) are language independent. If we assume common semantic grounding across languages, we can decouple the ontologies from the dialogue corpora and use a single ontology (i.e. its values\u2019 vector representations) across all languages. Since we know that high-performing English DST is attainable, we will ground the Italian and German ontologies (i.e. all slot-value pairs) to the original English ontology. The use of a single ontology coupled with cross-lingual vectors then allows us to combine the training data for multiple languages and train a single NBT model capable of performing belief tracking across all three languages at once. Given a high-quality cross-lingual vector space, combining the languages effectively increases the training set size and should therefore lead to improved performance across all languages."}, {"heading": "6.3 Results and Discussion", "text": "The DST performance of the NBT-CNN model on English, German and Italian WOZ 2.0 datasets is shown in Table 8. The first five rows show the performance when the model employs the five baseline\nvector spaces. The subsequent three rows show the performance of: a) distributional vector spaces; b) their monolingual specialisation; and c) their ENDE-IT-RU cross-lingual specialisation. The last row shows the performance of the multilingual DST model trained using ontology grounding, where the training data of all three languages was combined and used to train an improved model. Figure 2 investigates the usefulness of ontology grounding for bootstrapping DST models for new languages with less data: the two figures display the Italian / German performance of models trained using different proportions of the in-language training dataset. The topperforming dash-dotted curve shows the performance of the model trained using the language-specific dialogues and all of the English training data.\nThe results in Table 8 show that both types of specialisation improve over DST performance achieved using the distributional vectors or the five baseline bilingual spaces. Interestingly, the bilingual vectors of Vulic\u0301 and Korhonen (2016a) outperform ours for EN (but not for IT and DE) despite their weaker SimLex performance, showing that intrinsic evaluation does not capture all relevant aspects pertaining to word vectors\u2019 usability for downstream tasks.\nThe multilingual DST model trained using ontology grounding offers substantial performance im-\nprovements, with particularly large gains in the lowdata scenario investigated in Figure 2 (dash-dotted purple line). This figure also shows that the difference in performance between our mono- and crosslingual vectors is not very substantial. Again, the large disparity in SimLex scores induced only minor improvements in DST performance.\nIn summary, our results show that: a) semantically specialised vectors benefit DST performance; b) large gains in SimLex scores do not always induce large downstream gains; and c) high-quality crosslingual spaces facilitate transfer learning between languages and offer an effective method for bootstrapping DST models for lower-resource languages.\nFinally, German DST performance is substantially weaker than both English and Italian, corroborating our intuition that linguistic phenomena such as cases and compounding make German DST very challenging. We release these datasets in hope that multilingual DST evaluation can give the NLP community a tool for evaluating downstream performance of vector spaces for morphologically richer languages."}, {"heading": "7 Conclusion", "text": "We have presented a novel ATTRACT-REPEL method for injecting linguistic constraints into word vector space representations. The procedure semantically specialises word vectors by jointly injecting monoand cross-lingual synonymy and antonymy constraints, creating unified cross-lingual vector spaces which achieve state-of-the-art performance on the well-established SimLex-999 dataset and its multilingual variants. Next, we have shown that ATTRACTREPEL can induce high-quality vectors for lowerresource languages by tying them into bilingual vector spaces with high-resource ones. We also demonstrated that the substantial gains in intrinsic evaluation translate to gains in the downstream task of dialogue state tracking (DST), for which we release two novel non-English datasets (in German and Italian). Finally, we have shown that our semantically rich cross-lingual vectors facilitate language transfer in DST, providing an effective method for bootstrapping belief tracking models for new languages."}, {"heading": "7.1 Further Work", "text": "Our results, especially with DST, emphasise the need for improving vector space models for morphologically rich languages. Moreover, our intrinsic and task-based experiments exposed the discrepancies between the conclusions that can be drawn from these two types of evaluation. We consider these to be major directions for future work."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Anders Johannsen for his help with extracting BabelNet constraints. We would also like to thank our action editor Sebastian Pad\u00f3 and the anonymous TACL reviewers for their constructive feedback. Ivan Vulic\u0301, Roi Reichart and Anna Korhonen are supported by the ERC Consolida-\ntor Grant LEXICAL (number 648909). Roi Reichart is also supported by the Intel-ICRI grant: Hybrid Models for Minimally Supervised Information Extraction from Conversations."}], "references": [{"title": "A hybrid distributional and knowledge-based model of lexical semantics", "author": ["Nikolaos Aletras", "Mark Stevenson."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, *SEM, pages 20\u201329.", "citeRegEx": "Aletras and Stevenson.,? 2015", "shortCiteRegEx": "Aletras and Stevenson.", "year": 2015}, {"title": "Many languages, one parser", "author": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith."], "venue": "Transactions of the ACL, 4:431\u2013444.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "The Berkeley FrameNet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "Proceedings of ACL, pages 86\u201390.", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of ACL, pages 809\u2013815.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Knowledgepowered deep learning for word embedding", "author": ["Jiang Bian", "Bin Gao", "Tie-Yan Liu."], "venue": "Proceedings of ECML-PKDD, pages 132\u2013148.", "citeRegEx": "Bian et al\\.,? 2014", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research, 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath A.P. Chandar", "Stanislas Lauly", "Hugo Larochelle", "Mitesh M. Khapra", "Balaraman Ravindran", "Vikas C. Raykar", "Amrita Saha."], "venue": "Proceedings of NIPS, pages 1853\u20131861.", "citeRegEx": "Chandar et al\\.,? 2014", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of EMNLP, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Trans-gram, fast crosslingual word embeddings", "author": ["Jocelyn Coulmance", "Jean-Marc Marty", "Guillaume Wenzek", "Amine Benhalloum."], "venue": "Proceedings of EMNLP, pages 1109\u20131113.", "citeRegEx": "Coulmance et al\\.,? 2015", "shortCiteRegEx": "Coulmance et al\\.", "year": 2015}, {"title": "Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words", "author": ["Dmitry Davidov", "Ari Rappoport."], "venue": "Proceedings of ACL, pages 297\u2013304.", "citeRegEx": "Davidov and Rappoport.,? 2006", "shortCiteRegEx": "Davidov and Rappoport.", "year": 2006}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M. Schwartz", "John Makhoul."], "venue": "Proceedings of ACL, pages 1370\u20131380.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Eigenwords: Spectral word embeddings", "author": ["Paramveer S. Dhillon", "Dean P. Foster", "Lyle H. Ungar."], "venue": "Journal of Machine Learning Research, 16:3035\u20133078.", "citeRegEx": "Dhillon et al\\.,? 2015", "shortCiteRegEx": "Dhillon et al\\.", "year": 2015}, {"title": "Improving zero-shot learning by mitigating the hubness problem", "author": ["Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni."], "venue": "Proceedings of ICLR: Workshop Papers.", "citeRegEx": "Dinu et al\\.,? 2015", "shortCiteRegEx": "Dinu et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John C. Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Learning crosslingual word embeddings without bilingual corpora", "author": ["Long Duong", "Hiroshi Kanayama", "Tengfei Ma", "Steven Bird", "Trevor Cohn."], "venue": "Proceedings of EMNLP, pages 1285\u20131295.", "citeRegEx": "Duong et al\\.,? 2016", "shortCiteRegEx": "Duong et al\\.", "year": 2016}, {"title": "Representing multilingual data as linked data: The case of BabelNet 2.0", "author": ["Maud Ehrmann", "Francesco Cecconi", "Daniele Vannella", "John Philip Mccrae", "Philipp Cimiano", "Roberto Navigli"], "venue": "In Proceedings of LREC,", "citeRegEx": "Ehrmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ehrmann et al\\.", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of EACL, pages 462\u2013471.", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Non-distributional word vector representations", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of ACL, pages 464\u2013469.", "citeRegEx": "Faruqui and Dyer.,? 2015", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proceedings of NAACL, pages 1606\u20131615.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "ACM Transactions on Information Systems, 20(1):116\u2013 131.", "citeRegEx": "Finkelstein et al\\.,? 2002", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Simulating speech systems", "author": ["Norman M. Fraser", "G. Nigel Gilbert."], "venue": "Computer Speech and Language, 5(1):81\u201399.", "citeRegEx": "Fraser and Gilbert.,? 1991", "shortCiteRegEx": "Fraser and Gilbert.", "year": 1991}, {"title": "The Multilingual Paraphrase Database", "author": ["Juri Ganitkevitch", "Chris Callison-Burch."], "venue": "Proceedings of LREC, pages 4276\u20134283.", "citeRegEx": "Ganitkevitch and Callison.Burch.,? 2014", "shortCiteRegEx": "Ganitkevitch and Callison.Burch.", "year": 2014}, {"title": "PPDB: The Paraphrase Database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-burch."], "venue": "Proceedings of NAACL, pages 758\u2013764.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "SimVerb-3500: A large-scale evaluation set of verb similarity", "author": ["Daniela Gerz", "Ivan Vuli\u0107", "Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Proceedings of EMNLP, pages 2173\u20132182.", "citeRegEx": "Gerz et al\\.,? 2016", "shortCiteRegEx": "Gerz et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of AISTATS, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg."], "venue": "CoRR, abs/1510.00726.", "citeRegEx": "Goldberg.,? 2015", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "BilBOWA: Fast bilingual distributed representations without word alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado."], "venue": "Proceedings of ICML, pages 748\u2013756.", "citeRegEx": "Gouws et al\\.,? 2015", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Revisiting embedding features for simple semisupervised learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of EMNLP, pages 110\u2013120.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of ACL, pages 1234\u20131244.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "The Second Dialog State Tracking Challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Jason D. Wiliams."], "venue": "Proceedings of SIGDIAL, pages 263\u2013272.", "citeRegEx": "Henderson et al\\.,? 2014a", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "Proceedings of IEEE SLT, pages 360\u2013365.", "citeRegEx": "Henderson et al\\.,? 2014b", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "Proceedings of SIGDIAL, pages 292\u2013299.", "citeRegEx": "Henderson et al\\.,? 2014c", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of ICLR.", "citeRegEx": "Hermann and Blunsom.,? 2014a", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of ACL, pages 58\u201368.", "citeRegEx": "Hermann and Blunsom.,? 2014b", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics, 41(4):665\u2013695.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Translation invariant word embeddings", "author": ["Kejun Huang", "Matt Gardner", "Evangelos Papalexakis", "Christos Faloutsos", "Nikos Sidiropoulos", "Tom Mitchell", "Partha P. Talukdar", "Xiao Fu."], "venue": "Proceedings of EMNLP, pages 1084\u20131088.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "A Neural Network for Factoid Question Answering over Paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III."], "venue": "Proceedings of EMLNP, pages 633\u2013644.", "citeRegEx": "Iyyer et al\\.,? 2014", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "author": ["Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy."], "venue": "Proceedings of NAACL, pages 683\u2013693.", "citeRegEx": "Jauhar et al\\.,? 2015", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Any-language frame-semantic parsing", "author": ["Anders Johannsen", "H\u00e9ctor Mart\u00ednez Alonso", "Anders S\u00f8gaard."], "venue": "Proceedings of EMNLP, pages 2062\u20132066.", "citeRegEx": "Johannsen et al\\.,? 2015", "shortCiteRegEx": "Johannsen et al\\.", "year": 2015}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["Douwe Kiela", "Felix Hill", "Stephen Clark."], "venue": "Proceedings of EMNLP, pages 2044\u20132048.", "citeRegEx": "Kiela et al\\.,? 2015", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Adjusting word embeddings with semantic intensity orders", "author": ["Joo-Kyung Kim", "Marie-Catherine de Marneffe", "Eric Fosler-Lussier."], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP, pages 62\u201369.", "citeRegEx": "Kim et al\\.,? 2016a", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Intent detection using semantically enriched word embeddings", "author": ["Joo-Kyung Kim", "Gokhan Tur", "Asli Celikyilmaz", "Bin Cao", "Ye-Yi Wang."], "venue": "Proceedings of SLT.", "citeRegEx": "Kim et al\\.,? 2016b", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "Proceedings COLING, pages 1459\u20131474.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit, volume", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Texts in, meaning out: neural language models in semantic similarity task for Russian", "author": ["Andrey Kutuzov", "Igor Andreev."], "venue": "Proceedings of DIALOG.", "citeRegEx": "Kutuzov and Andreev.,? 2015", "shortCiteRegEx": "Kutuzov and Andreev.", "year": 2015}, {"title": "Hubness and pollution: Delving into cross-space mapping for zero-shot learning", "author": ["Angeliki Lazaridou", "Georgiana Dinu", "Marco Baroni."], "venue": "Proceedings of ACL, pages 270\u2013280.", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Separated by an Un-common Language: Towards Judgment Language Informed Vector Space Modeling", "author": ["Ira Leviant", "Roi Reichart."], "venue": "arXiv preprint: 1508.00106.", "citeRegEx": "Leviant and Reichart.,? 2015", "shortCiteRegEx": "Leviant and Reichart.", "year": 2015}, {"title": "Dependency-based word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of ACL, pages 302\u2013 308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Quan Liu", "Hui Jiang", "Si Wei", "Zhen-Hua Ling", "Yu Hu."], "venue": "Proceedings of ACL, pages 1501\u20131511.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 1st Workshop on Vector Space Modeling for NLP, pages 151\u2013159.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "arXiv preprint, CoRR, abs/1309.4168.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proceedings of NIPS, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["George A. Miller."], "venue": "Communications of the ACM, pages 39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "A dual embedding space model for document ranking", "author": ["Bhaskar Mitra", "Eric T. Nalisnick", "Nick Craswell", "Rich Caruana."], "venue": "CoRR, abs/1602.01137.", "citeRegEx": "Mitra et al\\.,? 2016", "shortCiteRegEx": "Mitra et al\\.", "year": 2016}, {"title": "Multi-domain dialog state tracking using recurrent neural networks", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "TsungHsien Wen", "Steve Young."], "venue": "Proceedings of ACL, pages 794\u2013799.", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? 2015", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Counter-fitting word vectors to linguistic constraints", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young."], "venue": "Proceedings of NAACL, pages 142\u2013148.", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? 2016", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2016}, {"title": "Neural Belief Tracker: Data-driven dialogue state tracking", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Tsung-Hsien Wen", "Steve Young."], "venue": "Proceedings of ACL.", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? 2017", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2017}, {"title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "author": ["Roberto Navigli", "Simone Paolo Ponzetto."], "venue": "Artificial Intelligence, 193:217\u2013250.", "citeRegEx": "Navigli and Ponzetto.,? 2012", "shortCiteRegEx": "Navigli and Ponzetto.", "year": 2012}, {"title": "Integrating distributional lexical contrast into word embeddings for antonymsynonym distinction", "author": ["Kim Anh Nguyen", "Sabine Schulte im Walde", "Ngoc Thang Vu."], "venue": "Proceedings of ACL, pages 454\u2013459.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Word Embedding-based Antonym Detection using Thesauri and Distributional Information", "author": ["Masataka Ono", "Makoto Miwa", "Yutaka Sasaki."], "venue": "Proceedings of NAACL, pages 984\u2013989.", "citeRegEx": "Ono et al\\.,? 2015", "shortCiteRegEx": "Ono et al\\.", "year": 2015}, {"title": "Encoding prior knowledge with eigenword embeddings", "author": ["Dominique Osborne", "Shashi Narayan", "Shay Cohen."], "venue": "Transactions of the ACL, 4:417\u2013430.", "citeRegEx": "Osborne et al\\.,? 2016", "shortCiteRegEx": "Osborne et al\\.", "year": 2016}, {"title": "Probabilistic distributional semantics", "author": ["Diarmuid \u00d3 S\u00e9aghdha", "Anna Korhonen."], "venue": "Computational Linguistics, 40(3):587\u2013631.", "citeRegEx": "S\u00e9aghdha and Korhonen.,? 2014", "shortCiteRegEx": "S\u00e9aghdha and Korhonen.", "year": 2014}, {"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevich", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of EMNLP, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multiview LSA: Representation learning via generalized CCA", "author": ["Pushpendre Rastogi", "Benjamin Van Durme", "Raman Arora."], "venue": "Proceedings of NAACL, pages 556\u2013566.", "citeRegEx": "Rastogi et al\\.,? 2015", "shortCiteRegEx": "Rastogi et al\\.", "year": 2015}, {"title": "Measuring Semantic Similarity of Words Using Concept Networks", "author": ["G\u00e1bor Recski", "Eszter Ikl\u00f3di", "Katalin Pajkossy", "Andras Kornai."], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP, pages 193\u2013200.", "citeRegEx": "Recski et al\\.,? 2016", "shortCiteRegEx": "Recski et al\\.", "year": 2016}, {"title": "Reasoning about Entailment with Neural Attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom."], "venue": "Proceedings of ICLR.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2016", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "AutoExtend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze."], "venue": "Proceedings of ACL, pages 1793\u2013 1803.", "citeRegEx": "Rothe and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2015}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction", "author": ["Roy Schwartz", "Roi Reichart", "Ari Rappoport."], "venue": "Proceedings of CoNLL, pages 258\u2013267.", "citeRegEx": "Schwartz et al\\.,? 2015", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of ACL, pages 455\u2013 465.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of EMNLP, pages 1631\u20131642.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Inverted indexing for cross-lingual NLP", "author": ["Anders S\u00f8gaard", "\u017deljko Agi\u0107", "H\u00e9ctor Mart\u00ednez Alonso", "Barbara Plank", "Bernd Bohnet", "Anders Johannsen."], "venue": "Proceedings ACL, pages 1713\u20131722.", "citeRegEx": "S\u00f8gaard et al\\.,? 2015", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2015}, {"title": "Leveraging monolingual data for crosslingual compositional word representations", "author": ["Hubert Soyer", "Pontus Stenetorp", "Akiko Aizawa."], "venue": "Proceedings of ICLR.", "citeRegEx": "Soyer et al\\.,? 2015", "shortCiteRegEx": "Soyer et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "Proceedings of ACL, pages 384\u2013394.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Cross-lingual models of word embeddings: An empirical comparison", "author": ["Shyam Upadhyay", "Manaal Faruqui", "Chris Dyer", "Dan Roth."], "venue": "Proceedings of ACL, pages 1661\u20131670.", "citeRegEx": "Upadhyay et al\\.,? 2016", "shortCiteRegEx": "Upadhyay et al\\.", "year": 2016}, {"title": "Is \"universal syntax\" universally useful for learning distributed representations", "author": ["Ivan Vuli\u0107", "Anna Korhonen"], "venue": "In Proceedings of ACL,", "citeRegEx": "Vuli\u0107 and Korhonen.,? \\Q2016\\E", "shortCiteRegEx": "Vuli\u0107 and Korhonen.", "year": 2016}, {"title": "On the role of seed lexicons in learning bilingual word embeddings", "author": ["Ivan Vuli\u0107", "Anna Korhonen."], "venue": "Proceedings of ACL, pages 247\u2013257.", "citeRegEx": "Vuli\u0107 and Korhonen.,? 2016b", "shortCiteRegEx": "Vuli\u0107 and Korhonen.", "year": 2016}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["Ivan Vuli\u0107", "Marie-Francine Moens."], "venue": "Proceedings of SIGIR, pages 363\u2013372.", "citeRegEx": "Vuli\u0107 and Moens.,? 2015", "shortCiteRegEx": "Vuli\u0107 and Moens.", "year": 2015}, {"title": "Bilingual distributed word representations from documentaligned comparable data", "author": ["Ivan Vuli\u0107", "Marie-Francine Moens."], "venue": "Journal of Artificial Intelligence Research, 55:953\u2013994.", "citeRegEx": "Vuli\u0107 and Moens.,? 2016", "shortCiteRegEx": "Vuli\u0107 and Moens.", "year": 2016}, {"title": "Hyperlex: A largescale evaluation of graded lexical entailment", "author": ["Ivan Vuli\u0107", "Daniela Gerz", "Douwe Kiela", "Felix Hill", "Anna Korhonen."], "venue": "CoRR, abs/1608.02117.", "citeRegEx": "Vuli\u0107 et al\\.,? 2016", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "David Vandyke", "Nikola Mrk\u0161i\u0107", "Milica Ga\u0161i\u0107", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young."], "venue": "Proceedings of EACL, pages 437\u2013449.", "citeRegEx": "Wen et al\\.,? 2017", "shortCiteRegEx": "Wen et al\\.", "year": 2017}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Transactions of the ACL, 3:345\u2013358.", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Charagram: Embedding words and sentences via character n-grams", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of EMNLP, pages 1504\u20131515.", "citeRegEx": "Wieting et al\\.,? 2016", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "RC-NET: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu."], "venue": "Proceedings of CIKM, pages 1219\u20131228.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Polarity inducing Latent Semantic Analysis", "author": ["Wen-Tau Yih", "Geoffrey Zweig", "John C. Platt."], "venue": "Proceedings of ACL, pages 1212\u20131222.", "citeRegEx": "Yih et al\\.,? 2012", "shortCiteRegEx": "Yih et al\\.", "year": 2012}, {"title": "POMDP-Based Statistical Spoken Dialog Systems: A Review", "author": ["Steve J. Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D. Williams."], "venue": "Proceedings of the IEEE, 101(5):1160\u20131179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Still talking to machines (cognitively speaking)", "author": ["Steve Young."], "venue": "Proceedings of INTERSPEECH, pages 1\u201310.", "citeRegEx": "Young.,? 2010", "shortCiteRegEx": "Young.", "year": 2010}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "Proceedings of ACL, pages 545\u2013550.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "Proceedings of EMNLP, pages 1393\u20131398.", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 53, "context": "The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; \u00d3 S\u00e9aghdha and Korhonen, 2014; Levy and Goldberg, 2014).", "startOffset": 218, "endOffset": 322}, {"referenceID": 65, "context": "The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; \u00d3 S\u00e9aghdha and Korhonen, 2014; Levy and Goldberg, 2014).", "startOffset": 218, "endOffset": 322}, {"referenceID": 49, "context": "The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; \u00d3 S\u00e9aghdha and Korhonen, 2014; Levy and Goldberg, 2014).", "startOffset": 218, "endOffset": 322}, {"referenceID": 20, "context": "processing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrk\u0161i\u0107 et al., 2016).", "startOffset": 140, "endOffset": 205}, {"referenceID": 83, "context": "processing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrk\u0161i\u0107 et al., 2016).", "startOffset": 140, "endOffset": 205}, {"referenceID": 57, "context": "processing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrk\u0161i\u0107 et al., 2016).", "startOffset": 140, "endOffset": 205}, {"referenceID": 36, "context": "Our evaluation shows that ATTRACT-REPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al.", "startOffset": 193, "endOffset": 212}, {"referenceID": 25, "context": ", 2015) and SimVerb-3500 (Gerz et al., 2016).", "startOffset": 25, "endOffset": 44}, {"referenceID": 59, "context": "We then deploy the ATTRACT-REPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations.", "startOffset": 119, "endOffset": 169}, {"referenceID": 17, "context": "We then deploy the ATTRACT-REPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations.", "startOffset": 119, "endOffset": 169}, {"referenceID": 48, "context": "2 We demonstrate its efficacy with state-of-theart results on the four languages in the Multilingual SimLex-999 dataset (Leviant and Reichart, 2015).", "startOffset": 120, "endOffset": 148}, {"referenceID": 38, "context": "question answering (Iyyer et al., 2014) or textual entailment (Rockt\u00e4schel et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 68, "context": ", 2014) or textual entailment (Rockt\u00e4schel et al., 2016).", "startOffset": 30, "endOffset": 56}, {"referenceID": 87, "context": "This task, which arises in the construction of statistical dialogue systems (Young et al., 2013), involves understanding the goals expressed by the user and updating the system\u2019s distribution over such goals as the conversation", "startOffset": 76, "endOffset": 96}, {"referenceID": 9, "context": "The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al.", "startOffset": 136, "endOffset": 160}, {"referenceID": 71, "context": "2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al.", "startOffset": 39, "endOffset": 150}, {"referenceID": 3, "context": "2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al.", "startOffset": 39, "endOffset": 150}, {"referenceID": 8, "context": "2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al.", "startOffset": 39, "endOffset": 150}, {"referenceID": 40, "context": "2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al.", "startOffset": 39, "endOffset": 150}, {"referenceID": 1, "context": "2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al.", "startOffset": 39, "endOffset": 150}, {"referenceID": 72, "context": ", 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 75, "context": ", 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others.", "startOffset": 35, "endOffset": 74}, {"referenceID": 29, "context": ", 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others.", "startOffset": 35, "endOffset": 74}, {"referenceID": 57, "context": "cialisation for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrk\u0161i\u0107 et al., 2016; Mrk\u0161i\u0107 et al., 2017), spoken language understanding (Kim et al.", "startOffset": 136, "endOffset": 178}, {"referenceID": 58, "context": "cialisation for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrk\u0161i\u0107 et al., 2016; Mrk\u0161i\u0107 et al., 2017), spoken language understanding (Kim et al.", "startOffset": 136, "endOffset": 178}, {"referenceID": 43, "context": ", 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and", "startOffset": 39, "endOffset": 77}, {"referenceID": 42, "context": ", 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and", "startOffset": 39, "endOffset": 77}, {"referenceID": 81, "context": "judging lexical entailment (Vuli\u0107 et al., 2016).", "startOffset": 27, "endOffset": 47}, {"referenceID": 54, "context": "Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al.", "startOffset": 100, "endOffset": 114}, {"referenceID": 2, "context": "Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Databases", "startOffset": 125, "endOffset": 145}, {"referenceID": 24, "context": "(PPDB) (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014; Pavlick et al., 2015).", "startOffset": 7, "endOffset": 95}, {"referenceID": 23, "context": "(PPDB) (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014; Pavlick et al., 2015).", "startOffset": 7, "endOffset": 95}, {"referenceID": 64, "context": "(PPDB) (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014; Pavlick et al., 2015).", "startOffset": 7, "endOffset": 95}, {"referenceID": 89, "context": "Learning from Scratch: some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015).", "startOffset": 150, "endOffset": 256}, {"referenceID": 85, "context": "Learning from Scratch: some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015).", "startOffset": 150, "endOffset": 256}, {"referenceID": 5, "context": "Learning from Scratch: some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015).", "startOffset": 150, "endOffset": 256}, {"referenceID": 41, "context": "Learning from Scratch: some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015).", "startOffset": 150, "endOffset": 256}, {"referenceID": 0, "context": "Learning from Scratch: some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015).", "startOffset": 150, "endOffset": 256}, {"referenceID": 53, "context": "Other ones modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 86, "context": ", 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasise word similarity over relatedness.", "startOffset": 64, "endOffset": 100}, {"referenceID": 50, "context": ", 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasise word similarity over relatedness.", "startOffset": 64, "endOffset": 100}, {"referenceID": 11, "context": "(2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in their pattern-based vector space.", "startOffset": 30, "endOffset": 59}, {"referenceID": 46, "context": ", 2012; Liu et al., 2015) to train word vectors which emphasise word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al.", "startOffset": 8, "endOffset": 120}, {"referenceID": 12, "context": "(2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings.", "startOffset": 119, "endOffset": 141}, {"referenceID": 12, "context": "(2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in their pattern-based vector space.", "startOffset": 119, "endOffset": 324}, {"referenceID": 11, "context": "(2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in their pattern-based vector space. Ono et al. (2015) combine both approaches, using thesauri and distributional data to train embeddings specialised for capturing antonymy.", "startOffset": 31, "endOffset": 145}, {"referenceID": 11, "context": "(2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in their pattern-based vector space. Ono et al. (2015) combine both approaches, using thesauri and distributional data to train embeddings specialised for capturing antonymy. Faruqui and Dyer (2015) use many", "startOffset": 31, "endOffset": 289}, {"referenceID": 20, "context": "Faruqui et al. (2015) and Jauhar et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "Faruqui et al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semanti-", "startOffset": 0, "endOffset": 47}, {"referenceID": 80, "context": "cally similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB.", "startOffset": 42, "endOffset": 64}, {"referenceID": 56, "context": "Mrk\u0161i\u0107 et al. (2016) build on the retrofitting approach by jointly injecting synonymy", "startOffset": 0, "endOffset": 21}, {"referenceID": 58, "context": "and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 42, "context": "Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 42, "context": "Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles of rich concept dictionaries to further improve a combined collection", "startOffset": 0, "endOffset": 148}, {"referenceID": 51, "context": "on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentencealigned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al.", "startOffset": 43, "endOffset": 87}, {"referenceID": 10, "context": "on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentencealigned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al.", "startOffset": 43, "endOffset": 87}, {"referenceID": 34, "context": ", 2015) or sentencealigned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 32, "endOffset": 130}, {"referenceID": 35, "context": ", 2015) or sentencealigned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 32, "endOffset": 130}, {"referenceID": 7, "context": ", 2015) or sentencealigned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 32, "endOffset": 130}, {"referenceID": 28, "context": ", 2015) or sentencealigned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 32, "endOffset": 130}, {"referenceID": 73, "context": "aligned data (S\u00f8gaard et al., 2015; Vuli\u0107 and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al.", "startOffset": 13, "endOffset": 58}, {"referenceID": 80, "context": "aligned data (S\u00f8gaard et al., 2015; Vuli\u0107 and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al.", "startOffset": 13, "endOffset": 58}, {"referenceID": 52, "context": ", 2015; Vuli\u0107 and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli\u0107 and Korhonen, 2016b; Duong et al., 2016).", "startOffset": 98, "endOffset": 216}, {"referenceID": 18, "context": ", 2015; Vuli\u0107 and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli\u0107 and Korhonen, 2016b; Duong et al., 2016).", "startOffset": 98, "endOffset": 216}, {"referenceID": 47, "context": ", 2015; Vuli\u0107 and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli\u0107 and Korhonen, 2016b; Duong et al., 2016).", "startOffset": 98, "endOffset": 216}, {"referenceID": 78, "context": ", 2015; Vuli\u0107 and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli\u0107 and Korhonen, 2016b; Duong et al., 2016).", "startOffset": 98, "endOffset": 216}, {"referenceID": 16, "context": ", 2015; Vuli\u0107 and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli\u0107 and Korhonen, 2016b; Duong et al., 2016).", "startOffset": 98, "endOffset": 216}, {"referenceID": 77, "context": "(2016) and Vuli\u0107 and Korhonen (2016b) for an overview of cross-lingual word embedding work.", "startOffset": 11, "endOffset": 38}, {"referenceID": 18, "context": "boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 66, "context": "boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 76, "context": "boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 79, "context": ", 2016), cross-lingual information retrieval (Vuli\u0107 and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (S\u00f8gaard et al.", "startOffset": 45, "endOffset": 88}, {"referenceID": 55, "context": ", 2016), cross-lingual information retrieval (Vuli\u0107 and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (S\u00f8gaard et al.", "startOffset": 45, "endOffset": 88}, {"referenceID": 73, "context": ", 2016), and transfer learning for resource-lean languages (S\u00f8gaard et al., 2015; Guo et al., 2015).", "startOffset": 59, "endOffset": 99}, {"referenceID": 30, "context": ", 2016), and transfer learning for resource-lean languages (S\u00f8gaard et al., 2015; Guo et al., 2015).", "startOffset": 59, "endOffset": 99}, {"referenceID": 83, "context": "This procedure, which we term ATTRACT-REPEL, builds on the Paragram (Wieting et al., 2015) and counter-fitting procedures", "startOffset": 68, "endOffset": 90}, {"referenceID": 57, "context": "(Mrk\u0161i\u0107 et al., 2016), both of which inject linguistic constraints into existing vector spaces to improve their ability to capture semantic similarity.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": "Comparison to Prior Work ATTRACT-REPEL draws inspiration from three methods: 1) retrofitting (Faruqui et al., 2015); 2) PARAGRAM (Wieting et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 83, "context": ", 2015); 2) PARAGRAM (Wieting et al., 2015); and 3) counter-fitting (Mrk\u0161i\u0107 et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 57, "context": ", 2015); and 3) counter-fitting (Mrk\u0161i\u0107 et al., 2016).", "startOffset": 32, "endOffset": 53}, {"referenceID": 15, "context": "(2015), we use the AdaGrad algorithm (Duchi et al., 2011) to train the word embeddings for five epochs, which", "startOffset": 37, "endOffset": 57}, {"referenceID": 82, "context": "Optimisation Following Wieting et al. (2015), we use the AdaGrad algorithm (Duchi et al.", "startOffset": 23, "endOffset": 45}, {"referenceID": 20, "context": "Similar to Faruqui et al. (2015), Wieting et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 20, "context": "Similar to Faruqui et al. (2015), Wieting et al. (2015) and Mrk\u0161i\u0107 et al.", "startOffset": 11, "endOffset": 56}, {"referenceID": 20, "context": "Similar to Faruqui et al. (2015), Wieting et al. (2015) and Mrk\u0161i\u0107 et al. (2016), we do not use early stopping.", "startOffset": 11, "endOffset": 81}, {"referenceID": 21, "context": "Hyperparameter Tuning We use Spearman\u2019s correlation of the final word vectors with the Multilingual WordSim-353 gold-standard association dataset (Finkelstein et al., 2002; Leviant and Reichart, 2015).", "startOffset": 146, "endOffset": 200}, {"referenceID": 48, "context": "Hyperparameter Tuning We use Spearman\u2019s correlation of the final word vectors with the Multilingual WordSim-353 gold-standard association dataset (Finkelstein et al., 2002; Leviant and Reichart, 2015).", "startOffset": 146, "endOffset": 200}, {"referenceID": 64, "context": "For the four SimLex languages, we employ four well-known, high-quality word vector collections: a) The Common Crawl GloVe English vectors from Pennington et al. (2014); b) German vectors from Vuli\u0107 and Korhonen (2016a); c) Italian vectors from Dinu et al.", "startOffset": 143, "endOffset": 168}, {"referenceID": 64, "context": "For the four SimLex languages, we employ four well-known, high-quality word vector collections: a) The Common Crawl GloVe English vectors from Pennington et al. (2014); b) German vectors from Vuli\u0107 and Korhonen (2016a); c) Italian vectors from Dinu et al.", "startOffset": 143, "endOffset": 219}, {"referenceID": 14, "context": "(2014); b) German vectors from Vuli\u0107 and Korhonen (2016a); c) Italian vectors from Dinu et al. (2015); and d) Russian vectors from Kutuzov", "startOffset": 83, "endOffset": 102}, {"referenceID": 53, "context": "In addition, for each of the 16 languages we also train the skip-gram with negative sampling variant of the word2vec model (Mikolov et al., 2013b), on the latest Wikipedia dump of each language, to induce 300-dimensional word vectors.", "startOffset": 123, "endOffset": 146}, {"referenceID": 59, "context": "Cross-Lingual Similarity We employ BabelNet, a multilingual semantic network automatically constructed by linking Wikipedia to WordNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014).", "startOffset": 135, "endOffset": 185}, {"referenceID": 17, "context": "Cross-Lingual Similarity We employ BabelNet, a multilingual semantic network automatically constructed by linking Wikipedia to WordNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014).", "startOffset": 135, "endOffset": 185}, {"referenceID": 20, "context": "Following Faruqui et al. (2015), who found PPDB constraints more beneficial than the WordNet ones,", "startOffset": 10, "endOffset": 32}, {"referenceID": 45, "context": "However, PPDB relies on large, high-quality parallel corpora such as Europarl (Koehn, 2005).", "startOffset": 78, "endOffset": 91}, {"referenceID": 36, "context": "Spearman\u2019s rank correlation with the SimLex-999 dataset (Hill et al., 2015) is used as the intrinsic eval-", "startOffset": 56, "endOffset": 75}, {"referenceID": 21, "context": "Unlike other gold standard resources such as WordSim-353 (Finkelstein et al., 2002) or MEN (Bruni et al.", "startOffset": 57, "endOffset": 83}, {"referenceID": 6, "context": ", 2002) or MEN (Bruni et al., 2014), SimLex-999 consists of word pairs scored by annotators instructed to discern between semantic similarity", "startOffset": 15, "endOffset": 35}, {"referenceID": 25, "context": "SimVerb-3500 (Gerz et al., 2016), a semantic similarity dataset that focuses on verb pair similarity.", "startOffset": 13, "endOffset": 32}, {"referenceID": 26, "context": "tional vectors trained on the latest Wikipedia dumps; and c) word vectors randomly initialised using the XAVIER initialisation (Glorot and Bengio, 2010).", "startOffset": 127, "endOffset": 152}, {"referenceID": 52, "context": "(Mikolov et al., 2013a), retrained using the constraints used by our model; and 2.", "startOffset": 0, "endOffset": 23}, {"referenceID": 34, "context": "(Hermann and Blunsom, 2014a; Gouws et al., 2015; Vuli\u0107 and Korhonen, 2016a; Vuli\u0107 and Moens, 2016).", "startOffset": 0, "endOffset": 98}, {"referenceID": 28, "context": "(Hermann and Blunsom, 2014a; Gouws et al., 2015; Vuli\u0107 and Korhonen, 2016a; Vuli\u0107 and Moens, 2016).", "startOffset": 0, "endOffset": 98}, {"referenceID": 80, "context": "(Hermann and Blunsom, 2014a; Gouws et al., 2015; Vuli\u0107 and Korhonen, 2016a; Vuli\u0107 and Moens, 2016).", "startOffset": 0, "endOffset": 98}, {"referenceID": 83, "context": "State-of-the-Art Wieting et al. (2016) note that the hyperparameters of the widely used Paragram-SL999", "startOffset": 17, "endOffset": 39}, {"referenceID": 83, "context": "vectors (Wieting et al., 2015) are tuned on SimLex999, and as such are not comparable to methods which holdout the dataset.", "startOffset": 8, "endOffset": 30}, {"referenceID": 57, "context": ", (Mrk\u0161i\u0107 et al., 2016; Recski et al., 2016)) as starting point does not yield meaningful high scores either.", "startOffset": 2, "endOffset": 44}, {"referenceID": 67, "context": ", (Mrk\u0161i\u0107 et al., 2016; Recski et al., 2016)) as starting point does not yield meaningful high scores either.", "startOffset": 2, "endOffset": 44}, {"referenceID": 82, "context": "706 score reported by Wieting et al. (2016) and sets a new high score for this dataset.", "startOffset": 22, "endOffset": 44}, {"referenceID": 25, "context": "628 reported by Gerz et al. (2016).", "startOffset": 16, "endOffset": 35}, {"referenceID": 27, "context": "This is a typical init method in neural nets research (Goldberg, 2015; Bengio et al., 2013).", "startOffset": 54, "endOffset": 91}, {"referenceID": 4, "context": "This is a typical init method in neural nets research (Goldberg, 2015; Bengio et al., 2013).", "startOffset": 54, "endOffset": 91}, {"referenceID": 18, "context": "English words which map to the same Irish word are likely to be synonyms, even if those English pairs are not present in the PPDB datasets (Faruqui and Dyer, 2014).", "startOffset": 139, "endOffset": 163}, {"referenceID": 52, "context": "Model EN-IT EN-DE EN IT EN DE (Mikolov et al., 2013a) 0.", "startOffset": 30, "endOffset": 53}, {"referenceID": 34, "context": "28 (Hermann and Blunsom, 2014a) 0.", "startOffset": 3, "endOffset": 31}, {"referenceID": 28, "context": "35 (Gouws et al., 2015) 0.", "startOffset": 3, "endOffset": 23}, {"referenceID": 80, "context": "33 (Vuli\u0107 and Moens, 2016) 0.", "startOffset": 3, "endOffset": 26}, {"referenceID": 88, "context": "In slot-based systems, application domains are defined by ontologies which enumerate the goals that users can express (Young, 2010).", "startOffset": 118, "endOffset": 131}, {"referenceID": 33, "context": "To overcome this problem, delexicalisation-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrk\u0161i\u0107 et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values.", "startOffset": 60, "endOffset": 149}, {"referenceID": 32, "context": "To overcome this problem, delexicalisation-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrk\u0161i\u0107 et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values.", "startOffset": 60, "endOffset": 149}, {"referenceID": 56, "context": "To overcome this problem, delexicalisation-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrk\u0161i\u0107 et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values.", "startOffset": 60, "endOffset": 149}, {"referenceID": 82, "context": "To overcome this problem, delexicalisation-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrk\u0161i\u0107 et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values.", "startOffset": 60, "endOffset": 149}, {"referenceID": 56, "context": "Mrk\u0161i\u0107 et al. (2016) showed that semantically specialised vector spaces can be used to automatically induce such lexicons for simple dialogue domains.", "startOffset": 0, "endOffset": 21}, {"referenceID": 58, "context": "Neural Belief Tracker (NBT) The NBT is a novel DST model which operates purely over distributed representations of words, learning to compose utterance and context representations which it then uses to decide which of the potentially many ontologydefined intents (goals) have been expressed by the user (Mrk\u0161i\u0107 et al., 2017).", "startOffset": 303, "endOffset": 324}, {"referenceID": 31, "context": "This dataset is based on the ontology used for the 2nd DST Challenge (DSTC2) (Henderson et al., 2014a).", "startOffset": 77, "endOffset": 102}, {"referenceID": 75, "context": "0 dataset introduced by Wen et al. (2017) and Mrk\u0161i\u0107 et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 52, "context": "(2017) and Mrk\u0161i\u0107 et al. (2017). This dataset is based on the ontology used for the 2nd DST Challenge (DSTC2) (Henderson et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 77, "context": "Interestingly, the bilingual vectors of Vuli\u0107 and Korhonen (2016a) outperform ours for EN (but not for IT and DE) despite their weaker SimLex performance, showing that intrinsic evaluation", "startOffset": 40, "endOffset": 67}, {"referenceID": 52, "context": "The multilingual DST model trained using ontology grounding offers substantial performance imWord Vector Space EN IT DE EN-IT/EN-DE (Mikolov et al., 2013a) 78.", "startOffset": 132, "endOffset": 155}, {"referenceID": 28, "context": "7 EN-IT/EN-DE (Gouws et al., 2015) 75.", "startOffset": 14, "endOffset": 34}, {"referenceID": 81, "context": "5 EN-IT/EN-DE (Vuli\u0107 et al., 2016) 72.", "startOffset": 14, "endOffset": 34}], "year": 2017, "abstractText": "We present ATTRACT-REPEL, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. ATTRACT-REPEL facilitates the use of constraints from monoand crosslingual resources, yielding semantically specialised cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct highquality vector spaces for a plethora of different languages, facilitating semantic transfer from highto lower-resource ones. The effectiveness of our approach is demonstrated with state-ofthe-art results on semantic similarity datasets in six languages. We next show that ATTRACTREPEL-specialised vectors boost performance in the downstream task of dialogue state tracking (DST) across multiple languages. Finally, we show that cross-lingual vector spaces produced by our algorithm facilitate the training of multilingual DST models, which brings further performance improvements.", "creator": "LaTeX with hyperref package"}}}