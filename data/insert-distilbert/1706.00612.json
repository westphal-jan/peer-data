{"id": "1706.00612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Attentive Convolutional Neural Network based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech", "abstract": "speech with emotion recognition is an important and therefore challenging task in the realm today of human - computer interaction. prior work proposed a variety of entity models specifications and feature sets valid for training a system. in this work, we conduct extensive experiments using an attentive convolutional neural network with multi - view pattern learning objective function. we compare system performance using different lengths combinations of the input signal, different types of acoustic features and different types of emotion speech ( improvised / scripted ). our experimental results on the interactive emotional motion capture ( iemocap ) database reveal that the recognition performance strongly implies depends on the type of speech data independent of the choice of input features. furthermore, we achieved state - of - the - art results on the improvised speech specification data of iemocap.", "histories": [["v1", "Fri, 2 Jun 2017 10:12:52 GMT  (161kb,D)", "http://arxiv.org/abs/1706.00612v1", "to appear in the proceedings of Interspeech 2017"]], "COMMENTS": "to appear in the proceedings of Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael neumann", "ngoc thang vu"], "accepted": false, "id": "1706.00612"}, "pdf": {"name": "1706.00612.pdf", "metadata": {"source": "CRF", "title": "Attentive Convolutional Neural Network based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech", "authors": ["Michael Neumann", "Ngoc Thang Vu"], "emails": ["michael.neumann@ims.uni-stuttgart.de", "thang.vu@ims.uni-stuttgart.de"], "sections": [{"heading": "1. Introduction", "text": "Speech emotion recognition has been attracting increasing attention recently. It is a challenging task due to the complexity of emotional expressions (affected by many factors such as age [1] and gender [2]) and the lack of a large dataset.\nDeep learning (DL) has become a state-of-the-art method for many tasks such as speech recognition, computer vision and natural language processing (NLP). Convolutional neural networks (CNN) proposed in [3, 4] are a special kind of neural networks that have been successfully used not only for computer vision but also for speech [5, 6, 7]. For speech recognition, CNN proved to be robust against noise compared to other DL models [8]. Furthermore, [9] showed that CNNs are suitable for small memory footprint keyword spotting due to the parameter sharing mechanism.\nMore recently, attention based recurrent neural networks have been successfully applied to a wide range of tasks such as handwriting generation [10], machine translation [11], image caption generation [12] and speech recognition [13]. Researchers have also started to use attention mechanisms for CNNs in NLP tasks [14, 15, 16]. This seems to be helpful when the input signal is rather long or complex.\nDL has been shown to significantly boost emotion recognition performance [17, 18, 19, 20, 21, 22]. Recently, several papers [23, 24] presented CNNs in combination with Long Short-Term Memory models (LSTM) to improve speech emotion recognition based on log Mel filter-banks (logMel) or raw signal. [24] demonstrated an end-to-end training from raw signal. This model, however, overfits easily due to the small amount of training data. Well known features, like MFCCs and logMel are fairly simple to extract and have a small number of dimensions which might be more suitable to a low-resource setting than raw signal.\nIn this paper, we propose an attentive convolutional neural network (ACNN) for emotion recognition which combines the strengths of CNNs and attention mechanisms. We focus on the comparison between different feature types. Furthermore, while previous models employed the complete signal to make predictions which costs recognition delays, we are interested in the robustness of the system against the signal length, i.e. finding the answer to the question: how long does the system need to wait to make an accurate prediction? Moreover, we analyze extensively performance differences between improvised and scripted speech. Finally, we report state-of-the-art results on the improvised subset of the IEMOCAP database."}, {"heading": "2. Model", "text": "The model we apply to predict emotional categories from speech is depicted in Figure 1. It consists of two main parts: a CNN with one convolutional layer and one pooling layer and an attention layer. The CNN learns the representation of the audio signal, while the attention layer computes the weighted sum of all the information extracted from different parts of the input. The output from the pooling layer and the attention vector are then fed into a fully connected softmax layer."}, {"heading": "2.1. Convolutional neural network", "text": "The input to the CNN is an audio signal divided into s overlapping segments represented by a d-dimensional feature vector. Thus, for each utterance, we form a matrix W \u2208 Rd\u00d7s as input. For the convolution operation we use 2D kernels K (with width |K|) spanning all d features. The following equation expresses this operation:\n(W \u2217K)(x, y) = d\u2211\ni=1 |K|\u2211 j=1 W (i, j) \u00b7K(x\u2212 i, y \u2212 j) (1)\nAfter the convolution, we use max pooling to find the most salient features. Then, all feature maps are concatenated to one feature vector which is the input to the softmax layer."}, {"heading": "2.2. Attention mechanism", "text": "For each vector xi in a sequence of inputs x, the attention weights \u03b1i can be computed as follows\n\u03b1i = exp(f(xi))\u2211 j exp(f(xj))\n(2)\nwhere f(x) is the scoring function. In this work, f(x) is the linear function f(x) = WTx, where W is a trainable parameter. The output of the attention layer, attentive x, is the weighted sum of the input sequence.\nattentive x = \u2211 i \u03b1ixi (3)\nar X\niv :1\n70 6.\n00 61\n2v 1\n[ cs\n.C L\n] 2\nJ un\n2 01\n7\nOur intuitions behind using an attention mechanism for emotion recognition are two-fold: a) speech emotion recognition is related to sentence classification with emotional content being differently distributed over the signal and b) the emotion of the whole signal is a composition of emotions from different parts of the signal. Therefore, attention mechanisms are suitable to first weight the information extracted from different pieces of the input and then combine them in a weighted sum. However, because the input signal is noisy, a max pooling layer is still helpful to only select the most salient features and filter noise. Therefore, we combine the CNN output vector after max pooling and the attention vector for the final softmax layer."}, {"heading": "2.3. Multi-view learning", "text": "Emotions can be represented in two ways, either as categorical labels (e.g. angry, happy) or as continuous labels in the 2D activation/valence space. In [22], it is shown that multi-view (MV) learning with both categorical and continuous labels for training can improve prediction results. Similarly, we extend our model to incorporate activation and valence information."}, {"heading": "3. Input Features", "text": "We use the following feature sets: (a) 26 logMel filter-banks, (b) 13 MFCCs, (c) a prosody feature set, and (d) the extended Geneva minimalistic acoustic parameter set (eGeMAPS). For all feature sets we apply mean and standard deviation normalization for each speaker independently.\nWe use the openSMILE toolkit [25] to extract all features. For logMel, MFCC, and prosody features, the audio signal is segmented into 25ms long frames with a 10ms shift. To extract logMel and MFCC features, a Hamming window is applied and the FFT with 512 points is computed. Then, we compute the logarithmic power of 26 Mel-frequency filter-banks over a\nrange from 0 to 6.5kHz. Finally, a discrete cosine transform (DCT) is applied to extract the first 13 MFCCs. The prosody feature set consists of the following features: PCM loudness, envelope of F0 contour, voicing probability, F0 contour, local jitter, differential jitter, and local shimmer.\nThe eGeMAPS is a hand-crafted feature set proposed for affective computing [26]. It consists of 25 low level descriptors (LLDs) containing frequency- and energy-related parameters and spectral parameters."}, {"heading": "4. Data", "text": "We use the Interactive Emotional Dyadic Motion Capture (IEMOCAP) database [27] for all experiments. It consists of about 12 hours of audiovisual data (speech, video, facial motion capture) from two recording scenarios: scripted play and improvised speech. Annotations are on turn level and consist of categorical labels (e.g. happy, sad, angry) and three continuous dimensions labeled with a discrete value from 1 to 5 each: activation, valence, dominance. For this study we use the same four categories as in [22, 28, 29, 30]: angry, happy, sad, and neutral. We merged happy and excited into one class: happy.1 To be comparable with related work and to find out more about differences between improvised and scripted speech, we take three subsets from the data: only improvised (2,943 turns), only scripted (2,588 turns), and all sessions (5,531 turns).\nThe mean length of all turns is 4.46s (max.: 34.1s, min.: 0.6s). Since the input length for a CNN has to be equal for all samples, we set the maximal length to 7.5s (mean duration plus standard deviation). Longer turns are cut at 7.5s and shorter ones are padded with zeros.\nWe group activation and valence labels into three categories each for the MV approach. The same range mapping as in [31] is used: low: [1,2]; medium: (2,4); high: [4,5]."}, {"heading": "5. Experimental Results", "text": ""}, {"heading": "5.1. Setup", "text": "The IEMOCAP data consists of five sessions with one male and one female speaker each. To train the models in a speakerindependent manner, we use leave-one-session-out cross validation. We take data from 8 speakers to construct training and development sets and use the remaining two speakers as test set.\nWe conduct two sets of experiments: Firstly, we compare the performance of CNN and ACNN (both with single-view (SV) and MV learning) regarding different input features. We run each combination of model, dataset and feature set six times with different random seeds. In doing so, we are able to report result variations due to random parameter initialization. We consider the averaged results produced this way more reliable than only reporting the single best number.\nSecondly, we intend to find out how much information in terms of length of an utterance is sufficient to predict the affective state. We train and test our model with decreasing utterance length (by cutting the speech signals at 7, 6, 5, 4, 3, 2, and 1 seconds respectively)."}, {"heading": "5.2. Hyper-parameters", "text": "Our CNN models are implemented with the Theano library [32, 33]. We use stochastic gradient descent with an adaptive\n1Class distribution: angry: 1,103; happy: 1,636; sad: 1,084; neutral: 1,708\nlearning rate (Adam [34]). For regularization dropout is applied to the last hidden layer [35]. The system\u2019s hyper-parameters are: 100 kernels with two different widths each (a total of 200 feature maps); a batch size of 30 for logMel and eGeMAPS, and 50 for MFCC; a dropout rate of 0.8; a pool size of 30, and stride of 3 for all configurations."}, {"heading": "5.3. Experiment 1: Different data and feature sets", "text": "For all experiments, we report weighted accuracy (WA, accuracy on the whole test set). All results are shown in Tables 1-3. The tables present averaged results across six runs and the respective minimum and maximum accuracy.\nImprovised speech (Table 1). The best performance is reached with logMel filter-banks. The ACNN with MV learning performs best with 62.11% mean accuracy. The single best result of 63.85% \u2013 which outperforms the state-of-the-art result of 62.85% reported by [36] \u2013 is reached with ACNN and SV learning.\nScripted speech (Table 2). Prediction results are in general notably lower than for improvised speech. For this dataset, MFCC and eGeMAPS features lead to higher accuracies than logMel. The best performance of 53.19% is achieved with the ACNN (MFCC with SV and eGeMAPS with MV).\nAll data (Table 3). MFCC and logMel features produce similar results, the accuracy with eGeMAPS is slightly lower, whereas prosody features perform notably worse. The best mean accuracy of 56.10% is achieved with logMel features using ACNN and MV learning. This model outperforms related work on the same data reported in [28, 29]. However, our focus does not lie on competing with state-of-the-art results (60.8% and 60.6% WA published in [22, 30]). In this work, we focus on the comparison of different input features, as well as the\ninterpretation of our results and a thorough error analysis (cf. section 6).\nFeature fusion. In addition to the results in Tables 1-3, we test early fusion of logMel and prosody features (only one run of each model configuration). These results show slight improvements for scripted data (53.69%, ACNN with MV), but decreasing results for the complete dataset and improvised speech. This suggests that the CNN model cannot learn more discriminatory features from this additional information. This might be due to the convolution kernels spanning all features.\nAll results show that prosody features alone perform worse than cepstral features like logMel and MFCC. In [37], the authors state that prosodic features are strongly speaker-dependent and that their use is debatable in speaker-independent emotion recognition. To confirm this with our results, a comparable speaker-dependent experiment would be necessary. We assume that the prosody feature set contains too little information (only seven features) to compete with the others. The performance differences between logMel, MFCC and eGeMAPS are in general small. This suggests that the CNN is able to learn high-level features equally from these different input features. To find out whether the same information is learnt by the model from different input, further investigation is needed. In general, MV learning improves the prediction only slightly, if at all. The attention mechanism brings slight improvements on the improvised and scripted data for most of the feature sets, but has almost no effect on the complete dataset. Further, we see that there is high variation between single runs of the same model/feature combination (up to 4.2% between min and max results).\nOverall, our model performs better on free speech (improvised) than on acted speech independent of the choice of fea-\ntures. These findings show that speech emotion recognition can be very sensitive to the type of speech data (in line with findings by [38]). Hence, it is important to carefully select suitable training data for a particular application."}, {"heading": "5.4. Experiment 2: Signal length", "text": "In the second experiment, we use the ACNN with MV learning to perform emotion recognition on signals with decreasing length. We use logMel and MFCC features because these performed best previously. Results are presented in Figure 2. In\ngeneral, accuracy decreases with shorter input. We observe a notable difference in the performance drop between improvised and scripted speech, especially with logMel features (3.4% and 7.5% drop). From these results we assume that in spontaneous speech, it is more likely that an utterance carries emotional content in the first seconds already, whereas in scripted speech it is more difficult to predict the emotion from only the first one or two seconds. In general, the results show that a relatively short snippet of a speech signal can be sufficient to perform emotion recognition with only a small accuracy loss. This is an important finding for the development of real-time applications which aim to make a prediction while the user is still speaking. Moreover, the training time of the system can be reduced."}, {"heading": "6. Error analysis", "text": "We analyze the predictions of the ACNN (logMel features, MV learning). Figures 3a-3c show the confusion matrices.\nFor improvised speech (Fig. 3a) the most striking observation is that the model predicts happy for 49.12% angry samples. This counter-intuitive mistake becomes more plausible when looking at the activation information. Both angry and happy have a high activation level. Hence, the system\u2019s frequent confusion is due to the fact that valence is harder to predict than activation [39, 24, 26]. The category sad is predicted best (73.01%). This observation is in line with findings by [37, 27]. Further, the neutral class is frequently confused with other classes. This seems plausible because the neutral state is located in the center of the activation-valence space, what makes the discrimination from other classes difficult.\nIn contrast, for scripted sessions the accuracy for angry is surprisingly high, and relatively low for sad and happy. In general, there are more errors in almost all classes. One reason for the high discrepancy in the class angry is the different class distribution (many angry samples in scripted sessions). But this does not explain all other differences. The analysis suggests that improvised speech is in general more variable and therefore makes it easier to discriminate affective states. Investigation with more data would be helpful to confirm these findings. Note the high percentage of sad samples predicted as happy (23.08%). To find out the reason for this frequent confusion, further analysis is necessary. The error distribution on the complete dataset (Fig. 3c) lies between those seen in Figures 3a and 3b. There are similar patterns as for improvised data (the angry/ happy confusion is not as severe)."}, {"heading": "7. Conclusion", "text": "We presented a comparison of different features for speech emotion recognition using an attentive CNN. The results with logMel, MFCC, and eGeMAPS features are similar, but notably lower with prosodic features. A reason for that could be the smaller number of features in the latter. The similar results suggest that for a CNN the particular choice of features is not as important as the model architecture and the amount and kind of training data. We found strong differences between improvised and scripted speech, obtaining better results on the first. Experiments with decreasing signal length showed that the performance decreases slightly, but remains at a relatively high level even for short signals down to two seconds. Future work includes testing the presented ACNN on a different database."}, {"heading": "8. References", "text": "[1] A. Mill, J. Allik et al., \u201cAge-related differences in emotion recog-\nnition ability: a cross-sectional study.\u201d Emotion, vol. 9, no. 5, p. 619, 2009.\n[2] T. Vogt and E. Andre\u0301, \u201cImproving automatic emotion recognition from speech via gender differentiation,\u201d in Proc. Language Resources and Evaluation Conference (LREC 2006), Genoa, 2006.\n[3] A. Waibel, T. Hanazawa et al., \u201cPhoneme recognition using timedelay neural networks,\u201d IEEE transactions on acoustics, speech, and signal processing, vol. 37, no. 3, pp. 328\u2013339, 1989.\n[4] Y. Le Cun, B. Boser et al., \u201cHandwritten digit recognition with a back-propagation network,\u201d in Advances in neural information processing systems, 1990.\n[5] O. Abdel-Hamid, A.-r. Mohamed et al., \u201cApplying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition,\u201d in 2012 IEEE international conference on Acoustics, speech and signal processing (ICASSP). IEEE, 2012, pp. 4277\u20134280.\n[6] T. N. Sainath, A.-r. Mohamed et al., \u201cDeep convolutional neural networks for lvcsr,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 8614\u2013 8618.\n[7] T. N. Sainath, B. Kingsbury et al., \u201cDeep convolutional neural networks for large-scale speech tasks,\u201d Neural Networks, vol. 64, pp. 39\u201348, 2015.\n[8] D. Palaz, R. Collobert et al., \u201cAnalysis of cnn-based speech recognition system using raw speech as input,\u201d in Proceedings of Interspeech, 2015.\n[9] T. Sainath and C. Parada, \u201cConvolutional neural networks for small-footprint keyword spotting,\u201d in Proceedings of Interspeech, 2015.\n[10] A. Graves, \u201cGenerating sequences with recurrent neural networks,\u201d arXiv preprint arXiv:1308.0850, 2013.\n[11] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[12] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio, \u201cShow, attend and tell: Neural image caption generation with visual attention.\u201d in ICML, vol. 14, 2015, pp. 77\u201381.\n[13] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, \u201cAttention-based models for speech recognition,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 577\u2013 585.\n[14] H. Adel and H. Schu\u0308tze, \u201cExploring different dimensions of attention for uncertainty detection,\u201d arXiv preprint arXiv:1612.06549, 2016.\n[15] F. Meng, Z. Lu, M. Wang, H. Li, W. Jiang, and Q. Liu, \u201cEncoding source language with convolutional neural network for machine translation,\u201d arXiv preprint arXiv:1503.01838, 2015.\n[16] W. Yin, H. Schu\u0308tze, B. Xiang, and B. Zhou, \u201cAbcnn: Attentionbased convolutional neural network for modeling sentence pairs,\u201d arXiv preprint arXiv:1512.05193, 2015.\n[17] A. Stuhlsatz, C. Meyer et al., \u201cDeep neural networks for acoustic emotion recognition: Raising the benchmarks,\u201d in 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2011, pp. 5688\u20135691.\n[18] N. E. Cibau et al., \u201cSpeech emotion recognition using a deep autoencoder,\u201d Proceedings of the XV Reunio\u0301n de Trabajo en Procesamiento de la Informacio\u0301n y Control (RPIC 2013), San Carlos de Bariloche, 2013.\n[19] L. Li, Y. Zhao et al., \u201cHybrid deep neural network\u2013hidden markov model (dnn-hmm) based speech emotion recognition,\u201d in Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on. IEEE, 2013, pp. 312\u2013317.\n[20] C. Huang et al., \u201cA research of speech emotion recognition based on deep belief network and svm,\u201d Mathematical Problems in Engineering, vol. 2014, 2014.\n[21] K. Han, D. Yu, and I. Tashev, \u201cSpeech emotion recognition using deep neural network and extreme learning machine.\u201d in Interspeech, 2014, pp. 223\u2013227.\n[22] R. Xia and Y. Liu, \u201cA multi-task learning framework for emotion recognition using 2d continuous space,\u201d IEEE Transactions on Affective Computing, 2015.\n[23] G. Keren and B. Schuller, \u201cConvolutional rnn: an enhanced model for extracting features from sequential data,\u201d arXiv preprint arXiv:1602.05875, 2016.\n[24] G. Trigeorgis, F. Ringeval et al., \u201cAdieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network,\u201d in ICASSP, 2016.\n[25] F. Eyben, F. Weninger et al., \u201cRecent developments in opensmile, the munich open-source multimedia feature extractor,\u201d in Proceedings of the 21st ACM international conference on Multimedia. ACM, 2013, pp. 835\u2013838.\n[26] F. Eyben, K. R. Scherer et al., \u201cThe geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing,\u201d IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2016.\n[27] C. Busso, M. Bulut et al., \u201cIemocap: Interactive emotional dyadic motion capture database,\u201d Language resources and evaluation, vol. 42, no. 4, pp. 335\u2013359, 2008.\n[28] Q. Jin, C. Li, S. Chen, and H. Wu, \u201cSpeech emotion recognition with acoustic and lexical features,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4749\u20134753.\n[29] S. Ghosh, E. Laksana, L.-P. Morency, and S. Scherer, \u201cRepresentation learning for speech emotion recognition,\u201d Interspeech 2016, pp. 3603\u20133607, 2016.\n[30] V. Rozgic, S. Ananthakrishnan, S. Saleem, R. Kumar, and R. Prasad, \u201cEnsemble of svm trees for multimodal emotion recognition,\u201d in Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific. IEEE, 2012, pp. 1\u20134.\n[31] A. Metallinou, M. Wollmer et al., \u201cContext-sensitive learning for enhanced audiovisual emotion classification,\u201d IEEE Transactions on Affective Computing, vol. 3, no. 2, pp. 184\u2013198, 2012.\n[32] J. Bergstra, O. Breuleux et al., \u201cTheano: A cpu and gpu math compiler in python,\u201d in Proc. 9th Python in Science Conf, 2010, pp. 1\u20137.\n[33] F. Bastien, P. Lamblin et al., \u201cTheano: new features and speed improvements. nips workshop on deep learning and unsupervised feature learning,\u201d NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.\n[34] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[35] N. Srivastava et al., \u201cDropout: a simple way to prevent neural networks from overfitting.\u201d Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[36] J. Lee and I. Tashev, \u201cHigh-level feature representation using recurrent neural network for speech emotion recognition.\u201d in INTERSPEECH, 2015, pp. 1537\u20131540.\n[37] V. Chernykh, G. Sterling, and P. Prihodko, \u201cEmotion recognition from speech with recurrent neural networks,\u201d arXiv preprint arXiv:1701.08071, 2017.\n[38] L. Tian, J. D. Moore, and C. Lai, \u201cEmotion recognition in spontaneous and acted dialogues,\u201d in Affective Computing and Intelligent Interaction (ACII), 2015 International Conference on. IEEE, 2015, pp. 698\u2013704.\n[39] B. Schuller, B. Vlasenko, F. Eyben, G. Rigoll, and A. Wendemuth, \u201cAcoustic emotion recognition: A benchmark comparison of performances,\u201d in Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 552\u2013557."}], "references": [{"title": "Age-related differences in emotion recognition ability: a cross-sectional study.", "author": ["A. Mill", "J. Allik"], "venue": "Emotion, vol. 9,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Improving automatic emotion recognition from speech via gender differentiation,", "author": ["T. Vogt", "E. Andr\u00e9"], "venue": "in Proc. Language Resources and Evaluation Conference (LREC", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Phoneme recognition using timedelay neural networks,", "author": ["A. Waibel", "T. Hanazawa"], "venue": "IEEE transactions on acoustics, speech, and signal processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1989}, {"title": "Handwritten digit recognition with a back-propagation network,", "author": ["Y. Le Cun", "B. Boser"], "venue": "Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition,", "author": ["O. Abdel-Hamid", "A.-r. Mohamed"], "venue": "IEEE international conference on Acoustics, speech and signal processing (ICASSP)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Deep convolutional neural networks for lvcsr,", "author": ["T.N. Sainath", "A.-r. Mohamed"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Deep convolutional neural networks for large-scale speech tasks,", "author": ["T.N. Sainath", "B. Kingsbury"], "venue": "Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input,", "author": ["D. Palaz", "R. Collobert"], "venue": "Proceedings of Interspeech,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Convolutional neural networks for small-footprint keyword spotting,", "author": ["T. Sainath", "C. Parada"], "venue": "Proceedings of Interspeech,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Generating sequences with recurrent neural networks,", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate,", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention.", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "in ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Attention-based models for speech recognition,", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exploring different dimensions of attention for uncertainty detection,", "author": ["H. Adel", "H. Sch\u00fctze"], "venue": "arXiv preprint arXiv:1612.06549,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Encoding source language with convolutional neural network for machine translation,", "author": ["F. Meng", "Z. Lu", "M. Wang", "H. Li", "W. Jiang", "Q. Liu"], "venue": "arXiv preprint arXiv:1503.01838,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks,", "author": ["A. Stuhlsatz", "C. Meyer"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Speech emotion recognition using a deep autoencoder,", "author": ["N.E. Cibau"], "venue": "Proceedings of the XV Reunio\u0301n de Trabajo en Procesamiento de la Informacio\u0301n y Control (RPIC", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Hybrid deep neural network\u2013hidden markov model (dnn-hmm) based speech emotion recognition,", "author": ["L. Li", "Y. Zhao"], "venue": "Affective Computing and Intelligent Interaction (ACII),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "A research of speech emotion recognition based on deep belief network and svm,", "author": ["C. Huang"], "venue": "Mathematical Problems in Engineering,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Speech emotion recognition using deep neural network and extreme learning machine.", "author": ["K. Han", "D. Yu", "I. Tashev"], "venue": "in Interspeech,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A multi-task learning framework for emotion recognition using 2d continuous space,", "author": ["R. Xia", "Y. Liu"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Convolutional rnn: an enhanced model for extracting features from sequential data,", "author": ["G. Keren", "B. Schuller"], "venue": "arXiv preprint arXiv:1602.05875,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network,", "author": ["G. Trigeorgis", "F. Ringeval"], "venue": "in ICASSP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Recent developments in opensmile, the munich open-source multimedia feature extractor,", "author": ["F. Eyben", "F. Weninger"], "venue": "Proceedings of the 21st ACM international conference on Multimedia", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing,", "author": ["F. Eyben", "K.R. Scherer"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Iemocap: Interactive emotional dyadic motion capture database,", "author": ["C. Busso", "M. Bulut"], "venue": "Language resources and evaluation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Speech emotion recognition with acoustic and lexical features,", "author": ["Q. Jin", "C. Li", "S. Chen", "H. Wu"], "venue": "in Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Representation learning for speech emotion recognition,", "author": ["S. Ghosh", "E. Laksana", "L.-P. Morency", "S. Scherer"], "venue": "Interspeech 2016,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Ensemble of svm trees for multimodal emotion recognition,", "author": ["V. Rozgic", "S. Ananthakrishnan", "S. Saleem", "R. Kumar", "R. Prasad"], "venue": "in Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Context-sensitive learning for enhanced audiovisual emotion classification,", "author": ["A. Metallinou", "M. Wollmer"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Theano: A cpu and gpu math compiler in python,", "author": ["J. Bergstra", "O. Breuleux"], "venue": "in Proc. 9th Python in Science Conf,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Theano: new features and speed improvements. nips workshop on deep learning and unsupervised feature learning,", "author": ["F. Bastien", "P. Lamblin"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Adam: A method for stochastic optimization,", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["N. Srivastava"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "High-level feature representation using recurrent neural network for speech emotion recognition.", "author": ["J. Lee", "I. Tashev"], "venue": "TERSPEECH,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Prihodko, \u201cEmotion recognition from speech with recurrent neural networks,", "author": ["V. Chernykh", "G. Sterling"], "venue": "arXiv preprint arXiv:1701.08071,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Emotion recognition in spontaneous and acted dialogues,", "author": ["L. Tian", "J.D. Moore", "C. Lai"], "venue": "Affective Computing and Intelligent Interaction (ACII),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Acoustic emotion recognition: A benchmark comparison of performances,", "author": ["B. Schuller", "B. Vlasenko", "F. Eyben", "G. Rigoll", "A. Wendemuth"], "venue": "Automatic Speech Recognition & Understanding,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "It is a challenging task due to the complexity of emotional expressions (affected by many factors such as age [1] and gender [2]) and the lack of a large dataset.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "It is a challenging task due to the complexity of emotional expressions (affected by many factors such as age [1] and gender [2]) and the lack of a large dataset.", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "Convolutional neural networks (CNN) proposed in [3, 4] are a special kind of neural networks that have been successfully used not only for computer vision but also for speech [5, 6, 7].", "startOffset": 48, "endOffset": 54}, {"referenceID": 3, "context": "Convolutional neural networks (CNN) proposed in [3, 4] are a special kind of neural networks that have been successfully used not only for computer vision but also for speech [5, 6, 7].", "startOffset": 48, "endOffset": 54}, {"referenceID": 4, "context": "Convolutional neural networks (CNN) proposed in [3, 4] are a special kind of neural networks that have been successfully used not only for computer vision but also for speech [5, 6, 7].", "startOffset": 175, "endOffset": 184}, {"referenceID": 5, "context": "Convolutional neural networks (CNN) proposed in [3, 4] are a special kind of neural networks that have been successfully used not only for computer vision but also for speech [5, 6, 7].", "startOffset": 175, "endOffset": 184}, {"referenceID": 6, "context": "Convolutional neural networks (CNN) proposed in [3, 4] are a special kind of neural networks that have been successfully used not only for computer vision but also for speech [5, 6, 7].", "startOffset": 175, "endOffset": 184}, {"referenceID": 7, "context": "For speech recognition, CNN proved to be robust against noise compared to other DL models [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "Furthermore, [9] showed that CNNs are suitable for small memory footprint keyword spotting due to the parameter sharing mechanism.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "More recently, attention based recurrent neural networks have been successfully applied to a wide range of tasks such as handwriting generation [10], machine translation [11], image caption generation [12] and speech recognition [13].", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "More recently, attention based recurrent neural networks have been successfully applied to a wide range of tasks such as handwriting generation [10], machine translation [11], image caption generation [12] and speech recognition [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "More recently, attention based recurrent neural networks have been successfully applied to a wide range of tasks such as handwriting generation [10], machine translation [11], image caption generation [12] and speech recognition [13].", "startOffset": 201, "endOffset": 205}, {"referenceID": 12, "context": "More recently, attention based recurrent neural networks have been successfully applied to a wide range of tasks such as handwriting generation [10], machine translation [11], image caption generation [12] and speech recognition [13].", "startOffset": 229, "endOffset": 233}, {"referenceID": 13, "context": "Researchers have also started to use attention mechanisms for CNNs in NLP tasks [14, 15, 16].", "startOffset": 80, "endOffset": 92}, {"referenceID": 14, "context": "Researchers have also started to use attention mechanisms for CNNs in NLP tasks [14, 15, 16].", "startOffset": 80, "endOffset": 92}, {"referenceID": 15, "context": "DL has been shown to significantly boost emotion recognition performance [17, 18, 19, 20, 21, 22].", "startOffset": 73, "endOffset": 97}, {"referenceID": 16, "context": "DL has been shown to significantly boost emotion recognition performance [17, 18, 19, 20, 21, 22].", "startOffset": 73, "endOffset": 97}, {"referenceID": 17, "context": "DL has been shown to significantly boost emotion recognition performance [17, 18, 19, 20, 21, 22].", "startOffset": 73, "endOffset": 97}, {"referenceID": 18, "context": "DL has been shown to significantly boost emotion recognition performance [17, 18, 19, 20, 21, 22].", "startOffset": 73, "endOffset": 97}, {"referenceID": 19, "context": "DL has been shown to significantly boost emotion recognition performance [17, 18, 19, 20, 21, 22].", "startOffset": 73, "endOffset": 97}, {"referenceID": 20, "context": "DL has been shown to significantly boost emotion recognition performance [17, 18, 19, 20, 21, 22].", "startOffset": 73, "endOffset": 97}, {"referenceID": 21, "context": "Recently, several papers [23, 24] presented CNNs in combination with Long Short-Term Memory models (LSTM) to improve speech emotion recognition based on log Mel filter-banks (logMel) or raw signal.", "startOffset": 25, "endOffset": 33}, {"referenceID": 22, "context": "Recently, several papers [23, 24] presented CNNs in combination with Long Short-Term Memory models (LSTM) to improve speech emotion recognition based on log Mel filter-banks (logMel) or raw signal.", "startOffset": 25, "endOffset": 33}, {"referenceID": 22, "context": "[24] demonstrated an end-to-end training from raw signal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [22], it is shown that multi-view (MV) learning with both categorical and continuous labels for training can improve prediction results.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "We use the openSMILE toolkit [25] to extract all features.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "The eGeMAPS is a hand-crafted feature set proposed for affective computing [26].", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "We use the Interactive Emotional Dyadic Motion Capture (IEMOCAP) database [27] for all experiments.", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "For this study we use the same four categories as in [22, 28, 29, 30]: angry, happy, sad, and neutral.", "startOffset": 53, "endOffset": 69}, {"referenceID": 26, "context": "For this study we use the same four categories as in [22, 28, 29, 30]: angry, happy, sad, and neutral.", "startOffset": 53, "endOffset": 69}, {"referenceID": 27, "context": "For this study we use the same four categories as in [22, 28, 29, 30]: angry, happy, sad, and neutral.", "startOffset": 53, "endOffset": 69}, {"referenceID": 28, "context": "For this study we use the same four categories as in [22, 28, 29, 30]: angry, happy, sad, and neutral.", "startOffset": 53, "endOffset": 69}, {"referenceID": 29, "context": "The same range mapping as in [31] is used: low: [1,2]; medium: (2,4); high: [4,5].", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "The same range mapping as in [31] is used: low: [1,2]; medium: (2,4); high: [4,5].", "startOffset": 48, "endOffset": 53}, {"referenceID": 1, "context": "The same range mapping as in [31] is used: low: [1,2]; medium: (2,4); high: [4,5].", "startOffset": 48, "endOffset": 53}, {"referenceID": 3, "context": "The same range mapping as in [31] is used: low: [1,2]; medium: (2,4); high: [4,5].", "startOffset": 76, "endOffset": 81}, {"referenceID": 4, "context": "The same range mapping as in [31] is used: low: [1,2]; medium: (2,4); high: [4,5].", "startOffset": 76, "endOffset": 81}, {"referenceID": 30, "context": "Our CNN models are implemented with the Theano library [32, 33].", "startOffset": 55, "endOffset": 63}, {"referenceID": 31, "context": "Our CNN models are implemented with the Theano library [32, 33].", "startOffset": 55, "endOffset": 63}, {"referenceID": 32, "context": "learning rate (Adam [34]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "For regularization dropout is applied to the last hidden layer [35].", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "85% reported by [36] \u2013 is reached with ACNN and SV learning.", "startOffset": 16, "endOffset": 20}, {"referenceID": 26, "context": "This model outperforms related work on the same data reported in [28, 29].", "startOffset": 65, "endOffset": 73}, {"referenceID": 27, "context": "This model outperforms related work on the same data reported in [28, 29].", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": "6% WA published in [22, 30]).", "startOffset": 19, "endOffset": 27}, {"referenceID": 28, "context": "6% WA published in [22, 30]).", "startOffset": 19, "endOffset": 27}, {"referenceID": 35, "context": "In [37], the authors state that prosodic features are strongly speaker-dependent and that their use is debatable in speaker-independent emotion recognition.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "These findings show that speech emotion recognition can be very sensitive to the type of speech data (in line with findings by [38]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 37, "context": "Hence, the system\u2019s frequent confusion is due to the fact that valence is harder to predict than activation [39, 24, 26].", "startOffset": 108, "endOffset": 120}, {"referenceID": 22, "context": "Hence, the system\u2019s frequent confusion is due to the fact that valence is harder to predict than activation [39, 24, 26].", "startOffset": 108, "endOffset": 120}, {"referenceID": 24, "context": "Hence, the system\u2019s frequent confusion is due to the fact that valence is harder to predict than activation [39, 24, 26].", "startOffset": 108, "endOffset": 120}, {"referenceID": 35, "context": "This observation is in line with findings by [37, 27].", "startOffset": 45, "endOffset": 53}, {"referenceID": 25, "context": "This observation is in line with findings by [37, 27].", "startOffset": 45, "endOffset": 53}], "year": 2017, "abstractText": "Speech emotion recognition is an important and challenging task in the realm of human-computer interaction. Prior work proposed a variety of models and feature sets for training a system. In this work, we conduct extensive experiments using an attentive convolutional neural network with multi-view learning objective function. We compare system performance using different lengths of the input signal, different types of acoustic features and different types of emotion speech (improvised/scripted). Our experimental results on the Interactive Emotional Motion Capture (IEMOCAP) database reveal that the recognition performance strongly depends on the type of speech data independent of the choice of input features. Furthermore, we achieved state-of-the-art results on the improvised speech data of IEMOCAP.", "creator": "LaTeX with hyperref package"}}}