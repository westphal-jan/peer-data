{"id": "1706.00637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Joint Matrix-Tensor Factorization for Knowledge Base Inference", "abstract": "while several matrix factorization ( mf ) and density tensor factorization ( tf ) models have been proposed for knowledge base ( kb ) inference, they have rarely been compared individually across various datasets. is there a single model that performs well across datasets? if not, what characteristics of a dataset determine the performance of mf and tf models? is there a joint tf + mf model component that performs robustly on all datasets? we perform an extensive evaluation to truly compare popular kb inference models across popular datasets in the literature. in her addition to answering the questions above, we remove a limitation in the standard attribute evaluation protocol for mf models, propose an extension to separate mf models so that they can better handle out - of - vocabulary ( oov ) conditional entity - pairs, and develop a novel combination of tf and mf models. we also analyze and explain the results based on intrinsic models and dataset characteristics. our best model is robust, and obtains strong results across all datasets.", "histories": [["v1", "Fri, 2 Jun 2017 11:34:37 GMT  (43kb)", "http://arxiv.org/abs/1706.00637v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["prachi jain", "shikhar murty", "mausam", "soumen chakrabarti"], "accepted": false, "id": "1706.00637"}, "pdf": {"name": "1706.00637.pdf", "metadata": {"source": "CRF", "title": "Joint Matrix-Tensor Factorization for Knowledge Base Inference", "authors": ["Prachi Jain", "Shikhar Murty", "Soumen Chakrabarti"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n00 63\n7v 1\n[ cs\n.A I]\n2 J\nun 2\nWhile several matrix factorization (MF) and tensor factorization (TF) models have been proposed for knowledge base (KB) inference, they have rarely been compared across various datasets. Is there a single model that performs well across datasets? If not, what characteristics of a dataset determine the performance of MF and TF models? Is there a joint TF+MF model that performs robustly on all datasets? We perform an extensive evaluation to compare popular KB inferencemodels across popular datasets in the literature. In addition to answering the questions above, we remove a limitation in the standard evaluation protocol for MF models, propose an extension to MF models so that they can better handle out-ofvocabulary (OOV) entity pairs, and develop a novel combination of TF and MF models. We also analyze and explain the results based on models and dataset characteristics. Our best model is robust, and obtains strong results across all datasets."}, {"heading": "1 Introduction", "text": "Inference over knowledge bases (KBs) has received significant attention within NLP research in the last decade. Most of the early works on this task focus on adapting probabilistic formalisms such as Markov Logic Networks and Bayesian Logic Programs for inferring new KB facts (Schoenmackers et al., 2008; Niu et al., 2012; Raghavan et al., 2012). The formalisms require a set of inference rules as input, which can be generated\n\u2217First two authors contributed equally to the paper\nautomatically using statistical regularities in KBs (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012; Jain and Mausam, 2016).\nRecent research on this task has integrated the two components of rule learning and fact inference into one joint deep learning framework. This eschews explicit representation and learning of inference rules, and instead employs a way to score a (possibly new) KB fact (e1, r, e2) directly. Various algorithms differ in their scoring functions, which score a KB fact using different model assumptions.\nThis line of research can be further subdivided into two broad categories: matrix factorization and tensor factorization . In both cases the models learn one or more embeddings of the relation r, however, they differ in their treatment of entities e1 and e2. Tensor factorization (TF) approaches (e.g., E (Riedel et al., 2013), TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), Rescal (Nickel et al., 2011) models) learn independent embeddings for e1 and e2, whereas matrix factorization (MF) methods (e.g., F (Riedel et al., 2013) model) learn an embedding per entity-pair (e1, e2). Except for one paper making some early progress (Singh et al., 2015), their relative benefits have not been studied in detail.\nMore importantly, MF and TF have been rarely compared on the same datasets. In particular, three popular KBs are commonly used for TF research (WN18, FB15K, FB15K-237) and one for MF research (NYT+FB, New York Times articles annotated with Freebase entities), but rarely has a model been tested on all four. To the best of our knowl-\nedge, no paper reports the performance of E and F models on WN18 or FB15K, TransE on FB15K-237 or NYT+FB, and DistMult on NYT+FB.\nContributions: We unify several closely related tasks into KB inference (KBI) from a combination of incomplete KBs and text corpus. Our goal is to design inference algorithms that work robustly across diverse input combinations and datasets.\nTo that end, we first compare E, TransE, F and DistMult (DM) models on all four datasets. The comparison reveals that subtle issues arise in the design of training and evaluation procedures when TF methods are compared against or combined with MF methods. Special care is needed to handle outof-vocabulary (OOV) entity-pairs during evaluation. Otherwise an MF algorithm may appear to perform better than it really does, as in the case of F\u2019s performance on FB15K-237 (Toutanova et al., 2015).\nIn response, we present the first unified KBI evaluation protocol that can meaningfully compare MF and DM approaches across several datasets. F\u2019s performance deteriorates using the KBI evaluation protocol. The main reason is an ad hoc handling of OOV entity-pairs by F. We then propose an enhancement of F that explicitly learns OOV entity-pair vectors. This significantly improves F\u2019s performance, but DistMult (DM) remains the most robust solution across all datasets.\nFurther analysis shows that datasets associated with TF approaches have high OOV-rate in most test folds, naturally resulting in F performing poorly. However, F performs well on the dataset with low OOV rate. Our final contribution is a robust joint algorithm combining DM and F, which is competitive with both models on all datasets, and also outperforms the joint models proposed earlier.\nAlong with the above results, we contribute opensource implementations1 of all the methods and testing protocols investigated."}, {"heading": "2 Background and Experimental Setup", "text": "We propose knowledge base inference (KBI) as a task that unifies several closely related tasks in prior work, particularly, knowledge base completion (KBC), link prediction, and relation extrac-\n1https://github.com/dair-iitd/kbi\ntion (RE). In KBC and link prediction, new tuples are inferred from an incomplete structured KB. In RE, relations are inferred between entities mentioned in an unstructured corpus. It is natural (Toutanova et al., 2015) to unify these paradigms, along with textual tuples from OpenIE (Etzioni et al., 2011).\nSpecifically, we are given an incomplete KB that consists of a set of entities E and relationsR. Rmay contain only semantic relations, only textual relations or a combination of both, as we want inference to benefit from structural regularities among unnormalized and canonical relations, even if these are not reconciled. The KB also contains T , a set of known valid tuples t \u2208 T . A tuple t = \u3008e1, r, e2\u3009 consists of a subject entity e1 \u2208 E , object entity e2 \u2208 E , and relation r \u2208 R. We use a shorthand ep12 to refer to entity pair (e1, e2). Our goal is to predict the validity of any new tuple not present in the KB.\nOur focus is on the numerous neural modelsM that learn distributed representations (embedding vectors \u2208 Rd) of entities and relations. At a high level, each model defines a way to compute a score for the tuple \u3008e1, r, e2\u3009 based on some factorization. There are two broad categories of factorization models \u2014 tensor factorization (TF) and matrix factorization (MF). Both these kinds of models learn one (or more) embedding of r denoted by ~r. However, they differ in their treatment of entities. TF models learn embeddings for each entity ~e1 and ~e2, whereas MF models learn a single embedding for each entity pair ~ep12. 2\nDifferent models differ primarily in the function \u03c6M (e1, r, e2) that combines these embeddings to score a tuple. A higher value of \u03c6M denotes a model\u2019s higher confidence that the tuple is valid. Table 1 lists the scoring functions used by four popular models, which are the focus of our paper. These are E, F (Riedel et al., 2013), TransE (Bordes et al., 2013), and DistMult (Yang et al., 2015). Of these, F is an MF model, since it uses the ~ep12 embeddings, while the rest three are TF models. Note that E learns two\n2Some models may also learn matrix embeddings instead of vectors (Nickel et al., 2011; Socher et al., 2013). We don\u2019t study these, as they are typically outperformed by the models implemented in this paper (Yang et al., 2015; Trouillon et al., 2016).\nembedding vectors ~rs and ~ro for a relation r. DM uses an element-wise multiplication \u2022 in its scoring function.\nOur choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockta\u0308schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).\nLoss functions: The models are trained such that tuples observed in the KB have higher scores than unobserved ones. Several loss functions have been proposed; we implement two common ones in this work: log-likelihood based loss and max margin loss. Both loss functions sample a negative set Neg(e1, r) for every tuple, computed as {\u3008e1, r, e \u2032 2\u3009|e \u2032 2 \u2208 E \u2227 \u3008e1, r, e \u2032 2\u3009 /\u2208 T }, i.e., tuples formed by uniformly sampling entities that are not apriori known to be valid. Similarly, the set Neg(r, e2) is sampled.\nTo define a log-likelihood based loss for M , Toutanova et al. (2015) first model an approximate3 conditional probability:\npM (e2|r, e1; \u03b8) =\nexp(\u03c6M (e1, r, e2; \u03b8)) \u2211\n\u3008e1,r,e\u20322\u3009\u2208Neg(e1,r) exp(\u03c6M (e1, r, e\u20322; \u03b8))\n(1)\nHere \u03b8 represents model parameters: the embeddings for each relation and entity (or entity pair). pM(e1|r, e2; \u03b8) is estimated similarly using\n3For a rigorous estimate, we need to include the numerator also in the denominator, and correct the denominator by the ratio of population to sample size.\nNeg(r, e2). The log-likelihood loss to minimize is\nLMll (T , \u03b8) = \u2212 [ \u2211 \u3008e1,r,e2\u3009\u2208T log pM (e1|r, e2; \u03b8)\n+ \u2211 \u3008e1,r,e2\u3009\u2208T log pM (e2|r, e1; \u03b8)\n]\n(2)\nOn the other hand, max-margin loss minimizes a margin-based ranking criterion (Bordes et al., 2013):\nLMmm(T , \u03b8) = \u2211\nt\u2208T\n\u2211\nt\u2032\u2208Neg(t)\n[ \u03b3 + \u03c6M (t\u2032)\u2212 \u03c6M (t) ]\n+ (3)\nwhere t = \u3008e1, r, e2\u3009, Neg(t) = Neg(e1, r) \u222a Neg(r, e2), \u03b3 is the margin and [x]+ = max{0, x}.\nFinally, note that since MFmodels operate over entity pairs, they do not need two Neg sets. They use one set where new entity pairs (e\u20321, e \u2032 2) are sampled such that \u3008e\u20321, r, e \u2032 2\u3009 /\u2208 T . These negative entity pairs are sampled only from the entity pairs found in T , since embeddings for only those pairs get learned.\nMF vs. TF Models: Limited comparisons have been made between the MF and TF families. Toutanova et al. (2015) compare F with some TF models on one dataset and find that F does not perform as well as TF. Singh et al. (2015) use a series of artificial experiments to conclude that MF models typically perform well on tasks where there is significant relation synonymy in the data, whereas TF models perform better when there are latent types for each relation that need to be predicted. Singh and Toutanova experiment on one real dataset each and show the value of (different) joint MF-TF models on those datasets. We revisit these in Section 5."}, {"heading": "2.1 Datasets", "text": "Most KB inference systems have used one or more of four popular KBs for evaluation. These include WN18 (eighteen Wordnet relations (Bordes et al., 2013)) and three datasets over Freebase (FB). One dataset is FB15K (Bordes et al., 2013) that has 1,345 relations. Another dataset is FB15K-237, which is a subset of FB15K comprising 237 relations that seldom overlap in terms of entity pairs (Toutanova et al., 2015). The fourth dataset is NYT+FB, which, along with FB triples, also includes dependency path-based textual relations from New York Times, the mentions of entities in which are aligned with entities in Freebase (Riedel et al., 2013).\nOur literature search reveals that no algorithm has been tested on all datasets. To the best of our knowledge, no paper reports results of E and F models on WN18 or FB-15K, TransE on FB15K-237 or NYT+FB, and DistMult on NYT+FB. To better understand the strengths and weaknesses of each model (especially TF vs. MF), we compare all models on all datasets. We also release their open source implementations for further research."}, {"heading": "2.2 Standard Evaluation Protocol", "text": "Since we wish to run these experiments at scale, we follow one of the common evaluation protocols that can be run completely automatically. This method splits the KB into train (Ttr) and test tuples (Tts). The system can access only Ttr during training. For each test tuple, \u3008e\u22171, r\n\u2217, e\u22172\u3009 \u2208 Tts, a query \u3008e\u22171, r\n\u2217, ?\u3009 is issued to the trained model M . The model then ranks all entities e2 \u2208 E by decreasing \u03c6M (e\u22171, r \u2217, e2). A higher rank of e \u2217 2 in this list suggests a better performance of the model. The metrics used to compare two algorithms are mean reciprocal rank (MRR) and the percentage of e\u22172s obtained in top 10 results (HITS@10).\nThe testing procedure is typically run with two modifications. First, it is possible that some of the e2s ranked higher than e \u2217 2 may form known valid tuples \u3008e\u22171, r \u2217, e2\u3009 \u2014 it is unfair to penalize the model for predicting these. The filtered metrics remove the set {e2|\u3008e \u2217 1, r\n\u2217, e2\u3009 \u2208 Ttr \u222a Tts} from the ranked list (Bordes et al., 2013).\nThe second modification applies primarily to MF models. In MF, an embedding is learned only for entity pairs that appear in Ttr. Therefore, it is futile to score every \u3008e\u22171, r\n\u2217, e2\u3009 over a large range of e2s, for most of which, ~ep12 is not even known. Instead, only those e2s in a smaller set\nE2 = {e2|\u2203r : \u3008e \u2217 1, r, e2\u3009 \u2208 Ttr \u222a Tts} (4)\nare considered as candidates for ranking (Toutanova et al., 2015; Verga et al., 2016b). If entity pair (e\u22171, e2) is not trained then a random vector is assumed for ~ep1\u22172."}, {"heading": "3 Comparison under Standard and Unified KBI Evaluation Protocols", "text": ""}, {"heading": "3.1 Training Details", "text": "We first re-implement all algorithms in a common framework written using Keras/Theano (Chollet, 2015; Theano Development Team, 2016). We use 100 dimensional vectors for all models. They are trained using mini-batch stochastic gradient descent with AdaGrad on K40 GPUs with a learning rate of 0.5. We pre-compute 200 negative samples per tuple. We set margin \u03b3 to 1 for max margin loss. Following previous work (Yang et al., 2015) all entity and entity-pair vectors are re-normalized to have a unit norm after each batch update. We use a batch size of 20,000 for training. We train all models for 200 epochs. We use early stopping on validation set (a small subset of training set), to prevent our models from overfitting.\nWe train each model on each dataset using both log-likelihood (LL) and max-margin (MM) loss functions. We pick the best loss function for every setting. In particular, we find that TransE performs much better with MM loss. LL loss works better or at par in all other models except that MM outperforms LL for DistMult on WN18 dataset.\nWe follow the train-dev-test splits used in previous experiments for FB15K, WN18, and FB15K237. The testsets Tts are 3\u201310% random samples from T . For NYT+FB, previous works had experimented on a test fold with only 80 correct tuples (Riedel et al., 2013). Since such a test set is rather small, and in keeping with our other data sets, we create our own train-test splits by randomly sampling about 2% tuples from T . Only tuples with FB relations are used in the test set similar to previous experiments on this dataset."}, {"heading": "3.2 Preliminary Results", "text": "The first four rows of Table 2 report the performance of all the models across the datasets. We observe DistMult (DM) to be an overall winner among tensor factorization models \u2013 E has good performance on FB15K-237, whereas TransE gets good scores on FB15K, however DM emerges the most robust. For TF models on three datasets (FB15K, FB15K237, WN18) our experiments are able to repli-\ncate (or improve upon) various results reported in prior works (Yang et al., 2015; Bordes et al., 2013; Toutanova et al., 2015).4 Since NYT+FB is a new test split, and F hasn\u2019t been tested on other datasets, those results can\u2019t be directly compared against previous work.\nWe also find that F outperforms DM on two datasets by wide margins and doesn\u2019t perform as well as DM on the other two. It appears that a qualitative analysis of DM vs. F will shed light on their relative strengths and weaknesses. Our analysis reveals a limitation in the standard evaluation protocol that can inflate F\u2019s performance scores for OOV entity pairs."}, {"heading": "3.3 KBI Evaluation Protocol", "text": "Recall the second modification from Section 2.2. When ranking possible entities e2 using the score \u03c6(e\u22171, r\n\u2217, e2) from MF models, the standard evaluation protocol operates over a subset E2, instead of all entities in E . This is because many entity pair embeddings (e\u22171, e2) are not even trained in the model,\n4(Yang et al., 2015) report a higher MRR for DM onWN18.\nand hence their scores will be meaningless. We call these OOV entity pairs. E2 contains all entities for which the entity pair (e\u22171, e2) is trained. But, additionally, all such e\u22172s are added to E2 that are gold entities for some query \u3008e\u22171, r\n\u2217, ?\u3009 in test set. If these are not trained, a random vector is assumed for them.\nTable 3(a) illustrates an extreme case where the gold entity pair (Bill Gates, Medina) is not seen in training, and only one e2 (Seattle) is seen with e \u2217 1. Here, the MRR for F model will be computed as 0.5 \u2014 a gross overestimation! Implicitly, (e\u22171, e \u2217 2) is getting ranked higher than all other OOV (e\u22171, e2)s, whereas they should all be equal. In other words, the mere presence of Tts in Eqn (4) leaks information.\nIdeally, an evaluation protocol for KBI, that is tolerant to OOV entity pairs, must assume all OOV entities at the same rank and output the average value over all possible rankings for them. In our enhanced protocol, we assume one random OOV entity pair vector (e\u22171, eoov), identify all e2 \u2208 E that are OOV, assign them all the same score from the model and compute aggregate scores based on all possible rankings of such OOV entities. In our example of\nTable 3(a), the MRR will be computed as the average of 12 , 1 3 , . . . which is a very small number.\nWe note that most existing MF models have been tested on test splits in which none of the gold entity pairs are OOV (except FB15k-237). Hence, the results reported in most previous papers are not affected by our proposed fix. Even otherwise, if variants of MFmodels are being compared among themselves, while they may overestimate performance somewhat, the relative ordering of various models may not be affected. On the other hand, OOVs become a central issue when MF models are compared against or combined with TF models, since realistic levels of sparsity are very different in the two models. We elaborate on this below."}, {"heading": "3.4 Results Adjusted for KBI Evaluation", "text": "When the KBI evaluation protocol is used, F\u2019s performance on all datasets drops drastically, to the extent that its performance is practically zero on two datasets, and extremely weak on the third. However, it continues to have the best numbers for NYT+FB. Our evaluation sanitizes the published numbers for F on FB15K-237 (Toutanova et al., 2015).\nWhy is there such a significant drop in F\u2019s scores? The answer lies in entity pair OOV rates for these datasets, i.e., the percentage of tuples in test set whose entity pairs were not seen while training. Table 4 reports some statistics about the datasets as well as their test sets. We notice that FB15K, FB15K-237 and WN18 all have a very high OOV rates, which is strongly correlated with poor performance of F. On the other hand, NYT+FB has a tiny OOV rate and F performs well on it.\nIndeed, it is obvious that if the gold entity pair is not even seen while training, an MF model won\u2019t be able to predict it, since it learns each entitypair vector separately. On the other hand, a TF model, by virtue of learning each entity vector sep-\narately (single entity OOVs are very infrequent in these datasets), could combine its knowledge of each individual entity for predicting unseen entitypairs. Singh et al. (2015) contribute some theoretical differences between MF and TF models (see Section 2). Our analysis on the basis of entity-pair OOVs adds to that understanding. Moreover, we believe that OOVs, and more generally, data sparsity, offer a more practical insight into differences between two model types \u2014 representation in MF necessitates more data points per entity pair, whereas TF is more robust to sparse datasets.\nWhy does DM model perform the best? While we do not have a conclusive answer to this question, we believe that two reasons could act in DM\u2019s favor. First, like F, DM also has a representation of an entity pair. However, rather than associating an opaque single vector with each entity pair (where the role of individual entities cannot be identified), DM composes the entity-pair vector using entity vectors, as ~e1 \u2022 ~e2. Thus, it is likely able to exploit some power of matrix factorization, while still being robust to data sparsity. Secondly, even TransE can be seen as composing an entity-pair vector (~e1\u2212 ~e2), but it is additive, whereas DM is multiplicative. Previous work on word vectors has shown that multiplicative scores often outperform additive ones as they amplify smaller differences and reduce larger ones (Levy and Goldberg, 2014; Stanovsky et al., 2015)."}, {"heading": "3.5 Most-Frequent Baselines", "text": "To improve our understanding of the difficulty of each dataset and the quality of each model, we introduce two baselines for our task. Given a query, \u3008e\u22171, r \u2217, ?\u3009 our first baseline ranks all entities based on the frequency of their occurrence with relation r\u2217, i.e., it orders each entity e2 based on the cardinality of the set {t|t = \u3008e1, r\n\u2217, e2\u3009 \u2227 t \u2208 Ttr}. A similar baseline orders each entity e2 based on its frequency of occurence with e\u22171, i.e., based on cardinality of the set {t|t = \u3008e\u22171, r, e2\u3009 \u2227 t \u2208 Ttr}. We name these baselines MFreq(e2|r \u2217) and MFreq(e2|e \u2217 1) respectively. Our motivation to introduce these is to check whether existing models are able to learn beyond such simple baselines or not.\nThe last two rows of Table 2 report the performance of these baselines. It is satisfying to see\nthat for FB15K and WN18 datasets, DM outperforms the baselines by large margins. However, for FB15K-237, DM is only marginally better than MFreq(e2|r \u2217). A closer analysis reveals that this dataset is constructed so that there is minimal entitypair overlap between relations. Thus, how would any model predict the best e2 for a query \u3008e \u2217 1, r\n\u2217, ?\u3009? If entity pairs haven\u2019t been repeated much, a natural approach may just find the most frequent entities seen with the relation and order based on frequency. We checked some high MRR predictions made by DM and found that often questions like, what is the language of a specific website were answered correctly as English. This is likely not because DM figured out the language of each website, but because English was the most frequent one.\nWe also observe that E\u2019s performance remains broadly similar to the performance of MFreq(e2|r \u2217). We attribute this to E\u2019s scoring function, since given e\u22171 and r \u2217, the only term relevant for ranking e2s is ~e2 \u22a4 \u00b7 ~ro, i.e., the model looks for compatibility with r\u2217 and ignores e\u22171 completely.\nFinally, for NYT+FB, MFreq(e2|e \u2217 1) beats F model significantly suggesting that while F is the best model on that dataset, it is not good enough. We explore this further in the next section."}, {"heading": "4 OOV Training for KB Inference", "text": "The previous section highlights the importance of OOV entity-pairs in the performance of MF models. In general, a robust model must gracefully handle unseen entities/entity-pairs. A natural extension is to explicitly model an OOV entity-pair vector for F model (and OOV entity vector for TF models). In particular, we represent a vector (eoov , eoov) vector for F and eoov for TF. 5 This modification means that OOV entity-pairs will have the same score.\n5We also tried learning several entity pair OOV vectors of the form (e1, eoov), but that didn\u2019t give us a better performance.\nOOV vectors can be trained in many ways. We develop two baselines that don\u2019t train the vectors explicitly. One baseline assigns a random value to (eoov , eoov). Another is an average baseline that computes (eoov , eoov) as the average of the vectors of all (e1, e2) pairs that occur only once in training.\nWe also propose a procedure to train the OOV vectors. The high-level motivation is that we wish to score a known tuple higher than a tuple with an OOV. To ensure this, we add (eoov , eoov) in the Neg set for each train tuple. This encourages the model to learn embeddings such that \u03c6F (e1, r, e2) > \u03c6F (eoov , r, eoov). Thus, we ensure that the performance of F is maintained when the gold entity pair is seen in training. Table 3(b) illustrates an example where the correct answer (New York) is seen with Tina Fey and OOV training doesn\u2019t displace its position. For a TF model, we follow an analogous procedure to train an OOV vector ~eoov. Results: Since the fractions of OOV entities (e\u22172s) in the testsets are rather small, OOV training doesn\u2019t benefit TF models much. However, it makes substantial improvements in F\u2019s performance. Table 5 compares trained OOV embeddings to the two baselines for F. We find that training of OOVs overall performs better (or at par) with averaging baseline. F\u2019s score improves tremendously on NYT+FB, to the extent that it is able to beat the MFreq(e2|e \u2217 1) baseline by a small margin. We conclude that OOV training is essential for realizing the full potential of MF models."}, {"heading": "5 Joint MF-TF Models", "text": "Background on Joint MF-TF Models: Recall that Singh et al. (2015) compare TF andMFmodels (particularly, E and F) and find that they have complementary strengths. In response they develop joint TF-MF models and find that they outperform individual models on artificial datasets and NYT+FB. Their best model (E+F) uses the scoring function\n\u03c6E+F = \u03c3(\u03c6E + \u03c6F ), where \u03c3 is the sigmoid function. We call this model an additive score (AS) joint model, since the scores of two models are added. Early works of Reidel et al. (2013) also experiment with a joint model for NYT+FB. Later, Toutanova et al. (2015) implement a joint E+DM+F model and tested it on FB15K-237 but no other datasets.\nWe are motivated by developing a model that is robust across all datasets. Do additive score E+F or additive score E+DM+F meet this requirement?\nAdditive loss (AL) joint model: Our goal is to develop one joint model that can at least match the performance of the best individual model for each dataset. We focus on joint DM+F models.\nPreliminary investigations reveal that additive score models can suffer substantial loss in performance on some datasets. Table 6 shows drop in performance in the DM component when trained jointly in additive score DM+F model. It clearly shows that DM\u2019s performance can reduce drastically due to joint training. A primary reason is that F scores overshadow DM (and E) scores.6 Moreover, the number of parameters inMFmodels (vectors for entity pairs) significantly outnumber those in TF models (vectors for entities). This can lead to significant overfitting.\nIn response, we develop a different class of joint models in which instead of adding the scores (\u03c6s), we add their loss functions: LDM+F = LDM +LF . We name these additive loss joint models (AL). We expect this to be more resilient to overshadowing, since the joint loss expects each models individual loss to decrease as much as possible. One may note that AL style of training is equivalent to training the\n6To calibrate them, we tried standardizing scores obtained from pre-trained models. We also tried to learn a slope and bias to push DM and F model scores to the same range simultaneously. We also tried sharing of relation parameters to allow information to flow from DM to MF. Unfortunately, none of the approaches were robust across datasets.\nmodels separately. However, joint training makes other extensions possible, such as regularization.\nRegularized additive loss (RAL): We extend the vanilla AL joint model to a regularized joint model in which the parameters of MF model are L2regularized. We expect this regularization to encourage a reduction in overfitting caused due to the large number of MF parameters. Overall, our final joint model has the loss function:\nLDM+F (\u03b8DM , \u03b8F ) = LDM (\u03b8DM )+LF (\u03b8F )+\u03bb \u2225 \u2225\u03b8F \u2225 \u2225\n2\nAt test time, for a query \u3008e\u22171, r \u2217, ?\u3009 an AL model cannot simply add the scores, since some entitypairs may be OOVs. We develop various backoff cases, reminiscent of traditional backoff in language models (Manning and Schu\u0308tze, 2001). For every e2:\n\u2022 Case 1: (e\u22171, e2) \u2208 Ttr. Score of tuple is \u03c6DM (e\u22171, r \u2217, e2) + \u03c6 F (e\u22171, r\n\u2217, e2). \u2022 Case 2: (e\u22171, e2) /\u2208 Ttr, but e2 is seen in training. Score of tuple is \u03c6DM (e\u22171, r\n\u2217, e2) + \u03c6F (eoov, r\n\u2217, eoov). \u2022 Case 3: e2 is not seen in training. Score of tuple is \u03c6DM (e\u22171, r \u2217, eoov)+\u03c6 F (eoov , r \u2217, eoov). Results: Table 7 compares the performance of individual models with joint models. Regularization penalty \u03bb is chosen over a small devset from within the training set. All joint models are trained using both max-margin and log-likelihood losses, and we report the better of the two.\nWe find that different additive score models (rows 3\u20135) perform well on some datasets, but are not robust across them. For example, in FB15K none of these are able to match up to DM\u2019s performance. We attribute this to overfitting by F, which makes the model believe that \u03c6F is predicting the tuple very well. This lets F override TF and reduces the joint model\u2019s need to learn the best TF model(s). Note that row 3 and row 5 are the models reported in (Singh et al., 2015) and (Toutanova et al., 2015), respectively.\nRows 6 and 7 report the results of additive loss DM+F models, both without and with regularization. As anticipated, adding the losses improves performance since both models get trained well. Moreover, regularization also helps considerably since now the model is not overwhelmed by too many F parameters. RAL version of DM+F achieves scores close to the best individual model on each dataset.\nIn some cases, its performance is marginally weaker, and in other cases it is slightly better. Overall, this model has the desired robustness across datasets.\nAnalysis: Row 8 of Table 7 also shows the accuracy of an oracle model that, for every test query, post-facto selects the model with the more accurate score (between DM and F). This upper bounds the performance expected from a perfect joint DM+F model, fixing the constituents. We find that the oracle is only 3-4 MRR percentage points better than our best model for two datasets, and the differences are much less for the other two. Overall, it suggests that our proposed joint model obtains a strong robust performance.\nTable 8 breaks down the performance of models on the subset of test queries that have OOVs and nonOOV gold entity pairs. This analysis is meaningful only for FB15K, since other datasets have extreme\nentity-pair OOV rates (see Table 4). We observe that while F has extremely poor performance on OOVs (and thus weak performance overall), it performs decently on non-OOVs. RAL DM+F is able to perform well on both OOVs and non-OOVs, whereas DM+F (AS) has poorer performance on both of them (although still better than vanilla F for OOVs). Also note that F is outperformed by DM even on non-OOVs; this refutes prior claims that F always performs better than TF models when test entity pairs are seen during training (Riedel et al., 2013; Toutanova et al., 2015)."}, {"heading": "6 Discussion and Future Work", "text": "We now list two observations that suggest important directions for future research in KB inference.\nDataset Characteristics: Our work subjects datasets to natural sanity checks. First, we introduce two most frequent baselines (Table 2) to understand the nature of the KBs. Second, we compute entitypair OOV rates (Table 4) as a rough predictor of the relative success of the TF and MF families. Finally, in Table 9, we report the singleton and doubleton percentages (for entity pairs). A singleton is an entity-pair occurring only once in the data (Ttr\u222aTts) and a doubleton is an entity pair that occurs exactly twice. Doubletons have a strong effect in the scenario painted in Table 3. We find that almost every dataset has some idiosyncrasy, which raises the question whether it is a good representative for the datasets found naturally.\nIn particular, WN-18 and FB15K-237 have near 100% entity-pair OOV rates, unlikely to be the case in real KBs. In FB15K-237 the best models are not much better than MFreq(e2|r \u2217) baseline. This is be-\ncause the dataset is artificially constructed to avoid relations with entity-pair overlap. But, this reduces its ability to make many interesting inferences. For NYT+FB, MFreq(e2|e \u2217 1) performance has a strong performance with 95% score on HITS@10. Moreover, learned models are able to improve its MRR by only about three percentage points. Statistics in Table 9 reveal that this could be because the dataset has an unusually high number of entity-pair doubletons: it is the only data set where doubletons by far outnumber singletons. It is unlikely that such a distribution occurs in a naturally occurring dataset. FB15K appears to pass our sanity tests. We believe that focus on better datasets will likely help us in better progress on KB inference.\nPath based inference: In KBs, a common type of inference is based on relation paths (or Hornclauses), e.g., (Michael Jordan, teaches at, Berkeley) and (Berkeley, is located in, California) implies (Michael Jordan, teaches in, California). To assess the ability of inference models to automatically learn such relation paths, we tested them on artificial datasets, where we provided many instances of two-hop paths with relations r1 and r2 implying a third relation r3. We find that none of the four models are effective at predicting such relations. A study similar to ours comparing the latest models that train over relation paths (Guu et al., 2015; Garc\u0131\u0301a-Dura\u0301n et al., 2015; Toutanova et al., 2016) will benefit our understanding of path-based inference."}, {"heading": "7 Related Work", "text": "Traditional methods for inference over KBs include random walks over knowledge graphs (Lao et al., 2011), natural logic inference (MacCartney and Manning, 2007), and use of statistical relational learning models such as Markov Logic Network, Bayesian Logic Programs, and Probabilistic Soft Logic (Schoenmackers et al., 2008; Raghavan et al., 2012; Wang and Cohen, 2015). These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gala\u0301rraga et al., 2013; Grycner et al., 2015;\nBerant et al., 2012; Jain and Mausam, 2016).\nNeural methods for KB inference combine both inference and rule learning into one unified framework to add new facts to the KB directly. Both MF and TF methods have been very popular with several extensions proposed for each. The original F model has been extended to incorporate first order logic rules, (Rockta\u0308schel et al., 2015; Demeester et al., 2016), to predict for relations not seen at training time (Verga et al., 2016a), etc. It has also been extended to generate embedding of a new entity-pair on the fly (Verga et al., 2016b). But that is different from our OOV method, since, at test time, they expect knowledge of several tuples between the same entity pair.\nSimilarly, other TF models also exist, for example, Parafac (Harshman, 1970), Rescal (Nickel et al., 2011) and NTN (Socher et al., 2013). These are older models which are shown to be outperformed by models evaluated in this paper. More recent models have also been introduced such as a model using holographic embeddings (Nickel et al., 2016), and another with asymmetric embeddings using complex vectors (Trouillon et al., 2016). It will be nice to compare these rigorously as well. The learned embeddings can use additional information such as typing (Chang et al., 2014), have been used to mine logical rules (Yang et al., 2015) and have been used for schema induction (Nimishakavi et al., 2016)."}, {"heading": "8 Conclusion", "text": "We extensively evaluate various tensor factorization (TF) and matrix factorization (MF) models for KB inference on all popular datasets. After replacing the standard evaluation protocol with our proposed OOV-cognizant KBI protocol, we find that DistMult (a TF model) is fairly robust across a variety of datasets, but F (an MF model) outperforms others on one dataset. F\u2019s performance increases further by training an OOV entity-pair vector. Finally, we propose joint models that combine DistMult and F. We find that adding the loss functions from both models with a regularization on F\u2019s parameters achieves the most robust results across all datasets.\nWe also present a series of analyses of our empirical results. First, our work increases our un-\nderstanding of relative strengths and weaknesses of MF and TF models given some important bulk characteristics of the data sets. Specifically, we establish a strong connection between accuracy of various approaches and the fraction of OOV test entity pairs, and the proportion between entity pair singletons and doubletons. Second, we find that our joint model achieves results at par with the best individual models for both OOV and non-OOV queries. As a by-product, we identify some peculiarities in existing datasets, which suggests a need to design better benchmark datasets.\nWe release our code for all models and evaluation protocols for further use by research community. In the future, we wish to study models that explicitly incorporate relation paths for KB inference."}], "references": [{"title": "Global learning of typed entailment rules", "author": ["Ido Dagan", "Jacob Goldberger"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Berant et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2011}, {"title": "Efficient treebased approximation for entailment graph learning. In The 50th Annual Meeting of the Association for Computational Linguistics", "author": ["Ido Dagan", "Meni Adler", "Jacob Goldberger"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Burges et al. (Burges et al.,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Kai-Wei Chang", "Wen-tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "Proceedings of the 2014 Conference", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "Regularizing relation representations by first-order implications", "author": ["Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": null, "citeRegEx": "Demeester et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Demeester et al\\.", "year": 2016}, {"title": "Open information extraction: The second generation", "author": ["Etzioni et al.2011] Oren Etzioni", "Anthony Fader", "Janara Christensen", "Stephen Soderland", "Mausam"], "venue": "In IJCAI,", "citeRegEx": "Etzioni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 2011}, {"title": "Amie: association rule mining under incomplete evidence in ontological knowledge bases", "author": ["Christina Teflioudi", "Katja Hose", "Fabian Suchanek"], "venue": "In Proceedings of the 22nd international conference on World Wide", "citeRegEx": "Gal\u00e1rraga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gal\u00e1rraga et al\\.", "year": 2013}, {"title": "Composing relationships with translations", "author": ["Antoine Bordes", "Nicolas Usunier"], "venue": "In Ma\u0300rquez et al. (Ma\u0300rquez et al.,", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Relly: Inferring hypernym relationships between relational phrases", "author": ["Grycner et al.2015] Adam Grycner", "Gerhard Weikum", "Jay Pujara", "James Foulds", "Lise Getoor"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Grycner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grycner et al\\.", "year": 2015}, {"title": "Traversing knowledge graphs in vector space", "author": ["Guu et al.2015] Kelvin Guu", "John Miller", "Percy Liang"], "venue": "In Ma\u0300rquez et al. (Ma\u0300rquez et al.,", "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Foundations of the parafac procedure: Models and conditions for an \u201cexplanatory\u201d multi-modal factor analysis", "author": ["Richard Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "Harshman.,? \\Q1970\\E", "shortCiteRegEx": "Harshman.", "year": 1970}, {"title": "Knowledge-guided linguistic rewrites for inference rule verification", "author": ["Jain", "Mausam2016] Prachi Jain", "Mausam"], "venue": "In Knight et al. (Knight et al.,", "citeRegEx": "Jain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2016}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W Cohen"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Dirt@ sbt@ discovery of inference rules from text", "author": ["Lin", "Pantel2001] Dekang Lin", "Patrick Pantel"], "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Lin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2001}, {"title": "Natural logic for textual inference", "author": ["MacCartney", "Manning2007] Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,", "citeRegEx": "MacCartney et al\\.,? \\Q2007\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2007}, {"title": "Foundations of statistical natural language processing", "author": ["Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2001}, {"title": "Patty: a taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu-", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Lorenzo Rosasco", "Tomaso A. Poggio"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Relation schema induction using tensor factorization with side information", "author": ["Nimishakavi", "Uday Singh Saini", "Partha P. Talukdar."], "venue": "Jian Su, Xavier Carreras,", "citeRegEx": "Nimishakavi et al\\.,? 2016", "shortCiteRegEx": "Nimishakavi et al\\.", "year": 2016}, {"title": "Elementary: Large-scale knowledge-base construction via machine learning and statistical inference", "author": ["Niu et al.2012] Feng Niu", "Ce Zhang", "Christopher R\u00e9", "Jude W. Shavlik"], "venue": "Int. J. Semantic Web Inf. Syst.,", "citeRegEx": "Niu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2012}, {"title": "Learning to read between the lines using bayesian logic programs", "author": ["Raymond J Mooney", "Hyeonseo Ku"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Raghavan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raghavan et al\\.", "year": 2012}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Injecting logical background knowledge into embeddings for relation extraction", "author": ["Sameer Singh", "Sebastian Riedel"], "venue": null, "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Scaling textual inference to the web", "author": ["Oren Etzioni", "Daniel S Weld"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schoenmackers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Schoenmackers et al\\.", "year": 2008}, {"title": "Learning first-order horn clauses from web text", "author": ["Oren Etzioni", "Daniel S Weld", "Jesse Davis"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schoenmackers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Schoenmackers et al\\.", "year": 2010}, {"title": "Towards combined matrix and tensor factorization for universal schema relation extraction", "author": ["Singh et al.2015] Sameer Singh", "Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2015}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Burges et al. (Burges et al.,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Open IE as an intermediate structure for semantic tasks", "author": ["Ido Dagan", "Mausam"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Stanovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stanovsky et al\\.", "year": 2015}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In Ma\u0300rquez et al. (Ma\u0300rquez et al.,", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Compositional learning of embeddings for relation paths in knowledge bases and text", "author": ["Xi Victoria Lin", "Wen-tau Yih", "Hoifung Poon", "Chris Quirk"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Toutanova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2016}, {"title": "Complex embeddings for simple link prediction", "author": ["Johannes Welbl", "Sebastian Riedel", "\u00c9ric Gaussier", "Guillaume Bouchard"], "venue": null, "citeRegEx": "Trouillon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trouillon et al\\.", "year": 2016}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["Verga et al.2016a] Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum"], "venue": "In Knight et al. (Knight et al.,", "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Generalizing to unseen entities and entity pairs with row-less universal schema. CoRR, abs/1606.05804", "author": ["Verga et al.2016b] Patrick Verga", "Arvind Neelakantan", "AndrewMcCallum"], "venue": null, "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Joint information extraction and reasoning: A scalable statistical relational learning approach", "author": ["Wang", "Cohen2015] William Yang Wang", "William W. Cohen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "In In International Conference on Learning Representations ICLR,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "\u2217First two authors contributed equally to the paper automatically using statistical regularities in KBs (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012; Jain and Mausam, 2016).", "startOffset": 104, "endOffset": 200}, {"referenceID": 0, "context": "\u2217First two authors contributed equally to the paper automatically using statistical regularities in KBs (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012; Jain and Mausam, 2016).", "startOffset": 104, "endOffset": 200}, {"referenceID": 17, "context": "\u2217First two authors contributed equally to the paper automatically using statistical regularities in KBs (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012; Jain and Mausam, 2016).", "startOffset": 104, "endOffset": 200}, {"referenceID": 23, "context": ", E (Riedel et al., 2013), TransE (Bordes et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": ", 2013), TransE (Bordes et al., 2013), DistMult (Yang et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 36, "context": ", 2013), DistMult (Yang et al., 2015),", "startOffset": 18, "endOffset": 37}, {"referenceID": 18, "context": "Rescal (Nickel et al., 2011) models) learn independent embeddings for e1 and e2, whereas matrix factorization (MF) methods (e.", "startOffset": 7, "endOffset": 28}, {"referenceID": 23, "context": ", F (Riedel et al., 2013) model) learn an embedding per entity-pair (e1, e2).", "startOffset": 4, "endOffset": 25}, {"referenceID": 27, "context": "Except for one paper making some early progress (Singh et al., 2015), their relative benefits have not been studied in detail.", "startOffset": 48, "endOffset": 68}, {"referenceID": 30, "context": "Otherwise an MF algorithm may appear to perform better than it really does, as in the case of F\u2019s performance on FB15K-237 (Toutanova et al., 2015).", "startOffset": 123, "endOffset": 147}, {"referenceID": 30, "context": "is natural (Toutanova et al., 2015) to unify these", "startOffset": 11, "endOffset": 35}, {"referenceID": 5, "context": "(Etzioni et al., 2011).", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "These are E, F (Riedel et al., 2013), TransE (Bordes et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 2, "context": ", 2013), TransE (Bordes et al., 2013), and DistMult (Yang et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 36, "context": ", 2013), and DistMult (Yang et al., 2015).", "startOffset": 22, "endOffset": 41}, {"referenceID": 18, "context": "Some models may also learn matrix embeddings instead of vectors (Nickel et al., 2011; Socher et al., 2013).", "startOffset": 64, "endOffset": 106}, {"referenceID": 28, "context": "Some models may also learn matrix embeddings instead of vectors (Nickel et al., 2011; Socher et al., 2013).", "startOffset": 64, "endOffset": 106}, {"referenceID": 36, "context": "We don\u2019t study these, as they are typically outperformed by the models implemented in this paper (Yang et al., 2015; Trouillon et al., 2016).", "startOffset": 97, "endOffset": 140}, {"referenceID": 32, "context": "We don\u2019t study these, as they are typically outperformed by the models implemented in this paper (Yang et al., 2015; Trouillon et al., 2016).", "startOffset": 97, "endOffset": 140}, {"referenceID": 30, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 32, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 4, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 24, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 27, "context": "Our choice of these models is guided by the fact that these algorithms either form the basis of several recent papers on KB inference or are popular baselines for comparison studies (Toutanova et al., 2015; Trouillon et al., 2016; Demeester et al., 2016; Rockt\u00e4schel et al., 2015; Verga et al., 2016b; Verga et al., 2016a; Singh et al., 2015).", "startOffset": 182, "endOffset": 342}, {"referenceID": 30, "context": "To define a log-likelihood based loss for M , Toutanova et al. (2015) first model an approximate conditional probability:", "startOffset": 46, "endOffset": 70}, {"referenceID": 2, "context": "On the other hand, max-margin loss minimizes a margin-based ranking criterion (Bordes et al., 2013):", "startOffset": 78, "endOffset": 99}, {"referenceID": 29, "context": "Toutanova et al. (2015) compare F with some TF models on one dataset and find that F does not perform as well as TF.", "startOffset": 0, "endOffset": 24}, {"referenceID": 27, "context": "Singh et al. (2015) use a series of artificial experiments to conclude that MF models typically perform well on tasks where there is significant relation synonymy in the data, whereas TF models perform better when there are latent types for each relation that need to be predicted.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "These include WN18 (eighteen Wordnet relations (Bordes et al., 2013)) and three datasets over Freebase (FB).", "startOffset": 47, "endOffset": 68}, {"referenceID": 2, "context": "(Bordes et al., 2013) that has 1,345 relations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "Another dataset is FB15K-237, which is a subset of FB15K comprising 237 relations that seldom overlap in terms of entity pairs (Toutanova et al., 2015).", "startOffset": 127, "endOffset": 151}, {"referenceID": 23, "context": "The fourth dataset is NYT+FB, which, along with FB triples, also includes dependency path-based textual relations from New York Times, the mentions of entities in which are aligned with entities in Freebase (Riedel et al., 2013).", "startOffset": 207, "endOffset": 228}, {"referenceID": 2, "context": "The filtered metrics remove the set {e2|\u3008e \u2217 1, r \u2217, e2\u3009 \u2208 Ttr \u222a Tts} from the ranked list (Bordes et al., 2013).", "startOffset": 91, "endOffset": 112}, {"referenceID": 30, "context": "are considered as candidates for ranking (Toutanova et al., 2015; Verga et al., 2016b).", "startOffset": 41, "endOffset": 86}, {"referenceID": 36, "context": "Following previous work (Yang et al., 2015) all entity and entity-pair vectors are re-normalized to have a unit norm after each batch update.", "startOffset": 24, "endOffset": 43}, {"referenceID": 23, "context": "For NYT+FB, previous works had experimented on a test fold with only 80 correct tuples (Riedel et al., 2013).", "startOffset": 87, "endOffset": 108}, {"referenceID": 36, "context": "cate (or improve upon) various results reported in prior works (Yang et al., 2015; Bordes et al., 2013; Toutanova et al., 2015).", "startOffset": 63, "endOffset": 127}, {"referenceID": 2, "context": "cate (or improve upon) various results reported in prior works (Yang et al., 2015; Bordes et al., 2013; Toutanova et al., 2015).", "startOffset": 63, "endOffset": 127}, {"referenceID": 30, "context": "cate (or improve upon) various results reported in prior works (Yang et al., 2015; Bordes et al., 2013; Toutanova et al., 2015).", "startOffset": 63, "endOffset": 127}, {"referenceID": 36, "context": "(Yang et al., 2015) report a higher MRR for DM onWN18.", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "Our evaluation sanitizes the published numbers for F on FB15K-237 (Toutanova et al., 2015).", "startOffset": 66, "endOffset": 90}, {"referenceID": 27, "context": "Singh et al. (2015) contribute some theoret-", "startOffset": 0, "endOffset": 20}, {"referenceID": 29, "context": "Previous work on word vectors has shown that multiplicative scores often outperform additive ones as they amplify smaller differences and reduce larger ones (Levy and Goldberg, 2014; Stanovsky et al., 2015).", "startOffset": 157, "endOffset": 206}, {"referenceID": 27, "context": "Background on Joint MF-TF Models: Recall that Singh et al. (2015) compare TF andMFmodels (particularly, E and F) and find that they have comple-", "startOffset": 46, "endOffset": 66}, {"referenceID": 27, "context": "Note that row 3 and row 5 are the models reported in (Singh et al., 2015) and (Toutanova et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 30, "context": ", 2015) and (Toutanova et al., 2015), respectively.", "startOffset": 12, "endOffset": 36}, {"referenceID": 23, "context": "Also note that F is outperformed by DM even on non-OOVs; this refutes prior claims that F always performs better than TF models when test entity pairs are seen during training (Riedel et al., 2013; Toutanova et al., 2015).", "startOffset": 176, "endOffset": 221}, {"referenceID": 30, "context": "Also note that F is outperformed by DM even on non-OOVs; this refutes prior claims that F always performs better than TF models when test entity pairs are seen during training (Riedel et al., 2013; Toutanova et al., 2015).", "startOffset": 176, "endOffset": 221}, {"referenceID": 9, "context": "A study similar to ours comparing the latest models that train over relation paths (Guu et al., 2015; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Toutanova et al., 2016) will benefit our understanding of path-based inference.", "startOffset": 83, "endOffset": 153}, {"referenceID": 7, "context": "A study similar to ours comparing the latest models that train over relation paths (Guu et al., 2015; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Toutanova et al., 2016) will benefit our understanding of path-based inference.", "startOffset": 83, "endOffset": 153}, {"referenceID": 31, "context": "A study similar to ours comparing the latest models that train over relation paths (Guu et al., 2015; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Toutanova et al., 2016) will benefit our understanding of path-based inference.", "startOffset": 83, "endOffset": 153}, {"referenceID": 12, "context": "graphs (Lao et al., 2011), natural logic inference (MacCartney and Manning, 2007), and use of statistical relational learning models", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": "such as Markov Logic Network, Bayesian Logic Programs, and Probabilistic Soft Logic (Schoenmackers et al., 2008; Raghavan et al., 2012; Wang and Cohen, 2015).", "startOffset": 84, "endOffset": 157}, {"referenceID": 22, "context": "such as Markov Logic Network, Bayesian Logic Programs, and Probabilistic Soft Logic (Schoenmackers et al., 2008; Raghavan et al., 2012; Wang and Cohen, 2015).", "startOffset": 84, "endOffset": 157}, {"referenceID": 26, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 17, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 6, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 8, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 1, "context": "These need (or benefit from) a background knowledge of inference rules, predominantly generated via extended distributional similarity (Lin and Pantel, 2001; Schoenmackers et al., 2010; Nakashole et al., 2012; Gal\u00e1rraga et al., 2013; Grycner et al., 2015; Berant et al., 2012; Jain and Mausam, 2016).", "startOffset": 135, "endOffset": 299}, {"referenceID": 24, "context": "The original F model has been extended to incorporate first order logic rules, (Rockt\u00e4schel et al., 2015; Demeester et al., 2016), to predict for relations not seen at training time (Verga et al.", "startOffset": 79, "endOffset": 129}, {"referenceID": 4, "context": "The original F model has been extended to incorporate first order logic rules, (Rockt\u00e4schel et al., 2015; Demeester et al., 2016), to predict for relations not seen at training time (Verga et al.", "startOffset": 79, "endOffset": 129}, {"referenceID": 10, "context": "Similarly, other TF models also exist, for example, Parafac (Harshman, 1970), Rescal (Nickel et al.", "startOffset": 60, "endOffset": 76}, {"referenceID": 18, "context": "Similarly, other TF models also exist, for example, Parafac (Harshman, 1970), Rescal (Nickel et al., 2011) and NTN (Socher et al.", "startOffset": 85, "endOffset": 106}, {"referenceID": 28, "context": ", 2011) and NTN (Socher et al., 2013).", "startOffset": 16, "endOffset": 37}, {"referenceID": 19, "context": "More recent models have also been introduced such as a model using holographic embeddings (Nickel et al., 2016), and another with asymmetric embeddings using complex vectors (Trouillon et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 32, "context": ", 2016), and another with asymmetric embeddings using complex vectors (Trouillon et al., 2016).", "startOffset": 70, "endOffset": 94}, {"referenceID": 3, "context": "The learned embeddings can use additional information such as typing (Chang et al., 2014), have been used to mine logical rules (Yang et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 36, "context": ", 2014), have been used to mine logical rules (Yang et al., 2015) and have been used for schema induction (Nimishakavi et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 20, "context": ", 2015) and have been used for schema induction (Nimishakavi et al., 2016).", "startOffset": 48, "endOffset": 74}], "year": 2017, "abstractText": "While several matrix factorization (MF) and tensor factorization (TF) models have been proposed for knowledge base (KB) inference, they have rarely been compared across various datasets. Is there a single model that performs well across datasets? If not, what characteristics of a dataset determine the performance of MF and TF models? Is there a joint TF+MF model that performs robustly on all datasets? We perform an extensive evaluation to compare popular KB inferencemodels across popular datasets in the literature. In addition to answering the questions above, we remove a limitation in the standard evaluation protocol for MF models, propose an extension to MF models so that they can better handle out-ofvocabulary (OOV) entity pairs, and develop a novel combination of TF and MF models. We also analyze and explain the results based on models and dataset characteristics. Our best model is robust, and obtains strong results across all datasets.", "creator": "LaTeX with hyperref package"}}}