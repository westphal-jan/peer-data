{"id": "1706.01069", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "CRNN: A Joint Neural Network for Redundancy Detection", "abstract": "this paper proposes a fully novel framework for detecting redundancy in supervised sentence categorisation. unlike traditional singleton neural network, our model now incorporates character - aware convolutional tagged neural network ( char - cnn ) with character - aware recurrent neural network ( char - rnn ) to form a convolutional recurrent neural network ( crnn ). our model benefits from char - cnn in that only salient features are selected and fed into the integrated char - rnn. char - rnn effectively learns long sequence semantics via sophisticated update mechanism. we compare our framework against the state - of - the - art text classification algorithms on four popular benchmarking corpus. for instance, our model achieves competing precision rate, recall ratio, and f1 score on the google - news data - set. for twenty - news - groups data stream, our algorithm obtains the optimum on precision rate, recall ratio, reliability and f1 score. for brown corpus, our framework obtains the overall best f1 score function and almost equivalent precision rate and recall odds ratio over the top competitor. for the question classification collection, crnn produces the optimal recall rate distribution and f1 score and comparable precision rate. we also analyse three different rnn hidden recurrent cells'impact on performance and their runtime efficiency. we observe that mgu achieves the optimal runtime and comparable performance against gru and lstm. for tfidf compiler based algorithms, we experiment with word2vec, glove, soap and sent2vec embeddings and report their performance differences.", "histories": [["v1", "Sun, 4 Jun 2017 13:12:45 GMT  (115kb)", "http://arxiv.org/abs/1706.01069v1", "Conference paper accepted at IEEE SMARTCOMP 2017, Hong Kong"]], "COMMENTS": "Conference paper accepted at IEEE SMARTCOMP 2017, Hong Kong", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinyu fu", "eugene ch'ng", "uwe aickelin", "simon see"], "accepted": false, "id": "1706.01069"}, "pdf": {"name": "1706.01069.pdf", "metadata": {"source": "CRF", "title": "CRNN: A Joint Neural Network for Redundancy Detection", "authors": ["Xinyu Fu", "Eugene Ch\u2019ng", "Uwe Aickelin", "Simon See"], "emails": ["Uwe.Aickelin}@nottingham.edu.cn", "ssee@nvidia.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n01 06\n9v 1\n[ cs\n.C L\n] 4\nJ un\ning redundancy in supervised sentence categorisation. Unlike\ntraditional singleton neural network, our model incorporates\ncharacter-aware convolutional neural network (Char-CNN)\nwith character-aware recurrent neural network (Char-RNN) to\nform a convolutional recurrent neural network (CRNN). Our\nmodel benefits from Char-CNN in that only salient features are\nselected and fed into the integrated Char-RNN. Char-RNN\neffectively learns long sequence semantics via sophisticated\nupdate mechanism. We compare our framework against the\nstate-of-the-art text classification algorithms on four popular\nbenchmarking corpus. For instance, our model achieves com-\npeting precision rate, recall ratio, and F1 score on the Google-\nnews data-set. For twenty-news-groups data stream, our al-\ngorithm obtains the optimum on precision rate, recall ratio,\nand F1 score. For Brown Corpus, our framework obtains the\nbest F1 score and almost equivalent precision rate and recall\nratio over the top competitor. For the question classification\ncollection, CRNN produces the optimal recall rate and F1\nscore and comparable precision rate. We also analyse three\ndifferent RNN hidden recurrent cells\u2019 impact on performance\nand their runtime efficiency. We observe that MGU achieves\nthe optimal runtime and comparable performance against\nGRU and LSTM. For TFIDF based algorithms, we experiment\nwith word2vec, GloVe, and sent2vec embeddings and report\ntheir performance differences.\nIndex Terms\u2014Sentence classification, RNN, CNN, LSTM,\nGRU, MGU, word2vec, GloVe"}, {"heading": "1. Introduction", "text": "Redundancy detection is the process pairing substrings of text to preclassified labels. With the growth of online data acquisition techniques and infrastructure, users have strong willingness to develop knowledge of the changing world. Research on redundancy detection began by traditional bagof-words (BOW), TFIDF frequency matrix, and n-gram language modelling [1], [2]. The recent advances on the deep neural networks further push the community in the area of text classification. In accordance with Krizhevsky et al.\n[3] and Collobert et al. [4], CNN has shown state-of-the-art performance on computer vision and varied natural language processing (NLP) problems. Whilst traditional methods are good at encoding term association relationships (stock and market), neural models tend to preserve word similarity relationships (stock and security) [5].\nWe propose the utilisation of Char-CNN to sift the most salient features because of the prevalence on deep neural network and its applications to various domains like the challenging online text classification and clustering [6], [7], [8]. Char-CNN also adds confidence to accuracy by learning subword information, misspelling or grammatical errors. These issues are likely to happen in a human derived works such as newswire streams and social media data sources [6], [7], [8]. By feeding the most atomic characters in the deep neural model, grammatical errors and misspelling keywords can be interpreted in such a way that appropriate linguistic knowledge can be learned. The systematic comparison against the four challenging benchmarking data-sets demonstrates the high level of performance obtained through our model.\nPlain CNN, however, suffers from remembering longterm dependencies over sequences, as opposed to the traditional n-gram language modelling [9]. Nevertheless, according to Brown et al. [2], the system memory takes to store trigram phrases is enormous, which demands high spatial and time complexity. One possible solution to this is RNN [6], [10]. The embedded RNN hidden state and the non-linear activation function are capable of capturing long textual sequence probability distribution over iteration [6], [10]. By replacing the simple element-wise sigmoid activation function with a more complex long short-term memory (LSTM), the integrity of expressing lengthy snippets can be further improved, in line with Hochreiter and Schmidhuber [11]. Cho et al. [10] proposed a gated recurrent unit (GRU) which can dynamically reserve the state information through the pipeline of input signal. Our model benefits from CharCNN on the regularities feature-distilling and Char-RNN upon the long-term dependencies learning.\nThe rest of the paper is organised as follows. Section 2 provides related literature reviews. Section 3 introduces the proposed model. Section 4 presents design of the model,\ndata-sets, data pre-processing, and training parameters configuration. Results and detailed discussion is demonstrated in section 5. We discuss the results and findings in section 6. Finally, section 7 concludes the work and presents the possible future pointers."}, {"heading": "2. Related Work", "text": "Until recently, scholars are aware that convolutional deep neural network (CDNN) can be applied to many classification tasks. The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3]. However, these approaches have limited application in redundancy detection.\nThe emergence of CDNN has also shed some lights on the text mining society [15]. Kim [16] developed a twochannel CNN, a small variation of Collobert et al. model [4]. Kim\u2019s model was specifically used for sentiment polarity scoring and questions classification. Zhang et al. [7] offered an empirical investigation on the application of characterlevel convolutional networks for binary textual classification. Further, Yin and Schutze [17] exhibited a multi-channel CNN for sentence classification. Zhang et al. [5] gave a multiple word embeddings based CNN model for sentence categorisation. Nevertheless, those frameworks were tested merely on simple classification or sentiment analysis.\nTraditionally, pure RNNs were applied to fields like statistical machine translation [10], polyphonous music modelling, speech signal understanding [18], and Python programme evaluation [19]. Currently, RNNs have seen promising results on text classification [18]. RNN is usually powered by the recurrent units such as tanh [18]. Advanced recurrent units such as GRU cells [10], LSTM [11], and MGU [20] are released recently. MGU is probably better than GRU in that the gating mechanism in MGU not only halves the number of units when building up the model, but also reduce the size of the model by optimising the number of parameters during network interaction [18]. Moreover, GRU cell is different from the LSTM in that a separate memory cells is not presented in the former one. However, there is no indication which approach has the best performance without further investigation [18].\nLai et al. [21] proposed a recurrent convolutional neural network (RCNN). The recurrent structure learns the specific word with its left and right context in Lai et al. [21]. In contrast to the approach taken by Lai et al. [21], inferred sentence structure in our model is obtained through the pre-trained word2vec embeddings [22] or GloVe [23] embeddings. Kim et al. [8] described a hybrid network with CNN-LSTM to capture syntactic information buried in the contextual data. Moreover, empirical surveying by Zhang et al. shows that DCNN is less effective when the data volume is less than several million [7]. Our proposed model is the hybrid of a convolutional network and recurrent structure with character level encoding. The joint framework fits\nwell with the four benchmarking data corpuses and yielded comparable performance."}, {"heading": "3. Ingredients of the Model", "text": "Our Char-CNN was tailored from the original model to make it suitable for this work. Our Char-CNN consists of a convolving layer, RELU, and max-pooling layer. The classical Char-CNN from Vosoughi et al. [6] has multilayer structure which seems inefficient when trained with the benchmarking data. We adapted Char-RNN from Cho et al. [10] and Hochreiter and Schmidhuber [11]. Our CharRNN model has three different variations including GRU [10], MGU [20], and LSTM [11]. Our Char-RNN is different from the tradition in that the input gate fits with the intermediate result from Char-CNN and the output gate follows modulated activation."}, {"heading": "3.1. Character-Aware Convolutional Neural Network", "text": "In our model, C represents the container of character quantisation and d is the character embedding size. The model was developed to support the same 70 characters supported by Vosoughi et al, which are presented below [6].\nSuppose that a term t \u2208 C is comprised of a sequence of characters with length k, and then the character-level word representation of t is denoted as Ck \u2208 Rd\u00d7k. Each character is encoded as a binary vector v \u2208 {0, 1}d. Because the character quantisation is composed of 70 non-space tokens, d is equal to 70 in this case.\nGiven the above input sequence, the 1-D convolving layer discrete kernel function f(i) \u2208 [1,m] \u2192 R and the proper padding algorithm p, the output feature map is\nhk(i) = RELU(\nm \u2211\ni=1\nCkf(i\u00d7 tk + o)), (1)\nwhere o = m\u2212k+1 is a free constant, m depicts the output size, and RELU stands for the rectified linear unit [24].\nThe temporal max-pooling layer operates in the following manner. Suppose that stride is highlighted by s,\ny(i) = maxmi=1C kf(i\u00d7 s\u2212 k), (2)\nNotice that padding scheme is not applied at the maxpooling layer."}, {"heading": "3.2. Character-Aware Recurrent Neural Network", "text": "We now describe three RNN recurrent units, namely GRU [10], MGU [20], and LSTM [11]. Compared to the traditional implementation on GRU, MGU, and LSTM, we implanted an aggregator to produce combined result from two ingredients of CRNN.\nOur LSTM is composed by five parametric arguments:\nmemory cell mjt , input gate i j t , forget gate f j t , output gate ojt , and aggregated output state h j t for each j-th element. m j t maintains the memory information of the j-th LSTM unit. The update of the memory cell is formulated as\nmjt = f j t c j t\u22121 + i j tm\u0303 j t . (3)\nm\u0303jt is defined as\nm\u0303jt = tanh(Wmxt + Umht\u22121). (4)\nThe input gate ijt is formulated by\nijt = \u03c6(Wixt + Uiht\u22121 + Vimt\u22121) j , (5)\nwhere Vi is a diagonal matrix and \u03c6 is an activation function. The modulation of the forget gate f jt is as follows\nf jt = \u03c6(Wfxt + Ufht\u22121 + Vfmt\u22121) j , (6)\nwhere Vf is a diagonal matrix. The output gate o j t is calculated as\nojt = \u03c6(Woxt + Uoht\u22121 + Vomt) j , (7)\nwhere \u03c6 is a nonlinear logistic function and Vo is a diagonal matrix. The output of the j-th LSTM element hjt is computed as\nhjt = o j t tanh(m j t ), (8)\nwhere ojt decides the exposed amount of memory content. Due to the adaptive forget gate, memory cell, and exposure mechanism, LSTM based Char-RNN is able to determine to what extent the variable length input sequence or output result is retained, which is analogous to the long-term memory dependency capturing.\nGRU cell is different from the LSTM in that the memory content exposure is not presented. GRU cell consists of a reset gate, an update gate, and an aggregated output gate. Memory information inside the GRU cell is fully exposable [18]. Besides, the location of the input gate in the LSTM unit and the corresponding reset gate in the GRU cell are different. The modulation inside a GRU cell is described as follows. GRU cell consists of a reset gate rjt , an update gate zjt , and an output o j t . The activation at timestep t of the j-th GRU cell is processed as a linear transformation on the previous state ojt\u22121 and the current state o\u0303 j t :\no\u0303jt = (1 \u2212 z j t )o j t\u22121 + z j t o\u0303 j t , (9)\nwhere the update gate zjt controls the proportion of the unit being changed. The update gate is defined as\nzjt = \u03c3(Wzxt + Uzot\u22121) j . (10)\nThe candidate output gate o\u0303jt is computed as\no\u0303jt = tanh(Wxt + U(rt \u2299 ot\u22121)) j , (11)\nwhere \u2299 is an element-wise multiplication and rjt is the reset gate. The rest gate rjt requires the previous recurrent output unit ot\u22121 and the current input sequence xt for the computation.\nrjt = \u03c6(Wrxt + Urot\u22121) j . (12)\nLong-term memory dependencies inside the GRU cell is accomplished by the sophisticated update mechanism. Without the reset or forget gate apparatus, the GRU cell is not able to utilise the model capacity effectively. Detailed discussion is available at Chung et al. [18].\nOur MGU consists of a forget gate and an aggregated hidden recurrent output. The recent MGU has shed some insights on our model. Zhou et al.\u2019s MGU cell has around 33% less training hyper-parameters and has equivalent performance with GRU [20]."}, {"heading": "4. Experimental Evaluation", "text": "In this section, we present our model design, data-sets statistics, data pre-processing, and experimental settings."}, {"heading": "4.1. Model Design", "text": "We refer to Table 1 as the global training parameters configuration. The \u2018VALID\u2019 padding refers to no artificial padding, in contrast with the \u2018SAME\u2019 padding. The initial learning rate is set to 0.01. Moreover, the global training process is optimised through the Adam optimiser [25]. The learning rate decay is defined by Adam optimiser [25].\n\u03b1t = lrt\u22121 \u00d7\n\u221a\n1\u2212 \u03b2t2\n1\u2212 \u03b2 (t\u22121) 1\n, (13)\nwhere lrt\u22121 is the learning rate for previous training step. \u03b21 is the exponential decay rate for the 1st moment estimates and \u03b22 is for the 2\nnd moment estimates [25]. In Figure 2, n stands for the padded input sequence length. \u039b indicates the kernel window size. F represents the number of filters applied at the convolving layer and P denotes the pooling window. The default stride is 1. The operation on the 1-D temporal convolving layer, followed by the RELU activation operation, leads to the high level features with frame size n \u2212 \u039b + 1. A squeeze function\nis performed to get a reduced matrix formation with filter size n\u2212 \u039b. Subsequently, two separate branches of feature computation are conducted. On one hand, the temporal maxpooling layer of Char-CNN yields an output with feature size (n \u2212 \u039b)/P . In the following layer of Char-CNN, a maximum reduction operation is operated over the sentence dimension to obtain an intermediate matrix with frame size (n\u2212 \u039b)/P \u2212 1.\nOn the other hand, as the GRU/MGU/LSTM based CharRNN only recognises input snippets in a list of sequences, we thus split and squeeze the matrix retrieved from the previous layer along with the F dimension to achieve a list of corresponding sequences. A GRU/MGU/LSTM unit embedded with H as the hidden size is constructed to acquire a matrix encoding. For the aggregation operation, it takes the output from the GRU/MGU/LSTM and the result matrix from the maximum reduction operation upon the Char-CNN and modulates the two encodings accordingly. Given the probability distribution Pr over the possible labels of choice from the penultimate layer, the softmax will predict the target label. This completes a full training cycle for our architecture.\nTowards the final output, the number of frames from the softmax layer adapts to the classification data itself. For instance, if it is a 55-class problem, then the output frame is 55. We evaluated the different conditions pair (i.e., (0.9, 0.1), (0.8, 0.2) . . . (0.1, 0.9)) on the aggregation layer. We found that a large proportion assigned to the Char-CNN (i.e., 0.7) and a small distribution on the Char-RNN (i.e., 0.3) achieves the best performance. Additionally, because drop-out layer does not always appear to be effective in practice, we therefore did not include any drop-out layers in this regard [15]."}, {"heading": "4.2. Data and Pre-processing", "text": "As the major focus of this work is to develop CRNN for enhancing the topics classification, the appropriate benchmark data-sets would be natural textual streams or raw sentential data. We utilised four popular classification data streams from the Internet for our study. They are listed below:\n\u2022 Google-news is a small portion of excerpts from the online platform giant Google [26], specifically the Google news channel. \u2022 Twenty-news-groups was originated from the well-known American newspaper publishers formed as twenty-news-groups, probably first appeared at Lang\u2019s work [27]. \u2022 Brown Corpus of Standard American English, often abbreviated as the Brown Corpus [28]. The Brown Corpus encompasses with one million tokens of American English texts sampled from 15 different textual categories. The Brown Corpus is created by Francis and Kucera at Brwon University in 1960s [28]. \u2022 Question Classification was created by Li and Roth [29]. The question classification collection contains 50 classes of texts. The specific data-set characteristic is shown at Table 2 below.\nTable 2 reports the summary statistics based on the benchmarking data corpus. We addressed the specificity of data with a six-dimension schema:\n1. the number of sentences in the data-set 2. vocabulary of the stream 3. the total number of terms in a stream 4. average sentential snippet length 5. meta-data inclusion 6. the number of labels or classes of the corpus\nDuring pre-processing in prior input features feeding, we performed standard NLP techniques such as removing English stop words and striping off the newsgroup related meta-data, which includes noisy headers, footers, and quotes. The vocabulary processor for the Word-CNN and Word-RNN, LinearSVM [30], KNN-WMD [31], and plain KNN requires the normalised BOW features when constructing the corpus vectors. The normalised BOW generated a global uni-gram based dictionary mapping. With the presence of the uni-gram indexer, we could readily remove low frequency terms and lengthy snippets. As a general rule-of-thumb, we set the bottom frequency to 10 and the maximum length to 500. On the other hand, Char-RNN and Char-CNN employ a byte processor to map snippets into sequence of identities for bytes.\nWe evaluated our model and the others according to a cross-validated training/testing data split as shown in Table 3. For Google-news, the full corpus is selected. For twentynews-groups, an arbitrary 1,100 snippets were taken from the total 18,000. 5,500 out of 57,340 were selected from the Brown Corpus and 5,500 out of 5,952 sentences were picked\nout from the question classification stream. Batch size defines a small amount of sentences involved in each epoch of training. For example, the batch size for the Google-news data is 50, indicating that each training epoch contains 40 samples. Similarly, the quantity of snippets for the twentynews-groups, Brown corpus, and the question classification stream is all 20 uniformly."}, {"heading": "5. Results", "text": "In this paper, we constructed a novel CRNN model which employs prominent feature-filtering from Char-CNN and long-term sequence understanding from Char-RNN. Our framework automatically learns grammatical errors and misspelling through subword information. Our architecture also benefits from the latest development on GRU unit, which reduces algorithmic runtime without compromise of the performance.\nIn general, we showed the results independently for each benchmarking data stream. Evaluation on precision, recall, and F1 score were demonstrated for each of our data-sets. Our experiment was conducted on a cross-validated training and testing data split. The participants for our experiment are listed in Table 4. There were 20 frameworks in total for our experimental evaluation. The token random indicates the arbitrary initialisation of word vector.\nAs shown in Figure 3a, 3b, and 3c (Google-news), our model ranked fifth on precision rate, with 2.44% less than the best one. For the recall rate and F1 score, our model yielded first and third respectively. Our architecture achieved 2.62% more recall and 0.57% less F1 score compared with the best one.\nFor twenty-news-groups collection, we referred to Figure 3d, 3e, and 3f. Our algorithm achieved the optimal precision rate, recall ratio, and F1 score, leading the next best with 3.68%, 1.00%, and 2.20% respectively. Our algorithm yielded a similar precision rate and recall rate. For the F1 score, the lowest score 34.65% was from Word-RNN and random embeddings combination. Our algorithm obtained much better precision rate, recall ratio, and F1 score than Word-RNNs and Word-CNNs.\nFor Brown Corpus, observations can be derived from Figure 3g, 3h, and 3i. In general, our framework achieved the next best on precision rate, 0.54% less than the CharCNN. CRNN ranked third on recall rate, having 1.40% difference to the optimum one. Our model obtained the best F1 score, with 0.05% more than the next best. From Figure 3g, 3h, and 3i, word level neural networks was worse than our model.\nFor question classification corpus, precision rate, recall rate, and F1 score were displayed in Figure 3j, 3k, and 3l respectively. From Figure 3j, our model ranked ninth on precision rate, with 2.31% less than the best performer. For the recall ratio and F1 score, our algorithm ranked first on both, achieving 4.23% and 0.77% more than the next best algorithm.\nWe have previously mentioned about the performance comparison on our model over the competitors. We now analyse the performance difference on Word-CNNs, LinearSVMs [30], KNN-WMDs, and KNN series.\nThe average precision rate and F1 score for WordCNN+word2vec was 0.91% and 0.32% higher than WordCNN+GloVe. Based on such a small scale data-set, this kind of difference was significant. Similarly, for Word-RNNs, the mean precision and F1 score for word2vec based was 1.06% and 0.47% higher than the Glove sponsored. The precision rate, recall rate, and F1 score difference between WordCNN+GloVe and Word-CNN+random was 0.31%, 0.12%, and 0.28% respectively. For Word-RNN+random and WordRNN+GloVe, the performance difference on precision rate, recall ratio, and F1 score was 2.64%, 3.43%, and 3.10% respectively.\nFor non-neural networks, we emphasized the LinearSVMs, KNN-WMDs and traditional KNNs. Sent2vec [32] encoding schema tries to interpret the sentential information into a single skip-thought vector rather than word level embeddings. From the corresponding results in Figure 3, we observed that the performance of LinearSVM+sent2vec was the worst in its series for all four benchmarking collections. This phenomenon also applied to KNN-WMD+sent2vec and KNN+sent2vec.\nWe explain non-neural networks in the context of\nword2vec and GloVe. For LinearSVM, both produced resemblant precision, recall, and F1 score. The only exception was question classification collection where word2vec encoding yielded 8.29% more recall and 5.11% more F1 score. For the KNN-WMDs, two embeddings posed almost identical impact under each measurement excepted the precision and F1 score on twenty-news-groups. Word2vec version yielded 6.15% more precision and 4.04% more F1 score. KNNs followed the above tendency. The only exception was twenty-news-groups of which word2vec version produced 5.99%, 7.00%, and 6.65% more precision, recall, and F1 score respectively.\nWe now analyse the evaluation results for KNN-WMDs and KNNs. The former one utilised WMD as the spatial distance function, the latter one applied plain cosine distance. From Figure 3, we can perceive that WMD dominant the contest over precision, recall, and F1 score. The average precision for the former one was 42.17, the latter one was 39.27. For the average recall rate, WMD sponsored models achieved 43.39% and plain KNNs obtained 37.43%. For the mean F1 score, the former one yielded 42.30%, the latter one produced 38.02%. It is obvious that WMD was much better at measuring the spatial dissimilarity.\nReferring to Figure 3, LSTM, GRU, and MGU produced almost equivalent result across four data-sets. This inspired us to conduct an additional experiment involving runtime. As can be perceived from Figure 4, MGU had the minimal average runtime under each benchmarking test."}, {"heading": "6. Discussion", "text": "To clarify the results in Figure 3 and 4 further, we present several findings in this section.\nThe most important conclusion from our experiments was that our model offered competing performance on all four data streams. For Google-news, our model obtained\n2.62% more recall ratio than the next best. Our framework achieved 2.44% less precision rate and 0.57% less F1 score than the best one. For twenty-news-groups, our algorithm produced the best precision rate, recall rate, and F1 score, with 3.68%, 1.00%, and 2.20% more than the next best. For Brown Corpus, our model yielded 0.05% more F1 score than the next optimum and 0.54% less precision rate and 1.40% less recall than the optimal one. For question classification stream, our algorithm achieved the best recall rate and F1 score, having 4.23% and 0.77% more than the next best. For precision ratio, our model obtained 2.31% less than the best performer. We attributed this to the superiority of the joint two neural networks, Char-CNN and Char-RNN specifically. The aggregation layer in CRNN combined together two neural structures to improve redundancy detection. We also perceived that character-level neural models often yielded better accuracy than word-level ones. Character-level encoding can capture subword information, misspelling, and grammatical errors, which formed a rich syntactical knowledge base.\nBased on the evaluation on word2vec, GloVe, and randomised word vector initialisation, we can perceive that pretrained word vectors exerted a positive effect on the classification tasks. Further, as word2vec contains a vocabulary of three million words and the volume of GloVe is 0.4 million, we could expect that the possibility of out-of-vocabulary for word2vec is much lower than GloVe. From the evaluation results, word2vec sponsored models performed better than the others.\nWe emphasised the difference between sentence level vectorisation (i.e., sent2vec [32]) and word level encodings on non-neural network frameworks. We summarise that smaller embeddings unit can lead to better performance. We attributed this to the rich word-level semantics understanding. Another important finding was that the substitution of word embeddings schema posed less effect on nonneural networks than neural models. This may be due to the way sentence similarity was computed. In non-neural models, spatial distance was often determined by true spatial distance.\nWe also observed that KNN-WMD gained better results than KNN. We attributed this to the accurate computation of the target word to the pivot word distance and summation of the corresponding distance piles. When combining base algorithm with the pre-trained word embeddings like word2vec and GloVe, the performance of KNN-WMD and KNN can be further boosted.\nEfficient runtime is important to us because CRNN involves large amount of training parameters. An effective encoding cell saves computational time and boosts throughput. From Figure 4, we observed that MUG required the minimum runtime without compromising performance. This conformed to the lowest number of training parameters within MGU."}, {"heading": "7. Conclusion", "text": "In this work, we applied the latest development on the character-aware deep neural classification network, Char-\nCNN and Char-RNN specifically, for the effective categorisation of redundant snippets. We extended the classical Char-CNN structure in a singleton to incorporate with the Char-RNN framework to form an efficient redundancy detection architecture. This novel framework benefits from the advantageous salient feature-filtering from the Char-CNN and the long term memory cells from the Char-RNN. We further explored the usefulness of an aggregation layer as a penultimate gate, gluing the encoding matrix generated from the Char-CNN and Char-RNN jointly for CRNN. We perceived that this enhances the detection accuracy. Evaluation on precision, recall, and F1 score indicated the efficacy of our framework. We also assessed the effects of applying different hidden units on the benchmarking data-sets and reported their performance and runtime statistics. We utilised the different word encoding schemes to deliver an exhaustive redundancy detection experiment. In the future, we hope to extend our model to a wide variety of multiclass benchmark data for a generic redundancy detection framework and performance evaluation."}], "references": [{"title": "Detections, bounds, and timelines: Umass and tdt-3", "author": ["J. Allan", "V. Lavrenko", "D. Malin", "R. Swan"], "venue": "Proceedings of topic detection and tracking workshop, 2000, pp. 167\u2013174.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics, vol. 18, no. 4, pp. 467\u2013479, 1992.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493\u2013 2537, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Y. Zhang", "S. Roller", "B. Wallace"], "venue": "arXiv preprint arXiv:1603.00968, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Tweet2vec: Learning tweet embeddings using character-level cnn-lstm encoder-decoder", "author": ["S. Vosoughi", "P. Vijayaraghavan", "D. Roy"], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2016, pp. 1041\u20131044.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 649\u2013657.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "journal of machine learning research, vol. 3, no. Feb, pp. 1137\u20131155, 2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Multi-column deep neural network for traffic sign classification", "author": ["D. CiresAn", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks, vol. 32, pp. 333\u2013338, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn"], "venue": "2012 IEEE international conference on Acoustics, speech and signal processing (ICASSP). IEEE, 2012, pp. 4277\u20134280.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Jaskowski"], "venue": "arXiv preprint arXiv:1605.02097, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Y. Zhang", "B. Wallace"], "venue": "arXiv preprint arXiv:1510.03820, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Multichannel variable-size convolution for sentence classification", "author": ["W. Yin", "H. Schutze"], "venue": "arXiv preprint arXiv:1603.04513, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. Glehre", "K. Cho", "Y. Bengio"], "venue": "International Conference on Machine Learning, 2015, pp. 2067\u20132075.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Minimal gated unit for recurrent neural networks", "author": ["G.-B. Zhou", "J. Wu", "C.-L. Zhang", "Z.-H. Zhou"], "venue": "International Journal of Automation and Computing, vol. 13, no. 3, pp. 226\u2013234, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["S. Lai", "L. Xu", "K. Liu", "J. Zhao"], "venue": "AAAI, 2015, pp. 2267\u20132273.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Using temporal idf for efficient novelty detection in text streams", "author": ["M. Karkali", "F. Rousseau", "A. Ntoulas", "M. Vazirgiannis"], "venue": "arXiv preprint arXiv:1401.1456, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning, 1995, pp. 331\u2013339.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "The brown corpus: A standard corpus of present-day edited american english", "author": ["W.N. Francis", "H. Kucera"], "venue": "Providence, RI: Department of Linguistics, Brown University [producer and distributor], 1979.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1979}, {"title": "Learning question classifiers", "author": ["X. Li", "D. Roth"], "venue": "Proceedings of the 19th international conference on Computational linguistics- Volume 1. Association for Computational Linguistics, 2002, pp. 1\u20137.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of machine learning research, vol. 9, no. Aug, pp. 1871\u20131874, 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1871}, {"title": "From word embeddings to document distances", "author": ["M.J. Kusner", "Y. Sun", "N.I. Kolkin", "K.Q. Weinberger"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), 2015, pp. 957\u2013966.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Advances in neural information processing systems, 2015, pp. 3294\u20133302.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Research on redundancy detection began by traditional bagof-words (BOW), TFIDF frequency matrix, and n-gram language modelling [1], [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "Research on redundancy detection began by traditional bagof-words (BOW), TFIDF frequency matrix, and n-gram language modelling [1], [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "[3] and Collobert et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], CNN has shown state-of-the-art performance on computer vision and varied natural language processing (NLP) problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Whilst traditional methods are good at encoding term association relationships (stock and market), neural models tend to preserve word similarity relationships (stock and security) [5].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "We propose the utilisation of Char-CNN to sift the most salient features because of the prevalence on deep neural network and its applications to various domains like the challenging online text classification and clustering [6], [7], [8].", "startOffset": 225, "endOffset": 228}, {"referenceID": 6, "context": "We propose the utilisation of Char-CNN to sift the most salient features because of the prevalence on deep neural network and its applications to various domains like the challenging online text classification and clustering [6], [7], [8].", "startOffset": 230, "endOffset": 233}, {"referenceID": 7, "context": "We propose the utilisation of Char-CNN to sift the most salient features because of the prevalence on deep neural network and its applications to various domains like the challenging online text classification and clustering [6], [7], [8].", "startOffset": 235, "endOffset": 238}, {"referenceID": 5, "context": "These issues are likely to happen in a human derived works such as newswire streams and social media data sources [6], [7], [8].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "These issues are likely to happen in a human derived works such as newswire streams and social media data sources [6], [7], [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "These issues are likely to happen in a human derived works such as newswire streams and social media data sources [6], [7], [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "Plain CNN, however, suffers from remembering longterm dependencies over sequences, as opposed to the traditional n-gram language modelling [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "[2], the system memory takes to store trigram phrases is enormous, which demands high spatial and time complexity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "One possible solution to this is RNN [6], [10].", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "One possible solution to this is RNN [6], [10].", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "The embedded RNN hidden state and the non-linear activation function are capable of capturing long textual sequence probability distribution over iteration [6], [10].", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "The embedded RNN hidden state and the non-linear activation function are capable of capturing long textual sequence probability distribution over iteration [6], [10].", "startOffset": 161, "endOffset": 165}, {"referenceID": 10, "context": "By replacing the simple element-wise sigmoid activation function with a more complex long short-term memory (LSTM), the integrity of expressing lengthy snippets can be further improved, in line with Hochreiter and Schmidhuber [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 9, "context": "[10] proposed a gated recurrent unit (GRU) which can dynamically reserve the state information through the pipeline of input signal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3].", "startOffset": 209, "endOffset": 213}, {"referenceID": 2, "context": "The prior applications of CDNN include traffic sign classification [12], robust multi-speaker speech recognition [13], visual deep reinforcement learning from crude 3D buffer information and experience replay [14], and ImageNet classification [3].", "startOffset": 243, "endOffset": 246}, {"referenceID": 14, "context": "The emergence of CDNN has also shed some lights on the text mining society [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "Kim [16] developed a twochannel CNN, a small variation of Collobert et al.", "startOffset": 4, "endOffset": 8}, {"referenceID": 3, "context": "model [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "[7] offered an empirical investigation on the application of characterlevel convolutional networks for binary textual classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Further, Yin and Schutze [17] exhibited a multi-channel CNN for sentence classification.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "[5] gave a multiple word embeddings based CNN model for sentence categorisation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Traditionally, pure RNNs were applied to fields like statistical machine translation [10], polyphonous music modelling, speech signal understanding [18], and Python programme evaluation [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "Traditionally, pure RNNs were applied to fields like statistical machine translation [10], polyphonous music modelling, speech signal understanding [18], and Python programme evaluation [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Traditionally, pure RNNs were applied to fields like statistical machine translation [10], polyphonous music modelling, speech signal understanding [18], and Python programme evaluation [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 17, "context": "Currently, RNNs have seen promising results on text classification [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "RNN is usually powered by the recurrent units such as tanh [18].", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "Advanced recurrent units such as GRU cells [10], LSTM [11], and MGU [20] are released recently.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "Advanced recurrent units such as GRU cells [10], LSTM [11], and MGU [20] are released recently.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "Advanced recurrent units such as GRU cells [10], LSTM [11], and MGU [20] are released recently.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "MGU is probably better than GRU in that the gating mechanism in MGU not only halves the number of units when building up the model, but also reduce the size of the model by optimising the number of parameters during network interaction [18].", "startOffset": 236, "endOffset": 240}, {"referenceID": 17, "context": "However, there is no indication which approach has the best performance without further investigation [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "[21] proposed a recurrent convolutional neural network (RCNN).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21], inferred sentence structure in our model is obtained through the pre-trained word2vec embeddings [22] or GloVe [23] embeddings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[21], inferred sentence structure in our model is obtained through the pre-trained word2vec embeddings [22] or GloVe [23] embeddings.", "startOffset": 103, "endOffset": 107}, {"referenceID": 22, "context": "[21], inferred sentence structure in our model is obtained through the pre-trained word2vec embeddings [22] or GloVe [23] embeddings.", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "[8] described a hybrid network with CNN-LSTM to capture syntactic information buried in the contextual data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "shows that DCNN is less effective when the data volume is less than several million [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "[6] has multilayer structure which seems inefficient when trained with the benchmarking data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] and Hochreiter and Schmidhuber [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[10] and Hochreiter and Schmidhuber [11].", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Our CharRNN model has three different variations including GRU [10], MGU [20], and LSTM [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Our CharRNN model has three different variations including GRU [10], MGU [20], and LSTM [11].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "Our CharRNN model has three different variations including GRU [10], MGU [20], and LSTM [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "The model was developed to support the same 70 characters supported by Vosoughi et al, which are presented below [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 23, "context": "where o = m\u2212k+1 is a free constant, m depicts the output size, and RELU stands for the rectified linear unit [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "We now describe three RNN recurrent units, namely GRU [10], MGU [20], and LSTM [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "We now describe three RNN recurrent units, namely GRU [10], MGU [20], and LSTM [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "We now describe three RNN recurrent units, namely GRU [10], MGU [20], and LSTM [11].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Memory information inside the GRU cell is fully exposable [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "\u2019s MGU cell has around 33% less training hyper-parameters and has equivalent performance with GRU [20].", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "01 Training Steps 1000 Optimiser Adam [25]", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "Moreover, the global training process is optimised through the Adam optimiser [25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "The learning rate decay is defined by Adam optimiser [25].", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "\u03b21 is the exponential decay rate for the 1 moment estimates and \u03b22 is for the 2 nd moment estimates [25].", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Additionally, because drop-out layer does not always appear to be effective in practice, we therefore did not include any drop-out layers in this regard [15].", "startOffset": 153, "endOffset": 157}, {"referenceID": 25, "context": "They are listed below: \u2022 Google-news is a small portion of excerpts from the online platform giant Google [26], specifically the Google news channel.", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "\u2022 Twenty-news-groups was originated from the well-known American newspaper publishers formed as twenty-news-groups, probably first appeared at Lang\u2019s work [27].", "startOffset": 155, "endOffset": 159}, {"referenceID": 27, "context": "\u2022 Brown Corpus of Standard American English, often abbreviated as the Brown Corpus [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "The Brown Corpus is created by Francis and Kucera at Brwon University in 1960s [28].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "\u2022 Question Classification was created by Li and Roth [29].", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "The vocabulary processor for the Word-CNN and Word-RNN, LinearSVM [30], KNN-WMD [31], and plain KNN requires the normalised BOW features when constructing the corpus vectors.", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "The vocabulary processor for the Word-CNN and Word-RNN, LinearSVM [30], KNN-WMD [31], and plain KNN requires the normalised BOW features when constructing the corpus vectors.", "startOffset": 80, "endOffset": 84}, {"referenceID": 6, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 63, "endOffset": 66}, {"referenceID": 19, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 194, "endOffset": 198}, {"referenceID": 31, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 231, "endOffset": 235}, {"referenceID": 30, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 248, "endOffset": 252}, {"referenceID": 31, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 285, "endOffset": 289}, {"referenceID": 31, "context": "Numbering Base Encoding a1 CRNN GRU a2 MGU a3 LSTM a4 Char-CNN [7] \u2013 a5 Char-RNN [20] \u2013 a6 Word-CNN [15] word2vec a7 GloVe a8 random a9 Word-RNN [19] word2vec a10 GloVe a11 random a12 LinearSVM [30] word2vec a13 GloVe a14 sent2vec [32] a15 KNN-WMD [31] word2vec a16 GloVe a17 sent2vec [32] a18 KNN word2vec a19 GloVe a20 sent2vec [32]", "startOffset": 330, "endOffset": 334}, {"referenceID": 29, "context": "We now analyse the performance difference on Word-CNNs, LinearSVMs [30], KNN-WMDs, and KNN series.", "startOffset": 67, "endOffset": 71}, {"referenceID": 31, "context": "Sent2vec [32] encoding schema tries to interpret the sentential information into a single skip-thought vector rather than word level embeddings.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": ", sent2vec [32]) and word level encodings on non-neural network frameworks.", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "This paper proposes a novel framework for detecting redundancy in supervised sentence categorisation. Unlike traditional singleton neural network, our model incorporates character-aware convolutional neural network (Char-CNN) with character-aware recurrent neural network (Char-RNN) to form a convolutional recurrent neural network (CRNN). Our model benefits from Char-CNN in that only salient features are selected and fed into the integrated Char-RNN. Char-RNN effectively learns long sequence semantics via sophisticated update mechanism. We compare our framework against the state-of-the-art text classification algorithms on four popular benchmarking corpus. For instance, our model achieves competing precision rate, recall ratio, and F1 score on the Googlenews data-set. For twenty-news-groups data stream, our algorithm obtains the optimum on precision rate, recall ratio, and F1 score. For Brown Corpus, our framework obtains the best F1 score and almost equivalent precision rate and recall ratio over the top competitor. For the question classification collection, CRNN produces the optimal recall rate and F1 score and comparable precision rate. We also analyse three different RNN hidden recurrent cells\u2019 impact on performance and their runtime efficiency. We observe that MGU achieves the optimal runtime and comparable performance against GRU and LSTM. For TFIDF based algorithms, we experiment with word2vec, GloVe, and sent2vec embeddings and report their performance differences.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}