{"id": "1706.01077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "Actor-Critic for Linearly-Solvable Continuous MDP with Partially Known Dynamics", "abstract": "in many robotic manufacturing applications, essentially some aspects of shaping the system dynamics curriculum can be modeled accurately while others are considerably difficult to obtain or model. frequently we present out a novel reinforcement learning ( rl ) scoring method for continuous state and action spaces that repeatedly learns with partial knowledge of the system and without active exploration. it usually solves linearly - solvable markov decision maker processes ( l - mdps ), which are well suited for continuous state and free action spaces, based on an actor - critic architecture. compared to previous rl methods for l - mdps classes and path integral methods which are model based, but the actor - critic learning does not need a model of the uncontrolled dynamics and, importantly, transition noise levels ; however, it requires knowing the control dynamics for the problem. we evaluate our method on two alternative synthetic test scoring problems, and one real - world problem assessed in simulation and assessments using real traffic data. our experiments demonstrate improved learning and policy performance.", "histories": [["v1", "Sun, 4 Jun 2017 14:02:01 GMT  (2570kb,D)", "http://arxiv.org/abs/1706.01077v1", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tomoki nishi", "prashant doshi", "michael r james", "danil prokhorov"], "accepted": false, "id": "1706.01077"}, "pdf": {"name": "1706.01077.pdf", "metadata": {"source": "CRF", "title": "Actor-Critic for Linearly-Solvable Continuous MDP with Partially Known Dynamics", "authors": ["Tomoki Nishi", "Prashant Doshi"], "emails": ["nishi@mosk.tytlabs.co.jp", "pdoshi@cs.uga.edu", "michael.james@tri.global", "danil.prokhorov@toyota.com"], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning (RL) offers a way of learning high-quality policies (control) for an agent by exploring its environment. Methods for RL have predominantly focused on domains with discrete state and actions. Those that operate on continuous states or actions resort to sampling or other approximations because of the difficulty in analytically solving the continuous Bellman equation [20]. In this regard, Todorov [16] introduced the linearly-solvable Markov decision process (L-MDP), a subclass of general MDPs, which allows us to quickly solve the continuous Bellman equation exactly under a class of structured dynamics and rewards. Specifically, the Bellman equation in L-MDPs is recast as a linearized differential, and its solution is efficiently obtained as a linear eigenfunction when the whole dynamics model is available [18]. As such, L-MDPs are particularly well suited for modeling robotic learning and planning, where the state and actions spaces are usually continuous.\nIn addition to continuous spaces, high-impact robotic applications such as autonomous vehicles impose an additional constraint on RL. They preclude an exhaustive exploration of the state and action space because it would be unacceptable for an autonomous vehicle to optimistically try maneuvers that would lead to a crash or even a near-miss, leaving much of the state and action spaces unexplored. However, at the same time, guaranteeing safe exploration has a high computational cost that is shown to be NP-hard [9].\nL-MDPs decompose the dynamics model into passive and active (control) dynamics with added actuator noise. In this paper, we present a new method for semi model-free RL for L-MDPs,\nar X\niv :1\n70 6.\n01 07\n7v 1\n[ cs\n.A I]\nwhich uses a partially-known system dynamics model. Specifically, the method requires the control dynamics model, which represents the effect of actions to be specified, but not the passive dynamics model nor the noise in the transitions. Knowing control dynamics is feasible in our motivating context of autonomous driving because advanced driver assistance systems such as adaptive cruise control are already available in most new vehicles, and these utilize a model of the control dynamics. More importantly, knowing such a model makes safe active exploration unnecessary. Furthermore, the correct actuator noise is often difficult to prespecify, which adds to this method\u2019s appeal.\nOur method called passive actor-critic (pAC) finds the policy within the standard two-step architecture of actor-critic methods comprising a policy improvement step (actor) and state evaluation (critic). Passive actor-critic combines the data collected on passive state transitions with the known control dynamics. In the context of L-MDPs, the critic estimates the expected value function using the linearized Bellman equation from the data and the actor improves the policy using the standard Bellman equation on data and the active dynamics model. In addition to the well-known radial basis function [18, 19], multi-layer neural networks are also introduced for approximating the value function with demonstrated performance improvements. The method is evaluated on two known synthetic domains and on our motivating domain of freeway merge by an autonomous car both in simulation and using real-world traffic data. Interestingly, pAC improves on previous model-based methods despite requiring reduced model specifications. Importantly, pAC finds policies that succeed in freeway merge on real data at rates exceeding 90%, motivating transition to real-world testing as future work."}, {"heading": "2 Related Work", "text": "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21]. These efficiently optimize control policies by solving the linearized Bellman in discrete- or continuous-state L-MDPs when the system dynamics is fully known. Our method relaxes this requirement using samples of passive dynamics, while knowing the control dynamics. We also introduce multi-layer neural networks for approximating the value functions in L-MDPs in addition to the previously used radial basis functions.\nUchibe and Doya [19] formulate Z-learning based on least-square TD learning for continuous LMDPs. The method optimizes the policy while requiring knowledge of the control dynamics and transition noise. The motion noise inherent in robotic platforms is often unknown due to which this method may not apply to robot learning. pAC learns the noise levels during optimization given sampled data and knowledge of control dynamics by minimizing the error between the value and action-value functions. This makes pAC well positioned for application to robot-based RL.\nAs pAC can learn from data containing samples of passive dynamics, it bears resemblance to batch RL methods [8]. A popular and model-free batch RL method is fitted Q-iteration [1, 2], which finds policy from collected data without a model of system dynamics. It searches for actions that minimize the Q-value, which requires that either the action space be discrete or the Q-function has structure such as being quadratic due to computational cost. In contrast, pAC uses policy that is analytically derived from the estimated Z-value, parameter for transition noise, and known control dynamics.\nPath integral control also learns a policy based on linearized Bellman equation [15, 3]. Unlike approaches for L-MDPs, path integral control can directly optimize the policy. However, the approach has to sample sample many trajectories under a training policy from a certain initial state. As we mentioned previously, we seek to avoid such active and potentially unsafe explorations in the real world.\nFinally, many RL methods that use neural networks for continuous MDPs have been recently proposed [5, 14]. These methods optimize a policy with active exploration while our method seeks to find a policy from data on passive state transitions (of the underlying Markov chain) with known control dynamics. On the other hand, the utility of networks to approximate the value function remains the same."}, {"heading": "3 Preliminaries", "text": "We briefly review L-MDPs for understanding our method. We focus on a discrete-time system with a real-valued state x \u2208 Rn and control input u \u2208 Rm, whose stochastic dynamics is defined as follows:\nxk+1 = xk +A(xk)\u2206t+Buk\u2206t+ diag (\u03c3) \u2206\u03c9, (1) where \u2206\u03c9 is differential Brownian motion simulated by a Gaussian N (0, I\u2206t), where I is the identity matrix. A(xk), Buk and \u03c3 \u2208 Rn denote the passive dynamics, control dynamics due to action, and the transition noise level, respectively (B is an input-gain matrix). \u2206t is a step size of time and k denotes a time index. System dynamics structured in this way are quite general: for example, models of many mechanical systems conform to these dynamics.\nL-MDP [17] is a subclass of MDPs [13] defined by a tuple, \u3008X ,U ,P,R\u3009, where X \u2286 Rn and U \u2286 Rm are continuous state and action spaces. P := {p(y|x,u) | x,y \u2208 X ,u \u2208 U} is a state transition model due to action, which is structured as in Eq. 1, andR := {r(x,u) | x \u2208 X ,u \u2208 U} is an immediate cost function with respect to state x and action u. A control policy u = \u03c0(x) is a function that maps a state x to an action u. The goal is to find a policy that minimizes the following average expected cost: Vavg := limn\u2192\u221e 1nE [\u2211n\u22121 k=0 r(xk, \u03c0(xk)) ] .\nGrondman et al. [4] notes that the Bellman equation for MDPs can be rewritten using the value function V (x) called V-value, state-action value function Q(x,u) called Q-value, and average value Vavg under an policy.\nVavg +Qk = rk + Ep(xk+1|xk,uk)[Vk+1]. (2)\nAs we may expect, Vk = minu\u2208U Qk. Ep(xk+1|xk)[\u00b7] is expectation over a probability distribution of state transition under the passive dynamics. Here and elsewhere, subscript k is values at time step k.\nAn L-MDP defines the cost of an action (control cost) to be the amount of stochastic effect it has on the system, adding it to the state cost:\nr(xk,uk) := q(xk)\u2206t+KL(p(xk+1|xk)||p(xk+1|xk,uk)). (3)\nHere, q(x) \u2265 0 is the state-cost function; KL(\u00b7||\u00b7) is the Kullback-Leibler (KL) divergence; p(xk+1|xk) models the passive dynamics while p(xk+1|xk,uk) represents the active or control dynamics of the system. L-MDPs further add a condition on the dynamics as shown below.\np(xk+1|xk) = 0\u21d2 \u2200uk p(xk+1|xk,uk) = 0.\nThis condition ensures that no action introduces new transitions that are not achievable under passive dynamics. The stochastic dynamical system represented by Eq. 1 satisfies this assumption naturally because the dynamic is Gaussian. However, systems that are deterministic under passive dynamics remain so under active dynamics. This condition is easily met in robotic systems where noise is prevalent.\nThe standard Bellman equation for MDPs can then be recast in L-MDPs to be a linearized differential equation for exponentially transformed value function of Eq. 2 (hereafter referred to as the linearized Bellman equation) [18]:\nZavgZk = e \u2212qk\u2206t Ep(xk+1|xk)[Zk+1], (4)\nwhere Zk := e\u2212Vk and Zavg := e\u2212Vavg . Here, Zk and Zavg are an exponentially transformed value function called Z-value and the average cost under an optimal policy, respectively. Because the passive and control dynamics with the Brownian noise are Gaussian, the KL divergence between these dynamics becomes,\nKL(p(xk+1|xk)||p(xk+1|xk,uk)) = 0.5u>k S\u22121uk\u2206t, (5)\nwhere S\u22121 := B>(diag(\u03c32i )) \u22121B and \u03c3i denotes i-th element of \u03c3. Then, the optimal control policy for L-MDPs can we derived as,\n\u03c0(xk) = \u2212SB> \u2202Vk \u2202xk . (6)"}, {"heading": "4 Passive Actor-Critic for L-MDP", "text": "We present a novel actor-critic method for continuous L-MDP, which we label as passive actorcritic (pAC). While the actor-critic method usually operates using samples collected actively in the environment [6], pAC finds a converged policy without exploration. Instead, it uses samples of passive state transitions and a known control dynamics model. pAC follows the usual two-step schema of actor-critic: a state evaluation step (critic), and a policy improvement step (actor).\n1. Critic: Estimate the Z-value and the average cost from the linearized Bellman equation using samples under passive dynamics;\n2. Actor: Improve a control policy by optimizing the Bellman equation given the known control dynamics model, and the Z-value and cost from the critic.\nWe provide details on these two components below."}, {"heading": "4.1 Estimation by Critic using Linearized Bellman", "text": "The critic step of pAC estimates Z-value and the average cost by minimizing the least-square error between the true Z-value and estimated one denoted by Z\u0302.\nmin \u03bd,Z\u0302avg\n1\n2 \u222b x ( Z\u0302avgZ\u0302(x;\u03bd)\u2212ZavgZ(x) )2 dx, (7)\ns.t. \u222b x Z\u0302(x;\u03bd)dx = C, \u2200x 0 < Z\u0302(x;\u03bd) \u2264 1 Z\u0302avg ,\nwhere \u03bd is a parameter vector of the approximation and C is a constant value used to avoid convergence to the trivial solution Z\u0302(x;\u03bd) = 0 for all x. The second constraint comes from \u2200x, Z(x) := e\u2212V (x) > 0 and \u2200x, q(x) \u2265 0. The latter implies that V + Vavg > 0, and note that ZavgZ(x) := e \u2212(V+Vavg), which is less than 1.\nWe minimize the least-square error in Eq. 7, Z\u0302avgZ\u0302k \u2212 ZavgZk, with TD-learning. The latter minimizes TD error instead of the least-square error that requires the true Z(x) and Zavg , which are not available. The TD error denoted as eik for linearized Bellman equation is defined using a sample (xk,xk+1) of passive dynamics as, eik := Z\u0302 i avgZ\u0302 i k \u2212 e\u2212qk Z\u0302ik+1, where the superscript i denotes the\niteration. Z\u0302avg here is updated using the gradient as follows:\nZ\u0302i+1avg = Z\u0302 i avg \u2212 \u03b1i1\n\u2202 ( eik )2\n\u2202Z\u0302avg = Z\u0302iavg \u2212 2\u03b1i1eikZ\u0302ik, (8)\nwhere \u03b1i1 is the learning rate, which may adapt with iterations.\nIn this work, we approximate the Z-value function in two ways: (i) using a linear combination of weighted RBFs, and (ii) using a neural network (NN). When a NN with an exponentiated activation function of output layer is used, the parameters \u03bd are updated with the following gradient based on backpropagation. 1\n\u2202\n\u2202\u03bdi\n( Z\u0302avgZ\u0302 i k \u2212 ZavgZk )2 \u2248 2eikZ\u0302iavg\n\u2202Z\u0302ik \u2202\u03bdi , (9)\nwhere eik is the TD error as defined previously.\nOn the other hand, when weighted RBFs are used, Z\u0302(x;\u03bd) := \u03bd>f(x), and a Lagrangian relaxation of the objective function is useful as it includes the three constraints weighted using Lagrangian parameters \u03bb1, \u03bb2 and \u03bb3. For convenience, denote \u03bd\u0303i as,\n\u03bd\u0303i := \u03bdi \u2212 2\u03b1i2eikZ\u0302iavg \u2202Z\u0302ik \u2202\u03bdi ,\n1e\u2212tanh(x) or e\u2212softplus(x) is used as an activation function of the output layer to satisfy the constraint Z\u0302 \u2265 0. The constraint \u222b x Z\u0302(x;\u03bd)dx = C is ignored in practice because convergence to \u2200x, Z\u0302(x;\u03bd) = 0 is rare. min ([1, e\u2212qk Z\u0302ik+1]) is used instead of e \u2212qk Z\u0302ik+1 to satisfy Z\u0302 \u2264 1/Zavg in Eq. 7.\nwhere \u03b1i2 is the learning rate. The update of \u03bd cognizant of the constraints is then,\n\u03bdi+1 = \u03bd\u0303i + \u2202\n\u2202\u03bd\u0303i\n( \u03bbi1 (\u222b x \u03bd\u0303i>f(x)dx\u2212 C ) + \u03bbi>2 ( \u03bd\u0303i \u2212 0 ) + \u03bbi3 ( Z\u0302k \u2212 1/Z\u0302avg )) ,\n= \u03bd\u0303i + \u03bbi11 + \u03bb i 2 + \u03bb i 3fk, (10)\nwhere we utilize \u222b x\nf(x)dx = 1 and replace the constraint \u2200x Z\u0302(x;\u03bd) > 0 by \u03bd > 0 because the constraint on \u03bd always satisfies the former. 1 is a vector of all ones.\nLagrangian parameters \u03bb1, \u03bb2 and \u03bb3 in each iteration are obtained by ensuring that the updated parameter vector \u03bdi+1 satisfies the three constraints in Eq. 7 for xk. Formally,\nZ\u0302(xk;\u03bd i+1) = (\u03bd\u0303i + \u03bbi11 + \u03bb i 2 + \u03bb i 3fk) >fk \u2264 1/Z\u0302avg, \u03bdi+1 = \u03bd\u0303i + \u03bbi11 + \u03bb i 2 + \u03bb\ni 3fk > 0 and,\u222b\nx\nZ\u0302(x;\u03bdi+1)dx = ( \u03bd\u0303i + \u03bbi11 + \u03bb i 2 + \u03bb i 3fk )> 1 = C."}, {"heading": "4.2 Actor Improvement using Standard Bellman", "text": "The actor component improves a policy by computing S (Eq. 5) using the estimated Z-values from the critic because we do not assume knowledge of noise level \u03c3. It is estimated by minimizing the least-square error between the V-value and the state-action Q-value:\nmin S\n1\n2 \u222b x ( Q\u0302(x, u\u0302(x))\u2212 V (x) )2 dx,\nwhere V is the true V-value and Q\u0302 is the estimated Q-value under the estimated action u\u0302(x). Notice from Eq. 6 that a value for S results in a policy as B is known. Thus, we seek the S that yields the optimal policy by minimizing the error because the Q-value equals V-value iff u\u0302 is maximizing.\nAnalogously to the critic, we minimize the least-square error given above, Q\u0302ik\u2212Vk, with TD-learning. To formulate the TD error for the standard Bellman update, let xk+1 be a sample at the next time step given state xk under passive dynamics, and let x\u0303k+1 := xk+1 +Bu\u0302k\u2206t be the next state using control dynamics. Rearranging terms of the Bellman update given in Eq. 2, the TD error dik is,\ndik := r(xk, u\u0302k) + V\u0302 i(x\u0303k+1)\u2212 V\u0302avg \u2212 V\u0302 ik .\nWe may use Eqs. 3 and 5 to replace the reward function,\nr(xk, u\u0302k) = (qk + 0.5u\u0302 > k (S\u0302 i)\u22121u\u0302k)\u2206t = qk\u2206t+ 0.5 \u2202V\u0302 ik \u2202xk\n>\nBS\u0302iB> \u2202V\u0302 ik \u2202xk \u2206t.\nThe last step is obtained by noting that u\u0302ik = \u2212S\u0302iB> \u2202V\u0302 ik \u2202xk\n, where S\u0302 denotes estimated S in Eq. 6. The estimated V-value and its derivative is calculated by utilizing the approximate Z-value function from the critic. S\u0302 is updated based on standard stochastic gradient descent using the TD error,\nS\u0302i+1 = S\u0302i \u2212 \u03b2i \u2202 \u2202S\u0302i\n( Q\u0302ik \u2212 Vk )2 \u2248 S\u0302i \u2212 2\u03b2idik\n\u2202dik \u2202S\u0302i ,\nwhere \u03b2 is the learning rate. The actor mitigates the impact of error from estimated Z-value by minimizing the approximated least-square error between V- and Q-values under the learned policy."}, {"heading": "4.3 Algorithm", "text": "We show a pseudo code of pAC in Algorithm 1. Z(x) and Zavg are estimated in the critic with samples, and S is done in the critic with samples, estimated Z\u0302(x) and Z\u0302avg . In the critic, feedback from the actor is not needed (unlike actor critic methods for MDPs) because the Z-value is approximated with samples from passive dynamics only. We emphasize that the actor and critic steps do not use the functions A and \u03c3 but does indeed rely on B, of Eq. 1. As such, the updates use a sample (xk,xk+1) of the passive dynamics, and the state cost qk. Consequently, pAC achieves semi model-free learning for L-MDPs.\nAlgorithm 1 passive Actor Critic 1: Initialize parameters Z\u03020avg,\u03bd0, S\u03020, \u03b101, \u03b20"}, {"heading": "5 Numerical Experiments", "text": "We evaluate pAC for L-MDPs on two synthetic domains, Car-on-a-Hill and Pendulum, also used previously on L-MDPs by Todorov [18]; and on our motivating domain of autonomous merging in a congested freeway."}, {"heading": "5.1 Problem settings", "text": "Car-on-a-Hill Car-on-a-Hill has a two-dimensional state space, x = [xp, xv]> where xp and xv denote position and velocity, respectively, and a one-dimensional action space. Table 1 shows the dynamics in detail. The state cost is given by,\nq(x) =4.0 ( exp ( \u22120.5(xp \u2212 1)2 \u2212 (xv + 1)2 ) + exp ( \u22120.5(xp + 1)2 \u2212 (xv \u2212 1)2 ) \u2212 2 ) .\nInitial states are randomly set in \u22122\u03c0 \u2264 xp \u2264 2\u03c0 and \u2212\u03c0 \u2264 xv \u2264 \u03c0.\nPendulum The Pendulum problem also has a two-dimensional state space similar to the Car-on-aHill and a one-dimensional action space. Table 1 shows the dynamics in detail. State cost is given by\nq(x) = 4.0 ( exp ( \u2212(xv \u2212 3)2 ) + exp ( \u2212(xv + 3)2 ) \u2212 2) ) .\nInitial states are randomly set in \u22122\u03c0 \u2264 xp \u2264 2\u03c0 and \u2212\u03c0 \u2264 xv \u2264 \u03c0.\nSimulated freeway merging This new contemporary domain simulates freeway merges by an automated vehicle. We refer the reader to Fig. 1 for establishing the four-dimensional state space. Here, x = [dx12, dv12, dx02, dv02]> where dxij and dvij denote the horizontal signed distance and relative velocity between cars i and j \u2208 [0, 1, 2]. The action space is one-dimensional (acceleration). Table 1 shows the dynamics in detail. The dynamics presume that the leading vehicle is driven with a constant speed v2 = 30[m/sec], and the following vehicle is driven by a known car-following model [12]. The acceleration of the vehicle a0(x) is calculated with \u2212\u03b1v\u03b22 dv02/(\u2212dx) \u03b3 02, where if the following vehicle is slower than the leading vehicle (dv02 < 0), \u03b1 = 1.55, \u03b2 = 1.08, \u03b3 = 1.65, otherwise \u03b1 = 2.15, \u03b2 = \u22121.65, \u03b3 = \u22120.89. The assumptions are used to simulate maneuvers of ambient vehicles in only the simulated freeway merge domain.\nThe state cost designed to motivate Car-1 to merge midway between Cars 0 and 2 with the same velocity as Car-0, is:\nq(x) = k1 \u2212 k1 exp ( \u2212k2 ( 1\u2212 2dx12\ndx02\n)2 \u2212 k3dv210 )\nwhere k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise). Initial states are randomly picked in \u2212100 < dx12 < 100 [m], \u221210 < dv12 < 10 [m/sec], \u2212100 < dx02 < \u22125 [m], and \u221210 < dx02 < 10 [m/sec]."}, {"heading": "5.2 Performance Evaluation", "text": "We compared pAC with two other methods: model-based learning based on quadratic programming (QP) and Z-learning. QP requires all system dynamics and approximates the Z-value with quadratic programming [17]. Z-learning assumes that B and \u03c3 are available to approximate Z-value using the critic. QP and Z-learning calculate the policy with Eq. 6. We are unaware of any fully model-free method for L-MDPs. model-free PI control (e.g. [15]) assume action cost is available and it is equivalent to the assumption of the known transition noise level in L-MDPs. Table 2 gives the prior knowledge requirement on components of the system dynamics in Eq. 1.\nGaussian RBFs are used to approximate the Z-value function in all methods and NNs are additionally used in Z-learning and pAC. RBFs were spaced uniformly in the range of sampled data: 400 RBFs were used for Car-on-a-Hill and Pendulum domains, and 4,096 RBFs were used for the Merging task. The standard deviations of the bases were 0.7 of the distance between the closest two bases in each dimension.\nA three-hidden layer perceptron with 200, 200 and 50 units in first, second and third hidden layers is used as the NN. The number of nodes in the input is same as the dimensions of state space and one output, respectively. The rectified linear function [10] is used as the hidden layers\u2019 activation function. The activation function in the output layer for Car-on-a-Hill is exp(\u2212tanh(x)) and exp(\u2212softplus(x)) for other domains. We estimated S as constant in pAC. Inputs of the perceptron were normalized to the range [0, 1].\nIn Fig. 2, we compared average cost calculated over periods of 10 seconds for Car-on-a-Hill and Pendulum, and 30 seconds for Merging, under learned policies in each domain. Observe that pAC finds a similar or better policy in comparison to other learning methods in all domains. QP could not learn any reasonable policy in Merging domains because of an issue referred to in [18]: QP might not converge to a principal Eigen pair, instead of converging to a 2nd or higher-order pair.\nThis improvement over Z-learning may come as a surprise because Z-learning makes greater use of model knowledge \u2013 it calculates S using the true B and \u03c3. Nevertheless, the presence of the additional actor step in pAC makes the difference. The actor additionally minimizes the error in Q-value to obtain S. This error is not minimized if S is obtained as in Eq. 5 due to the approximation error of Z-value. Subsequently, pAC estimates S differently, in a more targeted way to obtain a better policy.\nWe evaluated the rate of merging successfully in a time limit of 30 seconds starting from 125 different states in Freeway-Merge. We defined success as being between the leading and following vehicles after 30 seconds. Figure 3 (a) shows the success rate: pAC with RBFs and NN achieved 93% and 97% success rate respectively, which is comparable to Z-learning. All results shows pAC is comparable or better than Z-learning despite less prior knowledge instead of using actor step."}, {"heading": "5.3 Experiment on real-world traffic", "text": "The NGSIM data set contains vehicle trajectory data recorded by cameras mounted on top of a building for 45 minutes around the evening rush hour[11]. Vehicle trajectories were extracted using a vehicle tracking method from collected videos [7]. We extracted three-vehicle systems (Fig. 1) representing 637 freeway merge events.\nWe compared pAC and Z-learning based on RBFs and NN on the extracted data from NGSIM. Z-learning calculated a policy with transition noise \u03c3 estimated with a Gaussian process due to unknown true dynamics. We used the same state variables, action variable and reward function as used in the simulated Freeway-Merge domain. We calculated next states xk+1 under passive dynamics by subtracting state change caused by actions:xk+1 = xDk+1 \u2212B>uDk , where xDk+1 and uDk are the next state and the action recorded in the data set respectively. We resampled data to mitigate any imbalance and sparseness by sampling randomly a state and choosing a nearest data from the state using the approximate nearest neighbor method.\nSuccess was defined as being between Car-0 and Car-2 at the merging point, for those instances in the data set where Car-1 on entry ramp completed its merge. Trajectories of Car-0 and Car-2 were played back from recorded logs, and trajectories of Car-1 were simulated with the control dynamics and learned policies. The dataset was randomly partitioned into five sub-datasets where an almost equal number of trajectories were included. Four sub-dataset were used as training data and a remaining sub-dataset were used as the test data for testing the policy. Each method was evaluated five times for each different test data. Figure 3(b) shows the success rate for each method. Firstly, we note the significant performance improvement when NN is used as the function approximator. Both pAC and Z-learning with RBFs simply do not perform well. Specifically, pAC with NN achieved a 93% success rate significantly outperforming Z-learning with NN. The result shows that the actor, which\nminimizes the error of Q-value, improves performance more than the noise level estimation from the dataset. The model-free RL approaches cannot be applied here due to the need for active exploration."}, {"heading": "6 Concluding Remarks", "text": "We presented a novel method for semi model-free RL for L-MDPs \u2013 an important subclass of MDPs. The passive actor-critic optimizes a policy without active exploration, instead of using samples of the passive dynamics and knowledge of the control dynamics. This is a first formulation of the actor-critic schema in the context of L-MDPs. We evaluated the method using three domains. Results show that pAC achieves comparable or better performances than benchmarked methods despite less prior knowledge requirements. As such, pAC represents a significant step toward more efficient RL in continuous domains.\nData under passive dynamics and accurate models of control dynamics are needed for pAC. This may seem to limit applicability apparently but, as mentioned, it is well suited for contemporary robotic applications such as automated driving in real-world traffic. In this case, passive and control dynamics correspond to models of ambient and autonomous vehicles, respectively. pAC obtains a better policy with the car\u2019s own dynamics model, which is now commonly available, and uses data collected on maneuvers of the ambient vehicles whose models are usually not known. An evaluation of the learned policies toward freeway merging using a real traffic data set illustrates its usefulness for practical applications. We are interested in exploring additional challenges, e.g., entering roundabouts."}], "references": [{"title": "Fitted q-iteration in continuous action-space mdps. In Advances in neural information processing systems", "author": ["Andr\u00e1s Antos", "Csaba Szepesv\u00e1ri", "R\u00e9mi Munos"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Fitted q-iteration by functional networks for control problems", "author": ["Matteo Gaeta", "Vincenzo Loia", "Sergio Miranda", "Stefania Tomasiello"], "venue": "Applied Mathematical Modelling,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Policy search for path integral control", "author": ["Vicen\u00e7 G\u00f3mez", "Hilbert J Kappen", "Jan Peters", "Gerhard Neumann"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A survey of actorcritic reinforcement learning: Standard and natural policy gradients", "author": ["Ivo Grondman", "Lucian Busoniu", "Gabriel AD Lopes", "Robert Babuska"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Nicolas Heess", "Gregory Wayne", "David Silver", "Tim Lillicrap", "Tom Erez", "Yuval Tassa"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Actor-critic algorithms. In Advances in neural information processing systems", "author": ["Vijay R Konda", "John N Tsitsiklis"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Video-based vehicle trajectory data collection", "author": ["Vijay Gopal Kovvali", "Vassili Alexiadis", "PE Zhang"], "venue": "In Transportation Research Board 86th Annual Meeting,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Batch reinforcement learning", "author": ["Sascha Lange", "Thomas Gabel", "Martin Riedmiller"], "venue": "In Reinforcement learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Safe exploration in markov decision processes", "author": ["Teodor M Moldovan", "Pieter Abbeel"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Comparison of car-following models", "author": ["Johan Janson Olstam", "Andreas Tapani"], "venue": "Swedish National Road and Transport Research Institute, Project VTI meddelande,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Markov Decision Processes", "author": ["Martin L. Puterman"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A Generalized Path Integral Control Approach to Reinforcement Learning", "author": ["E. Theodorou", "Jonas Buchli", "Stefan Schaal"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Linearly-solvable markov decision problems", "author": ["Emanuel Todorov"], "venue": "In Advances in neural information processing systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Efficient computation of optimal actions", "author": ["Emanuel Todorov"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Eigenfunction approximation methods for linearly-solvable optimal control problems", "author": ["Emanuel Todorov"], "venue": "In Adaptive Dynamic Programming and Reinforcement Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Combining learned controllers to achieve new goals based on linearly solvable mdps", "author": ["Eiji Uchibe", "Kenji Doya"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Aggregation methods for lineary-solvable markov decision process", "author": ["Mingyuan Zhong", "Emanuel Todorov"], "venue": "In Proceedings of the World Congress of the International Federation of Automatic Control,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "In this regard, Todorov [16] introduced the linearly-solvable Markov decision process (L-MDP), a subclass of general MDPs, which allows us to quickly solve the continuous Bellman equation exactly under a class of structured dynamics and rewards.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "Specifically, the Bellman equation in L-MDPs is recast as a linearized differential, and its solution is efficiently obtained as a linear eigenfunction when the whole dynamics model is available [18].", "startOffset": 195, "endOffset": 199}, {"referenceID": 8, "context": "However, at the same time, guaranteeing safe exploration has a high computational cost that is shown to be NP-hard [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 16, "context": "In addition to the well-known radial basis function [18, 19], multi-layer neural networks are also introduced for approximating the value function with demonstrated performance improvements.", "startOffset": 52, "endOffset": 60}, {"referenceID": 17, "context": "In addition to the well-known radial basis function [18, 19], multi-layer neural networks are also introduced for approximating the value function with demonstrated performance improvements.", "startOffset": 52, "endOffset": 60}, {"referenceID": 15, "context": "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 16, "context": "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 18, "context": "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 17, "context": "Uchibe and Doya [19] formulate Z-learning based on least-square TD learning for continuous LMDPs.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "As pAC can learn from data containing samples of passive dynamics, it bears resemblance to batch RL methods [8].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "A popular and model-free batch RL method is fitted Q-iteration [1, 2], which finds policy from collected data without a model of system dynamics.", "startOffset": 63, "endOffset": 69}, {"referenceID": 1, "context": "A popular and model-free batch RL method is fitted Q-iteration [1, 2], which finds policy from collected data without a model of system dynamics.", "startOffset": 63, "endOffset": 69}, {"referenceID": 13, "context": "Path integral control also learns a policy based on linearized Bellman equation [15, 3].", "startOffset": 80, "endOffset": 87}, {"referenceID": 2, "context": "Path integral control also learns a policy based on linearized Bellman equation [15, 3].", "startOffset": 80, "endOffset": 87}, {"referenceID": 4, "context": "Finally, many RL methods that use neural networks for continuous MDPs have been recently proposed [5, 14].", "startOffset": 98, "endOffset": 105}, {"referenceID": 12, "context": "Finally, many RL methods that use neural networks for continuous MDPs have been recently proposed [5, 14].", "startOffset": 98, "endOffset": 105}, {"referenceID": 15, "context": "L-MDP [17] is a subclass of MDPs [13] defined by a tuple, \u3008X ,U ,P,R\u3009, where X \u2286 R and U \u2286 R are continuous state and action spaces.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "L-MDP [17] is a subclass of MDPs [13] defined by a tuple, \u3008X ,U ,P,R\u3009, where X \u2286 R and U \u2286 R are continuous state and action spaces.", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "[4] notes that the Bellman equation for MDPs can be rewritten using the value function V (x) called V-value, state-action value function Q(x,u) called Q-value, and average value Vavg under an policy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "2 (hereafter referred to as the linearized Bellman equation) [18]: ZavgZk = e \u2212qk\u2206t Ep(xk+1|xk)[Zk+1], (4)", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "While the actor-critic method usually operates using samples collected actively in the environment [6], pAC finds a converged policy without exploration.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "B [ 0, 1 ]> [ 0, 1 ]> [ 0.", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": "B [ 0, 1 ]> [ 0, 1 ]> [ 0.", "startOffset": 12, "endOffset": 20}, {"referenceID": 0, "context": "\u03c3 [ 0, 1 ]> [ 0, 2 ]> [ 0, 2.", "startOffset": 2, "endOffset": 10}, {"referenceID": 1, "context": "\u03c3 [ 0, 1 ]> [ 0, 2 ]> [ 0, 2.", "startOffset": 12, "endOffset": 20}, {"referenceID": 16, "context": "We evaluate pAC for L-MDPs on two synthetic domains, Car-on-a-Hill and Pendulum, also used previously on L-MDPs by Todorov [18]; and on our motivating domain of autonomous merging in a congested freeway.", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "Here, x = [dx12, dv12, dx02, dv02] where dxij and dvij denote the horizontal signed distance and relative velocity between cars i and j \u2208 [0, 1, 2].", "startOffset": 138, "endOffset": 147}, {"referenceID": 1, "context": "Here, x = [dx12, dv12, dx02, dv02] where dxij and dvij denote the horizontal signed distance and relative velocity between cars i and j \u2208 [0, 1, 2].", "startOffset": 138, "endOffset": 147}, {"referenceID": 10, "context": "The dynamics presume that the leading vehicle is driven with a constant speed v2 = 30[m/sec], and the following vehicle is driven by a known car-following model [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 67, "endOffset": 78}, {"referenceID": 9, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 67, "endOffset": 78}, {"referenceID": 9, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 67, "endOffset": 78}, {"referenceID": 9, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 99, "endOffset": 110}, {"referenceID": 9, "context": "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).", "startOffset": 99, "endOffset": 110}, {"referenceID": 15, "context": "QP requires all system dynamics and approximates the Z-value with quadratic programming [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "[15]) assume action cost is available and it is equivalent to the assumption of the known transition noise level in L-MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The rectified linear function [10] is used as the hidden layers\u2019 activation function.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "Inputs of the perceptron were normalized to the range [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 16, "context": "QP could not learn any reasonable policy in Merging domains because of an issue referred to in [18]: QP might not converge to a principal Eigen pair, instead of converging to a 2nd or higher-order pair.", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "Vehicle trajectories were extracted using a vehicle tracking method from collected videos [7].", "startOffset": 90, "endOffset": 93}], "year": 2017, "abstractText": "In many robotic applications, some aspects of the system dynamics can be modeled accurately while others are difficult to obtain or model. We present a novel reinforcement learning (RL) method for continuous state and action spaces that learns with partial knowledge of the system and without active exploration. It solves linearly-solvable Markov decision processes (L-MDPs), which are well suited for continuous state and action spaces, based on an actor-critic architecture. Compared to previous RL methods for L-MDPs and path integral methods which are model based, the actor-critic learning does not need a model of the uncontrolled dynamics and, importantly, transition noise levels; however, it requires knowing the control dynamics for the problem. We evaluate our method on two synthetic test problems, and one real-world problem in simulation and using real traffic data. Our experiments demonstrate improved learning and policy performance.", "creator": "LaTeX with hyperref package"}}}