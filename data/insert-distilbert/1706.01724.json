{"id": "1706.01724", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC", "abstract": "it is challenging to develop stochastic gradient based scalable approximate inference for a deep discrete latent open variable models ( lvms ), due to the difficulties in not only computing the gradients, but also adapting altering the step sizes to different optimal latent factors and hidden layers. coding for the poisson gamma belief network ( yd pgbn ), a recently proposed deep discrete lvm, we derive an intermediate alternative representation that and is referred to as deep latent dirichlet allocation ( ~ dlda ). exploiting data augmentation and marginalization techniques, we derive a block - diagonal fisher information matrix and use its natural inverse measures for the simplex - constrained global model parameters of dlda. exploiting that fisher information matrix with stochastic transformation gradient mcmc, wherein we present two topic - layer - adaptive stochastic gradient riemannian ( tlasgr ) mcmc that jointly learns weighted simplex - constrained global mode parameters across all variable layers and topics, with topic and layer specific learning rates. state - of - the - art results are demonstrated on embedded big data sets.", "histories": [["v1", "Tue, 6 Jun 2017 12:15:42 GMT  (1176kb,D)", "http://arxiv.org/abs/1706.01724v1", "Appearing in ICML 2017"]], "COMMENTS": "Appearing in ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO", "authors": ["yulai cong", "bo chen", "hongwei liu", "mingyuan zhou"], "accepted": true, "id": "1706.01724"}, "pdf": {"name": "1706.01724.pdf", "metadata": {"source": "CRF", "title": "Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC", "authors": ["Yulai Cong", "Bo Chen", "Hongwei Liu", "Mingyuan Zhou"], "emails": ["<bchen@mail.xidian.edu.cn>,", "<mingyuan.zhou@mccombs.utexas.edu>."], "sections": [{"heading": null, "text": "1. Introduction The increasing amount and complexity of data call for largecapacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo (SG-MCMC) that provides posterior samples in a non-batch learning setting (Welling & Teh, 2011; Patterson & Teh, 2013; Ma et al., 2015). Unfortunately, most deep LVMs,\n1National Laboratory of Radar Signal Processing, Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi\u2019an, China. 2McCombs School of Business, The University of Texas at Austin, Austin, TX 78712, USA. Correspondence to: Bo Chen <bchen@mail.xidian.edu.cn>, Mingyuan Zhou <mingyuan.zhou@mccombs.utexas.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nsuch as deep belief network (DBN) (Hinton et al., 2006) and deep Boltzmann machines (DBM) (Salakhutdinov & Hinton, 2009), use greedy layerwise training, without a principled way to jointly learn multilayers in an unsupervised manner (Bengio et al., 2007). While SG-MCMC has recently been successfully applied to several \u201cshallow\u201d LVMs, such as mixture models (Welling & Teh, 2011) and mixedmembership models (Patterson & Teh, 2013), it has been rarely applied to \u201cdeep\u201d ones, probably due to the lack of understanding on how to jointly learn the latent variables of different layers and adjust the layer and topic specific learning rates in a non-batch learning setting.\nTo investigate scalable SG-MCMC inference for deep LVMs, we focus our study on the recently proposed Poisson gamma belief network (PGBN), whose hidden layers are parameterized with gamma distributed hidden units and connected with Dirichlet distributed basis vectors (Zhou et al., 2016a). The PGBN is capable of extracting topics from a text corpus at multiple layers and outperforms a large number of topic modeling algorithms. However, the PGBN is currently trained with a batch Gibbs sampler that is not scalable to big data. In this paper, we focus on developing scalable multilayer joint inference for the PGBN.\nWe will show that scalable multilayer joint inference of the PGBN could be facilitated by its Fisher information matrix (FIM) (Amari, 1998; Girolami & Calderhead, 2011; Pascanu & Bengio, 2013), which, although seemingly impossible to derive and challenging to work with due to the need to compute the expectations over trigamma functions, is readily available under an alternative representation of the PGBN, referred to as deep latent Dirichlet allocation (DLDA). DLDA, derived by exploiting data augmentation and marginalization techniques on the PGBN, can be considered as a multilayer generalization of latent Dirichlet allocation (LDA) (Blei et al., 2003). Following a general framework for SG-MCMC (Ma et al., 2015), the block diagonal structure of the FIM of DLDA makes it be easily inverted to precondition the mini-batch based noisy gradients to exploit the second-order local curvature information, leading to topic-layer-adaptive step sizes based on the Riemannian manifold and the same asymptotic performance as a natural gradient based batch-learning algorithm (Amari, 1998; Pascanu & Bengio, 2013). To the best of our knowlar X iv :1\n70 6.\n01 72\n4v 1\n[ st\nat .M\nL ]\n6 J\nun 2\n01 7\nedge, this is the first time that the FIM of a deep LVM is shown to have an analytical and practical form. How we derive the FIM for the PGBN using data augmentation and marginalization techniques in this paper may serve as an example to help derive the FIMs for other deep LVMs.\nBesides presenting the analytical FIM of the PGBN, important for the marriage of a deep LVM and SG-MCMC, we make another contribution in showing how to facilitate SG-MCMC for an LVM equipped with simplex-constrained model parameters \u03c6k = (\u03c61k, . . . , \u03c6V k)\nT , which means\u2211V v=1 \u03c6vk = 1 and \u03c6vk \u2208 R+, where R+ := {x, x \u2265 0}, by using a reduced-mean simplex parameterization together with a fast sampling procedure recently introduced in Cong et al. (2017). Unlike other simplex parameterizations, the reduced-mean one does not make heuristic pseudolikelihood assumptions. Though it has previously been deemed unsound, it is successfully integrated into our SG-MCMC framework to deliver state-of-the-art results. Exploiting the analytical FIM of DLDA and novel inference for simplexconstrained parameters under a general SG-MCMC framework (Ma et al., 2015), we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC for DLDA, which automatically adjusts the learning rates of global model parameters across all layers and topics, without the need to set the same learning rate for all that is commonly used in practice due to the difficulty in identifying an appropriate combination of the learning rates for different layers and topics.\n2. PGBN and SG-MCMC The generative model of the Poisson gamma belief network (PGBN) (Zhou et al., 2016a) with L hidden layers, from top to bottom, is expressed as\n\u03b8 (L) j \u223c Gam\n( r, 1/c\n(L+1) j\n) ,\n\u00b7 \u00b7 \u00b7 \u03b8 (l) j \u223c Gam ( \u03a6(l+1)\u03b8 (l+1) j , 1/c (l+1) j ) ,\n\u00b7 \u00b7 \u00b7 x\n(1) j \u223c Pois\n( \u03a6(1)\u03b8\n(1) j\n) , \u03b8\n(1) j \u223c Gam\n( \u03a6(2)\u03b8\n(2) j ,\np (2) j\n1\u2212p(2)j\n) ,\n(1)\nwhere the jth observed or latent V -dimensional count vectorsx(1)j \u2208 ZV , where Z := {0, 1, . . .}, are factorized under the Poisson (Pois) likelihood; the hidden units \u03b8(l)j \u2208 RKl+ of layer l are factorized under the gamma (Gam) likelihood into the product of the basis vector matrix \u03a6(l) =( \u03c6\n(l) 1 , . . . ,\u03c6 (l) Kl ) \u2208 RKl\u22121\u00d7Kl+ and the hidden units of the\nnext layer, where\u03c6(l)k \u223c Dir ( \u03b7(l)1Kl\u22121 ) are Dirichlet (Dir) distributed and 1Kl\u22121 is a Kl\u22121-dimensional vector of all ones; the gamma shape parameters r = (r1, \u00b7 \u00b7 \u00b7 , rKL)T at the top layer are shared across all j; {1/c(l)j }3,L+1 are gamma scale parameters, where c(l)j \u223c Gam(e0, 1/f0), and c (2) j := ( 1 \u2212 p(2)j ) /p (2) j , where p (2) j \u223c Beta(a0, b0) are\nintroduced to help reduce the dependencies between \u03b8(1)jk and c(2)j . The PGBN in (1) can be further extended under the Bernoulli-Poisson link as b(1)j = 1 ( x (1) j > 0 ) , and under the Poisson randomized gamma link as y(1)j \u223c Gam ( x (1) j , 1/aj ) , where aj \u223c Gam(e0, 1/f0).\nThe PGBN infers a multilayer deep representation of the data, whose inferred basis vectors \u03c6(l)k at hidden layer l can be directly visualized as [\u220fl\u22121 t=1 \u03a6 (t) ] \u03c6 (l) k , which are their projections into the V -dimensional probability simplex. The information of the whole data set is compressed by the PGBN into the inferred sparse network {\u03a6(1), . . . ,\u03a6(L)}, where \u03c6(l)k1k2 indicates the connection strength between node (basis vector) k1 of layer l \u2212 1 and node k2 of layer l. Moreover, the network structure can be inferred from the data by combining the gamma-negative binomial process of Zhou & Carin (2015) with a greedy layer-wise training strategy. Extensive experiments in Zhou et al. (2016a) show that the PGBN can extract basis vectors that are very specific/abstract in the bottom layer and become increasingly more general when moving upwards from the bottom to top hidden layers, and the K1 hidden units \u03b8 (1) j in the first hidden layer, which are unsupervisedly extracted and regularized with the deep network, are well suited for out-of-sample prediction and being used as features for classification.\nDespite all these attractive model properties, the current inference of the PGBN relies on an upward-downward Gibbs sampler that requires processing all data in each iteration and hence often does not scale well to big data unless with parallel computing. To make its inference scalable to allow processing a large amount of data sufficiently fast on a regular personal computer, we resort to SG-MCMC that subsamples the data and utilizes stochastic gradients in each MCMC iteration to generate posterior samples for globally shared model parameters. Let us denote the posterior of model parameters z given the data X = {xj}1,J as p (z |X ) \u221d e\u2212H(z), with potential function H (z) = \u2212 ln p (z) \u2212\u2211j ln p (xj |z ). As in Theorem 1 of Ma et al. (2015), p (z |X ) is the stationary distribution of the dynamics defined by the stochastic differential equation (SDE) dz = f (z) dt + \u221a 2D (z)dW (t), if the deterministic drift f (z) is restricted to the form\nf (z) = \u2212 [D (z) + Q (z)]\u2207H (z) + \u0393 (z) , (2) \u0393i (z) = \u2211 j \u2202 \u2202zj [Dij (z) + Qij (z)] , (3)\nwhere D (z) is a positive semidefinite diffusion matrix, W(t) is a Wiener process, Q (z) is a skew-symmetric curl matrix, and \u0393i(z) is the ith element of the compensation vector \u0393(z). Thus one has a mini-batch update rule as\nzt+1 =zt + \u03b5t { \u2212 [ D(zt)+Q(zt) ] \u2207H\u0303(zt)+\u0393(zt) }\n+N ( 0, \u03b5t [ 2D (zt)\u2212 \u03b5tB\u0302t ]) , (4)\nwhere \u03b5t denotes step sizes, H\u0303 (z) = \u2212 ln p (z) \u2212 \u03c1 \u2211 x\u2208X\u0303 ln p (x |z ), X\u0303 the mini-batch, \u03c1 the ratio of the dataset size |X| to the mini-batch size |X\u0303|, and B\u0302t an estimate of the stochastic gradient noise variance satisfying a positive definite constraint as 2D (zt)\u2212 \u03b5tB\u0302t 0. As shown in Ma et al. (2015), stochastic gradient Riemannian Langevin dynamics (SGRLD) of Patterson & Teh (2013) is a special case with D (z) = G(z)\u22121,Q (z) = 0, B\u0302t = 0, where G (z) denotes the Fisher information matrix (FIM). SGRLD is designed to solve the inference on the probability simplex, where four different parameterizations of the simplex-constrained basis vectors are discussed, including reduced-mean, expanded-mean, reduced-natural, and expanded-natural. Here, we consider both expandedmean, previously shown to provide the best overall results, and reduced-mean, which, although discarded in Patterson & Teh (2013) due to its unstable gradients, is used in this paper to produce state-of-the-art results.\nLet us denote \u03c6k \u2208 RV+ as a vector on the probability simplex, \u03c6\u0302k \u2208 RV+ as a nonnegative vector, and \u03d5k \u2208 RV\u22121+ as a nonnegative vector constrained with \u03d5\u00b7k := \u2211V\u22121 v=1 \u03d5vk \u2264 1. For convenience, the symbol \u201c\u00b7\u201d will denote the operation of summing over the corresponding index. We use ( \u03c6\u03021k, \u00b7 \u00b7 \u00b7 , \u03c6\u0302V k )T/ \u2211 v \u03c6\u0302vk as an expanded-mean parameterization of \u03c6k and( \u03d51k, \u00b7 \u00b7 \u00b7 , \u03d5(V\u22121)k, 1\u2212 \u2211 v<V \u03d5vk )T as a reduced-mean parametrization of \u03c6k. SGRLD focuses on a single-layer model with a multinomial likelihood nk \u223c Mult (n\u00b7k,\u03c6k) and a Dirichlet distributed prior \u03c6k \u223c Dir (\u03b71V ). For inference, it adopts the expanded-mean parameterization of \u03c6k and makes a heuristic assumption that n\u00b7k \u223c Pois ( \u03c6\u0302\u00b7k ) . While that heuristic pseudolikelihood assumption of SGRLD is neither supported by the original generative model nor rigorously justified in theory, it converts a Dirichlet-multinomial model into a gamma-Poisson one, allowing a simple sampling equation for \u03c6\u0302k as\n( \u03c6\u0302k ) t+1 = \u2223\u2223\u2223 ( \u03c6\u0302k ) t +\u03b5t [ (nk+\u03b7)\u2212 ( n\u00b7k+\u03c6\u0302\u00b7k ) (\u03c6k)t ]\n+N ( 0, 2\u03b5tdiag [( \u03c6\u0302k ) t ])\u2223\u2223\u2223 , (5)\nwhere the absolute operation |\u00b7| is used to ensure positivevalued \u03c6\u0302k. Below we show how to eliminate that heuristic assumption by parameterizing \u03c6k with reduced-mean, and develop efficient SG-MCMC for the PGBN, which reduces to LDA when the number of hidden layers is one.\n3. Deep Latent Dirichlet Allocation While the original construction of PGBN in (1) makes it seemingly impossible to compute the FIM, as shown in Appendix A, we find that, by exploiting data augmentation and marginalization techniques, the PGBN generative model\ncan be rewritten under an alternative representation that marginalizes out all the gamma distributed hidden units, as shown in the following Lemma, where Log(\u00b7) denotes the logarithmic distribution (Johnson et al., 1997), m \u223c SumLog(x, p) represents the sum-logarithmic distribution generated with m = \u2211x i=1 ui, ui \u223c Log(p) (Zhou et al., 2016b). The proof is deferred to the Appendix. Lemma 3.1. Denote q(l+1)j = ln ( 1 + q (l) j /c (l+1) j ) for l = 1, . . . , L, where q(1)j := 1, which means q (l+1) j =\nln ( 1 + 1\nc (l+1) j\nln { 1 + 1\nc (l) j\nln [ 1 + \u00b7 \u00b7 \u00b7 ln ( 1 + 1\nc (2) j\n)]}) .\nWith p(l)j := 1 \u2212 e\u2212q (l) j and p\u0303 := q(L+1)\u00b7 /(c0 + q (L+1) \u00b7 ), one may re-express the hierarchical model of the PGBN as deep latent Dirichlet allocation (DLDA) as\nx (L+1) k\u00b7 \u223c Log(p\u0303),KL \u223c Pois[\u2212\u03b30 ln(1\u2212 p\u0303)],\nX(L+1) = \u2211KL\nk=1 x (L+1) k\u00b7 \u03b4\u03c6(L)\nk\n,\n( x (L+1) vj ) j \u223c Mult [ x (L+1) v\u00b7 , ( q (L+1) j ) j / q (L+1) \u00b7 ] ,\nm (L)(L+1) vj \u223c SumLog(x (L+1) vj , p (L+1) j ),\n\u00b7 \u00b7 \u00b7 x (l) vj = \u2211Kl k=1 x (l) vkj , ( x (l) vkj ) v \u223c Mult ( m (l)(l+1) kj ,\u03c6 (l) k ) ,\nm (l\u22121)(l) vj \u223c SumLog(x (l) vj , p (l) j ),\n\u00b7 \u00b7 \u00b7 x (1) vj = \u2211K1 k=1 x (1) vkj , ( x (1) vkj ) v \u223cMult ( m (1)(2) kj ,\u03c6 (1) k ) . (6)\nNote that the equations in the first four lines of (6) precisely represent a random count matrix generated from a gammanegative binomial process that can also be generated from\nx (L+1) kj \u223c Pois ( rkq (L+1) j ) , rk \u223c Gam (\u03b30/K, 1/c0) ,\nm (L)(L+1) kj \u223c SumLog ( x (L+1) kj , p (L+1) j ) (7)\nby letting K \u2192\u221e (Zhou et al., 2016b). When L = 1, the PGBN whose (rk,\u03c6k) are the points of a gamma process reduces to the gamma-negative binomial process PFA of Zhou & Carin (2015), whose alternative representation is provided in Corollary D.1 in the Appendix. Note that how we re-express the PGBN as DLDA is related to how Schein et al. (2016) re-express their Poisson\u2013gamma dynamic systems into an alternative representation that facilitates inference.\nDLDA, designed to infer a multilayer representation of observed or latent high-dimensional sparse count vectors, constrains all the basis vectors of different layers to probability simplices. It is clear from (6) that a data point backpropagates its counts through the network one layer at a time via a sum-logarithmic distribution to enlarge each element of a Kl-dimensional count vector, a multinomial distribution to partition that enlarged count vector into a Kl\u22121 \u00d7Kl count matrix, and then a row-sum operation to aggregate\nthat latent count matrix into a Kl\u22121-dimensional count vector, where K0 := V is the feature dimension. Below we show that such an alternative representation that repeats the enlarge-partition-augment operation brings significant benefits when it comes to deriving SG-MCMC inference with preconditioned gradients.\n3.1. Fisher Information Matrix of DLDA\nIn deep LVMs, whose parameters of different layers are often highly correlated to each other, it is often difficult to tune the step sizes of different layers together and hence one often chooses to train an unsupervised deep model in a greedy layer-wise manner (Bengio et al., 2007), which is a sensible but not optimal training strategy. To address that issue, we resort to the inverse of the FIM that is widely used to precondition the gradients to adjust the learning rates of different model parameters (Amari, 1998; Pascanu & Bengio, 2013; Ma et al., 2015; Li et al., 2016). However, it is often difficult to compute the FIMs of deep LVMs as\nG (z) = E\u2126|z [ \u2212 \u2202 2\n\u2202z2 ln p (\u2126 |z )\n] , (8)\nwhere z denotes the set of all global variables and \u2126 is the set of all observed and local variables.\nAlthough deriving the FIM for the PGBN generative model shown in (1) seems impossible, we find it to be straightforward under the alternative DLDA representation in (6). Since the likelihood of (6) is fully factorized with respect to the global parameters z, i.e., \u03c6(l)k and r, one may readily show the FIM G (z) of (6) has a block diagonal form as\ndiag [ I ( \u03d5\n(1) 1\n) , \u00b7 \u00b7 \u00b7 , I ( \u03d5\n(L) KL\n) , I (r) ] ; (9)\nwith the likelihood ( x (l) vkj ) v \u223c Mult ( m (l)(l+1) kj ,\u03c6 (l) k ) and the reduced-mean parameterization, we have\nI ( \u03d5\n(l) k\n) =\u2212E [ \u22022\n\u2202\u03d5 (l)2 k\nln (\u220f jMult [ (x (l) vkj)v;m (l)(l+1) kj ,\u03c6 (l) k\n])]\n= M (l) k\n[ diag ( 1/\u03d5\n(l) k ) +11T /(1\u2212\u03d5(l)\u00b7k ) ] , (10)\nwhere M (l)k := E [ m (l)(l+1) k\u00b7 ] =E [ x (l) \u00b7k\u00b7 ] . Similarly, with the likelihood x(L+1)kj \u223c Pois(rkq (L+1) j ), we have\nI (r) = M (L+1)diag (1/r) , (11)\nwhere M (L+1) := E [ q (L+1) \u00b7 ] .\nThe block diagonal structure of the FIM of DLDA makes it computationally appealing to apply its inverse for preconditioning. Under the framework suggested by (4), we adopt the similar settings used in SGRLD (Patterson & Teh, 2013) that lets D (z) = G(z)\u22121,Q (z) = 0, and B\u0302t = 0. While other more sophisticated settings described in Ma et al. (2015), including as special examples stochastic gradient Hamiltonian Monte Carlo in Chen et al. (2014) and\nstochastic gradient thermostats in Ding et al. (2014), may be used to further improve the performance, we choose this specific one to make a direct comparison with SGRLD.\nBy substituting the FIM G (z) and the adopted settings into (4), it is apparent that we only need to choose a single step size \u03b5t, relying on the FIM to automatically adjust the relatively learning rates for different parameters across all layers and topics. Moreover, the block-diagonal structure of G (z) will be carried over to its inverse D (z), making it simple to perform updating using (4), as described below.\n3.2. Inference on the Probability Simplex\nAs discussed in Section 2, to sample simplex-constrained model parameters for a Dirichlet-multinomial model, the SGRLD of Patterson & Teh (2013) adopts the expandedmean parameterization of simplex-constrained vectors and makes a pseudolikelihood assumption to simplify the derivation of update equations. In this paper, without replying on that pseudolikelihood assumption, we choose to use the reduced-mean parameterization of simplex-constrained vectors, despite being considered as an unsound choice in Patterson & Teh (2013). In the following discussion, we omit the layer-index superscript (l) for simplicity.\nWith the multinomial likelihood in (6) and the Dirichletmultinomial conjugacy, the conditional posterior of \u03c6k can be expressed as (\u03c6k | \u2212) \u223c Dir(x1k\u00b7 + \u03b7, . . . , xV k\u00b7 + \u03b7). Taking the gradient with respect to \u03d5k \u2208 RV\u22121+ on the summation of the negative log-likelihood of a mini-batch X\u0303 scaled by \u03c1 = |X|/|X\u0303| and the negative log-likelihood of the Dirichlet prior, we have\n\u2207\u03d5k [ \u2212H\u0303(\u03d5k) ] = \u03c1x\u0304:k\u00b7+\u03b7\u22121\n\u03d5k \u2212 \u03c1x\u0303V k\u00b7+\u03b7\u22121 1\u2212 \u03d5\u00b7k , (12)\nwhere x\u0303vk\u00b7 = \u2211\nj:xj\u2208X\u0303 xvkj and x\u0304:k\u00b7 := (x\u03031k\u00b7, . . . , x\u0303(V\u22121)k\u00b7)T . Note the gradient in (12) becomes unstable when some components of \u03d5k approach zeros, a key reason that this approach is mentioned but not further pursued in Patterson & Teh (2013).\nHowever, after preconditioning the noisy gradient with the inverse of the FIM, it is intriguing to find out that the stability issue completely disappears. More specifically, by plugging both the FIM in (10) and noisy gradient in (12) into the SGMCMC update in (4), a noisy estimate of the deterministic drift defined in (2) obtained using the current mini-batch can be expressed as\nI (\u03d5k) \u22121\u2207\u03d5k [ \u2212H\u0303 (\u03d5k) ] + \u0393 (\u03d5k)\n= M\u22121k [(\u03c1x\u0304:k\u00b7+\u03b7)\u2212(\u03c1x\u0303\u00b7k\u00b7+\u03b7V )\u03d5k] , (13)\nwhere \u0393 (\u03d5k) = M \u22121 k [1\u2212 V\u03d5k] according to (3), as derived in detail in Appendix B. Consequently, with [\u00b7]4 de-\nnoting the constraint that \u03d5vk \u2265 0 and \u2211V\u22121\nv=1 \u03d5vk \u2264 1, using (4), the sampling of \u03d5k becomes\n(\u03d5k)t+1 = [ (\u03d5k)t+ \u03b5t Mk [ (\u03c1x\u0304:k\u00b7+\u03b7)\u2212(\u03c1x\u0303\u00b7k\u00b7+\u03b7V )(\u03d5k)t ]\n+N ( 0, 2\u03b5t\nMk\n[ diag (\u03d5k)t\u2212(\u03d5k)t (\u03d5k) T t ])] 4 . (14)\nEven without the [\u00b7]4 constraint, the multivariate normal (MVN) simulation in (14), although easy to interpret and numerically stable, is computationally expensive if the Cholesky decomposition, with O((V \u2212 1)3) complexity (Golub & Van Loan, 2012), is adopted directly. Fortunately, using Theorem 2 of Cong et al. (2017), the special structure of its covariance matrix allows an equivalent but substantially more efficient simulation of O(V ) complexity by transforming a random variable drawn from a related MVN that has a diagonal covariance matrix. More specifically, the sampling of (14) can be efficiently realized in a V -dimensional space as\n(\u03c6k)t+1 = [ (\u03c6k)t+\n\u03b5t Mk\n[ (\u03c1x\u0303:k\u00b7+\u03b7)\u2212(\u03c1x\u0303\u00b7k\u00b7+\u03b7V )(\u03c6k)t ]\n+N (\n0, 2\u03b5t Mk diag (\u03c6k)t\n)]\n\u2220 , (15)\nwhere [\u00b7]\u2220 denotes the simplex constraint that \u03c6vk \u2265 0 and\u2211V v=1 \u03c6vk = 1. More details on simulating (14) and (15) can be found in Examples 1-3 of Cong et al. (2017).\nSimilarly, with the gamma-Poisson construction in (7), we have \u0393k (r) = 1/M (L+1), as in Appendix B, and\n\u2207rk [ \u2212H\u0303(r) ] =r\u22121k ( \u03c1x\u0303 (L+1) k\u00b7 + \u03b30 KL \u22121 ) \u2212 ( c0 + \u03c1q\u0303 (L+1) \u00b7 ) ,\n(16) which also becomes unstable if rk approaches zero. Substituting (16) and (11) into (4) leads to\nrt+1 = \u2223\u2223\u2223\u2223rt+ \u03b5t\nM (L+1)\n[( \u03c1x\u0303(L+1):\u00b7 +\n\u03b30 KL\n) \u2212rt ( c0+\u03c1q\u0303 (L+1) \u00b7 )]\n+N ( 0, 2\u03b5t\nM (L+1) diag (rt)\n)\u2223\u2223\u2223\u2223, (17)\nfor which there is no stability issue.\n3.3. Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC\nNote that M (L+1) and M (l)k for l \u2208 {1, . . . , L}, appearing as denominates in (17) and (15), respectively, are expectations that need to be approximately calculated. We update them using annealed weighting (Polatkan et al., 2015) as\nM (l) k = ( 1\u2212 \u03b5\u2032t ) M (l) k + \u03b5 \u2032 t\u03c1E [ x\u0303 (l) \u00b7k\u00b7 ] , (18)\nM (L+1) = ( 1\u2212 \u03b5\u2032t ) M (L+1) + \u03b5 \u2032 t\u03c1E [ q\u0303 (L+1) \u00b7 ] , (19)\nwhere E[\u00b7] denotes averaging over the collected MCMC samples. For simplicity, we set \u03b5 \u2032 t = \u03b5t in this paper, which is found to work well in practice.\nAlgorithm 1 TLASGR MCMC for DLDA (PGBN). Input: Data mini-batches; Output: Global parameters of DLDA (PGBN). 1: for t = 1, 2, \u00b7 \u00b7 \u00b7 do 2: /* Collect local information 3: Upward-downward Gibbs sampling (Zhou et al., 2016a) on\nthe tth mini-batch for x\u0303:k\u00b7, x\u0303\u00b7k\u00b7, x\u0303 (L+1) :\u00b7 , and q\u0303 (L+1) \u00b7 ;\n4: /* Update global parameters 5: for l = 1, \u00b7 \u00b7 \u00b7 , L and k = 1, \u00b7 \u00b7 \u00b7 ,Kl do 6: Update M (l)k with (18); then \u03c6 (l) k with (15); 7: end for 8: Update M (L+1) with (19) and then r with (17). 9: end for\nNote that as in (15) and (17), instead of having a single learning rate for all layers and topics, a common practice due to the difficulty to adapt the step sizes to different layers and/or topics, the proposed inference employs topic-layer-adaptive learning rates as \u03b5t/M (l) k , where M (L+1) k := M\n(L+1), adapting a single step size \u03b5t to different topics and layers by multiplying it with the weights 1/M (l)k for l \u2208 {1, . . . , L} and k \u2208 {1, . . . ,Kl}. We refer to the proposed inference algorithm with adaptive learning rates as topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC, as summarized in Algorithm 1 that is simple to implement.\n4. Related Work Both LDA (Blei et al., 2003) and the related Poisson factor analysis (PFA) (Zhou et al., 2012) are equipped with scalable inference, such as stochastic variational inference (SVI) (Hoffman et al., 2010; Mimno et al., 2012) and SGRLD (Patterson & Teh, 2013). However, both are shallow LVMs whose modeling capacities are often insufficient for big and complex data. The deep Poisson factor models of Gan et al. (2015) and Henao et al. (2015) are proposed to generalize PFA with deep structures, but both of them only explore the deep information in binary topic usage patterns instead of the full connection weights that are used in the PGBN. The proposed DLDA shares some similarities with the pachinko allocation model of Li & McCallum (2006) in that they both adopt layered construction and use Dirichlet distributed topics. Ranganath et al. (2015) propose deep exponential family (DEF), which differs from the PGBN in connecting adjacent layers via the gamma rate parameters and using black-box variational inference (BBVI) (Ranganath et al., 2014).\nSome commonly used neural networks, such as deep belief network (DBN) (Hinton et al., 2006) and deep Boltzmann machines (DBM) (Salakhutdinov & Hinton, 2009), have also been modified for text analysis (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al., 2013). Although they may work well for certain text analysis tasks, they are not naturally designed for count data and often yield latent structures that are not readily interpretable.\nThe neural variational document model (NVDM) of Miao et al. (2016), even though using deep neural networks in its variational auto-encoder (VAE) (Kingma & Welling, 2013), still relies on a single-layer model for data generalization.\nGenerally speaking, it is challenging to develop an efficient and principled multilayer joint learning algorithm for deep LVMs. Scalable variational inference, such as BBVI, often makes the restrictive mean-field assumption. Neural variational inference and learning (NVIL) relies on variance reduction techniques that are often difficult to be generalized for discrete LVMs (Mnih & Gregor, 2014; Rezende et al., 2014). When a SG-MCMC algorithm is used, a single learning rate is often applied for different variables across all layers (Welling & Teh, 2011; Neal et al., 2011; Chen et al., 2014; Ding et al., 2014). It is possible to improve SG-MCMC by adjusting its noisy gradients with some stochastic optimization technique, such as Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014), and RMSprop (Tieleman & Hinton, 2012). For example, Li et al. (2016) show that preconditioning the gradients with diagonal approximated FIM improves SG-MCMC in both training speed and predictive accuracy for supervised learning where gradients are easy to calculate. Other efforts exploiting similar preconditioning idea focus on shallow and/or binary models (Mimno et al., 2012; Patterson & Teh, 2013; Grosse & Salakhutdinov, 2015; Song et al., 2016; Simsekli et al., 2016), and it is unclear how that idea can be extended to deep LVMs whose gradients and FIM maybe difficult to approximate.\n5. Experiment results We present experimental results on three benchmark corpora: 20Newsgroups (20News), Reuters Corpus Volume I (RCV1) that is moderately large, and Wikipedia (Wiki) that is huge. 20News consists of 18,845 documents with a vocabulary size of 2,000, partitioned into 11,315 training documents and 7,531 test ones. RCV1 consists of 804,414 documents with a vocabulary size of 10,000, where 10,000 documents are randomly selected for testing. Wiki consists of 10 million documents randomly downloaded from Wikipedia using the scripts provided in Hoffman et al. (2010); as in Hoffman et al. (2010), Gan et al. (2015), and Henao et al. (2015), we use a vocabulary with 7,702 words and randomly select 1,000 documents for testing. To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al. (2015). To be consistent with the settings of Gan et al. (2015) and Henao et al. (2015), no precautions are taken in the scripts for Wikipedia to prevent a testing document from being downloaded into a mini-batch for training.\nWe consider two related performance measures. The first one is the commonly-used per-heldout-word perplexity cal-\nculated as follows: for each test document, we randomly select 80% of the word tokens to sample the local variables specific to the document, under the global model parameters of each MCMC iteration; after the burn-in period, we accumulate the first layer\u2019s Poisson rates in each collected MCMC sample; in the end, we normalize these accumulated Poisson rates to calculate the perplexity using the remaining 20% word tokens. Similar evaluation methods have been widely used, e.g., in Wallach et al. (2009), Paisley et al. (2011), and Zhou & Carin (2015). Although a good measure for overall performance, the per-heldout-word perplexity, calculated based on multiple collected MCMC samples of global parameters, may not be ideal to check the performance in real time to assess how efficient an iterative algorithm improves its performance as time increases. Therefore, we slightly modify it to provide a point per-heldout-word perplexity calculated based on only the global parameters of the most recent MCMC sample. For simplicity, we refer to (point) per-heldout-word perplexity as (point) perplexity.\nFor comparison, we consider LDA of Blei et al. (2003), focused topic model (FTM) of Williamson et al. (2010), replicated softmax (RSM) of Hinton & Salakhutdinov (2009), nested Hierarchical Dirichlet process (nHDP) of Paisley et al. (2015), DPFA of Gan et al. (2015), and DPFM of Henao et al. (2015). For these methods, the perplexity results are taken from Gan et al. (2015) and Henao et al. (2015). For the proposed algorithms, we set the mini-batch size as 200, and use as burn-in 2000 mini-batches for both 20News and RCV1 and 3500 mini-batches for Wiki. We collect 1500 samples to calculate perplexity. For point perplexity, given the global parameters of an MCMC sample, we sample the local variables with 600 iterations and collect one sample every two iterations during the last 400 iterations. The hyperparameters of DLDA are set as: \u03b7(l) = 1/Kl, a0 = b0 = 0.01, and \u03b30 = c0 = e0 = f0 = 1. Note \u03b7(l) and Kl are set similar to that of DPFM for fair comparisons, while other hyperparameters follow Zhou et al. (2016a).\nTo demonstrate the advantages of using the reduced-mean simplex parameterization and inverting the FIM for preconditioning to obtain topic-layer-adaptive learning rates, we consider four different inference methods:\n1) TLASGR: topic-layer-adaptive stochastic gradient Riemannian MCMC for DLDA, as described in Algorithm 1.\n2) TLFSGR: topic-layer-fixed stochastic gradient Riemannian MCMC for DLDA that replaces the adaptive learning rates \u03b5t/M (l) k of TLASGR with \u03b5t/( \u2211K1 k=1M (1) k /K1).\n3) SGRLD: updating each \u03c6(l)k under the expanded-mean parameterization as in (5), served as a good scalable baseline for comparison since it was shown in Patterson & Teh (2013) to perform significantly better than SVI. It differs from TLFSGR mainly in using a different parameterization for\nBoth TLASGR and TLFSGR differ from SGRLD mainly in how the global parameters \u03c6(l)k are updated. While TLASGR uses topic-layer-adaptive learning rates, both TLFSGR and SGRLD use a single learning rate, a common practice due to the difficulty of tuning the step sizes across layers and topics. We keep the same stepsize schedule of \u03b5t = a(1 + t/b)\n\u2212c as in Patterson & Teh (2013) and Ma et al. (2015).\nLet us first examine how various inference algorithms perform on 20News with a single-layer DLDA of size 128, which can be considered as a topic model that imposes an\nasymmetric prior on a document\u2019s proportion over these 128 topics. Under this setting, as shown in Fig. 1(a), TLFSGR clearly outperforms SGRLD in providing lower point perplexities as time progresses, which is not surprising as under the reduced-mean simplex parameterization, to derive its sampling equations, TLFSGR does not rely on a pseudolikelihood assumption that is adopted by SGRLD in its expanded-mean simplex parameterization. Moreover, TLASGR is found to further improve TLFSGR, suggesting that even for a single-layer model, replacing a fixed learning rate as \u03b5t/( \u2211K1 k=1M (1) k /K1) with topic-adaptive learning rates as \u03b5t/M (1) k could further improve the performance.\nLet us then examine how these algorithms perform on two larger corpora\u2014RCV1 and Wiki\u2014with a two-layer DLDA of size 128-64, which improves the single-layer one by capturing the co-occurrence patterns between the topics of the first layer with those of the second layer (Zhou et al., 2016a). As show in Figs. 1(b) and 1(c), it is clear that the proposed TLASGR performs the best for the two-layer DLDA and consistently outperforms TLFSGR as time progresses. In comparison, SGRLD quickly improves its performance as a function of time in the beginning but its point perplexity remains higher even after 10,000 seconds, whereas Gibbs sampling slowly improves its performance as a function of time in the beginning but moves its point perplexity closer and closer to that of TLASGR as time progresses.\nNote that for 20News, the point perplexity of the mini-batch based TLASGR quickly decreases as time increases, while that of Gibbs sampling decreases relatively slowly. That discrepancy of convergence rate as a function of time becomes much more evident for both RCV1 and Wiki, as shown in Figs. 1(b) and 1(c). This is expected as both RCV1 and Wiki are much larger corpora, for which a mini-batch based inference algorithm can already make significant progress in learning the global model parameters, before a batchlearning Gibbs sampler finishes a single iteration that needs to go through all documents.\nTo illustrate the working mechanism of TLASGR, we show how its inferred learning rates are adapted to different layers in Fig. 2. By contrast, TLFSGR admits a fixed learning rate that leads to worse performance. Several interesting observations can be made for TLASGR from Figs. 2(a)-2(c): 1) for \u03a6(l), higher layers prefer larger step sizes, which may be explained by the enlarge-partition-augment data generating mechanism of DLDA; 2) larger datasets prefer slower learning rates, reflected by the scales of the vertical axes; 3) and the relative learning rates between different layers vary across different datasets.\nTo further verify the excellent performance of DLDA inferred with TLASGR, we compare a wide variety of models and inference algorithms in Table 1. For 20News and RCV1, DLDA with Gibbs sampling performs the best in terms of perplexity and exhibits a clear trend of improvement as the number of hidden layers increases. For Wiki, a single iteration of the DLDA Gibbs sampler on the full corpus is so expensive in both time and memory that its performance is not reported. For DLDA on 20News and RCV1, TLASGR only slightly underperforms Gibbs sampling, and the performance degradation from Gibbs sampling to TLASGR is significantly smaller than that from MCMC to SVI for DPFM. That relative small degradation caused by changing from Gibbs sampling to the mini-batch based TLASGR could be attributed to the Fisher efficiency brought by the FIM. Generally speaking, in comparison to SGRLD, TLASGR brings a clear boost in performance, which is particularly evident for a deeper DLDA, and TLASGR consistently outperforms TLFSGR that does not adapt its learning rates to different topics and layers.\nMNIST. To further illustrate the advantages of using the inverse of the FIM for preconditioning in a deep generative model, and to visualize the benefits of automatically adjusting the relative learning rates of different hidden layers, we apply a three-layer Poisson randomized gamma gamma belief network (PRG-GBN) (Zhou et al., 2016a) to 60,000 MNIST digits and present the learned dictionary atoms after one full epoch, as shown in Fig. 3. It is clear that, with topic-layer-adaptive learning rates, which are made possible by utilizing the FIM, TLASGR provides more effective mini-batch based stochastic updates to allow better information propagation between different hidden layers, extracting more interpretable features at multiple layers.\n6. Conclusions For scalable multilayer joint inference of the Poisson gamma belief network (PGBN), we introduce an alternative representation of the PGBN, which is referred to as deep latent Dirichlet allocation (DLDA) that can be considered as a multilayer generalization of latent Dirichlet allocation. We show how to reparameterize the simplex constrained basis vectors, derive a block-diagonal Fisher information matrix (FIM), and efficiently compute the inverse of the FIM, leading to a stochastic gradient MCMC algorithm referred to as topiclayer-adaptive stochastic gradient Riemannian (TLASGR) MCMC. The proposed TLASGR-MCMC is able to jointly learn the parameters of different layers with topic-layeradaptive step sizes, which makes DLDA (PGBN) much more practical in a big data setting. Compelling experimental results on large text corpora and the MNIST dataset demonstrated the advantages of TLASGR-MCMC.\nAcknowledgements Bo Chen thanks the support of the Thousand Young Talent Program of China, NSFC (61372132), and NDPR9140A07010115DZ01019. Hongwei Liu thanks the support of NSFC for Distinguished Young Scholars (61525105).\nReferences Amari, S. Natural gradient works efficiently in learning.\nNeural Computation, 10(2):251\u2013276, 1998.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. Greedy layer-wise training of deep networks. In NIPS, pp. 153\u2013160, 2007.\nBlei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet allocation. JMLR, 3:993\u20131022, 2003.\nChen, T., Fox, E. B., and Guestrin, C. Stochastic gradient Hamiltonian Monte Carlo. In ICML, pp. 1683\u20131691, 2014.\nCong, Y., Chen, B., and Zhou, M. Fast simulation of hyperplane-truncated multivariate normal distributions. Bayesian Analysis Advance Publication, 2017.\nDing, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D., and Neven, H. Bayesian sampling using stochastic gradient thermostats. In NIPS, pp. 3203\u20133211, 2014.\nDuchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12(Jul):2121\u20132159, 2011.\nGan, Z., Chen, C., Henao, R., Carlson, D., and Carin, L. Scalable deep Poisson factor analysis for topic modeling. In ICML, pp. 1823\u20131832, 2015.\nGirolami, M. and Calderhead, B. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. JRSSB, 73(2):123\u2013214, 2011.\nGolub, Gene H and Van Loan, Charles F. Matrix computations, volume 3. JHU Press, 2012.\nGrosse, R. B. and Salakhutdinov, R. Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix. In ICML, pp. 2304\u20132313, 2015.\nHenao, R., Gan, Z., Lu, J., and Carin, L. Deep Poisson factor modeling. In NIPS, pp. 2782\u20132790, 2015.\nHinton, G. E. and Salakhutdinov, R. R. Replicated softmax: an undirected topic model. In NIPS, pp. 1607\u20131614, 2009.\nHinton, G. E., Osindero, S., and Teh, Y. W. A fast learning algorithm for deep belief nets. Neural Computation, 18 (7):1527\u20131554, 2006.\nHoffman, M. D., Bach, F. R., and Blei, D. M. Online learning for latent Dirichlet allocation. In NIPS, pp. 856\u2013 864, 2010.\nJohnson, N. L., Kotz, S., and Balakrishnan, N. Discrete Multivariate Distributions, volume 165. Wiley New York, 1997.\nKingma, D. and Ba, J. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\nKingma, Diederik P and Welling, Max. Auto-encoding variational Bayes. In ICLR, number 2014, 2013.\nLarochelle, H. and Lauly, S. A neural autoregressive topic model. In NIPS, 2012.\nLi, C., Chen, C., Carlson, D., and Carin, L. Preconditioned stochastic gradient Langevin dynamics for deep neural networks. AAAI, 2016.\nLi, W. and McCallum, A. Pachinko allocation: DAGstructured mixture models of topic correlations. In ICML, pp. 577\u2013584, 2006.\nMa, Y., Chen, T., and Fox, E. A complete recipe for stochastic gradient MCMC. In NIPS, pp. 2899\u20132907, 2015.\nMiao, Y., Yu, L., and Blunsom, P. Neural variational inference for text processing. In ICML, 2016.\nMimno, D., Hoffman, M. D., and Blei, D. M. Sparse stochastic inference for latent Dirichlet allocation. In ICML, pp. 362 \u2013 365, 2012.\nMnih, A. and Gregor, K. Neural variational inference and learning in belief networks. 2014.\nNeal, R. M. et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2:113\u2013162, 2011.\nPaisley, J., Wang, C., and Blei, D. The discrete infinite logistic normal distribution for mixed-membership modeling. In AISTATS, 2011.\nPaisley, J., Wang, C., Blei, D. M., and Jordan, M. I. Nested hierarchical dirichlet processes. PAMI, 37(2):256\u2013270, 2015.\nPascanu, R. and Bengio, Y. Revisiting natural gradient for deep networks. arXiv:1301.3584, 2013.\nPatterson, S. and Teh, Y. W. Stochastic gradient Riemannian Langevin dynamics on the probability simplex. In NIPS, pp. 3102\u20133110, 2013.\nPolatkan, G., Zhou, M., Carin, L., Blei, D., and Daubechies, I. A Bayesian nonparametric approach to image superresolution. PAMI, 37(2):346\u2013358, 2015.\nRanganath, R., Gerrish, S., and Blei, D. M. Black box variational inference. In AISTATS, 2014.\nRanganath, R., Tang, L., Charlin, L., and Blei, D. M. Deep exponential families. In AISTATS, 2015.\nRezende, Danilo J, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. In ICML, pp. 1278\u20131286, 2014.\nSalakhutdinov, R. and Hinton, G. E. Deep Boltzmann machines. In AISTATS, volume 1, pp. 3, 2009.\nSchein, A., Zhou, M., and Wallach, H. Poisson\u2013gamma dynamical systems. In NIPS, pp. 5006\u20135014, 2016.\nSimsekli, U., Badeau, R., Cemgil, A. T., and G., Richard. Stochastic quasi-Newton Langevin Monte Carlo. In ICML, 2016.\nSong, Z., Henao, R., Carlson, D., and Carin, L. Learning sigmoid belief networks via Monte Carlo expectation maximization. In AISTATS, pp. 1347\u20131355, 2016.\nSrivastava, N., Salakhutdinov, R. R., and Hinton, G. E. Modeling documents with deep Boltzmann machines. In UAI, 2013.\nTieleman, T. and Hinton, G. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4 (2), 2012.\nWallach, H. M., Murray, I., Salakhutdinov, R., and Mimno, D. Evaluation methods for topic models. In ICML, 2009.\nWelling, M. and Teh, Y.-W. Bayesian learning via stochastic gradient Langevin dynamics. In ICML, pp. 681\u2013688, 2011.\nWilliamson, S., Wang, C., Heller, K. A., and Blei, D. M. The IBP compound Dirichlet process and its application to focused topic modeling. In ICML, pp. 1151\u20131158, 2010.\nZeiler, M. D. Adadelta: an adaptive learning rate method. arXiv:1212.5701, 2012.\nZhou, M. and Carin, L. Negative binomial process count and mixture modeling. PAMI, 37(2):307\u2013320, 2015.\nZhou, M., Hannah, L., Dunson, D. B., and Carin, L. Betanegative binomial process and Poisson factor analysis. In AISTATS, pp. 1462\u20131471, 2012.\nZhou, M., Cong, Y., and Chen, B. Augmentable gamma belief networks. Journal of Machine Learning Research, 17(163):1\u201344, 2016a.\nZhou, M., Padilla, O., and Scott, J. G. Priors for random count matrices derived from a family of negative binomial processes. J. Amer. Statist. Assoc., 111(515):1144\u20131156, 2016b.\nSupplementary Material for Deep latent Dirichlet allocation with topic-layer-adaptive\nstochastic gradient Riemannian MCMC Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou\nA. Naive derivation of the Fisher information matrix of the Poisson gamma belief network\nFor simplicity, we take for example a two-layer Poisson gamma belief network (PGBN), expressed as\n\u03b8 (2) j \u223c Gam\n( r, 1/c\n(3) j\n) ,\nx (1) j \u223c Pois\n( \u03a6(1)\u03b8\n(1) j\n) ,\u03b8\n(1) j \u223c Gam\n( \u03a6(2)\u03b8\n(2) j ,\np (2) j\n1\u2212p(2)j\n) ,\n(20) and focus on a specific element \u03a6(2)vk only.\nWith the definition in (8), it is straight to show that the \u03a6(2)-relevant part in ln p (\u2126 |z ) is \u2211\nvj\n[ \u03a6(2)v: \u03b8 (2) :j ln ( c (2) j \u03b8 (1) vj ) \u2212 ln \u0393 ( \u03a6(2)v: \u03b8 (2) :j )] . (21)\nAccordingly, for \u03a6(2)vk , we have\nE [ \u2212 \u2202 2\n\u2202[\u03a6 (2) vk ]\n2 ln p (\u2126 |z )\n] = E [\u2211\nj\n\u03c8\u2032 ( \u03a6(2)v: \u03b8 (2) :j ) [ \u03b8 (2) :j ]2 ] ,\n(22) where \u03c8\u2032 (\u00b7) is the trigamma function. This expectation involving the trigamma function is difficult to calculate.\nB. Derivation of the \u0393 (\u00b7) functions in Section 3.2\nWith D (z) = G(z)\u22121,Q (z) = 0, and the block-diagonal Fisher information matrix (FIM) G(z) in (9), it is straight to show that \u2202\u2202\u03d5k [D (z) + Q (z)] is non-zero only in the \u03d5k-related block I (\u03d5k) in (10). Therefore, we focus on this block and have\n\u0393v (\u03d5k) = \u2211\nu\n\u2202\n\u2202\u03d5uk\n[ I\u22121vu (\u03d5k) ] , (23)\nwhere I\u22121 (\u03d5k) = M \u22121 k\n[ diag(\u03d5)\u2212\u03d5\u03d5T ] . Accordingly,\nwe have\n\u0393v (\u03d5k) = M \u22121 k \u2211 u \u2202 \u2202\u03d5uk [\u03b4u=v\u03d5uk \u2212 \u03d5vk\u03d5uk]\n= M\u22121k (1\u2212 V \u03d5vk). (24)\nSince G(z) is block-diagonal with its r-relevant block being I (r) = M (L+1)diag (1/r), according to (3), it is\nstraightforward to show that\n\u0393k (r) = \u2211\nu\n\u2202\n\u2202ru\n[ I\u22121ku (r) ] ,\n= \u2211\nu\n\u2202\n\u2202ru\n[ \u03b4u=k\nru M (L+1)\n] ,\n= 1/M (L+1).\n(25)\nC. Proof of Lemma 3.1 Note that the counts in x(l)vj \u223c Pois ( q (l) j \u2211Kl k=1 \u03c6 (l) vk\u03b8 (l) kj ) can be augmented as\nx (l) vj = \u2211Kl k=1 x (l) vkj ,\nx (l) vkj \u223c Pois ( q (l) j \u03c6 (l) vk\u03b8 (l) kj ) ,\n(26)\nwhich, according to Lemma 4.1 of Zhou et al. (2012), can be equivalently expressed as\n( x (l) vkj ) v \u223c Mult ( m (l)(l+1) kj ,\u03c6 (l) k ) ,\nm (l)(l+1) kj \u223c Pois ( q (l) j \u03b8 (l) kj ) ,\n(27)\nwhere m(l)(l+1)kj := \u2211Kl\u22121 v=1 x (l) vkj . Marginalizing out \u03b8 (l) vj \u223c\nGam (\u2211Kl+1\nk=1 \u03c6 (l+1) vk \u03b8 (l+1) kj , 1/c (l+1) j\n) from (27) leads to\nm (l)(l+1) vj \u223c NB (\u2211Kl+1 k=1 \u03c6 (l+1) vk \u03b8 (l+1) kj , p (l+1) j ) , (28)\nwhich can be augmented as\nm (l)(l+1) vj \u223c SumLog(x (l+1) vj , p (l+1) j ),\nx (l+1) vj \u223c Pois ( q (l+1) j \u2211Kl+1 k=1 \u03c6 (l+1) vk \u03b8 (l+1) kj ) . (29)\nWhen l = L, we have\nm (L)(L+1) kj \u223c NB(rk, p (L+1) j ), (30)\nmarginalizing the gamma processG \u223c GaP(G0, 1/c0) from which leads to a gamma-negative binomial process random count matrix, as expressed in the first two lines of (6).\nD. Corollary D.1 Corollary D.1. The gamma-negative binomial process PFA can be equivalently expressed as\n`k\u00b7 \u223c Log (\nq\u00b7 c0 + q\u00b7\n) , K \u223c Pois ( \u03b30 ln\nc0 + q\u00b7 c0\n) ,\nL = \u2211K\nk=1 `k\u00b7\u03b4\u03c6k ,\n( `kj ) j \u223c Mult [ `k\u00b7, ( qj ) j /q\u00b7 ] ,\nmkj \u223c SumLog(`kj , pj)\nxvj = \u2211K\nk=1 xvkj , (xvkj)v \u223c Mult (mkj ,\u03c6k) .\n(31)\nE. Visualizations of the extracted topics and networks\nIn the following, we provide some example results, obtained using DLDA where [K1,K2,K3] = [128, 64, 32] and \u03b7(l) = 1/Kl for the lth layer, on extracting multilayer representations/topics from both the RCV1 and Wiki corpora. Clearly interpretable results, which are similar to those reported in Zhou et al. (2016a) and hence omitted here for brevity, are also extracted from the 20Newsgroups corpus.\nE.1. RCV1\nFollowing the visualization techniques in Zhou et al. (2016a), we plot 54 example topics of layer one in Figure 4, the top 30 topics of layer two in Figure 5, and the top 30 topics of layer three in Figure 6. Figure 4 clearly shows that the topics of layer one are very specific. For example, topics 41, 71 and 62 in the first row are about \u201cGermany,\u201d \u201cPolish,\u201d and \u201cFrance,\u201d respectively; topics 53 and 54 in the second row are about \u201cairline\u201d and \u201cEuropean union,\u201d respectively; and topics 85 and 36 in the third row are about \u201cship & island\u201d and \u201ccomput & techn,\u201d respectively. By contrast, the topics of layers two and three, shown in Figures 5 and 6, respectively, are increasingly more general. Such topics can be better interpreted via the following informative tree structured visualizations. Note that a tree defined in this paper allows a child node of a layer to be connected to more than one parent node of the adjacent higher layer.\nShown in Figure 7 is a [10, 3, 1] tree rooted at node 4 of the top layer on \u201cbonds, rates, & credit markets.\u201d Apparently, the topics become more and more specific when moving from top to bottom following the branches. For example, the root node splits into three nodes from layers three to two, which focus differently on \u201ctreasury bill,\u201d \u201cdollar rate,\u201d and \u201cbond, credit, & debt,\u201d respectively. When moving from layers two to one, all three topics in layer two splits into multiple ones that is clearly more specific. For example, topics 1, 17, and 87 are about \u201cmonths,\u201d \u201cloan & credit,\u201d and \u201cbond & pay,\u201d respectively.\nShown in Figure 8 is another analogous tree rooted at node 24 of layer three. It is clear that, as the nodes of this tree, topics 55, 38, 34, and 30 of layer two are mainly about \u201cGermany,\u201d \u201cFrance,\u201d \u201cairline,\u201d and \u201clabor union,\u201d respectively. Moreover, these four topics at layer two are all connected to topic 8 of layer one, which is very specific on \u201coffice meetings.\u201d\nTo understand the relationships and distinctions between different trees, we construct subnetworks as shown in Figures 9-10. Figure 9 clearly shows that all three trees, rooted at nodes 16, 10, and 17 of layer three, respectively, are highly related to topic 3 of layer two on \u201clow & expect\u201d. However, the two trees rooted at node 10 and 17, respectively, both\nhave their own specificities. For example, topic 52 of layer two on \u201cwall street,\u201d is unique to node 10 of layer three, and topic 35 of layer two on \u201cIndia\u201d is unique to node 17 of layer three. Similar phenomena can also be observed from another subnetwork on \u201cChina,\u201d shown in Figure 10, where both nodes of the top layer are connected to topic 19 of layer two on \u201ccorp & techn,\u201d topic 36 on \u201cChina,\u201d and topic 12 on \u201cprofit & expect.\u201d Though related to each other, the tree rooted at node 18 of the top layer is also strongly connected to topic 31 on \u201cproject\u201d and topic 34 on \u201cairline,\u201d whereas the other one focuses differently on topic 49 on \u201ccar & Korea\u201d and topic 44 on \u201cgrowth rate\u201d.\nFigure 14 shows a tree rooted at node 1 of layer three on \u201cmusic & song,\u201d whose topics at layer two are about \u201csong & band\u201d and \u201cmusic, piano, & theatre,\u201d respectively. Figure 15 demonstrates another tree consisting of topic 9 of layer two on \u201cLondon & British,\u201d topic 50 on \u201cchurch & Catholic,\u201d and topic 25 on \u201cking & prince,\u201d which is mainly about \u201cUnited Kingdom.\u201d Given in Figure 16 is another tree on \u201cart & museum,\u201d where the left side is about \u201cart\u201d while the right is on \u201chistory & building.\u201d These trees are all clearly interpretable.\nIn the subnetwork shown in Figure 17, all three trees are related to topic 9 of layer two on \u201cLondon, British, & Sir.\u201d But they focus differently on topic 16 of layer two on \u201cIrish Americans,\u201d topic 24 on \u201clife, birth, education, career, family, & death,\u201d and topic 25 on \u201cking & prince,\u201d respectively. Similar phenomena can also be observed in Figure 18. Both trees are related to topic 52 of layer two on \u201cship\u201d and topic 49 on \u201cair,\u201d but the left one is about various means of transportation and communication while the right one is about various components of \u201cwar.\u201d Figure 19 shows another subnetwork on \u201cteam & race,\u201d where three trees, all include topic 6 of layer two, focus differently on \u201cgoals, clubs, & league,\u201d \u201cworld cup,\u201d and \u201crank, first, second, & third,\u201d respectively."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation,", "citeRegEx": "Amari,? \\Q1998\\E", "shortCiteRegEx": "Amari", "year": 1998}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS, pp", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Stochastic gradient Hamiltonian Monte Carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "In ICML, pp", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Fast simulation of hyperplane-truncated multivariate normal distributions", "author": ["Y. Cong", "B. Chen", "M. Zhou"], "venue": "Bayesian Analysis Advance Publication,", "citeRegEx": "Cong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cong et al\\.", "year": 2017}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"], "venue": "In NIPS,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "JMLR, 12(Jul):2121\u20132159,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Scalable deep Poisson factor analysis for topic modeling", "author": ["Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In ICML,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix", "author": ["R.B. Grosse", "R. Salakhutdinov"], "venue": "In ICML,", "citeRegEx": "Grosse and Salakhutdinov,? \\Q2015\\E", "shortCiteRegEx": "Grosse and Salakhutdinov", "year": 2015}, {"title": "Deep Poisson factor modeling", "author": ["R. Henao", "Z. Gan", "J. Lu", "L. Carin"], "venue": "In NIPS, pp", "citeRegEx": "Henao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henao et al\\.", "year": 2015}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "In NIPS, pp", "citeRegEx": "Hinton and Salakhutdinov,? \\Q2009\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Online learning for latent Dirichlet allocation", "author": ["M.D. Hoffman", "F.R. Bach", "D.M. Blei"], "venue": "In NIPS,", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "In NIPS,", "citeRegEx": "Larochelle and Lauly,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Lauly", "year": 2012}, {"title": "Preconditioned stochastic gradient Langevin dynamics for deep neural networks", "author": ["C. Li", "C. Chen", "D. Carlson", "L. Carin"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Pachinko allocation: DAGstructured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "In ICML, pp", "citeRegEx": "Li and McCallum,? \\Q2006\\E", "shortCiteRegEx": "Li and McCallum", "year": 2006}, {"title": "A complete recipe for stochastic gradient MCMC", "author": ["Y. Ma", "T. Chen", "E. Fox"], "venue": "In NIPS, pp", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Neural variational inference for text processing", "author": ["Y. Miao", "L. Yu", "P. Blunsom"], "venue": "In ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Sparse stochastic inference for latent Dirichlet allocation", "author": ["D. Mimno", "M.D. Hoffman", "D.M. Blei"], "venue": "In ICML, pp", "citeRegEx": "Mimno et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2012}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": null, "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "MCMC using Hamiltonian dynamics", "author": ["Neal", "R. M"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Neal and M,? \\Q2011\\E", "shortCiteRegEx": "Neal and M", "year": 2011}, {"title": "The discrete infinite logistic normal distribution for mixed-membership modeling", "author": ["J. Paisley", "C. Wang", "D. Blei"], "venue": "In AISTATS,", "citeRegEx": "Paisley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2011}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu and Bengio,? \\Q2013\\E", "shortCiteRegEx": "Pascanu and Bengio", "year": 2013}, {"title": "Stochastic gradient Riemannian Langevin dynamics on the probability simplex", "author": ["S. Patterson", "Y.W. Teh"], "venue": "In NIPS, pp", "citeRegEx": "Patterson and Teh,? \\Q2013\\E", "shortCiteRegEx": "Patterson and Teh", "year": 2013}, {"title": "A Bayesian nonparametric approach to image superresolution", "author": ["G. Polatkan", "M. Zhou", "L. Carin", "D. Blei", "I. Daubechies"], "venue": null, "citeRegEx": "Polatkan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Polatkan et al\\.", "year": 2015}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Deep exponential families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Poisson\u2013gamma dynamical systems", "author": ["A. Schein", "M. Zhou", "H. Wallach"], "venue": "In NIPS,", "citeRegEx": "Schein et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schein et al\\.", "year": 2016}, {"title": "Stochastic quasi-Newton Langevin Monte Carlo", "author": ["U. Simsekli", "R. Badeau", "A.T. Cemgil", "Richard"], "venue": "In ICML,", "citeRegEx": "Simsekli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Simsekli et al\\.", "year": 2016}, {"title": "Learning sigmoid belief networks via Monte Carlo expectation maximization", "author": ["Z. Song", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Modeling documents with deep Boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov", "G.E. Hinton"], "venue": "In UAI,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Evaluation methods for topic models", "author": ["H.M. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno"], "venue": "In ICML,", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Teh", "Y.-W"], "venue": "In ICML, pp", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "The IBP compound Dirichlet process and its application to focused topic modeling", "author": ["S. Williamson", "C. Wang", "K.A. Heller", "D.M. Blei"], "venue": "In ICML,", "citeRegEx": "Williamson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2010}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": null, "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}, {"title": "Negative binomial process count and mixture modeling", "author": ["M. Zhou", "L. Carin"], "venue": "PAMI, 37(2):307\u2013320,", "citeRegEx": "Zhou and Carin,? \\Q2015\\E", "shortCiteRegEx": "Zhou and Carin", "year": 2015}, {"title": "Betanegative binomial process and Poisson factor analysis", "author": ["M. Zhou", "L. Hannah", "D.B. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Augmentable gamma belief networks", "author": ["M. Zhou", "Y. Cong", "B. Chen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Priors for random count matrices derived from a family of negative binomial processes", "author": ["M. Zhou", "O. Padilla", "J.G. Scott"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "1/Kl for the lth layer, on extracting multilayer representations/topics from both the RCV1 and Wiki corpora", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2016\\E", "shortCiteRegEx": "Zhou", "year": 2016}, {"title": "2016a), we plot 54 example topics of layer one in Figure 4, the top 30 topics of layer two in Figure 5, and the top 30 topics of layer three", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2016\\E", "shortCiteRegEx": "Zhou", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "capacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo", "startOffset": 100, "endOffset": 211}, {"referenceID": 1, "context": "capacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo", "startOffset": 100, "endOffset": 211}, {"referenceID": 33, "context": "capacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo", "startOffset": 100, "endOffset": 211}, {"referenceID": 27, "context": "capacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo", "startOffset": 100, "endOffset": 211}, {"referenceID": 17, "context": "(SG-MCMC) that provides posterior samples in a non-batch learning setting (Welling & Teh, 2011; Patterson & Teh, 2013; Ma et al., 2015).", "startOffset": 74, "endOffset": 135}, {"referenceID": 10, "context": "such as deep belief network (DBN) (Hinton et al., 2006)", "startOffset": 34, "endOffset": 55}, {"referenceID": 1, "context": "manner (Bengio et al., 2007).", "startOffset": 7, "endOffset": 28}, {"referenceID": 17, "context": "Following a general framework for SG-MCMC (Ma et al., 2015), the block diagonal structure of the FIM of DLDA makes it be easily", "startOffset": 42, "endOffset": 59}, {"referenceID": 0, "context": "inverted to precondition the mini-batch based noisy gradients to exploit the second-order local curvature information, leading to topic-layer-adaptive step sizes based on the Riemannian manifold and the same asymptotic performance as a natural gradient based batch-learning algorithm (Amari, 1998; Pascanu & Bengio, 2013).", "startOffset": 284, "endOffset": 321}, {"referenceID": 17, "context": "work (Ma et al., 2015), we present topic-layer-adaptive", "startOffset": 5, "endOffset": 22}, {"referenceID": 43, "context": "of Zhou & Carin (2015) with a greedy layer-wise training", "startOffset": 3, "endOffset": 23}, {"referenceID": 40, "context": "Extensive experiments in Zhou et al. (2016a) show", "startOffset": 25, "endOffset": 45}, {"referenceID": 17, "context": "As in Theorem 1 of Ma et al. (2015), p (z |X ) is the stationary distribution of the dynamics defined by the stochastic differential equation (SDE) dz = f (z) dt + \u221a 2D (z)dW (t), if the deterministic drift f (z) is restricted to the form", "startOffset": 19, "endOffset": 36}, {"referenceID": 17, "context": "As shown in Ma et al. (2015), stochastic gradient Riemannian Langevin dynamics (SGRLD) of Patterson & Teh (2013) is a special case with D (z) = G(z)\u22121,Q (z) = 0, B\u0302t = 0, where G (z) denotes the Fisher information matrix (FIM).", "startOffset": 12, "endOffset": 29}, {"referenceID": 17, "context": "As shown in Ma et al. (2015), stochastic gradient Riemannian Langevin dynamics (SGRLD) of Patterson & Teh (2013) is a special case with D (z) = G(z)\u22121,Q (z) = 0, B\u0302t = 0, where G (z) denotes the Fisher information matrix (FIM).", "startOffset": 12, "endOffset": 113}, {"referenceID": 40, "context": "by letting K \u2192\u221e (Zhou et al., 2016b). When L = 1, the PGBN whose (rk,\u03c6k) are the points of a gamma process reduces to the gamma-negative binomial process PFA of Zhou & Carin (2015), whose alternative representation is provided in Corollary D.", "startOffset": 17, "endOffset": 181}, {"referenceID": 30, "context": "re-express the PGBN as DLDA is related to how Schein et al. (2016) re-express their Poisson\u2013gamma dynamic systems", "startOffset": 46, "endOffset": 67}, {"referenceID": 1, "context": "a greedy layer-wise manner (Bengio et al., 2007), which", "startOffset": 27, "endOffset": 48}, {"referenceID": 2, "context": "(2015), including as special examples stochastic gradient Hamiltonian Monte Carlo in Chen et al. (2014) and stochastic gradient thermostats in Ding et al.", "startOffset": 85, "endOffset": 104}, {"referenceID": 2, "context": "(2015), including as special examples stochastic gradient Hamiltonian Monte Carlo in Chen et al. (2014) and stochastic gradient thermostats in Ding et al. (2014), may", "startOffset": 85, "endOffset": 162}, {"referenceID": 3, "context": "using Theorem 2 of Cong et al. (2017), the special struc-", "startOffset": 19, "endOffset": 38}, {"referenceID": 3, "context": "can be found in Examples 1-3 of Cong et al. (2017).", "startOffset": 32, "endOffset": 51}, {"referenceID": 25, "context": "We update them using annealed weighting (Polatkan et al., 2015) as", "startOffset": 40, "endOffset": 63}, {"referenceID": 40, "context": "analysis (PFA) (Zhou et al., 2012) are equipped with scal-", "startOffset": 15, "endOffset": 34}, {"referenceID": 11, "context": "(Hoffman et al., 2010; Mimno et al., 2012) and SGRLD", "startOffset": 0, "endOffset": 42}, {"referenceID": 19, "context": "(Hoffman et al., 2010; Mimno et al., 2012) and SGRLD", "startOffset": 0, "endOffset": 42}, {"referenceID": 8, "context": "(2015) and Henao et al. (2015) are proposed to generalize", "startOffset": 11, "endOffset": 31}, {"referenceID": 26, "context": "Ranganath et al. (2015) propose deep exponential family (DEF), which differs from the PGBN in connecting adjacent", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "layers via the gamma rate parameters and using black-box variational inference (BBVI) (Ranganath et al., 2014).", "startOffset": 86, "endOffset": 110}, {"referenceID": 10, "context": "Some commonly used neural networks, such as deep belief network (DBN) (Hinton et al., 2006) and deep Boltzmann machines (DBM) (Salakhutdinov & Hinton, 2009), have also been modified for text analysis (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al.", "startOffset": 70, "endOffset": 91}, {"referenceID": 33, "context": ", 2006) and deep Boltzmann machines (DBM) (Salakhutdinov & Hinton, 2009), have also been modified for text analysis (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al., 2013).", "startOffset": 116, "endOffset": 197}, {"referenceID": 38, "context": ", 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba,", "startOffset": 18, "endOffset": 32}, {"referenceID": 15, "context": "ple, Li et al. (2016) show that preconditioning the gradients", "startOffset": 5, "endOffset": 22}, {"referenceID": 11, "context": "Wiki consists of 10 million documents randomly downloaded from Wikipedia using the scripts provided in Hoffman et al. (2010); as in Hoffman", "startOffset": 103, "endOffset": 125}, {"referenceID": 6, "context": "(2010), Gan et al. (2015), and Henao et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 6, "context": "(2010), Gan et al. (2015), and Henao et al. (2015), we use a vocabulary with 7,702 words and randomly select", "startOffset": 8, "endOffset": 51}, {"referenceID": 6, "context": "To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al.", "startOffset": 120, "endOffset": 138}, {"referenceID": 6, "context": "To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al. (2015). To be consistent with the settings of Gan et al.", "startOffset": 120, "endOffset": 162}, {"referenceID": 6, "context": "To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al. (2015). To be consistent with the settings of Gan et al. (2015) and Henao et al.", "startOffset": 120, "endOffset": 219}, {"referenceID": 6, "context": "To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al. (2015). To be consistent with the settings of Gan et al. (2015) and Henao et al. (2015), no precautions are taken in the scripts for Wikipedia to prevent a testing document from", "startOffset": 120, "endOffset": 243}, {"referenceID": 34, "context": ", in Wallach et al. (2009), Paisley et al.", "startOffset": 5, "endOffset": 27}, {"referenceID": 22, "context": "(2009), Paisley et al. (2011), and Zhou & Carin (2015).", "startOffset": 8, "endOffset": 30}, {"referenceID": 22, "context": "(2009), Paisley et al. (2011), and Zhou & Carin (2015). Although a good measure", "startOffset": 8, "endOffset": 55}, {"referenceID": 37, "context": "cused topic model (FTM) of Williamson et al. (2010), repli-", "startOffset": 27, "endOffset": 52}, {"referenceID": 6, "context": "(2015), DPFA of Gan et al. (2015), and DPFM of", "startOffset": 16, "endOffset": 34}, {"referenceID": 6, "context": "results are taken from Gan et al. (2015) and Henao et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 40, "context": "Note \u03b7 and Kl are set similar to that of DPFM for fair comparisons, while other hyperparameters follow Zhou et al. (2016a).", "startOffset": 103, "endOffset": 123}, {"referenceID": 8, "context": "(2015) and Henao et al. (2015). Note that for Wiki, DPFM", "startOffset": 11, "endOffset": 31}, {"referenceID": 40, "context": "4) Gibbs: the upward-downward Gibbs sampler in Zhou et al. (2016a).", "startOffset": 47, "endOffset": 67}], "year": 2017, "abstractText": "It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets.", "creator": "LaTeX with hyperref package"}}}