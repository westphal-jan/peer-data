{"id": "1706.01863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Marmara Turkish Coreference Corpus and Coreference Resolution Baseline", "abstract": "we informally describe the marmara turkish compound coreference corpus, which is an annotation of organizing the whole metu - sabanci turkish treebank with mentions and coreference chains. collecting typically nine or more independent annotations for each query document allowed for fully successful automatic adjudication. we therefore provide a baseline system for turkish mention detection and coreference resolution and evaluate it on the corpus.", "histories": [["v1", "Tue, 6 Jun 2017 17:25:36 GMT  (49kb,D)", "http://arxiv.org/abs/1706.01863v1", "Submitted to Natural Language Engineering"]], "COMMENTS": "Submitted to Natural Language Engineering", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["peter sch\\\"uller", "k\\\"ubra c{\\i}ng{\\i}ll{\\i}", "ferit tun\\c{c}er", "bar{\\i}\\c{s} g\\\"un s\\\"urmeli", "ay\\c{s}eg\\\"ul pekel", "ay\\c{s}e hande karatay", "hacer ezgi karaka\\c{s}"], "accepted": false, "id": "1706.01863"}, "pdf": {"name": "1706.01863.pdf", "metadata": {"source": "CRF", "title": "Marmara Turkish Coreference Corpus and Coreference Resolution Baseline", "authors": ["Peter Sch\u00fcller", "K\u00fcbra Cingilli", "Ferit Tun\u00e7er", "Bar\u0131\u015f G\u00fcn S\u00fcrmeli", "Ay\u015feg\u00fcl Pekel", "Ay\u015fe Hande Karatay", "Hacer Ezgi Karaka\u015f"], "emails": ["peter.schuller@marmara.edu.tr", "kubracingilli@gmail.com", "ferit@cryptolab.net", "barisgunsurmeli@gmail.com", "aysegulpekel@gmail.com", "handekaratay7@gmail.com", "hezgikarakas@sabanciuniv.edu"], "sections": [{"heading": "1 Introduction", "text": "Coreference Resolution is the task of identifying groups of phrases in a text that refer to the same discourse entity. Such referring phrases are called mentions, a set of mentions that all refer to the same discourse entity is called a chain. Annotated corpora are important resources for developing and evaluating automatic coreference resolution methods.\nTurkish is an agglutinative language and Turkish coreference resolution poses several challenges different from many other languages, in particular the absence of grammatical gender, the possibility of null pronouns in subject and object position, possessive pronouns that can be expressed as suffixes, and ambiguities among possessive and number morphemes, e.g., \u2018\u00e7ocuklar\u0131\u2019 can be analyzed as \u2018their children\u2019 or as \u2018his/her children\u2019, depending on context (Oflazer and Boz\u015fahin, 1994).\nNo coreference resolution corpus exists for Turkish so far. We here describe the result of an effort to create such a corpus based on the METU-Sabanci Turkish Treebank (Atalay et al., 2003; Oflazer\nar X\niv :1\n70 6.\n01 86\net al., 2003; Say et al., 2004), which is, to the best of our knowledge, the only publicly available Turkish Treebank.\nOur contributions are as follows.\n\u2022 We describe two stages of annotation: in Phase I, annotators created mentions and chains, which did not yield sufficient inter-annotator agreement. In Phase II, mentions were given to annotators who created only chains. We collected on average more than ten independent annotations per document for each document in the METU-Sabanci Turkish Treebank.\n\u2022 We describe annotator profiles and adjudication, which was done semi-automatically in Phase I and fully automatically in Phase II. We describe the principles of our automatic adjudication tool which uses a voting-like approach. Such an automatic approach is possible because we collected enough (on average 10) annotations per document.\n\u2022 We describe the XML format used to address documents, sentences, and tokens in the METUSabanci Turkish Treebank. We provide a public version of the corpus as XML, including tools to convert the corpus to CoNLL format. (For licensing reasons we cannot re-publish the Turkish Treebank data.)\n\u2022 We describe and provide a baseline method for mention detection and coreference resolution, compatible with the format of the corpus. We evaluate this baseline method on the corpus with leave-one-out cross-validation.\nSection 2 gives preliminaries of Coreference Resolution and the Turkish language and describes related work. Section 3 explains the annotation and adjudication process and discusses properties of the corpus, annotator profiles, and supporting tools. Section 4 describes the baseline system and its evaluation on the corpus. Section 5 concludes and gives an outlook on future work.\nWe provide the Marmara Turkish Coreference Corpus, tools, and the baseline system, at https://bitbucket.org/knowlp/marmara-turkish-coreference-corpus ."}, {"heading": "2 Preliminaries and Related Work", "text": "Next we give background and related work on coreference resolution, the Turkish language, and specific challenges of coreference resolution in Turkish."}, {"heading": "2.1 Coreference Resolution", "text": "Coreference resolution is the task of marking noun phrases that refer to the same discourse entity as coreferent. Mention detection, which identifies such noun phrases, is usually included in that task. Coreference resolution is not limited to resolving pronouns: in computer linguistics it was first introduced as a benchmark for deep semantic understanding of text in the message understanding conference series (Grishman and Sundheim, 1995) and later continued in the Automatic Content Extraction (ACE) program (Doddington et al., 2004) which required English coreference resolution as foundation for all tasks of years 2000\u20132004. The SemEval competition series followed ACE and featured the first multilingual coreference resolution challenge in 2010 (Recasens et al., 2010). Connected to that is the freely available, large, and multilingual OntoNotes (Hovy et al., 2006) corpus, which was used in the multilingual coreference task in SemEval 2012 (Pradhan et al., 2012) and contains coreference annotations for English, Arabic, and Mandarin (Pradhan et al., 2007).\nCoreference resolution has been surveyed by Ng (2010). Approaches are manifold and include machine-learning-based approaches, rule-based systems, and combinations of both. In most machinelearning approaches, equivalence relations of chains (sometimes called clusters) are assembled from predictions of the relatedness of pairs of mentions (mention-mention links). The earliest machine-learning approach is due to Soon et al. (2001), methods for building chains from link predictions include local greedy heuristics as done by Bengtson and Roth (2008) or Stoyanov and Eisner (2012), global optimization formulations such as relaxation labeling (Sapena et al., 2012) or ranking with ILP or Markov Logic (Culotta et al., 2007; Denis and Baldridge, 2009), and representations of trees of mention-mention links (Chang et al., 2013; Fernandes et al., 2012). The first rule-based algorithm for anaphora resolution\nwas done by Hobbs (1978). More recent rule-based systems merge chains based on several sets of rules in a multi-stage filtering approach (Lee et al., 2013), moreover there are hybrid systems combining rules and machine learning such as the one by Chen and Ng (2012). Other approaches use curated or distributed knowledge sources such as WordNet, Google distance, and Wikipedia (Poesio et al., 2004; Zheng et al., 2013).\nNote that anaphora resolution (Hirst, 1981; Mitkov, 2002) is a problem orthogonal to coreference resolution (van Deemter and Kibble, 2000), because anaphora resolution focuses on referring expressions that point to previous expressions in the text. Cataphora (pointing to later occurrences in the text) are exluded. On the other hand, different from most works on coreference, anaphora resolution includes bound pronouns that do not refer to concrete entities because they are quantified using, e.g., \u2018some\u2019 or \u2018none\u2019."}, {"heading": "2.2 Turkish", "text": "Turkish is a member of the family of Altaic languages, it is an agglutinative language where suffixes are attached to a root word. Derivational and inflectional suffixes are very productive (Oflazer, 1993; Oflazer et al., 1994) and are subject to vowel harmony from the root word. Morphological analysis is challenging due to ambiguities between different types of suffixes, for example \u2018izin\u2019 can mean \u2018your trace\u2019 (iz+P2Sg+Nom), \u2018trace\u2019 (iz+Pnon+Gen), or \u2018permission\u2019 (izin+Pnon+Nom) (Hakkani-T\u00fcr et al., 2002).\nThe METU-Sabanci Turkish Treebank (in the following just called Turkish Treebank) (Atalay et al., 2003; Oflazer et al., 2003; Say et al., 2004) contains a subset of the METU Turkish Corpus (Say et al., 2004) in tokenized form. Each token is analyzed morphologically and split into inflectional groups (IGs). Sentences are annotated with dependency parse information, where dependencies point to specific IGs within tokens. The Turkish Treebank splits tokens into IGs on derivational boundaries, for example, \u2018evimdekiler\u2019 (those in my house) is analyzed (Oflazer et al., 2003) as\nev+Noun+A3sg+P1sg+Loc^DB+Adj^DB+Noun+Zero+A3pl+Pnon+Nom where ^DB indicates derivation boundaries and the token consists of three IGs \u2018evimde\u2019 (in my house), \u2018ki\u2019 (adjectivization), and \u2018ler\u2019 (nominalization+plural). A CoNLL format that provides a CoNLL token corresponding to each IG of a token has been created for Turkish dependency parsing (Buchholz and Marsi, 2006).\nNamed entities in the Turkish Treebank are not marked specially, but multi-word named entities are represented as single tokens.\nTurkish Coreference Resolution. The following properties of Turkish are in particular relevant for coreference resolution. In the above example, \u2018those in my house\u2019 as well as \u2018my house\u2019 as well as \u2018my\u2019 could be coreferent with mentions in the document. However, neither \u2018my house\u2019 nor \u2018my\u2019 is available as a separate unit of analysis: both are parts of the first IG (\u2018evimde\u2019). Gender is not marked in Turkish with the exception of honorifics \u2018Bey\u2019 and \u2018Han\u0131m\u2019 which corresponds to English \u2018Mr\u2019 and \u2018Mrs\u2019. Moreover several common first names apply to both genders. Hence gender-based syntactic compatibility checks for mentions are only possible in some cases. Personal pronoun subjects in Turkish are usually realized as suffixes of the verb, e.g., \u2018gidiyoruz\u2019 (we are going) and \u2018gidiyorlar\u2019 (they are going) but they can also be realized explicitly as in \u2018biz gidiyoruz\u2019, depending on discourse conditions (Turan, 1996). Suffixes of proper nouns in written Turkish are systematically separated from the proper nouns using a single quote, e.g., \u2018T\u00fcrkiye\u2019den\u2019 (from Turkey) and \u2018T\u00fcrkiye\u2019deki\u2019 (the thing in Turkey). This systematic rule simplifies finding equal proper noun mentions in coreference resolution for Turkish.\nMost works about referring expressions in Turkish focus on anaphora resolution and not on full coreference resolution. One exception is the work of K\u00fc\u00e7\u00fck and Yaz\u0131c\u0131 (2008) on political news texts extracted from videos: they focus on Gazetteers for extracting mentions (without considering general NPs or syntactic information), provide a rule-based heuristic based on recency for creating coreference chains, and evaluate their approach on three documents (which are not part of the Turkish Treebank).\nIn the following we describe work on Turkish anaphora resolution, which is related to coreference resolution.\nTurkish Anaphora Resolution. Erkan and Akman (1998) describe an implementation of pronoun anaphora resolution in a framework for situation theory which is based on knowledge representation and logical reasoning. Hobb\u2019s na\u00efve pronoun resolution algorithm (Hobbs, 1978) was realized for Turkish and tested on 10 toy sentences (T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan, 2007).\nCentering theory (Grosz et al., 1995) is the foundation of several works on Turkish pronouns. Turan (1996) performed a study about discourse conditions for referring vs. nonreferring expressions and null vs. overt pronouns, and evaluated the theory on 2500 annotated tokens. Y\u00fcksel and Bozsahin (2002) created a system for generating referring expressions that was tested on a machine translation task. Furthermore, there is a theoretical model of anaphora resolution based on centering theory by Y\u0131ld\u0131r\u0131m et al. (2004).\nK\u00fc\u00e7\u00fck and Y\u00f6ndem (2007) described a system for finding and resolving Turkish pronominal anaphora and annotated 12266 anaphor candidate instances in the METU Turkish Corpus to evaluate their candidate extractor and decision tree learner for anaphora resolution. K\u0131l\u0131\u00e7aslan et al. (2009) performed a comprehensive study on pronoun resolution and evaluated various machine learning methods for resolving overt and null pronouns in a corpus of 20 children stories."}, {"heading": "3 Marmara Turkish Coreference Corpus", "text": "We next describe the annotation and adjudication process including formal adjudication criteria, key properties of the resulting corpus, and supporting tools."}, {"heading": "3.1 Annotation Process", "text": "Figure 1 visualizes the process that led to the final corpus. Annotations were collected in two phases: Phase I took place in October\u2013December 2015 and Phase II during October\u2013December 2016.\nAnnotations were collected from computer engineering students participating in a lecture on natural language processing, after educating them in basic linguistic analysis and coreference resolution. To achieve reasonable annotation quality, we aimed to keep annotation simple and therefore based it on few principles and examples. We designed an annotation manual (S\u00fcrmeli et al., 2016) for marking coreference according to the following principles:\n\u2022 all concrete entities that are mentioned more than once shall be annotated,\n\u2022 mentions shall be marked as the biggest possible span of tokens that describes the entity,\n\u2022 lists shall not be annotated (elements of lists can be annotated), and\n\u2022 predications shall not be annotated.\nNote that, by marking mentions as the biggest spans, mentions and potentially available appositives are annotated as single mentions, which is different from OntoNotes where appositives are a special type of coreference annotation. We do not mark predications because they are a different type of coreference as argued by van Deemter and Kibble (2000).\nIn Phase I, annotations were created by 19 annotators with the GATE (Cunningham et al., 2013; Gaizauskas et al., 1996) coreference annotation tool. This yielded on average 6.5 annotations per document for 21 documents in the Treebank. Adjudication of these documents was done semi-automatically (see Sections 3.2 and 3.4). However, due to low inter-annotator agreement about mention boundaries, decisions often depended on the adjudicator. Therefore, in order to make the setting simpler, we decided to collect more annotations with given mentions. We used the list of mentions resulting from the adjudicated documents of Phase I. Mentions for those 12 documents that were not annotated in Phase I were manually created in a collaboration of two annotators for each document.\nIn Phase II, 46 annotators were given CoNLL files with token and coreference columns where each mention was given in its own chain. Annotators created files with equalities between chain IDs and uploaded these files to a web service where they were checked for syntactical correctness. The submission file format is described in (S\u00fcrmeli et al., 2016). Phase II yielded 339 individual annotations of sufficient inter-annotator agreement to perform fully automatic adjudication (see next section).\nThis method of collecting annotation as text files might seem archaic, however, in practice, annotators were more comfortable with such a system than with the graphical user interface of GATE in Phase I. We were not able to use the BRAT (Stenetorp et al., 2012) annotation tool, because of difficulties representing sentence and word addresses in a way that they can be extracted from annotations."}, {"heading": "3.2 Adjudication", "text": "Table 1 shows key properties of the annotated corpus including inter-annotator agreement. Statistics are accumulated per genre as well as over the whole corpus. Coreference IAA scores IAA1 and IAA2 are calculated as described in (Passonneau, 2004) and (Passonneau et al., 2006), respectively. IAA1 is an adaption of Krippendorff \u03b1 (Krippendorff, 1980) where a pair of chains gets a score of 1 for a perfect match, 23 if one chain is a subset of the other one, 1 3 if they are intersecting, and 0 otherwise. (Krippendorff \u03b1 assigns 1 for a perfect match and 0 in all other cases.) IAA2 uses Jaccard distance instead of fixed scores for the above cases, which provides more realistic results if chains have heterogeneous sizes. (This is often the case in coreference corpora.) Over all documents, IAA1 is 76% and IAA2 is 90%. We observe\nworse IAA for genres that are based on writing as an art form, i.e., for short stories, novels, and the Other genre (a first-person narrative).\nThe amount of annotations per document, combined with the observed IAA, allows us to automatically adjudicate the corpus. This is different from other coreference annotations, in particular in OntoNotes, where two annotators created annotations followed by adjudication done by a single human expert (Weischedel et al., 2012). Automatic adjudication is based on combinatorial optimization, where we search for a solution of chains that has overall minimal divergence from all annotator inputs. Divergence is measured in terms of mention-mention links given and omitted by annotators.\nFormally, given a set M of mentions in a document, a chain is a subset of these mentions, and k annotators produce annotations (sets of chains that are nonintersecting) A1, . . . , Ak over M . A solution G is also a set of chains over M and we search for G such that the following objective becomes minimal:\u2211\nm,m\u2032 \u2208M i\u2208{1,...,k}\n2 \u00b7 a(m,m\u2032, Ai) \u00b7 na(m,m\u2032, G) + na(m,m\u2032, Ai) \u00b7 a(m,m\u2032, G) (1)\nwhere a(\u00b7 \u00b7 \u00b7 ) is an indicator function for mentions in the same chain within an annotation A, formally\na(m,m\u2032, A) = { 1 if there is a chain C \u2208A with {m,m\u2032}\u2286C 0 otherwise\nand na(\u00b7 \u00b7 \u00b7 ) is an indicator for mention pairs that are not in the same chain in an annotation A: na(m1,m2, A) = 1\u2212 a(m1,m2, A).\nObjective (1) incurs a cost of 2j for each mention pair that is not in the same chain in the solution G contrary to the opinion of j annotators. Moreover, (1) incurs a cost of l for each mention-pair that is in the same chain in G contrary to the opinion of l annotators. This makes optimal solutions ignore as little as possible information from annotators. The coefficient 2 has the effect, that not annotating a mention to be in some chain has less weight than annotating a mention as part of some chain. We introduced this preference into the objective, because not annotating some mention can be due to an oversight, while putting a mention into a chain is more likely done intentionally.\nNote that, if annotators produce different sets of mentions, we can build M from the union of all mentions produced by annotators with unused mentions becoming singleton partitions. We describe practical issues of our adjudication tool in Section 3.4."}, {"heading": "3.3 Corpus Properties and Annotator Profiles", "text": "Table 1 shows key properties of the annotated corpus, accumulated over genres and overall figures. Average number of tokens and mentions per genre varies a lot. In particular, the Essay genre contains texts discussing abstract concepts like \u2018home\u2019 and \u2018science\u2019 which are not annotated. The narrative in genre Other contains many person names which are repeatedly mentioned.\nColumn Ph1 indicates how many of the documents were annotated in both phases of the annotation process. For example the News genre contains 9 documents. Mentions of 2 News documents were obtained from Phase I, the others from Phase II (see Figure 1). We observe better IAA values for Phase II than for Phase I (not shown in table). This might be due to the high ratio of documents from genre News (7 out of 12) in Phase II, because we observed that IAA for News is higher than in other genres, independent from the annotation phase.\nBy comparing column CM (given mentions) and AM (annotated mentions) we see that annotators rarely use all mentions in the annotated chains. To reduce the chance that these mentions were omitted due to an oversight, the annotation submission system indicated which mentions were left unused.\nAnnotator Profiles. Anonymized learner profiles were collected from all students in Phase II (written permission for using and publishing the data was also obtained). Learner profiles include age, gender, native language, languages spoken at home, in primary and secondary school, foreign language knowledge, and the number of years the annotator spent in Turkish-speaking communities.\nAnnotators are on average 23 years old university students at Marmara University in Istanbul. Annotations were done after basic introduction to linguistics and coreference resolution and after discussing the annotation manual (S\u00fcrmeli et al., 2016).\nOut of 46 annotators, 29 are male and 17 are female. One annotator indicated Azerbaijani as a native language, all others indicated Turkish as one of their native languages. (Azerbaijani is close to Turkish.) Two annotators indicated Kurdish as further native language, and one each Arabic, English, and Macedonian. Primary and secondary school education was Turkish for 43 annotators, English for two and Azerbaijani for one. Moreover 43 annotators lived at least 20 years in predominantly Turkishspeaking communities, the remaining annotators answered 4, 5, and 14 years, respectively, for this question.\nAccording to this data we consider our annotators to be capable of understanding and annotating the texts in the corpus."}, {"heading": "3.4 Tools", "text": "For creating this corpus, we built several tools.\nDocument Extractor. The METU-Sabanci Turkish Treebank contains 1960 text fragments, distributed over 33 documents. Most documents are split over several XML files, however there is also one XML file containing two distinct documents. We provide a tool for extracting documents from the Turkish Treebank and store each document in a single XML file. The Turkish Treebank is licensed in a way that it cannot be redistributed with the Marmara Turkish Coreference Corpus, therefore the tool generates document files from a directory containing the unpacked Turkish Treebank. Our tool not only creates one XML file for each document, it also recodes all data to UTF-8 and fixes problematic (non-encoded) attributes that are present in the original corpus.\nCoreference XML Format. For representing coreference information we created an XML format that contains pointers to sentence and word IDs into documents extracted from the Turkish Treebank. A sample of such an XML file with two mentions and one chain is as follows.\n<coref> <mentions>\n<mention fromWordIX=\"1\" id=\"0\" sentenceNo=\"00016112313.1\" toWordIX=\"1\"> Prof._Dr._Semih_Koray\u2019\u0131n </mention> <mention fromWordIX=\"1\" id=\"2\" sentenceNo=\"00016112313.2\" toWordIX=\"1\"> Koray\n</mention> </mentions> <chains>\n<chain> <mention mentionId=\"0\">Prof._Dr._Semih_Koray\u2019\u0131n</mention> <mention mentionId=\"2\">Koray</mention>\n</chain> </chains>\n</coref>\nIn this example, \u2018Prof._Dr._Semih_Koray\u2019\u0131n\u2019 is a mention with ID 0 containing token 1 (called \u2018word\u2019 in the Treebank) in sentence \u201800016112313.1\u2019 of the document assembled from the Treebank. Moreover there is a chain containing that mention and another mention in the first token of sentence \u201800016112313.2\u2019.\nNote that the actual text within mentions is only for readability purposes, the information about mention content is fully represented in attributes.\nCoNLL \u21d4 XML Converters. As the CoNLL reference coreference scorer (Pradhan et al., 2014) is based on CoNLL format, we provide tools for converting a document and a coreference XML file into a CoNLL file (and vice versa). CoNLL format is also more readable than XML Format for humans. We use XML to be consistent with the Turkish Treebank and because the Treebank license prevents redistribution.\n(Semi-)automatic coreference adjudication tool. Merging several distinct coreference annotations into a single gold standard is a complex task, in particular if annotators do not agree on mentions. To simplify this task we created a tool that merges multiple annotations into a single solution according to objective (1) from Section 3.2. Manual intervention for editing mentions and chains is also possible, details about a preliminary (Phase I) version of the tool is described by Sch\u00fcller (2016). Note that, in Phase II we performed only automatic adjudication and did not need manual intervention.\nFor the purpose of this project, it was sufficient to use our tool directly on CoNLL files without a GUI. In the future, to make the tool accessible to a wider part of the community, we consider integrating it into an existing software, for example BART (Broscheit et al., 2010)."}, {"heading": "4 Baseline", "text": "We have created a baseline for mention detection, based on the work of Sapena et al. (2012), and for coreference resolution, inspired by Bengtson and Roth (2008). The baseline is based on Python and scikit-learn (Pedregosa et al., 2011). We considered to integrate also the Named Entity Recognition (NER) module of the ITU-pipeline (Eryi\u011fit, 2014) because NER is not annotated in the Turkish Treebank, however we found that the output the web service changed significantly several times during the development of the baseline. To facilitate reproducibility of results we decided to create a stand-alone baseline that allows reproducing our results using only the METU-Sabanci Turkish Treebank (Say et al., 2004) and scikit-learn.\nMention Detection. Our Mention Detection baseline marks all\n(i) noun phrases,\n(ii) pronouns, and\n(iii) capitalized common nouns or proper names that occur two or more times in the document\nas mentions, similar to the approach of Sapena et al. (2012) for English.\nCoreference Resolution. Our Coreference Resolution baseline is inspired by the work of Bengtson and Roth (2008) who describe a simple method with reasonable accuracy and without the need for custom machine learning models and algorithms that are specific to coreference resolution.\nAs input the baseline uses a set of candidate mentions (either gold or predicted), furthermore lemma and dependency parsing information for obtaining mention heads, as available in the Turkish Treebank. The type of a mention is marked as pronoun if the lemma of the head is one of ben, sen, biz, siz, bu, \u015fu, o, bura, \u015fura, ora, kendi, birbiri. To separate proper noun from noun phrase mention types, we realized our own heuristic which (i) collects all uppercase tokens not at sentence-initial position, (ii) strips case markers, and (iii) uses the resulting set of strings to mark all (including sentence-initial) tokens as proper nouns. All remaining mentions obtain the type noun phrase.\nBased on mention types and head information we create the following features for each mentionmention pair (m1,m2):\n(i) type of m1 and type of m2 (2 features),\n(ii) both mentions are pronouns, proper nouns, or noun phrases (3 features),\n(iii) heads of m1 and m2 match, and the same for head lemmas (2 features),\n(iv) last part of name is equal in m1 and m2, and\n(v) m1 is an acronym of m2,\n(vi) head of m1 is a substring of head of m2, and the same for head lemmas (2 features),\n(vii) m1 is an acronym of m2.\nFeatures (v)\u2013(vii) are asymmetric, that means exchanging m1 and m2 can change the feature value. For these features we also add the respective reverse direction feature, as well as the disjunction of features of both directions. Moreover we add all possible pairs of features (i)\u2013(ii) and (iii)\u2013(vii) to allow the machine learning to give separate weight to features (iii)\u2013(vii) per mention type.\nWe implemented two machine learning methods for predicting coreference based on classification (SVC) and regression (SVR).\nSVC is based on classification with a linear-kernel SVM (Corinna Cortes and Vapnik, 1995). Positive examples are mentions and their closest predecessors within all chains, while negative examples are all mention pairs that are not in the same chain with less than 100 mentions distance. For predicting chains, we first generate candidate mention pairs for all mentions except for noun phrases with pronoun predecessor, then we predict whether they are in the same chain using the SVM. Finally, each mention starts in its own chain and we go through mentions from the beginning of the document to the end, and merge mentions to (potentially several) previous chains for all predicted mention-mention links. We prevent merges that lead to chains with overlapping mentions.\nSVR is based on support vector regression with a linear-kernel SVM (Drucker et al., 1997) trained on the same examples as SVC. For prediction we generate the same candidate mentions as in SVC. For building chains we also start with one chain per mention, but this time we use the Best-Link (Bengtson and Roth, 2008) strategy: we iterate over mentions in order of occurrence in the document, and merge each mention with at most one predecessor chain if its highest-scored candidate link to a predecessor mention is above 0.1 and if the resulting chain does not contain overlapping mentions.\nIn addition to the above, when predicting coreference on predicted mentions, we include incorrect mentions predicted on the training documents to generate negative examples. We randomly sample at most as many incorrect mentions as already contained in the gold annotation. When predicting coreference on gold mentions, we train only on gold mentions. We balance example weight by class size (we have significantly more negative examples), and we use L2 regularization for both SVC and SVR."}, {"heading": "4.1 Evaluation", "text": "We evaluate our baseline using the CoNLL reference coreference scorer (Pradhan et al., 2014) and report MUC, B3, CEAFm, CEAFe, and BLANC scores. Intuitively, MUC (Vilain et al., 1995) computes precision and recall of mention-mention links over all gold chains compared with all predicted chains. B3 (Bagga and Baldwin, 1998) computes precision and recall over each individual mention which makes the score also applicable to singleton mentions. CEAFm (Luo, 2005) creates an optimal matching between predicted and gold mentions and evaluates the percentage of mentions correctly assigned to chains, while CEAFe does the same from the perspective of chains. BLANC (Recasens and Hovy, 2010) gives equal importance to mention-mention links and non-existing mention-mention links, without the need to consider per-mention or per-chain score accumulations.\nIn the following we compute scores per document and accumulate them into overall and genre-based scores using the number of tokens in each document as weight. Mention detection is done on the Turkish\nTreebank data and is a deterministic algorithm which does not require learning. Coreference resolution is done either on gold mentions (GM) or on predicted mentions (PM). Scores are obtained by leave-one-out cross-validation on all 33 documents of the corpus, yielding 33 folds. All scores are given in percent. For mention detection we report Precision and Recall, for coreference scores we report only F1.\nTable 2 shows baseline results for mention detection and coreference with SVC on gold mentions. We obtain 83.2% recall for mention detection over the whole Treebank. As expected and as intended, precision is much worse because we expect the coreference resolution step to eliminate spurious mentions. Coreference resolution yields a MUC score of 77.8%, while the stricter B3, CEAF, and BLANC scores are lower. The worst scores for BLANC, MUC, and CEAFm are obtained from genres Travel and Other, which is logical because these genres contain only one document each. Hence, in cross-validation, the training set contains only documents from other genres.\nTable 3 shows results for the best and worst document in the corpus, and overall average of scores for SVC, SVR on gold mentions and SVR on predicted mentions. Average is again weighted by document size. SVC on gold mentions is the same setup as in Table 2. For comparison we show again the average score, and additionally the best and worst score obtained from a single document. Using SVR on gold mentions yields scores similar to SVC except for BLANC where the worst document is 6.1% worse than with SVM. On predicted mentions, SVC puts nearly all mentions into a single chain, which yields reasonable MUC score but low other scores. Therefore, we omit SVC results on predicted mentions from the table, and show only results for SVR which has the systematic advantage of selecting only the best link. Naturally, coreference prediction on predicted mentions yields significantly worse results than on gold mentions, with an average MUC score of 32.2% and an average BLANC score of 18.1%. The document producing the worst BLANC score of 1.4% is a scientific article about the meaning of the word \u2018home\u2019, which is a text that contains mainly abstract examples about the animal world, hence the score is not surprising. The best BLANC score on predicted mentions is obtained from a story about street children which is about concrete events and individuals.\nAs this is only a baseline, we did not include more sophisticated features described in (Bengtson and Roth, 2008). For example, semantic features based on WordNet (Bilgin et al., 2004; Miller, 1995) could rule out certain predicted mentions as relevant and thus could improve precision of the baseline."}, {"heading": "5 Conclusion", "text": "The Turkish coreference corpus and the Turkish coreference resolution baseline are a starting point for further research.\nAnnotations on the token level have the consequence that coreference between morphemes and other tokens (or morphemes) cannot be annotated. While such coreference annotations are desirable, we decided to reduce the complexity of the annotation task by omitting such coreference links and by presenting tokens without morphological analysis to annotators. For future annotation projects it could be interesting to extend annotations to include morpheme coreference links. Scoring with the reference\nscorer tool would require development of a novel CoNLL representation for tokens that are split within IGs. If this representation is not used for annotation, development of a tool for such annotations is also necessary. We think this would also require annotators with a higher level of expertise than available for our study.\nAs the only pronoun anaphora annotations (K\u00fc\u00e7\u00fck and Y\u00f6ndem, 2007) that were done on the METUSabanci Turkish Treebank can no longer be found by the authors (personal communication), we have no possibility of validating the annotations performed in our annotation project.\nTo improve the mention detection baseline, information about appositives as well as finding a way to filter out generic mentions could be useful. To improve the coreference resolution baseline, adding more complex features by integrating Turkish WordNet (Bilgin et al., 2004), Turkish NER (\u015eeker and Eryi\u011fit, 2012), and Turkish WSD (\u0130lgen et al., 2012) could be helpful. For a full processing pipeline from plain text to coreference annotations, we need at least morphological analysis (Sak et al., 2007), disambiguation (Sak et al., 2007), and dependency parsing (Eryi\u011fit et al., 2008). Available tools are the ITU-pipeline (Eryi\u011fit, 2014) and the older Zemberek system (Ak\u0131n and Ak\u0131n, 2007)."}, {"heading": "Acknowledgements", "text": "We are grateful to Kemal Oflazer and Bilge Say for support about the METU-Sabanci Turkish Treebank, and to Dilek K\u00fc\u00e7\u00fck and Sava\u015f Y\u0131ld\u0131r\u0131m for support about their papers and datasets.\nThis work has been supported by The Scientific and Technological Research Council of Turkey (TUBITAK) under grant agreements 114E430 and 114E777.\nAuthor contributions are as follows (in chronological order): creating documents from Treebank fragments (B.G.S), evaluating GATE and BRAT annotation software (B.G.S., A.P.), creating the annotation manual (P.S., K.C., F.T., B.G.S.), performing manual adjudication in Phase I (K.C., B.G.S., A.P., H.E.K.), annotating mentions for those documents in Phase II that were not annotated in Phase I (K.C., H.E.K.), revising the manuscript as an English native-speaker (A.H.K.), writing the baseline software (P.S., F.T., A.P.), managing the project, performing baseline experiments, analyzing data and result, and writing the manuscript (P.S)."}], "references": [{"title": "Zemberek, An Open Source NLP Framework for Turkic Languages", "author": ["A.A. Ak\u0131n", "M.D. Ak\u0131n"], "venue": "Structure, 10:1\u20135.", "citeRegEx": "Ak\u0131n and Ak\u0131n,? 2007", "shortCiteRegEx": "Ak\u0131n and Ak\u0131n", "year": 2007}, {"title": "The Annotation Process in the Turkish Treebank", "author": ["N.B. Atalay", "K. Oflazer", "B. Say"], "venue": "International Workshop on Linguistically Interpreted Corpora (LINC).", "citeRegEx": "Atalay et al\\.,? 2003", "shortCiteRegEx": "Atalay et al\\.", "year": 2003}, {"title": "Algorithms for Scoring Coreference Chains", "author": ["A. Bagga", "B. Baldwin"], "venue": "International Conference on Language Resources and Evaluation (LREC), pages 563\u2013566.", "citeRegEx": "Bagga and Baldwin,? 1998", "shortCiteRegEx": "Bagga and Baldwin", "year": 1998}, {"title": "Understanding the value of features for coreference resolution", "author": ["E. Bengtson", "D. Roth"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 294\u2013303.", "citeRegEx": "Bengtson and Roth,? 2008", "shortCiteRegEx": "Bengtson and Roth", "year": 2008}, {"title": "Building a Wordnet for Turkish", "author": ["O. Bilgin", "\u00d6. \u00c7etino\u011flu", "K. Oflazer"], "venue": "Romanian Journal of Information Science and Technology, 7(1-2):163\u2013172.", "citeRegEx": "Bilgin et al\\.,? 2004", "shortCiteRegEx": "Bilgin et al\\.", "year": 2004}, {"title": "BART: A Multilingual Anaphora Resolution System", "author": ["S. Broscheit", "M. Poesio", "P.S. Ponzetto", "J.K. Rodriguez", "L. Romano", "O. Uryupina", "Y. Versley", "R. Zanoli"], "venue": "International Workshop on Semantic Evaluation (SemEval), pages 104\u2013107.", "citeRegEx": "Broscheit et al\\.,? 2010", "shortCiteRegEx": "Broscheit et al\\.", "year": 2010}, {"title": "CoNLL-X shared task on Multilingual Dependency Parsing", "author": ["S. Buchholz", "E. Marsi"], "venue": "Computational Natural Language Learning (CoNLL), pages 149\u2013164.", "citeRegEx": "Buchholz and Marsi,? 2006", "shortCiteRegEx": "Buchholz and Marsi", "year": 2006}, {"title": "A Constrained Latent Variable Model for Coreference Resolution", "author": ["Chang", "K.-W.", "R. Samdani", "D. Roth"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 601\u2013612.", "citeRegEx": "Chang et al\\.,? 2013", "shortCiteRegEx": "Chang et al\\.", "year": 2013}, {"title": "Combining the Best of Two Worlds: A Hybrid Approach to Multilingual Coreference Resolution", "author": ["C. Chen", "V. Ng"], "venue": "Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Shared Task, pages 56\u201363.", "citeRegEx": "Chen and Ng,? 2012", "shortCiteRegEx": "Chen and Ng", "year": 2012}, {"title": "Support Vector Networks", "author": ["Corinna Cortes", "V. Vapnik"], "venue": "Machine Learning, 20(3):273\u2013297.", "citeRegEx": "Cortes and Vapnik,? 1995", "shortCiteRegEx": "Cortes and Vapnik", "year": 1995}, {"title": "First-Order Probabilistic Models for Coreference Resolution", "author": ["A. Culotta", "M. Wick", "A. Mccallum"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 81\u201388.", "citeRegEx": "Culotta et al\\.,? 2007", "shortCiteRegEx": "Culotta et al\\.", "year": 2007}, {"title": "Getting more out of biomedical documents with GATE\u2019s full lifecycle open source text analytics", "author": ["H. Cunningham", "V. Tablan", "A. Roberts", "K. Bontcheva"], "venue": "PLoS computational biology, 9(2):e1002854.", "citeRegEx": "Cunningham et al\\.,? 2013", "shortCiteRegEx": "Cunningham et al\\.", "year": 2013}, {"title": "Global joint models for coreference resolution and named entity classification", "author": ["P. Denis", "J. Baldridge"], "venue": "Procesamiento del Lenguaje Natural, 42:87\u201396.", "citeRegEx": "Denis and Baldridge,? 2009", "shortCiteRegEx": "Denis and Baldridge", "year": 2009}, {"title": "The Automatic Content Extraction (ACE) Program-Tasks, Data, and Evaluation", "author": ["G. Doddington", "A. Mitchell", "M. Przybocki", "L. Ramshaw", "S. Strassel", "R. Weischedel"], "venue": "International Conference on Language Resources and Evaluation (LREC), pages 837\u2013840.", "citeRegEx": "Doddington et al\\.,? 2004", "shortCiteRegEx": "Doddington et al\\.", "year": 2004}, {"title": "Support vector regression machines", "author": ["H. Drucker", "C.J.C. Burges", "L. Kaufman", "A. Smola", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems, 9:155\u2013161.", "citeRegEx": "Drucker et al\\.,? 1997", "shortCiteRegEx": "Drucker et al\\.", "year": 1997}, {"title": "Situated Processing of Pronominal Anaphora", "author": ["T. Erkan", "V. Akman"], "venue": "Verarbeitung nat\u00fcrlicher Sprache (KONVENS), pages 369\u2013378. Bilkent University.", "citeRegEx": "Erkan and Akman,? 1998", "shortCiteRegEx": "Erkan and Akman", "year": 1998}, {"title": "ITU Turkish NLP Web Service", "author": ["G. Eryi\u011fit"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics (EACL) Demonstrations, pages 1\u20134.", "citeRegEx": "Eryi\u011fit,? 2014", "shortCiteRegEx": "Eryi\u011fit", "year": 2014}, {"title": "Dependency Parsing of Turkish", "author": ["G. Eryi\u011fit", "J. Nivre", "K. Oflazer"], "venue": "Computational Linguistics, 34(3):357\u2013389.", "citeRegEx": "Eryi\u011fit et al\\.,? 2008", "shortCiteRegEx": "Eryi\u011fit et al\\.", "year": 2008}, {"title": "Latent structure perceptron with feature induction for unrestricted coreference resolution", "author": ["E.R. Fernandes", "C.N. dos Santos", "R.L. Milidi\u00fa"], "venue": "In Joint Conference on EMNLP and CoNLL: Shared Task,", "citeRegEx": "Fernandes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fernandes et al\\.", "year": 2012}, {"title": "GATE: an environment to support research and development in natural language engineering", "author": ["R. Gaizauskas", "H. Cunningham", "Y. Wilks", "P. Rodgers", "K. Humphreys"], "venue": "International Conference on Tools with Artificial Intelligence (ICTAI), pages 58\u201366.", "citeRegEx": "Gaizauskas et al\\.,? 1996", "shortCiteRegEx": "Gaizauskas et al\\.", "year": 1996}, {"title": "Design of the MUC-6 evaluation", "author": ["R. Grishman", "B. Sundheim"], "venue": "Message Understanding Conference (MUC), pages 413\u2013422.", "citeRegEx": "Grishman and Sundheim,? 1995", "shortCiteRegEx": "Grishman and Sundheim", "year": 1995}, {"title": "Centering: A Framework for Modelling the Local Coherence of Discourse", "author": ["B.J. Grosz", "A.K. Joshi", "S. Weinstein"], "venue": "Computational Linguistics, 21(2):203\u2013225.", "citeRegEx": "Grosz et al\\.,? 1995", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Statistical morphological disambiguation for agglutinative languages", "author": ["D.Z. Hakkani-T\u00fcr", "K. Oflazer", "G. T\u00fcr"], "venue": "Computers and the Humanities, 36(4):381\u2013410.", "citeRegEx": "Hakkani.T\u00fcr et al\\.,? 2002", "shortCiteRegEx": "Hakkani.T\u00fcr et al\\.", "year": 2002}, {"title": "Anaphora in Natural Language Understanding: A Survey", "author": ["G. Hirst"], "venue": "Springer.", "citeRegEx": "Hirst,? 1981", "shortCiteRegEx": "Hirst", "year": 1981}, {"title": "Resolving Pronoun References", "author": ["J.R. Hobbs"], "venue": "Lingua, 44(4):311\u2013338.", "citeRegEx": "Hobbs,? 1978", "shortCiteRegEx": "Hobbs", "year": 1978}, {"title": "OntoNotes: the 90% solution", "author": ["E. Hovy", "M. Marcus", "M. Palmer", "L. Ramshaw", "R. Weischedel"], "venue": "Human Language Technology Conference of the NAACL, Short Papers, pages 57\u201360.", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Content Analysis: An Introduction to Its Methodology", "author": ["K. Krippendorff"], "venue": "Sage Publications.", "citeRegEx": "Krippendorff,? 1980", "shortCiteRegEx": "Krippendorff", "year": 1980}, {"title": "Identification of coreferential chains in video texts for semantic annotation of news videos", "author": ["D. K\u00fc\u00e7\u00fck", "A. Yaz\u0131c\u0131"], "venue": "International Symposium on Computer and Information Sciences (ISCIS).", "citeRegEx": "K\u00fc\u00e7\u00fck and Yaz\u0131c\u0131,? 2008", "shortCiteRegEx": "K\u00fc\u00e7\u00fck and Yaz\u0131c\u0131", "year": 2008}, {"title": "Automatic Identification of Pronominal Anaphora in Turkish Texts", "author": ["D. K\u00fc\u00e7\u00fck", "M.T. Y\u00f6ndem"], "venue": "International Conference on Computer and Information Science (ICIS).", "citeRegEx": "K\u00fc\u00e7\u00fck and Y\u00f6ndem,? 2007", "shortCiteRegEx": "K\u00fc\u00e7\u00fck and Y\u00f6ndem", "year": 2007}, {"title": "Learning-based pronoun resolution for Turkish with a comparative evaluation", "author": ["Y. K\u0131l\u0131\u00e7aslan", "E.S. G\u00fcner", "S. Y\u0131ld\u0131r\u0131m"], "venue": "Computer Speech & Language, 23(3):311\u2013331.", "citeRegEx": "K\u0131l\u0131\u00e7aslan et al\\.,? 2009", "shortCiteRegEx": "K\u0131l\u0131\u00e7aslan et al\\.", "year": 2009}, {"title": "Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules", "author": ["H. Lee", "A. Chang", "Y. Peirsman", "N. Chambers", "M. Surdeanu", "Dan Jurafsky"], "venue": "Computational Linguistics, 39(4):885\u2013916.", "citeRegEx": "Lee et al\\.,? 2013", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "On coreference resolution performance metrics", "author": ["X. Luo"], "venue": "Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 25\u201332.", "citeRegEx": "Luo,? 2005", "shortCiteRegEx": "Luo", "year": 2005}, {"title": "WordNet: a lexical database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM, 38(11):39\u2013", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Anaphora Resolution", "author": ["R. Mitkov"], "venue": "Longman.", "citeRegEx": "Mitkov,? 2002", "shortCiteRegEx": "Mitkov", "year": 2002}, {"title": "Supervised Noun Phrase Coreference Research: The First Fifteen Years", "author": ["V. Ng"], "venue": "Association for Computational Linguistics (ACL), pages 1396\u20131411.", "citeRegEx": "Ng,? 2010", "shortCiteRegEx": "Ng", "year": 2010}, {"title": "Two-level Description of Turkish Morphology", "author": ["K. Oflazer"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 472\u2013472.", "citeRegEx": "Oflazer,? 1993", "shortCiteRegEx": "Oflazer", "year": 1993}, {"title": "Turkish Natural Language Processing Initiative: An Overview", "author": ["K. Oflazer", "H.C. Boz\u015fahin"], "venue": "Turkish Symposium on Artificial Intelligence and Neural Networks.", "citeRegEx": "Oflazer and Boz\u015fahin,? 1994", "shortCiteRegEx": "Oflazer and Boz\u015fahin", "year": 1994}, {"title": "An Outline of Turkish Morphology", "author": ["K. Oflazer", "E. G\u00f6\u00e7men", "C. Boz\u015fahin"], "venue": "Technical report, NATO Science Division SfS III, Brussels.", "citeRegEx": "Oflazer et al\\.,? 1994", "shortCiteRegEx": "Oflazer et al\\.", "year": 1994}, {"title": "Building a Turkish Treebank", "author": ["K. Oflazer", "B. Say", "D.Z. Hakkani-T\u00fcr", "G. T\u00fcr"], "venue": "Treebanks, Building and Using Parsed Corpora, pages 261\u2013277. Springer Netherlands.", "citeRegEx": "Oflazer et al\\.,? 2003", "shortCiteRegEx": "Oflazer et al\\.", "year": 2003}, {"title": "Inter-annotator agreement on a multilingual semantic annotation task", "author": ["R. Passonneau", "N. Habash", "O. Rambow"], "venue": "International Conference on Language Resources and Evaluation (LREC), pages 1951\u20131956.", "citeRegEx": "Passonneau et al\\.,? 2006", "shortCiteRegEx": "Passonneau et al\\.", "year": 2006}, {"title": "Computing reliability for coreference annotation", "author": ["R.J. Passonneau"], "venue": "Language Resources and Evaluation (LREC), pages 1503\u20131506.", "citeRegEx": "Passonneau,? 2004", "shortCiteRegEx": "Passonneau", "year": 2004}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u20132830.", "citeRegEx": "Pedregosa et al\\.,? 2011", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Learning to resolve bridging references", "author": ["M. Poesio", "R. Mehta", "A. Maroudas", "J. Hitzeman"], "venue": "Annual Meeting on Association for Computational Linguistics (ACL). Paper #143.", "citeRegEx": "Poesio et al\\.,? 2004", "shortCiteRegEx": "Poesio et al\\.", "year": 2004}, {"title": "Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation", "author": ["S. Pradhan", "X. Luo", "M. Recasens", "E. Hovy", "V. Ng", "M. Strube"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL), pages 30\u201335.", "citeRegEx": "Pradhan et al\\.,? 2014", "shortCiteRegEx": "Pradhan et al\\.", "year": 2014}, {"title": "CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes", "author": ["S. Pradhan", "A. Moschitti", "N. Xue", "O. Uryupina", "Y. Zhang"], "venue": "Joint Conference on EMNLP and CoNLL: Shared Task, pages 1\u201340.", "citeRegEx": "Pradhan et al\\.,? 2012", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Unrestricted Coreference: Identifying Entities and Events in OntoNotes", "author": ["S.S. Pradhan", "L. Ramshaw", "R. Weischedel", "J. MacBride", "L. Micciulla"], "venue": "International Conference on Semantic Computing (ICSC 2007), pages 446\u2013453.", "citeRegEx": "Pradhan et al\\.,? 2007", "shortCiteRegEx": "Pradhan et al\\.", "year": 2007}, {"title": "BLANC: Implementing the Rand Index for Coreference Evaluation", "author": ["M. Recasens", "E. Hovy"], "venue": "Natural Language Engineering, 17(4):485\u2013510.", "citeRegEx": "Recasens and Hovy,? 2010", "shortCiteRegEx": "Recasens and Hovy", "year": 2010}, {"title": "SemEval-2010 Task 1: Coreference Resolution in Multiple Languages", "author": ["M. Recasens", "L. M\u00e0rquez", "E. Sapena", "M.A. Mart\u00ed", "M. Taul\u00e9", "V. Hoste", "M. Poesio", "Y. Versley"], "venue": "International Workshop on Semantic Evaluation (SemEval) at ACL, pages 1\u20138.", "citeRegEx": "Recasens et al\\.,? 2010", "shortCiteRegEx": "Recasens et al\\.", "year": 2010}, {"title": "Morphological disambiguation of Turkish text with perceptron algorithm", "author": ["H. Sak", "T. G\u00fcng\u00f6r", "M. Sara\u00e7lar"], "venue": "Intelligent Text Processing and Computational Linguistics, pages 107\u2013118.", "citeRegEx": "Sak et al\\.,? 2007", "shortCiteRegEx": "Sak et al\\.", "year": 2007}, {"title": "A Constraint-Based Hypergraph Partitioning Approach to Coreference Resolution", "author": ["E. Sapena", "L. Padro", "J. Turmo"], "venue": "Computational Linguistics, 39(4):847\u2013884.", "citeRegEx": "Sapena et al\\.,? 2012", "shortCiteRegEx": "Sapena et al\\.", "year": 2012}, {"title": "Development of a Corpus and a Treebank for Present day Written Turkish", "author": ["B. Say", "D. Zeyrek", "K. Oflazer", "U. \u00d6zge"], "venue": "\u0130mer, K. and Do\u011fan, G., editors, Current Research in Turkish Linguistics (Proceedings of the Eleventh International Conference of Turkish Linguistics, 2002), pages 182\u2013192. Eastern Mediterranean University Press.", "citeRegEx": "Say et al\\.,? 2004", "shortCiteRegEx": "Say et al\\.", "year": 2004}, {"title": "Adjudication of Coreference Annotations via Finding Optimal Repairs of Equivalence Relations", "author": ["P. Sch\u00fcller"], "venue": "International Workshop on Experimental Evaluation of Algorithms for Solving Problems with Combinatorial Explosion (RCRA), pages 57\u201371.", "citeRegEx": "Sch\u00fcller,? 2016", "shortCiteRegEx": "Sch\u00fcller", "year": 2016}, {"title": "A machine learning approach to coreference resolution of noun phrases", "author": ["W. Soon", "H. Ng", "D. Lim"], "venue": "Computational linguistics, 27(4):521\u2013544.", "citeRegEx": "Soon et al\\.,? 2001", "shortCiteRegEx": "Soon et al\\.", "year": 2001}, {"title": "Brat: a Web-based Tool for NLP-Assisted Text Annotation", "author": ["P. Stenetorp", "S. Pyysalo", "G. Topic", "T. Ohta", "S. Ananiadou", "J. Tsujii"], "venue": "European Chapter of the Association for Computational Linguistics (EACL), pages 102\u2013107.", "citeRegEx": "Stenetorp et al\\.,? 2012", "shortCiteRegEx": "Stenetorp et al\\.", "year": 2012}, {"title": "Easy-first Coreference Resolution", "author": ["V. Stoyanov", "J. Eisner"], "venue": "International Conference on Computational Linguistics (COLING), pages 2519\u20132534.", "citeRegEx": "Stoyanov and Eisner,? 2012", "shortCiteRegEx": "Stoyanov and Eisner", "year": 2012}, {"title": "Turkish coreference annotation manual (V2)", "author": ["B.G. S\u00fcrmeli", "K. C\u0131ng\u0131ll\u0131", "F. Tun\u00e7er", "P. Sch\u00fcller"], "venue": "https://tinyurl.com/jovfmua .", "citeRegEx": "S\u00fcrmeli et al\\.,? 2016", "shortCiteRegEx": "S\u00fcrmeli et al\\.", "year": 2016}, {"title": "A Computational Model for Resolving Pronominal Anaphora in Turkish Using Hobbs\u2019 Na\u00efve Algorithm", "author": ["P. T\u00fcfek\u00e7i", "Y. K\u0131l\u0131\u00e7aslan"], "venue": "International Journal of Computer, Electrical, Automation, Control and Information Engineering, 1(5):1402\u20131406.", "citeRegEx": "T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan,? 2007", "shortCiteRegEx": "T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan", "year": 2007}, {"title": "Null Vs", "author": ["\u00dc.D. Turan"], "venue": "Overt Subjects in Turkish. PhD thesis, University of Pennsylvania.", "citeRegEx": "Turan,? 1996", "shortCiteRegEx": "Turan", "year": 1996}, {"title": "On Coreferring: Coreference in MUC and Related Annotation Schemes", "author": ["K. van Deemter", "R. Kibble"], "venue": "Computational Linguistics,", "citeRegEx": "Deemter and Kibble,? \\Q2000\\E", "shortCiteRegEx": "Deemter and Kibble", "year": 2000}, {"title": "A Model-Theoretic Coreference Scoring Scheme", "author": ["M. Vilain", "J. Burger", "J. Aberdeen", "D. Connolly", "L. Hirschman"], "venue": "Message Understanding Conference (MUC), pages 45\u201352.", "citeRegEx": "Vilain et al\\.,? 1995", "shortCiteRegEx": "Vilain et al\\.", "year": 1995}, {"title": "OntoNotes Release 5.0", "author": ["R. Weischedel", "S. Pradhan", "L. Ramshaw", "J. Kaufman", "M. Franchini", "M. El-Bachouti", "N. Xue", "M. Palmer", "J.D. Hwang", "C. Bonial", "J. Choi", "A. Mansouri", "M. Foster", "Hawwary", "A.-a", "M. Marcus", "A. Taylor", "C. Greenberg", "E. Hovy", "R. Belvin", "A. Houston"], "venue": null, "citeRegEx": "Weischedel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weischedel et al\\.", "year": 2012}, {"title": "Contextually appropriate reference generation", "author": ["\u00d6. Y\u00fcksel", "C. Bozsahin"], "venue": "Natural Language Engineering, 8(1):69\u201389.", "citeRegEx": "Y\u00fcksel and Bozsahin,? 2002", "shortCiteRegEx": "Y\u00fcksel and Bozsahin", "year": 2002}, {"title": "A Computational Model for Anaphora Resolution in Turkish via Centering Theory: an Initial Approach", "author": ["S. Y\u0131ld\u0131r\u0131m", "Y. K\u0131l\u0131\u00e7aslan", "R.E. Ayka\u00e7"], "venue": "International Conference on Computational Intelligence, pages 124\u2013128.", "citeRegEx": "Y\u0131ld\u0131r\u0131m et al\\.,? 2004", "shortCiteRegEx": "Y\u0131ld\u0131r\u0131m et al\\.", "year": 2004}, {"title": "Dynamic Knowledge-Base Alignment for Coreference Resolution", "author": ["J. Zheng", "L. Vilnis", "S. Singh", "J.D. Choi", "A. McCallum"], "venue": "Computational Natural Language Learning (CoNLL), pages 153\u2013162.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}, {"title": "Building up Lexical Sample Dataset for Turkish Word Sense Disambiguation", "author": ["B. \u0130lgen", "E. Adal\u0131", "A.C. Tantu\u011f"], "venue": "International Symposium on Innovations in Intelligent Systems and Applications (INISTA), pages 1\u20135.", "citeRegEx": "\u0130lgen et al\\.,? 2012", "shortCiteRegEx": "\u0130lgen et al\\.", "year": 2012}, {"title": "Initial explorations on using CRFs for Turkish Named Entity Recognition", "author": ["G.A. \u015eeker", "G. Eryi\u011fit"], "venue": "International Conference on Computational Linguistics (COLING), pages 2459\u20132474.", "citeRegEx": "\u015eeker and Eryi\u011fit,? 2012", "shortCiteRegEx": "\u015eeker and Eryi\u011fit", "year": 2012}], "referenceMentions": [{"referenceID": 36, "context": ", \u2018\u00e7ocuklar\u0131\u2019 can be analyzed as \u2018their children\u2019 or as \u2018his/her children\u2019, depending on context (Oflazer and Boz\u015fahin, 1994).", "startOffset": 97, "endOffset": 125}, {"referenceID": 20, "context": "Coreference resolution is not limited to resolving pronouns: in computer linguistics it was first introduced as a benchmark for deep semantic understanding of text in the message understanding conference series (Grishman and Sundheim, 1995) and later continued in the Automatic Content Extraction (ACE) program (Doddington et al.", "startOffset": 211, "endOffset": 240}, {"referenceID": 13, "context": "Coreference resolution is not limited to resolving pronouns: in computer linguistics it was first introduced as a benchmark for deep semantic understanding of text in the message understanding conference series (Grishman and Sundheim, 1995) and later continued in the Automatic Content Extraction (ACE) program (Doddington et al., 2004) which required English coreference resolution as foundation for all tasks of years 2000\u20132004.", "startOffset": 311, "endOffset": 336}, {"referenceID": 47, "context": "The SemEval competition series followed ACE and featured the first multilingual coreference resolution challenge in 2010 (Recasens et al., 2010).", "startOffset": 121, "endOffset": 144}, {"referenceID": 25, "context": "Connected to that is the freely available, large, and multilingual OntoNotes (Hovy et al., 2006) corpus, which was used in the multilingual coreference task in SemEval 2012 (Pradhan et al.", "startOffset": 77, "endOffset": 96}, {"referenceID": 44, "context": ", 2006) corpus, which was used in the multilingual coreference task in SemEval 2012 (Pradhan et al., 2012) and contains coreference annotations for English, Arabic, and Mandarin (Pradhan et al.", "startOffset": 84, "endOffset": 106}, {"referenceID": 45, "context": ", 2012) and contains coreference annotations for English, Arabic, and Mandarin (Pradhan et al., 2007).", "startOffset": 79, "endOffset": 101}, {"referenceID": 49, "context": "(2001), methods for building chains from link predictions include local greedy heuristics as done by Bengtson and Roth (2008) or Stoyanov and Eisner (2012), global optimization formulations such as relaxation labeling (Sapena et al., 2012) or ranking with ILP or Markov Logic (Culotta et al.", "startOffset": 218, "endOffset": 239}, {"referenceID": 10, "context": ", 2012) or ranking with ILP or Markov Logic (Culotta et al., 2007; Denis and Baldridge, 2009), and representations of trees of mention-mention links (Chang et al.", "startOffset": 44, "endOffset": 93}, {"referenceID": 12, "context": ", 2012) or ranking with ILP or Markov Logic (Culotta et al., 2007; Denis and Baldridge, 2009), and representations of trees of mention-mention links (Chang et al.", "startOffset": 44, "endOffset": 93}, {"referenceID": 7, "context": ", 2007; Denis and Baldridge, 2009), and representations of trees of mention-mention links (Chang et al., 2013; Fernandes et al., 2012).", "startOffset": 90, "endOffset": 134}, {"referenceID": 18, "context": ", 2007; Denis and Baldridge, 2009), and representations of trees of mention-mention links (Chang et al., 2013; Fernandes et al., 2012).", "startOffset": 90, "endOffset": 134}, {"referenceID": 9, "context": "Coreference resolution is not limited to resolving pronouns: in computer linguistics it was first introduced as a benchmark for deep semantic understanding of text in the message understanding conference series (Grishman and Sundheim, 1995) and later continued in the Automatic Content Extraction (ACE) program (Doddington et al., 2004) which required English coreference resolution as foundation for all tasks of years 2000\u20132004. The SemEval competition series followed ACE and featured the first multilingual coreference resolution challenge in 2010 (Recasens et al., 2010). Connected to that is the freely available, large, and multilingual OntoNotes (Hovy et al., 2006) corpus, which was used in the multilingual coreference task in SemEval 2012 (Pradhan et al., 2012) and contains coreference annotations for English, Arabic, and Mandarin (Pradhan et al., 2007). Coreference resolution has been surveyed by Ng (2010). Approaches are manifold and include machine-learning-based approaches, rule-based systems, and combinations of both.", "startOffset": 312, "endOffset": 922}, {"referenceID": 9, "context": "Coreference resolution is not limited to resolving pronouns: in computer linguistics it was first introduced as a benchmark for deep semantic understanding of text in the message understanding conference series (Grishman and Sundheim, 1995) and later continued in the Automatic Content Extraction (ACE) program (Doddington et al., 2004) which required English coreference resolution as foundation for all tasks of years 2000\u20132004. The SemEval competition series followed ACE and featured the first multilingual coreference resolution challenge in 2010 (Recasens et al., 2010). Connected to that is the freely available, large, and multilingual OntoNotes (Hovy et al., 2006) corpus, which was used in the multilingual coreference task in SemEval 2012 (Pradhan et al., 2012) and contains coreference annotations for English, Arabic, and Mandarin (Pradhan et al., 2007). Coreference resolution has been surveyed by Ng (2010). Approaches are manifold and include machine-learning-based approaches, rule-based systems, and combinations of both. In most machinelearning approaches, equivalence relations of chains (sometimes called clusters) are assembled from predictions of the relatedness of pairs of mentions (mention-mention links). The earliest machine-learning approach is due to Soon et al. (2001), methods for building chains from link predictions include local greedy heuristics as done by Bengtson and Roth (2008) or Stoyanov and Eisner (2012), global optimization formulations such as relaxation labeling (Sapena et al.", "startOffset": 312, "endOffset": 1300}, {"referenceID": 3, "context": "(2001), methods for building chains from link predictions include local greedy heuristics as done by Bengtson and Roth (2008) or Stoyanov and Eisner (2012), global optimization formulations such as relaxation labeling (Sapena et al.", "startOffset": 101, "endOffset": 126}, {"referenceID": 3, "context": "(2001), methods for building chains from link predictions include local greedy heuristics as done by Bengtson and Roth (2008) or Stoyanov and Eisner (2012), global optimization formulations such as relaxation labeling (Sapena et al.", "startOffset": 101, "endOffset": 156}, {"referenceID": 30, "context": "More recent rule-based systems merge chains based on several sets of rules in a multi-stage filtering approach (Lee et al., 2013), moreover there are hybrid systems combining rules and machine learning such as the one by Chen and Ng (2012).", "startOffset": 111, "endOffset": 129}, {"referenceID": 42, "context": "Other approaches use curated or distributed knowledge sources such as WordNet, Google distance, and Wikipedia (Poesio et al., 2004; Zheng et al., 2013).", "startOffset": 110, "endOffset": 151}, {"referenceID": 63, "context": "Other approaches use curated or distributed knowledge sources such as WordNet, Google distance, and Wikipedia (Poesio et al., 2004; Zheng et al., 2013).", "startOffset": 110, "endOffset": 151}, {"referenceID": 23, "context": "Note that anaphora resolution (Hirst, 1981; Mitkov, 2002) is a problem orthogonal to coreference resolution (van Deemter and Kibble, 2000), because anaphora resolution focuses on referring expressions that point to previous expressions in the text.", "startOffset": 30, "endOffset": 57}, {"referenceID": 33, "context": "Note that anaphora resolution (Hirst, 1981; Mitkov, 2002) is a problem orthogonal to coreference resolution (van Deemter and Kibble, 2000), because anaphora resolution focuses on referring expressions that point to previous expressions in the text.", "startOffset": 30, "endOffset": 57}, {"referenceID": 22, "context": "was done by Hobbs (1978). More recent rule-based systems merge chains based on several sets of rules in a multi-stage filtering approach (Lee et al.", "startOffset": 12, "endOffset": 25}, {"referenceID": 8, "context": ", 2013), moreover there are hybrid systems combining rules and machine learning such as the one by Chen and Ng (2012). Other approaches use curated or distributed knowledge sources such as WordNet, Google distance, and Wikipedia (Poesio et al.", "startOffset": 99, "endOffset": 118}, {"referenceID": 35, "context": "Derivational and inflectional suffixes are very productive (Oflazer, 1993; Oflazer et al., 1994) and are subject to vowel harmony from the root word.", "startOffset": 59, "endOffset": 96}, {"referenceID": 37, "context": "Derivational and inflectional suffixes are very productive (Oflazer, 1993; Oflazer et al., 1994) and are subject to vowel harmony from the root word.", "startOffset": 59, "endOffset": 96}, {"referenceID": 22, "context": "Morphological analysis is challenging due to ambiguities between different types of suffixes, for example \u2018izin\u2019 can mean \u2018your trace\u2019 (iz+P2Sg+Nom), \u2018trace\u2019 (iz+Pnon+Gen), or \u2018permission\u2019 (izin+Pnon+Nom) (Hakkani-T\u00fcr et al., 2002).", "startOffset": 205, "endOffset": 231}, {"referenceID": 1, "context": "The METU-Sabanci Turkish Treebank (in the following just called Turkish Treebank) (Atalay et al., 2003; Oflazer et al., 2003; Say et al., 2004) contains a subset of the METU Turkish Corpus (Say et al.", "startOffset": 82, "endOffset": 143}, {"referenceID": 38, "context": "The METU-Sabanci Turkish Treebank (in the following just called Turkish Treebank) (Atalay et al., 2003; Oflazer et al., 2003; Say et al., 2004) contains a subset of the METU Turkish Corpus (Say et al.", "startOffset": 82, "endOffset": 143}, {"referenceID": 50, "context": "The METU-Sabanci Turkish Treebank (in the following just called Turkish Treebank) (Atalay et al., 2003; Oflazer et al., 2003; Say et al., 2004) contains a subset of the METU Turkish Corpus (Say et al.", "startOffset": 82, "endOffset": 143}, {"referenceID": 50, "context": ", 2004) contains a subset of the METU Turkish Corpus (Say et al., 2004) in tokenized form.", "startOffset": 53, "endOffset": 71}, {"referenceID": 38, "context": "The Turkish Treebank splits tokens into IGs on derivational boundaries, for example, \u2018evimdekiler\u2019 (those in my house) is analyzed (Oflazer et al., 2003) as ev+Noun+A3sg+P1sg+Loc^DB+Adj^DB+Noun+Zero+A3pl+Pnon+Nom where ^DB indicates derivation boundaries and the token consists of three IGs \u2018evimde\u2019 (in my house), \u2018ki\u2019 (adjectivization), and \u2018ler\u2019 (nominalization+plural).", "startOffset": 131, "endOffset": 153}, {"referenceID": 6, "context": "A CoNLL format that provides a CoNLL token corresponding to each IG of a token has been created for Turkish dependency parsing (Buchholz and Marsi, 2006).", "startOffset": 127, "endOffset": 153}, {"referenceID": 57, "context": ", \u2018gidiyoruz\u2019 (we are going) and \u2018gidiyorlar\u2019 (they are going) but they can also be realized explicitly as in \u2018biz gidiyoruz\u2019, depending on discourse conditions (Turan, 1996).", "startOffset": 161, "endOffset": 174}, {"referenceID": 27, "context": "One exception is the work of K\u00fc\u00e7\u00fck and Yaz\u0131c\u0131 (2008) on political news texts extracted from videos: they focus on Gazetteers for extracting mentions (without considering general NPs or syntactic information), provide a rule-based heuristic based on recency for creating coreference chains, and evaluate their approach on three documents (which are not part of the Turkish Treebank).", "startOffset": 29, "endOffset": 53}, {"referenceID": 24, "context": "Hobb\u2019s na\u00efve pronoun resolution algorithm (Hobbs, 1978) was realized for Turkish and tested on 10 toy sentences (T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan, 2007).", "startOffset": 42, "endOffset": 55}, {"referenceID": 56, "context": "Hobb\u2019s na\u00efve pronoun resolution algorithm (Hobbs, 1978) was realized for Turkish and tested on 10 toy sentences (T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan, 2007).", "startOffset": 112, "endOffset": 142}, {"referenceID": 21, "context": "Centering theory (Grosz et al., 1995) is the foundation of several works on Turkish pronouns.", "startOffset": 17, "endOffset": 37}, {"referenceID": 15, "context": "Erkan and Akman (1998) describe an implementation of pronoun anaphora resolution in a framework for situation theory which is based on knowledge representation and logical reasoning.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "Erkan and Akman (1998) describe an implementation of pronoun anaphora resolution in a framework for situation theory which is based on knowledge representation and logical reasoning. Hobb\u2019s na\u00efve pronoun resolution algorithm (Hobbs, 1978) was realized for Turkish and tested on 10 toy sentences (T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan, 2007). Centering theory (Grosz et al., 1995) is the foundation of several works on Turkish pronouns. Turan (1996) performed a study about discourse conditions for referring vs.", "startOffset": 0, "endOffset": 434}, {"referenceID": 15, "context": "Erkan and Akman (1998) describe an implementation of pronoun anaphora resolution in a framework for situation theory which is based on knowledge representation and logical reasoning. Hobb\u2019s na\u00efve pronoun resolution algorithm (Hobbs, 1978) was realized for Turkish and tested on 10 toy sentences (T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan, 2007). Centering theory (Grosz et al., 1995) is the foundation of several works on Turkish pronouns. Turan (1996) performed a study about discourse conditions for referring vs. nonreferring expressions and null vs. overt pronouns, and evaluated the theory on 2500 annotated tokens. Y\u00fcksel and Bozsahin (2002) created a system for generating referring expressions that was tested on a machine translation task.", "startOffset": 0, "endOffset": 629}, {"referenceID": 15, "context": "Erkan and Akman (1998) describe an implementation of pronoun anaphora resolution in a framework for situation theory which is based on knowledge representation and logical reasoning. Hobb\u2019s na\u00efve pronoun resolution algorithm (Hobbs, 1978) was realized for Turkish and tested on 10 toy sentences (T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan, 2007). Centering theory (Grosz et al., 1995) is the foundation of several works on Turkish pronouns. Turan (1996) performed a study about discourse conditions for referring vs. nonreferring expressions and null vs. overt pronouns, and evaluated the theory on 2500 annotated tokens. Y\u00fcksel and Bozsahin (2002) created a system for generating referring expressions that was tested on a machine translation task. Furthermore, there is a theoretical model of anaphora resolution based on centering theory by Y\u0131ld\u0131r\u0131m et al. (2004). K\u00fc\u00e7\u00fck and Y\u00f6ndem (2007) described a system for finding and resolving Turkish pronominal anaphora and annotated 12266 anaphor candidate instances in the METU Turkish Corpus to evaluate their candidate extractor and decision tree learner for anaphora resolution.", "startOffset": 0, "endOffset": 847}, {"referenceID": 15, "context": "Erkan and Akman (1998) describe an implementation of pronoun anaphora resolution in a framework for situation theory which is based on knowledge representation and logical reasoning. Hobb\u2019s na\u00efve pronoun resolution algorithm (Hobbs, 1978) was realized for Turkish and tested on 10 toy sentences (T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan, 2007). Centering theory (Grosz et al., 1995) is the foundation of several works on Turkish pronouns. Turan (1996) performed a study about discourse conditions for referring vs. nonreferring expressions and null vs. overt pronouns, and evaluated the theory on 2500 annotated tokens. Y\u00fcksel and Bozsahin (2002) created a system for generating referring expressions that was tested on a machine translation task. Furthermore, there is a theoretical model of anaphora resolution based on centering theory by Y\u0131ld\u0131r\u0131m et al. (2004). K\u00fc\u00e7\u00fck and Y\u00f6ndem (2007) described a system for finding and resolving Turkish pronominal anaphora and annotated 12266 anaphor candidate instances in the METU Turkish Corpus to evaluate their candidate extractor and decision tree learner for anaphora resolution.", "startOffset": 0, "endOffset": 872}, {"referenceID": 15, "context": "Erkan and Akman (1998) describe an implementation of pronoun anaphora resolution in a framework for situation theory which is based on knowledge representation and logical reasoning. Hobb\u2019s na\u00efve pronoun resolution algorithm (Hobbs, 1978) was realized for Turkish and tested on 10 toy sentences (T\u00fcfek\u00e7i and K\u0131l\u0131\u00e7aslan, 2007). Centering theory (Grosz et al., 1995) is the foundation of several works on Turkish pronouns. Turan (1996) performed a study about discourse conditions for referring vs. nonreferring expressions and null vs. overt pronouns, and evaluated the theory on 2500 annotated tokens. Y\u00fcksel and Bozsahin (2002) created a system for generating referring expressions that was tested on a machine translation task. Furthermore, there is a theoretical model of anaphora resolution based on centering theory by Y\u0131ld\u0131r\u0131m et al. (2004). K\u00fc\u00e7\u00fck and Y\u00f6ndem (2007) described a system for finding and resolving Turkish pronominal anaphora and annotated 12266 anaphor candidate instances in the METU Turkish Corpus to evaluate their candidate extractor and decision tree learner for anaphora resolution. K\u0131l\u0131\u00e7aslan et al. (2009) performed a comprehensive study on pronoun resolution and evaluated various machine learning methods for resolving overt and null pronouns in a corpus of 20 children stories.", "startOffset": 0, "endOffset": 1134}, {"referenceID": 55, "context": "We designed an annotation manual (S\u00fcrmeli et al., 2016) for marking coreference according to the following principles: \u2022 all concrete entities that are mentioned more than once shall be annotated, \u2022 mentions shall be marked as the biggest possible span of tokens that describes the entity, \u2022 lists shall not be annotated (elements of lists can be annotated), and", "startOffset": 33, "endOffset": 55}, {"referenceID": 11, "context": "In Phase I, annotations were created by 19 annotators with the GATE (Cunningham et al., 2013; Gaizauskas et al., 1996) coreference annotation tool.", "startOffset": 68, "endOffset": 118}, {"referenceID": 19, "context": "In Phase I, annotations were created by 19 annotators with the GATE (Cunningham et al., 2013; Gaizauskas et al., 1996) coreference annotation tool.", "startOffset": 68, "endOffset": 118}, {"referenceID": 55, "context": "The submission file format is described in (S\u00fcrmeli et al., 2016).", "startOffset": 43, "endOffset": 65}, {"referenceID": 53, "context": "We were not able to use the BRAT (Stenetorp et al., 2012) annotation tool, because of difficulties representing sentence and word addresses in a way that they can be extracted from annotations.", "startOffset": 33, "endOffset": 57}, {"referenceID": 32, "context": "Note that, by marking mentions as the biggest spans, mentions and potentially available appositives are annotated as single mentions, which is different from OntoNotes where appositives are a special type of coreference annotation. We do not mark predications because they are a different type of coreference as argued by van Deemter and Kibble (2000). In Phase I, annotations were created by 19 annotators with the GATE (Cunningham et al.", "startOffset": 19, "endOffset": 352}, {"referenceID": 40, "context": "Coreference IAA scores IAA1 and IAA2 are calculated as described in (Passonneau, 2004) and (Passonneau et al.", "startOffset": 68, "endOffset": 86}, {"referenceID": 39, "context": "Coreference IAA scores IAA1 and IAA2 are calculated as described in (Passonneau, 2004) and (Passonneau et al., 2006), respectively.", "startOffset": 91, "endOffset": 116}, {"referenceID": 26, "context": "IAA1 is an adaption of Krippendorff \u03b1 (Krippendorff, 1980) where a pair of chains gets a score of 1 for a perfect match, 2 3 if one chain is a subset of the other one, 1 3 if they are intersecting, and 0 otherwise.", "startOffset": 38, "endOffset": 58}, {"referenceID": 60, "context": "This is different from other coreference annotations, in particular in OntoNotes, where two annotators created annotations followed by adjudication done by a single human expert (Weischedel et al., 2012).", "startOffset": 178, "endOffset": 203}, {"referenceID": 55, "context": "Annotations were done after basic introduction to linguistics and coreference resolution and after discussing the annotation manual (S\u00fcrmeli et al., 2016).", "startOffset": 132, "endOffset": 154}, {"referenceID": 43, "context": "As the CoNLL reference coreference scorer (Pradhan et al., 2014) is based on CoNLL format, we provide tools for converting a document and a coreference XML file into a CoNLL file (and vice versa).", "startOffset": 42, "endOffset": 64}, {"referenceID": 5, "context": "In the future, to make the tool accessible to a wider part of the community, we consider integrating it into an existing software, for example BART (Broscheit et al., 2010).", "startOffset": 148, "endOffset": 172}, {"referenceID": 33, "context": "Merging several distinct coreference annotations into a single gold standard is a complex task, in particular if annotators do not agree on mentions. To simplify this task we created a tool that merges multiple annotations into a single solution according to objective (1) from Section 3.2. Manual intervention for editing mentions and chains is also possible, details about a preliminary (Phase I) version of the tool is described by Sch\u00fcller (2016). Note that, in Phase II we performed only automatic adjudication and did not need manual intervention.", "startOffset": 5, "endOffset": 451}, {"referenceID": 41, "context": "The baseline is based on Python and scikit-learn (Pedregosa et al., 2011).", "startOffset": 49, "endOffset": 73}, {"referenceID": 16, "context": "We considered to integrate also the Named Entity Recognition (NER) module of the ITU-pipeline (Eryi\u011fit, 2014) because NER is not annotated in the Turkish Treebank, however we found that the output the web service changed significantly several times during the development of the baseline.", "startOffset": 94, "endOffset": 109}, {"referenceID": 50, "context": "To facilitate reproducibility of results we decided to create a stand-alone baseline that allows reproducing our results using only the METU-Sabanci Turkish Treebank (Say et al., 2004) and scikit-learn.", "startOffset": 166, "endOffset": 184}, {"referenceID": 45, "context": "We have created a baseline for mention detection, based on the work of Sapena et al. (2012), and for coreference resolution, inspired by Bengtson and Roth (2008).", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "(2012), and for coreference resolution, inspired by Bengtson and Roth (2008). The baseline is based on Python and scikit-learn (Pedregosa et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 48, "context": "as mentions, similar to the approach of Sapena et al. (2012) for English.", "startOffset": 40, "endOffset": 61}, {"referenceID": 3, "context": "Our Coreference Resolution baseline is inspired by the work of Bengtson and Roth (2008) who describe a simple method with reasonable accuracy and without the need for custom machine learning models and algorithms that are specific to coreference resolution.", "startOffset": 63, "endOffset": 88}, {"referenceID": 14, "context": "SVR is based on support vector regression with a linear-kernel SVM (Drucker et al., 1997) trained on the same examples as SVC.", "startOffset": 67, "endOffset": 89}, {"referenceID": 3, "context": "For building chains we also start with one chain per mention, but this time we use the Best-Link (Bengtson and Roth, 2008) strategy: we iterate over mentions in order of occurrence in the document, and merge each mention with at most one predecessor chain if its highest-scored candidate link to a predecessor mention is above 0.", "startOffset": 97, "endOffset": 122}, {"referenceID": 43, "context": "1 Evaluation We evaluate our baseline using the CoNLL reference coreference scorer (Pradhan et al., 2014) and report MUC, B, CEAFm, CEAFe, and BLANC scores.", "startOffset": 83, "endOffset": 105}, {"referenceID": 59, "context": "Intuitively, MUC (Vilain et al., 1995) computes precision and recall of mention-mention links over all gold chains compared with all predicted chains.", "startOffset": 17, "endOffset": 38}, {"referenceID": 2, "context": "B (Bagga and Baldwin, 1998) computes precision and recall over each individual mention which makes the score also applicable to singleton mentions.", "startOffset": 2, "endOffset": 27}, {"referenceID": 31, "context": "CEAFm (Luo, 2005) creates an optimal matching between predicted and gold mentions and evaluates the percentage of mentions correctly assigned to chains, while CEAFe does the same from the perspective of chains.", "startOffset": 6, "endOffset": 17}, {"referenceID": 46, "context": "BLANC (Recasens and Hovy, 2010) gives equal importance to mention-mention links and non-existing mention-mention links, without the need to consider per-mention or per-chain score accumulations.", "startOffset": 6, "endOffset": 31}, {"referenceID": 3, "context": "As this is only a baseline, we did not include more sophisticated features described in (Bengtson and Roth, 2008).", "startOffset": 88, "endOffset": 113}, {"referenceID": 4, "context": "For example, semantic features based on WordNet (Bilgin et al., 2004; Miller, 1995) could rule out certain predicted mentions as relevant and thus could improve precision of the baseline.", "startOffset": 48, "endOffset": 83}, {"referenceID": 32, "context": "For example, semantic features based on WordNet (Bilgin et al., 2004; Miller, 1995) could rule out certain predicted mentions as relevant and thus could improve precision of the baseline.", "startOffset": 48, "endOffset": 83}, {"referenceID": 28, "context": "As the only pronoun anaphora annotations (K\u00fc\u00e7\u00fck and Y\u00f6ndem, 2007) that were done on the METUSabanci Turkish Treebank can no longer be found by the authors (personal communication), we have no possibility of validating the annotations performed in our annotation project.", "startOffset": 41, "endOffset": 65}, {"referenceID": 4, "context": "To improve the coreference resolution baseline, adding more complex features by integrating Turkish WordNet (Bilgin et al., 2004), Turkish NER (\u015eeker and Eryi\u011fit, 2012), and Turkish WSD (\u0130lgen et al.", "startOffset": 108, "endOffset": 129}, {"referenceID": 65, "context": ", 2004), Turkish NER (\u015eeker and Eryi\u011fit, 2012), and Turkish WSD (\u0130lgen et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 64, "context": ", 2004), Turkish NER (\u015eeker and Eryi\u011fit, 2012), and Turkish WSD (\u0130lgen et al., 2012) could be helpful.", "startOffset": 64, "endOffset": 84}, {"referenceID": 48, "context": "For a full processing pipeline from plain text to coreference annotations, we need at least morphological analysis (Sak et al., 2007), disambiguation (Sak et al.", "startOffset": 115, "endOffset": 133}, {"referenceID": 48, "context": ", 2007), disambiguation (Sak et al., 2007), and dependency parsing (Eryi\u011fit et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 17, "context": ", 2007), and dependency parsing (Eryi\u011fit et al., 2008).", "startOffset": 32, "endOffset": 54}, {"referenceID": 16, "context": "Available tools are the ITU-pipeline (Eryi\u011fit, 2014) and the older Zemberek system (Ak\u0131n and Ak\u0131n, 2007).", "startOffset": 37, "endOffset": 52}, {"referenceID": 0, "context": "Available tools are the ITU-pipeline (Eryi\u011fit, 2014) and the older Zemberek system (Ak\u0131n and Ak\u0131n, 2007).", "startOffset": 83, "endOffset": 104}], "year": 2017, "abstractText": "We describe the Marmara Turkish Coreference Corpus, which is an annotation of the whole METU-Sabanci Turkish Treebank with mentions and coreference chains. Collecting nine or more independent annotations for each document allowed for fully automatic adjudication. We provide a baseline system for Turkish mention detection and coreference resolution and evaluate it on the corpus.", "creator": "LaTeX with hyperref package"}}}