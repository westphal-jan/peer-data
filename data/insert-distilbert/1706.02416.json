{"id": "1706.02416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Generalized Value Iteration Networks: Life Beyond Lattices", "abstract": "alternatively in this paper, we introduce a generalized value iteration network ( sometimes gvin ), which is an end - to - end dynamic neural network planning module. gvin emulates the value iteration algorithm by using a novel graph convolution operator operator, which enables gvin to learn effectively and plan on irregular spatial spanning graphs. now we propose three novel differentiable kernels as numerical graph convolution operators build and show that the embedding based kernel achieves the best performance. we further propose episodic q - time learning, an improvement upon traditional local n - - step q - learning that stabilizes effective training for networks that contain necessarily a planning module. less lastly, we evaluate gvin on planning congestion problems in 2d parallel mazes, irregular graphs, and real - world street networks, showing that gvin generalizes well for both arbitrary graphs and unseen graphs of larger scale and outperforms perform a naive generalization of vin ( discretizing a spatial graph into a 2d image ).", "histories": [["v1", "Thu, 8 Jun 2017 00:04:05 GMT  (3121kb,D)", "http://arxiv.org/abs/1706.02416v1", "14 pages, conference"], ["v2", "Thu, 26 Oct 2017 15:23:18 GMT  (2969kb,D)", "http://arxiv.org/abs/1706.02416v2", "14 pages, conference"]], "COMMENTS": "14 pages, conference", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sufeng niu", "siheng chen", "hanyu guo", "colin targonski", "melissa c smith", "jelena kova\\v{c}evi\\'c"], "accepted": false, "id": "1706.02416"}, "pdf": {"name": "1706.02416.pdf", "metadata": {"source": "CRF", "title": "Generalized Value Iteration Networks: Life Beyond Lattices", "authors": ["Sufeng Niu", "Siheng Chen", "Hanyu Guo", "Jelena Kova\u010devi\u0107"], "emails": ["sniu@g.clemson.edu", "sihengc@andrew.cmu.edu", "hanyug@g.clemson.edu", "ctargon@g.clemson.edu", "smithmc@clemson.edu", "jelenak@cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep reinforcement learning (DRL) is a technique that uses sequential decision making to learn about an environment that lacks explicit rules and labels [22]. Recent developments in DRL have lead to enormous progress in autonomous driving [3], innovation in robot control [13], and humanlevel performance in both Atari games [15, 10] and the board game Go [20]. Given a reinforcement learning task, it is common practice to model the environment as an unknown Markov decision process (MDP) [1, 2]. The agent explores the underlying MDP and attempts to learn a mapping of highdimensional state space data to an optimal policy that maximizes the expected return. Reinforcement learning can be categorized into two classes of techniques: model-free [14, 16, 15] and model-based approaches [22, 8, 18]. Model-free approaches learn the policy directly by trial-and-error and attempt to avoid bias caused by a suboptimal environment model [22]; the majority of recent architectures for DRL follow the model-free approach [14, 16, 15]. Model-based approaches, on the other hand, allow for an agent to explicitly learn the mechanisms of an environment, which can lead to strong generalization abilities. A recent work, the value iteration network (VIN) [23], follows the modelbased approach; it combines recurrent convolutional neural networks and max-pooling to emulate the process of value iteration [1, 2]. As VIN learns an environment, it is able to plan shortest paths for new, unseen mazes.\nThe input data fed into deep learning systems is usually associated with regular structures. For example, speech signals and natural language have an underlying 1D sequential structure; images\n\u2217Equal contribution.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Barcelona, Spain.\nar X\niv :1\n70 6.\n02 41\n6v 1\n[ cs\n.L G\n] 8\nhave an underlying 2D lattice structure. To take advantage of this regularly structured data, deep learning uses a series of basic operations defined for the regular domain, such as convolution and uniform pooling. However, not all data is contained in regular structures. In urban science, traffic information is associated with road networks; in neuroscience, brain activity is associated with brain connectivity networks; in social sciences, users\u2019 profile information is associated with social networks. To learn from data with irregular structure, some recent works have extended the lattice structure to general graphs [7, 12] and redefined convolution and pooling operations on graphs; however, most works only evaluate data that has both a fixed and given graph. In addition, most lack the ability to generalize to new, unseen environments.\nIn this paper, we aim to enable an agent to self-\nlearn and plan the optimal paths in new, unseen spatial graphs by using model-based DRL and graph-based techniques; see example in Figure 1. This task is relevant to many realworld applications, such as route planning of self-driving cars and web crawling/navigation. The proposed method is more general than classical DRL, which only works for regular structures. Furthermore, the proposed method is scalable (computational complexity is proportional to the number of edges in the testing graph), handles various edge weight settings and adaptively learns the environment model. Note that the optimal path can be self-defined, and is not necessarily the shortest one.\nTo create GVIN, we generalize two aspects of the VIN. First, to work for irregular graphs, we propose a graph convolution operator that generalizes the original 2D convolution operator. With the new graph convolution operator, the proposed network captures the basic concepts of spatial graphs, such as direction, distance and edge weight. It also is able to transfer knowledge learned from one graph to others. Second, to improve reinforcement learning on irregular graphs, we propose a reinforcement learning algorithm, episodic Q-learning, which stabilizes the training of a network that contains a planning module. The original VIN is trained through either imitation learning, which requires a large number of ground-truth labels, or reinforcement learning, whose performance is relatively poor. With the proposed episodic Q-learning, the new network performs significantly better than VIN in the reinforcement learning mode. Since the proposed network generalizes the original VIN model, we call it the generalized value iteration network (GVIN). Note that a naive approach to generalize VIN is to discretize an spatial graph as a 2D image; however, this approach does not directly handle irregular structures and causes discretization error, which heavily downgrades the performance.\nThe main contributions of this paper are: \u2022 The proposed architecture, GVIN, generalizes the VIN [23] to handle both regular structures and irregular structures. GVIN offers an end-to-end architecture trained via reinforcement learning (no ground-truth labels); see Section 3.1; \u2022 The proposed graph convolution operator generalizes the 2D convolution and learns the concepts of direction and distance, which enables GVIN to transfer knowledge from one graph to another; see Section 3.2; \u2022 The proposed reinforcement learning algorithm, episodic Q-learning, extends the classical n-step Q-learning by changing the conditions of termination and significantly improves the performance of reinforcement learning for irregular graphs; see Section 3.3; and \u2022We conducted intensive experiments and demonstrate the generalization ability of GVIN within imitation learning and episodic Q-learning for various datasets, including synthetic 2D maze data, irregular graphs, and real-world maps (Minnesota high way and New York street map); we show that GVIN significantly outperforms VIN with discretization input on irregular structures; See Section 4."}, {"heading": "2 Background", "text": "Markov Decision Process. We consider an environment defined as an MDP that contains a set of states s \u2208 S, a set of actions a \u2208 A, a reward function Rs,a, and a series of transition probabilities\nPs\u2032,s,a, the probability of moving from the current state s to the next state s\u2032 given an action a. The goal of an MDP is to find a policy that maximizes the expected return (accumulated rewards) Rt = \u2211\u221e k=0 \u03b3\nkrt+k, where rt+k is the immediate reward at the (t+ k)th time stamp and \u03b3 \u2208 (0, 1] is the discount rate. A policy \u03c0a,s is the probability of taking action a when in state s. The value of state s under a policy \u03c0, v\u03c0s , is the expected return when starting in s and following \u03c0; that is, v\u03c0s = E[Rt|St = s]. The value of taking action a in state s under a policy \u03c0, q\u03c0(a)s , is the expected return when starting in s, taking the action a and following \u03c0; that is, q\u03c0(a)s = E[Rt|St = s,At = a]. There is always at least one policy that is better than or equal to all other policies, called an optimal policy \u03c0\u2217; that is, the optimal policy is \u03c0\u2217 = arg max\u03c0 v\u03c0s , the optimal state-value function is v\u2217s = max\u03c0 v \u03c0 s , and the optimal action-value function is q \u2217(a) s = max\u03c0 q \u03c0(a) s . To obtain \u03c0 \u2217 and v\u2217, we usually consider solving the Bellman equation. Value iteration is a popular algorithm used to solve the Bellman equation in the discrete state space; that is, we iteratively compute vs \u2190 maxa \u2211 s\u2032 Ps\u2032,s,a (Rs,a +\u03b3vs\u2032) until the result converges.\nDeep Reinforcement Learning. Although most works focus on the model-free approach, some recent model-based works also show promising results. VIN employs an embedded differentiable planning architecture, trained end-to-end via imitation learning [23]. VIN recovers the underlying MDP by using the standard value iteration algorithm. The Bellman equation is encoded within the convolutional neural network framework, and thereby, the policy can be obtained through backpropagation. VIN demonstrates the ability to diffuse the value function throughout a dynamic model that is position-invariant. However, VIN has some shortcomings: it is limited to regular lattice space, it requires imitation learning for maximum performance, and it is trained separately with a reactive policy. A different model-based work, Predictron, uses a learning and planning model that simulates a Markov reward process [21]. The architecture unrolls the \"imagined\" plan via a predictron core, where each core (multi-layer convolutional neural network) represents one step in a MDP. Similar to VIN, the predictron also approximates the true value function through end-to-end training; however, Predictron is limited to the tasks of value prediction and is relatively expensive when compared to VIN.\nDeep Learning with Graphs. A number of recent works in deep learning use neural networks to handle signals supported on graphs [17, 9, 11]. The principal idea is to generalize basic operations in the regular domain, such as filtering and pooling, to the graph domain based on spectral graph theory. For example, [6, 11] introduce hierarchical clustering on graphs and the spectrum of the graph Laplacian to neural networks; [7] generalizes classical convolutional neural networks by using graph coarsening and localized convolutional graph filtering; [12] considers semi-supervised learning with graphs by using graph-based convolutional neural networks; and [19] generalizes the classical recurrent neural networks by combining graph-based convolutional neural networks and recurrent neural networks to find dynamic patterns. A recent overview is provided in [5]. All of the mentioned works consider signals supported on a fixed and known graph, consequentially preventing the training parameters to be transferable to other graphs."}, {"heading": "3 Methodology", "text": "In this section, we propose a new model-based DRL framework, GVIN, which takes a general graph with a starting node and a goal as input and outputs the designed path. The overall goal of GVIN is to learn an underlying MDP that summarizes the optimal planning policy applied for arbitrary graphs. This requires GVIN to capture general knowledge about planning, which is structure and transition invariant and does not depend on any specific graph structure. A key component of a MDP is the transition matrix, which is needed to solve the Bellman equation. To train a general transition matrix that works for arbitrary graphs, similar to the VIN, we parameterize the transition matrix by using graph-based kernel functions. Each graph-based kernel function represents a unique action pattern. We train the parameters in GVIN by using episodic Q-learning, which makes reinforcement learning on irregular graphs practical."}, {"heading": "3.1 Framework", "text": "The input of GVIN includes a graph with a starting node and a goal node. During the training phase, GVIN trains the parameters by trial-and-error in various graphs; during the testing phase, GVIN plans the path based on the trained parameters. The framework includes the planning module (left) and the action module (right), shown in Figure 2. The planning module emulates value iteration by iteratively operating the graph convolution and max-pooling, and the action module takes the greedy action according to the state values.\nMathematically, we consider a directed, weighted spatial graph G = (V,X, E ,A), where V = {v1, ..., vN} is the node set, X \u2208 RN\u00d72 are the node embeddings with the ith row Xi \u2208 R2 being the embedding of the ith node (here we consider 2D spatial graphs, but the method is generalizable), E = {e1, ..., eM} is the edge set, and A \u2208 RN\u00d7N is the adjacency matrix, with the (i, j)th element Ai,j representing the edge weight between the ith and jth nodes. We consider a graph signal as a mapping from the nodes to real values. We use a graph signal g \u2208 {0, 1}N to encode the goal node, where g is one-sparse and only activates the goal node. Let r \u2208 RN , v \u2208 RN , and q \u2208 RN be the reward graph signal, the state-value graph signal, and the action-value graph signal, respectively. We represent the entire process in a matrix-vector form as follows,\nr = fR(g;wr), (1)\nP(a) = fP (G;wP(a)), (2)\nq(a) = P(a) (r + \u03b3v) , (3)\nv = max a\nq(a), (4)\nwhere P(a) is the graph convolution operator in the ath channel and wr and wP(a) are training parameters to parameterize r and P(a), respectively. In the preprocessing step (1), we extract features from the goal g to obtain the robust reward r via the feature-extract function fR(\u00b7), which is a convolutional neural network; in the graph convolution step (2), we train a set of graph convolution operators based on the graph G, on which we elaborate in Section 3.2; in (3) and (4), we emulate value iteration by using graph convolution to obtain the action-value graph signal q(a) in the ath channel and max-pooling to obtain the state-value graph signal v. As shown in Figure 2, we repeat the graph convolution operation (3) and max-pooling (4) for K iterations to obtain the final state-value graph signal v\u0302. When G is a 2D lattice, the planning module of GVIN degenerates to VIN.\nIn the training phase, we feed the final state-value graph signal v\u0302 to the action module. The original VIN extracts the action values from step (3) and trains the final action probabilities for eight directions; however, this is problematic for irregular graphs, as the number of actions (neighbors) at each node varies. To solve this, we consider converting v\u0302 to a pesudo action-value graph signal, q\u0302 \u2208 RN , whose sth element is q\u0302s = maxs\u2032\u2208Nei(s) v\u0302s\u2032 , representing the action value moving from s to one of its neighbors. The advantages of this approach come from the following three aspects: (1) the final state value of each node is obtained by using the maximum action values across all the channels, which is robust to small variations; (2) the pesudo action-value graph signal considers a unique action for each node and does not depend on the number of actions; that is, at each node, the agent queries the state values of its neighbors and always moves to the one with the highest value; and (3) the pesudo action-value graph signal considers local graph structure, because the next state is always chosen from one of the neighbors of the current state.\nWe then input the pesudo action-value graph signal to episodicQ-learning, which learns from trial-anderror experience and backpropagates to update all of the training parameters. In episodic Q-learning, each episode is obtained as follows: for each given starting node s0, the agent will move sequentially from st to st+1 by the -greedy strategy; that is, with probability (1\u2212 ), st+1 = arg maxs\u2032\u2208Nei(st) v\u0302s\u2032 and with probability , st+1 is randomly selected from one of the neighbors of st. An episode\nterminates when st+1 is the goal state or the maximum step threshold is reached. For each episode, we consider the loss function as, L(w) = \u2211T t=1 (Rt \u2212 q\u0302st) 2 , where q\u0302st is a function of the training parameters w = [wr,wP(a) ] in GVIN, T is the episode length and Rt is the expected return at time stamp t, defined as Rt = (rt+1 + \u03b3Rt+1), where \u03b3 is the discount factor, and rt is the immediate return at time stamp t. Additional details of the algorithm will be discussed in Section 3.3. In the testing phase, we obtain the action by greedily selecting the maximal state value; that is, st+1 = arg maxs\u2032\u2208Nei(st) v\u0302s\u2032 ."}, {"heading": "3.2 Graph Convolution", "text": "We now discuss the graph convolution operator in (2). In the original VIN, the inputs are images and the corresponding G is a 2D lattice graph, whose nodes are pixels. Each node has the same local structure, sitting on a grid and connecting with its immediate neighboring eight nodes via edges. In this case, it is easy to obtain a structure and translation invariant operator, because all images share the same 2D lattice structure. In this regular case, GVIN operates the same as VIN when we set G be the 2D lattice graph and each wP(a) be coefficients of a 3\u00d7 3 filter; in other words, VIN is a special case of GVIN when the underlying graph is a 2D lattice.\nIn irregular graphs, however, nodes may have diverse local structures, making it challenging to obtain a structure and translation invariant operator that transfers knowledge from one graph to another. We believe this is possible based on the way humans learn to navigate. For example, when we drive from one place to another, we use direction, distance, and local traffic condition as the principles to select a path no matter where we are. If GVIN can learn explicit principles such as these or some other hidden factors, it should be able to generalize excellently for irregular spatial graphs. To achieve this, we parameterize the graph convolution operator by using graph-based kernel functions. Here we consider three types, including the directional kernel, the spatial kernel, and the embedding kernel; see details in Supplementary 6.3."}, {"heading": "3.3 Training via Reinforcement Learning", "text": "We propose a new reinforcement learning algorithm, called episodic Q-learning, to train GVIN. The difference between episodic Q-learning and the n-step Q-learning is that the n-step Q-learning has a fixed episode duration and updates the training weights after n steps; while in episodic Q-learning, each episodic terminates until the agent reaches the goal or the maximum step threshold is reached, and we update the training weights after the entire episode. During experiments, we found that for both regular and irregular graphs, the policy planned by the original Q-learning keeps changing and does not converge due to the frequent updates. Similar to the Monte Carlo algorithms [22], episodic Q-learning first selects actions by using its exploration policy until we reach the goal. Afterwards, we accumulate the gradients during the entire episode and then update the training weights. This allows the agent to use a stable plan to complete an entire episode. This simple change greatly improves the performance (see Section 4.1). The pseudocode for the algorithm is presented in Algorithm 1 (Supplementary 6.2)."}, {"heading": "4 Experimental Results", "text": "In this section, we evaluate the proposed method on three types of graphs: 2D mazes, synthesized irregular graphs and real road networks. We first validate that the proposed GVIN is comparable to the original VIN for 2D mazes, which have regular lattice structure. We next show that the proposed GVIN automatically learns the concepts of direction and distance in synthesized irregular graphs through the reinforcement learning setting (without using any ground-truth labels). Finally, we use the pre-trained GVIN model to plan paths for the Minnesota road network and Manhattan street network. Additional experiment parameter settings are listed in the Supplementary materials 6.4."}, {"heading": "4.1 Revisting 2D Mazes", "text": "Given a starting point and a goal location, we consider planning the shortest paths for 2D mazes; see Figure 8(a) (Supplementary) as an example. We generate 22, 467 2D mazes (16\u00d7 16) using the\nsame scripts2 that VIN used. We use the same configuration as VIN (6/7 data for training and 1/7 data for testing). Here we consider four comparisons: VIN vs. GVIN, action-value based imitating learning vs. state-value based imitating learning, direction-guided GVIN vs. unguided GVIN, and reinforcement learning.\nFour metrics are used to quantify the planning performance, including prediction accuracy\u2014the probability of taking the ground-truth action at each state (higher means better); success rate\u2014the probability of successfully arriving at the goal from the start state without hitting any obstacles (higher means better); path difference\u2014the average length difference between the predicted path and the ground-truth path (lower means better); and expected reward\u2014the average accumulated reward (higher means better). We consider the rules as follows: the agent receives a +1 reward when reaching the goal, receives a \u22121 reward when hitting an obstacle, and each movement gets a \u22120.01 reward. The overall testing results are summarized in Table 1.\nVIN vs. GVIN. GVIN (red in Table 1) performs competitively with VIN (blue in Table 1), especially when GVIN uses direction-aware action-value based imitation learning (4th column in Table 1), which outperforms the others for all four metrics. Figure 8(b) (Supplementary) shows the value map learned from GVIN with direction-unaware state-value based imitation learning. We see negative values (in blue) at obstacles and positive values (in red) around the goal, which is similar to the value map that VIN reported in [23]. This validates that the proposed GVIN achieves state-of-art performance when assessed on 2D mazes.\nAction-value vs. State-value. VIN with action-value imitation learning slightly outperforms VIN with state-value imitation learning. Similarly, GVIN with action-value based imitation learning slightly outperforms GVIN with state-value based imitation learning. The results suggest that our action approximation method (Section 3.1) does not impact the performance while maintaining the ability to be extended to irregular graphs.\nDirection-aware GVIN vs. Unaware GVIN.\nDirection-aware GVIN slightly outperforms direction-unaware GVIN. This is reasonable because the fixed eight directions are ground truth for regular 2D mazes. It remains encouraging that the GVIN is able to find the groundtruth directions through imitation learning. As shown later, direction-unaware GVIN outperforms direction-aware GVIN in irregular graphs. Figures 4(a) and (b) show that the planning performance improves as the kernel exponential t in (5) increases due to the resolution in the reference direction being low when t is small. Figure 7 (in Supplementary) compares the ker-\nnel with the same reference direction, but two different kernel orders. When t = 5, the kernel activates wide-range directions; when t = 100, the kernel focuses on a small-range directions and has a higher resolution.\nReinforcement Learning. We also examine the performance of episodic Q-learning (Section 3.3) in VIN. Table 5 (Supplementary) shows that the episodic Q-learning algorithm outperforms the training method used in VIN (TRPO + curriculum learning). For the results reported in Table 5, we were able to train the VIN using our algorithm (episodic Q-learning) in just 200 epochs, while TRPO and\n2https://github.com/avivt/VIN\ncurriculum learning took 1000 epochs to train VIN, as reported in [23] (both algorithms used the same settings). Additionally, as shown in Figure 3, the episodic Q-learning algorithm demonstrates faster convergence and better overall performance when compared with Q-learning."}, {"heading": "4.2 Exploring Irregular Graphs", "text": "We consider four comparisons in the following experiments: Directional kernel vs. Spatial kernel vs. Embedding-based kernel, direction-aware vs. direction-unaware, scale generalization, and reinforcement learning vs. imitation learning. We use the same performance metrics as the previously discussed 2D maze experiments.\nDirectional Kernel vs. Spatial Kernel vs. Embedding-based Kernel. We first train the GVIN via imitation learning. Table 2 shows that the embedding-based kernel outperforms the other kernel methods in terms of both action prediction and path difference (5th column in Table 2), indicating that the embedding-based kernel captures the edge weight information (distance) within the neural network weights better than the other methods. The spatial kernel demonstrates higher accuracy and success rate when compared with the directional kernel, which suggests the effectiveness of using bin sampling. The direction-unaware method shows slightly better results for the spatial kernel, but has a larger success rate gain for the directional kernel. Figure 8(d) (Supplementary) shows the visualization of the learned value map which shares similar properties with the regular graph value map. We also train VIN (1st column) by converting graph to 2D image. As shown in the Table, the VIN is significantly failed (See Supplementary 6.4).\nFigures 4(c) and (d) show the planning performance for the irregular domain as the kernel order t in 5 increases. The results show that a larger t in the irregular domain has the opposite effect when compared with the regular domain. The observation is reasonable: in the irregular domain, the direction of each neighbor is extremely variable and a larger kernel order creates a narrower direction range (as seen in Figure7), thus resulting in information loss.\nReinforcement Learning. We then train the GVIN using episodic Q-learning to compare with imitation learning. As a baseline, we also train GVIN by using standard deep Q-learning techniques, including using an experience replay buffer and a target network. Both networks use the same kernel function (embedding-based kernel) and configurations. Figure 5 shows the comparison of the two algorithms\u2019 success rate and expected rewards during the training. Clearly, episodic Q-learning\nImitation Learning Reinforcement Learning w edge weight w/o edge weight w edge weight w/o edge weight\nMinnesota New York City Optimal |V| = 100 |V| = 10 Optimal |V| = 100 |V| = 10\nconverges to both a high success rate and a high expected rewards, but the standard deep Q-learning techniques fail to achieve reasonable results.\nScale Generalization. We also examine the\nscale generalization by training on 10-node graphs and then testing on 100-node graphs using the embedding-based kernel. When GVIN is trained on 10-node graphs via imitation learning, the performance is significantly hindered as shown in Table 2 (6th column). When GVIN is trained using episodic Q-learning, Table 2 (7th column) shows excellent generalization abilities that outperform all imitation learning based results for success rate and expected rewards. Compared with imitation learning, we also observe the performance decreases for path differences and action prediction.\nGraph with Edge Weights. We also inquired if GVIN could capture complex graphical relationships within the kernel. We set the true weighted shortest path to be Xi\u2212XjWij , where Xi\u2212Xj is the distance between two nodes and Wij is the edge weight. As shown in Table 3, imitation learning is trained on 100-node graphs, while reinforcement learning is trained on 10-node. We also examine the GVIN by excluding edge weights from the input to see if there are any effects on performance. Table 3 shows that for reinforcement learning, edge weights slightly help the agent find a more suitable policy. In imitation learning, edge weights in the input cause the policy to significantly fail."}, {"heading": "4.3 Validating Real Road Networks", "text": "To demonstrate the generalization capabilities of GVIN, we evaluate two real-world maps: the Minnesota high way maps, which contains 2642 nodes representing intersections and 6606 edges representing roads, and the New York City street map, which contains 5069 nodes representing intersections and 13368 edges representing roads. We use the same models trained on the graphs containing |V| = 100 and |V| = 10 nodes with the embedding-based kernel and using episodic Q-learning in Section 4.2, separately. We normalize the data coordinates between 0 and 1, and we set recurrence parameter toK = 200. We randomly pick start points and goal points 1000 different times. We use the A* algorithm as a baseline. Table 4 shows that both |V| = 100 and |V| = 10 generalize well on large scale data. The policy could reach the goal position with 100% in the experiments. One sample planned path is shown in the Supplementary material (Figures 9 and 10)."}, {"heading": "5 Conclusions", "text": "We have introduced GVIN, a differentiable, novel planning module capable of both regular and irregular graph navigation and impressive scale generalization. We also introduced episodic Qlearning, an algorithm designed to stabilize the training process of neural networks that contain a planning module. In future work, we intend to explore broader applications and approaches in an effort to encode the edge weight information into the network."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Computational Complexity", "text": "Let the input graph G have |V| nodes and |E| edges. In the testing phase, the computational complexities of (1), (3) and (4) are O(|V|), O(|E|) and O(|V|), respectively. Therefore, the total computational complexity is O(|V|+K(|E|+ |V|)), where K is number of iterations. For a spatial graph, the number of edges is usually proportional to the number of nodes; thus the computational complexity is O(K|V|), which is scalable to huge graphs.\n6.2 Episodic Q-learning\nWe highlight the differences between episodic Q-learning and the original n-step Q-learning in blue, including the initial expected return, the termination condition and the timing of updating the gradient.\nAlgorithm 1 Episodic Q-learning 1: input graph G and the goal sg 2: initialize global step counter T = 0 3: initialize GVIN parameters w = [wr,wP(a) ] 4: initialize parameter gradients \u2206w 5: repeat (one episode) 6: clear gradients \u2206w\u2190 0 7: t = 0 8: randomly pick a start node st 9: repeat (one action) 10: take action at according to the -greedy policy based on q (a) st 11: receive reward rt and new state st+1 12: t\u2190 t+ 1 13: until terminal st = sg or t > tmax 14: R = 0 15: for i = t : \u22121 : 0 do 16: R\u2190 ri + \u03b3R 17: accumulate gradients wrt w : \u2206w\u2190 \u2206w + \u2202(R\u2212q (a) st )2 \u2202w 18: end for 19: w\u2190 w \u2212\u2206w"}, {"heading": "20: T = T + 1", "text": "21: until T > Tmax"}, {"heading": "6.3 Graph-based Kernel Functions", "text": "Directional Kernel. We first consider direction. When we face several roads at an intersection, it is straightforward to pick the one whose direction points to the goal. We aim to use the directional kernel to capture edge direction and parameterize the graph convolution operation.\nThe (i, j)th element in the graph convolution operator models the probability of following the edge from i to j ; that is,\nPi,j = Ai,j \u00b7 L\u2211 `=1 w`K (t,\u03b8`) d (\u03b8ij) , where K (t,\u03b8`) d (\u03b8) = ( 1 + cos(\u03b8 \u2212 \u03b8`) 2 )t . (5)\nwhere w` is kernel coefficient, \u03b8ij is the direction of the edge connecting the ith and thejth nodes, which can be computed through the node embeddings Xi \u2208 R2 and Xj \u2208 R2, and K(t,\u03b8`)d (\u03b8) is the directional kernel with order t and reference direction \u03b8`, reflecting the center of the activation. The hyperparameters include the number of directional kernels L and the order t, reflecting the directional resolution (a larger t indicates more focus in one direction); see Figure 7. The kernel coefficient w` and the reference direction \u03b8` are the training parameters, which is wP in (2). Note that the graph\nconvolution operator P \u2208 RN\u00d7N is a sparse matrix and its sparsity pattern is the same with the input adjacency matrix, which ensures that the computation is cheap.\nThe intuition behind (5) is that each graph convolution\noperator represents a unique direction pattern. An edge is a 2D vector sampled from the 2D spatial plane. When the direction of the edge connecting i and j matches one or some of the L reference directions, we have a higher probability to follow the edge from i to j. In GVIN, we consider several channels. In each channel, we obtain an action value for each node, which represents the matching coefficient of a direction pattern. The max-pooling operation then selects the most matching direction pattern for each node.\nSpatial Kernel. We next consider both direction and distance. When all the roads at the current intersection opposite to the goal, it is straightforward to try the shortest one first. We thus include the edge length into the\nconsideration. The (i, j)th element in the graph convolution operator is then, Pi,j = Ai,j \u00b7 L\u2211 `=1 w`K (d`,t,\u03b8`) s (dij , \u03b8ij) , whereK (d`,t,\u03b8`) s (d, \u03b8) = I|d\u2212d`|\u2264 ( 1 + cos(\u03b8 \u2212 \u03b8`) 2 )t , (6) where dij is the distance between the ith and the jth nodes, which can be computed through the node embeddings Xi \u2208 R2 and Xj \u2208 R2, K(d`,t,\u03b8`)s (d, \u03b8) is the spatial kernel with reference distance d` and reference direction \u03b8` and the indicator function I|d\u2212d`|\u2264 = 1 when |d \u2212 d`| \u2264 and 0, otherwise. The hyperparameters include the number of directional kernels L, the order t, the reference distance d` and the distance threshold . The kernel coefficient w` and the reference direction \u03b8` are training parameters, which is wP in (2).\nCompared to the directional kernel, the spatial kernel\nadds another dimension, distance; in other words, the directional kernel is a special case of the spatial kernel when we ignore the distance. Each spatial kernel activates a localized area in the direction-distance plane. With the spatial kernel, the graph convolution operator (6) represents a unique direction-distance pattern; that is, if the direction/distance of the edge connecting i and j matches one or some of the L reference directions and distances, we have a higher probability to follow the edge from i to j.\nEmbedding-based Kernel. In the directional kernel and spatial kernel, we manually design the kernel and hint GVIN to learn useful direction-distance patterns. Now we\ndirectly feed the node embeddings and allow GVIN to automatically learn implicit hidden factors for general planning. The (i, j)th element in the graph convolution operator is then,\nPi,j = (Ii=j +Ai,j)\u221a\u2211 k(1 + Ak,j) \u2211 k(1 + Ai,k) \u00b7Kemb (Xi,Xj) , (7)\nwhere the indicator function Ii=j = 1 when i = j, and 0, otherwise, and the embedding-based kernel function is Kemb (Xi,Xj) = mnnet ( [Aij ,Xi\u2212Xj ,Xi,Xj ,Xi\u2212Xg ]), with mnnet(\u00b7) is a standard multi-layer neural network. The inputs include the edge weight Ai,j , Xi and Xj node embeddings, the difference between two node embeddings Xi\u2212Xj and the embedding difference between Xi and Xg (goal node embedding), which ensures the embedding kernel is structure and translation invariant. The training parameters wP in (2) are the weights in the multi-layer neural network. Note that the graph convolution operator P \u2208 RN\u00d7N is still a sparse matrix and its sparsity pattern is the same with the input adjacency matrix plus the identity matrix.\nIn the directional kernel and spatial kernel, we implicitly discretize the space based on the reference direction and distance; that is, for each input pair of given direction and distance, the kernel function outputs the response based on its closed reference direction and distance. In the embedding-based kernel, we do not set the reference direction and distance to discretize the space; instead, we use the multi-layer neural network to directly regress from an arbitrary edge (with edge weight and embedding representation) to a response value. The embedding-based kernel is thus more flexible than the directional kernel and spatial kernel and may learn hidden factors."}, {"heading": "6.4 Experiments Settings", "text": "Our implementation is based on Tensorflow with GPU-enabled platform. All experiments use the standard centered RMSProp algorithm as the optimizer with learning rate \u03b7 = 0.001 [24]. All reinforcement learning experiments use a discount of \u03b3 = 0.99, RMSProp decay factor of \u03b1 = 0.999, and exploration rate annealed linearly from 0.2 to 0.001 over the first 200 epochs."}, {"heading": "6.4.1 2D Mazes", "text": "Parameter Settings. The experiments are set up as follows. To preprocess the input data, we use the same two-layer CNN for both VIN and GVIN, where the first layer involves 150 kernels with size 3\u00d7 3 and the second layer involves a kernel with size 3\u00d7 3 for output. The transition probability matrix is parameterized by 10 convolution kernels with size 3\u00d7 3 in both VIN and GVIN. In GVIN, we use the directional kernel based method as shown in Equations 5 and 6 and we set ` = 8 to represent the eight reference directions. We consider two approaches to initialize the directions \u03b8`. In direction-aware approach, we fix \u03b8` as [0, \u03c0/4, \u03c0/2, ..., 7\u03c0/4]. In the direction-unaware approach, we set \u03b8` to be weights and train them via backpropagation. We set the recurrence K in GVIN to be 20 for 16\u00d7 16 2D mazes. In the regular domain, we set the kernel order t = 100 to be the default."}, {"heading": "6.4.2 Irregular Graphs", "text": "Parameter Settings. We evaluate our proposed methods in Section 3.2 for the irregular domain. Our experimental domain is a synthetic data which consists of N = 10000 irregular graphs, in which each graph contains 100 nodes. For each node in the graph, we define the coordinates which represent its spatial position ranged between 0 and 1. The dataset is split into 7672 graphs for training and 1428 graphs for testing. Additionally, to exam whether GVIN could handle weighted graphs, we also generated a synthetic dataset consisting of 100 node graphs and partitioned 42857 graphs for training and 7143 graphs for testing.\nFor the directional kernel and spatial kernel, we set the number of reference directions to ` = 8 and kernel order t = 20 to be default values for Equations 5 and 6. We also set \u03b8` to be 0 to 2\u03c0 with an interval of \u03c0/4 for direction-aware mode and we set \u03b8` to be trainable weights for direction-unaware mode. For the spatial kernel function, we set the number of bins d` to be 10 in Equation 6. In the embedding-based kernel, we use three layers of fully connected neural networks, where each layer uses ReLU(\u00b7) = max(0, \u00b7) as its activation function. For all three kernel methods, we set the graph convolution channel number to be 10 and K = 40 for recurrence.\nTraining VIN on Irregular Graphs. To show the strong generalization of GVIN, we evaluate the VIN on irregular graph by converting graph data format to 2D image. Each testing set contains reward map and obstacle map that sizes 100\u00d7 100 pixels. We use pre-trained weights from 28\u00d7 28 maze with all parameters tuned to be highest performance. To make the training and testing consistent, we set the rewards map and obstacle map the same settings as 2D maze: vertices and edges are marked as free path (value set to be 0), while the other area are marked as obstacles (value set to be 1). The edge path is generated via Bresenham\u2019s line algorithm [4]. We set recurrence K to be 200 so that the value iteration could cover the whole map."}, {"heading": "6.5 Minnesota and NYC Sample Planning", "text": ""}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we introduce a generalized value iteration network (GVIN), which<lb>is an end-to-end neural network planning module. GVIN emulates the value<lb>iteration algorithm by using a novel graph convolution operator, which enables<lb>GVIN to learn and plan on irregular spatial graphs. We propose three novel<lb>differentiable kernels as graph convolution operators and show that the embedding-<lb>based kernel achieves the best performance. We further propose episodic Q-<lb>learning, an improvement upon traditional n-stepQ-learning that stabilizes training<lb>for networks that contain a planning module. Lastly, we evaluate GVIN on planning<lb>problems in 2D mazes, irregular graphs, and real-world street networks, showing<lb>that GVIN generalizes well for both arbitrary graphs and unseen graphs of larger<lb>scale and outperforms a naive generalization of VIN (discretizing a spatial graph<lb>into a 2D image).", "creator": "LaTeX with hyperref package"}}}