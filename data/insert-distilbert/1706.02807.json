{"id": "1706.02807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Learning to Embed Words in Context for Syntactic Tasks", "abstract": "we present models for embedding words in the context ordering of semantic surrounding words. such models, which we refer to independently as token character embeddings, represent the characteristics chosen of a word that are specific to a given context, principles such as word sense, syntactic category, and semantic role. we explore rather simple, efficient token embedding models based on standard neural network architectures. we learn token embeddings on representing a large amount of unannotated text and help evaluate them as features for part - of - speech input taggers and dependency parsers trained on much smaller amounts of annotated data. we find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.", "histories": [["v1", "Fri, 9 Jun 2017 01:39:12 GMT  (1172kb)", "https://arxiv.org/abs/1706.02807v1", null], ["v2", "Mon, 12 Jun 2017 01:42:12 GMT  (1172kb)", "http://arxiv.org/abs/1706.02807v2", "Accepted by ACL 2017 Repl4NLP workshop"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lifu tu", "kevin gimpel", "karen livescu"], "accepted": false, "id": "1706.02807"}, "pdf": {"name": "1706.02807.pdf", "metadata": {"source": "CRF", "title": "Learning to Embed Words in Context for Syntactic Tasks", "authors": ["Lifu Tu Kevin Gimpel", "Karen Livescu"], "emails": ["lifu@ttic.edu", "kgimpel@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n02 80\n7v 2\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\nin the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data. We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes."}, {"heading": "1 Introduction", "text": "Word embeddings have enjoyed a surge of popularity in natural language processing (NLP) due to the effectiveness of deep learning and the availability of pretrained, downloadable models for embedding words. Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).\nThe majority of this work has focused on a single embedding for each word type in a vocabulary.1 We will refer to these as type embed-\n1A word type is an entry in a vocabulary, while a word token is an instance of a word type in a corpus.\ndings. However, the same word type can exhibit a range of linguistic behaviors in different contexts. To address this, some researchers learn multiple embeddings for certain word types, where each embedding corresponds to a distinct sense of the type (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014). But token-level linguistic phenomena go beyond word sense, and these approaches are only reliable for frequent words.\nSeveral kinds of token-level phenomena relate directly to NLP tasks. Word sense disambiguation relies on context to determine which sense is intended. POS tagging, dependency parsing, and semantic role labeling identify syntactic categories and semantic roles for each token. Sentiment analysis and related tasks like opinion mining seek to understand word connotations in context.\nIn this paper, we develop and evaluate models for embedding word tokens. Our token embeddings capture linguistic characteristics expressed in the context of a token. Unlike type embeddings, it is infeasible to precompute and store all possible (or even a significant fraction of) token embeddings. Instead, our token embedding models are parametric, so they can be applied on the fly to embed any word in its context.\nWe focus on simple and efficient token embedding models based on local context and standard neural network architectures. We evaluate our models by using them to provide features for downstream low-resource syntactic tasks: Twitter POS tagging and dependency parsing. We show that token embeddings can improve the performance of a non-structured POS tagger to match the state of the art Twitter POS tagger of Owoputi et al. (2013). We add our token embeddings to Tweeboparser (Kong et al., 2014), improving its performance and establishing a new state of the art for Twitter dependency parsing."}, {"heading": "2 Related Work", "text": "The most common way to obtain context-sensitive embeddings is to learn separate embeddings for distinct senses of each type. Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pin\u0303a and Johansson, 2015; Wu and Giles, 2015). Some use bilingual information (Guo et al., 2014; S\u030custer et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014).\nThese \u201cmulti-type\u201d embeddings are restricted to modeling phenomena expressed by a single clustering of tokens for each type. In contrast, token embeddings are capable of modeling information that cuts across phenomena categories. Further, as the number of clusters grows, learning multitype embeddings becomes more difficult due to data fragmentation. Instead, we learn parametric models that transform a type embedding and those of its context words into a representation for the token. While multi-type embeddings require more data for training, parametric models require less.\nThere is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (Ka\u030ageba\u0308ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al., 2011), or other architectures. However, learning to represent tokens in supervised training can suffer from limited data. We instead focus on learning token embedding models on unlabeled data, then use them to produce features for downstream tasks. So we focus on efficient architectures and unsupervised learning criteria.\nThe most closely related work consists of efforts to train LSTMs to represent tokens in context using unsupervised training objectives. Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of their translation targets, while Melamud et al. (2016) and Peters et al. (2017) use unsupervised learning with monolingual sentences. We exper-\niment with LSTM token embedding models as well, though we focus on different tasks: POS tagging and dependency parsing. We generally found that very small contexts worked best for these syntactic tasks, thereby limiting the usefulness of LSTMs as token embedding models."}, {"heading": "3 Token Embedding Models", "text": "We assume access to pretrained type embeddings. Let W denote a vocabulary of word types. For each word type x \u2208 W , we denote its type embedding by vx \u2208 R d.\nWe define a word sequence x = \u3008x1, x2, ..., x|x|\u3009 in which each entry xj is a word type, i.e., xj \u2208 W . We define a word token as an element in a word sequence. We consider the class of functions f that take a word sequence x and index j of a particular token in x and output a vector of dimensionality d\u2032. We will refer to choices for f(x, j) as encoders."}, {"heading": "3.1 Feedforward Encoders", "text": "Our first encoder is a basic feedforward neural network that embeds the sequence of words contained in a window of text surrounding word j. We use a fixed-size window containing word j, the w\u2032 words to its left, and the w\u2032 words to its right. We concatenate the vectors for each word type in this window and apply an affine transformation followed by a nonlinearity:\nfFF(x, j) = g (\nW (D)[vxj\u2212w\u2032 ;vx(j\u2212w\u2032)+1 ; ...;vxj+w\u2032 ] + b (D)\n)\nwhere g is an elementwise nonlinear function (e.g., tanh),W (D) is a d\u2032 by d(2w\u2032+1) parameter matrix, semicolon (;) denotes vertical concatenation, and b(D) \u2208 Rd \u2032 is a bias vector. We assume that x is padded with start-of-sequence and endof-sequence symbols as needed. The resulting d\u2032dimensional token embedding can be transformed by additional nonlinear layers.\nThis encoder does not distinguish word j other than by centering the window at its position. It is left to the training objectives to place emphasis on word j as needed (see Section 3.3). Varying w\u2032 will influence the phenomena captured by this encoder, with smaller windows capturing similarity in terms of local syntactic category (e.g., noun vs. verb) and larger windows helping to distinguish word senses or to identify properties of the discourse (e.g., topic or style)."}, {"heading": "3.2 Recurrent Neural Network Encoders", "text": "The above feedforward DNN encoder will be brittle with large window sizes. We therefore also consider encoders based on recurrent neural networks (RNNs). RNNs have recently enjoyed a great deal of interest in the deep learning, speech recognition, and NLP communities (Sundermeyer et al., 2012; Graves et al., 2013; Sutskever et al., 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000).\nWe use an LSTM to encode the sequence of words containing the token and take the final hidden vector as the d\u2032-dimensional encoding. While we can use longer sequences, such as the sentence containing the token (Kawakami and Dyer, 2015), we restrict the input sequence to a fixed-size context window around word j, so the input is identical to that of the feedforward encoder above. For the syntactic tasks we consider, we did not find large context windows to be helpful."}, {"heading": "3.3 Training", "text": "We consider unsupervised ways to train the encoders described above. Throughout training for both models, the type embeddings are kept fixed. We assume that we are given a corpus X = {x(i)} |X| i=1 of unannotated word sequences.\nOne widely-used family of unsupervised criteria is that of reconstruction error and its variants. These are used when training autoencoders, which use an encoder f to convert the input x to a vector followed by a decoder g that attempts to reconstruct the input from the vector. The typical loss function is the squared difference between the input and reconstructed input. We use a generalization that is sensitive to the position of elements. Since our primary interest is in learning useful representations for a particular token in its context, we use a weighted reconstruction error:\nlossWRE(f, g,x, j) =\n|x| \u2211\ni=1\n\u03c9i \u2016g(f(x, j))i \u2212 vxi\u2016 2 2\n(1)\nwhere g(f(x, j))i is the subvector of g(f(x, j)) corresponding to reconstructing vxi , and where \u03c9i is the weight for reconstructing the ith entry.\nFor our feedforward encoder f , we use analogous fully-connected layers in the decoder\ng, forming a standard autoencoder architecture. To train the LSTM encoder, we add an LSTM decoder to form a sequence-to-sequence (\u201cseq2seq\u201d) autoencoder (Sutskever et al., 2014; Li et al., 2015; Dai and Le, 2015). That is, we use one LSTM as the encoder f and another LSTM for the decoder g, initializing g\u2019s hidden state to the output of f . Since we use the same weighted reconstruction error described above, the decoder must output a single vector at each step rather than a distribution over word types. So we use an affine transformation on the LSTM decoder hidden vector at each step in order to generate the output vector for each step. Reconstruction error has efficiency advantages over log loss here in that it avoids the costly summation over the vocabulary."}, {"heading": "4 Qualitative Analysis", "text": "Before discussing downstream tasks, we perform a qualitative analysis to show what our token embedding models learn."}, {"heading": "4.1 Experimental Setup", "text": "We train a feedforward DNN token embedding model on a corpus of 300,000 unlabeled English tweets. We use a window size w\u2032 = 3 for the qualitative results reported here; for downstream tasks below, we will vary w\u2032. For training, we use our weighted reconstruction error (Eq. 1). The encoder uses one hidden layer of size 512 followed by the token embedding layer of size d\u2032 = 256. The decoder also uses a single hidden layer of size 512. We use ReLU activations except the final encoder/decoder layers which use linear activations.\nIn preliminary experiments we compared 3 weighting schemes for \u03c9 in the objective: for token index j, \u201cuniform\u201d weighting sets \u03c9i = 1 for all i; \u201cfocused\u201d sets \u03c9j = 2 and \u03c9i = 1 for i 6= j; and \u201ctapered\u201d sets \u03c9j = 4, \u03c9j\u00b11 = 3, \u03c9j\u00b12 = 2, and 1 otherwise. The non-uniform schemes place more emphasis on reconstructing the target token, and we found them to slightly outperform uniform weighting. Unless reported otherwise, we use focused weighting for all experiments below.\nWe train using stochastic gradient descent with momentum for 1 epoch, saving the model that reaches the best objective value on a held-out validation set of 3,000 unlabeled tweets. For the type embeddings used as input to our token embedding model, we train 100-dimensional skip-gram embeddings on 56 million English tweets using the\nword2vec toolkit (Mikolov et al., 2013)."}, {"heading": "4.2 Nearest Neighbor Analysis", "text": "We inspect the ability of the encoder to distinguish different senses of ambiguous types. Table 1 shows query tokens (Q) followed by their four nearest neighbor tokens (with the same type), all from our held-out set of 3,000 tweets. We choose two polysemous words that are common in tweets: \u201c2\u201d and \u201cso\u201d. As queries, we select tokens that express different senses. The word \u201c2\u201d can be both a number (left) and a synonym of \u201cto\u201d (right). The word \u201cso\u201d is both an intensifier (left) and a connective (right). We find that the nearest neighbors, though generally differing in context words, have the same sense and same POS tag.\nIn Table 2 we consider nearest neighbors that may have different word types from the query type. For each query word, we permit the nearest neighbor search to consider tokens from the following set: {\u201c4\u201d, \u201cfor\u201d, \u201c2\u201d, \u201cto\u201d, \u201ctoo\u201d, \u201c1\u201d, \u201cone\u201d}. In the first two queries, we find that tokens of \u201c4\u201d have nearest neighbors with different word types but the same syntactic category. That is, tokens of different word types are more similar to the query than tokens of the same type. We see this again with neighbors of \u201c2\u201d used as a synonym for \u201cto\u201d. The encoder appears to be doing a kind of canonicalization of nonstandard word uses, which suggests applications for token embeddings in normalization of social media text (Clark and Araki, 2011). See neighbor 8, in which \u201ctoo\u201d is understood as having the intended meaning despite its misleading surface form."}, {"heading": "4.3 Visualization", "text": "In order to gain a better qualitative understanding of the token embeddings, we visualize the learned token embeddings using tSNE (Maaten and Hinton, 2008). We learn token embeddings as above except with w\u2032 = 1.\nFigure 1 shows a two-dimensional visualization of token embeddings for the word type \u201c4\u201d. For this visualization, we embed tokens in the POSannotated tweet datasets from Gimpel et al. (2011) and Owoputi et al. (2013), so we have their gold standard POS tags. We show the left and right context words (using w\u2032 = 1) along with the token and its gold standard POS tag. We find that tokens of \u201c4\u201d with the same gold POS tag are close in the embedded space, with prepositions appearing in the upper part of the plot and numbers appearing in the lower part."}, {"heading": "5 Downstream Tasks", "text": "We evaluate our token embedding models on two downstream tasks: POS tagging and dependency parsing. Given an input sequence x = \u3008x1, x2, ..., xn\u3009, we want to predict its tag sequence and dependency parse. We focus on Twitter since there is limited annotated data but abundant unlabeled data for training token embeddings."}, {"heading": "5.1 Part-of-Speech Tagging", "text": "Baseline We use a simple feedforward DNN as our baseline tagger. It is a local classifier that predicts the tag for a token independently of all other predictions for the tweet. That is, it does not use structured prediction. The input to the network is the type embedding of the word to be tagged concatenated with the type embeddings ofw words on either side. The DNN contains two hidden layers followed by one softmax layer. Figure 2(a) shows this architecture forw = 1when predicting the tag of 4 in the tweet thanks 4 follow. We concatenate a 10-dimensional binary feature vector computed for the word being tagged (Table 3).2\nWe train the tagger by minimizing the log loss (cross entropy) on the training set, performing early stopping on the validation set, and reporting accuracy on the test set. We consider both learning the type embeddings (\u201cupdating\u201d) and keeping\n2The definition of punctuation is taken from Python\u2019s string.punctuation.\nthem fixed. When we update the embeddings we include an \u21132 regularization term penalizing the divergence from the initial type embeddings.\nToken Embedding Tagger When using token embeddings, we concatenate the d\u2032-dimensional token embedding to the tagger input. The rest of\nthe architecture is the same as the baseline tagger. Figure 2(b) shows the model when using type embedding window size w = 0 and token embedding window size w\u2032 = 1.\nWhile training the DNN tagger with the token embeddings, we do not fine-tune the token embedding encoder parameters, leaving them fixed."}, {"heading": "5.2 Dependency Parser", "text": "Baseline As our baseline, we use a simple DNN to do parent prediction independently for each word. That is, we use a local classifier that scores parents for a word. To infer a parse at test time, we independently choose the highestscoring parent for each word. We also use our classifier\u2019s scores as additional features in TweeboParser (Kong et al., 2014).\nOur parent prediction DNN has two hidden layers and an output layer with 1 unit. This unit corresponds to a value S(xi, xj) that serves as the score for a dependency arc with child word xi and parent word xj . The input to the DNN is the concatenation of the type embeddings for xi and xj , the type embeddings of w words on either side of xi and xj , the features for xi and xj from Table 3, and features for the pair, including relative positions, direction, and distance (shown in Table 4).3\nFor a sentence of length n, the loss function we\n3When considering the root attachment (i.e., xj is the wall symbol $), the type embeddings for xj and its neighbors are all zeroes, the feature vector for xj is all zeroes, and the dependency pair features are all zeroes except the first and last.\nuse for a single arc (xi, xj) follows:\nlossarc(xi, xj) =\n\u2212 S(xi, xj) + log\n\n\nn \u2211\nk=0,k 6=i\nexp{S(xi, xk)}\n\n\n(2)\nwhere k = 0 indicates the root attachment for xi. We sum over all possible parents even though the model only computes a score for a binary decision.4 Where head(xi) returns the annotated parent for xi, the loss for a sequence x is:\nn \u2211\ni=1\nlossarc(xi,head(xi)) (3)\nAfter training, we predict the parent for a word xi\nas follows:\nhead(xi) = argmax k 6=i S(xi, xk) (4)\nToken Embedding Parser For the token embedding parser, we use the d\u2032-dimensional token embeddings for xi and xj . We simply concatenate the two token embeddings to the input of the DNN parser. When xj = $, the token embedding for xj is all zeroes. The other parts of the input are the same as the baseline parser. While training this parser, we do not optimize the token embedding encoder parameters. As with the tagger, we tune over the decision to keep type embeddings fixed or update them during learning, again using \u21132 regularization when doing so. We tune this decision for both the baseline parser and the parser that uses token embeddings."}, {"heading": "6 Experimental Setup", "text": "For training the token embedding models, we mostly use the same settings as in Section 4.1 for the qualitative analysis. The only difference is that we train the token embedding models for 5 epochs, again saving the model that reaches the best objective value on a held-out set of 3,000 unlabeled tweets. We also experiment with several values for the context window size w\u2032 and the hidden layer size, reported below.\n4We found this to work better than only summing over the exponentiated scores of an arc or no arc for the pair \u3008xi, xj\u3009.\ni n j n \u2206 = 1 \u2206 = 2 3 \u2264 \u2206 \u2264 5 6 \u2264 \u2206 \u2264 10 \u2206 \u2265 11 i < j i > j xj is wall symbol\nTable 4: Dependency pair features for arc with child xi and parent xj in an n-word sentence and where \u2206 = |i\u2212 j|. The final feature is 1 if xj is the wall symbol ($), indicating a root attachment for xi. In that case, all features are zero except for the first and last."}, {"heading": "6.1 Part-of-Speech Tagging", "text": "We use the annotated tweet datasets from Gimpel et al. (2011) and Owoputi et al. (2013). For training, we combine the 1000-tweet OCT27TRAIN set and the 327-tweet OCT27DEV development set. For validation, we use the 500-tweet OCT27TEST test set and for final testing we use the 547-tweet DAILY547 test set. The DNN tagger uses two hidden layers of size 512 with ReLU nonlinearities and a final softmax layer of size 25 (one for each tag). The input type embeddings are the same as in the token embedding model. We train using stochastic gradient descent with momentum and early stopping on the validation set."}, {"heading": "6.2 Dependency Parsing", "text": "We use data from Kong et al. (2014), dividing their 717 training tweets randomly into a 573- tweet train set and a 144-tweet validation set. We use their 201-tweet TEST-NEW as our test set. Kong et al. annotated whether particular tokens are contained in the syntactic structure of each tweet (\u201ctoken selection\u201d). We use the same automatic token selection (TS) predictions as they did, which are 97.4% accurate. We use a pipeline architecture in which unselected tokens are not considered as possible parents when performing the summation in Eq. 2 or the argmax in Eq. 4. Like Kong et al., we use gold standard POS tags and gold standard TS during training and tuning. For final testing on TEST-NEW, we use automatically-predicted POS tags and automatic TS (using their same automatic predictions for both). Like them, we use attachment F1 score (%) for evaluation. Our DNN parsers use two hidden layers of size 1024 with ReLU nonlinearities. The final layer has size 1 (the score S(xi, xj)). We train using SGD with momentum."}, {"heading": "7 Results", "text": ""}, {"heading": "7.1 Part-of-Speech Tagging", "text": "We first train our baseline tagger without the binary feature vector using different amounts of training data and window sizes w \u2208 {0, 1, 2, 3}.\npercentage of all training data(%) 20 40 60 80\nac cu\nra cy\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\nBaseline(0) Baseline(1) Baseline(2) Baseline(3) TokenEmbedding(0+1) TokenEmbedding(0+2) TokenEmbedding(0+3)\nFigure 3: Tagging results. \u201cBaseline(w)\u201d refers to the baseline tagger with context of \u00b1w words; \u201cTokenEmbedding(w+w\u2032)\u201d refers to the token embedding tagger with tagger context of \u00b1w words and token embedding context of \u00b1w\u2032 words.\nFigure 3 shows accuracies on the validation set. When using only 10% of the training data, the baseline tagger with w = 0 performs best. As the amount of training data increases, the larger window sizes begin to outperform w = 0, and with the full training set, w = 1 performs best.\nFigure 3 also shows the results of our token embedding tagger for w = 0 and w\u2032 \u2208 {1, 2, 3}.5 We see consistent gains when using token embeddings, higher than the best baseline window for all values of w\u2032, though the best performance is obtained with w\u2032 = 1. When using small amounts of data, the baseline accuracy drops when increasing w, but the token embedding tagger is much more robust, always outperforming the w = 0 baseline.\nWe then perform experiments using the full training set, showing results in Table 5. For all\n5We used focused weighting for the results in Figure 3 using \u03c9j = 2, but found slightly more stable results by increasing \u03c9j to 3, still keeping the other weights to 1. Our final tagging results use \u03c9j = 3.\nexperiments with the baseline DNN tagger, we fix w = 1; when using token embeddings, we fix w = 0 and w\u2032 = 1. We also consider updating the initial word type embeddings during tagger training (\u201cupdating\u201d) and using the binary feature vector for the center word (\u201cfeatures\u201d).\nUsing token embeddings consistently outperforms using type embeddings alone. On the test set, we see gains from token embeddings across all settings, ranging from 0.5 to 1.2. The gains from DNN and seq2seq token embeddings are similar (possibly because we again use w = 0 and w\u2032 = 1 for the latter). The baseline taggers improve substantially by updating type embeddings or adding features (settings (2) or (3)), but adding token embeddings still yields additional improvements. When we use token embeddings but remove the type embedding for the word being tagged (denoted \u201c*\u201d), DNN TEs can still improve over the baseline, though seq2seq TEs yield lower accuracy. This suggests that the seq2seq TE model is focusing on other information in the window that is not necessarily related to the center word.\nComparison to State of the Art. Owoputi et al.\n(2013) achieve 92.8% on this train/test setup, using structured prediction and additional features from annotated and curated resources. We add several additional features inspired by theirs. We use features based on their generated Brown clusters, namely, binary vectors representing indicators for cluster string prefixes of length 2, 4, 6, and 8. We add tag dictionary features constructed from the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993). We use the concatenation of the binary tag vectors for the three most common tags in the tag dictionary for the word being tagged. We use the 10-dimensional binary feature vector and a binary feature indicating whether the word begins with a capital letter. All features above are used for the center word as well as one word to the left and one word to the right.\nWe add several more features only for the word being tagged. We use name list features, adding a binary feature for each name list used by Owoputi et al. (2013), where the feature indicates membership on the corresponding name list of the word being tagged. We also include character n-gram count features for n \u2208 {2, 3}, adding features for the 3,133 bi/trigrams that appear 3 or more times in the tagging training data.\nAfter adding these features, we increase the hidden layer size to 2048. We use dropout, using a dropout rate of 0.2 for the input layer and 0.4 for the hidden layers. The other settings remain the same. The results are shown in Table 6. Our new baseline tagger improves from 89.2% to 92.1% on validation, and improves further with updating.\nWe then add DNN token embeddings to this new baseline. When doing so, we set w = 0, as in all earlier experiments. We add two sets of DNN token embedding features to the tagger, one with w\u2032 = 1 and another with w\u2032 = 3. The results improve by 0.4 over the strongest baseline on the test set, matching the accuracy of Owoputi et al. (2013). This is notable since they used structured prediction while we use a simple local classifier, enabling fast and maximally-parallelizable test-time inference."}, {"heading": "7.2 Dependency Parsing", "text": "We show results with our head predictors in Table 7. The baseline head predictor actually does best with w = 0. The predictors with token embeddings are able to leverage larger context: with DNN token embeddings, performance is best with\nw\u2032 = 1 while with seq2seq token embeddings, performance is strong with w\u2032 = 1 and 2. When using token embeddings, we actually found it beneficial to drop the center word type embedding from the input, only using it indirectly through the token embedding functions. We use w = \u22121 to indicate this setting.\nThe upper part of Table 8 shows the results when we simply use our parsers to output the highest-scoring parents for each word in the test set. Token embeddings are more helpful for this task than type embeddings, improving performance from 73.0 to 75.8 for DNN token embeddings and improving to 75.0 for the seq2seq token embeddings.\nWe also use our head predictors to add a new feature to TweeboParser (Kong et al., 2014). TweeboParser uses a feature on every candidate arc corresponding to the score under a first-order dependency model trained on the Penn Treebank. We add a similar feature corresponding to the arc score under our model from our head predictors. Because TweeboParser results are nondeterministic, presumably due to floating point precision, we train TweeboParser 10 times for both its baseline configuration and all settings using our additional features, using TweeboParser\u2019s default hyperparameters each time. We report means and standard deviations.\nThe final results are shown in the lower part of Table 8. While adding the feature from the baseline parser hurts performance slightly (80.6\u2192 80.5), adding token embeddings improves performance. Using the feature from our DNN TE head predictor improves performance to 81.5, establishing a new state of the art for Twitter dependency parsing."}, {"heading": "8 Conclusion", "text": "We have presented a simple and efficient way of learning representations of words in their con-\ntexts using unlabeled data, and have shown how they can be used to improve syntactic analysis of Twitter. Qualitatively, our token embeddings are shown to encode sense and POS information, grouping together tokens of different types with similar in-context meanings. Quantitatively, using token embeddings in simple predictors consistently improves performance, even rivaling the performance of strong structured prediction baselines. Our code and trained token embedding models are publicly available at the authors\u2019 websites. Future work includes further exploration of token embedding models, unsupervised objectives, and their integration with supervised predictors."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers, Chris Dyer, and Lingpeng Kong. We also thank the developers of Theano (Theano Development Team, 2016) and Lasagne (Dieleman et al., 2015) as well as NVIDIA Corporation for donating GPUs used in this research."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. of ACL", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proc. of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Context-dependent word representation for neural machine translation", "author": ["Heeyoul Choi", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Choi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "Text normalization in social media: progress, problems and applications for a pre-processing system of casual English", "author": ["Eleanor Clark", "Kenji Araki"], "venue": "Procedia-Social and Behavioral Sciences", "citeRegEx": "Clark and Araki.,? \\Q2011\\E", "shortCiteRegEx": "Clark and Araki.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le"], "venue": "In Advances in NIPS", "citeRegEx": "Dai and Le.,? \\Q2015\\E", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Lasagne: First release. http://dx.doi.org/10.5281/zenodo.27878", "author": ["Sander Dieleman", "Jan Schl\u00fcter", "Colin Raffel", "Eben Olson", "S\u00f8ren Kaae S\u00f8nderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma", "Eric Battenberg", "Jack Kelly"], "venue": null, "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural Computation", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Semi supervised preposition-sense disambiguation using multilingual data", "author": ["Hila Gonen", "Yoav Goldberg"], "venue": "In Proc. of COLING", "citeRegEx": "Gonen and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Gonen and Goldberg.", "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Proc. of ICASSP", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Learning sense-specific word embeddings by exploiting bilingual resources", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of COLING", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric Huang", "Richard Socher", "Christopher D. Manning", "Andrew Ng"], "venue": "In Proc. of ACL", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Ontologically groundedmulti-sense representation learning for semantic vector space models", "author": ["Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "Jauhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Neural context embeddings for automatic discovery of word senses", "author": ["Mikael K\u00e5geb\u00e4ck", "Fredrik Johansson", "Richard Johansson", "Devdatt Dubhashi"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "K\u00e5geb\u00e4ck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "K\u00e5geb\u00e4ck et al\\.", "year": 2015}, {"title": "Learning to represent words in context with multilingual supervision", "author": ["Kazuya Kawakami", "Chris Dyer"], "venue": "In Proc. of ICLR Workshop", "citeRegEx": "Kawakami and Dyer.,? \\Q2015\\E", "shortCiteRegEx": "Kawakami and Dyer.", "year": 2015}, {"title": "A dependency parser for tweets", "author": ["Lingpeng Kong", "Nathan Schneider", "Swabha Swayamdipta", "Archna Bhatia", "Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "In Proc. of EMNLP", "citeRegEx": "Li and Jurafsky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Thang Luong", "Dan Jurafsky"], "venue": "In Proc. of ACL", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In Proc. of AAAI", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "context2vec: Learning generic context embedding with bidirectional LSTM", "author": ["Oren Melamud", "Jacob Goldberger", "Ido Dagan"], "venue": "In Proc. of CoNLL", "citeRegEx": "Melamud et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["TomasMikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in NIPS", "citeRegEx": "TomasMikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "TomasMikolov et al\\.", "year": 2013}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proc. of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Semi-supervised sequence tagging with bidirectional language models", "author": ["Matthew E. Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power"], "venue": "In Proc. of ACL", "citeRegEx": "Peters et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2017}, {"title": "A simple and efficient method to generate word sense representations", "author": ["Luis Nieto Pi\u00f1a", "Richard Johansson"], "venue": "In Proc. of RANLP", "citeRegEx": "Pi\u00f1a and Johansson.,? \\Q2015\\E", "shortCiteRegEx": "Pi\u00f1a and Johansson.", "year": 2015}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["Lin Qiu", "Yong Cao", "Zaiqing Nie", "Yong Rui"], "venue": "In Proc. of AAAI", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Proc. of NAACL", "citeRegEx": "Reisinger and Mooney.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Proc. of Interspeech", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Bilingual learning of multi-sense embeddings with discrete autoencoders", "author": ["Simon \u0160uster", "Ivan Titov", "Gertjan van Noord"], "venue": "In Proc. of NAACL", "citeRegEx": "\u0160uster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "\u0160uster et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In Proc. of COLING", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc. of ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "k-embeddings: Learning conceptual embeddings for words using context", "author": ["Thuy Vu", "D. Stott Parker"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "Vu and Parker.,? \\Q2016\\E", "shortCiteRegEx": "Vu and Parker.", "year": 2016}, {"title": "Sense-aware semantic analysis: A multi-prototype word representation model using Wikipedia", "author": ["Zhaohui Wu", "C. Lee Giles"], "venue": "In Proc. of AAAI", "citeRegEx": "Wu and Giles.,? \\Q2015\\E", "shortCiteRegEx": "Wu and Giles.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al.", "startOffset": 42, "endOffset": 113}, {"referenceID": 27, "context": "Many embedding models have been developed (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al.", "startOffset": 42, "endOffset": 113}, {"referenceID": 36, "context": ", 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).", "startOffset": 199, "endOffset": 283}, {"referenceID": 4, "context": ", 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).", "startOffset": 199, "endOffset": 283}, {"referenceID": 0, "context": ", 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).", "startOffset": 199, "endOffset": 283}, {"referenceID": 39, "context": ", 2014) and have been shown to improve performance on NLP tasks, including part-of-speech (POS) tagging, named entity recognition, semantic role labeling, dependency parsing, and machine translation (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Zou et al., 2013).", "startOffset": 199, "endOffset": 283}, {"referenceID": 31, "context": "To address this, some researchers learn multiple embeddings for certain word types, where each embedding corresponds to a distinct sense of the type (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014).", "startOffset": 149, "endOffset": 216}, {"referenceID": 12, "context": "To address this, some researchers learn multiple embeddings for certain word types, where each embedding corresponds to a distinct sense of the type (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014).", "startOffset": 149, "endOffset": 216}, {"referenceID": 35, "context": "To address this, some researchers learn multiple embeddings for certain word types, where each embedding corresponds to a distinct sense of the type (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014).", "startOffset": 149, "endOffset": 216}, {"referenceID": 16, "context": "We add our token embeddings to Tweeboparser (Kong et al., 2014), improving its performance and establishing a new state of the art for Twitter dependency parsing.", "startOffset": 44, "endOffset": 63}, {"referenceID": 25, "context": "We show that token embeddings can improve the performance of a non-structured POS tagger to match the state of the art Twitter POS tagger of Owoputi et al. (2013). We add our token embeddings to Tweeboparser (Kong et al.", "startOffset": 141, "endOffset": 163}, {"referenceID": 37, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 31, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 12, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 35, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 1, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 29, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 38, "context": "Most of these methods cluster tokens into senses and learn vectors for each cluster (Vu and Parker, 2016; Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Chen et al., 2014; Pi\u00f1a and Johansson, 2015; Wu and Giles, 2015).", "startOffset": 84, "endOffset": 237}, {"referenceID": 10, "context": "Some use bilingual information (Guo et al., 2014; \u0160uster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al.", "startOffset": 31, "endOffset": 96}, {"referenceID": 33, "context": "Some use bilingual information (Guo et al., 2014; \u0160uster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al.", "startOffset": 31, "endOffset": 96}, {"referenceID": 8, "context": "Some use bilingual information (Guo et al., 2014; \u0160uster et al., 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al.", "startOffset": 31, "endOffset": 96}, {"referenceID": 25, "context": ", 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al.", "startOffset": 100, "endOffset": 149}, {"referenceID": 17, "context": ", 2016; Gonen and Goldberg, 2016), nonparametric methods to avoid specifying the number of clusters (Neelakantan et al., 2014; Li and Jurafsky, 2015), topic models (Liu et al.", "startOffset": 100, "endOffset": 149}, {"referenceID": 20, "context": ", 2014; Li and Jurafsky, 2015), topic models (Liu et al., 2015), grounding to WordNet (Jauhar et al.", "startOffset": 45, "endOffset": 63}, {"referenceID": 13, "context": ", 2015), grounding to WordNet (Jauhar et al., 2015), or senses defined as sets of POS tags for each type (Qiu et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 30, "context": ", 2015), or senses defined as sets of POS tags for each type (Qiu et al., 2014).", "startOffset": 61, "endOffset": 79}, {"referenceID": 14, "context": "There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (K\u00e5geb\u00e4ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al.", "startOffset": 168, "endOffset": 251}, {"referenceID": 19, "context": "There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (K\u00e5geb\u00e4ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al.", "startOffset": 168, "endOffset": 251}, {"referenceID": 2, "context": "There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (K\u00e5geb\u00e4ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al.", "startOffset": 168, "endOffset": 251}, {"referenceID": 23, "context": "There is prior work in developing representations for tokens in the context of unsupervised or supervised training, whether with long short-term memory (LSTM) networks (K\u00e5geb\u00e4ck et al., 2015; Ling et al., 2015; Choi et al., 2016; Melamud et al., 2016), convolutional networks (Collobert et al.", "startOffset": 168, "endOffset": 251}, {"referenceID": 4, "context": ", 2016), convolutional networks (Collobert et al., 2011), or other architectures.", "startOffset": 32, "endOffset": 56}, {"referenceID": 15, "context": "Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of their translation targets, while Melamud et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of their translation targets, while Melamud et al. (2016) and Peters et al.", "startOffset": 0, "endOffset": 151}, {"referenceID": 15, "context": "Kawakami and Dyer (2015) use multilingual data to learn token embeddings that are predictive of their translation targets, while Melamud et al. (2016) and Peters et al. (2017) use unsupervised learning with monolingual sentences.", "startOffset": 0, "endOffset": 176}, {"referenceID": 32, "context": "RNNs have recently enjoyed a great deal of interest in the deep learning, speech recognition, and NLP communities (Sundermeyer et al., 2012; Graves et al., 2013; Sutskever et al., 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 114, "endOffset": 185}, {"referenceID": 9, "context": "RNNs have recently enjoyed a great deal of interest in the deep learning, speech recognition, and NLP communities (Sundermeyer et al., 2012; Graves et al., 2013; Sutskever et al., 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 114, "endOffset": 185}, {"referenceID": 34, "context": "RNNs have recently enjoyed a great deal of interest in the deep learning, speech recognition, and NLP communities (Sundermeyer et al., 2012; Graves et al., 2013; Sutskever et al., 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 114, "endOffset": 185}, {"referenceID": 11, "context": ", 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000).", "startOffset": 90, "endOffset": 143}, {"referenceID": 7, "context": ", 2014), most frequently used with \u201cgated\u201d connections like long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000).", "startOffset": 90, "endOffset": 143}, {"referenceID": 15, "context": "While we can use longer sequences, such as the sentence containing the token (Kawakami and Dyer, 2015), we restrict the input sequence to a fixed-size context window around word j, so the input is identical to that of the feedforward encoder above.", "startOffset": 77, "endOffset": 102}, {"referenceID": 34, "context": "To train the LSTM encoder, we add an LSTM decoder to form a sequence-to-sequence (\u201cseq2seq\u201d) autoencoder (Sutskever et al., 2014; Li et al., 2015; Dai and Le, 2015).", "startOffset": 105, "endOffset": 164}, {"referenceID": 18, "context": "To train the LSTM encoder, we add an LSTM decoder to form a sequence-to-sequence (\u201cseq2seq\u201d) autoencoder (Sutskever et al., 2014; Li et al., 2015; Dai and Le, 2015).", "startOffset": 105, "endOffset": 164}, {"referenceID": 5, "context": "To train the LSTM encoder, we add an LSTM decoder to form a sequence-to-sequence (\u201cseq2seq\u201d) autoencoder (Sutskever et al., 2014; Li et al., 2015; Dai and Le, 2015).", "startOffset": 105, "endOffset": 164}, {"referenceID": 3, "context": "The encoder appears to be doing a kind of canonicalization of nonstandard word uses, which suggests applications for token embeddings in normalization of social media text (Clark and Araki, 2011).", "startOffset": 172, "endOffset": 195}, {"referenceID": 21, "context": "In order to gain a better qualitative understanding of the token embeddings, we visualize the learned token embeddings using tSNE (Maaten and Hinton, 2008).", "startOffset": 130, "endOffset": 155}, {"referenceID": 26, "context": "(2011) and Owoputi et al. (2013), so we have their gold standard POS tags.", "startOffset": 11, "endOffset": 33}, {"referenceID": 16, "context": "We also use our classifier\u2019s scores as additional features in TweeboParser (Kong et al., 2014).", "startOffset": 75, "endOffset": 94}, {"referenceID": 26, "context": "(2011) and Owoputi et al. (2013). For training, we combine the 1000-tweet OCT27TRAIN set and the 327-tweet OCT27DEV development set.", "startOffset": 11, "endOffset": 33}, {"referenceID": 16, "context": "We use data from Kong et al. (2014), dividing their 717 training tweets randomly into a 573tweet train set and a 144-tweet validation set.", "startOffset": 17, "endOffset": 36}, {"referenceID": 26, "context": "Last row is best result from Owoputi et al. (2013).", "startOffset": 29, "endOffset": 51}, {"referenceID": 22, "context": "We add tag dictionary features constructed from the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993).", "startOffset": 101, "endOffset": 122}, {"referenceID": 25, "context": "Owoputi et al. (2013) achieve 92.", "startOffset": 0, "endOffset": 22}, {"referenceID": 26, "context": "We use name list features, adding a binary feature for each name list used by Owoputi et al. (2013), where the feature indicates membership on the corresponding name list of the word being tagged.", "startOffset": 78, "endOffset": 100}, {"referenceID": 26, "context": "4 over the strongest baseline on the test set, matching the accuracy of Owoputi et al. (2013). This is notable since they used structured prediction while we use a simple local classifier, enabling fast and maximally-parallelizable test-time inference.", "startOffset": 72, "endOffset": 94}, {"referenceID": 16, "context": "We also use our head predictors to add a new feature to TweeboParser (Kong et al., 2014).", "startOffset": 69, "endOffset": 88}, {"referenceID": 6, "context": "We also thank the developers of Theano (Theano Development Team, 2016) and Lasagne (Dieleman et al., 2015) as well as NVIDIA Corporation for donating GPUs used in this research.", "startOffset": 83, "endOffset": 106}], "year": 2017, "abstractText": "We present models for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as word sense, syntactic category, and semantic role. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for part-of-speech taggers and dependency parsers trained on much smaller amounts of annotated data. We find that predictors endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.", "creator": "LaTeX with hyperref package"}}}