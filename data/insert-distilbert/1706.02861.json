{"id": "1706.02861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Assigning personality/identity to a chatting machine for coherent conversation generation", "abstract": "endowing a chatbot with personality or an identity is quite challenging but critical to deliver more realistic and natural conversations. in this paper, we address the issue limitation of generating responses that are coherent to manage a pre - specified agent profile. we design these a model consisting of several three modules : a profile detector to decide whether this a post should thus be responded using the profile and which key should be addressed, a bidirectional decoder to generate responses forward and backward starting from a selected profile value, and a position detector that predicts a word position from which decoding should start given a selected profile value. we show that a general conversation data from social media can be used accurately to generate profile - coherent responses. manual and automatic evaluation shows that our model can now deliver more coherent, natural, and diversified specific responses.", "histories": [["v1", "Fri, 9 Jun 2017 08:13:21 GMT  (329kb,D)", "https://arxiv.org/abs/1706.02861v1", null], ["v2", "Mon, 12 Jun 2017 06:17:45 GMT  (0kb,I)", "http://arxiv.org/abs/1706.02861v2", "an error on author information"], ["v3", "Wed, 21 Jun 2017 07:40:57 GMT  (330kb,D)", "http://arxiv.org/abs/1706.02861v3", "an error on author information"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qiao qian", "minlie huang", "haizhou zhao", "jingfang xu", "xiaoyan zhu"], "accepted": false, "id": "1706.02861"}, "pdf": {"name": "1706.02861.pdf", "metadata": {"source": "CRF", "title": "Assigning Personality/Identity to a Chatting Machine for Coherent Conversation Generation", "authors": ["Qiao Qian", "Minlie Huang", "Haizhou Zhao", "Jingfang Xu", "Xiaoyan Zhu"], "emails": ["qianqiaodecember29@126.com,", "aihuang@tsinghua.edu.cn,", "zhaohaizhou@sogou-inc.com,", "xujingfang@sogou-inc.com,", "zxy-dcs@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Generating human-level conversations by machine has been a long-term goal of AI since the Turing Test (Turing, 1950). However, as argued by (Vinyals and Le, 2015), the current conversation generation models are still unable to deliver realistic conversations to pass the Test. Amongst the many limitations, the lack of a coherent personality is one of the most challenging difficulties. Though personality is a well-defined concept in psychology (Norman, 1963; Gosling et al., 2003), while in this paper, the personality of a chatbot refers to the character that the bot plays or performs during conversational interactions. In this scenario, personality can be viewed as a composite of the identity (the background and profile) that\na chatbot is endowed with, linguistic style that an agent exhibits during interactions (Walker et al., 1997), and many more explicit and implicit cues that may portray character. Though personality is a more abstract and broad concept, we use personality, profile, and identity interchangeably in this paper.\nEndowing a chatbot with personality is well motivated by the simple example as shown in Table 1. We can clearly see that a general sequenceto-sequence (Seq2Seq) model cannot exhibit coherent personality/identity while our model is more coherent to a given identity. The motivation is also verified by (Yu et al., 2016) which reports that users ask for much personal information of a chatbot in human-machine interaction. It is\nar X\niv :1\n70 6.\n02 86\n1v 3\n[ cs\n.C L\n] 2\n1 Ju\nn 20\n17\nevident that personal information of a chatbot is much attended by users during conversation, particularly at the early stage of interaction.\nThe recent work dealing with personality in large-scale conversation generation can be seen in (Li et al., 2016) where speaker-specific conversation style is learned by user embedding. Another work which models user personalization can be seen in (Al-Rfou et al., 2016), with a similar technique of user embedding. Both studies require dialogue data from each user to model her/his personality.\nThe major departure to the previous works lies in: First, we address the problem of endowing a chatbot with a given identity. Such a task requires chatbots to generate not only consistent responses, but also responses that are coherent to its prespecified identity/personality. Second, the previous works on personality modeling require conversation data from each user, however, it\u2019s impractical here since dialogue data from the chatbot are unavailable before the release of the chatbot. Instead of just learning personality from dialogue data, our work can assign a desired identity to a chatbot by making use of general conversation data from social media. Our contributions lie in two folds:\n\u2022 We investigate the problem of endowing a chatbot with a given identity and enabling a chatbot to generate responses that are coherent to its given identity. Instead of learning personality from dialogue data, our work can assign a desired identity to a chatbot.\n\u2022 To address the problem, we propose a model consisting of a profile detector, a position detector, and a bidirectional decoder. Post-level and session-level evaluation shows that when giving an agent profile, our model can generate more coherent responses with more language variety."}, {"heading": "2 Related Work", "text": "There has been a large amount of work for dialogue/conversation generation. These works can be categorized into task-oriented (Young et al., 2013; Wen et al., 2016; Bordes and Weston, 2016) or chat-based. Recently, researchers found that social data such as Twitter/Weibo posts and replies (Ritter et al., 2011; Shang et al., 2015), and\nmovie dialogues can be used to learn and generate spoken language.\nLarge-scale conversation generation with social media data was firstly proposed in (Ritter et al., 2011) and has been greatly advanced by applying sequence-to-sequence models (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015, 2016). Many studies are focusing on improving the generation quality. These works include: dealing with unknown words (Gu et al., 2016; Gulcehre et al., 2016), avoiding universal responses (Jiwei Li, 2016), generating more diverse and meaningful responses (Xing et al., 2017; Mou et al., 2016), and many more.\nAs argued by (Vinyals and Le, 2015), it\u2019s still quite impossible for current chatbots to pass the Turing Test, while one of the reasons is the lack of a coherent personality. Though personality has been well defined in psychology (Norman, 1963), it is implicit, subtle, and challenging to formally define in statistical language generation. Linguistic style can be an indicator of personality (Mairesse and Walker, 2006; Mairesse et al., 2007), and conversation can be clues for personality recognition (Walker et al., 1997, 2012). In reverse, spoken language can be generated in accordance to particular personality (Mairesse and Walker, 2007).\nA first attempt to model persona can be seen in (Li et al., 2016) where the authors proposed to learn speaker-specific conversation style by user embedding. Our work differs from this work significantly: our task is to endow the chatbot with a fixed personality while (Li et al., 2016) learns personalized conversational styles. In other words, our task requires to generate not only consistent responses, but also responses that are coherent to the chatbot\u2019s prespecified identity. Further, (Li et al., 2016) requires many dialogue data from each user while our work has no such requirement.\nAnother related work is generative question answering (GenQA) (Yin et al., 2015) which generates a response containing an answer extracted from a knowledge base (KB). However, endowing a chatbot with personality is more than just question answering over KB, where there arise challenging problems such as semantic reasoning and conversation style modeling. Further, GenQA requires that the answer from KB must appear in the response to provide sufficient supervision while\nour work avoids the limitation by applying a position detector during training."}, {"heading": "3 Model", "text": ""}, {"heading": "3.1 Task Definition", "text": "The task can be formally defined as follows: given a post x = x1x2 \u00b7 \u00b7 \u00b7xn, and an agent profile defined as a set of key-value pairs {< ki, vi > |i = 1, 2, \u00b7 \u00b7 \u00b7 ,K}, the task aims to generate a response y = y1y2 \u00b7 \u00b7 \u00b7 ym that is coherent to the agent profile. The generation process can be briefly stated as below:\nP (y|x, {< ki, vi >}) =P (z = 0|x) \u00b7 P fr(y|x) +P (z = 1|x) \u00b7 P bi(y|x, {< ki, vi >})\n(1)\nwhere P (z|x) is the probability of using the agent profile given post x, which is computed by the Profile Detector; P fr(y|x) =\u220fm\nt=1P fr(yt|y<t,x) is given by a general forward decoder, the same as (Sutskever et al., 2014), and P bi(y|x, {< ki, vi >}) is given by a Bidirectional Decoder which will be described later.\nNote that the post/response pairs < x,y > are collected from social media, and the agent profile value may not occur in the response y. This leads to the discrepancy between training and test, which will be addressed in the Position Detector section."}, {"heading": "3.2 Overview", "text": "Our model works as follows (see Figure 1): given a post, the profile detector will predict whether the agent profile should be used. If not, a general seq2seq decoder will be used to generate the response; otherwise, the profile detector will further select an appropriate profile key and\nits value. Starting from the selected profile value, a response will be generated forward and backward by the bidirectional decoder. To better train the bidirectional decoder (see Figure 2), the position detector addresses the discrepancy issue between training and test, by predicting a word position from which decoding should start given the selected profile value. Note that the position detector will not participate in generation during test."}, {"heading": "3.3 Encoder", "text": "The encoder aims to encode a post to a vector representation. Given a post x = x1x2 \u00b7 \u00b7 \u00b7xn, the hidden states of the post h = (h1, h2, \u00b7 \u00b7 \u00b7, hn) are obtained by a gated recurrent unit (GRU) (Chung et al., 2014), as follows:\nht = GRU(ht\u22121, xt) (2)\nwhere xt is the embedding of the t-th word."}, {"heading": "3.4 Profile Detector", "text": "The profile detector has two roles: first to detect whether the post should be responded with the agent profile, and second to select a specific profile < key, value > to be addressed in the decoder. The first role of the profile detector is defined by the probability P (z|x) (z \u2208 {0, 1}) where z = 1 means the agent profile should be used. For instance, if the post is \u201chow old is your father\u201d, P (z = 1|x) \u2248 0, while if the post is \u201chow old are you\u201d, P (z = 1|x) \u2248 1. P (z|x) is a binary classifier trained on supervised data. More formally, the probability is computed as follows:\nP (z|x) = P (z|h\u0303) = \u03c3(Wph\u0303) (3)\nwhere Wp is the parameter of the classifier and h\u0303 = \u2211 j hj , simply the sum of all hidden states, but other elaborated methods such as attentionbased models are also applicable.\nThe second role of the profile detector is to decide which profile value should be addressed in a generated response. This is implemented as follows:\n\u03b2i = MLP([h\u0303, ki, vi])\n= f(W \u00b7 [h\u0303; ki; vi]) (4)\nwhereW is the weight and ki/vi is the embedding of a profile key/value respectively, they are all parameters of our model. h\u0303 = \u2211 j hj is the representation of the post. f is a nonlinear activation\nfunction, in this equation f is a softmax function over all \u03b2i. The above equation can be viewed as a multi-class classifier that produces a probability distribution over profile keys.\nThe optimal profile value is selected with the maximal probability: v\u0303 = vj where j = argmaxi(\u03b2i). As long as a profile value v\u0303 is obtained, the decoding process will be determined by the bidirectional decoder, as follows:\nP bi(y|x, {< ki, vi >}) = P bi(y|x, v\u0303) (5)"}, {"heading": "3.5 The Bidirectional Decoder", "text": "This decoder aims to generate a response in which a profile value will be mentioned. Inspired by (Mou et al., 2016), we design a bidirectional decoder which consists of a backward decoder and forward decoder, but with a key difference that a position detector is employed to predict a start decoding position.\nSuppose a generated response is y = (yb, v\u0303,yf ) = (yb1, \u00b7 \u00b7 \u00b7 , ybt\u22121, v\u0303, y f t+1, \u00b7 \u00b7 \u00b7 , y f m) where v\u0303 is a selected profile value. The bidirectional decoder will generate yb in a backward direction and yf forward. The backward decoder (P b) generates yb from the given profile value v\u0303 to the start of the response. The forward decoder (P f )1 generates yf from v\u0303 to the end of the response, but takes as input the already generated first half, yb. The process is defined formally as follows:\nP bi(y|x, v\u0303) = P b(yb|x, v\u0303) \u2217 P f (yf |yb,x, v\u0303)) P b(yb|x, v\u0303) = 1\u220f\nj=t\u22121 P b(ybj |yb>j ,x, v\u0303)\nP f (yf |yb,x, v\u0303) = m\u220f\nj=t+1\nP f (yfj |y f <j ,y b,x, v\u0303)\n(6)\nIn order to encode more contexts in the forward decoder, the first half of generated response (yb), along with the profile value (v\u0303), serves as initial input to the forward decoder. The probability P b and P f is calculated via\nP b(ybj |yb>j ,x, v\u0303) \u221dMLP([sbj ; ybj+1; cbj ]) P f (yfj |y f <j ,y b,x, v\u0303) \u221dMLP([sfj ; y f j\u22121; c f j ])\n(7)\nwhere s(\u2217)j is the state of the corresponding decoder, c(\u2217)j is the context vector, and \u2217 \u2208 {b, f} 1Note that this decoder is different from P fr(yt|y<t,x).\nwhere b indicates the backward decoder and f the forward. The vectors are updated as follows:\ns (\u2217) j = GRU(s (\u2217) j+l, [y (\u2217) j+l; c (\u2217) j ])\nc (\u2217) j = n\u2211 t=1 \u03b1 (\u2217) j,t ht\n(8)\nwhere \u03b1(\u2217)j,t \u221d MLP([s (\u2217) j+l, ht]) can be viewed as the similarity between decoder state s(\u2217)j+l and encoder hidden state ht, l = 1 when \u2217 = b (backward), and l = \u22121 when \u2217 = f (forward). And these MLPs have the same form as Eq.4, but with different parameters."}, {"heading": "3.6 Position Detector", "text": "The position detector is designed to provide more supervision to the bidirectional decoder, which is only used during training. As mentioned, the bidirectional decoder starts from a profile value to generate the entire sequence at the test stage. However, in our training dataset, the profile values may be rarely mentioned in the responses. For instance, given the profile key value pair <\u7231 \u597d, \u51b0\u7403> (< hobby, hockey >), the value \u51b0 \u7403(hockey) rarely occurs in the training corpus. In other words, even though we have a training instance (x,y, < k, v >), the value (v) may not occur in y at all. Hence, the bidirectional decoder is not aware from which word decoding should start. This leads to the discrepancy between training and test: during training, the decoder is unaware of the start decoding position but during test, the start decoding word is given.\nThis issue makes our work differ substantially from previous approaches where supervision is directly observable either between post and response (Gu et al., 2016) or between response and knowledge base (Yin et al., 2015). Experiments also show that the position detector contributes much to the performance improvement than a random position picking strategy (Mou et al., 2016).\nThe position detector is designed to provide a start decoding position to the decoder during training. For instance, given a post x =\u201c\u4f60-1 \u6709-2 \u4ec0\u4e48-3 \u7279\u957f-4 \uff1f-5 (what\u2019s your speciality?)2\u201d and a response y =\u201c\u6211-1\u975e\u5e38-2\u64c5\u957f-3\u5c0f\u63d0\u74344(I am good at playing violin)\u201d, and a profile key value pair \u201c<\u7279\u957f,\u94a2\u7434> (< hobby, piano >)\u201d, the position detector will predict that \u201c\u5c0f\u63d0\u7434-4 (violin)\u201d in the response can be replaced by the profile value \u201c\u94a2\u7434(piano)\u201d to ensure grammaticality. The predicted position \u201c\u5c0f\u63d0\u7434-4 (violin)\u201d is then passed to the decoder (see Eq. 6) to signal the start decoding position.\nIn order to find an appropriate position at which the profile value can be replaced, we need to estimate the probability: P (j|y1y2 \u00b7 \u00b7 \u00b7 ym, < k, v > )), 1 \u2264 j \u2264 m which indicates how likely the word yj can be replaced by the profile value v.\nWe apply a simple technique to approximate the probability: a word can be replaced by a given profile value if the word has maximal similarity.\nP (j|y, < k, v >)) \u221d cos(yj , v) (9)\nwhere cos(yj , v) denotes the cosine similarity between a word in a response and a profile value. More elaborated techniques, for instance, language models, will be studied as future work."}, {"heading": "3.7 Loss Function and Training", "text": "Two loss functions are defined: one on the generation probability and the other on the profile detector. The first loss is defined as below:\nL1(\u03b8,D(c), D(x,y)) =\u2212 \u2211\n(x,y)\u2208D(c)\u222aD(pr) logP (y|x, {< ki, vi >})\n=\u2212 \u2211\n(x,y)\u2208D(c) logP fr(y|x)\n\u2212 \u2211\n(x,y)\u2208D(pr) logP bi(y|x, v\u0303)\n(10)\n2The number indicates the position of each word.\nThe first term is the negative log likelihood of observing D(c) and the second term for D(pr). v\u0303 is a word in y whose position is predicted by the position detector during training. D(pr) consists of pairs where a post is related to a profile key and its response gives a meaningful reaction to the post, and D(c) has only general post-response pairs.\nThe two decoders (P fr andP bi) have no shared parameters. Since the number of instances in D(c) is much larger than that of D(pr), we apply a twostage training strategy: D(c) will be used to train P bi at the early stage for several epoches, where v\u0303 is a randomly chosen word in a response, and then D(pr) for further training at the later stage.\nThe above formulation generally adopts the hard form of P (z|x) (see Eq. 3): P (z = 1|x) = 1 for profile-related pairs and P (z = 1|x) = 0 for others. In order to better supervise the learning of the profile detector, we define the second loss and add it to the first one with a weight \u03b1 as the overall loss (i.e., L = L1 + \u03b1L2):\nL2(\u03b8,D(pb), D(pr)) =\u2212 \u2211\n(x,y,z)\u2208D(pb) logP (z|x)\n\u2212 \u2211\n(x,y,k\u0302)\u2208D(pr)\nK\u2211 j=1 \u03b2\u0302jlog\u03b2j\n(11)\nwhere the first term is for binary prediction of using profile or not, and the second for profile key selection. k\u0302 is the profile key whose value should be addressed, K is the total number of keys, \u03b2 is the predicted distribution over profile keys as defined by Eq. 4, and \u03b2\u0302 is one-hot representation of the gold distribution over keys. < x,y, z > is obtained by manual annotation while (x,y, k\u0302) is obtained by matching the keywords and synonyms in the profile with the post, which is noisy. This works well in practice and reduces manual labors largely."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Data Preparation", "text": "We prepare several datasets: Weibo Dataset (WD) - D: We collect 9, 697, 651 post-response pairs from Weibo. The dataset is used for trainingP fr(y|x) andP bi(y|x, v\u0303) at the early stage and 7,000 pairs are used for validation to make early stop. Profile Binary Subset (PB - D(pb) \u2208 D): We\nextract 76, 930 pairs from WD for 6 profile keys ({name, gender, age, city, weight, constellation}) with about 200 regular expression patterns. The dataset is annotated by 13 annotators. Each pair is manually labeled to positive if a post is asking for a profile value and the response is a logic reaction to the post, or negative otherwise.\nThis dataset is used to train the binary classifier (P (z|x)) (see D(pb) in Eq. 11). 3,000 pairs are used for test and the remainder for training. The statistics of the dataset is shown in the supplementary file. Profile Related Subset (PR - D(pr) \u2208 D(pb)): This dataset only contains pairs whose posts are positive in PB. In total, we have 42, 193 such pairs. This dataset is used to train the bidirectional decoder. Manual Dataset (MD): This dataset has 600 posts written by 4 human curators, including 50 negative and 50 positive posts for each key. A positive post for a profile key (e.g., how old are you?) means that it should be responded by a profile value, while a negative post (e.g., how old is your sister?) should not. This dataset is used to test the performance on real conversation data rather than social media data.\nAll datasets are available upon request. Implementation details of the model are shown in the supplementary."}, {"heading": "4.2 Human Evaluation", "text": "We evaluate our model at both post and session level. At the post level, we define three metrics (naturalness, logic, and correctness) to evaluate the response generated by each model. At the session level, we evaluate the models from the aspects of consistency and variety to justify the performance in the real conversational setting.\nWe name our model Identity-Coherent Conversation Machine (ICCM) and compare it with several baselines: Seq2Seq: a general sequence to sequence model (Sutskever et al., 2014). Seq2Seq + Profile Value (+PV): if the profile detector decides that a profile value should be used (P (z|x) > 0.5), the response is simply the value of the key decided by the profile detector (see Eq. 4); otherwise, a general seq2seq decoder will be used. Seq2Seq + Profile Value Decoding (+PVD): the response is generated by a general seq2seq de-\ncoder which starts decoding forwardly from the value of the selected key. ICCM-Pos: Instead of using a predicted position obtained by the position detector to start the decoding process, the bidirectional decoder in this setting randomly picks a word in a response during training, the same as (Mou et al., 2016)."}, {"heading": "4.2.1 Post-level Evaluation", "text": "To conduct post-level evaluation, we use 600 posts from MD, 50 positive/negative posts respectively for each key. Each post is input to all the models to get the corresponding responses. Thus, each post has 5 responses and these responses are randomly shuffled and then presented to two curators. Post-response pairs are annotated according to the following metrics, based on a 1/0 scoring schema: Naturalness (Nat.) measures the fluency and grammaticality of a response. Too short responses will be judged as lack of naturalness. Logic measures whether the response is a logical reaction to a post. For instance, for post \u201chow old are you\u201d, a logical response could be \u201cI am 3 years old\u201d or \u201cI do not know\u201d. Correctness (Cor.) measures whether the response provides a correct answer to a post given the profile. For instance, for post \u201chow old are you\u201d, if the profile has a key value pair like < age, 3 >, responses like \u201cI am 18\u201d will be judged as wrong.\nEach response is judged by two curators. The Cohen\u2019s Kappa statistics are 0.46, 0.75 and 0.82 for naturalness, logic, and correctness respectively. Naturalness has a rather lower Kapp because it is more difficult to judge.\nResults in Table 4 support the following statements: First, our model is better than all other baselines in all metrics, indicating that our model can generate more natural, logical, and correct responses; Second, in comparison to simply responding with a profile value (Seq2Seq+PV) where the responses are generally too short, our model can generate more natural responses; Third, the position detection contributes to better generation, in comparison to a random position (ICCM vs. ICCM-Pos). Exemplar responses generated by these models are shown in Table 3 which also demonstrate the effectiveness of our model."}, {"heading": "4.2.2 Session-level Evaluation", "text": "In order to compare these models in real conversation sessions, we randomly generate sessions\nbased on MD. For each profile key, we randomly choose 3 positive posts3 from MD, generate responses to the 3 posts for each model, and obtain a session of 3 post-response pairs. In this way, 240 sessions are generated, and each key has 40 sessions. The sessions are manually checked with the following metrics: Consistency measures whether there are contradictory responses with respect to the given profile. Score 1 indicates that all the three responses are consistent to the profile, and score 0 otherwise. Variety measures the language variety of the three responses in a session. Score 1 indicates that the linguistic patterns and wordings are different between any two of them, and score 0 otherwise.\nResults are shown in Table 6 and we show some session examples in Table 5.\nWe can clearly see the following observations: 1) Our model is remarkably better than all the baselines w.r.t both metrics. Results of our model against Seq2Seq+PVD indicate that the bidirectional decoder can generate responses of much richer language variety. The results of ICCM-Pos show that the position detector improves consistency and variety remarkably. 2) if simply respond with a profile value (Seq2Seq+PV), the model can obtain good consistency but very bad language variety, which is in\n3A positive post must be responded with a profile value.\nline with the intuition. 3) The general Seq2Seq model is too weak to generate consistent or linguistically various responses."}, {"heading": "4.3 Automatic Evaluation", "text": "We also present results of automatic evaluation for the profile and position detector."}, {"heading": "4.3.1 Profile Detection", "text": "The profile detector is evaluated from two aspects: whether a profile should be used or not (P (z = 1|x)), and whether a profile key is correctly chosen. Note that the prediction of profile key selection is cascaded on that of P (z = 1|x).\nThe classifiers are trained on Weibo social data. Results in Table 7 show that the profile detector\nobtains fairly good accuracy. But the classifiers have a remarkable drop when test on the manual dataset (comparing two rows: MD(600) vs. PB(3000)). This indicates the difference between Weibo social data and real human conversations."}, {"heading": "4.3.2 Position Detection", "text": "As mentioned previously, the position detector plays a key role in improving the naturalness, logic, and correctness of responses (see ICCM vs. ICCM-pos in Table 4), and the consistency and variety of conversational sessions (see Table 6). Thus, it is necessary to evaluate the performance of this module separately.\nWe randomly sample 200 post-response pairs from PR for each key (1200 pairs in total), and then manually annotate the optimal position from which decoding should start. The results are shown in Table 8. The position for most keys can be estimated accurately while for name the prediction is bad. This is because the value of the key rarely occurs in our corpus, and the embeddings of such values are not fully trained. Nevertheless, the results are better than a random word picking strategy (ICCM vs. ICCM-Pos)."}, {"heading": "4.4 Extensibility", "text": "The effectiveness of our model is verified on six profile keys, but much manual labors are required. We will show the extensibility of the model by evaluating it on four additional keys: hobby, idol, speciality, and employer.\nFirstly, for the 4 keys, we extract 16, 332 postresponse pairs from WD with 79 hand-crafted patterns and each pair is noisily mapped to one of the keys with these patterns. These new pairs, along\nwith the old pairs on the six pairs, are used to retrain the model. Secondly, we construct a test dataset consisting of 400 posts, 50 positive and 50 negative human-written posts for each key. Responses from our model and Seq2Seq are obtained and then evaluated. The manual labor exists only in hand-crafting the 79 patterns.\nResults show that our model has a relative 10% drop on the new keys with respect to logic and correctness, and remains unchanged in naturalness. Nevertheless, our model is still much better than the Seq2Seq model. The baseline has no drop in naturalness and logic because this model does not rely on profile."}, {"heading": "5 Conclusion and Future Work", "text": "We present a model that can generate responses that are coherent to a pre-specified agent profile. Instead of learning personality from dialogue data, our work can assign a desired identity to a chatbot. Experiments show that our model is effective to generate more coherent and various conversations.\nOur work is a very small step to endow a chatbot with its own personality, which is an important issue for a chatbot to pass the Turing Test. There are many future directions: Conversation style: we have demonstrated that general conversation data can be used to generate profile-coherent responses. Can conversation style that is coherent to a chatbot\u2019s personality be modeled without stylistic dialogue data? Learning conversation styles such as talking like young girls or old adults, and introverts or extroverts, will be an interesting direction. Semantic reasoning: Endowing a chatbot with personality/identity arises the issue of semantic reasoning. When the user asks \u201care you married?\u201d or \u201cdo you play women basketball?\u201d to a five-year-old boy agent, a coherent response requires common sense knowledge and reasoning, however, this is extremely challenging."}, {"heading": "A Statistics for Profile Binary Dataset", "text": "We extract 76, 930 pairs from WD for 6 profile keys ({name, gender, age, city, weight, constellation}) with about 200 hand-crafted patterns. And then they are annotated to positive or negative. A positive post asks for a profile key of chatbot and its response gives a meaningful reaction, such as \u201cCould you tell me your age\u201d, while a negative post is irrelevant to any profile key, such as \u201cGuess how old I am\u201d. The statistics of the dataset is shown in Table 10.\nB Implementation Details\nIn our experiments, the encoder and attentive decoders are all have 4 layers of GRUs with a 512- dimensional hidden state. The dimension of word embedding is set to 100. The vocabulary size is limited to 40, 000. The word embeddings are pretrained on an unlabeled corpus (about 60, 000, 000\nWeibo pairs) by word2vec. And the other parameters are initialized by sampling from a uniform distribution U(\u2212sqrt(3/n), sqrt(3/n)), where n is the dimension of parameters. Training is conducted by stochastic gradient descent (SGD) with a mini-batch of 128 pairs. The learning rate is initialized with 0.5 and the decay factor is 0.99."}], "references": [{"title": "Conversational contextual cues: The case of personalization and history for response ranking", "author": ["Rami Al-Rfou", "Marc Pickett", "Javier Snaider", "Yunhsuan Sung", "Brian Strope", "Ray Kurzweil."], "venue": "arXiv preprint arXiv:1606.00372 .", "citeRegEx": "Al.Rfou et al\\.,? 2016", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2016}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston."], "venue": "arXiv preprint arXiv:1605.07683 .", "citeRegEx": "Bordes and Weston.,? 2016", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A very brief measure of the big-five personality domains", "author": ["Samuel D Gosling", "Peter J Rentfrow", "William B Swann."], "venue": "Journal of Research in personality 37(6):504\u2013528.", "citeRegEx": "Gosling et al\\.,? 2003", "shortCiteRegEx": "Gosling et al\\.", "year": 2003}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational.", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Michel Galley Chris Brockett Jianfeng Gao Bill Dolan Jiwei Li."], "venue": "The 2016 Conference of the North American Chapter of the Association. pages 110\u2013119.", "citeRegEx": "Li.,? 2016", "shortCiteRegEx": "Li.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Automatic recognition of personality in conversation", "author": ["Fran\u00e7ois Mairesse", "Marilyn Walker."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers. Association for Computational Lin-", "citeRegEx": "Mairesse and Walker.,? 2006", "shortCiteRegEx": "Mairesse and Walker.", "year": 2006}, {"title": "Personage: Personality generation for dialogue", "author": ["Fran\u00e7ois Mairesse", "Marilyn Walker."], "venue": "Annual Meeting-Association For Computational Linguistics. page 496.", "citeRegEx": "Mairesse and Walker.,? 2007", "shortCiteRegEx": "Mairesse and Walker.", "year": 2007}, {"title": "Using linguistic cues for the automatic recognition of personality in conversation and text", "author": ["Fran\u00e7ois Mairesse", "Marilyn A Walker", "Matthias R Mehl", "Roger K Moore."], "venue": "Journal of artificial intelligence research pages 457\u2013500.", "citeRegEx": "Mairesse et al\\.,? 2007", "shortCiteRegEx": "Mairesse et al\\.", "year": 2007}, {"title": "Sequence to backward and forward sequences: A content-introducing approach", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": null, "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Toward an adequate taxonomy of personality attributes: Replicated factor structure in peer nomination personality ratings", "author": ["Warren T Norman."], "venue": "The Journal of Abnormal and Social Psychology 66(6):574.", "citeRegEx": "Norman.,? 1963", "shortCiteRegEx": "Norman.", "year": 1963}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 583\u2013593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau."], "venue": "CoRR abs/1507.04808.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "Proceedings of the Association for Computational Linguistics. pages 1577\u20131586.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "The 2015", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Computing machinery and intelligence", "author": ["Alan M Turing."], "venue": "Mind 59(236):433\u2013460.", "citeRegEx": "Turing.,? 1950", "shortCiteRegEx": "Turing.", "year": 1950}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869 .", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Improving linguistic style: social and affective bases for agent personality", "author": ["Marilyn A. Walker", "Janet E. Cahn", "Stephen J. Whittaker."], "venue": "In: Mller, J. (Ed.), Proceedings of Autonomous Agents.", "citeRegEx": "Walker et al\\.,? 1997", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "An annotated corpus of film dialogue for learning and characterizing character style", "author": ["Marilyn A Walker", "Grace I Lin", "Jennifer Sawyer."], "venue": "LREC. pages 1373\u20131378.", "citeRegEx": "Walker et al\\.,? 2012", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "A networkbased end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young."], "venue": "arXiv preprint arXiv:1604.04562 .", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Topic aware neural response generation", "author": ["Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma."], "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence.", "citeRegEx": "Xing et al\\.,? 2017", "shortCiteRegEx": "Xing et al\\.", "year": 2017}, {"title": "Neural generative question answering", "author": ["Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li."], "venue": "Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence. pages 2972\u20132978.", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams."], "venue": "Proceedings of the IEEE 101(5):1160\u20131179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Strategy and policy learning for nontask-oriented conversational systems", "author": ["Zhou Yu", "Ziyu Xu", "Alan W Black", "Alex I Rudnicky."], "venue": "17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 404.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Generating human-level conversations by machine has been a long-term goal of AI since the Turing Test (Turing, 1950).", "startOffset": 102, "endOffset": 116}, {"referenceID": 20, "context": "However, as argued by (Vinyals and Le, 2015), the current conversation generation models are still unable to deliver realistic conversations to pass the Test.", "startOffset": 22, "endOffset": 44}, {"referenceID": 12, "context": "Though personality is a well-defined concept in psychology (Norman, 1963; Gosling et al., 2003), while in this paper, the personality of a chatbot refers to the character that the bot plays or performs during conversational interactions.", "startOffset": 59, "endOffset": 95}, {"referenceID": 3, "context": "Though personality is a well-defined concept in psychology (Norman, 1963; Gosling et al., 2003), while in this paper, the personality of a chatbot refers to the character that the bot plays or performs during conversational interactions.", "startOffset": 59, "endOffset": 95}, {"referenceID": 21, "context": "In this scenario, personality can be viewed as a composite of the identity (the background and profile) that a chatbot is endowed with, linguistic style that an agent exhibits during interactions (Walker et al., 1997), and many more explicit and implicit cues that may portray character.", "startOffset": 196, "endOffset": 217}, {"referenceID": 27, "context": "The motivation is also verified by (Yu et al., 2016) which reports that users ask for much personal information of a chatbot in human-machine interaction.", "startOffset": 35, "endOffset": 52}, {"referenceID": 7, "context": "The recent work dealing with personality in large-scale conversation generation can be seen in (Li et al., 2016) where speaker-specific conversation style is learned by user embedding.", "startOffset": 95, "endOffset": 112}, {"referenceID": 0, "context": "Another work which models user personalization can be seen in (Al-Rfou et al., 2016), with a similar technique of user embedding.", "startOffset": 62, "endOffset": 84}, {"referenceID": 26, "context": "These works can be categorized into task-oriented (Young et al., 2013; Wen et al., 2016; Bordes and Weston, 2016) or chat-based.", "startOffset": 50, "endOffset": 113}, {"referenceID": 23, "context": "These works can be categorized into task-oriented (Young et al., 2013; Wen et al., 2016; Bordes and Weston, 2016) or chat-based.", "startOffset": 50, "endOffset": 113}, {"referenceID": 1, "context": "These works can be categorized into task-oriented (Young et al., 2013; Wen et al., 2016; Bordes and Weston, 2016) or chat-based.", "startOffset": 50, "endOffset": 113}, {"referenceID": 13, "context": "Recently, researchers found that social data such as Twitter/Weibo posts and replies (Ritter et al., 2011; Shang et al., 2015), and movie dialogues can be used to learn and generate spoken language.", "startOffset": 85, "endOffset": 126}, {"referenceID": 16, "context": "Recently, researchers found that social data such as Twitter/Weibo posts and replies (Ritter et al., 2011; Shang et al., 2015), and movie dialogues can be used to learn and generate spoken language.", "startOffset": 85, "endOffset": 126}, {"referenceID": 13, "context": "Large-scale conversation generation with social media data was firstly proposed in (Ritter et al., 2011) and has been greatly advanced by applying sequence-to-sequence models (Sutskever et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 18, "context": ", 2011) and has been greatly advanced by applying sequence-to-sequence models (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015, 2016).", "startOffset": 78, "endOffset": 193}, {"referenceID": 20, "context": ", 2011) and has been greatly advanced by applying sequence-to-sequence models (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015, 2016).", "startOffset": 78, "endOffset": 193}, {"referenceID": 17, "context": ", 2011) and has been greatly advanced by applying sequence-to-sequence models (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015, 2016).", "startOffset": 78, "endOffset": 193}, {"referenceID": 16, "context": ", 2011) and has been greatly advanced by applying sequence-to-sequence models (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015, 2016).", "startOffset": 78, "endOffset": 193}, {"referenceID": 4, "context": "These works include: dealing with unknown words (Gu et al., 2016; Gulcehre et al., 2016), avoiding universal responses (Jiwei Li, 2016), generating more diverse and meaningful responses (Xing et al.", "startOffset": 48, "endOffset": 88}, {"referenceID": 5, "context": "These works include: dealing with unknown words (Gu et al., 2016; Gulcehre et al., 2016), avoiding universal responses (Jiwei Li, 2016), generating more diverse and meaningful responses (Xing et al.", "startOffset": 48, "endOffset": 88}, {"referenceID": 24, "context": ", 2016), avoiding universal responses (Jiwei Li, 2016), generating more diverse and meaningful responses (Xing et al., 2017; Mou et al., 2016), and many more.", "startOffset": 105, "endOffset": 142}, {"referenceID": 11, "context": ", 2016), avoiding universal responses (Jiwei Li, 2016), generating more diverse and meaningful responses (Xing et al., 2017; Mou et al., 2016), and many more.", "startOffset": 105, "endOffset": 142}, {"referenceID": 20, "context": "As argued by (Vinyals and Le, 2015), it\u2019s still quite impossible for current chatbots to pass the Turing Test, while one of the reasons is the lack of a coherent personality.", "startOffset": 13, "endOffset": 35}, {"referenceID": 12, "context": "Though personality has been well defined in psychology (Norman, 1963), it is implicit, subtle, and challenging to formally define in statistical language generation.", "startOffset": 55, "endOffset": 69}, {"referenceID": 8, "context": "Linguistic style can be an indicator of personality (Mairesse and Walker, 2006; Mairesse et al., 2007), and conversation can be clues for personality recognition (Walker et al.", "startOffset": 52, "endOffset": 102}, {"referenceID": 10, "context": "Linguistic style can be an indicator of personality (Mairesse and Walker, 2006; Mairesse et al., 2007), and conversation can be clues for personality recognition (Walker et al.", "startOffset": 52, "endOffset": 102}, {"referenceID": 9, "context": "In reverse, spoken language can be generated in accordance to particular personality (Mairesse and Walker, 2007).", "startOffset": 85, "endOffset": 112}, {"referenceID": 7, "context": "A first attempt to model persona can be seen in (Li et al., 2016) where the authors proposed to learn speaker-specific conversation style by user embedding.", "startOffset": 48, "endOffset": 65}, {"referenceID": 7, "context": "Our work differs from this work significantly: our task is to endow the chatbot with a fixed personality while (Li et al., 2016) learns personalized conversational styles.", "startOffset": 111, "endOffset": 128}, {"referenceID": 7, "context": "Further, (Li et al., 2016) requires many dialogue data from each user while our work has no such requirement.", "startOffset": 9, "endOffset": 26}, {"referenceID": 25, "context": "Another related work is generative question answering (GenQA) (Yin et al., 2015) which generates a response containing an answer extracted from a knowledge base (KB).", "startOffset": 62, "endOffset": 80}, {"referenceID": 18, "context": "where P (z|x) is the probability of using the agent profile given post x, which is computed by the Profile Detector; P fr(y|x) = \u220fm t=1P (yt|y<t,x) is given by a general forward decoder, the same as (Sutskever et al., 2014), and P bi(y|x, {< ki, vi >}) is given by a Bidirectional Decoder which will be described later.", "startOffset": 199, "endOffset": 223}, {"referenceID": 2, "context": "Given a post x = x1x2 \u00b7 \u00b7 \u00b7xn, the hidden states of the post h = (h1, h2, \u00b7 \u00b7 \u00b7, hn) are obtained by a gated recurrent unit (GRU) (Chung et al., 2014), as follows:", "startOffset": 130, "endOffset": 150}, {"referenceID": 11, "context": "Inspired by (Mou et al., 2016), we design a bidirectional decoder which consists of a backward decoder and forward decoder, but with a key difference that a position detector is employed to predict a start decoding position.", "startOffset": 12, "endOffset": 30}, {"referenceID": 4, "context": "This issue makes our work differ substantially from previous approaches where supervision is directly observable either between post and response (Gu et al., 2016) or between response and knowledge base (Yin et al.", "startOffset": 146, "endOffset": 163}, {"referenceID": 25, "context": ", 2016) or between response and knowledge base (Yin et al., 2015).", "startOffset": 47, "endOffset": 65}, {"referenceID": 11, "context": "Experiments also show that the position detector contributes much to the performance improvement than a random position picking strategy (Mou et al., 2016).", "startOffset": 137, "endOffset": 155}, {"referenceID": 18, "context": "We name our model Identity-Coherent Conversation Machine (ICCM) and compare it with several baselines: Seq2Seq: a general sequence to sequence model (Sutskever et al., 2014).", "startOffset": 149, "endOffset": 173}, {"referenceID": 11, "context": "ICCM-Pos: Instead of using a predicted position obtained by the position detector to start the decoding process, the bidirectional decoder in this setting randomly picks a word in a response during training, the same as (Mou et al., 2016).", "startOffset": 220, "endOffset": 238}], "year": 2017, "abstractText": "Endowing a chatbot with personality or an identity is quite challenging but critical to deliver more realistic and natural conversations. In this paper, we address the issue of generating responses that are coherent to a pre-specified agent profile. We design a model consisting of three modules: a profile detector to decide whether a post should be responded using the profile and which key should be addressed, a bidirectional decoder to generate responses forward and backward starting from a selected profile value, and a position detector that predicts a word position from which decoding should start given a selected profile value. We show that general conversation data from social media can be used to generate profile-coherent responses. Manual and automatic evaluation shows that our model can deliver more coherent, natural, and diversified responses.", "creator": "LaTeX with hyperref package"}}}