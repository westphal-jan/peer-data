{"id": "1706.02921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "End-to-End Musical Key Estimation Using a Convolutional Neural Network", "abstract": "we present an end - to - end system for musical key width estimation, based on a convolutional neural network. the proposed system not initially only out - performs existing key estimation methods proposed in the academic literature ; it is also capable of learning a unified model for diverse musical genres that performs comparably to existing systems specialised for specific genres. concurrently our experiments confirm that different genres do nevertheless differ in their interpretation estimates of tonality, and understand thus a system tuned automatically e. g. for pop music performs subpar on pieces of electronic music. they also reveal that such cross - genre setups evoke specific types of error ( predicting the relative or parallel minor ). however, using the data - driven prediction approach proposed in this paper, we can frequently train models that deal with multiple musical styles adequately, and without major financial losses in accuracy.", "histories": [["v1", "Fri, 9 Jun 2017 12:33:23 GMT  (242kb,D)", "http://arxiv.org/abs/1706.02921v1", "First published in the Proceedings of the 25th European Signal Processing Conference (EUSIPCO-2017) in 2017, published by EURASIP"]], "COMMENTS": "First published in the Proceedings of the 25th European Signal Processing Conference (EUSIPCO-2017) in 2017, published by EURASIP", "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["filip korzeniowski", "gerhard widmer"], "accepted": false, "id": "1706.02921"}, "pdf": {"name": "1706.02921.pdf", "metadata": {"source": "CRF", "title": "End-to-End Musical Key Estimation Using a Convolutional Neural Network", "authors": ["Filip Korzeniowski", "Gerhard Widmer"], "emails": ["filip.korzeniowski@jku.at"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe key of a piece of (Western) music defines its harmonic center and thus plays a vital role in the piece\u2019s tonality. It gives meaning to the harmonic progression of a piece, and provides the backdrop for the build-up and release of harmonic tension. Thus, if we want to automatically detect tension and its release in musical audio, if we want to find similarities in harmonic structure in recordings of songs, or if we want to assist disc jockeys or creators of electronic music in finding appropriate musical samples to work with, we need reliable methods that extract the key from a musical piece. It is therefore of no surprise that the automatic key estimation is a long-standing task in the music information retrieval community.\nMany key estimation systems proposed in the academic literature rely on the same fundamental pipeline. First, a time-frequency representation, like the spectrogram or the constant-q-transform, is computed from the audio. Then, a variant of a pitch-class-profile, also called chroma feature, is extracted for every time frame. Here, the goal is to get an octave-independent representation of pitch classes that is robust to timbre. Finally, these features are aggregated over time and matched with feature templates for each key, where the key with the best matching template is selected. Examples of such systems are [1]\u2013[3], and, most recently, [4]. Such systems typically report a single global key for a piece. This limitation is reasonable for a variety of genres (like pop/rock or electronic music), but fails to cope with pieces that contain key modulations (as common in classical music).\nAnother line of work considers estimating key and chords simultaneously, e.g. [5]\u2013[8]. Such systems aim at exploiting the musicological relationship between key and chords in order\nto improve the accuracy of both. They typically estimate local keys, and are thus able to cope with key modulations. However, while they bear the potential to explain the harmonic content of musical audio more holistically, dedicated systems currently seem to achieve better results1.\nThis paper targets the estimation of a single global key for pieces of musical audio. In contrast to previous works, we abandon hand-crafting or tuning elements in the key estimation pipeline. Instead, we employ a convolutional neural network that encompasses the three stages of pre-processing, feature extraction, and classification. Although neural networks have been used for key detection (e.g. [9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input. Our system operates directly on the spectrogram, and it can estimate all its parameters from the data. To our knowledge, this is the first work that replaces the complete key estimation pipeline with a model that can be optimised in an end-to-end manner2."}, {"heading": "II. METHOD", "text": "Our system consists of two steps: first, we compute from the audio a logarithmically filtered log-magnitude spectrogram. This process is detailed in Sec. II-A. Then, we feed this timefrequency representation to the convolutional neural network, described in Sec. II-B, for classification.\nA. Input Processing\nWe input a spectral representation of the audio to our models. Based on our previous work on extracting harmonic information from audio [11], [12], we first compute from the audio the magnitude spectrogram |S| (frame size of 8192 at 5 frames per second, where the sample rate is 44.1 kHz); then, we apply a filterbank B4Log with logarithmically spaced triangular filters (24 bands per octave, from 65 Hz to 2100 Hz), which results in a time-frequency representation in which the fundamental frequencies of notes are spaced linearly; finally,\n1See results of the yearly MIREX challenges at www.music-ir.org/mirex. 2One might argue that [7] is also an \u201cend-to-end\u201d system that detects both chords and keys. However, they used feature extraction methods heavily based on expert knowledge (tuning correction, harmonic-percussive source separation, beat synchronisation, frequency-split chroma computation), and trained a dynamic Bayesian network whose structure is based on domain knowledge. Also, its key estimation performance was never evaluated.\n1\nar X\niv :1\n70 6.\n02 92\n1v 1\n[ cs\n.L G\n] 9\nJ un\n2 01\n7\nwe logarithmise the magnitudes of the filtered spectrogram to compress the value range, resulting in the logarithmically filtered log-magnitude spectrogram\nL = log ( 1 +B4Log |S| ) .\nThis representation is similar to a constant-q transform, but is much cheaper to compute. Additionally, as shown in [13], the constant-q transform does not necessarily lead to better results in tasks relying on pitch information. All computations are done using the madmom library [14]."}, {"heading": "B. Model", "text": "The proposed neural network is designed to encompass all stages of the classic key estimation pipeline: a pre-processing stage of convolutional layers, a dense layer that projects the feature maps into a short representation at the time-frame level, a global averaging layer that aggregates this representation over time, and a softmax classification layer that predicts the global key of a piece. Figure 1 shows our model\u2019s architecture: five convolutional layers with 8 feature maps computed by 5\u00d75 kernels, followed by a dense layer with 48 units applied frame-wise; this projection is then averaged over time and classified using a 24-way softmax layer. All layers (except the softmax layer) use the exponential-linear activation function [15].\nThe convolutional layers constitute the first part of the \u201cfeature extraction\u201d equivalent in traditional key estimation systems. They are intended to process the input spectrogram, deal with detrimental factors such as noise or slight detuning,\nand, together with the projection layer, compute a short framewise description of harmonic content. This part of the network can process inputs of arbitrary lengths. Its output is aggregated in the following layers.\nBefore classification, an averaging layer reduces the extracted representation to a fixed-length vector. We could employ other, more powerful methods (like recurrent layers), but we found in preliminary experiments that they fail to achieve better results.\nFinally, a softmax classification layer predicts the global key for the audio. We restrict ourselves to major and minor modes only, resulting in 24 possible classes (12 tonics \u00d7 {major,minor}) as output. This is a common restriction, since most musical pieces are in either major or minor, and as of now, there are no datasets with reliable song-level annotations of other modes."}, {"heading": "C. Training", "text": "We train the model using stochastic gradient descent with momentum, back-propagating through the network the categorical cross-entropy error between true key label yi and network output y\u0302i, and apply weight decay with a factor of 10\u22124 for regularisation. The initial learning rate is 0.001, with a momentum factor of 0.9. If validation accuracy did not increase within 10 epochs, we halve the learning rate and continue training with the parameters that gave the best results until then. After 100 epochs, we select the model that achieved the best validation accuracy."}, {"heading": "III. EXPERIMENTS", "text": "Our experiments aim at 1) comparing the proposed system to reference systems, and 2) examining the effect that the type of training data (in our case, the musical genre) has on the results. Template-based algorithms require specialised key templates for genres like electronic dance music in order to perform well [4]. We want to see if, and how strongly, our system is affected by this. In the following, we discuss the data, the evaluation metrics, the reference systems, and the different set-ups of our system that we used in the experiments."}, {"heading": "A. Data", "text": "We use three datasets in the course of our experiments: the GiantSteps key dataset [16], the GiantSteps-MTG key dataset, and a subset of the McGill Billboard dataset [17].\nThe GiantSteps Key Dataset3 comprises 604 two-minute audio previews from www.beatport.com, with key ground truth for each excerpt. It consists of various sub-genres of electronic music. We use this dataset for testing purposes only, and will refer to it as GS.\nThe GiantSteps MTG Key dataset4, collected by A\u0301ngel Faraldo from the Music Technology Group at Universitat Pompeu Fabra, comprises 1486 two-minute audio previews from the same source. These excerpts are distinct from the ones in the GS dataset. From this set, we only use excerpts\n3https://github.com/GiantSteps/giantsteps-key-dataset 4https://github.com/GiantSteps/giantsteps-mtg-key-dataset\nlabelled with a single key and a high confidence for training (1077 pieces). We will refer to this dataset as GSMTG.\nThe McGill Billboard Dataset5 comprises 742 unique songs sampled from the American Billboard charts between 1958 and 1991, and thus consists of mostly pop and rock music. Unfortunately, only the tonic, and not the mode (major or minor), is annotated for each piece. We therefore estimate the mode using the tonic and chord annotations, following a simple procedure for each piece: 1) select all chords whose root is the tonic; 2) if more than 90% of these chords are major, the key mode is assumed to be major, and vice-versa for minor; 3) else, discard the song, because we cannot confidently estimate the mode. Similarly, we discard songs with multiple annotated tonics. This leaves us with 625 songs with key annotations. We then divide the set into subsets of 62.5% for training, 12.5% for validation, and 25% for testing. The exact division and key ground truths are available online6. We will refer to this dataset as BBTV and BBTE for the train/validation and test sub-sets, respectively."}, {"heading": "B. Data Augmentation", "text": "The datasets provide only few training data (1077 in GSMTG, 391 in BBTV), compared to datasets used in computer vision (e.g. 40000 in CIFAR-10 or 60000 in MNIST). The generalisation capability of deep neural networks, however, depends on a large number of training samples. We thus have to rely on data augmentation to increase the number of training examples artificially.\nSeveral augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20]. Since pitch shifting is an expensive time-domain operation, most works manipulate the time-frequency representation to emulate it. In this work, however, we found that using a time-domain pitch shifting algorithm7 directly on the audio gave better results in terms of classification accuracy. We therefore shift each training song in the range of -4 to +7 semitones (and adjust the target key accordingly), which increases the amount of training data by a factor of 12."}, {"heading": "C. Metrics", "text": "Evaluating key estimation results requires a more detailed quantitative analysis than computing accuracy scores. In particular, although we consider the task to be a simple 24- way classification problem when designing the system, some classes are semantically closer to each other than others. For example, the key of A-minor is called the \u201crelative minor\u201d to the key of C-major, as they share all pitch classes, and differ only in the tonic. Therefore, it is reasonable to consider some errors to be more severe than others.\nThe MIREX evaluation campaign8 developed an evaluation strategy and introduced a single weighted measure that reflects\n5http://ddmal.music.mcgill.ca/research/billboard 6http://www.cp.jku.at/people/korzeniowski/bb.zip 7We used the SoX software available at http://sox.sourceforge.net/. 8http://www.music-ir.org/mirex\nthe above considerations. Following their guidelines, key predictions can fall into the following categories: \u2022 Correct: if the tonic and the mode (major/minor) of\nprediction and target correspond. \u2022 Fifth: if the tonic of the prediction is the fifth of the\ntarget (or vice versa), and modes correspond. \u2022 Relative Minor/Major: if modes differ and either a)\nthe predicted mode is minor and the predicted tonic is 3 semitones below the target, or b) the predicted mode is major and the predicted tonic is 3 semitones above the target. \u2022 Parallel Minor/Major: if modes differ but the predicted tonic matches the target. \u2022 Other: Prediction errors not caught by any category, i.e. the most severe errors.\nWe first compute the ratio of predictions that fall into each category. We then calculate the MIREX weighted score as w = rc+0.5\u00b7rf+0.3\u00b7rr+0.2\u00b7rp, where rc, rf , rr, and rp are the ratios of the correct, fifth, relative minor/major, and parallel minor/major, respectively. These ratios reveal more about the capability of the algorithms than accuracy (i.e., the \u201ccorrect\u201d ratio) alone. They allow us to see the kind of mistakes the system makes, and at the same time, assign a single number for comparing its performance with others.\nThe ratios of the individual error categories cannot be compared in isolation, but only in context with the other ratios. The only numbers that can be compared individually are the weighted score (because it aggregates all error types), percentage of correct classifications (because it corresponds to classification accuracy), and the \u201cother\u201d error ratio (because it tells us how often a system predicts unrelated keys). That is why, in Table I, we will highlight the best results only for these categories."}, {"heading": "D. Setups and Reference Systems", "text": "We train our method in three configurations: CK1, trained on GSMTG; CK2, trained on BBTV; and CK3, trained on both GSMTG and BBTV. We evaluate each of the trained models on GS and on BBTE. This way, we observe the system\u2019s performance when trained on the same genres as it is tested on, when trained on a different genre (the cross-genre setup), and when trained on multiple genres (to see if it can learn a unified model for different genres).\nWe compare our method against the Queen Mary Key Detector (QM) [21] and three variations of the method presented in [4] (EDMA, EDMM, EDMT). For both systems, open source implementations are available9. QM consists of a hand-crafted pre-processing stage and correlates the obtained chromagrams with key profiles based on Bach\u2019s Well Tempered Klavier. The three EDM systems also use a handcrafted pre-processing stage, but use different key profiles for classification: EDMA uses key profiles automatically derived from a set of electronic music; EDMM uses hand-tuned profiles\n9http://vamp-plugins.org/plugin-doc/qm-vamp-plugins.html https://github.com/angelfaraldo/edmkey\nbased on the automatically derived ones (effectively disabling the prediction of major keys); EDMT uses profiles based on European classical music. Of all submissions to MIREX, EDMM and QM achieved the best results on the electronic and classical music datasets used for evaluation, respectively. We thus consider both to be state of the art."}, {"heading": "E. Results", "text": "Table I shows the evaluation results of all training configurations of our proposed model, and of the reference systems. We determine the statistical significance of the results using a Wilcoxon signed rank test, with the error types representing the ranks. If trained on the correct genre, our model clearly outperforms the reference systems: 74.3 vs. 70.1 (\u03b1 = 0.001) for the GiantSteps dataset, and 83.9 vs. 78.7 (\u03b1 = 0.014) for the Billboard dataset.\nExamining the cross-genre setups, we observe a significant drop in key estimation accuracy: tested on GS (electronic music), a model trained on BBTV (pop/rock) achieves a weighted score of only 57.3, compared to 74.3 when trained on electronic music from GSMTG. However, we also see that the number of severe mistakes (category \u201cother\u201d) that our system commits in this setup is not higher than those of the reference systems: the model only predicts a completely unrelated key 17.4% of the time\u2014similar to the reference systems specialised on this genre (17.6% and 18.5% for EDMA and EDMM, respectively); vice-versa, when trained on GSMTG and evaluated on BBTE, it achieves the lowest rate of severe mistakes (4.2%).\nThe most common error occurring in these cross-genre setups is predicting the wrong mode (resulting in parallel minor/major) and predicting the relative minor/major key. This suggests that while the model is still able recognise some fundamental concepts of tonality, finer characteristics vary too much between pieces of different genres.\nWe wanted to see if the proposed model can be trained to provide a good unified key estimator for multiple genres by\ncombining training data. The resulting system CK3 does not reach the performance of the specialised ones (69.2 vs 74.3 on GS, 79.7 vs. 83.9 on BBTE); however, on GS, it performs as well as EDMM, which is tuned manually to give good results on electronic music datasets (69.2 vs. 70.1, \u03b1 = 0.94). It still performs better than EDMA, which is also trained on electronic music, but without manual post-training adaptations (69.2 vs. 65.5, \u03b1 = 0.02).\nThe numbers presented for the EDM* systems for the GiantSteps dataset differ from the ones originally reported in [4]. This is mainly because we applied a stricter criterion for the \u201cfifth\u201d category: we require the predicted mode to match the target mode, while [4] ignores the mode for this category. Also, according to personal correspondence with the author, changes in the library used in the original implementation worsened the results relative to the original ones."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "We have presented a global key estimation system based on a convolutional neural network. Compared to previous work, this model can be automatically trained end-to-end, without the need of expert knowledge in feature design or specific pre-processing steps such as tuning correction or spectral whitening.\nWe have shown experimentally that the model performs state-of-the-art on datasets of electronic music and pop/rock music. Additionally, we are planning to evaluate the proposed model on more genres, e.g. classical music.\nAnother feature of the proposed model is its ability to adapt to multiple types of music without changing the model itself; it just needs to be re-trained with a training set extended to the type of music of interest. While it still showed good performance in such a scenario, it did not reach the level of its specialised counterparts.\nA clear limitation of the proposed method is that it only estimates a global key for a complete piece. While this is adequate for certain types of music, other types (e.g. classical\nmusic) involve key modulations that our method currently cannot capture. A possible easy fix could be to apply our model using a sliding window over the spectrogram. Extending the proposed method in such a way is left to future work.\nFinally, we have to keep in mind that even with data augmentation, we are still working with small datasets. Although we increase the number of training samples by a factor of 12 using pitch-shifting, this is not equivalent to having available 12 times as many musical pieces: the musical content of the artificial data points is still the same as in the seed data point, just in a different key. We expect the system\u2019s performance to improve once more training data is available."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by the European Research Council (ERC) under the EUs Horizon 2020 Framework Programme (ERC Grant Agreement number 670035, project \u201cCon Espressione\u201d). The Tesla K40 used for this research was donated by the NVIDIA Corporation."}], "references": [{"title": "Musical Key Extraction From Audio", "author": ["S. Pauws"], "venue": "Proceedings of the 5th International Society for Music Information Retrieval Conference (ISMIR), Barcelona, Spain, Oct. 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "What\u2019s Key for Key? The Krumhansl-Schmuckler Key- Finding Algorithm Reconsidered", "author": ["D. Temperley"], "venue": "Music Perception: An Interdisciplinary Journal, vol. 17, no. 1, pp. 65\u2013100, Oct. 1999.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Signal Processing Parameters for Tonality Estimation", "author": ["K. Noland", "M. Sandler"], "venue": "Audio Engineering Society Convention 122. Audio Engineering Society, May 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Key Estimation in Electronic Dance Music", "author": ["\u00c1. Faraldo", "E. G\u00f3mez", "S. Jord\u00e0", "P. Herrera"], "venue": "Advances in Information Retrieval, N. Ferro, F. Crestani, M.-F. Moens, J. Mothe, F. Silvestri, G. M. Di Nunzio, C. Hauff, and G. Silvello, Eds. Cham: Springer International Publishing, 2016, vol. 9626, pp. 335\u2013347.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Simultaneous Estimation of Chords and Musical Context From Audio", "author": ["M. Mauch", "S. Dixon"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1280\u20131289, Aug. 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining Musicological Knowledge About Chords and Keys in a Simultaneous Chord and Local Key Estimation System", "author": ["J. Pauwels", "J.-P. Martens"], "venue": "Journal of New Music Research, vol. 43, no. 3, pp. 318\u2013330, Jul. 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "An End-to- End Machine Learning System for Harmonic Analysis of Music", "author": ["Y. Ni", "M. McVicar", "R. Santos-Rodriguez", "T. De Bie"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 6, pp. 1771\u20131783, Aug. 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony", "author": ["B. Di Giorgi", "M. Zanoni", "A. Sarti", "S. Tubaro"], "venue": "Proceedings of the 8th International Workshop on Multidimensional Systems, Erlangen, Germany, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Key detection through pitch class distribution model and ANN", "author": ["J. Sun", "H. Li", "L. Lei"], "venue": "2009 16th International Conference on Digital Signal Processing, Jul. 2009, pp. 1\u20136.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio-based music classification with a pretrained convolutional network", "author": ["S. Dieleman", "P. Brakel", "B. Schrauwen"], "venue": "12th International Society for Music Information Retrieval Conference (ISMIR-2011). University of Miami, 2011, pp. 669\u2013674.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A Fully Convolutional Deep Auditory Model for Musical Chord Recognition", "author": ["F. Korzeniowski", "G. Widmer"], "venue": "Proceedings of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP), Salerno, Italy, Sep. 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature Learning for Chord Recognition: The Deep Chroma Extractor", "author": ["\u2014\u2014"], "venue": "Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR), New York, USA, Aug. 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "On the Potential of Simple Framewise Approaches to Piano Transcription", "author": ["R. Kelz", "M. Dorfer", "F. Korzeniowski", "S. B\u00f6ck", "A. Arzt", "G. Widmer"], "venue": "Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR), New York, USA, Aug. 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Madmom: A new Python Audio and Music Signal Processing Library", "author": ["S. B\u00f6ck", "F. Korzeniowski", "J. Schl\u00fcter", "F. Krebs", "G. Widmer"], "venue": "Proceedings of the 24th ACM International Conference on Multimedia (ACMMM), Amsterdam, Netherlands, Oct. 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "International Conference on Learning Representations (ICLR), arXiv:1511.07289, San Juan, Puerto Rico, Feb. 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections.", "author": ["P. Knees", "A. Faraldo", "P. Herrera", "R. Vogl", "S. B\u00f6ck", "F. H\u00f6rschl\u00e4ger", "M. Le Goff"], "venue": "Proceedings of the 16th", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis.", "author": ["J.A. Burgoyne", "J. Wild", "I. Fujinaga"], "venue": "Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Exploring Data Augmentation for Improved Singing Voice Detection with Neural Networks.", "author": ["J. Schl\u00fcter", "T. Grill"], "venue": "Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Improved Techniques for Automatic Chord Recognition from Music Audio Signals", "author": ["T. Cho"], "venue": "Dissertation, New York University, New York, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Rethinking Automatic Chord Recognition with Convolutional Neural Networks", "author": ["E.J. Humphrey", "J.P. Bello"], "venue": "11th International Conference on Machine Learning and Applications (ICMLA). Boca Raton, USA: IEEE, Dec. 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "MIREX 2016 Entry: Vamp Plugins from the Centre for Digital Music", "author": ["C. Cannam", "M. Mauch", "M.E. Davies", "S. Dixon", "C. Landone", "K. Noland", "M. Levy", "M. Zanoni", "D. Stowell", "L.A. Figueira"], "venue": "MIREX, Tech. Rep., 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Examples of such systems are [1]\u2013[3], and, most recently, [4].", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "Examples of such systems are [1]\u2013[3], and, most recently, [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "Examples of such systems are [1]\u2013[3], and, most recently, [4].", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "[5]\u2013[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[5]\u2013[8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 8, "context": "[9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "[9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input.", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "[9], [10]), the existing approaches rely on a hand-crafted feature extraction stage: in [9], a pitch class distribution matrix is fed into a neural network classifier; in [10], beat-aligned chroma and timbre features are used as input.", "startOffset": 171, "endOffset": 175}, {"referenceID": 10, "context": "Based on our previous work on extracting harmonic information from audio [11], [12], we first compute from the audio the magnitude spectrogram |S| (frame size of 8192 at 5 frames per second, where the sample rate is 44.", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "Based on our previous work on extracting harmonic information from audio [11], [12], we first compute from the audio the magnitude spectrogram |S| (frame size of 8192 at 5 frames per second, where the sample rate is 44.", "startOffset": 79, "endOffset": 83}, {"referenceID": 6, "context": "2One might argue that [7] is also an \u201cend-to-end\u201d system that detects both chords and keys.", "startOffset": 22, "endOffset": 25}, {"referenceID": 14, "context": "All layers are followed by exponential-linear activations [15], except the last, which is followed by a softmax.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "Additionally, as shown in [13], the constant-q transform does not necessarily lead to better results in tasks relying on pitch information.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "All computations are done using the madmom library [14].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "softmax layer) use the exponential-linear activation function [15].", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "Template-based algorithms require specialised key templates for genres like electronic dance music in order to perform well [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 15, "context": "We use three datasets in the course of our experiments: the GiantSteps key dataset [16], the GiantSteps-MTG key dataset,", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "and a subset of the McGill Billboard dataset [17].", "startOffset": 45, "endOffset": 49}, {"referenceID": 17, "context": "Several augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "Several augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "Several augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "Several augmentation techniques for audio input have been explored [18], with pitch shifting being particularly popular in harmony-related tasks [11], [19], [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 20, "context": "Detector (QM) [21] and three variations of the method presented in [4] (EDMA, EDMM, EDMT).", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "Detector (QM) [21] and three variations of the method presented in [4] (EDMA, EDMM, EDMT).", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "CK* DENOTE THE PROPOSED MODEL TRAINED ON DIFFERENT DATA SETS, EDM* AND QM DENOTE REFERENCE SYSTEMS BY [4], [21].", "startOffset": 102, "endOffset": 105}, {"referenceID": 20, "context": "CK* DENOTE THE PROPOSED MODEL TRAINED ON DIFFERENT DATA SETS, EDM* AND QM DENOTE REFERENCE SYSTEMS BY [4], [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "The numbers presented for the EDM* systems for the GiantSteps dataset differ from the ones originally reported in [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "This is mainly because we applied a stricter criterion for the \u201cfifth\u201d category: we require the predicted mode to match the target mode, while [4] ignores the mode for this category.", "startOffset": 143, "endOffset": 146}], "year": 2017, "abstractText": "We present an end-to-end system for musical key estimation, based on a convolutional neural network. The proposed system not only out-performs existing key estimation methods proposed in the academic literature; it is also capable of learning a unified model for diverse musical genres that performs comparably to existing systems specialised for specific genres. Our experiments confirm that different genres do differ in their interpretation of tonality, and thus a system tuned e.g. for pop music performs subpar on pieces of electronic music. They also reveal that such cross-genre setups evoke specific types of error (predicting the relative or parallel minor). However, using the data-driven approach proposed in this paper, we can train models that deal with multiple musical styles adequately, and without major losses in accuracy.", "creator": "LaTeX with hyperref package"}}}