{"id": "1706.03254", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "On Hash-Based Work Distribution Methods for Parallel Best-First Search", "abstract": "parallel best - first search algorithms such either as hash distributed a * ( hda * ) distribute work among the processes using a global hash function. however we analyze the search and communication overheads of state - of - the - kind art hash - based parallel based best - first search algorithms, design and show confidently that although zobrist hashing, the standard block hash function used by hda *, achieves good load balance for many domains, it incurs significant logistics communication or overhead since almost theoretically all digitally generated nodes are transferred to a wholly different processor then than their parents. basically we propose deep abstract zobrist hashing, a new work distribution method for parallel search which, ostensibly instead of always computing a hash value based on the expected raw features of a state, uses a feature projection function to generate a set of abstract features which results in a visually higher locality, resulting in reduced communications overhead. we show efficiently that abstract zobrist hashing then outperforms similarly previous methods on search domains using hand - coded, shallow domain specific feature projection functions. we collectively then propose grazhda *, a graph - partitioning based approach to automatically generating deep feature projection functions. grazhda * seeks mechanisms to approximate the partitioning of the actual search space graph by partitioning the domain transition graph, an abstraction of the state space graph. we show that grazhda * outperforms previous methods on domain - independent planning.", "histories": [["v1", "Sat, 10 Jun 2017 17:05:46 GMT  (6133kb,D)", "http://arxiv.org/abs/1706.03254v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["yuu jinnai", "alex fukunaga"], "accepted": false, "id": "1706.03254"}, "pdf": {"name": "1706.03254.pdf", "metadata": {"source": "CRF", "title": "On Hash-Based Work Distribution Methods for Parallel Best-First Search", "authors": ["Yuu Jinnai", "Alex Fukunaga"], "emails": ["DDYUUDD@GMAIL.COM", "FUKUNAGA@IDEA.C.U-TOKYO.AC.JP"], "sections": [{"heading": null, "text": "among the processes using a global hash function. We analyze the search and communication overheads of state-of-the-art hash-based parallel best-first search algorithms, and show that although Zobrist hashing, the standard hash function used by HDA*, achieves good load balance for many domains, it incurs significant communication overhead since almost all generated nodes are transferred to a different processor than their parents. We propose Abstract Zobrist hashing, a new work distribution method for parallel search which, instead of computing a hash value based on the raw features of a state, uses a feature projection function to generate a set of abstract features which results in a higher locality, resulting in reduced communications overhead. We show that Abstract Zobrist hashing outperforms previous methods on search domains using hand-coded, domain specific feature projection functions. We then propose GRAZHDA*, a graph-partitioning based approach to automatically generating feature projection functions. GRAZHDA* seeks to approximate the partitioning of the actual search space graph by partitioning the domain transition graph, an abstraction of the state space graph. We show that GRAZHDA* outperforms previous methods on domain-independent planning."}, {"heading": "1. Introduction", "text": "The A* algorithm (Hart, Nilsson, & Raphael, 1968) is used in many areas of AI, including planning, scheduling, path-finding, and sequence alignment. Parallelization of A* can yield speedups as well as a way to overcome memory limitations \u2013 the aggregate memory available in a cluster can allow problems that can\u2019t be solved using a single machine to be solved. Thus, designing scalable, parallel search algorithms is an important goal. The major issues which need to be addressed when designing parallel search algorithms are search overhead (states which are unnecessarily generated by parallel search but not by sequential search), communications overhead (overheads associated with moving work among threads), and coordination overhead (synchronization overhead).\nHash Distributed A* (HDA*) is a parallel best-first search algorithm in which each processor executes A* using local open/closed lists, and generated nodes are assigned (sent) to processors according to a global hash function (Kishimoto, Fukunaga, & Botea, 2013). HDA* can be used in distributed memory systems as well as multi-core, shared memory machines, and has been shown to scale up to hundreds of cores with little search overhead.\nThe performance of HDA* depends on the hash function used for assigning nodes to processors. Kishimoto, Fukunaga, and Botea (2009, 2013) showed that using the Zobrist hash function (1970), HDA* could achieve good load balance and low search overhead. Burns et al (2010) noted that\nar X\niv :1\n70 6.\n03 25\n4v 1\n[ cs\n.A I]\n1 0\nJu n\n20 17\nZobrist hashing incurs a heavy communication overhead because many nodes are assigned to processes that are different from their parents, and proposed AHDA*, which used an abstraction-based hash function originally designed for use with PSDD (Zhou & Hansen, 2007) and PBNF (Burns et al., 2010). Abstraction-based work distribution achieves low communication overhead, but at the cost of high search overhead.\nIn this paper, we investigate node distribution methods for HDA*. We start by reviewing previous approaches to work distribution in parallel best-first search, including the HDA* framework (Section 2). Then, in Section 3, we present an in-depth investigation of parallel overheads in stateof-the-art parallel best-first search methods. We begin by investigating why search overhead occurs on parallel best-first search by analyzing how node expansion order in HDA* diverges from that of A*. If the expansion order of a parallel search algorithm is strictly the same as A*, there is no search overhead, so divergence in expansion order is a useful indicator for understanding search overhead. We show that although HDA* incurs some search overhead due to load imbalance and startup overhead, HDA* using the Zobrist Hash function incurs significantly less search overhead than other methods. However, while HDA* with Zobrist hashing successfully achieves low search overhead, we show that communication overhead is actually as important as search overhead in determining the overall efficiency for parallel search, and Zobrist hashing results in very high communications overhead, resulting in poor performance on the grid pathfinding problem.\nNext, in Section 4, we propose Abstract Zobrist hashing (AZH), which achieves both low search overhead and communication overhead by incorporating the strengths of both Zobrist hashing and abstraction. While the Zobrist hash value of a state is computed by applying an incremental hash function to the set of features of a state, AZH first applies a feature projection function mapping features to abstract features, and the Zobrist hash value of the abstract features (instead of the raw features) is computed. We show that on the 24-puzzle, 15-puzzle, and multiple sequence problem, AZH with hand-crafted, domain-specific feature projection function significantly outperform previous methods on a multicore machine with up to 16 cores.\nThen, we propose a domain-independent method to automatically generate an efficient feature projection function for AZH framework. We first show that a work distribution can be modeled as graph partitioning (Section 5). However, standard graph partitioning techniques for workload distribution in scientific computation are inapplicable to heuristic search because the state space is defined implicitly. Then, in Section 6, we propose GRAZHDA*, a new domain-independent method for automatically generating a work distribution function, which, instead of partitioning the actual state space graph (which is impractical), generates an approximation by partitioning a domain transition graph. We then propose a sparsity-based objective function for GRAZHDA*, and experimentally show that GRAZHDA* using the sparsity objective function outperforms all previous variants of HDA* on domain-independent planning, using experiments run on a 48-core cluster as well as a cloud-based cluster with 128 cores. We conclude the paper with a summary of our results and directions for future work (Section 7).\nPortions of this work has been previously presented in two conference papers (Jinnai & Fukunaga, 2016a, 2016b), corresponding to Section 4, as well as parts of Section 2. The two major, new contributions of this journal paper are: (1) GRAZHDA* which defines an objective function that can be used to control the tradeoff between communications and search overhead (Section 5, 6), and (2) analysis of parallel overheads in HDA*, as well as a revisited comparison of HDA* with PBNF (Section 3 and 4.1.4). All of the experimental data in Section 6.3 is new \u2013 the experimental data for OZHDA*, GAZHDA*, DAHDA*, and FAZHDA* use the newer CGL-B merge & shrink heuris-\ntic function (Helmert, Haslum, Hoffmann, & Nissim, 2014), in contrast to the previous conference paper (Jinnai & Fukunaga, 2016a) which used the older LFPA merge&shrink heuristic (Helmert, Haslum, & Hoffmann, 2007). In addition, while the conference papers were limited to single multicore machines (Jinnai & Fukunaga, 2016a) and clusters with up to 48 cores (Jinnai & Fukunaga, 2016b), this paper includes an evaluation of the new HDA* variants on a 128 core cloud environment (Section 6.3.2). Finally, Table 10 in Appendix A shows new experimental results comparing DAHDA* vs AHDA* (Burns et al., 2010) which were not included in the conference paper which introduced DAHDA* (Jinnai & Fukunaga, 2016b)."}, {"heading": "2. Preliminaries and Background", "text": "In this section, we first define the three major classes of overheads that pose a challenge for parallel search (Section 2.1). We then survey parallel best-first search algorithms (Section 2.2) and review the HDA* framework (Section 2.3). We then review the two previous approaches which have been proposed for the HDA* framework, Zobrist hashing (Section 2.4) and abstraction (Section 2.5)."}, {"heading": "2.1 Parallel Overheads", "text": "Although an ideal parallel best-first search algorithm would achieve an n-fold speedup on n threads, several overheads can prevent parallel search from achieving linear speedup.\nCommunication Overhead (CO): 1 Communication overhead refers to the cost of exchanging information between threads. In this paper we define communication overhead as the ratio of nodes transferred to other threads: CO := # nodes sent to other threads# nodes generated . CO is detrimental to performance because of delays due to message transfers (e.g., network communications), as well as access to data structures such as message queues. In general, CO increases with the number of threads. If nodes are assigned randomly to the threads, CO will be proportional to 1\u2212 1#thread .\nSearch Overhead (SO): Parallel search usually expands more nodes than sequential A*. In this paper we define search overhead as SO := # nodes expanded in parallel#nodes expanded in sequential search\u22121. SO can arise due to inefficient load balance (LB), where we define load balance asLB := Maximum number of nodes assigned to a threadAverage number of nodes assigned to a thread . If load balance is poor, a thread which is assigned more nodes than others will become a bottleneck \u2013 other threads spend their time expanding less promising nodes, resulting in search overhead. Search overhead is not only critical to the walltime performance, but also to the space efficiency. Even in distributed memory environment, RAM per core is still an important issue to consider.\nCoordination (Synchronization) Overhead: In parallel search, coordination overhead occurs when a thread has to wait in idle for an operation of other threads. Even when a parallel search itself does not require synchronization, coordination overhead can be incurred due to contention for the memory bus (Burns et al., 2010; Kishimoto et al., 2013).\nThere is a fundamental trade-off between CO and SO. Increasing communication can reduce search overhead at the cost of communication overhead, and vice-versa.\n1. In this paper, CO stands for communication overhead, not coordination overhead."}, {"heading": "2.2 Parallel Best-First Search Algorithms", "text": "The key to achieving a good speedup in parallel best-first search is to minimize communication, search, and coordination overhead. In this section, we survey previous approaches. Figure 1 presents a visual classification of these approaches which summarizes the discussion below.\nParallel A* (PA*) (Irani & Shih, 1986) is a straightforward parallelization of A* which uses a single, shared open list (in this paper, we refer to this algorithm as \u201cPA*\u201d, and use \u201cparallel A*\u201d to refer to the family of parallel algorithms based on A*). Since worker processes always expand the best node from the shared open list, this minimizes search overhead by eliminating the burst effects. However, node reexpansions are possible in PA* because (as with most other parallel A* variants including HDA*) PA* does not guarantee that a state has an optimal g-value when expanded. Phillips, Likhachev, and Koenig have proposed PA*SE, a mechanism for reducing node reexpansions in PA* (2014) which only expands nodes when their g-values are optimal, ensuring that nodes are not reexpanded.\nKumar, Ramesh, and Rao (1988) identified two classes of approaches to open list management in parallel A*. PA* and its variants are instances of a centralized approach which shares a single open\nlist among all processes. However, concurrent access to the shared open list becomes a bottleneck and inherently limits the scalability of this approach unless the cost of expanding each node is extremely expensive, even if lock-free data structures are used (Burns et al., 2010). A decentralized approach addresses this bottleneck by assigning each process to a separate open list. Each process executes a best-first search using its own local open list. While decentralized approaches eliminate coordination overhead incurred by a shared open list, load balancing becomes a problem.\nThere are several approaches to load balancing in decentralized best-first search. The simplest approach is a randomized strategy which sends generated states to a randomly selected neighbor processes (Kumar et al., 1988). The problem with this strategy is that duplicate nodes are not detected unless they are fortuitously sent to the same process, which can result in a tremendous amount of search overhead due to nodes which are redundantly expanded by multiple processors.\nParallel Retracting A* (PRA*) (Evett, Hendler, Mahanti, & Nau, 1995) uses a hash-based work distribution to address simultaneously address both load balancing and duplicate detection. In PRA*, each process owns its local open and closed list. A global hash function maps each state to exactly one process which owns the state. Thus, hash-based work distribution solves the problem of duplicate detection and elimination, because each state has exactly one owner. When generating a state, PRA* distributes it to the corresponding owner synchronously. However, synchronous node sending was shown to degrade performance on domains with fast node expansion, such as grid pathfinding and sliding-tile puzzle (Burns et al., 2010).\nTransposition-Table Driven Work Scheduling (TDS) (Romein, Plaat, Bal, & Schaeffer, 1999) is a distributed memory, parallel IDA* with hash-based work distribution. In contrast to PRA*, TDS sends a state to its owner process asynchronously.\nAn alternate approach for load balancing, which originated in a line work for using multiple processes in external memory search (Korf & Schultze, 2005; Niewiadomski, Amaral, & Holte, 2006; Jabbar & Edelkamp, 2006), is based on structured abstraction. Given a state space graph and a projection function, an abstract state graph is (implicitly) generated by projecting states from the original state space graph into abstract nodes. For example, an abstract space for the sliding tile puzzle domain can be created by projecting all nodes with the blank tile at position b to the same abstract state. While the use of abstractions as the basis for heuristic functions has a long history (Pearl, 1984), the use of abstractions as a mechanism for partitioning search states originated in Structured Duplicate Detection (SDD), an external memory search which stores explored states on disk (Zhou & Hansen, 2004). In SDD, an n-block is defined as the set of all nodes which map to the same abstract node. SDD uses n-blocks to provide a solution to duplicate detection. For any node n which belongs to n-block B, the duplicate detection scope of n is defined as the set of n-blocks which can possibly contain duplicates of n, and duplicate checks can be restricted to the duplication detection scope, thereby avoiding the need to look for a duplicate of n outside this scope. SDD exploits this property for external memory search by expanding nodes within a single n-block B at a time and keeping the duplicate detection scope of the nodes in B in RAM, avoiding costly I/O. Unlike stack-slicing, which requires leveled search space, SDD is applicable to any state-space search problem. Parallel Structured Duplicate Detection (PSDD) is a parallel search algorithm which exploits n-blocks to address both synchronization overhead and communication overhead (Zhou & Hansen, 2007). Each processor is exclusively assigned to an n-block and its neighboring n-blocks (which are the duplication detection scopes). By exclusively assigning nblocks with disjoint duplicate detection scopes to each processor, synchronization during duplicate detection is eliminated. While PSDD used disjoint duplicate detection scopes to parallelize breadth-\nfirst heuristic search (Zhou & Hansen, 2006a), Parallel Best-NBlocks First (PBNF) (Burns et al., 2010) extends PSDD to best-first search on multicore machine by ensuring that n-blocks with the best current f -values are assigned to processors. Since livelock is possible in PBNF on domains with infinite state spaces, Burns et al proposed SafePBNF, a livelock-free version of PBNF (2010). Burns et al (2010) also proposed AHDA*, a variant of HDA* which uses an abstraction-based node distribution function. AHDA* is described below in Section 2.5.\nEfficient abstractions can also be generated by exploiting prior knowledge of the structure of the state-space and/or machines on which search is performed. Stack-slicing projects states to their path costs to achieve efficient communication in depth-first search (Holzmann, 2008), and is useful in domains with levelled graphs, where each state can be reached only by a unique path cost, such as model checking (Holzmann & Bos\u0302nac\u0302ki, 2007) (thus enabling dupicate detection). LOcal HAshing of nodes (LOHA) applies path cost-based partitioning in A* search to reduce the number of internode communication in a hypercube multiprocessor (Mahapatra & Dutt, 1997)."}, {"heading": "2.3 Hash Distributed A* (HDA*)", "text": "Hash Distributed A* (HDA*) (Kishimoto et al., 2013) is a parallel A* algorithm which incorporates the idea of hash-based work distribution from PRA* (Evett et al., 1995) and asynchronous communication from TDS (Romein et al., 1999). In HDA*, each processor has its own open/closed lists. A global hash function assigns a unique owner thread to every search node. Each thread T repeatedly executes the following:\n1. T checks its message queue if any new nodes are in. For all new nodes n in T \u2019s message queue, if it is not in the open list (not a duplicate), put n in the open list.\n2. Expand node n with the highest priority in the open list. For every generated node c, compute hash value H(c), and send c to the thread that owns H(c).\nHDA* has two features which make it attractive as a parallel search algorithm. First, there is little coordination overhead because HDA* communicates asynchronously, and locks for an access to shared open/closed lists are not required because each thread has its own local open/closed list. Second, the work distribution mechanism is simple, requiring only a hash function. However, the effect of the hash function was not evaluated empirically, and the importance of the choice of hash function may not have been fully understood or appreciated \u2013 at least one subsequent work which evaluated HDA* used an implementation of HDA* which failed to achieve uniform distribution of the nodes (see Section 3.2)."}, {"heading": "2.4 Zobrist Hashing (HDA\u2217[Z ]) and Operator-Based Zobrist Hashing (HDA\u2217[Zoperator ])", "text": "Since the work distribution in HDA* is completely determined by a global hash function, the choice of the hash function is crucial to its performance. Kishimoto et al. (2009, 2013) noted that it was desirable to use a hash function which uniformly distributed nodes among processors, and used the Zobrist hash function (1970), described below. The Zobrist hash value of a state s, Z(s), is calculated as follows. For simplicity, assume that s is represented as an array of n propositions, s = (x0, x1, ..., xn). Let R be a table containing preinitialized random bit strings (Algorithm 2).\nZ(s) := R[x0] xor R[x1] xor \u00b7 \u00b7 \u00b7 xor R[xn] (1)\nAlgorithm 1: HDA\u2217[Z ] Input: s = (x0, x1, ..., xn) 1 hash\u2190 0; 2 for each xi \u2208 s do 3 hash\u2190 hash xor R[xi]; 4 Return hash;\nAlgorithm 2: Initialize HDA\u2217[Z ] Input: F : a set of features 1 for each x \u2208 F do 2 R[x]\u2190 random(); 3 Return R\nIn the rest of the paper, we refer to the original version of HDA* by Kishimoto et al. (2009, 2013), which used Zobrist hashing, as ZHDA* or HDA\u2217[Z ].\nZobrist hashing seeks to distribute nodes uniformly among all processes, without any consideration of the neighborhood structure of the search space graph. As a consequence, communication overhead is high. Assume an ideal implementation that assigns nodes uniformly among threads. Every generated node is sent to another thread with probability 1 \u2212 1#threads . Therefore, with 16 threads, > 90% of the nodes are sent to other threads, so communication costs are incurred for the vast majority of node generations.\nOperator-based Zobrist hashing (OZHDA*) (Jinnai & Fukunaga, 2016b) partially addresses this problem by manipulating the random bit strings in the randomized bitstring table R such that for some selected states S, there are some operators A(s) for s \u2208 S such that the successors of s which are generated when a \u2208 A(s) is applied to s are guaranteed to have the same Zobrist hash value as s, forcing them to be assigned the same processor as s. Although Jinnai and Fukunaga showed that OZHDA* reduces communication overhead compared to Zobrist hashing (2016b), it may result in increased search overhead compared to HDA\u2217[Z ](the extent of which is unpredictable).\n2.5 Abstraction (HDA\u2217[P ,Astate ])\nIn order to minimize communication overhead in HDA*, Burns et al. (2010) proposed AHDA*, which uses abstraction based node assignment. The abstraction strategy in AHDA* applies the state space partitioning technique used in PBNF (Burns et al., 2010) and PSDD (Zhou & Hansen, 2007), which projects nodes in the state space to abstract states. After mapping states to abstract states, the AHDA* implementation by Burns et al. (2010) assigns abstract states to processors using a perfect hashing and a modulus operator.\nThus, nodes that are projected to the same abstract state are assigned to the same thread. If the abstraction function is defined so that children of node n are usually in the same abstract state as n, then communication overhead is minimized. The drawback of this method is that it focuses solely on minimizing communication overhead, and there is no mechanism for equalizing load balance, which can lead to high search overhead.\nHDA* with abstraction can be characterized by two parameters to decide its behavior \u2013 a hashing strategy and an abstraction strategy. The AHDA* implementation by Burns et al. (2010) implemented the hashing strategy using a perfect hashing and a modulus operator, and an abstraction strategy following the construction for SDD (Zhou & Hansen, 2006b) (for domain-independent planning), or a hand-crafted abstraction (for the sliding tiles puzzle and grid path-finding domains). Note that an abstraction strategy can itself be seen as a type of hashing strategy, but in this paper, we make the distinction between the method used to project states onto some cluster of states (abstraction) and methods which are used to map states (or abstract states) to processors (hashing).\nJinnai and Fukunaga (2016b) showed that AHDA* with a static Nmax threshold performed poorly for a benchmark set with varying difficulty because a fixed size abstract graph results in very poor load balance, and implemented Dynamic AHDA* (DAHDA*) which dynamically sets the size of the abstract graph according to the number of features (the state space size is exponential in the number of features). We evaluate DAHDA* in detail in Appendix A.\n2.6 Classification of HDA* variants and a Uniform Notation for HDA* variants (HDA\u2217[hash, abstraction])\nAt least 12 variants of HDA* have been proposed and evaluated in the previous literature. Each variant of HDA* can be characterized according to two parameters: a hashing strategy used (e.g., Zobrist hashing or perfect hashing), and an abstraction strategy (which corresponds to the strategy used to cluster states or features before the hashing, e.g., state projection based on SDD).\nTable 1 shows all of the HDA* variants that are discussed in this paper. In order to be able to clearly distinguish among these variants, we use the notation HDA\u2217[hash, abstraction] throughout this paper, where \u201chash\u201d is the hashing strategy of HDA* and \u201cabstraction\u201d is the abstraction strategy. Variants that do not use any abstraction strategy are denoted by HDA\u2217[hash]. In cases where the unified notation is lengthy, we use the abbreviated name in the text (e.g., \u201cFAZHDA*\u201d for HDA\u2217[Z ,Afeature/DTGfluency ]).\nFor example, we denote AHDA* (Burns et al., 2010) using a perfect hashing and a hand-crafted abstraction as HDA\u2217[P ,Astate ], and AHDA* using a perfect hashing and a SDD abstraction as HDA\u2217[P ,Astate/SDD ]. We denote HDA* with Zobrist hashing without any clustering (i.e., the original version of HDA* by Kishimoto et al. 2009, 2013) as HDA\u2217[Z ]. We denote OZHDA* as HDA\u2217[Zoperator ], where Zoperator stands for Zobrist hashing using operator-based initialization."}, {"heading": "3. Analysis of Parallel Overheads in Multicore Best-First Search", "text": "As discussed in Section 2.1, there are three broad classes of parallel overheads in parallel search: search overhead (SO), communications overhead (CO), and coordination (synchronization) overhead. Since state-of-the-art parallel search algorithms such as HDA* and PBNF have successfully eliminated coordination overhead, the remaining overheads are SO and CO. Previous work has focused on evaluating SO quantitatively because SO is fundamental overhead to the algorithm itself whereas CO is due to machine environment which is difficult to evaluate and control. Thus, in this section, we first evaluate the SO of HDA\u2217[Z ] and SafePBNF.\nKishimoto et al. previously analyzed search overhead for HDA\u2217[Z ] (2013). They measuredR<, R=, andR>, the fraction of expanded nodes with f < f\u2217, f = f\u2217, and f > f\u2217 (where f\u2217 is optimal cost), respectively. They also measured Rr, the fraction of nodes which were reexpanded. All admissible search algorithms must expand all nodes with f < f\u2217 in order to guarantee optimality.\nIn addition, some of the nodes with f = f\u2217 nodes are expanded. Thus, SO is the sum of R>, Rr, and some fraction of R=. These metrics enable estimating the SO on instances which are too hard to solve in sequential A*. Burns et al. analyzed the quality of nodes expanded by SafePBNF and HDA\u2217[P ,Astate ] by comparing the number of nodes expanded according to their f values, and showed that HDA\u2217[P ,Astate ] expands nodes with larger f value (lower quality nodes) compared to SafePBNF (2010).\nWhile these previous works measure the amount of search overhead, they do not provide a quantitative explanation for why such overheads occur. In addition, previous work has not directly compared HDA\u2217[Z ] and SafePBNF, as Burns et al. (2010) compared SafePBNF to \u2014HDA\u2217[P ,Astate ]\nand another variation of HDA* which uses a suboptimal hash function, which we refer to as HDA\u2217[P ] in this paper.\nIn this section, we propose a method to analyze SO and explain search overhead in HDA* and SafePBNF. In light of the observation of this analysis, we revisit the comparison of HDA* vs. SafePBNF on sliding-tile puzzle and grid path finding. We then analyze the impact communications overhead has on overall performance."}, {"heading": "3.1 Search Overhead and the Order of Node Expansion on Combinatorial Search", "text": "Consider the global order in which states are expanded by a parallel search algorithm. If a parallel A* algorithm expands states in exactly the same order as A*, then by definition, there is no search overhead. We ran A* and HDA\u2217[Z ] on 100 randomly generated instances of the 15-puzzle on Intel Xeon E5410 2.33 GHz CPU with 16 GB RAM, using a 15-puzzle solver based on the solver code used in the work of Burns et al. (2010). We recorded the order in which states were expanded. We used a random generator by Burns to generate random instances2. The results from runs on 2 representative instances (one \u201ceasy\u201d instance which A* solves after 8966 expansions, and one \u201cdifficult\u201d instance which A* solves after 4265772 expansions), are shown in Figure 2, 3 and 4 (The results on the other difficult/easy problems were similar to these representative instances \u2013 aggregate results are presented in Sections 3.1.3-3.1.4).\nIn Figures 2, 3 and 4, the horizontal axis represents the order in which state s is expanded by parallel search (HDA* or SafePBNF). The vertical axis represents the A* expansion order of state s, which is the order in which sequential A* expands node s. Note that although standard A* would terminate after finding an optimal solution, we modified sequential A* for this set of experiments so that it continues to search even after the optimal solution has been found. This is because parallel search expands nodes that are not expanded by sequential A* (i.e., search overhead), and we want to know for all states expanded by parallel search which are not usually expanded by sequential A*, how much the parallel search has diverged from the behavior of sequential A*.\nThe line y = x corresponds to an ideal, strict A* ordering in which the parallel expansion ordering is identical to the A* expansion order. The cross marks (\u201cGoal\u201d) in the figures represents the (optimal) solution found by A*, and the vertical line from the goal shows the total number of node expansions in A*. Thus, all nodes above this line results in SO. Note that unlike sequential A*, parallel A* can not terminate immediately after finding a solution, even if the heuristic is consistent, because when parallel A* finds an optimal solution it is possible that some nodes with f < f\u2217 have not been expanded (because they are assigned to a processor which is different from the processor where the solution was found).\nAlthough the traditional definition of A* (Hart et al., 1968) specifies that nodes are expanded in order of nondecreasing f -value (i.e., best-first ordering), this is not sufficient to define a canonical node expansion ordering for sequential A* because many nodes can have the same f -value. A tiebreaking policy can be used to impose a unique, canonical expansion ordering for sequential A*. Our sequential A* uses a LIFO tie-breaking policy, which has been shown to result in good performance on the 15-puzzle (Burns, Hatem, Leighton, & Ruml, 2012), as well as domain-independent planning (Asai & Fukunaga, 2016). In addition, all of our HDA* variants, as well as SafePBNF uses LIFO tie-breaking for each local open list. Thus, by \u201cstrict A*\u201d order, we mean \u201cthe order in\n2. The instance generator is at https://github.com/eaburns/pbnf/tree/master/tile-gen\nwhich A* with LIFO tie-breaking expands nodes\u201d, and in Figures 2, 3 and 4 compare the expansion ordering of this ordering vs. HDA*/SafePBNF with local LIFO tiebreaking.\nTo verify that the results are not dependent on the particular tie-breaking policy, Figures 3f and 4f show results where both sequential A* and the parallel algorithms use FIFO tie-breaking. These show that the results are not qualitatively affected by the choice of tie-breaking policy.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n0\n2 00\n0\n4 00\n0\n6 00\n0\n8 00\n0\n1 00\n00\n1 20\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(a) HDA\u2217[Z ] on an easy instance with 8 threads. Both band and burst effects are more significant than with 4 threads.\n0\n5000\n10000\n15000\n20000\n25000\n0\n2 00\n0\n4 00\n0\n6 00\n0\n8 00\n0\n1 00\n00\n1 20\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(b) HDA\u2217[P ,Astate ] on an easy instance with 8 threads. HDA\u2217[P ,Astate ] has a significantly bigger band compared to HDA\u2217[Z ].\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n0\n5 00\n0\n1 00\n00\n1 50\n00\n2 00\n00\n2 50\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(c) SafePBNF on an easy instance with 8 threads. As threads in SafePBNF requires exclusive access to nblocks, the expansion order differs significantly from A* (and HDA* variants).\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n0\n1 00\n0\n2 00\n0\n3 00\n0\n4 00\n0\n5 00\n0\n6 00\n0\n7 00\n0\n8 00\n0\n9 00\n0\n1 00\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(d) HDA\u2217[Z ] on an easy instance with 8 threads with artificially slowed expansion rate. The band effect remains clear, indicating that the band effect is not an accidental overhead cause by communications or lock contention.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n0\n5 00\n0\n1 00\n00\n1 50\n00\n2 00\n00\n2 50\n00\n3 00\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(e) HDA\u2217[P ] on an easy instance with 8 threads. HDA\u2217[P ] has a significantly bigger band compared to other methods and many threads are expanding unpromising (high f value) nodes. As a result, HDA\u2217[P ] expands > 25000 nodes to solve the instance which A* solves with 8966 expansions.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n0\n2 00\n0\n4 00\n0\n6 00\n0\n8 00\n0\n1 00\n00\n1 20\n00\n1 40\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(f) HDA\u2217[Z ] using FIFO tiebreaking on an easy instance with 8 threads (vs. A* using FIFO tiebreaking).\n0 5000\n10000 15000 20000 25000 30000\n0 2 00 0 4 00 0 6 00 0 8 00 0 1 00 00 1 20 00 1 40 00\nA *\nex\npa\nns io\nn or\nde r\nparallel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5\nThread 6\nThread 7\nThread 8\nStrict Order Goal\n0 5000\n10000 15000 20000 25000 30000\n0\n2\n00 0\n4\n00 0\n6\n00 0\n8\n00 0\n1\n00\n00\n1\n20\n00\n1\n40\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5 Thread 6 Thread 7 Thread 8 Strict Order\nGoal\n0 5000\n10000 15000 20000 25000 30000\n0 2 00 0 4 00 0 6 00 0 8 00 0 1 00 00 1 20 00 1 40 00\nA *\nex pa\nns io\nn or\nde r\npa allel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5 Thread 6 Thread 7 Thread 8 Strict Order Goal\nFigure 3: Comparison of parallel vs sequential node expansion order on an easy instance of the 15-Puzzle with 8 threads.\n0 500000 1e+06\n1.5e+06 2e+06\n2.5e+06 3e+06\n3.5e+06 4e+06\n4.5e+06 5e+06\n0\n5 00\n00 0\n1 e+\n06\n1 .5\ne+ 06\n2 e+\n06\n2 .5\ne+ 06\n3 e+\n06\n3 .5\ne+ 06\n4 e+\n06\n4 .5\ne+ 06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(a) HDA\u2217[Z ] on a difficult instance with 8 threads. As the instance is difficult enough, the relative significance of burst effect becomes negligible.\n0\n1e+06\n2e+06\n3e+06\n4e+06\n5e+06\n6e+06\n7e+06\n8e+06\n0\n1 e+\n06\n2 e+\n06\n3 e+\n06\n4 e+\n06\n5 e+\n06\n6 e+\n06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(b) HDA\u2217[P ,Astate ] on a difficult instance with 8 threads. As with the easy instance, HDA\u2217[P ,Astate ] has a bigger band than HDA\u2217[Z ] on a difficult instance.\n0\n2e+06\n4e+06\n6e+06\n8e+06\n1e+07\n1.2e+07\n1.4e+07\n1.6e+07\n0\n5 00\n00 0\n1 e+\n06\n1 .5\ne+ 06\n2 e+\n06\n2 .5\ne+ 06\n3 e+\n06\n3 .5\ne+ 06\n4 e+\n06\n4 .5\ne+ 06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(c) SafePBNF on a difficult instance with 8 threads. Because SafePBNF requires each thread to explore each nblock exclusively, the order of node expansion differs significantly from A*. SafePBNF retains exploring promising nodes by switching nblocks at the cost of communication and coordination overhead.\n0 500000 1e+06\n1.5e+06 2e+06\n2.5e+06 3e+06\n3.5e+06 4e+06\n4.5e+06 5e+06\n0\n5 00\n00 0\n1 e+\n06\n1 .5\ne+ 06\n2 e+\n06\n2 .5\ne+ 06\n3 e+\n06\n3 .5\ne+ 06\n4 e+\n06\n4 .5\ne+ 06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(d) HDA\u2217[Z ] on a difficult instance with 8 threads with artificially slowed expansion rate. We did not observe a significant difference from HDA\u2217[Z ]without slow expansion.\n0\n2e+06\n4e+06\n6e+06\n8e+06\n1e+07\n1.2e+07\n1.4e+07\n1.6e+07\n1.8e+07\n0\n1 e+\n06\n2 e+\n06\n3 e+\n06\n4 e+\n06\n5 e+\n06\n6 e+\n06\n7 e+\n06\n8 e+\n06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(e) HDA\u2217[P ] on a difficult instance with 8 threads. HDA\u2217[P ] has the biggest band effect, significantly diverged from A*. HDA\u2217[P ] expands > 7, 000, 000 nodes to solve the instance which A* solves with 4, 000, 000 expansions.\n0\n1e+06\n2e+06\n3e+06\n4e+06\n5e+06\n6e+06\n0\n5 00\n00 0\n1 e+\n06\n1 .5\ne+ 06\n2 e+\n06\n2 .5\ne+ 06\n3 e+\n06\n3 .5\ne+ 06\n4 e+\n06\n4 .5\ne+ 06\n5 e+\n06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(f) HDA\u2217[Z ] using FIFO tiebreaking on a difficult instance with 8 threads (vs. A* using FIFO tiebreaking).\n0 5000\n10000 15000 20000 25000 30000\n0 2 00 0 4 00 0 6 00 0 8 00 0 1 00 00 1 20 00 1 40 00\nA *\nex\npa\nns io\nn or\nde r\nparallel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5\nThread 6\nThread 7\nThread 8\nStrict Order\nGoal\n0 5000\n10000 15000 20000 25000 30000\n0\n2\n00 0\n4\n00 0\n6\n00 0\n8\n00 0\n1\n00\n00\n1\n20\n00\n1\n40\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5 Thread 6 Thread 7 Thread 8 Strict Order\nGoal\n0 5000\n10000 15000 20000 25000 30000\n0 2 00 0 4 00 0 6 00 0 8 00 0 1 00 00 1 20 00 1 40 00\nA *\nex pa\nns io\nn or\nde r\npa allel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5 Thread 6 Thread 7 Thread 8 Strict Order Goal\nFigure 4: Comparison of node expansion order on a difficult instance of the 15-Puzzle with 8 threads. The average node expansion order divergence of scores are HDA\u2217[Z ]: d\u0304 = 10, 330.6, HDA\u2217[Z ] (slowed): d\u0304 = 8, 812.1, HDA\u2217[P ,Astate ]: d\u0304 = 245, 818, HDA\u2217[P ]: d\u0304 = 4, 469, 340, SafePBNF: d\u0304 = 140, 629.4.\nBy analyzing the results, we observed three causes of search overhead on HDA*, (1) Band Effect, the divergence from the A* order due to load imbalance, (2) Burst Effect, an initialization overhead, and (3) node reexpansions. Below, we explain and discuss each of these overheads."}, {"heading": "3.1.1 BAND EFFECT", "text": "The order in which states are expanded by HDA\u2217[Z ] is fairly consistent with sequential A*. However, there is some divergence from the strict A* ordering, within a \u201cband\u201d that is symmetrical around the strict A* ordering line. For example, in Figure 2a, we have highlighted a band showing that the (approximately) 5000\u2019th state expanded by HDA* corresponds a strict A* order between 4500-5500 (i.e., a band width of approximately 1000 at this point in the search). The width of the band tends to increase as the number of threads increases (see the bands in Figure 2a, 2b, 3a). Although the width of the band tends to increase as the search progresses, the rate of growth is relatively small. Also, the harder the instance (i.e., the larger the number of nodes expanded by A*), the narrower the band tends to be (Figure 4a).\nA simple explanation for this band effect is load imbalance. Suppose we use 2 threads, and assume that threads t1 and t2 share p and 1\u2212p of the nodes with f value = fi for each fi. Consider the n\u2019th node expanded by t1. This should roughly correspond to the np \u2019th node expanded by sequential A*; at the same time, t2 should expand the node which roughly corresponds to the n1\u2212p \u2019th node expanded by sequential A*. In this case, the band size is |np \u2212 n 1\u2212p |. Therefore, if p = 0.5 (perfect load balance), the band is small, and as p diverges from 0.5, the band size becomes larger. One possible, alternative interpretation of the band effect is that it is somehow related to or caused by other factors such as communications overhead or lock contention. To test this, we ran HDA\u2217[Z ] on 8 cores where the state expansion code was intentionally slowed down by adding a meaningless but time-consuming computation to each state expansion.3 If the band effect was caused by communications or lock contention related issues, it should not manifest itself if the node expansion rate is so slow that the relative cost of communications and synchronization is very small. However, as shown in Figure 3d and 4d, the band effect remains clearly visible even when the node expansion rate is very slow, indicating that the band effect is not an accidental overhead caused by communications or lock contention (similar results were obtained for other instances).\nObservation 1 The band effect on HDA\u2217[Z ] represents load imbalance between threads. The width of the band determines the extent to which superlinear speedup or search overhead (compared to sequential A*) can occur. Furthermore, the band effect is independent of node evaluation rate.\nThe expansion order of SafePBNF is shown in Figure 3c and 4c. Because SafePBNF requires each thread to explore each nblock (and duplicated detection scope) exclusively, the order of node expansion is significantly different from A*. However, SafePBNF tries to explore promising nodes by switching among nblocks to focus on nblocks which contain the most promising nodes. This requires communication and coordination overhead, which increases the walltime by about <10% of the time on the 15-puzzle (Burns et al., 2010).\n3. At the beginning of the search on each thread, we initialize a thread-local, global integer i to 7. On each thread, after each node expansion, we perform the following computation 100,000 times: j = 11i mod 9999943, and then set i \u2190 j. This is a heavy computation with a small memory footprint and is intended to occupy the thread without causing additional memory accesses."}, {"heading": "3.1.2 BURST EFFECT", "text": "At the beginning of the search, it is possible for the node expansion order of HDA* to deviate significantly from strict A* order due to a temporary \u201cburst effect\u201d. Since there is some variation in the amount of time it takes to initialize each individual thread and populate all of the thread open lists with \u201cgood\u201d nodes, it is possible that some threads may start out expanding nodes in poor regions of the search space because good nodes have not yet been sent to their open lists from threads that have not yet completed their initialization. For example, suppose that n1 is a child of the root node n0, and n1 has a significantly worse f -value than other descendants of n0. Sequential A* will not expand n1 until all nodes with lower f -values have been expanded. However, at the beginning of search, n1 may be assigned to a thread t1 whose queue q1 is empty, in which case t1 will immediately expand n1. The children of n1 may also have f -values which are significantly worse than other descendants of n0, but if those children of n1 are in turn assigned to threads with queues that are (near) empty or otherwise populated by other \u201cbad\u201d nodes with poor f -values, then those children will get expanded, and so on. Thus, at the beginning of the search, many such bad nodes will be expanded because all queues are initially empty, bad nodes will continue to be expanded until the queues are filled with \u201cgood\u201d nodes. As the search progresses, all queues will be filled with good nodes, and the search order will more closely approximate that of sequential A*.\nFurthermore, these burst-overhead nodes tend to be reached through suboptimal paths (because states necessary for better paths are unavailable during the burst phase), and therefore tend to be revisited later via shorter paths, contributing to revisited node overhead.\nThe burst phenomenon is clearly illustrated in Figure 2b and 3a, which shows the behavior of HDA\u2217[Z ] with 8 threads on a small 15-puzzle problem (solved by A* in 8966 expansions). The large vertically oriented cluster at the left of the figure shows that states with a strict A* order of over 30,000 are being expanded within the first 2,000 expansions by HDA*. The A* implementation we used expands over 85,248 nodes per second (the node expansion includes overhead for storing node information in the local data structure, thus slower than base implementation by Burns et al.), this burst phenomenon is occurring within the first 0.023 seconds of search.\nFigure 4a shows that on a harder problem instance which requires > 4,000,000 state expansions by A*, the overall effect of this initial burst overhead is negligible.\nFigure 3d shows that when the node expansion rate is artificially slowed down, the burst effect is not noticeable even if the number of states expansions necessary to solve the problem with A* is small (< 10,000). This is consistent with our explanation above that the burst effect is caused by brief, staggered initialization of the threads \u2013 when state expansions are slow, the staggered start becomes irrelevant.\nFrom the above, we can conclude that the burst effect is only significant when the problem can be solved very quickly (< 0.88 seconds) by A* and the node expansion rate is fast enough that the staggered initialization can cause a measurable effect.\nThe practical significance of the burst effect depends on the characteristics of the application domain. In puzzle-solving domains, the time scales are usually such that the burst effect is inconsequential. However, in domains such as real-time path planning, the total time available for planning can be just as a fraction of a second, so the burst effect can have a significant effect.\nObservation 2 The burst effect in HDA\u2217[Z ] can dominate search behavior on easy problems, resulting in large search overhead. However, the burst effect is insignificant on harder problems, as well as when node expansion rate is slow.\nThe burst effect is less pronounced in SafePBNF compared to HDA\u2217[Z ], because a thread in SafePBNF prohibits other threads from exploring its duplicate detection scope. The nodes shown in Figure 3c are actually band effect, which means that it is persistent through the search (Figure 4c)."}, {"heading": "3.1.3 NODE REEXPANSIONS", "text": "With a consistent heuristic, A* never reexpands a node once it is saved in the closed list, because the first time a node is expanded, we are guaranteed to have reached through a lowest-cost path to that node. However, in parallel best-first search, nodes may need to be reexpanded even if they are in the closed list. For example, in HDA*, each processor selects the best (lowest f -cost) node in its local open list, but the selected node may not have the current globally lowest f -value. As a result, although HDA* tends to find shortest paths to a node first, the paths may not be lowest-cost paths, and some node n which is expanded by some thread in HDA* may have been reached through a suboptimal path, and must later be reexpanded after it is reached through a lower-cost path.\nThis is not a significant overhead for unit-cost domains because shorter paths always have smaller cost. In fact, we observed that HDA\u2217[Z ], HDA\u2217[P ,Astate ] and SafePBNF had low reexpansion rates for on the 15-puzzle. For HDA\u2217[Z ] with 8 threads, the average reexpansion rate Rr was 2.61\u00d7 10\u22125 for 100 instances.\nNode reexpansions are more problematic in non-unit cost domains, because a shorter path does not always mean a smaller cost. (Kobayashi et al., 2011) analyzed node reexpansion on multiple sequence alignment which HDA\u2217[Z ] suffers from high node duplication rate. We discuss node reexpansions by HDA* on the multiple sequence alignment problem in Section 4.1.3."}, {"heading": "3.1.4 THE IMPACT OF WORK DISTRIBUTION METHOD ON THE ORDER OF NODE EXPANSION", "text": "In addition to HDA\u2217[Z ], we investigated the order of node expansion on HDA\u2217[P ,Astate ], HDA\u2217[P ], and SafePBNF. The abstraction used for HDA\u2217[P ,Astate ] ignores the positions of all tiles except tiles 1,2, and 3 (we tried (1) ignoring all tiles except tiles 1,2, and 3, (2) ignoring all tiles except tiles 1,2,3, and 4, (3) mapping cells to rows, and (5) mapping cells to the blocks , and chose (1) because it performed the best). HDA\u2217[P ] is an instance of HDA* which is called \u201cHDA*\u201d in the work of Burns et al. (2010). Unlike the original HDA* by Kishimoto et al. (2009), which uses Zobrist hashing, HDA\u2217[P ] uses a perfect hashing scheme which maps permutations (tile positions) to lexicographic indices (thread IDs) by Korf and Schultze (2005). A perfect hashing scheme computes a unique mapping from permutations (abstract state encoding) to lexicographic indices (thread ID)4. While this encoding is effective for its original purpose of efficient representation of states for externalmemory search, it was not designed for the purpose of work distribution. For SafePBNF, we used the configuration used in (Burns et al., 2010).\nFigures 3 and 4 compare the expansion orders of HDA\u2217[Z ], HDA\u2217[P ,Astate ], HDA\u2217[P ], and SafePBNF. Although some trends are obvious by visual inspection, e.g., the band effect is larger for HDA\u2217[P ,Astate ] than on HDA\u2217[Z ], a quantitative comparison is useful to gain more insight.\n4. The permutation encoding used by HDA*[P] is defined as: H(s) = c1k! + c2(k \u2212 1)! + ... + ck1! where the position of tile p(i) is the ci-th smallest number in the set {1, 2, 3, ..., 16} \\ {c1, c2, ...ci\u22121}. State s is sent to a process with process id H(s) mod n, where n is the number of processes. Therefore, if n = 8 then H(s)modn = {ck\u221223!+ck\u221212!+ck1!}, thus it only depends on the relative positions of tiles 12, 13, and 14. In addition, processes with odd/even id only send nodes to processes with odd/even id unless the position of 14 changes.\nThus, we calculated the average divergence of each algorithm, where divergence of a parallel search algorithm B on a problem instance I is defined as follows: Let NA\u2217(s) be the order in which state s is expanded by A*, and let NB(s) be order in which s is expanded by B, and let V (A\u2217, B) be the set of all states expanded by both A* and P . In case s is reexpanded by an algorithm, we use the first expansion order. Then the divergence of B from A* on instance I is d(I) = \u2211 s\u2208V (A\u2217,B) |NA\u2217(s) \u2212NB(s)| / |V (A\u2217, B)|. We computed the average divergence d\u0304 for 50 most difficult instances in the instance set. In addition to the divergence d, we calculated the average number of premature expansions p, which is the number of nodes expanded before all nodes with lower f value than that node are expanded. Unlike the divergence, the number of premature expansions is not significantly influenced by the expansion order within the same f value.\nThe average divergence and premature expansions for these difficult instances are shown in Table 2. These results indicate that the order of node expansion of HDA\u2217[Z ] is the most similar to that of A*. Therefore, HDA\u2217[Z ] is expected to have the least SO. The abstraction-based methods, HDA\u2217[P ,Astate ] and SafePBNF, have significantly higher divergence than HDA\u2217[Z ], which is not surprising, since by design, these methods do not seek to simulate A* expansion order. Finally, HDA\u2217[P ] has a huge divergence, and is expected to have very high SO \u2013 it is somewhat surprising that a work distribution function can have divergence (and search overhead) which is so much higher than methods that focus entirely on reducing communications overhead such as HDA\u2217[P ,Astate ]. We evaluate the SO and speedup of each method below in Section 3.2.\n3.2 Revisiting HDA* (HDA\u2217[Z ], HDA\u2217[P ,Astate ], HDA\u2217[Z ,Astate ], HDA\u2217[P ]) vs. SafePBNF for Admissible Search\nPrevious work compared HDA\u2217[P ], HDA\u2217[P ,Astate ], and SafePBNF on the 15-puzzle and grid pathfinding problems (Burns et al., 2010). They also compared SafePBNF with HDA\u2217[P ,Astate ] on domain-independent planning. The overall conclusion of this previous study was that among the algorithms evaluated, SafePBNF performed best for optimal search. We now revisit this evaluation, in light of the results in the previous section, as well as recent improvements to implementation techniques. There are three issues to note regarding the experimental settings used by Burns et al.:\nFirstly, the previous comparison did not include HDA\u2217[Z ], the original HDA* which uses Zobrist hashing (Kishimoto et al., 2009, 2013). Burns et al. evaluated two variants of HDA*: HDA\u2217[P ] (which was called \u201cHDA*\u201d in their paper) and HDA\u2217[P ,Astate ] (called \u201cAHDA*\u201d in their paper). As shown above, the node expansion order of HDA\u2217[Z ] has a much smaller divergence from A* compared to SafePBNF and HDA\u2217[P ,Astate ]. While HDA\u2217[Z ] seeks to minimizes search overhead and both HDA\u2217[P ,Astate ] as well as SafePBNF seeks to reduce communications overhead,\nHDA\u2217[P ] minimizes neither communications nor search overheads (as shown above, it has much higher expansion order divergence than all other methods), so HDA\u2217[P ] is not a good representative of the HDA* framework. Therefore, a direct comparison of SafePBNF and HDA\u2217[P ,Astate ] (which minimize communications overhead) to HDA\u2217[Z ] (which minimizes search overhead) is necessary in order to understand how these opposing objectives affect performance.\nSecondly, the 15-puzzle and grid search instances used in the previous study only required a small amount of search, so the behavior of these algorithms on difficult problems has not been compared. In the previous study, the grid domains consisted of 5000x5000 grids, and the 15-puzzle instances were all solvable within 3 million expansions by A*. Since grid pathfinding solvers can generate 106 nodes per second, and 15-puzzle solvers can generate 0.5 \u00d7 106 nodes per second, these instances are solvable in under a second by a 8-core parallel search algorithm. As shown in section 3.1, when the search only takes a fraction of a second, HDA* incurs significant search overhead due to the burst effect, but the burst effect is a startup overhead whose impact is negligible on problem instances that require more search.\nThirdly, in the previous study, for all algorithms, a binary heap implementation for the open list priority queue was used, which incurs O(logN) costs for insertion. This introduces a bias for PBNF over all of the HDA* variants. PBNF uses a separate binary heap for each n-block \u2013 splitting the open list into many binary heaps greatly decreases the N in the O(logN) cost node insertions compared to algorithms such as HDA* which use a single open list per thread. However, it has been shown that a bucket implementation (O(1) for all operations) results in significantly faster performance on state-of-the-art A* implementations (Burns et al., 2012).\nTherefore, we revisit the comparison of HDA* and SafePBNF by (1) using Zobrist hashing for HDA* (i.e., HDA\u2217[Z ]) in order to minimize search overhead (2) using both easy instances (solvable in < 1 second) and hard instances (requiring up to 1000 seconds to solve with sequential A*) of the sliding tiles and grid path-finding domains in order to isolate the startup costs associated with the burst effect, and (3) using both bucket and heap implementations of the open list in order to isolate the effect of data structure efficiency (as opposed to search efficiency).\nFor the 15-puzzle, we used the standard set of 100 instances by Korf (1985), and used the Manhattan Distance heuristic. We used the same configuration used in Section 3.1.4 for all algorithms (except without the instrumentation to storing the expansion order information for each state). For the 24-puzzle, we used 30 instances randomly generated which could be solved within 1000 seconds by sequential A*, and used the pattern database heuristic (Korf & Felner, 2002). The abstraction used by HDA\u2217[P ,Astate ], HDA\u2217[Z ,Astate ], and SafePBNF ignores the numbers on all of the tiles except tiles 1,2,3,4, and 5 (we tried (1) ignoring all tiles except tiles 1-5, (2) ignoring all tiles except tiles 1-6, (3) ignoring all tiles except tiles 1-4, (4) mapping cells to rows, and (5) mapping cells to the blocks, and chose (1), the best performer). For (4-way unit-cost) grid path finding, we used 60 instances based obtained by randomly generating 5000x5000 grids where 0.45 of the cells are obstacles. We used Manhattan distance as a heuristic. The abstraction used for HDA\u2217[P ,Astate ] and HDA\u2217[Z ,Astate ] maps 100x100 nodes to an abstract node, which performed the best among 5x5, 10x10, 50x50, 100x100, and 500x500 (Section 3.3). For SafePBNF we used the same configuration used in previous work (Burns et al., 2010). The queue of free nblocks is implemented using binary tree as there were no significant difference in performance using vector implementation.\nFigure 5 compares the number of instances solved as a function of wall-clock time by HDA\u2217[Z ], HDA\u2217[P ,Astate ], HDA\u2217[P ], and SafePBNF. The results show that on the 15-puzzle and 24-puzzle, grid pathfinding, PBNF initially outperforms HDA\u2217[Z ], but as more time is consumed, HDA\u2217[Z ]\nsolves more instances than PBNF, i.e., PBNF outperforms HDA\u2217[Z ] on easier problems due to the burst effect (Section 3.1.2), while HDA\u2217[Z ] outperforms SafePBNF on more difficult instances because after the initial burst effect subsides, HDA\u2217[Z ] diverges less from A* node expansion order and therefore incurs less search overhead.\nObservation 3 HDA\u2217[Z ] significantly outperforms SafePBNF on 15-puzzle and 24-puzzle instances that require a significant amount of search. On instances that can be solved quickly, SafePBNF outperforms HDA\u2217[Z ] due to the burst effect.\nComparing the results for the 15-puzzle for the bucket open list implementation (Figure 5a) and the heap open list implementation (Figure 5b), we observe that all of the HDA* variants benefit from using a bucket open list implementation. Not surprisingly, for the more difficult problems, the benefit of the more efficient data structure (O(1) vs. O(logN) insertion for N states) becomes more significant. PBNF does not benefit as much from the bucket open list because in PBNF, there is a separate queue associated with each n-block, so the difference between bucket and heap implementations is O(1) vs O(logN/B), where B is the number of n-blocks.\nFigure 6 compares the number of solved instances within the number of node expanded. Due to the burst effect, with small number of expansions, HDA\u2217[Z ] solves fewer instances compared to SafePBNF, especially in grid domain.\n3.2.1 ON THE EFFECT OF HASHING STRATEGY IN AHDA* (HDA\u2217[Z ,Astate ] VS. HDA\u2217[P ,Astate ])\nIn addition to the original implementation of AHDA* (Burns et al., 2010), which distributes abstract states using a perfect hashing (HDA\u2217[P ,Astate ]), we implemented HDA\u2217[Z ,Astate ] which uses Zobrist hashing to distribute. Interestingly, Figure 6 shows that both HDA\u2217[Z ,Astate ] and HDA\u2217[P ,Astate ] achieved lower search overhead than HDA\u2217[P ] in 15-puzzle. A possible explanation is that the abstraction is hand-crafted so that the abstract nodes are sized equally and distributed evenly in the search space. On the other hand, as an abstract state is already a large set of nodes, distributing abstract states using Zobrist hashing (HDA\u2217[Z ,Astate ]) does not yield significantly better search overhead compared to HDA\u2217[P ,Astate ]."}, {"heading": "3.3 The Effect of Communication Overhead on Speedup", "text": "Although HDA\u2217[Z ] is competitive with the abstraction-based methods (HDA\u2217[P ,Astate ] and SafePBNF) on the sliding tile puzzle domains, Figure 5d shows that HDA\u2217[P ,Astate ] and SafePBNF significantly outperformed HDA\u2217[Z ] in the grid path-finding domain. Interestingly, Figure 6d shows that HDA\u2217[P ,Astate ] and HDA\u2217[Z ] solve roughly the same number problems, given the same number of node expansions. This indicates that the performance difference between HDA\u2217[P ,Astate ] and HDA\u2217[Z ] on the grid domain is not due to search overhead, but rather due to the fact that HDA\u2217[P ,Astate ] is able to expand nodes faster than HDA\u2217[Z ]. In previous work, Burns et al showed that HDA\u2217[P ] suffers from high communications overhead on the grid domain (2010).5\nAlthough HDA* uses asynchronous communication, sending/receiving message require access to data structure such as message queues. Communication costs is crucial in grid path finding because the node expansion rate is extremely high in grid path-finding. Fast node expansion means that the relative time to send a node is higher. Our grid solver expands 955,789 node/second, much faster than our 15-puzzle (bucket) solver (565,721 node/second). Thus, the relative cost of communication in grid domain is twice as high as that of 15-puzzle.\nTo understand the impact of communications overhead, we evaluated the speedup, communications overhead (CO), and search overhead (SO) of HDA\u2217[P ,Astate ] with different abstraction sizes. The abstraction used for HDA\u2217[P ,Astate ] maps k \u00d7 k blocks in the grid to a single abstract state. Note that in this domain, an abstraction size of 1 corresponds to HDA\u2217[P ]. Table 3 shows the results. As the size of the k \u00d7 k block increases, communications is reduced, and as a result, 100x100 HDA\u2217[P ,Astate ] is faster than HDA\u2217[Z ] and HDA\u2217[P ] although it has the same amount of SO. However, there is a point of diminishing returns due to load imbalance \u2013 in the extreme case when the entire N \u00d7N grid is mapped to a single abstract state, there would be no communications but only 1 processor would have work. Thus, a 500x500 abstraction results in worse performance than a 100x100 abstraction.\nTable 3: Comparison of speedup, communication overhead, and search overhead of HDA\u2217[P ,Astate ] on grid path finding using different abstraction size. CO: communication over-\nhead (= # nodes sent to other threads# nodes generated ), SO: search overhead (= # nodes expanded in parallel #nodes expanded in sequential search \u2212 1).\nabstraction size speedup CO SO HDA\u2217[Z ] 2.61 0.87 0.05 1x1 (= HDA\u2217[P ]) 2.57 0.87 0.05 5x5 3.50 0.19 0.05 10x10 3.82 0.10 0.06 50x50 4.16 0.02 0.06 100x100 4.22 0.01 0.05 500x500 3.24 0.01 0.42\n5. Burns et al. evaluated HDA* (HDA\u2217[P ]) on the grid problem using a perfect hash function processor(s) = (x\u00b7ymax+ y) mod p (p is the number of processes) of the state location for work distribution. This hash function results in different behavior according to the number of processes. If (ymax mod p) = 0, then all cells in each row have the same hash value, but all pairs of adjacent rows are guaranteed to have different hash values. If (ymax mod p) 6= 0, all pairs adjacent cells are guaranteed to have different hash values. Both conditions result in high communication overhead, thus HDA\u2217[P ,Astate ] (100x100) significantly outperformed both condition.\nNote that while this experiment was run on a a single multicore machine using pthreads and low-level instructions (try lock) for moving states among processors, communications overhead becomes an even more serious issue using interprocess communication (e.g. MPI) on distributed environment because the communication cost for each message is higher on such environments.\nObservation 4 SafePBNF and HDA\u2217[P ,Astate ] outperform HDA\u2217[Z ] on the grid pathfinding problem, even though SafePBNF and HDA\u2217[P ,Astate ] require more node expansions than HDA\u2217[Z ]. Communications overhead accounts for the poor performance of HDA\u2217[Z ] on grid pathfinding.\n3.4 Summary of the Parallel Overheads for HDA\u2217[Z ] and HDA\u2217[P ,Astate ]\nTable 4 summarizes the comparison of the Zobrist hashing based HDA\u2217[Z ] and structured abstraction based HDA\u2217[P ,Astate ] work distribution strategies on the sliding-tile puzzle and grid pathfinding domains. As we showed in Section 3.1.4 and 3.2, HDA\u2217[Z ] outperforms HDA\u2217[P ,Astate ] on sliding-tile puzzle domain because HDA\u2217[P ,Astate ] suffers from high SO. On the other hand, HDA\u2217[P ,Astate ] outperforms HDA\u2217[Z ] on grid pathfinding because HDA\u2217[Z ] has high CO (Section 3.3). In summary, both HDA\u2217[Z ] and HDA\u2217[P ,Astate ] have clear weakness \u2013 HDA\u2217[Z ] has no mechanism which explicitly seeks to reduce the amount of communication, whereas HDA\u2217[P ,Astate ] has no mechanism which explicitly minimizes load balancing."}, {"heading": "4. Abstract Zobrist Hashing(AZH)", "text": "As we discussed in Section 3, both search and communication overheads have a significant impact on the performance of HDA*, and methods that only address one of these overheads are insufficient. HDA\u2217[Z ], which uses Zobrist hashing, assigns nodes uniformly to processors, achieving near-perfect load balance, but at the cost of incurring communications costs on almost all state generations. On the other hand, abstraction-based methods such as PBNF and HDA\u2217[P ,Astate ] significantly reduce communications overhead by trying to keep generated states at the same processor as where they were generated, but this results in significant search overhead because all of the productive search may be performed at 1 node, while all other nodes are searching unproduc-\ntive nodes which would not be expanded by A*. Thus, we need a more balanced approach which simultaneously addresses both search and communication overheads.\nAbstract Zobrist hashing (AZH) is a hybrid hashing strategy which augments the Zobrist hashing framework with the idea of projection from abstraction, incorporating the strengths of both methods. The AZH value of a state, AZ(s) is:\nAZ(s) := R[A(x0)] xor R[A(x1)] xor \u00b7 \u00b7 \u00b7 xor R[A(xn)] (2)\nwhere A is a feature projection function, a many-to-one mapping from each raw feature to an abstract feature, and R is a pre-computed table for each abstract feature.\nThus, AZH is a 2-level, hierarchical hash, where raw features are first projected to abstract features, and Zobrist hashing is applied to the abstract features. In other words, we project state s to an abstract state s\u2032 = (A(x0), A(x1), ..., A(xn)), and AZ(s) = Z(s\u2032). Figure 7 illustrates the computation of the AZH value for an 8-puzzle state.\nAZH seeks to combine the advantages of both abstraction and Zobrist hashing. Communication overhead is minimized by building abstract features that share the same hash value (abstract features are analogous to how abstraction projects state to abstract states), and load balance is achieved by applying Zobrist hashing to the abstract features of each state.\nCompared to Zobrist hashing, AZH incurs less CO due to abstract feature-based hashing. While Zobrist hashing assigns a hash value for each node independently, AZH assigns the same hash value to all nodes which share the same abstract features for all features, reducing the number of node transfers. Also, in contrast to abstraction-based node assignment, which minimizes communications but does not optimize load balance and search overhead, AZH seeks good load balance, because the node assignment considers all features in the state, rather than just a subset.\nAlgorithm 3: Initialize HDA\u2217[Z ,Afeature ] Input: F : a set of features, A: a mapping from features to abstract features (abstraction strategy) 1 for each a \u2208 {A(x)|x \u2208 F} do 2 R\u2032[a]\u2190 random(); 3 for each x \u2208 F do 4 R[x]\u2190 R\u2032[A(x)]; 5 Return R\nAZH is simple to implement, requiring only an additional projection per feature compared to Zobrist hashing, and we can pre-compute this projection at initialization (Algorithm 3). Thus, there is no additional runtime overhead per node during the search. In fact, except for initialization, the same code to Zobrist hashing can be used (Algorithm 1). The projection function A(x) can be generated either hand-crafted or automated. Following the notation of AHDA* in Section 2.5, we denote AZHDA* with hand crafted feature abstraction as HDA\u2217[Z ,Afeature ], where Afeature stands for feature abstraction. The key difference of HDA\u2217[Z ,Afeature ] from HDA\u2217[Z ,Astate ] is that HDA\u2217[Z ,Afeature ] applies abstraction to each feature and applies Zobrist hashing to abstract features, whereas HDA\u2217[Z ,Astate ] applies abstraction to a state and applies Zobrist hashing to the abstract state."}, {"heading": "4.1 Evaluation of Work Distribution Methods on Domain-Specific Solvers", "text": "We evaluated the performance of the following HDA* variants on several standard benchmark domains with different characteristics.\n\u2022 HDA\u2217[Z ,Afeature ]: HDA* using AZH\n\u2022 HDA\u2217[Z ]: HDA* using Zobrist hashing (Kishimoto et al., 2009)\n\u2022 HDA\u2217[P ,Astate ]: HDA* using Abstraction based work distribution (Burns et al., 2010)\n\u2022 HDA\u2217[P ]: HDA* using a perfect hash function (Burns et al., 2010)\nThe experiments were run on an Intel Xeon E5-2650 v2 2.60 GHz CPU with 128 GB RAM, using up to 16 cores.\nThe 15-puzzle experiments in Section 4.1.1 incorporated enhancements from the more recent work by Burns et al. Burns et al. to the code used in Section 3.1, which is based on the code by\nBurns et al. (2010), which includes HDA\u2217[P ], HDA\u2217[P ,Astate ], and SafePBNF (we implemented 15-puzzle HDA\u2217[Z ] and HDA\u2217[Z ,Afeature ] as an extension of their code).\nFor the 24-puzzle and multiple sequence alignment (MSA), we used our own implementation of HDA* for overall performance (different from the code used in Section 3.2), using the Pthreads library, try lock for asynchronous communication, and the Jemalloc memory allocator (Evans, 2006). We implemented the open list as a 2-level bucket (Burns et al., 2012) for the 15-puzzle and 24- puzzle, and a binary heap for MSA (binary heap was faster for MSA).\nNote that although we evaluated HDA\u2217[Z ], HDA\u2217[P ,Astate ], and SafePBNF on the on the grid pathfinding problem in Section 3, we do not evaluate HDA\u2217[Z ,Afeature ] on the grid pathfinding problem because in the case of grid pathfinding, the obvious feature projection function for HDA\u2217[Z ,Afeature ] corresponds to the abstraction used by HDA\u2217[P ,Astate ]."}, {"heading": "4.1.1 15-PUZZLE", "text": "We solved 100 randomly generated instances with solvers using the Manhattan distance heuristic. These are not the same instances as the 100 instances used in Section 3.1 because the solver used for this experiment was faster than the solver used in Section 3.16, and some of the instances used in Section 3.1 were too easy for an evaluation of parallel efficiency.7 We selected instances which were sufficiently difficult enough to avoid the results being dominated by the initial startup overhead of the burst effect (Section 3.1.2) \u2013 sequential A* required an average of 52.3 seconds to solve these instances. In addition to HDA\u2217[Z ,Afeature ], HDA\u2217[Z ], and HDA\u2217[P ,Astate ], we also evaluated SafePBNF (Burns et al., 2010) and HDA\u2217[P ].\nThe projections A(xi) (abstract features) we used for AZH in HDA\u2217[Z ,Afeature ] are shown in Figure 8b. The configurations for the other work distribution methods (HDA\u2217[Z ], HDA\u2217[P ,Astate ], SafePBNF, and HDA\u2217[P ]) were the same as in Section 3.1.\nFirst, as discussed in Section 2, high search overhead is correlated with load balance. Figure 9, which shows the relationship between load balance and search overhead, indicates a very strong correlation between high load imbalance and search overhead. We discuss the relationship of load balance and search overhead in detail in Section 5.2.\nFigure 10a shows the efficiency (= speedup#cores ) of each method. HDA \u2217[P ] performed extremely poorly compared to all other HDA* variants and SafePBNF. The reason is clear from Figure 10b, which shows the communication and search overheads. HDA\u2217[P ] has both extremely high search\n6. In Section 3.1, the code is based on the code used in the work of Burns et al. (2010), while the code used in this section incorporated all of the enhancements from their more recent work on efficient sliding tile solver code (Burns et al., 2012) 7. This was intentional \u2013 in Section 3.1, we needed a distribution of instances that included easy instances to highlight the burst effect (Section 3.1.2) as well as for comparison with other methods 3.2.\noverhead and communication overhead compared to all other methods. This shows that the hash function used by HDA\u2217[P ] is not well-suited as a work distribution function.\nHDA\u2217[P ,Astate ] had the lowest CO among HDA* variants (Figure 10b), and significantly outperformed HDA\u2217[P ]. However, HDA\u2217[P ,Astate ] has worse LB than HDA\u2217[Z ] (Figure 9), resulting in higher SO. For the 15-puzzle, this tradeoff is not favorable for HDA\u2217[P ,Astate ], and Figures 10a-9 show that HDA\u2217[Z ], which has significantly better LB and SO, outperforms HDA\u2217[P ,Astate ].\nAccording to Figure 10a, SafePBNF outperforms HDA\u2217[P ,Astate ], and is comparable to HDA\u2217[Z ] on the 15-puzzle. Although our definition of communication overhead does not apply to SafePBNF, SO for SafePBNF was comparable to HDA\u2217[P ,Astate ], 0.11/0.17/0.24 on 4/8/16 threads.\nHDA\u2217[Z ,Afeature ] significantly outperformed HDA\u2217[Z ], HDA\u2217[P ,Astate ], and SafePBNF. As shown in Figure 10b, although HDA\u2217[Z ,Afeature ] had higher SO than HDA\u2217[Z ] and higher CO than HDA\u2217[P ,Astate ], it achieved a balance between these overheads which resulted in high overall efficiency. The tradeoff between CO and SO depends on each domain and instance. By tuning the size of the abstract feature, we can choose a suitable tradeoff."}, {"heading": "4.1.2 24-PUZZLE", "text": "We generated a set of 100 random instances that could be solved by A* within 1000 seconds. For the same reason as with the 15-puzzle experiments above in Section 4.1.1, these are different from the 24-puzzle instances used in 3.2. We chose the hardest instances solvable given the memory limitation (128GB). The average runtime of sequential A* on these instances was 219.0 seconds. The average solution length of our 24-puzzle instances was 92.9 (the average solution length in th epreious work by Korf and Felner (2002) was 100.8). We used a disjoint pattern database heuristic (Korf & Felner, 2002). For the sliding-tile puzzle, the disjoint pattern database heuristic is much more efficient than Manhattan distance, thus the average walltime of 24-puzzle with disjoint pattern database heuristic was much faster than that of 15-puzzle with Manhattan distance heuristic, even though the 24-puzzle search space is much larger than the 15-puzzle search space. Figure 8d shows the feature projections we used for 24-puzzle. For HDA\u2217[Z ] and HDA\u2217[P ,Astate ], we used same configurations as in Section 3.2. The abstraction used by SafePBNF ignores the numbers on all of the tiles except tiles 1,2,3,4, and 5 (we tried (1) ignoring all tiles except blank and tiles 1-2, (2)\nignoring all tiles except blank and tiles 1-3, (3) ignoring all tiles except blank and tiles 1-4, (4) ignoring all tiles except tiles 1-3, (5) ignoring all tiles except tiles 1-4, (6) ignoring all tiles except tiles 1-5, and chose (6), the best performer).\nFigure 10c shows the efficiency of each method. As with the 15-puzzle, HDA\u2217[Z ,Afeature ] significantly outperformed HDA\u2217[Z ] and HDA\u2217[P ,Astate ], and Figure 10d shows that as with the 15-puzzle, HDA\u2217[Z ] and HDA\u2217[P ,Astate ] succeed in mitigating only one of the overheads (SO or CO). In contrast, HDA\u2217[Z ,Afeature ] outperformed both HDA\u2217[Z ] and HDA\u2217[P ,Astate ] as its SO was comparable to that of HDA\u2217[Z ] while its CO was roughly equal to that of HDA\u2217[P ,Astate ]."}, {"heading": "4.1.3 MULTIPLE SEQUENCE ALIGNMENT", "text": "Multiple Sequence Alignment (MSA) is the problem of finding a minimum-cost alignment of a set of DNA or amino acid sequences by inserting gaps in each sequence. MSA can be solved by finding the min-cost path between corners in a n-dimensional grid, where each dimension corresponds to the position of each sequence. We used 60 benchmark instances, consisting of 10 actual amino acid sequences from BAliBASE 3.0 (Thompson, Koehl, Ripp, & Poch, 2005), and 50 randomly generated instances. The BAliBASE instances we used are: BB12021, BB12022, BB12036, BBS11010, BBS11026, BBS11035, BBS11037, BBS12016, BBS12023, BBS12032. We generated random instances by 1. select number of sequences n from 4 to 9 uniformly randomly, 2. For each sequence select a number of acids l from 5000/n \u2217 0.9 < l < 5000/n \u2217 1.1, 3. choose each acid uniformly random from 20 acids. Edge costs are based on the PAM250 matrix score with gap penalty 8 (Pearson, 1990). Since there was no significant difference between the behavior of HDA* among actual and random instances, we report the average of all 60 instances. We used the pairwise sequence alignment heuristic (Korf, Zhang, Thayer, & Hohwald, 2005).\nThe features for Zobrist hashing and AZH were the positions of each sequence. For AZH, we grouped 4 positions per row into an abstract feature. Thus, with n sequences, nodes in the n-dimensional hypercube with edge length l share the same hash value. The abstraction used by HDA\u2217[P ,Astate ] only considers the position of the longest sequence and ignores the others. We chose this abstraction for HDA\u2217[P ,Astate ] as it performed the best among (1) only considering the position of the longest sequence, (2) only considering the two longest sequences, and (3) only considering the three longest sequences. We also evaluated the performance of Hyperplane Work Distribution (Kobayashi et al., 2011). HDA\u2217[Z ] suffers from node reexpansion in non-unit cost domains such as MSA. Hyperplane work distribution seeks to reduce node reexpansions by mapping the n-dimension grid to hyperplanes (denoted as HDA\u2217[Hyperplane]). For HDA\u2217[Hyperplane], we determined the plane thickness d using the tuning method by Kobayashi et al. (2011) where \u03bb = 0.003, which yielded the best performance among 0.0003, 0.003, 0.03, and 0.3.\nFigure 10e compares the efficiency of each method, and Figure 10f shows the CO and SO. HDA\u2217[Z ,Afeature ] outperformed the other methods. With 4 or 8 threads, HDA\u2217[Z ,Afeature ] had smaller SO than HDA\u2217[Z ]. This is because like HDA\u2217[Hyperplane], HDA\u2217[Z ,Afeature ] reduced the amount of duplicated nodes in some domains compared to HDA\u2217[Z ]. Our MSA solver expands 300,000 nodes/second, which is relatively slow compared to, e.g., our 24-puzzle solver, which expands 1,400,000 node/sec. When node expansions are slow, the relative importance of CO decreases, and SO has a more significant impact on performance in MSA than in the 15/24-Puzzles. Thus, HDA\u2217[P ,Astate ], which incurs higher SO, did not perform well compared to HDA\u2217[Z ]. HDA\u2217[Hyperplane] did not perform well, but it was designed for large-scale, distributed search,\nand we observed HDA\u2217[Hyperplane] to be more efficient on difficult instances than on easier instances \u2013 it is included in this evaluation only to provide another point of reference for evaluating HDA\u2217[Z ] and HDA\u2217[Z ,Afeature ].\n4.1.4 NODE EXPANSION ORDER OF HDA\u2217[Z ,Afeature ]\n0\n1e+06\n2e+06\n3e+06\n4e+06\n5e+06\n6e+06\n7e+06\n8e+06\n0\n5 00\n00 0\n1 e+\n06\n1 .5\ne+ 06\n2 e+\n06\n2 .5\ne+ 06\n3 e+\n06\n3 .5\ne+ 06\n4 e+\n06\n4 .5\ne+ 06\n5 e+\n06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(a) HDA\u2217[Z ,Afeature ] on a difficult instance with 8 threads.\n0 500000 1e+06\n1.5e+06 2e+06\n2.5e+06 3e+06\n3.5e+06 4e+06\n4.5e+06 5e+06\n0\n5 00\n00 0\n1 e+\n06\n1 .5\ne+ 06\n2 e+\n06\n2 .5\ne+ 06\n3 e+\n06\n3 .5\ne+ 06\n4 e+\n06\n4 .5\ne+ 06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(b) HDA\u2217[Z ] on a difficult instance with 8 threads (copy of Figure 4a).\n0\n1e+06\n2e+06\n3e+06\n4e+06\n5e+06\n6e+06\n7e+06\n8e+06\n0\n1 e+\n06\n2 e+\n06\n3 e+\n06\n4 e+\n06\n5 e+\n06\n6 e+\n06\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\n(c) HDA\u2217[P ,Astate ] on a difficult instance with 8 threads (copy of Figure 4b).\n0 5000\n10000 15000 20000 25000 30000\n0 2 00 0 4 00 0 6 00 0 8 00 0 1 00 00 1 20 00 1 40 00\nA *\nex\npa\nns io\nn or\nde r\nparallel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5\nThread 6\nThread 7\nThread 8\nStrict Order\nGoal\n0 5000\n10000 15000 20000 25000 30000\n0\n2\n00 0\n4\n00 0\n6\n00 0\n8\n00 0\n1\n00\n00\n1\n20\n00\n1\n40\n00\nA *\nex pa\nns io\nn or\nde r\nparallel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5 Thread 6 Thread 7 Thread 8 Strict Order\nGoal\n0 5000\n10000 15000 20000 25000 30000\n0 2 00 0 4 00 0 6 00 0 8 00 0 1 00 00 1 20 00 1 40 00\nA *\nex pa\nns io\nn or\nde r\npa allel expansion order\nThread 1 Thread 2 Thread 3 Thread 4 Thread 5 Thread 6 Thread 7 Thread 8 Strict Order Goal\nFigure 11: Comparison of HDA\u2217[Z ,Afeature ] node expansion order vs. sequential A* node expansion order on a difficult instance of the 15-puzzle with 8 threads. The average node expansion order divergence scores for difficult instances are HDA\u2217[Z ]: d\u0304 = 10330.6, HDA\u2217[P ,Astate ]: d\u0304 = 245818, HDA\u2217[Z ,Afeature ]: d\u0304 = 76932.2. AZHDA has a bigger band effect than HDA\u2217[Z ], but smaller than HDA\u2217[P ,Astate ]. Although the band of HDA\u2217[Z ,Afeature ] appears to be as large as HDA\u2217[P ,Astate ], the actual divergence score d\u0304 is higher on HDA\u2217[P ,Astate ] as HDA\u2217[P ,Astate ] expands more nodes.\nIn Section 3.1.4, in order to see why search overhead occurs in HDA* and PBNF, we analyzed how the node expansion order of parallel search diverges from that of sequential A*. Figure 11 shows the expansion order of HDA\u2217[Z ,Afeature ] on a difficult instance (HDA\u2217[Z ] and HDA\u2217[P ,Astate ] are included for comparison). HDA\u2217[Z ,Afeature ] has a bigger band effect than HDA\u2217[Z ], but smaller than that of HDA\u2217[P ,Astate ]. The average divergence of nodes for difficult instances are HDA\u2217[Z ]: d\u0304 = 10330.6, HDA\u2217[P ,Astate ]: d\u0304 = 245818, HDA\u2217[Z ,Afeature ]: d\u0304 = 76932.2. Note that although the band effect of HDA\u2217[Z ,Afeature ] in Figure 11a appears to be as large as the band effect of HDA\u2217[P ,Astate ] in Figure 4b, the actual divergence score d\u0304 is significantly higher on HDA\u2217[P ,Astate ] (d\u0304 = 245818) than on HDA\u2217[Z ,Afeature ] (d\u0304 = 76932.2) because HDA\u2217[P ,Astate ] expanded more nodes (>5,000,000 nodes) than HDA\u2217[Z ,Afeature ] (>4,500,000 nodes), HDA\u2217[P ,Astate ] has significantly larger divergence than HDA\u2217[Z ,Afeature ]."}, {"heading": "4.2 Automated, Domain Independent Abstract Feature Generation", "text": "In Section 4.1, we evaluated hand-crafted, domain-specific feature projection functions for instances of the HDA* framework (HDA\u2217[Z ], HDA\u2217[P ], HDA\u2217[P ,Astate ], HDA\u2217[Z ,Afeature ]), and showed that AZH outperformed previous methods. Next, we turn our focus to fully automated, domain-\nindependent methods for generating feature projection functions which can be used when a formal model of a domain (such as PDDL/SAS+ for classical planning) is available.\nFrom now on, we discuss domain-independent methods for work distribution. Table 5 summarizes the previously proposed methods and their abbreviations.\nFor HDA\u2217[Z ], automated domain-independent feature generation for classical planning problems represented in the SAS+ representation (Ba\u0308ckstro\u0308m & Nebel, 1995) is straightforward (Kishimoto et al., 2013). For each possible assignment of value k to variable vi in a SAS+ representation, e.g., vi = k, there is a binary proposition xi,k (i.e., the corresponding STRIPS propositional representation). Each such proposition xi,k is a feature to which a randomly generated bit string is assigned, and the Zobrist hash value of the state can be computed by xor\u2019ing the propositions that describe a state, as in Equation 1.\nFor AHDA*, the abstract representation of the state space can be generated by ignoring some of the features (SAS+ variables) and using the rest of the features to represent the abstraction. Burns et al. used the greedy abstraction algorithm by Zhou and Hansen (2006b) to select the subset of features, which we refer to as SDD abstraction. It adds one atom group to the abstract graph at a time, choosing the atom group which minimizes the maximum out-degree of the abstract graph, until the graph size (number of abstract nodes) reaches the threshold given by a parameter. As we saw in Section 4.1, the hashing strategy for abstract state has little effect on the performance. We used the implementation of AHDA* with Zobrist hashing and SDD abstraction (HDA\u2217[Z ,Astate/SDD ]).\nFor AZHDA* (HDA\u2217[Z ,Afeature ]), the feature projection function which generates abstract features from raw features plays a critical role in determining the performance of AZHDA*, because AZHDA* relies on the feature projection in order to reduce communications overhead. In this section, we discuss two methods to automatically generate the feature projection function for AZH. Greedy abstract feature generation (GreedyAFG), which partitions each domain transition graph (DTG) into 2 abstract features, and fluency-based abstract feature generation (FluencyAFG), an ex-\ntension of GreedyAFG which filters the DTGs to partition according to a fluency-based criterion. GreedyAFG and FluencyAFG seek to generate efficient feature projection functions without an explicit model of what to optimize. Further details on GreedyAFG and FluencyAFG can be found in our previous conference paper (Jinnai & Fukunaga, 2016b)."}, {"heading": "4.2.1 GREEDY ABSTRACT FEATURE GENERATION (GAZHDA*)", "text": "Greedy abstract feature generation (GreedyAFG) is a simple, domain-independent abstract feature generation method, which partitions each feature into 2 abstract features (Jinnai & Fukunaga, 2016a). GreedyAFG first identifies atom groups (Edelkamp, 2001) and its domain transition graph (DTG). Atom group is a set of mutually exclusive propositions from which exactly one will be true for each reachable state, e.g., the values of a SAS+ multi-valued variable (Ba\u0308ckstro\u0308m & Nebel, 1995). GreedyAFG maps each atom group X into 2 abstract features S1 and S2, based on X\u2019s undirected DTG (nodes are values, edges are transitions), as follows: (1) assign the minimal degree node (node with the least number of edges between other nodes) to S1; (2) greedily add to S1 the unassigned node which shares the most edges with nodes in S1; (3) while |S1| < |X|/2 repeat step 2; (4) assign all unassigned nodes to S2. Due to the loop criterion in step 3, this procedure guarantees a perfectly balanced bisection of the DTGs, i.e., |S2| \u2264 |S1| \u2264 |S2| + 1, so load balancing is minimized. A(xi) in Equation 2 corresponds to the mapping from xi to S1, S2, and Ri is defined over S1 and S2. We denote GAZHDA* as HDA\u2217[Z ,Afeature/DTGgreedy ], as it applies feature abstraction (FA) by cutting DTGs using GreedyAFG.\nAlgorithm 4: Greedy Abstract Feature Generation Input: X: an atom group 1 Assign the minimal degree node (node with the least number of edges between other nodes) to S1; 2 while |S1| < |G|/2 do 3 Greedily add to S1 the unassigned node which shares the most edges with nodes in S1; 4 Assign all unassigned nodes to S2.; 5 Return (S1, S2);"}, {"heading": "4.2.2 FLUENCY-DEPENDENT ABSTRACT FEATURE GENERATION (FAZHDA*)", "text": "Since the hash value of the state changes if any abstract feature value changes, GreedyAFG fails to prevent high CO when any abstract feature changes its value very frequently, e.g., in the blocks domain, every operator in the domain changes the value of the SAS+ variable representing the state of the robot\u2019s hand ( handempty\u2194 not-handempty). Fluency-dependent abstract feature generation (FluencyAFG) overcomes this limitation (Jinnai & Fukunaga, 2016b). The fluency of a variable v is the number of ground actions which change the value of the v divided by the total number of ground actions in the problem. By ignoring variables with high fluency, FluencyAFG was shown to be quite successful in reducing CO and increasing speedup compared to GreedyAFG.\nA problem with fluency is that in the AZHDA* framework, CO is associated with a change in value of an abstract feature, not the feature itself. However, FluencyAFG is based on the frequency with which features (not abstract features) change. This leads FluencyAFG to exclude variables from consideration unnecessarily, making it difficult to achieve good LB (in general, the more vari-\nables are excluded, the more difficult it becomes to reduce LB). Figure 12 shows how fluency-based filtering is applied to the blocks domain. The process of fluency-based filtering which ignores a subset of features can be described as an instance of abstraction. Therefore, we denote FAZHDA* as HDA\u2217[Z ,Afeature/DTGfluency ], as it applies fluency-based abstraction, and then GAZHDA*."}, {"heading": "5. A Graph Partitioning-Based Model for Work Distribution", "text": "Although GAZHDA* and FAZHDA*, the domain-independent abstract feature generation methods discussed in Section 4.2, seek to reduce communications overhead compared to HDA\u2217[Z ], they are not based on an explicit model which enables the prediction of the actual communications overhead achieved during the search. Furthermore, the impact of these methods on search overhead is completely unspecified, and thus, it is not possible to predict the parallel efficiency achieved during the search. Previous work relied on ad hoc, control parameter tuning in order to achieve good performance (Jinnai & Fukunaga, 2016b). In this section, we first show that a work distribution method can be modeled as a partition of the search space graph, and that communication overhead and load balance can be understood as the number of cut edges and balance of the partition, respectively. Using this model, we introduce a metric, estimated efficiency, and we experimentally show that the metric has a strong correlation to the actual efficiency. This leads to the GRAZHDA* feature generation method described in Section 6."}, {"heading": "5.1 Work Distribution as Graph Partitioning", "text": "Work distribution methods for hash-based parallel search distribute nodes by assigning a process to each node in the state space. Our goal is to design a work distribution method which maximizes efficiency by reducing CO, SO, and load balance (LB). In particular, given a problem instance, we\nseek a principled method of quickly, automatically generating a work distribution method (hash function) for HDA* for that particular problem instance. We propose an approach which is based on optimizing a priori estimates of CO, SO, and LB. In our approach, given a problem, we search a space of hash functions, using these estimates of CO, SO, LB as the basis for a (cheap) evaluation function for this search in the space of hash functions. To enable this, we first develop a model for estimating algorithm performance based on the notion of a workload graph.\nTo guarantee the optimality of a solution, a parallel search method needs to expand a goal node and all nodes with f < f\u2217 (relevant nodes S). The workload distribution of a parallel search can be modeled as a partitioning of an undirected, unit-cost workload graphGW which is isomorphic to the relevant search space graph, i.e., nodes in GW correspond to states in the search space with f < f\u2217 and goal nodes, and edges in the workload graph correspond to edges in the search space between nodes with f < f\u2217 and goal nodes. The distribution of nodes among p processors corresponds to a p-way partition of GW , where nodes in partition Si are assigned to process pi.\nThe workload graph GW only includes nodes with f < f\u2217, for the following reason. We are ultimately trying to develop a method for quickly estimating SO, CO, and LB for a work distribution scheme S without actually running S. In principle, if we knew exactly the actual portion of the graph which is explored by HDA* with a particular partitioning scheme, then this would allow us to accurately compute search efficiency. However, that requires running HDA* until a solution is found, so this is impractical, and we need an approximation of the actual explored nodes. The set of nodes with f < f\u2217 is a reasonable approximation to the nodes which are explored by HDA*, because these are the set of nodes which must be expanded regardless of the hash function (partitioning method). Depending on the hash function, some nodes with f \u2265 f\u2217 are expanded, but it is not possible to know how many such nodes will be expanded without actually running HDA* with that hash function. Therefore, although the workload graph underestimates the size of the actual relevant search space, it is a reasonable approximation. While underestimating the relevant search space is not ideal, the converse (considering states which are irrelevant to the actual HDA*) is problematic. For example, if we consider the entire search space (i.e., including all nodes with f \u2265 f\u2217) would be mapped to processors if the search algorithm continued to execute until the space is exhausted, then HDA\u2217[P ] (Section 4.1.1) successfully partitions the space evenly, i.e., \u201cperfect load balance\u201d. However, as shown in Figure 9, HDA\u2217[P ] has the worst load balance in the actual experiment. This is because the distribution of HDA\u2217[P ] is highly biased in the search space so that the relevant state space (f \u2264 f\u2217), which is a small fraction of the state space, is distributed unevenly. Considering only the nodes with f < f\u2217 allows us to capture this bias. This example also illustrates how using a perfect hashing which balances the partitions for the entire search space does not does not achieve good performance unless the partitions are also balanced with respect to portion of the the search space which is actually explored by the search algorithm.\nGiven a partitioning of GW , LB and CO can be estimated directly from the structure of the graph, without having to run HDA* and measure LB and CO experimentally, i.e., it is possible to predict and analyze the efficiency of a workload distribution method without actually executing HDA*. Therefore, although it is necessary to run A* or HDA* once to generate a workload graph,8 we can subsequently compare the LB and CO of many partitioning methods without re-running HDA* for each partitioning method. LB corresponds to load balance of the partitions and CO is the\n8. Hence, this is not yet a practical method for automatic hash function generation \u2013 a further approximation of this model which does not require generating the workload graph, and yields a practical method is described in Section 6.\nnumber of edges between partitions over the number of total edges, i.e.,\nCO =\n\u2211p i \u2211p j>iE(Si, Sj)\u2211p\ni \u2211p j\u2265iE(Si, Sj) , LB = |Smax| mean|Si| , (3)\nwhere |Si| is the number of nodes in partition Si, E(Si, Sj) is the number of edges between Si and Sj , |Smax| is the maximum of |Si| over all processes, and mean|S| = |S|p .\nNext, consider the relationship between SO and LB. It has been shown experimentally that an inefficient LB leads to high SO, but to our knowledge, there has been no previous analysis on how LB leads to SO in parallel best-first search. Assume that the number of duplicate nodes is negligible9, and every process expands nodes at the same rate. Since HDA* needs to expand all nodes in S, each process expands |Smax| nodes before HDA* terminates. As a consequence, process pi expands |Smax| \u2212 |Si| nodes not in the relevant set of nodes S. By definition, such irrelevant nodes are search overhead, and therefore, we can express the overall search overhead as:\nSO = p\u2211 i (|Smax| \u2212 |Si|)\n= p(LB \u2212 1). (4)"}, {"heading": "5.2 Parallel Efficiency and Graph Partitioning", "text": "In this section we develop a metric to estimate the walltime efficiency as a function of CO and SO. First, we define time efficiency effactual := speedup #cores , where speedup = TN/T1, Tn is the runtime on N cores and T1 the runtime on 1 core. Our ultimate goal is to maximize effactual . Communication Efficiency: Assume that the communication cost between every pair of processors is identical. If tcom is the time spent sending nodes from one core to another10, and tproc is the time spent processing nodes (including node generation and evaluation). Hence communication efficiency, the degradation of efficiency by communication cost, is effc = 11+cCO , where c = tcom tproc\n. Search Efficiency: Assuming all cores expand nodes at the same rate and that there are no idle cores, HDA* with p processes expands np nodes in the same wall-clock time A* requires to expand n nodes. Therefore, search efficiency, the degradation of efficiency by search overhead, is effs =\n1 1+SO .\nUsing CO and LB (and SO from Equation 4), we can estimate the time efficiency effactual . effactual is proportional to the product of communication and search efficiency: effactual \u221d effc \u00b7effs . There are overheads other than CO and SO such as hardware overhead (i.e. memory bus contention) that affect performance (Burns et al., 2010; Kishimoto et al., 2013), but we assume that CO and SO are the dominant factors in determining efficiency.\n9. The number of duplicate node is closely related to LB and CO. If the order of node expansion is exactly the same as A*, then the number of duplicate is 0. The duplicate nodes occur when LB is suboptimal and the order of node expansion diverges from A*. The other cause of duplicate is CO. Even if the load balance is optimal, the optimal path may be disturbed by communication latency and suboptimal path may be discovered first, resulting in duplicate nodes. Therefore, optimizing LB and CO leads to reducing duplicate nodes. 10. In a multicore environment, the cost of \u201csending\u201d a node from thread p1 to p2 is the time required to obtain access to the incoming queue for p2 (via a successful try lock instruction).\nWe define estimated efficiency effesti as effesti := effc \u00b7 effs , and we use this metric to estimate the actual performance (efficiency) of a work distribution method.\neffesti = effc \u00b7 effs = 1\n(1 + cCO)(1 + SO)\n= 1\n(1 + cCO)(1 + p(LB \u2212 1)) (5)\n5.2.1 EXPERIMENT: effesti MODEL VS. ACTUAL EFFICIENCY\nTo validate the usefulness of effesti , we evaluated the correlation of effesti and actual efficiency on the following HDA* variants discussed in Section 6.1 on domain-independent planning.\n\u2022 FAZHDA*: HDA\u2217[Z ,Afeature/DTGfluency ], AZHDA* using fluency-based filtering (FluencyAFG). \u2022 GAZHDA*: HDA\u2217[Z ,Afeature/DTGgreedy ], AZHDA* using greedy abstract feature generation (GreedyAFG). \u2022 OZHDA*: HDA\u2217[Zoperator ], Operator-based Zobrist hashing (Sec. 2.4). \u2022 DAHDA*: HDA\u2217[Z ,Astate/SDDdynamic ], AHDA* (Burns et al., 2010) with dynamic abstraction size\nthreshold (Appendix A).\n\u2022 ZHDA*: HDA\u2217[Z ], HDA* using Zobrist hashing (Kishimoto et al., 2013) (Sec. 2.4).\nWe implemented these HDA* variants on top of the Fast Downward classical planner using the merge&shrink heuristic (Helmert et al., 2014) (abstraction size =1000). We parallelized Fast Downward using using MPICH3. We selected a set of IPC benchmark instances that are difficult enough so that parallel performance differences could be observed. We ran experiments on a cluster of 6 machines, each with an 8-core Intel Xeon E5410 2.33 GHz CPU with 16 GB RAM, and 1000Mbps Ethernet interconnect. For FAZHDA*, we ignored 30% of the variables with the highest fluency as it performed the best out of 10%, 20%, 30%, 50%, and 70%. DAHDA* uses at most 30% of the total number of features in the problem instance (we tested 10%, 30%, 50%, and 70% and found that 30% performed the best). We packed 100 states per MPI message in order to reduce the number of messages (Romein et al., 1999).\nTable 7 shows the speedups (time for 1 process / time for 48 processes). We included the time for initializing work distribution methods (for all runs, the initializations completed in \u2264 1 second), but excluded the time for initializing the abstraction table for the merge&shrink heuristic. From the measured runtimes, we can compute actual efficiency effactual . Then, we calculated the performance estimated effesti as follows. We generated the workload graph GW for each instance (i.e., enumerated all nodes with f \u2264 f\u2217 and edges between these nodes), and calculated LB, CO, SO, and effesti using Eqs 3-5. Figure 13, which compares estimated efficiency effesti vs. the actual measured efficiency effactual , indicates a strong correlation between effesti and effactual . Using least-square regression to estimate the coefficient a in effactual = a \u00b7 effesti , we obtained a = 0.86 with variance of residuals 0.013. Note that a < 1.0 because there are other sources of overhead which not accounted for in effesti , (e.g. memory bus contention) which affect performance (Burns et al., 2010; Kishimoto et al., 2013).\nObservation 5 The effesti metric for a partitioning scheme, which can be computed from the workload distribution graph (without running HDA* using that partitioning scheme), is strongly correlated with the actual measured efficiency effactual of HDA*."}, {"heading": "6. Graph Partitioning-Based Abstract Feature Generation (GRAZHDA*)", "text": "A standard approach to workload balancing in parallel scientific computing is graph partitioning, where the workload is represented as a graph, and a partitioning of the graph according to some objective (usually the cut-edge ratio metric) represents the allocation of the workload among the processors (Hendrickson & Kolda, 2000; Buluc, Meyerhenke, Safro, Sanders, & Schulz, 2015).\nIn Section 5, we showed that work distributions for parallel search on an implicit graph can be modeled as partitions of a workload graph which is isomorphic to the search space, and that this workload graph can be used to estimate the CO and LB of a work distribution. If we were given a workload graph, then by defining a graph cut objective such that partitioning the nodes in the search space (with f \u2264 f\u2217) corresponds to maximizing the efficiency, we would have a method of generating an optimal workload distribution. Unfortunately, this is impractical as the workload graph is an explicit representation of the relevant state space graph, i.e., this a solution to the search problem itself!\nHowever, a practical alternative is to apply graph partitioning to a graph which serves an approximate, proxy for the actual state space graph. We propose GRaph partitioning-based Abstract Zobrist HDA* (GRAZHDA*), which approximates the optimal graph partitioning-based strategy by partitioning domain transition graphs (DTG). Given a classical planning problem represented in SAS+, the domain transition graph (DTG) of a SAS+ variable X , DX(E, V ), is a directed graph where vertices V corresponds to the possible values of a variable X , edges E represent transitions among the values of X , and (v, v\u2032) \u2208 E iff there is an operator (action) o with v \u2208 del(o) and v\u2032 \u2208 add(o) (Jonsson & Ba\u0308ckstro\u0308m, 1998).\nListing 1: Sliding-tile puzzle PDDL\n( d e f i n e ( domain s t r i p s \u2212s l i d i n g \u2212 t i l e ) ( : requirements : s t r i p s ) ( : p r e d i c a t e s\n( t i l e ? x ) ( p o s i t i o n ? x ) ( a t ? t ? x ? y ) ( b l a n k ? x ? y ) ( i n c ? p ? pp ) ( dec ? p ? pp ) )\n( : a c t i o n move\u2212up : parameters ( ? omf ? px ? py ? by ) : p r e c o n d i t i o n ( and\n( t i l e ?omf ) ( p o s i t i o n ? px ) ( p o s i t i o n ? py ) ( p o s i t i o n ? by ) ( dec ? by ? py ) ( b l a n k ? px ? by ) ( a t ?omf ? px ? py ) )\n: e f f e c t ( and ( not ( b l a n k ? px ? by ) ) ( not ( a t ?omf ? px ? py ) ) ( b l a n k ? px ? py ) ( a t ?omf ? px ? by ) ) )\n( : a c t i o n move\u2212 l e f t . .\nThe DTGs for a problem provide a highly compressed representation which reflects the structure of the search space, and is easily extracted automatically from the formal domain description (e.g., PDDL/SAS+). We expect DTGs to be good proxies for the search space because DTGs tend to be orthogonal to each other \u2013 otherwise the propositions of the DTG is redundant (this is not always true as PDDL may contain dual representations, e.g. sokoban).\nGRAZHDA* partitions each DTG into two abstract features according to an objective function. That is, each DTG is partitioned into two subsets S1 and S2. ProjectionA(x) is defined on the value of the DTG, and returns 1 or 0 depending on whether S1 or S2 it is included in. Abstract Zobrist hashing is then applied using these abstract features (random table R in Equation 2 is defined on S1 and S2). In GRAZHDA*, AZH uses each partition of the DTG as an abstract feature, assigning a hash value to each abstract feature (Figure 14). Since the AZH value of a state is the XOR of the hash values of the abstract features (Equation 2), 2 nodes in the state space are in different partitions if and only if they are partitioned in any of the DTGs. Therefore, GRAZHDA* generates 2n partitions from n DTGs, which are then projected to the p processors (by taking the hash value modulo p, processor(s) = hashvalue(s) mod p).11 We denote GRAZHDA* as HDA\u2217[Z ,Afeature/DTG ], where DTG stands for DTG-partitioning."}, {"heading": "6.1 Previous Methods and Their Relationship to GRAZHDA*", "text": "In this section we show that previously proposed methods for the HDA* framework can be interpreted as instances of GRAZHDA*. First, we define a DTG-partitioning as follows: given s = (v0, v1, ..., vn), a DTG-partitioning maps a state s to an abstract state s\u2032 = (A0[v0], A1[v1], ..., An[vn]), where Ai[vi] is defined by a graph partitioning on each DTG while optimizing given objective function. DTG-partitioning corresponds to AF/DTG for an abstraction strategy. Then, in order to model non-DTG based methods, we refer to all other methods which map a state space to an abstract\n11. In HDA* the owner of a state is computed as processor(s) = hashvalue(s) mod p, so it is possible that states with different hash values are assigned to the same thread. Also, while extremely unlikely, it is theoretically possible that s and s\u2032 may have the same hash value even if they have different abstract features due to the randomized nature of Zobrist hashing (in all our HDA* variants, we detect such collisions by always comparing the values stored in the hash table whenever hash keys point to a nonempty hash table entry).\nstate space with or without objectives a clustering. For example, by ignoring subset of the variables, we get an abstract state s\u2032 = (v0, ..., vm) where m < n. Clustering corresponds to any abstraction strategy other than DTG-partitioning. Using this terminology, the relationship between GRAZHDA* and previous methods is summarized in Figure 15.\nFirst, HDA\u2217[Z ], the original Zobrist-hashing based HDA* (Kishimoto et al., 2009, 2013), corresponds to an extreme case where every node in DTG is assigned to a different partition (for all Ai, Ai[vi] 6= Ai[v\u2032i] if vi 6= v\u2032i).\nGAZHDA* (GreedyAFG) (Jinnai & Fukunaga, 2016a), described in Section 4.2.1 is in fact applying DTG-partitioning whose objective function is to minimize LB as the primary objective, with a secondary objective of (greedily) minimizing CO, as it tries to assign the most connected node but does not optimize. Thus, GAZHDA* an instance of GRAZHDA*.\nAHDA* (Burns et al., 2010) (Section 2.5), FAZHDA* (Jinnai & Fukunaga, 2016b) (Section 4.2.2), OZHDA* (Jinnai & Fukunaga, 2016b) (Section 2.4), and DAHDA* (Jinnai & Fukunaga,\n2016b) (Section2.5), are instances of GRAZHDA* with clustering, which map the state space graph to an abstract state space graph, and then apply DTG-partitioning to the abstract state space graph so that the nodes mapped to the same abstract state are guaranteed to be assigned to the same partition, so that there no communication overhead is incurred when generating a node that is in the same abstract state as its parent.\nAHDA* generates an abstract state space by ignoring some of the features (DTGs) in the state representation and then it applies hashing to the abstract state space. Ignoring part of the state representation can be interpreted as a clustering of nodes so that all of the nodes in a cluster are allocated to the same processor. The problem with AHDA* is the criteria used to determine which features to ignore (conversely, which features to take into account). It minimizes the highest degree of the abstract nodes, as the abstraction method used by AHDA* was originally proposed for duplicate detection of external search (Zhou & Hansen, 2006b). However, this doe not correspond to a natural objective function which optimizes parallel work distribution objective such as edge cut or load balancing. Therefore, although the projection of AHDA* result in significantly reduced CO, it does not explicitly try to optimize it; CO is reduced as a fortunate side-effect of generating efficient abstract state space for external search. DAHDA* (Jinnai & Fukunaga, 2016b) improves upon AHDA* by dynamically tuning the number of DTGs which are ignored (see Appendix A), but the state projection mechanism is the same as AHDA*.\nFAZHDA* is a variant of GAZHDA*, which, instead of using all the variables as GAZHDA* does, FAZHDA* ignores some of the variables in the state based on their fluency, which is defined\nas the number of ground actions which change the value of the variable divided by the total number of ground actions in the problem. As we pointed out above for AHDA*, ignoring variables can be described as a clustering. Although fluency-based filtering is intended to reduce CO, ignoring high fluency variables is only a heuristic which sometimes succeeds in reducing CO, but sometimes fails, since fluency is defined on the frequency of the change of the feature (value), but the change of abstract feature is what incurs CO. Even if the fluency of a variable is 1.0, the value may change within an abstract feature, thus eliminating the DTG does not improve any CO whatsoever. Fluency-based filtering only takes into account of the fluency of the variable, whereas GRAZHDA* framework looks into each transition in the DTG to choose how to treat the variable.\nOZHDA* clusters nodes connected with selected operators and applies Zobrist hashing, so that the selected operator does not cost communication. The clustering of OZHDA* is bottom-up, in the sense that state space nodes connected with selected operators are directly clustered, instead of using SAS+ variables or DTGs. The problem with OZHDA* is that the clustering is ad hoc and unbalanced \u2013 some of the nodes are clustered but the others are not, and the choice of which nodes to cluster or not is not explicitly optimized. The clustered nodes are then partitioned by assigning each node to a separate partition, as with ZHDA* (see above), but this is dangerous, since OZHDA* ends up treating clustered nodes and original nodes equally, without considering that the clustered nodes should have larger edge cut costs than original single nodes. Thus, although the clustering done by OZHDA* is intended to reduce CO, it comes at the price of load balance \u2013 the edge costs for the (implicit) workload graph are not aggregated when the clusters are formed, so load balance is being sacrificed without an explicit objective function controlling the tradeoff.\nThus, we have shown that all previous methods for work distribution in the HDA* framework can be viewed as instances of GRAZHDA* using ad hoc criteria for clustering and optimization."}, {"heading": "6.2 Effective Objective Functions for GRAZHDA*", "text": "In the previous section, we showed that previous variants of HDA* can be seen as instances of GRAZHDA* which partitioned the workload graph based on ad hoc criteria. However, since the GRAZHDA* framework formulates workload distribution as a graph partitioning problem, a natural idea is to design an objective function for the partitioning which directly leads to a desired tradeoff between search and communication overheads, resulting in good overall efficiency. Fortunately, a metric which can be used as the basis for such an objective is available: effesti .\nIn Section 5.2.1, we showed that effesti , based on the workload is an effective predictor for the actual efficiency of a work distribution strategy. In this section, we propose approximations to effesti which can be used as objective functions for the DTG partitioning in GRAZHDA*.\nIn principle, in order to maximize the performance of GRAZHDA*, it is desirable to have a function which approximates effesti as closely as possible. However, since GRAZHDA* partitions the domain transition graph as opposed to the actual workload graph (which is isomorphic to the search space graph), and the DTG is only an approximation to the actual workload graph, a perfect approximation of effesti is not feasible. Fortunately, in practice, it turns out that using a straightforward approximation of effesti as an objective function for GRAZHDA* result in good performance when compared to previous work distribution methods."}, {"heading": "6.2.1 SPARSEST CUT OBJECTIVE FUNCTION (GRAZHDA*/SPARSITY)", "text": "One straightforward objective function which is clearly related to effesti is a sparsest cut objective, which maximizes sparsity, defined as\nsparsity := \u220fp i |Si|\u2211p\ni \u2211p j>iE(Si, Sj) , (6)\nwhere p is the number of partitions (= number of processors), |Si| is the number of nodes in partition Si divided by the total number of nodes, E(Si, Sj) is the sum of edge weights between partition Si and Sj . Consider the relationship between the sparsity of a state space graph for a search problem and the effesti metric defined in the previous section. By equations 5 and 3, sparsity simultaneously considers both LB and CO, as the numerator \u220fp i |Si| corresponds to LB and the\ndenominator \u2211p\ni \u2211p j>iE(Si, Sj) corresponds to CO.\nSparsity is used as a metric for parallel workloads in computer networks (Leighton & Rao, 1999; Jyothi, Singla, Godfrey, & Kolla, 2014), but to our knowledge this is the first proposal to use sparsity in the context of parallel search of an implicit graph.\nFigure 16 shows the sparsest cut of a DTG (for the variable representing package location) in the standard logistics domain. Each edge in a DTG corresponds to a transition of its value. Edge costs we represent the ratio of operators which corresponds to its transition over the total number of operators in the DTG. For example in logistics, each edge corresponds to 2 operators, one in each direction ( (drive-truck ?truck pos0 pos1) and (drive-truck ?truck pos1 pos0), or (fly-airplane ?plane pos0 pos1) and (fly-airplane ?plane pos1 pos0) ). The total number of operator in the graph is 120, thus we for each edge is 2/120 = 1/60. We use this to calculate sparsity (Equation 6). Maximizing sparsity results in cutting only 1 edge (Figure 16): it cuts the graph with |S1| \u00b7 |S2| = 10/16 \u00b7 6/16, and edge cuts E(S1, S2) = 1 \u00b7 we, thus sparsity = |S1|\u00b7|S2|E(S1,S2) = 26.72, whereas the partition by GreedyAFG results in cutting 21 edges (sparsity = 0.71). The problem with GreedyAFG is that it imposes a hard constraint requiring the partition to be perfectly balanced. While this optimizes load balance, locality (i.e., the number of cut edges) is sacrificed. GRAZHDA*/sparsity takes into account both load balance and CO without the hard constraint of bisection, resulting in a partitioning which preserves more locality.\n6.2.2 EXPERIMENT: VALIDATING THE RELATIONSHIP BETWEEN SPARSITY AND effesti\nTo validate the correlation between sparsity and estimated efficiency effesti , we used the METIS (approximate) graph partitioning package (Karypis & Kumar, 1998) to partition modified versions of the search spaces of the instances used in Fig. 17a. We partitioned each instance 3 times, where each run had a different set of random, artificial constraints added to the instance (we chose 50% of the nodes randomly and forced METIS to distribute them equally among the partitions \u2013 these constraints degrade the achievable sparsity). Figure 17b compares sparsity vs. effesti on partitions generated by METIS with random constraints. There is a clear correlation between sparsity and effesti . Thus, partitioning a graph to maximize sparsity should maximize the effesti objective, which should in turn maximize actual walltime efficiency."}, {"heading": "6.2.3 PARTITIONING THE DTGS", "text": "Given an objective function such as sparsity, GRAZHDA* partitions each DTG into two abstract features, as described above in Section 6. Since each domain transition graph typically only has fewer than 10 nodes, we compute the optimal partition for both objective functions with a straightforward depth-first branch-and-bound procedure. It is possible that branch-and-bound becomes impractical in case a domain has very large DTGs, or we may develop a more complicated objective function for partitioning the DTGs. In such cases, we can use heuristic partitioning methods such as the FM algorithm (Fiduccia & Mattheyses, 1982). However, to date, branch-and-bound has been sufficient \u2013 in all of the standard IPC benchmark domains we evaluated, the abstract feature generation procedure (which includes partitioning all of the DTGs) take less than 4 seconds on every instance we tested (most instances take < 1 second)."}, {"heading": "6.3 Evaluation of Automated, Domain-Independent Work Distribution Methods", "text": "In addition to the methods in Section 5.2.1, we evaluated the performance of GRAZHDA*/sparsity. We used CGL-B (CausalGraph-Goal-Level&Bisimulation) merge&shrink heuristic (Helmert et al., 2014), which is more efficient and recently proposed than LFPA merge&shrink (Helmert et al., 2007) used in a previous conference paper which evaluated GAZHDA* and FAZHDA* (Jinnai & Fukunaga, 2016b). For example in Block10-1, CGL-B expands 11,065,451 nodes while LFPA 51,781,104 expands nodes. We set the abstraction size for merge&shrink to 1000. The choice of heuristic affects the behavior of parallel search if the heuristics have different node expansion rate, because it affects the relative cost of communication. As CGL-B and LFPA have roughly the same node expansion rate, we did not observe a significant difference on the effect of work distribution methods. Therefore, we show the result using CGL-B because it runs faster on sequential A*. We discuss the effect of node expansion rate in Section 6.3.4. We did not apply fluency-based filtering (Section 4.2.2) and used all DTGs in GRAZHDA*/sparsity because it did not improve the performance.\nFigure 17a shows effesti for the various work distribution methods, including GRAZHDA* (see Section 5.2.1 for experimental setup and list of methods included in comparison). To evaluate how these methods compare to an ideal (but impractical) model which actually applies graph partitioning to the entire search space (instead of partitioning DTG as done by GRAZHDA*), we also evaluated IdealApprox, a model which partitions the entire state space graph using the METIS (approximate) graph partitioner (Karypis & Kumar, 1998). IdealApprox first enumerates a graph containing all nodes with f \u2264 f\u2217 and edges between these nodes and ran METIS with the sparsity objective (Equation 6) to generate the partition for the work distribution. Generating the input graph for METIS takes an enormous amount of time (much longer than the search itself), so IdealApprox is clearly an impractical model, but it provides a useful approximation for an ideal work distribution which can be used to evaluate practical methods.\nNot surprisingly, IdealApprox has the highest effesti , but among all of the practical methods, GRAZHDA*/sparsity has the highest effesti overall. As we saw in Section 5.2.1 that effesti is a good estimate of actual efficiency, the result suggest that GRAZHDA*/sparsity outperforms other\nmethods. In fact, as shown in Table 6 and 7, GRAZHDA*/sparsity achieved a good balance between CO and SO and had the highest actual speedup overall, significantly outperforming all other previous methods. Note that as IdealApprox is only an approximation of the sparsest-cut, other methods can sometimes achieve better effesti ."}, {"heading": "6.3.1 THE EFFECT OF THE NUMBER OF CORES ON SPEEDUP", "text": "Figure 18 shows the speedup of the algorithms as the number of cores increased from 8 to 48. GRAZHDA*/sparsity outperformed consistently outperformed the other methods. The performance gap between the better methods (GRAZHDA*/sparsity, FAZHDA*, OZHDA*, DAHDA*) and the baseline ZHDA* increases with the number of the cores. This is because as the number of cores increases, communications overheads increases with the number of cores, and our new work distribution method successfully mitigates communications overhead."}, {"heading": "6.3.2 CLOUD ENVIRONMENT RESULTS", "text": "In addition to the 48 core cluster, we evaluated GRAZHDA*/sparsity on an Amazon EC2 cloud cluster with 128 virtual cores (vCPUs) and 480GB aggregated RAM (a cluster of 32 m1.xlarge EC2 instances, each with 4 vCPUs, 3.75 GB RAM/core. This is a less favorable environment for parallel search compared to a \u201cbare-metal\u201d cluster because physical processors are shared with other users and network performance is inconsistent (Iosup, Ostermann, Yigitbasi, Prodan, Fahringer, & Epema, 2011). We intentionally chose this configuration to evaluate work distribution methods in an environment which is significantly different from our other experiments. Table 8 shows that as with the smaller-scale cluster results, GRAZHDA*/sparsity outperformed other methods in this large-scale cloud environment."}, {"heading": "6.3.3 24-PUZZLE EXPERIMENTS", "text": "We evaluated GRAZHDA*/sparsity on the 24-puzzle using the same configuration as Section 4.1.2. Abstract feature generated by GRAZHDA*/sparsity is shown in Figure 19d. We compared GRAZHDA*/sparsity (automated abstract feature generation) vs. AZHDA* with the hand-crafted work distribution (HDA\u2217[Z ,Afeature ]) (Figure 8d) and HDA\u2217[Z ]. With 8 cores, the speedups were\n7.84 (GRAZHDA*/sparsity), 7.85 (HDA\u2217[Z ,Afeature ]), and 5.95 (HDA\u2217[Z ]). Thus, the completely automated GRAZHDA*/sparsity is competitive with a carefully hand-designed work distribution method. For the 15-puzzle, the partition generated by GRAZHDA*/sparsity exactly corresponds to the hand-crafted hash function of Figure 8b, so the performance is identical."}, {"heading": "6.3.4 EVALUATION OF PARALLEL SEARCH OVERHEADS AND PERFORMANCE IN LOW COMMUNICATIONS-COST ENVIRONMENTS", "text": "In previous experiments, we compared work distribution functions using domain-specific solvers with very fast node generation rates (Section 4.1), as well as domain-independent planning using a fast heuristic function (Section 6.3). Next, we evaluate search overheads and performance when node generation rates are low due to expensive node evaluations. In such domains, the impact of communications overheads is minimal because overheads for queue insertion, buffering, etc. are\nnegligible compared to the computation costs associated with node generation and evaluation. As a consequence, search overhead is the dominant factor which determines search performance.\nIn particular, we evaluate different parallel work distribution strategies when applied to domainindependent planning using the landmark-cut (LM-cut) heuristic, a state-of-the-art heuristic which is a relatively expensive heuristic. While there is no dominance relationship among planners using cheap heuristics such as merge&shrink heuristics (which require only a table lookup during search) and expensive heuristics such as LM-cut, recent work in forward-search based planning has focused on heuristics which tend to be slow, such as heuristics that require the solution of a linear program at every search node (Pommerening, Ro\u0308ger, Helmert, & Bonet, 2014; Imai & Fukunaga, 2015), so parallel strategies that focus on minimizing search overheads is of practical importance. Previous evaluations of parallel work distribution strategies in domain-independent planning used relatively fast heuristics. Kishimoto et al. (2013), as well as Jinnai and Fukunaga (2016a, 2016b) used merge&shrink abstraction based heuristics. Zhou and Hansen (2007) and Burns et al. (2010)used the max-pair heuristic (Haslum & Geffner, 2000).Thus, this is the first evaluation of parallel forward search for domain-independent planning using an expensive heuristic.\nTo evaluate the effect of SO and CO with the LM-cut heuristic, we compared the performance of ZHDA*, DAHDA*, and GRAZHDA*/sparsity as representatives of methods which optimize SO, CO, and both SO and CO, respectively. The instances used for this experiment are different from the experiments using merge&shrink (Table 7), because some of the instances used for the merge&shrink experiments were too easy to solve with LM-cut and not suitable for evaluating parallel algorithms. The average node expansion rate by sequential A* on the selected instances was 3886.02 node/sec. Compared to the expansion rate with merge&shrink heuristic used in Section 6.3 (56378.03 node/sec), the expansion rate is 14.5 times slower. Therefore, the relative cost of communication is expected to be smaller with LM-cut than merge&shrink heuristic.\nTable 9a shows the results on a single multicore machine with 8 cores. Overall, GRAZHDA*/sparsity outperformed ZHDA* and DAHDA*. Interestingly, although GRAZHDA*/sparsity has higher SO, it was still faster than ZHDA* because of lower CO. Even with this low communication cost environment, CO continues to be one of the major overhead for HDA*.\nTable 9b shows the results on a commodity cluster with 48 cores. As in the multicore environment, GRAZHDA*/sparsity outperformed ZHDA* and DAHDA*. However, the relative speedup of ZHDA* to GRAZHDA*/sparsity is higher with LM-cut (0.75) than with merge&shrink (0.66) (note that we used different instance set, so it may due to other factors). Some of the instances ( trucks9, visitall11-07-half) are too easy for a distributed environment, and therefore on these instances,\nhigh SO is incurred due to the burst effect (Section 3.1.2). Therefore, some of the instances have high SO even in ZHDA* where good LB is achieved."}, {"heading": "7. Conclusions", "text": "We investigated node distribution methods for HDA*, and showed that previous methods suffered from high communication overhead (HDA\u2217[Z ]), high search overhead (HDA\u2217[P ,Astate ]), or both (HDA\u2217[P ]), which limited their efficiency. We proposed Abstract Zobrist hashing, a new distribution method which combines the strengths of both Zobrist hashing and abstraction, and AZHDA* (HDA\u2217[Z ,Afeature ]), a new variant of HDA* which is based on AZH. Our experimental results showed that AZHDA* achieves a successful trade-off between communication and search overheads, resulting in better performance than previous work distribution methods with hand-crafted abstract features.\nWe then extended the investigation to automated, domain-independent approaches for generate work distribution. We formulated work distribution as graph partitioning, and proposed and validated effesti , a model of search and communication overheads for HDA* which can be used to predict the actual walltime efficiency. We proposed and evaluated GRAZHDA*, a new topdown approach to work distribution for parallel best-first search in the HDA* framework which approximate the optimal graph partitioning by partitioning domain transition graphs according to an objective function such as sparsity.\nWe experimentally showed that GRAZHDA*/sparsity significantly improves both estimated efficiency (effesti ) as well as the actual performance (walltime efficiency) compared to previous work distribution methods. Our results demonstrate the viability of approximating the partitioning of the entire search space by applying graph partitioning to an abstraction of the state space (i.e., the DTG). While our results indicate that sparsity works well as a partitioning objective for GRAHZDA*, it is possible that a different objective function might yield better results, since DTG-partitioning is only an approximation to GW partitioning. We have experimented with another objective MIN(CO+LB) objective, which minimizes (CO+LB), and found that the performance is comparable to sparsity. Investigation of other objective functions is a direction for future work.\nDespite significant improvements compared to previous work distribution approaches, there is room for improvement. The gap between the effesti metric for GRAZHDA* and an ideal model (IdealApprox) in Figure 17a represents the gap between actually partitioning the state space graph (as IdealApprox does) vs. the approximation obtained by the GRAZHDA* DTG partitioning. Closing this gap in effesti should lead to corresponding improvements in actual walltime efficiency, and poses challenges for future work. One possible approach to closing this gap is to partition a merged DTG which represents multiple SAS+ variables instead of partitioning a DTG of a single SAS+ variable. As merged DTGs have a richer representation of the state space graph, partitioning them using an objective function may result in a better approximation of the ideal partitioning. This approach is similar to merge-and-shrink heuristic (Helmert et al., 2014) which merging multiple DTGs into abstract state space to better estimate the state-space graph.\nIn this paper, we assumed identical distance between each two cores. However, communication costs vary among pairs of processors in distributed search, especially in cloud cluster environments. Furthermore, as the number of cores scales to thousands or tens of thousands or more, some consideration of core locality is likely to be necessary. Incorporating the technique to distribute nodes\nconsidering the locality of processors such as LOHA&QE (Mahapatra & Dutt, 1997) may further improve the performance.\nImplementing intra-node communications as interthread communication (OpenMP) is shown to improve the performance on a hash-based parallel suboptimal search (Vidal, Vernhes, & Infantes, 2012). The technique should also improve the performance of HDA*.\nDynamic adjustment of the partitioning on Structured Duplicate Detection has shown to be effective for external search (Zhou & Hansen, 2011). We may further improve the performance of HDA* by adjusting the hash function in the course of the search.\nFinally, GPU-based massively parallel search has recently been shown to be successful (Zhou & Zeng, 2015). Investigation of tradeoffs between communication and search overhead in a heterogeneous algorithm which seeks to effectively utilize all normal cores as well as GPU cores using a framework based on abstract feature-based hashing is a direction for future work."}, {"heading": "Appendix A. Dynamic AHDA* (DAHDA*), an improvement to AHDA* for distributed memory systems", "text": "This section presents an improvement to AHDA* (Burns et al., 2010). In our experiments, we used AHDA* as one of the baselines for evaluating our new AZHDA* strategies. The baseline implementation of AHDA* (HDA\u2217[Z ,Astate/SDD ]) is based on the greedy abstraction algorithm described in (Zhou & Hansen, 2006b), and selects a subset of DTGs (atom groups). The greedy abstraction algorithm adds one DTG to the abstract graph (G) at a time, choosing the DTG which minimizes the maximum out-degree of the abstract graph, until the graph size (# of nodes) reaches the threshold given by a parameter Nmax. PSDD requires a Nmax to be derived from the size of the available RAM. We found that AHDA* with a static Nmax threshold as in PSDD performed poorly for a benchmark set with varying difficulty because a fixed size abstract graph results in very poor load balance. While poor load balance can lead to low efficiency and poor performance, a bad choice for Nmax can be catastrophic when the system has a relatively small amount of RAM per core, as poor load balance causes concentrated memory usage in the overloaded processors, resulting in early memory exhaustion (i.e., AHDA* crashes because a thread/process which is allocated a large number of states exhausts its local heap).\nThe AHDA* results in Table 10 are for a 48-core cluster, 2GB/core, and uses Nmax = 102, 103, 104, 105, 106 nodes based on Fast-Downward (Helmert, 2006) using merge&shrink heuristic (Helmert et al., 2014). Smaller Nmax results in lower CO, but when Nmax is too small for the problem, load imbalance results in a concentration of the nodes and memory exhaustion. Although the total amount of RAM in current systems is growing, the amount of RAM per core has remained relatively small because the number of cores has also been increasing (and is expected to continue increasing). Thus, this is a significant issue with the straightforward implementation of AHDA* which uses a static Nmax. To avoid this problem, Nmax must be set dynamically according to the size of the state space for each instance. Thus, we implemented Dynamic AHDA* (DAHDA* = HDA\u2217[Z ,Astate/SDDdynamic ]), which dynamically set the size of the abstract graph according to the number of DTGs (the state space size is exponential in the # of DTGs). We set the threshold of the total number of features in the DTGs to be 30% of the total number of features in the problem instance (we tested 10%, 30%, 50%, and 70% and found that 30% performed best). Note that the threshold is relative to the number of features, not the state space size as in AHDA*, which is\nexponential in the # features. Therefore, DAHDA* tries to take into account of certain amount of features, whereas AHDA* sometimes use only a fraction of features."}, {"heading": "Appendix B. Experimental results with standard deviations", "text": "Cont. Table 9.\nInstance GAZHDA* OZHDA* [Z ,Afeauture/DTGgreedy ] [Zoperator ] speedup CO SO speedup CO SO Blocks10-0 21.81 (3.26) 0.99 (0.00) 0.12 (0.30) 15.47 (4.37) 0.98 (0.00) 0.34 (0.34) Blocks11-1 29.20 (3.22) 0.99 (0.00) 0.03 (0.16) 29.20 (4.99) 0.99 (0.00) 0.03 (0.21) Elevators08-5 29.35 (2.77) 0.65 (0.04) -0.00 (0.36) 21.86 (0.47) 0.09 (0.00) 0.44 (0.03) Elevators08-6 34.52 (4.09) 0.24 (0.00) -0.09 (0.00) 32.70 (2.96) 0.41 (0.00) 0.22 (0.03) Gripper8 21.86 (0.58) 0.81 (0.00) 0.06 (0.02) 24.77 (3.56) 0.98 (0.04) 0.14 (0.00) Logistics00-10-1 11.68 (0.95) 0.85 (0.00) 0.25 (0.00) 11.68 (2.14) 0.85 (0.00) 0.25 (0.05) Miconic11-0 13.15 (3.27) 0.53 (0.00) 0.24 (0.16) 37.86 (0.81) 0.02 (0.00) 0.02 (0.02) Miconic11-2 8.53 (0.97) 0.53 (0.00) 0.74 (0.16) 36.86 (0.65) 0.02 (0.00) 0.07 (0.01) NoMprime5 18.55 (0.69) 0.95 (0.00) -0.06 (0.01) 16.66 (0.44) 0.94 (0.00) 0.00 (0.02) Nomystery10 18.98 (4.04) 0.42 (0.00) -0.07 (0.06) 21.61 (1.44) 0.74 (0.00) 0.11 (0.04) Openstacks08-19 22.14 (1.19) 0.38 (0.01) 0.21 (0.05) 17.11 (1.28) 0.34 (0.00) 0.32 (0.13) Openstacks08-21 25.67 (0.82) 0.15 (0.00) 0.31 (0.04) 39.34 (0.52) 0.92 (0.00) 0.05 (0.11) Parcprinter11-11 16.85 (2.71) 0.74 (0.00) 0.41 (0.49) 15.98 (1.44) 0.82 (0.00) 0.56 (0.03) Parking11 28.43 (1.01) 0.98 (0.00) 0.02 (0.03) 26.76 (3.07) 0.97 (0.00) 0.07 (0.14) Pegsol11-18 16.22 (0.27) 0.77 (0.00) 0.05 (0.01) 26.17 (0.26) 0.34 (0.00) -0.03 (0.00) PipesNoTk10 15.58 (0.36) 0.98 (0.00) 0.01 (0.00) 15.22 (0.35) 0.98 (0.00) 0.02 (0.00) PipesTk12 19.84 (3.18) 0.99 (0.01) 0.01 (0.00) 21.40 (0.94) 0.88 (0.00) 0.04 (0.02) PipesTk17 26.64 (0.20) 0.98 (0.00) 0.00 (0.00) 28.82 (0.13) 0.88 (0.00) 0.00 (0.00) Rovers6 33.49 (1.01) 0.56 (0.00) 0.01 (0.02) 41.00 (2.13) 0.31 (0.00) 0.03 (0.02) Scanalyzer08-6 20.28 (2.22) 0.77 (0.00) 0.01 (0.00) 23.70 (1.53) 0.66 (0.00) 0.01 (0.00) Scanalyzer11-6 16.36 (3.89) 0.65 (0.00) 0.49 (0.16) 38.82 (1.64) 0.30 (0.00) 0.09 (0.01) Average 21.39 (1.94) 0.71 (0.00) 0.13 (0.10) 25.86 (1.67) 0.64 (0.00) 0.13 (0.06) Total walltime 398.75 (36.16) 331.18 (21.39) Instance DAHDA* ZHDA*\n[Z ,Astate/SDDdynamic ] [Z ] speedup CO SO speedup CO SO\nBlocks10-0 25.11 (4.89) 0.88 (0.00) 0.08 (0.05) 14.93 (4.05) 0.98 (0.00) 0.30 (0.25) Blocks11-1 24.88 (2.00) 0.91 (0.00) 0.21 (0.01) 27.98 (2.28) 0.98 (0.00) 0.07 (0.09) Elevators08-5 27.59 (4.07) 0.83 (0.01) -0.03 (0.05) 27.54 (2.72) 0.98 (0.01) -0.03 (0.03) Elevators08-6 15.28 (1.77) 0.88 (0.00) 0.31 (0.06) 18.19 (3.15) 0.96 (0.00) 0.06 (0.14) Gripper8 21.80 (2.92) 0.98 (0.04) 0.08 (0.05) 21.66 (3.42) 0.98 (0.01) 0.08 (0.03) Logistics00-10-1 17.52 (0.80) 0.84 (0.00) 0.00 (0.00) 16.09 (0.56) 0.99 (0.00) 0.00 (0.02) Miconic11-0 46.05 (0.87) 0.01 (0.00) 0.08 (0.01) 7.40 (2.74) 0.96 (0.00) 0.13 (0.04) Miconic11-2 33.81 (1.35) 0.01 (0.00) 0.18 (0.00) 14.67 (2.65) 0.96 (0.00) 0.05 (0.06) NoMprime5 18.46 (0.59) 0.90 (0.00) -0.05 (0.01) 16.63 (0.57) 0.98 (0.00) -0.02 (0.01) Nomystery10 28.41 (2.29) 0.60 (0.00) -0.07 (0.10) 21.68 (3.30) 0.99 (0.00) -0.07 (0.22) Openstacks08-19 24.54 (1.05) 0.24 (0.00) 0.18 (0.03) 25.99 (3.40) 0.99 (0.00) -0.05 (0.19) Openstacks08-21 26.72 (1.06) 0.13 (0.00) 0.28 (0.05) 39.06 (2.71) 0.92 (0.00) -0.00 (0.12) Parcprinter11-11 7.00 (2.91) 0.19 (0.01) 4.38 (1.54) 19.15 (2.95) 0.97 (0.00) 0.08 (0.16) Parking11 28.84 (0.82) 0.52 (0.00) 0.07 (0.02) 27.09 (3.55) 0.98 (0.00) 0.04 (0.16) Pegsol11-18 22.16 (0.83) 0.34 (0.00) -0.01 (0.02) 16.97 (1.05) 0.98 (0.00) 0.03 (0.03) PipesNoTk10 15.58 (0.46) 0.98 (0.00) 0.01 (0.00) 11.22 (0.38) 0.98 (0.00) 0.03 (0.00) PipesTk12 25.12 (0.31) 0.67 (0.00) 0.00 (0.00) 19.78 (0.36) 0.98 (0.00) 0.00 (0.00) PipesTk17 31.16 (0.58) 0.60 (0.00) 0.01 (0.00) 26.27 (4.15) 0.98 (0.01) 0.00 (0.00) Rovers6 25.48 (2.86) 0.05 (0.00) 0.26 (0.07) 30.01 (2.50) 0.76 (0.00) 0.00 (0.07) Scanalyzer08-6 21.23 (2.62) 0.94 (0.00) 0.00 (0.00) 16.54 (0.43) 0.98 (0.00) 0.01 (0.00) Scanalyzer11-6 19.51 (3.55) 0.50 (0.00) 0.46 (0.14) 20.36 (0.66) 0.98 (0.00) 0.05 (0.01) Average 24.11 (1.84) 0.57 (0.00) 0.31 (0.11) 20.53 (2.27) 0.96 (0.00) 0.01 (0.08) Total walltime 377.86 (28.85) 433.23 (47.90)"}], "references": [{"title": "Tiebreaking strategies for a* search: How to explore the final frontier", "author": ["M. Asai", "A. Fukunaga"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Asai and Fukunaga,? \\Q2016\\E", "shortCiteRegEx": "Asai and Fukunaga", "year": 2016}, {"title": "Complexity results for SAS+ planning", "author": ["C. B\u00e4ckstr\u00f6m", "B. Nebel"], "venue": "Computational Intelligence,", "citeRegEx": "B\u00e4ckstr\u00f6m and Nebel,? \\Q1995\\E", "shortCiteRegEx": "B\u00e4ckstr\u00f6m and Nebel", "year": 1995}, {"title": "Recent advances in graph partitioning", "author": ["A. Buluc", "H. Meyerhenke", "I. Safro", "P. Sanders", "C. Schulz"], "venue": "arXiv preprint arXiv:1311.3144", "citeRegEx": "Buluc et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Buluc et al\\.", "year": 2015}, {"title": "Best-first heuristic search for multicore machines", "author": ["E. Burns", "S. Lemons", "W. Ruml", "R. Zhou"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Burns et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Burns et al\\.", "year": 2010}, {"title": "Implementing fast heuristic search code", "author": ["E.A. Burns", "M. Hatem", "M.J. Leighton", "W. Ruml"], "venue": "In Proceedings of the Annual Symposium on Combinatorial Search,", "citeRegEx": "Burns et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Burns et al\\.", "year": 2012}, {"title": "Planning with pattern databases", "author": ["S. Edelkamp"], "venue": "In European Conference on Planning (ECP),", "citeRegEx": "Edelkamp,? \\Q2001\\E", "shortCiteRegEx": "Edelkamp", "year": 2001}, {"title": "A scalable concurrent malloc (3) implementation for FreeBSD", "author": ["J. Evans"], "venue": "In Proc. BSDCan Conference", "citeRegEx": "Evans,? \\Q2006\\E", "shortCiteRegEx": "Evans", "year": 2006}, {"title": "PRA*: Massively parallel heuristic search", "author": ["M. Evett", "J. Hendler", "A. Mahanti", "D. Nau"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "Evett et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Evett et al\\.", "year": 1995}, {"title": "A linear-time heuristic for improving network partitions", "author": ["C.M. Fiduccia", "R.M. Mattheyses"], "venue": "In Conference on Design Automation,", "citeRegEx": "Fiduccia and Mattheyses,? \\Q1982\\E", "shortCiteRegEx": "Fiduccia and Mattheyses", "year": 1982}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["P.E. Hart", "N.J. Nilsson", "B. Raphael"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "Hart et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hart et al\\.", "year": 1968}, {"title": "Admissible heuristics for optimal planning", "author": ["P. Haslum", "H. Geffner"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Haslum and Geffner,? \\Q2000\\E", "shortCiteRegEx": "Haslum and Geffner", "year": 2000}, {"title": "The Fast Downward planning system", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Helmert,? \\Q2006\\E", "shortCiteRegEx": "Helmert", "year": 2006}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Helmert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Helmert et al\\.", "year": 2007}, {"title": "Merge-and-shrink abstraction: A method for generating lower bounds in factored state spaces", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann", "R. Nissim"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Helmert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Helmert et al\\.", "year": 2014}, {"title": "Graph partitioning models for parallel computing", "author": ["B. Hendrickson", "T.G. Kolda"], "venue": "Parallel computing,", "citeRegEx": "Hendrickson and Kolda,? \\Q2000\\E", "shortCiteRegEx": "Hendrickson and Kolda", "year": 2000}, {"title": "A stack-slicing algorithm for multi-core model checking", "author": ["G.J. Holzmann"], "venue": "Electronic Notes in Theoretical Computer Science,", "citeRegEx": "Holzmann,? \\Q2008\\E", "shortCiteRegEx": "Holzmann", "year": 2008}, {"title": "The design of a multicore extension of the SPIN model checker", "author": ["G.J. Holzmann", "D. Bo\u015dna\u0109ki"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "Holzmann and Bo\u015dna\u0109ki,? \\Q2007\\E", "shortCiteRegEx": "Holzmann and Bo\u015dna\u0109ki", "year": 2007}, {"title": "On a practical, integer-linear programming model for delete-free tasks and its use as a heuristic for cost-optimal planning", "author": ["T. Imai", "A. Fukunaga"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Imai and Fukunaga,? \\Q2015\\E", "shortCiteRegEx": "Imai and Fukunaga", "year": 2015}, {"title": "Performance analysis of cloud computing services for many-tasks scientific computing", "author": ["A. Iosup", "S. Ostermann", "M.N. Yigitbasi", "R. Prodan", "T. Fahringer", "D.H. Epema"], "venue": "IEEE Transactions on Parallel and Distributed Systems,", "citeRegEx": "Iosup et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Iosup et al\\.", "year": 2011}, {"title": "Parallel A* and AO* algorithms: An optimality criterion and performance evaluation", "author": ["K. Irani", "Y. Shih"], "venue": "In International Conference on Parallel Processing,", "citeRegEx": "Irani and Shih,? \\Q1986\\E", "shortCiteRegEx": "Irani and Shih", "year": 1986}, {"title": "Parallel external directed model checking with linear I/O. In Verification, Model Checking, and Abstract Interpretation", "author": ["S. Jabbar", "S. Edelkamp"], "venue": "7th International Conference,", "citeRegEx": "Jabbar and Edelkamp,? \\Q2006\\E", "shortCiteRegEx": "Jabbar and Edelkamp", "year": 2006}, {"title": "Abstract Zobrist hash: An efficient work distribution method for parallel best-first search", "author": ["Y. Jinnai", "A. Fukunaga"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Jinnai and Fukunaga,? \\Q2016\\E", "shortCiteRegEx": "Jinnai and Fukunaga", "year": 2016}, {"title": "Automated creation of efficient work distribution functions for parallel best-first search", "author": ["Y. Jinnai", "A. Fukunaga"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Jinnai and Fukunaga,? \\Q2016\\E", "shortCiteRegEx": "Jinnai and Fukunaga", "year": 2016}, {"title": "State-variable planning under structural restrictions: Algorithms and complexity", "author": ["P. Jonsson", "C. B\u00e4ckstr\u00f6m"], "venue": "Artificial Intelligence,", "citeRegEx": "Jonsson and B\u00e4ckstr\u00f6m,? \\Q1998\\E", "shortCiteRegEx": "Jonsson and B\u00e4ckstr\u00f6m", "year": 1998}, {"title": "Measuring and understanding throughput of network topologies", "author": ["S.A. Jyothi", "A. Singla", "P. Godfrey", "A. Kolla"], "venue": "arXiv preprint arXiv:1402.2531", "citeRegEx": "Jyothi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jyothi et al\\.", "year": 2014}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on scientific Computing,", "citeRegEx": "Karypis and Kumar,? \\Q1998\\E", "shortCiteRegEx": "Karypis and Kumar", "year": 1998}, {"title": "Evaluation of a simple, scalable, parallel bestfirst search strategy", "author": ["A. Kishimoto", "A. Fukunaga", "A. Botea"], "venue": "Artificial Intelligence,", "citeRegEx": "Kishimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kishimoto et al\\.", "year": 2013}, {"title": "Scalable, parallel best-first search for optimal sequential planning", "author": ["A. Kishimoto", "A.S. Fukunaga", "A. Botea"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Kishimoto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kishimoto et al\\.", "year": 2009}, {"title": "Evaluations of Hash Distributed A* in optimal sequence alignment", "author": ["Y. Kobayashi", "A. Kishimoto", "O. Watanabe"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Kobayashi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2011}, {"title": "Depth-first iterative deepening: An optimal admissible tree search", "author": ["R. Korf"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf,? \\Q1985\\E", "shortCiteRegEx": "Korf", "year": 1985}, {"title": "Disjoint pattern database heuristics", "author": ["R.E. Korf", "A. Felner"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf and Felner,? \\Q2002\\E", "shortCiteRegEx": "Korf and Felner", "year": 2002}, {"title": "Large-scale parallel breadth-first search", "author": ["R.E. Korf", "P. Schultze"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Korf and Schultze,? \\Q2005\\E", "shortCiteRegEx": "Korf and Schultze", "year": 2005}, {"title": "Parallel best-first search of state-space graphs: A summary of results", "author": ["V. Kumar", "K. Ramesh", "V.N. Rao"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Kumar et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 1988}, {"title": "Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms", "author": ["T. Leighton", "S. Rao"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Leighton and Rao,? \\Q1999\\E", "shortCiteRegEx": "Leighton and Rao", "year": 1999}, {"title": "Scalable global and local hashing strategies for duplicate pruning in parallel A* graph search", "author": ["N.R. Mahapatra", "S. Dutt"], "venue": "IEEE Transactions on Parallel and Distributed Systems,", "citeRegEx": "Mahapatra and Dutt,? \\Q1997\\E", "shortCiteRegEx": "Mahapatra and Dutt", "year": 1997}, {"title": "Sequential and parallel algorithms for frontier A* with delayed duplicate detection", "author": ["R. Niewiadomski", "J.N. Amaral", "R.C. Holte"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Niewiadomski et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Niewiadomski et al\\.", "year": 2006}, {"title": "Heuristics - Intelligent Search Strategies for Computer Problem Solving. Addison\u2013 Wesley", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1984\\E", "shortCiteRegEx": "Pearl", "year": 1984}, {"title": "Rapid and sensitive sequence comparison with FASTP and FASTA", "author": ["W.R. Pearson"], "venue": "Methods in enzymology,", "citeRegEx": "Pearson,? \\Q1990\\E", "shortCiteRegEx": "Pearson", "year": 1990}, {"title": "PA*SE: Parallel A* for slow expansions", "author": ["M. Phillips", "M. Likhachev", "S. Koenig"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Phillips et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Phillips et al\\.", "year": 2014}, {"title": "LP-based heuristics for costoptimal planning", "author": ["F. Pommerening", "G. R\u00f6ger", "M. Helmert", "B. Bonet"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Pommerening et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pommerening et al\\.", "year": 2014}, {"title": "Transposition table driven work scheduling in distributed search", "author": ["J.W. Romein", "A. Plaat", "H.E. Bal", "J. Schaeffer"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Romein et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Romein et al\\.", "year": 1999}, {"title": "BAliBASE 3.0: Latest developments of the multiple sequence alignment benchmark. Proteins: Structure", "author": ["J.D. Thompson", "P. Koehl", "R. Ripp", "O. Poch"], "venue": "Function and Genetics (PROTEINS),", "citeRegEx": "Thompson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Thompson et al\\.", "year": 2005}, {"title": "Parallel AI planning on the SCC", "author": ["V. Vidal", "S. Vernhes", "G. Infantes"], "venue": "In 4th Many-core Applications Research Community (MARC) Symposium,", "citeRegEx": "Vidal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2012}, {"title": "Structured duplicate detection in external-memory graph search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zhou and Hansen,? \\Q2004\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2004}, {"title": "Breadth-first heuristic search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "Artificial Intelligence,", "citeRegEx": "Zhou and Hansen,? \\Q2006\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2006}, {"title": "Domain-independent structured duplicate detection", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zhou and Hansen,? \\Q2006\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2006}, {"title": "Parallel structured duplicate detection", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zhou and Hansen,? \\Q2007\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2007}, {"title": "Dynamic state-space partitioning in external-memory graph search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Zhou and Hansen,? \\Q2011\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2011}, {"title": "Massively parallel A* search on a GPU", "author": ["Y. Zhou", "J. Zeng"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zhou and Zeng,? \\Q2015\\E", "shortCiteRegEx": "Zhou and Zeng", "year": 2015}, {"title": "A new hashing method with application for game playing", "author": ["YUU", "JINNAI", "ALEX", "A.L. FUKUNAGA Zobrist"], "venue": "reprinted in International Computer Chess Association Journal (ICCA),", "citeRegEx": "YUU et al\\.,? \\Q1970\\E", "shortCiteRegEx": "YUU et al\\.", "year": 1970}], "referenceMentions": [{"referenceID": 3, "context": "Zobrist hashing incurs a heavy communication overhead because many nodes are assigned to processes that are different from their parents, and proposed AHDA*, which used an abstraction-based hash function originally designed for use with PSDD (Zhou & Hansen, 2007) and PBNF (Burns et al., 2010).", "startOffset": 273, "endOffset": 293}, {"referenceID": 3, "context": "Finally, Table 10 in Appendix A shows new experimental results comparing DAHDA* vs AHDA* (Burns et al., 2010) which were not included in the conference paper which introduced DAHDA* (Jinnai & Fukunaga, 2016b).", "startOffset": 89, "endOffset": 109}, {"referenceID": 3, "context": "Even when a parallel search itself does not require synchronization, coordination overhead can be incurred due to contention for the memory bus (Burns et al., 2010; Kishimoto et al., 2013).", "startOffset": 144, "endOffset": 188}, {"referenceID": 26, "context": "Even when a parallel search itself does not require synchronization, coordination overhead can be incurred due to contention for the memory bus (Burns et al., 2010; Kishimoto et al., 2013).", "startOffset": 144, "endOffset": 188}, {"referenceID": 3, "context": "However, concurrent access to the shared open list becomes a bottleneck and inherently limits the scalability of this approach unless the cost of expanding each node is extremely expensive, even if lock-free data structures are used (Burns et al., 2010).", "startOffset": 233, "endOffset": 253}, {"referenceID": 32, "context": "The simplest approach is a randomized strategy which sends generated states to a randomly selected neighbor processes (Kumar et al., 1988).", "startOffset": 118, "endOffset": 138}, {"referenceID": 3, "context": "However, synchronous node sending was shown to degrade performance on domains with fast node expansion, such as grid pathfinding and sliding-tile puzzle (Burns et al., 2010).", "startOffset": 153, "endOffset": 173}, {"referenceID": 36, "context": "While the use of abstractions as the basis for heuristic functions has a long history (Pearl, 1984), the use of abstractions as a mechanism for partitioning search states originated in Structured Duplicate Detection (SDD), an external memory search which stores explored states on disk (Zhou & Hansen, 2004).", "startOffset": 86, "endOffset": 99}, {"referenceID": 3, "context": "first heuristic search (Zhou & Hansen, 2006a), Parallel Best-NBlocks First (PBNF) (Burns et al., 2010) extends PSDD to best-first search on multicore machine by ensuring that n-blocks with the best current f -values are assigned to processors.", "startOffset": 82, "endOffset": 102}, {"referenceID": 15, "context": "Stack-slicing projects states to their path costs to achieve efficient communication in depth-first search (Holzmann, 2008), and is useful in domains with levelled graphs, where each state can be reached only by a unique path cost, such as model checking (Holzmann & Bo\u015dna\u0109ki, 2007) (thus enabling dupicate detection).", "startOffset": 107, "endOffset": 123}, {"referenceID": 3, "context": "first heuristic search (Zhou & Hansen, 2006a), Parallel Best-NBlocks First (PBNF) (Burns et al., 2010) extends PSDD to best-first search on multicore machine by ensuring that n-blocks with the best current f -values are assigned to processors. Since livelock is possible in PBNF on domains with infinite state spaces, Burns et al proposed SafePBNF, a livelock-free version of PBNF (2010). Burns et al (2010) also proposed AHDA*, a variant of HDA* which uses an abstraction-based node distribution function.", "startOffset": 83, "endOffset": 388}, {"referenceID": 3, "context": "first heuristic search (Zhou & Hansen, 2006a), Parallel Best-NBlocks First (PBNF) (Burns et al., 2010) extends PSDD to best-first search on multicore machine by ensuring that n-blocks with the best current f -values are assigned to processors. Since livelock is possible in PBNF on domains with infinite state spaces, Burns et al proposed SafePBNF, a livelock-free version of PBNF (2010). Burns et al (2010) also proposed AHDA*, a variant of HDA* which uses an abstraction-based node distribution function.", "startOffset": 83, "endOffset": 408}, {"referenceID": 26, "context": "Hash Distributed A* (HDA*) (Kishimoto et al., 2013) is a parallel A* algorithm which incorporates the idea of hash-based work distribution from PRA* (Evett et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 7, "context": ", 2013) is a parallel A* algorithm which incorporates the idea of hash-based work distribution from PRA* (Evett et al., 1995) and asynchronous communication from TDS (Romein et al.", "startOffset": 105, "endOffset": 125}, {"referenceID": 40, "context": ", 1995) and asynchronous communication from TDS (Romein et al., 1999).", "startOffset": 48, "endOffset": 69}, {"referenceID": 26, "context": "Kishimoto et al. (2009, 2013) noted that it was desirable to use a hash function which uniformly distributed nodes among processors, and used the Zobrist hash function (1970), described below.", "startOffset": 0, "endOffset": 175}, {"referenceID": 21, "context": "Although Jinnai and Fukunaga showed that OZHDA* reduces communication overhead compared to Zobrist hashing (2016b), it may result in increased search overhead compared to HDA\u2217[Z ](the extent of which is unpredictable).", "startOffset": 9, "endOffset": 115}, {"referenceID": 3, "context": "The abstraction strategy in AHDA* applies the state space partitioning technique used in PBNF (Burns et al., 2010) and PSDD (Zhou & Hansen, 2007), which projects nodes in the state space to abstract states.", "startOffset": 94, "endOffset": 114}, {"referenceID": 3, "context": "In order to minimize communication overhead in HDA*, Burns et al. (2010) proposed AHDA*, which uses abstraction based node assignment.", "startOffset": 53, "endOffset": 73}, {"referenceID": 3, "context": "In order to minimize communication overhead in HDA*, Burns et al. (2010) proposed AHDA*, which uses abstraction based node assignment. The abstraction strategy in AHDA* applies the state space partitioning technique used in PBNF (Burns et al., 2010) and PSDD (Zhou & Hansen, 2007), which projects nodes in the state space to abstract states. After mapping states to abstract states, the AHDA* implementation by Burns et al. (2010) assigns abstract states to processors using a perfect hashing and a modulus operator.", "startOffset": 53, "endOffset": 431}, {"referenceID": 3, "context": "The AHDA* implementation by Burns et al. (2010) implemented the hashing strategy using a perfect hashing and a modulus operator, and an abstraction strategy following the construction for SDD (Zhou & Hansen, 2006b) (for domain-independent planning), or a hand-crafted abstraction (for the sliding tiles puzzle and grid path-finding domains).", "startOffset": 28, "endOffset": 48}, {"referenceID": 3, "context": "The AHDA* implementation by Burns et al. (2010) implemented the hashing strategy using a perfect hashing and a modulus operator, and an abstraction strategy following the construction for SDD (Zhou & Hansen, 2006b) (for domain-independent planning), or a hand-crafted abstraction (for the sliding tiles puzzle and grid path-finding domains). Note that an abstraction strategy can itself be seen as a type of hashing strategy, but in this paper, we make the distinction between the method used to project states onto some cluster of states (abstraction) and methods which are used to map states (or abstract states) to processors (hashing). Jinnai and Fukunaga (2016b) showed that AHDA* with a static Nmax threshold performed poorly for a benchmark set with varying difficulty because a fixed size abstract graph results in very poor load balance, and implemented Dynamic AHDA* (DAHDA*) which dynamically sets the size of the abstract graph according to the number of features (the state space size is exponential in the number of features).", "startOffset": 28, "endOffset": 668}, {"referenceID": 3, "context": "For example, we denote AHDA* (Burns et al., 2010) using a perfect hashing and a hand-crafted abstraction as HDA\u2217[P ,Astate ], and AHDA* using a perfect hashing and a SDD abstraction as HDA\u2217[P ,Astate/SDD ].", "startOffset": 29, "endOffset": 49}, {"referenceID": 26, "context": "Kishimoto et al. previously analyzed search overhead for HDA\u2217[Z ] (2013). They measuredR<, R=, andR>, the fraction of expanded nodes with f < f\u2217, f = f\u2217, and f > f\u2217 (where f\u2217 is optimal cost), respectively.", "startOffset": 0, "endOffset": 73}, {"referenceID": 27, "context": "4] (Kishimoto et al., 2009)", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "4] (Burns et al., 2010) HDA\u2217[P ,Astate ] AHDA* with perfect hashing and state-based abstraction [Sec 2.", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "5] (Burns et al., 2010)", "startOffset": 3, "endOffset": 23}, {"referenceID": 28, "context": "3) (Kobayashi et al., 2011)", "startOffset": 3, "endOffset": 27}, {"referenceID": 27, "context": "2] (Kishimoto et al., 2009)", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "4] trivial variant of HDA\u2217[P ,Astate/SDD ], which was ussed for classical planning in (Burns et al., 2010); uses Zobristbased hashing instead of perfect hashing.", "startOffset": 86, "endOffset": 106}, {"referenceID": 3, "context": "Burns et al. analyzed the quality of nodes expanded by SafePBNF and HDA\u2217[P ,Astate ] by comparing the number of nodes expanded according to their f values, and showed that HDA\u2217[P ,Astate ] expands nodes with larger f value (lower quality nodes) compared to SafePBNF (2010).", "startOffset": 0, "endOffset": 273}, {"referenceID": 3, "context": "In addition, previous work has not directly compared HDA\u2217[Z ] and SafePBNF, as Burns et al. (2010) compared SafePBNF to \u2014HDA\u2217[P ,Astate ]", "startOffset": 79, "endOffset": 99}, {"referenceID": 9, "context": "Although the traditional definition of A* (Hart et al., 1968) specifies that nodes are expanded in order of nondecreasing f -value (i.", "startOffset": 42, "endOffset": 61}, {"referenceID": 3, "context": "33 GHz CPU with 16 GB RAM, using a 15-puzzle solver based on the solver code used in the work of Burns et al. (2010). We recorded the order in which states were expanded.", "startOffset": 97, "endOffset": 117}, {"referenceID": 3, "context": "This requires communication and coordination overhead, which increases the walltime by about <10% of the time on the 15-puzzle (Burns et al., 2010).", "startOffset": 127, "endOffset": 147}, {"referenceID": 28, "context": "(Kobayashi et al., 2011) analyzed node reexpansion on multiple sequence alignment which HDA\u2217[Z ] suffers from high node duplication rate.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "For SafePBNF, we used the configuration used in (Burns et al., 2010).", "startOffset": 48, "endOffset": 68}, {"referenceID": 3, "context": "HDA\u2217[P ] is an instance of HDA* which is called \u201cHDA*\u201d in the work of Burns et al. (2010). Unlike the original HDA* by Kishimoto et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 3, "context": "HDA\u2217[P ] is an instance of HDA* which is called \u201cHDA*\u201d in the work of Burns et al. (2010). Unlike the original HDA* by Kishimoto et al. (2009), which uses Zobrist hashing, HDA\u2217[P ] uses a perfect hashing scheme which maps permutations (tile positions) to lexicographic indices (thread IDs) by Korf and Schultze (2005).", "startOffset": 70, "endOffset": 143}, {"referenceID": 3, "context": "HDA\u2217[P ] is an instance of HDA* which is called \u201cHDA*\u201d in the work of Burns et al. (2010). Unlike the original HDA* by Kishimoto et al. (2009), which uses Zobrist hashing, HDA\u2217[P ] uses a perfect hashing scheme which maps permutations (tile positions) to lexicographic indices (thread IDs) by Korf and Schultze (2005). A perfect hashing scheme computes a unique mapping from permutations (abstract state encoding) to lexicographic indices (thread ID)4.", "startOffset": 70, "endOffset": 318}, {"referenceID": 3, "context": "Previous work compared HDA\u2217[P ], HDA\u2217[P ,Astate ], and SafePBNF on the 15-puzzle and grid pathfinding problems (Burns et al., 2010).", "startOffset": 111, "endOffset": 131}, {"referenceID": 4, "context": "However, it has been shown that a bucket implementation (O(1) for all operations) results in significantly faster performance on state-of-the-art A* implementations (Burns et al., 2012).", "startOffset": 165, "endOffset": 185}, {"referenceID": 3, "context": "For SafePBNF we used the same configuration used in previous work (Burns et al., 2010).", "startOffset": 66, "endOffset": 86}, {"referenceID": 3, "context": "However, it has been shown that a bucket implementation (O(1) for all operations) results in significantly faster performance on state-of-the-art A* implementations (Burns et al., 2012). Therefore, we revisit the comparison of HDA* and SafePBNF by (1) using Zobrist hashing for HDA* (i.e., HDA\u2217[Z ]) in order to minimize search overhead (2) using both easy instances (solvable in < 1 second) and hard instances (requiring up to 1000 seconds to solve with sequential A*) of the sliding tiles and grid path-finding domains in order to isolate the startup costs associated with the burst effect, and (3) using both bucket and heap implementations of the open list in order to isolate the effect of data structure efficiency (as opposed to search efficiency). For the 15-puzzle, we used the standard set of 100 instances by Korf (1985), and used the Manhattan Distance heuristic.", "startOffset": 166, "endOffset": 832}, {"referenceID": 3, "context": "In addition to the original implementation of AHDA* (Burns et al., 2010), which distributes abstract states using a perfect hashing (HDA\u2217[P ,Astate ]), we implemented HDA\u2217[Z ,Astate ] which uses Zobrist hashing to distribute.", "startOffset": 52, "endOffset": 72}, {"referenceID": 27, "context": "\u2022 HDA\u2217[Z ,Afeature ]: HDA* using AZH \u2022 HDA\u2217[Z ]: HDA* using Zobrist hashing (Kishimoto et al., 2009)", "startOffset": 76, "endOffset": 100}, {"referenceID": 3, "context": "\u2022 HDA\u2217[P ,Astate ]: HDA* using Abstraction based work distribution (Burns et al., 2010) \u2022 HDA\u2217[P ]: HDA* using a perfect hash function (Burns et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 3, "context": ", 2010) \u2022 HDA\u2217[P ]: HDA* using a perfect hash function (Burns et al., 2010)", "startOffset": 55, "endOffset": 75}, {"referenceID": 6, "context": "2), using the Pthreads library, try lock for asynchronous communication, and the Jemalloc memory allocator (Evans, 2006).", "startOffset": 107, "endOffset": 120}, {"referenceID": 4, "context": "We implemented the open list as a 2-level bucket (Burns et al., 2012) for the 15-puzzle and 24puzzle, and a binary heap for MSA (binary heap was faster for MSA).", "startOffset": 49, "endOffset": 69}, {"referenceID": 3, "context": "In addition to HDA\u2217[Z ,Afeature ], HDA\u2217[Z ], and HDA\u2217[P ,Astate ], we also evaluated SafePBNF (Burns et al., 2010) and HDA\u2217[P ].", "startOffset": 94, "endOffset": 114}, {"referenceID": 4, "context": "(2010), while the code used in this section incorporated all of the enhancements from their more recent work on efficient sliding tile solver code (Burns et al., 2012) 7.", "startOffset": 147, "endOffset": 167}, {"referenceID": 3, "context": "1, the code is based on the code used in the work of Burns et al. (2010), while the code used in this section incorporated all of the enhancements from their more recent work on efficient sliding tile solver code (Burns et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 29, "context": "9 (the average solution length in th epreious work by Korf and Felner (2002) was 100.", "startOffset": 54, "endOffset": 77}, {"referenceID": 37, "context": "Edge costs are based on the PAM250 matrix score with gap penalty 8 (Pearson, 1990).", "startOffset": 67, "endOffset": 82}, {"referenceID": 28, "context": "We also evaluated the performance of Hyperplane Work Distribution (Kobayashi et al., 2011).", "startOffset": 66, "endOffset": 90}, {"referenceID": 28, "context": "We also evaluated the performance of Hyperplane Work Distribution (Kobayashi et al., 2011). HDA\u2217[Z ] suffers from node reexpansion in non-unit cost domains such as MSA. Hyperplane work distribution seeks to reduce node reexpansions by mapping the n-dimension grid to hyperplanes (denoted as HDA\u2217[Hyperplane]). For HDA\u2217[Hyperplane], we determined the plane thickness d using the tuning method by Kobayashi et al. (2011) where \u03bb = 0.", "startOffset": 67, "endOffset": 419}, {"referenceID": 3, "context": "5) (Burns et al., 2010) addressed ZHDA* HDA\u2217[Z ] not optimized (Sec.", "startOffset": 3, "endOffset": 23}, {"referenceID": 27, "context": "4) (Kishimoto et al., 2009) addressed", "startOffset": 3, "endOffset": 27}, {"referenceID": 26, "context": "For HDA\u2217[Z ], automated domain-independent feature generation for classical planning problems represented in the SAS+ representation (B\u00e4ckstr\u00f6m & Nebel, 1995) is straightforward (Kishimoto et al., 2013).", "startOffset": 178, "endOffset": 202}, {"referenceID": 3, "context": "Burns et al. used the greedy abstraction algorithm by Zhou and Hansen (2006b) to select the subset of features, which we refer to as SDD abstraction.", "startOffset": 0, "endOffset": 78}, {"referenceID": 5, "context": "GreedyAFG first identifies atom groups (Edelkamp, 2001) and its domain transition graph (DTG).", "startOffset": 39, "endOffset": 55}, {"referenceID": 3, "context": "memory bus contention) that affect performance (Burns et al., 2010; Kishimoto et al., 2013), but we assume that CO and SO are the dominant factors in determining efficiency.", "startOffset": 47, "endOffset": 91}, {"referenceID": 26, "context": "memory bus contention) that affect performance (Burns et al., 2010; Kishimoto et al., 2013), but we assume that CO and SO are the dominant factors in determining efficiency.", "startOffset": 47, "endOffset": 91}, {"referenceID": 3, "context": "\u2022 DAHDA*: HDA\u2217[Z ,Astate/SDDdynamic ], AHDA* (Burns et al., 2010) with dynamic abstraction size threshold (Appendix A).", "startOffset": 45, "endOffset": 65}, {"referenceID": 26, "context": "\u2022 ZHDA*: HDA\u2217[Z ], HDA* using Zobrist hashing (Kishimoto et al., 2013) (Sec.", "startOffset": 46, "endOffset": 70}, {"referenceID": 13, "context": "We implemented these HDA* variants on top of the Fast Downward classical planner using the merge&shrink heuristic (Helmert et al., 2014) (abstraction size =1000).", "startOffset": 114, "endOffset": 136}, {"referenceID": 40, "context": "We packed 100 states per MPI message in order to reduce the number of messages (Romein et al., 1999).", "startOffset": 79, "endOffset": 100}, {"referenceID": 3, "context": "memory bus contention) which affect performance (Burns et al., 2010; Kishimoto et al., 2013).", "startOffset": 48, "endOffset": 92}, {"referenceID": 26, "context": "memory bus contention) which affect performance (Burns et al., 2010; Kishimoto et al., 2013).", "startOffset": 48, "endOffset": 92}, {"referenceID": 3, "context": "AHDA* (Burns et al., 2010) (Section 2.", "startOffset": 6, "endOffset": 26}, {"referenceID": 13, "context": "We used CGL-B (CausalGraph-Goal-Level&Bisimulation) merge&shrink heuristic (Helmert et al., 2014), which is more efficient and recently proposed than LFPA merge&shrink (Helmert et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 12, "context": ", 2014), which is more efficient and recently proposed than LFPA merge&shrink (Helmert et al., 2007) used in a previous conference paper which evaluated GAZHDA* and FAZHDA* (Jinnai & Fukunaga, 2016b).", "startOffset": 78, "endOffset": 100}, {"referenceID": 9, "context": "While there is no dominance relationship among planners using cheap heuristics such as merge&shrink heuristics (which require only a table lookup during search) and expensive heuristics such as LM-cut, recent work in forward-search based planning has focused on heuristics which tend to be slow, such as heuristics that require the solution of a linear program at every search node (Pommerening, R\u00f6ger, Helmert, & Bonet, 2014; Imai & Fukunaga, 2015), so parallel strategies that focus on minimizing search overheads is of practical importance. Previous evaluations of parallel work distribution strategies in domain-independent planning used relatively fast heuristics. Kishimoto et al. (2013), as well as Jinnai and Fukunaga (2016a, 2016b) used merge&shrink abstraction based heuristics.", "startOffset": 403, "endOffset": 694}, {"referenceID": 9, "context": "While there is no dominance relationship among planners using cheap heuristics such as merge&shrink heuristics (which require only a table lookup during search) and expensive heuristics such as LM-cut, recent work in forward-search based planning has focused on heuristics which tend to be slow, such as heuristics that require the solution of a linear program at every search node (Pommerening, R\u00f6ger, Helmert, & Bonet, 2014; Imai & Fukunaga, 2015), so parallel strategies that focus on minimizing search overheads is of practical importance. Previous evaluations of parallel work distribution strategies in domain-independent planning used relatively fast heuristics. Kishimoto et al. (2013), as well as Jinnai and Fukunaga (2016a, 2016b) used merge&shrink abstraction based heuristics. Zhou and Hansen (2007) and Burns et al.", "startOffset": 403, "endOffset": 812}, {"referenceID": 3, "context": "Zhou and Hansen (2007) and Burns et al. (2010)used the max-pair heuristic (Haslum & Geffner, 2000).", "startOffset": 27, "endOffset": 47}, {"referenceID": 13, "context": "This approach is similar to merge-and-shrink heuristic (Helmert et al., 2014) which merging multiple DTGs into abstract state space to better estimate the state-space graph.", "startOffset": 55, "endOffset": 77}, {"referenceID": 3, "context": "This section presents an improvement to AHDA* (Burns et al., 2010).", "startOffset": 46, "endOffset": 66}, {"referenceID": 11, "context": "The AHDA* results in Table 10 are for a 48-core cluster, 2GB/core, and uses Nmax = 102, 103, 104, 105, 106 nodes based on Fast-Downward (Helmert, 2006) using merge&shrink heuristic (Helmert et al.", "startOffset": 136, "endOffset": 151}, {"referenceID": 13, "context": "The AHDA* results in Table 10 are for a 48-core cluster, 2GB/core, and uses Nmax = 102, 103, 104, 105, 106 nodes based on Fast-Downward (Helmert, 2006) using merge&shrink heuristic (Helmert et al., 2014).", "startOffset": 181, "endOffset": 203}], "year": 2017, "abstractText": "Parallel best-first search algorithms such as Hash Distributed A* (HDA*) distribute work among the processes using a global hash function. We analyze the search and communication overheads of state-of-the-art hash-based parallel best-first search algorithms, and show that although Zobrist hashing, the standard hash function used by HDA*, achieves good load balance for many domains, it incurs significant communication overhead since almost all generated nodes are transferred to a different processor than their parents. We propose Abstract Zobrist hashing, a new work distribution method for parallel search which, instead of computing a hash value based on the raw features of a state, uses a feature projection function to generate a set of abstract features which results in a higher locality, resulting in reduced communications overhead. We show that Abstract Zobrist hashing outperforms previous methods on search domains using hand-coded, domain specific feature projection functions. We then propose GRAZHDA*, a graph-partitioning based approach to automatically generating feature projection functions. GRAZHDA* seeks to approximate the partitioning of the actual search space graph by partitioning the domain transition graph, an abstraction of the state space graph. We show that GRAZHDA* outperforms previous methods on domain-independent planning.", "creator": "TeX"}}}