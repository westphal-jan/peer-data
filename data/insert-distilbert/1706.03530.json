{"id": "1706.03530", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Candidate sentence selection for language learning exercises: from a comprehensive framework to an empirical evaluation", "abstract": "we present a framework and its implementation relying only on natural language processing methods, which aims at the identification of exercise item meaning candidates from corpora. the hybrid system combining heuristics and applied machine learning methods includes a number of relevant selection criteria. we focus on two fundamental aspects : linguistic complexity learning and the dependence upon of the extracted sentences on their original context. previous work progressing on exercise generation addressed these two criteria only to a limited extent, and a refined overall candidate sentence selection framework vocabulary appears also to be lacking. in addition to a detailed description implementation of the system, we present the results of an empirical evaluation conducted with language teachers and learners interviewed which indicate the usefulness of the system for educational purposes. we have integrated our system into a freely available online pattern learning platform.", "histories": [["v1", "Mon, 12 Jun 2017 09:21:45 GMT  (314kb,D)", "http://arxiv.org/abs/1706.03530v1", "To appear in Traitement Automatique des Langues (TAL) Journal, Special issue on NLP for Learning and Teaching"]], "COMMENTS": "To appear in Traitement Automatique des Langues (TAL) Journal, Special issue on NLP for Learning and Teaching", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ildik\\'o pil\\'an", "elena volodina", "lars borin"], "accepted": false, "id": "1706.03530"}, "pdf": {"name": "1706.03530.pdf", "metadata": {"source": "CRF", "title": "Candidate sentence selection for language learning exercises: from a comprehensive framework to an empirical evaluation", "authors": ["Ildik\u00f3 Pil\u00e1n", "Elena Volodina", "Lars Borin"], "emails": ["lars.borin}@gu.se"], "sections": [{"heading": null, "text": "R\u00c9SUM\u00c9. Nous proposons un syst\u00e8me de traitement automatique de la langue ayant pour but l\u2019identification de phrases candidates tir\u00e9es de corpus. Le syst\u00e8me hybride allie une approche heuristique \u00e0 des m\u00e9thodes d\u2019apprentissage automatis\u00e9 et int\u00e8gre un nombre de crit\u00e8res de s\u00e9lection pertinents. Nous nous concentrons sur deux aspects fondamentaux : la complexit\u00e9 linguistique et la d\u00e9pendance des phrases extraites envers leur contexte d\u2019origine. Les travaux ant\u00e9rieurs en g\u00e9n\u00e9ration automatique d\u2019exercices n\u2019ont port\u00e9 sur ces deux crit\u00e8res que de fa\u00e7on limit\u00e9e, et un cadre fin de s\u00e9lection de phrases candidates semble \u00e9galement faire d\u00e9faut. En plus d\u2019une description d\u00e9taill\u00e9e du syst\u00e8me, cet article rapporte les r\u00e9sultats d\u2019une \u00e9valuation empirique r\u00e9alis\u00e9e avec des enseignants de langues et des apprenants portant sur l\u2019utilit\u00e9 du syst\u00e8me \u00e0 des fins \u00e9ducatives.\nKEYWORDS: exercise generation, corpus example selection, user-based evaluation.\nMOTS-CL\u00c9S : g\u00e9n\u00e9ration d\u2019exercices, concordances, \u00e9valuation orient\u00e9e vers l\u2019utilisateur.\nTAL. Volume 57 \u2013 n\u25e63/2016, pages 1 \u00e0 25\nar X\niv :1\n70 6.\n03 53\n0v 1\n2 TAL. Volume 57 \u2013 n\u25e63/2016"}, {"heading": "1. Introduction", "text": "Several tasks related to foreign and second language (L2) learning can be partly or entirely automatized with the help of Natural Language Processing (NLP) tools. One such task is exercise generation, whose automation offers both self-directed learning opportunities and support for teaching professionals\u2019 practice. The pedagogical relevance and practical usefulness of such solutions, however, would need to be further improved before these systems can become widely used in language instruction. During our work, we aimed at maintaining a pedagogical angle, on the one hand, by incorporating statistical information from existing hand-written teaching materials into our selection criteria and, on the other hand, by evaluating the performance of our system with L2 teachers and learners.\nPractice plays an important role in L2 learning for the development of both receptive and productive skills (DeKeyser, 2007). Corpora as potential practice material are readily available in large quantities, however, their use in L2 teaching has been both supported and opposed in previous years, O\u2019Keeffe et al. (2007) present an overview of this debate. Corpora offer a large amount of diverse examples at a low cost, and their use has been shown to have a positive effect on learners\u2019 progress (Cobb, 1997; Cresswell, 2007). Moreover, corpora are evidence of real-life language use which, however, might be hard for learners to process (Kilgarriff, 2009). Nonauthentic, teacher-constructed materials have also been subject to criticism. While this approach benefits from teachers\u2019 expert knowledge, these materials are \u201cbased on intuition about how we use language, rather than actual evidence of use\u201d (O\u2019Keeffe et al., 2007, p. 21). We aim at bringing together intuition and evidence about language use by employing insights from coursebooks for selecting examples from real-life corpora (e.g. news texts, novels).\nRecent years have seen a number of efforts in the NLP community to automatically generate exercise items (e.g. (Arregik, 2011; Smith et al., 2010; Sumita et al., 2005)). Most of these, however, tend to neglect what criteria sentences should fulfil in order to be suitable as exercise items and, instead, build on either a predefined set of manually selected sentences, or require merely a certain linguistic pattern (e.g. a particular word) to be present in the sentence (see Section 2.2). When selecting sentences from corpora, however, there are a number of additional aspects that sentences need to adhere to in order to be usable and understandable in isolation. These have been previously explored mostly in a lexicographic context (Kilgarriff et al., 2008), but they are also relevant for language teaching (Kilgarriff, 2009). Two fundamental questions in this respect are: (i) Can the sentence function in isolation, outside its larger textual context? (ii) Is the complexity of the linguistic content of the sentence suitable for the intended L2 learner(s)? We will refer to the former as context independence and to the latter as L2 complexity.\nLanguage learners pass through different learning stages (levels) reflecting the development and improvement of their competences. A scale of such levels is CEFR, the Common European Framework of Reference for Languages (Council of Eu-\nSentence selection for exercises 3\nrope, 2001). The CEFR defines proficiency levels on a six-point scale: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (mastery). A subset of language learners\u2019 competences are linguistic competences, which include, among others, lexical, grammatical and semantic competences. When assessing L2 complexity, we concentrate on linguistic competences required for reading comprehension since these can be matched to linguistic elements observable in language samples written for learners at different CEFR levels.\nBoth context independence and L2 complexity emerged as a main reason for discarding candidate sentences in previous evaluations (Arregik, 2011; Pil\u00e1n et al., 2013), but thorough methods targeting these aspects have not been proposed up to date to our knowledge. Our approach, building on previous attempts at selecting sentences, contributes to previous research by offering a comprehensive set of criteria and by performing a more sophisticated selection in terms of the two fundamental aspects just mentioned, context independence and L2 complexity. We propose a hybrid system with both rule-based and machine learning driven components that encompasses a wide range of aspects. Incorporating rules makes the system customizable to users\u2019 needs and thus relevant for a wide range of application scenarios including vocabulary and grammatical exercises of different formats, as well as vocabulary examples. An evaluation with teachers and students indicates that our system identifies sentences that are, in general, of a suitable level of difficulty for learners. The algorithm is available to the general public free of charge both as a customizable sentence selection interface and as a web service. The development of automatically generated exercises using the selected sentences is also in progress. Our target language is Swedish, a language for which the number of L2 learners has grown rapidly over recent years (Statistics Sweden, 2016). Although the current implementation is based on resources and tools for Swedish, the methods described can serve as an example for future implementations of exercise item candidate selection systems for other languages.\nThis paper is structured as follows. In Section 2, we provide an overview of the related literature. Then, in Section 3, we describe our sentence selection framework in detail together with its implementation. Finally, in Section 4, we present and discuss the results of a user-based evaluation of the system."}, {"heading": "2. Related work", "text": "In this section, we provide an overview of the related literature which includes sentence selection strategies for both vocabulary examples and exercise items as well as studies on readability and CEFR level prediction.\n2.1. Sentence selection for vocabulary examples\nGDEX, Good Dictionary Examples (Hus\u00e1k, 2010; Kilgarriff et al., 2008) is an algorithm for selecting sentences from corpora for the purposes of illustrating the\n4 TAL. Volume 57 \u2013 n\u25e63/2016\nmeaning and the usage of a lexical unit. It incorporates a number of linguistic criteria (e.g. sentence length, vocabulary frequency, anaphoric pronouns) based on which example candidates are ranked. Some of these are related to context dependence (e.g. incompleteness of sentences, presence of personal pronouns), but they are somewhat coarse-grained criteria without a focus on syntactic aspects.\nBesides English, the algorithm has also been successfully implemented for other languages. Kosem et al. (2011) and Tiberius and Kinable (2015) explore GDEX configurations for Slovene and Dutch respectively, aiming at identifying the optimal parameter settings for these languages for lexicographic projects. Didakowski et al. (2012) propose an example selection algorithm similar to GDEX for German. A fundamental difference of this method compared to the ranking mechanism of GDEX is having \"hard criteria\" which, if not met, result in sentences being excluded. GDEX has also inspired a Swedish algorithm for sentence selection (Volodina et al., 2012) and it has been employed also for generating gap-fill exercises (Smith et al., 2010). Furthermore, a number of machine learning approaches have been explored for these purposes in recent years (Geyken et al., 2015; Lemnitzer et al., 2015; Ljube\u0161ic\u0301 and Peronja, 2015). Example sentence selection for illustrating lexical items has also been addressed from a language teaching perspective by Segler (2007), where a set of selection criteria used by teachers was modelled with logistic regression. The main dimensions examined include syntactic complexity and similarity between the original context of a word and an example sentence.\n2.2. Sentence selection for exercise item generation\nIn a language-learning scenario, corpus example sentences can be useful both as exercise items and as vocabulary examples. Sentences used in exercises are also known as seed sentences (Sumita et al., 2005) or carrier sentences (Smith et al., 2010) in the Intelligent, i.e. NLP-enhanced, Computer-Assisted Language Learning (ICALL) literature.\nPrevious work on exercise item generation has taken into consideration a rather limited amount of aspects when selecting seed sentences. In some cases, sentences are only required to contain a lexical item or a linguistic pattern that constitutes the target of the exercise, but context dependence and L2 complexity are not explicitly addressed (Sumita et al., 2005; Mitkov et al., 2006; Arregik, 2011; Wojatzki et al., 2016). LanguageMuse (Burstein et al., 2012), a system supporting teachers in generating activities for learners of English, also belongs to this category. The sentences are selected from texts provided by teachers, the criteria of selection being the presence of a specific linguistic element that constitutes the target of the exercise: a lexical entity, a syntactic structure or a discourse relation.\nAnother alternative has been using dictionary examples as seed sentences, e.g. from WordNet (Pino and Eskenazi, 2009). Such sentences are inherently contextindependent, however, they impose some limitations on which linguistic aspects can\nSentence selection for exercises 5\nbe targeted in the exercises and they are not adjusted to finer-grained L2 learning levels. A system using GDEX for seed sentence selection is described in Smith et al. (2010), who underline the importance of the well-formedness of a sentence and determine a sufficient amount of context in terms of sentence length. Lee and Luo (2016) describe an ICALL system for fill-in-the-blanks preposition learning exercises, where seed sentences are checked for their lexical difficulty based on the level of the words according to a graded vocabulary lists. Pil\u00e1n et al. (2014) present and compare two algorithms for candidate sentence selection for Swedish, using both rule-based and machine learning methods. Context dependence, which has not been specifically targeted in their system, emerged as a key issue underlying suboptimal candidate sentences during an empirical evaluation.\n2.3. Readability and proficiency level classification\nThe degree of complexity in the linguistic content of sentences and texts is one of the aspects underlying not only proficiency levels, but also readability. Readability measures typically classify texts into school grade levels or into a binary category of easy- vs. hard-to-read, but the term has also been used in the context of CEFR level classification, e.g. Xia et al. (2016), Fran\u00e7ois and Fairon (2012). In recent years a number of NLP-based readability models have been proposed not only for English (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Graesser et al., 2011; Vajjala and Meurers, 2012; Collins-Thompson, 2014), but also for other languages, e.g. Italian (Dell\u2019Orletta et al., 2011) and Swedish (Heimann M\u00fchlenbock, 2013). The linguistic features explored so far for this task include information, among others, from part-of-speech (POS) taggers and dependency parsers. Cognitively motivated features have also been proposed, for example, in the Coh-Metrix (Graesser et al., 2011). Although the majority of previous work focuses primarily on text-level analysis, the concept of sentence-level readability has also emerged and attracted an increasing interest in recent years (Pil\u00e1n et al., 2013; Vajjala and Meurers, 2014; Dell\u2019Orletta et al., 2014).\nThe prediction of proficiency levels for L2 teaching materials using supervised machine learning methods has been explored for English (Heilman et al., 2007; Huang et al., 2011; Zhang et al., 2013; Salesky and Shen, 2014; Xia et al., 2016), French (Fran\u00e7ois and Fairon, 2012), Portuguese (Branco et al., 2014), Chinese (Sung et al., 2015) and, without the use of NLP, for Dutch (Velleman and van der Geest, 2014).\nReadability formulas for the Swedish language have a long tradition. One of the most popular, easy-to-compute formulas is LIX (L\u00e4sbarhetsindex, \u2018Readability index\u2019) proposed by Bj\u00f6rnsson (1968). This measure combines the average number of words per sentence in the text with the percentage of long words, i.e. tokens consisting of more than six characters. Besides traditional formulas, supervised machine learning approaches have also been tested. A Swedish document-level readability model is described by Heimann M\u00fchlenbock (2013) and Falkenjack et al. (2013). Pil\u00e1n et al.\n6 TAL. Volume 57 \u2013 n\u25e63/2016\n(2016), on the other hand, investigate L2 complexity for Swedish both at document and sentence level."}, {"heading": "3. HitEx: a candidate sentence selection framework and its implementation", "text": "In this section, we present our candidate sentence selection framework, HitEx (Hitta Exempel \u2018Find Exemples\u2019) and its implementation. After an overall description, we introduce and motivate each selection criteria in Sections 3.2 to 3.7.\n3.1. Overall description\nIn Table 1, we show the selection criteria belonging to the proposed framework, grouped into broader categories. Each criterion is used to scan a sentence for the presence (or the absence) of linguistic elements associated to its \"goodness\", i.e. its suitability for the intended use. Most criteria target aspects that are negatively correlated to the goodness of a sentence. Certain selection criteria are associated with one (or more) numeric parameter(s) that users can set, e.g. the minimum and maximum number of tokens for the sentence length criterion. The categories concerning the search term, well-formedness and context independence can be considered generic criteria that are applicable for a number of different use cases, e.g. different exercise types, vocabulary examples, while the rest of the criteria are more specific for exercise item generation. In general, the sources that served as basis for these criteria include previous literature (Section 2), L2 curricula and the qualitative results of previous user-based evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2014).\nWe implemented a hybrid system which uses a combination of machine-learning methods for assessing L2 complexity and heuristic rules for all other criteria. The motivation behind using rules is, on the one hand, that certain linguistic elements are easily identifiable with such methods. On the other hand, a sufficient amount of training data encompassing the range of all possible exercise types would be extremely costly to create. Moreover, explicit rules make the sentence selection customizable to users\u2019 task-specific needs which increases the applicability of HitEx to a diverse set of situations. The criterion of L2 complexity has been implemented using machine learning methods since its assessment comprises multiple linguistic dimensions and data was available for approaching this sub-problem in a data-driven fashion. A few selection criteria in our framework are re-implementations of those described by Volodina et al. (2012) and Pil\u00e1n et al. (2014). Major additions to previous work include: (i) L2 complexity assessment on a 5-level scale, vs. a previously available binary classification model by Pil\u00e1n et al. (2014), (ii) typicality and (iii) the assessment of context dependence. Sensitive vocabulary filtering and the use of word frequencies from SVALex (Fran\u00e7ois et al., 2016), a word list based on coursebook texts, are also novel aspects that we incorporated with the aim of making the sentence selection algorithm more pedagogically aware.\nSentence selection for exercises 7\nOur implementation relies on a number of different NLP resources. Our system searches for sentence candidates via Korp (Borin et al., 2012), an online infrastructure providing access to a variety of (mostly) Swedish corpora. The concordance web service of Korp provides a list of corpus examples containing a certain user-specified search term, e.g. an uninflected word, lemma or a grammatical structure. Through Korp, a large variety of text genres are available such as novels, blogs, news and easyto-read texts. All corpora are annotated at different linguistic levels, which include lemmatization, part-of-speech (POS) tagging and dependency parsing. HitEx assesses sentences based on these annotations as well as information from a number of Swedish lexical-semantic resources. A major lexical resource used is SALDO (Borin et al., 2013) which is based on lexical-semantic closeness between word senses organized in a tree structure.\nAs a first step in our sentence scoring algorithm, for each candidate sentence s \u2208 S, we apply a linguistic criterion c \u2208 C to s either as a filter f \u2208 F or as a ranker r \u2208 R, that is C = F \u222a R. The application of each criterion ck to all the sentences, ck(S) = Vck is a set of criterion values (vck \u2208 Vck ). Vck = {0, 1} when ck \u2208 F and Vck \u2208 R when ck \u2208 R; for instance, when ck is the proper names criterion used as a ranker, vcksi corresponds to the number of times a proper name appears in si \u2208 S. If si contains an undesired linguistic element associated to ck \u2208 F , then vcksi = 1, and si is excluded from the ranking of suitable candidates. Further details about how we obtain Vck are outlined in Sections 3.2 to 3.7. Some criteria encode binary characteristics (e.g. interrogative sentence), therefore, only ck \u2208 F holds for these. We present these in italics in Table 1.\n8 TAL. Volume 57 \u2013 n\u25e63/2016\nTo rank non-filtered sentences, we compute a goodness score Gsi \u2208 N, which reflects the degree to which si is a suitable candidate based on R. When ck \u2208 R, R = R+\u222aR\u2212, where r+ \u2208R+ is a positive ranker for positively correlated properties with goodness, namely typicality and SVALex frequencies; and r\u2212 \u2208R\u2212 is a negative ranker that includes all other criteria. Based on Vck , we compute an intermediate (per-criterion) goodness score (subGcksi ) for each si, by sorting S based on Vck and assigning the ranking position of si according to Vck to subGcksi . Consequently, the number of subscores will be equal to the number of ck \u2208 R selected. During this sorting, for si \u2208 S and sj \u2208 S, for r+ subGcksi \u2265 subGcksj \u21d4 vcksi \u2265 vcksj holds, and for r\u2212 subGcksi \u2265 subGcksj \u21d4 vcksi \u2264 vcksj applies. In other words, we rank S based on an ascending order of goodness if ck \u2208 R+ and a descending order of badness if ck \u2208 R\u2212. Therefore, more suitable candidates receive a higher subGcksi . For example, suppose r \u2212 k is proper names and si contains 2 proper names, while sj contains none; then subGcksi = 1 and subGcksj = 2. The score Gsi is then computed by summing all subscores, that is Gsi = \u2211 subGcksi . Finally, candidate sentences are ordered in a decreasing order based onGsi . A weighting scheme similar to GDEX would be possible with the availability of data specific for the end use of the sentences from where to estimate these weights. At the current stage, all ranking criteria contribute equally to Gsi . Suboptimal sentences containing elements to filter can also be retained and ranked separately, if so wished, based on the amount of F matched. The final results include, for each si \u2208 S, its summed overall score (Gsi ), its final rank and detailed information per selection criteria, as the screenshot presenting the system\u2019s graphical user interface in Figure 1 in Section 3.8 shows. In the following subsections, we present each criterion in detail, grouped into categories.\n3.2. Search term\nA search term corresponds to one (or more) linguistic element(s) that users would like the selected sentences to contain. It can be either a lexical element such as an inflected word or a lemma; or a grammatical pattern, e.g. verbs in a certain tense followed by a noun. The presence of a search term is guaranteed through the mere use of the Korp concordance web service which only returns sentences containing the searched expression. In some application scenarios, repeated matches of the search term may be considered suboptimal (Kosem et al., 2011, p. 157), therefore we include this aspect among our criteria. Similarly, there might be a preference for the position of the search term in the sentence in some use cases such as dictionary examples (Kilgarriff et al., 2008).\n3.3. Well-formedness\nGood candidate sentences from corpora should be structurally and lexically wellformed (Kilgarriff et al., 2008; Hus\u00e1k, 2010). We incorporate two criteria targeting the former aspect: one can check sentences for the presence of a dependency root,\nSentence selection for exercises 9\nand ellipsis, i.e. the lack of a subject or a finite verb (all verb forms except infinitive, supine and participle) inspired by Volodina et al. (2012). The completeness criterion checks the beginning and the end of a sentence for orthographic clues such as capital letters and punctuation, in a similar fashion to Pil\u00e1n (2016). A large amount of nonlemmatized tokens, i.e. tokens for which no matching dictionary form could be identified (in the SALDO lexicon in our case), are also preferably avoided (Hus\u00e1k, 2010, p. 15). These are mostly cases of spelling or optical character recognition errors, foreign words, infrequent compounds, etc. A large portion of non-alphabetical tokens could be e.g. a sign of mark-up traces in web material, which has a negative influence on the L2 complexity and the usability of a sentence (Hus\u00e1k, 2010, p. 15). Users can specify a constant as a threshold for these criteria to determine the allowed amount of non-lemmatized and non-alphabetical tokens in a sentence.\n3.4. Context independence\nSince sentences originally form part of coherent texts, a crucial aspect to take into consideration during selection is whether sentences would be meaningful also as a stand-alone unit without their original, larger context. The presence of linguistic elements responsible for connecting sentences at a syntactic or semantic level is therefore suboptimal (Kilgarriff et al., 2008). We incorporate a number of criteria for capturing this aspect which we described also in Pil\u00e1n (2016).\nSyntactic aspects include structural connectives, i.e. conjunctions and subjunctions (Webber et al., 2003). Two concepts connected by structural connectives may appear in separate sentences which give rise to context dependence. Our system considers sentences with connectives in sentence-initial position context dependent unless the sentence consists of more than one clause. Connectives that are paired conjunctions are also allowed (e.g. antingen ... eller \u2018either ... or\u2019).\nAnaphoric expressions referring to previously mentioned information are aspects related to the semantic dimension. Our pronominal anaphora criterion targets mentions of the third person singular pronouns den \u2018it\u2019 (common gender) and det \u2018it\u2019 (neuter gender) as well as the demonstrative pronouns (e.g. denna \u2018this\u2019, s\u00e5dan \u2018such\u2019 etc.). The non-anaphoric use of det (e.g. in clefts: It is carrots that they eat.), however, is not counted here. Such cases can be distinguished based on the output of the dependency parser: these occurrences of det are tagged as expletive (pleonastic). Pronouns followed by a relative clause introduced by som \u2018which\u2019 are also considered non-anaphoric.\nUnder adverbial anaphora, we count time and location adverbs that behave anaphorically (e.g. d\u00e5 \u2018then\u2019) (Webber et al., 2003). Another group of adverbs relevant for anaphora are those expressing logical relations (e.g. ist\u00e4llet \u2018instead\u2019), which are also referred to as discourse connectives (Webber et al., 2003). Based on Teleman et al. (1999), a list of anaphoric adverbs has been collected and sentences are checked for the occurrence of any of the listed items.\n10 TAL. Volume 57 \u2013 n\u25e63/2016\n3.5. L2 complexity\nThe aspect of L2 complexity has been assessed with the help of a supervised machine learning algorithm based on a number of different linguistic dimensions. We used the CEFR level classifier for sentences that we previously described in Pil\u00e1n et al. (2016). The source of the training data was single sentences from COCTAILL (Volodina et al., 2014), a corpus of coursebook texts for L2 Swedish. Such single sentences occurred either in the form of lists or so-called language examples, sentences exemplifying a lexical or a grammatical pattern. The feature set used for assessing L2 complexity is presented in Table 2. This set consists of five subgroups of features: count-based, lexical, morphological, syntactic, and semantic features.\nCount features are based on the number of characters and tokens (T), extra-long words being tokens longer than 13 characters. LIX, a traditional Swedish readability formula (see Section 2) combines the sum of the average number of words per sentence in the text and the percentage of tokens longer than six characters (Bj\u00f6rnsson, 1968). Bi-logarithmic and a square root type-token ratio (TTR) related to vocabulary richness (Heimann M\u00fchlenbock, 2013) are also computed.\nLexical features incorporate information from the KELLY list (Volodina and Kokkinakis, 2012), a word list with frequencies calculated from a corpus of web texts (thus completely independent of the sentences in the dataset). KELLY provides a suggested CEFR level per each listed lemma based on frequency bands. For some feature values, incidence scores (IS) normalized values per 1,000 tokens are computed, which reduces the influence of sentence length. Word forms or lemmas themselves are not used as features, the IS of their corresponding CEFR level is considered instead. Difficult tokens are those that belong to levels above the overall CEFR level of the text. Moreover, we consider the IS of tokens not present in KELLY (OOV IS), the IS of tokens for which the lemmatizer could not identify a corresponding lemma (No lemma IS), as well as average KELLY log frequencies.\nMorphological features include both IS and variational scores, i.e. the ratio of a category to the ratio of lexical tokens: nouns (N), verbs (V), adjectives (ADJ) and adverbs (ADV). The IS of all lexical categories as well as the IS of punctuation, particles, sub- and conjunctions (SJ, CJ) are taken into consideration. In Swedish, a special group of verbs ending in -s are called s-verbs (S-VB). These can indicate either a reciprocal verb, a passive construction or a deponent verb (active in meaning but passive in form). Due to their morphological and semantic peculiarity, they are explicitly targeted in L2 grammar books (Fasth and Kannermark, 1997). Nominal ratio (Hultman and Westman, 1977) is another readability formula proposed for Swedish that corresponds to the ratio of nominal categories, i.e. nouns, prepositions (PP) and participles to the ratio of verbal categories, namely pronouns (PR), adverbs, and verbs. Relative structures consist of relative adverbs, determiners, pronouns and possessives.\nSyntactic features are based, among others, on the length (depth) and the direction of dependency arcs (DepArc). These aspects are related to readers working memory load when processing sentences (Gibson, 1998). For similar reasons, we consider\nSentence selection for exercises 11\nalso relative clauses as well as pre- and post-modifiers, which include, for example, adjectives and prepositional phrases respectively.\nSemantic features draw on information from the SALDO lexicon. We use the average number of senses per token and the average number of noun senses per noun. Polysemous words can be demanding for readers as they need to be disambiguated for a full understanding of the sentence (Graesser et al., 2011).\n12 TAL. Volume 57 \u2013 n\u25e63/2016\nPil\u00e1n et al. (2016) utilizing the feature set described above report 63.4% accuracy using a logistic regression classifier for the identification of CEFR levels with an exact match, and 92% accuracy for classifications within a distance of one CEFR level. Besides the features outlined above, also the lack of culture-specific knowledge can be a factor influencing L2 complexity, as well as learners\u2019 knowledge of other languages. We, however, do not address these dimensions in the current stage due to a lack of relevant data.\n3.6. Additional structural criteria\nBesides the aspects mentioned above, a number of additional structural criteria are available which proved to be relevant either based on previous evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2013) or evidence from coursebooks (Volodina et al., 2014). One such aspect is negative wording which is preferable to avoid in exercise items (Frey et al., 2005). All tokens with the dependency tag of negation adverbials fall under this criterion. Under the interrogative sentence criterion, we handle direct questions ending with a question mark. To detect direct speech, we have compiled a list of verbs denoting the act of speaking based on the Swedish FrameNet (Heppin and Gronostaj, 2012). The list contains 107 verbs belonging to frames relevant to speaking (e.g. viska \u2018whisper\u2019 from the Communication manner frame). This is combined with POS tag patterns composed of a minor delimiter (e.g. dash, comma) or a pairwise delimiter (e.g. quotation marks), followed by a speaking verb (optionally combined with auxiliary verbs), followed by a pronoun or a proper name. Both questions and sentences containing direct speech tend to be less common as exercise items, incorporating these among our criteria allows users to avoid such sentences if so wished.\nAnswers to polar (or close-ended) questions are rarely employed as exercise items and they were also negatively perceived in previous evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2013). This aspect is also relevant to the dependence of a sentence on a wider context. The algorithm tries to capture sentences of this type based on POS patterns: sentence-initial adverbs and interjections (e.g. ja \u2018yes\u2019, nej \u2018no\u2019) preceded and followed by minor delimiters where the initial delimiter is optional for interjections. Modal verbs were identified based on a small set of verbs used typically (but not exclusively) as modal verbs (e.g. kan \u2018can\u2019, \u2018know\u2019) where the dependency relation tag indicating a verb group excludes the non-auxiliary use. Sentence length, a criterion which is also part of GDEX, is measured as the number of tokens including punctuation in our system.\n3.7. Additional lexical criteria\nHitEx includes also options for filtering and ranking sentences based on information from lexical resources for ensuring an explicit control of this crucial aspect (Segler, 2007). Sentences containing difficult words, i.e. words whose CEFR level is above the target CEFR level according to the KELLY list, can be penalized or filtered.\nSentence selection for exercises 13\nBesides KELLY, we also integrated into our system information from the SVALex list based on word frequencies from coursebook texts. Sentences with words absent from SVALex or with words below the average frequency threshold for the target CEFR level are thus additional scoring criteria. Another criterion involves the presence of proper names which, although undesirable for dictionary examples (Kilgarriff et al., 2008), may be familiar and easy to understand for L2 students (Segler, 2007). Both proper names and abbreviations were counted based on the POS tagger output.\nIn a pedagogical setting, certain sensitive vocabulary items and topics tend to be avoided by coursebook writers. These are also referred to as PARSNIPs, which stands for Politics, Alcohol, Religion, Sex, Narcotics, Isms 1 and Pork (Gray, 2010). Some topics are perceived as taboos cross-culturally, such as swear words, while others may be more culture-bound. We compiled a word list starting with an initial group of seed words from more generally undesirable domains (e.g. swear words) collected from different lexical and collaborative online resources (e.g. Wikipedia 2) complemented with a few manually added entries. Furthermore, we expanded this automatically with all child node senses from SALDO for terms which represent sensitive topics (e.g. narkotika \u2018narcotics\u2019, svordom \u2018profanities\u2019, m\u00f6rda \u2018murder\u2019, etc.) so that synonyms and hyperonyms would also be included. A few common English swear words that are frequently used in Swedish in an informal context (e.g. blog texts) were also incorporated. The current list of 263 items is not exhaustive and can be expanded in the future. The implementation allows teaching professionals to make the pedagogical decision of tailoring the subset of topics to use to a specific group of learners during the sentence selection.\nTypicality can be an indication of more or less easily recognizable meaning of concepts without a larger context (Barsalou, 1982). We assessed the typicality of sentences with the help of a co-occurrence measure: Lexicographers\u2019 Mutual Information (LMI) score (Kilgarriff et al., 2004). LMI measures the probability of two words co-occurring together in a corpus and it offers the advantage of balancing out the preference of the Mutual Information score for low-frequency words (Bordag, 2008). We used a web service offered by Korp for computing LMI scores based on Swedish corpora. As a first step in computing the LMI scores, we collected nouns and verbs in the KELLY and SVALex lists (removing duplicates), which resulted in a list of 12,484 items. Then using these, we estimated LMI scores for all noun-verb combinations (nouns being subjects or objects) as well as LMI for nouns and their attributes using Korp. Counts were based on 8 corpora of different genres amounting to a total of 209,110,000 tokens. The resulting list of commonly co-occurring word pairs consisted of 99,112 entries. Only pairs with a threshold of LMI \u2265 50 were included. The typicality value of a candidate sentence corresponded to the sum of all LMI scores available in the compiled list for each noun and verb in the sentence.\n1. An ideology or practice, typically ending with the suffix -ism, e.g. anarchism. 2. https://www.wikipedia.org.\n14 TAL. Volume 57 \u2013 n\u25e63/2016\n3.8. Integration into an online platform\nTo provide access to our sentence selection algorithm to others, we have integrated it into a freely available learning platform, L\u00e4rka. 3 With the help of a graphical interface, shown in Figure 1, users can perform a sentence selection customized to their needs. Under the advanced options menu, users can choose which selection criteria presented in Table 1 to activate as filters or rankers. Moreover, the selection algorithm will serve as a seed sentence provider for automatically generated multiple-choice exercises for language learners within the same platform. The sentence selection algorithm is also available as a web service that others can easily integrate in their own systems.\n3. https://spraakbanken.gu.se/larkalabb/hitex.\nSentence selection for exercises 15"}, {"heading": "4. A user-based evaluation", "text": "The main objective when developing our candidate selection algorithm was to identify seed sentences for L2 learning exercises. In absence of an annotated dataset for this task in Swedish, we tested the performance of HitEx with the help of a userbased evaluation. We assessed the goodness of the candidate sentences in two ways: (i) through L2 teachers confirming their suitability, (ii) by inspecting whether L2 learners\u2019 degree of success in solving exercise items constructed based on these candidates matched what is typically expected in L2 teaching. This provided us with information about the extent to which the set of criteria proposed in Section 3 was useful for identifying suitable seed sentences. The evaluation sentences and the associated results will be available as a dataset on https://spraakbanken.gu.se/eng/resources.\n4.1. Participants\nThe participants consisted of 5 teachers of L2 Swedish from different institutions and 19 students from a language school targeting young adults newly arrived to Sweden. Participating students were between ages 16 and 19 with a variety of native languages including several Somali and Dari speakers. The proportion of female and male students was approximately equal. The CEFR level of students is assessed on a regular basis with a two-month interval in their school. In our evaluation, as a point of reference for students\u2019 CEFR level, we referred to the levels achieved on their latest assessment test. According to this, 3 students were at A1 level, and the remaining 16 were a 50\u201350% split between A2 and B1 levels.\n4.2. Material and task\nTo create the evaluation material, we retrieved a set of sentences from Korp for CEFR levels A1\u2013C1 using HitEx. To perform the Korp concordance search, we used lemmas from SVALex whose level corresponded to the level of the sentences we aimed at identifying. We used a lemma-based search and the parts of speech included nouns, verbs and adjectives. The sentences have been selected from 10 different corpora including novels and news texts. For each search lemma, a maximum of 300 matching Korp sentences were fed to the sentence selection algorithm, out of which only the top ranked candidate for each lemma was included in the evaluation material. Most selection criteria were used as filters, but typicality, proper names, KELLY and SVALex frequencies were used as rankers. Modal verbs were allowed in the sentences and the position of the search term was not restricted. Sentence length was set to a minimum of 6 and a maximum of 20 tokens. The threshold used for the percentage of non-alphabetic and non-lemmatized tokens was 30%.\nTeachers received 330 sentences to evaluate, evenly distributed across the 5 CEFR levels A1\u2013C1. The sentences were divided into two subgroups based on their level, at least two teachers rating each sentence. One set consisted of A1\u2013B1 level sentences\n16 TAL. Volume 57 \u2013 n\u25e63/2016\nThe sentence...\n1 ... doesn\u2019t satisfy the criterion. 2 ... satisfies the criterion to a smaller extent. 3 ... satisfies the criterion to a larger extent. 4 ... satisfies the criterion entirely.\nTable 3 \u2013 Evaluation scale.\nand the other of sentences within levels B1\u2013C1. (B1 level sentences have been evenly split between the two subsets.) There was a common subset of 30 sentences from all 5 CEFR levels which was rated by all 5 teachers. Besides an overall score per sentence reflecting the performance of the combination of all criteria from Table 1, we elicited teacher judgements targeting two criteria in particular, which were focal points during the implementation of HitEx, namely context independence and L2 complexity (see Sections 3.4 and 3.5 respectively). No specific exercise type needed to be considered for evaluating these aspects, but rather a more application-neutral scenario of a learner reading the sentence. Teachers rated the three dimensions on a 4-point scale as defined in Table 3. Besides these aspects, teachers were also required to suggest an alternative CEFR level if they did not agree with the one predicted by the system.\nTo investigate further whether our selection criteria with the chosen setting produced good seed sentence candidates at the CEFR levels predicted by our L2 complexity criteria, we observed L2 learners\u2019 performance on exercise items created out of these sentences. Exercise generation requires a number of additional steps after the selection of seed sentences, many of which are open research problems. Therefore, we opted for a semi-automatic approach to the generation of these exercises. We manually controlled the combination of sentences into exercises and the selection of a distractor, an incorrect answer option which did not fit into any sentence, in order to reduce potential ambiguity in answer options. A subset of the sentences given to teachers were used as exercise items so that teachers\u2019 ratings and students\u2019 answers could be correlated.\nThe exercise type chosen was word bank, a type of matching exercise, since this posed less challenges when selecting distractors compared to multiple-choice items. Word bank exercises consist of a list of words followed by a list of sentences, each containing a gap. Learners\u2019 task is to identify which word is missing from which sentence. We created worksheets consisting of word bank exercises in Google Forms 4. To lower the probability of answering correctly by chance, we added a distractor. Students had to provide their answers in a grid following the list of candidate words and the gapped sentences. The missing word to identify (and its position) corresponded to the search term used to retrieve the sentence from Korp. Worksheets consisted of 9 exercises with 5 sentences each, amounting to a total of 45 sentences. (The only\n4. https://docs.google.com/forms/.\nexception was A1 level, where students had 2 exercises less.) Students had 60 minutes to work with the exercises, including 5 minutes of instructions. Students worked individually in a computer lab, access to external resources was not allowed.\nThe difficulty of the exercises varied along two dimensions: in terms of their CEFR level and in terms of the similarity of the morpho-syntactic form of the candidate words included in the word bank. A worksheet for a certain level contained 5 exercises from the same level as well as 2 exercises from one level below and one level above. In 5 exercises, the word bank consisted of lexical items with the same morpho-syntactic form (e.g. only plural neuter gender nouns), while 4 exercises had a word bank with mixed POS. The latter group of exercises was somewhat easier, since, besides lexical-semantic knowledge, students could identify the correct solution also based on grammatical clues such as inflectional endings.\n4.3. Results and discussion\nBelow, we present teachers\u2019 and students\u2019 results on the evaluation material.\n4.3.1. Teachers\nTo understand to which extent our set of criteria was able to select suitable seed sentences overall as well as specifically in terms of L2 complexity and context independence, we computed average values and standard deviation (STDEV) over L2 teachers\u2019 ratings. (8 sentences had to be excluded between A1-B1 levels due to missing values.) The results are presented in Table 4.\nAs for the criterion of context independence, 80% of the sentences were found suitable (received an average score higher than 2.5), and 61% of the sentences received score 3 or 4 from at least half of the evaluators. Besides rating the three dimensions in Table 4, teachers also provided an alternative CEFR level in case they did not agree with the CEFR level suggested by the system. HitEx correctly assessed L2 complexity for 64% of sentences based on teachers\u2019 averaged CEFR label, and in 80% of the cases the system\u2019s CEFR level coincided with at least one teacher\u2019s decision. Besides comparing system-assigned and teacher-assigned levels, we measured also the inter-rater agreement (IAA) among the teachers. We used Krippendorff\u2019s \u03b1 measuring observed and expected disagreement, since it is suitable for multiple raters. An \u03b1 = 1 corresponds to complete agreement, while \u03b1 = 0 is equivalent to chance agreement. The\ninter-rater agreement results among teachers are presented in Table 5. The extent of agreement among teachers was considerably higher than chance agreement, but it still remained below what is commonly considered as reliability threshold in annotation tasks, namely \u03b1 = 0.8. CEFR level assignment for sentences thus seems to be a hard task even for teaching professionals.\nBesides inter-rater agreement in terms of \u03b1, we considered also the distance between the CEFR levels assigned by all teachers compared both to each other and to HitEx (Table 6). This would provide us information about the degree to which teachers\u2019 accepted our system\u2019s assessment of L2 complexity. CEFR levels were mapped to integer values for this purpose, and averaged pairwise distances between the levels have been computed in all cases. Surprisingly, teachers agreed with each other exactly on the CEFR level of sentences in only half of the cases, which shows that the exact CEFR level assignment on a 5 point scale is rather difficult even for humans. The percentage of sentences that teachers agreed on with HitEx completely (distance of 0) was slightly (4%) higher than the extent to which teachers agreed with each other. This may be due to the fact that teachers were confirming the system-assigned CEFR levels, but did not have information about each others\u2019 answers. Teacher-assigned CEFR levels remained within 1 level of difference when compared to each other in almost all cases and compared to the system for 92% of the sentences. All in all, the automatic CEFR levels predicted by HitEx were accepted by teachers in the majority of cases within 1 level distance.\nFinally, we computed the Spearman correlation coefficient for teachers\u2019 scores of overall suitability and the two target criteria, L2 complexity and context independence,\nto gain insight into how strongly associated these two aspects were to seed sentence quality according to our evaluation data. The correlation over all sentences was \u03c1 = 0.34 for L2 complexity and \u03c1 = 0.53 for context dependence. The maximum possible value is \u03c1 = 1 for a positive correlation and \u03c1 = \u22121 for a negative one. Both criteria were thus positively associated with overall suitability: the more understandable and context-independent a sentence was, the more suitable our evaluators found it overall. Out of the two criteria, context dependence showed a somewhat stronger correlation.\n4.3.2. Students\nFirst, based on students\u2019 responses, we computed item difficulty for each exercise item, which corresponds to the percentage of students correctly answering an item, a higher value thus indicating an easier item (Crocker and Algina, 1986). The average item difficulty over all exercises was 0.62, corresponding to 62% of students correctly answering items on average. Table 7 shows additional average item difficulty scores divided per CEFR level, exercise type (distractors with same or different morphosyntactic form) and POS. Values were averaged only over the exercise items that were of the same CEFR level as the answering students\u2019 level according to the system.\nTo be able to measure whether the item difficulty observed in our students\u2019 results matched the values one would typically expect in L2 teaching, we calculated the ideal item difficulty (IID) score for our exercises, which takes into consideration correct answers based on chance. We used the formula proposed by Thompson and Levitov (1985) presented in [1], where PC is the probability of correct answers by chance.\nIID = PC + 1\u2212 PC\n2 [1]\nOur exercises consisted of 5 gapped items and 6 answer options in the word bank. Students had, thus, a chance of 1/6 for filling in the first item, 1/5 for the second item etc., which corresponds to an average PC of (0.167+ 0.2+ 0.25+ 0.333+ 0.5)/5 = 0.29 for the whole exercise and, consequently, an IID score of 0.645, that is 64.5% of students correctly answering. The observed item difficulty averaged over all students\n20 TAL. Volume 57 \u2013 n\u25e63/2016\nand exercise items of our evaluation was 62%, which is only slightly lower than the ideal item difficulty. If we break down this average to students\u2019 CEFR levels, we can notice that for A1 students the exercises were considerably more challenging than they should have been according to the ideal threshold. Only 51% of them responded correctly A1-level exercise items. Our sample size was particularly small, however, at this level, thus further evaluations with additional students would be required to confirm this tendency. A2 and B1 level students produced considerably better results: averaging over exercise types and POS, 66.5% and 69.5% of them respectively answered correctly the items of their levels. This indicates that the set of criteria proposed in Section 3 can successfully select seed sentences for L2 exercises for students of A2 and B1 levels.\nContrary to what one might expect, exercise items with distractors bearing different morpho-syntactic forms proved to be actually harder for students compared to items with the same POS and inflection based on our evaluation data. The latter would be inherently harder since only lexico-semantic information can contribute to solving the exercises without the help of grammatical clues. As the item difficulty values show in Table 7, approximately 14% more students answered correctly exercise items with distractors with the same morpho-syntactic tags, an outcome, which, however, may also depend on the inherent difficulty of the sentences presented. As mentioned in Section 4.2, the work sheets also included exercises constructed with sentences belonging to one CEFR level higher and lower than students\u2019 level. This allowed us to further assess whether the CEFR levels suggested based on the L2 complexity criterion were appropriate. We display in Figure 2 students\u2019 performance based on the CEFR level of exercise items comparing the system-assigned and the teacher-suggested CEFR levels for the items.\nAs Figure 2 shows, at A1 level students answered a larger amount of items according to the CEFR level determined by teachers (63%, vs. 48% with the system-assigned CEFR). The percentage of correct answers at A2 and B1 levels, however, showed more consistency with the levels assigned by our L2 complexity criterion: 64% (A2) and 69% (B1) correct answers based on our system\u2019s CEFR levels, vs. 60% (A2) and 56% (B1) with teacher-assigned levels. When considering these scores, however, it is worth noting that both teachers and the system were assessing only seed sentence difficulty, not the overall difficulty of the exercises. A few additional aspects play a role in determining the difficulty of exercise items, e.g. the selected distractors (Beinborn et al., 2014), nevertheless the observed tendencies in error rates provide useful insights into the suitability of seed sentences in terms of L2 complexity."}, {"heading": "5. Conclusion", "text": "We presented a comprehensive framework and its implementation for selecting sentences useful in the L2 learning context. The framework, among others, includes the assessment of L2 complexity in sentences and their independence of the surrounding context, both of which are relevant for a wide range of application scenarios. To\nSentence selection for exercises 21\nour knowledge, this is the first comprehensive study addressing automatic seed sentence selection for L2 learning exercises. We invested considerable effort into creating a system that would yield pedagogically more relevant results. We conducted an empirical evaluation with L2 teachers and learners to gain insights into how successfully the proposed framework can be used for the identification of seed sentences for L2 exercises. Although the sample size was somewhat limited, the evaluation yielded very promising results. On average, the selected sentences lived up to teachers\u2019 expectations on L2 complexity, context independence and overall suitability. The exercises constructed with the use of the selected sentences were overall somewhat hard for beginners, but they were of an appropriate difficulty level for learners at subsequent stages. Moreover, learners\u2019 error rates at some levels correlated even slightly better with the CEFR levels predicted by our system than the averaged levels assigned by teachers. All in all, the evaluation indicated that the described system has good potentials to enhance language instruction by either assisting teaching professional when creating practice material or by providing self-learning opportunities for learners in the form of automatically generated exercises. Although our main focus was on seed sentences selection, the proposed system can be useful also for the identification of dictionary example sentences.\nFuture work could include a version of the system aware of word senses, both as search terms and as entries in the word lists applied. This would also enable searching\n22 TAL. Volume 57 \u2013 n\u25e63/2016\nfor sentences belonging to specific topics or domains. Moreover, additional information about learners\u2019 lexical knowledge could be incorporated, for example, based on learner-written essays. Another valuable direction of further research would be the extension of the algorithm to multiple languages, for example through the use of universal POS and dependency tags. Finally, collecting additional data on how learners perform on the exercises constructed out of the selected sentences could also provide further indication on the quality and usefulness of the proposed algorithm."}, {"heading": "6. References", "text": "Arregik I. A., Automatic Exercise Generation Based on Corpora and Natural Language Processing Techniques, PhD thesis, Universidad del Pa\u00eds Vasco, 2011.\nBarsalou L. W., \u201cContext-independent and context-dependent information in concepts\u201d, Memory & Cognition, vol. 10, no 1, p. 82-93, 1982.\nBeinborn L., Zesch T., Gurevych I., \u201cPredicting the difficulty of language proficiency tests\u201d, Transactions of the Association for Computational Linguistics, vol. 2, p. 517-529, 2014.\nBj\u00f6rnsson C. H., L\u00e4sbarhet, Liber, 1968.\nBordag S., \u201cA comparison of co-occurrence and similarity measures as simulations of context\u201d, International Conference on Intelligent Text Processing and Computational Linguistics, Springer, p. 52-63, 2008.\nBorin L., Forsberg M., L\u00f6nngren L., \u201cSALDO: a touch of yin to WordNet\u2019s yang\u201d, Language Resources and Evaluation, vol. 47, no 4, p. 1191-1211, 2013.\nBorin L., Forsberg M., Roxendal J., \u201cKorp - the corpus infrastructure of Spr\u00e5kbanken\u201d, LREC, p. 474-478, 2012.\nBranco A., Rodrigues J., Costa F., Silva J., Vaz R., \u201cRolling out text categorization for language learning assessment supported by language technology\u201d, Computational Processing of the Portuguese Language, Springer, p. 256-261, 2014.\nBurstein J., Shore J., Sabatini J., Moulder B., Holtzman S., Pedersen T., \u201cThe language musesm\nsystem: linguistically focused instructional authoring\u201d, ETS Research Report Series, 2012.\nCobb T., \u201cIs there any measurable learning from hands-on concordancing?\u201d, System, vol. 25, no 3, p. 301-315, 1997.\nCollins-Thompson K., \u201cComputational assessment of text readability: a survey of current and future research\u201d, Recent Advances in Automatic Readability Assessment and Text Simplification. International Journal of Applied Linguistics, vol. 6, p. 97-135, 2014.\nCollins-Thompson K., Callan J. P., \u201cA language modeling approach to predicting reading difficulty\u201d, HLT-NAACL, p. 193-200, 2004.\nCouncil of Europe, Common European Framework of Reference for Languages: Learning, Teaching, Assessment, Press Syndicate of the University of Cambridge, 2001.\nCresswell A., \u201cGetting to \u2019know\u2019 connectors? Evaluating data-driven learning in a writing skills course\u201d, Language and Computers, vol. 61, no 1, p. 267-287, 2007.\nCrocker L., Algina J., Introduction to classical and modern test theory., ERIC, 1986.\nDeKeyser R., Practice in a Second Language: Perspectives from Applied Linguistics and Cognitive Psychology, Cambridge University Press, 2007.\nSentence selection for exercises 23\nDell\u2019Orletta F., Montemagni S., Venturi G., \u201cREAD-IT: Assessing readability of Italian texts with a view to text simplification\u201d, Proceedings of the Second Workshop on Speech and Language Processing for Assistive Technologies, p. 73-83, 2011.\nDell\u2019Orletta F., Wieling M., Cimino A., Venturi G., Montemagni S., \u201cAssessing the readability of sentences: which corpora and features?\u201d, ACL 2014, vol. , p. 163, 2014.\nDidakowski J., Lemnitzer L., Geyken A., \u201cAutomatic example sentence extraction for a contemporary German dictionary\u201d, Proceedings EURALEX, p. 343-349, 2012.\nFalkenjack J., Heimann M\u00fchlenbock K., J\u00f6nsson A., \u201cFeatures indicating readability in Swedish text\u201d, Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013), p. 27-40, 2013.\nFasth C., Kannermark A., Form i focus: \u00f6vningsbok i svensk grammatik. Del B, Folkuniv. F\u00f6rlag, Lund, 1997.\nFran\u00e7ois T., Fairon C., \u201cAn AI readability formula for French as a foreign language\u201d, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, p. 466-477, 2012.\nFran\u00e7ois T., Volodina E., Pil\u00e1n I., Tack A., \u201cSVALex: a CEFR-graded lexical resource for Swedish foreign and second language learners\u201d, Proceedings of LREC, 2016.\nFrey B. B., Petersen S., Edwards L. M., Pedrotti J. T., Peyton V., \u201cItem-writing rules: collective wisdom\u201d, Teaching and Teacher Education, vol. 21, no 4, p. 357-364, 2005.\nGeyken A., P\u00f6litz C., Bartz T., \u201cUsing a Maximum Entropy Classifier to link \u201cgood\u201d corpus examples to dictionary senses\u201d, Electronic Lexicography in the 21st Century: Linking Lexical Data in the Digital Age. Proceedings of the eLex 2015 conference, p. 304-314, 2015.\nGibson E., \u201cLinguistic complexity: locality of syntactic dependencies\u201d, Cognition, vol. 68, no 1, p. 1-76, 1998.\nGraesser A. C., McNamara D. S., Kulikowich J. M., \u201cCoh-Metrix providing multilevel analyses of text characteristics\u201d, Educational Researcher, vol. 40, no 5, p. 223-234, 2011.\nGray J., The Construction of English: Culture, Consumerism and Promotion in the ELT Global Coursebook, Palgrave Macmillan, 2010.\nHeilman M. J., Collins-Thompson K., Callan J., Eskenazi M., \u201cCombining lexical and grammatical features to improve readability measures for first and second language texts\u201d, Proceedings of NAACL HLT, p. 460-467, 2007.\nHeimann M\u00fchlenbock K., \u201cI see what you mean\u201d, Data Linguistica 24, 2013.\nHeppin K. F., Gronostaj M. T., \u201cThe rocky road towards a Swedish FrameNet \u2013 creating SweFN\u201d, LREC, p. 256-261, 2012.\nHuang Y.-T., Chang H.-P., Sun Y., Chen M. C., \u201cA robust estimation scheme of reading difficulty for second language learners\u201d, 11th IEEE International Conference on Advanced Learning Technologies (ICALT), IEEE, p. 58-62, 2011.\nHultman T. G., Westman M., Gymnasistsvenska, Liber, 1977.\nHus\u00e1k M., Automatic Retrieval of Good Dictionary Examples, Bachelor Thesis, Brno., 2010.\nKilgarriff A., \u201cCorpora in the classroom without scaring the students\u201d, Proceedings from the 18th International Symposium on English Teaching, 2009.\nKilgarriff A., Hus\u00e1k M., McAdam K., Rundell M., Rychly\u0300 P., \u201cGDEX: automatically finding good dictionary examples in a corpus\u201d, Proceedings of Euralex, 2008.\n24 TAL. Volume 57 \u2013 n\u25e63/2016\nKilgarriff A., Rychly P., Smrz P., Tugwell D., \u201cThe Sketch Engine\u201d, Proceedings of Euralex, p. 105-116, 2004.\nKosem I., Hus\u00e1k M., McCarthy D., \u201cGDEX for Slovene\u201d, Electronic Lexicography in the 21st\nCentury: New Applications for New Users: Proceedings of eLex 2011, p. 151-159, 2011.\nLee J., Luo M., \u201cPersonalized exercises for preposition learning\u201d, Proceedings of ACL-2016 System Demonstrations, p. 115-120, 2016.\nLemnitzer L., P\u00f6litz C., Didakowski J., Geyken A., \u201cCombining a rule-based approach and machine learning in a good-example extraction task for the purpose of lexicographic work on contemporary standard German\u201d, Proceedings of eLex 2015, p. 21-31, 2015.\nLjube\u0161ic\u0301 N., Peronja M., \u201cPredicting corpus example quality via supervised machine learning\u201d, Electronic Lexicography in the 21st Century: Linking Lexical Data in the Digital Age. Proceedings of eLex 2015, p. 477-485, 2015.\nMitkov R., An Ha L., Karamanis N., \u201cA Computer-aided environment for generating multiplechoice test items\u201d, Nat. Lang. Eng., vol. 12, no 2, p. 177-194, 2006.\nO\u2019Keeffe A., McCarthy M., Carter R., From Corpus to Classroom: Language use and Language Teaching, Cambridge University Press, 2007.\nPil\u00e1n I., \u201cDetecting context dependence in exercise item candidates selected from corpora\u201d, Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, p. 151-161, 2016.\nPil\u00e1n I., Volodina E., Johansson R., \u201cRule-based and machine learning approaches for second language sentence-level readability\u201d, Proceedings of the 9th Workshop on Innovative Use of NLP for Building Educational Applications, p. 174-184, June, 2014.\nPil\u00e1n I., Vajjala S., Volodina E., \u201cA readable read: automatic assessment of language learning materials based on linguistic complexity\u201d, International Journal of Computational Linguistics and Applications, vol. 7, no 1, p. 143-159, 2016. Presented at CICLing 2015. <http://www.ijcla.bahripublications.com/2016-1/ IJCLA-2016-1-pp-143-159-preprint.pdf>.\nPil\u00e1n I., Volodina E., Johansson R., \u201cAutomatic selection of suitable sentences for language learning exercises\u201d, 20 Years of EUROCALL: Learning from the Past, Looking to the Future, Proceedings of EUROCALL, p. 218-225, 2013.\nPino J., Eskenazi M., \u201cSemi-automatic generation of cloze question distractors effect of students\u2019 L1\u201d, Proceedings of SLaTE, p. 65-68, 2009.\nSalesky E., Shen W., \u201cExploiting morphological, grammatical, and semantic correlates for improved text difficulty assessment\u201d, Proceedings of the 9th Workshop on Innovative Use of NLP for Building Educational Applications, p. 155-162, June, 2014.\nSchwarm S. E., Ostendorf M., \u201cReading level assessment using support vector machines and statistical language models\u201d, Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, p. 523-530, 2005.\nSegler T. M., Investigating the Selection of Example Sentences for Unknown Target Words in ICALL Reading Texts for L2 German, PhD thesis, University of Edinburgh, 2007.\nSmith S., Avinesh P., Kilgarriff A., \u201cGap-fill tests for language learners: corpus-driven item generation\u201d, Proceedings of ICON-2010: 8th International Conference on Natural Language Processing, p. 1-6, 2010.\nSentence selection for exercises 25\nStatistics Sweden, \u201cFinland och Irak de tv\u00e5 vanligaste f\u00f6delsel\u00e4nderna bland utrikes f\u00f6dda\u201d, 2016. <http://www.scb.se/sv_/Hitta-statistik/Artiklar/ Finland-och-Irak-de-tva-vanligaste-fodelselanderna-bland-utrikes-fodda>.\nSumita E., Sugaya F., Yamamoto S., \u201cMeasuring non-native speakers\u2019 proficiency of English by using a test with automatically-generated fill-in-the-blank questions\u201d, Proceedings of the 2nd workshop on Building Educational Applications Using NLP, p. 61-68, 2005.\nSung Y.-T., Lin W.-C., Dyson S. B., Chang K.-E., Chen Y.-C., \u201cLeveling L2 texts through readability: combining multilevel linguistic features with the CEFR\u201d, The Modern Language Journal, vol. 99, no 2, p. 371-391, 2015.\nTeleman U., Hellberg S., Andersson E., Svenska Akademiens grammatik, Svenska Akademien/Norstedts ordbok (distr.), 1999.\nThompson B., Levitov J. E., \u201cUsing microcomputers to score and evaluate items\u201d, Collegiate Microcomputer, vol. 3, no 2, p. 163-168, 1985.\nTiberius C., Kinable D., \u201cUsing and configuring GDEX for Dutch\u201d, 2015. Slides presented at the ENeL COST Action meeting, <http://www.elexicography.eu/wp-content/ uploads/2015/04/ENeLWG3_GDEX4Dutch.pdf>.\nVajjala S., Meurers D., \u201cOn improving the accuracy of readability classification using insights from second language acquisition\u201d, Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications, p. 163-173, 2012.\nVajjala S., Meurers D., \u201cAssessing the relative reading level of sentence pairs for text simplification\u201d, Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL-14), 2014.\nVelleman E., van der Geest T., \u201cOnline test tool to determine the CEFR reading comprehension level of text\u201d, Procedia Computer Science, vol. 27, p. 350-358, 2014.\nVolodina E., Johansson R., Johansson Kokkinakis S., \u201cSemi-automatic selection of best corpus examples for Swedish: initial algorithm evaluation\u201d, Proceedings of the Workshop on NLP for CALL, vol. 80, p. 59-70, 2012.\nVolodina E., Kokkinakis S. J., \u201cIntroducing the Swedish Kelly-list, a new lexical e-resource for Swedish.\u201d, LREC, p. 1040-1046, 2012.\nVolodina E., Pil\u00e1n I., Eide S. R., Heidarsson H., \u201cYou get what you annotate: a pedagogically annotated corpus of coursebooks for Swedish as a second language\u201d, Proceedings of the 3rd workshop on NLP for CALL, vol. 107, p. 128-142, 2014.\nWebber B., Stone M., Joshi A., Knott A., \u201cAnaphora and discourse structure\u201d, Computational Linguistics, vol. 29, no 4, p. 545-587, 2003.\nWojatzki M., Melamud O., Zesch T., \u201cBundled gap filling: a new paradigm for unambiguous cloze exercises\u201d, Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, p. 172-181, 2016.\nXia M., Kochmar E., Briscoe T., \u201cText readability assessment for second language learners\u201d, Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, p. 12-22, 2016.\nZhang L., Liu Z., Ni J., \u201cFeature-based assessment of text readability\u201d, 7th International Conference on Internet Computing for Engineering and Science (ICICSE), IEEE, p. 51-54, 2013."}], "references": [{"title": "Automatic Exercise Generation Based on Corpora and Natural Language Processing Techniques", "author": ["A. Arregik I"], "venue": "PhD thesis, Universidad del Pai\u0301s Vasco,", "citeRegEx": "I.,? \\Q2011\\E", "shortCiteRegEx": "I.", "year": 2011}, {"title": "Context-independent and context-dependent information in concepts", "author": ["W. Barsalou L"], "venue": "Memory & Cognition,", "citeRegEx": "L.,? \\Q1982\\E", "shortCiteRegEx": "L.", "year": 1982}, {"title": "Predicting the difficulty of language proficiency tests", "author": ["L. Beinborn", "T. Zesch", "I. Gurevych"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Beinborn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beinborn et al\\.", "year": 2014}, {"title": "A comparison of co-occurrence and similarity measures as simulations of context", "author": ["S. Bordag"], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics,", "citeRegEx": "Bordag,? \\Q2008\\E", "shortCiteRegEx": "Bordag", "year": 2008}, {"title": "Korp - the corpus infrastructure of Spr\u00e5kbanken", "author": ["L. Borin", "M. Forsberg", "J. Roxendal"], "venue": "LREC, p", "citeRegEx": "Borin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Borin et al\\.", "year": 2012}, {"title": "Rolling out text categorization for language learning assessment supported by language technology", "author": ["A. Branco", "J. Rodrigues", "F. Costa", "J. Silva", "R. Vaz"], "venue": "Computational Processing of the Portuguese Language,", "citeRegEx": "Branco et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Branco et al\\.", "year": 2014}, {"title": "Is there any measurable learning from hands-on concordancing?", "author": ["T. Cobb"], "venue": "System, vol. 25,", "citeRegEx": "Cobb,? \\Q1997\\E", "shortCiteRegEx": "Cobb", "year": 1997}, {"title": "Computational assessment of text readability: a survey of current and future research\u201d, Recent Advances in Automatic Readability Assessment and Text Simplification", "author": ["K. Collins-Thompson"], "venue": "International Journal of Applied Linguistics,", "citeRegEx": "Collins.Thompson,? \\Q2014\\E", "shortCiteRegEx": "Collins.Thompson", "year": 2014}, {"title": "A language modeling approach to predicting reading difficulty", "author": ["K. Collins-Thompson", "P. Callan J"], "venue": null, "citeRegEx": "Collins.Thompson and J.,? \\Q2004\\E", "shortCiteRegEx": "Collins.Thompson and J.", "year": 2004}, {"title": "Getting to \u2019know\u2019 connectors? Evaluating data-driven learning in a writing skills course", "author": ["A. Cresswell"], "venue": "Language and Computers,", "citeRegEx": "Cresswell,? \\Q2007\\E", "shortCiteRegEx": "Cresswell", "year": 2007}, {"title": "Introduction to classical and modern test theory", "author": ["L. Crocker", "J. Algina"], "venue": "ERIC,", "citeRegEx": "Crocker and Algina,? \\Q1986\\E", "shortCiteRegEx": "Crocker and Algina", "year": 1986}, {"title": "Practice in a Second Language: Perspectives from Applied Linguistics and Cognitive Psychology", "author": ["R. DeKeyser"], "venue": null, "citeRegEx": "DeKeyser,? \\Q2007\\E", "shortCiteRegEx": "DeKeyser", "year": 2007}, {"title": "READ-IT: Assessing readability of Italian texts with a view to text simplification", "author": ["F. Dell\u2019Orletta", "S. Montemagni", "G. Venturi"], "venue": "Proceedings of the Second Workshop on Speech and Language Processing for Assistive Technologies,", "citeRegEx": "Dell.Orletta et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dell.Orletta et al\\.", "year": 2011}, {"title": "Assessing the readability of sentences: which corpora and features?", "author": ["F. Dell\u2019Orletta", "M. Wieling", "A. Cimino", "G. Venturi", "S. Montemagni"], "venue": "ACL 2014,", "citeRegEx": "Dell.Orletta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dell.Orletta et al\\.", "year": 2014}, {"title": "Automatic example sentence extraction for a contemporary German dictionary", "author": ["J. Didakowski", "L. Lemnitzer", "A. Geyken"], "venue": "Proceedings EURALEX,", "citeRegEx": "Didakowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Didakowski et al\\.", "year": 2012}, {"title": "Features indicating readability in Swedish text", "author": ["J. Falkenjack", "K. Heimann M\u00fchlenbock", "A. J\u00f6nsson"], "venue": "Proceedings of the 19 Nordic Conference of Computational Linguistics (NODALIDA", "citeRegEx": "Falkenjack et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Falkenjack et al\\.", "year": 2013}, {"title": "Form i focus: \u00f6vningsbok i svensk grammatik", "author": ["C. Fasth", "A. Kannermark"], "venue": "Del B, Folkuniv. Fo\u0308rlag,", "citeRegEx": "Fasth and Kannermark,? \\Q1997\\E", "shortCiteRegEx": "Fasth and Kannermark", "year": 1997}, {"title": "An AI readability formula for French as a foreign language", "author": ["T. Fran\u00e7ois", "C. Fairon"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Fran\u00e7ois and Fairon,? \\Q2012\\E", "shortCiteRegEx": "Fran\u00e7ois and Fairon", "year": 2012}, {"title": "SVALex: a CEFR-graded lexical resource for Swedish foreign and second language learners", "author": ["T. Fran\u00e7ois", "E. Volodina", "I. Pil\u00e1n", "A. Tack"], "venue": "Proceedings of LREC,", "citeRegEx": "Fran\u00e7ois et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fran\u00e7ois et al\\.", "year": 2016}, {"title": "Using a Maximum Entropy Classifier to link \u201cgood\u201d corpus examples to dictionary senses\u201d, Electronic Lexicography in the 21 Century: Linking Lexical Data in the Digital Age", "author": ["A. Geyken", "C. P\u00f6litz", "T. Bartz"], "venue": "Proceedings of the eLex 2015 conference,", "citeRegEx": "Geyken et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geyken et al\\.", "year": 2015}, {"title": "Linguistic complexity: locality of syntactic dependencies", "author": ["E. Gibson"], "venue": "Cognition, vol. 68,", "citeRegEx": "Gibson,? \\Q1998\\E", "shortCiteRegEx": "Gibson", "year": 1998}, {"title": "Coh-Metrix providing multilevel analyses of text characteristics", "author": ["C. Graesser A", "S. McNamara D", "M. Kulikowich J"], "venue": "Educational Researcher,", "citeRegEx": "A. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "A. et al\\.", "year": 2011}, {"title": "The Construction of English: Culture, Consumerism and Promotion in the ELT Global Coursebook", "author": ["J. Gray"], "venue": "Palgrave Macmillan,", "citeRegEx": "Gray,? \\Q2010\\E", "shortCiteRegEx": "Gray", "year": 2010}, {"title": "Combining lexical and grammatical features to improve readability measures for first and second language texts", "author": ["J. Heilman M", "K. Collins-Thompson", "J. Callan", "M. Eskenazi"], "venue": "Proceedings of NAACL HLT,", "citeRegEx": "M. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "M. et al\\.", "year": 2007}, {"title": "I see what you mean", "author": ["K. Heimann M\u00fchlenbock"], "venue": "Data Linguistica", "citeRegEx": "M\u00fchlenbock,? \\Q2013\\E", "shortCiteRegEx": "M\u00fchlenbock", "year": 2013}, {"title": "The rocky road towards a Swedish FrameNet \u2013 creating SweFN", "author": ["F. Heppin K", "T. Gronostaj M"], "venue": "LREC, p", "citeRegEx": "K. and M.,? \\Q2012\\E", "shortCiteRegEx": "K. and M.", "year": 2012}, {"title": "A robust estimation scheme of reading difficulty for second language learners", "author": ["Huang Y.-T", "Chang H.-P", "Sun Y", "Chen M. C"], "venue": "IEEE International Conference on Advanced Learning Technologies (ICALT),", "citeRegEx": "Y..T. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Y..T. et al\\.", "year": 2011}, {"title": "Automatic Retrieval of Good Dictionary Examples", "author": ["M. Hus\u00e1k"], "venue": "Bachelor Thesis, Brno.,", "citeRegEx": "Hus\u00e1k,? \\Q2010\\E", "shortCiteRegEx": "Hus\u00e1k", "year": 2010}, {"title": "Corpora in the classroom without scaring the students", "author": ["A. Kilgarriff"], "venue": "Proceedings from the 18 International Symposium on English Teaching,", "citeRegEx": "Kilgarriff,? \\Q2009\\E", "shortCiteRegEx": "Kilgarriff", "year": 2009}, {"title": "GDEX: automatically finding good dictionary examples in a corpus", "author": ["A. Kilgarriff", "M. Hus\u00e1k", "K. McAdam", "M. Rundell", "P. Rychl\u1ef3"], "venue": "Proceedings of Euralex,", "citeRegEx": "Kilgarriff et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kilgarriff et al\\.", "year": 2008}, {"title": "The Sketch Engine", "author": ["A. Kilgarriff", "P. Rychly", "P. Smrz", "D. Tugwell"], "venue": "Proceedings of Euralex,", "citeRegEx": "Kilgarriff et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kilgarriff et al\\.", "year": 2004}, {"title": "Personalized exercises for preposition learning", "author": ["J. Lee", "M. Luo"], "venue": "Proceedings of ACL-2016 System Demonstrations,", "citeRegEx": "Lee and Luo,? \\Q2016\\E", "shortCiteRegEx": "Lee and Luo", "year": 2016}, {"title": "Combining a rule-based approach and machine learning in a good-example extraction task for the purpose of lexicographic work on contemporary standard German", "author": ["L. Lemnitzer", "C. P\u00f6litz", "J. Didakowski", "A. Geyken"], "venue": "Proceedings of eLex", "citeRegEx": "Lemnitzer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lemnitzer et al\\.", "year": 2015}, {"title": "Predicting corpus example quality via supervised machine learning\u201d, Electronic Lexicography in the 21 Century: Linking Lexical Data in the Digital Age", "author": ["N. Ljube\u0161i\u0107", "M. Peronja"], "venue": "Proceedings of eLex", "citeRegEx": "Ljube\u0161i\u0107 and Peronja,? \\Q2015\\E", "shortCiteRegEx": "Ljube\u0161i\u0107 and Peronja", "year": 2015}, {"title": "A Computer-aided environment for generating multiplechoice test items", "author": ["R. Mitkov", "L. An Ha", "N. Karamanis"], "venue": "Nat. Lang. Eng., vol. 12,", "citeRegEx": "Mitkov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mitkov et al\\.", "year": 2006}, {"title": "From Corpus to Classroom: Language use and Language Teaching", "author": ["A. O\u2019Keeffe", "M. McCarthy", "R. Carter"], "venue": null, "citeRegEx": "O.Keeffe et al\\.,? \\Q2007\\E", "shortCiteRegEx": "O.Keeffe et al\\.", "year": 2007}, {"title": "Detecting context dependence in exercise item candidates selected from corpora", "author": ["I. Pil\u00e1n"], "venue": "Proceedings of the 11 Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Pil\u00e1n,? \\Q2016\\E", "shortCiteRegEx": "Pil\u00e1n", "year": 2016}, {"title": "Rule-based and machine learning approaches for second language sentence-level readability", "author": ["I. Pil\u00e1n", "E. Volodina", "R. Johansson"], "venue": "Proceedings of the 9 Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Pil\u00e1n et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pil\u00e1n et al\\.", "year": 2014}, {"title": "A readable read: automatic assessment of language learning materials based on linguistic complexity", "author": ["I. Pil\u00e1n", "S. Vajjala", "E. Volodina"], "venue": "International Journal of Computational Linguistics and Applications,", "citeRegEx": "Pil\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pil\u00e1n et al\\.", "year": 2015}, {"title": "Automatic selection of suitable sentences for language learning exercises\u201d, 20 Years of EUROCALL: Learning from the Past, Looking to the Future", "author": ["I. Pil\u00e1n", "E. Volodina", "R. Johansson"], "venue": "Proceedings of EUROCALL,", "citeRegEx": "Pil\u00e1n et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pil\u00e1n et al\\.", "year": 2013}, {"title": "Semi-automatic generation of cloze question distractors effect of students\u2019 L1", "author": ["J. Pino", "M. Eskenazi"], "venue": "Proceedings of SLaTE,", "citeRegEx": "Pino and Eskenazi,? \\Q2009\\E", "shortCiteRegEx": "Pino and Eskenazi", "year": 2009}, {"title": "Exploiting morphological, grammatical, and semantic correlates for improved text difficulty assessment", "author": ["E. Salesky", "W. Shen"], "venue": "Proceedings of the 9 Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Salesky and Shen,? \\Q2014\\E", "shortCiteRegEx": "Salesky and Shen", "year": 2014}, {"title": "Reading level assessment using support vector machines and statistical language models", "author": ["E. Schwarm S", "M. Ostendorf"], "venue": "Proceedings of the 43 Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "S. and Ostendorf,? \\Q2005\\E", "shortCiteRegEx": "S. and Ostendorf", "year": 2005}, {"title": "Investigating the Selection of Example Sentences for Unknown Target Words in ICALL Reading Texts for L2 German", "author": ["M. Segler T"], "venue": "PhD thesis, University of Edinburgh,", "citeRegEx": "T.,? \\Q2007\\E", "shortCiteRegEx": "T.", "year": 2007}, {"title": "Gap-fill tests for language learners: corpus-driven item generation", "author": ["S. Smith", "P. Avinesh", "A. Kilgarriff"], "venue": "Proceedings of ICON-2010: 8 International Conference on Natural Language Processing,", "citeRegEx": "Smith et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2010}, {"title": "Measuring non-native speakers\u2019 proficiency of English by using a test with automatically-generated fill-in-the-blank questions", "author": ["E. Sumita", "F. Sugaya", "S. Yamamoto"], "venue": "Proceedings of the 2 workshop on Building Educational Applications Using NLP,", "citeRegEx": "Sumita et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sumita et al\\.", "year": 2005}, {"title": "Leveling L2 texts through readability: combining multilevel linguistic features with the CEFR", "author": ["Sung Y.-T", "Lin W.-C", "Dyson S. B", "Chang K.-E", "Chen Y.-C"], "venue": "The Modern Language Journal,", "citeRegEx": "Y..T. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Y..T. et al\\.", "year": 2015}, {"title": "Using microcomputers to score and evaluate items", "author": ["B. Thompson", "E. Levitov J"], "venue": "Collegiate Microcomputer,", "citeRegEx": "Thompson and J.,? \\Q1985\\E", "shortCiteRegEx": "Thompson and J.", "year": 1985}, {"title": "Using and configuring GDEX for Dutch", "author": ["C. Tiberius", "D. Kinable"], "venue": null, "citeRegEx": "Tiberius and Kinable,? \\Q2015\\E", "shortCiteRegEx": "Tiberius and Kinable", "year": 2015}, {"title": "On improving the accuracy of readability classification using insights from second language acquisition", "author": ["S. Vajjala", "D. Meurers"], "venue": "Proceedings of the 7 Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Vajjala and Meurers,? \\Q2012\\E", "shortCiteRegEx": "Vajjala and Meurers", "year": 2012}, {"title": "Assessing the relative reading level of sentence pairs for text simplification", "author": ["S. Vajjala", "D. Meurers"], "venue": "Proceedings of the 14 Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Vajjala and Meurers,? \\Q2014\\E", "shortCiteRegEx": "Vajjala and Meurers", "year": 2014}, {"title": "Online test tool to determine the CEFR reading comprehension level of text", "author": ["E. Velleman", "T. van der Geest"], "venue": "Procedia Computer Science,", "citeRegEx": "Velleman and Geest,? \\Q2014\\E", "shortCiteRegEx": "Velleman and Geest", "year": 2014}, {"title": "Semi-automatic selection of best corpus examples for Swedish: initial algorithm evaluation", "author": ["E. Volodina", "R. Johansson", "S. Johansson Kokkinakis"], "venue": "Proceedings of the Workshop on NLP for CALL,", "citeRegEx": "Volodina et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Volodina et al\\.", "year": 2012}, {"title": "Introducing the Swedish Kelly-list, a new lexical e-resource for Swedish.", "author": ["E. Volodina", "J. Kokkinakis S"], "venue": "LREC, p", "citeRegEx": "Volodina and S.,? \\Q2012\\E", "shortCiteRegEx": "Volodina and S.", "year": 2012}, {"title": "You get what you annotate: a pedagogically annotated corpus of coursebooks for Swedish as a second language", "author": ["E. Volodina", "I. Pil\u00e1n", "R. Eide S", "H. Heidarsson"], "venue": "Proceedings of the 3 workshop on NLP for CALL,", "citeRegEx": "Volodina et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Volodina et al\\.", "year": 2014}, {"title": "Bundled gap filling: a new paradigm for unambiguous cloze exercises", "author": ["M. Wojatzki", "O. Melamud", "T. Zesch"], "venue": "Proceedings of the 11 Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Wojatzki et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wojatzki et al\\.", "year": 2016}, {"title": "Text readability assessment for second language learners", "author": ["M. Xia", "E. Kochmar", "T. Briscoe"], "venue": "Proceedings of the 11 Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Xia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "Practice plays an important role in L2 learning for the development of both receptive and productive skills (DeKeyser, 2007).", "startOffset": 108, "endOffset": 124}, {"referenceID": 6, "context": "Corpora offer a large amount of diverse examples at a low cost, and their use has been shown to have a positive effect on learners\u2019 progress (Cobb, 1997; Cresswell, 2007).", "startOffset": 141, "endOffset": 170}, {"referenceID": 9, "context": "Corpora offer a large amount of diverse examples at a low cost, and their use has been shown to have a positive effect on learners\u2019 progress (Cobb, 1997; Cresswell, 2007).", "startOffset": 141, "endOffset": 170}, {"referenceID": 28, "context": "Moreover, corpora are evidence of real-life language use which, however, might be hard for learners to process (Kilgarriff, 2009).", "startOffset": 111, "endOffset": 129}, {"referenceID": 1, "context": "Practice plays an important role in L2 learning for the development of both receptive and productive skills (DeKeyser, 2007). Corpora as potential practice material are readily available in large quantities, however, their use in L2 teaching has been both supported and opposed in previous years, O\u2019Keeffe et al. (2007) present an overview of this debate.", "startOffset": 36, "endOffset": 320}, {"referenceID": 44, "context": "(Arregik, 2011; Smith et al., 2010; Sumita et al., 2005)).", "startOffset": 0, "endOffset": 56}, {"referenceID": 45, "context": "(Arregik, 2011; Smith et al., 2010; Sumita et al., 2005)).", "startOffset": 0, "endOffset": 56}, {"referenceID": 29, "context": "These have been previously explored mostly in a lexicographic context (Kilgarriff et al., 2008), but they are also relevant for language teaching (Kilgarriff, 2009).", "startOffset": 70, "endOffset": 95}, {"referenceID": 28, "context": ", 2008), but they are also relevant for language teaching (Kilgarriff, 2009).", "startOffset": 58, "endOffset": 76}, {"referenceID": 39, "context": "Both context independence and L2 complexity emerged as a main reason for discarding candidate sentences in previous evaluations (Arregik, 2011; Pil\u00e1n et al., 2013), but thorough methods targeting these aspects have not been proposed up to date to our knowledge.", "startOffset": 128, "endOffset": 163}, {"referenceID": 27, "context": "GDEX, Good Dictionary Examples (Hus\u00e1k, 2010; Kilgarriff et al., 2008) is an algorithm for selecting sentences from corpora for the purposes of illustrating the", "startOffset": 31, "endOffset": 69}, {"referenceID": 29, "context": "GDEX, Good Dictionary Examples (Hus\u00e1k, 2010; Kilgarriff et al., 2008) is an algorithm for selecting sentences from corpora for the purposes of illustrating the", "startOffset": 31, "endOffset": 69}, {"referenceID": 52, "context": "GDEX has also inspired a Swedish algorithm for sentence selection (Volodina et al., 2012) and it has been employed also for generating gap-fill exercises (Smith et al.", "startOffset": 66, "endOffset": 89}, {"referenceID": 44, "context": ", 2012) and it has been employed also for generating gap-fill exercises (Smith et al., 2010).", "startOffset": 72, "endOffset": 92}, {"referenceID": 19, "context": "Furthermore, a number of machine learning approaches have been explored for these purposes in recent years (Geyken et al., 2015; Lemnitzer et al., 2015; Ljube\u0161i\u0107 and Peronja, 2015).", "startOffset": 107, "endOffset": 180}, {"referenceID": 32, "context": "Furthermore, a number of machine learning approaches have been explored for these purposes in recent years (Geyken et al., 2015; Lemnitzer et al., 2015; Ljube\u0161i\u0107 and Peronja, 2015).", "startOffset": 107, "endOffset": 180}, {"referenceID": 33, "context": "Furthermore, a number of machine learning approaches have been explored for these purposes in recent years (Geyken et al., 2015; Lemnitzer et al., 2015; Ljube\u0161i\u0107 and Peronja, 2015).", "startOffset": 107, "endOffset": 180}, {"referenceID": 38, "context": "(2011) and Tiberius and Kinable (2015) explore GDEX configurations for Slovene and Dutch respectively, aiming at identifying the optimal parameter settings for these languages for lexicographic projects.", "startOffset": 11, "endOffset": 39}, {"referenceID": 13, "context": "Didakowski et al. (2012) propose an example selection algorithm similar to GDEX for German.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": ", 2015; Lemnitzer et al., 2015; Ljube\u0161i\u0107 and Peronja, 2015). Example sentence selection for illustrating lexical items has also been addressed from a language teaching perspective by Segler (2007), where a set of selection criteria used by teachers was modelled with logistic regression.", "startOffset": 8, "endOffset": 197}, {"referenceID": 45, "context": "Sentences used in exercises are also known as seed sentences (Sumita et al., 2005) or carrier sentences (Smith et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 44, "context": ", 2005) or carrier sentences (Smith et al., 2010) in the Intelligent, i.", "startOffset": 29, "endOffset": 49}, {"referenceID": 45, "context": "In some cases, sentences are only required to contain a lexical item or a linguistic pattern that constitutes the target of the exercise, but context dependence and L2 complexity are not explicitly addressed (Sumita et al., 2005; Mitkov et al., 2006; Arregik, 2011; Wojatzki et al., 2016).", "startOffset": 208, "endOffset": 288}, {"referenceID": 34, "context": "In some cases, sentences are only required to contain a lexical item or a linguistic pattern that constitutes the target of the exercise, but context dependence and L2 complexity are not explicitly addressed (Sumita et al., 2005; Mitkov et al., 2006; Arregik, 2011; Wojatzki et al., 2016).", "startOffset": 208, "endOffset": 288}, {"referenceID": 55, "context": "In some cases, sentences are only required to contain a lexical item or a linguistic pattern that constitutes the target of the exercise, but context dependence and L2 complexity are not explicitly addressed (Sumita et al., 2005; Mitkov et al., 2006; Arregik, 2011; Wojatzki et al., 2016).", "startOffset": 208, "endOffset": 288}, {"referenceID": 40, "context": "from WordNet (Pino and Eskenazi, 2009).", "startOffset": 13, "endOffset": 38}, {"referenceID": 0, "context": "be targeted in the exercises and they are not adjusted to finer-grained L2 learning levels. A system using GDEX for seed sentence selection is described in Smith et al. (2010), who underline the importance of the well-formedness of a sentence and determine a sufficient amount of context in terms of sentence length.", "startOffset": 72, "endOffset": 176}, {"referenceID": 0, "context": "be targeted in the exercises and they are not adjusted to finer-grained L2 learning levels. A system using GDEX for seed sentence selection is described in Smith et al. (2010), who underline the importance of the well-formedness of a sentence and determine a sufficient amount of context in terms of sentence length. Lee and Luo (2016) describe an ICALL system for fill-in-the-blanks preposition learning exercises, where seed sentences are checked for their lexical difficulty based on the level of the words according to a graded vocabulary lists.", "startOffset": 72, "endOffset": 336}, {"referenceID": 0, "context": "Lee and Luo (2016) describe an ICALL system for fill-in-the-blanks preposition learning exercises, where seed sentences are checked for their lexical difficulty based on the level of the words according to a graded vocabulary lists. Pil\u00e1n et al. (2014) present and compare two algorithms for candidate sentence selection for Swedish, using both rule-based and machine learning methods.", "startOffset": 31, "endOffset": 253}, {"referenceID": 49, "context": "In recent years a number of NLP-based readability models have been proposed not only for English (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Graesser et al., 2011; Vajjala and Meurers, 2012; Collins-Thompson, 2014), but also for other languages, e.", "startOffset": 97, "endOffset": 235}, {"referenceID": 7, "context": "In recent years a number of NLP-based readability models have been proposed not only for English (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Graesser et al., 2011; Vajjala and Meurers, 2012; Collins-Thompson, 2014), but also for other languages, e.", "startOffset": 97, "endOffset": 235}, {"referenceID": 12, "context": "Italian (Dell\u2019Orletta et al., 2011) and Swedish (Heimann M\u00fchlenbock, 2013).", "startOffset": 8, "endOffset": 35}, {"referenceID": 39, "context": "Although the majority of previous work focuses primarily on text-level analysis, the concept of sentence-level readability has also emerged and attracted an increasing interest in recent years (Pil\u00e1n et al., 2013; Vajjala and Meurers, 2014; Dell\u2019Orletta et al., 2014).", "startOffset": 193, "endOffset": 267}, {"referenceID": 50, "context": "Although the majority of previous work focuses primarily on text-level analysis, the concept of sentence-level readability has also emerged and attracted an increasing interest in recent years (Pil\u00e1n et al., 2013; Vajjala and Meurers, 2014; Dell\u2019Orletta et al., 2014).", "startOffset": 193, "endOffset": 267}, {"referenceID": 13, "context": "Although the majority of previous work focuses primarily on text-level analysis, the concept of sentence-level readability has also emerged and attracted an increasing interest in recent years (Pil\u00e1n et al., 2013; Vajjala and Meurers, 2014; Dell\u2019Orletta et al., 2014).", "startOffset": 193, "endOffset": 267}, {"referenceID": 12, "context": "(2016), Fran\u00e7ois and Fairon (2012). In recent years a number of NLP-based readability models have been proposed not only for English (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Graesser et al.", "startOffset": 8, "endOffset": 35}, {"referenceID": 41, "context": "The prediction of proficiency levels for L2 teaching materials using supervised machine learning methods has been explored for English (Heilman et al., 2007; Huang et al., 2011; Zhang et al., 2013; Salesky and Shen, 2014; Xia et al., 2016), French (Fran\u00e7ois and Fairon, 2012), Portuguese (Branco et al.", "startOffset": 135, "endOffset": 239}, {"referenceID": 56, "context": "The prediction of proficiency levels for L2 teaching materials using supervised machine learning methods has been explored for English (Heilman et al., 2007; Huang et al., 2011; Zhang et al., 2013; Salesky and Shen, 2014; Xia et al., 2016), French (Fran\u00e7ois and Fairon, 2012), Portuguese (Branco et al.", "startOffset": 135, "endOffset": 239}, {"referenceID": 17, "context": ", 2016), French (Fran\u00e7ois and Fairon, 2012), Portuguese (Branco et al.", "startOffset": 16, "endOffset": 43}, {"referenceID": 5, "context": ", 2016), French (Fran\u00e7ois and Fairon, 2012), Portuguese (Branco et al., 2014), Chinese (Sung et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": "One of the most popular, easy-to-compute formulas is LIX (L\u00e4sbarhetsindex, \u2018Readability index\u2019) proposed by Bj\u00f6rnsson (1968). This measure combines the average number of words per sentence in the text with the percentage of long words, i.", "startOffset": 54, "endOffset": 125}, {"referenceID": 0, "context": "One of the most popular, easy-to-compute formulas is LIX (L\u00e4sbarhetsindex, \u2018Readability index\u2019) proposed by Bj\u00f6rnsson (1968). This measure combines the average number of words per sentence in the text with the percentage of long words, i.e. tokens consisting of more than six characters. Besides traditional formulas, supervised machine learning approaches have also been tested. A Swedish document-level readability model is described by Heimann M\u00fchlenbock (2013) and Falkenjack et al.", "startOffset": 54, "endOffset": 465}, {"referenceID": 0, "context": "One of the most popular, easy-to-compute formulas is LIX (L\u00e4sbarhetsindex, \u2018Readability index\u2019) proposed by Bj\u00f6rnsson (1968). This measure combines the average number of words per sentence in the text with the percentage of long words, i.e. tokens consisting of more than six characters. Besides traditional formulas, supervised machine learning approaches have also been tested. A Swedish document-level readability model is described by Heimann M\u00fchlenbock (2013) and Falkenjack et al. (2013). Pil\u00e1n et al.", "startOffset": 54, "endOffset": 494}, {"referenceID": 52, "context": "In general, the sources that served as basis for these criteria include previous literature (Section 2), L2 curricula and the qualitative results of previous user-based evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2014).", "startOffset": 181, "endOffset": 224}, {"referenceID": 37, "context": "In general, the sources that served as basis for these criteria include previous literature (Section 2), L2 curricula and the qualitative results of previous user-based evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2014).", "startOffset": 181, "endOffset": 224}, {"referenceID": 18, "context": "Sensitive vocabulary filtering and the use of word frequencies from SVALex (Fran\u00e7ois et al., 2016), a word list based on coursebook texts, are also novel aspects that we incorporated with the aim of making the sentence selection algorithm more pedagogically aware.", "startOffset": 75, "endOffset": 98}, {"referenceID": 1, "context": "We implemented a hybrid system which uses a combination of machine-learning methods for assessing L2 complexity and heuristic rules for all other criteria. The motivation behind using rules is, on the one hand, that certain linguistic elements are easily identifiable with such methods. On the other hand, a sufficient amount of training data encompassing the range of all possible exercise types would be extremely costly to create. Moreover, explicit rules make the sentence selection customizable to users\u2019 task-specific needs which increases the applicability of HitEx to a diverse set of situations. The criterion of L2 complexity has been implemented using machine learning methods since its assessment comprises multiple linguistic dimensions and data was available for approaching this sub-problem in a data-driven fashion. A few selection criteria in our framework are re-implementations of those described by Volodina et al. (2012) and Pil\u00e1n et al.", "startOffset": 98, "endOffset": 942}, {"referenceID": 1, "context": "We implemented a hybrid system which uses a combination of machine-learning methods for assessing L2 complexity and heuristic rules for all other criteria. The motivation behind using rules is, on the one hand, that certain linguistic elements are easily identifiable with such methods. On the other hand, a sufficient amount of training data encompassing the range of all possible exercise types would be extremely costly to create. Moreover, explicit rules make the sentence selection customizable to users\u2019 task-specific needs which increases the applicability of HitEx to a diverse set of situations. The criterion of L2 complexity has been implemented using machine learning methods since its assessment comprises multiple linguistic dimensions and data was available for approaching this sub-problem in a data-driven fashion. A few selection criteria in our framework are re-implementations of those described by Volodina et al. (2012) and Pil\u00e1n et al. (2014). Major additions to previous work include: (i) L2 complexity assessment on a 5-level scale, vs.", "startOffset": 98, "endOffset": 966}, {"referenceID": 1, "context": "We implemented a hybrid system which uses a combination of machine-learning methods for assessing L2 complexity and heuristic rules for all other criteria. The motivation behind using rules is, on the one hand, that certain linguistic elements are easily identifiable with such methods. On the other hand, a sufficient amount of training data encompassing the range of all possible exercise types would be extremely costly to create. Moreover, explicit rules make the sentence selection customizable to users\u2019 task-specific needs which increases the applicability of HitEx to a diverse set of situations. The criterion of L2 complexity has been implemented using machine learning methods since its assessment comprises multiple linguistic dimensions and data was available for approaching this sub-problem in a data-driven fashion. A few selection criteria in our framework are re-implementations of those described by Volodina et al. (2012) and Pil\u00e1n et al. (2014). Major additions to previous work include: (i) L2 complexity assessment on a 5-level scale, vs. a previously available binary classification model by Pil\u00e1n et al. (2014), (ii) typicality and (iii) the assessment of context dependence.", "startOffset": 98, "endOffset": 1136}, {"referenceID": 4, "context": "Our system searches for sentence candidates via Korp (Borin et al., 2012), an online infrastructure providing access to a variety of (mostly) Swedish corpora.", "startOffset": 53, "endOffset": 73}, {"referenceID": 29, "context": "Similarly, there might be a preference for the position of the search term in the sentence in some use cases such as dictionary examples (Kilgarriff et al., 2008).", "startOffset": 137, "endOffset": 162}, {"referenceID": 29, "context": "Good candidate sentences from corpora should be structurally and lexically wellformed (Kilgarriff et al., 2008; Hus\u00e1k, 2010).", "startOffset": 86, "endOffset": 124}, {"referenceID": 27, "context": "Good candidate sentences from corpora should be structurally and lexically wellformed (Kilgarriff et al., 2008; Hus\u00e1k, 2010).", "startOffset": 86, "endOffset": 124}, {"referenceID": 48, "context": "the lack of a subject or a finite verb (all verb forms except infinitive, supine and participle) inspired by Volodina et al. (2012). The completeness criterion checks the beginning and the end of a sentence for orthographic clues such as capital letters and punctuation, in a similar fashion to Pil\u00e1n (2016).", "startOffset": 109, "endOffset": 132}, {"referenceID": 34, "context": "The completeness criterion checks the beginning and the end of a sentence for orthographic clues such as capital letters and punctuation, in a similar fashion to Pil\u00e1n (2016). A large amount of nonlemmatized tokens, i.", "startOffset": 162, "endOffset": 175}, {"referenceID": 29, "context": "The presence of linguistic elements responsible for connecting sentences at a syntactic or semantic level is therefore suboptimal (Kilgarriff et al., 2008).", "startOffset": 130, "endOffset": 155}, {"referenceID": 28, "context": "The presence of linguistic elements responsible for connecting sentences at a syntactic or semantic level is therefore suboptimal (Kilgarriff et al., 2008). We incorporate a number of criteria for capturing this aspect which we described also in Pil\u00e1n (2016).", "startOffset": 131, "endOffset": 259}, {"referenceID": 43, "context": "Based on Teleman et al. (1999), a list of anaphoric adverbs has been collected and sentences are checked for the occurrence of any of the listed items.", "startOffset": 9, "endOffset": 31}, {"referenceID": 54, "context": "The source of the training data was single sentences from COCTAILL (Volodina et al., 2014), a corpus of coursebook texts for L2 Swedish.", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "The aspect of L2 complexity has been assessed with the help of a supervised machine learning algorithm based on a number of different linguistic dimensions. We used the CEFR level classifier for sentences that we previously described in Pil\u00e1n et al. (2016). The source of the training data was single sentences from COCTAILL (Volodina et al.", "startOffset": 14, "endOffset": 257}, {"referenceID": 16, "context": "Due to their morphological and semantic peculiarity, they are explicitly targeted in L2 grammar books (Fasth and Kannermark, 1997).", "startOffset": 102, "endOffset": 130}, {"referenceID": 20, "context": "These aspects are related to readers working memory load when processing sentences (Gibson, 1998).", "startOffset": 83, "endOffset": 97}, {"referenceID": 52, "context": "Besides the aspects mentioned above, a number of additional structural criteria are available which proved to be relevant either based on previous evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2013) or evidence from coursebooks (Volodina et al.", "startOffset": 159, "endOffset": 202}, {"referenceID": 39, "context": "Besides the aspects mentioned above, a number of additional structural criteria are available which proved to be relevant either based on previous evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2013) or evidence from coursebooks (Volodina et al.", "startOffset": 159, "endOffset": 202}, {"referenceID": 54, "context": ", 2013) or evidence from coursebooks (Volodina et al., 2014).", "startOffset": 37, "endOffset": 60}, {"referenceID": 52, "context": "Answers to polar (or close-ended) questions are rarely employed as exercise items and they were also negatively perceived in previous evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2013).", "startOffset": 146, "endOffset": 189}, {"referenceID": 39, "context": "Answers to polar (or close-ended) questions are rarely employed as exercise items and they were also negatively perceived in previous evaluations (Volodina et al., 2012; Pil\u00e1n et al., 2013).", "startOffset": 146, "endOffset": 189}, {"referenceID": 29, "context": "Another criterion involves the presence of proper names which, although undesirable for dictionary examples (Kilgarriff et al., 2008), may be familiar and easy to understand for L2 students (Segler, 2007).", "startOffset": 108, "endOffset": 133}, {"referenceID": 22, "context": "These are also referred to as PARSNIPs, which stands for Politics, Alcohol, Religion, Sex, Narcotics, Isms 1 and Pork (Gray, 2010).", "startOffset": 118, "endOffset": 130}, {"referenceID": 30, "context": "We assessed the typicality of sentences with the help of a co-occurrence measure: Lexicographers\u2019 Mutual Information (LMI) score (Kilgarriff et al., 2004).", "startOffset": 129, "endOffset": 154}, {"referenceID": 3, "context": "LMI measures the probability of two words co-occurring together in a corpus and it offers the advantage of balancing out the preference of the Mutual Information score for low-frequency words (Bordag, 2008).", "startOffset": 192, "endOffset": 206}, {"referenceID": 10, "context": "First, based on students\u2019 responses, we computed item difficulty for each exercise item, which corresponds to the percentage of students correctly answering an item, a higher value thus indicating an easier item (Crocker and Algina, 1986).", "startOffset": 212, "endOffset": 238}, {"referenceID": 0, "context": "To be able to measure whether the item difficulty observed in our students\u2019 results matched the values one would typically expect in L2 teaching, we calculated the ideal item difficulty (IID) score for our exercises, which takes into consideration correct answers based on chance. We used the formula proposed by Thompson and Levitov (1985) presented in [1], where PC is the probability of correct answers by chance.", "startOffset": 187, "endOffset": 341}, {"referenceID": 2, "context": "the selected distractors (Beinborn et al., 2014), nevertheless the observed tendencies in error rates provide useful insights into the suitability of seed sentences in terms of L2 complexity.", "startOffset": 25, "endOffset": 48}], "year": 2017, "abstractText": "We present a framework and its implementation relying on Natural Language Processing methods, which aims at the identification of exercise item candidates from corpora. The hybrid system combining heuristics and machine learning methods includes a number of relevant selection criteria. We focus on two fundamental aspects: linguistic complexity and the dependence of the extracted sentences on their original context. Previous work on exercise generation addressed these two criteria only to a limited extent, and a refined overall candidate sentence selection framework appears also to be lacking. In addition to a detailed description of the system, we present the results of an empirical evaluation conducted with language teachers and learners which indicate the usefulness of the system for educational purposes. We have integrated our system into a freely available online learning platform. R\u00c9SUM\u00c9. Nous proposons un syst\u00e8me de traitement automatique de la langue ayant pour but l\u2019identification de phrases candidates tir\u00e9es de corpus. Le syst\u00e8me hybride allie une approche heuristique \u00e0 des m\u00e9thodes d\u2019apprentissage automatis\u00e9 et int\u00e8gre un nombre de crit\u00e8res de s\u00e9lection pertinents. Nous nous concentrons sur deux aspects fondamentaux : la complexit\u00e9 linguistique et la d\u00e9pendance des phrases extraites envers leur contexte d\u2019origine. Les travaux ant\u00e9rieurs en g\u00e9n\u00e9ration automatique d\u2019exercices n\u2019ont port\u00e9 sur ces deux crit\u00e8res que de fa\u00e7on limit\u00e9e, et un cadre fin de s\u00e9lection de phrases candidates semble \u00e9galement faire d\u00e9faut. En plus d\u2019une description d\u00e9taill\u00e9e du syst\u00e8me, cet article rapporte les r\u00e9sultats d\u2019une \u00e9valuation empirique r\u00e9alis\u00e9e avec des enseignants de langues et des apprenants portant sur l\u2019utilit\u00e9 du syst\u00e8me \u00e0 des fins \u00e9ducatives.", "creator": "LaTeX with hyperref package"}}}