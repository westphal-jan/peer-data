{"id": "1706.03581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition", "abstract": "we design an enriched distributed deep temporal recurrent visual attention access model ( edram ) - an improved attention - based architecture created for multiple object recognition. the proposed complexity model is a functional fully differentiable unit that can be optimized end - to - end by using inverse stochastic gradient descent ( sgd ). the optical spatial transformer ( hc st ) was employed as visual attention mechanism which allows to learn the geometric transformation of objects within images. combined with adapting the combination function of the spatial transformer and the powerful recurrent architecture, the proposed edram can localize and recognize objects simultaneously. utilizing edram has been evaluated on examining two publicly available survey datasets including mnist cluttered ( with 70k cluttered digits ) and svhn ( with up to 250k real world images set of house numbers ). experiments show that it obtains superior performance as compared with utilizing the state - of - the - art models.", "histories": [["v1", "Mon, 12 Jun 2017 11:55:35 GMT  (833kb,D)", "http://arxiv.org/abs/1706.03581v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["artsiom ablavatski", "shijian lu", "jianfei cai"], "accepted": false, "id": "1706.03581"}, "pdf": {"name": "1706.03581.pdf", "metadata": {"source": "CRF", "title": "Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition", "authors": ["Artsiom Ablavatski", "Shijian Lu", "Jianfei Cai"], "emails": ["stuaa@i2r.a-star.edu.sg,", "slu@i2r.a-star.edu.sg", "asjfcai@ntu.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "Recurrent models of visual attention have demonstrated superior performance on a variety of recognition and classification tasks [16, 2, 19, 22] in recent year. A recurrent model of visual attention is a task-driven agent interacting with a visual environment which observes the environment via a bandwidth-limited sensor at each time stamp. Recurrent models consist of two crucial components: an attention mechanism and a recurrent network. The first component, a simple attention mechanism as introduced by Mnih et al. [16], has demonstrated great success on recognizing digits within the Street View House Number (SVHN) dataset [6]. However, this mechanism is restricted by its\nPubliished as a conference paper at 2017 IEEE Winter Conference on Applications of Computer Vision (WACV)\nsimplicity which extracts a fixed amount of patches on each iteration with predefined scales. The recently introduced more flexible and sophisticated visual attention mechanisms [7, 12] achieved state-of-the-art results on the 60 \u00d7 60 MNIST Cluttered dataset [19]. The attention mechanisms advance in the patch extraction allowing to cover any 2D affine transformation of objects in the image. Hence the mechanisms are robust to most geometric transformations and have demonstrated superior performance comparable with the humans. The second component, the recurrent network also plays an essential in the recurrent models of visual attention. More powerful recurrent architecture employed in the Deep Recurrent Visual Attention Model [2] significantly outperformed the original Recurrent Attention Model proposed by Mnih et al. [16] while leaving the rest of the systems identical.\nInspired by deep recurrent visual attention model and the power of the visual attention mechanism [12], we propose an Enriched Deep Recurrent Visual Attention model (EDRAM) that consists of a flexible and powerful attention mechanism along with a smart and light-weight recurrent neural network. The proposed technique is fully differentiable and has been trained end-to-end by using Stochastic Gradient Descent (SGD). It obtained superior performance as evaluated on two publicly available datasets including the multi-digit SVHN dataset [6] as illustrated in Fig. 1a and the MNIST Cluttered [16] as illustrated in Fig. 1b."}, {"heading": "2. Related work", "text": "Recurrent models of visual attention have been attracting increasing interest in recent years. The Recurrent Attention Model (RAM) proposed by Mnih et al. [16] employs a recurrent neural network (RNN) to integrate visual information (image patch or glimpse) over time. By REINFORCE optimization of the network [21], they achieved\nar X\niv :1\n70 6.\n03 58\n1v 1\n[ cs\n.C V\n] 1\n2 Ju\na huge reduction of computational cost as well as state-ofthe-art performance on the MNIST Cluttered dataset. Ba et al. [2] extended the glimpse network for multiple object recognition with visual attention. They introduced the Deep Recurrent Visual Attention Model (DRAM) that integrates a simple visual attention mechanism with the neural network based on Long Short-Term Memory (LSTM) gated recurrent unit [9]. The REINFORCE learning rule, employed in [16] to train their attention model, was used to learn the network \u201cwhere\u201d and \u201cwhat\u201d. Though the DRAM achieved superior result in a number of tasks such as the MNIST pair classification and SVHN recognition, the attention mechanism used is straightforward by extracting patches of fixed scales only, where the potential of the visual attention mechanism is far from fully exploited.\nIn comparison to the simple attention mechanism, Gregor et al. [7] introduced Selective Attention Model that created a differentiable, end-to-end trainable system which greatly improves the recognition accuracy on the MNIST Cluttered dataset. The idea of this attention mechanism is to position N \u00d7 N set of Gaussian filters forming a grid centered at the particular spatial coordinates with which rectangle patches of different scales can be extracted. More recently, Jaderberg et al. [12] proposed the Spatial Transformer Network that can deal with not only scales, but also any 2D affine transformation of objects in images. Using Spatial Transformer in combination with the convolutional neural network, Jaderberg et al. [11] achieved state-of-theart result on the SVHN dataset \u2013 3.6% error rate.\nThough different deep attention models have been designed, all existing models have various constraints. For example, the Differentiable RAM [7] is fully differen-\ntiable and can be trained using SGD, but the model has a weak network architecture and does not scale well to real world tasks. The DRAM [2] has a powerful architecture, but the sampling strategy makes the whole network non-differentiable. Building on the long line of the previous attempts of attention-based visual processing methods [2, 16, 12, 19, 7], the proposed EDRAM expands the idea of using recurrent connections inside the attention mechanism [19] and improves the Deep Recurrent Visual Attention Model (DRAM) [2] from several aspects as follows:\n\u2022 It made previously non-differentiable architecture fully differentiable by using the Spatial Transformer. It allowed us to optimize the network parameters end-toend by using SGD with backpropagation framework.\n\u2022 A flexible loss function was designed based on Cross Entropy and Mean Squared Error function with which the EDRAM can be trained efficiently.\n\u2022 It obtains superior performance on the MNIST Cluttered and SVHN datasets as compared with state-ofthe-art methods."}, {"heading": "3. Enriched Deep Recurrent Visual Attention", "text": "Model\nThe underlying idea of the EDRAM is to combine a powerful and computationally simple recurrent neural network with a flexible and adjustable attention mechanism (ST), while making the whole network fully differentiable and trainable through SGD."}, {"heading": "3.1. Network Architecture", "text": "Inspired by the model proposed by Ba et al. [2], our network architecture was designed to satisfy the complexity requirements and the capability to learn a very nonlinear classification function. It can be decomposed into several subcomponents including an attention mechanism, a context network, a glimpse network, a recurrent network, a classification network and an emission network as illustrated in Fig. 2. Each sub-component can be referred by a \u201cnetwork\u201d because it is a typical a multi-layered neural network.\nThe context network receives a down-sampled lowresolution image as input and processes it through a threelayered convolutional neural network. It produces a feature vector r(2)0 that serves as an initialization of a hidden state of the second LSTM unit in the recurrent network.\nThe attention mechanism reads an image patch xt by using the transformation parameters A\u03b8,t (more details to be described in Section 3.2) that have been predicted on the previous iteration. Parameters for the first iteration are defined in a way that the attention mechanism reads the whole image without transformation. An algorithm for the read operation and transformation parameters will be defined in the next subsection.\nDifferent from the DRAM where the glimpse network is responsible only for producing discriminative features for the classification network, the glimpse network in EDRAM\nintegrates the localization network from the Spatial Transformer. Therefore, the glimpse network in the EDRAM contains a number of convolution layers followed by the max pooling layer. The number of convolutions varies and depends on the difficulty of a recognition task. The result of the convolution layers and the transformation parameters A\u03b8,t are used in multiple isolated fully-connected layers. The outputs of the isolated fully-connected layers are combined together by element-wise multiplication to form the final glimpse feature vector. This type of combination \u201cwhat\u201d and \u201cwhere\u201d was proposed by Larochelle and Hinton et al. [14].\nThe recurrent network contains two LSTM units stacked one above the other with hidden states r(1)t and r (2) t . The first LSTM receives the glimpse feature vector and uses the hidden state r(1)t to produce a feature vector for the classification network. Based on the hidden state of the first LSTM r (1) t and the hidden state r (2) t , the second LSTM unit produces a feature vector for the emission network. The hidden state r(1)t of the first LSTM is independent of the hidden state r(2)t of the second LSTM and the glimpse parameters A\u03b8,t. This means that a prediction of the classification network depends only on an extracted patch and is independent of a location and the transformation parameters.\nThe classification and emission networks map feature vectors from different layers of the recurrent network (by using fully-connected layers) to predict labels yt and the transformation parameters A\u03b8,t+1 for the next iteration, respectively. The classification network has two fullyconnected layers followed by softmax output layer. The emission network employs the fully-connected layer to predict the transformation parameters.\nThe EDRAM processes the image in a sequential manner with T steps. At each time step t, the model receives the parameters of transformation A\u03b8,t and uses the attention mechanism to extract the patch xt at the location as defined by the parameters \u03b83, \u03b86 (to be described in Section 3.2) in the transformation matrix A\u03b8,t. The model uses the observation xt, processed by the glimpse network, and the parameters A\u03b8,t to update its internal states r (1) t , r (2) t and produce the parameters A\u03b8,t+1 for the next step. Besides that, the model makes a prediction yt based on the internal states r(1)t of the first LSTM. The attention mechanism controls the number of pixels in the patch xt, which is usually much smaller than the number of pixels in the original image."}, {"heading": "3.2. Spatial Transformer attention mechanism", "text": "Instead of using the nondifferentiable attention mechanism that simply reads patches at the given location (x, y), our Spatial Transformer attention mechanism is inspired by the differentiable Spatial Transformer Networks as proposed by Jaderberg et al. [12]. The original Spatial Trans-\nformer Networks only contains a localization network, grid generator and sampler. All parts are fully differentiable that enables the optimization of the whole model end-to-end by using gradient descent within a standard backpropagation framework.\nThe major constraint of the original Spatial Transformer Networks is that the transformation parameters for the grid generator were obtained using the standalone localization network, which introduces a larger number of parameters and increases the computational cost. Another drawback of the Spatial Transformer Networks is the supervision for the prediction when it is used in an iterative manner: the Spatial Transformer Networks was introduced only for feedforward networks and predicts the transformation parameters only based on the current input data without using the information from the previous steps. The transformation parameters with a better accuracy could be obtained when the Spatial Transformer can incorporate information from the previous steps.\nTo overcome the downsides of the original Spatial Transformer Networks we propose a solution that incorporates information from previous iterations and at the same time reduces the number of parameters needed for the localization network. Our Spatial Transformer attention mechanism in-\ntegrates the localization network into the glimpse network, which is responsible for both classification and transformation information flow. Technically, it means that backpropagation of the classification error through the glimpse network will affect on the localization error of the emission network and backpropagation of the localization error will affect on the prediction accuracy of the classification network. A flexible loss function therefore needs to be designed for the EDRAM to control the backpropagation of the errors and to learn both accurate transformation parameters and correct classification labels. Besides, sharing network parameters between the prediction and transformation information flows results in the reduction of the computational cost. The overall scheme of the localization network of the EDRAM is illustrated in Fig. 3\nThe Spatial Transformer is responsible for an affine transformation (zoom, rotation and skew) of mesh grid points xgi , y g i according to the parameters A\u03b8,t.\nA\u03b8,t = [ \u03b81,t \u03b82,t \u03b83,t \u03b84,t \u03b85,t \u03b86,t ] (1)\n( xsi ysi ) = A\u03b8,t xgiygi 1  = [\u03b81,t\u03b82,t\u03b83,t \u03b84,t\u03b85,t\u03b86,t ]xgiygi 1  (2) where \u03b81,t, \u03b85,t \u2013 determine a zoom, \u03b82,t, \u03b84,t \u2013 determine a skewness in x, y directions respectively and \u03b83,t, \u03b86,t \u2013 determine the center position of the mesh grid.\nSince the mesh grid of points (Grid generator in Fig. 3) does not correspond exactly to one particular point xsi , y s i in the input image, the bilinear interpolation (Sampler in Fig. 3) is used to output the fixed scale patch Xt from the input image I for further processing.\nXt = H\u2211 n W\u2211 m Inmmax(0, 1\u2212|xsi \u2212m|)max(0, 1\u2212|ysi \u2212n|) (3) Then the partial derivatives for the bilinear sampling (3) w.r.t the sampling grid coordinates can be formulated as follows:\ndXt dxsi = H\u2211 n W\u2211 m Inmmax(0, 1\u2212|ysi\u2212n|)  0 if |m\u2212 xsi | \u2265 1 1 if m \u2265 xsi \u22121 if m < xsi\n(4)\ndXt dysi = H\u2211 n W\u2211 m Inmmax(0, 1\u2212|xsi\u2212m|)  0 if |n\u2212 ysi | \u2265 1 1 if n \u2265 ysi \u22121 if n < ysi (5) The sub-differentiable sampling allows backpropagation of the loss to the sampling grid coordinates which leads to flow back the gradients to the transformation parameters A\u03b8,t\nand to Emission Network in Fig. 3. In addition, to encourage the Spatial Transformer attention mechanism to learn more accurate transformation parameters we allow backpropagation of the loss in opposite direction through the supervision of the parameters obtained on the previous iteration (see Learning \u201cWhere\u201d in the next section). This contribute in a precise localization of the extracted patches by the attention mechanism after only a few epochs of training.\nThe Spatial Transformer attention mechanism is fully differentiable and satisfies both requirements of flexibility and adjustability. Hence, it allows us to train the attention mechanism with standard backpropagation."}, {"heading": "3.3. Learning \u201cWhere\u201d and \u201cWhat\u201d", "text": "In the context of multiple object recognition, the network should locate the necessary objects of an image (\u201cWhere\u201d) and successfully recognize them (\u201cWhat\u201d) in order to achieve the desired performance. Hence, the objective function should penalize any false positives predictions as well as incorrect recognitions at true positive locations.\nThe loss function is designed to force the EDRAM to recognize necessary objects in a finite number of steps. For each object in the image we allow the network to make a fixed number of predictions N . The network produces a final class prediction for the given object by averaging the predictions. Suppose we have S targets in the image, the loss function will be calculated only for N \u00d7 S steps:\nL = 1\nN S\u2211 i=1 N\u2211 j=1 Li,j (6)\nThe loss function Li,j for each iteration includes a weighted summation of the Cross Entropy loss for the given glimpse Lyi,j and weighted Mean Squared Error of the transformation parameters LA\u03b8i,j :\nLi,j = \u03b11 \u2217 Lyi,j + \u03b12 \u2217 L A\u03b8 i,j (7)\nwhere Lyi,j can be interpreted as \u201cwhat to look\u201d and L A\u03b8 i,j \u2014 \u201cwhere to look\u201d. The hyperparameters \u03b11 and \u03b12 give a good trade-off between classification and transformation loss, forcing the model to simultaneously optimize for a better patch extraction and for better recognition.\nLyi,j = \u2212 log pi,j,ygt (8)\nLA\u03b8i,j = 6\u2211 k=1 \u03b2k \u2217 (\u03b8k,i,j \u2212 \u03b8gtk,i,j) 2 (9)\nwhere pi,j,ygt is a predicted class probability on a groundtruth position, \u03b81,i,j , . . . , \u03b86,i,j \u2014 elements of the matrix A\u03b8 and \u03b8 gt 1,gi,j , . . . , \u03b8 gt 6,i,j \u2014 ground truth values for iteration i, j. The hyperparameters \u03b21, . . . , \u03b26 force the network\nto pay more attention to critical parameters such as width, height and coordinates of the mesh grid and ignore unimportant parameters such as the skewness."}, {"heading": "4. Experiments", "text": "EDRAM has been evaluated over the MNIST Cluttered dataset [16, 2] as well as a real-world object recognition task by using the Street View House Numbers (SVHN) dataset [17].\nSince the attention mechanism is fully differentiable, the proposed network is trained with standard backpropagation and SGD by using Adam optimization algorithm [13]. Gradient step clipping techniques are applied [15, 18] to ensure the absence of the gradient exploding during the learning over the recursive structures. The value of the thresholded norm is chosen equal to 10. Following Cooijmans [4], the Batch Normalization proposed by Iofee and Szegedy [10] is used on the MNIST Cluttered dataset to estimate the statistics independently for each iteration. Theano [3], Blocks and Fuel [20] are used to implement and conduct the experiments with the MNIST Cluttered and SVHN datasets.\nFor comparison, we take the results from the latest works by Almahairi et al. [1] \u2014 Dynamic Capasity Networks (DCN), Jaderberg et al. [12] \u2014 ST-CNN, Ba et al. [2] \u2014 DRAM and the first result obtained on the SVHN dataset by Goodfellow et al. [6] \u2014 11 layer CNN. DCN is an attention-based ensemble of convolutional networks of different capacities. The family of DRAM models includes the original Deep Recurrent Visual Attention Model (Single DRAM), DRAM with Monte Carlo sampling policy (Single DRAM MC avg.) and ensemble of two models of different recognition order (forward-backward DRAM MC avg.). The 11 layer CNN and ST-CNN are the feedforward networks containing several convolutions layers followed by fully-connected layers. In addition, the STCNN includes one (ST-CNN Single) or several (ST-CNN Multi) Spatial Transformers between convolutional layers with separate localization networks."}, {"heading": "4.1. MNIST Cluttered", "text": "We first evaluate EDRAM on the 100 \u00d7 100 MNIST Cluttered dataset [16], where each image contains randomly located hand-written digit surrounded with digit-like fragments. The dataset has 60000 images for training and 10000 for testing.\nAt each time step, a glimpse of the size 26 \u00d7 26 from the input image is fed to the network and the model predicts parameters of extraction for the next iteration as well as a class label. The model produces the final classification result after a fixed number of glimpses (6 in our case). In this experiment, we use ReLU activations for all layers except the recurrent network, where standard tanh activation in LSTM units are employed. Context network takes\ndown-sampled 12 \u00d7 12 image and projects it into a vector using 3 convolutional layers (without any activations) with filter sizes {5, 3, 3} and number of filters {16, 16, 32}, respectively.\nWe use 6 convolutional layers in the glimpse network. The size of filters in each convolution of the glimpse network is chosen to be 3 and the numbers of filters in 6 convolutions are 64, 64, 128, 128, 160 and 192. The max pooling is made with size {2, 2} and stride {2, 2} after second and fourth convolutional layers. Zero-padding of half of the filter size is used in the first, third, forth and fifth convolutions of the glimpse network. There are 512 LSTM units and 1024 hidden units in each fully-connected layer of the model.\nOptimal values for hyperparameters \u03b11 and \u03b12 were found to be 1 by random search. To encourage the network to learn precise location of the target the weights \u03b21, \u03b23, \u03b25 and \u03b26 set to 1 whereas \u03b22 and \u03b24 are 0.5. A learning rate of 10\u22124 is used for training the model and exponentially reduced by a factor of 10 when the training loss plateaus. The model is initialized with a uniform distribution for recurrent and convolutional units with the range of [\u22120.01, 0.01] and a Gaussian distribution for fully-connected layers with a variance 0.001. A mini-batch size of 128 is used to estimate the gradient directions.\nThe results in Table 1 demonstrate more then 2\u00d7 improvement in the test error as compared to the state-of-theart models on the MNIST Cluttered dataset. With the help of the proposed Spatial Transformer attention mechanism and the designed objective function, the network is able to\nlearn where to find a digit in the cluttered background and how to recognize it accurately. Moreover, making the network fully differentiable allows it, to train end-to-end by standard back propagation and to use the batch normalization, which leads to faster convergence. Fig. 4 illustrates the process of the attention mechanism where each row shows how the Spatial Transformer attention mechanism locates a digit on the image iteration by iteration accurately.\nWe show in Fig. 5 how the test error on the MNIST Cluttered dataset decreases when the number of glimpses is increased. We can see that the test error is decreasing almost linearly with increasing the number of patches and after 6 glimpses it saturates and performance does not improve significantly. So, 6 glimpses is a good trade-off between the accuracy and computation cost of the model."}, {"heading": "4.2. SVHN", "text": "We also evaluate EDRAM on the multi-digit SVHN dataset and compared it with the state-of-the-art models. The SVHN dataset contains around 250k real world images of digits taken from pictures of house fronts. There are between 1 and 5 digits in each image, with a large variability in scale and spatial arrangement. Following the experimental setup in [6, 2], the test set is formed from 13k images and the rest of data (train and extra sets) is used to train the networks.\nThe data is preprocessed by generating tightly cropped 64 \u00d7 64 images with multi-digits at the center and similar data augmentation is used to create 54\u00d7 54 jittered images during training. Similar to [2], RGB images are converted into grayscale. The model is trained to classify all the digits in an image sequentially with objective function as defined in Eq. (6). Patches are given to classify each digit in the image. As images in the SVHN dataset have at most 5 digit sequences, the overall amount of iterations is 18 that equals 5\u00d7 3 plus 3 patches for a terminal label.\nThe heuristic of learning two separate models of different reading orders (forward, backward) as proposed in [2] is adapted. As the localization network is integrated in the glimpse network and the transformation parameters are different for the models, the weights are not shared between forward and backward models. Fig. 1a illustrates how the proposed EDRAM extracts glimpses accurately around digits.\nIn the experiments, a square extraction window of size 34 \u00d7 34 pixels is used. Initialization of the network is chosen identically to the MNIST Cluttered experiment. An initial value for learning rate is 10\u22124. A mini-batch size of 128 is used to estimate the gradient direction. For all units except LSTM blocks, ReLU activation function was used. For the stacked LSTM blocks, standard tanh activation was used. As SVHN dataset provides only size and location for digit bounding box, only 4 parameters \u03b81, \u03b83, \u03b85, \u03b86 from transformation matrix A\u03b8 are used to estimate loss function LA\u03b8i,j . The parameters of skewness \u03b82 and \u03b82 in the proposed system are learned by themselves for better prediction accuracy. The loss function hyperparameters are chosen to be identical to the experiment with MNIST Cluttered dataset. The training took 5 days on a single modern GPU.\nThe proposed approach obtained state-of-the-art performance in recognition of multiple objects from the real world as shown in Table 2 while having 1.7\u00d7 less parameters (37M/22M \u2248 1.7) than previous state-of-the-art model ST-CNN Multi (see Table 3). This proves the effectiveness of the developed objective function that forces the network to locate the desired objects on images and successfully rec-\nognize them one by one. The usage of the attention mechanism allows the network to ignore redundant information from images and extract patches that are necessary for the prediction of the correct class labels.\nThe computational cost of the neural networks (NN) depends on the number of parameters, with more parameters the model needs more space to be stored and more floatingpoint operations (FLOPs) to execute to produce the final output. This creates a difficulty of applying NNs on different embedded platforms with limited memory and processing units, like mobile phones [8]. Besides that, significant redundancy has been reported in many state-of-the-art neural network models [5]. The integration of the separate localization network into the glimpse network allows to achieve a significant computation cost reduction in comparison with other state-of-the-art models. Table 3 shows the number of parameters of the proposed model in comparison with other deep convolutional neural networks. Though we only matched the performance on the SVHN dataset, our network contains 1.7 times less parameters than the stateof-the-art ST-CNN Multi."}, {"heading": "5. Conclusions", "text": "This paper presents an Enriched Deep Recurrent Visual Attention Model that is fully differentiatiable and trainable end-to-end using SGD. The EDRAM outperforms the state-of-the-art result on the MNIST Cluttered dataset and matches the state-of-the-art models on a multi-digit house number recognition task. It requires a smaller amount of parameters and less computation resources, thereby proving that attention mechanism has a big impact on accuracy and efficiency of the model."}], "references": [{"title": "Dynamic capacity networks", "author": ["A. Almahairi", "N. Ballas", "T. Cooijmans", "Y. Zheng", "H. Larochelle", "A. Courville"], "venue": "arXiv preprint arXiv:1511.07838,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Recurrent batch normalization", "author": ["T. Cooijmans", "N. Ballas", "C. Laurent", "A. Courville"], "venue": "arXiv preprint arXiv:1603.09025,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["I.J. Goodfellow", "Y. Bulatov", "J. Ibarz", "S. Arnoud", "V. Shet"], "venue": "arXiv preprint arXiv:1312.6082,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Synthetic data and artificial neural networks for natural scene text recognition", "author": ["M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1406.2227,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["H. Larochelle", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Statistical Language Models Based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Recurrent spatial transformer networks", "author": ["S.K. S\u00f8nderby", "C.K. S\u00f8nderby", "L. Maal\u00f8e", "O. Winther"], "venue": "arXiv preprint arXiv:1509.05329,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-Farley", "J. Chorowski", "Y. Bengio"], "venue": "arXiv preprint arXiv:1506.00619,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Recurrent models of visual attention have demonstrated superior performance on a variety of recognition and classification tasks [16, 2, 19, 22] in recent year.", "startOffset": 129, "endOffset": 144}, {"referenceID": 1, "context": "Recurrent models of visual attention have demonstrated superior performance on a variety of recognition and classification tasks [16, 2, 19, 22] in recent year.", "startOffset": 129, "endOffset": 144}, {"referenceID": 18, "context": "Recurrent models of visual attention have demonstrated superior performance on a variety of recognition and classification tasks [16, 2, 19, 22] in recent year.", "startOffset": 129, "endOffset": 144}, {"referenceID": 21, "context": "Recurrent models of visual attention have demonstrated superior performance on a variety of recognition and classification tasks [16, 2, 19, 22] in recent year.", "startOffset": 129, "endOffset": 144}, {"referenceID": 15, "context": "[16], has demonstrated great success on recognizing digits within the Street View House Number (SVHN) dataset [6].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[16], has demonstrated great success on recognizing digits within the Street View House Number (SVHN) dataset [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "The recently introduced more flexible and sophisticated visual attention mechanisms [7, 12] achieved state-of-the-art results on the 60 \u00d7 60 MNIST Cluttered dataset [19].", "startOffset": 84, "endOffset": 91}, {"referenceID": 11, "context": "The recently introduced more flexible and sophisticated visual attention mechanisms [7, 12] achieved state-of-the-art results on the 60 \u00d7 60 MNIST Cluttered dataset [19].", "startOffset": 84, "endOffset": 91}, {"referenceID": 18, "context": "The recently introduced more flexible and sophisticated visual attention mechanisms [7, 12] achieved state-of-the-art results on the 60 \u00d7 60 MNIST Cluttered dataset [19].", "startOffset": 165, "endOffset": 169}, {"referenceID": 1, "context": "More powerful recurrent architecture employed in the Deep Recurrent Visual Attention Model [2] significantly outperformed the original Recurrent Attention Model proposed by Mnih et al.", "startOffset": 91, "endOffset": 94}, {"referenceID": 15, "context": "[16] while leaving the rest of the systems identical.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Inspired by deep recurrent visual attention model and the power of the visual attention mechanism [12], we propose an Enriched Deep Recurrent Visual Attention model (EDRAM) that consists of a flexible and powerful attention mechanism along with a smart and light-weight recurrent neural network.", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "It obtained superior performance as evaluated on two publicly available datasets including the multi-digit SVHN dataset [6] as illustrated in Fig.", "startOffset": 120, "endOffset": 123}, {"referenceID": 15, "context": "1a and the MNIST Cluttered [16] as illustrated in Fig.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "[16] employs a recurrent neural network (RNN) to integrate visual information (image patch or glimpse) over time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "By REINFORCE optimization of the network [21], they achieved ar X iv :1 70 6.", "startOffset": 41, "endOffset": 45}, {"referenceID": 5, "context": "Figure 1: Examples of recognized images of the Street View House Numbers dataset [6] in (a) and the MNIST Cluttered dataset [16] in (b) where the green-color boxes show the localization and the digits at the box top-left corner show the recognition results.", "startOffset": 81, "endOffset": 84}, {"referenceID": 15, "context": "Figure 1: Examples of recognized images of the Street View House Numbers dataset [6] in (a) and the MNIST Cluttered dataset [16] in (b) where the green-color boxes show the localization and the digits at the box top-left corner show the recognition results.", "startOffset": 124, "endOffset": 128}, {"referenceID": 1, "context": "[2] extended the glimpse network for multiple object recognition with visual attention.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "They introduced the Deep Recurrent Visual Attention Model (DRAM) that integrates a simple visual attention mechanism with the neural network based on Long Short-Term Memory (LSTM) gated recurrent unit [9].", "startOffset": 201, "endOffset": 204}, {"referenceID": 15, "context": "The REINFORCE learning rule, employed in [16] to train their attention model, was used to learn the network \u201cwhere\u201d and \u201cwhat\u201d.", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "[7] introduced Selective Attention Model that created a differentiable, end-to-end trainable system which greatly improves the recognition accuracy on the MNIST Cluttered dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] proposed the Spatial Transformer Network that can deal with not only scales, but also any 2D affine transformation of objects in images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] achieved state-of-theart result on the SVHN dataset \u2013 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "For example, the Differentiable RAM [7] is fully differentiable and can be trained using SGD, but the model has a weak network architecture and does not scale well to real world tasks.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "The DRAM [2] has a powerful architecture, but the sampling strategy makes the whole network non-differentiable.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "Building on the long line of the previous attempts of attention-based visual processing methods [2, 16, 12, 19, 7], the proposed EDRAM expands the idea of using recurrent connections inside the attention mechanism [19] and improves the Deep Recurrent Visual Attention Model (DRAM) [2] from several aspects as follows:", "startOffset": 96, "endOffset": 114}, {"referenceID": 15, "context": "Building on the long line of the previous attempts of attention-based visual processing methods [2, 16, 12, 19, 7], the proposed EDRAM expands the idea of using recurrent connections inside the attention mechanism [19] and improves the Deep Recurrent Visual Attention Model (DRAM) [2] from several aspects as follows:", "startOffset": 96, "endOffset": 114}, {"referenceID": 11, "context": "Building on the long line of the previous attempts of attention-based visual processing methods [2, 16, 12, 19, 7], the proposed EDRAM expands the idea of using recurrent connections inside the attention mechanism [19] and improves the Deep Recurrent Visual Attention Model (DRAM) [2] from several aspects as follows:", "startOffset": 96, "endOffset": 114}, {"referenceID": 18, "context": "Building on the long line of the previous attempts of attention-based visual processing methods [2, 16, 12, 19, 7], the proposed EDRAM expands the idea of using recurrent connections inside the attention mechanism [19] and improves the Deep Recurrent Visual Attention Model (DRAM) [2] from several aspects as follows:", "startOffset": 96, "endOffset": 114}, {"referenceID": 6, "context": "Building on the long line of the previous attempts of attention-based visual processing methods [2, 16, 12, 19, 7], the proposed EDRAM expands the idea of using recurrent connections inside the attention mechanism [19] and improves the Deep Recurrent Visual Attention Model (DRAM) [2] from several aspects as follows:", "startOffset": 96, "endOffset": 114}, {"referenceID": 18, "context": "Building on the long line of the previous attempts of attention-based visual processing methods [2, 16, 12, 19, 7], the proposed EDRAM expands the idea of using recurrent connections inside the attention mechanism [19] and improves the Deep Recurrent Visual Attention Model (DRAM) [2] from several aspects as follows:", "startOffset": 214, "endOffset": 218}, {"referenceID": 1, "context": "Building on the long line of the previous attempts of attention-based visual processing methods [2, 16, 12, 19, 7], the proposed EDRAM expands the idea of using recurrent connections inside the attention mechanism [19] and improves the Deep Recurrent Visual Attention Model (DRAM) [2] from several aspects as follows:", "startOffset": 281, "endOffset": 284}, {"referenceID": 1, "context": "[2], our network architecture was designed to satisfy the complexity requirements and the capability to learn a very nonlinear classification function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "EDRAM has been evaluated over the MNIST Cluttered dataset [16, 2] as well as a real-world object recognition task by using the Street View House Numbers (SVHN) dataset [17].", "startOffset": 58, "endOffset": 65}, {"referenceID": 1, "context": "EDRAM has been evaluated over the MNIST Cluttered dataset [16, 2] as well as a real-world object recognition task by using the Street View House Numbers (SVHN) dataset [17].", "startOffset": 58, "endOffset": 65}, {"referenceID": 16, "context": "EDRAM has been evaluated over the MNIST Cluttered dataset [16, 2] as well as a real-world object recognition task by using the Street View House Numbers (SVHN) dataset [17].", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "Since the attention mechanism is fully differentiable, the proposed network is trained with standard backpropagation and SGD by using Adam optimization algorithm [13].", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Gradient step clipping techniques are applied [15, 18] to ensure the absence of the gradient exploding during the learning over the recursive structures.", "startOffset": 46, "endOffset": 54}, {"referenceID": 17, "context": "Gradient step clipping techniques are applied [15, 18] to ensure the absence of the gradient exploding during the learning over the recursive structures.", "startOffset": 46, "endOffset": 54}, {"referenceID": 3, "context": "Following Cooijmans [4], the Batch Normalization proposed by Iofee and Szegedy [10] is used on the MNIST Cluttered dataset to estimate the statistics independently for each iteration.", "startOffset": 20, "endOffset": 23}, {"referenceID": 9, "context": "Following Cooijmans [4], the Batch Normalization proposed by Iofee and Szegedy [10] is used on the MNIST Cluttered dataset to estimate the statistics independently for each iteration.", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "Theano [3], Blocks and Fuel [20] are used to implement and conduct the experiments with the MNIST Cluttered and SVHN datasets.", "startOffset": 7, "endOffset": 10}, {"referenceID": 19, "context": "Theano [3], Blocks and Fuel [20] are used to implement and conduct the experiments with the MNIST Cluttered and SVHN datasets.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "[1] \u2014 Dynamic Capasity Networks (DCN), Jaderberg et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] \u2014 ST-CNN, Ba et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] \u2014 DRAM and the first result obtained on the SVHN dataset by Goodfellow et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] \u2014 11 layer CNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "We first evaluate EDRAM on the 100 \u00d7 100 MNIST Cluttered dataset [16], where each image contains randomly located hand-written digit surrounded with digit-like fragments.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "35% RAM [16], 8 glimpses, 12\u00d7 12 , 4 scales 8.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "11% Differentiable RAM [7], 8 glimpses, 12\u00d7 12 3.", "startOffset": 23, "endOffset": 26}, {"referenceID": 11, "context": "36% ST-CNN Single [12] 1.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "7% DCN [1], 8 glimpses, 14\u00d7 14 1.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "Following the experimental setup in [6, 2], the test set is formed from 13k images and the rest of data (train and extra sets) is used to train the networks.", "startOffset": 36, "endOffset": 42}, {"referenceID": 1, "context": "Following the experimental setup in [6, 2], the test set is formed from 13k images and the rest of data (train and extra sets) is used to train the networks.", "startOffset": 36, "endOffset": 42}, {"referenceID": 1, "context": "Similar to [2], RGB images are converted into grayscale.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "The heuristic of learning two separate models of different reading orders (forward, backward) as proposed in [2] is adapted.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "11 layer CNN [6] 3.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "96% Single DRAM [2] 5.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "[2] 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "9% ST-CNN Single [12] 3.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "7% ST-CNN Multi [12] 3.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "10 layer CNN 51 Single DRAM [2] 14 Single DRAM MC avg.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "[2] 14 forward-backward DRAM MC avg.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] 28 ST-CNN Single [12] 33 ST-CNN Multi [12] 37", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[2] 28 ST-CNN Single [12] 33 ST-CNN Multi [12] 37", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "[2] 28 ST-CNN Single [12] 33 ST-CNN Multi [12] 37", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "This creates a difficulty of applying NNs on different embedded platforms with limited memory and processing units, like mobile phones [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "Besides that, significant redundancy has been reported in many state-of-the-art neural network models [5].", "startOffset": 102, "endOffset": 105}], "year": 2017, "abstractText": "We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) \u2014 an improved attention-based architecture for multiple object recognition. The proposed model is a fully differentiable unit that can be optimized end-to-end by using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was employed as visual attention mechanism which allows to learn the geometric transformation of objects within images. With the combination of the Spatial Transformer and the powerful recurrent architecture, the proposed EDRAM can localize and recognize objects simultaneously. EDRAM has been evaluated on two publicly available datasets including MNIST Cluttered (with 70K cluttered digits) and SVHN (with up to 250k real world images of house numbers). Experiments show that it obtains superior performance as compared with the state-of-the-art models.", "creator": "LaTeX with hyperref package"}}}