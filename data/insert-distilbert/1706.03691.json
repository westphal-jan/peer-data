{"id": "1706.03691", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Certified Defenses for Data Poisoning Attacks", "abstract": "machine learning systems trained on user - provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. while recent work has proposed a number of attacks and defenses, little is understood about the worst - case loss of a defense in the face of a sufficiently determined attacker. we therefore address this by constructing approximate upper ground bounds justified on the loss across a broad family of attacks, for defenders that first perform outlier disaster removal followed subsequently by empirical risk minimization. our bound comes paired with a candidate attack trigger that nearly realizes the bound, giving us a powerful tool for quickly assessing defenses on a given generic dataset. empirically, we find that even under a simple defense, the mnist - 1 - 7 and dogfish datasets are resilient to attack, while in contrast the imdb domain sentiment dataset can just be driven from 12 % to 23 % test error by adding only 3 % poisoned data.", "histories": [["v1", "Fri, 9 Jun 2017 16:26:49 GMT  (864kb,D)", "http://arxiv.org/abs/1706.03691v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["jacob steinhardt", "pang wei koh", "percy liang"], "accepted": true, "id": "1706.03691"}, "pdf": {"name": "1706.03691.pdf", "metadata": {"source": "CRF", "title": "Certified Defenses for Data Poisoning Attacks", "authors": ["Jacob Steinhardt", "Pang Wei Koh"], "emails": ["jsteinha@stanford.edu", "pangwei@cs.stanford.edu", "pliang@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Traditional security systems seek to ensure that an attacker can never access or modify critical parts of a system, by creating clear boundaries between the system and outside world. In machine learning, however, the most critical ingredient of all \u2013 the training data \u2013 comes directly from the outside world. For a system trained on user data, an attacker can inject malicious data simply by creating a user account. Such data poisoning attacks require us to re-think what it means for a system to be secure.\nThe focus of the present work is on data poisoning attacks against classification algorithms, first studied by Biggio et al. (2012) and later by a number of others (Xiao et al., 2012; 2015b; Newell et al., 2014; Mei and Zhu, 2015b; Burkard and Lagesse, 2017; Koh and Liang, 2017). This body of work has demonstrated data poisoning attacks that can degrade classifier accuracy, sometimes dramatically. Moreover, while some defenses have been proposed against specific attacks (Laishram and Phoha, 2016), few have been stress-tested against a determined attacker.\nAre there defenses that are robust to a large class of data poisoning attacks? The main difficulty in answering this question is the near-limitless space of possible attacks. Because of this, it is impossible to conclude from empirical success alone that a defense that works against a known set of attacks will not fail against some new attack.\nIn this paper, we address this difficulty by presenting a framework for providing certified defenses and near-optimal attacks via no-regret learning. Our framework applies to defenders that (i) remove outliers residing outside a feasible set, then (ii) minimize a margin-based loss on the remaining data. For such defenders, we can generate approximate upper bounds on the efficacy of any data poisoning attack, along with a candidate attack that nearly matches the bound. We consider two different settings: first, where the outlier detector is trained independently and cannot be affected by the poisoned data, and second, where the data poisoning can attack the outlier detector as well.\nIn the first setting, we apply our framework to an \u201coracle\u201d defense that knows the true class centroids and removes points that are far away from the centroid of the corresponding class. While previous work showed successful attacks on the MNIST-1-7 (Biggio et al., 2012) and Dogfish (Koh and Liang, \u2217Equal contribution.\nar X\niv :1\n70 6.\n03 69\n1v 1\n[ cs\n.L G\n] 9\nJ un\n2 01\n7\n2017) image datasets in the absence of any defenses, we show (Section 4) that no attack is possible against this oracle \u2014 e.g., the test error of an SVM on either dataset is at most 4%, even after adding 30% poisoned data. On the other hand, our candidate attack increases classification test error on the IMDB sentiment corpus from 12% to 23% with only 3% poisoned data.\nFor the second setting, we consider a more realistic defender that uses the empirical (poisoned) centroids. For small amounts of poisoned data (\u2264 5%) we can still establish the resilience of MNIST1-7 and Dogfish to attack (Section 5). However, with more poisoned data, the attacker can subvert the outlier removal to obtain stronger attacks \u2014 e.g., increasing test error on MNIST-1-7 by 37% with 30% poisoned data."}, {"heading": "2 Problem Setting", "text": "Consider a prediction task from an input x \u2208 X (e.g., Rd) to an output y \u2208 Y (e.g., {\u22121,+1} for binary classification). Let ` be a non-negative loss function\u2014e.g., for linear classification with the hinge loss, `(\u03b8;x, y) = max(0, 1\u2212 y\u3008\u03b8, x\u3009) for a model \u03b8 \u2208 \u0398 \u2286 Rd and data point (x, y). Given a true data-generating distribution p\u2217 over X \u00d7 Y , define the test loss as L(\u03b8) = E(x,y)\u223cp\u2217 [`(\u03b8;x, y)]. We consider the causative attack model (Barreno et al., 2010), which consists of a game between two players: the defender (who seeks to learn a model \u03b8), and the attacker (who wants the learner to learn a bad model). The game proceeds as follows:\n\u2022 n data points are drawn from p\u2217 to produce a clean training dataset Dc. \u2022 The attacker adaptively chooses a \u201cpoisoned\u201d dataset Dp of n poisoned points, where \u2208 [0, 1]\nparametrizes the attacker\u2019s resources. \u2022 The defender trains on the full dataset Dc \u222a Dp to produce a model \u03b8\u0302, and incurs test loss L(\u03b8\u0302).\nThe defender\u2019s goal is to minimize the quantity L(\u03b8\u0302) while the attacker\u2019s goal is to maximize it.\nRemarks. We assume the attacker has full knowledge of the defender\u2019s algorithm and of the clean training data Dc. While this may seem generous to the attacker, it is widely considered poor practice to rely on secrecy for security (Kerckhoffs, 1883; Biggio et al., 2014a); moreover, a determined attacker can often reverse-engineer necessary system details (Tram\u00e8r et al., 2016).\nThe causative attack model allows the attacker to add points but not modify existing ones. Indeed, systems constantly collect new data (e.g., product reviews, user feedback on social media, or insurance claims), whereas modification of existing data would require first compromising the system.\nAttacks that attempt to increase the overall test loss L(\u03b8\u0302), known as indiscriminate availability attacks (Barreno et al., 2010), can be thought of as a denial-of-service attack. This is in contrast to targeted attacks on individual examples or sub-populations (e.g., Burkard and Lagesse, 2017). Both have serious security implications, but we focus on denial-of-service attacks, as they compromise the model in a broad sense and interfere with fundamental statistical properties of learning algorithms."}, {"heading": "2.1 Data Sanitization Defenses", "text": "A defender who trains na\u00efvely on the full (clean + poisoned) data Dc \u222a Dp is doomed to failure, as even a single poisoned point can in some cases arbitrarily change the model (Liu and Zhu, 2016; Park et al., 2017). In this paper, we consider data sanitization defenses (Cretu et al., 2008), which examine the full dataset and try to remove the poisoned points, for example by deleting outliers. Formally, the defender constructs a feasible set F \u2286 X \u00d7 Y and trains only on points in F :\n\u03b8\u0302 def = argmin\n\u03b8\u2208\u0398 L(\u03b8; (Dc \u222a Dp) \u2229 F), where L(\u03b8;S)\ndef = \u2211 (x,y)\u2208S `(\u03b8;x, y). (1)\nGiven such a defense F , we would like to compute an upper bound on the worst possible test loss over any attacker (choice of Dp)\u2014in symbols, maxDp L(\u03b8\u0302). Such a bound would certify that the defender incurs at most some level of loss no matter what the attacker does. We consider two classes of data sanitization defenses:\n\u2022 Fixed defenses, where F does not depend on Dp. One example for text classification is letting F be documents that contain only licensed words (Newell et al., 2014). Other examples are oracle\ndefenders that depend on the true distribution p\u2217. While such defenders are not implementable in practice, they provide bounds: if even an oracle can be attacked, then we should be worried. \u2022 Data-dependent defenses, where F depends on Dc \u222a Dp. These defenders try to estimate p\u2217 from Dc \u222a Dp and thus are implementable in practice. However, they open up a new line of attack wherein the attacker could choose the poisoned data Dp to change the feasible set F .\nExample: binary classification. Let \u00b5+ def = E[x | y = +1] and \u00b5\u2212 def = E[x | y = \u22121] be the centroids of the positive and negative classes. A natural data sanitization strategy is to remove points that are too far away from the corresponding centroid. We consider two ways of doing this: the sphere defense, which removes points outside a spherical radius, and the slab defense, which first projects points onto the line between the centroids and then discards points that are too far on this line:\nFsphere def = {(x, y) : \u2016x\u2212 \u00b5y\u20162 \u2264 ry}, Fslab def = {(x, y) : |\u3008x\u2212 \u00b5y, \u00b5y \u2212 \u00b5\u2212y\u3009| \u2264 sy}. (2)\nHere ry, sy are thresholds (e.g. chosen so that 30% of the data is removed). Note that both defenses are oracles (\u00b5y depends on p\u2217); in Section 5, we consider versions that estimate \u00b5 from Dc \u222a Dp. Figure 1 depicts both defenses on the MNIST-1-7 and IMDB datasets. For MNIST-1-7, it appears the constraints make it difficult for an attacker to damage the model. In the next section, we will present a method for certifying this. In contrast, IMDB looks far more attackable, which we will verify as well."}, {"heading": "3 Attack, Defense, and Duality", "text": "Recall that we are interested in the worst-case test loss maxDp L(\u03b8\u0302). To make progress, we consider three approximations: (i) we pass from the test loss to the training loss on the clean data, which are related by standard concentration arguments as long as we regularize the model appropriately (see Appendix B for details); (ii) we consider the training loss on the full (clean + poisoned) data, which upper bounds the loss on the clean data due to non-negativity of the loss. For any model \u03b8, we have:\nL(\u03b8) (i) \u2248 1 n L(\u03b8;Dc) (ii) \u2264 1 n L(\u03b8;Dc \u222a Dp). (3)\nWhile (ii) could be a source of looseness, it is tight if \u03b8\u0302 fits the poisoned data well (as we show is often true empirically). Finally, (iii) rather than requiring the defender to filter points outside the feasible set (which might remove good points in Dc), we consider attacks in the feasible set (Dp \u2286 F) and have the defender train on all of Dc \u222a Dp. Training on Dc \u222a Dp rather than (Dc \u222a Dp) \u2229 F seemed to yield similar results, but the latter is harder to provably verify and could therefore in theory be a source of vulnerability. Putting it all together, the worst-case test loss from any attack Dp with n\nAlgorithm 1 Online learning algorithm for generating an upper bound and candidate attack. Input: clean data Dc of size n, feasible set F , radius \u03c1, poisoned fraction , step size \u03b7. Initialize z(0) \u2190 0, \u03bb(0) \u2190 1\u03b7 , \u03b8\n(0) \u2190 0, U\u2217 \u2190\u221e. for t = 1, . . . , n do\nCompute (x(t), y(t)) = argmax(x,y)\u2208F `(\u03b8 (t\u22121);x, y). U\u2217 \u2190 min ( U\u2217, 1nL(\u03b8 (t\u22121);Dc) + `(\u03b8(t\u22121);x(t), y(t)) ) . g(t) \u2190 1n\u2207L(\u03b8 (t\u22121);Dc) + \u2207`(\u03b8(t\u22121);x(t), y(t)). Update: z(t) \u2190 z(t\u22121) \u2212 g(t), \u03bb(t) \u2190 max(\u03bb(t\u22121), \u2016z (t)\u20162 \u03c1 ), \u03b8 (t) \u2190 z (t) \u03bb(t) .\nend for Output: upper bound U\u2217 and candidate attack Dp = {(x(t), y(t))} nt=1.\nelements is approximately upper bounded as follows:\nmax Dp L(\u03b8\u0302) (i) \u2248 max Dp 1 n L(\u03b8\u0302;Dc) (ii) \u2264max Dp 1 n L(\u03b8\u0302;Dc \u222a Dp)\n(iii) \u2248 max Dp\u2286F min \u03b8\u2208\u0398 1 n L(\u03b8;Dc \u222a Dp) def = M. (4)\nHere the final step is because \u03b8\u0302 is chosen to minimize L(\u03b8;Dc \u222a Dp). The minimax loss M is the central quantity that we will focus on in the sequel; it has duality properties that will yield insight into the nature of the optimal attack. Intuitively, the attacker that achieves M is trying to maximize the loss on the full dataset by adding poisoned points in the feasible set F ."}, {"heading": "3.1 Fixed Defenses: Computing the Minimax Loss via Online Learning", "text": "We now focus on computing the minimax loss M (4) when F is independent of Dp (fixed defenses). In the process of computing M, we will also produce candidate attacks. Our algorithm is based on no-regret online learning, which models a game between a learner and nature and thus is a natural fit to our data poisoning setting. For simplicity of exposition we assume \u0398 is an `2-ball of radius \u03c1.\nOur algorithm, shown in Algorithm 1, is very simple: in each iteration, it alternates between finding the worst attack point (x(t), y(t)) with respect to the current model \u03b8(t\u22121) and updating the model in the direction of the attack point, producing \u03b8(t). The attack Dp is the set of points thus found. To derive the algorithm, we simply swap min and max in (4) to get an upper bound on M, after which the optimal attack set Dp \u2286 F for a fixed \u03b8 is realized by a single point (x, y) \u2208 F :\nM \u2264 min \u03b8\u2208\u0398 max Dp\u2286F\n1 n L(\u03b8;Dc \u222a Dp) = min \u03b8\u2208\u0398 U(\u03b8), where U(\u03b8) def= 1 n L(\u03b8;Dc) + max (x,y)\u2208F `(\u03b8;x, y).\n(5)\nNote that U(\u03b8) upper bounds M for any model \u03b8. Algorithm 1 follows the natural strategy of minimizing U(\u03b8) to iteratively tighten this upper bound. In the process, the iterates {(x(t), y(t))} form a candidate attack Dp whose induced loss 1nL(\u03b8\u0302;Dc \u222a Dp) is a lower bound on M. We can monitor the duality gap between lower and upper bounds on M to ascertain the quality of the bounds.\nMoreover, assuming the loss ` is convex in \u03b8, U(\u03b8) is convex in \u03b8 (regardless of the structure of F , which could even be discrete). In this case, if we minimize U(\u03b8) using any online learning algorithm with sublinear regret, the duality gap vanishes. This is summarized as follows (proof in Appendix A): Proposition 1. Assume the loss ` is convex. Suppose that an online learning algorithm (e.g., Algorithm 1) is used to minimize U(\u03b8), and that the parameters (x(t), y(t)) maximize the loss `(\u03b8(t\u22121);x, y) for the iterates \u03b8(t\u22121) of the online learning algorithm. Let U\u2217 = min nt=1 U(\u03b8\n(t)). Also suppose that the learning algorithm has regret Regret(T ) after T time steps. Then, for the attack Dp = {(x(t), y(t))} nt=1, the corresponding parameter \u03b8\u0302 (defined in (1)) satisfies:\n1 n L(\u03b8\u0302;Dc \u222a Dp) \u2264M \u2264 U\u2217 and U\u2217 \u2212 1 n L(\u03b8\u0302;Dc \u222a Dp) \u2264 Regret( n) n . (6)\nHence, any algorithm whose average regret Regret( n) n is small will have a nearly optimal candidate attack Dp. The particular algorithm depicted in Algorithm 1 is a variant of regularized dual\naveraging (Xiao, 2010), which is an alternative to gradient descent that often converges faster for norm-constrained optimization problems. In summary, we have a simple learning algorithm that simultaneously computes upper bounds on the minimax loss as well as a candidate attack, which provides a lower bound. Of course, the minimax loss M is only an approximation to the true worst case test loss (via (4)). We examine the tightness of this approximation empirically in Section 4."}, {"heading": "3.2 Data-Dependent Defenses: Upper and Lower Bounds", "text": "We now turn our attention to data-dependent defenders, where the feasible set F depends on the data Dc \u222a Dp (and hence can be influenced by the attacker). For example, consider the slab defense (see (2)) that uses the empirical (poisoned) mean instead of the true mean:\nFslab(Dp) def = {(x, y) : |\u3008x\u2212 \u00b5\u0302y(Dp), \u00b5\u0302y(Dp)\u2212 \u00b5\u0302\u2212y(Dp)\u3009| \u2264 sy}, (7)\nwhere \u00b5\u0302y(Dp) is the empirical mean over Dc \u222a Dp; the notation F(Dp) tracks the dependence of the feasible set on Dp. Similarly to Section 3.1, we analyze the minimax loss M, which we can bound as in (5): M \u2264 min\u03b8\u2208\u0398 maxDp\u2286F(Dp) 1nL(\u03b8;Dc \u222a Dp). However, unlike in (5), it is no longer the case that the optimalDp places all points at a single location due to the dependence of F on Dp; we must jointly maximize over the full set Dp. To improve tractability, we take a continuous relaxation: we think of Dp as a probability distribution with mass 1 n on each point in Dp, and relax this to allow any probability distribution \u03c0p. The constraint then becomes supp(\u03c0p) \u2286 F(Dp) (where supp denotes the support), and the analogue to (5) is\nM \u2264 min \u03b8\u2208\u0398\nU\u0303(\u03b8), where U\u0303(\u03b8) def= 1\nn L(\u03b8;Dc) + max supp(\u03c0p)\u2286F(\u03c0p) E\u03c0p [`(\u03b8;x, y)]. (8)\nThis suggests employing the same online learning Algorithm 1 to minimize U\u0303(\u03b8). Indeed, this is what we shall do, but there are a few caveats:\n\u2022 The maximization problem in the definition of U\u0303(\u03b8) is in general quite difficult. We will, however, solve a specific instance in Section 5.\n\u2022 The constraint set for \u03c0p is non-convex, so duality (Proposition 1) no longer holds.\nWe will run Algorithm 1, at each iteration obtaining a distribution \u03c0(t)p and upper bound U\u0303(\u03b8(t)). Then, for each \u03c0(t)p we will generate a candidate attack by sampling n points from \u03c0 (t) p , and take the best resulting attack. In Section 4 we will see that despite a lack of rigorous theoretical guarantees, this often leads to good upper bounds and attacks in practice."}, {"heading": "4 Experiments I: Oracle Defenses", "text": "An advantage of our framework is that we obtain a tool that can be easily run on new datasets and defenses to learn about the robustness of the defense and gain insight into potential attacks. We first study two image datasets: MNIST-1-7, and the Dogfish dataset used by Koh and Liang (2017). For MNIST-1-7, following Biggio et al. (2012), we considered binary classification between the digits\n1 and 7; this left us with n = 13007 training examples of dimension 784. For Dogfish, which is a binary classification task, we used the same neural net features as in Koh and Liang (2017), so that each of the n = 1800 training images is represented by a 2048-dimensional vector. For this and subsequent experiments, our loss ` is the hinge loss (i.e., we train an SVM).\nWe consider the combined oracle slab and sphere defense from Section 2.1: F = Fslab \u2229 Fsphere. To run Algorithm 1, we need to maximize the loss over (x, y) \u2208 F . Note that maximizing the hinge loss `(\u03b8;x, y) is equivalent to minimizing \u3008\u03b8, x\u3009. Therefore, we can solve the following quadratic program (QP) for each y \u2208 {+1,\u22121} and take the one with higher loss:\nminimize y\u3008\u03b8, x\u3009 subject to \u2016x\u2212 \u00b5y\u201622 \u2264 r2y, |\u3008x\u2212 \u00b5y, \u00b5y \u2212 \u00b5\u2212y\u3009| \u2264 sy. (9)\nThe results of Algorithm 1 are given in Figures 2a and 2b; here and elsewhere, we used a combination of CVXPY (Diamond and Boyd, 2016), YALMIP (L\u00f6fberg, 2004), SeDuMi (Sturm, 1999), and Gurobi (Gurobi Optimization, Inc., 2016) to solve the optimization. We plot the upper bound U\u2217 computed by Algorithm 1, as well as the train and test loss induced by the corresponding attack Dp. Except for small , the model \u03b8\u0302 fits the poisoned data almost perfectly, and hence the loss L(\u03b8\u0302;Dc) on the clean data nearly matches its upper bound L(\u03b8\u0302;Dc \u222aDp) (which in turn matches U\u2217). On both datasets, U\u2217 is small (< 0.1 with = 0.3), showing that they are resilient to attack under the oracle defense; indeed, the maximum test 0-1 error on either dataset, for up to 0.3, was 4%.\nWe compare our attack to two baselines in Figure 2c \u2014 the gradient descent method employed by Biggio et al. (2012) and Mei and Zhu (2015b), and a simple baseline that inserts copies of points from Dc with the opposite label (subject to the flipped points lying in F ). Our attack consistently performs better; for the gradient algorithm, this seems to be due to local minima.\u2217 Finally, we visualize our attack in Figure 1a. Interestingly, though the attack was free to place points anywhere, most of the attack is tightly concentrated around a single point at the boundary of F for the negative class."}, {"heading": "4.1 Text Data: Handling Integrity Constraints", "text": "We next consider attacks on text data. Beyond the the sphere and slab constraints, a valid attack on text data must satisfy additional integrity constraints (Newell et al., 2014) \u2014 if we encode input text t by a vector x = \u03c6(t) of indicator features, then each entry of \u03c6(t) is a non-negative integer.\u2020\nAlgorithm 1 still applies in this case \u2014 the only difference is that the QP from Section 4 has the added constraint x \u2208 Zd\u22650 and hence becomes an integer quadratic program (IQP), which can be computationally expensive to solve. We can still obtain upper bounds simply by relaxing the integrity constraints; the only issue is that the points x(t) in the corresponding attack will have continuous values, and hence don\u2019t correspond to actual text inputs. To address this, we can use an IQP solver such as Gurobi to find an approximately optimal feasible x. This yields a valid candidate attack, but it might not be optimal if the solver doesn\u2019t find near-optimal solutions. \u2217Though Mei and Zhu (2015b) state that their cost is convex, they communicated to us that this is incorrect. \u2020Note that in the previous section, we ignored such integrity constraints for simplicity.\nWe ran both the upper bound relaxation and the IQP solver on two text datasets, the Enron spam corpus (Metsis et al., 2006) and the IMDB sentiment corpus (Maas et al., 2011). The Enron training set consists of n = 4137 e-mails (30% spam and 70% non-spam), with d = 5166 distinct words. The IMDB training set consists of n = 25000 product reviews with d = 89527 distinct words. We used bag-of-words features, which yields test accuracy 97% and 88%, respectively, in the absence of poisoned data. IMDB was too large for Gurobi to even approximately solve the IQP, so we resorted to a randomized rounding heuristic to convert the continuous relaxation to an integer solution.\nResults are given in Figure 3; there is a relatively large gap between the upper bound and the attack. Despite this, the attacks are relatively successful. Most striking is the attack on IMDB, which increases test error from 12% to 23% for = 0.03, despite having to pass the oracle defender.\nTo understand why the attacks are so much more successful in this case, we can consult Figure 1b. In contrast to MNIST-1-7, for IMDB the defenses place few constraints on the attacker. This seems to be a consequence of the high dimensionality of IMDB and the large number of irrelevant features, which increase the size of F without a corresponding increase in separation between the classes."}, {"heading": "5 Experiments II: Data-Dependent Defenses", "text": "We now revisit the MNIST and Dogfish data sets. Before, we saw that they were unattackable provided we had an oracle defender that knew the true class means. If we instead consider a data-dependent defender that uses the empirical (poisoned) means, how much can this change the attackability of these data sets? In this section, we will see that the answer is quite a lot.\nAs described in Section 3.2, we can still use our framework to obtain upper and lower bounds even in this data-dependent case, although the bounds won\u2019t necessarily match. The main difficulty is in computing U\u0303(\u03b8), which involves a potentially intractable maximization (see (8)). However, for 2-class SVMs there is a tractable semidefinite programming algorithm; the full details are in Appendix D, but the rough idea is the following: we can show that the optimal distribution \u03c0p in (8) is supported on at most 4 points (one support vector and one non-support vector in each class). Moreover, for a fixed \u03c0p, the constraints and objective depend only on inner products between the 4 attack points together with the class means \u00b5 (on the clean data) and the model \u03b8. Thus, we can solve for the optimal attack locations with a 42-variable SDP. Then in an outer loop, we randomly sample \u03c0p over the 3-dimensional simplex and take the one with the highest loss. Running this algorithm on MNIST-1-7 yields the results in Figure 4a. On the test set, our = 0.3 attack leads to a 0.67 increase in hinge loss and a 0.36 increase in 0-1 loss. Similarly, on the Dogfish dataset, our = 0.3 attack achieves a 0.67 increase in test hinge loss and a 0.32 increase in 0-1 loss.\nThe geometry of the attack is depicted in Figure 4b. By carefully choosing the location of the attack, the attacker can place points that lie substantially outside original (clean) feasible set. Thus,\nthere appears to be substantial danger in employing data-dependent defenders \u2014 beyond the greater difficulty of analyzing them, they seem to actually be more vulnerable to attack."}, {"heading": "6 Related Work", "text": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al. (2014a), Papernot et al. (2016b), and Gardiner and Nagaraja (2016) for some recent surveys.\nOur contribution relates to the long line of work on data poisoning attacks; beyond linear classifiers, others have studied the LASSO (Xiao et al., 2015a), clustering (Biggio et al., 2013; 2014c), PCA (Rubinstein et al., 2009), topic modeling (Mei and Zhu, 2015a), collaborative filtering (Li et al., 2016), neural networks (Yang et al., 2017), and other models (Mozaffari-Kermani et al., 2015; Vuurens et al., 2011; Wang, 2016). There have also been a number of demonstrated vulnerabilities in deployed systems (Newsome et al., 2006; Laskov and \u0160rndic\u0300, 2014; Biggio et al., 2014b). We provide formal scaffolding to this line of work, by providing a tool for certifying a defense against a range of attacks.\nA striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a). These exhibit security vulnerabilities at test time, whereas data poisoning is a vulnerability at training time. Adversarial training (Goodfellow et al., 2015) and generative adversarial networks (Goodfellow et al., 2014) are similarly focused on test time; they both improve test performance by altering the training objective. Recent adversarial attacks on reinforcement learners (Huang et al., 2017; Behzadan and Munir, 2017; Lin et al., 2017) blend train and test vulnerabilities.\nFinally, a number of authors have studied the theoretical question of learning in the presence of adversarial errors, under a priori distributional assumptions on the data. Robust algorithms have been exhibited for mean and covariance estimation and clustering (Diakonikolas et al., 2016; Lai et al., 2016; Charikar et al., 2017), classification (Klivans et al., 2009; Awasthi et al., 2014), regression (Nasrabadi et al., 2011; Nguyen and Tran, 2013; Chen et al., 2013; Bhatia et al., 2015) and crowdsourced data aggregation (Steinhardt et al., 2016)."}, {"heading": "7 Discussion", "text": "In this paper we have presented a tool for studying data poisoning defenses, which goes beyond empirical validation by providing certificates for a large family of attacks. Having applied this framework to binary SVMs, there are a number of extensions we can consider: e.g. to other loss functions or to multiclass classification. We can also consider defenses beyond the sphere and slab considered here \u2014 for instance, sanitizing text data using a language model. In all these cases, the minimax loss M gives us a natural starting point for investigating both attack and defense.\nSeparately, the bound L(\u03b8\u0302) / M was useful because M admits the natural minimax formulation (5), but the worst-case L(\u03b8\u0302) can be expressed directly as a bilevel optimization problem (Mei and Zhu, 2015b), which is intractable in general but admits a number of heuristics (Bard, 1999). Bilevel optimization has been considered in the related setting of Stackelberg games (Br\u00fcckner and Scheffer, 2011; Br\u00fcckner et al., 2012; Zhou and Kantarcioglu, 2016), and is natural to apply here as well.\nTo conclude, we quote Biggio et al., who call for the following methodology for evaluating defenses:\nTo pursue security in the context of an arms race it is not sufficient to react to observed attacks, but it is also necessary to proactively anticipate the adversary by predicting the most relevant, potential attacks through a what-if analysis; this allows one to develop suitable countermeasures before the attack actually occurs, according to the principle of security by design.\nThe existing paradigm for such proactive anticipation is to design various hypothetical attacks against which to test the defenses. However, such an evaluation is fundamentally limited because it leaves open the possibility that there is a more clever attack that we failed to think of. Our approach provides a first step towards surpassing this limitation, by not just anticipating but certifying the reliability of a defender, thus implicitly considering an infinite number of attacks before they occur."}, {"heading": "A Proof of Proposition 1", "text": "Proposition 1 follows by standard duality arguments which we reproduce here. First recall the definition of Regret: for a sequence of loss functions ft(\u03b8), t = 1, . . . , T , and an algorithm with iterates \u03b8(1), . . . , \u03b8(T ), regret is defined as\nRegret(T ) def = T\u2211 t=1 ft(\u03b8 (t))\u2212min \u03b8\u2208\u0398 T\u2211 t=1 ft(\u03b8). (10)\nIn our particular case we take ft(\u03b8) = 1nL(\u03b8;Dc) + `(\u03b8;x (t+1), y(t+1)). Hence\nft(\u03b8 (t)) =\n1 n L(\u03b8(t);Dc) + `(\u03b8(t);x(t+1), y(t+1))\n= 1\nn L(\u03b8(t);Dc) + max (x,y)\u2208F `(\u03b8(t);x, y) = U(\u03b8(t)). (11)\nSubstituting into (10) and averaging over T , we have\nRegret(T )\nT =\n1\nT T\u2211 t=1 U(\u03b8(t))\u2212min \u03b8\u2208\u0398\n( 1\nn L(\u03b8;Dc) + T T\u2211 t=1 `(\u03b8;x(t), y(t))\n) . (12)\nFor t = n the right-hand term is equal to 1nL(\u03b8;Dc \u2229 {(x (t), y(t))} nt=1). Letting Dp = {(x(t), y(t)} nt=1 and upper-bounding the min over \u03b8 by the value at \u03b8\u0302, we obtain\n1 n L(\u03b8\u0302;Dc \u222a Dp) \u2265 1 T T\u2211 t=1 U(\u03b8(t))\u2212 RegretT T , (13)\nand in particular 1nL(\u03b8\u0302;Dc \u222a Dp) \u2265 U \u2217 \u2212 Regret(T )T , as was to be shown."}, {"heading": "B Defending Against Overfitting Attacks", "text": "In Section 3 we claimed that it is possible to defend against overfitting defenses with appropriate regularization. In this section we justify this claim. The key is the classical theory of uniform convergence, which allows us to say that, with probability 1\u2212 \u03b4, the following uniform bound holds:\u2223\u2223\u2223 1\nN \u2211 (x,y)\u2208Dc `(\u03b8;x, y)\u2212Ex,y\u223cp\u2217 [`(\u03b8;x, y)] \u2223\u2223\u2223 \u2264 E(N, \u03c1, \u03b4), (14)\nwhere E is an error bound that is roughly \u03c1 \u221a\nlog(1/\u03b4) N . More precisely, Kakade et al. (2009) show the\nfollowing: Theorem 1 (Corollary 5 of Kakade et al. (2009)). Let `(\u03b8;x, y) be any margin-based loss: `(\u03b8;x, y) = \u03c6(y\u3008\u03b8, x\u3009), where \u03c6 is 1-Lipschitz. Then the bound (14) holds with probability 1 \u2212 \u03b4,\nfor E(N, \u03c1, \u03b4) = \u03c1R (\u221a\n4 n +\n\u221a log(1/\u03b4)\n2n\n) , where R is such that \u2016x\u20162 \u2264 R with probability 1.\nBy setting \u03c1 appropriately relative to R and n we can therefore guarantee that the train and test losses in (14) are close together, and therefore rule out any overfitting attack (because any attack that makes the test loss high would also have to make the train loss high)."}, {"heading": "C Regret Bound for Adaptive RDA", "text": "Our optimization algorithm (Algorithm 1) is similar in spirit to Regularized Dual Averaging (Xiao, 2010), but the known regret bounds for RDA do not apply directly because the regularizer is chosen adaptively to ensure the norm constraint \u2016\u03b8\u20162 \u2264 \u03c1 holds. In fact, a somewhat different analysis is required in this case, closer in spirit to that given by Steinhardt et al. (2014) for sparse linear regression. While the details would take us beyond the scope of this paper, we state the regret bound here:\nTheorem 2. After T steps of the update in Algorithm 1, the regret of Algorithm 1 can be bounded as\nRegret(T ) \u2264 \u03c1 2\n2\u03b7 + T\u2211 t=1 \u2016g(t)\u201622 2\u03bbt . (15)\nWe make two observations: first, since \u03bbt \u2265 1\u03b7 necessarily, by setting \u03b7 to be on the order of 1\u221a T we\ncan ensure average regret O(1/ \u221a T ). On the other hand, in many instances \u03bbt will actually increase linearly with t (in order to enforce the norm constraints \u2016\u03b8\u20162 \u2264 \u03c1) in which case the average regret decreases at the faster rate O( log(T )T ). In either case, the average regret goes to 0 as T \u2192\u221e."}, {"heading": "D Semidefinite Program for U\u0303(\u03b8)", "text": "Here we elaborate on the semidefinite program for U\u0303(\u03b8) that was discussed in Section 5. Recall the definition of U\u0303(\u03b8):\nU\u0303(\u03b8) = 1\nn L(Dc) + max supp(\u03c0p)\u2286F(\u03c0p) E\u03c0p [`(\u03b8;x, y)]. (16)\nOur goal is to solve the maximization over \u03c0p in the special case that F is defined by the datadependent sphere and slab defenses (with empirical centroids) and `(\u03b8;x, y) = max(1\u2212 y\u3008\u03b8, x\u3009, 0) is the hinge loss. First, we argue that the optimal \u03c0p without loss of generality is supported on at most four points (xa,+, 1), (xb,+, 1), (xa,\u2212,\u22121), and (xb,\u2212,\u22121), where the a points are support vectors and the b points are non-support vectors.\nIndeed, suppose that there are two distinct support vectors which both lie in the positive class. Then replacing them both with their midpoint does not affect either F(\u03c0p) or E\u03c0p [`(\u03b8;x, y)]; moreover, since F(\u03c0p) is convex for fixed \u03c0p both points are still feasible. A similar argument applies to the non-support vectors and to the negative class, so that indeed we may assume there are at most the four distinct points above in supp(\u03c0p).\nNow, let \u03c0a,+, \u03c0a,\u2212, \u03c0b,+, and \u03c0b,\u2212 be the weights of these points under \u03c0p. Letting \u00b5+ and \u00b5\u2212 be the empirical means of the positive and negative class over Dc, and p+ and p\u2212 the empirical probability of the two classes, we have the following expression for \u00b5\u0302y:\n\u00b5\u0302y(\u03c0p) = py\u00b5y + \u03c0a,yxa,y + \u03c0b,yxb,y\npy + \u03c0a,y + \u03c0b,y . (17)\nMoreover, the objective E\u03c0p [`(\u03b8;x, y)] may be written as\nE\u03c0p [`(\u03b8;x, y)] = \u03c0a,+(1\u2212 \u3008\u03b8, xa,+\u3009) + \u03c0a,\u2212(1 + \u3008\u03b8, xa,\u2212\u3009), (18) using the assumption that the xa are support vectors and the xb are not.\nNow, the sphere and slab constraints may be written as\n|\u3008xi,y \u2212 \u00b5\u0302y, \u00b5\u0302y \u2212 \u00b5\u0302\u2212y\u3009| \u2264 sy, (19) \u3008xi,y \u2212 \u00b5\u0302y, xi,y \u2212 \u00b5\u0302y\u3009 \u2264 r2y (20)\nfor i \u2208 {a, b}, y \u2208 {+1,\u22121}. We also have the constraints 1\u2212 y\u3008\u03b8, xa,y\u3009 \u2265 0 (21) 1\u2212 y\u3008\u03b8, xb,y\u3009 \u2264 0 (22)\nfor y \u2208 {+1,\u22121} (encoding the constraints that the xa are support vectors and the xb are not). A careful examination reveals that, for fixed \u03c0{a,b},{+,\u2212}, all terms in (18-22) can be written as linear inequality constraints in the inner products between the 7 vectors xa,+, xa,\u2212, xb,+, xb,\u2212, \u00b5+, \u00b5\u2212, \u03b8. Therefore, by changing variables to the 7\u00d7 7 Gram matrix G among these vectors, we can express the maximization in (16) as a semidefinite program over these variables, with equality constraints for the known inner products between \u00b5+, \u00b5\u2212, and \u03b8.\nMoreover, for any matrix G 0 satisfying these equality constraints, it is possible to recover vectors xa,+, xa,\u2212, xb,+, and xb,\u2212 (depending on \u00b5+, \u00b5\u2212, and \u03b8) whose inner products match the Gram\nmatrix G. Precisely, if G = [ G11 G12 G21 G22 ] is the Gram matrix (with block 1 being the 4 vectors x and block 2 being the 3 known vectors \u00b5{+,\u2212}, \u03b8), then for any vectors v{a,b},{+,\u2212} orthogonal to the span of \u00b5+, \u00b5\u2212, and \u03b8, we can take\n[ xa,+ xa,\u2212 xb,+ xb,\u2212 ] = [ va,+ va,\u2212 vb,+ vb,\u2212 ]A+ [ \u00b5+ \u00b5\u2212 \u03b8 ]B, (23)\nwhere A>A = G11 \u2212G12G\u202022G21 and B = G \u2020 22G21, and \u2020 denotes pseudoinverse. This allows us to compute not only the optimal objective value, but to actually recover vectors x realizing it.\nTo finish, we must handle the fact that the weights \u03c0{a,b},{+,\u2212} are not known. However, they comprise only a 3-dimensional parameter space, and hence we can approximate the maximum over all \u03c0{a,b},{+,\u2212} through Monte Carlo simulation (i.e., randomly sample the weights a sufficiently large number of times and take the best)."}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M.F. Balcan", "P.M. Long"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Practical Bilevel Optimization: Algorithms and Applications", "author": ["J.F. Bard"], "venue": null, "citeRegEx": "Bard.,? \\Q1999\\E", "shortCiteRegEx": "Bard.", "year": 1999}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J.D. Tygar"], "venue": "Machine Learning,", "citeRegEx": "Barreno et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2010}, {"title": "Vulnerability of deep reinforcement learning to policy induction attacks", "author": ["V. Behzadan", "A. Munir"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Behzadan and Munir.,? \\Q2015\\E", "shortCiteRegEx": "Behzadan and Munir.", "year": 2015}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Biggio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2012}, {"title": "Is data clustering in adversarial settings secure", "author": ["B. Biggio", "I. Pillai", "S.R. Bul\u00f2", "D. Ariu", "M. Pelillo", "F. Roli"], "venue": "In Workshop on Artificial Intelligence and Security AISec),", "citeRegEx": "Biggio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2013}, {"title": "Security evaluation of pattern classifiers under attack. Knowledge and Data Engineering", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Transactions on,", "citeRegEx": "Biggio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2014}, {"title": "Poisoning behavioral malware clustering", "author": ["B. Biggio", "K. Rieck", "D. Ariu", "C. Wressnegger", "I. Corona", "G. Giacinto", "F. Roli"], "venue": "In Workshop on Artificial Intelligence and Security (AISec),", "citeRegEx": "Biggio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2014}, {"title": "Poisoning completelinkage hierarchical clustering", "author": ["B. Biggio", "B.S. Rota", "P. Ignazio", "M. Michele", "M.E. Zemene", "P. Marcello", "R. Fabio"], "venue": "In Workshop on Structural, Syntactic, and Statistical Pattern Recognition,", "citeRegEx": "Biggio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2011}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Br\u00fcckner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Br\u00fcckner et al\\.", "year": 2012}, {"title": "Analysis of causative attacks against SVMs learning from data streams", "author": ["C. Burkard", "B. Lagesse"], "venue": "In International Workshop on Security And Privacy Analytics,", "citeRegEx": "Burkard and Lagesse.,? \\Q2017\\E", "shortCiteRegEx": "Burkard and Lagesse.", "year": 2017}, {"title": "Hidden voice commands", "author": ["N. Carlini", "P. Mishra", "T. Vaidya", "Y. Zhang", "M. Sherr", "C. Shields", "D. Wagner", "W. Zhou"], "venue": "In USENIX Security,", "citeRegEx": "Carlini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carlini et al\\.", "year": 2016}, {"title": "Learning from untrusted data", "author": ["M. Charikar", "J. Steinhardt", "G. Valiant"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Charikar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2017}, {"title": "Robust high dimensional sparse regression and matching pursuit", "author": ["Y. Chen", "C. Caramanis", "S. Mannor"], "venue": "arXiv,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Robust estimators in high dimensions without the computational intractability", "author": ["I. Diakonikolas", "G. Kamath", "D. Kane", "J. Li", "A. Moitra", "A. Stewart"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Diakonikolas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diakonikolas et al\\.", "year": 2016}, {"title": "CVXPY: A Python-embedded modeling language for convex optimization", "author": ["S. Diamond", "S. Boyd"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Diamond and Boyd.,? \\Q2016\\E", "shortCiteRegEx": "Diamond and Boyd.", "year": 2016}, {"title": "On the security of machine learning in malware c&c detection: A survey", "author": ["J. Gardiner", "S. Nagaraja"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Gardiner and Nagaraja.,? \\Q2016\\E", "shortCiteRegEx": "Gardiner and Nagaraja.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Adversarial attacks on neural network policies. arXiv, 2017", "author": ["S. Huang", "N. Papernot", "I. Goodfellow", "Y. Duan", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Learning halfspaces with malicious noise", "author": ["A.R. Klivans", "P.M. Long", "R.A. Servedio"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Klivans et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Klivans et al\\.", "year": 2009}, {"title": "Understanding black-box predictions via influence functions", "author": ["P.W. Koh", "P. Liang"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Koh and Liang.,? \\Q2017\\E", "shortCiteRegEx": "Koh and Liang.", "year": 2017}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": null, "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Agnostic estimation of mean and covariance", "author": ["K.A. Lai", "A.B. Rao", "S. Vempala"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Lai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2016}, {"title": "Curie: A method for protecting SVM classifier from poisoning", "author": ["R. Laishram", "V.V. Phoha"], "venue": "attack. arXiv,", "citeRegEx": "Laishram and Phoha.,? \\Q2016\\E", "shortCiteRegEx": "Laishram and Phoha.", "year": 2016}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["P. Laskov", "N. \u0160rndic"], "venue": "In Symposium on Security and Privacy,", "citeRegEx": "Laskov and \u0160rndic\u0300.,? \\Q2014\\E", "shortCiteRegEx": "Laskov and \u0160rndic\u0300.", "year": 2014}, {"title": "Data poisoning attacks on factorization-based collaborative filtering", "author": ["B. Li", "Y. Wang", "A. Singh", "Y. Vorobeychik"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Tactics of adversarial attack on deep reinforcement learning agents", "author": ["Y. Lin", "Z. Hong", "Y. Liao", "M. Shih", "M. Liu", "M. Sun"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2017}, {"title": "The teaching dimension of linear learners", "author": ["J. Liu", "X. Zhu"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Liu and Zhu.,? \\Q2016\\E", "shortCiteRegEx": "Liu and Zhu.", "year": 2016}, {"title": "YALMIP: A toolbox for modeling and optimization in MATLAB", "author": ["J. L\u00f6fberg"], "venue": "CACSD,", "citeRegEx": "L\u00f6fberg.,? \\Q2004\\E", "shortCiteRegEx": "L\u00f6fberg.", "year": 2004}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "The security of latent Dirichlet allocation", "author": ["S. Mei", "X. Zhu"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Mei and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu.", "year": 2015}, {"title": "Spam filtering with naive Bayes \u2013 which naive Bayes", "author": ["V. Metsis", "I. Androutsopoulos", "G. Paliouras"], "venue": "In CEAS,", "citeRegEx": "Metsis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Metsis et al\\.", "year": 2006}, {"title": "Systematic poisoning attacks on and defenses for machine learning in healthcare", "author": ["M. Mozaffari-Kermani", "S. Sur-Kolay", "A. Raghunathan", "N.K. Jha"], "venue": "IEEE Journal of Biomedical and Health Informatics,", "citeRegEx": "Mozaffari.Kermani et al\\.,? \\Q1893\\E", "shortCiteRegEx": "Mozaffari.Kermani et al\\.", "year": 1893}, {"title": "On the practicality of integrity attacks on document-level sentiment analysis", "author": ["A. Newell", "R. Potharaju", "L. Xiang", "C. Nita-Rotaru"], "venue": "In Workshop on Artificial Intelligence and Security (AISec),", "citeRegEx": "Newell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Newell et al\\.", "year": 2014}, {"title": "Paragraph: Thwarting signature learning by training maliciously", "author": ["J. Newsome", "B. Karp", "D. Song"], "venue": "In International Workshop on Recent Advances in Intrusion Detection,", "citeRegEx": "Newsome et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Newsome et al\\.", "year": 2006}, {"title": "Exact recoverability from dense corrupted observations via `1-minimization", "author": ["N.H. Nguyen", "T.D. Tran"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nguyen and Tran.,? \\Q2017\\E", "shortCiteRegEx": "Nguyen and Tran.", "year": 2017}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Towards the science of security and privacy in machine learning. arXiv, 2016b", "author": ["N. Papernot", "P. McDaniel", "A. Sinha", "M. Wellman"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Resilient linear classification: an approach to deal with attacks on training data", "author": ["S. Park", "J. Weimer", "I. Lee"], "venue": "In International Conference on Cyber-Physical Systems,", "citeRegEx": "Park et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Park et al\\.", "year": 2017}, {"title": "Antidote: Understanding and defending against poisoning of anomaly detectors", "author": ["B. Rubinstein", "B. Nelson", "L. Huang", "A.D. Joseph", "S. Lau", "S. Rao", "N. Taft", "J. Tygar"], "venue": "In ACM SIGCOMM Conference on Internet measurement conference,", "citeRegEx": "Rubinstein et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2009}, {"title": "The statistics of streaming sparse regression", "author": ["J. Steinhardt", "S. Wager", "P. Liang"], "venue": null, "citeRegEx": "Steinhardt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Steinhardt et al\\.", "year": 2014}, {"title": "Avoiding imposters and delinquents: Adversarial crowdsourcing and peer prediction", "author": ["J. Steinhardt", "G. Valiant", "M. Charikar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Steinhardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Steinhardt et al\\.", "year": 2016}, {"title": "Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones", "author": ["J.F. Sturm"], "venue": "Optimization Methods and Software,", "citeRegEx": "Sturm.,? \\Q1999\\E", "shortCiteRegEx": "Sturm.", "year": 1999}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Stealing machine learning models via prediction APIs", "author": ["F. Tram\u00e8r", "F. Zhang", "A. Juels", "M.K. Reiter", "T. Ristenpart"], "venue": "In USENIX Security,", "citeRegEx": "Tram\u00e8r et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tram\u00e8r et al\\.", "year": 2016}, {"title": "How much spam can you take? An analysis of crowdsourcing results to increase accuracy", "author": ["J. Vuurens", "A.P. de Vries", "C. Eickhoff"], "venue": "ACM SIGIR Workshop on Crowdsourcing for Information Retrieval,", "citeRegEx": "Vuurens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vuurens et al\\.", "year": 2011}, {"title": "Combating Attacks and Abuse in Large Online Communities", "author": ["G. Wang"], "venue": "PhD thesis, University of California Santa Barbara,", "citeRegEx": "Wang.,? \\Q2016\\E", "shortCiteRegEx": "Wang.", "year": 2016}, {"title": "Is feature selection secure against training data poisoning", "author": ["H. Xiao", "B. Biggio", "G. Brown", "G. Fumera", "C. Eckert", "F. Roli"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Xiao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Support vector machines under adversarial label contamination", "author": ["H. Xiao", "B. Biggio", "B. Nelson", "C. Eckert", "F. Roli"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "Generative poisoning attack method against neural networks", "author": ["C. Yang", "Q. Wu", "H. Li", "Y. Chen"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Modeling adversarial learning as nested Stackelberg games", "author": ["Y. Zhou", "M. Kantarcioglu"], "venue": "In Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Zhou and Kantarcioglu.,? \\Q2016\\E", "shortCiteRegEx": "Zhou and Kantarcioglu.", "year": 2016}, {"title": "Let `(\u03b8;x, y) be any margin-based loss: `(\u03b8;x, y) = \u03c6(y\u3008\u03b8, x\u3009), where \u03c6 is 1-Lipschitz. Then the bound (14) holds with probability 1 \u2212 \u03b4, for E(N", "author": ["N . More precisely", "Kakade"], "venue": "Kakade et al", "citeRegEx": "precisely and Kakade,? \\Q2009\\E", "shortCiteRegEx": "precisely and Kakade", "year": 2009}], "referenceMentions": [{"referenceID": 34, "context": "(2012) and later by a number of others (Xiao et al., 2012; 2015b; Newell et al., 2014; Mei and Zhu, 2015b; Burkard and Lagesse, 2017; Koh and Liang, 2017).", "startOffset": 39, "endOffset": 154}, {"referenceID": 10, "context": "(2012) and later by a number of others (Xiao et al., 2012; 2015b; Newell et al., 2014; Mei and Zhu, 2015b; Burkard and Lagesse, 2017; Koh and Liang, 2017).", "startOffset": 39, "endOffset": 154}, {"referenceID": 21, "context": "(2012) and later by a number of others (Xiao et al., 2012; 2015b; Newell et al., 2014; Mei and Zhu, 2015b; Burkard and Lagesse, 2017; Koh and Liang, 2017).", "startOffset": 39, "endOffset": 154}, {"referenceID": 24, "context": "Moreover, while some defenses have been proposed against specific attacks (Laishram and Phoha, 2016), few have been stress-tested against a determined attacker.", "startOffset": 74, "endOffset": 100}, {"referenceID": 4, "context": "The focus of the present work is on data poisoning attacks against classification algorithms, first studied by Biggio et al. (2012) and later by a number of others (Xiao et al.", "startOffset": 111, "endOffset": 132}, {"referenceID": 4, "context": "While previous work showed successful attacks on the MNIST-1-7 (Biggio et al., 2012) and Dogfish (Koh and Liang, \u2217Equal contribution.", "startOffset": 63, "endOffset": 84}, {"referenceID": 2, "context": "We consider the causative attack model (Barreno et al., 2010), which consists of a game between two players: the defender (who seeks to learn a model \u03b8), and the attacker (who wants the learner to learn a bad model).", "startOffset": 39, "endOffset": 61}, {"referenceID": 45, "context": ", 2014a); moreover, a determined attacker can often reverse-engineer necessary system details (Tram\u00e8r et al., 2016).", "startOffset": 94, "endOffset": 115}, {"referenceID": 2, "context": "Attacks that attempt to increase the overall test loss L(\u03b8\u0302), known as indiscriminate availability attacks (Barreno et al., 2010), can be thought of as a denial-of-service attack.", "startOffset": 107, "endOffset": 129}, {"referenceID": 28, "context": "A defender who trains na\u00efvely on the full (clean + poisoned) data Dc \u222a Dp is doomed to failure, as even a single poisoned point can in some cases arbitrarily change the model (Liu and Zhu, 2016; Park et al., 2017).", "startOffset": 175, "endOffset": 213}, {"referenceID": 39, "context": "A defender who trains na\u00efvely on the full (clean + poisoned) data Dc \u222a Dp is doomed to failure, as even a single poisoned point can in some cases arbitrarily change the model (Liu and Zhu, 2016; Park et al., 2017).", "startOffset": 175, "endOffset": 213}, {"referenceID": 34, "context": "One example for text classification is letting F be documents that contain only licensed words (Newell et al., 2014).", "startOffset": 95, "endOffset": 116}, {"referenceID": 50, "context": "averaging (Xiao, 2010), which is an alternative to gradient descent that often converges faster for norm-constrained optimization problems.", "startOffset": 10, "endOffset": 22}, {"referenceID": 16, "context": "We first study two image datasets: MNIST-1-7, and the Dogfish dataset used by Koh and Liang (2017). For MNIST-1-7, following Biggio et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 4, "context": "For MNIST-1-7, following Biggio et al. (2012), we considered binary classification between the digits", "startOffset": 25, "endOffset": 46}, {"referenceID": 21, "context": "For Dogfish, which is a binary classification task, we used the same neural net features as in Koh and Liang (2017), so that each of the n = 1800 training images is represented by a 2048-dimensional vector.", "startOffset": 95, "endOffset": 116}, {"referenceID": 15, "context": "(9) The results of Algorithm 1 are given in Figures 2a and 2b; here and elsewhere, we used a combination of CVXPY (Diamond and Boyd, 2016), YALMIP (L\u00f6fberg, 2004), SeDuMi (Sturm, 1999), and Gurobi (Gurobi Optimization, Inc.", "startOffset": 114, "endOffset": 138}, {"referenceID": 29, "context": "(9) The results of Algorithm 1 are given in Figures 2a and 2b; here and elsewhere, we used a combination of CVXPY (Diamond and Boyd, 2016), YALMIP (L\u00f6fberg, 2004), SeDuMi (Sturm, 1999), and Gurobi (Gurobi Optimization, Inc.", "startOffset": 147, "endOffset": 162}, {"referenceID": 43, "context": "(9) The results of Algorithm 1 are given in Figures 2a and 2b; here and elsewhere, we used a combination of CVXPY (Diamond and Boyd, 2016), YALMIP (L\u00f6fberg, 2004), SeDuMi (Sturm, 1999), and Gurobi (Gurobi Optimization, Inc.", "startOffset": 171, "endOffset": 184}, {"referenceID": 4, "context": "We compare our attack to two baselines in Figure 2c \u2014 the gradient descent method employed by Biggio et al. (2012) and Mei and Zhu (2015b), and a simple baseline that inserts copies of points from Dc with the opposite label (subject to the flipped points lying in F ).", "startOffset": 94, "endOffset": 115}, {"referenceID": 4, "context": "We compare our attack to two baselines in Figure 2c \u2014 the gradient descent method employed by Biggio et al. (2012) and Mei and Zhu (2015b), and a simple baseline that inserts copies of points from Dc with the opposite label (subject to the flipped points lying in F ).", "startOffset": 94, "endOffset": 139}, {"referenceID": 34, "context": "Beyond the the sphere and slab constraints, a valid attack on text data must satisfy additional integrity constraints (Newell et al., 2014) \u2014 if we encode input text t by a vector x = \u03c6(t) of indicator features, then each entry of \u03c6(t) is a non-negative integer.", "startOffset": 118, "endOffset": 139}, {"referenceID": 31, "context": "\u2217Though Mei and Zhu (2015b) state that their cost is convex, they communicated to us that this is incorrect.", "startOffset": 8, "endOffset": 28}, {"referenceID": 32, "context": "We ran both the upper bound relaxation and the IQP solver on two text datasets, the Enron spam corpus (Metsis et al., 2006) and the IMDB sentiment corpus (Maas et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 30, "context": ", 2006) and the IMDB sentiment corpus (Maas et al., 2011).", "startOffset": 38, "endOffset": 57}, {"referenceID": 2, "context": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al.", "startOffset": 169, "endOffset": 191}, {"referenceID": 2, "context": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al. (2014a), Papernot et al.", "startOffset": 169, "endOffset": 214}, {"referenceID": 2, "context": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al. (2014a), Papernot et al. (2016b), and Gardiner and Nagaraja (2016) for some recent surveys.", "startOffset": 169, "endOffset": 239}, {"referenceID": 2, "context": "Due to their increased use in security-critical settings such as malware detection, there has been an explosion of work on the security of machine learning systems; see Barreno et al. (2010), Biggio et al. (2014a), Papernot et al. (2016b), and Gardiner and Nagaraja (2016) for some recent surveys.", "startOffset": 169, "endOffset": 273}, {"referenceID": 5, "context": ", 2015a), clustering (Biggio et al., 2013; 2014c), PCA (Rubinstein et al.", "startOffset": 21, "endOffset": 49}, {"referenceID": 40, "context": ", 2013; 2014c), PCA (Rubinstein et al., 2009), topic modeling (Mei and Zhu, 2015a), collaborative filtering (Li et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 26, "context": ", 2009), topic modeling (Mei and Zhu, 2015a), collaborative filtering (Li et al., 2016), neural networks (Yang et al.", "startOffset": 70, "endOffset": 87}, {"referenceID": 51, "context": ", 2016), neural networks (Yang et al., 2017), and other models (Mozaffari-Kermani et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 46, "context": ", 2017), and other models (Mozaffari-Kermani et al., 2015; Vuurens et al., 2011; Wang, 2016).", "startOffset": 26, "endOffset": 92}, {"referenceID": 47, "context": ", 2017), and other models (Mozaffari-Kermani et al., 2015; Vuurens et al., 2011; Wang, 2016).", "startOffset": 26, "endOffset": 92}, {"referenceID": 35, "context": "There have also been a number of demonstrated vulnerabilities in deployed systems (Newsome et al., 2006; Laskov and \u0160rndic\u0300, 2014; Biggio et al., 2014b).", "startOffset": 82, "endOffset": 152}, {"referenceID": 25, "context": "There have also been a number of demonstrated vulnerabilities in deployed systems (Newsome et al., 2006; Laskov and \u0160rndic\u0300, 2014; Biggio et al., 2014b).", "startOffset": 82, "endOffset": 152}, {"referenceID": 44, "context": "A striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a).", "startOffset": 186, "endOffset": 301}, {"referenceID": 18, "context": "A striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a).", "startOffset": 186, "endOffset": 301}, {"referenceID": 11, "context": "A striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a).", "startOffset": 186, "endOffset": 301}, {"referenceID": 22, "context": "A striking recent security vulnerability discovered in machine learning systems is adversarial test images that can fool image classifiers despite being imperceptible from normal images (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini et al., 2016; Kurakin et al., 2016; Papernot et al., 2016a).", "startOffset": 186, "endOffset": 301}, {"referenceID": 18, "context": "Adversarial training (Goodfellow et al., 2015) and generative adversarial networks (Goodfellow et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 17, "context": ", 2015) and generative adversarial networks (Goodfellow et al., 2014) are similarly focused on test time; they both improve test performance by altering the training objective.", "startOffset": 44, "endOffset": 69}, {"referenceID": 27, "context": "Recent adversarial attacks on reinforcement learners (Huang et al., 2017; Behzadan and Munir, 2017; Lin et al., 2017) blend train and test vulnerabilities.", "startOffset": 53, "endOffset": 117}, {"referenceID": 14, "context": "Robust algorithms have been exhibited for mean and covariance estimation and clustering (Diakonikolas et al., 2016; Lai et al., 2016; Charikar et al., 2017), classification (Klivans et al.", "startOffset": 88, "endOffset": 156}, {"referenceID": 23, "context": "Robust algorithms have been exhibited for mean and covariance estimation and clustering (Diakonikolas et al., 2016; Lai et al., 2016; Charikar et al., 2017), classification (Klivans et al.", "startOffset": 88, "endOffset": 156}, {"referenceID": 12, "context": "Robust algorithms have been exhibited for mean and covariance estimation and clustering (Diakonikolas et al., 2016; Lai et al., 2016; Charikar et al., 2017), classification (Klivans et al.", "startOffset": 88, "endOffset": 156}, {"referenceID": 20, "context": ", 2017), classification (Klivans et al., 2009; Awasthi et al., 2014), regression (Nasrabadi et al.", "startOffset": 24, "endOffset": 68}, {"referenceID": 0, "context": ", 2017), classification (Klivans et al., 2009; Awasthi et al., 2014), regression (Nasrabadi et al.", "startOffset": 24, "endOffset": 68}, {"referenceID": 13, "context": ", 2014), regression (Nasrabadi et al., 2011; Nguyen and Tran, 2013; Chen et al., 2013; Bhatia et al., 2015) and crowdsourced data aggregation (Steinhardt et al.", "startOffset": 20, "endOffset": 107}, {"referenceID": 42, "context": ", 2015) and crowdsourced data aggregation (Steinhardt et al., 2016).", "startOffset": 42, "endOffset": 67}, {"referenceID": 1, "context": "Separately, the bound L(\u03b8\u0302) / M was useful because M admits the natural minimax formulation (5), but the worst-case L(\u03b8\u0302) can be expressed directly as a bilevel optimization problem (Mei and Zhu, 2015b), which is intractable in general but admits a number of heuristics (Bard, 1999).", "startOffset": 270, "endOffset": 282}, {"referenceID": 9, "context": "Bilevel optimization has been considered in the related setting of Stackelberg games (Br\u00fcckner and Scheffer, 2011; Br\u00fcckner et al., 2012; Zhou and Kantarcioglu, 2016), and is natural to apply here as well.", "startOffset": 85, "endOffset": 166}, {"referenceID": 52, "context": "Bilevel optimization has been considered in the related setting of Stackelberg games (Br\u00fcckner and Scheffer, 2011; Br\u00fcckner et al., 2012; Zhou and Kantarcioglu, 2016), and is natural to apply here as well.", "startOffset": 85, "endOffset": 166}], "year": 2017, "abstractText": "Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our bound comes paired with a candidate attack that nearly realizes the bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.", "creator": "LaTeX with hyperref package"}}}