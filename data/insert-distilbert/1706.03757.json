{"id": "1706.03757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Semantic Entity Retrieval Toolkit", "abstract": "unsupervised learning of low - single dimensional, semantic representations of words and entities has recently gained intense attention. in this paper we describe precisely the functional semantic entity retrieval toolkit ( sert ) that provides implementations of our previously published entity tag representation models. the toolkit provides a unified interface to different representation learning integration algorithms, fine - grained structures parsing configuration structures and can be used transparently with gpus. in addition, users can easily modify existing models or implement their own models in the framework. after model training, sert can be used to rank entities according to a textual query and extract the learned entity / word representation for future use in downstream analysis algorithms, such as clustering or recommendation.", "histories": [["v1", "Mon, 12 Jun 2017 17:51:05 GMT  (916kb,D)", "http://arxiv.org/abs/1706.03757v1", "Submitted to Neu-IR '17 SIGIR Workshop on Neural Information Retrieval"], ["v2", "Mon, 17 Jul 2017 14:30:49 GMT  (920kb,D)", "http://arxiv.org/abs/1706.03757v2", "SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17). 2017"]], "COMMENTS": "Submitted to Neu-IR '17 SIGIR Workshop on Neural Information Retrieval", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["christophe van gysel", "maarten de rijke", "evangelos kanoulas"], "accepted": false, "id": "1706.03757"}, "pdf": {"name": "1706.03757.pdf", "metadata": {"source": "META", "title": "Semantic Entity Retrieval Toolkit", "authors": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "emails": ["cvangysel@uva.nl", "derijke@uva.nl", "e.kanoulas@uva.nl"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Information systems \u2192Content analysis and feature selection;\nKEYWORDS Neural information retrieval; Entity retrieval; Toolkit"}, {"heading": "1 INTRODUCTION", "text": "e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [10] and product search [8]. Representations are learned from a document collection and domain-speci c associations between documents and entities. Expert nding is the task of nding the right person with the appropriate skills or knowledge [1] and an association indicates document authorship (e.g., academic papers) or involvement in a project (e.g., annual progress reports). In the case of product search, an associated document is a product description or review [8].\nIn this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [8, 10]. Beyond a uni ed interface that combines di erent models, the toolkit allows for ne-grained parsing con guration and GPU-based training through integration with eano [3, 6]. Users can easily extend existing models or implement their own models within the uni ed framework. A er model training, SERT can compute matching scores between an entity and a piece of text (e.g., a query). is matching score can then be used for ranking entities, or as a feature in a downstream machine learning system, such as the learning to rank component of a search engine. In addition, the learned representations can be\n\u2217 e toolkit is licensed under the permissive MIT open-source license and can be found at h ps://github.com/cvangysel/SERT.\nextracted and used as feature vectors in entity clustering or recommendation tasks [9]. e toolkit is licensed under the permissive MIT open-source license; see the footnote on the rst page."}, {"heading": "2 THE TOOLKIT", "text": "SERT is organized as a pipeline of utilities as depicted in Figure 1. First, a collection of documents and entity associations is processed and packaged using a numerical format (\u00a72.1). Low-dimensional representations of words and entities are then learned (\u00a72.2) and a erwards the representations can be used to make inferences (\u00a72.3)."}, {"heading": "2.1 Collection parsing and preparation", "text": "To begin, SERT constructs a vocabulary that will be used to tokenize the document collection. Non-signi cant words that are too frequent (e.g., stopwords), noisy (e.g., single characters) and rare words are ltered out. Words that do not occur in the dictionary are ignored. A erwards, word sequences are extracted from the documents and stored together with the associated entities in the numerical format provided by NumPy [7]. Word sequences can be extracted consecutively or a stride can be speci ed to extract nonconsecutive windows. In addition, a hierarchy of word sequence extractors can be applied to extract skip-grams, i.e., word sequences where a number of tokens are skipped a er selecting a token [4]. To support short documents, a special-purpose padding token can be used to ll up word sequences that are longer than a particular document.\nA er word sequence extraction, a weight can be assigned to each word sequence/entity pair that can be used to re-weight the training objective. For example, in the case of expert nding [10], this weight is the reciprocal of the document length of the document where the sequence was extracted from. is avoids a bias in the objective towards long documents. An alternative option that exists within the toolkit is to resample word sequence/entity pairs such that every entity is associated with the same number of word sequences, as used for product search [8].\nar X\niv :1\n70 6.\n03 75\n7v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\nclass ExampleModel(VectorSpaceLanguageModelBase):\ndef __init__(self, *args, **kwargs): super(ExampleModel, self).__init__(\n*args, **kwargs)\n# Define model architecture. input_layer = InputLayer(\nshape=(self.batch_size, self.window_size))\n...\ndef loss_fn(pred, actual, _): # Compute symbolic loss between # predicted/actual entities.\n# The framework deals with underlying boilerplate. self._finalize(loss_fn, ....)\ndef get_representations(self): # Returns the representations and parameters # to be extracted.\nCode snippet 1: Illustrative example of the SERT model interface. e full interface supports more functionality omitted here for brevity. Users can de ne a symbolic graph of computation using the eano library [6] in combination with Lasagne [3]."}, {"heading": "2.2 Representation learning", "text": "A er the collection has been processed and packaged in a machinefriendly format, representations of words and entities can be learned. e toolkit includes implementations of state-of-the-art representation learning models that were applied to expert nding [10] and product search [8]. Users of the toolkit can use these implementations to learn representations out-of-the-box or adapt the algorithms to their needs. In addition, users can implement their own models by extending an interface provided by the framework. Code snippet 1 shows an example of a model implemented in the SERT toolkit where users can de ne a symbolic cost function that will be optimized using eano [6]. Due to the componentized organization of the toolkit (Figure 1), modeling and text processing are separated from each other. Consequently, researchers can focus on modeling and representation learning only. In addition, any improvements to the collection processing (\u00a72.1) collectively bene ts all models implemented in SERT."}, {"heading": "2.3 Entity ranking & other uses of the representations", "text": "Once a model has been trained, SERT can be used to rank entities w.r.t. a textual query. e concrete implementation used to rank entities depends on the model that was trained. In the most generic case, a matching score is computed for every entity and entities are ranked in decreasing order of his score. However, in the special case when the model is interpreted as a metric vector space [2, 8], SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5]. A er ranking, SERT outputs the entity rankings as a TREC-compatible le that can be used as input to the trec eval1 evaluation utility.\n1h ps://github.com/usnistgov/trec eval\nApart from entity ranking, the learned representations and modelspeci c parameters can be extracted conveniently from the models through the interface (get representations in Snippet 1) and used for down-stream tasks such as clustering, recommendation and determining entity importance as shown in [9]."}, {"heading": "3 CONCLUSIONS", "text": "In this paper we described the Semantic Entity Retrieval Toolkit, a toolkit that learns latent representations of words and entities. e toolkit contains implementations of state-of-the-art entity representations algorithms [8, 10] and consists of three components: text processing, representation learning and inference. Users of the toolkit can easily make changes to existing model implementations or contribute their own models by extending an interface provided by the SERT framework.\nFuture work includes integration with Pyndri [11] such that document collections indexed with Indri can transparently be used to train entity representations. In addition, integration with machine learning frameworks besides eano, such as TensorFlow and PyTorch, will make it easier to integrate existing models into SERT.\nAcknowledgments. is research was supported by Ahold, Amsterdam Data Science, Blendle, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the ESF Research Network Program ELIAS, the Google Faculty Research Award scheme, the Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shi s project, the Microso Research Ph.D. program, the Netherlands eScience Center under project number 027.012.105, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scienti c Research (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, 652.002.001, 612.- 001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors."}], "references": [{"title": "O\u0082 the Beaten Path: Let\u2019s Replace Term-Based Retrieval with k-NN Search", "author": ["Leonid Boytsov", "David Novak", "Yury Malkov", "Nyberg Eric"], "venue": "In CIKM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A closer look at skip-gram modelling", "author": ["David Guthrie", "Ben Allison", "Wei Liu", "Louise Guthrie", "Yorick Wilks"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "An empirical comparison of exact nearest neighbour algorithms", "author": ["Ashraf M Kibriya", "Eibe Frank"], "venue": "In ECMLPKDD. Springer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "\u008ce NumPy Array: A Structure for E\u0081cient Numerical Computation", "author": ["St\u00e9fan van der Walt", "S. Chris Colbert", "Ga\u00ebl Varoquaux"], "venue": "Computing in Science & Engineering 13,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Learning Latent Vector Spaces for Product Search", "author": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "venue": "In CIKM. ACM", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Structural Regularities in Expert Vector Spaces", "author": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "venue": "In ICTIR. ACM", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Unsupervised, E\u0081cient and Semantic Expertise Retrieval", "author": ["Christophe Van Gysel", "Maarten de Rijke", "Marcel Worring"], "venue": "In WWW. ACM", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Pyndri: a Python Interface to the Indri Search Engine", "author": ["Christophe Van Gysel", "Evangelos Kanoulas", "Maarten de Rijke"], "venue": "In ECIR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}], "referenceMentions": [{"referenceID": 6, "context": "\u008ce unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a\u008aention for the entity-oriented tasks of expert \u0080nding [10] and product search [8].", "startOffset": 168, "endOffset": 172}, {"referenceID": 4, "context": "\u008ce unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a\u008aention for the entity-oriented tasks of expert \u0080nding [10] and product search [8].", "startOffset": 192, "endOffset": 195}, {"referenceID": 4, "context": "In the case of product search, an associated document is a product description or review [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [8, 10].", "startOffset": 158, "endOffset": 165}, {"referenceID": 6, "context": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [8, 10].", "startOffset": 158, "endOffset": 165}, {"referenceID": 5, "context": "extracted and used as feature vectors in entity clustering or recommendation tasks [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "A\u0089erwards, word sequences are extracted from the documents and stored together with the associated entities in the numerical format provided by NumPy [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": ", word sequences where a number of tokens are skipped a\u0089er selecting a token [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "For example, in the case of expert \u0080nding [10], this weight is the reciprocal of the document length of the document where the sequence was extracted from.", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "An alternative option that exists within the toolkit is to resample word sequence/entity pairs such that every entity is associated with the same number of word sequences, as used for product search [8].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "\u008ce toolkit includes implementations of state-of-the-art representation learning models that were applied to expert \u0080nding [10] and product search [8].", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "\u008ce toolkit includes implementations of state-of-the-art representation learning models that were applied to expert \u0080nding [10] and product search [8].", "startOffset": 146, "endOffset": 149}, {"referenceID": 0, "context": "However, in the special case when the model is interpreted as a metric vector space [2, 8], SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5].", "startOffset": 84, "endOffset": 90}, {"referenceID": 4, "context": "However, in the special case when the model is interpreted as a metric vector space [2, 8], SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5].", "startOffset": 84, "endOffset": 90}, {"referenceID": 2, "context": "However, in the special case when the model is interpreted as a metric vector space [2, 8], SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5].", "startOffset": 201, "endOffset": 204}, {"referenceID": 5, "context": "com/usnistgov/trec eval Apart from entity ranking, the learned representations and modelspeci\u0080c parameters can be extracted conveniently from the models through the interface (get representations in Snippet 1) and used for down-stream tasks such as clustering, recommendation and determining entity importance as shown in [9].", "startOffset": 322, "endOffset": 325}, {"referenceID": 4, "context": "\u008ce toolkit contains implementations of state-of-the-art entity representations algorithms [8, 10] and consists of three components: text processing, representation learning and inference.", "startOffset": 90, "endOffset": 97}, {"referenceID": 6, "context": "\u008ce toolkit contains implementations of state-of-the-art entity representations algorithms [8, 10] and consists of three components: text processing, representation learning and inference.", "startOffset": 90, "endOffset": 97}, {"referenceID": 7, "context": "Future work includes integration with Pyndri [11] such that document collections indexed with Indri can transparently be used to train entity representations.", "startOffset": 45, "endOffset": 49}], "year": 2017, "abstractText": "Unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a\u008aention. In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models. \u008ce toolkit provides a uni\u0080ed interface to di\u0082erent representation learning algorithms, \u0080ne-grained parsing con\u0080guration and can be used transparently with GPUs. In addition, users can easily modify existing models or implement their own models in the framework. A\u0089er model training, SERT can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms, such as clustering or recommendation.", "creator": "LaTeX with hyperref package"}}}