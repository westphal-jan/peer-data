{"id": "1706.03847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Recurrent Neural Networks with Top-k Gains for Session-based Recommendations", "abstract": "performance rnns have been shown to be excellent models performing for sequential data and in particular for session - based user behavior. the use of rnns provides impressive performance benefits over classical methods in session - based recommendations. in this work we introduce a novel problem ranking loss function tailored for rnns in recommendation scoring settings. the better performance of such loss over other alternatives, along with further tricks and matching improvements described in this work, allow to achieve an overall improvement speed of up to 35 % in terms of mrr and output recall @ 20 over previous session - based rnn solutions and up to 51 % over classical data collaborative filtering treatment approaches. unlike data augmentation - based improvements, our method does not increase training times significantly.", "histories": [["v1", "Mon, 12 Jun 2017 20:49:23 GMT  (288kb,D)", "http://arxiv.org/abs/1706.03847v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bal\\'azs hidasi", "alexandros karatzoglou"], "accepted": false, "id": "1706.03847"}, "pdf": {"name": "1706.03847.pdf", "metadata": {"source": "META", "title": "Recurrent Neural Networks with Top-k Gains for Session-based Recommendations ", "authors": ["Bal\u00e1zs Hidasi", "Alexandros Karatzoglou"], "emails": ["balazs.hidasi@gravityrd.com", "alexk@tid.es", "Recall@20"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Information systems \u2192 Collaborative ltering; \u2022 Computing methodologies \u2192 Neural networks;\nKEYWORDS recurrent neural networks, loss function, ranking, session-based recommendation"}, {"heading": "1 INTRODUCTION", "text": "Session-based recommendation is a very common recommendation problem that is encountered in many domains such as e-commerce, classi ed sites, music and video recommendation. In the sessionbased setting, past user history logs are typically not available (either because the user is new or not logged in or not tracked) and recommender systems have to rely only on the actions of the user in the current sessions to provide accurate recommendations. Until recently many of these recommendations tasks were tackled mainly using relatively simple methods such as item-based collaborative ltering [16] or content-based methods. Recurrent Neural Networks (RNNs) have emerged from the deep learning literature as powerful methods for modeling sequential data. These models have been successfully applied in speech recognition, translation, time series forecasting and signal processing. In recommender systems RNNs have been recently applied to the session-based recommendation setting with impressive results [6].\nThe advantage of RNNs over traditional similarity-based methods for recommendation is that they can e ectively model the whole session of user interactions (clicks, views, etc.). By modeling the whole session RNNs can in e ect learn the \u2018theme\u2019 of the session and thus provide recommendations with increased accuracy (between 20%-30%) over traditional methods.\nRNNs in session-based recommendation have been adapted to the task of recommendation. One of the main objectives in recommendation is to rank items by user preference; i.e. the exact ranking or scoring of items in the tail of the item list (items that the user will not like) is not that important, but it is very important to rank correctly the items that the user will like at the top of the list ( rst 5, 10 or 20 positions). To achieve this with machine learning one has to typically utilize learning to rank techniques(see e.g. [3]) and in particular ranking objectives and loss functions. The current session-based RNN approaches use ranking loss functions and, in particular, pairwise ranking loss functions. As in most deep learning approaches the choice of a good ranking loss can have a very signi cant in uence on performance. Since deep learning methods need to propagate gradients over several layers and in the case of RNNs \u2019back in time\u2019 over previous steps, to optimize the model parameters, the quality of these gradients originating from the loss function in uences the quality of the optimization and the model parameters. Moreover the nature of the recommendation task, which typically entails large output spaces (due to large number of items), poses unique challenges that have to be taken into account as well when designing a proper ranking loss function. We will see that the way this large output space issue is tackled is very crucial in achieving good performance.\nIn this work we analyze ranking loss functions used in RNNs for session-based recommendations, this analysis leads to a new set of ranking loss functions that increase the performance of the RNN up to 30% over previous commonly used losses without incurring in signi cant computational overheads. We essentially devise a new class of loss functions that combines learnings from the deep learning and the learning to rank literature. Experimental results on several datasets coming from industry validate these impressive improvements, in terms of Mean Reciprocal Rank (MRR) and Recall@20. With these improvements the di erence between RNNs and conventional memory-based collaborative ltering jumps to 51% in terms of MRR and Recall@20 demonstrating the potential that deep learning methods bring to the area of Recommender Systems."}, {"heading": "1.1 Related Work", "text": "One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user pro le is the item-to-item recommendation approach [12, 16]. In this setting, an item-to-item similarity matrix is precomputed from the available session data, that is items that are often clicked together in sessions are deemed to be similar. This similarity matrix is then simply used during the session to recommend the most similar items to the one the user has currently clicked.\nar X\niv :1\n70 6.\n03 84\n7v 1\n[ cs\n.L G\n] 1\n2 Ju\nn 20\n17\nLong Short-Term Memory (LSTM) [9] networks are a type of RNNs that have been shown to solve the optimization issues the plague vanilla-type RNNs. LSTM\u2019s include additional gates that regulate when and how much of the input to take into account and when to reset the hidden state. A slightly simpli ed version of LSTM \u2013 that still maintains all their properties \u2013 are Gated Recurrent Units (GRUs) [4], which we use in this work. Recurrent Neural Networks have been used with success in the area of session-based recommendations; [6] proposed a Recurrent Neural Network with a pairwise ranking loss for this task, [18] proposed data augmentation techniques to improve the performance of the RNN for sessionbased recommendations; these techniques have though the side e ect of increasing training times as a single session is split into several sub-sessions for training. Session-based RNNs have been augmented [7] with feature information, such as text and images from the clicked/consumed items, showing improved performance over the plain models. RNNs have also been used in more standard user-item collaborative ltering settings where the aim is to model the evolution of the user and items factors [20],[5] where the results are less striking, with the proposed methods barely outperforming standard matrix factorization methods. This is to be expected as there is no strong evidence on major user taste evolution in a single domain in the timeframes of the available datasets and sequential modeling of items that are not \u2019consumed\u2019 in sessions such as movies might not bring major bene ts.\nAnother area touched upon in this work are loss functions tailored to recommender systems requirements. This typically means ranking loss functions. In this area there has been work particularly in the context of matrix factorization techniques. One of the rst learning to rank techniques for collaborative ltering was introduced in [19]. Essentially a listwise loss function was introduced along with an alternating bundle method for optimization of the factors. Further ranking loss function for collaborative ltering were introduced in [17] [14] and [11]. Note that the fact that these loss functions work well in matrix factorization does not guarantee in any way that they are an optimal choice for RNNs as backpropagation requirements are stronger than those posed by simple SGD. We will in fact see that BPR, a popular choice of loss function, needs to be signi cantly modi ed to extract optimal results in the case of RNNs for session-based recommendations. Another work related to sampling large output spaces in deep networks for e cient loss computations for language models is the \u2019blackout\u2019 method [10], where essentially a sampling procedure similar to the one used in [6] is applied in order to e ciently compute the categorical cross-entropy loss."}, {"heading": "2 SAMPLING THE OUTPUT", "text": "In the remainder of the paper we will refer to the RNN algorithm implemented in [6] as GRU4Rec, the name of the implementation published by the authors on github 1. In this section we revisit how GRU4Rec samples negative feedback on the output and discuss its importance. We extend this sampling with an option for additional samples and argue that it is crucial for increasing recommendation accuracy.\n1https://github.com/hidasib/GRU4Rec\nIn each training step, GRU4Rec takes the item of the current event \u2013 represented by a one-hot vector \u2013 on its input. The output of the network is a set of scores over the items, corresponding to their likelihood of being the next item in the session. The training iterates through all events in sequence. The complexity of the training is O(NE (H2 +HNO )) where NE is the number of training events, H is the number of hidden units and NO is the number of outputs, for which scores are computed. Computing scores for all items is very impractical, since it makes the network unscalable2. Therefore GRU4Rec uses a sampling mechanism and during training computes the scores for a subset of the items only.\nInstead of making a forward and backward pass on one training example only then moving to the next, the network is fed with a bundle of examples and is trained on the mean gradient. This common practice is called mini-batch training and has several bene ts, e.g. utilizing the parallelization capabilities of current hardware better, thus training faster, and producing more stable gradients than stochastic gradient training and thus converging faster. GRU4Rec introduced mini-batch based sampling [6]. For each example in the mini-batch, the other examples of the same mini-batch serve as negative examples (see Figure 1).3 This method is practical from the implementation point of view and can be also implemented e ciently for GPUs.\nThe network can be trained with one of three di erent listwise ranking loss functions (see Section 3). Each of the loss functions requires a score for the target item (i.e. for the item which was the actual next item) and score(s) for at least one negative sample (i.e. item other than the target). One property of ranking losses is that they can only learn if the score of the target item does not exceed that of the negative samples by a large margin, otherwise the items are already in the right order, so there is nothing to be learned. Therefore, when utilizing a sampling procedure, it is crucial that high scoring items make it among the negative samples. Whether an item has a high score, depends on the context (item sequence) the scores are actually computed for. Popular items generally score high in many situations, making popularitybased sampling a good sampling strategy. Mini-batch sampling is basically a form of popularity-based sampling, since the training iterates through all events, thus the probability of an item acting as a negative sample is proportional to its support. The problem with 2While it can still result in an acceptable training time for smaller datasets, especially if the number of items is only a few tens of thousand, algorithms scaling with the product of the number of events and items can not scale up for larger datasets. 3e.g.: Assume a mini-batch of 32 examples, with one desired output (target) for each example. Scores are computed for all 32 items for each of the 32 examples resulting in 32 \u00d7 32 = 1024 scores. Thus we have 31 scores of negative examples for each of the targets.\npopularity-based sampling is that learning can slow down after the algorithm learns to (generally) rank target items above popular ones, and thus can still be inaccurate with ranking long tail high scoring items. On the other hand, uniform sampling slows down learning, due to the high number of low scoring negative samples, but might produce an overall more accurate model if trained inde nitely. In our experience, popularity-based sampling generally produces better results.\nTying sampling to the mini-batches has several practical bene ts, but is too restrictive for three reasons. (1) Mini-batch sizes are generally small, ranging from few tens to few hundreds. If the number of items is large, the small sample size further hinders the chance of including all of the high scoring negative examples. (2) Mini-batch size has direct e ect on the training. E.g. we found that training with smaller mini-batch sizes (30-100) produces more accurate models, but training with larger ones is faster on the GPU due to parallelization. (3) The sampling method is inherently popularity-based, which, while being a generally good strategy, might not be optimal for all datasets.\nTherefore we extend the sampling of GRU4Rec with additional samples. We sample NA items which are shared by the examples of the mini-batch, i.e. the same samples are used for each example4. These additional samples are used along with the NB \u2212 1 samples coming from the mini-batch (popularity) sampling. Additional samples can be sampled in any way, we chose to sample proportional to supp\u03b1i , where suppi is the support of the item and \u03b1 is the parameter of the sampling. \u03b1 = 0 and \u03b1 = 1 gives uniform and popularity-based sampling respectively.\nAdding more samples naturally increases the complexity, since NO increases from NB to NA + NB . However, the computations are easily parallelizable, thus there is no actual increase in the training time on modern GPUs up to a certain sample size (see Section 4.1). The e cient implementation of this sampling however is not trivial. Sampling according to a distribution on GPUs is slow, thus it should be handled by the CPU. The sampled item IDs can be given to the GPU along with the item IDs of the mini-batch. Sampling the distribution takes some time every time a new minibatch is formed, thus GPU execution is frequently interrupted, making GPU utilization low and thus training slow. On the top of that, sampling a few items at once is less e cient than sampling lots of them, even on CPU. Therefore we implemented a cache that pre-samples and stores lots of negative samples. Training uses up these samples and the cache is recomputed once it is empty. We found that pre-sampling 10-100 million item IDs signi cantly improve training speed when compared to using no cache at all."}, {"heading": "3 LOSS FUNCTION DESIGN", "text": "In this section we examine the loss functions implemented in GRU4Rec and identify their weaknesses. We propose two ways to stabilize the numerical instability of the cross-entropy loss, we show how learning with the TOP1 and BPR pairwise losses degrades as we add more samples to the output, and propose a family of loss functions based on pairwise losses that alleviates this problem. We note that, while our aim is to improve GRU4Rec, the loss functions\n4However, the scores of these samples will be still di erent per example, because of the di ering item sequences they are based on.\nproposed in this section can be also used with other models, such as matrix factorization."}, {"heading": "3.1 Categorical cross-entropy", "text": "Categorical cross-entropy measures the distance of a proposed (discrete) probability distribution q from the target distribution p as de ned by (1).\nH (p,q) = \u2212 N\u2211 j=1 pj logqj (1)\nThis loss is often used in machine learning and deep learning in particular for multi-class classi cation problems. Next item recommendation can be interpreted as classi cation, where the class labels are the items in the system and item sequences need to be assigned with the label of the item that follows. In a single-label scenario \u2013 such as next item recommendation \u2013 the target distribution is a one-hot vector over the set of items, with the coordinate corresponding to the target item set to 1. The proposed distribution consists of the scores assigned to the items by the algorithm. The output scores need to be transformed to form a distribution. It is common practice to use the softmax transformation (2), which is a continuous approximation of the max operation. This naturally aligns with the sentiment that the label with the highest score is assigned to the sequence.\nsi = eri\u2211N j=1 e r j (2)\nCross-entropy in itself is a pointwise loss, as it is the sum of independent losses de ned over the coordinates. Combining it with softmax introduces listwise properties into the loss, since the loss now cannot be separated over coordinates. Putting them together we get the following loss function over the scores (assuming that the target item is indexed by i):\nLxe = \u2212 log si = \u2212 log eri\u2211N j=1 e r j (3)\nFixing the instability: One of the losses available in GRU4Rec was cross-entropy with softmax scores. [6] reported slightly better results than with other losses, but deemed the loss to be unstable for a large fraction of the hyperparameter space and thus advised against its use. This instability comes from the limited numerical precision. Assuming that there is a k for which rk ri , si becomes very small and rounded to 0, because of the limited precision. The loss then computes log 0, which is unde ned. Two ways to circumvent this problem are as follow: (a) compute \u2212 log(si + \u03f5), where \u03f5 is a very small value (we use 10\u221224); (b) compute \u2212 log si directly as \u2212ri + log \u2211N j=1 e\nr j . The former introduces some noise, while the latter does not allow the separated use of the transformation and the loss, but both methods stabilize the loss. We did not observe any di erences in the results of the two variants."}, {"heading": "3.2 Ranking losses: TOP1 & BPR", "text": "GRU4Rec o ers two loss functions based on pairwise losses. Pairwise losses compare the score of the target to a negative example (i.e. any item other than the target). The loss is high if the target\u2019s\nscore is higher than that of the negative example. GRU4Rec computes scores for multiple negative samples per each target, and thus the loss function is composed as the average of the individual pairwise losses. This results in a listwise loss function, which is composed of pairwise losses.\nOne of the loss functions is coined TOP1 (4). It is a heuristically put together loss consisting of two parts. The rst part aims to push the target score above the score of the samples, while the second part lowers the score of negative samples towards zero. The latter acts as a regularizer, but instead of constraining the model weights directly, it penalizes high scores on the negative examples. Since all items act as a negative score in one training example or another, it generally pushes the scores down.\nLtop1 = 1 NS NS\u2211 j=1 \u03c3 (r j \u2212 ri ) + \u03c3 (r2j ) (4)\nThe other loss function (5) is based on the popular Bayesian Personalized Ranking (BPR) [15] loss. Here the negative log-probability of the target score exceeding the sample scores is minimized (i.e. the probability of target scores being above sample scores is maximized). The non-continuous P(ri > r j ) is approximated by \u03c3 (ri \u2212 r j ).\nLbpr = \u2212 1 NS NS\u2211 j=1 log\u03c3 (ri \u2212 r j ) (5)\n3.2.1 Vanishing gradients. Taking the average of individual pairwise losses has an undesired side e ect. Examining the gradients for the TOP1 and BPR losses w.r.t. the target score ri , ((6) and (7) respectively) reveals that under certain circumstances gradients vanish and thus learning stops. With pairwise losses, one generally wants to have negative samples with high scores, as those samples produce high gradients. Or intuitively, if the score of the negative sample is already well below that of the target, there is nothing to learn from that negative sample anymore. For this discussion we will denote samples where r j ri irrelevant. For an irrelevant sample \u03c3 (r j \u2212 ri ) in ((6) and 1 \u2212 \u03c3 (ri \u2212 r j ) (7) will be close to zero. Therefore, any irrelevant sample adds basically nothing to the total gradient. Meanwhile the gradient is always discounted by the total number of negative samples. By increasing the number of samples, the number of irrelevant samples increases faster than that of including relevant samples, since the majority of items is irrelevant as a negative sample. This is especially true for non-popularity-based sampling and high sample numbers. Therefore these losses start to vanish as the number of samples increase, which is counterintuitive and hurts the full potential of the algorithm.56\n\u2202Ltop1 \u2202ri = \u2212 1 NS NS\u2211 j=1 \u03c3 (r j \u2212 ri ) ( 1 \u2212 \u03c3 (r j \u2212 ri ) ) (6)\n5Simply removing the discounting factor does not solve this problem, since it is equivalent of multiplying the learning rate by NS . This would destabilize learning due to introducing high variance into the updates. 6For BPR, there is the option of maximizing the sum of individual pairwise probabilities\u2211NS j=1 P (ri > r j ), i.e. minimizing \u2212 log \u2211NS j=1 \u03c3 (ri \u2212 r j ). However, this loss has even worse properties.\n\u2202Lbpr \u2202ri = \u2212 1 NS NS\u2211 j=1 ( 1 \u2212 \u03c3 (ri \u2212 r j ) ) (7)\nNote, that TOP1 is sensitive to relevant examples where r j ri , which is an oversight in the design of the loss. While this is unlikely to happen, it cannot be outruled. For example, when comparing a niche target to a very popular sample \u2013 especially during the early phase of learning \u2013 the target score might be much lower than the sample score.\nWe concentrated on the gradients w.r.t. the target score, but a similar issue can be observed for the gradients on the negative scores. The gradient w.r.t. the score of a negative sample is the gradient of the pairwise loss between the target and the sample divided by the number of negative samples. This means that even if all negative samples would be relevant, their updates would still diminish as their number grows."}, {"heading": "3.3 Ranking-max loss function family", "text": "To overcome the vanishing of gradients as the number of samples increase, we propose a new family of listwise loss functions, based on individual pairwise losses. The idea is to have the target score compared with the most relevant sample score, which is the maximal score amongst the samples. The general structure of the loss is described by (8).\nLpairwise\u2212max ( ri , {r j }NSj=1 ) = Lpairwise(ri ,max\nj r j ) (8)\nThe maximum selection is non-di erentiable and thus cannot be used with gradient descent. Therefore we use the softmax scores to preserve di erentiability. Here, the softmax transformation is only used on the negative examples (i.e. ri is excluded), since we are looking from the maximum score amongst the negative examples. This naturally results in loss functions where each negative sample is taken into account proportional to its likelihood of having the maximal score. Based on this general idea, we now derive the TOP1max and BPR-max loss functions.\nTOP1-max: The TOP1-max loss is fairly straightforward. The regularizing part does not necessarily need to be only applied for the maximal negative score, however we found that this gave the best results, thus kept it this way. The continuous approximation to the maximum selection entails summing over the individual losses weighted by the corresponding softmax scores sj , giving us the TOP1-max loss (9).\nLtop1\u2212max = NS\u2211 j=1 sj ( \u03c3 (r j \u2212 ri ) + \u03c3 (r2j ) ) (9)\nThe gradient of TOP1-max (10) is the softmax weighted average7 of individual pairwise gradients. If r j is much lower than the maximum of negative scores, its weight will be almost zero and more weight will be placed on examples with scores close to the maximum. This solves the issue of vanishing gradients with more samples, because irrelevant samples will be just ignored, while the gradient will point towards the gradient of the relevant samples. Of course, if all samples are irrelevant, the gradient become near 7\u2211 sj = 1\nzero, but this is not a problem, because if the target score is greater than all sample scores, there is nothing to learn. Unfortunately, the sensitivity to large sample scores of TOP1 is still an issue as it is the consequence of the pairwise loss and not the aggregation.\n\u2202Ltop1\u2212max \u2202ri = \u2212 NS\u2211 j=1 sj\u03c3 (r j \u2212 ri ) ( 1 \u2212 \u03c3 (r j \u2212 ri ) ) (10)\nBPR-max: Going back to the probability interpretation of BPR, the goal is to maximize the probability of the target score being higher than the maximal sample score rmax = maxj r j . This can be rewritten using conditional probabilities:\nP(ri > rmax) = NS\u2211 j=1 P(ri > r j |r j = rmax)P(r j = rmax) (11)\nP(ri > r j ) and P(r j = rmax) is approximated by \u03c3 (ri \u2212 r j ) (as in the original BPR loss) and the softmax score sj respectively. We then want to minimize the negative log-probability, which gives us the loss:\nLbpr\u2212max = \u2212 log NS\u2211 j=1 sj\u03c3 (ri \u2212 r j ) (12)\nThe gradient of BPR-max (13) is the weighted average of individual BPR gradients, where the weights are sj\u03c3 (ri\u2212r j ). The relative importance of negative samples j and k is \u03c3 (ri\u2212r j )sj\u03c3 (ri\u2212rk )sk = erj +e\u2212ri +rj +rk erk +e\u2212ri +rj +rk\n, which behaves like softmax weights if ri r j + rk or if both ri and rk are small. Otherwise it is a smoothed softmax. This means that while ri is small, the weights are distributed more evenly, yet clear emphasis will be given to higher sample scores. As ri becomes higher, the focus shifts quickly to the samples with high scores. This is an ideal behaviour.\n\u2202Lbpr\u2212max \u2202ri\n= \u2212 \u2211NS j=1 sj\u03c3 (ri \u2212 r j ) ( 1 \u2212 \u03c3 (ri \u2212 r j ) )\u2211NS j=1 sj\u03c3 (ri \u2212 r j )\n(13)\nThe gradient w.r.t. a negative sample \u2013 with both the BPR-max and TOP1-max \u2013 is proportional to the softmax score of the example, meaning that only the items, near the maximum will be updated. This is bene cial, because if the score of a negative sample is low, it doesn\u2019t need to be updated. If the score of a sample is much higher than that of the others it will be the only one updated and the gradient will coincide with the gradient of the pairwise loss between the target and the sample score. In a more balanced setting the gradient is between the aforementioned gradient and 0. For example the gradient of BPR-max w.r.t. a negative sample\u2019s score is as follows:\n\u2202Lbpr\u2212max \u2202rk = sk \u2212 sk\u03c3 2(ri \u2212 rk )\u2211NS j=1 sj\u03c3 (ri \u2212 r j )\n(14)\nFigure 2 depicts how the gradients of BPR and BPR-max behave given the rank of the target item8. The rank of the target is the number of negative scores exceeding it, e.g. rank 0 means that the target score is higher than all sample scores. Lower rank means that there are fewer negative samples that are relevant. The gure depicts the median negative gradient w.r.t. the target score in two cases, measured on a dataset sample during the 1st and 10th epochs (i.e. beginning and end of the training): (left) no additional samples were used, only the other examples from a mini-batch of size 32; (middle & right) 2048 additional negative samples were added. The rightmost gure focuses on the rst 200 ranks of the gure in the middle. The gradient is slightly higher for BPR when there are more relevant samples (i.e. high ranks). This is natural, since BPR-max focuses on samples closest to the maximum value and ignores other, but still relevant samples. This entails slightly slower learning for BPR-max when the target item is ranked at the end of the list, but the di erence is not really signi cant. On the other hand, the gradient of BPR quickly vanishes as the number of relevant samples decrease (i.e. low ranks). The point of vanishing is relative to the total sample size. With small sample size, BPR\u2019s gradient starts vanishing around rank 5 (the BPR-max does not vanish until rank 0); meanwhile, with 8Similar trends can be observed when comparing TOP1 and TOP1-max, even though the shape of the curves is quite di erent from that of BPR.\nmore samples, the BPR gradient is very low, even for rank 100-500 (again, the gradient BPR-max starts decreasing signi cantly later). This means that BPR can hardly push target scores up in the ranking after a certain point, which comes earlier as the number of sample size increases. BPR-max, on the other hand, behaves well and is able to improve the score all the way.\n3.3.1 BPR-maxwith score regularization. Even though we showed that the heuristic TOP1 loss is sensitive to relevant samples with very high scores, it was found to be performing better than BPR in [6]. According to our observation, the same is true for the relation of TOP1-max and BPR-max. Part of the reasons lies in the rare occurrence of r j ri while r j \u2248 0 simultaneously. If only the rst condition is met, the gradient w.r.t. ri might vanish, but the regularizing part of TOP1 makes sure that r j is moved towards zero, which might even make the update possible for ri next time (e.g. if r j was negative, moving it towards zero decreases the di erence with ri ). The score regularization in TOP1 is very bene cial to the overall learning process, so even though the loss might not be theoretically optimal, it can achieve good results. GRU4Rec support two forms of regularization with every loss: dropout and `2 regularization of the model parameters. The regularization of TOP1 is used on the top of these. According to our experiments, the `2 regularization of model parameters decreases the model performance. Our assumption is that some of the model weights \u2013 such as the weight matrices for computing the update and reset gate \u2013 should not be regularized. Penalizing high output scores takes care of constraining the model, even without explicitly regularizing the weights.\nTherefore we added score regularization to the BPR-max loss function as well. We tried several ways of score regularization. In the best performing one we conditioned the sample scores on independent, zero mean Gaussians with variance inversely proportional to the softmax score (15). This entails stronger regularization on scores closer to the maximum, which is ideal in our case. P ( ri > rmax |{r j }NSj=1 ) NS\u220f j=1 P(r j ) = P ( ri > rmax |{r j }NSj=1 ) NS\u220f j=1 N ( 0, c sj ) (15)\nWe minimize the negative log-probability and do continuous approximations as before, resulting in the nal form of the BPRmax loss function (16). The regularization term is a simple, softmax weighted `2 regularization over the scores. \u03bb is the regularization parameter, which we usually set to 1.\nLbpr\u2212max = \u2212 log NS\u2211 j=1 sj\u03c3 (ri \u2212 r j ) + \u03bb NS\u2211 j=1 sjr 2 j (16)"}, {"heading": "4 EXPERIMENTS", "text": "Experimental setup: We evaluated the proposed improvements \u2013 xed cross-entropy loss, ranking-max loss functions & adding additional samples \u2013 on four dataset. RSC15 is based on the dataset of RecSys Challange 20159, which contains click and buy events from an online webshop. We only kept the click data. VIDEO and VIDXL are proprietary datasets containing watch events from an online\n9http://2015.recsyschallenge.com\nvideo service. Finally, CLASS is a proprietary dataset containing item page view events from an online classi ed site. Datasets were subjugated to minor preprocessing then split into train and test sets so that a whole session either belongs to the train or to the test set. The split is based on the time of the rst event of the sessions. The datsets and the split are exactly the same for RSC15 as in [6]; and for VIDXL and CLASS as in [7]. VIDEO is of the same source as in [6], but a slightly di erent subset. Table 1 overviews the main properties of the datasets.\nEvaluation is done under the next item prediction scenario, that is we iterate over test sessions and events therein. For each event, the algorithm guesses the item of the next event of that session. Since the size of the VIDXL test set is large, we compare the target item\u2019s score to that of the 50,000 most popular items during testing, similarly to [7]. While this evaluation for VIDXL overestimates the performance, the comparison of algorithms remain fair [2]. As recommender systems can only recommend a few items at once, the actual item a user might pick should be amongst the rst few items of the list. Therefore, our primary evaluation metric is recall@20 that is the proportion of cases having the desired item amongst the top-20 items in all test cases. Recall does not consider the actual rank of the item as long as it is amongst the top-N. This models certain practical scenarios well where there is no highlighting of recommendations and the absolute order does not matter. Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)[8, 13]. The second metric used in the experiments is MRR@20 (Mean Reciprocal Rank). That is the average of reciprocal ranks of the desired items. The reciprocal rank is set to zero if the rank is above 20. MRR takes into account the rank of the item, which is important in cases where the order of recommendations matter (e.g. the lower ranked items are only visible after scrolling).\nThe natural baseline we use is the original GRU4Rec algorithm, upon which we aim to improve. We consider the results with the originally proposed TOP1 loss and tanh activation function on the output to be the baseline. The hidden layer has 100 units. We also indicate the performance of item-kNN, a natural baseline for next item prediction. Results for RSC15, VIDXL and CLASS are taken directly from corresponding papers [6, 7] and measured with the optimal hyperparameters in [6] for VIDEO. We do separate hyperparameter optimization on a separate validation set for the proposed improvements.\nThe methods are implemented under the Theano framework [1] in python. Experiments were run on various GPUs, training times\nwere measured on an unloaded Titan X (Maxwell) GPU. Code is available publicly on GitHub10 for reproducibility."}, {"heading": "4.1 Using additional samples", "text": "The rst set of experiments examines the e ect of additional negative samples on recommendation accuracy. Experiments were performed on the CLASS and the VIDEO datasets. Since results are quite similar we excluded the VIDEO results to save some space. Figure 3 depicts the performance of the network with TOP1, cross-entropy, TOP1-max and BPR-max losses. Recommendation accuracy was measured with di erent number of additional samples, as well as in the case when all scores are computed and there is no sampling. As we discussed earlier, this latter scenario is a more theoretical one, because it is not scalable. As theory suggests (see Section 3), the TOP1 loss does not cope well with lots of samples. There is a slight increase in performance with a few extra samples, as the chance of having relevant samples increases; but performance quickly degrades as sample size grow, thus lots of irrelevant samples are included. On the other hand, all three of the other losses react well to adding more samples. The point of diminishing return is around a few thousand of extra samples for cross-entropy. TOP1-max starts to slightly lose accuracy after that. BPR-max improves with more samples all the way, but slightly loses accuracy when all items are used.\nAdding extra samples increase computational cost, yet due to easy parallelization on modern GPUs most of this cost is alleviated. Figure 4 shows the training times at di erent sample sizes. Please note the logarithmic scale. The actual training time depends on not just the dataset, but model parameters (especially mini-batch size) and how certain operators used for computing the loss are supported by the framework. The trend, however, is similar to for all losses. For example, the full training of the network is around 10 minutes (with the settings for cross-entropy or TOP1-max), which 10https://github.com/hidasib/GRU4Rec\ndoes not increase with even 512 extra samples. At the point of diminishing returns, i.e. at 2048 extra samples, training time is around 15 minutes, which is also totally acceptable. After that, training times grow quickly, due to exceeding the parallelization capabilities of the GPU we used. The trend is similar on the VIDEO dataset, with training times starting around 50 minutes, starting to increase at 2048 extra samples (to 80 minutes) and quickly above thereafter. This means that the proposed method can be used with zero too little additional cost in practice, unlike data augmentation methods. It is also clear that GRU4Rec can work just as well with a few thousands of negative examples as with the whole itemset, thus it can be kept scalable.\nIn the next experiment we looked at the \u03b1 parameter of the sampling. Figure 5 depicts performance with di erent \u03b1 values for cross-entropy, TOP1-max and BPR-max losses. Cross-entropy favors higher \u03b1 values with low sample sizes and low \u03b1 values for large samples. This is inline with our discussion in Section 2: popular samples are useful when the sample size is very limited and at the beginning of the training, but might be exhausted quickly, thus switching to a more balanced sampling can be bene cial if we have the means to (e.g. large enough sample size). Also, the uniform sampling in this case is supplemented by the few popularity based samples of the mini-batch sampling. The ranking-max losses, on the other hand, seem to prefer the middle road with a slight preference towards higher values, while the extremes perform the worst. We assume that this is mostly due to (a) them basing on pairwise losses, where popular samples are usually desired; (b) and the score regularization: with popularity based sampling the scores of the most popular items would be decreased beyond what is desirable."}, {"heading": "4.2 Loss-functions", "text": "We measure the performance gain of the proposed improvements over the baselines. The big accuracy improvement comes from the combination of additional samples and the loss functions ( xed\ncross-entropy, TOP1-max and BPR-max). Table 2 showcases our most important results. Besides the original RGU4Rec and the itemkNN, we included results with cross-entropy (XE) loss without additional sampling to con rm that the xed cross-entropy loss still performs just slightly better than TOP1. The increase with sampling and the proper loss function is stunning as the best results exceed the accuracy of the original GRU4Rec by 15 \u2212 35% and that of item-kNN by up to 51%. The best results on RSC15 are basically the same what was achieved by very costly data augmentation and additional ne tuning by [18]. Unlike our method where increase in the training time is little to none. The data augmentation could also be applied simultaneously with our improvements, probably resulting in some additional increase of accuracy. TOP1-max and BPR-max performs similarly (2 of 4) or better (2 of 4; +3 \u2212 7% improvement) than cross-entropy when using extra samples."}, {"heading": "5 CONCLUSION", "text": "We introduced a new class of loss function that together with an improved sampling strategy have provided impressive top-k gains for RNNs for session-based recommendations. We believe that these new losses could be more generally applicable and along with the corresponding sampling strategies also provide top-k gains for\ndi erent recommendations settings and algorithms such as e.g. matrix factorization or autoencoders. It is also conceivable that these techniques could also provide similar bene ts in the area of Natural Language Processing a domain that shares signi cant similarities to the recommendation domain in terms of machine learning (e.g. ranking, retrieval) and data structure (e.g. sparse large input and output space)."}], "references": [{"title": "Precision-oriented Evaluation of Recommender Systems: An Algorithmic Comparison", "author": ["Alejandro Bellogin", "Pablo Castells", "Ivan Cantador"], "venue": "In RecSys\u201911: 5th ACM Conf. on Recommender Systems. 333\u2013336", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "From RankNet to LambdaRank to LambdaMART: An Overview", "author": ["Chris J.C. Burges"], "venue": "Technical Report. https://www.microsoft.com/en-us/research/ publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In SSST-8: 8th Workshop on Syntax, Semantics and Structure in Statistical Translation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Collaborative ltering with recurrent neural networks", "author": ["Robin Devooght", "Hugues Bersini"], "venue": "arXiv preprint arXiv:1608.07400", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Session-based Recommendations with Recurrent Neural Networks", "author": ["Bal\u00e1zs Hidasi", "Alexandros Karatzoglou", "Linas Baltrunas", "Domonkos Tikk"], "venue": "International Conference on Learning Representations (2016)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations", "author": ["Bal\u00e1zs Hidasi", "Massimo Quadrana", "Alexandros Karatzoglou", "Domonkos Tikk"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys \u201916)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback", "author": ["B. Hidasi", "D. Tikk"], "venue": "In ECML-PKDD\u201912, Part II. Number 7524 in LNCS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Shihao Ji", "SVN Vishwanathan", "Nadathur Satish", "Michael J Anderson", "Pradeep Dubey"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "OrdRec: An Ordinal Model for Predicting Personalized Item Rating Distributions", "author": ["Yehuda Koren", "Joe Sill"], "venue": "In Proceedings of the Fifth ACMConference on Recommender Systems (RecSys \u201911)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Amazon.com recommendations: Item-toitem collaborative ltering", "author": ["G. Linden", "B. Smith", "J. York"], "venue": "Internet Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Enlister: Baidu\u2019s Recommender System for the Biggest Chinese Q&A Website", "author": ["Qiwen Liu", "Tianjian Chen", "Jing Cai", "Dianhai Yu"], "venue": "In RecSys-12: Proc. of the 6th ACM Conf. on Recommender Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "author": ["Ste en Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt- Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Arti cial Intelligence (UAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In UAI\u201909: 25th Conf. on Uncertainty in Arti cial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Item-based collaborative ltering recommendation algorithms", "author": ["Badrul Sarwar", "George Karypis", "Joseph Konstan", "John Riedl"], "venue": "Int. Conf. on World Wide Web", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "CLiMF: Learning to Maximize Reciprocal Rank with Collaborative Less-is-more Filtering", "author": ["Yue Shi", "Alexandros Karatzoglou", "Linas Baltrunas", "Martha Larson", "Nuria Oliver", "Alan Hanjalic"], "venue": "In Proceedings of the Sixth ACM Conference on Recommender Systems (RecSys", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Improved Recurrent Neural Networks for Session-based Recommendations. In Proceedings of the 1stWorkshop on Deep Learning for Recommender Systems (DLRS 2016)", "author": ["Yong Kiam Tan", "Xinxing Xu", "Yong Liu"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "COFIRANK Maximum Margin Matrix Factorization for Collaborative Ranking", "author": ["Markus Weimer", "Alexandros Karatzoglou", "Quoc Viet Le", "Alex Smola"], "venue": "In Proceedings of the 20th International Conference on Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Recurrent Recommender Networks", "author": ["Chao-Yuan Wu", "Amr Ahmed", "Alex Beutel", "Alexander J. Smola", "How Jing"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM \u201917)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}], "referenceMentions": [{"referenceID": 14, "context": "Until recently many of these recommendations tasks were tackled mainly using relatively simple methods such as item-based collaborative ltering [16] or content-based methods.", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "In recommender systems RNNs have been recently applied to the session-based recommendation setting with impressive results [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "[3]) and in particular ranking objectives and loss functions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user pro le is the item-to-item recommendation approach [12, 16].", "startOffset": 183, "endOffset": 191}, {"referenceID": 14, "context": "One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user pro le is the item-to-item recommendation approach [12, 16].", "startOffset": 183, "endOffset": 191}, {"referenceID": 7, "context": "Long Short-Term Memory (LSTM) [9] networks are a type of RNNs that have been shown to solve the optimization issues the plague vanilla-type RNNs.", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "A slightly simpli ed version of LSTM \u2013 that still maintains all their properties \u2013 are Gated Recurrent Units (GRUs) [4], which we use in this work.", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Recurrent Neural Networks have been used with success in the area of session-based recommendations; [6] proposed a Recurrent Neural Network with a pairwise ranking loss for this task, [18] proposed data augmentation techniques to improve the performance of the RNN for sessionbased recommendations; these techniques have though the side e ect of increasing training times as a single session is split into several sub-sessions for training.", "startOffset": 100, "endOffset": 103}, {"referenceID": 16, "context": "Recurrent Neural Networks have been used with success in the area of session-based recommendations; [6] proposed a Recurrent Neural Network with a pairwise ranking loss for this task, [18] proposed data augmentation techniques to improve the performance of the RNN for sessionbased recommendations; these techniques have though the side e ect of increasing training times as a single session is split into several sub-sessions for training.", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "Session-based RNNs have been augmented [7] with feature information, such as text and images from the clicked/consumed items, showing improved performance over the plain models.", "startOffset": 39, "endOffset": 42}, {"referenceID": 18, "context": "RNNs have also been used in more standard user-item collaborative ltering settings where the aim is to model the evolution of the user and items factors [20],[5] where the results are less striking, with the proposed methods barely outperforming standard matrix factorization methods.", "startOffset": 153, "endOffset": 157}, {"referenceID": 3, "context": "RNNs have also been used in more standard user-item collaborative ltering settings where the aim is to model the evolution of the user and items factors [20],[5] where the results are less striking, with the proposed methods barely outperforming standard matrix factorization methods.", "startOffset": 158, "endOffset": 161}, {"referenceID": 17, "context": "One of the rst learning to rank techniques for collaborative ltering was introduced in [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "Further ranking loss function for collaborative ltering were introduced in [17] [14] and [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "Further ranking loss function for collaborative ltering were introduced in [17] [14] and [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "Further ranking loss function for collaborative ltering were introduced in [17] [14] and [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "Another work related to sampling large output spaces in deep networks for e cient loss computations for language models is the \u2019blackout\u2019 method [10], where essentially a sampling procedure similar to the one used in [6] is applied in order to e ciently compute the categorical cross-entropy loss.", "startOffset": 145, "endOffset": 149}, {"referenceID": 4, "context": "Another work related to sampling large output spaces in deep networks for e cient loss computations for language models is the \u2019blackout\u2019 method [10], where essentially a sampling procedure similar to the one used in [6] is applied in order to e ciently compute the categorical cross-entropy loss.", "startOffset": 217, "endOffset": 220}, {"referenceID": 4, "context": "In the remainder of the paper we will refer to the RNN algorithm implemented in [6] as GRU4Rec, the name of the implementation published by the authors on github 1.", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "GRU4Rec introduced mini-batch based sampling [6].", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "[6] reported slightly better results than with other losses, but deemed the loss to be unstable for a large fraction of the hyperparameter space and thus advised against its use.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "The other loss function (5) is based on the popular Bayesian Personalized Ranking (BPR) [15] loss.", "startOffset": 88, "endOffset": 92}, {"referenceID": 4, "context": "Even though we showed that the heuristic TOP1 loss is sensitive to relevant samples with very high scores, it was found to be performing better than BPR in [6].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "The datsets and the split are exactly the same for RSC15 as in [6]; and for VIDXL and CLASS as in [7].", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "The datsets and the split are exactly the same for RSC15 as in [6]; and for VIDXL and CLASS as in [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "VIDEO is of the same source as in [6], but a slightly di erent subset.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "Since the size of the VIDXL test set is large, we compare the target item\u2019s score to that of the 50,000 most popular items during testing, similarly to [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "While this evaluation for VIDXL overestimates the performance, the comparison of algorithms remain fair [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)[8, 13].", "startOffset": 96, "endOffset": 103}, {"referenceID": 11, "context": "Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)[8, 13].", "startOffset": 96, "endOffset": 103}, {"referenceID": 4, "context": "Results for RSC15, VIDXL and CLASS are taken directly from corresponding papers [6, 7] and measured with the optimal hyperparameters in [6] for VIDEO.", "startOffset": 80, "endOffset": 86}, {"referenceID": 5, "context": "Results for RSC15, VIDXL and CLASS are taken directly from corresponding papers [6, 7] and measured with the optimal hyperparameters in [6] for VIDEO.", "startOffset": 80, "endOffset": 86}, {"referenceID": 4, "context": "Results for RSC15, VIDXL and CLASS are taken directly from corresponding papers [6, 7] and measured with the optimal hyperparameters in [6] for VIDEO.", "startOffset": 136, "endOffset": 139}, {"referenceID": 16, "context": "The best results on RSC15 are basically the same what was achieved by very costly data augmentation and additional ne tuning by [18].", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior. The use of RNNs provides impressive performance bene ts over classical methods in session-based recommendations. In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings. The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative ltering approaches. Unlike data augmentation-based improvements, our method does not increase training times signi cantly.", "creator": "LaTeX with hyperref package"}}}