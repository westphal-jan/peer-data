{"id": "1706.04892", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Second-Order Kernel Online Convex Optimization with Adaptive Sketching", "abstract": "automated kernel online internal convex optimization ( koco ) is a tool framework combining the expressiveness of non - parametric kernel approximation models with the regret / guarantees of continuous online polynomial learning. first - order koco methods such as functional gradient descent require only $ \\ weighted mathcal { o } ( t ) $ time and space per query iteration, and, when the only information mapped on the losses result is their convexity, achieve a minimax optimal $ \\ mathcal { return o } ( \\ sqrt { t } ) $ regret. nonetheless, many attribute common losses in kernel problems, such as squared loss, logistic loss, and squared hinge loss posses stronger curvature that can be concurrently exploited. in doing this case, second - order robust koco methods achieve $ \\ reverse mathcal { o } ( \\ log ( \\ text { det } ( \\ boldsymbol { k } ) ) ) $ regret, thus which we show currently scales as $ \\ mathcal { o } ( d _ { \\ text { eff } } \\ log t ) $, where $ d _ { \\ text { eff } } $ is invariably the effective dimension map of the problem piece and is usually itself much smaller than $ \\ mathcal { o } ( \\ sqrt { t } ) $. the main drawback procedure of second - order methods is their much higher $ \\ mathcal { o } ( _ t ^ 2 ) $ space and time complexity. in this paper, we introduce kernel gradient online newton step ( kons ), a new second - value order ( koco algorithms method that also achieves $ \\ mathcal { o } ( d _ { \\ text { eff } } \\ log t ) $ regret. to address the computational complexity of second - order operations methods, \" we introduce a new matrix sketching an algorithm operating for the weighted kernel matrix $ \\ boldsymbol { k } _ t $, and show optimization that for improving a chosen parameter $ \\ gamma \\ leq below 1 $ 1 our sketched - kons reduces $ the space and time complexity by a factor of $ \\ gamma ^ 2 $ to $ \\ mathcal { o } ( t ^ 2 \\ gamma ^ 2 ) $ space and time distortion per iteration, while incurring yields only $ th 1 / \\ 4th gamma $ times more regret.", "histories": [["v1", "Thu, 15 Jun 2017 14:33:08 GMT  (54kb)", "http://arxiv.org/abs/1706.04892v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["daniele calandriello", "alessandro lazaric", "michal valko"], "accepted": true, "id": "1706.04892"}, "pdf": {"name": "1706.04892.pdf", "metadata": {"source": "META", "title": "Second-Order Kernel Online Convex Optimization with Adaptive Sketching", "authors": ["Daniele Calandriello", "Alessandro Lazaric", "Michal Valko"], "emails": ["<daniele.calandriello@inria.fr>."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n04 89\n2v 1\n[ st\nat .M\nL ]\n1 5\nJu n\n20 17\nframework combining the expressiveness of nonparametric kernel models with the regret guarantees of online learning. First-order KOCO methods such as functional gradient descent require onlyO(t) time and space per iteration, and, when the only information on the losses is their con-\nvexity, achieve a minimax optimal O( \u221a T ) regret. Nonetheless, many common losses in kernel problems, such as squared loss, logistic loss, and squared hinge loss posses stronger curvature that can be exploited. In this case, second-order KOCOmethods achieveO(log(Det(K))) regret, which we show scales as O(deff logT ), where deff is the effective dimension of the problem and is usually much smaller than O( \u221a T ). The main drawback of second-order methods is their much higher O(t2) space and time complexity. In this paper, we introduce kernel online Newton step (KONS), a new second-order KOCOmethod that also achievesO(deff logT ) regret. To address the computational complexity of second-order methods, we introduce a new matrix sketching algorithm for the kernel matrixKt, and show that for a chosen parameter \u03b3 \u2264 1 our Sketched-KONS reduces the space and time complexity by a factor of \u03b32 toO(t2\u03b32) space and time per iteration, while incurring only 1/\u03b3 times more regret."}, {"heading": "1. Introduction", "text": "Online convex optimization (OCO) (Zinkevich, 2003) models the problem of convex optimization over Rd as a game over t \u2208 {1, . . . , T } time steps between an adversary and the player. In its linear version, that we refer to as linearOCO (LOCO), the adversary chooses a sequence of arbitrary convex losses \u2113t and points xt, and a player chooses weightswt and predicts x T twt. The goal of the player is to\n1SequeL team, INRIA Lille - Nord Europe. Correspondence to: Daniele Calandriello <daniele.calandriello@inria.fr>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nminimize the regret, defined as the difference between the losses of the predictions obtained using the weights played by the player and the best fixed weight in hindsight given all points and losses.\nGradient descent. For this setting, Zinkevich (2003) showed that simple gradient descent (GD), combined with a smart choice for the stepsize \u03b7t of the gradient updates, achieves a O( \u221a dT ) regret with a O(d) space and time cost per iteration. When the only assumption on the losses is simple convexity, this upper bound matches the corresponding lower bound (Luo et al., 2016), thus making first-order methods (e.g.,GD) essentially unimprovable in a minimax sense. Nonetheless, when the losses have additional curvature properties, Hazan et al. (2006) show that online Newton step (ONS), an adaptive method that exploits second-order (second derivative) information on the losses, can achieve a logarithmic regret O(d log T ). The downside of this adaptive method is the largerO(d2) space and per-step time complexity, since second-order updates require to construct, store, and invert Ht, a preconditioner matrix related to the Hessian of the losses used to correct the first-order updates.\nKernel gradient descent. For linear models, such as the ones considered in LOCO, a simple way to create more expressive models is to map them in some high-dimensional space, the feature space, and then use the kernel trick (Scho\u0308lkopf & Smola, 2001) to avoid explicitly computing their high-dimensional representation. Mapping to a larger space allows the algorithm to better fit the losses chosen by the adversary and reduce its cumulative loss. As a drawback, the Kernel OCO (KOCO) problem1 is fundamentally harder than LOCO, due to 1) the fact that an infinite parametrization makes regret bounds scaling with the dimension d meaningless and 2) the size of the model, and therefore time and space complexities, scales with t itself, making these methods even less performant than LOCO algorithms. Kernel extensions of LOCO algorithms have been proposed for KOCO, such as functional GD (e.g., NORMA, Kivinen et al., 2004) which\nachieves a O( \u221a T ) regret with a O(t) space and time cost per iteration. For second-order methods, the Second-\n1This setting is often referred to as online kernel learning or kernel-based online learning in the literature.\nOrder Perceptron (Cesa-Bianchi et al., 2005) or NAROW (Orabona & Crammer, 2010) for generic curved losses and Recursive Kernel Least Squares (Zhdanov & Kalnishkan, 2010) or Kernel AAR (Gammerman et al., 2004) for the specific case of \u21132 losses provide bounds that scale with the log-determinant of the kernel-matrix. As we show, this quantity is closely related to the effective dimension dTeff of the of the points xt, and scales as O(dTeff logT ), playing a similar role as the O(d log T ) bound from LOCO. Approximate GD. To trade off between computational complexity ( smaller than O(d2) ) and improved regret (close to O(d log T )), several methods try approximate second-order updates, replacing Ht with an approximate H\u0303t that can be efficiently stored and inverted. AdaGrad (Duchi et al., 2011) and ADAM (Kingma & Ba, 2015) reweight the gradient updates on a per-coordinate basis using a diagonal H\u0303t, but these methods ultimately only improve the regret dependency on d and leave the\u221a T component unchanged. Sketched-ONS, by Luo et al. (2016), uses matrix sketching to approximateHt with a rrank sketch H\u0303t, that can be efficiently stored and updated in O(dr2) time and space, close to the O(d) complexity of diagonal approximations. More importantly, SketchedONS achieves a much smaller regret compared to diagonal approximations: When the true Ht is of low-rank r, it recovers a O(r logT ) regret bound logarithmic in T . Unfortunately, due to the sketch approximation, a new term appears in the bound that scales with the spectra ofHt, and in some cases can grow much larger thanO(log T ). Approximate kernel GD. Existing approximate GD methods for KOCO focus only on first-order updates, trying to reduce the O(t) per-step complexity. Budgeted methods, such as Budgeted-GD (Wang et al., 2012) and budgeted variants of the perceptron (Cavallanti et al., 2007; Dekel et al., 2008; Orabona et al., 2008) explicitly limit the size of the model, using some destructive budget maintenance procedure (e.g., removal, projection) to constrain the natural model growth over time. Alternatively, functional approximation methods in the primal (Lu et al., 2016) or dual (Le et al., 2016) use non-linear embedding techniques, such as random feature expansion (Le et al., 2013), to reduce the KOCO problem to a LOCO problem and solve\nit efficiently. Unfortunately, to guarantee O( \u221a T ) regret using less than O(t) space and time per round w.h.p., all of these methods require additional assumptions, such as points xt coming from a distribution or strong convexity on the losses. Moreover, as approximate first-order meth-\nods, they can at most hope to match the O( \u221a T ) regret of exact GD, and among second-order kernel methods, no approximation scheme has been proposed that can provably maintain the same O(log T ) regret as exact GD. In addition, approximatingHt is harder for KOCO, since we cannot directly access the matrix representation of Ht in the\nfeature-space, making diagonal approximation impossible, and low-rank sketching harder.\nContributions In this paper, we introduce Kernel-ONS, an extension to KOCO of the ONS algorithm. As a secondorder method, KONS achieves a O(dteff logT ) regret on a variety of curved losses, and runs in O(t2) time and space. To alleviate the computational complexity, we propose SKETCHED-KONS, the first approximate secondorder KOCO methods, that approximates the kernel matrix with a low-rank sketch. To compute this sketch we propose a new online kernel dictionary learning, kernel online row sampling, based on ridge leverage scores. By adaptively increasing the size of its sketch, SKETCHED-KONS provides a favorable regret-performance trade-off, where for a given factor \u03b3 \u2264 1, we can increase the regret by a linear 1/\u03b3 factor to O(dteff log(T )/\u03b3) while obtaining a quadratic \u03b32 improvement in runtime, thereby achieving O(t2\u03b32) space and time cost per iteration."}, {"heading": "2. Background", "text": "In this section, we introduce linear algebra and RKHS notation, and formally state the OCO problem in an RKHS (Scho\u0308lkopf & Smola, 2001).\nNotation. We use upper-case bold letters A for matrices, lower-case bold letters a for vectors, lower-case letters a for scalars. We denote by [A]ij and [a]i the (i, j) element of a matrix and i-th element of a vector respectively. We denote by IT \u2208 RT\u00d7T , the identity matrix of dimension T and byDiag(a) \u2208 RT\u00d7T , the diagonal matrix with the vector a \u2208 RT on the diagonal. We use eT,i \u2208 RT to denote the indicator vector of dimension T for element i. When the dimension of I and ei is clear from the context, we omit the T . We also indicate with I the identity operator. We use A B to indicate that A \u2212 B is a positive semi-definite (PSD) matrix. With \u2016 \u00b7 \u2016we indicate the operator \u21132-norm. Finally, the set of integers between 1 and T is denoted by [T ] := {1, . . . , T }. Kernels. Given an arbitrary input space X and a positive definite kernel functionK : X\u00d7X \u2192 R, we indicate the reproducing kernel Hilbert space (RKHS) associated with K asH. We choose to represent our Hilbert spaceH as a feature space where, given K, we can find an associated feature map \u03d5 : X \u2192 H, such that K(x,x\u2032) can be expressed as an inner product K(x,x\u2032) = \u3008\u03d5(x), \u03d5(x\u2032)\u3009\nH . With a\nslight abuse of notation, we represent our feature space as an high-dimensional vector space, or in other words H \u2286 RD , where D is very large or potentially infinite. With this notation, we can write the inner product simply as K(x,x\u2032) = \u03d5(x)T\u03d5(x\u2032), and for any function fw \u2208 H, we can represent it as a (potentially infinite) set of weights w such that fw(x) = \u03d5(x) Tw. Given points {xi}ti=1, we\nshorten \u03d5(xi) = \u03c6i and define the feature matrix \u03a6t = [\u03c61, . . . ,\u03c6t] \u2208 RD\u00d7t. Finally, to denote the inner product between two arbitrary subsets a and b of columns of \u03a6T we use Ka,b = \u03a6 T a\u03a6b. With this notation, we can write the empirical kernel matrix as Kt = K[t],[t] = \u03a6 T t\u03a6t, the vector with all the similarities between a new point and the old ones as k[t\u22121],t = \u03a6 T t\u22121\u03c6t, and the kernel evaluated at a specific point as kt,t = \u03c6 T t\u03c6t. Throughout the rest of the paper, we assume that K is normalized and \u03c6Tt\u03c6t = 1. Kernelized online convex optimization. In the general OCO framework with linear prediction, the optimization process is a gamewhere at each time step t \u2208 [T ] the player 1 receives an input xt \u2208 X from the adversary, 2 predicts y\u0302t = fwt(xt) = \u03d5(xt) Twt = \u03c6 T twt,\n3 incurs loss \u2113t(y\u0302t), with \u2113t a convex and differentiable function chosen by the adversary,\n4 observes the derivative g\u0307t = \u2113 \u2032 t(y\u0302t).\nSince the player uses a linear combination \u03c6Ttwt to compute y\u0302t, having observed g\u0307t, we can compute the gradient,\ngt = \u2207\u2113t(y\u0302t) = g\u0307t\u2207(\u03c6Ttwt\u22121) = g\u0307t\u03c6t.\nAfter t timesteps, we indicate with Dt = {xi}ti=1, the dataset containing the points observed so far. In the rest of the paper we consider the problem of kernelized OCO (KOCO) where H is arbitrary and potentially nonparametric. We refer to the special parametric case H = R d and \u03c6t = xt as linear OCO (LOCO).\nIn OCO, the goal is to design an algorithm that returns a solution that performs almost as well as the best-in-class, thus we must first define our comparison class. We define the feasible set as St = {w : |\u03c6Ttw| \u2264 C} and S = \u2229Tt=1St. This comparison class contains all functions fw whose output is contained (clipped) in the interval [\u2212C,C] on all points x1, . . . , xT . Unlike the often used constraint on \u2016w\u2016H (Hazan et al., 2006; Zhu & Xu, 2015), comparing against clipped functions (Luo et al., 2016; Gammerman et al., 2004; Zhdanov & Kalnishkan, 2010) has a clear interpretation even when passing from R d toH. Moreover, S is invariant to linear transformations of H and suitable for practical problems where it is often easier to choose a reasonable interval for the predictions y\u0302t rather than a bound on the norm of a (possibly noninterpretable) parametrization w. We can now define the regret as\nRT (w) = \u2211T\nt=1 \u2113t(\u03c6\nT twt)\u2212 \u2113t(\u03c6Ttw)\nand denote with RT = RT (w \u2217), the regret w.r.t.w\u2217 = argminw\u2208S \u2211T t=1 \u2113t(\u03c6 T\ntw), i.e., the best fixed function in S. We work with the following assumptions on the losses.\nAlgorithm 1 One-shot KONS\nInput: Feasible parameter C, stepsizes \u03b7t, regulariz. \u03b1 1: Initializew0 = 0,g0 = 0, b0 = 0,A0 = \u03b1I 2: for t = {1, . . . , T } do 3: receive xt 4: compute bs as in Lem. 2 5: compute ut = A \u22121 t\u22121( \u2211t\u22121 s=0 bsgs)\n6: compute yt = \u03d5(xt) Tut 7: predict y\u0302t = \u03d5(xt) Twt = yt \u2212 h(yt) 8: observe gt, updateAt = At\u22121 + \u03b7tgtg T\nt\n9: end for\nAssumption 1. The loss function \u2113t satisfies |\u2113\u2032t(y)| \u2264 L whenever y \u2264 C. Note that this is equivalent to assuming Lipschitzness of the the loss w.r.t. y and it is weaker than assuming something on the norm of the gradient \u2016gt\u2016, since \u2016gt\u2016 = |g\u0307t|\u2016\u03c6t\u2016. Assumption 2. There exists \u03c3t \u2265 0 such that for all u,w \u2208 S , lt(w) = \u2113t(\u03c6Ttw) is lower-bounded by\nlt(w) \u2265 lt(u) +\u2207lt(u)T(w\u2212u) + \u03c3t 2 (\u2207lt(u)T(w\u2212u))2.\nThis condition is weaker than strong convexity and it is satisfied by all exp-concave losses (Hazan et al., 2006). For example, the squared loss lt(w) = (yt \u2212 xTtw)2 is not strongly convex but satisfies Asm. 2 with \u03c3t = 1/(8C\n2) whenw \u2208 S."}, {"heading": "3. Kernelized Online Newton Step", "text": "The online Newton step algorithm, originally introduced by Hazan et al. (2006), is a projected gradient descent that uses the following update rules\nut = wt\u22121 \u2212A\u22121t\u22121gt\u22121, wt = \u03a0 At\u22121\nSt (ut),\nwhere \u03a0 At\u22121 St (ut) = argminw\u2208St \u2016ut \u2212 w\u2016At\u22121 is an oblique projection on a set St with matrix At\u22121. If St is the set of vectors with bounded prediction in [\u2212C,C] as by Luo et al. (2016), then the projection reduces to\nwt = \u03a0 At\u22121 St (ut) = ut \u2212\nh(\u03c6Ttut)\n\u03c6TtA \u22121 t\u22121\u03c6t\nA\u22121t\u22121\u03c6t, (1)\nwhere h(z) = sign(z)max{|z| \u2212 C, 0} computes how much z is above or below the interval [\u2212C,C]. When At = I/\u03b7t, ONS is equivalent to vanilla projected gradient descent, which in LOCO achieves O( \u221a dT ) regret (Zinkevich, 2003). In the same setting, Hazan et al. (2006) shows that choosing At = \u2211t s=1 \u03b7sgsg T\ns + \u03b1I makes ONS an efficient reformulation of follow the approximate\nleader (FTAL). While traditional follow-the-leader algorithms play the weight wt = argminw\u2208St \u2211t\u22121\ns=1 lt(w), FTAL replaces the loss lt with a convex approximation using Asm. 2, and plays the minimizer of the surrogate function. As a result, under Asm. 1-2 and when \u03c3t \u2265 \u03c3 > 0, FTAL achieves a logarithmic O(d log T ) regret. FTAL\u2019s solution path can be computed in O(d2) time using ONS updates, and further speedups were proposed by Luo et al. (2016) using matrix sketching.\nUnfortunately, in KOCO, vectors \u03c6t and weights wt cannot be explicitly represented, and most of the quantities used in vanilla ONS (Eq. 1) cannot be directly computed. Instead, we derive a closed form alternative (Alg. 1) that can be computed in practice. Using a rescaled variant of our feature vectors \u03c6t, \u03c6t = g\u0307t \u221a \u03b7t\u03c6t = \u221a \u03b7tgt and \u03a6t = [\u03c61, . . . ,\u03c6t], we can rewrite At = \u03a6t\u03a6 T t + \u03b1I and \u03a6 T\nt\u03a6t = Kt, where the empirical kernel matrix Kt is computed using the rescaled kernel K(xi,xj) = g\u0307i \u221a \u03b7ig\u0307j \u221a \u03b7jK(xi,xj) instead of the original K, or equivalently Kt = DtKtDt with Dt = Diag({g\u0307i\u221a\u03b7i}ti=1) the rescaling diagonal matrix. We begin by noting that\ny\u0302t = \u03c6 T twt = \u03c6 T t ( ut \u2212 h(\u03c6Ttut)\n\u03c6TtA \u22121 t\u22121\u03c6t\nA\u22121t\u22121\u03c6t\n)\n= \u03c6Ttut \u2212 h(\u03c6Ttut) \u03c6TtA\n\u22121 t\u22121\u03c6t\n\u03c6TtA \u22121 t\u22121\u03c6t\n= yt \u2212 h(yt).\nAs a consequence, if we can find a way to compute yt, then we can obtain y\u0302t without explicitly computingwt. Before that, we first derive a non-recursive formulation of ut.\nLemma 1. In Alg. 1 we introduce\nbi = [bt]i = g\u0307i \u221a \u03b7i ( y\u0302i \u2212 h(yi)\n\u03c6 T iA \u22121 i\u22121\u03c6i\n) \u2212 1\u221a\n\u03b7i\nand compute ut as\nut = A \u22121 t\u22121\u03a6t\u22121bt\u22121.\nThen, ut is equal to the same quantity in Eq. 1 and the sequence of predictions y\u0302t is the same in both algorithms.\nWhile the definition of bt and ut still requires performing operations in the (possibly infinitely dimensional) feature space, in the following we show that bt and the prediction yt can be conveniently computed using only inner products.\nLemma 2. All the components bi = [bt]i of the vector introduced in Lem. 1 can be computed as\ng\u0307i \u221a \u03b7i ( y\u0302i \u2212\n\u03b1h(yi)\nki,i \u2212 k T [i\u22121],i(Ki\u22121 + \u03b1I) \u22121k[i\u22121],i\n\u2212 1 \u03b7i\n) .\nThen, we can compute\nyt = 1\n\u03b1 kT[t\u22121],tDt\u22121(bt\u22121 \u2212 (Kt\u22121 + \u03b1I)\u22121Kt\u22121bt\u22121).\nSince Alg. 1 is equivalent to ONS (Eq. 1), existing regret bounds for ONS directly applies to its kernelized version.\nProposition 1 (Luo et al., 2016). For any sequence of losses \u2113t satisfying Asm. 1-2, the regret RT of Alg. 1 is bounded by RT \u2264 \u03b1\u2016w\u2217\u20162 +RG +RD with\nRG := T\u2211\nt=1\ngTtA \u22121 t gt =\nT\u2211\nt=1\n\u03c6 T t (\u03a6t\u03a6 T t + \u03b1I) \u22121\u03c6t/\u03b7t\nRD :=\nT\u2211\nt=1\n(wt \u2212w\u2217)T(At \u2212At\u22121\u2212\u03c3tgtgTt )(wt \u2212w\u2217)\n=\nT\u2211\nt=1\n(\u03b7t \u2212 \u03c3t)g\u03072t (\u03c6Tt (wt \u2212w\u2217))2.\nIn the d-dimensional LOCO, choosing a decreasing stepsize \u03b7t = \u221a d/(C2L2t) allows ONS to achieve a\nO(CL \u221a dT ) regret for the cases where \u03c3t = 0. When \u03c3t \u2265 \u03c3 > 0 (e.g., when the functions are exp-concave) we can set \u03b7t = \u03c3t and improve the regret toO(d log(T )). Unfortunately, these quantities hold little meaning for KOCO\nwithD-dimensional features, since aO( \u221a D) regret can be very large or even infinite. On the other hand, we expect the regret of KONS to depend on quantities that are more strictly related to the kernelKt and its complexity. Definition 1. Given a kernel function K, a set of points Dt = {xi}ti=1 and a parameter \u03b1 > 0, we define the \u03b1ridge leverage scores (RLS) of point i as\n\u03c4t,i=e T t,iK T t (Kt+\u03b1I) \u20131et,i=\u03c6 T i (\u03a6t\u03a6 T t +\u03b1I) \u20131\u03c6i, (2)\nand the effective dimension of Dt as\ndteff(\u03b1) =\nt\u2211\ni=1\n\u03c4t,i = Tr ( Kt(Kt + \u03b1It) \u22121 ) . (3)\nIn general, leverage scores have been used to measure the correlation between a point i w.r.t. the other t \u2212 1 points, and therefore how essential it is in characterizing the dataset (Alaoui & Mahoney, 2015). As an example, if \u03c6i is completely orthogonal to the other points, \u03c4t,i = \u03c6 T i (\u03c6i\u03c6 T i + \u03b1I) \u22121\u03c6i \u2264 1/(1 + \u03b1) and its RLS is maximized, while in the case where all the points xi are identical, \u03c4t,i = \u03c6 T i (t\u03c6i\u03c6 T i +\u03b1I) \u22121\u03c6i \u2264 1/(t+\u03b1) and its RLS is minimal. While the previous definition is provided for a generic kernel functionK, we can easily instantiate it on K and obtain the definition of \u03c4 t,i. By recalling the first regret term in the decomposition of Prop. 1, we notice that\nRG =\nT\u2211\nt=1\n\u03c6 T t (\u03a6t\u03a6 T t + \u03b1I) \u22121\u03c6t/\u03b7t =\nT\u2211\nt=1\n\u03c4 t,t/\u03b7t,\nwhich reveals a deep connection between the regret of KONS and the cumulative sum of the RLS. In other words, the RLS capture how much the adversary can increase the regret by picking orthogonal directions that have not been seen before. While in LOCO, this can happen at most d times (hence the dependency on d in the final regret, which is mitigated by a suitable choice of \u03b7t), in KOCO, RG can grow linearly with time, since large H can have infinite near-orthogonal directions. Nonetheless, the actual growth rate is now directly related to the complexity of the sequence of points chosen by the adversary and the kernel function K. While the effective dimension dteff(\u03b1) is related to the capacity of the RKHS H on the points in Dt and it has been shown to characterize the generalization error in batch linear regression (Rudi et al., 2015), we see that RG is rather related to the online effective dimension d t onl(\u03b1) = \u2211\ni \u03c4 i,i. Nonetheless, we show that the two quantities are also strictly related to each other. Lemma 3. For any datasetDT , any \u03b1 > 0 we have\nd T onl(\u03b1) := T\u2211\nt=1\n\u03c4 t,t \u2264 log(Det(KT /\u03b1+ I))\n\u2264 dTeff(\u03b1)(1 + log(\u2016KT \u2016/\u03b1+ 1)).\nWe first notice that in the first inequality we relate d T\nonl(\u03b1) to the log-determinant of the kernel matrixKT . This quantity appears in a large number of works on online linear prediction (Cesa-Bianchi et al., 2005; Srinivas et al., 2010) where they were connected to the maximal mutual information gain in Gaussian processes. Finally, the second inequality shows that in general the complexity of online learning is only a factor logT (in the worst case) away from the complexity of batch learning. At this point, we can generalize the regret bounds of LOCO to KOCO.\nTheorem 1. For any sequence of losses \u2113t satisfying Asm. 1-2, let \u03c3 = mint \u03c3t. If \u03b7t \u2265 \u03c3 \u2265 0 for all t and \u03b1 \u2264 \u221a T , the regret of Alg. 1 is upper-bounded as\nRT \u2264 \u03b1\u2016w\u2217\u20162 + dTonl(\u03b1)/\u03b7T + 4C2L2 T\u2211\nt=1\n(\u03b7t \u2212 \u03c3).\nIn particular, if for all t we have \u03c3t \u2265 \u03c3 > 0, setting \u03b7t = \u03c3 we obtain\nRT \u2264 \u03b1\u2016w\u2217\u20162 + 2dTeff ( \u03b1/(\u03c3L2) ) log(2\u03c3L2T ) \u03c3 ,\notherwise, \u03c3 = 0 and setting \u03b7t = 1/(LC \u221a t) we obtain\nRT \u2264 \u03b1\u2016w\u2217\u20162 + 4LC \u221a TdTeff(\u03b1/L 2) log(2L2T ).\nComparison to LOCO algorithms. We first notice that the effective dimension dTeff(\u03b1) can be seen as a soft rank\nAlgorithm 2 Kernel Online Row Sampling (KORS)\nInput: Regularization \u03b1, accuracy \u03b5, budget \u03b2 1: Initialize I0 = \u2205 2: for t = {0, . . . , T \u2212 1} do 3: receive \u03c6t 4: construct temporary dictionary It := It\u22121 \u222a (t, 1) 5: compute p\u0303t = min{\u03b2\u03c4\u0303t,t, 1} using It and Eq. 4 6: draw zt \u223c B(p\u0303t) and if zt = 1, add (t, 1/p\u0303t) to It 7: end for\nforKT and that it is smaller than the rank r for any \u03b1. 2 For exp-concave functions (i.e., \u03c3 > 0), we slightly improve over the bound of Luo et al. (2016) from O(d logT ) down to O(dTeff(\u03b1) log T ) \u2264 O(r logT ), where r is the (unknown) rank of the dataset. Furthermore, when \u03c3=0, setting \u03b7t = \u221a 1/(L2C2t) gives us a regret O( \u221a TdTeff(\u03b1))\u2264\nO( \u221a Tr), which is potentially much smaller thanO( \u221a Td). Furthermore, if an oracle provided us in advance with dTeff(\u03b1), setting \u03b7t = \u221a dTeff(\u03b1)/(L 2C2t) gives a regret O( \u221a dTeff(\u03b1)T ) \u2264 O( \u221a rT ).\nComparison to KOCO algorithms. Simple functional gradient descent (e.g., NORMA, Kivinen et al., 2004)\nachieves a O( \u221a T ) regret when properly tuned (Zhu & Xu, 2015), regardless of the loss function. For the special case of squared loss, Zhdanov & Kalnishkan (2010) show that Kernel Ridge Regression achieves the same O(log(Det(KT /\u03b1+ I))) regret as achieved by KONS for general exp-concave losses."}, {"heading": "4. Kernel Online Row Sampling", "text": "Although KONS achieves a low regret, storing and inverting the K matrix requires O(t2) space and O(t3) time, which becomes quickly unfeasible as t grows. To improve space and time efficiency, we replace Kt with an accurate low-rank approximation K\u0303t, constructed using a carefully chosen dictionary It of points from Dt. We extend the online row sampling (ORS) algorithm of Cohen et al. (2016) to the kernel setting and obtain Kernel-ORS (Alg. 2). There are two main obstacles to overcome in the adaptation of ORS: From an algorithmic perspective we need to find a computable estimator for the RLS, since \u03c6t cannot be accessed directly, while from an analysis perspective we must prove that our space and time complexity does not scale with the dimension of \u03c6t (as Cohen et al. 2016), as it can potentially be infinite.\nWe define a dictionary It as a collection of (index, weight) tuples (i, 1/p\u0303i) and the associated selection matrix St \u2208\n2This can be easily seen as dTeff(\u03b1) = \u2211 t \u03bbt/(\u03bbt +\u03b1), where\n\u03bbt are the eigenvalues of KT .\nR t\u00d7t as a diagonal matrix with 1/ \u221a p\u0303i for all i \u2208 It and 0 elsewhere. We also introduceAItt = \u03a6tStS T t\u03a6 T\nt+\u03b1I as an approximation of At constructed using the dictionary It. At each time step, KORS temporarily adds t with weight 1 to the dictionary It\u22121 and constructs the temporary dictionary It,\u2217 and the corresponding selection matrix St,\u2217 and approximation A It,\u2217\nt . This augmented dictionary can be\neffectively used to compute the RLS estimator,\n\u03c4\u0303t,i = (1 + \u03b5)\u03c6t ( A It,\u2217 t )\u22121 \u03c6t (4) = 1+\u03b5\u03b1 ( kt,t \u2212 k T [t],tSt,\u2217(S T t,\u2217KtSt,\u2217 + \u03b1I) \u22121STt,\u2217k[t],t ) .\nWhile we introduced a similar estimator before (Calandriello et al., 2017), here we modified it so that \u03c4\u0303t,i is an overestimate of the actual \u03c4 t,i. Note that all rows and columns for which St,\u2217 is zero (all points outside the temporary dictionary It,\u2217) do not influence the estimator, so they can be excluded from the computation. As a consequence, denoting by |It,\u2217| the size of the dictionary, \u03c4\u0303t,i can be efficiently computed in O(|It,\u2217|2) space and O(|It,\u2217|2) time (using an incremental update of Eq. 4). After computing the RLS, KORS randomly chooses whether to include a point in the dictionary using a coin-flip with probability p\u0303t = min{\u03b2\u03c4\u0303t,t, 1} and weight 1/p\u0303t, where \u03b2 is a parameter. The following theorem gives us at each step guarantees on the accuracy of the approximate matrices AItt and of estimates \u03c4\u0303t,t, as well as on the size |It| of the dictionary. Theorem 2. Given parameters 0 < \u03b5 \u2264 1, 0 < \u03b1, 0 < \u03b4 < 1, let \u03c1 = 1+\u03b51\u2212\u03b5 and run Algorithm 2 with \u03b2 \u2265 3 log(T/\u03b4)/\u03b52. Then w.p. 1\u2212 \u03b4, for all steps t \u2208 [T ],\n(1) (1\u2212 \u03b5)At AItt (1 + \u03b5)At. (2) The dictionary\u2019s size |It| = \u2211t s=1 zs is bounded by\nt\u2211\ns=1\nzs \u2264 3 t\u2211\ns=1\np\u0303s \u2264 dtonl(\u03b1) 3\u03c1\u03b2\n\u03b52 \u2264 dteff(\u03b1)\n6\u03c1 log2 ( 2T \u03b4 )\n\u03b52 .\n(3) Satisfies \u03c4t,t \u2264 \u03c4\u0303t,t \u2264 \u03c1\u03c4t,t.\nMoreover, the algorithm runs in O(dteff(\u03b1)2 log4(T )) space, and O\u0303(dteff(\u03b1)2) time per iteration.\nThe most interesting aspect of this result is that the dictionary It generated by KORS allows to accurately approximate the At = \u03a6t\u03a6 T\nt + \u03b1I matrix up to a small (1 \u00b1 \u03b5) multiplicative factor with a small time and space complexity, which makes it a natural candidate to sketch KONS."}, {"heading": "5. Sketched ONS", "text": "Building on KORS, we now introduce a sketched variant of KONS that can efficiently trade off between computational\nAlgorithm 3 SKETCHED-KONS\nInput: Feasible parameter C, stepsizes \u03b7t, regulariz. \u03b1 1: Initializew0 = 0,g0 = 0, b0 = 0, A\u03030 = \u03b1I 2: Initialize independent run of KORS\n3: for t = {1, . . . , T } do 4: receive xt 5: compute u\u0303t = A\u0303 \u22121 t\u22121( \u2211t\u22121 s=0 b\u0303sgs) 6: compute y\u0306t = \u03d5(xt) Tu\u0303t 7: predict y\u0303t = \u03d5(xt) Tw\u0303t = y\u0306t \u2212 h(y\u0306t), observe gt\n8: compute \u03c4\u0303t,t using KORS (Eq. 4) 9: compute p\u0303t = max{min{\u03b2\u03c4\u0303t,t, 1}, \u03b3} 10: draw zt \u223c B(p\u0303t) 11: update A\u0303t = A\u0303t\u22121 + \u03b7tztgtg T\nt\n12: end for\nperformance and regret. Alg. 3 runs KORS as a black-box estimating RLS \u03c4\u0303t, that are then used to sketch the original matrix At with a matrix A\u0303t = \u2211t s=1 \u03b7tztgtg T t , where at each step we add the current gradient gtg T t only if the coin flip zt succeeded. Unlike KORS, the elements added to A\u0303t are not weighted, and the probabilities p\u0303t used for the coins zt are chosen as the maximum between \u03c4\u0303t,t, and a parameter 0 \u2264 \u03b3 \u2264 1. Let Rt be the unweighted counterpart of St, that is [Rt]i,j = 0 if [St]i,j = 0 and [Rt]i,j = 1 if [St]i,j 6= 0. Then we can efficiently compute the coefficients b\u0303t and predictions y\u0303t as follows.\nLemma 4. LetEt = R T tKtRt+\u03b1I be an auxiliary matrix, then all the components b\u0303i = [b\u0303t]i used in Alg. 3 can be computed as\ng\u0307i \u221a \u03b7i ( y\u0303i \u2212\n\u03b1h(y\u0306i)\nki,i \u2212 k T [i\u22121],iRi\u22121E \u22121 i\u22121Ri\u22121k[i\u22121],i\n\u2212 1 \u03b7i\n) .\nThen we can compute\ny\u0306t = 1\n\u03b1\n( kT[t\u22121],tDt\u22121bt\u22121\n\u2212 kT[t\u22121],tDt\u22121Rt\u22121E\u22121t\u22121Rt\u22121Kt\u22121bt\u22121 ) .\nNote that since the columns in Rt are selected without weights, (RTtKtRt + \u03b1I) \u22121 can be updated efficiently using block inverse updates, and only when A\u0303t changes. While the specific reason for choosing the unweighted sketch A\u0303t instead of the weighted version A It t used in KORS is discussed further in Sect. 6, the following corollary shows that A\u0303t is as accurate as A It t in approximating At up to the smallest sampling probability p\u0303 \u03b3 t . Corollary 1. Let p\u0303\u03b3min = min T t=1 p\u0303 \u03b3 t . Then w.h.p., we have\n(1\u2212 \u03b5)p\u0303minAt p\u0303minAItt A\u0303t.\nWe can now state the main result of this section. Since for SKETCHED-KONS we are interested not only in regret\nminimization, but also in space and time complexity, we do not consider the case \u03c3 = 0, because when the function does not have any curvature, standard GD already achieves\nthe optimal regret of O( \u221a T ) (Zhu & Xu, 2015) while requiring onlyO(t) space and time per iteration. Theorem 3. For any sequence of losses \u2113t satisfying Asm. 1-2, let \u03c3 = mint \u03c3t and \u03c4min = min T t=1 \u03c4 t,t. When\n\u03b7t \u2265 \u03c3 > 0 for all t, \u03b1 \u2264 \u221a T , \u03b2 \u2265 3 log(T/\u03b4)/\u03b52, if we set \u03b7t = \u03c3 then w.p. 1\u2212 \u03b4 the regret of Alg. 3 satisfies\nR\u0303T \u2264 \u03b1\u2016w\u2217\u20162 + 2 dTeff\n( \u03b1/(\u03c3L2) ) log(2\u03c3L2T )\n\u03c3max{\u03b3, \u03b2\u03c4min} , (5)\nand the algorithm runs in O(dteff(\u03b1)2 + t2\u03b32) time and O(dteff(\u03b1)2 + t2\u03b32) space complexity for each iteration t.\nProof sketch: Given these guarantees, we need to bound RG and RD . Bounding RD is straightforward, since by construction SKETCHED-KONS adds at most \u03b7tgtg T\nt\nto A\u0303t at each step. To bound RG instead, we must take into account that an unweighted A\u0303t = \u03a6tRtR T t\u03a6 T t + \u03b1I can be up to p\u0303min distant from the weighted \u03a6tStS T t\u03a6 T t for which we have guarantees. Hence the max{\u03b3, \u03b2\u03c4min} term appearing at the denominator."}, {"heading": "6. Discussion", "text": "Regret guarantees. From Eq. 5 we can see that when \u03c4min is not too small, setting \u03b3 = 0 we recover the guarantees of exact KONS. Since usually we do not know \u03c4min, we can choose to set \u03b3 > 0, and as long as \u03b3 \u2265 1/ polylogT , we preserve a (poly)-logarithmic regret.\nComputational speedup. The time required to compute k[t\u22121],t, kt,t, and k T\n[t\u22121],tDt\u22121bt\u22121 gives a minimumO(t) per-step complexity. Note thatKt\u22121bt\u22121 can also be computed incrementally in O(t) time. Denoting the size of the dictionary at time t as Bt = O\u0303(deff(\u03b1)t + t\u03b3), computing [b\u0303t]i and k T [t\u22121],tDt\u22121Rt\u22121E \u22121 t\u22121Rt\u22121Kt\u22121bt\u22121 requires an additionalO(B2t ) time. When \u03b3 \u2264 dteff(\u03b1)/t, each iteration takes O(dteff(\u03b1)2) to compute \u03c4\u0303t,t incrementally using KORS, O(dteff(\u03b1)2) time to update A\u0303\u22121t and O(dteff(\u03b1)2) time to compute [bt]t. When \u03b3 > d t eff(\u03b1)/t, each iteration still takes O(dteff(\u03b1)2) to compute \u03c4\u0303t,t using KORS and O(t2\u03b32) time to update the inverse and compute [bt]t. Therefore, in the case when \u03c4min is not too small, our runtime is of the order O(dteff(\u03b1)2 + t), which is almost as small as the O(t) runtime of GD but with the advantage of a second-order method logarithmic regret. Moreover, when \u03c4min is small and we set a large \u03b3, we can trade off a 1/\u03b3 increase in regret for a \u03b32 decrease in space and time complexity when compared to exact KONS (e.g., setting \u03b3 = 1/10 would correspond to a tenfold increase in regret, but a hundred-fold reduction in computational complexity).\nAsymptotic behavior. Notice however, that space and time complexity, grow roughly with a term \u2126(tmints=1 p\u0303s) \u223c \u2126(tmax{\u03b3, \u03b2\u03c4min}), so if this quantity does not decrease over time, the computational cost of SKETCHED-KONS will remain large and close to exact KONS. This is to be expected, since SKETCHED-KONS must always keep an accurate sketch in order to guarantee a logarithmic regret bound. Note that Luo et al. (2016) took an opposite approach for LOCO, where they keep a fixed-size sketch but possibly pay in regret, if this fixed size happens to be too small. Since a non-logarithmic regret is achievable simply running vanilla GD, we rather opted for an adaptive sketch at the cost of space and time complexity. In batch optimization, where \u2113t does not change over time, another possibility is to stop updating the solution once \u03c4min becomes too small. When Hs is the Hessian of \u2113 in ws, then the quantity gTtH \u22121 t gt, in the context of Newton\u2019s method, is called Newton decrement and it corresponds up to constant factors to \u03c4min. Since a stopping condition based on Newton\u2019s decrement is directly related to the near-optimality of the current wt (Nesterov & Nemirovskii, 1994), stopping when \u03c4min is small also provides guarantees about the quality of the solution.\nSampling distribution. Note that although \u03b3 > 0 means that all columns have a small uniform chance of being selected for inclusion in A\u0303t, this is not equivalent to uniformly sampling columns. It is rather a combination of a RLS-based sampling to ensure that columns important to reconstruct At are selected and a threshold on the probabilities to avoid too much variance in the estimator.\nBiased estimator and results in expectation. The random approximation A\u0303t is biased, since E[\u03a6tRtR T t\u03a6 T t ] = \u03a6t Diag({\u03c4 t,t})\u03a6 T t 6= \u03a6t\u03a6 T t . Another option would be to use a weighted and unbiased A\u0303\u2032t = \u2211t s=1 \u03b7szs/p\u0303sgsg T s approximation, used in KORS and a common choice in matrix approximation methods, see e.g., Alaoui & Mahoney, 2015. Due to its unbiasedness, this variant would automatically achieve the same logarithmic regret as exact KONS in expectation (similar to the result obtained by Luo et al., 2016, using Gaussian random projection in LOCO). While any unbiased estimator, e.g., uniform sampling of gt, would achieve this result, RLS-based sampling already provides strong reconstruction guarantees sufficient to bound RG. Nonetheless, the weights 1/p\u0303s may cause large variations in A\u0303t over consecutive steps, thus leading to a large regret RD in high probability.\nLimitations of dictionary learning approaches and open problems. From the discussion above, it appears that a weighted, unbiased dictionary may not achieve highprobability logarithmic guarantee because of the high variance coming from sampling. On the other hand, if we want to recover the regret guarantee, we may have to pay for it\nwith a large dictionary. This may actually be due to the analysis, the algorithm, or the setting. An important property of the dictionary learning approach used in KORS is that it can only add but not remove columns and potentially re-weight them. Notice that in the batch setting (Alaoui & Mahoney, 2015; Calandriello et al., 2017), the sampling of columns does not cause any issue and we can have strong learning guarantees in high probability with a small dictionary. Alternative sketching methods such as Frequent Directions (FD, Ghashami et al., 2016a) do create new atoms as learning progresses. By restricting to composing dictionaries from existing columns, we only have the degree of freedom of the weights of the columns. If we set the weights to have an unbiased estimate, we achieve an accurate RG but suffer a huge regret in RD. On the other hand, we can store the columns unweighted to have small RD but large RG. This could be potentially fixed if we knew how to remove less important columns from dictionary to gain some slack in RD.\nWe illustrate this problem with following simple scenario. The adversary always presents to the learner the same point x (with associated \u03c6), but for the loss it alternates between \u21132t(wt) = (C \u2212 \u03c6Twt)2 on even steps and \u21132t+1(wt) = (\u2212C \u2212 \u03c6Twt)2 on odd steps. Then, \u03c3t = \u03c3 = 1/(8C2), and we have a gradient that always points in the same \u03c6 direction, but switches sign at each step. The optimal solution in hindsight is asymptoticallyw = 0 and let this be also our starting point w0. We also set \u03b7t = \u03c3, since this is what ONS would do, and \u03b1 = 1 for simplicity.\nFor this scenario, we can compute several useful quantities in closed form, in particular, RG and RD,\nRG \u2264\nT\u2211\nt=1\ng\u03072t\u2211 t\ns=1 g\u03072s\u03c3 + \u03b1\n\u2264\nT\u2211\nt=1\nC2\nC2\u03c3t+ \u03b1 \u2264 O(log T ),\nRD = \u2211t\ns=1 (\u03b7t \u2212 \u03c3)(w\nT tgt) 2 = 0.\nNote that although the matrixAt is rank 1 at each time step, vanilla ONS does not take advantage of this easy data, and would store it all with a O(t2) space in KOCO. As for the sketched versions of ONS, sketching using FD (Luo et al., 2016) would adapt to this situation, and only store a single copy of gt = g, achieving the desired regret with a much smaller space. Notice that in this example, the losses \u2113t are effectively strongly convex, and even basic gradient descent with a stepsize \u03b7t = 1/t would achieve logarithmic regret (Zhu & Xu, 2015) with even smaller space. On the other hand, we show how the dictionary-based sketching has difficulties in minimizing the regret bound from Prop. 1 in our simple scenario. In particular, consider an arbitrary (possibly randomized) algorithm that is allowed only to reweight atoms in the dictionary and not to create new ones (as FD). In our example, this translates to choosing a schedule of weights ws\nand set A\u0303t = \u2211t s=1 ws\u03c6s\u03c6s = Wt\u03c6\u03c6 with total weight\nW = WT = \u2211T\ns=1 ws and space complexity equal to the number of non-zero weights B = |{ws 6= 0}|. We can show that there is no schedule for this specific class of algorithms with good performance due to the following three conflicting goals.\n(1) To mantain RG small, \u2211t\ns=1 ws should be as large as possible, as early as possible.\n(2) To mantain RD small, we should choose weights wt > 1 as few times as possible, since we accumulate max{wt \u2212 1, 0} regret every time.\n(3) To mantain the space complexity small, we should\nchoose only a few wt 6= 0. To enforce goal (3), we must choose a schedule with no more than B non-zero entries. Given the budget B, to satisfy goal (2) we should use all the B budget in order to exploit as much as possible the max{wt \u2212 1, 0} in RD, or in other words we should use exactly B non-zero weights, and none of these should be smaller than 1. Finally, to minimize RG we should raise the sum \u2211t\ns=1 ws as quickly as possible, settling on a schedule where w1 = W \u2212 B and ws = 1 for all the otherB weights. It easy to see that if we want logarithmic RG, W needs to grow as T , but doing so with a logarithmic B would make RD = T \u2212 B = \u2126(T ). Similarly, keeping W = B in order to reduce RD would increase RG. In particular notice, that the issue does not go away even if we know the RLS perfectly, because the same reasoning applies. This simple example suggests that dictionary-based sketching methods, which are very successful in batch scenarios, may actually fail in achieving logarithmic regret in online optimization.\nThis argument raises the question on how to design alternative sketching methods for the second-order KOCO. A first approach, discussed above, is to reduce the dictionary size dropping columns that become less important later in the process, without allowing the adversary to take advantage of this forgetting factor. Another possibility is to deviate from the ONS approach and RD + RG regret decomposition. Finally, as our counterexample in the simple scenario hints, creating new atoms (either through projection or merging) allows for better adaptivity, as shown by FD (Ghashami et al., 2016a) based methods in LOCO. However, the kernelization of FD does not appear to be straighforward. The most recent step in this direction (in particular, for kernel PCA) is only able to deal with finite feature expansions (Ghashami et al., 2016b) and therefore its application to kernels is limited.\nAcknowledgements The research presented was supported by\nFrench Ministry of Higher Education and Research, Nord-\nPas-de-Calais Regional Council and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and\nBoB (n.ANR-16-CE23-0003)"}, {"heading": "A. Preliminary results", "text": "We begin with a generic linear algebra identity that is be used throughout our paper. Proposition 2. For anyX \u2208 Rn\u00d7m matrix and \u03b1 > 0,\nXXT(XXT + \u03b1I)\u22121 = X(XTX+ \u03b1I)\u22121XT\nand\n(XXT + \u03b1I)\u22121 = 1\n\u03b1 \u03b1I(XXT + \u03b1I)\u22121\n= 1\n\u03b1 (XXT \u2212XXT + \u03b1I)(XXT + \u03b1I)\u22121\n= 1\n\u03b1 (I\u2212XXT(XXT + \u03b1I)\u22121)\n= 1\n\u03b1 (I\u2212X(XTX+ \u03b1I)\u22121XT).\nProposition 3. For any matrix or linear operatorX, if a selection matrix S satisfies\n\u2016(XXT + \u03b1I)\u22121/2(XXT \u2212XSSTXT)(XXT + \u03b1I)\u22121/2\u2016 \u2264 \u03b5,\nwe have\n(1\u2212 \u03b5)XtXTt \u2212 \u03b5\u03b1I XtStSTtXTt (1 + \u03b5)XtXTt + \u03b5\u03b1I.\nProposition 4. LetKt = U\u039bU T and\u03a6t = V\u03a3U T, then\n\u2016(\u03a6t\u03a6Tt + \u03b1I)\u22121/2\u03a6t(I\u2212 SsSTs)\u03a6Tt (\u03a6t\u03a6Tt + \u03b1I)\u22121/2\u2016 = \u2016(\u03a3\u03a3T + \u03b1I)\u22121/2\u03a3UT(I\u2212 SsSTs)U\u03a3T(\u03a3\u03a3T + \u03b1I)\u22121/2\u2016 = \u2016(\u039b+ \u03b1I)\u22121/2\u039b1/2UT(I\u2212 SsSTs)U(\u039b1/2)T(\u039b+ \u03b1I)\u22121/2\u2016 = \u2016(Kt + \u03b1I)\u22121/2K1/2t (I\u2212 SsSTs)K1/2t (Kt + \u03b1I)\u22121/2\u2016.\nWe also use the following concentration inequality for martingales. Proposition 5 (Tropp, 2011, Thm. 1.2). Consider a matrix martingale {Yk : k = 0, 1, 2, . . .} whose values are selfadjoint matrices with dimension d and let {Xk : k = 1, 2, 3, . . .} be the difference sequence. Assume that the difference sequence is uniformly bounded in the sense that\n\u2016Xk\u20162 \u2264 R almost surely for k = 1, 2, 3, . . . .\nDefine the predictable quadratic variation process of the martingale as\nWk :=\nk\u2211\nj=1\nE [ X2j \u2223\u2223\u2223 {Xs}j\u22121s=0 ] , for k = 1, 2, 3, . . . .\nThen, for all \u03b5 \u2265 0 and \u03c32 > 0,\nP ( \u2203k \u2265 0 : \u2016Yk\u20162 \u2265 \u03b5 \u2229 \u2016Wk\u2016 \u2264 \u03c32 ) \u2264 2d \u00b7 exp { \u2212 \u03b5 2/2\n\u03c32 +R\u03b5/3\n} .\nProposition 6 (Calandriello et al., 2017, App.D.4). Let {zs}ts=1 be independent Bernoulli random variables, each with success probability ps, and denote their sum as d = \u2211t s=1 ps \u2265 1. Then,3\nP\n( t\u2211\ns=1\nzs \u2265 3d ) \u2264 exp{\u22123d(3d\u2212 (log(3d) + 1))} \u2264 exp{\u22122d}\n3This is a simple variant of Chernoff bound where the Bernoulli random variables are not identically distributed."}, {"heading": "B. Proofs for Section 3", "text": "Proof of Lem. 1. We begin by applying the definition of ut+1 and collecting A \u22121 t , which can always be done since, for \u03b1 > 0,At is invertible,\nut+1 = wt \u2212A\u22121t gt = A\u22121t (Atwt \u2212 gt). We focus now on the last term and use the definition ofAt,\nAtwt \u2212 gt = At\u22121wt + \u03b7tgtgTtwt \u2212 gt = At\u22121ut \u2212At\u22121rt + ( \u221a \u03b7tg T twt \u2212 1/ \u221a \u03b7t)\u03c6t.\nLooking at At\u22121rt and using the assumption g\u0307t 6= 0,\nAt\u22121rt = h(\u03c6Ttut)\n\u03c6TtA \u22121 t\u22121\u03c6t\nAt\u22121A \u22121 t\u22121\u03c6t\n= h(\u03c6Ttut)\n\u03c6TtA \u22121 t\u22121\u03c6t g\u03072t \u03b7t g\u03072t \u03b7t \u03c6t\n= g\u0307t \u221a \u03b7th(\u03c6 T tut)\n\u03c6 T tA \u22121 t\u22121\u03c6t\n\u03c6t.\nPutting together all three terms, and using the fact that gTtwt = g\u0307t\u03c6twt = g\u0307ty\u0302t and denoting bt = [bt]t we have\nut+1 = A \u22121 t (Atwt \u2212 gt)\n= A\u22121t (At\u22121ut + bt\u03c6t) = A\u22121t (At\u22121(wt\u22121 \u2212A\u22121t\u22121gt\u22121) + bt\u03c6t) = A\u22121t (At\u22121wt\u22121 \u2212 gt\u22121 + bt\u03c6t) = A\u22121t (At\u22122wt\u22122 \u2212 gt\u22122 + bt\u22121\u03c6t\u22121 + bt\u03c6t) = A\u22121t (A0w0 + \u2211t\ns=1 bs\u03c6s).\nProof of Lem. 2. Throughout this proof, we make use of the linear algebra identity from Prop. 2. We begin with the reformulation of [bt]t. In particular, the only term that we need to reformulate is\n\u03c6tA \u22121 t\u22121\u03c6t = \u03c6t(\u03a6t\u22121\u03a6\nT t\u22121 + \u03b1I) \u22121\u03c6t\n= 1\n\u03b1 \u03c6t(I\u2212\u03a6t\u22121(\u03a6\nT t\u22121\u03a6t\u22121 + \u03b1I) \u22121\u03a6 T t\u22121)\u03c6t\n= 1\n\u03b1 (\u03c6\nT t\u03c6t \u2212 \u03c6 T t\u03a6t\u22121(\u03a6 T t\u22121\u03a6t\u22121 + \u03b1I) \u22121\u03a6 T t\u22121\u03c6t)\n= 1\n\u03b1 (kt,t \u2212 k\nT [t\u22121],t(Kt\u22121 + \u03b1I) \u22121k[t\u22121],t).\nFor yt, we have\nyt = \u03c6 T tut = \u03c6 T tA \u22121 t\u22121\u03a6t\u22121bt\u22121\n= \u03c6Tt (\u03a6t\u22121\u03a6 T t\u22121 + \u03b1I) \u22121\u03a6t\u22121bt\u22121 = 1\n\u03b1 \u03c6Tt (I\u2212\u03a6t\u22121(\u03a6\nT t\u22121\u03a6t\u22121 + \u03b1I) \u22121\u03a6 T t\u22121)\u03a6t\u22121bt\u22121\n= 1\n\u03b1 \u03c6Tt\u03a6t\u22121Dt\u22121(bt\u22121 \u2212 (Kt\u22121 + \u03b1I)\u22121Kt\u22121bt\u22121)\n= 1\n\u03b1 kT[t\u22121],tDt\u22121(bt\u22121 \u2212 (Kt\u22121 + \u03b1I)\u22121Kt\u22121bt\u22121).\nProof of Lem. 3. We prove the lemma for a generic kernel K and kernel matrix KT . Then, Lem. 3 simply follows by applying the proof to K andKT . From the definition of \u03c4t,t we have\nT\u2211\nt=1\n\u03c4t,t = T\u2211\nt=1\n\u03c6Tt (\u03a6t\u03a6 T t + \u03b1I) \u22121\u03c6t =\nT\u2211\nt=1\n(\u03c6Tt / \u221a \u03b1) (\u03a6t\u03a6 T t /\u03b1+ I) \u22121 (\u03c6t/ \u221a \u03b1) \u2264 log(Det(\u03a6T\u03a6TT /\u03b1+ I)),\nwhere the last passage is proved by Hazan et al. (2006). Using Sylvester\u2019s determinant identity,\nDet(\u03a6T\u03a6 T T /\u03b1+ I) = Det(\u03a6 T T\u03a6T /\u03b1+ I) =\nT\u220f\nt=1\n(\u03bbt/\u03b1+ 1),\nwhere \u03bbt are the eigenvalues of\u03a6 T T\u03a6T = KT . Then,\nT\u2211\nt=1\n\u03c4t,t \u2264 log (\u220fT\nt=1 (\u03bbt/\u03b1+ 1)\n) =\n\u2211T t=1 log(\u03bbt/\u03b1+ 1).\nWe can decompose this as\nT\u2211\nt=1\nlog(\u03bbt/\u03b1+ 1) =\nT\u2211\nt=1\nlog(\u03bbt/\u03b1+ 1)\n( \u03bbt/\u03b1+ 1\n\u03bbt/\u03b1+ 1\n)\n=\nT\u2211\nt=1\nlog(\u03bbt/\u03b1+ 1) \u03bbt/\u03b1\n\u03bbt/\u03b1+ 1 +\nT\u2211\nt=1\nlog(\u03bbt/\u03b1+ 1)\n\u03bbt/\u03b1+ 1\n\u2264 log(\u2016KT \u2016/\u03b1+ 1) T\u2211\nt=1\n\u03bbt \u03bbt + \u03b1 +\nT\u2211\nt=1\nlog(\u03bbt/\u03b1+ 1)\n\u03bbt/\u03b1+ 1\n\u2264 log(\u2016KT \u2016/\u03b1+ 1)dTeff(\u03b1) + T\u2211\nt=1\n(\u03bbt/\u03b1+ 1)\u2212 1 \u03bbt/\u03b1+ 1\n= log(\u2016KT \u2016/\u03b1+ 1)dTeff(\u03b1) + dTeff(\u03b1),\nwhere the first inequality is due to \u2016KT\u2016 \u2265 \u03bbt for all t and the monotonicity of log(\u00b7), and the second inequality is due to log(x) \u2264 x\u2212 1.\nProof of Thm 1. We need to bound RT (w \u2217), and we use Prop. 1. For RD nothing changes from the parametric case, and we use Asm. 1 and the definition of the set S to bound\nRD = \u2211T\nt=1 (\u03b7t \u2212 \u03c3t)g\u03072t (\u03c6Tt (wt \u2212w))2 \u2264 \u2211T t=1 (\u03b7t \u2212 \u03c3)L2(|\u03c6Ttwt|+ |\u03c6Ttw|)2 \u2264 4L2C2 \u2211T t=1 (\u03b7t \u2212 \u03c3).\nFor RG, we reformulate\n\u2211T t=1 gTtA \u22121 t gt = \u2211T t=1 \u03b7t \u03b7t gTtA \u22121 t gt = \u2211T t=1 1 \u03b7t \u03c6 T tA \u22121 t \u03c6t\n\u2264 1 \u03b7T \u2211T t=1 \u03c6 T tA \u22121 t \u03c6t = 1 \u03b7T \u2211T t=1 \u03c4 t,t = donl(\u03b1)/\u03b7T \u2264 d T eff(\u03b1) \u03b7T (1 + log(\u2016KT \u2016/\u03b1+ 1)),\nwhere d T\neff(\u03b1) andKT are computed using the rescaled kernel K. Let us remind ourselves the definitionD = Diag ( {g\u0307t\u221a\u03b7t}Tt=1 ) . Since \u03b7t 6= 0 and g\u0307t 6= 0 for all t, D is invertible and we have \u03bbmin(D \u22122) = minTt=1 1/(g\u0307 2 t \u03b7t) \u2265 1/(L2\u03b71). For simplicity, we assume \u03b71 = \u03c3, leaving the case \u03b71 = 1/1 = 1 as a\nspecial case. We derive\nd T\neff(\u03b1) = Tr(KT (KT + \u03b1I) \u22121)\n= Tr(DKTD(DKTD+ \u03b1DD \u22122D)\u22121) = Tr(DKTDD \u22121(KT + \u03b1D \u22122)\u22121D\u22121) = Tr(KT I(KT + \u03b1D \u22122)\u22121D\u22121D) = Tr(KT (KT + \u03b1D \u22122)\u22121) \u2264 Tr(KT (KT + \u03b1\u03bbmin(D\u22122)I)\u22121) \u2264 Tr ( KT ( KT + \u03b1 \u03c3L2 I )\u22121) = dTeff ( \u03b1/(\u03c3L2) ) .\nSimilarly,\nlog(\u2016KT \u2016/\u03b1+ 1) \u2264 log(Tr(KT )/\u03b1+ 1) \u2264 log(\u03c3L2 Tr(Kt)/\u03b1+ 1) \u2264 log(\u03c3L2T/\u03b1+ 1) \u2264 log(2\u03c3L2T/\u03b1),\nsince Tr(Kt) = \u2211T t=1 kt,t = \u2211T t=1 \u03c6 T t\u03c6t \u2264 \u2211T t=1 1 = T ."}, {"heading": "C. Proofs for Section 4", "text": "Proof of Thm. 2. We derive the proof for a generic K with its induced \u03c6t = \u03d5(xt) and Kt. Then, SKETCHED-KONS (Alg. 3) applies this proof to the rescaled \u03c6t andKt. Our goal is to prove that Alg. 2 generates accurate and small dictionaries at all time steps t \u2208 [T ]. More formally, a dictionary Is is \u03b5-accurate w.r.t.Dt when\n\u2016(\u03a6t\u03a6Tt + \u03b1I)\u22121/2\u03a6t(I\u2212 SsSTs)\u03a6Tt (\u03a6t\u03a6Tt + \u03b1I)\u22121/2\u2016 = \u2016(Kt + \u03b1I)\u22121/2K1/2t (I\u2212 SsSTs)K1/2t (Kt + \u03b1I)\u22121/2\u2016 \u2264 \u03b5,\nwhere we used Prop. 4 to move from feature to primal space.\nWe also introduce the projection operators,\nvt,i :=((Kt + \u03b1I) \u22121Kt) 1/2et,i\nPt :=(Kt + \u03b1I) \u22121/2K 1/2 t K 1/2 t (Kt + \u03b1I)\n\u22121/2 = t\u2211\ns=1\nvt,sv T t,s = VtV T t\nP\u0303t :=(Kt + \u03b1I) \u22121/2K 1/2 t StS T tK 1/2 t (Kt + \u03b1I) \u22121/2 =\nt\u2211\ns=1\nzt p\u0303t vt,sv T t,s = VtStS T tV T t ,\nwhere the zt variables are the {0, 1} random variables sampled by Alg. 2. Note that with this notation we have\n\u2016vt,ivTt,i\u2016 = \u2016((Kt + \u03b1I)\u22121Kt)1/2et,ieTt,i(Kt(Kt + \u03b1I)\u22121)1/2\u2016 = eTt,i((Kt + \u03b1I) \u22121Kt) 1/2(Kt(Kt + \u03b1I) \u22121)1/2et,i = e T t,i(Kt + \u03b1I) \u22121Ktet,i = \u03c4t,i.\nWe can now formalize the event \u201csome of the guarantees of Alg. 2 do not hold\u201d and bound the probability of this event. In particular, let\nYt := P\u0303t \u2212Pt = t\u2211\ns=1\n( zt p\u0303t \u2212 1 ) vt,sv T t,s.\nWe want to show\nP ( \u2203t \u2208 [T ] : \u2016Yt\u2016 \u2265 \u03b5\ufe38 \ufe37\ufe37 \ufe38\nAt\n\u222a \u2211t\ns=1 zt \u2265 3\u03b2dtonl(\u03b1) \ufe38 \ufe37\ufe37 \ufe38 Bt\n) \u2264 \u03b4,\nwhere event At refers to the case when the intermediate dictionary It fails to accurately approximate Kt at some step t \u2208 [T ] and event Bt considers the case when the memory requirement is not met (i.e., too many columns are kept in a dictionary It at a certain time t \u2208 [T ]). Step 1: Splitting the problem. We can conveniently decompose the previous joint (negative) event into two separate conditions as\nP\n( T\u22c3\nt=1\n( At \u222aBt )) = P\n({ T\u22c3\nt=1\nAt\n}) + P ({ T\u22c3\nt=1\nBt\n}) \u2212 P ({ T\u22c3\nt=1\nAt\n} \u2229 { T\u22c3\nt=1\nBt\n})\n= P\n({ T\u22c3\nt=1\nAt\n}) + P   { T\u22c3\nt=1\nBt\n} \u2229 { T\u22c3\nt=1\nAt\n}\u2201  = P ({ T\u22c3\nt=1\nAt\n}) + P ({ T\u22c3\nt=1\nBt\n} \u2229 { T\u22c2\nt=1\nA\u2201t\n})\n= P\n({ T\u22c3\nt=1\nAt\n}) + P ( T\u22c3\nt=1\n{ Bt \u2229 { T\u22c2\nt\u2032=1\nA\u2201t\u2032\n}}) .\nApplying this reformulation and a union bound, we obtain\nP ( \u2203t \u2208 [T ] : \u2016Yt\u2016 \u2265 \u03b5 \u222a \u2211t s=1 zt \u2265 3\u03b2dtonl(\u03b1) )\n\u2264 T\u2211\nt=1\nP (\u2016Yt\u2016 \u2265 \u03b5) + T\u2211\nt=1\nP\n( t\u2211\ns=1\nzs \u2265 3\u03b2dtonl(\u03b1) \u2229 {\u2200 t\u2032 \u2208 {1, . . . , t} : \u2016Yt\u2016 \u2264 \u03b5} ) .\nTo conclude the proof, we show in Step 2 and 3, that each of the failure events happens with probability less than \u03b42T . Step 2: Bounding the accuracy. We first point out that dealing with Yt is not trivial since the process {Yt}Tt=1 is composed by matrices of different size, that cannot be directly compared. Denote with Sts the matrix constructed by (1) taking Ss and adding t\u2212 s rows of zeros to its bottom to extend it, and (2) adding t\u2212 s indicator columns et,i for all i > s. We begin by reformulatingYt as a random processY t 0,Y t 1, . . . ,Y t t with differencesX t s defined as\nXts = ( zs p\u0303s \u2212 1 ) vt,sv T t,s, Y t k = k\u2211\ns=1\nXts =\nk\u2211\ns=1\n( zs p\u0303s \u2212 1 ) vt,sv T t,s = Vt(S t k(S t k) T \u2212 I)VTt .\nWe introduce the freezing probabilities\nps = p\u0303s \u00b7 I{\u2016Yts\u22121\u2016 < \u03b5}+ 1 \u00b7 I{\u2016Yts\u22121\u2016 \u2265 \u03b5}\nand the associated process Y t\ns based on the coin flips zs performed using ps instead of the original p\u0303s as in Alg. 2. In other words, this process is such that if at any time s\u2212 1 the accuracy condition is not met, then for all steps from s on the algorithm stops updating the dictionary. We also defineYt = Y t t. Then we have\nP (\u2016Yt\u2016 \u2265 \u03b5) \u2264 P ( \u2016Yt\u2016 \u2265 \u03b5 ) ,\nso we can simply bound the latter to bound the former. To show the usefulness of the freezing process, consider the step s where the process froze, or more formally define s as the step where \u2016Yts\u2016 < \u03b5 and \u2016Yts+1\u2016 \u2265 \u03b5. Then for all s \u2264 s, we can combine Prop. 3, the definition ofVt, and the guarantee that \u2016Yts\u2016 < \u03b5 to obtain\n\u03a6sSsS T s\u03a6s \u03a6tSts(Sts)T\u03a6t \u03a6t\u03a6Tt + \u03b5(\u03a6t\u03a6Tt + \u03b3I),\nwhere in the first inequality we used the fact that Sts is simply obtained by bordering Ss. Applying the definition of p\u0303s, when p\u0303s < 1 we have\nps = p\u0303s = \u03b2\u03c4\u0303s,s = \u03b2(1 + \u03b5)\u03c6 T s(\u03a6sSsS T s\u03a6 T s + \u03b3I) \u22121\u03c6s\n\u2265 \u03b2(1 + \u03b5)\u03c6Ts(\u03a6s\u03a6Ts + \u03b5(\u03a6t\u03a6Tt + \u03b3I) + \u03b3I)\u22121\u03c6s = \u03b2(1 + \u03b5) 1\n1 + \u03b5 \u03c6Ts(\u03a6t\u03a6 T t + \u03b3I) \u22121\u03c6s,= \u03b2\u03c4t,s,\nwhich shows that our estimates of RLS are upper bounds on the true values.\nFrom this point onwards we focus on a specific t, and omit the index fromY t s,X t s and vt,s. We can now verify thatYs is a martingale, by showing that Xs is zero mean. Denote with Fk = {Xs}ks=1 the filtration of the process. When ps = 1, either because \u03b2p\u0303s \u2265 1 or because the process is frozen, we have Xs = 0 and the condition is satisfied. Otherwise, we have\nE [ Xs \u2223\u2223 Fs\u22121 ] = E [( zs ps \u2212 1 ) vsv T s \u2223\u2223\u2223\u2223 Fs\u22121 ] = ( E [zs | Fs\u22121] ps \u2212 1 ) vsv T s = ( ps ps \u2212 1 ) vsv T s = 0,\nwhere we use the fact that ps is fixed conditioned on Fs\u22121 and its the (conditional) expectation of zs. Since Yt is a martingale, we can use Prop. 5. First, we find R. Again, when ps = 1 we haveXs = 0 and R \u2265 0. Otherwise,\n\u2225\u2225\u2225\u2225 ( zs ps \u2212 1 ) vsv T s \u2225\u2225\u2225\u2225 \u2264 \u2223\u2223\u2223\u2223 ( zs ps \u2212 1 )\u2223\u2223\u2223\u2223 \u2016vsvTs\u2016 \u2264 1 ps \u03c4t,s \u2264 \u03c4t,s \u03b2\u03c4t,s = 1 \u03b2 := R.\nFor the total variation, we expand\nWt :=\nt\u2211\ns=1\nE [ X 2\ns\n\u2223\u2223\u2223 Fs\u22121 ] = t\u2211\ns=1\nE [( zs ps \u2212 1 )2 \u2223\u2223\u2223\u2223\u2223 Fs\u22121 ] vsv T svsv T s\n=\nt\u2211\ns=1\n( E [ z2s p2s \u2223\u2223\u2223\u2223 Fs\u22121 ] \u2212 E [ 2 zs ps \u2223\u2223\u2223\u2223 Fs\u22121 ] + 1 ) vsv T svsv T s\n=\nt\u2211\ns=1\n( E [ zs\np2s\n\u2223\u2223\u2223\u2223 Fs\u22121 ] \u2212 1 ) vsv T svsv T s = t\u2211\ns=1\n( E [ zs\np2s\n\u2223\u2223\u2223\u2223 Fs\u22121 ] \u2212 1 ) vsv T svsv T s\n= t\u2211\ns=1\n( 1\nps \u2212 1\n) vsv T svsv T s = t\u2211\ns=1\n( vTsvs\nps \u2212 vTsvs\n) vsv T s = t\u2211\ns=1\n( \u03c4t,s ps \u2212 \u03c4t,s ) vsv T s ,\nwhere we used the fact that z2s = zs and E[zs|Fs\u22121] = ps. We can now bound this quantity as\n\u2225\u2225Wt \u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 t\u2211\ns=1\nE [ X 2\ns \u2223\u2223\u2223 Fs\u22121 ]\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 t\u2211\ns=1\n( \u03c4t,s ps \u2212 \u03c4t,s ) vsv T s \u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225 t\u2211\ns=1\n\u03c4t,s ps vsv T s \u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225 t\u2211\ns=1\n\u03c4t,s \u03b2\u03c4t,s vsv T s \u2225\u2225\u2225\u2225\u2225\n= 1\n\u03b2 \u2225\u2225\u2225\u2225\u2225 t\u2211\ns=1\nvsv T\ns \u2225\u2225\u2225\u2225\u2225 = 1 \u03b2 \u2016VtVTt \u2016 = 1 \u03b2 \u2016Pt\u2016 \u2264 1 \u03b2 := \u03c32.\nTherefore, if we let \u03c32 = 1/\u03b2 and R = 1/\u03b2, we have P (\u2016Yt\u2016 \u2265 \u03b5) \u2264 P ( \u2016Yt\u2016 \u2265 \u03b5 ) = P ( \u2016Yt\u2016 \u2265 \u03b5 \u2229 \u2016Yt\u2016 \u2264 \u03c32 ) + P ( \u2016Yt\u2016 \u2265 \u03b5 \u2229 \u2016Yt\u2016 \u2265 \u03c32 )\n\u2264 P ( \u2016Yt\u2016 \u2265 \u03b5 \u2229 \u2016Yt\u2016 \u2264 \u03c32 ) + P ( \u2016Yt\u2016 \u2265 \u03c32 ) \u2264 2t exp { \u2212\u03b5 2\n2 1 1 \u03b2 (1 + \u03b5/3)\n} + 0 \u2264 2t exp { \u2212\u03b5 2\u03b2\n3\n} \u00b7\nStep 3: Bounding the space. We want to show that\nP\n( t\u2211\ns=1\nzs \u2265 3\u03b2dtonl(\u03b1) \u2229 {\u2200 t\u2032 \u2208 {1, . . . , t} : \u2016Yt\u2016 \u2264 \u03b5} ) .\nAssume, without loss of generality, that for all s \u2208 [t] we have \u03b2\u03c4s,s \u2264 1, and introduce the independent Bernoulli random variables z\u0302s \u223c B(\u03b2\u03c4s,s). Thanks to the intersection with the event {\u2200 t\u2032 \u2208 {1, . . . , t} : \u2016Yt\u2016 \u2264 \u03b5}, we know that all dictionaries Is are \u03b5-accurate, and therefore for all s we have p\u0303s \u2264 \u03b2\u03c4\u0303s,s \u2264 \u03b2\u03c4s,s. Thus z\u0302s stochastically dominates zs and we have\nP\n( t\u2211\ns=1\nzs \u2265 3\u03b2dtonl(\u03b1) \u2229 {\u2200 t\u2032 \u2208 {1, . . . , t} : \u2016Yt\u2016 \u2264 \u03b5} ) \u2264 P ( t\u2211\ns=1\nz\u0302s \u2265 3\u03b2dtonl(\u03b1) ) .\nApplying Prop. 6 to \u2211t s=1 z\u0302s and knowing that \u2211t s=1 pt = \u2211t s=1 \u03b2\u03c4s,s = \u03b2d t onl(\u03b1), we have\nP\n( t\u2211\ns=1\nz\u0302s \u2265 3\u03b2dtonl(\u03b1) ) \u2264 exp{\u22123\u03b2dtonl(\u03b1)(3\u03b2dtonl(\u03b1) \u2212 (log(3\u03b2dtonl(\u03b1)) + 1))} \u2264 exp{\u22122\u03b2dtonl(\u03b1)}.\nAssuming dtonl(\u03b1) \u2265 1, we have that exp{\u22122\u03b2dtonl(\u03b1)} \u2264 exp{\u22122\u03b2} \u2264 exp{\u2212 log((T/\u03b4)2)} \u2264 \u03b42/T 2 \u2264 \u03b4/(2T ) as long as 2\u03b4 \u2264 2 \u2264 T ."}, {"heading": "D. Proofs for Section 5", "text": "Proof of Lemma 4. Through this proof, we make use of the linear algebra identity from Prop. 2. We begin with the refor-\nmulation of b\u0303i = [b\u0303t]i. In particular, the only term that we need to reformulate is\n\u03c6tA\u0303 \u22121 t\u22121\u03c6t = \u03c6t(\u03a6t\u22121Rt\u22121R T t\u22121\u03a6 T t\u22121 + \u03b1I) \u22121\u03c6t\n= 1\n\u03b1 \u03c6t(I\u2212\u03a6t\u22121Rt\u22121(RTt\u22121\u03a6\nT t\u22121\u03a6t\u22121Rt\u22121 + \u03b1I) \u22121RTt\u22121\u03a6 T t\u22121)\u03c6t\n= 1\n\u03b1 (\u03c6\nT t\u03c6t \u2212 \u03c6 T t\u03a6t\u22121Rt\u22121(R T t\u22121\u03a6 T t\u22121\u03a6t\u22121Rt\u22121 + \u03b1I) \u22121RTt\u22121\u03a6 T t\u22121\u03c6t)\n= 1\n\u03b1 (kt,t \u2212 k\nT [t\u22121],tRt\u22121(R T t\u22121Kt\u22121Rt\u22121 + \u03b1I) \u22121RTt\u22121k[t\u22121],t).\nFor y\u0306t, we have\ny\u0306t = \u03c6 T t u\u0303t = \u03c6 T t A\u0303 \u22121 t\u22121\u03a6t\u22121b\u0303t\u22121\n= \u03c6Tt (\u03a6t\u22121Rt\u22121R T t\u22121\u03a6 T t\u22121 + \u03b1I) \u22121\u03a6t\u22121b\u0303t\u22121 = 1\n\u03b1 \u03c6Tt\n( I\u2212\u03a6t\u22121Rt\u22121(RTt\u22121\u03a6 T t\u22121\u03a6t\u22121Rt\u22121 + \u03b1I) \u22121RTt\u22121\u03a6 T t\u22121 ) \u03a6t\u22121b\u0303t\u22121\n= 1\n\u03b1 \u03c6Tt\u03a6t\u22121Dt\u22121\n( b\u0303t\u22121 \u2212Rt\u22121(RTt\u22121Kt\u22121Rt\u22121 + \u03b1I)\u22121RTt\u22121Kt\u22121b\u0303t\u22121 )\n= 1\n\u03b1 kT[t\u22121],tDt\u22121\n( b\u0303t\u22121 \u2212Rt\u22121(RTt\u22121Kt\u22121Rt\u22121 + \u03b1I)\u22121RTt\u22121Kt\u22121b\u0303t\u22121 ) .\nProof of Theorem 3. Since the only thing that changed is the formulation of the At matrix, the bound from Prop. 1 still applies. In particular, we have that the regret R\u0303T of Alg. 3 is bounded as\nR\u0303(w) \u2264\u03b1\u2016w\u20162 A0\n+ \u2211T\nt=1 gTt A\u0303 \u22121 t gt + \u2211T t=1 (wt \u2212w)T(A\u0303t \u2212 A\u0303t\u22121 \u2212 \u03c3tgtgTt )(wt \u2212w).\nFrom Thm. 2, we have that KORS succeeds with high probability. In particular, using the guarantees of the \u03b5-accuracy (1), we can bound for the case \u03b7t = \u03c3 as\ngTt A\u0303 \u22121 t gt = \u03b7t \u03b7t gTt (\u03a6tRtR T t\u03a6 T t + \u03b1I) \u22121gt = 1 \u03b7t \u03c6 T t (\u03a6tRtR T t\u03a6 T t + \u03b1I) \u22121\u03c6t\n= 1\n\u03b7t p\u0303min p\u0303min \u03c6 T t (\u03a6tRtR T t\u03a6 T t + \u03b1I) \u22121\u03c6t = 1 \u03b7t 1 p\u0303min \u03c6 T t\n( 1\np\u0303min \u03a6tRtR\nT t\u03a6 T t + \u03b1I )\u22121 \u03c6t\n\u2264 1 \u03b7t 1 p\u0303min \u03c6 T t\n( \u03a6tStS T t\u03a6 T t + \u03b1I )\u22121 \u03c6t\n\u2264 1 p\u0303min\u03b7t \u03c6 T t ((1\u2212 \u03b5)\u03a6t\u03a6 T t \u2212 \u03b5\u03b1I+ \u03b1I)\u22121\u03c6t = 1\n(1\u2212 \u03b5)\u03c3p\u0303min \u03c6\nT t (\u03a6t\u03a6 T t + \u03b1I) \u22121\u03c6t = \u03c4 t,t (1\u2212 \u03b5)\u03c3p\u0303min ,\nwhere in the first inequality we used the fact that the weight matrix St contains weights such that 1/ \u221a p\u0303min \u2265 1/ \u221a p\u0303t, in the second inequality we used the \u03b5-accuracy, and finally, we used \u03b7t = \u03c3 and the definition of \u03c4 t,t. Therefore,\nRG = \u2211T\nt=1 gTt A\u0303 \u22121 t gt \u2264\n1 (1\u2212 \u03b5)\u03c3p\u0303min \u2211T t=1 \u03c4 t,t \u2264\ndonl(\u03b1)\n(1\u2212 \u03b5)\u03c3max{\u03b2\u03c4min, \u03b3} \u00b7\nTo bound RD, we have\n\u2211T t=1 (wt \u2212w)T(A\u0303t \u2212 A\u0303t\u22121 \u2212 \u03c3tgtgTt )(wt \u2212w) = \u2211T t=1 (wt \u2212w)T (\u03b7tztgtgTt \u2212 \u03c3tgtgTt ) (wt \u2212w)\n\u2264 \u2211T\nt=1 (\u03c3 \u2212 \u03c3t)(gTt (wt \u2212w))2 \u2264 0."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Kernel online convex optimization (KOCO) is a<lb>framework combining the expressiveness of non-<lb>parametric kernel models with the regret guaran-<lb>tees of online learning. First-order KOCO meth-<lb>ods such as functional gradient descent require<lb>onlyO(t) time and space per iteration, and, when<lb>the only information on the losses is their con-<lb>vexity, achieve a minimax optimal O(<lb>\u221a<lb>T ) re-<lb>gret. Nonetheless, many common losses in ker-<lb>nel problems, such as squared loss, logistic loss,<lb>and squared hinge loss posses stronger curvature<lb>that can be exploited. In this case, second-order<lb>KOCOmethods achieveO(log(Det(K))) regret,<lb>which we show scales as<lb>O(deff logT ), where<lb>deff is the effective dimension of the problem and<lb>is usually much smaller than O(<lb>\u221a<lb>T ). The main<lb>drawback of second-order methods is their much<lb>higher O(t2) space and time complexity. In this<lb>paper, we introduce kernel online Newton step<lb>(KONS), a new second-order KOCOmethod that<lb>also<lb>achievesO(deff logT ) regret. To address the<lb>computational complexity of second-order meth-<lb>ods, we introduce a new matrix sketching algo-<lb>rithm for the kernel matrixKt, and show that for<lb>a chosen parameter \u03b3 \u2264 1 our Sketched-KONS<lb>reduces the space and time complexity by a fac-<lb>tor of \u03b3 toO(t2\u03b32) space and time per iteration,<lb>while incurring only 1/\u03b3 times more regret.", "creator": "LaTeX with hyperref package"}}}