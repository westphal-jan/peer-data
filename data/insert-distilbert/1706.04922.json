{"id": "1706.04922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "DSRIM: A Deep Neural Information Retrieval Model Enhanced by a Knowledge Resource Driven Representation of Documents", "abstract": "the possible state - of -... the - art solutions to the vocabulary mismatch in information retrieval ( ir ) mainly aim at leveraging either the relational semantics provided by external resources or the distributional semantics, recently investigated by deep knowledge neural approaches. guided by creating the intuition that the relational semantics might negatively improve the effectiveness of deep neural approaches, we propose the deep domain semantic resource consistency inference model ( dsrim ) that relies on : 1 ) a representation of raw - data that carefully models the relational semantics of text corpus by jointly considering objects and relations formally expressed in describing a real knowledge resource, and 2 ) an end - to - end neural architecture that learns the query - document relevance by silently leveraging the distributional and relational semantics of documents entries and queries. the experimental evaluation carried out on two trec datasets reconstructed from trec terabyte sources and trec cds tracks relying respectively on wordnet and indexed mesh resources, falsely indicates that our model outperforms strictly state - of - the - art semantic and deep neural ir models.", "histories": [["v1", "Thu, 15 Jun 2017 15:24:32 GMT  (1164kb,D)", "https://arxiv.org/abs/1706.04922v1", null], ["v2", "Thu, 27 Jul 2017 12:32:30 GMT  (1160kb,D)", "http://arxiv.org/abs/1706.04922v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["gia-hung nguyen", "laure soulier", "lynda tamine", "nathalie bricon-souf"], "accepted": false, "id": "1706.04922"}, "pdf": {"name": "1706.04922.pdf", "metadata": {"source": "META", "title": "DSRIM: A Deep Neural Information Retrieval Model Enhanced by a Knowledge Resource Driven Representation of Documents", "authors": ["Gia-Hung Nguyen", "Laure Soulier", "Lynda Tamine", "Nathalie Bricon-Souf"], "emails": ["gia-hung.nguyen@irit.fr", "laure.soulier@lip6.fr", "tamine@irit.fr", "nathalie.souf@irit.fr"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Information systems \u2192 Retrieval models and ranking; \u2022Computing methodologies \u2192 Semantic networks; Neural networks;\nKEYWORDS Ad-hoc IR, knowledge resource, semantic document representation, deep neural architecture"}, {"heading": "1 INTRODUCTION", "text": "Tackling the vocabulary mismatch has been a long-standing and major goal in information retrieval (IR). To infer and match discrete word senses within the context of documents and queries being matched, one line of work makes use of hand-labeled external knowledge resources such as linguistic resources and knowledge graphs. In IR, such resources allow to exploit objects and their relations (e.g., synonymy, hyperonymy) within, e.g., query or document expansion [1, 36] to lower the vocabulary mismatch between queries and documents; this is referred to as the relational semantics.\nis is the author\u2019s pre-print version of the work. It is posted here for your personal use, not for redistribution. Please cite the de nitive version which will be published in Proceedings of ICTIR 2017\nAnother line of work a empts to automatically infer hidden word senses from corpora using word collocations by performing dimensionality reduction techniques, such as latent semantic indexing [8] or, more recently, representation learning [21, 29] leading to distributional semantics. e la er was successfully exploited within deep neural networks for supporting search tasks [10, 12, 30]. One of the rst contributions in the eld relies on siamese architectures, opposing queries and documents in a two-branch network [12, 30]. However, these architectures are not yet mature since the learning of a relevance function su ers from several limitations: 1) tackling traditional IR models (e.g., BM25 or language models) remains a di cult task [10\u201312] and 2) learning the relevance function on full text does not allow the network convergence, even though evaluated on search logs of commercial search engines, leading to focus on a query-document title matching [10, 12, 30].\nGuided by the intuition that the relational semantics could complement distributional semantics and the motivation that siamese networks are under-explored in IR [24], we investigate how to leverage both knowledge resources and siamese architecture to perform ad-hoc IR. Speci cally, we rst model in a low-dimensional vector the relational semantics of text by jointly considering objects and relations expressed in knowledge resources. en, we investigate the hypothesis that combining the distributional and the relational representations of text would enhance its representation for a ranking task. To the best of our knowledge, this is one of the rst approach combining distributional and relational semantics in a neural architecture with the goal to enhance document-query ranking. More particularly, our contributions are twofold: \u2022 A Deep Semantic Resource Inference Model (DSRIM) leveraging:\n- An input raw level representation of queries and documents relying on a knowledge resource-driven representation. More particularly, the premise of the la er representation relies on assumptions that a text is a bag of identi ed objects from a knowledge resource, and that semantically similar texts are deemed to entail similar/related objects. To deal with a large number of object-toobject relations, we propose the relation mapping method that aims at projecting pairs in a low-dimensional space of object clusters. Our method is exible since it can be used with any resource providing objects and relations between objects.\n- An end-to-end siamese neural network which learns an enhanced document-ranking function using input vectors combining\nar X\niv :1\n70 6.\n04 92\n2v 2\n[ cs\n.I R\n] 2\n7 Ju\nl 2 01\nboth the distributional and the knowledge resource-driven representations of document/query. \u2022A thorough experimental evaluation aiming at assessing the quality of the knowledge resource-driven representation and the e ectiveness of DSRIM. We use two TREC datasets, namely TREC PubMed CDS and TREC GOV2 Terabyte, and two knowledge resources, respectively MeSH1 and WordNet2. It is worth mentioning that, unlike previous work [12, 30] experimentally evaluated on document titles, our experiments are performed using full-texts.\ne rest of this paper is organized as follows. A er reviewing the related work in Section 2, we motivate and then describe the DSRIM model in Section 3. Section 4 details the experimental protocol. Section 5 discusses the experimental results. Section 6 concludes the paper and introduces perspectives."}, {"heading": "2 RELATEDWORK", "text": ""}, {"heading": "2.1 On the Semantic Representation of Words, Documents, Objects, and Relations.", "text": "e potential of word semantic representations learned through neural approaches has been introduced in [21, 29], opening several perspectives in NLP and IR tasks. Indeed, several work focuses on the representation of sentences [22], documents [16, 32], and also objects and relations expressed in knowledge resources [5, 14, 20, 33]. Within the la er, the main principle consists in learning the representation of objects and relations on the basis of object-relation-object triplets, relying on the assumption that the embedding of object oi should be close to the embedding translation of object oj by relation r , namely oi ' oj + r (TransE model) [5]. Extensions have been proposed considering, e.g., di erent object representations according to the semantic relation type (TransH) [33] or a dynamic mapping between objects and relations constrained by their diversity (TransD) [14]. Moreover, knowledge resources have been used to enhance the distributed representation of words for representing their underlying concepts [9, 18, 37, 38]. Faruqui et al. [9] propose a \u201cretro ing\u201d technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to re ne their associated word embeddings. Other work [37, 38] introduces an end-to-end approach that rather adjusts the objective function of the neural language model by either leveraging the relational and categorical knowledge to learn a higher quality word embeddings (RC-NET model) [37] or extending the CBOW model using prior relational knowledge [38]."}, {"heading": "2.2 On using Knowledge Resources in IR.", "text": "Both general/speci c linguistic bases (e.g., WordNet or UMLS respectively) and large-scale knowledge graphs (e.g., Freebase) represent external resources that o er valuable information about word semantics through objects (e.g., words, entities, or concepts) and their associated relations (e.g., \u201cis-a\u201d, \u201cpart-of\u201d). Based on the use of such resources, a rst line of work in IR aims at increasing the likelihood of term overlap between queries and documents through query expansion [25, 36] or document expansion [1]. Among models expanding queries, Xiong et al. [36] propose two algorithms\n1h ps://www.nlm.nih.gov/mesh/ 2h p://wordnet.princeton.edu\nrelying on the category of terms in FreeBase. While the unsupervised approach estimates the similarity between the category distribution of terms in documents and queries, the supervised approach exploits the ground truth to estimate the in uence of terms. Authors in [25] propose a query expansion technique using terms extracted from multiple sources of information. For each query term, candidate expansion terms in top retrieved documents are ranked by combining their importance in pseudo-relevant documents and their semantic similarity based on their de nition in WordNet. Unlikely, Agirre et al. [1] propose a document expansion technique based on the use of a random walk algorithm identifying from WordNet the most related concepts. e second line of work leverages relations modeled in knowledge resources at the document ranking level [35]. For instance, authors in [35] propose a learning-to-rank algorithm based on objects of knowledge resources that are related to a given pair of query-document."}, {"heading": "2.3 On using Deep Neural Networks in IR.", "text": "Recently, a large amount of work has shown that deep learning approaches are highly e cient in several IR tasks (e.g., text matching [12], question-answering [4]). A rst category of work uses neural models for IR tasks [2, 23, 39] to integrate embeddings in IR relevance functions. e second category of work, closer to our contribution, consists in end-to-end scoring models that learn the relevance of document-query pairs via latent semantic features [11, 12]. For instance, the Deep Semantic Structured Model (DSSM) [12] applies a siamese deep feed-forward network on document and query representations obtained by a word hashing method. e network aims at learning their latent representations and then measuring their relevance score using a cosine similarity. As an extension of the DSSM, Shen et al. [31] use a convolutional-pooling structure, called Convolutional Latent Semantic Model (CLSM). In the same mind, Severyn and Moschi i [30] apply a convolution to learn the optimal representation of short text pairs as well as the similarity function. However, these model parameters are hard to learn, which leads authors to only consider the matching between query-title pairs. Guo et al. [10] also argue that tackling traditional IR models (e.g., BM25 or language models) remains a di cult task. To bypass this limitation, another line of work [10, 19, 26] rather aims at building a local interaction map between inputs, and then uses deep neural networks to learn hierarchical interaction patterns. e DeepMatch model [19] integrates a topic model into a fully connected deep network based on the word interaction matrix while the MatchPyramid model [26] applies a convolution to an interaction matrix estimated on the basis of word representation. Guided by the intuition that interaction matrix is more appropriate for the global matching and lacks of the term importance consideration, authors in [10] model local interactions of query-document terms through occurrence histograms."}, {"heading": "3 DSRIM: DEEP SEMANTIC RESOURCE INFERENCE MODEL", "text": ""}, {"heading": "3.1 Motivation", "text": "e literature review highlights that: 1) plain text and knowledge resources are complementary for both learning distributional representations and enhancing IR e ectiveness [1, 25, 36], and that\n2) neural approaches in IR, and more particularly siamese architectures, have a great potential for ad-hoc search but could still be improved to compete with traditional IR models [10]. In this contribution, we address the problem of bridging the semantic gap in IR by leveraging both deep learning approaches [12, 30] and valid knowledge expressed in knowledge resources [25, 36]. In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource. With this in mind, we investigate the potential of siamese neural architectures, such as DSSM [12], on full-text retrieval. In this paper, we speci cally address the two main following research questions as illustrated in Figure 1:\n\u2022 RQ1: How to model the relational semantics of texts at the raw data level by jointly leveraging objects and their relations expressed in knowledge resources? \u2022 RQ2: How to learn the query-document relevance function by combining the relational and distributional semantics of text in a siamese neural architecture?\nBelow, we detail our contributions w.r.t. each research question."}, {"heading": "3.2 Knowledge Resource driven Representation", "text": "Our aim is to model a text representation that conveys their semantics with respect to a knowledge resource. e premise of this representation relies on two assumptions: (A1) a text is a bag of identi ed objects from a knowledge resource, and (A2) semantically similar texts are deemed to entail similar and related objects.\nFormally, a knowledge resource is built upon a relational graph G = (V ,E) where V is a node set and E is a edge set. Each node vi =< oi ,desci , > includes an object oi (e.g., word, entity) and its textual label desci (e.g., preferred entry). Each object oi is associated to a distributional representation xdi (e.g., its ParagraphVector [2] obtained on the basis of its textual labels desci ). Each edge ei,i\u2032 expresses a semantic relation between objects oi and oi\u2032 . We suppose that given the set O of objects in the knowledge resource G, we can identify, for each text T , a set O(T ) \u2282 O of objects o.\nWhile assumption A1 is easy to formalize through a binary vector modeling objects oi \u2208 V or a vector combining their distributional representation xdi , it does not allow to ful ll assumption A2. To cope with this issue, the perspective of a vector representing objectobject pairs could be a good option to simultaneously capture: 1) the objects belonging to a text and 2) their similarity as well as their relatedness. However, the large number of potential pairwise objects, or more precisely object-to-object relations, in a knowledge resource would lead to a high dimensional and sparse vector. To face this issue, we propose the relation mapping method, that: 1) similarly to the word hashing method [12], aims at reducing the dimensionality and the sparsity of the vector representation to make it scalable, and 2) allows building representations of both objects belonging to text T and their relations according to assumption A2. We describe below our approach for achieving these two sub-goals. \u2022 Sub-goal 1) Text representation vector space: A naive approach consists in considering objects from the knowledge resource as unit vectors of a |V |-dimensional space. Even if the number of objects in the resource is signi cantly lower than the number of object-to-object relations, the scalability of the underlying framework remains questionable. To t with sub-goal 1) and lower the dimensionality of the vectorial representation space, we rather\nconsider clusters of objects as representative of each dimension of the vectorial space. Assuming that object-to-object relations might express topical relatedness between objects, we propose to build k topical clusters c j of objects oi \u2208 O assumed to be mutually independent. e la er refers to the referential R = {c1, . . . , ck } of the knowledge resource. In practice, we use the k-means clustering algorithm on the topical representation of objects, where the number of topical clusters k would be experimentally tuned (see Section 5.1). us, we consider a k-dimensional space, in which k is the number of topical clusters of objects. \u2022 Sub-goal 2) Knowledge resource-driven text representation: e representation xKR of text T is a k-dimensional vector xKR = (xKR1 , ...,x KR k ). To ful ll sub-goal 2, our intuition is that two documents are likely to be similar if they mention objects that are gathered around the same topical clusters. Naturally, the degree of similarity between those documents would depend on the average relatedness and similarity of their objects with each object in the topical clusters c j of the referential R. is refers to as a transitive property, illustrated in Figure 2. Each document D1 and D2 is modeled through a 2-dimensional vector in which each element represents a topical cluster. e gray levels in the document representation express the relatedness and similarity degree of document objects with respect to the topical clusters. Although documents D1 and D2 are not characterized by the same objects, they are as close to the referential, and accordingly, have similar representations. We compute each element xKRj as a combination of the importancew T j of topical cluster c j given textT and the relatedness Sr elat (c j ,O(T )) of objects O(T ) belonging to text T w.r.t. topical cluster c j :\nxKRj = w T j \u2217 Sr elat (c j ,O(T )) (1)\n3.2.1 Topical cluster importance score. e importance score wTj of topical cluster c j expresses to what extent the set O(T ) of objects belonging to textT are topically similar to objects belonging to topical cluster c j . Intuitively, the more topically similar the objects mentioned in the representations of texts T and T \u2032 with respect to the topical clusters, the more similar texts T and T \u2032. Assuming that objects belonging to a text represent a topical cluster, we rely on previous work dealing with clustering similarity [15] suggesting to estimate the similarity between two sets of objects by aggregating similarities between objects of these two di erent sets. More formally, the topical cluster importance score between topical cluster c j and object set O(T ) is estimated as:\nwTj = A\u0434\u0434 Function(om,on )\u2208O (T )\u00d7c j simt (om ,on ) (2)\nwhere A\u0434\u0434 Function expresses an aggregation function (we consider here the maximum to capture the best topical similarity between objects); simt estimates the topical similarity between vector representations of objects (here, the cosine similarity between the vectorial representations of object textual descriptions).\n3.2.2 Topical cluster-text relatedness score. e topical clustertext relatedness score Sr elat (c j ,O(T )) measures to what extent objects oi \u2208 O(T ) belonging to text T are related to those of topical cluster c j . Our intuition is that if the objects mentioned in texts T andT \u2032 are related to the representative of the same topical clusters, texts T and T \u2032 are more likely to be similar. Having in mind that state-of-the-art relatedness measures [28] rely on the computation\nof paths between objects, a scalable way allowing to measure this score is to consider the relatedness of objects O(T ) with respect to a representative object R(c j ) of topical cluster c j (e.g., the most frequent object in the collection among objects belonging to topical cluster c j ). e impact of the method used for identifying the representative is experimentally investigated (see Section 6.1). More formally, given a representative object R(c j ) of topical cluster c j , the topical cluster-text relatedness Sr elat (c j ,O(T )) estimates the path length between object R(c j ) and the object set O(T ):\nSr elat (c j ,O(T )) = \u2211\nom \u2208O (T ) log (1 + simr (R(c j ),om )) \u00b7\nav\u0434 no |O(T )| (3)\nwhere om is an object of the object setO(T ) characterizing textT . simr is a relatedness measure between objects (here the Leacock measure [17]); av\u0434 no is the average number of objects by document in the collection. e normalization factor av\u0434 no|O (T ) | avoids bias due to di erences in text lengths in terms of the number of objects."}, {"heading": "3.3 Model Architecture", "text": "3.3.1 Input. We propose to characterize each text T (whether extracted from a document or a query) by an input vector xinput = (xt ,xKR ) modeled as a vector composed of two parts: \u2022 Plain text representation xt . is feature represents words of full textT . Based on previous ndings highlighting the e ectiveness of distributed semantic representations to tackle the issue of large vocabulary in IR, we use the ParagraphVector model [2]. \u2022 Knowledge resource-driven representation xKR . is feature expresses the objects belonging to text T and their semantic relations expressed in the knowledge resource. is representation is built upon the relation mapping method (see Section 3.2).\n3.3.2 Learning the latent representation. For each sub-network branch, the input vector xinput of text T is projected into a latent space by means of L hidden layers li (i = 1, \u00b7 \u00b7 \u00b7 ,L) so as to obtain a latent semantic vector y combining the distributional and relational semantics of text T . Each hidden layer li and the latent semantic vector y are respectively obtained by non-linear transformations:\nl0 = xinput\nli = f (Wi\u22121 \u00b7 li\u22121 + bi\u22121) i = 1, ...,L (4) y = f (WL \u00b7 lL + bL)\nwhereWi and bi are respectively the weight matrix and bias term of the ith layer. e activation function f (x) performs a non-linear transformation, namely the ReLU: f (x) = max(0,x), which has been commonly used in deep learning works [? ]. e use of the ReLU function is motivated by the fact that it does not saturate to 1 when x is high in contrast to the hyperbolic tangent [12], avoiding to face to the gradient vanishing problem.\ne latent semantic vectorsyD andyQ of document D and query Q obtained through the non-linear transformations are used to estimate the document-query cosine similarity score R(D |Q).\n3.3.3 Loss function. Since retrieval tasks refer to a ranking problem, we optimize the parameters of the neural network using a pairwise ranking loss based on the distance \u2206 of similarity between relevant document-query pairs, noted (Q,D+), and irrelevant document-query pairs, noted (Q,D\u2212p ). Unlike [12], it worth\nmentioning that we use the hinge loss function, more adapted for learning-to-rank tasks [6? ]. To do so, we build a sample of document-query pairs in which we oppose, for the same query Q , one relevant document D+ for n irrelevant documents D\u2212p , p = [1..n], as suggested in [12]. e di erence \u2206 between the similarity of the relevant pair (Q,D+) and the irrelevant ones (Q,D\u2212p ) is de ned as:\n\u2206 = [ sim(Q,D+) \u2212 n\u2211 p=1 sim(Q,D\u2212p ) ]\n(5)\nwhere sim(\u2022, \u2022) is the output of the neural network. en, the DSRIM network is trained to maximize the similarity distance \u2206 using the hinge loss function L: L =max(0,\u03b1 \u2212 \u2206) where \u03b1 is the margin of L, depending on the \u2206 range."}, {"heading": "4 EVALUATION PROTOCOL", "text": ""}, {"heading": "4.1 Datasets", "text": "We consider two datasets (statistics are presented in Table 1): \u2022 e GOV23 dataset gathering .gov sites used in the TREC Terabyte campaign. We use topics from the 2004, 2005, and 2006 campaigns and the narrative part of each topic as a query. \u2022 e PMC OpenAccess4 dataset with biomedical full-texts from PubMed used in the TREC-CDS campaign. e summaries of topics of the 2014 and 2015 evaluation campaigns are used as queries."}, {"heading": "4.2 Implementation Details and Evaluation Methodology", "text": "To build the input layer, we pre-train a ParagraphVector model on the plain text corpus for learning vector xt . e vectors are sized to 100, as suggested in [2]. e concepts used for building our knowledge resource-driven representation are extracted using appropriate tools, namely SenseRelate on WordNet5 resource [27] for the GOV2 dataset and Cxtractor6 relying on MaxMatcher [41] applied on the 2015-MeSH version7 for the PMC dataset. We used for both the \u2019IS-A\u2019 relation. For modeling the representation xKR , we tune two parameters: 1) the number of topical clusters: we set the number k of topical clusters to k \u2208 {100, 200}; 2) the choice of the representative object R(c j ) within each topical cluster: we use three strategies: id fmin , namely the most frequent object; id fmax , the less frequent one; and centroid , the closest object to the centroid. Concerning our model architecture, we set the number of hidden layers to 2 with a hidden vector size equals to 64 leading to an output layer of 32 nodes. Similarly to [12], the number n of irrelevant document-query pairs opposed to a relevant one is 4 (Equation 5). Relevant/irrelevant document-query sets are randomly extracted from each dataset ground truth, supplying graded relevance judgments from 0 to 2 (relevance criteria: 1 and 2).\nTo train our model parameters, we apply the 5-fold crossvalidation method. e topics in each dataset are divided into 5 folds. For each fold retained as the test set for model evaluation, the other 4 folds are used to train and validate the model. e nal 3h p://ir.dcs.gla.ac.uk/test collections/gov2-summary.htm 4h ps://www.ncbi.nlm.nih.gov/pmc/tools/open list/ 5h p://wordnet.princeton.edu 6h ps://sourceforge.net/projects/cxtractor/ 7h ps://www.nlm.nih.gov/mesh/\nretrieval results are averaged over the test results on 5 folds. e model is optimized using a 5-sample mini-batch stochastic gradient descent (SGD) regularized with a dropout equals to 0.3. Our model generally converges a er 50 epochs over the training dataset.\nFor evaluating the ranking performance of our model and the di erent baselines, we perform a re-ranking [10] which is carried out over the top 2,000 documents retrieved by the BM25 model on Lucene. Final results are estimated using the top 1000 documents of each re-ranking model according to the MAP metric."}, {"heading": "4.3 Baselines", "text": "To evaluate the quality of our knowledge resource-driven representation, we use two models building representations of documents: \u2022 Top concepts: A naive version of our knowledge resourcedriven representation selecting the top k frequent objects in the document collection as the representative objects (k \u2208 {100, 200}). \u2022 LDA: e well-known LDA topic model representing topic clusters from plain text [? ] (vs. topical cluster relying on concepts and relations in DSRIM).\nTo evaluate our model e ectiveness, we use three types of baselines: 1) Exact term matching models to highlight the impact of both leveraging relational semantics and deep learning approaches: \u2022 BM25: e well-known probabilistic model (BM25). \u2022 LM-DI: e language model based on Dirichlet smoothing [40]. 2) Enhanced semantic matching models to outline the impact of a deep neural model guided by knowledge resources for capturing text semantics: \u2022 LM-QE: A language model applying a concept-based query expansion technique [25] in which candidate terms are ranked based on their similarity with descriptions in the knowledge resource. Default parameters mentioned in the paper are used. \u2022 LM-LDA: e LM-LDA is a latent topical model using the language modeling framework [34]. 3) Deep neural semantic matching models, also based on a siamese architecture, to highlight the impact of combining relational and distributional semantics in neural approaches: \u2022 DSSM: e state-of-the-art DSSM model [12]. We adopt the publicly released code8 with default parameter values. We evaluate the DSSM on full-text documents. \u2022 CLSM: e DSSM extension in which the feed-forward network is replaced by a convolution. We also apply the publicly released CLSM code8 on full-texts and use the default parameter values.\nTo measure the impact of the di erent evidence sources taken into consideration for representing texts, we use three scenarios:\n8h ps://www.microso .com/en-us/research/project/dssm/\n\u2022 DSRIMp2v : Our proposed neural model based on an input representation of texts restricted to the plain text, namely xt . \u2022 DSRIMkr : Our proposed neural model based on our knowledge resource-driven representation of text, namely xKR . \u2022DSRIMkr+p2v : Our proposed neural model based on an enhanced representation of texts combining plain text representation xt and our knowledge resource-driven representation xKR ."}, {"heading": "5 RESULTS", "text": ""}, {"heading": "5.1 Analyzing the Semantic Representation of Documents", "text": "In this section, we propose to analyze our knowledge resourcedriven representation through a twofold objective: 1) identifying the optimal parameter se ing of the vectorial representation and 2) assessing the validity of the built document vectors xKR .\nWe assess the vectorial representation quality based on the intuition that semantically similar texts, modeled as bags of concepts, should have similar vectorial representations built following our approach; such representations should also discard non-similar documents [16, 21]. In practice, given a randomly selected document (called a \u201cpivotal document\u201d), a good vectorial representation should 1) ensure that the distance between the pivotal document and each other document of the collection is non-uniform, and 2) maximize the distance between its most similar documents and its less similar ones. To this end: 1) we rst identify for each given pivotal document, the set Dp+ of its 10 most semantically similar documents and the set Dp\u2212 of the 10 less semantically similar documents over the whole dataset using a concept-oriented metric proposed in [7], called in the remaining the Corley measure; and 2) then we compute the average cosine similarity of the representations of the pivotal documents with the sets Dp+ and D p \u2212 . Table 2 presents the comparative results for 100 randomly selected pivotal documents and suggests the following statements: \u2022 e di erence in terms of cosine value range between both datasets (higher for GOV2) conjectures that representing texts using objects and relations expressed in a knowledge resource seems to be more di cult for the PMC dataset. is could be explained by the fact that this dataset focuses on a particular application domain\n(namely, the medical vs. general for GOV2) that might imply a more technical vocabulary. \u2022 Regarding the method used for de ning the vectorial representation space (sub-goal 1; Section 4.1), we can see that our proposed approach for identifying the referential based on the object clustering is more e ective than both baselines, respectively Top concept and LDA. Indeed, the similarity di erences of both document sets Dp+ and D p \u2212 obtained by the baselines are very small (< 0.11 for both datasets, except LDA for PMC, vs. higher than 0.15 for our clustering approach). It is worth to mention that the Top concept baseline particularly fails to discriminate between the most/less similar documents for both datasets given the high values of cosine values (> 0.90). Also, the small cosine values obtained using the LDA baseline for the most similar documents (< 0.5) show that the LDA representation is not able to build close document representations. In contrast, we outline that cosine values for our clustering approach seem to be more intuitive, with an average cosine for the GOV2 dataset higher than 0.6 for the most similar documents and lower than 0.6 for the less similar ones (respectively 0.5 for the PMC dataset). ese statements suggest that our referential building approach based on topical clustering seems reasonable. \u2022 Focusing on the methods used for the knowledge resourcedriven representation (sub-goal2; Section 4.1) and more particularly, the one used for choosing the topical cluster representative, we can notice that the average similarities between pivotal documents and the set of top similar ones are more important for a higher number of clusters (e.g., up to 0.6485 for k = 200 vs. 0.5455 for k = 100 for the PMC dataset). Also, this se ing allows obtaining higher di erences between the most vs. less similar documents (with at least 0.2251 vs. 0.1945 for respectively k = 200 and k = 100 for the PMC dataset, and 0.1781 vs. 0.1518 for the GOV2 dataset). ese results highlight the importance of achieving a reasonable ratio between the knowledge resource size (in terms of the number of object-object-relations) and the number of representative clusters of objects to be er capture the semantic representation of documents. With this in mind, the best scenario for k = 200 allowing to distinguish the most vs. the less similar documents consists in selecting the closest object to the cluster centroid (centroid) as the representative object for the PMC dataset while these are no signi cant di erences between the three methods for the GOV2\ndataset. Given that the centroid method is more intuitive with the assumptions used for building the referential, we retain the se ing with 200 topical clusters and the centroid method for encoding the representative object."}, {"heading": "5.2 Measuring the Model E ectiveness", "text": "We present here the performance of our model on both datasets GOV2 and PMC. Table 3 shows a summary of e ectiveness values in terms of MAP for our model and the di erent baselines. Comparing di erent con gurations of our approach, namely DSRIMp2v , DSRIMkr , and DSRIMkr+p2v , we can see that the DSRIM model applied only on our knowledge resource-driven representation xKR provides signi cant be er performance (pvalue<0.001) according to the MAP metric than the one with only the plain text-based representation xt (e.g., respectively 0.0307 and 0.0183 for the PMC dataset). is result reinforces the intuition claimed in recent work dealing with the use of text representations based on local interactions of terms and/or non-learned features [10]. Moreover, when combining the distributional and the relational semantics through the DSRIMkr+p2v model, we could see that the MAP value slightly increases, with for instance a significant improvement of +67.09% and +87.98% for the GOV2 and PMC datasets respectively with respect to DSRIMp2v . is opens interesting perspectives in the combination of those word-sense approaches as we claim in this paper.\nWith this in mind, we comment the baseline results with respect to the DSRIMkr+p2v model. From a general point of view, we can see, on the one hand, that exact matching models are nonsigni cantly di erent from our proposed model, with a particular attention to the GOV2 dataset with small improvements with respect to BM25 (+4.84%) and LM-DI (+17.61%). On the other hand, our approach overpasses semantic and deep matching models with signi cant improvements. For instance, our model reports signi cant be er results for the GOV2 dataset according to the MAP compared with the LM-QE, LM-LDA, DSSM, and CLSM models for which our model obtains a MAP value up to +410.41% of improvement rate. ose observations are similar for both datasets, highlighting the fact that our model is e ective for leveraging general (WordNet) as well as domain-oriented (MeSH) knowledge resources. More particularly, we can formulate the following statements: \u2022 e BM25 and the language models are well-known as strong IR baselines which are di cult to outperform with deep matching\nmodels learned with small training datasets that do not allow to generalize the task. e results presented in Table 3 lead us to conrm this statement. However, it is worth noting that, in contrast to most previous neural approaches based on siamese architecture [12, 30, 31] that rank short documents (titles) and use large-scale real collection for training their model, we rather experiment our model on long full-text document collections (average length is 1132.8 words for GOV2 and 477.1 for PMC). To get a be er understanding of these results, we investigate to what extent the e ectiveness of our model depends on the level of di culty of queries. More particularly, we classify queries according to three levels of di culty (\u201ceasy\u201d, \u201cmedium\u201d, \u201cdi cult\u201d) using the k-means algorithm applied on the BM25 MAP values. Statistics of each class are presented in Table 4. We can outline that, for the PMC dataset, di cult queries signi cantly include more terms and more objects than easy and medium ones. However, there is no signi cant di erences between the di erent query types with respect to the number of terms and objects for the GOV2 dataset. Focusing on the retrieval e ectiveness, it can be seen that DSRIMkr+p2v improvements according to BM25 are both positive and signi cant for di cult queries for both GOV2 and PMC. Moreover, it is worth mentioning that the improvement rates for di cult queries (+63.60% for the PMC dataset) are signi cantly di erent from the ones for medium and easy queries (respectively \u221225.78% and \u22120.22% for the PMC dataset, with no signi cant improvement di erence between easy and medium queries, p > 0.5). Interestingly, combining the improvement rates and the number of objects for medium queries of the GOV2 dataset, we can see that the signi cant e ectiveness decrease of our model (\u22125.15%) could be explained by the lowest number of objects associated with this query set. ese results highlight that leveraging the relational semantics through our knowledge resource-driven representation is more e ective for solving di cult queries. is is coherent since those queries are generally characterized by a high number of words and extracted objects. Accordingly, we can reasonably argue that our model is particularly devoted to lowering the semantic gap between word-based and concept-based representations of documents and queries which probably favors the discrimination between relevant and irrelevant documents. \u2022 e LM-QE baseline performs a knowledge resource-based query expansion. Since the DSRIM outperforms the LM-QE model, we can suggest that the semantic based representations of documents and queries which are learned starting from the input built upon\nthe relation mapping method, is more e ective than the expanded queries with relevant object descriptors. \u2022 e LDA-LM model is based on a probabilistic generative model able to identify relevant topics. Our model generally outperforms this baseline with a signi cant improvement of 89.95% for the MAP metric on the PMC dataset. is is consistent with previous work [12], highlighting the e ectiveness of deep latent representations of texts in comparison to those obtained by generative models. \u2022 In the category of neural IR models, our model outperforms the DSSM and the CLSM models (with a MAP reaching 0.0418 and 0.0095 for both datasets respectively). ese results suggest that the integration of relational as well as the distributional semantics at the document level (rather than the word level) into the input representation allows enhancing the learning of the deep neural matching model while considering small collections (instead of real search logs) and full texts (instead of titles). Interestingly, the convolutional CLSM model initially overpassing the DSSM in [31] through experiments carried out on a large-scale real-world data, is less e ective than the DSSM. One explanation might be that it is trained using TREC collections characterized by a limited number of queries (as also shown in [10]). A further analysis based on the cosine similarity between document-query vectors of input and output relevant pairs obtained using both DSSM and DSRIM highlights that the use of evidence from relational semantics underlying queries and documents allows a be er discrimination between relevant and irrelevant documents. Indeed, the similarity improvement between input/output representations is more important for our model than for the DSSM model for both datasets: 166.88% for DSSM vs. 271.51% for DSRIM for the GOV2 dataset, 5.91% for DSSM vs. 71.71% for the PMC dataset."}, {"heading": "6 CONCLUSION", "text": "We propose the DSRIM model, a deep neural IR model that leverages both distributional semantics through the ParagraphVector algorithm, and relational semantics, through a knowledge resourcedriven representation of texts aiming at jointly modeling embedded objects and structured relations between objects. Experimental evaluation on two TREC datasets, namely the GOV2 and the PMC Open Access, are performed to evaluate the quality of the input representations as well as their impact on document ranking e ectiveness. Results show that 1) our knowledge resource-driven representation allows to discriminate semantically similar from non-semantically similar texts, and that 2) our model overpasses semantic-driven approaches as well as state-of-the-art neural IR models. In the near future, we plan to further the knowledge resource-driven representation by taking into account both the heterogeneity of objects and the heterogeneity of the relations between objects."}, {"heading": "ACKNOWLEDGEMENT", "text": "is research was supported by the French FUI research program SparkInData."}], "references": [{"title": "Document expansion based on WordNet for robust IR", "author": ["Eneko Agirre", "Xabier Arregi", "Arantxa Otegi"], "venue": "In ICCL", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Analysis of the Paragraph Vector Model for Information Retrieval", "author": ["Qingyao Ai", "Liu Yang", "Jiafeng Guo", "W. Bruce Cro"], "venue": "In ICTIR", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Improving Language Estimation with the Paragraph Vector Model for Ad-hoc Retrieval", "author": ["Qingyao Ai", "Liu Yang", "Jiafeng Guo", "W. Bruce Cro"], "venue": "In SIGIR", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "\u008bestion Answering with Subgraph Embeddings", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In EMNLP", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Translating Embeddings for Modeling Multi-relational Data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "In NIPS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Ranking Measures and Loss Functions in Learning to Rank", "author": ["Wei Chen", "Tie yan Liu", "Yanyan Lan", "Zhi ming Ma", "Hang Li"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Measuring the semantic similarity of texts. In Workshop on empirical modeling of semantic equivalence and entailment", "author": ["Courtney Corley", "Rada Mihalcea"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Indexing by latent semantic analysis", "author": ["Sco\u008a Deerwester", "Susan T. Dumais", "George W. Furnas", "\u008comas K. Landauer", "Richard Harshman"], "venue": "Journal of the American Society for Information Science 41,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Retro\u0080\u008aing Word Vectors to Semantic Lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A Deep Relevance Matching Model for Ad-hoc Retrieval", "author": ["Jiafeng Guo", "Yixing Fan", "Qingyao Ai", "W Bruce Cro"], "venue": "In CIKM", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In CIKM", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "SensEmbed: Learning Sense Embeddings for Word and Relational Similarity", "author": ["Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In ACL", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Step-Wise Clustering Procedures", "author": ["Benjamin King"], "venue": "J. Amer. Statist. Assoc", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1967}, {"title": "Distributed Representations of Sentences and Documents", "author": ["\u008boc V Le", "Tomas Mikolov"], "venue": "In ICML", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Combining local context and WordNet similarity for word sense identi\u0080cation", "author": ["Claudia Leacock", "Martin Chodorow"], "venue": "WordNet: An electronic lexical database 49,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Constraining Word Embeddings by Prior Knowledge \u2013 Application to Medical Information Retrieval", "author": ["Xiaojie Liu", "Jian-Yun Nie", "Alessandro Sordoni"], "venue": "AIRS", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A deep architecture for matching short texts", "author": ["Zhengdong Lu", "Hang Li"], "venue": "In NIPS", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Embedded Representations of Lexical and Knowledge- Base Semantics", "author": ["Andrew McCallum"], "venue": "In ICTIR. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "E\u0081cient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Je\u0082rey Dean"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Je\u0082rey Dean"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "A dual embedding space model for document ranking", "author": ["Bhaskar Mitra", "Eric Nalisnick", "Nick Craswell", "Rich Caruana"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Toward a deep neural approach for knowledge-based IR", "author": ["Gia-Hung Nguyen", "Lynda Tamine", "Laure Soulier", "Nathalie Bricon-Souf"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Improving query expansion using WordNet", "author": ["Dipasree Pal", "Mandar Mitra", "Kalyankumar Da\u008aa"], "venue": "Journal of the Association for Information Science and Technology 65,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Text Matching As Image Recognition", "author": ["Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Shengxian Wan", "Xueqi Cheng"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "WordNet::SenseRelate::AllWords: A Broad Coverage Word Sense Tagger \u008cat Maximizes Semantic Relatedness", "author": ["Ted Pedersen", "Varada Kolhatkar"], "venue": "In NAACL-Demonstrations", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Measures of semantic similarity and relatedness in the biomedical domain", "author": ["Ted Pedersen", "Serguei V.S. Pakhomov", "Siddharth Patwardhan", "Christopher G. Chute"], "venue": "Journal of Biomedical Informatics 40,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Je\u0082rey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In EMNLP", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Aliaksei Severyn", "Alessandro Moschi\u008ai"], "venue": "In SIGIR", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In CIKM", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings", "author": ["Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "In SIGIR", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In AAAI", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "LDA-based document models for ad-hoc retrieval", "author": ["Xing Wei", "W Bruce Cro"], "venue": "In SIGIR", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "EsdRank: Connecting \u008bery and Documents \u008crough External Semi-Structured Data", "author": ["Chenyan Xiong", "Jamie Callan"], "venue": "In CIKM", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "\u008bery expansion with Freebase", "author": ["Chenyan Xiong", "Jamie Callan"], "venue": "In ICTIR. ACM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu"], "venue": "In CIKM", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Improving Lexical Embeddings with Semantic Knowledge", "author": ["Mo Yu", "Mark Dredze"], "venue": "In ACL", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Estimating Embedding Vectors for \u008beries", "author": ["Hamed Zamani", "W. Bruce Cro"], "venue": "In ICTIR. ACM,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval", "author": ["Chengxiang Zhai", "John La\u0082erty"], "venue": "In SIGIR", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2001}, {"title": "MaxMatcher: Biological Concept Extraction Using Approximate Dictionary Lookup", "author": ["Xiaohua Zhou", "Xiaodan Zhang", "Xiaohua Hu"], "venue": "In PRICAI. Springer", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": ", query or document expansion [1, 36] to lower the vocabulary mismatch between queries and documents; this is referred to as the relational semantics.", "startOffset": 30, "endOffset": 37}, {"referenceID": 35, "context": ", query or document expansion [1, 36] to lower the vocabulary mismatch between queries and documents; this is referred to as the relational semantics.", "startOffset": 30, "endOffset": 37}, {"referenceID": 7, "context": "Please cite the de\u0080nitive version which will be published in Proceedings of ICTIR 2017 Another line of work a\u008aempts to automatically infer hidden word senses from corpora using word collocations by performing dimensionality reduction techniques, such as latent semantic indexing [8] or, more recently, representation learning [21, 29] leading to distributional semantics.", "startOffset": 279, "endOffset": 282}, {"referenceID": 20, "context": "Please cite the de\u0080nitive version which will be published in Proceedings of ICTIR 2017 Another line of work a\u008aempts to automatically infer hidden word senses from corpora using word collocations by performing dimensionality reduction techniques, such as latent semantic indexing [8] or, more recently, representation learning [21, 29] leading to distributional semantics.", "startOffset": 326, "endOffset": 334}, {"referenceID": 28, "context": "Please cite the de\u0080nitive version which will be published in Proceedings of ICTIR 2017 Another line of work a\u008aempts to automatically infer hidden word senses from corpora using word collocations by performing dimensionality reduction techniques, such as latent semantic indexing [8] or, more recently, representation learning [21, 29] leading to distributional semantics.", "startOffset": 326, "endOffset": 334}, {"referenceID": 9, "context": "\u008ce la\u008aer was successfully exploited within deep neural networks for supporting search tasks [10, 12, 30].", "startOffset": 92, "endOffset": 104}, {"referenceID": 11, "context": "\u008ce la\u008aer was successfully exploited within deep neural networks for supporting search tasks [10, 12, 30].", "startOffset": 92, "endOffset": 104}, {"referenceID": 29, "context": "\u008ce la\u008aer was successfully exploited within deep neural networks for supporting search tasks [10, 12, 30].", "startOffset": 92, "endOffset": 104}, {"referenceID": 11, "context": "One of the \u0080rst contributions in the \u0080eld relies on siamese architectures, opposing queries and documents in a two-branch network [12, 30].", "startOffset": 130, "endOffset": 138}, {"referenceID": 29, "context": "One of the \u0080rst contributions in the \u0080eld relies on siamese architectures, opposing queries and documents in a two-branch network [12, 30].", "startOffset": 130, "endOffset": 138}, {"referenceID": 9, "context": ", BM25 or language models) remains a di\u0081cult task [10\u201312] and 2) learning the relevance function on full text does not allow the network convergence, even though evaluated on search logs of commercial search engines, leading to focus on a query-document title matching [10, 12, 30].", "startOffset": 50, "endOffset": 57}, {"referenceID": 10, "context": ", BM25 or language models) remains a di\u0081cult task [10\u201312] and 2) learning the relevance function on full text does not allow the network convergence, even though evaluated on search logs of commercial search engines, leading to focus on a query-document title matching [10, 12, 30].", "startOffset": 50, "endOffset": 57}, {"referenceID": 11, "context": ", BM25 or language models) remains a di\u0081cult task [10\u201312] and 2) learning the relevance function on full text does not allow the network convergence, even though evaluated on search logs of commercial search engines, leading to focus on a query-document title matching [10, 12, 30].", "startOffset": 50, "endOffset": 57}, {"referenceID": 9, "context": ", BM25 or language models) remains a di\u0081cult task [10\u201312] and 2) learning the relevance function on full text does not allow the network convergence, even though evaluated on search logs of commercial search engines, leading to focus on a query-document title matching [10, 12, 30].", "startOffset": 269, "endOffset": 281}, {"referenceID": 11, "context": ", BM25 or language models) remains a di\u0081cult task [10\u201312] and 2) learning the relevance function on full text does not allow the network convergence, even though evaluated on search logs of commercial search engines, leading to focus on a query-document title matching [10, 12, 30].", "startOffset": 269, "endOffset": 281}, {"referenceID": 29, "context": ", BM25 or language models) remains a di\u0081cult task [10\u201312] and 2) learning the relevance function on full text does not allow the network convergence, even though evaluated on search logs of commercial search engines, leading to focus on a query-document title matching [10, 12, 30].", "startOffset": 269, "endOffset": 281}, {"referenceID": 23, "context": "Guided by the intuition that the relational semantics could complement distributional semantics and the motivation that siamese networks are under-explored in IR [24], we investigate how to leverage both knowledge resources and siamese architecture to perform ad-hoc IR.", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "It is worth mentioning that, unlike previous work [12, 30] experimentally evaluated on document titles, our experiments are performed using full-texts.", "startOffset": 50, "endOffset": 58}, {"referenceID": 29, "context": "It is worth mentioning that, unlike previous work [12, 30] experimentally evaluated on document titles, our experiments are performed using full-texts.", "startOffset": 50, "endOffset": 58}, {"referenceID": 20, "context": "\u008ce potential of word semantic representations learned through neural approaches has been introduced in [21, 29], opening several perspectives in NLP and IR tasks.", "startOffset": 103, "endOffset": 111}, {"referenceID": 28, "context": "\u008ce potential of word semantic representations learned through neural approaches has been introduced in [21, 29], opening several perspectives in NLP and IR tasks.", "startOffset": 103, "endOffset": 111}, {"referenceID": 21, "context": "Indeed, several work focuses on the representation of sentences [22], documents [16, 32], and also objects and relations expressed in knowledge resources [5, 14, 20, 33].", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "Indeed, several work focuses on the representation of sentences [22], documents [16, 32], and also objects and relations expressed in knowledge resources [5, 14, 20, 33].", "startOffset": 80, "endOffset": 88}, {"referenceID": 31, "context": "Indeed, several work focuses on the representation of sentences [22], documents [16, 32], and also objects and relations expressed in knowledge resources [5, 14, 20, 33].", "startOffset": 80, "endOffset": 88}, {"referenceID": 4, "context": "Indeed, several work focuses on the representation of sentences [22], documents [16, 32], and also objects and relations expressed in knowledge resources [5, 14, 20, 33].", "startOffset": 154, "endOffset": 169}, {"referenceID": 13, "context": "Indeed, several work focuses on the representation of sentences [22], documents [16, 32], and also objects and relations expressed in knowledge resources [5, 14, 20, 33].", "startOffset": 154, "endOffset": 169}, {"referenceID": 19, "context": "Indeed, several work focuses on the representation of sentences [22], documents [16, 32], and also objects and relations expressed in knowledge resources [5, 14, 20, 33].", "startOffset": 154, "endOffset": 169}, {"referenceID": 32, "context": "Indeed, several work focuses on the representation of sentences [22], documents [16, 32], and also objects and relations expressed in knowledge resources [5, 14, 20, 33].", "startOffset": 154, "endOffset": 169}, {"referenceID": 4, "context": "Within the la\u008aer, the main principle consists in learning the representation of objects and relations on the basis of object-relation-object triplets, relying on the assumption that the embedding of object oi should be close to the embedding translation of object oj by relation r , namely oi ' oj + r (TransE model) [5].", "startOffset": 317, "endOffset": 320}, {"referenceID": 32, "context": ", di\u0082erent object representations according to the semantic relation type (TransH) [33] or a dynamic mapping between objects and relations constrained by their diversity (TransD) [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": ", di\u0082erent object representations according to the semantic relation type (TransH) [33] or a dynamic mapping between objects and relations constrained by their diversity (TransD) [14].", "startOffset": 179, "endOffset": 183}, {"referenceID": 8, "context": "Moreover, knowledge resources have been used to enhance the distributed representation of words for representing their underlying concepts [9, 18, 37, 38].", "startOffset": 139, "endOffset": 154}, {"referenceID": 17, "context": "Moreover, knowledge resources have been used to enhance the distributed representation of words for representing their underlying concepts [9, 18, 37, 38].", "startOffset": 139, "endOffset": 154}, {"referenceID": 36, "context": "Moreover, knowledge resources have been used to enhance the distributed representation of words for representing their underlying concepts [9, 18, 37, 38].", "startOffset": 139, "endOffset": 154}, {"referenceID": 37, "context": "Moreover, knowledge resources have been used to enhance the distributed representation of words for representing their underlying concepts [9, 18, 37, 38].", "startOffset": 139, "endOffset": 154}, {"referenceID": 8, "context": "[9] propose a \u201cretro\u0080\u008aing\u201d technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to re\u0080ne their associated word embeddings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "Other work [37, 38] introduces an end-to-end approach that rather adjusts the objective function of the neural language model by either leveraging the relational and categorical knowledge to learn a higher quality word embeddings (RC-NET model) [37] or extending the CBOW model using prior relational knowledge [38].", "startOffset": 11, "endOffset": 19}, {"referenceID": 37, "context": "Other work [37, 38] introduces an end-to-end approach that rather adjusts the objective function of the neural language model by either leveraging the relational and categorical knowledge to learn a higher quality word embeddings (RC-NET model) [37] or extending the CBOW model using prior relational knowledge [38].", "startOffset": 11, "endOffset": 19}, {"referenceID": 36, "context": "Other work [37, 38] introduces an end-to-end approach that rather adjusts the objective function of the neural language model by either leveraging the relational and categorical knowledge to learn a higher quality word embeddings (RC-NET model) [37] or extending the CBOW model using prior relational knowledge [38].", "startOffset": 245, "endOffset": 249}, {"referenceID": 37, "context": "Other work [37, 38] introduces an end-to-end approach that rather adjusts the objective function of the neural language model by either leveraging the relational and categorical knowledge to learn a higher quality word embeddings (RC-NET model) [37] or extending the CBOW model using prior relational knowledge [38].", "startOffset": 311, "endOffset": 315}, {"referenceID": 24, "context": "Based on the use of such resources, a \u0080rst line of work in IR aims at increasing the likelihood of term overlap between queries and documents through query expansion [25, 36] or document expansion [1].", "startOffset": 166, "endOffset": 174}, {"referenceID": 35, "context": "Based on the use of such resources, a \u0080rst line of work in IR aims at increasing the likelihood of term overlap between queries and documents through query expansion [25, 36] or document expansion [1].", "startOffset": 166, "endOffset": 174}, {"referenceID": 0, "context": "Based on the use of such resources, a \u0080rst line of work in IR aims at increasing the likelihood of term overlap between queries and documents through query expansion [25, 36] or document expansion [1].", "startOffset": 197, "endOffset": 200}, {"referenceID": 35, "context": "[36] propose two algorithms", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Authors in [25] propose a query expansion technique using terms", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "[1] propose a document expansion technique based on the use of a random walk algorithm identifying from WordNet the most related concepts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "\u008ce second line of work leverages relations modeled in knowledge resources at the document ranking level [35].", "startOffset": 104, "endOffset": 108}, {"referenceID": 34, "context": "For instance, authors in [35] pro-", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": ", text matching [12], question-answering [4]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": ", text matching [12], question-answering [4]).", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "A \u0080rst category of work uses neural models for IR tasks [2, 23, 39] to integrate embeddings in IR relevance functions.", "startOffset": 56, "endOffset": 67}, {"referenceID": 22, "context": "A \u0080rst category of work uses neural models for IR tasks [2, 23, 39] to integrate embeddings in IR relevance functions.", "startOffset": 56, "endOffset": 67}, {"referenceID": 38, "context": "A \u0080rst category of work uses neural models for IR tasks [2, 23, 39] to integrate embeddings in IR relevance functions.", "startOffset": 56, "endOffset": 67}, {"referenceID": 10, "context": "\u008ce second category of work, closer to our contribution, consists in end-to-end scoring models that learn the relevance of document-query pairs via latent semantic features [11, 12].", "startOffset": 172, "endOffset": 180}, {"referenceID": 11, "context": "\u008ce second category of work, closer to our contribution, consists in end-to-end scoring models that learn the relevance of document-query pairs via latent semantic features [11, 12].", "startOffset": 172, "endOffset": 180}, {"referenceID": 11, "context": "For instance, the Deep Semantic Structured Model (DSSM) [12] applies a siamese deep feed-forward network on document and query representations obtained by a word hashing method.", "startOffset": 56, "endOffset": 60}, {"referenceID": 30, "context": "[31] use a convolutional-pooling structure, called Convolutional Latent Semantic Model (CLSM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "In the same mind, Severyn and Moschi\u008ai [30] apply a convolution to learn the optimal representation of short text pairs as well as the similarity function.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "[10] also argue that tackling traditional IR models (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "To bypass this limitation, another line of work [10, 19, 26] rather aims at building a local interaction map between inputs, and then uses deep neural networks to learn hierarchical interaction patterns.", "startOffset": 48, "endOffset": 60}, {"referenceID": 18, "context": "To bypass this limitation, another line of work [10, 19, 26] rather aims at building a local interaction map between inputs, and then uses deep neural networks to learn hierarchical interaction patterns.", "startOffset": 48, "endOffset": 60}, {"referenceID": 25, "context": "To bypass this limitation, another line of work [10, 19, 26] rather aims at building a local interaction map between inputs, and then uses deep neural networks to learn hierarchical interaction patterns.", "startOffset": 48, "endOffset": 60}, {"referenceID": 18, "context": "\u008ce DeepMatch model [19] integrates a topic model into a fully connected deep network based on the word interaction matrix while the MatchPyramid model [26] applies a convolution to an interaction matrix estimated on the basis of word representation.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "\u008ce DeepMatch model [19] integrates a topic model into a fully connected deep network based on the word interaction matrix while the MatchPyramid model [26] applies a convolution to an interaction matrix estimated on the basis of word representation.", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "Guided by the intuition that interaction matrix is more appropriate for the global matching and lacks of the term importance consideration, authors in [10] model local interactions of query-document terms through occurrence histograms.", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "\u008ce literature review highlights that: 1) plain text and knowledge resources are complementary for both learning distributional representations and enhancing IR e\u0082ectiveness [1, 25, 36], and that", "startOffset": 173, "endOffset": 184}, {"referenceID": 24, "context": "\u008ce literature review highlights that: 1) plain text and knowledge resources are complementary for both learning distributional representations and enhancing IR e\u0082ectiveness [1, 25, 36], and that", "startOffset": 173, "endOffset": 184}, {"referenceID": 35, "context": "\u008ce literature review highlights that: 1) plain text and knowledge resources are complementary for both learning distributional representations and enhancing IR e\u0082ectiveness [1, 25, 36], and that", "startOffset": 173, "endOffset": 184}, {"referenceID": 9, "context": "2) neural approaches in IR, and more particularly siamese architectures, have a great potential for ad-hoc search but could still be improved to compete with traditional IR models [10].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "In this contribution, we address the problem of bridging the semantic gap in IR by leveraging both deep learning approaches [12, 30] and valid knowledge expressed in knowledge resources [25, 36].", "startOffset": 124, "endOffset": 132}, {"referenceID": 29, "context": "In this contribution, we address the problem of bridging the semantic gap in IR by leveraging both deep learning approaches [12, 30] and valid knowledge expressed in knowledge resources [25, 36].", "startOffset": 124, "endOffset": 132}, {"referenceID": 24, "context": "In this contribution, we address the problem of bridging the semantic gap in IR by leveraging both deep learning approaches [12, 30] and valid knowledge expressed in knowledge resources [25, 36].", "startOffset": 186, "endOffset": 194}, {"referenceID": 35, "context": "In this contribution, we address the problem of bridging the semantic gap in IR by leveraging both deep learning approaches [12, 30] and valid knowledge expressed in knowledge resources [25, 36].", "startOffset": 186, "endOffset": 194}, {"referenceID": 10, "context": "In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource.", "startOffset": 47, "endOffset": 63}, {"referenceID": 11, "context": "In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource.", "startOffset": 47, "endOffset": 63}, {"referenceID": 25, "context": "In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource.", "startOffset": 47, "endOffset": 63}, {"referenceID": 29, "context": "In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource.", "startOffset": 47, "endOffset": 63}, {"referenceID": 4, "context": "In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource.", "startOffset": 219, "endOffset": 233}, {"referenceID": 8, "context": "In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource.", "startOffset": 219, "endOffset": 233}, {"referenceID": 32, "context": "In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource.", "startOffset": 219, "endOffset": 233}, {"referenceID": 36, "context": "In contrast to previous work in deep IR models [11, 12, 26, 30] relying only on the distributional semantics of texts and work on the semantic representation of objects and relations only leveraging knowledge resources [5, 9, 33, 37], our main concern is to estimate a relevance function leveraging a semantic representation of documents that simultaneously takes into consideration objects and their pairwise relations expressed in a knowledge resource.", "startOffset": 219, "endOffset": 233}, {"referenceID": 11, "context": "With this in mind, we investigate the potential of siamese neural architectures, such as DSSM [12], on full-text retrieval.", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": ", its ParagraphVector [2] obtained on the basis of its textual labels desci ).", "startOffset": 22, "endOffset": 25}, {"referenceID": 11, "context": "To face this issue, we propose the relation mapping method, that: 1) similarly to the word hashing method [12], aims at reducing the dimensionality and the sparsity of the vector representation to make it scalable, and 2) allows building representations of both objects belonging to text T and their relations according to assumption A2.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "Assuming that objects belonging to a text represent a topical cluster, we rely on previous work dealing with clustering similarity [15] suggesting to estimate the similarity between two sets of objects by aggregating similarities between objects of these two di\u0082erent sets.", "startOffset": 131, "endOffset": 135}, {"referenceID": 27, "context": "Having in mind that state-of-the-art relatedness measures [28] rely on the computation of paths between objects, a scalable way allowing to measure this score is to consider the relatedness of objects O(T ) with respect to a representative object R(c j ) of topical cluster c j (e.", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "simr is a relatedness measure between objects (here the Leacock measure [17]); av\u0434 no is the average number of objects by document in the collection.", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "Based on previous \u0080ndings highlighting the e\u0082ectiveness of distributed semantic representations to tackle the issue of large vocabulary in IR, we use the ParagraphVector model [2].", "startOffset": 176, "endOffset": 179}, {"referenceID": 11, "context": "\u008ce use of the ReLU function is motivated by the fact that it does not saturate to 1 when x is high in contrast to the hyperbolic tangent [12], avoiding to face to the gradient vanishing problem.", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "Unlike [12], it worth", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "n], as suggested in [12].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "\u008ce vectors are sized to 100, as suggested in [2].", "startOffset": 45, "endOffset": 48}, {"referenceID": 26, "context": "\u008ce concepts used for building our knowledge resource-driven representation are extracted using appropriate tools, namely SenseRelate on WordNet5 resource [27] for the GOV2 dataset and Cxtractor6 relying on MaxMatcher [41] applied on the 2015-MeSH version7 for the PMC dataset.", "startOffset": 154, "endOffset": 158}, {"referenceID": 40, "context": "\u008ce concepts used for building our knowledge resource-driven representation are extracted using appropriate tools, namely SenseRelate on WordNet5 resource [27] for the GOV2 dataset and Cxtractor6 relying on MaxMatcher [41] applied on the 2015-MeSH version7 for the PMC dataset.", "startOffset": 217, "endOffset": 221}, {"referenceID": 11, "context": "Similarly to [12], the number n of irrelevant document-query pairs opposed to a relevant one is 4 (Equation 5).", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "For evaluating the ranking performance of our model and the di\u0082erent baselines, we perform a re-ranking [10] which is carried", "startOffset": 104, "endOffset": 108}, {"referenceID": 39, "context": "\u2022 LM-DI: \u008ce language model based on Dirichlet smoothing [40].", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "2) Enhanced semantic matching models to outline the impact of a deep neural model guided by knowledge resources for capturing text semantics: \u2022 LM-QE: A language model applying a concept-based query expansion technique [25] in which candidate terms are ranked based on their similarity with descriptions in the knowledge resource.", "startOffset": 219, "endOffset": 223}, {"referenceID": 33, "context": "\u2022 LM-LDA: \u008ce LM-LDA is a latent topical model using the language modeling framework [34].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "3) Deep neural semantic matching models, also based on a siamese architecture, to highlight the impact of combining relational and distributional semantics in neural approaches: \u2022 DSSM: \u008ce state-of-the-art DSSM model [12].", "startOffset": 217, "endOffset": 221}, {"referenceID": 15, "context": "We assess the vectorial representation quality based on the intuition that semantically similar texts, modeled as bags of concepts, should have similar vectorial representations built following our approach; such representations should also discard non-similar documents [16, 21].", "startOffset": 271, "endOffset": 279}, {"referenceID": 20, "context": "We assess the vectorial representation quality based on the intuition that semantically similar texts, modeled as bags of concepts, should have similar vectorial representations built following our approach; such representations should also discard non-similar documents [16, 21].", "startOffset": 271, "endOffset": 279}, {"referenceID": 6, "context": "To this end: 1) we \u0080rst identify for each given pivotal document, the set D + of its 10 most semantically similar documents and the set D \u2212 of the 10 less semantically similar documents over the whole dataset using a concept-oriented metric proposed in [7], called in the remaining the Corley measure; and 2) then we compute the average cosine similarity of the representations of the pivotal documents with the sets D + and D p \u2212 .", "startOffset": 253, "endOffset": 256}, {"referenceID": 9, "context": "\u008cis result reinforces the intuition claimed in recent work dealing with the use of text representations based on local interactions of terms and/or non-learned features [10].", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "However, it is worth noting that, in contrast to most previous neural approaches based on siamese architecture [12, 30, 31] that rank short documents (titles) and use large-scale real collection for training their model, we rather experiment our model on long full-text document collections (average length is 1132.", "startOffset": 111, "endOffset": 123}, {"referenceID": 29, "context": "However, it is worth noting that, in contrast to most previous neural approaches based on siamese architecture [12, 30, 31] that rank short documents (titles) and use large-scale real collection for training their model, we rather experiment our model on long full-text document collections (average length is 1132.", "startOffset": 111, "endOffset": 123}, {"referenceID": 30, "context": "However, it is worth noting that, in contrast to most previous neural approaches based on siamese architecture [12, 30, 31] that rank short documents (titles) and use large-scale real collection for training their model, we rather experiment our model on long full-text document collections (average length is 1132.", "startOffset": 111, "endOffset": 123}, {"referenceID": 11, "context": "[12], highlighting the e\u0082ectiveness of deep latent representations of texts in comparison to those obtained by generative models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Interestingly, the convolutional CLSM model initially overpassing the DSSM in [31] through experiments carried out on a large-scale real-world data, is less e\u0082ective than the DSSM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "One explanation might be that it is trained using TREC collections characterized by a limited number of queries (as also shown in [10]).", "startOffset": 130, "endOffset": 134}], "year": 2017, "abstractText": "\u008ce state-of-the-art solutions to the vocabulary mismatch in information retrieval (IR) mainly aim at leveraging either the relational semantics provided by external resources or the distributional semantics, recently investigated by deep neural approaches. Guided by the intuition that the relational semantics might improve the e\u0082ectiveness of deep neural approaches, we propose the Deep Semantic Resource Inference Model (DSRIM) that relies on: 1) a representation of raw-data that models the relational semantics of text by jointly considering objects and relations expressed in a knowledge resource, and 2) an end-to-end neural architecture that learns the query-document relevance by leveraging the distributional and relational semantics of documents and queries. \u008ce experimental evaluation carried out on two TREC datasets from TREC Terabyte and TREC CDS tracks relying respectively on WordNet and MeSH resources, indicates that our model outperforms state-of-the-art semantic and deep neural IR models.", "creator": "LaTeX with hyperref package"}}}