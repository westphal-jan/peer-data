{"id": "1706.04964", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Learning Deep ResNet Blocks Sequentially using Boosting Theory", "abstract": "deep neural networks are known to be difficult to train due to the instability of back - propagation. a deep \\ emph { residual network } ( resnet ) with identity loops remedies this by stabilizing gradient computations. we prove a boosting theory for maintaining the resnet architecture. we construct $ t $ weak module classifiers, considering each contains two of possibly the $ t $ layers, such that the combined strong learner is a resnet. assume therefore, we introduce an alternative deep resnet training algorithm, \\ emph { boostresnet }, which is particularly suitable in non - differentiable architectures. our best proposed algorithm merely requires a sequential training of $ t $ \" shallow enrichment resnets \" which basically are inexpensive. we even prove that the training error decays exponentially with the depth $ t $ if the \\ emph { weak module classifiers } that we train but perform slightly better than some weak baseline. in other words, we propose a weak learning condition and prove a boosting theory for resnet under the weak learning condition. our results apply to general multi - paradigm class resnets. a generalization error bound based on margin theory is proved and suggests resnet's resistant to overfitting under network with $ l _ 1 $ norm bounded weights.", "histories": [["v1", "Thu, 15 Jun 2017 16:59:07 GMT  (327kb)", "http://arxiv.org/abs/1706.04964v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["furong huang", "jordan ash", "john langford", "robert schapire"], "accepted": false, "id": "1706.04964"}, "pdf": {"name": "1706.04964.pdf", "metadata": {"source": "CRF", "title": "Learning Deep ResNet Blocks Sequentially using Boosting Theory", "authors": ["Furong Huang"], "emails": ["furongh@cs.umd.edu", "jordantash@gmail.com", "jcl@microsoft.com", "schapire@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n04 96\n4v 1\n[ cs\n.L G\n] 1"}, {"heading": "1 Introduction", "text": "Deep neural networks have elicited breakthrough successes in machine learning, especially in image classification and object recognition (Krizhevsky et al. , 2012; Sermanet et al. , 2013; Simonyan & Zisserman, 2014; Zeiler & Fergus, 2014) in recent years. As the number of layers increases, the nonlinear network becomes more powerful, deriving richer features from input data. Empirical studies suggest that challenging tasks in image classification (He et al. , 2015; Ioffe & Szegedy, 2015; Simonyan & Zisserman, 2014; Szegedy et al. , 2015) and object recognition (Girshick, 2015; Girshick et al. , 2014; He et al. , 2014; Long et al. , 2015; Ren et al. , 2015) often require \u201cdeep\u201d networks, consisting of tens or hundreds of layers. Theoretical analyses have further justified the power of deep networks (Mhaskar & Poggio, 2016) compared to shallow networks.\nHowever deep neural networks are difficult to train despite their intrinsic viability. Stochastic gradient descent with back-propagation (BP) (LeCun et al. , 1989) and its variants are commonly used to solve the non-convex optimization problem. A major challenge that exists for training both shallow and deep networks is vanishing/exploding gradients (Bengio et al. , 1994; Glorot & Bengio, 2010). Recent works have proposed normalization techniques (Glorot & Bengio, 2010; LeCun et al. , 2012; Ioffe & Szegedy, 2015; Saxe et al. , 2013) to effectively ease the problem and achieve convergence. In training deep networks, however, a surprising training performance degradation is observed\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n(He & Sun, 2015; Srivastava et al. , 2015; He et al. , 2016): the training performance degrades rapidly along with the increased network depth after some saturation point. This training performance degradation is unexpected as one can easily construct a deep network identical to a shallow network by enforcing any part of the deep network to be the same as the shallow network and the rest layers to be identity maps. He et al. (He et al. , 2016) have presented a residual network (ResNet) learning framework to ease the training of networks that are substantially deeper than those used previously. And they explicitly reformulate the layers as learning residual functions with reference to the layer inputs by adding identity loops to the layers. It is shown in (Hardt & Ma, 2016) that identity loops ease the problem of spurious local optima in shallow networks. Srivastava et al. (Srivastava et al. , 2015) introduce a novel architecture that enables the optimization of networks with virtually arbitrary depth through the use of a learned gating mechanism for regulating i34nformation flow.\nAlthough empirical evidence shows that these deep residual networks are easier to optimize than non-residual ones, there lacks a theoretical justification for this observation. For example, there have been no performance guarantees on the training or testing error for networks of this deep residual architecture. Furthermore, the entire training still relies on the unstable end-to-end back-propagation which are susceptible to suboptimal solutions (Ge et al. , 2015) in deep networks."}, {"heading": "1.1 Summary of Results", "text": "We propose a novel framework, multi-channel telescoping sum boosting (defined in Section 4), to characterize a feed forward ResNet in Section 3. We show that the top level (final) output of a ResNet is a telescoping sum of its pairs of consecutive module differences. Theoretical analyses such as training error guarantees and generalization error bounds for telescoping sum boosting are provided.\nTo remedy the problem of training error degradation as depth increases, we introduce a guaranteed learning algorithm, called BoostResNet, to train modules of ResNet sequentially. BoostResNet adaptively selects training samples or changes the cost function (Section 4 Theorem 4.2). BoostResNet trains the ResNet with guarantees: the training error decays exponentially with the depth of the network. As discussed later in Section 4.4, the generalization error of BoostResNet is analyzed and advice to avoid over-fitting is provided. Our procedure trains each residual block sequentially, only requiring that each provides a better-than-random-guessing prediction of dataset labels.\nOur BoostResNet algorithm enjoys several superiorities over the end-to-end back-propagation (e2eBP) training convention despite the theoretical guarantees. First, BoostResNet is substantially more memory efficient than e2eBP as the former requires only two modules of the network to be in the GPU whereas the latter inevitably keeps all modules in the GPU. The advantage is crucial for efficient training for deep networks. Also, BoostResNet is more computational efficient than e2eBP since each e2eBP step involves back propagating through the entire deep network.\nExperimentally, we compare BoostResNet with e2eBP over two types of feed forward ResNets, multilayer perceptron residual network (MLP-ResNet) and convolutional neural network residual network (CNN-ResNet), on multiple datasets. BoostResNet shows drastic performance improvement under the MLP-ResNet architecture. Under CNN-ResNet, a slightly faster convergence for BoostResNet is observed. The tremendous advantages of BoostResNet on memory and computation efficiency are justified as well in the experiments. Our multi-channel telescoping sum boosting learning framework is not limited to ResNet and can be extended to other even non-differentiable nonlinear hypothesis units, such as decision trees and tensor decompositions."}, {"heading": "1.2 Related Works", "text": "Training deep neural networks has been an active research area in the past few years. The main optimization challenge lies in the highly non-convex nature of the loss function. There are two main ways to address this optimization problem, one is to select a loss function and network architecture that have better geometric properties, and the other is to improve the network\u2019s learning procedure.\nLoss function and architecture selection In neural network optimization, there are many predefined loss functions and criteria that are commonly used. For instance, mean squared error, negative log likelihood, margin criterion and so forth. There are extensive works (Girshick, 2015; Rubinstein & Kroese, 2013; Tygert et al. , 2015) on selecting or modifying loss functions to prevent\nempirical difficulties such as exploding/vanishing gradients or slow learning (Balduzzi et al. , 2017). However, there are no rigorous principles for selecting a loss function in general. Other works consider variations of the MLP or CNN by adding identity skip connections (He et al. , 2016), allowing information to bypass particular layers. However, no theoretical guarantees on the training error are provided despite breakthrough empirical successes. Hardt et al. (Hardt & Ma, 2016) have shown the advantage of identity loops in linear neural networks with theoretical justifications, however the linear setting is unrealistic in practice.\nLearning algorithm design There have been extensive works on improving BP (LeCun et al. , 1989). For instance, momentum (Qian, 1999), Nesterov accelerated gradient (Nesterov, 1983), Adagrad (Duchi et al. , 2011) and its extension Adadelta (Zeiler, 2012). Most recently, Adaptive Moment Estimation (Adam) (Kingma & Ba, 2014), a combination of momentum and adagrad, has received substantial success in practice. All these methods are modifications of SGD, but our method only requires an arbitrary oracle, which does not necessarily need to be an SGD solver. Bengio et al. (Bengio et al. , 2006) introduce single hidden layer convex neural networks, and propose a gradient boosting algorithm to learn the weights of the linear classifier, however a generalization of their method to deep networks with more than one hidden layer is not possible. Shalev-Shwartz (Shalev-Shwartz, 2014) proposes a selfieBoost which boosts the accuracy of a single network. Our algorithm is different as we instead construct ensembles of classifiers. AdaBoost (Cortes et al. , 2016) also considers ensembles of classifiers, but they require connections between top layer and all other lower level layers, which are not amenable to a standard ResNet architecture.\n2 Preliminaries\nA residual neural network (ResNet) is composed of stacked entities referred to as modules. Each module consists of a multiple-layer neural network. Commonly used modules include MLP and CNN.\nThroughout this paper, we consider training examples (x, y) = {(x1, y1), (x2, y2), . . . , (xm, ym)} \u2208 D, where D is the distribution of the data. We will use S to denote the samples.\nA Module of ResNet Let each module map its input to ft(\u00b7) where t denotes the level of the modules. Each module ft(x) is a nonlinear unit with n channels, i.e., ft(x) \u2208 Rn. In multilayer perceptron residual network (MLP-ResNet), ft(x) is a shallow MLP, for instance, a connected nonlinear ft(x) = V\u0303 \u22a4 t \u03c3(W\u0303 \u22a4 t x) where W\u0303t \u2208 Rn\u00d7k, V\u0303t \u2208 Rk\u00d7n and \u03c3 is a nonlinear operator such as sigmoidal function or relu function. Similarly, in convolutional neural network residual network (CNN-ResNet), the function ft(x) represents the t-th convolutional module. Then the t-th module outputs gt+1(x)\ngt+1(x) = ft(gt(x)) + gt(x), (1)\nwhere x is the input fed to the ResNet. See Figure 1 for an illustration of a ResNet, which consists of stacked modules with identity loops.\nOutput of ResNet Due to the recursive relation specified in Equation (1), the output of the T -th module is equal to the summation over lower module outputs, i.e., gT+1(x) = fT (gT (x))+gT (x) =\u2211T\nt=1 ft(gt(x)), where g1(x) = x. For classification tasks, the final output of a ResNet given input x is rendered after a linear classifier w \u2208 Rn on representation gT+1(x):\ny\u0302 = \u03c3\u0303 (F (x)) = \u03c3\u0303(w\u22a4gT+1(x)) = \u03c3\u0303 ( w\u22a4 T\u2211\nt=1\nft(gt(x))\n) (2)\nwhere \u03c3\u0303(\u00b7) denotes a map from representation to labels \u03c3\u0303(z) : z \u2192 Y . For instance \u03c3\u0303(z) = sign(z) for binary classification, or \u03c3\u0303(z) = argmax\ni zi for multi-class classification. The form of the\noutput from a classification ResNet is thus as follows. The parameters of a depth-T ResNet are\n{w, {ft(\u00b7), \u2200t \u2208 T }}. A ResNet training involves training the classifier w and the weights of modules ft(\u00b7) \u2200t \u2208 [T ] when training samples (x1, y1), (x2, y2), . . . , (xm, ym) are available.\nBoosting Boosting (Freund & Schapire, 1995) assumes the availability of a weak learning algorithm which, given labeled training examples, produces a weak classifier (a.k.a. base classifier) ht(x). The goal of boosting is to improve the performance of the weak learning algorithm. The key idea behind boosting is to choose training sets for the weak classifier in such a fashion as to force it to infer something new about the data each time it is called. The weak learning algorithm will finally combine many weak classifiers ht(x) into a single combined strong classifier \u2211T\nt=1 ht(x) whose prediction power is strong.\nFrom empirical experiences, ResNet remedies the problem of increased training error in deeper neural networks. We are curious about whether there is a theoretical justification that the training error of ResNet asymptotically converges to 0 as the depth T increases. More importantly, we are interested in proposing a new algorithm that avoids end-to-end back-propagation (e2eBP) through the deep network and thus is immune to the instability of the non-convex optimization."}, {"heading": "3 ResNet in Telescoping Sum Boosting Framework", "text": "As we recall from Equation 2, ResNet indeed has a similar form as the strong classifier in boosting. The key difference is that boosting is an ensemble of estimated hypotheses \u2211T\nt=1 ht(x) whereas\nResNet is an ensemble of estimated feature representations F (x) = \u2211T\nt=1 ft(gt(x)). To solve this problem, we introduce an auxiliary linear classifier wt on top of each ResNet module ft(gt(x)) to construct a hypothesis module. Formally, a hypothesis module is defined as\not(x) def = w\u22a4t gt(x) \u2208 R (3)\nin the binary classification setting. Therefore ot+1(x) = w \u22a4 t+1[ft(gt(x)) + gt(x)] as gt+1(x) = ft(gt(x)) + gt(x). In the multi-class setting, let C be the number of classes, we define hypothesis module ot(x) def = W\u22a4t gt(x) \u2208 RC where linear classifier Wt \u2208 Rn\u00d7C is a matrix instead of a vector. Our analysis applies to both binary and multi-class, but we will focus on the binary class for simplicity in the main text and defer the multi-class analysis to the Appendix E.\nNow the naive ensemble of the hypothesis module is not a ResNet unless all the auxiliary linear classifiers wt\u2019s are equivalent to the ResNet\u2019s top classifier w. However note that the input gt+1(x) of the t+1-th module is the output ft(gt(x))+gt(x) of the t-th module, one has to train the modules sequentially. Therefore the common auxiliary linear classifier assumption prevents us from training the T hypothesis module sequentially and is thus unrealistic. We design a weak module classifier using the idea of telescoping sum as follows.\nDefinition 3.1. A weak module classifier is defined as ht(x) def = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x) where ot(x) def = w\u22a4t gt(x) is a hypothesis module. We call the boosting framework a \u201ctelescoping sum boosting\u201d if the weak learners are restricted to the form of the weak module classifier.\nResNet: Ensemble of Weak Module Classifier Recall that the T -th module of a ResNet outputs gT+1(x), which is fed to the top/final linear classifier for the final classification. We show that an ensemble of the weak module classifiers is equivalent to a ResNet\u2019s final output. We state it formally in Lemma 3.2.\nLemma 3.2. Let the input gt(x) of the t-th module be the output of the previous module, i.e., gt+1(x) = ft(gt(x)) + gt(x), then the summation of T weak module classifiers is identical to the output, F (x) in Equation 2, of the depth-T ResNet,\nF (x) \u2261 1 \u03b1T+1\nT\u2211\nt=1\nht(x), (4)\nwhere the weak module classifier is ht(x) def = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x) and the hypothesis module is ot(x) def = w\u22a4t gt(x).\nSee Appendix A for the proof. Note that we will abusively call F (x) the output of ResNet although a \u03c3\u0303 function is applied on top of F (x), mapping the output to the label space Y . We now analyze the telescoping sum boosting framework in Section 4."}, {"heading": "4 Telescoping Sum Boosting for Binary Classification", "text": "Recall that the weak module classifier is defined as ht(x) = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x). We restrict to bounded classifiers |ot(x)| \u2264 1. Throughout this paper, we will assume that the covariance between exp(\u2212yot+1(x)) and exp(yot(x)) is non-positive. We propose a learning algorithm whose training error decays exponentially with the number of weak module classifiers T under a weak learning condition in the following."}, {"heading": "4.1 Weak Learning Condition", "text": "Let \u03b3\u0303t def = Ei\u223cDt\u22121 [yiot(xi)] > 0 be the edge of the hypothesis module ot(x), where Dt\u22121 is the sample weight. A naive weak learning condition would be \u03b3\u03032t+1 \u2212 \u03b3\u03032t \u2265 \u03b3\u2032 2 > 0. However this naive weak learning condition is too strong. Even when \u03b3\u0303t is close to 1, we still seek weak learner which performances consistently better than \u03b3\u2032. Instead, we consider a much weaker weak learning condition as follows.\nDefinition 4.1 (\u03b3-Weak Learning Condition). Let \u03b3\u0303t = Ei\u223cDt\u22121 [yot(x)] > 0. A weak module classifier ht(x) = \u03b1t+1ot+1 \u2212\u03b1tot satisfies the \u03b3-weak learning condition with respect to a pair of distributions (Dt, Dt\u22121) if \u03b3\u03032t+1\u2212\u03b3\u0303 2 t\n1\u2212\u03b3\u03032t \u2265 \u03b32 > 0.\nThe weak learning condition is motivated by the learning theory and it is met in practice as shown\nin Figure 5a. For each weak module classifier, \u03b3t def = \u221a \u03b3\u03032t+1\u2212\u03b3\u03032t 1\u2212\u03b3\u03032t is defined as the \u201cedge\u201d which characterizes the correlation between the true labels y and the weak module classifier ht(x) over the training samples. The condition specified in Definition 4.1 is extremely mild as it requires the weak module classifier ht(x) to perform only slightly better than random guessing. As the hypothesis module ot(x) is bounded by 1, we obtain that |\u03b3\u0303t| \u2264 1."}, {"heading": "4.2 BoostResNet", "text": "We now propose a novel training algorithm for telescoping sum boosting under the setting of binaryclass classification as in Algorithm 1. In particular, we introduce a training procedure for deep ResNet in Algorithm 1 and 2, called BoostResNet, which only requires sequential training shallow ResNets. Each of the shallowResNet ft(gt(x))+gt(x) is combinedwith an auxiliary linear classifier wt+1 to form a hypothesis module ot+1(x). The weights of the ResNet are trained on these shallow ResNets and the auxiliary linear classifiers wt+1 are discarded (except for the top classifier). The training algorithm is therefore a module-by-module procedure following a bottom-to-up fashion as the outputs of the t-th module gt+1(x) are fed as the training examples to the next t+ 1-th module.\nAlgorithm 1 BoostResNet: telescoping sum boosting for binary-class classification Input: m labeled samples [(xi, yi)]m where yi \u2208 {\u22121,+1} and a threshold \u03b3 Output: {ft(\u00b7), \u2200t} andwT+1 \u22b2 Discardwt+1, \u2200t 6= T 1: Initialize t \u2190 0, \u03b3\u03030 \u2190 0, \u03b10 \u2190 0, o0(x) \u2190 0 2: Initialize sample weights at round 0: D0(i) \u2190 1/m, \u2200i \u2208 [m] 3: while \u03b3t > \u03b3 do 4: ft(\u00b7), \u03b1t+1,wt+1, ot+1(x) \u2190 Algorithm 2(gt(x), Dt, ot(x), \u03b1t)\n5: Compute \u03b3t \u2190 \u221a\n\u03b3\u03032t+1\u2212\u03b3\u03032t 1\u2212\u03b3\u03032t\n\u22b2 where \u03b3\u0303t+1 \u2190 Ei\u223cDt [yiot+1(xi)]\n6: UpdateDt+1(i) \u2190 Dt(i) exp(\u2212yiht(xi))m\u2211 i=1 Dt(i) exp[\u2212yiht(xi)] \u22b2 where ht(x) = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x) 7: t \u2190 t+ 1 8: end while 9: T \u2190 t\u2212 1\nTheorem 4.2. [ Training error bound ] The training error of a T -module telescoping sum boosting using Algorithm 1 and 2 decays exponentially with the number of modules T ,\nPr i\u223cS\n( \u03c3\u0303 ( \u2211\nt\nht (xi) ) 6= yi ) \u2264 e\u2212 12T\u03b32\nAlgorithm 2 BoostResNet: oracle implementation for training a ResNet module\nInput: gt(x),Dt,ot(x) and \u03b1t Output: ft(\u00b7), \u03b1t+1,wt+1 and ot+1(x) 1: (ft, \u03b1t+1,wt+1) \u2190 arg min\n(f,\u03b1,v) m\u2211 i=1 Dt(i) exp ( \u2212yi\u03b1v\u22a4 [f(gt(xi)) + gt(xi)] + yi\u03b1tot(xi) )\n2: ot+1(x) \u2190 w\u22a4t+1 [ft(gt(x)) + gt(x)]\nif \u2200t \u2208 [T ] the weak module classifier ht(x) satisfies the \u03b3-weak learning condition defined in Definition 4.1 and the covariance between exp(\u2212yot+1(x)) and exp(yot(x)) is non-positive \u2200t.\nThe training error of Algorithm 1 and 2 is guaranteed to decay exponentially with the ResNet depth even when each weak learning module ht(x) performs only slightly better than random (i.e., \u03b3 > 0). The assumption of the covariance between exp(\u2212yot+1(x)) and exp(yot(x)) being non-positive is suggesting that the weak module classifiers should not be adversarial, which is a reasonable assumption for ResNet. Refer to Appendix E for the algorithm and theoretical guarantees for multi-class classification."}, {"heading": "4.3 Oracle Implementation for ResNet", "text": "In Algorithm 2, the implementation of the oracle is equivalent to\n(ft, \u03b1t+1,wt+1) = arg min (f,\u03b1,v)\n1\nm\nm\u2211\ni=1\nexp ( \u2212yi\u03b1v\u22a4 [f(gt(xi)) + gt(xi)] ) (5)\nIn practice, there are various ways to implement Equation (5). For instance, Janzamin et. al. (Janzamin et al. , 2015) propose a tensor decomposition technique which decomposes a tensor formed by some transformation of the features x combined with labels y and recovers the weights of a one-hidden layer neural network with guarantees. One can also use back-propagation as numerous works have shown that gradient based training are relatively stable on shallow networks with identity loops (Hardt & Ma, 2016; He et al. , 2016)."}, {"heading": "4.4 Generalization Error Analysis", "text": "In this section, we analyze the generalization error to understand the possibility of over-fitting under Algorithm 1. The strong classifier or the ResNet is F (x) = \u2211 t ht(x)\n\u03b1T+1 . Now we define the margin for\nexample (x, y) as yF (x).\nFor simplicity, we consider MLP-ResNet with multiple channels n and assume that the weight vector connecting a neuron at layer t with its precedent layer neurons is l1 norm bounded by \u039bt,t\u22121. Recall that there exists a linear classifier w on top, and we restrict to l1 norm bounded classifiers, i.e., \u2016w\u20161 \u2264 C0 < \u221e. The expected training examples are l\u221e norm bounded r\u221e def = ES\u223cD [ maxi\u2208[m]\u2016xi\u2016\u221e ] < \u221e. We introduce Lemma 4.3 according to Lemma 2 from (Cortes et al. , 2016).\nLemma 4.3. Let D be a distribution over X \u00d7 Y and S be a sample of m examples chosen independently at random according to D. With probability at least 1\u2212 \u03b4, for \u03b8 > 0, the strong classifier F (x) (ResNet) satisfies that\nPr D (yF (x) \u2264 0) \u2264 Pr S (yF (x) \u2264 \u03b8) + 4C0r\u221e \u03b8\n\u221a log(2n)\n2m\nT\u2211\nt=1\n\u039bt + 2\n\u03b8\n\u221a logT\nm + \u03b2(\u03b8,m, T, \u03b4) (6)\nwhere \u039bt def = \u220ft t\u2032=1 2\u039bt\u2032,t\u2032\u22121 and \u03b2(\u03b8,m, T, \u03b4) def = \u221a\u2308 4 \u03b82 log ( \u03b82m log T )\u2309 log T m + log 2 \u03b4 2m .\nFrom Lemma 4.3, we obtain a generalization error bound in terms of margin boundPrS (yF (x) \u2264 \u03b8) and network complexity 4C0r\u221e\n\u03b8\n\u221a log(2n)\n2m \u2211T t=1 \u039bt+ 2 \u03b8 \u221a log T m +\u03b2(\u03b8,m, T, \u03b4). Larger margin bound\n(larger \u03b8) contributes positively to generalization accuracy, and l1 norm bounded weights (smaller\n\u2211T t=1 \u039bt ) are beneficial to control network complexity and to avoid overfitting. The dominant term in the network complexity is 4C0r\u221e \u03b8 \u221a log(2n) 2m \u2211T t=1 \u039bt which scales as least linearly with the depth T . See appendix C for the proof.\nThis lemma suggests that stronger weak module classifiers which produce higher accuracy predictions and larger edges, will yield larger margins and suffer less from over-fitting. The larger the\nvalue of \u03b8, the smaller the term 4C0r\u221e \u03b8\n\u221a log(2n)\n2m \u2211T t=1 \u039bt+ 2 \u03b8 \u221a log T m +\u03b2(\u03b8,m, T, \u03b4) is. With larger\nedges on the training set and when \u03b3\u0303T+1 < 1, we are able to choose larger values of \u03b8 while keeping the error term zero or close to zero."}, {"heading": "5 Experiments", "text": "We compare our proposed BoostResNet algorithm with e2eBP training a ResNet on the MNIST (LeCun et al. , 1998) and street view house numbers (SVHN) (Netzer et al. , 2011) benchmark datasets. Two different types of architectures are tested: multilayer perceptron residual network (MLP-ResNet) and convolutional neural network residual network (CNN-ResNet). In each experiment the architecture of both algorithms is identical, and they are both initialized with the same random seed. Our experiments were programmed in the Torch deep learning framework for Lua and executed on NVIDIA Tesla P100 GPUs.\nIn our training, a mini-batch size 100 is used for both BoostResNet and e2eBP. The learning rate is initialized at 1e-2 with a decaying rate of 1e-4. The optimization method used in e2eBP is the stateof-the-art Adaptive Moment Estimation (Adam) Kingma & Ba (2014) variant of SGD introduced earlier. The oracle we used in BoostResNet to solve the weak module classifier is Adam as well, but could be extended to tensor methods Janzamin et al. (2015), decision trees Safavian & Landgrebe (1991) or other nonlinear classifiers for non-differentiable data.\nResNet-MLP on MNIST The MNIST database (LeCun et al. , 1998) of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The data contains 10 number of classes. We test the performance of BoostResNet on MLP-ResNet using MNIST dataset, and compare it with e2eBP baseline. Each residual block is composed of an MLP with a single, 1024- dimensional hidden layer. The training and testing error between BoostResNet and e2eBP is in Figure 2 as a function of depth. Surprisingly, we observe a training error degradation for e2eBP although the ResNet\u2019s identity loop is supposed to alleviate this problem. Despite the presence of identity loops, the e2eBP eventually is susceptible to spurious local optima. Our proposed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases.\nResNet-CNN on SVHN SVHN (Netzer et al. , 2011) is a real-world image dataset, obtained from house numbers in Google Street View images, for recognizing digits and numbers in natural scene images and is therefore significantly harder than MNIST. Many of the images contain some distractors at the sides. The dataset contains over 600,000 digit images, an order of magnitude more labeled data. There are 604,388 digits for training, 26,032 digits for testing. We test the performance of BoostResNet on CNN-ResNet using SVHN dataset, and compare it with e2eBP. Each residual block is composed of a CNN using 15 3\u00d7 3 filters. The training and testing error between BoostResNet and e2eBP is in Figure 3. The training error degradation of e2eBP is alleviated on CNN-ResNet, but the learning is relatively slow compare to our BoostResNet.\nWe further investigate performance of e2eBP and BoostResNet on a shallower fine tuned network of 25 residual blocks and 50 layers in total 1. Each residual block contains convolution(5x5), batch normalization, ReLU, convolution(5x5), batch normalization, addition with identity loop and ReLU. The learning rate is fixed to be 10e-4 with no decay as suggested. The accuracy result is shown in Figure 4. Both algorithms are comparable using e2eBP\u2019s fine tuned network architecture and hyper parameters although the parameters of BoostResNet is not fine tuned.\nWeak Learning Condition Check The weak learning condition (Definition 4.1) inspired by learning theory is checked in Figure 5. The required better than random guessing edge \u03b3t is depicted in Figure 5a, it is always greater than 0 and our weak learning condition is thus non-vacuous. In Figure 5b, the representationswe learned using BoostResNet is increasingly better (for this classification task) as the depth increases.\nComputational and Memory Efficiency It is worth noting that BoostResNet training is memory efficient as the training process only requires parameters of two consecutive residual blocks to be in memory. Given that the limited GPU memory being one of the main bottlenecks for computational efficiency, BoostResNet requires significantly less training time than e2eBP in deep networks as a result of reduced communication overhead and the speed-up in shallow gradient forwarding and back-propagation. LetM1 be the memory required for one module, andM2 be the memory required for one linear classifier, the memory consumption isM1 +M2 by BoostResNet andM1T +M2 by e2eBP. Let the flops needed for gradient update over one module and one linear classifier be C1 and C2 respectively, the computation cost is C1 + C2 by BoostResNet and C1T + C2 by BoostResNet.\n1The network is from https://github.com/facebook/fb.resnet.torch.\nIn practice, a 50 layer CNN-ResNet requires 3 days to train using e2eBP but 1 day to train using BoostResNet."}, {"heading": "6 Conclusions and Future Works", "text": "Our proposed BoostResNet algorithm achieves exponentially decaying (with the depth T ) training error under the weak learning condition. BoostResNet is much more computationally efficient compared to end-to-end back-propagation in deep ResNet. More importantly, the memory required by BoostResNet is trivial compared to end-to-end back-propagation. It is particularly beneficial given the limited GPU memory and large network depth. Our learning framework is natural for nondifferentiable data. For instance, our learning framework is amendable to take weak learning oracles using tensor decomposition techniques. Tensor decomposition, a spectral learning framework with theoretical guarantees, is applied to learning one layer MLP in (Janzamin et al. , 2015). We plan to extend our learning framework to non-differentiable data using general weak learning oracles."}, {"heading": "A Proof for Lemma 3.2: the strong learner is a ResNet", "text": "Proof. In our algorithm, the input of the next module is the output of the current module\ngt+1(x) = ft(gt(x)) + gt(x), (7)\nwe thus obtain that each weak learning module is\nht(x) = \u03b1t+1w \u22a4 t+1(ft(gt(x)) + gt(x))\u2212 \u03b1tw\u22a4t gt(x) (8)\n= \u03b1t+1w \u22a4 t+1gt+1(x)\u2212 \u03b1tw\u22a4t gt(x), (9)\nand similarly\nht+1 = \u03b1t+2w \u22a4 t+2gt+2(x)\u2212 \u03b1t+1w\u22a4t+1gt+1(x). (10)\nTherefore the sum over ht(x) and ht+1(x) is\nht(x) + ht+1(x) = \u03b1t+2w \u22a4 t+2gt+2(x) \u2212 \u03b1tw\u22a4t gt(x) (11)\nAnd we further see that the weighted summation over all ht(x) is a telescoping sum T\u2211\nt=1\nht(x) = \u03b1T+1w \u22a4 T+1gT+1(x)\u2212 \u03b11w\u22a41 g1(x) = \u03b1T+1w\u22a4T+1gT+1(x). (12)"}, {"heading": "B Proof for Theorem 4.2: binary class telescoping sum boosting theory", "text": "Proof. We will use a 0-1 loss to measure the training error. In our analysis, the 0-1 loss is bounded by exponential loss.\nThe training error is therefore bounded by\nPr i\u223cD1\n(p(\u03b1T+1w \u22a4 T+1gT+1(xi)) 6= yi) (13)\n= m\u2211\ni=1\nD1(i)1{\u03c3\u0303(\u03b1T+1w\u22a4T+1gT+1(xi)) 6= yi} (14)\n=\nm\u2211\ni=1\nD1(i)1\n{ \u03c3\u0303 ( T\u2211\nt=1\nht(xi) ) 6= yi } (15)\n\u2264 m\u2211\ni=1\nD1(i) exp { \u2212yi T\u2211\nt=1\nht(xi)\n} (16)\n=\nm\u2211\ni=1\nDT+1(i)\nT\u220f\nt=1\nZt (17)\n= T\u220f\nt=1\nZt (18)\nwhere Zt = m\u2211 i=1 Dt(i) exp (\u2212yiht(xi)).\nWe choose \u03b1t+1 to minimize Zt.\n\u2202Zt \u2202\u03b1t+1\n= \u2212 m\u2211\ni=1\nDt(i)yiot+1 exp (\u2212yiht(xi)) (19)\n= \u2212Zt m\u2211\ni=1\nDt+1(i)yiot+1(i) = 0 (20)\nFurthermore each learning module is bounded as we see in the following analysis. We obtain\nZt = m\u2211\ni=1\nDt(i)e \u2212yiht(xi) (21)\n=\nm\u2211\ni=1\nDt(i)e \u2212\u03b1t+1yiot+1(xi)+\u03b1tyiot(xi) (22)\n\u2264 m\u2211\ni=1\nDt(i)e \u2212\u03b1t+1yiot+1(xi)\nm\u2211\ni=1\nDt(i)e \u03b1tyiot(xi) (23)\n=\nm\u2211\ni=1\nDt(i)e \u2212\u03b1t+1\n1+yiot+1(xi) 2 +\u03b1t+1 1\u2212yiot+1(xi) 2\nm\u2211\ni=1\nDt(i)e \u03b1t\n1+yiot(xi) 2 \u2212\u03b1t 1\u2212yiot(xi)\n2 (24)\n\u2264 m\u2211\ni=1\nDt(i)\n( 1 + yiot+1(xi)\n2 e\u2212\u03b1t+1 + 1\u2212 yiot+1(xi) 2\ne\u03b1t+1 ) \u00b7 (25)\nm\u2211\ni=1\nDt(i)\n( 1 + yiot(xi)\n2 e\u03b1t + 1\u2212 yiot(xi) 2\ne\u2212\u03b1t )\n(26)\n=\nm\u2211\ni=1\nDt(i)\n( 1 + yiot+1(xi)\n2 e\u2212\u03b1t+1 + 1\u2212 yiot+1(xi) 2\ne\u03b1t+1 ) e\u03b1t + e\u2212\u03b1t\n2 (27)\n= m\u2211\ni=1\nDt(i)\n( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 yiot+1(xi)\n) e\u03b1t + e\u2212\u03b1t\n2 (28)\n=\n( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 \u03b3\u0303t\n) e\u03b1t + e\u2212\u03b1t\n2 (29)\nEquation (23) is due to the non-positive correlation between exp(\u2212yot+1(x)) and exp(yot(x)). Jensen\u2019s inequality in Equation (26) holds only when |yiot+1(xi)| \u2264 1 which is satisfied by the definition of the weak learning module.\nThe algorithm chooses \u03b1t+1 to minimize Zt. We achieve an upper bound on Zt,\n\u221a 1\u2212\u03b3\u03032t\n1\u2212\u03b3\u03032t\u22121 by\nminimizing the bound in Equation (29)\nZt \u2264 ( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 \u03b3\u0303t\n) e\u03b1t + e\u2212\u03b1t\n2\n\u2223\u2223\u2223\u2223 \u03b1t+1= 1 2 ln( 1+\u03b3\u0303t 1\u2212\u03b3\u0303t ) (30)\n= \u221a 1\u2212 \u03b3\u03032t 1\u2212 \u03b3\u03032t\u22121 = \u221a 1\u2212 \u03b32t (31)\nTherefore over the T modules, the training error is upper bounded as follows\nPr i\u223cD\n(p(\u03b1T+1w \u22a4 T+1gT+1(xi))) 6= yi) \u2264\nT\u220f\nt=1\n\u221a 1\u2212 \u03b32t \u2264 T\u220f\nt=1\n\u221a 1\u2212 \u03b32 = exp ( \u22121 2 T\u03b32 ) (32)\nOverall, Algorithm 1 leads us to consistent learning of ResNet."}, {"heading": "C Proof for Lemma 4.3: Generalization Bound", "text": "Rademacher complexity technique is powerful for measuring the complexity of H any family of functions h : X \u2192 R, based on easiness of fitting any dataset using classifiers in H (where X is any space). Let S =< x1, . . . , xm > be a sample of m points in X . The empirical Rademacher complexity ofH with respect to S is defined to be\nRS(H) def= E\u03c3 [ sup h\u2208H 1 m m\u2211\ni=1\n\u03c3ih(xi)\n] (33)\nwhere \u03c3 is the Rademacher variable. The Rademacher complexity on m data points drawn from distribution D is defined by Rm(H) = ES\u223cD [RS(H)] . (34) Proposition C.1. (Theorem 1 Cortes et al. (2014)) LetH be a hypothesis set admitting a decomposition H = \u222ali=1Hi for some l > 1. Hi are distinct hypothesis sets. Let S be a random sequence of m points chosen independently from X according to some distribution D. For \u03b8 > 0 and any H = \u2211T t=1 ht, with probability at least 1\u2212 \u03b4,\nPr D (yH(x) \u2264 0) \u2264 Pr S (yH(x) \u2264 \u03b8) + 4 \u03b8\nT\u2211\nt=1\nRm(Hkt) + 2\n\u03b8\n\u221a log l\nm\n+\n\u221a\n\u2308 4 \u03b82 log\n( \u03b82m\nlog l\n) \u2309 log l\nm +\nlog 2 \u03b4\n2m (35)\nfor all ht \u2208 Hkt . Lemma C.2. Let h\u0303 = w\u0303\u22a4 f\u0303 , where w\u0303 \u2208 Rn, f\u0303 \u2208 Rn. Let H\u0303 and F\u0303 be two hypothesis sets, and h\u0303 \u2208 H\u0303 , f\u0303j \u2208 F\u0303 , \u2200j \u2208 [n]. The Rademacher complexity of H\u0303 and F\u0303 with respect to m points from D are related as follows\nRm(H\u0303) = \u2016w\u0303\u20161Rm(F\u0303). (36)\nC.1 ResNet Module Hypothesis Space\nLet n be the number of channels in ResNet, i.e., the number of input or output neurons in a module ft(gt(x)). We have proved that ResNet is equivalent as\nF (x) = w\u22a4 T\u2211\nt=1\nf(gt(x)) (37)\nWe define the family of functions that each neuron ft,j , \u2200j \u2208 [n] belong to as Ft = {x \u2192 ut\u22121,j(\u03c3 \u25e6 ft\u22121)(x) : ut\u22121,j \u2208 Rn, \u2016ut\u22121,j\u20161 \u2264 \u039bt,t\u22121, ft\u22121,i \u2208 Ft\u22121} (38) where ut\u22121,j denotes the vector of weights for connections from unit j to a lower layer t \u2212 1, \u03c3 \u25e6 ft\u22121 denotes element-wise nonlinear transformation on ft\u22121. The output layer of each module is connected to the output layer of previous module. We consider 1-layer modules for convenience of analysis.\nTherefore in ResNet with probability at least 1\u2212 \u03b4,\nPr D (yF (x) \u2264 0) \u2264 Pr S (yF (x) \u2264 \u03b8) + 4 \u03b8\nT\u2211\nt=1\n\u2016w\u20161Rm(Ft) + 2\n\u03b8\n\u221a logT\nm\n+\n\u221a\n\u2308 4 \u03b82 log\n( \u03b82m\nlogT\n) \u2309 logT\nm +\nlog 2 \u03b4\n2m (39)\nfor all ft \u2208 Ft.\nDefine the maximum infinity norm over samples as r\u221e def = ES\u223cD [ maxi\u2208[m]\u2016xi\u2016\u221e ] and the product of l1 norm bound on weights as \u039bt def = \u220ft\nt\u2032=1 2\u039bt\u2032,t\u2032\u22121. According to lemma 2 of Cortes et al. (2016), the empirical Rademacher complexity is bounded as a function of r\u221e, \u039bt and n:\nRm(Ft) \u2264 r\u221e\u039bt \u221a log(2n)\n2m (40)\nOverall, with probability at least 1\u2212 \u03b4,\nPr D (yF (x) \u2264 0) \u2264 Pr S (yF (x) \u2264 \u03b8) +\n4\u2016w\u20161r\u221e \u221a log(2n) 2m\n\u03b8\nT\u2211\nt=1\n\u039bt\n+ 2\n\u03b8\n\u221a logT\nm +\n\u221a\n\u2308 4 \u03b82 log\n( \u03b82m\nlogT\n) \u2309 logT\nm +\nlog 2 \u03b4\n2m (41)\nfor all ft \u2208 Ft."}, {"heading": "D Proof for Theorem D: Margin and Generalization Bound", "text": "Theorem D.1. [ Generalization error bound ] Given algorithm 1, the fraction of training examples with margin at most \u03b8 is at most (1 + 21\u221a \u03b3\u0303T+1 \u22121 ) \u03b8 2 exp(\u2212 12\u03b32T ). And the generalization error PrD(yF (x) \u2264 0) satisfies\nPr D (yF (x) \u2264 0) \u2264 (1 + 21\n\u03b3\u0303T+1 \u2212 1)\n\u03b8 2 exp(\u22121\n2 \u03b32T )\n+ 4C0r\u221e\n\u03b8\n\u221a log(2n)\n2m\nT\u2211\nt=1\n\u039bt + 2\n\u03b8\n\u221a logT\nm + \u03b2(\u03b8,m, T, \u03b4) (42)\nwith probability at least 1\u2212 \u03b4 for \u03b2(\u03b8,m, T, \u03b4) def= \u221a\u2308\n4 \u03b82\nlog (\n\u03b82m log T )\u2309 log T m + log 2 \u03b4 2m .\nNow the proof for Theorem D is the following.\nProof. The fraction of examples in sample set S being smaller than \u03b8 is bounded\nPr S (yF (x) \u2264 \u03b8) \u2264 1 m\nm\u2211\ni=1\n1{yiF (xi) \u2264 \u03b8} (43)\n= 1\nm\nm\u2211\ni=1\n1{yi T\u2211\nt=1\nht(xi) \u2264 \u03b8\u03b1T+1} (44)\n\u2264 1 m\nm\u2211\ni=1\nexp(\u2212yi T\u2211\nt=1\nht(xi) + \u03b8\u03b1T+1) (45)\n= exp(\u03b8\u03b1T+1) 1\nm\nm\u2211\ni=1\nexp(\u2212yi T\u2211\nt=1\nht(xi)) (46)\n= exp(\u03b8\u03b1T+1)\nT\u220f\nt=1\nZt (47)\nTo bound exp(\u03b8\u03b1T+1) = \u221a (1+\u03b3\u0303T+11\u2212\u03b3\u0303T+1 ) \u03b8 , we first bound \u03b3\u0303T+1: We know that \u2211T t=1 \u220fT t\u2032=t+1(1 \u2212 \u03b32t\u2032)\u03b3 2 t \u2264 (1 \u2212 \u03b32)T\u2212t\u03b32 for all \u2200\u03b3t \u2265 \u03b32 + \u01eb if \u03b32 \u2265 1\u2212\u01eb2 . Therefore \u2200 \u03b3t \u2265 \u03b32 + \u01eb and \u03b32 \u2265 1\u2212\u01eb2\n\u03b3\u03032T+1 = (1\u2212 \u03b32T )\u03b3\u03032T + \u03b32T (48)\n=\nT\u2211\nt=1\nT\u220f\nt\u2032=t+1\n(1 \u2212 \u03b32t\u2032)\u03b32t + T\u220f\nt=1\n(1\u2212 \u03b32t )\u03b3\u030321 (49)\n\u2264 T\u2211\nt=1\n(1\u2212 \u03b32)T\u2212t\u03b32 + (1\u2212 \u03b32)T \u03b3\u030321 (50)\n=\nT\u22121\u2211\nt=0\n(1\u2212 \u03b32)t\u03b32 + (1\u2212 \u03b32)T \u03b3\u030321 (51)\n= 1\u2212 (1\u2212 \u03b32)T + (1\u2212 \u03b32)T \u03b3\u030321 (52) = 1\u2212 (1\u2212 \u03b3\u030321)(1\u2212 \u03b32)T (53)\nTherefore\nPr S (yF (x) \u2264 \u03b8) \u2264 exp(\u03b8\u03b1T+1)\nT\u220f\nt=1\nZt (54)\n= ( 1 + \u03b3\u0303T+1 1\u2212 \u03b3\u0303T+1 ) \u03b8 2\nT\u220f\nt=1\nZt (55)\n= ( 1 + \u03b3\u0303T+1 1\u2212 \u03b3\u0303T+1 ) \u03b8 2\nT\u220f\nt=1\n\u221a 1\u2212 \u03b32t (56)\n= (1 + 2\n1 \u03b3\u0303T+1\n\u2212 1) \u03b8 2 exp(\u22121 2 \u03b32T ) (57)\n\u2264 (1 + 21\u221a 1\u2212(1\u2212\u03b3\u030321)(1\u2212\u03b32)T \u2212 1) \u03b8 2 exp(\u22121 2 \u03b32T ) (58)\nAs T \u2192 \u221e, PrS(yF (x) \u2264 \u03b8) \u2264 0 as exp(\u2212 12\u03b32T ) decays faster than (1 + 21\u221a 1\u2212(1\u2212\u03b3\u030321 )(1\u2212\u03b3 2)T \u22121 ) \u03b8 2 ."}, {"heading": "E Telescoping Sum Boosting for Multi-calss Classification", "text": "Recall that the weak module classifier is defined as\nht(x) = \u03b1t+1ot+1(x)\u2212 \u03b1tot(x) \u2208 RC , (59) where ot(x) \u2208 \u2206C\u22121. The weak learning condition for multi-class classification is different from the binary classification stated in the previous section, although minimal demands placed on the weak module classifier require prediction better than random on any distribution over the training set intuitively.\nWe now define the weak learning condition. It is again inspired by the slightly better than random idea, but requires a more sophisticated analysis in the multi-class setting.\nE.1 Cost Matrix\nIn order to characterize the training error, we introduce the cost matrix C \u2208 Rm\u00d7C where each row denote the cost incurred by classifying that example into one of the C categories. We will bound the training error using exponential loss, and under the exponential loss function defined as in Definition F.1, the optimal cost function used for best possible training error is therefore determined.\nLemma E.1. The optimal cost function under the exponential loss is\nCt(i, l) = { exp (st(xi, l)\u2212 st(xi, yi)) if l 6= yi \u2212 \u2211\nl\u2032 6=yi exp (st(xi, l\n\u2032)\u2212 st(xi, yi)) if l = yi (60)\nwhere st(x) = t\u2211\n\u03c4=1 h\u03c4 (x).\nE.2 Weak Learning Condition\nDefinition E.2. Let \u03b3\u0303t+1 = \u2212\nm\u2211 i=1 <Ct(i,:),ot+1(xi)>\nm\u2211\ni=1\n\u2211\nl 6=yi\nCt(i,l) and \u03b3\u0303t =\n\u2212 m\u2211\ni=1 <Ct\u22121(i,:),ot(xi)>\nm\u2211\ni=1\n\u2211\nl 6=yi\nCt\u22121(i,l) . A multi-class\nweak module classifier ht(x) = \u03b1t+1ot+1(x) \u2212 \u03b1tot(x) satisfies the \u03b3-weak learning condition if \u03b3\u03032t+1\u2212\u03b3\u0303 2 t\n1\u2212\u03b3\u03032t \u2265 \u03b32 > 0.\nWe propose a novel learning algorithm using the optimal edge-over-randomcost function for training ResNet under multi-class classification task as in Algorithm 3.\nAlgorithm 3 BoostResNet: telescoping sum boosting for multi-class classification Input: Given (x1, y1), . . . (xm, ym) where yi \u2208 Y = {1, . . . , C} and a threshold \u03b3 Output: {ft(\u00b7),\u2200t} andWT+1 \u22b2 Discardwt+1, \u2200t 6= T 1: Initialize t \u2190 0, \u03b3\u03030 \u2190 1, \u03b10 \u2190 0, o0 \u2190 0 \u2208 RC , s0(xi, l) = 0, \u2200i \u2208 [m], l \u2208 Y 2: Initialize cost functionC0(i, l) \u2190 { 1 if l 6= yi 1\u2212 C if l = yi\n3: while \u03b3t > \u03b3 do 4: ft(\u00b7), \u03b1t+1,Wt+1, ot+1(x) \u2190 Algorithm 4(gt(x),Ct, ot(x), \u03b1t) 5: Compute \u03b3t \u2190 \u221a\n\u03b3\u03032t+1\u2212\u03b3\u03032t 1\u2212\u03b3\u03032t\n\u22b2 where \u03b3\u0303t+1 \u2190 \u2212\nm\u2211\ni=1 Ct(i,:)\u00b7ot+1(xi) m\u2211\ni=1\n\u2211\nl 6=yi\nCt(i,l)\n6: Update st+1(xi, l) \u2190 st(xi, l) + ht(xi, l) \u22b2 where ht(xi, l) = \u03b1t+1ot+1(xi, l)\u2212 \u03b1tot(xi, l) 7: Update cost functionCt+1(i, l) \u2190 { est+1(xi,l)\u2212st+1(xi,yi) if l 6= yi \u2212 \u2211\nl\u2032 6=yi est+1(xi,l \u2032)\u2212st+1(xi,yi) if l = yi\n8: t \u2190 t+ 1 9: end while 10: T \u2190 t\u2212 1\nAlgorithm 4 BoostResNet: oracle implementation for training a ResNet module (multi-class)\nInput: gt(x),st,ot(x) and \u03b1t Output: ft(\u00b7), \u03b1t+1,Wt+1 and ot+1(x) 1: (ft, \u03b1t+1,Wt+1) \u2190 arg min\n(f,\u03b1,V ) m\u2211 i=1 \u2211 l 6=yi e\u03b1V \u22a4[f(gt(xi),l)\u2212f(gt(xi),yi)+gt(xi,l)\u2212gt(xi,yi)]\n2: ot+1(x) \u2190 W\u22a4t+1 [ft(gt(x)) + gt(x)]\nTheorem E.3. The training error of a T -module ResNet using Algorithm 3and 4 decays exponentially with the depth of the ResNet T ,\nC \u2212 1 m\nm\u2211\ni=1\nLexp\u03b7 (sT (xi)) \u2264 (C \u2212 1)e\u2212 1 2T\u03b3 2\n(61)\nif the weak module classifier ht(x) satisfies the \u03b3-weak learning condition \u2200t \u2208 [T ].\nThe exponential loss function defined as in Definition F.1\nE.3 Oracle Implementation\nWe implement an oracle to minimize Zt def = m\u2211 i=1 \u2211 l 6=yi est(xi,l)\u2212st(xi,yi)eht(xi,l)\u2212ht(xi,yi) given current state st and hypothesis module ot(x). Therefore minimizing Zt is equivalent to the following.\nmin (f,\u03b1,V )\nm\u2211\ni=1\n\u2211\nl 6=yi\nest(xi,l)\u2212st(xi,yi)e\u2212\u03b1t(ot(xi,l)\u2212ot(xi,yi))e\u03b1V \u22a4[f(gt(xi),l)\u2212f(gt(xi),yi)+gt(xi,l)\u2212gt(xi,yi)]\n(62)\n\u2261 min (f,\u03b1,V )\nm\u2211\ni=1\n\u2211\nl 6=yi\ne\u03b1V \u22a4[f(gt(xi),l)\u2212f(gt(xi),yi)+gt(xi,l)\u2212gt(xi,yi)] (63)\n\u2261 min \u03b1,f,v\nm\u2211\ni=1\ne\u2212\u03b1v \u22a4[f(xi,yi)+gt(xi,yi)]\n\u2211\nl 6=yi\ne\u03b1v \u22a4[f(xi,l)+gt(xi,l)] (64)"}, {"heading": "F Proof for Theorem E.3 multi-class boosting theory", "text": "Proof. To characterize the training error, we use the exponential loss function\nDefinition F.1. Define loss function for a multi-class hypothesisH(xi) on a sample (xi, yi) as\nLexp\u03b7 (H(xi), yi) = \u2211\nl 6=yi\nexp ((H(xi, l)\u2212H(xi, yi))) . (65)\nDefine the accumulated weak learner st(xi, l) = t\u2211\nt\u2032=1\nht\u2032(xi, l) and the loss Zt =\nm\u2211 i=1 \u2211 l 6=yi exp(st(xi, l)\u2212 st(xi, yi)) exp(ht(xi, l)\u2212 ht(xi, yi)).\nRecall that st(xi, l) = t\u2211\nt\u2032=1\nht\u2032(xi, l) = \u03b1t+1W \u22a4 t+1gt+1(xi), the loss for a T -module multiclass\nresnet is thus\nPr i\u223cD1\n(p(\u03b1T+1W \u22a4 T+1gT+1(xi)) 6= yi) \u2264\n1\nm\nm\u2211\ni=1\nLexp\u03b7 (sT (xi)) (66)\n\u2264 1 m\nm\u2211\ni=1\n\u2211\nl 6=yi\nexp (\u03b7(sT (xi, l)\u2212 sT (xi, yi))) (67)\n\u2264 1 m ZT (68)\n=\nT\u220f\nt=1\nZt Zt\u22121\n(69)\nNote that Z0 = 1 m as the initial accumulated weak learners s0(xi, l) = 0.\nThe loss fraction between module t and t\u2212 1, Zt Zt\u22121 , is related to Zt\u2212Zt\u22121 as ZtZt\u22121 = Zt\u2212Zt\u22121 Zt\u22121 +1.\nThe Zt is bounded Zt = m\u2211\ni=1\n\u2211\nl 6=yi\nexp(st(xi, l)\u2212 st(i, yi) + ht(xi, l)\u2212 ht(xi, yi)) (70)\n\u2264 m\u2211\ni=1\n\u2211\nl 6=yi\nest(xi,l)\u2212st(xi,yi)e\u03b1t+1ot+1(xi,l)\u2212\u03b1t+1ot+1(xi,yi) m\u2211\ni=1\n\u2211\nl 6=yi\nest(xi,l)\u2212st(xi,yi)e\u2212\u03b1tot(xi,l)+\u03b1tot(xi,yi)\n(71)\n\u2264 m\u2211\ni=1\n\u2211\nl 6=yi\nest(xi,l)\u2212st(xi,yi) ( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2\n(ot+1(xi, yi)\u2212 ot+1(xi, l)) )\nm\u2211\ni=1\n\u2211\nl 6=yi\nest\u22121(xi,l)\u2212st\u22121(xi,yi) ( e\u03b1t + e\u2212\u03b1t\n2\n) (72)\n=( e\u2212\u03b1t+1 + e\u03b1t+1 \u2212 2\n2 Zt\u22121 + e\u03b1t+1 \u2212 e\u2212\u03b1t+1 2 m\u2211\ni=1\n< Ct(xi, :), ot+1(xi, :) >)\n( e\u03b1t + e\u2212\u03b1t\n2\n)\n(73)\n\u2264(e \u2212\u03b1t+1 + e\u03b1t+1 \u2212 2\n2 Zt\u22121 + e\u03b1t+1 \u2212 e\u2212\u03b1t+1 2 m\u2211\ni=1\n< Ct(xi, :), U\u03b3\u0303t(xi, :) >)\n( e\u03b1t + e\u2212\u03b1t\n2\n)\n(74)\n=( e\u2212\u03b1t+1 + e\u03b1t+1 \u2212 2\n2 Zt\u22121 + e\u03b1t+1 \u2212 e\u2212\u03b1t+1 2\n(\u2212\u03b3\u0303t)Zt\u22121) ( e\u03b1t + e\u2212\u03b1t\n2\n) (75)\nTherefore Zt Zt\u22121 \u2264 ( e\u2212\u03b1t+1 + e\u03b1t+1 2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 \u03b3\u0303t )( e\u03b1t + e\u2212\u03b1t 2 ) (76)\nThe algorithm chooses \u03b1t+1 to minimize Zt. We achieve an upper bound on Zt,\n\u221a 1\u2212\u03b3\u03032t\n1\u2212\u03b3\u0303t\u221212\nby minimizing the bound in Equation (76)\nZt \u2264 ( e\u2212\u03b1t+1 + e\u03b1t+1\n2 + e\u2212\u03b1t+1 \u2212 e\u03b1t+1 2 \u03b3\u0303t\n) e\u03b1t + e\u2212\u03b1t\n2\n\u2223\u2223\u2223\u2223 \u03b1t+1= 1 2 ln( 1+\u03b3\u0303t 1\u2212\u03b3\u0303t ) (77)\n= \u221a 1\u2212 \u03b3\u03032t 1\u2212 \u03b3\u03032t\u22121 = \u221a 1\u2212 \u03b32t (78)\nTherefore over the T modules, the training error is upper bounded as follows\nPr i\u223cD\n(p(\u03b1T+1w \u22a4 T+1gT+1(xi))) 6= yi) \u2264\nT\u220f\nt=1\n\u221a 1\u2212 \u03b32t \u2264 T\u220f\nt=1\n\u221a 1\u2212 \u03b32 = exp ( \u22121 2 T\u03b32 ) (79)\nOverall, Algorithm 3 and 4 leads us to consistent learning of ResNet."}, {"heading": "G Minimal Weak Learning Condition", "text": "Mukherjee and SchapireMukherjee & Schapire (2013) introduced a sufficient and necessary weak learning condition based on a concept called edge over random.\nIt is intuitive to restrict the cost matrices to a space of edge-over-random cost-matrices Ceor who put the least cost on the correct label. Definition G.1. The space of edge-over-random cost matrices Ceor \u2286 Rm\u00d7C is the space of cost matrices C \u2208 Ceor that satisfies\nC(i, yi) \u2264 C(i, l), \u2200l 6= yi, \u2200i \u2208 {1, . . . ,m}. (80)\nLet us define a random classifier as a baseline predictor B \u2208 Rm\u00d7C , where each row lies on a simplexB(i, :) \u2208 \u2206{1, . . . , C}. We consider the space of baselines, called edge-over-random,who have a faint clue about the correct answer. Definition G.2. The space of edge-over-random baselines Mukherjee & Schapire (2013) Beor\u03b3 \u2286 R\nm\u00d7C is the space of baselinesB \u2208 Beor\u03b3 that satisfy B(i, yi) \u2265 B(i, l) + \u03b3, \u2200l 6= yi, \u2200i \u2208 {1, . . . ,m}. (81)\nNote that cost function for sample xi is C(i, :) \u2208 R1\u00d7C , weak module classifier vector h(txi) \u2208 RC and baseline predictorB(i, :) \u2208 R1\u00d7C . Definition G.3. A multi-class weak module classifier ht(x) = \u03b1t+1ot+1\u2212\u03b1tot satisfies the \u03b3-weak learning condition if \u2200C \u2208 Ceor, \u2203 hV,f(\u00b7),\u03b1t such that\nm\u2211\ni=1\nC(i, :)h V,f(\u00b7),W t (xi) \u2264 max\nB\u2208Beor\u03b3\nm\u2211\ni=1\nC(i, :)B(i, :)\u22a4. (82)\nLemma G.4. A weak classifier space H is boostable if and only if H satisfies the weak-learning condition defined in Definition G.3.\nRefer to Mukherjee and Schapire Mukherjee & Schapire (2013) for the proof."}, {"heading": "H Experiments", "text": "We investigate e2eBP training performance on various depth ResNet. Surprisingly, we observe a training error degradation for e2eBP although the ResNet\u2019s identity loop is supposed to alleviate this problem. Despite the presence of identity loops, the e2eBP eventually is susceptible to spurious local optima. This phenomenon is explored further in Figures 6a and 6b, which respectively show how training and test accuracies vary throughout the fitting process. Our proposed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases."}], "references": [{"title": "The Shattered Gradients Problem: If resnets are the answer, then what is the question? arXiv preprint arXiv:1702.08591", "author": ["Balduzzi", "David", "Frean", "Marcus", "Leary", "Lennox", "JP Lewis", "Ma", "Kurt Wan-Duo", "McWilliams", "Brian"], "venue": null, "citeRegEx": "Balduzzi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2017}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Deep boosting. Pages 1179\u20131187", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Syed", "Umar"], "venue": "of: Proceedings of the 31st International Conference on Machine Learning (ICML-14)", "citeRegEx": "Cortes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2014}, {"title": "Adaptive subgradientmethods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "Pages 23\u201337", "citeRegEx": "Freund et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1995}, {"title": "Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition", "author": ["Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang"], "venue": null, "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Fast r-cnn", "author": ["Girshick", "Ross."], "venue": "Pages 1440\u20131448 of: Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "Girshick and Ross.,? 2015", "shortCiteRegEx": "Girshick and Ross.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation. Pages 580\u2013587", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "of: Proceedings of the IEEE conference on computer vision and pattern recognition", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks. Pages 249\u2013256", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "of: Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Identity Matters in Deep Learning", "author": ["Hardt", "Moritz", "Ma", "Tengyu"], "venue": "arXiv preprint arXiv:1611.04231", "citeRegEx": "Hardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2016}, {"title": "Convolutional neural networks at constrained time cost. Pages 5353\u20135360", "author": ["He", "Kaiming", "Sun", "Jian"], "venue": "of: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition. Pages 346\u2013361", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "of: European Conference on Computer Vision. Springer", "citeRegEx": "He et al\\.,? \\Q2014\\E", "shortCiteRegEx": "He et al\\.", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "Pages 1026\u20131034 of: Proceedings of the IEEE international conference on computer vision", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition. Pages 770\u2013778", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "of: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima"], "venue": "arXiv preprint arXiv:1506.08473", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. Pages 1097\u20131105 of: Advances in neural information processing systems", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient backprop. Pages 9\u201348 of: Neural networks: Tricks of the trade", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation. Pages 3431\u20133440", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "of: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Deep vs. shallow networks: An approximation theory perspective", "author": ["Mhaskar", "Hrushikesh N", "Poggio", "Tomaso"], "venue": "Analysis and Applications,", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "A theory of multiclass boosting", "author": ["Mukherjee", "Indraneel", "Schapire", "Robert E"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mukherjee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2013}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)", "author": ["Nesterov", "Yurii."], "venue": "Pages 543\u2013547 of: Doklady an SSSR, vol. 269.", "citeRegEx": "Nesterov and Yurii.,? 1983", "shortCiteRegEx": "Nesterov and Yurii.", "year": 1983}, {"title": "Reading digits in natural images with unsupervised feature learning. Page 5 of: NIPS workshop on deep learning and unsupervised feature", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On the momentum term in gradient descent learning algorithms", "author": ["Qian", "Ning."], "venue": "Neural networks, 12(1), 145\u2013151.", "citeRegEx": "Qian and Ning.,? 1999", "shortCiteRegEx": "Qian and Ning.", "year": 1999}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. Pages 91\u201399 of: Advances in neural information processing systems", "author": ["Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross", "Sun", "Jian"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning", "author": ["Rubinstein", "Reuven Y", "Kroese", "Dirk P"], "venue": null, "citeRegEx": "Rubinstein et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2013}, {"title": "A survey of decision tree classifier methodology", "author": ["Safavian", "S Rasoul", "Landgrebe", "David"], "venue": "IEEE transactions on systems, man, and cybernetics,", "citeRegEx": "Safavian et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Safavian et al\\.", "year": 1991}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "SelfieBoost: A Boosting Algorithm for Deep Learning", "author": ["Shalev-Shwartz", "Shai."], "venue": "arXiv preprint arXiv:1411.3436.", "citeRegEx": "Shalev.Shwartz and Shai.,? 2014", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Convolutional networks and learning invariant to homogeneous multiplicative scalings", "author": ["Tygert", "Mark", "Szlam", "Arthur", "Chintala", "Soumith", "Ranzato", "Marc\u2019Aurelio", "Tian", "Yuandong", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1506.08230", "citeRegEx": "Tygert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tygert et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Zeiler", "Matthew D."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler and D.,? 2012", "shortCiteRegEx": "Zeiler and D.", "year": 2012}, {"title": "Visualizing and understanding convolutional networks. Pages 818\u2013833", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "of: European conference on computer vision. Springer", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "LetH be a hypothesis set admitting a decomposition H", "author": ["Cortes"], "venue": null, "citeRegEx": "Cortes,? \\Q2014\\E", "shortCiteRegEx": "Cortes", "year": 2014}, {"title": "Let us define a random classifier as a baseline predictor B \u2208 Rm\u00d7C , where each row lies on a simplexB(i", "author": [], "venue": "Beor", "citeRegEx": "C..,? \\Q2013\\E", "shortCiteRegEx": "C..", "year": 2013}, {"title": "A weak classifier space H is boostable if and only if H satisfies the weak-learning condition defined in Definition G.3", "author": ["Lemma G"], "venue": null, "citeRegEx": "G.4.,? \\Q2013\\E", "shortCiteRegEx": "G.4.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "5 Experiments We compare our proposed BoostResNet algorithm with e2eBP training a ResNet on the MNIST (LeCun et al. , 1998) and street view house numbers (SVHN) (Netzer et al. , 2011) benchmark datasets. Two different types of architectures are tested: multilayer perceptron residual network (MLP-ResNet) and convolutional neural network residual network (CNN-ResNet). In each experiment the architecture of both algorithms is identical, and they are both initialized with the same random seed. Our experiments were programmed in the Torch deep learning framework for Lua and executed on NVIDIA Tesla P100 GPUs. In our training, a mini-batch size 100 is used for both BoostResNet and e2eBP. The learning rate is initialized at 1e-2 with a decaying rate of 1e-4. The optimization method used in e2eBP is the stateof-the-art Adaptive Moment Estimation (Adam) Kingma & Ba (2014) variant of SGD introduced earlier.", "startOffset": 103, "endOffset": 876}, {"referenceID": 15, "context": "The oracle we used in BoostResNet to solve the weak module classifier is Adam as well, but could be extended to tensor methods Janzamin et al. (2015), decision trees Safavian & Landgrebe (1991) or other nonlinear classifiers for non-differentiable data.", "startOffset": 127, "endOffset": 150}, {"referenceID": 15, "context": "The oracle we used in BoostResNet to solve the weak module classifier is Adam as well, but could be extended to tensor methods Janzamin et al. (2015), decision trees Safavian & Landgrebe (1991) or other nonlinear classifiers for non-differentiable data.", "startOffset": 127, "endOffset": 194}, {"referenceID": 2, "context": "(Theorem 1 Cortes et al. (2014)) LetH be a hypothesis set admitting a decomposition H = \u222ai=1Hi for some l > 1.", "startOffset": 11, "endOffset": 32}, {"referenceID": 2, "context": "According to lemma 2 of Cortes et al. (2016), the empirical Rademacher complexity is bounded as a function of r\u221e, \u039bt and n: Rm(Ft) \u2264 r\u221e\u039bt \u221a log(2n) 2m (40) Overall, with probability at least 1\u2212 \u03b4,", "startOffset": 24, "endOffset": 45}, {"referenceID": 38, "context": "G Minimal Weak Learning Condition Mukherjee and SchapireMukherjee & Schapire (2013) introduced a sufficient and necessary weak learning condition based on a concept called edge over random.", "startOffset": 24, "endOffset": 84}, {"referenceID": 38, "context": "G Minimal Weak Learning Condition Mukherjee and SchapireMukherjee & Schapire (2013) introduced a sufficient and necessary weak learning condition based on a concept called edge over random. It is intuitive to restrict the cost matrices to a space of edge-over-random cost-matrices Ceor who put the least cost on the correct label. Definition G.1. The space of edge-over-random cost matrices Ceor \u2286 Rm\u00d7C is the space of cost matrices C \u2208 Ceor that satisfies C(i, yi) \u2264 C(i, l), \u2200l 6= yi, \u2200i \u2208 {1, . . . ,m}. (80) Let us define a random classifier as a baseline predictor B \u2208 Rm\u00d7C , where each row lies on a simplexB(i, :) \u2208 \u2206{1, . . . , C}. We consider the space of baselines, called edge-over-random,who have a faint clue about the correct answer. Definition G.2. The space of edge-over-random baselines Mukherjee & Schapire (2013) Beor \u03b3 \u2286 R m\u00d7C is the space of baselinesB \u2208 Beor \u03b3 that satisfy B(i, yi) \u2265 B(i, l) + \u03b3, \u2200l 6= yi, \u2200i \u2208 {1, .", "startOffset": 24, "endOffset": 832}, {"referenceID": 38, "context": "i=1 C(i, :)B(i, :)\u22a4. (82) Lemma G.4. A weak classifier space H is boostable if and only if H satisfies the weak-learning condition defined in Definition G.3. Refer to Mukherjee and Schapire Mukherjee & Schapire (2013) for the proof.", "startOffset": 4, "endOffset": 218}], "year": 2017, "abstractText": "Deep neural networks are known to be difficult to train due to the instability of back-propagation. A deep residual network (ResNet) with identity loops remedies this by stabilizing gradient computations. We prove a boosting theory for the ResNet architecture. We construct T weak module classifiers, each contains two of the T layers, such that the combined strong learner is a ResNet. Therefore, we introduce an alternative Deep ResNet training algorithm, BoostResNet, which is particularly suitable in non-differentiable architectures. Our proposed algorithm merely requires a sequential training of T \u201cshallow ResNets\u201d which are inexpensive. We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. Our results apply to general multi-class ResNets. A generalization error bound based on margin theory is proved and suggests ResNet\u2019s resistant to overfitting under network with l1 norm bounded weights.", "creator": "LaTeX with hyperref package"}}}