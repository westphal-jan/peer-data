{"id": "1706.05086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Evaluating Noisy Optimisation Algorithms: First Hitting Time is Problematic", "abstract": "a key part of any evolutionary algorithm is fitness evaluation. when fitness evaluations are corrupted by noise, as happens in many real - high world problems as a consequence of various types of uncertainty, a strategy is needed in order to cope with this. resampling foraging is one of the most common strategies, whereby each solution is evaluated many times in order to fundamentally reduce the variance of the fitness estimates. when evaluating the performance of a noisy optimisation algorithm, a key consideration is the stopping condition for the algorithm. a frequently used repeated stopping condition in runtime analysis, known as \" first hitting time \", is to stop the algorithm as soon as it first encounters the optimal solution. however, this is unrealistic for real - world problems, as if the optimal solution were already known, there would be no need to search for it. this paper argues that the use of first hitting time, despite being a long commonly used approach, is significantly flawed and overestimates the quality of many algorithms in real - world cases, where the optimum is not known in advance and has to be genuinely searched for. a better alternative is to measure the quality of the solution an ensemble algorithm returns after purchasing a fixed evaluation budget, i. e., to focus on final solution quality. below this paper argues that focussing on final game solution quality is more realistic and demonstrates cases where the results produced by each algorithm evaluation method lead to very different conclusions regarding the quality of each noisy optimisation algorithm.", "histories": [["v1", "Tue, 13 Jun 2017 09:44:34 GMT  (233kb,D)", "https://arxiv.org/abs/1706.05086v1", "4 pages, 4 figurs, 1 table"], ["v2", "Wed, 12 Jul 2017 11:20:05 GMT  (233kb,D)", "http://arxiv.org/abs/1706.05086v2", "4 pages, 4 figurs, 1 table"]], "COMMENTS": "4 pages, 4 figurs, 1 table", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["simon m lucas", "jialin liu", "diego p\\'erez-li\\'ebana"], "accepted": false, "id": "1706.05086"}, "pdf": {"name": "1706.05086.pdf", "metadata": {"source": "CRF", "title": "Evaluating Noisy Optimisation Algorithms: First Hitting Time is Problematic", "authors": ["Simon M. Lucas", "Jialin Liu"], "emails": ["sml@essex.ac.uk", "jialin.liu@essex.ac.uk", "dperez@essex.ac.uk"], "sections": [{"heading": null, "text": "When evaluating the performance of a noisy optimisation algorithm, a key consideration is the stopping condition for the algorithm. A frequently used stopping condition in runtime analysis, known as \u201cFirst Hitting Time\u201d, is to stop the algorithm as soon as it encounters the optimal solution. However, this is unrealistic for real-world problems, as if the optimal solution were already known, there would be no need to search for it.\nThis paper argues that the use of First Hitting Time, despite being a commonly used approach, is significantly flawed and overestimates the quality of many algorithms in real-world cases, where the optimum is not known in advance and has to be genuinely searched for. A better alternative is to measure the quality of the solution an algorithm returns after a fixed evaluation budget, i.e., to focus on final solution quality. This paper argues that focussing on final solution quality is more realistic and demonstrates cases where the results produced by each algorithm evaluation method lead to very different conclusions regarding the quality of each noisy optimisation algorithm.\nI. INTRODUCTION\nNoisy optimisation problems are important and commonplace in real-world applications, where evaluating the fitness of a solution may be subject to various forms of uncertainty. This paper deals with how to evaluate the performance of a noisy optimisation algorithm. The main purpose of the paper is to demonstrate that a commonly used measure, First Hitting Time (FHT), can give very misleading results: results which will not be indicative of real-world performance. The divergence between the performance measured by FHT and the real-world utility of the algorithm may be very large, as will be shown in the results (Section IV). The problem arises when an optimisation algorithm stumbles upon the optimal solution without being confident that it has found it. If the performance evaluator rewards an algorithm for merely visiting the optimum as opposed to returning the optimum as its best guess, then a significantly inflated estimate of the optimiser\u2019s performance may be given.\nEvaluation of noisy optimisation algorithms: Commonly used measures of noisy optimisation algorithms are FHT when the search space is finite or countable, and the convergence rate in continuous domains. The FHT is the first time at which an optimum1 is visited. FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5]. In the noisy case, some resampling techniques2 are often used aiming at cancelling the effect of noise [6], [7], [4]. In most of the previous work [1], [2], [4], the runtime analysis is based on the assumption that once the optimal solution is hit, an absorbing state is reached and the optimisation terminates. This assumption is not realistic in real-world applications, as if the optimal solution were known, no optimisation or search would be required.\nA better alternative to FHT is to measure the quality of the solution an algorithm returns after a fixed evaluation budget, i.e., to focus on final solution quality. This can focus on any measure of solution quality, but in this paper for ease of comparison and for clarity we focus on the success rate defined as the percentage of trials that the optimiser returns the optimal solution as its final solution.\nIn this paper we demonstrate the impact of choosing an appropriate methodology for evaluating a noisy optimisation algorithm. In particular, we do this by showing two test problems (described in Section II) where choosing an inapplicable evaluation methodology can lead to the opposite conclusion compared to choosing a more applicable one. We test two different optimisation algorithms: the Random Mutation Hill Climber (RMHC) and the (1+1)-EA under the two different evaluation methods, in order to demonstrate that the results are not unique to a single optimisation algorithm. These are described in Section III. Section IV summarises the results and discusses the dramatic impact that the evaluation method can have on the estimated performance of noisy optimisation algorithms and the prediction of the optimal resampling rate. Finally, Section V concludes that First Hitting Time should not\n1In this paper, the term \u201coptimum\u201d refers to the optimal solution, \u201coptimal value\u201d refers to the noise-free fitness value of the optimum.\n2The term \u201csampling\u201d refers to sample the search points (solutions), while \u201cresampling\u201d stands for evaluating the identical solution more than once.\nar X\niv :1\n70 6.\n05 08\n6v 2\n[ cs\n.N E\n] 1\n2 Ju\nl 2 01\n7\nbe used to evaluate the performance of a noisy optimisation algorithm."}, {"heading": "II. TEST PROBLEMS", "text": "We considered two problems: the OneMax problem corrupted by additive Gaussian noise, and an artificial game outcome optimisation problem."}, {"heading": "A. OneMax problem with additive Gaussian noise", "text": "The n-bit OneMax problem corrupted by additive Gaussian noise is formalised as:\nf(x) = n\u2211 i=1 xi +N (0, 1), (1)\nwhere x is an n-bit binary string, N (\u00b5, \u03c32) denotes Gaussian noise with mean \u00b5 and variance \u03c32. This problem is referred to as the Noisy OneMax problem in the rest of the paper."}, {"heading": "B. Optimisation of game outcome", "text": "We also consider an artificial game outcome optimisation problem, referred to as the Noisy PMax problem, in the rest of the paper. The bit-string x can be:\n(P1) a parameter setting of an AI agent to play a given game (P2) or a parameter setting of a game for a given AI agent. The pretext for this problem is that we are trying to optimise the winning rate for a given agent but due to the stochastic nature of the game and / or the agent(s) playing it, the outcome of every simulation of the game is a win or a loss with a given probability. This leads to a very noisy optimisation problem.\nThe Noisy PMax problem was directly inspired by recent work on evolving game parameters in order to maximise the winning rate of a skilful agent over less skilful ones [8].\nIn this artificial model, x is treated as an n-bit binary number, and the true winning rate of x is defined as\nPwin(x) = V alue(x)\n(2n \u2212 1) , (2)\nwhere V alue(x) denotes the numeric value of x located between 0 and 2n \u2212 1. Thus, the outcome of a game is either loss (0) or win (1) with probability Pwin(x) (Eq. 2), as formalised in Eq. 3.\nG(x) = { 1 with probability Pwin(x), 0 otherwise. (3)\nWhile resampling a game r times, the average outcome, 1 r \u2211r i=1G(x), is the winning rate measured over r repetitions of the game. By the Law of Large Numbers (LLN), for an r large enough, 1r \u2211r i=1G(x) converges to Pwin(x)."}, {"heading": "III. EXPERIMENTAL SETTINGS", "text": "Both the RMHC and the (1+1)-EA with mutation probability 1n (n is the solution dimension) have been applied for optimising each of the two problems using each of the two evaluation methods. In each of the following two cases we measure the percentage of times that the optimiser returns the optimal solution.\nAlgorithm 1 Random Mutation Hill Climber (RMHC) with resampling rate r \u2208 N+ to maximise a noisy binary problem. n is the problem dimension. fitness(i)(x) refers to the ith call to the noisy fitness function for the given x.\nUniformly randomly initialise x = (x1, x2, . . . , xn) \u2208 {0, 1}n while Stopping condition not met do . (E1) or (E2)\nx\u2032 = x Uniformly randomly select dimension d \u2208 {1, . . . , n} x\u2032d = 1\u2212 x\u2032d if 1r \u2211r i=1 fitness (i)(x\u2032) \u2265 1r \u2211r i=1 fitness (i)(x) then\nx = x\u2032\nend if end while return x\n(E1) We use the FHT such that the first time the optimiser visits the optimum, it returns that as its solution. If it uses the entire evaluation budget then it returns its current solution. (E2) The optimiser returns its current solution when the evaluation budget has been used up, without prior knowledge of the optimal solution.\nNote that case (E1) assumes that the optimal solution is already known. During evaluation, each solution is evaluated r times where r varies from 1 to 50. We call r the resampling rate. The pseudocode of RMHC and (1+1)-EA with resamplings are given in Algorithms 1 and 2. Note that in each algorithm the parent is re-evaluated at each iteration. Each experiment has been repeated 10,000 times using uniformly randomly initialised solutions. We chose the number of dimensions n = 10. This small number of dimensions helps to emphasise the difference in the results of the alternative evaluation methods, though differences are also present for higher-dimensional problems. The evaluation budget (i.e. the number of raw fitness evaluations allowed) was set to T = 500 for all experiments."}, {"heading": "IV. RESULTS", "text": "Figures 1 and 2 plot the percentage of times that an optimal solution is returned by the (1+1)-EA (left) and the RMHC (right). We focus on a case where the search space is small (solution dimension n = 10), but the evaluation budget is also small. The details of the algorithm are identical except for the stopping conditions presented in (E1) and (E2) respectively (Section III). To highlight the difference between results using stopping conditions (E1) and (E2), the figures of the best and worst success rates in each experiment are summarised in Table I."}, {"heading": "A. Noisy OneMax problem", "text": "When each algorithm operates for a fixed maximum number of fitness evaluations (T = 500), the RMHC finds the optimal solution significantly more frequently than the (1+1)-EA (Figure 1). Table I illustrates that the (1+1)-EA using (E1) as the\nAlgorithm 2 (1+1)-EA with mutation probability 1n and resampling rate r \u2208 N+ to maximise a noisy n-dimensional binary problem. fitness(i)(x) refers to the ith call to the noisy fitness function for the given x.\nstopping condition gives the best success rate as 85.61% using (E1) but the true best success rate is only 38.98% without assuming knowing the optimal solution in advance; the RMHC using (E1) as the stopping condition gives the best success rate as 97.44%, dropping to 65.44% if (E2) is used. Qian et al. [2] stated that resampling was useless in the 10-bit Noisy OneMax problem described in this paper and Liu et al. [4] concluded that in the same noise model, the optimal resampling rate for the RMHC increased with the problem dimension, and for the 10-bit Noisy OneMax problem, the optimal resampling rate was 1. In both works [2], [4], the analysis was based on the condition that the optimisation terminated as soon as the optimum was found and there was no limit to the maximum number of fitness evaluations. Indeed, in line with those predictions, Figure 1b shows that the optimal resampling rate for the RMHC, using the FHT and the maximum number of fitness evaluations as the stopping conditions, is 1, while the rate for the RMHC using only the maximum number of fitness evaluations as the stopping condition is 7. The important point to note is that using FHT leads to a false conclusion regarding both the optimal success rate and the corresponding optimal resampling rate."}, {"heading": "B. Noisy PMax problem", "text": "Figure 2 shows the percentage of times that an optimal solution is returned over 10, 000 optimisation trials by the\n(1+1)-EA (left) and the RMHC (right) in the Noisy PMax problem described in Section II-B, using the two stopping conditions discussed previously. Table I illustrates that the (1+1)-EA using (E1) as the stopping condition gives the best success rate as 19.56%, but the true best success rate (without the assumption that the optimal solution is known) is only 1%; the RMHC using (E1) as the stopping condition gives the best success rate as 28.67%, but the true best success rate is only 1.14%. The percentage of times that an optimal solution is returned using the stopping condition (E1) is much higher than the percentage obtained when using the stopping condition (E2). However, having prior knowledge of the optimal solution and the access to the noise-free fitness is rare in real-world applications. Hence, the runtime analysis using the stopping condition (E1) is too optimistic and unrealistic."}, {"heading": "V. CONCLUSION", "text": "To the best of the authors\u2019 knowledge, the current work around runtime analysis or prediction of the optimal resampling rate is based on the assumption that the optimal solution is known. Once the optimal noise-free fitness level is reached or the optimal solution is found, the optimisation stops immediately, thus the absorbing state is reached and the solution won\u2019t escape from the absorbing state. In real-world applications, it is unlikely that we know the optimal solution in advance since there will be no reason to search for it.\nThe main conclusion is that First Hitting Time should not be used to evaluate the performance of noisy optimisation algorithms since it can dramatically over-estimate the performance of an algorithm, and may significantly under-estimate the amount of resampling required in order to obtain optimal performance. The conclusion is supported by the results of running two simple but widely used optimisation algorithms on two different test problems."}], "references": [{"title": "Analysis of the (1+ 1) EA for a Noisy OneMax", "author": ["S. Droste"], "venue": "Genetic and Evolutionary Computation\u2013GECCO 2004. Springer, 2004, pp. 1088\u20131099.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "On the Effectiveness of Sampling for Evolutionary Optimization in Noisy Environments", "author": ["C. Qian", "Y. Yu", "Y. Jin", "Z.-H. Zhou"], "venue": "Parallel Problem Solving from Nature\u2013PPSN XIII. Springer, 2014, pp. 302\u2013311.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of Runtime of Optimization Algorithms for Noisy Functions over Discrete Codomains", "author": ["Y. Akimoto", "S. Astete-Morales", "O. Teytaud"], "venue": "Theoretical Computer Science, vol. 605, pp. 42\u201350, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal Resampling for the Noisy OneMax Problem", "author": ["J. Liu", "M. Fairbank", "D. P\u00e9rez-Li\u00e9bana", "S.M. Lucas"], "venue": "arXiv preprint arXiv:1607.06641, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "On the Effectiveness of Sampling for Evolutionary Optimization in Noisy Environments", "author": ["C. Qian", "Y. Yu", "K. Tang", "X. Yao", "Y. Jin", "Z.-H. Zhou"], "venue": "Evolutionary Computation, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-Parameter Black-Box Optimization Benchmarking 2010: presentation of the Noisy Functions", "author": ["S. Finck", "N. Hansen", "R. Ros", "A. Auger"], "venue": "Technical Report 2009/21, Research Center PPE, Tech. Rep., 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Noisy Evolutionary Optimization Algorithms-A Comprehensive Survey", "author": ["P. Rakshit", "A. Konar", "S. Das"], "venue": "Swarm and Evolutionary Computation, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "FHT is widely used to evaluate the performance of noisy optimisation algorithms in problems with discrete search space, such as various noisy versions of the OneMax problem [1], [2], [3], [4], [5].", "startOffset": 193, "endOffset": 196}, {"referenceID": 5, "context": "In the noisy case, some resampling techniques2 are often used aiming at cancelling the effect of noise [6], [7], [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "In the noisy case, some resampling techniques2 are often used aiming at cancelling the effect of noise [6], [7], [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "In the noisy case, some resampling techniques2 are often used aiming at cancelling the effect of noise [6], [7], [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "In most of the previous work [1], [2], [4], the runtime analysis is based on the assumption that once the optimal solution is hit, an absorbing state is reached and the optimisation terminates.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "In most of the previous work [1], [2], [4], the runtime analysis is based on the assumption that once the optimal solution is hit, an absorbing state is reached and the optimisation terminates.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "In most of the previous work [1], [2], [4], the runtime analysis is based on the assumption that once the optimal solution is hit, an absorbing state is reached and the optimisation terminates.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "[2] stated that resampling was useless in the 10-bit Noisy OneMax problem described in this paper and Liu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] concluded that in the same noise model, the optimal resampling rate for the RMHC increased with the problem dimension, and for the 10-bit Noisy OneMax problem, the optimal resampling rate was 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "In both works [2], [4], the analysis was based on the condition that the optimisation terminated as soon", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "In both works [2], [4], the analysis was based on the condition that the optimisation terminated as soon", "startOffset": 19, "endOffset": 22}], "year": 2017, "abstractText": "A key part of any evolutionary algorithm is fitness evaluation. When fitness evaluations are corrupted by noise, as happens in many real-world problems as a consequence of various types of uncertainty, a strategy is needed in order to cope with this. Resampling is one of the most common strategies, whereby each solution is evaluated many times in order to reduce the variance of the fitness estimates. When evaluating the performance of a noisy optimisation algorithm, a key consideration is the stopping condition for the algorithm. A frequently used stopping condition in runtime analysis, known as \u201cFirst Hitting Time\u201d, is to stop the algorithm as soon as it encounters the optimal solution. However, this is unrealistic for real-world problems, as if the optimal solution were already known, there would be no need to search for it. This paper argues that the use of First Hitting Time, despite being a commonly used approach, is significantly flawed and overestimates the quality of many algorithms in real-world cases, where the optimum is not known in advance and has to be genuinely searched for. A better alternative is to measure the quality of the solution an algorithm returns after a fixed evaluation budget, i.e., to focus on final solution quality. This paper argues that focussing on final solution quality is more realistic and demonstrates cases where the results produced by each algorithm evaluation method lead to very different conclusions regarding the quality of each noisy optimisation algorithm.", "creator": "LaTeX with hyperref package"}}}