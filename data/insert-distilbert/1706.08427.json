{"id": "1706.08427", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2017", "title": "Approximate Steepest Coordinate Descent", "abstract": "we propose a new selection rule for the coordinate selection in random coordinate descent methods for huge - scale optimization. providing the analytical efficiency of choosing this novel scheme is provably better than the efficiency of uniformly random selection, and can reach the efficiency of steepest coordinate descent ( scd ), enabling an acceleration of once a factor of up to $ n $, the sheer number of random coordinates. in many practical applications, our scheme can be implemented at no extra cost and computational efficiency very close to the faster uniform directed selection. numerical fitness experiments with lasso and ridge root regression show promising improvements, in line with our theoretical improvement guarantees.", "histories": [["v1", "Mon, 26 Jun 2017 15:07:02 GMT  (1061kb,D)", "http://arxiv.org/abs/1706.08427v1", "appearing at ICML 2017"]], "COMMENTS": "appearing at ICML 2017", "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["sebastian u stich", "anant raj", "martin jaggi"], "accepted": true, "id": "1706.08427"}, "pdf": {"name": "1706.08427.pdf", "metadata": {"source": "META", "title": "Approximate Steepest Coordinate Descent", "authors": ["Sebastian U. Stich", "Anant Raj", "Martin Jaggi"], "emails": ["<sebastian.stich@epfl.ch>."], "sections": [{"heading": "1. Introduction", "text": "Coordinate descent (CD) methods have attracted a substantial interest the optimization community in the last few years (Nesterov, 2012; Richta\u0301rik & Taka\u0301c\u030c, 2016). Due to their computational efficiency, scalability, as well as their ease of implementation, these methods are the state-of-theart for a wide selection of machine learning and signal processing applications (Fu, 1998; Hsieh et al., 2008; Wright, 2015). This is also theoretically well justified: The complexity estimates for CD methods are in general better than the estimates for methods that compute the full gradient in one batch pass (Nesterov, 2012; Nesterov & Stich, 2017).\nIn many CD methods, the active coordinate is picked at random, according to a probability distribution. For smooth functions it is theoretically well understood how the sampling procedure is related to the efficiency of the scheme and which distributions give the best complexity estimates (Nesterov, 2012; Zhao & Zhang, 2015; AllenZhu et al., 2016; Qu & Richta\u0301rik, 2016; Nesterov & Stich, 2017). For nonsmooth and composite functions \u2014 that appear in many machine learning applications \u2014 the pic-\n1EPFL 2Max Planck Institute for Intelligent Systems. Correspondence to: Sebastian U. Stich <sebastian.stich@epfl.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nture is less clear. For instance in (Shalev-Shwartz & Zhang, 2013; Friedman et al., 2007; 2010; Shalev-Shwartz & Tewari, 2011) uniform sampling (UCD) is used, whereas other papers propose adaptive sampling strategies that change over time (Papa et al., 2015; Csiba et al., 2015; Osokin et al., 2016; Perekrestenko et al., 2017).\nA very simple deterministic strategy is to move along the direction corresponding to the component of the gradient with the maximal absolute value (steepest coordinate descent, SCD) (Boyd & Vandenberghe, 2004; Tseng & Yun, 2009). For smooth functions this strategy yields always better progress than UCD, and the speedup can reach a factor of the dimension (Nutini et al., 2015). However, SCD requires the computation of the whole gradient vector in each iteration which is prohibitive (except for special applications, cf. Dhillon et al. (2011); Shrivastava & Li (2014)).\nIn this paper we propose approximate steepest coordinate descent (ASCD), a novel scheme which combines the best parts of the aforementioned strategies: (i) ASCD maintains an approximation of the full gradient in each iteration and selects the active coordinate among the components of this vector that have large absolute values \u2014 similar to SCD; and (ii) in many situations the gradient approximation can be updated cheaply at no extra cost \u2014 similar to UCD. We show that regardless of the errors in the gradient approximation (even if they are infinite), ASCD performs always better than UCD.\nSimilar to the methods proposed in (Tseng & Yun, 2009) we also present variants of ASCD for composite problems. We confirm our theoretical findings by numerical experiments for Lasso and Ridge regression on a synthetic dataset as well as on the RCV1 (binary) dataset.\nStructure of the Paper and Contributions. In Sec. 2 we review the existing theory for SCD and (i) extend it to the setting of smooth functions. We present (ii) a novel lower bound, showing that the complexity estimates for SCD and UCD can be equal in general. We (iii) introduce ASCD and the save selection rules for both smooth (Sec. 3) and to composite functions (Sec. 5). We prove that (iv) ASCD performs always better than UCD (Sec. 3) and (v) it can reach the performance of SCD (Sec. 6). In Sec. 4 we discuss important applications where the gradient estimate can efficiently be maintained. Our theory is supported by nu-\nar X\niv :1\n70 6.\n08 42\n7v 1\n[ cs\n.L G\n] 2\n6 Ju\nn 20\n17\nmerical evidence in Sec. 7, which reveals that (vi) ASCD performs extremely well on real data.\nNotation. Define [x]i := \u3008x, ei\u3009 with ei the standard unit vectors in Rn. We abbreviate \u2207if := [\u2207f ]i. A convex function f : Rn \u2192 R with coordinate-wise LiLipschitz continuous gradients1 for constants Li > 0, i \u2208 [n] := {1, . . . , n}, satisfies by the standard reasoning\nf(x+ \u03b7ei) \u2264 f(x) + \u03b7\u2207if(x) + Li2 \u03b7 2 (1)\nfor all x \u2208 Rn and \u03b7 \u2208 R. A function is coordinate-wise L-smooth if Li \u2264 L for i = 1, . . . , n. For an optimization problem minx\u2208Rn f(x) define X? := arg minx\u2208Rn f(x) and denote by x? \u2208 Rn an arbitrary element x? \u2208 X?."}, {"heading": "2. Steepest Coordinate Descent", "text": "In this section we present SCD and discuss its theoretical properties. The functions of interest are composite convex functions F : Rn \u2192 R of the form\nF (x) := f(x) + \u03a8(x) (2)\nwhere f is coordinate-wise L-smooth and \u03a8 convex and separable, that is that is \u03a8(x) = \u2211n i=1 \u03a8i([x]i). In the first part of this section we focus on smooth problems, i.e. we assume that \u03a8 \u2261 0.\nCoordinate descent methods with constant step size generate a sequence {xt}t\u22650 of iterates that satisfy the relation\nxt+1 = xt \u2212 1L\u2207itf(x)eit . (3)\nIn UCD the active coordinate it is chosen uniformly at random from the set [n], it \u2208u.a.r. [n]. SCD chooses the coordinate according to the Gauss-Southwell (GS) rule:\nit = arg max i\u2208[n]\n\u2207i |f(xt)| . (4)"}, {"heading": "2.1. Convergence analysis", "text": "With the quadratic upper bound (1) one can easily get a lower bound on the one step progress\nE [f(xt)\u2212 f(xt+1) | xt] \u2265 Eit [ 1 2L |\u2207itf(xt)| 2 ] . (5)\nFor UCD and SCD the expression on the right hand side evaluates to\n\u03c4UCD(xt) := 1 2nL \u2016\u2207f(xt)\u2016 2 2 \u03c4SCD(xt) := 1 2L \u2016\u2207f(xt)\u2016 2 \u221e\n(6)\nWith Cauchy-Schwarz we find\n1 n\u03c4SCD(xt) \u2264 \u03c4UCD(xt) \u2264 \u03c4SCD(xt) . (7)\n1|\u2207if(x+ \u03b7ei)\u2212\u2207if(x)| \u2264 Li |\u03b7| , \u2200x \u2208 Rn, \u03b7 \u2208 R.\nHence, the lower bound on the one step progress of SCD is always at least as large as the lower bound on the one step progress of UCD. Moreover, the one step progress could be even lager by a factor of n. However, it is very difficult to formally prove that this linear speed-up holds for more than one iteration, as the expressions in (7) depend on the (a priori unknown) sequence of iterates {xt}t\u22650.\nStrongly Convex Objectives. Nutini et al. (2015) present an elegant solution of this problem for \u00b52-strongly convex functions2. They propose to measure the strong convexity of the objective function in the 1-norm instead of the 2-norm. This gives rise to the lower bound\n\u03c4SCD(xt) \u2265 \u00b51L (f(xt)\u2212 f(x ?)) , (8)\nwhere \u00b51 denotes the strong convexity parameter. By this, they get a uniform upper bound on the convergence that does not directly depend on local properties of the function, like for instance \u03c4SCD(xt), but just on \u00b51. It always holds \u00b51 \u2264 \u00b52, and for functions where both quantities are equal, SCD enjoys a linear speedup over UCD.\nSmooth Objectives. When the objective function f is just smooth (but not necessarily strongly convex), then the analysis mentioned above is not applicable. We here extend the analysis from (Nutini et al., 2015) to smooth functions. Theorem 2.1. Let f : Rn \u2192 R be convex and coordinatewise L-smooth. Then for the sequence {xt}t\u22650 generated by SCD it holds:\nf(xt)\u2212 f(x?) \u2264 2LR21 t , (9)\nfor R1 := max x?\u2208X? { max x\u2208Rn [\u2016x\u2212 x?\u20161 | f(x) \u2264 f(x0)] } .\nProof. In the proof we first derive a lower bound on the one step progress (Lemma A.1), similar to the analysis in (Nesterov, 2012). The lower bound for the one step progress of SCD can in each iteration differ up to a factor of n from the analogous bound derived for UCD (similar as in (7)). All details are given in Section A.1 in the appendix.\nNote that the R1 is essentially the diameter of the level set at f(x0) measured in the 1-norm. In the complexity estimate of UCD, R21 in (9) is replaced by nR 2 2, where R2 is the diameter of the level at f(x0) measured in the 2-norm (cf. Nesterov (2012); Wright (2015)). As in (7) we observe with Cauchy-Schwarz\n1 nR 2 1 \u2264 R22 \u2264 R21 , (10)\ni.e. SCD can accelerate up to a factor of n over to UCD.\n2A function is \u00b5p-strongly convex in the p-norm, p \u2265 1, if f(y) \u2265 f(x) + \u3008\u2207f(x),y \u2212 x\u3009+ \u00b5p\n2 \u2016y \u2212 x\u20162p, \u2200x,y \u2208 R n."}, {"heading": "2.2. Lower bounds", "text": "In the previous section we provided complexity estimates for the methods SCD and UCD and showed that SCD can converge up to a factor of the dimension n faster than UCD. In this section we show that this analysis is tight. In Theorem 2.2 below we give a function q : Rn \u2192 R, for which the one step progress \u03c4SCD(xt) \u2248 \u03c4UCD(xt) up to a constant factor, for all iterates {xt}t\u22650 generated by SCD.\nBy a simple technique we can also construct functions for which the speedup is exactly equal to an arbitrary factor \u03bb \u2208 [1, n]. For instance we can consider functions with a (separable) low dimensional structure. Fix integers s, n such that ns \u2248 \u03bb, define the function f : R n \u2192 R as\nf(x) := q(\u03c0s(x)) (11)\nwhere \u03c0s denotes the projection to Rs (being the first s out of n coordinates) and q : Rs \u2192 R is the function from Theorem 2.2. Then\n\u03c4SCD(xt) \u2248 \u03bb \u00b7 \u03c4UCD(xt) , (12)\nfor all iterates {xt}t\u22650 generated by SCD. Theorem 2.2. Consider the function q(x) = 12 \u3008Qx,x\u3009 for Q := In \u2212 99100nJn, where Jn = 1n1 T n , n > 2. Then there exists x0 \u2208 Rn such that for the sequence {xt}t\u22650 generated by SCD it holds\n\u2016\u2207q(xt)\u20162\u221e \u2264 4 n \u2016\u2207q(xt)\u2016 2 2 . (13)\nProof. In the appendix we discuss a family of functions defined by matrices Q := (\u03b1 \u2212 1) 1nJn + In and define corresponding parameters 0 < c\u03b1 < 1 such that for x0 defined as [x0]i = ci\u22121\u03b1 for i = 1, . . . , n, SCD cycles through the coordinates, that is, the sequence {xt}t\u22650 generated by SCD satisfies\n[xt]1+(t\u22121 mod n) = c n \u03b1 \u00b7 [xt\u22121]1+(t\u22121 mod n) . (14)\nWe verify that for this sequence property (13) holds."}, {"heading": "2.3. Composite Functions", "text": "The generalization of the GS rule (4) to composite problems (2) with nontrival \u03a8 is not straight forward. The \u2018steepest\u2019 direction is not always meaningful in this setting; consider for instance a constrained problem where this rule could yield no progress at all when stuck at the boundary.\nNutini et al. (2015) discuss several generalizations of the Gauss-Southwell rule for composite functions. The GSs rule is defined to choose the coordinate with the most negative directional derivative (Wu & Lange, 2008). This rule is identical to (4) but requires the calculation of subgradients of \u03a8i. However, the length of a step could be\narbitrarily small. In contrast, the GS-r rule was defined to pick the coordinate direction that yields the longest step (Tseng & Yun, 2009). The rule that enjoys the best theoretical properties (cf. Nutini et al. (2015)) is the GS-q rule, which is defined as to maximize the progress assuming a quadratic upper bound on f (Tseng & Yun, 2009). Consider the coordinate-wise models\nVi(x, y, s) := sy + L 2 y 2 + \u03a8i([x]i + y) , (15)\nfor i \u2208 [n]. The GS-q rule is formally defined as\niGS\u2212q = arg min i\u2208[n] min y\u2208R\nVi(x, y,\u2207if(x)) . (16)"}, {"heading": "2.4. The Complexity of the GS rule", "text": "So far we only studied the iteration complexity of SCD, but we have disregarded the fact that the computation of the GS rule (4) can be as expensive as the computation of the whole gradient. The application of coordinate descent methods is only justified if the complexity to compute one directional derivative is approximately n times cheaper than the computation of the full gradient vector (cf. Nesterov (2012)). By Theorem 2.2 this reasoning also applies to SCD. A class of function with this property is given by functions F : Rn \u2192 R\nF (x) := f(Ax) + n\u2211 i=1 \u03a8i([x]i) (17)\nwhere A is a d \u00d7 n matrix, and where f : Rd \u2192 R, and \u03a8i : Rn \u2192 R are convex and simple, that is the time complexity T for computing their gradients is linear: T (\u2207yf(y),\u2207x\u03a8(x) = O(d + n). This class of functions includes least squares, logistic regression, Lasso, and SVMs (when solved in dual form).\nAssuming the matrix is dense, the complexity to compute the full gradient of F is T (\u2207xF (x)) = O(dn). If the value w = Ax is already computed, one directional derivative can be computed in time T (\u2207iF (x)) = O(d). The recursive update of w after one step needs the addition of one column of matrix A with some factors and can be done in time O(d). However, we note that recursively updating the full gradient vector takes time O(dn) and consequently the computation of the GS rule cannot be done efficiently.\nNutini et al. (2015) consider sparse matrices, for which the computation of the Gauss-Southwell rule becomes traceable. In this paper, we propose an alternative approach. Instead of updating the exact gradient vector, we keep track of an approximation of the gradient vector and recursively update this approximation in time O(n log n). With these updates, the use of coordinate descent is still justified in case d = \u2126(n).\nAlgorithm 1 Approximate SCD (ASCD) Input: f , x0, T , \u03b4-gradient oracle g, methodM Initialize [g\u03030]i = 0, [r0]i =\u221e for i \u2208 [n]. for t = 0 to T do\nFor i \u2208 [n] define compute u.-and l.-bounds [ut]i := max{|[g\u0303t]i \u2212 [rt]i| , |[g\u0303t]i + [rt]i|} [`t]i := miny\u2208R{|y| | [g\u0303t]i\u2212[rt]i \u2264 y \u2264 [g\u0303t]i+[rt]i} av(I) := 1|I| \u2211 i\u2208I [`t] 2 i compute active set\nIt := arg minI \u2223\u2223{I \u2286 [n] | [ut]2i < av(I),\u2200i /\u2208 I}\u2223\u2223\nPick it \u2208u.a.r. arg maxi\u2208It{[`]i} active coordinate (xt+1, [g\u0303t+1]it , [rt+1]it) :=M(xt,\u2207itf(xt))\n\u03b3t := [xt+1]it \u2212 [xt]it update\u2207f(xt+1) estimate Update [g\u0303t+1]j := [g\u0303t]j + \u03b3tgitj(xt), j 6= it Update [rt+1]j := [rt]j + \u03b3t\u03b4itj , j 6= it\nend for"}, {"heading": "3. Algorithm", "text": "Is it possible to get the significantly improved convergence speed from SCD, when one is only willing to pay the computational cost of only the much simpler UCD? In this section, we give a formal definition of our proposed approximate SCD method which we denote ASCD.\nThe core idea of the algorithm is the following: While performing coordinate updates, ideally we would like to efficiently track the evolution of all elements of the gradient, not only the one coordinate which is updated in the current step. The formal definition of the method is given in Algorithm 1 for smooth objective functions. In each iteration, only one coordinate is modified according to some arbitrary update rule M. The coordinate update rule M provides two things: First the new iterate xt+1, and secondly also an estimate g\u0303 of the it-th entry of the gradient at the new iterate3. Formally,\n(xt+1, g\u0303, r) :=M(xt,\u2207itf(xt)) (18)\nsuch that the quality of the new gradient estimate g\u0303 satisfies\n|\u2207itf(xt+1)\u2212 g\u0303| \u2264 r . (19)\nThe non-active coordinates are updated with the help of gradient oracles with accuracy \u03b4 \u2265 0 (see next subsection for details). The scenario of exact updates of all gradient entries is obtained for accuracy parameters \u03b4 = r = 0 and in this case ASCD is identical to SCD."}, {"heading": "3.1. Safe bounds for gradient evolution", "text": "ASCD maintains lower and upper bounds for the absolute values of each component of the gradient ([`]i \u2264\n3For instance, for updates by exact coordinate optimization (line-search), we have g\u0303 = r = 0.\n|\u2207if(x)| \u2264 [u]i). These bounds allow to identify the coordinates on which the absolute values of the gradient are small (and hence cannot be the steepest one). More precisely, the algorithm maintains a set It of active coordinates (similar in spirit as in active set methods, see e.g. Kim & Park (2008); Wen et al. (2012)). A coordinate j is excluded from It if the estimated progress in this direction (cf. (5)) is lower than the average of the estimated progress along coordinate directions in It, [ut]2j < 1|It| \u2211 i\u2208It [`t] 2 i . The active set It can be computed in O(n log n) time by sorting. All other operations take linear O(n) time.\nGradient Oracle. The selection mechanism in ASCD crucially relies on the following definition of a \u03b4-gradient oracle. While the update M delivers the estimated active entry of the new gradient, the additional gradient oracle is used to update all other coordinates j 6= it of the gradient; as in the last two lines of Algorithm 1. Definition 3.1 (\u03b4-gradient oracle). For a function f : Rn \u2192 R and indices i, j \u2208 [n], a (i, j)-gradient oracle with error \u03b4ij \u2265 0 is a function gij : Rn \u2192 R satisfying \u2200x \u2208 Rn,\u2200\u03b3 \u2208 R:\n|\u2207jf(x+ \u03b3ei)\u2212 \u03b3gij(x)| \u2264 |\u03b3| \u03b4ij . (20)\nWe denote by a \u03b4-gradient oracle a family {gij}i,j\u2208[n] of \u03b4ij-gradient oracles.\nWe discuss the availability of good gradient oracles for many problem classes in more detail in Section 4. For example for least squares problems and general linear models, a \u03b4-gradient oracle is for instance given by a scalar product estimator as in (24) below. Note that ASCD can also handle very bad estimates, as long as the property (20) is satisfied (possibly even with accuracy \u03b4ij =\u221e).\nInitialization. In ASCD the initial estimate g\u03030 of the gradient is just arbitrarily set to 0, with uncertainty r0 = \u221e. Hence in the worst case it takes \u0398(n log n) iterations until each coordinate gets picked at least once (cf. Dawkins (1991)) and until corresponding gradient estimates are set to a realistic value. If better estimates of the initial gradient are known, they can be used for the initialization as long as a strong error bound as in (19) is known as well. For instance the initialization can be done with \u2207f(x0) if one is willing to compute this vector in one batch pass.\nConvergence Rate Guarantee. We present our first main result showing that the performance of ASCD is provably between UCD and SCD. First observe that if in Algorithm 1 the gradient oracle is always exact, i.e. \u03b4ij \u2261 0, and if g\u03030 is initialized with \u2207f(x0), then in each iteration |\u2207itf(xt)| = \u2016\u2207f(xt)\u2016\u221e and ASCD identical to SCD. Lemma 3.1. Let imax := arg maxi\u2208[n] |\u2207if(xt)|. Then imax \u2208 It, for It as in Algorithm 1.\nProof. This is immediate from the definitions of It and the upper and lower bounds. Suppose imax /\u2208 It, then there exists j 6= imax such that [`t]j > [ut]imax , and consequently |\u2207jf(xt)| > |\u2207imaxf(xt)|.\nTheorem 3.2. Let f : Rn \u2192 R be convex and coordinatewise L-smooth, let \u03c4UCD, \u03c4SCD, \u03c4ASCD denote the expected one step progress (6) of UCD, SCD and ASCD, respectively, and suppose all methods use the same step-size ruleM. Then\n\u03c4UCD(x) \u2264 \u03c4ASCD(x) \u2264 \u03c4SCD(x) \u2200x \u2208 Rn . (21)\nProof. By (5) we get \u03c4ASCD(x) = 12L|I| \u2211 i\u2208I |\u2207if(x)|\n2, where I denotes the corresponding index set of ASCD when at iterate x. Note that for j /\u2208 I it must hold that |\u2207jf(x)|2 \u2264 [u]2j < 1|I| \u2211 i\u2208I [`] 2 i \u2264 1|I| \u2211 i\u2208I |\u2207if(x)| 2 by definition of I.\nObserve that the above theorem holds for all gradient oracles and coordinate update variants, as long as they are used with corresponding quality parameters r (as in (19)) and \u03b4ij (as in (20)) as part of the algorithm.\nHeuristic variants. Below also propose three heuristic variants of ASCD. For all these variants the active set It can be computed O(n), but the statement of Theorem 3.2 does not apply. These variants only differ from ASCD in the choice of the active set in Algorithm 1:\nu-ASCD: It := arg maxi\u2208[n][ut]i `-ASCD: It := arg maxi\u2208[n][`t]i a-ASCD: It := { i \u2208 [n] | [ut]i \u2265 maxi\u2208[n][`t]i\n}"}, {"heading": "4. Approximate Gradient Update", "text": "In this section we argue that for a large class of objective functions of interest in machine learning, the change in the gradient along every coordinate direction can be estimated efficiently.\nLemma 4.1. Consider F : Rn \u2192 R as in (17) with twice-differentiable f : Rd \u2192 R. Then for two iterates xt,xt+1 \u2208 Rn of a coordinate descent algorithm, i.e. xt+1 = xt + \u03b3teit , there exists a x\u0303 \u2208 Rn on the line segment between xt and xt+1, x\u0303 \u2208 [xt,xt+1] with\n\u2207iF (xt+1)\u2212\u2207iF (xt) = \u03b3t\u3008ai,\u22072f(Ax\u0303)ait\u3009 \u2200i 6= it (22) where ai denotes the i-th column of the matrix A.\nProof. For coordinates i 6= it the gradient (or subgradient set) of \u03a8i([xt]i) does not change. Hence it suffices to calculate the change\u2207f(xt+1)\u2212\u2207f(xt). This is detailed in the appendix.\nLeast-Squares with Arbitrary Regularizers. The least squares problem is defined as problem (17) with f(Ax) = 1 2 \u2016Ax\u2212 b\u2016 2 2 for a b \u2208 Rd. This function is twice differentiable with\u22072f(Ax) = In. Hence (22) reduces to\n\u2207iF (xt+1)\u2212\u2207iF (xt) = \u03b3t\u3008ai,ait\u3009 \u2200i 6= it . (23)\nThis formulation gives rise to various gradient oracles (20) for the least square problems. For for i 6= it we easily verify that the condition (20) is satisfied:\n1. g1ij := \u3008ai,ait\u3009; \u03b4ij = 0, 2. g2ij := max {\u2212\u2016ai\u2016 \u2016aj\u2016 ,min {S(i, j), \u2016ai\u2016 \u2016aj\u2016}}; \u03b4ij = \u2016ai\u2016 \u2016aj\u2016, where S : [n]\u00d7[n] denotes a function with the property |S(i, j)\u2212 \u3008ai,aj\u3009| \u2264 \u2016ai\u2016 \u2016aj\u2016 , \u2200i, j \u2208 [n] (24) 3. g3ij := 0; \u03b4ij = \u2016ai\u2016 \u2016aj\u2016, 4. g4ij \u2208u.a.r. [\u2212\u2016ai\u2016 \u2016aj\u2016 , \u2016ai\u2016 \u2016aj\u2016]; \u03b4ij = \u2016ai\u2016 \u2016aj\u2016.\nOracle g1 can be used in the rare cases where the dot product matrix is accessible to the optimization algorithm without any extra cost. In this case the updates will all be exact. If this matrix is not available, then the computation of each scalar product takes time O(d). Hence, they cannot be recomputed on the fly, as argued in Section 2.4. In contrast, the oracles g3 and g4 are extremely cheap to compute, but the error bounds are worse. In the numerical experiments in Section 7 we demonstrate that these oracles perform surprisingly well.\nThe oracle g2 can for instance be realized by lowdimensional embeddings, such as given by the JohnsonLindenstrauss lemma (cf. Achlioptas (2003); Matous\u030cek (2008)). By embedding each vector in a lower-dimensional space of dimensionO ( \u22122 log n ) and computing the scalar products of the embedding in time O(log n), relation (24) is satisfied.\nUpdating the gradient of the active coordinate. So far we only discussed the update of the passive coordinates. For the active coordinate the best strategy depends on the update ruleM from (18). If exact line search is used, then 0 \u2208 \u2207itf(xt+1). For other update rules we can update the gradient \u2207itf(xt+1) with the same gradient oracles as for the other coordinates, however we need also to take into account the change of the gradient of \u03a8i([xt]i). If \u03a8i is simple, like for instance in ridge or lasso, the subgradients at the new point can be computed efficiently.\nBounded variation. In many applications the Hessian \u22072f(Ax\u0303) is not so simple as in the case of square loss. If we assume that the Hessian of f is bounded, i.e. \u22072f(Ax) M \u00b7 In for a constant M \u2265 0, \u2200x \u2208 Rn, then it is easy to see that the following holds :\n\u2212M\u2016ai\u2016\u2016aj\u2016 \u2264 \u3008ai,\u22072f(Ax\u0303)ait\u3009 \u2264M\u2016ai\u2016\u2016aj\u2016 .\nUsing this relation, we can define gradient oracles for more general functions, by taking the additional approximation factor M into account. The quality can be improved, if we have access to local bounds on \u22072f(Ax).\nHeuristic variants. By design, ASCD is robust to high errors in the gradient estimations \u2013 the steepest descent direction is always contained in the active set. However, instead of using only the very crude oracle g4 to approximate all scalar products, it might be advantageous to compute some scalar products with higher precision. We propose to use a caching technique to compute the scalar products with high precision for all vectors in the active set (and storing a matrix of size O(It \u00d7 n)). This presumably works well if the active set does not change much over time."}, {"heading": "5. Extension to Composite Functions", "text": "The key ingredients of ASCD are the coordinate-wise upper and lower bounds on the gradient and the definition of the active set It which ensures that the steepest descent direction is always kept and that only provably bad directions are removed from the active set. These ideas can also be generalized to the setting of composite functions (2). We already discussed some popular GS-\u2217 update rules in the introduction in Section 2.3.\nImplementing ASCD for the GS-s rule is straight forward, and we comment on the GS-r in the appendix in Sec. D.2. Here we exemplary detail the modification for the GS-q rule (16), which turns out to be the most evolved (the same reasoning also applies to the GSL-q rule from (Nutini et al., 2015)). In Algo. 2 we show the construction \u2014 based just on approximations of the gradient of the smooth part f \u2014 of the active set I. For this we compute upper and lower bounds v,w on miny\u2208R V (x, y,\u2207if(x)), such that\n[v]i \u2264 min y\u2208R V (x, y,\u2207if(x) \u2264 [w]i \u2200i \u2208 [n] . (25)\nThe selection of the active coordinate is then based on these bounds. Similar as in Lemma 3.1 and Theorem 3.2 this set has the property iGS\u2212q \u2208 I, and directions are only discarded in such a way that the efficiency of ASCD-q cannot drop below the efficiency of UCD. The proof can be found in the appendix in Section D.1."}, {"heading": "6. Analysis of Competitive Ratio", "text": "In Section 3 we derived in Thm. 3.2 that the one step progress of ASCD is between the bounds on the onestep progress of UCD and SCD. However, we know that the efficiency of the latter two methods can differ much, up to a factor of n. In this section we will argue that in certain cases where SCD performs much better than UCD, ASCD will accelerate as well. To measure this effect, we could for\nAlgorithm 2 Adaptation of ASCD for GS-q rule Input: Gradient estimate g\u0303, error bounds r. For i \u2208 [n] define: compute u.-and l.-bounds [u]i := [g\u0303]i + [r]i, [`]i := [g\u0303]i \u2212 [r]i [u?]i := arg miny\u2208R V (x, y, [u]i) minimize the model [`?]i := arg miny\u2208R V (x, y, [`]i)\ncompute u.-and l. bounds on miny\u2208R V (x, y,\u2207if(x)) [\u03c9u]i := V (x, [u\n?]i, [u]i)+max{0, [u?]i([`]i \u2212 [u]i)} [\u03c9`]i := V (x, [`\n?]i, [`]i) + max{0, [`?]i([u]i \u2212 [`]i)} [v]i := min {V (x, [u?]i, [u]i), V (x, [`?]i, [`]i)} [w]i := min {[\u03c9u]i, [\u03c9`]i,\u03a8i([x]i)} av(I) := 1|I| \u2211 i\u2208I [w]i compute active set It := arg minI |{I \u2286 [n] | [v]i > av(I),\u2200i /\u2208 I}|\ninstance consider the ratio:\n%t := \u2223\u2223{i \u2208 It | |\u2207if(xt)| \u2265 12 \u2016\u2207f(xt)\u2016\u221e}\u2223\u2223 |It| , (26)\nFor general functions this expression is a bit cumbersome to study, therefore we restrict our discussion to the class of objective functions (11) as introduced in Sec. 2.2. Of course not all real-world objective functions will fall into this class, however this problem class is still very interesting in our study, as we will see in the following, because it will highlight the ability (or disability) of the algorithms to eventually identify the right set of \u2018active\u2019 coordinates.\nFor the functions with the structure (11) (and q as in Thm. 2.2), the active set falls into the first s coordinates. Hence it is reasonable to approximate %t by the competitive ratio\n\u03c1t := |It \u2229 [s]| |It| . (27)\nIt is also reasonable to assume that in the limit, (t \u2192 \u221e), a constant fraction of the [s] will be contained in the active set It (it might not hold [s] \u2286 It \u2200t, as for instance with exact line search the directional derivative vanishes just after the update). In the following theorem we calculate \u03c1t for (t\u2192\u221e), the proof is given in the appendix. Theorem 6.1. Let f : Rn \u2192 R be of the form (11). For indices i /\u2208 [s] define Ki := {t | i /\u2208 It, i \u2208 It\u22121}. For j \u2208 Ki define T ij := min {t\u2212 j | i \u2208 Ij+t}, i.e. the number of iterations outside the active set, T i\u221e := limt\u2192\u221e Ej\u2208Ki [ T ij | j > k ] , and the average T\u221e :=\nEi/\u2208[s] [ T i\u221e ] . If there exists a constant c > 0 such that limt\u2192\u221e |[s] \u2229 It| = cs, then (with the notation \u03c1\u221e := limt\u2192\u221e E [\u03c1t]),\n\u03c1\u221e \u2265 2cs\ncs+ n\u2212 s\u2212 T\u221e + \u221a \u03b8 , (28)\nwhere \u03b8 \u2261 \u03b8 := n2 + (c\u2212 1)2s2 + 2n((c\u2212 1)s\u2212 T\u221e) + 2(1 + c)sT\u221e + T\n2 \u221e. Especially, \u03c1\u221e \u2265 1\u2212 n\u2212sT\u221e .\nIn Figure 1 we compare the lower bound (28) of the competitive ratio in the limit (t \u2192 \u221e) with actual measurements of \u03c1t for simulated example with parameters n = 100, s = 10, c = 1 and various T\u221e \u2208 {50, 100, 400}. We initialized the active set I0 = [s], but we see that the equilibrium is reached quickly."}, {"heading": "6.1. Estimates of the competitive ratio", "text": "Based on this Thm. 6.1, we can now estimate the competitive ratio in various scenarios. On the class (11) it holds c \u2248 1 as we argued before. Hence the competitive ratio (28) just depends on T\u221e. This quantity measures how many iterations a coordinate j /\u2208 [s] is in average outside of the active set It. From the lower bound we see that the competitive ratio \u03c1t approaches a constant for (t \u2192 \u221e) if T\u221e = \u0398 (n), for instance \u03c1\u221e \u2265 0.8 if T\u221e \u2265 5n.\nAs an approximation to T\u221e, we estimate the quantities T j t0 defined in Thm. 6.1. T jt0 denotes the number of iterations it takes until coordinate j enters the active set again, assuming it left the active set at iteration t0 \u2212 1. We estimate T jt0 \u2265 T\u0302 , where T\u0302 denotes maximum number of iterations such that\nt0+T\u0302\u2211 t=t0 \u03b3t\u03b4iij \u2264 1 s s\u2211 k=1 \u2223\u2223\u2223\u2207kf (xt0+T\u0302)\u2223\u2223\u2223 \u2200j /\u2208 [s]. (29) For smooth functions, the steps \u03b3t = \u0398 (|\u2207itf(xt)|) and if we additionally assume that the errors of the gradient oracle are uniformly bounded \u03b4ij \u2264 \u03b4, the sum in (29) simplifies to \u03b4 \u2211t0+T\u0302 t=t0 |\u2207itf(xt)|.\nFor smooth, but not strongly convex function q, the norms of the gradient changes very slowly, with a rate independent of s or n, and we get T\u0302 = \u0398 ( 1 \u03b4 ) . Hence, the competitive\nratio is constant for \u03b4 = \u0398 (\n1 n\n) .\nFor strongly convex function q, the norm of the gradient decreases linearly, say \u2016\u2207f(xt)\u201622 \u221d e\u03bat for \u03ba \u2248 1 s . I.e. it decreases by half after each \u0398 (s) iterations. Therefore to guarantee T\u0302 = \u0398 (n) it needs to hold \u03b4 = e\u2212\u0398( n s ). This result seems to indicate that the use of ACDM is only\njustified if s is large, for instance s \u2265 14n. Otherwise the convergence on q is too fast, and the gradient approximations are too weak. However, notice that we assumed \u03b4 to be an uniform bound on all errors. If the errors have large discrepancy the estimates become much better (this holds for instance on datasets where the norm data vectors differs much, or when caching techniques as mentioned in Sec. 4 are employed)."}, {"heading": "7. Empirical Observations", "text": "In this section we evaluate the empirical performance of ASCD on synthetic and real datasets. We consider the following regularized general linear models:\nmin x\u2208Rn\n1 2\u2016Ax\u2212 b\u2016 2 2 + \u03bb 2 \u2016x\u2016 2 2 , (30)\nmin x\u2208Rn\n1 2\u2016Ax\u2212 b\u2016 2 2 + \u03bb\u2016x\u20161 , (31)\nthat is, l2-regularized least squares (30) as well as l1regularized linear regression (Lasso) in (31), respectively.\nDatasets. The datasets A \u2208 Rd\u00d7n in problems (30) and (31) were chosen as follows for our experiments. For the synthetic data, we follow the same generation procedure as described in (Nutini et al., 2015), which generates very sparse data matrices. For completeness, full details of the data generation process are also provided in the appendix in Sec. E. For the synthetic data we choose n = 5000 for problem (31) and n = 1000 for problem (30). Dimension d = 1000 is fixed for both cases. For real datasets, we perform the experimental evaluation on RCV1 (binary,training), which consists of 20, 242 samples, each of dimension 47, 236 (Lewis et al., 2004). We use the un-normalized version with all non-zeros values set to 1 (bag-of-words features).\nGradient oracles and implementation details. On the RCV1 dataset, we approximate the scalar products with the oracle g4 that was introduced in Sec. 4. This oracle is extremely cheap to compute, as the norms \u2016ai\u2016 of the columns of A only need to be computed once. On the synthetic data, we simulate the oracle g2 for various precisions values . For this, we sample a value uniformly at random from the allowed error interval (24). Figs. 2d and 3d show the convergence for different accuracies. For the l1-regularized problems, we used ASCD with the GS-s rule (the experiments in (Nutini et al., 2015) revealed almost identical performance of the different GS-\u2217 rules). We compare the performance of UCD, SCD and ASCD. We also implement the heuristic version a-ASCD that was introduced in Sec. 3. All algorithm variants use the same step size rule (i.e. the methodM in Algorithm 1). We use exact line search for the experiment in Fig. 3c, for all others we used a fixed step size rule (the convergence is slower\nfor all algorithms, but the different effects of the selection of the active coordinate is more distinctly visible). ASCD is either initialized with the true gradient (Figs. 2a, 2b, 2d, 3c, 3d) or arbitrarely (with error bounds \u03b4 =\u221e) in Figs. 3a and 3b (Fig. 2c compares both initializations). Fig. 2 shows results on the synthetic data, Fig. 3 on the RCV1 dataset. All plots show also the size of the active set It. The plots 3c and 3d are generated on a subspace of RCV1, with 10000 and 5000 randomly chosen columns, respectively.\nHere are the highlights of our experimental study:\n1. No initialization needed. We observe (see e.g. Figs. 2c,3a, 3b) that initialization with the true gradient values is not needed at beginning of the optimization process (the cost of the initialization being as expensive as one epoch of ASCD). Instead, the algorithm performs strong in terms of learning the active set on its own, and the set converges very fast after just one epoch.\n2. High errors toleration. The gradient oracle g4 gives very crude approximations, however the convergence of ASCD is excellent on RCV1 (Fig. 3). Here the size of the true active set is very small (in the order of 0.1% on RCV1) and ASCD is able to identify this set. Fig. 3d shows that almost nothing can be gained from more precise (and more expensive) oracles.\n3. Heuristic a-ASCD performs well. The convergence behavior of ASCD follows theory. For the heuristic version a-ASCD (which computes the active set slightly\nfaster, but Thm. 3.2 does not hold) performs identical to ASCD in practice (cf. Figs. 2, 3), and sometimes slightly better. This is explained by the active set used in ASCD typically being larger than the active set of aASCD (Figs. 2a,2b, 3a, 3b)."}, {"heading": "8. Concluding Remarks", "text": "We proposed ASCD, a novel selection mechanism for the active coordinate in CD methods. Our scheme enjoys three favorable properties: (i) its performance can reach the performance steepest CD \u2014 both in theory and practice, (ii) the performance is never worse than uniform CD, (iii) in many important applications, the scheme it can be implemented at no extra cost per iteration.\nASCD calculates the active set in a safe manner, and picks the active coordinate uniformly at random from this smaller set. It seems possible that an adaptive sampling strategy on the active set could boost the performance even further. Here we only study CD methods where a single coordinate gets updated in each iteration. ASCD can immediately also be generalized to block-coordinate descent methods. However, the exact implementation in a distributed setting can be challenging.\nFinally, it is an interesting direction to extend ASCD also to the stochastic gradient descent setting (not only heuristically, but with the same strong guarantees as derived in this paper)."}, {"heading": "A. On Steepest Coordinate Descent", "text": "A.1. Convergence on Smooth Functions\nLemma A.1 (Lower bound on the one step progress on smooth functions). Let f : Rn \u2192 R be convex and coordinate-wise L-smooth. For a sequence of iterates {xt}t\u22650 define the progress measure\n\u2206(xt) := 1 E [f(xt+1)\u2212 f(x?) | xt] \u2212 1 f(xt)\u2212 f(x?) . (32)\nFor sequences {xt}t\u22650 generated by SCD it holds:\n\u2206SCD(xt) \u2265 1\n2L \u2016xt \u2212 x?\u201621 , t \u2265 0 , (33)\nand for a sequences generated by UCD:\n\u2206UCD(xt) \u2265 1\n2nL \u2016xt \u2212 x?\u201622 , t \u2265 0 . (34)\nIt is important to note that the lower bounds presented in Equations (33) and (34) are quite tight and equality is almost achievable under special conditions. When comparing the per-step progress of these two methods, we find \u2014 similarly as in (7) \u2014 the relation\n1 n \u2206SCD(xt) \u2264 \u2206UCD(xt) \u2264 \u2206SCD(xt) , (35)\nthat is, SCD can boost the performance over the random coordinate descent up to the factor of n. This also holds for a sequence of consecutive updates, as show in Theorem 2.1.\nProof of Lemma A.1. Define f? := f(x?). From the smoothness assumption (1), we get\nf(xt+1) (5) \u2264 f(xt)\u2212 1\n2L \u2016\u2207f(xt)\u20162\u221e\n\u21d2 ( f(xt+1)\u2212 f? ) \u2264 ( f(xt)\u2212 f? ) \u2212 1\n2L \u2016\u2207f(xt)\u20162\u221e (36)\nNow from the property of a convex function and Ho\u0308lder\u2019s inequality:\nf(xt)\u2212 f? \u2264 \u3008\u2207f(xt),xt \u2212 x?\u3009 \u2264 \u2016\u2207f(xt)\u2016\u221e\u2016xt \u2212 x?\u20161 (37)\nHence, ( f(xt)\u2212 f? )2 \u2264 \u2016\u2207f(xt)\u20162\u221e\u2016xt \u2212 x?\u201621 \u21d2 \u2016\u2207f(xt)\u20162\u221e \u2265 ( f(xt)\u2212 f? )2 \u2016xt \u2212 x?\u201621\n(38)\nFrom Equations (36) and (38),\n1( f(xt+1)\u2212 f? ) \u2212 1( f(xt)\u2212 f? ) \u2265 1 2L\u2016xt \u2212 x?\u201621\n(39)\nWhich concludes the proof.\nWe like to remark, that the one step progress for UCD can be written as (Nesterov, 2012; Wright, 2015):\n1( E[f(xt+1)|xt]\u2212 f? ) \u2212 1( f(xt)\u2212 f? ) \u2265 1 2Ln\u2016xt \u2212 x?\u201622\n(40)\nProof of Theorem 2.1. From Lemma A.1,\n1( f(xt+1)\u2212 f? ) \u2212 1( f(xt)\u2212 f? ) \u2265 1 2L\u2016xt \u2212 x?\u201621\nNow summing up the above equation for t = 0 till t\u2212 1, we get:\n1( f(xt)\u2212 f? ) \u2212 1( f(x0)\u2212 f? ) \u2265 1 2L t\u22121\u2211 i=0\n1\n\u2016xt \u2212 x?\u201621\n\u21d2 1( f(xt)\u2212 f? ) \u2265 1 2L t\u22121\u2211 i=0\n1\n\u2016x0 \u2212 x?\u201621\n\u21d2 1( f(xt)\u2212 f? ) \u2265 t 2LR21 \u21d2 f(xt)\u2212 f? \u2264 2LR21 t\nWhich concludes the proof.\nA.2. Lower bounds\nIn this section we provide the proof of Theorem 2.2. Our result is slightly more general, we will proof the following (and Theorem 2.2 follows by the choice \u03b1 = 0.01 < 13 ). Theorem A.2. Consider the function q(x) = 12 \u3008Qx,x\u3009 for Q := (\u03b1 \u2212 1) 1 nJn + In, where Jn = 1n1 T n and 0 < \u03b1 < 1 2 , n > 2. Then there exists x0 \u2208 Rn such that for the sequence {xt}t\u22650 generated by SCD it holds\n\u2016\u2207q(xt)\u20162\u221e \u2264 3 + 3\u03b1\nn \u2016\u2207q(xt)\u201622 . (41)\nIn the proof below we will construct a special x0 \u2208 Rn that has the claimed property. However, we would like to remark that this is not very crucial. We observe that for functions as in Theorem A.2 almost any initial iterate (x not aligned with the coordinate axes) the sequence {xt}t\u22650 of iterates generated by SCD suffers from the same issue, i.e. relation (41) holds for iteration counter t sufficiently large. We do not prove this formally, but demonstrate this behavior in Figure 4. We see that the steady state is almost reached after 2n iterations.\nProof of Theorem A.2. Define the parameter c\u03b1 by the equation( 1 +\n\u03b1\u2212 1 n\n) cn\u22121\u03b1 = ( 1\u2212 \u03b1 n ) Sn\u22121(c\u03b1) (42)\ncn\u22121\u03b1 = ( 1\u2212 \u03b1 n ) Sn(c\u03b1) (43)\nwhere Sn(c\u03b1) = \u2211n\u22121 i=0 c n \u03b1; and define x0 as [x0]i = c i\u22121 \u03b1 for i = 1, . . . , n. In Lemma A.3 below we show that c\u03b1 \u2265 1\u2212 3n\u03b1.\nWe now show that SCD cycles through the coordinates, i.e. the sequence {xt}t\u22650 generated by SCD satisfies\n[xt]1+(t\u22121 mod n) = c n \u03b1 \u00b7 [xt\u22121]1+(t\u22121 mod n) . (44)\nObserve \u2207f(x0) = Qx0. Hence the GS rule picks i1 = 1 in the first iteration. The iterate is updated as follows:\n[x1]1 (3) = [x0]1 \u2212 [Qx0]1 Q11\n(45)\n= 1\u2212 (\u03b1\u2212 1) 1nSn(c\u03b1) + 1\n(\u03b1\u2212 1) 1n + 1 (46)\n= (\u03b1\u2212 1) 1n (1\u2212 Sn(c\u03b1))\n(\u03b1\u2212 1) 1n + 1 (47)\n= (\u03b1\u2212 1) 1n (c n \u03b1 \u2212 c\u03b1Sn(c\u03b1))\n(\u03b1\u2212 1) 1n + 1 (48)\n(42) =\n(\u03b1\u2212 1) 1nc n \u03b1 + c n \u03b1\n(\u03b1\u2212 1) 1n + 1 = cn\u03b1 (49)\nThe relation (44) can now easily be checked by the same reasoning and induction.\nIt remains to verify that for this sequence property (41) holds. This is done in Lemma A.4. Note that\u2207f(x0) = Qx0 = g, where g is defined as in the lemma, and that all gradients \u2207f(xt) are up to scaling and reordering of the coordinates equivalent to the vector g.\nLemma A.3. Let 0 < \u03b1 < 12 and 0 < c\u03b1 < 1 defined by equation (42), where Sn(c\u03b1) = \u2211n\u22121 i=0 c n \u03b1. Then c\u03b1 \u2265 1 \u2212 4n\u03b1 for \u03b1 \u2208 [0, 12 ].\nProof. Using the summation formula for geometric series, Sn(c\u03b1) = 1\u2212cn\u03b1 1\u2212c\u03b1 we derive\n\u03b1 (42) = 1\u2212 nc\nn\u22121 \u03b1 Sn(c\u03b1) = 1\u2212 n(1\u2212 c\u03b1)c n\u22121 \u03b1\n1\u2212 cn\u03b1\ufe38 \ufe37\ufe37 \ufe38 :=\u03a8(c\u03b1) . (50)\nWith Taylor expansion we observe that\n\u03a8 ( 1\u2212 3\u03b1\nn\n) \u2265 \u03b1 , \u03a8 ( 1\u2212 2\u03b1\nn\n) \u2264 \u03b1 (51)\nwhere the first inequality only hold for n > 2 and \u03b1 \u2264\u2208 [0, 12 ]. Hence any solution to (50) must satisfy c\u03b1 \u2265 1\u2212 3 n\u03b1.\nLemma A.4. Let c\u03b1 as in (42). Let g \u2208 Rn be defined as\n[g]i = (\u03b1\u2212 1) 1nSn(c\u03b1) + c i\u22121 \u03b1\n1 + \u03b1\u22121n (52)\nThen\nmax i\u2208[n] \u2016g\u20162\u221e 1 n \u2016g\u2016 2 2 \u2264 3 + 3\u03b1 . (53)\nProof. Observe\n[g]i = (\u03b1\u2212 1) 1n\n( Sn\u22121(c\u03b1) + c n\u22121 \u03b1 ) + cn\u22121\u03b1 + (c i\u22121 \u03b1 \u2212 cn\u22121\u03b1 )\n1 + \u03b1\u22121n (54)\n(42) = ci\u22121\u03b1 \u2212 cn\u22121\u03b1 1 + \u03b1\u22121n\n(55)\nThus [g]1 > [g]2 > \u00b7 \u00b7 \u00b7 > [g]n and the maximum is attained at\n\u03c9(g) := [g]21\n1 n \u2211n i=1[g] 2 i\n= c2\u03b1 ( c2\u03b1 \u2212 1 ) ( 1\u2212 cn\u22121\u03b1 )2 n\n2cn+1\u03b1 + 2c n+2 \u03b1 \u2212 2c2n+1\u03b1 + (n\u2212 1)c2n+2\u03b1 \u2212 c2\u03b1 \u2212 nc2n\u03b1\n(56)\nFor c\u03b1 \u2265 1\u2212 3n\u03b1 and \u03b1 \u2264 1 2 , this latter expression can be estimated as\n\u03c9(g) \u2264 3 + 3\u03b1 (57)\nespecially \u03c9(g) \u2264 4 for \u03b1 \u2264 13 ."}, {"heading": "B. Approximate Gradient Update", "text": "In this section we will prove Lemma 4.1. Consider first the following simpler case, where we assume f is given as in least squares, i.e. f(x) := 12 \u2016Ax\u2212 b\u2016 2.\nIn the tth iteration, we choose coordinate it to optimize upon and the update from xt+1 to xt can be written as xt+1 = xt + \u03b3teit . Now for any coordinate i other than it, it is fairly easy to compute the change in the gradient of the other coordinates. We already observed that [xt]j does not change, hence the sub-gradient set of \u03a8j([xt]j) and \u03a8j([xt+1]j) are equal. For the change in \u2207f , consider the analysis below:\n\u2207iF (xt+1)\u2212\u2207iF (xt) = a>i (Axt+1 \u2212 b)\u2212 a>i (Axt \u2212 b) (58) = a>i ( A(xt+1 \u2212 xt) ) (59)\n= a>i ( A(xt + \u03b3teit \u2212 xt) ) (60)\n= a>i ( \u03b3Aeit ) = \u03b3ta > i ait (61)\nEquation (60) comes from the update of xt to xt+1.\nBy the same reasoning, we can now derive the general proof.\nProof of Lemma 4.1. Consider a composite function F as given in Lemma 4.1. By the same reasoning as above, the two sub-gradient sets of \u03a8j([xt]j) and \u03a8j([xt+1]j) are identical, for every passive coordinate j 6= it. The gradient of F can be written as: \u2207iF (\u03b1t) = a>i \u2207f(A\u03b1t) For any arbitrary passive coordinate j 6= it the change of the gradient can be computed as follows:\n\u2207jF (xt+1)\u2212\u2207jF (xt) = a>j \u2207f(Axt+1)\u2212 a>j \u2207f(Axt) = a>j (\u2207f(Axt+1)\u2212\u2207f(Axt)) (62)\n= a>j ( \u2207f ( A(xt + \u03b3teit) ) \u2212\u2207f ( Axt )) \u2217 = \u2329 A>\u22072f(Ax\u0303)aj ,xt+1 \u2212 xt\n\u232a = \u2329 \u03b3t\u22072f(Ax\u0303)aj , A(xt+1 \u2212 xt)\n\u232a = \u03b3ta > j \u22072f(Ax\u0303)ait (63)\nHere x\u0303 is a point on the line segment between [xt]it and [xt+1]it which can be found by the Mean Value Theorem."}, {"heading": "C. Algorithm and Stability", "text": "Proof of Theorem 6.1. As we are interested to study the expected competitive ration E [\u03c1t] for t \u2192 \u221e, we can assume mixing and consider only the steady state.\nDefine \u03b1t \u2208 [0, 1] s.t. \u03b1t(n \u2212 s) = |{i \u2208 It | i > s}|. I.e. \u03b1t(n \u2212 s) denotes the number of indices in |It| which do not belong to the set [s].\nDenote \u03b1\u221e := limt\u2192\u221e \u03b1t. By equilibrium considerations, the probability that an index i /\u2208 [s] gets picked (and removed from the active set), i.e. 1\u2212 \u03c1\u221e, must be equal to the probability that an index j /\u2208 [s] enters the active set. Hence\n(1\u2212 \u03b1\u221e)(n\u2212 s) T\u221e = 1\u2212 \u03c1\u221e = \u03b1\u221e(n\u2212 s) \u03b1\u221e(n\u2212 s) + cs . (64)\nWe deduce the quadratic relation \u03b1\u221eT\u221e = (1\u2212 \u03b1\u221e) (\u03b1\u221e(n\u2212 s) + cs) with solution\n\u03b1\u221e = n\u2212 (1 + c)s\u2212 T\u221e +\n\u221a n2 + (c\u2212 1)2s2 + 2n((c\u2212 1)s\u2212 T\u221e) + 2(1 + c)sT\u221e + T 2\u221e\n2(n\u2212 s) . (65)\nDenote \u03b8 := n2 + (c\u2212 1)2s2 + 2n((c\u2212 1)s\u2212 T\u221e) + 2(1 + c)sT + T 2\u221e. Hence,\n\u03c1\u221e (64) =\ncs\n\u03b1\u221e(n\u2212 s) + cs (65) =\n2cs\ncs+ n\u2212 s\u2212 T\u221e + \u221a \u03b8 . (66)\nWe now verify the provided lower bound on \u03c1\u221e:\n\u03c1\u221e (64) = 1\u2212 (1\u2212 \u03b1\u221e)(n\u2212 s) T\u221e \u2265 1\u2212 n\u2212 s T\u221e . (67)\nThis bound is sharp for large values of T\u221e, (T\u221e > 2n, say), but trivial for T\u221e \u2264 n\u2212 s."}, {"heading": "D. GS rule for Composite Functions", "text": "D.1. GS-q rule\nIn this section we show how ASCD can be implemented for the GS-q rule. Define the coordinate-wise model\nVi(x, y, s) := sy + L\n2 y2 + \u03a8i(xi + y) (68)\nThe GS-q rule is defined as (cf. Nutini et al. (2015))\ni = arg min i\u2208[n] min y\u2208R\nV (x, y,\u2207if(x)) (69)\nFirst we show that the vectors v and w defined in Algorithm 2 gives valid upper and lower bounds on the value of miny\u2208R V (x, y,\u2207if(x)). We start with the lower bound v:\nSuppose we have upper and lower bounds, ` \u2264 \u2207if(x) \u2264 u on one component of the gradient. Define \u03b1 \u2208 [0, 1] such that \u2207if(x) = (1\u2212 \u03b1)`+ \u03b1u. Note that\n(1\u2212 \u03b1)Vi(x, y, `) + \u03b1Vi(x, y, u) = Vi(x, y,\u2207if(x)) (70)\nHence,\nmin { min y Vi(x, y, u),min y Vi(x, y, `) } \u2264 min y Vi(x, y,\u2207if(x)) . (71)\nThe derivation of the upper bounds w is a bit more cumbersome. Define `? := arg miny\u2208R Vi(x, y, `), u ? := arg miny\u2208R Vi(x, y, u) and observe:\nVi(x, u ?,\u2207if(x)) = Vi(x, u?, u)\u2212 (u\u2212\u2207if(x))u? \u2264 Vi(x, u?, u)\u2212 uu? + max{uu?, `u?} =: \u03c9u (72) Vi(x, ` ?,\u2207if(x)) = Vi(x, `?, `)\u2212 (`\u2212\u2207if(x))`? \u2264 Vi(x, `?, `)\u2212 ``? + max{u`?, ``?} =: \u03c9` (73)\nVi(x, 0,\u2207if(x)) = \u03a8i([x]i) (74)\nHence miny Vi(x, y,\u2207if(x)) \u2264 min{\u03c9`, \u03c9u,\u03a8i([x]i)}.\nNote\n\u03c9u = Vi(x, u ?, u) + max{0, (`\u2212 u)u?} (75) \u03c9` = Vi(x, ` ?, `) + max{0, (u\u2212 `)`?} (76)\nwhich coincides with the formulas in Algorithm 2.\nIt remains to show that the computation of the active set is save, i.e. that the progress achieved by ASCD as defined in Algorithm 2 is always better than the progress achieved by UCD. Let I be defined as in Algorithm 2. Then\n1 |I| \u2211 i\u2208I min y\u2208R Vi(x, y,\u2207if(x)) \u2264 1 n \u2211 i\u2208[n] min y\u2208R Vi(x, y,\u2207if(x)) (77)\n= 1\nn min y\u2208Rn \u2211 i\u2208[n] Vi(x, y,\u2207if(x)) . (78)\nUsing this observation, and the same lines of reasoning as given in (Lee & Seung, 1999, Section H.3), it follows immediately that the one step progress of ASCD is at least as good as the for UCD.\nD.2. GS-r rule\nWith the notation [y?]i := arg miny\u2208R Vi(x, y,\u2207if(x)), the GS-r rule is defined as (cf. Lee & Seung (1999))\ni = arg max i\u2208[n]\n|[y?]i| . (79)\nIn order to implement ASCD for GS-r, we need therefore to maintain lower and upper bounds on the values |[y?]i|.\nSuppose we have upper and lower bounds, ` \u2264 \u2207if(x) \u2264 u on one component of the gradient. Define `? := arg miny\u2208R Vi(x, y, `), u ? := arg miny\u2208R Vi(x, y, u), then y ? is contained in the line segment between `? and u?. Hence as in Algorithm 1, the lower and upper bounds can be defined as\n[ut]i := max y\u2208R {`? \u2264 y \u2264 u?} (80)\n[`t]i := min y\u2208R {`? \u2264 y \u2264 u?} (81)\nHowever, note that in (Nutini et al., 2015) it is established that GS-r rule can be worse than UCD in general. Hence we cannot expect that ASCD for the GS-r rule is better than UCD in general. However, the by the choice of the active set, the index chosen by the GS-r rule is always contained in the active set, and ASCD approaches GS-r for small errors."}, {"heading": "E. Experimental Details", "text": "We generate a matrix A \u2208 Rm\u00d7n from the standard normal N (0, 1) distribution. m is kept fixed at 1000 but n is chosen 1000 for the l2 regularized least squares regression and 5000 for l1 regularized counterpart. 1 is added to each entry (to induce a dependency between columns), multiplied each column by a sample from N (0, 1) multiplied by ten (to induce different Lipschitz constants across the coordinates), and only kept each entry of A non-zero with probability 10 log(n)n . This is exactly the same procedure which has been discussed in (Nutini et al., 2015)."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["Achlioptas", "Dimitris"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Achlioptas and Dimitris.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas and Dimitris.", "year": 2003}, {"title": "Even faster accelerated coordinate descent using non-uniform sampling", "author": ["Z Allen-Zhu", "Z Qu", "P Richtarik", "Y. Yuan"], "venue": null, "citeRegEx": "Allen.Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allen.Zhu et al\\.", "year": 2016}, {"title": "Convex optimization", "author": ["Boyd", "Stephen P", "Vandenberghe", "Lieven"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2004}, {"title": "Stochastic Dual Coordinate Ascent with Adaptive Probabilities", "author": ["Csiba", "Dominik", "Qu", "Zheng", "Richt\u00e1rik", "Peter"], "venue": "In ICML 2015 - Proceedings of the 32th International Conference on Machine Learning,", "citeRegEx": "Csiba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Csiba et al\\.", "year": 2015}, {"title": "Siobhan\u2019s problem: The coupon collector revisited", "author": ["Dawkins", "Brian"], "venue": "The American Statistician,", "citeRegEx": "Dawkins and Brian.,? \\Q1991\\E", "shortCiteRegEx": "Dawkins and Brian.", "year": 1991}, {"title": "Nearest Neighbor based Greedy Coordinate Descent", "author": ["Dhillon", "Inderjit S", "Ravikumar", "Pradeep", "Tewari", "Ambuj"], "venue": "In NIPS 2014 - Advances in Neural Information Processing Systems", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Pathwise coordinate optimization", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "H\u00f6fling", "Holger", "Tibshirani", "Robert"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "Regularization Paths for Generalized Linear Models via Coordinate Descent", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "Journal of Statistical Software,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Penalized regressions: The bridge versus the lasso", "author": ["Fu", "Wenjiang J"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Fu and J.,? \\Q1998\\E", "shortCiteRegEx": "Fu and J.", "year": 1998}, {"title": "A Dual Coordinate Descent Method for Large-scale Linear SVM", "author": ["Hsieh", "Cho-Jui", "Chang", "Kai-Wei", "Lin", "Chih-Jen", "Keerthi", "S Sathiya", "S. Sundararajan"], "venue": "In the 25th International Conference on Machine Learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method", "author": ["Kim", "Hyunsoo", "Park", "Haesun"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Kim et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2008}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Lee", "Daniel D", "Seung", "H Sebastian"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "On variants of the johnsonlindenstrauss lemma", "author": ["Matou\u0161ek", "Ji\u0159\u0131"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Matou\u0161ek and Ji\u0159\u0131\u0301.,? \\Q2008\\E", "shortCiteRegEx": "Matou\u0161ek and Ji\u0159\u0131\u0301.", "year": 2008}, {"title": "Efficiency of coordinate descent methods on hugescale optimization problems", "author": ["Nesterov", "Yu"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nesterov and Yu.,? \\Q2012\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 2012}, {"title": "Efficiency of the accelerated coordinate descent method on structured optimization problems", "author": ["Nesterov", "Yurii", "Stich", "Sebastian U"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nesterov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Nesterov et al\\.", "year": 2017}, {"title": "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection", "author": ["Nutini", "Julie", "Schmidt", "Mark W", "Laradji", "Issam H", "Friedlander", "Michael P", "Koepke", "Hoyt A"], "venue": "In ICML,", "citeRegEx": "Nutini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nutini et al\\.", "year": 2015}, {"title": "Adaptive Sampling for Incremental Optimization Using Stochastic Gradient Descent", "author": ["Papa", "Guillaume", "Bianchi", "Pascal", "Cl\u00e9men\u00e7on", "St\u00e9phan"], "venue": "ALT 2015 - 26th International Conference on Algorithmic Learning Theory, pp", "citeRegEx": "Papa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papa et al\\.", "year": 2015}, {"title": "Coordinate descent with arbitrary sampling i: algorithms and complexity", "author": ["Qu", "Zheng", "Richt\u00e1rik", "Peter"], "venue": "Optimization Methods and Software,", "citeRegEx": "Qu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2016}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2016}, {"title": "Stochastic Methods for l1-regularized Loss Minimization", "author": ["Shalev-Shwartz", "Shai", "Tewari", "Ambuj"], "venue": "JMLR, 12:1865\u20131892,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "JMLR, 14:567\u2013599,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)", "author": ["Shrivastava", "Anshumali", "Li", "Ping"], "venue": "In NIPS 2014 - Advances in Neural Information Processing Systems", "citeRegEx": "Shrivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2014}, {"title": "A coordinate gradient descent method for nonsmooth separable minimization", "author": ["Tseng", "Paul", "Yun", "Sangwoon"], "venue": "Mathematical Programming,", "citeRegEx": "Tseng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tseng et al\\.", "year": 2009}, {"title": "On the convergence of an active-set method for 1 minimization", "author": ["Wen", "Zaiwen", "Yin", "Wotao", "Zhang", "Hongchao", "Goldfarb", "Donald"], "venue": "Optimization Methods and Software,", "citeRegEx": "Wen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2012}, {"title": "Coordinate descent algorithms", "author": ["Wright", "Stephen J"], "venue": "Mathematical Programming,", "citeRegEx": "Wright and J.,? \\Q2015\\E", "shortCiteRegEx": "Wright and J.", "year": 2015}, {"title": "Coordinate descent algorithms for lasso penalized regression", "author": ["Wu", "Tong Tong", "Lange", "Kenneth"], "venue": "Ann. Appl. Stat.,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Stochastic optimization with importance sampling for regularized loss minimization", "author": ["Zhao", "Peilin", "Zhang", "Tong"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Due to their computational efficiency, scalability, as well as their ease of implementation, these methods are the state-of-theart for a wide selection of machine learning and signal processing applications (Fu, 1998; Hsieh et al., 2008; Wright, 2015).", "startOffset": 207, "endOffset": 251}, {"referenceID": 6, "context": "For instance in (Shalev-Shwartz & Zhang, 2013; Friedman et al., 2007; 2010; Shalev-Shwartz & Tewari, 2011) uniform sampling (UCD) is used, whereas other papers propose adaptive sampling strategies that change over time (Papa et al.", "startOffset": 16, "endOffset": 106}, {"referenceID": 17, "context": ", 2007; 2010; Shalev-Shwartz & Tewari, 2011) uniform sampling (UCD) is used, whereas other papers propose adaptive sampling strategies that change over time (Papa et al., 2015; Csiba et al., 2015; Osokin et al., 2016; Perekrestenko et al., 2017).", "startOffset": 157, "endOffset": 245}, {"referenceID": 3, "context": ", 2007; 2010; Shalev-Shwartz & Tewari, 2011) uniform sampling (UCD) is used, whereas other papers propose adaptive sampling strategies that change over time (Papa et al., 2015; Csiba et al., 2015; Osokin et al., 2016; Perekrestenko et al., 2017).", "startOffset": 157, "endOffset": 245}, {"referenceID": 16, "context": "For smooth functions this strategy yields always better progress than UCD, and the speedup can reach a factor of the dimension (Nutini et al., 2015).", "startOffset": 127, "endOffset": 148}, {"referenceID": 3, "context": ", 2015; Csiba et al., 2015; Osokin et al., 2016; Perekrestenko et al., 2017). A very simple deterministic strategy is to move along the direction corresponding to the component of the gradient with the maximal absolute value (steepest coordinate descent, SCD) (Boyd & Vandenberghe, 2004; Tseng & Yun, 2009). For smooth functions this strategy yields always better progress than UCD, and the speedup can reach a factor of the dimension (Nutini et al., 2015). However, SCD requires the computation of the whole gradient vector in each iteration which is prohibitive (except for special applications, cf. Dhillon et al. (2011); Shrivastava & Li (2014)).", "startOffset": 8, "endOffset": 624}, {"referenceID": 3, "context": ", 2015; Csiba et al., 2015; Osokin et al., 2016; Perekrestenko et al., 2017). A very simple deterministic strategy is to move along the direction corresponding to the component of the gradient with the maximal absolute value (steepest coordinate descent, SCD) (Boyd & Vandenberghe, 2004; Tseng & Yun, 2009). For smooth functions this strategy yields always better progress than UCD, and the speedup can reach a factor of the dimension (Nutini et al., 2015). However, SCD requires the computation of the whole gradient vector in each iteration which is prohibitive (except for special applications, cf. Dhillon et al. (2011); Shrivastava & Li (2014)).", "startOffset": 8, "endOffset": 649}, {"referenceID": 16, "context": "Nutini et al. (2015) present an elegant solution of this problem for \u03bc2-strongly convex functions2.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "We here extend the analysis from (Nutini et al., 2015) to smooth functions.", "startOffset": 33, "endOffset": 54}, {"referenceID": 24, "context": "Kim & Park (2008); Wen et al. (2012)).", "startOffset": 19, "endOffset": 37}, {"referenceID": 16, "context": "Here we exemplary detail the modification for the GS-q rule (16), which turns out to be the most evolved (the same reasoning also applies to the GSL-q rule from (Nutini et al., 2015)).", "startOffset": 161, "endOffset": 182}, {"referenceID": 16, "context": "For the synthetic data, we follow the same generation procedure as described in (Nutini et al., 2015), which generates very sparse data matrices.", "startOffset": 80, "endOffset": 101}, {"referenceID": 12, "context": "For real datasets, we perform the experimental evaluation on RCV1 (binary,training), which consists of 20, 242 samples, each of dimension 47, 236 (Lewis et al., 2004).", "startOffset": 146, "endOffset": 166}, {"referenceID": 16, "context": "For the l1-regularized problems, we used ASCD with the GS-s rule (the experiments in (Nutini et al., 2015) revealed almost identical performance of the different GS-\u2217 rules).", "startOffset": 85, "endOffset": 106}], "year": 2017, "abstractText": "We propose a new selection rule for the coordinate selection in coordinate descent methods for huge-scale optimization. The efficiency of this novel scheme is provably better than the efficiency of uniformly random selection, and can reach the efficiency of steepest coordinate descent (SCD), enabling an acceleration of a factor of up to n, the number of coordinates. In many practical applications, our scheme can be implemented at no extra cost and computational efficiency very close to the faster uniform selection. Numerical experiments with Lasso and Ridge regression show promising improvements, in line with our theoretical guarantees.", "creator": "LaTeX with hyperref package"}}}