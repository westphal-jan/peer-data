{"id": "1706.08606", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2017", "title": "Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study", "abstract": "deep neural networks ( dnns ) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. while past work sought to advance our understanding of these models, none has made use of the rich history of abstract problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. to explore the potential value of these tools, we chose a well - established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to dnns. using datasets of stimuli inspired by the original cognitive psychology experiments, we actually find that state - of - the - art one shot learning models trained on imagenet exhibit a similar bias to that observed perception in humans : they prefer to significantly categorize objects according to shape rather than apparent color. the magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite generating nearly equivalent classification performance. these results demonstrate the capability of tools from classical cognitive psychology for exposing hidden microscopic computational biological properties of dnns, while concurrently providing emerging us with a computational model for human word learning.", "histories": [["v1", "Mon, 26 Jun 2017 21:31:18 GMT  (1552kb,D)", "http://arxiv.org/abs/1706.08606v1", "ICML 2017"], ["v2", "Thu, 29 Jun 2017 17:52:55 GMT  (1552kb,D)", "http://arxiv.org/abs/1706.08606v2", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["samuel ritter", "david g t barrett", "adam santoro", "matt m botvinick"], "accepted": true, "id": "1706.08606"}, "pdf": {"name": "1706.08606.pdf", "metadata": {"source": "META", "title": "Cognitive Psychology for Deep Neural Networks:  A Shape Bias Case Study ", "authors": ["Samuel Ritter", "David G.T. Barrett", "Adam Santoro", "Matt M. Botvinick"], "emails": ["<ritters@google.com>,", "<barrett@google.com>."], "sections": [{"heading": "1. Introduction", "text": "During the last half-decade deep learning has significantly improved performance on a variety of tasks (for a review, see LeCun et al. (2015)). However, deep neural network (DNN) solutions remain poorly understood, leaving many\n*Equal contribution 1DeepMind, London, UK. Correspondence to: Samuel Ritter <ritters@google.com>, David G.T. Barrett <barrett@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nto think of these models as black boxes, and to question whether they can be understood at all (Bornstein, 2016; Lipton, 2016). This opacity obstructs both basic research seeking to improve these models, and applications of these models to real world problems (Caruana et al., 2015).\nRecent pushes have aimed to better understand DNNs: tailor-made loss functions and architectures produce more interpretable features (Higgins et al., 2016; Raposo et al., 2017) while output-behavior analyses unveil previously opaque operations of these networks (Karpathy et al., 2015). Parallel to this work, neuroscience-inspired methods such as activation visualization (Li et al., 2015), ablation analysis (Zeiler & Fergus, 2014) and activation maximization (Yosinski et al., 2015) have also been applied.\nAltogether, this line of research developed a set of promising tools for understanding DNNs, each paper producing a glimmer of insight. Here, we propose another tool for the kit, leveraging methods inspired not by neuroscience, but instead by psychology. Cognitive psychologists have long wrestled with the problem of understanding another opaque intelligent system: the human mind. We contend that the search for a better understanding of DNNs may profit from the rich heritage of problem descriptions, theories, and experimental tools developed in cognitive psychology. To test this belief, we performed a proof-ofconcept study on state-of-the-art DNNs that solve a particularly challenging task: one-shot word learning. Specifically, we investigate Matching Networks (MNs) (Vinyals et al., 2016), which have state-of-the-art one-shot learning performance on ImageNet and we investigate an Inception Baseline model (Szegedy et al., 2015a).\nFollowing the approach used in cognitive psychology, we began by hypothesizing an inductive bias our model may use to solve a word learning task. Research in developmental psychology shows that when learning new words, humans tend to assign the same name to similarly shaped items rather than to items with similar color, texture, or size. To test the hypothesis that our DNNs discover this same \u201cshape bias\u201d, we probed our models using datasets and an experimental setup based on the original shape bias studies (Landau et al., 1988).\nar X\niv :1\n70 6.\n08 60\n6v 1\n[ st\nat .M\nL ]\n2 6\nJu n\n20 17\nOur results are as follows: 1) Inception networks trained on ImageNet do indeed display a strong shape bias. 2) There is high variance in the bias between Inception networks initialized with different random seeds, demonstrating that otherwise identical networks converge to qualitatively different solutions. 3) MNs also have a strong shape bias, and this bias closely mimics the bias of the Inception model that provides input to the MN. 4) By emulating the shape bias observed in children, these models provide a candidate computational account for human one-shot word learning. Altogether, these results show that the technique of testing hypothesized biases using probe datasets can yield both expected and surprising insights about solutions discovered by trained DNNs1."}, {"heading": "2. Inductive Biases, Statistical Learners and Probe Datasets", "text": "Before we delve into the specifics of the shape bias and one-shot word learning, we will describe our approach in the general context of inductive biases, probe datasets, and statistical learning. Suppose we have some data {yi, xi}Ni=1 where yi = f(xi). Our goal is to build a model of the data g(.) to optimize some loss function L measuring the disparity between y and g(x), e.g., L = \u2211 i ||yi\u2212 g(xi)||2. Perhaps this data x is images of ImageNet objects to be classified, images and histology of tumors to be classified as benign or malignant (Kourou et al., 2015), or medical history and vital measurements to be classified according to likely pneumonia outcomes (Caruana et al., 2015).\nA statistical learner such as a DNN will minimize L by discovering properties of the input x that are predictive of the labels y. These discovered predictive properties are, in effect, the properties of x for which the trained model has an inductive bias. Examples of such properties include the shape of ImageNet objects, the number of nodes of a tumor, or a particular constellation of blood test values that often precedes an exacerbation of pneumonia symptoms.\nCritically, in real-world datasets such as these, the discovered properties are unlikely to correspond to a single feature of the input x; instead they correspond to complex conjunctions of those features. We could describe one of these properties using a function h(x), which, for example, returns the shape of the focal object given an ImageNet im-\n1The use of behavioral probes to understand neural network function has been extensively applied within psychology itself, where neural networks have been employed as models of human brain function (Rumelhart et al., 1988; Plaut et al., 1996; Rogers & McClelland, 2004; Mareschal et al., 2000). To our knowledge, work applying behavioral probes to DNNs in machine learning has been quite limited; we only are aware of Zoran et al. (2015) and Goodfellow et al. (2009), who used psychophysics-like experiments to better understand image processing models.\nage, or the number of nodes given a scan of tumor. Indeed, one way to articulate the difficulty in understanding DNNs is to say that we often can\u2019t intuitively describe these conjunctions of features h(x); although we often have numerical representations in intermediate DNN layers, they\u2019re often too arcane for us to interpret.\nWe advocate for addressing this problem using the following hypothesis-driven approach: First, propose a property hp(x) that the model may be using. Critically, it\u2019s not necessary that hp(x) be a function that can be evaluated using an automated method. Instead, the intention is that hp(x) is a function that humans (e.g. ML researchers and practitioners) can intuitively evaluate. hp(x) should be a property that is believed to be relevant to the problem, such as object shape or number of tumor nodes.\nAfter proposing a property, the next step is to generate predictions about how the model should behave when given various inputs, if in fact it uses a bias with respect to the property hp(x). Then, construct and carry out an experiment wherein those predictions are tested. In order to execute such an experiment, it typically will be necessary to craft a set of probe examples x that cover a relevant portion of the range of hp(x), for example a variety of object shapes. The results of this experiment will either support or fail to support the hypothesis that the model uses hp(x) to solve the task. This process can be especially valuable in situations where there is little or no training data available in important regions of the input space, and a practitioner needs to know how the trained model will behave in that region.\nPsychologists have developed a repertoire of such hypotheses and experiments in their effort to understand the human mind. Here we explore the application of one of these theory-experiment pairs to state of the art one-shot learning models. We will begin by describing the historical backdrop for the human one-shot word learning experiments that we will then apply to our DNNs."}, {"heading": "3. The problem of word learning; the solution of inductive biases", "text": "Discussions of one-shot word learning in the psychological literature inevitably begin with the philosopher W.V.O. Quine, who broke this problem down and described one of its most computationally challenging components: there are an enormous number of tenable hypotheses that a learner can use to explain a single observed example. To make this point, Quine penned his now-famous parable of the field linguist who has gone to visit a culture whose language is entirely different from our own (Quine, 1960). The linguist is trying to learn some words from a helpful native, when a rabbit runs past. The native declares \u201cgava-\ngai\u201d, and the linguist is left to infer the meaning of this new word. Quine points out that the linguist is faced with an abundance of possible inferences, including that \u201cgavagai\u201d refers to rabbits, animals, white things, that specific rabbit, or \u201cundetached parts of rabbits\u201d. Quine argues that indeed there is an infinity of possible inferences to be made, and uses this conclusion to bolster the assertion that meaning itself cannot be defined in terms of internal mental events2.\nContrary to Quine\u2019s intentions, when this example was introduced to the developmental psychology community by Macnamara (1972), it spurred them not to give up on the idea of internal meaning, but instead to posit and test for cognitive biases that enable children to eliminate broad swaths of the hypothesis space (Bloom, 2000). A variety of hypothesis-eliminating biases were then proposed including the whole object bias, by which children assume that a word refers to an entire object and not its components (Markman, 1990); the taxonomic bias, by which children assume a word refers to the basic level category an object belongs to (Markman & Hutchinson, 1984); the mutual exclusivity bias, by which children assume that a word only refers to one object category (Markman & Wachtel, 1988); the shape bias, with which we are concerned here (Landau et al., 1988); and a variety of others (Bloom, 2000). These biases were tested empirically in experiments wherein children or adults were given an object (or picture of an object) along with a novel name, then were asked whether the name should apply to various other objects.\nTaken as a whole, this work yielded a computational level (Marr, 1982) account of word learning whereby people make use of biases to eliminate unlikely hypotheses when inferring the meaning of new words. Other contrasting and complementary approaches to explaining word learning exist in the psychological literature, including association learning (Regier, 1996; Colunga & Smith, 2005) and Bayesian inference (Xu & Tenenbaum, 2007). We leave the application of these theories to deep learning models to future work, and focus on determining what insight can be gained by applying a hypothesis elimination theory and methodology.\nWe begin the present work with the knowledge that part of the hypothesis elimination theory is correct: the models surely use some kind of inductive biases since they are statistical learning machines that successfully model the mapping between images and object labels. However, several questions remain open. What predictive properties did our DNNs find? Do all of them find the same properties? Are any of those properties interpretable to humans? Are they the same properties that children use? How do these biases change over the course of training?\nTo address these questions, we carry out experiments analogous to those of Landau et al. (1988). This enables us to\ntest whether the shape bias \u2013 a human interpretable feature used by children when learning language \u2013 is visible in the behavior of MNs and Inception networks. Furthermore we are able to test whether these two models, as well as different instances of each of them, display the same bias. In the next section we will describe in detail the one-shot word learning problem, and the MNs and Inception networks we use to solve it."}, {"heading": "4. One-shot word learning models and training", "text": ""}, {"heading": "4.1. One-shot word learning task", "text": "The one-shot word learning task is to label a novel data example x\u0302 (e.g. a novel probe image) with a novel class label y\u0302 (e.g. a new word) after only a single example. More specifically, given a support set S = {(xi, yi) : i \u2208 [1, k]}, of images xi and their associated labels yi, and an unlabelled probe image x\u0302, the one-shot learning task is to identify the true label of the probe image y\u0302 from the support set labels {yi : i \u2208 [1, k]}:\ny\u0302 = argmax y\nP (y|x\u0302, S). (1)\nWe assume that the image labels yi are represented using a one-hot encoding and that P (y|x\u0302, S) is parameterised by a DNN, allowing us to leverage the ability of deep networks to learn powerful representations."}, {"heading": "4.2. Inception: baseline one-shot learning model", "text": "In our simplest baseline one-shot architecture, a probe image x\u0302 is given the label of the nearest neighbour from the support set:\ny\u0302 = y\n(x, y) = arg min (xi,yi)\u2208S\nd(h(xi), h(x\u0302)) (2)\nwhere d is a distance function. The function h is parameterised by Inception \u2013 one of the best performing ImageNet classification models (Szegedy et al., 2015a). Specifically, h returns features from the last layer (the softmax input) of a pre-trained Inception classifier, where the Inception classifier is trained using rms-prop, as described in Szegedy et al. (2015b), section 8. With these features as input and cosine distance as the distance function, the classifier in equation 2 achieves 87.6% accuracy on one-shot classification on the ImageNet dataset (Vinyals et al., 2016). Henceforth, we call the Inception classifier together with the nearest-neighbor component the Inception Baseline (IB) model.\n2Unlike Quine, we use a pragmatic definition of meaning - a human or model understands the meaning of a word if they assign that word to new instances of objects in the correct category."}, {"heading": "4.3. Matching Nets model architecture and training", "text": "We also investigate a state-of-the-art one-shot learning architecture called Matching Nets (MN) (Vinyals et al., 2016). MNs are a fully differentiable neural network architecture with state-of-the-art one shot learning performance on ImageNet (93.2% one-shot labelling accuracy).\nMNs are trained to assign label y\u0302 to probe image x\u0302 according to equation 1 using an attention mechanism a acting on image embeddings stored in the support set S:\na(x\u0302, xi) = ed(f(x\u0302,S),g(xi,S))\u2211 j e d(f(x\u0302,S),g(xj ,S)) , (3)\nwhere d is a cosine distance and where f and g provide context-dependent embeddings of x\u0302 and xi (with context S). The embedding g(xi, S) is a bi-directional LSTM (Hochreiter & Schmidhuber, 1997) with the support set S provided as an input sequence. The embedding f(x\u0302, S) is an LSTM with a read-attention mechanism operating over the entire embedded support set. The input to the LSTM is given by the penultimate layer features of a pre-trained deep convolutional network, specifically Inception, as in our baseline IB model described above (Szegedy et al., 2015a).\nThe training procedure for the one-shot learning task is critical if we want MNs to classify a probe image x\u0302 after viewing only a single example of this new image class in its support set (Hochreiter et al., 2001; Santoro et al., 2016).\nTo train MNs we proceed as follows: (1) At each step of training, the model is given a small support set of images and associated labels. In addition to the support set, the model is fed an unlabelled probe image x\u0302; (2) The model parameters are then updated to improve classification accuracy of the probe image x\u0302 given the support set. Parameters are updated using stochastic gradient descent with a learning rate of 0.1; (3) After each update, the labels {yi : i \u2208 [1, k]} in the training set are randomly re-assigned to new image classes (the label indices are randomly permuted, but the image labels are not changed). This is a critical step. It prevents MNs from learning a consistent mapping between a category and a label. Usually, in classification, this is what we want, but in one-shot learning we want to train our model for classification after viewing a single in-class example from the support set. Formally, our objective function is:\nL = EC\u223cT ES\u223cC,B\u223cC  \u2211 (x,y)\u2208B logP (y|x, S)  (4) where T is the set of all possible labelings of our classes, S is a support set sampled with a class labelling C \u223c T and B is a batch of probe images and labels, also with the same randomly chosen class labelling as the support set.\nNext we will describe the probe datasets we used to test for the shape bias in the IB and MNs after ImageNet training."}, {"heading": "5. Data for bias discovery", "text": ""}, {"heading": "5.1. Cognitive Psychology Probe Data", "text": "The Cognitive Psychology Probe Data (CogPsyc data) that we use consists of 150 images of objects (Figure 1). The images are arranged in triples consisting of a probe image, a shape-match image (that matches the probe in colour but not shape), and a color-match image (that matches the probe in shape but not colour). In the dataset there are 10 triples, each shown on 5 different backgrounds, giving a total of 50 triples.3\nThe images were generously provided by cognitive psychologist Linda Smith. The images are photographs of stimuli used previously in shape bias experiments conducted in the Cognitive Development Lab at Indiana University. The potentially confounding variables of background content and object size are controlled in this dataset."}, {"heading": "5.2. Probe Data from the wild", "text": "We have also assembled a real-world dataset consisting of 90 images of objects (30 triples) collected using Google Image Search. Again, the images are arranged in triples consisting of a probe, a shape-match and a colour-match. For the probe image, we chose images of real objects that are unlikely to appear in standard image datasets such as ImageNet. In this way, our data contains the irregularity of the real world while also probing our models\u2019 properties outside of the image space covered in our training data. For the shape-match image, we chose an object with a similar shape (but with a very different colour), and for the colourmatch image, we chose an object with a similar colour (but with a very different shape). For example, one triple consists of a silver tuning fork as the probe, a silver guitar capo as the colour match, and a black tuning fork as the shape match. Each photo in the dataset contains a single object on a white background.\nWe collected this data to strengthen our confidence in the results obtained for the CogPsych dataset and to demonstrate the ease with which such probe datasets can be constructed. One of the authors crafted this dataset solely using Google Image Search in the span of roughly two days\u2019 work. Our results with this dataset, especially the fact that the bias pattern over time matches the results from the well established CogPsych dataset, support the contention that DNN practitioners can collect effective probe datasets with minimal time expenditure using readily available tools.\n3 The CogPsyc dataset is available at http://www. indiana.edu/\u02dccogdev/SB_testsets.html"}, {"heading": "6. Results", "text": ""}, {"heading": "6.1. Shape bias in the Inception Baseline Model", "text": "First, we measured the shape bias in IB: we used a pretrained Inception classifier (with 94% top-5 accuracy) to provide features for our nearest-neighbour one-shot classifier, and probed the model using the CogPsyc dataset. Specifically, for a given probe image x\u0302, we loaded the shape-match image xs and corresponding label ys, along with the colour-match image xc and corresponding label yc into memory, as the support set S = {(xs, ys), (xc, yc)}. We then calculated y\u0302 using Equation 2. Our model assigned either yc or ys to the probe image. To estimate the shape bias Bs, we calculated the proportion of shape labels assigned to the probe:\nBs = E(\u03b4(y\u0302 \u2212 ys)), (5)\nwhere E is an expectation across probe images and \u03b4 is the Dirac delta function.\nWe ran all IB experiments using both Euclidean and cosine distance as the distance function. We found that the results for the two distance functions were qualitatively similar, so we only report results for Euclidean distance.\nWe found the shape bias of IB to be Bs = 0.68. Similarly, the shape bias of IB using our real-world dataset was Bs = 0.97. Together, these results strongly suggest that IB\ntrained on ImageNet has a stronger bias towards shape than colour.\nNote that, as expected, the shape bias of this model is qualitatively similar across datasets while being quantitatively different - largely because the datasets themselves are quite different. Indeed, the datasets were chosen to be quite different so that we could explore a broad space of possibilities. In particular, our CogPsyc dataset backgrounds have much larger variability than our real-world dataset backgrounds, and our real-world dataset objects have much greater variability than the CogPsyc dataset objects."}, {"heading": "6.2. Shape bias in the Matching Nets Model", "text": "Next, we probed the MNs using a similar procedure. We used the IB trained in the previous section to provide the input features for the MN as described in section 4.3. Then, following the training procedure outlined in section 4.3 we trained MNs for one-shot word learning on ImageNet, achieving state-of-the-art performance, as reported in (Vinyals et al., 2016). Then, repeating the analysis above, we found that MNs have a shape of bias Bs = 0.7 using our CogPsyc dataset and a bias of Bs = 1 using the real-world dataset. It is interesting to note that these bias values are very similar to the IB bias values."}, {"heading": "6.3. Shape bias statistics: within models and across models", "text": "The observation of a shape bias immediately raises some important questions. In particular: (1) Does this bias depend on the initial values of the parameters in our model? (2) Does the size of the shape bias depend on model performance? (3) When does shape bias emerge during training - before model convergence or afterwards? (4) How does shape bias compare between models, and within models?\nTo answer these questions, we extended the shape bias analysis described above to calculate the shape bias in a population of IB models and in a population of MN models with different random initialization (Figs. 2 and 5).\n(1) We first calculated the dependence of shape bias on the initialization of IB (Fig. 2). Surprisingly, we observed a strong variability, depending on the initialization. For the CogPsyc dataset, the average shape bias was Bs = 0.628 with standard deviation \u03c3Bs = 0.049 at the end of training and for the real-world dataset the average shape bias was Bs = 0.958 with \u03c3Bs = 0.037.\n(2) Next, we calculated the dependence of shape bias on model performance. For the CogPsych dataset, the correlation between bias and classification accuracy was \u03c1 = 0.15, with tn=15 = 0.55, pone tail = 0.29, and for the real-world dataset, the correlation was \u03c1 = \u22120.06 with tn=15 = \u22120.22, pone tail = 0.42. Therefore, fluctuations\nin the bias cannot be accounted for by fluctuations in classification accuracy. This is not surprising, because the classification accuracy of all models was similar at the end of training, while the shape bias was variable. This demonstrates that models can have variable behaviour along important dimensions (e.g., bias) while having the same performance measured by another (e.g., accuracy).\n(3) Next we explored the emergence of the shape bias during training (Fig. 2a,c; Fig. 5a,c). At the start of training, the average shape bias of these models was Bs = 0.448 with standard deviation \u03c3Bs = 0.0835 on the CogPsyc dataset and Bs = 0.593 with \u03c3Bs = 0.073 on the real-world dataset. We observe that a shape bias began to emerge very early during training, long before convergence.\n(4) Finally, we compare shape bias within models during training, and between models at the end of training. During training, the shape bias within IB fluctuates signifi-\ncantly (Fig. 2 b; Fig. 5b). In contrast, the shape bias does not fluctuate during training of the MN. Instead, the MN model inherits its shape bias characteristics at the start of training from the IB that provides it with input embeddings (Fig. 4) and this shape-bias remains constant throughout training. Moreover, there is no evidence that the MN and corresponding IB bias values are different from each other (paired t-test, p = 0.167). Note that we do not fine-tune the Inception model providing input while training the MN. We do this so that we can observe the shape-bias properties of the MN independent of the IB model properties."}, {"heading": "7. Discussion", "text": ""}, {"heading": "7.1. A shape bias case study", "text": "Our psychology-inspired approach to understanding DNNs produced a number of insights. Firstly, we found that both IB and MNs trained on ImageNet display a strong shape bias. This is an important result for practitioners who routinely use these models - especially for applications where it is known a priori that colour is more important than shape. As an illustrative example, if a practitioner planned to build a one-shot fruit classification system, they should proceed with caution if they plan to use pre-trained ImageNet models like Inception and MNs because fruit are often defined according to colour features rather than shape. In applications where a shape bias is desirable (as is more often the case than not), this result provides reassurance that the models are behaving sensibly in the presence of ambiguity.\nThe second surprising finding was the large variability in shape bias, both within models during training and across models, depending on the randomly chosen initialisation of our model. This variability can arise because our models are not being explicitly optimised for shape biased categorisation. This is an important result because it shows that not all models are created equally - some models will have a stronger preference for shape than others, even though they are architecturally identical and have almost identical classification accuracy.\nOur third finding \u2013 that MNs retain the shape bias statistics of the downstream Inception network \u2013 demonstrates the possibility for biases to propagate across model components. In this case, the shape bias propagates from the Inception model through to the MN memory modules. This result is yet another cautionary observation; when combin-\ning multiple modules together, we must be aware of contamination by unknown properties across modules. Indeed, a bias that is benign in one module might only have a detrimental effect when combined later with other modules.\nA natural question immediately arises from these results - how can we remove an unwanted bias or induce a desirable bias? The biases under consideration are properties of an architecture and dataset synthesized together by an optimization procedure. As such, the observation of a shapebias is partly a result of the statistics of natural imagelabellings as captured in the ImageNet dataset, and partly a result of the architecture attempting to extract these statistics. Therefore, on discovering an unwanted bias, a practitioner can either attempt to change the model architecture to explicitly prevent the bias from emerging, or, they can attempt to manipulate the training data. If neither of these are possible - for example, if the appropriate data manipulation is too expensive, or, if the bias cannot be easily suppressed in the architecture, it may be possible to do zero-th order optimization of the models. For example, one may perform post-hoc model selection either using early stopping or by selecting a suitable model from the set of initial seeds.\nAn important caveat to note is that behavioral tools often do not provide insight into the neural mechanisms. In our case, the DNN mechanism whereby model parameters and input images interact to give rise to a shape bias have not been elucidated, nor did we expect this to happen. Indeed, just as cognitive psychology often does for neuroscience, our new computational level insights can provide a starting point for research at the mechanistic level. For example, in future work it would be interesting to use gradient-based visualization or neuron ablation techniques to augment the current results by identifying the mechanisms underlying the shape bias. The convergence of evidence from such\nintrospective methods with the current behavioral method would create a richer account of these models\u2019 solutions to the one-shot word learning problem."}, {"heading": "7.2. Modelling human word learning", "text": "There have been previous attempts to model human word learning in the cognitive science literature (Colunga & Smith, 2005; Xu & Tenenbaum, 2007; Schilling et al., 2012; Mayor & Plunkett, 2010). However, none of these models are capable of one-shot word learning on the scale of real-world images. Because MNs both solve the task at scale and emulate hallmark experimental findings, we propose MNs as a computational-level account of human one-shot word learning. Another feature of our results supports this contention: in our model the shape bias increases dramatically early in training (Fig. 2a); similarly, humans show the shape bias much more strongly as adults than as children, and older children show the bias more strongly than younger children (Landau et al., 1988).\nAs a good cognitive model should, our DNNs make testable predictions about word-learning in humans. Specifically, the current results predict that the shape bias should vary across subjects as well as within a subject over the course of development. They also predict that for humans with adult-level one-shot word learning abilities, there should be no correlation between shape bias magnitude and oneshot-word learning capability.\nAnother promising direction for future cognitive research would be to probe MNs for additional biases in order to predict novel computational properties in humans. Probing a model in this way is much faster than running human behavioural experiments, so a wider range of hypotheses for human word learning may be rapidly tested."}, {"heading": "7.3. Cognitive Psychology for Deep Neural Networks", "text": "Through the one-shot learning case study, we demonstrated the utility of leveraging techniques from cognitive psychology for understanding the computational properties of DNNs. There is a wide ranging literature in cognitive psychology describing techniques for probing a spectrum of behaviours in humans. Our work here leads the way to the study of artificial cognitive psychology - the application of these techniques to better understand DNNs.\nFor example, it would be useful to apply work from the massive literature on episodic memory (Tulving, 1985) to the recent flurry of episodic memory architectures (Blundell et al., 2016; Graves et al., 2016), and to apply techniques from the semantic cognition literature (Lamberts & Shanks, 2013) to recent models of concept formation (Higgins et al., 2016; Gregor et al., 2016; Raposo et al., 2017). More generally, the rich psychological literature will be-\ncome increasingly useful for understanding deep reinforcement learning agents as they learn to solve increasingly complex tasks."}, {"heading": "8. Conclusion", "text": "In this work, we have demonstrated how techniques from cognitive psychology can be leveraged to help us better understand DNNs. As a case study, we measured the shape bias in two powerful yet poorly understood DNNs - Inception and MNs. Our analysis revealed previously unknown properties of these models. More generally, our work leads the way for future exploration of DNNs using the rich body of techniques developed in cognitive psychology."}, {"heading": "Acknowledgements", "text": "We would like to thank Linda Smith and Charlotte Wozniak for providing the Cognitive Psychology probe dataset; Charles Blundell for reviewing our paper prior to submission; Oriol Vinyals, Daan Wierstra, Peter Dayan, Daniel Zoran, Ian Osband and Karen Simonyan for helpful discussions; James Besley for legal assistance; and the DeepMind team for support."}], "references": [{"title": "How children learn the meanings of words. MIT press", "author": ["Bloom", "Paul"], "venue": null, "citeRegEx": "Bloom and Paul.,? \\Q2000\\E", "shortCiteRegEx": "Bloom and Paul.", "year": 2000}, {"title": "Model-free episodic control", "author": ["Blundell", "Charles", "Uria", "Benigno", "Pritzel", "Alexander", "Li", "Yazhe", "Ruderman", "Avraham", "Leibo", "Joel Z", "Rae", "Jack", "Wierstra", "Daan", "Hassabis", "Demis"], "venue": "arXiv preprint arXiv:1606.04460,", "citeRegEx": "Blundell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2016}, {"title": "Is artificial intelligence permanently inscrutable? Despite new biology-like tools, some insist interpretation is impossible", "author": ["Bornstein", "Aaron"], "venue": null, "citeRegEx": "Bornstein and Aaron.,? \\Q2016\\E", "shortCiteRegEx": "Bornstein and Aaron.", "year": 2016}, {"title": "From the lexicon to expectations about kinds: a role for associative learning", "author": ["Colunga", "Eliana", "Smith", "Linda B"], "venue": "Psychological review,", "citeRegEx": "Colunga et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Colunga et al\\.", "year": 2005}, {"title": "Measuring invariances in deep networks. In Advances in neural information processing", "author": ["Goodfellow", "Ian", "Lee", "Honglak", "Le", "Quoc V", "Saxe", "Andrew", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Towards conceptual compression", "author": ["Gregor", "Karol", "Besse", "Frederic", "Rezende", "Danilo Jimenez", "Danihelka", "Ivo", "Wierstra", "Daan"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Gregor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2016}, {"title": "Early visual concept learning with unsupervised deep learning", "author": ["Higgins", "Irina", "Matthey", "Loic", "Glorot", "Xavier", "Pal", "Arka", "Uria", "Benigno", "Blundell", "Charles", "Mohamed", "Shakir", "Lerchner", "Alexander"], "venue": "arXiv preprint arXiv:1606.05579,", "citeRegEx": "Higgins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning to learn using gradient descent", "author": ["Hochreiter", "Sepp", "Younger", "A Steven", "Conwell", "Peter R"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Machine learning applications in cancer prognosis and prediction", "author": ["Kourou", "Konstantina", "Exarchos", "Themis P", "Konstantinos P", "Karamouzis", "Michalis V", "Fotiadis", "Dimitrios I"], "venue": "Computational and structural biotechnology journal,", "citeRegEx": "Kourou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kourou et al\\.", "year": 2015}, {"title": "Knowledge Concepts and Categories", "author": ["Lamberts", "Koen", "Shanks", "David"], "venue": null, "citeRegEx": "Lamberts et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lamberts et al\\.", "year": 2013}, {"title": "The importance of shape in early lexical learning", "author": ["Landau", "Barbara", "Smith", "Linda B", "Jones", "Susan S"], "venue": "Cognitive development,", "citeRegEx": "Landau et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Landau et al\\.", "year": 1988}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Li", "Jiwei", "Chen", "Xinlei", "Hovy", "Eduard", "Jurafsky", "Dan"], "venue": "arXiv preprint arXiv:1506.01066,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "The mythos of model interpretability", "author": ["Lipton", "Zachary C"], "venue": "arXiv preprint arXiv:1606.03490,", "citeRegEx": "Lipton and C.,? \\Q2016\\E", "shortCiteRegEx": "Lipton and C.", "year": 2016}, {"title": "Cognitive basis of language learning in infants", "author": ["Macnamara", "John"], "venue": "Psychological review,", "citeRegEx": "Macnamara and John.,? \\Q1972\\E", "shortCiteRegEx": "Macnamara and John.", "year": 1972}, {"title": "A connectionist account of asymmetric category learning in early infancy", "author": ["Mareschal", "Denis", "French", "Robert M", "Quinn", "Paul C"], "venue": "Developmental psychology,", "citeRegEx": "Mareschal et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Mareschal et al\\.", "year": 2000}, {"title": "Constraints children place on word meanings", "author": ["Markman", "Ellen M"], "venue": "Cognitive Science,", "citeRegEx": "Markman and M.,? \\Q1990\\E", "shortCiteRegEx": "Markman and M.", "year": 1990}, {"title": "Children\u2019s sensitivity to constraints on word meaning: Taxonomic versus thematic relations", "author": ["Markman", "Ellen M", "Hutchinson", "Jean E"], "venue": "Cognitive psychology,", "citeRegEx": "Markman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Markman et al\\.", "year": 1984}, {"title": "Children\u2019s use of mutual exclusivity to constrain the meanings of words", "author": ["Markman", "Ellen M", "Wachtel", "Gwyn F"], "venue": "Cognitive psychology,", "citeRegEx": "Markman et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Markman et al\\.", "year": 1988}, {"title": "Vision: A computational investigation into the human representation and processing of visual information, henry holt and co", "author": ["Marr", "David"], "venue": null, "citeRegEx": "Marr and David.,? \\Q1982\\E", "shortCiteRegEx": "Marr and David.", "year": 1982}, {"title": "A neurocomputational account of taxonomic responding and fast mapping in early word learning", "author": ["Mayor", "Julien", "Plunkett", "Kim"], "venue": "Psychological review,", "citeRegEx": "Mayor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mayor et al\\.", "year": 2010}, {"title": "Understanding normal and impaired word reading: computational principles in quasiregular domains", "author": ["Plaut", "David C", "McClelland", "James L", "Seidenberg", "Mark S", "Patterson", "Karalyn"], "venue": "Psychological review,", "citeRegEx": "Plaut et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Plaut et al\\.", "year": 1996}, {"title": "Discovering objects and their relations from entangled scene representations", "author": ["Raposo", "David", "Santoro", "Adam", "Barrett", "David G.T", "Pascanu", "Razvan", "Lillicrap", "Timothy", "Battaglia", "Peter"], "venue": "arXiv preprint arXiv:1702.05068,", "citeRegEx": "Raposo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Raposo et al\\.", "year": 2017}, {"title": "The human semantic potential: Spatial language and constrained connectionism", "author": ["Regier", "Terry"], "venue": null, "citeRegEx": "Regier and Terry.,? \\Q1996\\E", "shortCiteRegEx": "Regier and Terry.", "year": 1996}, {"title": "Semantic cognition: A parallel distributed processing approach", "author": ["Rogers", "Timothy T", "McClelland", "James L"], "venue": "MIT press,", "citeRegEx": "Rogers et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2004}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Taking development seriously: Modeling the interactions in the emergence of different word learning biases", "author": ["Schilling", "Savannah M", "Sims", "Clare E", "Colunga", "Eliana"], "venue": "In CogSci,", "citeRegEx": "Schilling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schilling et al\\.", "year": 2012}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Szegedy", "Christian", "Vanhoucke", "Vincent", "Ioffe", "Sergey", "Shlens", "Jonathon", "Wojna", "Zbigniew"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Timothy", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1606.04080,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Word learning as bayesian inference", "author": ["Xu", "Fei", "Tenenbaum", "Joshua B"], "venue": "Psychological review,", "citeRegEx": "Xu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2007}, {"title": "Understanding neural networks through deep visualization", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Nguyen", "Anh", "Fuchs", "Thomas", "Lipson", "Hod"], "venue": "arXiv preprint arXiv:1506.06579,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European Conference on Computer Vision, pp", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Learning ordinal relationships for mid-level vision", "author": ["Zoran", "Daniel", "Isola", "Phillip", "Krishnan", "Dilip", "Freeman", "William T"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Zoran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoran et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Recent pushes have aimed to better understand DNNs: tailor-made loss functions and architectures produce more interpretable features (Higgins et al., 2016; Raposo et al., 2017) while output-behavior analyses unveil previously opaque operations of these networks (Karpathy et al.", "startOffset": 133, "endOffset": 176}, {"referenceID": 23, "context": "Recent pushes have aimed to better understand DNNs: tailor-made loss functions and architectures produce more interpretable features (Higgins et al., 2016; Raposo et al., 2017) while output-behavior analyses unveil previously opaque operations of these networks (Karpathy et al.", "startOffset": 133, "endOffset": 176}, {"referenceID": 9, "context": ", 2017) while output-behavior analyses unveil previously opaque operations of these networks (Karpathy et al., 2015).", "startOffset": 93, "endOffset": 116}, {"referenceID": 13, "context": "Parallel to this work, neuroscience-inspired methods such as activation visualization (Li et al., 2015), ablation analysis (Zeiler & Fergus, 2014) and activation maximization (Yosinski et al.", "startOffset": 86, "endOffset": 103}, {"referenceID": 31, "context": ", 2015), ablation analysis (Zeiler & Fergus, 2014) and activation maximization (Yosinski et al., 2015) have also been applied.", "startOffset": 79, "endOffset": 102}, {"referenceID": 29, "context": "Specifically, we investigate Matching Networks (MNs) (Vinyals et al., 2016), which have state-of-the-art one-shot learning performance on ImageNet and we investigate an Inception Baseline model (Szegedy et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 12, "context": "To test the hypothesis that our DNNs discover this same \u201cshape bias\u201d, we probed our models using datasets and an experimental setup based on the original shape bias studies (Landau et al., 1988).", "startOffset": 173, "endOffset": 194}, {"referenceID": 10, "context": "Perhaps this data x is images of ImageNet objects to be classified, images and histology of tumors to be classified as benign or malignant (Kourou et al., 2015), or medical history and vital measurements to be classified according to likely pneumonia outcomes (Caruana et al.", "startOffset": 139, "endOffset": 160}, {"referenceID": 22, "context": "The use of behavioral probes to understand neural network function has been extensively applied within psychology itself, where neural networks have been employed as models of human brain function (Rumelhart et al., 1988; Plaut et al., 1996; Rogers & McClelland, 2004; Mareschal et al., 2000).", "startOffset": 197, "endOffset": 292}, {"referenceID": 16, "context": "The use of behavioral probes to understand neural network function has been extensively applied within psychology itself, where neural networks have been employed as models of human brain function (Rumelhart et al., 1988; Plaut et al., 1996; Rogers & McClelland, 2004; Mareschal et al., 2000).", "startOffset": 197, "endOffset": 292}, {"referenceID": 15, "context": ", 1996; Rogers & McClelland, 2004; Mareschal et al., 2000). To our knowledge, work applying behavioral probes to DNNs in machine learning has been quite limited; we only are aware of Zoran et al. (2015) and Goodfellow et al.", "startOffset": 35, "endOffset": 203}, {"referenceID": 4, "context": "(2015) and Goodfellow et al. (2009), who used psychophysics-like experiments to better understand image processing models.", "startOffset": 11, "endOffset": 36}, {"referenceID": 12, "context": "A variety of hypothesis-eliminating biases were then proposed including the whole object bias, by which children assume that a word refers to an entire object and not its components (Markman, 1990); the taxonomic bias, by which children assume a word refers to the basic level category an object belongs to (Markman & Hutchinson, 1984); the mutual exclusivity bias, by which children assume that a word only refers to one object category (Markman & Wachtel, 1988); the shape bias, with which we are concerned here (Landau et al., 1988); and a variety of others (Bloom, 2000).", "startOffset": 514, "endOffset": 535}, {"referenceID": 12, "context": "To address these questions, we carry out experiments analogous to those of Landau et al. (1988). This enables us to test whether the shape bias \u2013 a human interpretable feature used by children when learning language \u2013 is visible in the behavior of MNs and Inception networks.", "startOffset": 75, "endOffset": 96}, {"referenceID": 29, "context": "6% accuracy on one-shot classification on the ImageNet dataset (Vinyals et al., 2016).", "startOffset": 63, "endOffset": 85}, {"referenceID": 28, "context": "The function h is parameterised by Inception \u2013 one of the best performing ImageNet classification models (Szegedy et al., 2015a). Specifically, h returns features from the last layer (the softmax input) of a pre-trained Inception classifier, where the Inception classifier is trained using rms-prop, as described in Szegedy et al. (2015b), section 8.", "startOffset": 106, "endOffset": 339}, {"referenceID": 29, "context": "We also investigate a state-of-the-art one-shot learning architecture called Matching Nets (MN) (Vinyals et al., 2016).", "startOffset": 96, "endOffset": 118}, {"referenceID": 8, "context": "The training procedure for the one-shot learning task is critical if we want MNs to classify a probe image x\u0302 after viewing only a single example of this new image class in its support set (Hochreiter et al., 2001; Santoro et al., 2016).", "startOffset": 189, "endOffset": 236}, {"referenceID": 26, "context": "The training procedure for the one-shot learning task is critical if we want MNs to classify a probe image x\u0302 after viewing only a single example of this new image class in its support set (Hochreiter et al., 2001; Santoro et al., 2016).", "startOffset": 189, "endOffset": 236}, {"referenceID": 29, "context": "3 we trained MNs for one-shot word learning on ImageNet, achieving state-of-the-art performance, as reported in (Vinyals et al., 2016).", "startOffset": 112, "endOffset": 134}, {"referenceID": 27, "context": "There have been previous attempts to model human word learning in the cognitive science literature (Colunga & Smith, 2005; Xu & Tenenbaum, 2007; Schilling et al., 2012; Mayor & Plunkett, 2010).", "startOffset": 99, "endOffset": 192}, {"referenceID": 12, "context": "2a); similarly, humans show the shape bias much more strongly as adults than as children, and older children show the bias more strongly than younger children (Landau et al., 1988).", "startOffset": 159, "endOffset": 180}, {"referenceID": 1, "context": "For example, it would be useful to apply work from the massive literature on episodic memory (Tulving, 1985) to the recent flurry of episodic memory architectures (Blundell et al., 2016; Graves et al., 2016), and to apply techniques from the semantic cognition literature (Lamberts & Shanks, 2013) to recent models of concept formation (Higgins et al.", "startOffset": 163, "endOffset": 207}, {"referenceID": 6, "context": ", 2016), and to apply techniques from the semantic cognition literature (Lamberts & Shanks, 2013) to recent models of concept formation (Higgins et al., 2016; Gregor et al., 2016; Raposo et al., 2017).", "startOffset": 136, "endOffset": 200}, {"referenceID": 5, "context": ", 2016), and to apply techniques from the semantic cognition literature (Lamberts & Shanks, 2013) to recent models of concept formation (Higgins et al., 2016; Gregor et al., 2016; Raposo et al., 2017).", "startOffset": 136, "endOffset": 200}, {"referenceID": 23, "context": ", 2016), and to apply techniques from the semantic cognition literature (Lamberts & Shanks, 2013) to recent models of concept formation (Higgins et al., 2016; Gregor et al., 2016; Raposo et al., 2017).", "startOffset": 136, "endOffset": 200}], "year": 2017, "abstractText": "Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.", "creator": "LaTeX with hyperref package"}}}