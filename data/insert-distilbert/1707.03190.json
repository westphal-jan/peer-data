{"id": "1707.03190", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2017", "title": "Accelerated Variance Reduced Stochastic ADMM", "abstract": "recently, many variance reduced stochastic alternating direction method of multipliers ( admm ) methods ( # e. g. \\ ratio sag - admm, sdca - admm hmm and svrg - admm ) have greatly made exciting progress such as linear convergence rates for strongly convex problems. however, the best known convergence rate for general convex problems is o ( 1 / t ) as opposed to o ( + 1 / t ^ 2 ) of accelerated batch arithmetic algorithms, where $ t $ is the number of iterations. thus thus, there still remains a gap in convergence rates between existing stochastic admm and batch algorithms. to bridge this gap, we introduce the momentum acceleration trick for batch optimization into the stochastic variance reduced gradient based admm ( svrg - admm ), which leads to an accelerated ( asvrg - admm ) method. then we design two different momentum term update rules for strongly convex and general convex cases. we prove immediately that asvrg - admm converges linearly for relatively strongly convex problems. besides having a low per - iteration complexity as existing stochastic admm methods, asvrg - admm improves the convergence rate on general convex problems from o ( 1 / t ) to o ( 1 / t ^ { 2 ). our experimental results show the effectiveness of asvrg - admm.", "histories": [["v1", "Tue, 11 Jul 2017 09:29:46 GMT  (120kb,D)", "http://arxiv.org/abs/1707.03190v1", "16 pages, 5 figures, Appears in Proceedings of the 31th AAAI Conference on Artificial Intelligence (AAAI), San Francisco, California, USA, pp. 2287--2293, 2017"]], "COMMENTS": "16 pages, 5 figures, Appears in Proceedings of the 31th AAAI Conference on Artificial Intelligence (AAAI), San Francisco, California, USA, pp. 2287--2293, 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yuanyuan liu", "fanhua shang", "james cheng"], "accepted": true, "id": "1707.03190"}, "pdf": {"name": "1707.03190.pdf", "metadata": {"source": "CRF", "title": "Accelerated Variance Reduced Stochastic ADMM", "authors": ["Yuanyuan Liu", "Fanhua Shang", "James Cheng"], "emails": ["jcheng}@cse.cuhk.edu.hk"], "sections": [{"heading": null, "text": "Introduction In this paper, we consider a class of composite convex optimization problems\nmin x\u2208Rd1 f(x) + h(Ax), (1)\nwhere A\u2208Rd2\u00d7d1 is a given matrix, f(x) := 1n \u2211n i=1fi(x), each fi(x) is a convex function, and h(Ax) is convex but possibly non-smooth. With regard to h(\u00b7), we are interested in a sparsity-inducing regularizer, e.g. `1-norm, group Lasso and nuclear norm. When A is an identity matrix, i.e. A = Id1 , the above formulation (1) arises in many places in machine learning, statistics, and operations research (Bubeck 2015), such as logistic regression, Lasso and support vector machine (SVM). We mainly focus on the large sample regime. In this regime, even first-order batch methods, e.g. FISTA (Beck and Teboulle 2009), become computationally burdensome due to their per-iteration complexity of O(nd1). As a result, stochastic gradient descent (SGD) with per-iteration complexity of O(d1) has\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nwitnessed tremendous progress in the recent years. Especially, a number of stochastic variance reduced gradient methods such as SAG (Roux, Schmidt, and Bach 2012), SDCA (Shalev-Shwartz and Zhang 2013) and SVRG (Johnson and Zhang 2013) have been proposed to successfully address the problem of high variance of the gradient estimate in ordinary SGD, resulting in a linear convergence rate (for strongly convex problems) as opposed to sub-linear rates of SGD. More recently, the Nesterov\u2019s acceleration technique (Nesterov 2004) was introduced in (Allen-Zhu 2016; Hien et al. 2016) to further speed up the stochastic variancereduced algorithms, which results in the best known convergence rates for both strongly convex and general convex problems. This motivates us to integrate the momentum acceleration trick into the stochastic alternating direction method of multipliers (ADMM) below.\nWhen A is a more general matrix, i.e. A 6=Id1 , the formulation (1) becomes many more complicated problems arising from machine learning, e.g. graph-guided fuzed Lasso (Kim, Sohn, and Xing 2009) and generalized Lasso (Tibshirani and Taylor 2011). To solve this class of composite optimization problems with an auxiliary variable y = Ax, which are the special case of the general ADMM form,\nmin x\u2208Rd1,y\u2208Rd2 f(x) + h(y), s.t. Ax+By = c, (2)\nthe ADMM is an effective optimization tool (Boyd et al. 2011), and has shown attractive performance in a wide range of real-world problems, such as big data classification (Nie et al. 2014). To tackle the issue of high periteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), Wang and Banerjee (2012), Suzuki (2013) and Ouyang et al. (2013) proposed some online or stochastic ADMM algorithms. However, all these variants only achieve the convergence rate of O(1/ \u221a T ) for general convex problems and O(log T/T ) for strongly convex problems, respectively, as compared with the O(1/T 2) and linear convergence rates of accelerated batch algorithms (Nesterov 1983), e.g. FISTA, where T is the number of iterations. By now several accelerated and faster converging versions of stochastic ADMM, which are all based on variance reduction techniques, have been proposed, e.g. SAG-ADMM (Zhong and Kwok 2014b), SDCA-ADMM (Suzuki 2014) and SVRG-ADMM (Zheng and Kwok 2016). With regard to strongly convex problems, ar X iv :1\n70 7.\n03 19\n0v 1\n[ cs\n.L G\n] 1\n1 Ju\nl 2 01\n7\nSuzuki (2014) and Zheng and Kwok (2016) proved that linear convergence can be obtained for the special ADMM form (i.e. B = \u2212Id2 and c = 0) and the general ADMM form, respectively. In SAG-ADMM and SVRG-ADMM, an O(1/T ) convergence rate can be guaranteed for general convex problems, which implies that there still remains a gap in convergence rates between the stochastic ADMM and accelerated batch algorithms.\nTo bridge this gap, we integrate the momentum acceleration trick in (Tseng 2010) for deterministic optimization into the stochastic variance reduction gradient (SVRG) based stochastic ADMM (SVRG-ADMM). Naturally, the proposed method has low per-iteration time complexity as existing stochastic ADMM algorithms, and does not require the storage of all gradients (or dual variables) as in SCAS-ADMM (Zhao, Li, and Zhou 2015) and SVRGADMM (Zheng and Kwok 2016), as shown in Table 1. We summarize our main contributions below.\n\u2022 We propose an accelerated variance reduced stochastic ADMM (ASVRG-ADMM) method, which integrates both the momentum acceleration trick in (Tseng 2010) for batch optimization and the variance reduction technique of SVRG (Johnson and Zhang 2013).\n\u2022 We prove that ASVRG-ADMM achieves a linear convergence rate for strongly convex problems, which is consistent with the best known result in SDCA-ADMM (Suzuki 2014) and SVRG-ADMM (Zheng and Kwok 2016).\n\u2022 We also prove that ASVRG-ADMM has a convergence rate of O(1/T 2) for non-strongly convex problems, which is a factor of T faster than SAG-ADMM and SVRG-ADMM, whose convergence rates are O(1/T ).\n\u2022 Our experimental results further verified that our ASVRG-ADMM method has much better performance than the state-of-the-art stochastic ADMM methods.\nRelated Work Introducing y = Ax \u2208Rd2 , problem (1) becomes\nmin x\u2208Rd1,y\u2208Rd2\nf(x) + h(y), s.t. Ax\u2212 y = 0. (3)\nAlthough (3) is only a special case of the general ADMM form (2), when B = \u2212Id2 and c = 0, the stochastic (or online) ADMM algorithms and theoretical results in (Wang and Banerjee 2012; Ouyang et al. 2013; Zhong and Kwok 2014b; Zheng and Kwok 2016) and this paper are all for the more general problem (2). To minimize (2), together with\nthe dual variable \u03bb, the update steps of batch ADMM are\nyk = argminy h(y) + \u03b2 2 \u2016Axk\u22121+By\u2212c+\u03bbk\u22121\u2016 2, (4) xk = argminx f(x) + \u03b2 2 \u2016Ax+Byk\u2212c+\u03bbk\u22121\u2016\n2, (5) \u03bbk = \u03bbk\u22121 +Axk +Byk \u2212 c, (6)\nwhere \u03b2>0 is a penalty parameter. To extend the batch ADMM to the online and stochastic settings, the update steps for yk and \u03bbk remain unchanged. In (Wang and Banerjee 2012; Ouyang et al. 2013), the update step of xk is approximated as follows:\nxk = argmin x xT\u2207fik(xk\u22121) +\n1\n2\u03b7k \u2016x\u2212 xk\u22121\u20162G\n+ \u03b2\n2 \u2016Ax+Byk\u2212c+\u03bbk\u22121\u20162,\n(7)\nwhere we draw ik uniformly at random from [n] := {1, . . . , n}, \u03b7k \u221d 1/ \u221a k is the step-size, and \u2016z\u20162G = zTGz with given positive semi-definite matrix G, e.g. G = Id1 in (Ouyang et al. 2013). Analogous to SGD, the stochastic ADMM variants use an unbiased estimate of the gradient at each iteration. However, all those algorithms have much slower convergence rates than their batch counterpart, as mentioned above. This barrier is mainly due to the variance introduced by the stochasticity of the gradients. Besides, to guarantee convergence, they employ a decaying sequence of step sizes \u03b7k, which in turn impacts the rates.\nMore recently, a number of variance reduced stochastic ADMM methods (e.g. SAG-ADMM, SDCA-ADMM and SVRG-ADMM) have been proposed and made exciting progress such as linear convergence rates. SVRG-ADMM in (Zheng and Kwok 2016) is particularly attractive here because of its low storage requirement compared with the algorithms in (Zhong and Kwok 2014b; Suzuki 2014). Within each epoch of SVRG-ADMM, the full gradient p\u0303 =\u2207f(x\u0303) is first computed, where x\u0303 is the average point of the previous epoch. Then \u2207fik(xk\u22121) and \u03b7k in (7) are replaced by\n\u2207\u0303fIk(xk\u22121) = 1 |Ik| \u2211 ik\u2208Ik (\u2207fik(xk\u22121)\u2212\u2207fik(x\u0303)) + p\u0303 (8)\nand a constant step-size \u03b7, respectively, where Ik \u2282 [n] is a mini-batch of size b (which is a useful technique to reduce the variance). In fact, \u2207\u0303fIk(xk\u22121) is an unbiased estimator of the gradient \u2207f(xk\u22121), i.e. E[\u2207\u0303fIk(xk\u22121)]=\u2207f(xk\u22121).\nAccelerated Variance Reduced Stochastic ADMM\nIn this section, we design an accelerated variance reduced stochastic ADMM method for both strongly convex and general convex problems. We first make the following assumptions: Each convex fi(\u00b7) is Li-smooth, i.e. there exists a constant Li>0 such that \u2016\u2207fi(x)\u2212\u2207fi(y)\u2016\u2264Li\u2016x\u2212y\u2016, \u2200x, y \u2208 Rd, and L , maxi Li; f(\u00b7) is \u00b5-strongly convex, i.e. there is \u00b5 > 0 such that f(x) \u2265 f(y) +\u2207f(y)T(x\u2212 y)+ \u00b52 \u2016x\u2212y\u2016\n2 for all x, y \u2208Rd; The matrix A has full row rank. The first two assumptions are common in the analysis of first-order optimization methods, while the last one has\nAlgorithm 1 ASVRG-ADMM for strongly-convex case Input: m, \u03b7, \u03b2 > 0, 1 \u2264 b \u2264 n. Initialize: x\u03030= z\u03030, y\u03030, \u03b8, \u03bb\u03030=\u2212 1\u03b2 (A\nT )\u2020\u2207f(x\u03030). 1: for s = 1, 2, . . . , T do 2: xs0 = z s 0 = x\u0303 s\u22121, ys0 = y\u0303 s\u22121, \u03bbs0 = \u03bb\u0303\ns\u22121; 3: p\u0303 = \u2207f(x\u0303s\u22121); 4: for k = 1, 2, . . . ,m do 5: Choose Ik\u2286 [n]of size b, uniformly at random; 6: ysk=argminy h(y)+ \u03b2 2 \u2016Az s k\u22121+By\u2212 c+\u03bbsk\u22121\u20162;\n7: zsk=z s k\u22121\u2212\n\u03b7(\u2207\u0303fIk(x s k\u22121)+\u03b2A T(Azsk\u22121+By s k\u2212c+\u03bb s k\u22121))\n\u03b3\u03b8 ; 8: xsk=(1\u2212 \u03b8)x\u0303s\u22121 + \u03b8zsk; 9: \u03bbsk=\u03bb s k\u22121 +Az s k +By s k \u2212 c;\n10: end for 11: x\u0303s= 1m \u2211m k=1x s k, y\u0303 s=(1\u2212\u03b8)y\u0303s\u22121+ \u03b8m \u2211m k=1y s k, 12: \u03bb\u0303s=\u2212 1\u03b2 (A T )\u2020\u2207f(x\u0303s); 13: end for Output: x\u0303T , y\u0303T .\nbeen used in the convergence analysis of batch ADMM (?; Nishihara et al. 2015; Deng and Yin 2016) and stochastic ADMM (Zheng and Kwok 2016).\nThe Strongly Convex Case In this part, we consider the case of (2) when each fi(\u00b7) is convex,L-smooth, and f(\u00b7) is \u00b5-strongly convex. Recall that this class of problems include graph-guided Logistic Regression and SVM as notable examples. To efficiently solve this class of problems, we incorporate both the momentum acceleration and variance reduction techniques into stochastic ADMM. Our algorithm is divided into T epochs, and each epoch consists of m stochastic updates, where m is usually chosen to be O(n) as in (Johnson and Zhang 2013).\nLet z be an important auxiliary variable, its update rule is given as follows. Similar to (Zhong and Kwok 2014b; Zheng and Kwok 2016), we also use the inexact Uzawa method (Zhang, Burger, and Osher 2011) to approximate the sub-problem (7), which can avoid computing the inverse of the matrix ( 1\u03b7 Id1+\u03b2A\nTA). Moreover, the momentum weight 0\u2264 \u03b8s \u2264 1 (the update rule for \u03b8s is provided below) is introduced into the proximal term 12\u03b7\u2016x\u2212xk\u22121\u2016 2 G similar to that of (7), and then the sub-problem with respect to z is formulated as follows:\nmin z (z \u2212zsk\u22121)T \u2207\u0303fIk(xsk\u22121)+ \u03b8s\u22121 2\u03b7 \u2016z \u2212zsk\u22121\u20162G\n+ \u03b2\n2 \u2016Az +Bysk \u2212 c+ \u03bbsk\u22121\u20162,\n(9)\nwhere \u2207\u0303fIk(xsk\u22121) is defined in (8), \u03b7 < 12L , and G = \u03b3Id1\u2212 \u03b7\u03b2 \u03b8s\u22121 ATA with \u03b3 \u2265 \u03b3min \u2261 \u03b7\u03b2\u2016A TA\u20162 \u03b8s\u22121 +1 to ensure that G I similar to (Zheng and Kwok 2016), where \u2016\u00b7\u20162 is the spectral norm, i.e. the largest singular value of the matrix. Furthermore, the update rule for x is given by\nxsk= x\u0303 s\u22121+\u03b8s\u22121(z s k\u2212 x\u0303s\u22121)=(1\u2212\u03b8s\u22121)x\u0303s\u22121+\u03b8s\u22121zsk, (10)\nwhere \u03b8s\u22121(zsk \u2212 x\u0303s\u22121) is the key momentum term (similar to those in accelerated batch methods (Nesterov 2004)), which helps accelerate our algorithm by using the iterate of the previous epoch, i.e. x\u0303s\u22121. Similar to xsk, y\u0303\ns = (1\u2212 \u03b8s\u22121)y\u0303 s\u22121+ \u03b8s\u22121m \u2211m k=1y s k. Moreover, \u03b8s can be set to a constant \u03b8 in all epochs of our algorithm, which must satisfy 0 \u2264 \u03b8 \u2264 1\u2212 \u03b4(b)/(\u03b1\u22121), where \u03b1 = 1L\u03b7 > 1+ \u03b4(b), and \u03b4(b) is defined below. The optimal value of \u03b8 is provided in Proposition 1 below. The detailed procedure is shown in Algorithm 1, where we adopt the same initialization technique for \u03bb\u0303s as in (Zheng and Kwok 2016), and (\u00b7)\u2020 is the pseudo-inverse. Note that, when \u03b8=1, ASVRG-ADMM degenerates to SVRG-ADMM in (Zheng and Kwok 2016).\nThe Non-Strongly Convex Case In this part, we consider general convex problems of the form (2) when each fi(\u00b7) is convex, L-smooth, and h(\u00b7) is not necessarily strongly convex (but possibly non-smooth). Different from the strongly convex case, the momentum weight \u03b8s is required to satisfy the following inequalities:\n1\u2212 \u03b8s \u03b82s \u2264 1 \u03b82s\u22121 and 0 \u2264 \u03b8s \u2264 1\u2212 \u03b4(b) \u03b1\u2212 1 , (11)\nwhere \u03b4(b) := n\u2212bb(n\u22121) is a decreasing function with respect to the mini-batch size b. The condition (20) allows the momentum weight to decease, but not too fast, similar to the requirement on the step-size \u03b7k in classical SGD and stochastic ADMM (?). Unlike batch acceleration methods, the weight must satisfy both inequalities in (20).\nMotivated by the momentum acceleration techniques in (Tseng 2010; Nesterov 2004) for batch optimization, we give the update rule of the weight \u03b8s for the mini-batch case:\n\u03b8s =\n\u221a \u03b84s\u22121+ 4\u03b8\n2 s\u22121 \u2212 \u03b82s\u22121 2 and \u03b80 = 1\u2212 \u03b4(b) \u03b1\u2212 1 . (12)\nFor the special case of b = 1, we have \u03b4(1) = 1 and \u03b80 = 1\u2212 1\u03b1\u22121 , while b=n (i.e. batch version), \u03b4(n)=0 and \u03b80= 1. Since {\u03b8s} is decreasing, then \u03b8s \u2264 1\u2212 \u03b4(b)\u03b1\u22121 is satisfied. The detailed procedure is shown in Algorithm 2, which has many slight differences in the initialization and output of each epoch from Algorithm 1. In addition, the key difference between them is the update rule for the momentum weight \u03b8s. That is, \u03b8s in Algorithm 1 can be set to a constant, while that in Algorithm 2 is adaptively adjusted as in (12).\nConvergence Analysis This section provides the convergence analysis of our ASVRG-ADMM algorithms (i.e. Algorithms 1 and 2) for strongly convex and general convex problems, respectively. Following (Zheng and Kwok 2016), we first introduce the following function P (x, y) := f(x)\u2212f(x\u2217)\u2212\u2207f(x\u2217)T(x\u2212 x\u2217)+h(y)\u2212h(y\u2217)\u2212h\u2032(y\u2217)T(y\u2212 y\u2217) as a convergence criterion, where h\u2032(y) denotes the (sub)gradient of h(\u00b7) at y. Indeed, P (x, y)\u2265 0 for all x, y \u2208 Rd. In the following, we give the intermediate key results for our analysis.\nAlgorithm 2 ASVRG-ADMM for general convex case Input: m, \u03b7, \u03b2 > 0, 1 \u2264 b \u2264 n. Initialize: x\u03030 = z\u03030, y\u03030, \u03bb\u03030, \u03b80 = 1\u2212 L\u03b7\u03b4(b)1\u2212L\u03b7 .\n1: for s = 1, 2, . . . , T do 2: xs0=(1\u2212\u03b8s\u22121)x\u0303s\u22121+\u03b8s\u22121z\u0303s\u22121, ys0= y\u0303s\u22121, \u03bbs0= \u03bb\u0303s\u22121; 3: p\u0303 = \u2207f(x\u0303s\u22121), zs0= z\u0303s\u22121; 4: for k = 1, 2, . . . ,m do 5: Choose Ik\u2286 [n]of size b, uniformly at random; 6: ysk=argminy h(y)+ \u03b2 2 \u2016Az s k\u22121+By\u2212 c+\u03bbsk\u22121\u20162;\n7: zsk=z s k\u22121\u2212\n\u03b7(\u2207\u0303fIk(x s k\u22121)+\u03b2A T(Azsk\u22121+By s k\u2212c+\u03bb s k\u22121))\n\u03b3\u03b8s\u22121 ;\n8: xsk=(1\u2212 \u03b8s\u22121)x\u0303s\u22121 + \u03b8s\u22121zsk; 9: \u03bbsk=\u03bb s k\u22121 +Az s k +By s k \u2212 c;\n10: end for 11: x\u0303s= 1m \u2211m k=1x s k, y\u0303 s=(1\u2212\u03b8s\u22121)y\u0303s\u22121+ \u03b8s\u22121m \u2211m k=1y s k,\n12: \u03bb\u0303s=\u03bbsm, z\u0303 s=zsm, \u03b8s=\n\u221a \u03b84s\u22121+4\u03b8 2 s\u22121\u2212\u03b8 2 s\u22121\n2 ; 13: end for Output: x\u0303T , y\u0303T .\nLemma 1.\nE[\u2016\u2207\u0303fIk(xsk\u22121)\u2212\u2207f(xsk\u22121)\u20162] \u22642L\u03b4(b) [ f(x\u0303s\u22121)\u2212f(xsk\u22121)+(xsk\u22121\u2212 x\u0303s\u22121)T\u2207f(xsk\u22121) ] ,\nwhere \u03b4(b)= n\u2212bb(n\u22121)\u22641 and 1 \u2264 b \u2264 n. Lemma 2. Using the same notation as in Lemma 1, let (x\u2217, y\u2217, \u03bb\u2217) denote an optimal solution of problem (2), and {(zsk, xsk, ysk, \u03bbsk, x\u0303s, y\u0303s)} be the sequence generated by Algorithm 1 or 2 with \u03b8s \u2264 1\u2212 \u03b4(b)\u03b1\u22121, where \u03b1= 1 L\u03b7 . Then the following holds for all k,\nE [ P (x\u0303s, y\u0303s)\u2212 \u03b8s\u22121\nm m\u2211 k=1 ( (x\u2217\u2212zsk)TAT\u03d5sk+(y\u2217\u2212ysk)TBT\u03d5sk\n)]\n\u2264E\n[ P (x\u0303s\u22121, y\u0303s\u22121)\n1/(1\u2212\u03b8s\u22121) + \u03b82s\u22121\n( \u2016x\u2217\u2212 zs0\u20162G\u2212\u2016x\u2217\u2212 zsm\u20162G ) 2m\u03b7 ]\n+ \u03b2\u03b8s\u22121 2m E\n[ \u2016Azs0\u2212Ax\u2217\u20162\u2212\u2016Azsm\u2212Ax\u2217\u20162+ m\u2211 k=1 \u2016\u03bbsk\u2212\u03bbsk\u22121\u20162 ]\nwhere \u03d5sk = \u03b2(\u03bb s k \u2212 \u03bb\u2217).\nThe detailed proofs of Lemmas 3 and 4 are provided in the Supplementary Material.\nLinear Convergence Our first main result is the following theorem which gives the convergence rate of Algorithm 1. Theorem 1. Using the same notation as in Lemma 4 with given \u03b8 \u2264 1\u2212 \u03b4(b)\u03b1\u22121 , and suppose f(\u00b7) is \u00b5-strongly convex and Lf -smooth, and m is sufficiently large so that\n\u03c1= \u03b8\u2016\u03b8G+\u03b7\u03b2ATA\u20162\n\u03b7m\u00b5\ufe38 \ufe37\ufe37 \ufe38 1\n+1\u2212\u03b8\ufe38\ufe37\ufe37\ufe38 2 + Lf\u03b8\n\u03b2m\u03c3min(AAT )\ufe38 \ufe37\ufe37 \ufe38 3 <1, (13)\nwhere \u03c3min(AAT ) is the smallest eigenvalue of the positive semi-definite matrix AAT , and G is defined in (9). Then\nE [ P (x\u0303T, y\u0303T ) ] \u2264 \u03c1TP (x\u03030, y\u03030).\nThe proof of Theorem 1 is provided in the Supplementary Material. From Theorem 1, one can see that ASVRGADMM achieves linear convergence, which is consistent with that of SVRG-ADMM, while SCAS-ADMM has only an O(1/T ) convergence rate. Remark 1. Theorem 1 shows that our result improves slightly upon the rate \u03c1 in (Zheng and Kwok 2016) with the same \u03b7 and \u03b2. Specifically, as shown in (13), \u03c1 consists of three components, corresponding to those of Theorem 1 in (Zheng and Kwok 2016). In Algorithm 1, recall that here \u03b8 \u2264 1 and G is defined in (9). Thus, both the first and third terms in (13) are slightly smaller than those of Theorem 1 in (Zheng and Kwok 2016). In addition, one can set \u03b7=1/8L (i.e. \u03b1=8) and \u03b8=1\u2212\u03b4(b)/(\u03b1\u22121)=1\u2212\u03b4(b)/7. Thus, the second term in (13) equals to \u03b4(b)/7, while that of SVRG-ADMM is approximately equal to 4L\u03b7\u03b4(b)/(1\u2212 4L\u03b7\u03b4(b))\u2265 \u03b4(b)/2. In summary, the convergence bound of SVRG-ADMM can be slightly improved by ASVRG-ADMM.\nSelecting Scheme of \u03b8 The rate \u03c1 in (13) of Theorem 1 can be expressed as the function with respect to the parameters \u03b8 and \u03b2 with given m, \u03b7, Lf , L,A, \u00b5. Similar to (Nishihara et al. 2015; Zheng and Kwok 2016), one can obtain the optimal parameter \u03b2\u2217 = \u221a Lf\u00b5/(\u03c3min(AAT )\u2016ATA\u20162), which produces a smaller rate \u03c1. In addition, as shown in (13), all the three terms are with respect to the weight \u03b8. Therefore, we give the following selecting scheme for \u03b8. Proposition 1. Given \u03baf = Lf/\u00b5, \u03b2\u2217, \u03ba = L/\u00b5, b, A, and let \u03c9= \u2016ATA\u20162/\u03c3min(AAT ), we set m> 2\u03ba+2 \u221a \u03baf\u03c9 and \u03b7=1/(L\u03b1), where \u03b1= m\u22122 \u221a \u03baf\u03c9\n2\u03ba +\u03b4(b)+1. Then the optimal \u03b8\u2217 of Algorithm 1 is given by\n\u03b8\u2217 = m\u2212 2\u221a\u03baf\u03c9\nm\u2212 2\u221a\u03baf\u03c9 + 2\u03ba(\u03b4(b) + 1) .\nThe proof of Proposition 1 is provided in the Supplementary Material.\nConvergence Rate of O(1/T 2) We first assume that z \u2208 Z , where Z is a convex compact set with diameter DZ = supz1,z2\u2208Z \u2016z1\u2212z2\u2016, and the dual variable \u03bb is also bounded with D\u03bb = sup\u03bb1,\u03bb2 \u2016\u03bb1\u2212\u03bb2\u2016. For Algorithm 2, we give the following result. Theorem 2. Using the same notation as in Lemma 4 with \u03b80=1\u2212 \u03b4(b)\u03b1\u22121 , then we have\nE [ P (x\u0303T, y\u0303T )+ \u03b3\u2016Ax\u0303T+By\u0303T\u2212 c\u2016 ] \u2264 4(\u03b1\u22121)\u03b4(b) ( P (x\u03030, y\u03030) + \u03b3\u2016Ax\u03030+By\u03030\u2212 c\u2016 ) (\u03b1\u2212 1\u2212 \u03b4(b))2(T+1)2\n+ 2L\u03b1\u2016x\u2217\u2212 x\u03030\u20162G m(T + 1)2\n+ 2\u03b2 ( \u2016ATA\u20162D2Z + 4D2\u03bb ) m(T+1) . (14)\nThe proof of Theorem 2 is provided in the Supplementary Material. Theorem 2 shows that the convergence bound consists of the three components, which converge asO(1/T 2),O(1/mT 2) andO(1/mT ), respectively, while the three components of SVRG-ADMM converge as O(1/T ), O(1/mT ) and O(1/mT ). Clearly, ASVRGADMM achieves the convergence rate of O(1/T 2) as opposed to O(1/T ) of SVRG-ADMM and SAG-ADMM (m T ). All the components in the convergence bound of SCAS-ADMM converge as O(1/T ). Thus, it is clear from this comparison that ASVRG-ADMM is a factor of T faster than SAG-ADMM, SVRG-ADMM and SCAS-ADMM.\nConnections to Related Work Our algorithms and convergence results can be extended to the following settings. When the mini-batch size b= n and m=1, then \u03b4(n)=0, that is, the first term of (14) vanishes, and ASVRG-ADMM degenerates to the batch version. Its convergence rate becomes O(D2x\u2217/(T+1) 2 +D2Z/(T+1)+ D2\u03bb/(T+1)) (which is consistent with the optimal result for accelerated deterministic ADMM methods (Goldstein et al. 2014; Lu et al. 2016)), where Dx\u2217 = \u2016x\u2217\u2212 x\u03030\u2016G. Many empirical risk minimization problems can be viewed as the special case of (1) when A = I . Thus, our method can be extended to solve them, and has anO(1/T 2+1/(mT 2)) rate, which is consistent with the best known result as in (AllenZhu 2016; Hien et al. 2016).\nExperiments In this section, we use our ASVRG-ADMM method to solve the general convex graph-guided fuzed Lasso, strongly convex graph-guided logistic regression and graph-guided SVM problems. We compare ASVRG-ADMM with the following state-of-the-art methods: STOC-ADMM (Ouyang et al.\n2013), OPG-ADMM (Suzuki 2013), SAG-ADMM (Zhong and Kwok 2014b), and SCAS-ADMM (Zhao, Li, and Zhou 2015) and SVRG-ADMM (Zheng and Kwok 2016). All methods were implemented in MATLAB, and the experiments were performed on a PC with an Intel i5-2400 CPU and 16GB RAM.\nGraph-Guided Fused Lasso We first evaluate the empirical performance of the proposed method for solving the graph-guided fuzed Lasso problem:\nmin x\n1\nn n\u2211 i=1 `i(x) + \u03bb1\u2016Ax\u20161, (15)\nwhere `i is the logistic loss function on the feature-label pair (ai, bi), i.e., log(1+exp(\u2212biaTi x)), and \u03bb1\u2265 0 is the regularization parameter. Here, we set A = [G; I] as in (Ouyang et al. 2013; Zhong and Kwok 2014b; Azadi and Sra 2014; Zheng and Kwok 2016), where G is the sparsity pattern of the graph obtained by sparse inverse covariance selection (Banerjee, Ghaoui, and d\u2019Aspremont 2008). We used four publicly available data sets1 in our experiments, as listed in Table 2. Note that except STOC-ADMM, all the other algorithms adopted the linearization of the penalty term \u03b22 \u2016Ax\u2212y+z\u2016 2 to avoid the inversion of 1\u03b7kId1+\u03b2A TA at each iteration, which can be computationally expensive for large matrices. The parameters of ASVRG-ADMM are set as follows: m=2n/b and \u03b3=1 as in (Zhong and Kwok 2014b; Zheng and Kwok 2016), as well as \u03b7 and \u03b2.\nFigure 1 shows the training error (i.e. the training objective value minus the minimum) and testing loss of all the algorithms for the general convex problem on the four data\n1http://www.csie.ntu.edu.tw/\u02dccjlin/ libsvmtools/datasets/\nsets. SAG-ADMM could not generate experimental results on the HIGGS data set because it ran out of memory. These figures clearly indicate that the variance reduced stochastic ADMM algorithms (including SAG-ADMM, SCASADMM, SVRG-ADMM and ASVRG-ADMM) converge much faster than those without variance reduction techniques, e.g. STOC-ADMM and OPG-ADMM. Notably, ASVRG-ADMM consistently outperforms all other algorithms in terms of the convergence rate under all settings, which empirically verifies our theoretical result that ASVRG-ADMM has a faster convergence rate of O(1/T 2), as opposed to the best known rate of O(1/T ).\nGraph-Guided Logistic Regression We further discuss the performance of ASVRG-ADMM for solving the strongly convex graph-guided logistic regression problem (Ouyang et al. 2013; Zhong and Kwok 2014a):\nmin x\n1\nn n\u2211 i=1 ( `i(x) + \u03bb2 2 \u2016x\u201622 ) + \u03bb1\u2016Ax\u20161. (16)\nDue to limited space and similar experimental phenomena on the four data sets, we only report the experimental results on the a9a and w8a data sets in Figure 2, from which we observe that SVRG-ADMM and ASVRG-ADMM achieve comparable performance, and they significantly outperform the other methods in terms of the convergence rate, which is\nconsistent with their linear (geometric) convergence guarantees. Moreover, ASVRG-ADMM converges slightly faster than SVRG-ADMM, which shows the effectiveness of the momentum trick to accelerate variance reduced stochastic ADMM, as we expected.\nGraph-Guided SVM Finally, we evaluate the performance of ASVRG-ADMM for solving the graph-guided SVM problem,\nmin x\n1\nn n\u2211 i=1 ( [1\u2212 biaTi x]+ + \u03bb2 2 \u2016x\u201622 ) + \u03bb1\u2016Ax\u20161, (17)\nwhere [x]+ = max(0, x) is the non-smooth hinge loss. To effectively solve problem (17), we used the smooth Huberized hinge loss in (Rosset and Zhu 2007) to approximate the hinge loss. For the 20newsgroups dataset2, we randomly divide it into 80% training set and 20% test set. Following (Ouyang et al. 2013), we set \u03bb1=\u03bb2=10\u22125, and use the one-vs-rest scheme for the multi-class classification.\nFigure 3 shows the average prediction accuracies and standard deviations of testing accuracies over 10 different runs. Since STOC-ADMM, OPG-ADMM, SAG-ADMM and SCAS-ADMM consistently perform worse than SVRGADMM and ASVRG-ADMM in all settings, we only report the results of STOC-ADMM. We observe that SVRGADMM and ASVRG-ADMM consistently outperform the classical SVM and STOC-ADMM. Moreover, ASVRGADMM performs much better than the other methods in all settings, which again verifies the effectiveness of our ASVRG-ADMM method.\nConclusions In this paper, we proposed an accelerated stochastic variance reduced ADMM (ASVRG-ADMM) method, in which we combined both the momentum acceleration trick for batch optimization and the variance reduction technique. We designed two different momentum term update rules for strongly convex and general convex cases, respectively. Moreover, we also theoretically analyzed the convergence properties of ASVRG-ADMM, from which it is clear that ASVRG-ADMM achieves linear convergence andO(1/T 2) rates for both cases. Especially, ASVRG-ADMM is at least a factor of T faster than existing stochastic ADMM methods for general convex problems.\n2http://www.cs.nyu.edu/\u02dcroweis/data.html\nAcknowledgements We thank the reviewers for their valuable comments. The authors are supported by the Hong Kong GRF 2150851 and 2150895, and Grants 3132964 and 3132821 funded by the Research Committee of CUHK.\nReferences [Allen-Zhu 2016] Allen-Zhu, Z. 2016. Katyusha: Accelerated variance reduction for faster sgd. arXiv:1603.05953v4.\n[Azadi and Sra 2014] Azadi, S., and Sra, S. 2014. Towards an optimal stochastic alternating direction method of multipliers. In Proc. 31st Int. Conf. Mach. Learn. (ICML), 620\u2013 628.\n[Baldassarre and Pontil 2013] Baldassarre, L., and Pontil, M. 2013. Advanced topics in machine learning part II: 5. Proximal methods. University Lecture.\n[Banerjee, Ghaoui, and d\u2019Aspremont 2008] Banerjee, O.; Ghaoui, L. E.; and d\u2019Aspremont, A. 2008. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. J. Mach. Learn. Res. 9:485\u2013516.\n[Beck and Teboulle 2009] Beck, A., and Teboulle, M. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sci. 2(1):183\u2013202.\n[Boyd et al. 2011] Boyd, S.; Parikh, N.; Chu, E.; Peleato, B.; and Eckstein, J. 2011. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn. 3(1):1\u2013122.\n[Bubeck 2015] Bubeck, S. 2015. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn. 8:231\u2013358.\n[Deng and Yin 2016] Deng, W., and Yin, W. 2016. On the global and linear convergence of the generalized alternating direction method of multipliers. J. Sci. Comput. 66:889\u2013916.\n[Goldstein et al. 2014] Goldstein, T.; ODonoghue, B.; Setzer, S.; and Baraniuk, R. 2014. Fast alternating direction optimization methods. SIAM J. Imaging Sciences 7(3):1588\u2013 1623.\n[Hien et al. 2016] Hien, L. T. K.; Lu, C.; Xu, H.; and Feng, J. 2016. Accelerated stochastic mirror descent algorithms for composite non-strongly convex optimization. arXiv:1605.06892v2.\n[Johnson and Zhang 2013] Johnson, R., and Zhang, T. 2013. Accelerating stochastic gradient descent using predictive variance reduction. In Proc. Adv. Neural Inf. Process. Syst. (NIPS), 315\u2013323.\n[Kim, Sohn, and Xing 2009] Kim, S.; Sohn, K. A.; and Xing, E. P. 2009. A multivariate regression approach to association analysis of a quantitative trait network. Bioinformatics 25:i204\u2013i212.\n[Koneeny et al. 2016] Koneeny, J.; Liu, J.; Richtarik, P.; ; and Takae, M. 2016. Mini-batch semi-stochastic gradient descent in the proximal setting. IEEE J. Sel. Top. Sign. Proces. 10(2):242\u2013255.\n[Lan 2012] Lan, G. 2012. An optimal method for stochastic composite optimization. Math. Program. 133:365\u2013397.\n[Lu et al. 2016] Lu, C.; Li, H.; Lin, Z.; and Yan, S. 2016. Fast proximal linearized alternating direction method of multiplier with parallel splitting. In Proc. 30th AAAI Conf. Artif. Intell., 739\u2013745.\n[Nesterov 1983] Nesterov, Y. 1983. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady 27(2):372\u2013376.\n[Nesterov 2004] Nesterov, Y. 2004. Introductory Lectures on Convex Optimization: A Basic Course. Boston: Kluwer Academic Publ.\n[Nie et al. 2014] Nie, F.; Huang, Y.; XiaoqianWang; and Huang, H. 2014. Linear time solver for primal svm. In Proc. 31st Int. Conf. Mach. Learn. (ICML), 505\u2013513.\n[Nishihara et al. 2015] Nishihara, R.; Lessard, L.; Recht, B.; Packard, A.; and Jordan, M. I. 2015. A general analysis of the convergence of ADMM. In Proc. 32nd Int. Conf. Mach. Learn. (ICML), 343\u2013352.\n[Ouyang et al. 2013] Ouyang, H.; He, N.; Tran, L. Q.; and Gray, A. 2013. Stochastic alternating direction method of multipliers. In Proc. 30th Int. Conf. Mach. Learn. (ICML), 80\u201388.\n[Rosset and Zhu 2007] Rosset, S., and Zhu, J. 2007. Piecewise linear regularized solution paths. Ann. Statist. 35(3):1012\u20131030.\n[Roux, Schmidt, and Bach 2012] Roux, N. L.; Schmidt, M.; and Bach, F. 2012. A stochastic gradient method with an exponential convergence rate for finite training sets. In Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2672\u20132680.\n[Shalev-Shwartz and Zhang 2013] Shalev-Shwartz, S., and Zhang, T. 2013. Stochastic dual coordinate ascent methods for regularized loss minimization. J. Mach. Learn. Res. 14:567\u2013599.\n[Suzuki 2013] Suzuki, T. 2013. Dual averaging and proximal gradient descent for online alternating direction multiplier method. In Proc. 30th Int. Conf. Mach. Learn. (ICML), 392\u2013 400.\n[Suzuki 2014] Suzuki, T. 2014. Stochastic dual coordinate ascent with alternating direction method of multipliers. In Proc. 31st Int. Conf. Mach. Learn. (ICML), 736\u2013744.\n[Tibshirani and Taylor 2011] Tibshirani, R. J., and Taylor, J. 2011. The solution path of the generalized lasso. Annals of Statistics 39(3):1335\u20131371.\n[Tseng 2010] Tseng, P. 2010. Approximation accuracy, gradient methods, and error bound for structured convex optimization. Math. Program. 125:263\u2013295.\n[Wang and Banerjee 2012] Wang, H., and Banerjee, A. 2012. Online alternating direction method. In Proc. 29th Int. Conf. Mach. Learn. (ICML), 1119\u20131126.\n[Zhang, Burger, and Osher 2011] Zhang, X.; Burger, M.; and Osher, S. 2011. A unified primal-dual algorithm framework based on Bregman iteration. J. Sci. Comput. 46(1):20\u201346.\n[Zhao, Li, and Zhou 2015] Zhao, S.-Y.; Li, W.-J.; and Zhou, Z.-H. 2015. Scalable stochastic alternating direction method of multipliers. arXiv:1502.03529v3.\n[Zheng and Kwok 2016] Zheng, S., and Kwok, J. T. 2016. Fast-and-light stochastic admm. In Proc. 25th Int. Joint Conf. Artif. Intell.,, 2407\u20132613.\n[Zhong and Kwok 2014a] Zhong, L. W., and Kwok, J. T. 2014a. Accelerated stochastic gradient method for composite regularization. In Proc. 17th Int. Conf. Artif. Intell. Statist., 1086\u20131094.\n[Zhong and Kwok 2014b] Zhong, L. W., and Kwok, J. T. 2014b. Fast stochastic alternating direction method of multipliers. In Proc. 31st Int. Conf. Mach. Learn. (ICML), 46\u201354.\nSupplementary Materials for \u201cAccelerated Variance Reduced Stochastic ADMM\u201d In this supplementary material, we give the detailed proofs for two important lemmas (i.e., Lemmas 1 and 2), two key theorems (i.e., Theorems 1 and 2) and a proposition (i.e., Proposition 1).\nProof of Lemma 1: Our convergence analysis will use a bound on the variance term E[\u2016\u2207\u0303fIk(xsk\u22121)\u2212\u2207f(xsk\u22121)\u20162], as shown in Lemma 1. Before giving the proof of Lemma 1, we first give the following lemma. Lemma 3. Since each fj(x) is convex, Lj-smooth (j=1, . . . , n), then the following holds\n\u2016\u2207fik(xsk\u22121)\u2212\u2207fik(x\u0303s\u22121)\u20162 \u22642L [ fik(x\u0303 s\u22121)\u2212fik(xsk\u22121)+\u3008\u2207fik(xsk\u22121), xsk\u22121\u2212x\u0303s\u22121\u3009 ] ,\n(18)\nwhere ik \u2208 [n], and L := maxj Lj .\nProof. This result follows immediately from Theorem 2.1.5 in (Nesterov 2004).\nProof of Lemma 1:\nProof. \u2207\u0303fik(xsk\u22121)=\u2207fik(xsk\u22121)\u2212\u2207fik(x\u0303s\u22121)+\u2207f(x\u0303s\u22121). Taking expectation over the random choice of ik, we have E [\u2225\u2225\u2225\u2207\u0303fik(xsk\u22121)\u2212\u2207f(xsk\u22121)\u2225\u2225\u22252]\n=E [\u2225\u2225(\u2207f(x\u0303s\u22121)\u2212\u2207f(xsk\u22121))\u2212(\u2207fik(x\u0303s\u22121)\u2212\u2207fik(xsk\u22121))\u2225\u22252]\n\u2264E [\u2225\u2225\u2207fik(xsk\u22121)\u2212\u2207fik(x\u0303s\u22121)\u2225\u22252]\n\u2264 2LE [ fik(x\u0303 s\u22121)\u2212 fik(xsk\u22121) + \u3008\u2207fik(xsk\u22121), xsk\u22121\u2212 x\u0303s\u22121\u3009 ]\n=2L ( f(x\u0303s\u22121)\u2212 f(xsk\u22121) + \u3008\u2207f(xsk\u22121), xsk\u22121 \u2212 x\u0303s\u22121\u3009 ) ,\n(19)\nwhere the first inequality follows from the fact that E[\u2016E[x] \u2212 x\u20162] = E[\u2016x\u20162] \u2212 \u2016E[x]\u20162, and the second inequality is due to Lemma 3 given above. Note that the similar result for (19) was also proved in (Allen-Zhu 2016) (see Lemma 3.4 in (Allen-Zhu 2016)). Next, we extend the result to the mini-batch setting.\nLet b be the size of mini-batch Ik. We prove the result of Lemma 1 for the mini-batch case, i.e. b \u2265 2. E [\u2225\u2225\u2225\u2207\u0303fIk(xsk\u22121)\u2212\u2207f(xsk\u22121)\u2225\u2225\u22252]\n=E \u2225\u2225\u2225\u2225\u22251b\u2211 i\u2208Ik ( \u2207fi(xsk\u22121)\u2212\u2207fi(x\u0303s\u22121) ) +\u2207f(x\u0303s\u22121)\u2212\u2207f(xsk\u22121) \u2225\u2225\u2225\u2225\u2225 2 \n= n\u2212b b(n\u22121)\nE [\u2225\u2225\u2207fi(xsk\u22121)\u2212\u2207f(xsk\u22121)\u2212\u2207fi(x\u0303s\u22121)+\u2207f(x\u0303s\u22121)\u2225\u22252]\n\u22642L(n\u2212b) b(n\u22121)\n( f(x\u0303s\u22121)\u2212f(xsk\u22121)+ \u2329 \u2207f(xsk\u22121), xsk\u22121\u2212x\u0303s\u22121 \u232a) ,\nwhere the second equality follows from Lemma 4 in (Koneeny et al. 2016), and the inequality holds due to the result in (19).\nProof of Lemma 2: Before proving the key Lemma 2, we first give the following a property (Baldassarre and Pontil 2013; Lan 2012), which is useful for the convergence analysis of ASVRG-ADMM.\nProperty 1. Given any w1, w2, w3, w4 \u2208 Rd, then we have\n\u3008w1 \u2212 w2, w1 \u2212 w3\u3009 = 1\n2\n( \u2016w1 \u2212 w2\u20162 + \u2016w1 \u2212 w3\u20162 \u2212 \u2016w2 \u2212 w3\u20162 ) ,\n\u3008w1 \u2212 w2, w3 \u2212 w4\u3009 = 1\n2\n( \u2016w1\u2212w4\u20162\u2212\u2016w1\u2212w3\u20162+\u2016w2\u2212w3\u20162\u2212\u2016w2\u2212w4\u20162 ) .\nIn order to prove Lemma 2, we first give and prove the following two key lemmas.\nLemma 4. Since \u03b7 = 1L\u03b1 , 1\u2212 \u03b8s\u22121 \u2265 \u03b4(b) \u03b1\u22121 , and \u03b4(b) = n\u2212b b(n\u22121) , then we have\nE [ f(x\u0303s)\u2212 f(x\u2217)\u2212 \u3008\u2207f(x\u2217), x\u0303s \u2212 x\u2217\u3009 \u2212 \u03b8s\u22121\nm m\u2211 k=1 \u2329 AT\u03d5sk, x \u2217 \u2212 zsk \u232a]\n\u2264E [ (1\u2212\u03b8s\u22121) ( f(x\u0303s\u22121)\u2212 f(x\u2217)\u2212 \u3008\u2207f(x\u2217), x\u0303s\u22121 \u2212 x\u2217\u3009 ) + L\u03b1\u03b82s\u22121 2m ( \u2016x\u2217 \u2212 zs0\u20162 \u2212 \u2016x\u2217 \u2212 zsm\u20162 )] .\nProof. Let gk = 1b \u2211 ik\u2208Ik ( \u2207fik(xsk\u22121)\u2212\u2207fik(x\u0303s\u22121) ) + \u2207f(x\u0303s\u22121). Since the function f is convex, differentiable with an Lf -Lipschitz-continuous gradient, where Lf \u2264 L = maxj=1,...,n Lj , then\nf(xsk) \u2264 f(xsk\u22121) + \u2329 \u2207f(xsk\u22121), xsk \u2212 xsk\u22121 \u232a + L\u03b1\n2 \u2225\u2225xsk \u2212 xsk\u22121\u2225\u22252 \u2212 L(\u03b1\u22121)2 \u2225\u2225xsk \u2212 xsk\u22121\u2225\u22252 = f(xsk\u22121) + \u2329 gk, x s k \u2212 xsk\u22121 \u232a + L\u03b1\n2 \u2016xsk \u2212 xsk\u22121\u20162\n+ \u2329 \u2207f(xsk\u22121)\u2212 gk, xsk \u2212 xsk\u22121 \u232a \u2212 L(\u03b1\u22121)\n2 \u2016xsk \u2212 xsk\u22121\u20162.\n(20)\nUsing Lemma 1, then we get E [\u2329 \u2207f(xsk\u22121)\u2212 gk, xsk \u2212 xsk\u22121 \u232a \u2212 L(\u03b1\u22121)\n2 \u2016xsk \u2212 xsk\u22121\u20162 ] \u2264E [ 1\n2L(\u03b1\u22121) \u2016\u2207f(xsk\u22121)\u2212 gk\u20162 + L(\u03b1\u22121) 2 \u2016xsk\u2212xsk\u22121\u20162 \u2212 L(\u03b1\u22121) 2 \u2016xsk\u2212xsk\u22121\u20162 ] \u2264 \u03b4(b) \u03b1\u22121 ( f(x\u0303s\u22121)\u2212 f(xsk\u22121) + \u2329 \u2207f(xsk\u22121), xsk\u22121 \u2212 x\u0303s\u22121 \u232a) ,\n(21)\nwhere the first inequality holds due to the Young\u2019s inequality, and the second inequality follows from Lemma 1. Taking the expectation over the random choice of Ik and substituting the inequality (21) into the inequality (20), we have\nE[f(xsk)] \u2264 f(xsk\u22121) + E [\u2329 gk, x s k \u2212 xsk\u22121 \u232a + L\u03b1\n2 \u2016xsk \u2212 xsk\u22121\u20162 ] + \u03b4(b)\n\u03b1\u22121 [ f(x\u0303s\u22121)\u2212 f(xsk\u22121) + \u2329 \u2207f(xsk\u22121), xsk\u22121 \u2212 x\u0303s\u22121 \u232a] = f(xsk\u22121) + E [\u2329 gk, x s k \u2212 v\u2217 + v\u2217 \u2212 xsk\u22121 \u232a + L\u03b1\n2 \u2016xsk \u2212 xsk\u22121\u20162 ] + \u03b4(b)\n\u03b1\u22121 [ f(x\u0303s\u22121)\u2212 f(xsk\u22121) + \u2329 \u2207f(xsk\u22121), xsk\u22121 \u2212 x\u0303s\u22121 \u232a] = f(xsk\u22121) + E [ \u3008gk, xsk \u2212 v\u2217\u3009+ L\u03b1\n2 \u2016xsk \u2212 xsk\u22121\u20162\n] + \u03b4(b) \u03b1\u2212 1 ( f(x\u0303s\u22121)\u2212 f(xsk\u22121) ) + \u2329 \u2207f(xsk\u22121), v\u2217\u2212xsk\u22121+ \u03b4(b)\n\u03b1\u22121 (xsk\u22121\u2212x\u0303s\u22121)\n\u232a +E [ \u3008\u22121 b \u2211 ik\u2208Ik \u2207fik(x\u0303s\u22121)+\u2207f(x\u0303s\u22121), v\u2217\u2212xsk\u22121\u3009 ] ,\n= f(xsk\u22121) + E [ \u3008gk, xsk \u2212 v\u2217\u3009+ L\u03b1\n2 \u2016xsk \u2212 xsk\u22121\u20162\n] + \u03b4(b) \u03b1\u22121 ( f(x\u0303s\u22121)\u2212 f(xsk\u22121) ) + \u2329 \u2207f(xsk\u22121), v\u2217 \u2212 xsk\u22121 + \u03b4(b)\n\u03b1\u22121 (xsk\u22121 \u2212 x\u0303s\u22121)\n\u232a ,\n(22)\nwhere v\u2217=(1 \u2212 \u03b8s\u22121)x\u0303s\u22121 + \u03b8s\u22121x\u2217, the second equality holds due to that \u3008gk, v\u2217 \u2212 xsk\u22121\u3009 = \u3008 1b \u2211 ik\u2208Ik\u2207fik(x s k\u22121), v\n\u2217 \u2212 xsk\u22121\u3009 + \u3008\u2212 1b \u2211 ik\u2208Ik\u2207fik(x\u0303 s\u22121) + \u2207f(x\u0303s\u22121), v\u2217 \u2212 xsk\u22121\u3009 and E[ 1b \u2211 ik\u2208Ik\u2207fik(x s k\u22121)] = \u2207f(xsk\u22121), and the last equality\nfollows from the fact that E [ \u3008\u2212 1b \u2211 ik\u2208Ik\u2207fik(x\u0303 s\u22121) +\u2207f(x\u0303s\u22121), v\u2217 \u2212 xsk\u22121\u3009 ] = 0.\nFurthermore, \u2329 \u2207f(xsk\u22121), v\u2217 \u2212 xsk\u22121 + \u03b4(b)\n\u03b1\u22121 (xsk\u22121 \u2212 x\u0303s\u22121) \u232a = \u2329 \u2207f(xsk\u22121), (1\u2212 \u03b8s\u22121)x\u0303s\u22121 + \u03b8s\u22121x\u2217 \u2212 xsk\u22121 + \u03b4(b)\n\u03b1\u22121 (xsk\u22121 \u2212 x\u0303s\u22121) \u232a = \u2329 \u2207f(xsk\u22121), \u03b8s\u22121x\u2217 + (1\u2212 \u03b8s\u22121 \u2212 \u03b4(b)\n\u03b1\u22121 )x\u0303s\u22121 +\n\u03b4(b) \u03b1\u22121 xsk\u22121 \u2212 xsk\u22121 \u232a \u2264 f ( \u03b8s\u22121x \u2217 + (1\u2212 \u03b8s\u22121 \u2212 \u03b4(b)\n\u03b1\u22121 )x\u0303s\u22121 +\n\u03b4(b) \u03b1\u22121 xsk\u22121\n) \u2212 f(xsk\u22121)\n\u2264 \u03b8s\u22121f(x\u2217) + (1\u2212 \u03b8s\u22121 \u2212 \u03b4(b)\n\u03b1\u22121 )f(x\u0303s\u22121) +\n\u03b4(b) \u03b1\u22121 f(xsk\u22121)\u2212 f(xsk\u22121),\n(23)\nwhere the first inequality holds due to the fact that \u3008\u2207f(x), y \u2212 x\u3009 \u2264 f(y) \u2212 f(x), and the last inequality follows from the convexity of the function f and the assumption that 1\u2212 \u03b8s\u22121 \u2212 \u03b4(b)\u03b1\u22121 \u2265 0.\nSubstituting the inequality (23) into the inequality (22), we have E[f(xsk)] \u2264 f(xsk\u22121) + E [ \u3008gk, xsk \u2212 v\u2217\u3009+ L\u03b1\n2 \u2016xsk \u2212 xsk\u22121\u20162\n] + \u03b4(b) \u03b1\u22121 ( f(x\u0303s\u22121)\u2212 f(xsk\u22121) ) + \u03b8s\u22121f(x \u2217) + (1\u2212 \u03b8s\u22121 \u2212 \u03b4(b)\n\u03b1\u22121 )f(x\u0303s\u22121) +\n\u03b4(b) \u03b1\u22121 f(xsk\u22121)\u2212 f(xsk\u22121)\n= \u03b8s\u22121f(x \u2217) + (1\u2212 \u03b8s\u22121)f(x\u0303s\u22121) + E [ \u3008gk, xsk \u2212 v\u2217\u3009+ L\u03b1\n2 \u2016xsk \u2212 xsk\u22121\u20162\n] .\n(24)\nFrom the optimality condition of (9) with respect to zsk and \u03b7 = 1 L\u03b1 , we have\u2329\ngk + \u03b2A T (Azsk +By s k \u2212 c) + \u03b2AT\u03bbsk\u22121 + L\u03b1\u03b8s\u22121G(zsk \u2212 zsk\u22121), z \u2212 zsk\n\u232a \u2265 0, for any z \u2208 Z,\nwhere Z is a convex compact set. Since xsk = \u03b8s\u22121zsk + (1 \u2212 \u03b8s\u22121)x\u0303s\u22121 and v\u2217 = \u03b8s\u22121x\u2217 + (1 \u2212 \u03b8s\u22121)x\u0303s\u22121, and the above inequality with z = x\u2217, we obtain\n\u3008gk, xsk \u2212 v\u2217\u3009 = \u03b8s\u22121 \u3008gk, zsk \u2212 x\u2217\u3009 \u2264\u03b2\u03b8s\u22121 \u2329 AT (Azsk +By s k \u2212 b) +AT\u03bbsk\u22121, x\u2217 \u2212 zsk \u232a + L\u03b1\u03b82s\u22121 \u2329 G(zsk \u2212 zsk\u22121), x\u2217 \u2212 zsk \u232a \u2264\u03b2\u03b8s\u22121 \u2329 AT (Azsk +By s k \u2212 b) +AT\u03bbsk\u22121, x\u2217 \u2212 zsk \u232a + L\u03b1\u03b82s\u22121\n2\n( \u2016x\u2217 \u2212 zsk\u22121\u20162G \u2212 \u2016x\u2217 \u2212 zsk\u20162G \u2212 \u2016zsk\u22121 \u2212 zsk\u20162G ) \u2264\u03b2\u03b8s\u22121 \u2329 AT\u03bbsk, x \u2217 \u2212 zsk \u232a + L\u03b1\u03b82s\u22121\n2\n( \u2016x\u2217 \u2212 zsk\u22121\u20162G \u2212 \u2016x\u2217 \u2212 zsk\u20162G \u2212 \u2016zsk\u22121 \u2212 zsk\u20162G ) ,\n(25)\nwhere the second inequity follows from Property 1. Using the optimality condition \u2207f(x\u2217) + \u03b2AT\u03bb\u2217 = 0 of problem (2) and let \u03d5sk = \u03b2 (\u03bb s k \u2212 \u03bb\u2217), then\n\u03b8s\u22121 \u2329 \u03b2AT\u03bbsk, x \u2217 \u2212 zsk \u232a\n= \u03b8s\u22121 \u3008\u2207f(x\u2217), zsk \u2212 x\u2217\u3009+ \u03b8s\u22121 \u2329 \u03b2AT\u03bb\u2217, zsk \u2212 x\u2217 \u232a + \u03b8s\u22121 \u2329 \u03b2AT\u03bbsk, x \u2217 \u2212 zsk \u232a\n= \u03b8s\u22121 \u3008\u2207f(x\u2217), zsk \u2212 x\u2217\u3009+ \u03b8s\u22121\u3008\u03d5sk, x\u2217 \u2212 zsk\u3009 . Taking the expectation of both sides of (25) over the random choice of Ik, we have\nE[\u3008gk, xsk \u2212 v\u2217\u3009] \u2264E [ \u03b8s\u22121 \u3008\u2207f(x\u2217), zsk \u2212 x\u2217\u3009+\u03b8s\u22121 \u2329 AT\u03d5sk, x \u2217\u2212 zsk \u232a + L\u03b1\u03b82s\u22121\n2\n( \u2016x\u2217\u2212 zsk\u22121\u20162G \u2212 \u2016x\u2217\u2212 zsk\u20162G \u2212 \u2016zsk\u22121\u2212 zsk\u20162G )] .\n(26)\nSubstituting the inequality (26) into the inequality (24), and xsk \u2212 xsk\u22121 = (1 \u2212 \u03b8s\u22121)x\u0303s\u22121 + \u03b8s\u22121zsk \u2212 (1 \u2212 \u03b8s\u22121)x\u0303s\u22121 \u2212 \u03b8s\u22121z s k\u22121 = \u03b8s\u22121(z s k \u2212 zsk\u22121), we obtain\nE [ f(xsk)\u2212 f(x\u2217)\u2212 \u03b8s\u22121\u3008\u2207f(x\u2217), zsk \u2212 x\u2217\u3009 \u2212 \u03b8s\u22121\u3008AT\u03d5sk, x\u2217 \u2212 zsk\u3009 ] \u2264 (1\u2212\u03b8s\u22121)[f(x\u0303s\u22121)\u2212 f(x\u2217)] +\nL\u03b1\u03b82s\u22121 2\nE [ \u2016x\u2217 \u2212 zsk\u22121\u20162G \u2212 \u2016x\u2217 \u2212 zsk\u20162G \u2212 \u2016zsk\u22121 \u2212 zsk\u20162G\u2212I ] \u2264 (1\u2212\u03b8s\u22121)[f(x\u0303s\u22121)\u2212 f(x\u2217)] +\nL\u03b1\u03b82s\u22121 2\nE [ \u2016x\u2217 \u2212 zsk\u22121\u20162G \u2212 \u2016x\u2217 \u2212 zsk\u20162G ] ,\nwhere the last inequality holds due to G I in Algorithms 1 and 2, that is, \u2016zsk\u22121 \u2212 zsk\u20162G\u2212I \u2265 0. Using the update rule xsk = (1\u2212 \u03b8s\u22121)x\u0303s\u22121 + \u03b8s\u22121zsk and subtracting (1\u2212\u03b8s\u22121)\u3008\u2207f(x\u2217), x\u0303s\u22121 \u2212 x\u2217\u3009 from both sides, we have\nE [ f(xsk)\u2212 f(x\u2217)\u2212 \u3008\u2207f(x\u2217), xsk \u2212 x\u2217\u3009 \u2212 \u03b8s\u22121\u3008AT\u03d5sk, x\u2217 \u2212 zsk\u3009 ] \u2264E [ (1\u2212\u03b8s\u22121) ( f(x\u0303s\u22121)\u2212 f(x\u2217)\u2212 \u3008\u2207f(x\u2217), x\u0303s\u22121 \u2212 x\u2217\u3009 ) + L\u03b1\u03b82s\u22121\n2\n( \u2016x\u2217 \u2212 zsk\u22121\u20162G \u2212 \u2016x\u2217 \u2212 zsk\u20162G )] .\n(27)\nSince x\u0303s = 1m \u2211m k=1 x s k, and taking the expectation over the random choice of the history of random variables I1, . . . , Im on\nthe inequality (27), summing it over k = 1, . . . ,m at the s-th stage and f( 1m \u2211m k=1 x s k) \u2264 1m \u2211m k=1 f(x s k), we have\nE [ f(x\u0303s)\u2212 f(x\u2217)\u2212 \u3008\u2207f(x\u2217), x\u0303s \u2212 x\u2217\u3009 \u2212 \u03b8s\u22121\nm m\u2211 k=1 \u3008AT\u03d5sk, x\u2217 \u2212 zsk\u3009\n]\n\u2264E [ (1\u2212\u03b8s\u22121)(f(x\u0303s\u22121)\u2212 f(x\u2217)\u2212 \u3008\u2207f(x\u2217), x\u0303s\u22121 \u2212 x\u2217\u3009) +\nL\u03b1\u03b82s\u22121 2m\n( \u2016x\u2217 \u2212 zs0\u20162G \u2212 \u2016x\u2217 \u2212 zsm\u20162G )] .\nThis completes the proof.\nLemma 5. Let y\u0303s = (1\u2212 \u03b8s\u22121)y\u0303s\u22121 + \u03b8s\u22121m \u2211m k=1 y s k, then\nE [ h(y\u0303s)\u2212 h(y\u2217)\u2212 h\u2032(y\u2217)T (y\u0303s \u2212 y\u2217)\u2212 \u03b8s\u22121\nm m\u2211 k=1 \u3008BT\u03d5sk, y\u2217 \u2212 ysk\u3009 ] \u2264(1\u2212 \u03b8s\u22121)E [ h(y\u0303s\u22121)\u2212 h(y\u2217)\u2212 h\u2032(y\u2217)T (y\u0303s\u22121 \u2212 y\u2217)\n] + \u03b2\u03b8s\u22121 2m E [ \u2016Azs0 +By\u2217 \u2212 c\u20162 \u2212 \u2016Azsm +By\u2217 \u2212 c\u20162 + m\u2211 k=1 \u2016\u03bbsk \u2212 \u03bbsk\u22121\u20162 ] .\nProof. Since \u03bbsk = \u03bb s k\u22121 +Az s k +By s k \u2212 c, and using Lemma 3 in (Zheng and Kwok 2016), we obtain\nE [ h(ysk)\u2212 h(y\u2217)\u2212 h\u2032(y\u2217)T (ysk \u2212 y\u2217)\u2212 \u3008BT\u03d5sk, y\u2217 \u2212 ysk\u3009 ] \u2264 \u03b2\n2 E [ \u2016Azsk\u22121 +By\u2217 \u2212 c\u20162 \u2212 \u2016Azsk +By\u2217 \u2212 c\u20162 + \u2016\u03bbsk \u2212 \u03bbsk\u22121\u20162 ] .\nUsing the update rule y\u0303s = (1 \u2212 \u03b8s\u22121)y\u0303s\u22121 + \u03b8s\u22121m \u2211m k=1 y s k, h(y\u0303 s) \u2264 (1 \u2212 \u03b8s\u22121)h(y\u0303s\u22121) + \u03b8s\u22121m \u2211m k=1 h(y s k), and taking expectation over whole history and summing the above inequality over k = 1, . . . ,m, we have\nE [ h(y\u0303s)\u2212 h(y\u2217)\u2212 h\u2032(y\u2217)T (y\u0303s \u2212 y\u2217)\u2212 \u03b8s\u22121\nm m\u2211 k=1 \u3008BT\u03d5sk, y\u2217 \u2212 ysk\u3009\n]\n\u2264\u03b8s\u22121 m E [ m\u2211 k=1 ( h(ysk)\u2212 h(y\u2217)\u2212 h\u2032(y\u2217)T (ysk \u2212 y\u2217)\u2212 \u3008BT\u03d5sk, y\u2217 \u2212 ysk\u3009 )] + (1\u2212 \u03b8s\u22121)E [ h(y\u0303s\u22121)\u2212 h(y\u2217)\u2212 h\u2032(y\u2217)T (y\u0303s\u22121 \u2212 y\u2217)\n] \u2264 \u03b2\u03b8s\u22121\n2m E\n[ \u2016Azs0 +By\u2217 \u2212 c\u20162 \u2212 \u2016Azsm +By\u2217 \u2212 c\u20162 + m\u2211 k=1 \u2016\u03bbsk \u2212 \u03bbsk\u22121\u20162 ]\n+ (1\u2212 \u03b8s\u22121)E [ h(y\u0303s\u22121)\u2212 h(y\u2217)\u2212 h\u2032(y\u2217)T (y\u0303s\u22121 \u2212 y\u2217) ] .\nThis completes the proof.\nProof of Lemma 2:\nProof. Using Lemmas 4 and 5 and the definition of P (x, y), we have\nE [ P (x\u0303s, y\u0303s)\u2212 \u03b8s\u22121\nm m\u2211 k=1 ( \u3008AT\u03d5sk, x\u2217 \u2212 zsk\u3009+ \u3008BT\u03d5sk, y\u2217 \u2212 ysk\u3009\n)]\n\u2264 (1\u2212 \u03b8s\u22121)E [ P (x\u0303s\u22121, y\u0303s\u22121) ] + L\u03b1\u03b82s\u22121 2m E [ \u2016x\u2217 \u2212 zs0\u20162G \u2212 \u2016x\u2217 \u2212 zsm\u20162G ] + \u03b2\u03b8s\u22121 2m E [ \u2016Azs0 +By\u2217 \u2212 c\u20162 \u2212 \u2016Azsm +By\u2217 \u2212 c\u20162 + m\u2211 k=1 \u2016\u03bbsk \u2212 \u03bbsk\u22121\u20162 ]\n=E [ (1\u2212\u03b8s\u22121)P (x\u0303s\u22121, y\u0303s\u22121) + \u03b82s\u22121 ( \u2016x\u2217\u2212 zs0\u20162G\u2212\u2016x\u2217\u2212 zsm\u20162G ) 2m\u03b7 ]\n+ \u03b2\u03b8s\u22121 2m E\n[ \u2016Azs0\u2212Ax\u2217\u20162\u2212\u2016Azsm\u2212Ax\u2217\u20162+ m\u2211 k=1 \u2016\u03bbsk\u2212\u03bbsk\u22121\u20162 ] .\nThis completes the proof.\nProof of Theorem 1: Let (x\u2217, y\u2217) be an optimal solution of the convex problem (2), and \u03bb\u2217 the corresponding Lagrange multiplier that maximizes the dual. Then x\u2217, y\u2217 and \u03bb\u2217 satisfy the following Karush-Kuhn-Tucker (KKT) conditions:\n\u03b2AT\u03bb\u2217 +\u2207f(x\u2217) = 0, \u03b2BT\u03bb\u2217 + h\u2032(y\u2217) = 0, Ax\u2217 +By\u2217 = c.\nBefore giving the proof of Theorem 1, we first present the following lemmas (Zheng and Kwok 2016).\nLemma 6. Let \u03d5k = \u03b2(\u03bbk \u2212 \u03bb\u2217), and \u03bbk = \u03bbk\u22121 +Axk +Byk \u2212 c, then\nE [ \u2212(Axk +Byk \u2212 c)T\u03d5k ] = \u03b2 2 E [ \u2016\u03bbk\u22121 \u2212 \u03bb\u2217\u20162 \u2212 \u2016\u03bbk \u2212 \u03bb\u2217\u20162 \u2212 \u2016\u03bbk \u2212 \u03bbk\u22121\u20162 ] .\nLemma 7. Since the matrix A has full row rank, then\n\u03bb\u2217 = \u2212 1 \u03b2 (AT )\u2020\u2207f(x\u2217).\nLemma 8. Let \u03bb\u0303s\u22121 = \u03bbs0 = \u2212 1\u03b2 (A \u2020)\u2207f(x\u0303s\u22121), and \u03bb\u2217 = \u2212 1\u03b2 (A \u2020)\u2207f(x\u2217), then\n\u2016\u03bb\u0303s\u22121 \u2212 \u03bb\u2217\u20162 \u2264 2Lf \u03b22\u03c3min(AAT )\n( f(x\u0303s\u22121)\u2212 f(x\u2217)\u2212\u2207f(x\u2217)T (x\u0303s\u22121 \u2212 x\u2217) ) .\nProof of Theorem 1:\nProof. Using the update rule \u03bbsk = \u03bb s k\u22121 +Az s k +By s k \u2212 c, then\nm\u2211 k=1 [ \u3008AT\u03d5sk, x\u2217 \u2212 zsk\u3009+ \u3008BT\u03d5sk, y\u2217 \u2212 ysk\u3009+ \u3008Azsk +Bysk \u2212 c, \u03d5sk\u3009 ] = 0,\nwhere \u03d5sk = \u03b2 (\u03bb s k \u2212 \u03bb\u2217). Using Lemma 6, we have\n\u2212 m\u2211 k=1 \u3008Azsk +Bysk \u2212 c, \u03d5sk\u3009 = \u03b2 2 m\u2211 k=1 ( \u2016\u03bbsk\u22121 \u2212 \u03bb\u2217\u20162 \u2212 \u2016\u03bbsk \u2212 \u03bb\u2217\u20162 \u2212 \u2016\u03bbsk\u22121 \u2212 \u03bbsk\u20162 ) = \u03b2\n2\n( \u2016\u03bbs0 \u2212 \u03bb\u2217\u20162 \u2212 \u2016\u03bbsm \u2212 \u03bb\u2217\u20162 \u2212 m\u2211 k=1 \u2016\u03bbsk\u22121 \u2212 \u03bbsk\u20162 ) .\nCombining Lemma 2 and the above results with \u03b8s\u22121 = \u03b8 at all stages, zs0 = x\u0303 s\u22121 and \u03bbs0 = \u03bb\u0303 s\u22121, we have\nE[P (x\u0303s, y\u0303s)]\n\u2264 (1\u2212\u03b8)E [ P (x\u0303s\u22121, y\u0303s\u22121) ] + L\u03b1\u03b82 2m E [ \u2016x\u2217 \u2212 x\u0303s\u22121\u20162G \u2212 \u2016x\u2217 \u2212 zsm\u20162G ] + \u03b2\u03b8 2m E [ \u2016Ax\u0303s\u22121 +By\u2217 \u2212 c\u20162 \u2212 \u2016Azsm +By\u2217 \u2212 c\u20162 + \u2016\u03bb\u0303s\u22121 \u2212 \u03bb\u2217\u20162 \u2212 \u2016\u03bbsm \u2212 \u03bb\u2217\u20162\n] \u2264 (1\u2212\u03b8)E [ P (x\u0303s\u22121, y\u0303s\u22121) ] + L\u03b1\u03b82\n2m E [ \u2016x\u2217 \u2212 x\u0303s\u22121\u20162G ] + \u03b2\u03b8 2m E [ \u2016Ax\u0303s\u22121 \u2212Ax\u2217\u20162 + \u2016\u03bb\u0303s\u22121 \u2212 \u03bb\u2217\u20162 ] =(1\u2212\u03b8)E [ P (x\u0303s\u22121, y\u0303s\u22121) ] + 1 2m E [ \u2016x\u2217 \u2212 x\u0303s\u22121\u20162L\u03b1\u03b82G+\u03b2\u03b8ATA ] + \u03b2\u03b8 2m E [ \u2016\u03bb\u0303s\u22121 \u2212 \u03bb\u2217\u20162\n] \u2264 (1\u2212\u03b8)E [ P (x\u0303s\u22121, y\u0303s\u22121) ] + \u2016L\u03b1\u03b82G+ \u03b2\u03b8ATA\u20162\n2m E [ \u2016x\u2217 \u2212 x\u0303s\u22121\u20162 ] + \u03b2\u03b8 2m E [ \u2016\u03bb\u0303s\u22121 \u2212 \u03bb\u2217\u20162 ] \u2264 (1\u2212\u03b8)E [ P (x\u0303s\u22121, y\u0303s\u22121) ] + \u2016L\u03b1\u03b82G+\u03b2\u03b8ATA\u20162\nm\u00b5\n( f(x\u0303s\u22121)\u2212 f(x\u2217)\u2212\u2207f(x\u2217)T (x\u0303s\u22121\u2212 x\u2217) ) + \u03b2\u03b8 2m E [ \u2016\u03bb\u0303s\u22121\u2212 \u03bb\u2217\u20162 ] ,\nwhere the second inequality holds due to the fact that Ax\u2217 + By\u2217 \u2212 c = 0, and the last inequality follows from the definition of the strongly convex function.\nUsing \u03bb\u2217 = \u2212 1\u03b2 (A \u2020)\u2207f(x\u2217), the update rule \u03bbs0 = \u2212 1\u03b2 (A \u2020)\u2207f(x\u0303s\u22121), and Lemma 8, we have\n\u2016\u03bbs\u22121 \u2212 \u03bb\u2217\u20162 \u2264 2Lf \u03b22\u03c3min(AAT )\n[ f(x\u0303s\u22121)\u2212 f(x\u2217)\u2212\u2207f(x\u2217)T (x\u0303s\u22121 \u2212 x\u2217) ] .\nCombining the above results, h(y\u0303s\u22121)\u2212h(x\u2217)\u2212h\u2032(y\u2217)T (y\u0303s\u22121\u2212 y\u2217) \u2265 0, \u03b7 = 1L\u03b1 and the definition of P (x, y), then we have\nE[P (x\u0303s, y\u0303s)] \u2264 ( 1\u2212 \u03b8 + \u2016L\u03b1\u03b8\n2G+ \u03b2\u03b8ATA\u20162 m\u00b5 + Lf\u03b8 \u03b2m\u03c3min(AAT )\n) E [ P (x\u0303s\u22121, y\u0303s\u22121) ] = ( 1\u2212 \u03b8 + \u2016\u03b8\n2G+ \u03b7\u03b2\u03b8ATA\u20162 \u03b7m\u00b5 + Lf\u03b8 \u03b2m\u03c3min(AAT )\n) E [ P (x\u0303s\u22121, y\u0303s\u22121) ] .\nThis completes the proof.\nProof of Proposition 1: Proof. Since\nG = \u03b3Id1 \u2212 \u03b7\u03b2\u2217ATA\n\u03b8 , \u03b3 = \u03b7\u03b2\u2217\u2016ATA\u20162 \u03b8 + 1, and \u03b2\u2217 \u2261\n\u221a Lf\u00b5\n\u2016ATA\u20162\u03c3min(AAT ) ,\nthen the rate \u03c1 in Theorem 1 is rewritten as follows:\n\u03c1(\u03b8) = \u03b8\u2016\u03b8G+\u03b7\u03b2\u2217ATA\u20162\n\u03b7m\u00b5 + 1\u2212 \u03b8 + Lf\u03b8 \u03b2\u2217m\u03c3min(AAT )\n= \u03b8(\u03b7\u03b2\u2217\u2016ATA\u20162 + \u03b8)/(\u03b7m\u00b5) + 1\u2212 \u03b8 + Lf\u03b8 \u221a \u2016ATA\u20162/\u03c3min(AAT ) m \u221a Lf\u00b5\n= (\u03b82/\u03b7 + \u03b2\u2217\u03b8\u2016ATA\u20162)/(m\u00b5) + 1\u2212 \u03b8 + \u03b8 \u221a Lf/\u00b5 \u221a \u03c9\nm\n= \u03ba\u03b1\u03b82\nm +\n2\u03b8 \u221a \u03baf\u03c9\nm + 1\u2212 \u03b8,\n(28)\nwhere \u03b1 = 1L\u03b7 . Therefore, the rate \u03c1 can be expressed as a simple convex function with respect to \u03b8 by fixing other variable. To minimize the quadratic function \u03c1 with respect to \u03b8, then we have\n\u03b8\u2217 = m\u2212 2\u221a\u03baf\u03c9\n2\u03ba\u03b1 .\nRecall from the body of this paper that \u03b1 = m\u22122 \u221a \u03baf\u03c9\n2\u03ba + \u03b4(b) + 1. Then it is not difficult to verify that\n\u03b8\u2217 = m\u2212 2\u221a\u03baf\u03c9\n2\u03ba\u03b1 = m\u2212 2\u221a\u03baf\u03c9 [(m\u2212 2\u221a\u03baf\u03c9) + 2\u03ba(\u03b4(b) + 1)]\n\u2264 m\u2212 2\u221a\u03baf\u03c9\n[(m\u2212 2\u221a\u03baf\u03c9) + 2\u03ba\u03b4(b)] = 1\u2212 \u03b4(b) [(m\u2212 2\u221a\u03baf\u03c9)/(2\u03ba) + \u03b4(b)]\n= 1\u2212 \u03b4(b) \u03b1\u2212 1 .\nBy the above result, we have\n\u03b8\u2217 = m\u2212 2\u221a\u03baf\u03c9\n2\u03ba\u03b1 = m\u2212 2\u221a\u03baf\u03c9 m\u2212 2\u221a\u03baf\u03c9 + 2\u03ba(\u03b4(b) + 1) . (29)\nIt is not difficult to verify that 0 < \u03c1(\u03b8\u2217) < 1 is satisfied. Thus, \u03b8\u2217 is the optimal solution of (28) with \u03b1 = m\u22122 \u221a \u03baf\u03c9\n2\u03ba +\u03b4(b)+1. This proof is completed.\nProof of Theorem 2: Before giving the proof of Theorem 2, we first present the following lemma (Zheng and Kwok 2016).\nLemma 9. Let \u03d5k = \u03b2(\u03bbk \u2212 \u03bb\u2217) and any \u03d5 = \u03b2\u03bb, and \u03bbk = \u03bbk\u22121 +Axk +Byk \u2212 c, then\nE [ \u2212(Axk +Byk \u2212 c)T (\u03d5k \u2212 \u03d5) ] = \u03b2 2 E [ \u2016\u03bbk\u22121 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbk \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbk \u2212 \u03bbk\u22121\u20162 ] .\nProof of Theorem 2:\nProof. For any \u03d5 = \u03b2\u03bb, we have m\u2211 k=1 ( \u3008AT\u03d5sk, x\u2217 \u2212 zsk\u3009+ \u3008BT\u03d5sk, y\u2217 \u2212 ysk\u3009+ \u3008Azsk +Bysk \u2212 c, \u03d5sk \u2212 \u03d5\u3009 ) = \u2212 m\u2211 k=1 \u3008Azsk +Bysk \u2212 c, \u03d5\u3009,\nwhere \u03d5sk = \u03b2 (\u03bb s k \u2212 \u03bb\u2217). Using Lemma 9, we have\n\u2212 m\u2211 k=1 \u3008Azsk +Bysk \u2212 c, \u03d5sk \u2212 \u03d5\u3009 = \u03b2 2 m\u2211 k=1 ( \u2016\u03bbsk\u22121 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbsk \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbsk\u22121 \u2212 \u03bbsk\u20162 ) = \u03b2\n2\n( \u2016\u03bbs0 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbsm \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 m\u2211 k=1 \u2016\u03bbsk\u22121 \u2212 \u03bbsk\u20162 ) .\nCombining the above results and Lemma 2, we have\nE [ P (x\u0303s, y\u0303s)\u2212 \u03b8s\u22121\nm m\u2211 k=1 \u3008Azsk +Bysk \u2212 c, \u03d5\u3009\n]\n\u2264 (1\u2212\u03b8s\u22121)E [ P (x\u0303s\u22121, y\u0303s\u22121) ] + L\u03b1\u03b82s\u22121 2m E [ \u2016x\u2217 \u2212 zs0\u20162G \u2212 \u2016x\u2217 \u2212 zsm\u20162G ] + \u03b2\u03b8s\u22121 2m E [ \u2016Azs0 +By\u2217 \u2212 c\u20162 \u2212 \u2016Azsm +By\u2217 \u2212 c\u20162 + \u2016\u03bbs0 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbsm \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 ] .\n(30)\nUsing the update rule of x\u0303s = 1m \u2211m k=1 x s k, we have\nx\u0303s = 1\nm m\u2211 k=1 xsk = 1 m m\u2211 k=1 ( \u03b8s\u22121z s k + (1\u2212 \u03b8s\u22121)x\u0303s\u22121 ) = (1\u2212 \u03b8s\u22121)x\u0303s\u22121 + \u03b8s\u22121 m m\u2211 k=1 zsk.\nRecall that\ny\u0303s = (1\u2212 \u03b8s\u22121)y\u0303s\u22121 + \u03b8s\u22121 m m\u2211 k=1 ysk.\nSubtracting (1\u2212\u03b8s\u22121)\u3008Ax\u0303s\u22121 +By\u0303s\u22121 \u2212 c, \u03d5\u3009 and dividing both sides by \u03b82s\u22121, we have 1\n\u03b82s\u22121 E[P (x\u0303s, y\u0303s)\u2212 \u3008Ax\u0303s +By\u0303s \u2212 c, \u03d5\u3009]\n\u2264 (1\u2212\u03b8s\u22121) \u03b82s\u22121\nE [ P (x\u0303s\u22121, y\u0303s\u22121)\u2212 \u3008Ax\u0303s\u22121 +By\u0303s\u22121 \u2212 c, \u03d5\u3009 ] + L\u03b1 2m E [ \u2016x\u2217 \u2212 zs0\u20162G \u2212 \u2016x\u2217 \u2212 zsm\u20162G ] + \u03b2\n2m\u03b8s\u22121 E [ \u2016Azs0 +By\u2217 \u2212 c\u20162 \u2212 \u2016Azsm +By\u2217 \u2212 c\u20162 + \u2016\u03bbs0 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbsm \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 ] .\n(31)\nBy the update rule in (12), we have (1 \u2212 \u03b8s)/\u03b82s = 1/\u03b82s\u22121. Since zs0 = zs\u22121m , z\u03030 = x\u03030, and summing over all stages (s = 1, \u00b7 \u00b7 \u00b7 , T ), we have\n1 \u03b82T\u22121 E [ P (x\u0303T, y\u0303T )\u2212 \u3008Ax\u0303T +By\u0303T \u2212 c, \u03d5\u3009 ] \u2264 (1\u2212\u03b80)\n\u03b820 E [ P (x\u03030, y\u03030)\u2212 \u3008Ax\u03030 +By\u03030 \u2212 c, \u03d5\u3009 ] + L\u03b1 2m E [ \u2016x\u2217 \u2212 x\u03030\u20162G \u2212 \u2016x\u2217 \u2212 z0m\u20162G ] +\nT\u2211 s=1\n\u03b2 2m\u03b8s\u22121 E [ \u2016Azs0 \u2212Ax\u0303\u2217\u20162 \u2212 \u2016Azsm \u2212Ax\u0303\u2217\u20162 + \u2016\u03bbs0 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbsm \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 ] .\n(32)\nSince (1\u2212 \u03b8s)/\u03b82s = 1/\u03b82s\u22121, then\n0 \u2264 1 \u03b8s \u2212 1 \u03b8s\u22121 \u2264 1 \u03b8s \u2212 \u221a 1\u2212 \u03b8s \u03b8s = 1 1 + \u221a 1\u2212 \u03b8s < 1.\nUsing zs0 = z s\u22121 m and \u03bb s 0 = \u03bb s\u22121 m , we have\nT\u2211 s=1\n\u03b2 2m\u03b8s\u22121 E [ \u2016Azs0 \u2212Ax\u0303\u2217\u20162 \u2212 \u2016Azsm \u2212Ax\u0303\u2217\u20162 + \u2016\u03bbs0 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 \u2212 \u2016\u03bbsm \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 ] \u2264E [ \u03b2(1\u2212 \u03b80) 2m\u03b80 (\u2016Az10 \u2212Ax\u0303\u2217\u20162 + \u2016\u03bb10 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162) + \u03b2 2m T\u2211 s=1 ( \u2016Azs0 \u2212Ax\u0303\u2217\u20162 + \u2016\u03bbs0 \u2212 \u03bb\u2217 \u2212 \u03bb\u20162 )]\n\u2264\u03b2(T \u2212 1 + 1/\u03b80) 2m\nE [ \u2016ATA\u20162D2Z + 4D2\u03bb ] ,\n(33)\nwhere the first inequality follows from that 1/\u03b8s \u2212 1/\u03b8s\u22121 < 1, z10 = x\u03030 and \u03bb10 = \u03bb\u03030, and the last inequality holds due to that \u2016Azs0 \u2212Ax\u0303\u2217\u20162 \u2264 \u2016ATA\u20162D2Z and \u2016\u03bbs0 \u2212 \u03bb\u2217 \u2212 \u03bb\u2016 \u2264 \u2016\u03bbs0 \u2212 \u03bb\u2217\u2016+ \u2016\u03bb\u2016 \u2264 2D\u03bb.\nBy (32) and (33) with \u03b8s \u2264 2/(s+ 2) for all s, we have E [ P (x\u0303T, y\u0303T )\u2212 \u3008Ax\u0303T +By\u0303T \u2212 c, \u03d5\u3009 ] \u2264 4\u03c4\n(T+1)2 ( P (x\u03030, y\u03030)\u2212 \u3008Ax\u03030 +By\u03030 \u2212 c, \u03d5\u3009 ) +\n2L\u03b1\nm(T+1)2 \u2016x\u2217 \u2212 x\u03030\u20162G + 2\u03b2(T\u22121+1/\u03b80) m(T+1)2\n( \u2016ATA\u20162D2Z + 4D2\u03bb ) ,\n\u2264 4\u03c4 (T+1)2\n( P (x\u03030, y\u03030)\u2212 \u3008Ax\u03030 +By\u03030 \u2212 c, \u03d5\u3009 ) +\n2L\u03b1\nm(T+1)2 \u2016x\u2217 \u2212 x\u03030\u20162G +\n2\u03b2\nm(T+1)\n( \u2016ATA\u20162D2Z + 4D2\u03bb ) ,\nwhere \u03b80 = 1 \u2212 \u03b4(b)\u03b1\u22121 and \u03c4 = (1 \u2212 \u03b80)/\u03b8 2 0 = (\u03b1\u22121)\u03b4(b) (\u03b1\u22121\u2212\u03b4(b))2 . Setting \u03d5 = \u03b3 Ax\u0303T+By\u0303T\u2212c \u2016Ax\u0303T+By\u0303T\u2212c\u2016 with \u03b3 \u2264 \u03b2D\u03bb such that \u2016\u03bb\u2016 = \u2016\u03d5/\u03b2\u2016 \u2264 D\u03bb, and \u2212\u3008Ax\u03030 +By\u03030 \u2212 c, \u03d5\u3009 \u2264 \u2016\u03d5\u2016\u2016Ax\u03030 +By\u03030 \u2212 c\u2016 \u2264 \u03b3\u2016Ax\u03030 +By\u03030 \u2212 c\u2016, we have\nE [ P (x\u0303T, y\u0303T ) + \u03b3\u2016Ax\u0303T +By\u0303T \u2212 c\u2016 ] \u2264 4(\u03b1\u2212 1)\u03b4(b)\n(\u03b1\u2212 1\u2212 \u03b4(b))2(T + 1)2 ( P (x\u03030, y\u03030) + \u03b3\u2016Ax\u03030 +By\u03030 \u2212 c\u2016 ) +\n2L\u03b1\nm(T + 1)2 \u2016x\u2217 \u2212 x\u03030\u20162G\n+ 2\u03b2\nm(T + 1)\n( \u2016ATA\u20162D2Z + 4D2\u03bb ) .\nThis completes the proof."}], "references": [{"title": "Towards an optimal stochastic alternating direction method of multipliers", "author": ["Azadi", "S. Sra 2014] Azadi", "S. Sra"], "venue": "In Proc. 31st Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "Azadi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azadi et al\\.", "year": 2014}, {"title": "Advanced topics in machine learning part II: 5. Proximal methods. University Lecture", "author": ["Baldassarre", "L. Pontil 2013] Baldassarre", "M. Pontil"], "venue": null, "citeRegEx": "Baldassarre et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldassarre et al\\.", "year": 2013}, {"title": "Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data", "author": ["Ghaoui Banerjee", "O. d\u2019Aspremont 2008] Banerjee", "L.E. Ghaoui", "A. d\u2019Aspremont"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Banerjee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Beck", "A. Teboulle 2009] Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sci", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Boyd"], "venue": null, "citeRegEx": "Boyd,? \\Q2011\\E", "shortCiteRegEx": "Boyd", "year": 2011}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["Deng", "W. Yin 2016] Deng", "W. Yin"], "venue": "J. Sci. Comput", "citeRegEx": "Deng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2016}, {"title": "Fast alternating direction optimization methods", "author": ["Goldstein"], "venue": "SIAM J. Imaging Sciences", "citeRegEx": "Goldstein,? \\Q2014\\E", "shortCiteRegEx": "Goldstein", "year": 2014}, {"title": "Accelerated stochastic mirror descent algorithms for composite non-strongly convex optimization. arXiv:1605.06892v2", "author": ["Hien"], "venue": null, "citeRegEx": "Hien,? \\Q2016\\E", "shortCiteRegEx": "Hien", "year": 2016}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "R. Zhang 2013] Johnson", "T. Zhang"], "venue": "In Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "A multivariate regression approach to association analysis of a quantitative trait network. Bioinformatics 25:i204\u2013i212", "author": ["Sohn Kim", "S. Xing 2009] Kim", "K.A. Sohn", "E.P. Xing"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "2016. Mini-batch semi-stochastic gradient descent in the proximal setting", "author": ["Koneeny"], "venue": "IEEE J. Sel. Top. Sign. Proces", "citeRegEx": "Koneeny,? \\Q2016\\E", "shortCiteRegEx": "Koneeny", "year": 2016}, {"title": "An optimal method for stochastic composite optimization", "author": ["G. Lan 2012] Lan"], "venue": "Math. Program", "citeRegEx": "Lan,? \\Q2012\\E", "shortCiteRegEx": "Lan", "year": 2012}, {"title": "Fast proximal linearized alternating direction method of multiplier with parallel splitting", "author": ["Lu"], "venue": "In Proc. 30th AAAI Conf. Artif. Intell.,", "citeRegEx": "Lu,? \\Q2016\\E", "shortCiteRegEx": "Lu", "year": 2016}, {"title": "Linear time solver for primal svm", "author": ["Nie"], "venue": "In Proc. 31st Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "Nie,? \\Q2014\\E", "shortCiteRegEx": "Nie", "year": 2014}, {"title": "A general analysis of the convergence of ADMM", "author": ["Nishihara"], "venue": "In Proc. 32nd Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "Nishihara,? \\Q2015\\E", "shortCiteRegEx": "Nishihara", "year": 2015}, {"title": "Stochastic alternating direction method of multipliers", "author": ["Ouyang"], "venue": "In Proc. 30th Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "Ouyang,? \\Q2013\\E", "shortCiteRegEx": "Ouyang", "year": 2013}, {"title": "Piecewise linear regularized solution paths", "author": ["Rosset", "S. Zhu 2007] Rosset", "J. Zhu"], "venue": null, "citeRegEx": "Rosset et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rosset et al\\.", "year": 2007}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Schmidt Roux", "N.L. Bach 2012] Roux", "M. Schmidt", "F. Bach"], "venue": "In Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shalev-Shwartz", "S. Zhang 2013] Shalev-Shwartz", "T. Zhang"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "The solution path of the generalized lasso", "author": ["Tibshirani", "R.J. Taylor 2011] Tibshirani", "J. Taylor"], "venue": "Annals of Statistics", "citeRegEx": "Tibshirani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2011}, {"title": "Online alternating direction method", "author": ["Wang", "H. Banerjee 2012] Wang", "A. Banerjee"], "venue": "In Proc. 29th Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "A unified primal-dual algorithm framework based on Bregman iteration", "author": ["Burger Zhang", "X. Osher 2011] Zhang", "M. Burger", "S. Osher"], "venue": "J. Sci. Comput", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Scalable stochastic alternating direction method of multipliers. arXiv:1502.03529v3", "author": ["Li Zhao", "S.-Y. Zhou 2015] Zhao", "W.-J. Li", "Z.-H. Zhou"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Fast-and-light stochastic admm", "author": ["Zheng", "S. Kwok 2016] Zheng", "J.T. Kwok"], "venue": "In Proc. 25th Int. Joint Conf. Artif. Intell.,,", "citeRegEx": "Zheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2016}, {"title": "Accelerated stochastic gradient method for composite regularization", "author": ["Zhong", "L.W. Kwok 2014a] Zhong", "J.T. Kwok"], "venue": "In Proc. 17th Int. Conf. Artif. Intell. Statist.,", "citeRegEx": "Zhong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2014}, {"title": "Fast stochastic alternating direction method of multipliers", "author": ["Zhong", "L.W. Kwok 2014b] Zhong", "J.T. Kwok"], "venue": "In Proc. 31st Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "Zhong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "the ADMM is an effective optimization tool (Boyd et al. 2011), and has shown attractive performance in a wide range of real-world problems, such as big data classification (Nie et al. 2014). To tackle the issue of high periteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), Wang and Banerjee (2012), Suzuki (2013) and Ouyang et al.", "startOffset": 44, "endOffset": 346}, {"referenceID": 4, "context": "the ADMM is an effective optimization tool (Boyd et al. 2011), and has shown attractive performance in a wide range of real-world problems, such as big data classification (Nie et al. 2014). To tackle the issue of high periteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), Wang and Banerjee (2012), Suzuki (2013) and Ouyang et al.", "startOffset": 44, "endOffset": 361}, {"referenceID": 4, "context": "the ADMM is an effective optimization tool (Boyd et al. 2011), and has shown attractive performance in a wide range of real-world problems, such as big data classification (Nie et al. 2014). To tackle the issue of high periteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), Wang and Banerjee (2012), Suzuki (2013) and Ouyang et al. (2013) proposed some online or stochastic ADMM algorithms.", "startOffset": 44, "endOffset": 386}], "year": 2017, "abstractText": "Recently, many variance reduced stochastic alternating direction method of multipliers (ADMM) methods (e.g. SAGADMM, SDCA-ADMM and SVRG-ADMM) have made exciting progress such as linear convergence rates for strongly convex problems. However, the best known convergence rate for general convex problems is O(1/T ) as opposed to O(1/T ) of accelerated batch algorithms, where T is the number of iterations. Thus, there still remains a gap in convergence rates between existing stochastic ADMM and batch algorithms. To bridge this gap, we introduce the momentum acceleration trick for batch optimization into the stochastic variance reduced gradient based ADMM (SVRG-ADMM), which leads to an accelerated (ASVRG-ADMM) method. Then we design two different momentum term update rules for strongly convex and general convex cases. We prove that ASVRG-ADMM converges linearly for strongly convex problems. Besides having a low per-iteration complexity as existing stochastic ADMM methods, ASVRG-ADMM improves the convergence rate on general convex problems from O(1/T ) to O(1/T ). Our experimental results show the effectiveness of ASVRG-ADMM. Introduction In this paper, we consider a class of composite convex optimization problems min x\u2208Rd1 f(x) + h(Ax), (1) where A\u2208Rd2\u00d7d1 is a given matrix, f(x) := 1 n \u2211n i=1fi(x), each fi(x) is a convex function, and h(Ax) is convex but possibly non-smooth. With regard to h(\u00b7), we are interested in a sparsity-inducing regularizer, e.g. `1-norm, group Lasso and nuclear norm. When A is an identity matrix, i.e. A = Id1 , the above formulation (1) arises in many places in machine learning, statistics, and operations research (Bubeck 2015), such as logistic regression, Lasso and support vector machine (SVM). We mainly focus on the large sample regime. In this regime, even first-order batch methods, e.g. FISTA (Beck and Teboulle 2009), become computationally burdensome due to their per-iteration complexity of O(nd1). As a result, stochastic gradient descent (SGD) with per-iteration complexity of O(d1) has Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. witnessed tremendous progress in the recent years. Especially, a number of stochastic variance reduced gradient methods such as SAG (Roux, Schmidt, and Bach 2012), SDCA (Shalev-Shwartz and Zhang 2013) and SVRG (Johnson and Zhang 2013) have been proposed to successfully address the problem of high variance of the gradient estimate in ordinary SGD, resulting in a linear convergence rate (for strongly convex problems) as opposed to sub-linear rates of SGD. More recently, the Nesterov\u2019s acceleration technique (Nesterov 2004) was introduced in (Allen-Zhu 2016; Hien et al. 2016) to further speed up the stochastic variancereduced algorithms, which results in the best known convergence rates for both strongly convex and general convex problems. This motivates us to integrate the momentum acceleration trick into the stochastic alternating direction method of multipliers (ADMM) below. When A is a more general matrix, i.e. A 6=Id1 , the formulation (1) becomes many more complicated problems arising from machine learning, e.g. graph-guided fuzed Lasso (Kim, Sohn, and Xing 2009) and generalized Lasso (Tibshirani and Taylor 2011). To solve this class of composite optimization problems with an auxiliary variable y = Ax, which are the special case of the general ADMM form, min x\u2208Rd1,y\u2208Rd2 f(x) + h(y), s.t. Ax+By = c, (2) the ADMM is an effective optimization tool (Boyd et al. 2011), and has shown attractive performance in a wide range of real-world problems, such as big data classification (Nie et al. 2014). To tackle the issue of high periteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), Wang and Banerjee (2012), Suzuki (2013) and Ouyang et al. (2013) proposed some online or stochastic ADMM algorithms. However, all these variants only achieve the convergence rate of O(1/ \u221a T ) for general convex problems and O(log T/T ) for strongly convex problems, respectively, as compared with the O(1/T ) and linear convergence rates of accelerated batch algorithms (Nesterov 1983), e.g. FISTA, where T is the number of iterations. By now several accelerated and faster converging versions of stochastic ADMM, which are all based on variance reduction techniques, have been proposed, e.g. SAG-ADMM (Zhong and Kwok 2014b), SDCA-ADMM (Suzuki 2014) and SVRG-ADMM (Zheng and Kwok 2016). With regard to strongly convex problems, ar X iv :1 70 7. 03 19 0v 1 [ cs .L G ] 1 1 Ju l 2 01 7 Table 1: Comparison of convergence rates and memory requirements of some stochastic ADMM algorithms. General convex Strongly-convex Space requirement SAG-ADMM O(1/T ) unknown O(d1d2+nd1) SDCA-ADMM unknown linear rate O(d1d2+n) SCAS-ADMM O(1/T ) O(1/T ) O(d1d2) SVRG-ADMM O(1/T ) linear rate O(d1d2) ASVRG-ADMM O(1/T ) linear rate O(d1d2) Suzuki (2014) and Zheng and Kwok (2016) proved that linear convergence can be obtained for the special ADMM form (i.e. B = \u2212Id2 and c = 0) and the general ADMM form, respectively. In SAG-ADMM and SVRG-ADMM, an O(1/T ) convergence rate can be guaranteed for general convex problems, which implies that there still remains a gap in convergence rates between the stochastic ADMM and accelerated batch algorithms. To bridge this gap, we integrate the momentum acceleration trick in (Tseng 2010) for deterministic optimization into the stochastic variance reduction gradient (SVRG) based stochastic ADMM (SVRG-ADMM). Naturally, the proposed method has low per-iteration time complexity as existing stochastic ADMM algorithms, and does not require the storage of all gradients (or dual variables) as in SCAS-ADMM (Zhao, Li, and Zhou 2015) and SVRGADMM (Zheng and Kwok 2016), as shown in Table 1. We summarize our main contributions below. \u2022 We propose an accelerated variance reduced stochastic ADMM (ASVRG-ADMM) method, which integrates both the momentum acceleration trick in (Tseng 2010) for batch optimization and the variance reduction technique of SVRG (Johnson and Zhang 2013). \u2022 We prove that ASVRG-ADMM achieves a linear convergence rate for strongly convex problems, which is consistent with the best known result in SDCA-ADMM (Suzuki 2014) and SVRG-ADMM (Zheng and Kwok 2016). \u2022 We also prove that ASVRG-ADMM has a convergence rate of O(1/T ) for non-strongly convex problems, which is a factor of T faster than SAG-ADMM and SVRG-ADMM, whose convergence rates are O(1/T ). \u2022 Our experimental results further verified that our ASVRG-ADMM method has much better performance than the state-of-the-art stochastic ADMM methods. Related Work Introducing y = Ax \u2208R2 , problem (1) becomes min x\u2208Rd1,y\u2208Rd2 f(x) + h(y), s.t. Ax\u2212 y = 0. (3) Although (3) is only a special case of the general ADMM form (2), when B = \u2212Id2 and c = 0, the stochastic (or online) ADMM algorithms and theoretical results in (Wang and Banerjee 2012; Ouyang et al. 2013; Zhong and Kwok 2014b; Zheng and Kwok 2016) and this paper are all for the more general problem (2). To minimize (2), together with the dual variable \u03bb, the update steps of batch ADMM are yk = argminy h(y) + \u03b2 2 \u2016Axk\u22121+By\u2212c+\u03bbk\u22121\u2016 , (4) xk = argminx f(x) + \u03b2 2 \u2016Ax+Byk\u2212c+\u03bbk\u22121\u2016 , (5) \u03bbk = \u03bbk\u22121 +Axk +Byk \u2212 c, (6) where \u03b2>0 is a penalty parameter. To extend the batch ADMM to the online and stochastic settings, the update steps for yk and \u03bbk remain unchanged. In (Wang and Banerjee 2012; Ouyang et al. 2013), the update step of xk is approximated as follows: xk = argmin x x\u2207fik(xk\u22121) + 1 2\u03b7k \u2016x\u2212 xk\u22121\u2016G + \u03b2 2 \u2016Ax+Byk\u2212c+\u03bbk\u22121\u2016, (7) where we draw ik uniformly at random from [n] := {1, . . . , n}, \u03b7k \u221d 1/ \u221a k is the step-size, and \u2016z\u2016G = zGz with given positive semi-definite matrix G, e.g. G = Id1 in (Ouyang et al. 2013). Analogous to SGD, the stochastic ADMM variants use an unbiased estimate of the gradient at each iteration. However, all those algorithms have much slower convergence rates than their batch counterpart, as mentioned above. This barrier is mainly due to the variance introduced by the stochasticity of the gradients. Besides, to guarantee convergence, they employ a decaying sequence of step sizes \u03b7k, which in turn impacts the rates. More recently, a number of variance reduced stochastic ADMM methods (e.g. SAG-ADMM, SDCA-ADMM and SVRG-ADMM) have been proposed and made exciting progress such as linear convergence rates. SVRG-ADMM in (Zheng and Kwok 2016) is particularly attractive here because of its low storage requirement compared with the algorithms in (Zhong and Kwok 2014b; Suzuki 2014). Within each epoch of SVRG-ADMM, the full gradient p\u0303 =\u2207f(x\u0303) is first computed, where x\u0303 is the average point of the previous epoch. Then \u2207fik(xk\u22121) and \u03b7k in (7) are replaced by \u2207\u0303fIk(xk\u22121) = 1 |Ik| \u2211 ik\u2208Ik (\u2207fik(xk\u22121)\u2212\u2207fik(x\u0303)) + p\u0303 (8) and a constant step-size \u03b7, respectively, where Ik \u2282 [n] is a mini-batch of size b (which is a useful technique to reduce the variance). In fact, \u2207\u0303fIk(xk\u22121) is an unbiased estimator of the gradient \u2207f(xk\u22121), i.e. E[\u2207\u0303fIk(xk\u22121)]=\u2207f(xk\u22121). Accelerated Variance Reduced Stochastic ADMM In this section, we design an accelerated variance reduced stochastic ADMM method for both strongly convex and general convex problems. We first make the following assumptions: Each convex fi(\u00b7) is Li-smooth, i.e. there exists a constant Li>0 such that \u2016\u2207fi(x)\u2212\u2207fi(y)\u2016\u2264Li\u2016x\u2212y\u2016, \u2200x, y \u2208 R, and L , maxi Li; f(\u00b7) is \u03bc-strongly convex, i.e. there is \u03bc > 0 such that f(x) \u2265 f(y) +\u2207f(y)(x\u2212 y)+ \u03bc2 \u2016x\u2212y\u2016 2 for all x, y \u2208R; The matrix A has full row rank. The first two assumptions are common in the analysis of first-order optimization methods, while the last one has Algorithm 1 ASVRG-ADMM for strongly-convex case Input: m, \u03b7, \u03b2 > 0, 1 \u2264 b \u2264 n. Initialize: x\u0303= z\u0303, \u1ef9, \u03b8, \u03bb\u0303=\u2212 1 \u03b2 (A T )\u2207f(x\u03030). 1: for s = 1, 2, . . . , T do 2: x0 = z s 0 = x\u0303 s\u22121, y 0 = \u1ef9 s\u22121, \u03bb0 = \u03bb\u0303 s\u22121; 3: p\u0303 = \u2207f(x\u0303s\u22121); 4: for k = 1, 2, . . . ,m do 5: Choose Ik\u2286 [n]of size b, uniformly at random; 6: y k=argminy h(y)+ \u03b2 2 \u2016Az s k\u22121+By\u2212 c+\u03bbk\u22121\u2016; 7: z k=z s k\u22121\u2212 \u03b7(\u2207\u0303fIk(x s k\u22121)+\u03b2A (Az k\u22121+By s k\u2212c+\u03bb s k\u22121)) \u03b3\u03b8 ; 8: xk=(1\u2212 \u03b8)x\u0303s\u22121 + \u03b8z k; 9: \u03bbk=\u03bb s k\u22121 +Az s k +By s k \u2212 c; 10: end for 11: x\u0303= 1 m \u2211m k=1x s k, \u1ef9 s=(1\u2212\u03b8)\u1ef9s\u22121+ \u03b8 m \u2211m k=1y s k, 12: \u03bb\u0303=\u2212 1 \u03b2 (A T )\u2020\u2207f(x\u0303s); 13: end for Output: x\u0303 , \u1ef9 . been used in the convergence analysis of batch ADMM (?; Nishihara et al. 2015; Deng and Yin 2016) and stochastic ADMM (Zheng and Kwok 2016). The Strongly Convex Case In this part, we consider the case of (2) when each fi(\u00b7) is convex,L-smooth, and f(\u00b7) is \u03bc-strongly convex. Recall that this class of problems include graph-guided Logistic Regression and SVM as notable examples. To efficiently solve this class of problems, we incorporate both the momentum acceleration and variance reduction techniques into stochastic ADMM. Our algorithm is divided into T epochs, and each epoch consists of m stochastic updates, where m is usually chosen to be O(n) as in (Johnson and Zhang 2013). Let z be an important auxiliary variable, its update rule is given as follows. Similar to (Zhong and Kwok 2014b; Zheng and Kwok 2016), we also use the inexact Uzawa method (Zhang, Burger, and Osher 2011) to approximate the sub-problem (7), which can avoid computing the inverse of the matrix ( 1 \u03b7 Id1+\u03b2A A). Moreover, the momentum weight 0\u2264 \u03b8s \u2264 1 (the update rule for \u03b8s is provided below) is introduced into the proximal term 1 2\u03b7\u2016x\u2212xk\u22121\u2016 2 G similar to that of (7), and then the sub-problem with respect to z is formulated as follows: min z (z \u2212z k\u22121) \u2207\u0303fIk(xk\u22121)+ \u03b8s\u22121 2\u03b7 \u2016z \u2212z k\u22121\u2016G + \u03b2 2 \u2016Az +By k \u2212 c+ \u03bbk\u22121\u2016, (9) where \u2207\u0303fIk(xk\u22121) is defined in (8), \u03b7 < 1 2L , and G = \u03b3Id1\u2212 \u03b7\u03b2 \u03b8s\u22121 AA with \u03b3 \u2265 \u03b3min \u2261 \u03b7\u03b2\u2016A A\u20162 \u03b8s\u22121 +1 to ensure that G I similar to (Zheng and Kwok 2016), where \u2016\u00b7\u20162 is the spectral norm, i.e. the largest singular value of the matrix. Furthermore, the update rule for x is given by xk= x\u0303 +\u03b8s\u22121(z s k\u2212 x\u0303)=(1\u2212\u03b8s\u22121)x\u0303+\u03b8s\u22121z k, (10) where \u03b8s\u22121(z k \u2212 x\u0303s\u22121) is the key momentum term (similar to those in accelerated batch methods (Nesterov 2004)), which helps accelerate our algorithm by using the iterate of the previous epoch, i.e. x\u0303s\u22121. Similar to xk, \u1ef9 s = (1\u2212 \u03b8s\u22121)\u1ef9 s\u22121+ \u03b8s\u22121 m \u2211m k=1y s k. Moreover, \u03b8s can be set to a constant \u03b8 in all epochs of our algorithm, which must satisfy 0 \u2264 \u03b8 \u2264 1\u2212 \u03b4(b)/(\u03b1\u22121), where \u03b1 = 1 L\u03b7 > 1+ \u03b4(b), and \u03b4(b) is defined below. The optimal value of \u03b8 is provided in Proposition 1 below. The detailed procedure is shown in Algorithm 1, where we adopt the same initialization technique for \u03bb\u0303 as in (Zheng and Kwok 2016), and (\u00b7)\u2020 is the pseudo-inverse. Note that, when \u03b8=1, ASVRG-ADMM degenerates to SVRG-ADMM in (Zheng and Kwok 2016). The Non-Strongly Convex Case In this part, we consider general convex problems of the form (2) when each fi(\u00b7) is convex, L-smooth, and h(\u00b7) is not necessarily strongly convex (but possibly non-smooth). Different from the strongly convex case, the momentum weight \u03b8s is required to satisfy the following inequalities: 1\u2212 \u03b8s \u03b82 s \u2264 1 \u03b82 s\u22121 and 0 \u2264 \u03b8s \u2264 1\u2212 \u03b4(b) \u03b1\u2212 1 , (11) where \u03b4(b) := n\u2212b b(n\u22121) is a decreasing function with respect to the mini-batch size b. The condition (20) allows the momentum weight to decease, but not too fast, similar to the requirement on the step-size \u03b7k in classical SGD and stochastic ADMM (?). Unlike batch acceleration methods, the weight must satisfy both inequalities in (20). Motivated by the momentum acceleration techniques in (Tseng 2010; Nesterov 2004) for batch optimization, we give the update rule of the weight \u03b8s for the mini-batch case: \u03b8s = \u221a \u03b84 s\u22121+ 4\u03b8 2 s\u22121 \u2212 \u03b8 s\u22121 2 and \u03b80 = 1\u2212 \u03b4(b) \u03b1\u2212 1 . (12) For the special case of b = 1, we have \u03b4(1) = 1 and \u03b80 = 1\u2212 1 \u03b1\u22121 , while b=n (i.e. batch version), \u03b4(n)=0 and \u03b80= 1. Since {\u03b8s} is decreasing, then \u03b8s \u2264 1\u2212 \u03b4(b) \u03b1\u22121 is satisfied. The detailed procedure is shown in Algorithm 2, which has many slight differences in the initialization and output of each epoch from Algorithm 1. In addition, the key difference between them is the update rule for the momentum weight \u03b8s. That is, \u03b8s in Algorithm 1 can be set to a constant, while that in Algorithm 2 is adaptively adjusted as in (12). Convergence Analysis This section provides the convergence analysis of our ASVRG-ADMM algorithms (i.e. Algorithms 1 and 2) for strongly convex and general convex problems, respectively. Following (Zheng and Kwok 2016), we first introduce the following function P (x, y) := f(x)\u2212f(x\u2217)\u2212\u2207f(x\u2217)T(x\u2212 x\u2217)+h(y)\u2212h(y\u2217)\u2212h\u2032(y\u2217)T(y\u2212 y\u2217) as a convergence criterion, where h\u2032(y) denotes the (sub)gradient of h(\u00b7) at y. Indeed, P (x, y)\u2265 0 for all x, y \u2208 R. In the following, we give the intermediate key results for our analysis. Algorithm 2 ASVRG-ADMM for general convex case Input: m, \u03b7, \u03b2 > 0, 1 \u2264 b \u2264 n. Initialize: x\u0303 = z\u0303, \u1ef9, \u03bb\u0303, \u03b80 = 1\u2212 L\u03b7\u03b4(b) 1\u2212L\u03b7 . 1: for s = 1, 2, . . . , T do 2: x0=(1\u2212\u03b8s\u22121)x\u0303+\u03b8s\u22121z\u0303, y 0= \u1ef9s\u22121, \u03bb0= \u03bb\u0303s\u22121; 3: p\u0303 = \u2207f(x\u0303s\u22121), z 0= z\u0303s\u22121; 4: for k = 1, 2, . . . ,m do 5: Choose Ik\u2286 [n]of size b, uniformly at random; 6: y k=argminy h(y)+ \u03b2 2 \u2016Az s k\u22121+By\u2212 c+\u03bbk\u22121\u2016; 7: z k=z s k\u22121\u2212 \u03b7(\u2207\u0303fIk(x s k\u22121)+\u03b2A (Az k\u22121+By s k\u2212c+\u03bb s k\u22121)) \u03b3\u03b8s\u22121 ; 8: xk=(1\u2212 \u03b8s\u22121)x\u0303 + \u03b8s\u22121z k; 9: \u03bbk=\u03bb s k\u22121 +Az s k +By s k \u2212 c; 10: end for 11: x\u0303= 1 m \u2211m k=1x s k, \u1ef9 =(1\u2212\u03b8s\u22121)\u1ef9+ \u03b8s\u22121 m \u2211m k=1y s k, 12: \u03bb\u0303=\u03bbm, z\u0303 =z m, \u03b8s= \u221a \u03b84 s\u22121+4\u03b8 2 s\u22121\u2212\u03b8 2 s\u22121 2 ; 13: end for Output: x\u0303 , \u1ef9 . Lemma 1. E[\u2016\u2207\u0303fIk(xk\u22121)\u2212\u2207f(xk\u22121)\u2016] \u22642L\u03b4(b) [ f(x\u0303)\u2212f(xk\u22121)+(xk\u22121\u2212 x\u0303)\u2207f(xk\u22121) ] , where \u03b4(b)= n\u2212b b(n\u22121)\u22641 and 1 \u2264 b \u2264 n. Lemma 2. Using the same notation as in Lemma 1, let (x\u2217, y\u2217, \u03bb\u2217) denote an optimal solution of problem (2), and {(z k, xk, y k, \u03bbk, x\u0303, \u1ef9)} be the sequence generated by Algorithm 1 or 2 with \u03b8s \u2264 1\u2212 \u03b4(b) \u03b1\u22121, where \u03b1= 1 L\u03b7 . Then the following holds for all k, E [ P (x\u0303, \u1ef9)\u2212 \u03b8s\u22121 m m \u2211 k=1 ( (x\u2217\u2212zs k)A\u03c6k+(y\u2212y k)B\u03c6k )] \u2264E [ P (x\u0303s\u22121, \u1ef9s\u22121) 1/(1\u2212\u03b8s\u22121) + \u03b8 s\u22121 ( \u2016x\u2217\u2212 z 0\u2016G\u2212\u2016x\u2212 z m\u2016G )", "creator": "LaTeX with hyperref package"}}}