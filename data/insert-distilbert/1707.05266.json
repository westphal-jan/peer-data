{"id": "1707.05266", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2017", "title": "A Simple Language Model based on PMI Matrix Approximations", "abstract": "in approaching this study, we introduce a new approach for learning language models by training them to estimate word - context pointwise spatial mutual information ( pmi ), and then deriving randomly the desired conditional probabilities from pmi at test time. specifically, we show that with minor modifications to word2vec's algorithm, we cannot get principled language models that also are closely related to solving the well - established noise contrastive estimation ( nce ) based language models. a distinctly compelling aspect of our approach is that our models routinely are trained actors with the same simple negative sampling objective function that is commonly used in word2vec to gradually learn word embeddings.", "histories": [["v1", "Mon, 17 Jul 2017 16:21:46 GMT  (28kb)", "http://arxiv.org/abs/1707.05266v1", "Accepted to EMNLP 2017"]], "COMMENTS": "Accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oren melamud", "ido dagan", "jacob goldberger"], "accepted": true, "id": "1707.05266"}, "pdf": {"name": "1707.05266.pdf", "metadata": {"source": "CRF", "title": "A Simple Language Model based on PMI Matrix Approximations", "authors": ["Oren Melamud", "Jacob Goldberger"], "emails": ["oren.melamud@ibm.com", "dagan@cs.biu.ac.il", "jacob.goldberger@biu.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 7.\n05 26\n6v 1\n[ cs\n.C L\n] 1\n7 Ju\nl 2 01\n7\nfor learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec\u2019s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings."}, {"heading": "1 Introduction", "text": "Language models (LMs) learn to estimate the probability of a word given a context of preceding words. Recurrent Neural Network (RNN) language models recently outperformed traditional n-gram LMs across a range of tasks (Jozefowicz et al., 2016). However, an important practical issue associated with such neuralnetwork LMs is the high computational cost incurred. The key factor that limits the scalability of traditional neural LMs is the computation of the normalization term in the softmax output layer, whose cost is linearly proportional to the size of the word vocabulary.\nSeveral methods have been proposed to cope with this scaling issue by replacing the softmax with a more computationally efficient component at train time.1 These include importance sam-\n1An alternative recent approach for coping with large word vocabularies is to represent words as compositions of sub-word units, such as individual characters. This approach\npling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al., 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012). NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016). NCE-based language models achieved near state-of-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are closely related to the method presented in this paper.\nContinuous word embeddings were initially introduced as a \u2018by-product\u2019 of learning neural language models (Bengio and et al, 2003). However, they were later adopted in many other NLP tasks, and the most popular recent word embedding learning models are no longer proper language models. In particular, the skip-gram with negative sampling (NEG) embedding algorithm (Mikolov et al., 2013) as implemented in the word2vec toolkit, has become one of the most popular such models today. This is largely attributed to its scalability to huge volumes of data, which is critical for learning high-quality embeddings. Recently, Levy and Goldberg (2014) offered a motivation for the NEG objective function, showing that by maximizing this function, the skip-gram algorithm implicitly attempts to factorize a wordcontext pointwise mutual information (PMI) matrix. Melamud and Goldberger (2017) rederived this result by offering an information-theory interpretation of NEG.\nThe NEG objective function is considered a simplification of the NCE\u2019s objective, unsuitable for learning language models (Dyer, 2014). However,\nhas notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper.\nin this study, we show that despite its simplicity, it can be used in a principled way to effectively train a language model, based on PMI matrix factorization. More specifically, we use NEG to train a model for estimating the PMI between words and their preceding contexts, and then derive conditional probabilities from PMI at test time. The obtained PMI-LM can be viewed as a simple variant of word2vec\u2019s algorithm, where the context of a predicted word is the preceding sequence of words, rather than a single word within a context window (skip-gram), or a bag-of-context-words (CBOW).\nOur analysis shows that the proposed PMI-LM is very closely related to NCE language models (NCE-LMs). Similar to NCE-LMs, PMILM avoids the dependency of train run-time on the size of the word vocabulary by sampling from a negative (noise) distribution. Furthermore, conveniently, it also has a notably more simplified objective function formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implementations of NCE language models (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).\nFinally, we report on a perplexity evaluation of PMI and NCE language models on two standard language modeling datasets. The evaluation yielded comparable results, supporting our theoretical analysis."}, {"heading": "2 NCE-based Language Modeling", "text": "Noise Contrastive Estimation (NCE) has recently been used to learn language models efficiently. NCE transforms the parameter learning problem into a binary classifier training problem. Let p(w|c) be the probability of a word w given a context c that represents its entire preceding context, and let p(w) be a \u2018noise\u2019 word distribution (e.g. a unigram distribution). The NCE approach assumes that the word w is sampled from a mixture distribution 1 k+1\n(p(w|c) + kp(w)) such that the noise samples are k times more frequent than samples from the \u2018true\u2019 distribution p(w|c). Let y be a binary random variable such that y = 0 and y = 1 correspond to a noise sample and a true sample, respectively, i.e. p(w|c, y = 0) = p(w) and p(w|c, y = 1) = p(w|c). Assume the distribution p(w|c) has the following parametric form:\npnce(w|c) = 1\nZc exp(~w \u00b7 ~c+ bw) (1)\nsuch that ~w and ~c are vector representations of the word w and its context c. Applying Bayes rule, it can be easily verified that:\npnce(y = 1|w, c) = (2)\n\u03c3(~w \u00b7 ~c+ bw \u2212 logZc \u2212 log(p(w)k))\nwhere \u03c3() is the sigmoid function. NCE uses Eq. (2) and the following objective function to train a binary classifier that decides which distribution was used to sample w:\nSnce = \u2211\nw,c\u2208D\n[\nlog p(1|w, c) +\nk \u2211\ni=1\nlog p(0|ui, c) ]\n(3)\nsuch that w, c go over all the word-context co-occurrences in the learning corpus D and u1, ..., uk are \u2018noise\u2019 samples drawn from the word unigram distribution.\nNote that the normalization factor Zc is not a free parameter and to obtain its value, one needs to compute Zc = \u2211\nw\u2208V exp(~w\u00b7~c+bw) for each context c, where V is the word vocabulary. This computation is typically not feasible due to the large vocabulary size and the exponentially large number of possible contexts and therefore it was heuristically circumvented by prior work. Mnih and Teh (2012) found empirically that setting Zc = 1 didn\u2019t hurt the performance (see also discussion in (Andreas and Klein, 2015)). Chen et al. (2015) reported that setting log(Zc) = 9 gave them the best results. Recent works (Vaswani et al., 2013; Zoph et al., 2016) used Zc = 1 and also initialized NCE\u2019s bias term from Eq. (2) to bw = \u2212 log |V |. They reported that without these heuristics the training procedure did not converge to a meaningful model.\nIn the following section, we describe our proposed language model, which is derived from word2vec\u2019s interpretation as a low-rank PMI matrix approximation. Interestingly, this model turns out to be a close variant of NCE language models, but with a simplified objective function that avoids the need for the normalization factor Zc and the bias terms."}, {"heading": "3 PMI-based Language Modeling", "text": "The skip-gram negative sampling word embedding algorithm represents each word w and each\ncontext word c as d-dimensional vectors, with the purpose that words that are \u201csimilar\u201d to each other will have similar vector representations. The algorithm optimizes the following NEG objective function (Mikolov et al., 2013):\nSneg = \u2211\nw,c\u2208D\n[ log \u03c3(~w \u00b7 ~c) + k \u2211\ni=1\nlog \u03c3(\u2212~ui \u00b7 ~c) ]\n(4)\nsuch that w, c go over all the word-context cooccurrences in the learning corpus D, u1, ..., uk are words independently sampled from the word unigram distribution, ~x is the embedding of x and \u03c3() is the sigmoid function. The objective function Sneg can be viewed as a log-likelihood function of a binary logistic regression classifier that treats a sample from a joint word-context distribution as a positive instance, and two independent samples from the word and context unigram distributions as a negative instance, while k is the proportion between negative and positive instances. Levy and Goldberg (2014) showed that this objective function achieves its maximal value when for every word-context pair w, c:\n~w \u00b7 ~c = pmik(w, c) = log p(w|c)\nkp(w) (5)\nwhere pmik(w, c) is the word-context PMI matrix. Actually achieving this maximal value is typically infeasible, since the embedding dimensionality is intentionally limited. Therefore, learning word and context embeddings that optimize skip-gram\u2019s NEG objective function (4) can be viewed as finding a low-rank approximation of the word-context PMI matrix. An explicit expression of the approximation criterion optimized by the skip-gram algorithm can be found in (Melamud and Goldberger, 2017).\nOur study is based on two simple observations regarding this finding of Levy and Goldberg (2014). First, Equation (5) can be reformulated as follows to derive an estimate of the conditional distribution p(w|c):\np\u0302(w|c) \u221d exp(~w \u00b7 ~c)p(w) (6)\nwhere the constant k is dropped since p(w|c) is a distribution. Second, while the above analysis had been originally applied to the case of wordcontext joint distributions p(w, c), it is easy to see that the PMI matrix approximation analysis also holds for every Euclidean embedding of a joint\ndistribution p(x, y) of any two given random variables X and Y . In particular, we note that it holds for word-context joint distributions p(w, c), where w is a single word, but c represents its entire preceding context, rather than just a single context word, and ~c is a vector representation of this entire context. Altogether, this allows us to use word2vec\u2019s NEG objective function (4) to approximate the language modeling conditional probability p\u0302(w|c) (6), with c being the entire preceding context of the predicted word w.\nWe next describe the design details of the proposed PMI-based language modeling. We use a simple lookup table for the word representation ~w, and an LSTM recurrent neural network to obtain a low dimensional representation of the entire preceding context~c. These representations are trained to maximize the NEG objective in Eq. (4), where this time w goes over every word token in the corpus, and c is its preceding context. We showed above that optimizing this objective seeks to obtain the best low-dimensional approximation of the PMI matrix associated with the joint distribution of the word and its preceding context (Eq. (5)). Hence, based on Eq. (6), for a reasonable embedding dimensionality and a good model for representing the preceding context, we expect p\u0302(w|c) to be a good estimate of the language modeling conditional distribution.\nAt test time, to obtain a proper distribution, we perform a normalization operation as done by all other comparable models. The train and test steps of the proposed language modeling algorithm are shown in algorithm box 1.\nNote that while the NCE approach (1) learns to explicitly estimate normalized conditional distributions, our model learns to approximate the PMI matrix. Hence, we have no real motivation to include additional learned normalization parameters, as considered in comparable NCE language models (Mnih and Teh, 2012; Zoph et al., 2016).\nThe NEG and NCE objective functions share a\nsimilar form:\nS = \u2211\nw,c\n[ log s(w, c)+ k \u2211\ni=1\nlog(1\u2212s(ui, c)) ] (7)\nwith the differences summarized in Table 1. The comparison shows that PMI-LM\u2019s NEG objective function is much simpler. Furthermore, due to the component log(p(w)k)) in NCE\u2019s objective function, its input to the sigmoid function is sensitive to\nAlgorithm 1 PMI Language Modeling\nTraining phase: - Use a simple lookup table for the word representation and an LSTM recurrent neural network to obtain the preceding context representation. - Train the word and preceding context embeddings to maximize the objective:\nSneg = \u2211\nw,c\u2208D\n[\nlog \u03c3(~w \u00b7~c)+\nk \u2211\ni=1\nlog \u03c3(\u2212~ui \u00b7~c) ]\nsuch that w and c go over every word and it preceding context in the corpus D, and u1, ..., uk are words independently sampled from the unigram distribution p(w).\nTest phase: The conditional probability estimate for a word w given a preceding context c is:\np\u0302(w|c) = exp(~w \u00b7 ~c)p(w) \u2211\nv\u2208V exp(~v \u00b7 ~c)p(v)\nwhere V is the word vocabulary.\nthe variable values in the unigram distribution, and therefore potentially more difficult to concentrate around zero with low variance to facilitate effective back-propagation. This may explain heuristics used by prior work for initializing the values of bw (Vaswani et al., 2013; Zoph et al., 2016)."}, {"heading": "4 Experiments", "text": "The goal of the evaluation described in this section is to empirically establish PMI-LM as a sound language model. We do so by comparing its performance with the well-established NCE-LM, using the popular perplexity measure on two standard datasets, under the same terms. We describe our hyperparameter choices below and stress that for a fair comparison, we followed prior best practices and avoided hyperparameter optimization in favor of PMI-LM. All of the models described here-\nafter were implemented using the Chainer toolkit (Tokui et al., 2015).\nFor our NCE baseline, we used the heuristics that worked well in (Vaswani et al., 2013; Zoph et al., 2016), initializing NCE\u2019s bias term from Eq. (2) to bw = \u2212 log |V |, where V is the word vocabulary, and using Zc = 1.\nThe first dataset we used is a version of the Penn Tree Bank (PTB), commonly used to evaluate language models.2 It consists of 929K training words, 73K validation words and 82K test words with a 10K word vocabulary. To build and train the compared models in this setting, we followed the work of Zaremba et al. (2014), who achieved excellent results on this dataset. Specifically, we used a 2-layer 300-hidden-units LSTMwith a 50% dropout ratio to represent the preceding (left-side) context of a predicted word.3 We represented endof-sentence as a special <eos> token and predicted this token like any other word. During training, we performed truncated back-propagationthrough-time, unrolling the LSTM for 20 steps at a time without ever resetting the LSTM state. We trained our model for 39 epochs using Stochastic Gradient Descent (SGD) with a learning rate of 1, which is decreased by a factor of 1.2 after every epoch starting after epoch 6. We clipped the norms of the gradient to 5 and used a mini-batch size of 20. We set the negative sampling parameter to k = 100 following Zoph et al. (2016), who showed highly competitive performance with NCE LMs trained with this number of samples.\nAs the second dataset, we used the much larger WMT 1B-word benchmark introduced by Chelba et al. (2013). This dataset comprises about 0.8B training words and has a held-out set partitioned into 50 subsets. The test set is the first subset in the held-out, comprising 159K words, including the <eos> tokens. We used the second subset as the validation set with 165K words. The orig-\n2Available from Tomas Mikolov at: http://www.fit.vutbr.cz/\u02dcimikolov/rnnlm/simple-examples.tgz\n3Zaremba et al. (2014) used larger models with more units and also applied dropout to the output of the top LSTM layer, which we did not.\ninal vocabulary size of this dataset is 0.8M words after converting all words that occur less than 3 times in the corpus to an <unk> token. However, we followed previous works (Williams et al., 2015; Ji et al., 2016) and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources. To build and train our models, we used a similar method to the one used with PTB, with the following differences. We used a single-layer 512- hidden-unit LSTM to represent the preceding context. We followed Jozefowicz et al. (2016), who found a 10% dropout rate to be sufficient for relatively small models fitted to this large training corpus. We trained our model for only one epoch using the Adam optimizer (Kingma and Ba, 2014) with default parameters, which we found to converge more quickly and effectively than SGD. We used a mini-batch size of 1000.\nThe perplexity results achieved by the compared models appear in Table 2. As can be seen, the performance of our PMI-LM is competitive, slightly outperforming the NCE-LM on both test sets. To put these numbers in a broader context, we note that state-of-the-art results on these datasets are notably better. For example, on the small PTB test set, Zaremba et al. (2014) achieved 78.4 perplexity with a larger LSTM model and using the more costly softmax component. On the larger WMT dataset, Jozefowicz et al. (2016) achieved 46.1 and 43.7 perplexity numbers using NCE and importance sampling respectively, and with much larger LSTM models trained over the full vocabulary, rather than our trimmed one. They also achieved 23.7 with an ensemble method, which is the best result on this dataset to date. Yet, as intended, we argue that our experimental results affirm the claim that PMI-LM is a sound language model on par with NCE-LM."}, {"heading": "5 Conclusions", "text": "In this work, we have shown that word2vec\u2019s negative sampling objective function, popularized in the context of learning word representations, can\nalso be used to effectively learn parametric language models. These language models are closely related to NCE language models, but utilize a simpler, potentially more robust objective function. More generally, our theoretical analysis shows that any word2vec model trained with negative sampling can be used in a principled way to estimate the conditional distribution p(w|c), by following our proposed procedure at test time."}, {"heading": "Acknowledgments", "text": "This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}], "references": [{"title": "When and why are loglinear models self-normalizing? In NAACL", "author": ["J. Andreas", "D. Klein"], "venue": null, "citeRegEx": "Andreas and Klein.,? \\Q2015\\E", "shortCiteRegEx": "Andreas and Klein.", "year": 2015}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J. Senecal"], "venue": "AISTATS.", "citeRegEx": "Bengio and Senecal,? 2003", "shortCiteRegEx": "Bengio and Senecal", "year": 2003}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson."], "venue": "arXiv preprint arXiv:1312.3005.", "citeRegEx": "Chelba et al\\.,? 2013", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Strategies for training large vocabulary neural language models", "author": ["W. Chen", "D. Grangier", "M. Auli."], "venue": "CoRR, abs/1512.04906.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "author": ["X. Chen", "X. Liu", "M. Gales", "P.C. Woodland."], "venue": "ICASSP.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Notes on noise contrastive estimation and negative sampling", "author": ["C. Dyer."], "venue": "arXiv preprint arXiv:1410.8251.", "citeRegEx": "Dyer.,? 2014", "shortCiteRegEx": "Dyer.", "year": 2014}, {"title": "Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyvarinen."], "venue": "Journal of Machine Learning Research, 13:307\u2013 361.", "citeRegEx": "Gutmann and Hyvarinen.,? 2012", "shortCiteRegEx": "Gutmann and Hyvarinen.", "year": 2012}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["S. Ji", "S. Vishwanathan", "N. Satish", "A. Nadathur", "J. Michael", "P. Dubey."], "venue": "ICLR.", "citeRegEx": "Ji et al\\.,? 2016", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu."], "venue": "arXiv preprint arXiv:1602.02410.", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Informationtheory interpretation of the skip-gram negativesampling objective function", "author": ["O. Melamud", "J. Goldberger."], "venue": "Proceedings of ACL.", "citeRegEx": "Melamud and Goldberger.,? 2017", "shortCiteRegEx": "Melamud and Goldberger.", "year": 2017}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Minh", "G.E. Hinton."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Minh and Hinton.,? 2008", "shortCiteRegEx": "Minh and Hinton.", "year": 2008}, {"title": "A fast and simple algorithm for training neural probabilistic languagemodels", "author": ["A. Mnih", "Y.W. Teh."], "venue": "ICML.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch."], "venue": "CoRR, abs/1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton."], "venue": "Workshop on Machine Learning Systems (LearningSys) in The 29th Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang."], "venue": "EMNLP.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Scaling recurrent neural network language models", "author": ["W. Williams", "N. Prasad", "D. Mrva", "T. Ash", "T. Robinson."], "venue": "ICASSP.", "citeRegEx": "Williams et al\\.,? 2015", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Simple, fast noise-contrastive estimation for large RNN vocabularies", "author": ["B. Zoph", "A. Vaswani", "J. May", "K. Knight."], "venue": "NAACL.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "Recurrent Neural Network (RNN) language models recently outperformed traditional n-gram LMs across a range of tasks (Jozefowicz et al., 2016).", "startOffset": 116, "endOffset": 141}, {"referenceID": 13, "context": "This approach pling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 7, "context": "This approach pling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al., 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012).", "startOffset": 101, "endOffset": 118}, {"referenceID": 6, "context": ", 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012).", "startOffset": 47, "endOffset": 76}, {"referenceID": 14, "context": "NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 17, "context": "NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 153, "endOffset": 213}, {"referenceID": 4, "context": "NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 153, "endOffset": 213}, {"referenceID": 20, "context": "NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 153, "endOffset": 213}, {"referenceID": 8, "context": "NCE-based language models achieved near state-of-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are closely related to the method presented in this paper.", "startOffset": 96, "endOffset": 140}, {"referenceID": 3, "context": "NCE-based language models achieved near state-of-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are closely related to the method presented in this paper.", "startOffset": 96, "endOffset": 140}, {"referenceID": 12, "context": "In particular, the skip-gram with negative sampling (NEG) embedding algorithm (Mikolov et al., 2013) as implemented in the word2vec toolkit, has become one of the most pop-", "startOffset": 78, "endOffset": 100}, {"referenceID": 5, "context": "The NEG objective function is considered a simplification of the NCE\u2019s objective, unsuitable for learning language models (Dyer, 2014).", "startOffset": 122, "endOffset": 134}, {"referenceID": 9, "context": "Recently, Levy and Goldberg (2014) offered a motivation for the NEG objective function, showing that by maximizing this function, the skip-gram algorithm implicitly attempts to factorize a wordcontext pointwise mutual information (PMI) matrix.", "startOffset": 10, "endOffset": 35}, {"referenceID": 9, "context": "Recently, Levy and Goldberg (2014) offered a motivation for the NEG objective function, showing that by maximizing this function, the skip-gram algorithm implicitly attempts to factorize a wordcontext pointwise mutual information (PMI) matrix. Melamud and Goldberger (2017) rederived this result by offering an information-theory interpretation of NEG.", "startOffset": 10, "endOffset": 274}, {"referenceID": 8, "context": "has notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper.", "startOffset": 19, "endOffset": 67}, {"referenceID": 15, "context": "has notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper.", "startOffset": 19, "endOffset": 67}, {"referenceID": 17, "context": "Furthermore, conveniently, it also has a notably more simplified objective function formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implementations of NCE language models (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 256, "endOffset": 316}, {"referenceID": 4, "context": "Furthermore, conveniently, it also has a notably more simplified objective function formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implementations of NCE language models (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 256, "endOffset": 316}, {"referenceID": 20, "context": "Furthermore, conveniently, it also has a notably more simplified objective function formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implementations of NCE language models (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016).", "startOffset": 256, "endOffset": 316}, {"referenceID": 0, "context": "Mnih and Teh (2012) found empirically that setting Zc = 1 didn\u2019t hurt the performance (see also discussion in (Andreas and Klein, 2015)).", "startOffset": 110, "endOffset": 135}, {"referenceID": 17, "context": "Recent works (Vaswani et al., 2013; Zoph et al., 2016) used Zc = 1 and also initialized NCE\u2019s bias term from Eq.", "startOffset": 13, "endOffset": 54}, {"referenceID": 20, "context": "Recent works (Vaswani et al., 2013; Zoph et al., 2016) used Zc = 1 and also initialized NCE\u2019s bias term from Eq.", "startOffset": 13, "endOffset": 54}, {"referenceID": 11, "context": "Mnih and Teh (2012) found empirically that setting Zc = 1 didn\u2019t hurt the performance (see also discussion in (Andreas and Klein, 2015)).", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Mnih and Teh (2012) found empirically that setting Zc = 1 didn\u2019t hurt the performance (see also discussion in (Andreas and Klein, 2015)). Chen et al. (2015) reported that setting log(Zc) = 9 gave them the best results.", "startOffset": 111, "endOffset": 157}, {"referenceID": 12, "context": "The algorithm optimizes the following NEG objective function (Mikolov et al., 2013):", "startOffset": 61, "endOffset": 83}, {"referenceID": 10, "context": "Levy and Goldberg (2014) showed that this objective function achieves its maximal value when for every word-context pair w, c:", "startOffset": 0, "endOffset": 25}, {"referenceID": 11, "context": "imation criterion optimized by the skip-gram algorithm can be found in (Melamud and Goldberger, 2017).", "startOffset": 71, "endOffset": 101}, {"referenceID": 10, "context": "Our study is based on two simple observations regarding this finding of Levy and Goldberg (2014). First, Equation (5) can be reformulated as follows to derive an estimate of the conditional distribution p(w|c):", "startOffset": 72, "endOffset": 97}, {"referenceID": 14, "context": "Hence, we have no real motivation to include additional learned normalization parameters, as considered in comparable NCE language models (Mnih and Teh, 2012; Zoph et al., 2016).", "startOffset": 138, "endOffset": 177}, {"referenceID": 20, "context": "Hence, we have no real motivation to include additional learned normalization parameters, as considered in comparable NCE language models (Mnih and Teh, 2012; Zoph et al., 2016).", "startOffset": 138, "endOffset": 177}, {"referenceID": 17, "context": "This may explain heuristics used by prior work for initializing the values of bw (Vaswani et al., 2013; Zoph et al., 2016).", "startOffset": 81, "endOffset": 122}, {"referenceID": 20, "context": "This may explain heuristics used by prior work for initializing the values of bw (Vaswani et al., 2013; Zoph et al., 2016).", "startOffset": 81, "endOffset": 122}, {"referenceID": 16, "context": "All of the models described hereafter were implemented using the Chainer toolkit (Tokui et al., 2015).", "startOffset": 81, "endOffset": 101}, {"referenceID": 17, "context": "For our NCE baseline, we used the heuristics that worked well in (Vaswani et al., 2013; Zoph et al., 2016), initializing NCE\u2019s bias term from Eq.", "startOffset": 65, "endOffset": 106}, {"referenceID": 20, "context": "For our NCE baseline, we used the heuristics that worked well in (Vaswani et al., 2013; Zoph et al., 2016), initializing NCE\u2019s bias term from Eq.", "startOffset": 65, "endOffset": 106}, {"referenceID": 17, "context": "For our NCE baseline, we used the heuristics that worked well in (Vaswani et al., 2013; Zoph et al., 2016), initializing NCE\u2019s bias term from Eq. (2) to bw = \u2212 log |V |, where V is the word vocabulary, and using Zc = 1. The first dataset we used is a version of the Penn Tree Bank (PTB), commonly used to evaluate language models. It consists of 929K training words, 73K validation words and 82K test words with a 10K word vocabulary. To build and train the compared models in this setting, we followed the work of Zaremba et al. (2014), who achieved excellent results on this dataset.", "startOffset": 66, "endOffset": 537}, {"referenceID": 20, "context": "We set the negative sampling parameter to k = 100 following Zoph et al. (2016), who showed highly competitive performance with NCE LMs trained with this number of samples.", "startOffset": 60, "endOffset": 79}, {"referenceID": 2, "context": "As the second dataset, we used the much larger WMT 1B-word benchmark introduced by Chelba et al. (2013). This dataset comprises about 0.", "startOffset": 83, "endOffset": 104}, {"referenceID": 19, "context": "tgz Zaremba et al. (2014) used larger models with more units and also applied dropout to the output of the top LSTM layer, which we did not.", "startOffset": 4, "endOffset": 26}, {"referenceID": 18, "context": "However, we followed previous works (Williams et al., 2015; Ji et al., 2016) and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "startOffset": 36, "endOffset": 76}, {"referenceID": 7, "context": "However, we followed previous works (Williams et al., 2015; Ji et al., 2016) and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "startOffset": 36, "endOffset": 76}, {"referenceID": 9, "context": "We trained our model for only one epoch using the Adam optimizer (Kingma and Ba, 2014) with default parameters, which we found to converge more quickly and effectively than SGD.", "startOffset": 65, "endOffset": 86}, {"referenceID": 7, "context": ", 2015; Ji et al., 2016) and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources. To build and train our models, we used a similar method to the one used with PTB, with the following differences. We used a single-layer 512hidden-unit LSTM to represent the preceding context. We followed Jozefowicz et al. (2016), who found a 10% dropout rate to be sufficient for relatively small models fitted to this large training corpus.", "startOffset": 8, "endOffset": 430}, {"referenceID": 19, "context": "For example, on the small PTB test set, Zaremba et al. (2014) achieved 78.", "startOffset": 40, "endOffset": 62}, {"referenceID": 8, "context": "On the larger WMT dataset, Jozefowicz et al. (2016) achieved 46.", "startOffset": 27, "endOffset": 52}], "year": 2017, "abstractText": "In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec\u2019s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.", "creator": "LaTeX with hyperref package"}}}