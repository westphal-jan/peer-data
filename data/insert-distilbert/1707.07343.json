{"id": "1707.07343", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2017", "title": "A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events", "abstract": "we present a sequential model for temporal relation classification between intra - sentence events. the key observation is that the two overall dominant syntactic structure and compositional meanings of the multi - word context between events are important for adequately distinguishing among fine - grained intermediate temporal relations. specifically, our approach may first extracts a sequence of context recognition words that indicates the underlying temporal relation possible between two events, which seems well align with the dependency path between two event mentions. the context noun word sequence, together fitted with a parts - of - speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, clues are then provided as input to bidirectional recurrent temporal neural network ( lstm ) models. the neural nets learn compositional syntactic and semantic representations combinations of contexts surrounding the context two identical events and predict overall the temporal relation compared between them. evaluation of the proposed approach performed on timebank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features employed as input that imitates previous feature based models.", "histories": [["v1", "Sun, 23 Jul 2017 20:47:02 GMT  (134kb,D)", "http://arxiv.org/abs/1707.07343v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["prafulla kumar choubey", "ruihong huang"], "accepted": true, "id": "1707.07343"}, "pdf": {"name": "1707.07343.pdf", "metadata": {"source": "CRF", "title": "A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events", "authors": ["Prafulla Kumar Choubey", "Ruihong Huang"], "emails": ["huangrh)@tamu.edu"], "sections": [{"heading": null, "text": "We present a sequential model for temporal relation classification between intrasentence events. The key observation is that the overall syntactic structure and compositional meanings of the multi-word context between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The neural nets learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models."}, {"heading": "1 Introduction", "text": "Identifying temporal relations between events is crucial to constructing events timeline. It has direct application in tasks such as question answering, event timeline generation and document summarization.\nPrevious works studied this task as the classification problem based on discrete features defined over lexico-syntactic, semantic and discourse features. However, these features are often derived from local contexts of two events and are only capable of capturing direct evidences indicating the temporal relation. Specifically, when two events are distantly located or are separated by other events in between, feature based approaches often fail to utilize compositional evidences, which are hard to encode using discrete features.\nConsider the example sentence in Figure 1. Here, the first two temporal re-\nar X\niv :1\n70 7.\n07 34\n3v 1\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n7\nlations, dispute after rel1 invasion and invation ibefore rel2 buildup, involve events that are close by and discrete features, such as dependency relations and bag-of-words extracted from local contexts of two events, might be sufficient to correctly detect their relations. However, for the temporal relation dispute after rel3 buildup, the context between the two events is long, complex and involves another event (invasion) as well, which makes it challenging for any individual feature or feature combinations to capture the temporal relation.\nWe propose that the overall syntactic structure of in-between contexts including the linear order of words as well as the compositional semantics of multi-word contexts are critical for predicting the temporal relation between two events. Furthermore, the most important syntactic and semantic structures are derived along dependency paths between two event mentions1. This aligns well with the observation that semantic composition relates to grammatical dependency relations (Monroe and Wang, 2014; Reddy et al., 2016).\nOur approach defines rules on dependency parse trees to extract temporal relation indicating contexts. First, we extract the dependency path between two event mentions. Then we apply two heuristic rules to enrich extracted dependency paths and deal with complex syntactic structures such as punctuations. Empirically, we found that parts-of-speech tags (POS) and dependency sequences generated following the dependency path provide evidences to predict the temporal relation as well.\nWe use neural net sequence models to capture structural and semantic compositionality in describing temporal relations between events. Specifically, we generate three sequences for each dependency path, the word sequence, the POS tag sequence and the dependency relation sequence. Using the three types of sequences as input, we train bi-directional LSTM models that consume each of the three sequences and model compositional structural information, both syntactically and semantically.\nThe evaluation shows that each type of sequences is useful to temporal relation classification between events. Our complete neural net model taking all the three types of sequences per-\n1In this paper, we restrict ourselves to study temporal relation classification between event mentions that are within one sentence.\nforms the best, which clearly outperforms feature based models."}, {"heading": "2 Related Works", "text": "Most of the previous works on temporal relation classification are based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for classifying temporal relations. Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification.\nThe following works mostly expanded the feature sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs to capture syntactic context. Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse. We found that modeling the entire path as one sequence provides greater compositional evidence on the temporal relation. In addition, modifiers attached to the words in a path with specific dependency relations like nmod:tmod are also informative.\nNg (2013) proposed a hybrid system for temporal relation classification that combines the learned classifier with 437 hand-coded rules. Their system first applied high-accuracy rules and then used the learned classifier, trained on rich features including those high-accuracy rules as features, to classify the cases that were not handled by the rules. Ng et al. (2013) also showed the effectiveness of different discourse analysis frameworks for this task. Later Mirza and Tonelli (2014) showed that a simpler approach based on lexico-syntactic features achieved results comparable to Ng (2013). They also reported that dependency order between events, either governordependent or dependent-governor, was not useful in their experiments. However, we show that dependency relations, when modeled as a sequence, contribute significantly to this task."}, {"heading": "3 Temporal Link Labeling", "text": "In this section, we describe the task of temporal relation classification, dataset, context words se-\nquence extraction model and the used recurrent neural net based classifier."}, {"heading": "3.1 Task description", "text": "Early works on temporal relation classification Mani et al. (2006); Chambers et al. (2007) and the first two versions of TempEval (Verhagen et al., 2007, 2010) simplified the task by considering only six relation types. They combined the pair of relation types that are the inverse of each other and ignored the relations during and during inv. Then TempEval-3 (Uzzaman et al., 2013) extended the task to complete 14 class classification problem and all later works have considered all 14 relations. Our model performs 14-class classification following the recent works, as this is arguably more challenging (Ng, 2013). Also, we consider gold annotated event pairs, mainly because the corpus is small and distribution of relations is very skewed. All previous works focusing on the problem of classifying temporal relation types assumed gold annotation."}, {"heading": "3.2 Dataset", "text": "We have used TimeBank corpus v1.2 for training and evaluating our model. The corpus consists of 14 temporal relations between 2308 event pairs, which are within the same sentence. These relations (Saur\u0131 et al., 2006) are simultaneous, before, after, ibefore, iafter, begins, begun by, ends, ended by, includes, is included, during, during inv, identity. Six pairs among them are inverse of each other and other two types are commutative (e1Re2 \u2261 e2Re1, R \u2208 {identical, simultaneous}). Our sequential model requires that relation\nshould always be between e1 and e2, where e1 occurs before e2 in the sentence. Therefore, before extracting the sequence, we inverted the relation types in cases where relation type was annotated in opposite order. Final distribution of dataset is given in Table 1."}, {"heading": "3.3 Extracting Context Word Sequence", "text": "First, we extract words that are in the dependency path between two event mentions. However, event pairs can be very far in a sentence and are involved in complex syntactic structures. Therefore, we also apply two heuristic rules to deal with complex syntactic structures, e.g., two event mentions are in separate clauses and have a punctuation sign in their context. We describe our specific rules below. We used the Stanford parser (Chen and Manning, 2014) for generating dependency relations and parts-of-speech tags and all notations follow enhanced universal dependencies (De Marneffe and Manning, 2008).\nRule 1 (punctuation): Comma directly influences the meaning in text and omitting it may alter the meaning of phrase. Therefore, include comma if it precedes or follows e1, e2 or their modifiers.\nRule 2 (children): Modifiers like now, then, will, yesterday, subsequent, when, was, etc. contains information on the temporal order of events and help in grounding events to the timeline. These modifiers are often related to event mentions with a specific class of dependency relations. Include all such children of e1, e2 and other words in the path between them, which are connected with dependency relations nmod:tmod, mark, case, aux, conj, expl, cc, cop, amod, advmod, punct, ref."}, {"heading": "3.4 Sequences and Classifier", "text": "We form three sequences on the extracted context words (with t words), which are based on (i) parts-of-speech tags: PT = p1, p2, ..., pt (ii) dependency relations: DT = d1, d2, ..., dn2 and (iii) word forms: WT = w1, w2, ..., wt.\nWe transform each pi and di to a one-hot vector and each wi to a pre-trained embedding vector (Pennington et al., 2014). Then each sequence of vectors are encoded using their corresponding forward (LSTMf ) and backward (LSTMb) LSTM layers.\nClassifier: Figure 2 shows an overview of our model. It consists of six LSTM (Hochreiter and\n2we only consider dependency relations for words in path connecting e1 and e2.\nSchmidhuber, 1997) layers, three of them encode feature sequences in forward order and remaining in reverse order. LSTM layers for POS tag and dependency relation have 50 neurons and have dropouts of 0.20. LSTM layers for word form have 100 neurons and have dropout of 0.25. All LSTM layers use \u2019tanh\u2019 activation function. Forward and backward embeddings of all sequences are concatenated and fed into another neural layer with 14 neurons corresponding to 14 fine-grained temporal relations. This neural layer uses softmax activation function. We train model for 100 iterations using rmsprop optimizer on batch size of 100 and error defined by categorical crossentropy (Chollet, 2015) ."}, {"heading": "4 Evaluation", "text": "We evaluate our model using accuracy which has been used in previous research works for temporal relation classification. We also compare model performance using per-class F-score and macro Fscore. We briefly describe all the systems we have used for evaluation.\nMajority Class: assigns \u201cafter\u201d relation to all event pairs.\nUnidirectional LSTMs: use single LSTM layer to encode each sequence (POS tags, dependency relation and word forms) individually for extracted phrase in forward order.\nBidirectional LSTMs: use two LSTM layers to encode each sequence individually, taken from POS tags, dependency and word forms sequences. The first layer encodes sequence in forward and second in reverse order.\n2 Sequences: bi-directional LSTM based models considering all combinations of two sequences taken from POS tags, dependency and word forms sequences.\nFull model: our complete sequential model considering POS, dependency and word forms sequences.\nDirect dependency path: the same as Full model except that the two heuristic rules were not applied in extracting sequences.\nBaseline I: a neural network classifier using discrete features described in Mirza and Tonelli (2014); Ng (2013). The features used are: POS tag, dependency relation, token and lemma of e1(e2); dependency relations between e1(e2) and their children; binary features indicating if e1 and e2 are related with the \u2019happensbefore\u2019 or the \u2019similar\u2019 relation according to VerbOcean (Chklovski and Pantel, 2004), if e1 and e2 have the same POS tag, or if e1(e2) is the root and e1 modifies (or governs) e2; the dependency relation between e1 and e2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e1(e2); signal words (Derczynski and Gaizauskas, 2012) and entity distance between e1 and e2. These features are concatenated and fed into an output neural layer with 14 neurons.\nBaseline II: a neural network classifier using POS tags and word forms of words in the surface path as input. The surface path consists of words that lie in between two event mentions based on the original sentence. The classifier uses four LSTM layers to encode both POS tag and word sequences in forward and backward order. The output neural layer and parameters for all LSTM layers are kept the same as the Full model.\nBaseline III: a neural network classifier based on event embeddings for both event mentions that were learned using bidirectional LSTMs (Kiperwasser and Goldberg, 2016). The learning uses two LSTM layers, each with 150 neurons and dropout of 0.2, to embed the forward and backward representations for each event mention. The input to LSTM layers are sequences of concatenated word embeddings and POS tags; each sequence corresponding to 19 context words to the left or to the right side of an event mention for the forward or the backward LSTM layer respectively. Event embeddings are then concatenated and fed into an output neural layer with 14 neurons.\nAll baselines are trained using rmsprop optimizer on an objective function defined by categorical cross entropy and their output layer uses softmax activation function."}, {"heading": "4.1 Results and Discussion", "text": "Table 2 reports accuracy scores for all the systems. We see that simple sequential models outperform the strong feature based system, Baseline I, which used various discrete features. Note that dependency relation and POS tag sequences alone achieve reasonably high accuracies. This implies that an important aspect of temporal relation is contained in the syntactic context of event mentions. Moreover, Mirza and Tonelli (2014) observed that discrete features based on dependency parse tree did not contribute to improving their classifier\u2019s accuracy. On the contrary, using\nthe sequence of dependency relations yields a high accuracy in our setting which signifies the advantages of using sequential representations for this task. Our Full Model achieves a performance gain of 11.35% over Baseline I.\nWe developed two more baselines (Baseline II and III) that do not require syntactic information as well as the Direct dependency path model that used no rules. The Full Model outperformed them by 9.42%, 8.57% and 4.07% respectively. This affirms that the most useful syntactic and semantic structures are derived along dependency paths and additional context words, including prepositions, signal words and punctuations that are indirectly attached to event words, entail evidence on temporal relations as well.\nTable 3 compares precision, recall and F1 scores of our Full Model with Baseline I. Our model performs reasonably well compared to the baseline system for most of the classes. In addition, it is able to identify relations present in small proportion like begun by, ibefore, iafter etc., which the baseline system couldn\u2019t identify. A similar observation was also reported by Mirza and Tonelli (2014) that relation types begins, ibefore, ends and during are difficult to identify using feature based systems, which often generate false positives for before and after relations."}, {"heading": "5 Conclusion and Future work", "text": "In this paper, we have focused on modeling syntactic structural information and compositional semantics of contexts in predicting temporal relations between events in the same sentence. Our approach extracts lexical and syntactic sequences from contexts between two events and feed them to recurrent neural nets. The evaluation shows that our sequential models are promising in distinguishing among fine-grained temporal relations.\nIn the future, we will extend our sequential models to predict temporal relations for event pairs spanning across multiple sentences, for instance by incorporating discourse relations between sentences in a sequence."}, {"heading": "Acknowledgments", "text": "We want to thank our anonymous reviewers for their insightful review comments and suggestions that helped making evaluations more extensive."}], "references": [{"title": "Cleartk-timeml: A minimalist approach to tempeval 2013", "author": ["Steven Bethard."], "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM), volume 2, pages 10\u201314.", "citeRegEx": "Bethard.,? 2013", "shortCiteRegEx": "Bethard.", "year": 2013}, {"title": "Cu-tmp: Temporal relation classification using syntactic and semantic features", "author": ["Steven Bethard", "James H Martin."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, pages 129\u2013132. Association for Computational Linguis-", "citeRegEx": "Bethard and Martin.,? 2007", "shortCiteRegEx": "Bethard and Martin.", "year": 2007}, {"title": "Navytime: Event and time ordering from raw text", "author": ["Nathanael Chambers."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Chambers.,? 2013", "shortCiteRegEx": "Chambers.", "year": 2013}, {"title": "Classifying temporal relations between events", "author": ["Nathanael Chambers", "Shan Wang", "Dan Jurafsky."], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 173\u2013176. Association for Computa-", "citeRegEx": "Chambers et al\\.,? 2007", "shortCiteRegEx": "Chambers et al\\.", "year": 2007}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Naist", "author": ["Yuchang Cheng", "Masayuki Asahara", "Yuji Matsumoto."], "venue": "japan: Temporal relation identification using dependency parsed tree. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 245\u2013248. Association for Com-", "citeRegEx": "Cheng et al\\.,? 2007", "shortCiteRegEx": "Cheng et al\\.", "year": 2007}, {"title": "Verbocean: Mining the web for fine-grained semantic verb relations", "author": ["Timothy Chklovski", "Patrick Pantel."], "venue": "EMNLP, volume 4, pages 33\u201340.", "citeRegEx": "Chklovski and Pantel.,? 2004", "shortCiteRegEx": "Chklovski and Pantel.", "year": 2004}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Stanford typed dependencies manual", "author": ["Marie-Catherine De Marneffe", "Christopher D Manning."], "venue": "Technical report, Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Using signals to improve automatic classification of temporal relations", "author": ["Leon Derczynski", "Robert Gaizauskas."], "venue": "arXiv preprint arXiv:1203.5055.", "citeRegEx": "Derczynski and Gaizauskas.,? 2012", "shortCiteRegEx": "Derczynski and Gaizauskas.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1603.04351.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Extracting narrative timelines as temporal dependency structures", "author": ["Oleksandr Kolomiyets", "Steven Bethard", "MarieFrancine Moens."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-", "citeRegEx": "Kolomiyets et al\\.,? 2012", "shortCiteRegEx": "Kolomiyets et al\\.", "year": 2012}, {"title": "Uttime: Temporal relation classification using deep syntactic features", "author": ["Natsuda Laokulrat", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama"], "venue": null, "citeRegEx": "Laokulrat et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Laokulrat et al\\.", "year": 2013}, {"title": "Machine learning of temporal relations", "author": ["Inderjeet Mani", "Marc Verhagen", "Ben Wellner", "Chong Min Lee", "James Pustejovsky."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting", "citeRegEx": "Mani et al\\.,? 2006", "shortCiteRegEx": "Mani et al\\.", "year": 2006}, {"title": "Classifying temporal relations with simple features", "author": ["Paramita Mirza", "Sara Tonelli."], "venue": "EACL, volume 14, pages 308\u2013317.", "citeRegEx": "Mirza and Tonelli.,? 2014", "shortCiteRegEx": "Mirza and Tonelli.", "year": 2014}, {"title": "Dependency parsing features for semantic parsing", "author": ["Will Monroe", "Yushi Wang"], "venue": null, "citeRegEx": "Monroe and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Monroe and Wang.", "year": 2014}, {"title": "Exploiting discourse analysis for article-wide temporal classification", "author": ["Jun-Ping Ng", "Min-Yen Kan", "Ziheng Lin", "Vanessa Wei Feng", "Bin Chen", "Jian Su", "Chew Lim Tan."], "venue": "EMNLP, pages 12\u201323.", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "Classifying temporal relations with rich linguistic knowledge", "author": ["Vincent Ng"], "venue": null, "citeRegEx": "Ng.,? \\Q2013\\E", "shortCiteRegEx": "Ng.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Transforming dependency structures to logical forms for semantic parsing", "author": ["Siva Reddy", "Oscar T\u00e4ckstr\u00f6m", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata."], "venue": "Transactions of the Association for Computational", "citeRegEx": "Reddy et al\\.,? 2016", "shortCiteRegEx": "Reddy et al\\.", "year": 2016}, {"title": "Timeml annotation guidelines", "author": ["Roser Saur\u0131", "Jessica Littman", "Bob Knippen", "Robert Gaizauskas", "Andrea Setzer", "James Pustejovsky"], "venue": null, "citeRegEx": "Saur\u0131 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Saur\u0131 et al\\.", "year": 2006}, {"title": "Tempeval-3: Evaluating events, time expressions, and temporal relations", "author": ["Naushad UzZaman", "Hector Llorens", "James Allen", "Leon Derczynski", "Marc Verhagen", "James Pustejovsky."], "venue": "arXiv preprint arXiv:1206.5333.", "citeRegEx": "UzZaman et al\\.,? 2012", "shortCiteRegEx": "UzZaman et al\\.", "year": 2012}, {"title": "Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations", "author": ["Naushad Uzzaman", "Hector Llorens", "Leon Derczynski", "Marc Verhagen", "James Allen", "James Pustejovsky"], "venue": null, "citeRegEx": "Uzzaman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uzzaman et al\\.", "year": 2013}, {"title": "Semeval-2007 task 15: Tempeval temporal relation identification", "author": ["Marc Verhagen", "Robert Gaizauskas", "Frank Schilder", "Mark Hepple", "Graham Katz", "James Pustejovsky."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Verhagen et al\\.,? 2007", "shortCiteRegEx": "Verhagen et al\\.", "year": 2007}, {"title": "Semeval-2010 task 13: Tempeval-2", "author": ["Marc Verhagen", "Roser Sauri", "Tommaso Caselli", "James Pustejovsky."], "venue": "Proceedings of the 5th international workshop on semantic evaluation, pages 57\u201362. Association for Computational Linguistics.", "citeRegEx": "Verhagen et al\\.,? 2010", "shortCiteRegEx": "Verhagen et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification.", "startOffset": 6, "endOffset": 29}, {"referenceID": 5, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 1, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 22, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 0, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 12, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 2, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 13, "context": "ture sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013).", "startOffset": 10, "endOffset": 158}, {"referenceID": 0, "context": ", 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs", "startOffset": 8, "endOffset": 167}, {"referenceID": 13, "context": "Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "Ng et al. (2013) also showed the effectiveness of different discourse analysis frameworks for this task.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "Later Mirza and Tonelli (2014) showed that a simpler approach based on lexico-syntactic features achieved results comparable to Ng (2013).", "startOffset": 6, "endOffset": 31}, {"referenceID": 15, "context": "Later Mirza and Tonelli (2014) showed that a simpler approach based on lexico-syntactic features achieved results comparable to Ng (2013). They also reported that dependency order between events, either governordependent or dependent-governor, was not useful in their experiments.", "startOffset": 6, "endOffset": 138}, {"referenceID": 12, "context": "Early works on temporal relation classification Mani et al. (2006); Chambers et al.", "startOffset": 48, "endOffset": 67}, {"referenceID": 2, "context": "(2006); Chambers et al. (2007) and the first two versions of TempEval (Verhagen et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 23, "context": "Then TempEval-3 (Uzzaman et al., 2013) extended the task to complete 14 class classification problem and all later works have considered", "startOffset": 16, "endOffset": 38}, {"referenceID": 18, "context": "Our model performs 14-class classification following the recent works, as this is arguably more challenging (Ng, 2013).", "startOffset": 108, "endOffset": 118}, {"referenceID": 21, "context": "relations (Saur\u0131 et al., 2006) are simultaneous,", "startOffset": 10, "endOffset": 30}, {"referenceID": 19, "context": "We transform each pi and di to a one-hot vector and each wi to a pre-trained embedding vector (Pennington et al., 2014).", "startOffset": 94, "endOffset": 119}, {"referenceID": 7, "context": "entropy (Chollet, 2015) .", "startOffset": 8, "endOffset": 23}, {"referenceID": 6, "context": "The features used are: POS tag, dependency relation, token and lemma of e1(e2); dependency relations between e1(e2) and their children; binary features indicating if e1 and e2 are related with the \u2019happensbefore\u2019 or the \u2019similar\u2019 relation according to VerbOcean (Chklovski and Pantel, 2004), if e1 and e2 have the same POS tag, or if e1(e2) is the root and e1 modifies (or governs) e2; the dependency relation between e1 and e2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e1(e2); signal words (Derczynski and Gaizauskas, 2012) and entity distance between e1 and e2.", "startOffset": 262, "endOffset": 290}, {"referenceID": 9, "context": "The features used are: POS tag, dependency relation, token and lemma of e1(e2); dependency relations between e1(e2) and their children; binary features indicating if e1 and e2 are related with the \u2019happensbefore\u2019 or the \u2019similar\u2019 relation according to VerbOcean (Chklovski and Pantel, 2004), if e1 and e2 have the same POS tag, or if e1(e2) is the root and e1 modifies (or governs) e2; the dependency relation between e1 and e2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e1(e2); signal words (Derczynski and Gaizauskas, 2012) and entity distance between e1 and e2.", "startOffset": 547, "endOffset": 580}, {"referenceID": 16, "context": "Tonelli (2014); Ng (2013). The features used are: POS tag, dependency relation, token and lemma of e1(e2); dependency relations between e1(e2) and their children; binary features indicating if e1 and e2 are related with the \u2019happensbefore\u2019 or the \u2019similar\u2019 relation according to VerbOcean (Chklovski and Pantel, 2004), if e1 and e2 have the same POS tag, or if e1(e2) is the root and e1 modifies (or governs) e2; the dependency relation between e1 and e2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e1(e2); signal words (Derczynski and Gaizauskas, 2012) and entity distance between e1 and e2.", "startOffset": 16, "endOffset": 26}, {"referenceID": 11, "context": "Baseline III: a neural network classifier based on event embeddings for both event mentions that were learned using bidirectional LSTMs (Kiperwasser and Goldberg, 2016).", "startOffset": 136, "endOffset": 168}, {"referenceID": 15, "context": "Moreover, Mirza and Tonelli (2014) observed that discrete features based on depen-", "startOffset": 10, "endOffset": 35}], "year": 2017, "abstractText": "We present a sequential model for temporal relation classification between intrasentence events. The key observation is that the overall syntactic structure and compositional meanings of the multi-word context between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The neural nets learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.", "creator": "LaTeX with hyperref package"}}}