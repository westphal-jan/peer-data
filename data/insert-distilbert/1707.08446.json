{"id": "1707.08446", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2017", "title": "All that is English may be Hindi: Enhancing language identification through automatic ranking of likeliness of word borrowing in social media", "abstract": "in this paper, we simply present a set representation of computational methods to identify the likeliness of a word being borrowed, based systematically on the signals from social media. in terms of spearman correlation coefficient values, our methods perform more than two trillion times better ( nearly 0. 62 ) in predicting the borrowing likeliness compared to the best performing using baseline ( nearly 0. 26 ) reported in literature. based on this likeliness estimate range we asked annotators to re - value annotate the language tags of foreign words in predominantly native contexts. in 88 percent of cases the annotators felt that recently the foreign language attribute tag should be replaced by static native language tag, thus indicating maybe a huge dynamic scope for improvement objectives of automatic language identification systems.", "histories": [["v1", "Tue, 25 Jul 2017 04:17:42 GMT  (511kb)", "https://arxiv.org/abs/1707.08446v1", "11 pages. arXiv admin note: substantial text overlap witharXiv:1703.05122"], ["v2", "Sat, 29 Jul 2017 04:47:16 GMT  (511kb)", "http://arxiv.org/abs/1707.08446v2", "11 pages, accepted in the 2017 conference on Empirical Methods on Natural Language Processing(EMNLP 2017) arXiv admin note: substantial text overlap witharXiv:1703.05122"]], "COMMENTS": "11 pages. arXiv admin note: substantial text overlap witharXiv:1703.05122", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jasabanta patro", "bidisha samanta", "saurabh singh", "abhipsa basu", "prithwish mukherjee", "monojit choudhury", "animesh mukherjee"], "accepted": true, "id": "1707.08446"}, "pdf": {"name": "1707.08446.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jasabantapatro@iitkgp.ac.in,", "monojitc@microsoft.com,", "animeshm@cse.iitkgp.ernet.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 7.\n08 44\n6v 2\n[ cs\n.C L\n] 2\n9 Ju\nl 2 01\nIn this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from social media. In terms of Spearman\u2019s correlation values, our methods perform more than two times better (\u223c 0.62) in predicting the borrowing likeliness compared to the best performing baseline (\u223c 0.26) reported in literature. Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts. In 88% of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems."}, {"heading": "1 Introduction", "text": "In social media communication, multilingual people often switch between languages, a phenomenon known as code-switching or codemixing (Auer, 1984). This makes language identification and tagging, which is perhaps a pre-requisite for almost all other language processing tasks that follow, a challenging problem (Barman et al., 2014). In code-mixing people are subconsciously aware of the foreign origin of the code-mixed word or the phrase. A related but linguistically and cognitively distinct phenomenon is lexical borrowing (or simply, borrowing), where a word or phrase from a foreign language say L2 is used as a part of the vocabulary of native language say L1. For instance, in Dutch, the English word \u201csale\u201d is now used more frequently\nthan the Dutch equivalent \u201cuitverkoop\u201d. Some English words like \u201cshop\u201d are even inflected in Dutch as \u201cshoppen\u201d and heavily used. While it is difficult in general to ascertain whether a foreign word or phrase used in an utterance is borrowed or just an instance of code-mixing (Bali et al., 2014), one tell tale sign is that only proficient multilinguals can code-mix, while even monolingual speakers can use borrowed words because, by definition, these are part of the vocabulary of a language. In other words, just because an English speaker understands and uses the word \u201ctortilla\u201d does not imply that she can speak or understand Spanish. A borrowed word from L2 initially appears frequently in speech, then gradually in print media like newspaper and finally it loses its origin\u2019s identity and is used in L1 resulting in an inclusion in the dictionary of L1 (Myers-Scotton, 2002; Thomason, 2003). Borrowed words often take several years before they formally become part of L1 dictionary. This motivates our research question \u201cis early-stage automatic identification of likely to be borrowed words possible?\u201d. This is known to be a hard problem because (i) it is a socio-linguistic phenomenon closely related to acceptability and frequency, (ii) borrowing is a dynamic process; new borrowed words enter the lexicon of a language as old words, both native and borrowed, might slowly fade away from usage, and (iii) it is a population level phenomenon that necessitates data from a large portion of the population unlike standard natural language corpora that typically comes from a very small set of authors. Automatic identification of borrowed words in social media content (SMC) can improve language tagging by recommending the tagger to tag the language of the borrowed words as L1 instead\nof L2. The above reasons motivate us to resort to the social media (in particular, Twitter), where a large population of bilingual/multilingual speakers are known to often tweet in code-mixed colloquial languages (Carter et al., 2013; Solorio et al., 2014; Vyas et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017). We designed our methodology to work for any pair of languages L1 and L2 subject to the availability of sufficient SMC. In the current study, we consider Hindi asL1 and English as L2.\nThe main stages of our research are as follows:\nMetrics to quantify the likeliness of borrowing from social media signals: We define three novel and closely similar metrics that serve as social signals indicating the likeliness of borrowing. We compare the likeliness of borrowing as predicted by our model and a baseline model with that from the ground truth obtained from human judges.\nGround truth generation: We launch an extensive survey among 58 human judges of various age groups and various educational backgrounds to collect responses indicating if each of the candidate foreign word is likely borrowed.\nApplication: We randomly selected some words that have a high, low and medium borrowing likeliness as predicted by our metrics. Further, we randomly selected one tweet for each of the chosen words. The chosen words in almost all of these tweets have L2 as their language tag while a majority of the surrounding words have a tag L1. We asked expert annotators to re-evaluate the language tags of the chosen words and indicate if they would prefer to switch this tag from L2 from L1.\nFinally, our key results are outlined below:\n1. We obtained the Spearman\u2019s rank correlation between the ground-truth ranking and the ranking based on our metrics as \u223c 0.62 for all the three variants which is more than double the value (\u223c 0.26) if we use the most competitive baseline (Bali et al., 2014) available in the literature.\n2. Interestingly, the responses of the judges in the age group below 30 seem to correspond even better with our metrics. Since language change is brought about mostly by the younger population, this might possibly mean that our metrics are able to capture the early signals of borrowing.\n3. Those users that mix languages the least in their tweets present the best signals of borrowing in case they do mix the languages (correlation of our metrics estimated from the tweets of these users\nwith that of the ground truth is \u223c 0.65).\n4. Finally, we obtain an excellent re-annotation accuracy of 88% for the words falling in the surely borrowed category as predicted by our metrics."}, {"heading": "2 Related work", "text": "In linguistics code-mixing and borrowing have been studied under the broader scope of language change and evolution. Linguists have for a long time focused on the sociological and the conversational necessity of borrowing and mixing in multilingual communities (see Auer (1984) and Muysken (1996) for a review). In particular, Sankoff et al. (1990) describes the complexity of choosing features that are indicative of borrowing. This work further showed that it is not always true that only highly frequent words are borrowed; nonce words could also be borrowed along with the frequent words. More recently, (Nzai et al., 2014) analyzed the formal conversation of Spanish-English multilingual people and found that code mixing/borrowing is not only restricted to daily speech but is also prevalent in formal conversations. (Hadei, 2016) showed that phonological integration could be evaluated to understand the phenomenon of word borrowing. Along similar lines, (Sebonde, 2014) showed morphological and syntactic features could be good indicators for numerical borrowings. (Senaratne, 2013) reported that in many languages English words are likely to be borrowed in both formal and semi-formal text.\nMixing in computer mediated communication and social media: (Sotillo, 2012) investigated various types of code-mixing in a corpora of 880 SMS text messages. The author observed that most often mixing takes place at the beginning of a sentence as well as through simple insertions. Similar observations about chat and email messages have been reported in (Bock, 2013; Negro\u0301n, 2009). However, studies of code-mixing with Chinese-English bilinguals from Hong Kong (Li, 2009) and Macao (San, 2009) brings forth results that contrast the aforementioned findings and indicate that in these societies code-mixing is driven more by linguistic than social motivations.\nRecently, the advent of social media has immensely propelled the research on code-mixing and borrowing as a dynamic social phenomena. (Hidayat, 2012) noted that in Facebook, users mostly preferred inter-sentential mix-\ning and showed that 45% of the mixing originated from real lexical needs, 40% was used for conversations on a particular topic and the rest 5% for content clarification. In contrast, (Das and Gamba\u0308ck, 2014) showed that in case of Facebook messages, intra-sentential mixing accounted for more than half of the cases while inter-sentential mixing accounted only for about one-third of the cases. In fact, in the First Workshop on Computational Approaches to Code Switching a shared task on code-mixing in tweets was launched and four different codemixed corpora were collected from Twitter as a part of the shared task (Solorio et al., 2014). Language identification task has also been handled for English-Hindi and English-Bengali code-mixed tweets in (Das and Gamba\u0308ck, 2013). Part-ofspeech tagging have been recently done for codemixed English-Hindi tweets (Solorio and Liu, 2008; Vyas et al., 2014).\nDiachronic studies: As an aside, it is interesting to note that the availability of huge volumes of timestamped data (tweet streams, digitized books) is now making it possible to study various linguistic phenomena quantitatively over different timescales. For instance, (Sagi et al., 2009) uses latent semantic analysis for detection and tracking of changes in word meaning, whereas (Frermann and Lapata, 2016) presents a Bayesian approach for the same problem. (Peirsman et al., 2010) presents a distributed model for automatic identification of lexical variation between language varieties. (Bamman and Crane, 2011) discusses a method for automatically identifying word sense variation in a dated collection of historical books . (Mitra et al., 2014) presents a computational method for automatic identification of change in word senses across different timescales. (Cook et al., 2014) presents a method for novel sense identification of words over time.\nDespite these diverse and rich research agendas in the field of code-switching and lexical dynamics, there has not been much attempt to quantify the likeliness of borrowing of foreign words in a language. The only work that makes an attempt in this direction is (Bali et al., 2014), which is described in detail in Sec 3.1. One of the primary challenges faced by any quantitative research on lexical borrowing is that borrowing is a social phenomenon, and therefore, it is difficult to identify suitable indicators of such a lexical diffusion pro-\ncess unless one has access to a large populationlevel data. In this work, we show for the first time how certain simple and closely related signals encoding the language usage of social media users can help us construe appropriate metrics to quantify the likeliness of borrowing of a foreign word."}, {"heading": "3 Methodology", "text": "In this section, we present the baseline metric and propose three new metrics that quantify the likeliness of borrowing."}, {"heading": "3.1 Baseline metric", "text": "Baseline metric \u2013 We consider the log( FL2 FL1 ) value proposed in (Bali et al., 2014) as the baseline metric. Here FL2 denotes the frequency of the L1 transliterated form of the word w in the standard L1 newspaper corpus. FL1 , on the other hand, denotes the frequency of the L1 translation of the word w in the same newspaper corpus. For our experiments discussed in the later sections, both the transliteration and the translation of the words have been done by a set of volunteers who are native L1 speakers. The authors in (Bali et al., 2014) claim that the more positive the value of this metric is for a word w, the higher is the likeliness of its being borrowed. The more negative the value is, the higher are the chances that the word w is an instance of code-mixing. Ranking \u2013 Based on the values obtained from the above metric for a set of target words, we rank these words; words with high positive values feature at the top of the rank list and words with high negative values feature at the bottom of the list. For two words having the same log( FL2 FL1 ) value, we resolve the conflict by assigning each of these the average of their two rank positions. In a subsequent section, we shall compare this rank list with the one obtained from the ground truth responses."}, {"heading": "3.2 Proposed metric", "text": "In this section, we present three novel and closely related metrics based on the language usage patterns of the users of social media (in specific, Twitter). In order to define our metrics, we need all the words to be language tagged. The different tags that a word can have are: L1, L2, NE (Named Entity) and Others. Based on the word level tag, we also create a tweet level tag as follows:\n1. L1: Almost every word (> 90%) in the tweet is tagged as L1.\n2. L2: Almost every word (> 90%) in the tweet is tagged as L2. 3. CML1: Code-mixed tweet but majority (i.e.,\n> 50%) of the words are tagged as L1. 4. CML2: Code-mixed tweet but majority (i.e.,\n> 50%) of the words are tagged as L2. 5. CMEQ: Code-mixed tweet having very sim-\nilar number of words tagged as L1 and L2 respectively. 6. Code Switched: There is a trail of L1 words\nfollowed by a trail of L2 words or vice versa. Using the above classification, we define the\nfollowing metrics: Unique User Ratio (UUR) \u2013 The Unique User Ratio for word usage across languages is defined as follows:\nUUR(w) = UL1 + UCML1\nUL2 (1)\nwhere UL1 (UL2 , UCML1) is the number of unique users who have used the word w in a L1 (L2, CML1) tweet at least once. Unique Tweet Ratio (UTR) \u2013 The Unique Tweet Ratio for word usage across languages is defined as follows:\nUTR(w) = TL1 + TCML1\nTL2 (2)\nwhere TL1 (TL2 , TCML1) is the total number of L1 (L2, CML1) tweets which contain the word w. Unique Phrase Ratio (UPR) \u2013 The Unique Phrase Ratio for word usage across languages is defined as follows:\nUPR(w) = PL1 PL2\n(3)\nwhere PL1 (PL2 ) is the number of L1 (L2) phrases which contain the word w. Note that unlike the definitions of UUR and UTR that exploit the word level language tags, the definition of UPR exploits the phrase level language tags. Ranking \u2013 We prepare a separate rank list of the target words based on each of the three proposed metrics \u2013 UUR, UTR and UPR. The higher the value of each of this metric the higher is the likeliness of the word w to be borrowed and higher up it is in the rank list. In a subsequent section, we shall compare these rank lists with the one prepared from the ground truth responses."}, {"heading": "4 Experiments", "text": "In this section we discuss the dataset for our experiments, the evaluation criteria and the ground truth preparation scheme."}, {"heading": "4.1 Datasets and preprocessing", "text": "In this study, we consider code-mixed tweets gathered from Hindi-English bilingual Twitter users in order to study the effectiveness of our proposed metrics. The native language L1 is Hindi and the foreign language L2 is English. To bootstrap the data collection process, we used the language tagged tweets presented in (Rudra et al., 2016). In addition to this, we also crawled tweets (between Nov 2015 and Jan 2016) related to 28 hashtags representing different Indian contexts covering important topics such as sports, religion, movies, politics etc. This process results in a set of 811981 tweets. We language-tag (see details later in this section) each tweet so crawled and find that there are 3577 users who use mixed language for tweeting. Next, we systematically crawled the time lines of these 3577 users between Feb 2016 and March 2016 to gather more mixed language tweets. Using this two step process we collected a total of 1550714 distinct tweets. From this data, we filtered out tweets that are not written in romanized script, tweets having only URLs and tweets having empty content. Post filtering we obtained 725173, tweets which we use for the rest of the analysis. The datasets can be downloaded from http://cnerg.org/borrow\nLanguage tagging: We tag each word in a tweet with the language of its origin using the method outlined in (Gella et al., 2013). Hi represents a predominantly Hindi tweet, En represents a predominantly English tweet, CMH (CME) represents code-mixed tweets with more Hindi (English) words, CMEQ represents code-mixed tweets with almost equal number of Hindi and English words and CS represents code-switched tweets (the number and % of tweets in each of the above six categories are presented in the supplementary material). Like the word level, the tagger also provides a phrase level language tag. Once again, the different tags that an entire phrase can have are: En, Hi and Oth (Other). The metrics defined in the previous section are computed using these language tags.\nNewspaper dataset for the baseline: As we had discussed in the previous section, for the construction of the baseline ranking we need to resort to counting the frequency of the foreign words (i.e., English words) and their Hindi translations in a newspaper corpus as has been outlined in (Bali et al., 2014). For this purpose, we use the\nFIRE dataset built from the Hindi Jagaran newspaper corpus1 which is written in Devanagari script."}, {"heading": "4.2 Target word selection", "text": "We first compute the most frequent foreign (i.e., English words) in our tweet corpus. Since we are interested in the frequency of the English word only when it appears as a foreign word we do not consider the (i) Hi tweets since they do not have any foreign word, (ii) En tweets since here the English words are not foreign words and the (iii) code-switched tweets. Based on the frequency of usage of English as a foreign word, we select the top 1000 English words. Removal of stop words and text normalization leaves beyond 230 nouns (see supplementary material for the list of words). Final selection of target words: In language processing, context plays an important role in understanding different properties of a word. For our study, we also attempt to use the language tags as features of the context words for a given target word. Our hypothesis here is that there should exist classes of words that have similar context features and the likelihood of being borrowed in each class should be different. For example, when an English word is surrounded by mostly Hindi words it seems to be more likely borrowed. We present two examples in the box below to illustrate this.\nExample I: @****** Welcome. Film jaroor dekhna. Nahi to injection ready hai. Example II: @***** Trust @***** Kuch to ache se karo sirji.... Har jagah bhaagte rehna is not a good thing.\nIn Example I the English word \u201cfilm\u201d is surrounded by mostly Hindi words. On the other hand, in Example II the English word \u201cthing\u201d is surrounded mostly by English words. Note that the word \u201cfilm\u201d is very commonly used by Hindi monolingual speakers and is therefore highly likely to have been borrowed unlike the English word \u201cthing\u201d which is arguably an instance of mixing. This socio-linguistic difference seems to be very appropriately captured by the language tag of the surrounding words of these two words in the respective tweets. Based on this hypothesis we arrange the 230 words into contextually\n1Jagaran corpus: http:/fire.irsi.res.in/fire/static/data\nsimilar groups (see supplementary material for the grouping details). Finally, using the baseline metric log(FE FH\n) (E: English, H: Hindi), we proportionately choose words from these groups as follows: Words with very high or very low values of log(FE FH ) (hlws) \u2013 we select words having the highest and the lowest values of log(FE FH\n) from each of the context groups. This constitutes a set of 30 words. Note that these words are baseline-biased and therefore the metric should be able to discriminate them well. Words with medium values of log(FE FH\n) (mws) \u2013 we selected 27 words having not so high and not so low log(FE FH\n) at uniformly at random. Full set of words (full) \u2013 Thus, in total we selected 57 target words for the purpose of our evaluation. We present these words in the box below. Baseline-biased words \u2013 \u2019thing\u2019, \u2019way\u2019, \u2019woman\u2019, \u2019press\u2019, \u2019wrong\u2019, \u2019well\u2019, \u2019matter\u2019, \u2019reason\u2019, \u2019question\u2019, \u2019guy\u2019, \u2019moment\u2019, \u2019week\u2019, \u2019luck\u2019, \u2019president\u2019, \u2019body\u2019, \u2019job\u2019, \u2019car\u2019, \u2019god\u2019, \u2019gift\u2019, \u2019status\u2019, \u2019university\u2019, \u2019lyrics\u2019, \u2019road\u2019, \u2019politics\u2019, \u2019parliament\u2019, \u2019review\u2019, \u2019scene\u2019, \u2019seat\u2019, \u2019film\u2019, \u2019degree\u2019 Randomly selected words \u2013 \u2019people\u2019, \u2019play\u2019, \u2019house\u2019, \u2019service\u2019, \u2019rest\u2019, \u2019boy\u2019, \u2019month\u2019, \u2019money\u2019, \u2019cool\u2019, \u2019development\u2019, \u2019group\u2019, \u2019friend\u2019, \u2019day\u2019, \u2019performance\u2019, \u2019school\u2019, \u2019blue\u2019, \u2019room\u2019, \u2019interview\u2019, \u2019share\u2019, \u2019request\u2019, \u2019traffic\u2019, \u2019college\u2019, \u2019star\u2019, \u2019class\u2019, \u2019superstar\u2019, \u2019petrol\u2019, \u2019uncle\u2019"}, {"heading": "4.3 Evaluation criteria", "text": "We present a four step approach for evaluation as follows. Wemeasure (i) how well the UUR,UTR and UPR based ranking of the hlws set, themws set and the full set correlate with the ground truth ranking (discussed in the next section) in comparison to the rank given by the baseline metric, (ii) how well the different rank ranges obtained from our metric align with the ground truth as compared to the baseline metric, (iii) whether there are some systematic effects of the age group of the survey participants on the rank correspondence, (iv) how our metrics if computed from the tweets of users who (a) rarely mix languages, (b) almost always mix languages and (c) are in between (a) and (b), align with the ground truth. Rank correlation: We measure the standard Spearman\u2019s rank correlation (\u03c1) (Zar, 1972) pairwise between rank lists generated by (i) UUR (ii) UTR (iii) UPR (iv) baseline metric and the ground truth.\nWe shall describe the next four measurements taking UUR as the running example. The same\ncan be extended verbatim for the other two similar metrics.\nRank ranges: We split each of the three rank lists (UUR, ground truth and baseline) into five different equal-sized ranges as follows \u2013 (i) surely borrowed (SB) containing top 20% words from each list, (ii) likely borrowed (LB) containing the next 20% words from each list, (iii) borderline (BL) constituting the subsequent 20% words from each list, (iv) likely mixed (LM) comprising the next 20% words from each list and (v) surely mixed (SM) having the last 20% words from each rank list. Therefore, we have three sets of five buckets, one set each for UUR, the ground truth and the baseline based rank list.\nNext we calculate the bucket-wise correspondence between (i) the UUR and the ground truth set and (ii) the baseline and the ground truth set in terms of standard precision and recall measures. For our purpose, we adapt these measures as follows. G: ground truth bucket set, Bb: baseline bucket set, Ub: UUR bucket set; BS \u2208 {Bb, Ub}, T (type of bucket) = {SB, LB, BL, LM, SM}; bt = words in type t bucket from BS, gt = words in type t bucket from G, t \u2208 T ; tpt (no. of true positives) = |bt \u2229 gt|, fpt (no. of false positives) = |bt \u2212 gt|, tnt (no. of true negatives) = |gt \u2212 bt|; Bucket-wise precision and recall therefore: precision(bt) = tpt\nfpt+tpt ; recall(bt) = tpt tnt+tpt\nFor a given set, we obtained the overall macro precision (recall) by averaging the precision (recall) values over the five buckets. For a given set, we also obtained the overall micro precision by first adding the true positives across all the buckets and then normalizing by the sum of the true and the false positives over all the buckets. We take an equivalent approach for obtaining themicro recall.\nAge group effect: Here we construct two ground truth rank lists one using the responses of the participants with age below 30 (young population) and the other using the responses of the rest of the participants (elderly population). Next we repeat the above two evaluations considering each of the new ground truth rank lists.\nExtent of language mixing: Here we divide all the 3577 users into three categories \u2013 (i) High (users who have more than 20% of tweets as codemixed), (ii) Mid (users who have 7\u201320% of their\ntweets as code-mixed, and (iii) Low (users who have less than 7% of their tweets as code-mixed). We create three UUR based rank lists for each of these three user categories and respectively compare them with the ground truth rank list."}, {"heading": "4.4 Ground truth preparation", "text": "Since it is very difficult to obtain a suitable ground truth to validate the effectiveness of our proposed ranking scheme, we launched an online survey to collect human judgment for each of the 57 target words. Online survey We conducted the online survey2 among 58 volunteers majority of whom were either native language(Hindi) speakers or had very high proficiency in reading and writing in that language. The participants were selected from different age groups and different educational backgrounds. Every participant was asked to respond to a multiple choice question about each of the 57 target words. Therefore, for every single target word, 58 responses were gathered. The multiple choice question had the following three options and the participants were asked to select the one they preferred the most and found more natural \u2013 (i) a Hindi sentence with the target word as the only English word, (ii) the same Hindi sentence in (i) but with the target word replaced by its Hindi translation and (iii) none of the above two options. There were no time restrictions imposed while gathering the responses, i.e., the volunteers theoretically had unlimited time to decide their responses for each target word. Language preference factor For each target word, we compute a language preference factor (LPF ) defined as (CountEn \u2212 CountHi), where CountHi refers to the number of survey participants who preferred the sentence containing the Hindi translation of the target word while CountEn refers to the number of survey participants who preferred the sentence containing the target word itself. More positive values of LPF denotes higher usage of target word as compared to its Hindi translation and therefore higher likeliness of the word being borrowed.\nGround truth rank list generation We generate the ground truth rank list based on the LPF score of a target word. The word with the highest value of LPF appears at the top of the ground truth rank list and so on in that order. Tie breaking between\n2Survey portal: https://goo.gl/forms/L0kJm8BNMhRj0jA53\ntarget words having equal LPF values is done by assigning average rank to each of these words. Age group based rank list: As discussed in the previous section, we prepare the age group based rank lists by first splitting the responses of the survey participants in two groups based on their age \u2013 (i) young population (age < 30) and (ii) elderly population (age \u2265 30). For each group we then construct a separate LPF based ranking of the target words."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Correlation among rank lists", "text": "The Spearman\u2019s rank correlation coefficient (\u03c1) of the rank lists for the hlws set, themws set and the full set according to the baseline metric, UUR, UTR and UPR with respect to the ground truth metric LPF are noted in table 1. We observe that for the full set, the \u03c1 between the rank lists obtained from all the three metrics UUR, UTR, and UPR with respect to the ground truth is \u223c 0.62 which is more than double the \u03c1 (\u223c 0.26) between the baseline and the ground truth rank list. This clearly shows that the proposed metrics are able to identify the likeliness of borrowing quite accurately and far better than the baseline. Further, a remarkable observation is that our metrics outperform the baseline metric even for the hlws set that is baseline-biased. Likewise, for the mws set, our metrics outperform the baseline indicating a superior recall on arbitrary words. The ranking of the full set of words obtained from the ground truth, the baseline and the UUR metric is available in the supplementary material.\nWe present the subsequent results for the full set and the UUR metric. The results obtained for the other two metrics UTR and UPR are very similar and therefore not shown."}, {"heading": "5.2 Rank list alignment across rank ranges", "text": "The number of target words falling in each bucket across the three rank lists are the same and are\nnoted in table 2. Thus, the precision and recall as per the definition are also the same. The bucketwise precision/recall for the baseline and UUR with respect to the ground truth are noted in table 3. We observe that while in the SB bucket both the baseline and UUR perform equally well, for all the other buckets UUR massively outperforms the baseline. This implies that for the case where the likeliness of borrowing is the strongest, the baseline does as good as UUR. However, as one moves down the rank list, UUR turns out to be a considerably better predictor than the baseline. The overall macro and micro precision/recall as shown in table 4 further strengthens our observation that UUR is a better metric than the baseline."}, {"heading": "5.3 Age group based analysis", "text": "As already discussed earlier, we split the ground truth responses based on the age group of the survey participants. We split the responses into two groups \u2013 (i) young population (age < 30) and (ii) elderly population (age \u2265 30) so that there are almost equal number of responses in both the groups (see supplementary material for the exact distribution).\nRank correlation: The Spearman\u2019s rank correlation of UUR and the baseline rank lists with these two ground truth rank lists are shown in table 5. Interestingly, the correlation between UUR rank list and the young population ground truth is better\nthan the elderly population ground truth. This possibly indicates that UUR is able to predict recent borrowings more accurately. However, note that the UUR rank list has a much higher correlation with both the ground truth rank lists as compared to the baseline rank list. Rank ranges: Table 6 shows the bucket-wise precision and recall for UUR and the baseline metrics with respect to two new ground truths. For the young population once again the number of words in each bucket for all the three sets is the same thus making the values of the precision and the recall same. In fact, the precision/recall for this ground truth is exactly same as in the case of the original ground truth.\nIn contrast, when we consider the ground truth based on the responses of the elderly population, the number of words across the different buckets are different across the three sets. In this case, we observe that the precision/recall values are better for the UUR metric in SB, LB and LM buckets.\nFinally, the overall macro and micro precision and recall for both the age groups are noted in table 7. Once again, for both the young and the elderly population based ground truths, the macro andmicro precision and recall values for the UUR metric are higher compared to that of the baseline."}, {"heading": "5.4 Extent of language mixing", "text": "As mentioned earlier, we divide the set of 3577 users into three categories. The Spearman\u2019s correlation between UUR and the ground truth for each of these buckets are given in table 8. As we can see, for Low bucket the \u03c1 value is maximum.\nThis points to the fact that the signals of borrowing is strongest from the users who rarely mix languages."}, {"heading": "6 Re-annotation results", "text": "In order to conduct the re-annotation experiments we performed the following. To begin with, we ranked all the 230 English nouns in non-increasing order of their UUR values. We then randomly selected 20 words each having (i) high UUR (top 20%) values (call TOP ), (ii) low UUR (bottom 20%) values (call BOT ), and (iii) middle UUR (middle 20%) values (call MID). This makes a total of 60 words. Using this word list we extracted one tweet each that contained the (foreign) word from this list along with all other words in the tweet tagged in Hindi (Hall). We similarly prepared another such list of 60 words and extracted one tweet each in which most of the other words were tagged in Hindi (Hmost).\nWe presented the selected words and the corresponding tweets to a set of annotators and asked them to annotate these selected words once again. Over all the words, we calculated the mean (\u00b5E\u2192H) and the standard deviation (\u03c3E\u2192H ) of the fraction of cases where the annotators altered the tag of the selected word from English to Hindi. The average inter-annotator agreement (Fleiss, 1971) for our experiments was found to be as high as 0.64. For the words in the TOP list, the fraction of times the tag is altered is 0.91 (0.85) with an inter-annotator agreement of 0.84 (0.80) for the Hall (Hmost) category. In other words, on average, in as high as 88% of cases the annotators altered the tags of the words that are highly likely to be\nborrowed (i.e., TOP ) in a largely Hindi context (i.e., Hall or Hmost). Table 9 shows the fractional changes for all the other possible cases. An interesting observation is that the annotators rarely flipped the tags for the words in theBOT list (i.e., the sure mixing cases) in either of the Hall or the Hmost contexts. These results strongly support the inclusion of our metric in the design of future automatic language tagging tasks."}, {"heading": "7 Discussion and conclusion", "text": "In this paper, we proposed a few new metrics for estimating the likliness of borrowing that rely on signals from large scale social media data. Our best metric is two-fold better than the previous metric in terms of the accuracy of the prediction. There are some interesting linguistic aspects of borrowing as well as certain assumptions regarding the social media users that have important repercussions on this work and its potential extensions, which are discussed in this section.\nTypes of borrowing: Linguists define broadly three forms of borrowing, (i) cultural, (ii) core, and (iii) therapeutic borrowings. In cultural borrowing, a foreign word gets borrowed into native language to fill a lexical gap. This is because there is no equivalent native language word present to represent the same foreign word concept. For instance, the English word \u2018computer\u2019 has been borrowed in many Indian languages since it does not have a corresponding term in those languages3. In core borrowing, on the other hand, a foreign word replaces its native language translation in the native language vocabulary. This occurs due to overwhelming use of the foreign word over native language translation as a matter of prestige, ease of use etc. For example, the English word \u2018school\u2019 has become much more prevalent than its corresponding Hindi translation \u2018vidyalaya\u2019 among the native Hindi speakers. Finally, therapeutic bor-\n3Words for \u201ccomputer\u201d were coined in many Indian languages through morphological derivation from the term for \u201ccompute\u201d, however, none of these words are used in either formal or informal contexts.\nrowing refers to borrowing of words to avoid taboo and homonomy in the native language. In this paper, although we did not perform any category based studies, most of our focus was on core borrowing.\nLanguage of social media users: We assumed that if a user is predominantly using Hindi words in a tweet then the chances of him/her being a native Hindi speaker should be high, since, while the number of English native speakers in India is 0.02%, the number of Hindi native speakers is 41.03%4. This assumption has also been made in earlier studies (Rudra et al., 2016). Note that even if a user is not a native Hindi speaker but a proficient (or semi-proficient) Hindi speaker, the main results of our analysis should hold. For instance, consider two foreign words \u2018a\u2019 and \u2018b\u2019. If \u2018a\u2019 is frequently borrowed in the native language, then the proficient speaker would also tend to borrow \u2018a\u2019 similar to a native speaker. Even if due to lack of adequate native vocabulary, the nonnative speaker borrows the word \u2018b\u2019 in some cases, these spurious signals should get eliminated since we are making an aggregate level statistics over a large population.\nFuture directions: It would be interesting to understand and develop theoretical justification for the metrics. Further, it would be useful to study and classify various other linguistic phenomena closely related to core borrowing, such as: (i) loanword, where a form of a foreign word and its meaning or one component of its meaning gets borrowed, (ii) calques, where a foreign word or idiom is translated into existing words of native language, and (iii) semantic loan, where the word in the native language already exists but an additional meaning is borrowed from another language and added to existing meaning of the word.\nFinally, we would also like to incorporate our findings into other standard tasks of multilingual IR and multilingual speech synthesis (for example to render the appropriate native accent to the borrowed word)."}], "references": [{"title": "Bilingual Conversation", "author": ["P. Auer."], "venue": "John Benjamins.", "citeRegEx": "Auer.,? 1984", "shortCiteRegEx": "Auer.", "year": 1984}, {"title": "i am borrowing ya mixing?", "author": ["K. Bali", "J. Sharma", "M. Choudhury", "Y. Vyas"], "venue": null, "citeRegEx": "Bali et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bali et al\\.", "year": 2014}, {"title": "Measuring historical word sense variation", "author": ["David Bamman", "Gregory Crane."], "venue": "Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries, pages 1\u201310. ACM.", "citeRegEx": "Bamman and Crane.,? 2011", "shortCiteRegEx": "Bamman and Crane.", "year": 2011}, {"title": "Code mixing: A challenge for language identification in the language of social media", "author": ["Utsab Barman", "Amitava Das", "Joachim Wagner", "Jennifer Foster."], "venue": "EMNLP 2014, 13.", "citeRegEx": "Barman et al\\.,? 2014", "shortCiteRegEx": "Barman et al\\.", "year": 2014}, {"title": "Cyber socialising: Emerging genres and registers of intimacy among young south african students", "author": ["Z. Bock."], "venue": "Language Matters: Studies in the Languages of Africa, 44(2):68\u201391.", "citeRegEx": "Bock.,? 2013", "shortCiteRegEx": "Bock.", "year": 2013}, {"title": "Micro-blog language identification: Overcoming the limitations of short, unedited and idiomatic text", "author": ["S. Carter", "W. Weerkamp", "M. Tsagkias."], "venue": "Language Resources and Evaluation, 47(1):195\u2013 215.", "citeRegEx": "Carter et al\\.,? 2013", "shortCiteRegEx": "Carter et al\\.", "year": 2013}, {"title": "Novel word-sense identification", "author": ["Paul Cook", "Jey Han Lau", "Diana McCarthy", "Timothy Baldwin."], "venue": "COLING, pages 1624\u20131635.", "citeRegEx": "Cook et al\\.,? 2014", "shortCiteRegEx": "Cook et al\\.", "year": 2014}, {"title": "code-mixing in social media text: The last language identification frontier", "author": ["A. Das", "B. Gamb\u00e4ck"], "venue": null, "citeRegEx": "Das and Gamb\u00e4ck.,? \\Q2013\\E", "shortCiteRegEx": "Das and Gamb\u00e4ck.", "year": 2013}, {"title": "Identifying languages at the word level in code-mixed indian social media text", "author": ["A. Das", "B. Gamb\u00e4ck."], "venue": "ICON, pages 169\u2013178.", "citeRegEx": "Das and Gamb\u00e4ck.,? 2014", "shortCiteRegEx": "Das and Gamb\u00e4ck.", "year": 2014}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L Fleiss."], "venue": "Psychological bulletin, 76(5):378.", "citeRegEx": "Fleiss.,? 1971", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "A bayesian model of diachronic meaning change", "author": ["Lea Frermann", "Mirella Lapata."], "venue": "Transactions of the Association for Computational Linguistics, 4:31\u201345.", "citeRegEx": "Frermann and Lapata.,? 2016", "shortCiteRegEx": "Frermann and Lapata.", "year": 2016}, {"title": "Query word labeling and back transliteration for indian languages: Shared task system description", "author": ["S. Gella", "J. Sharma", "K. Bali."], "venue": "FIRE Working Notes, 3.", "citeRegEx": "Gella et al\\.,? 2013", "shortCiteRegEx": "Gella et al\\.", "year": 2013}, {"title": "Single word insertions as codeswitching or established borrowing", "author": ["M. Hadei"], "venue": "International Journal of Linguistics,", "citeRegEx": "Hadei.,? \\Q2016\\E", "shortCiteRegEx": "Hadei.", "year": 2016}, {"title": "An analysis of code switching used by facebookers (a case study in a social network site)", "author": ["T. Hidayat."], "venue": "BA thesis. English Education Study Program, College of Teaching and Education (STKIP), Bandung, Indonesia.", "citeRegEx": "Hidayat.,? 2012", "shortCiteRegEx": "Hidayat.", "year": 2012}, {"title": "Incorporating dialectal variability for socially equitable language identification", "author": ["David Jurgens", "Yulia Tsvetkov", "Dan Jurafsky."], "venue": "ACL 2017.", "citeRegEx": "Jurgens et al\\.,? 2017", "shortCiteRegEx": "Jurgens et al\\.", "year": 2017}, {"title": "Cantonese-english code-switching research in hong-kong: a y2k review", "author": ["D.C.S. Li."], "venue": "World Englishes, 19(3):305\u2013322.", "citeRegEx": "Li.,? 2009", "shortCiteRegEx": "Li.", "year": 2009}, {"title": "That\u2019s sick dude!: Automatic identification of word sense change across different timescales", "author": ["Sunny Mitra", "Ritwik Mitra", "Martin Riedl", "Chris Biemann", "Animesh Mukherjee", "Pawan Goyal."], "venue": "arXiv preprint arXiv:1405.4392.", "citeRegEx": "Mitra et al\\.,? 2014", "shortCiteRegEx": "Mitra et al\\.", "year": 2014}, {"title": "Code-switching and grammatical theory", "author": ["P. Muysken."], "venue": "One speaker, two languages: Crossdisciplinary perspectives on code-switching, pages 177\u2013198. Cambridge University Press.", "citeRegEx": "Muysken.,? 1996", "shortCiteRegEx": "Muysken.", "year": 1996}, {"title": "Contact linguistics: Bilingual encounters and grammatical outcomes", "author": ["C. Myers-Scotton."], "venue": "Oxford University Press.", "citeRegEx": "Myers.Scotton.,? 2002", "shortCiteRegEx": "Myers.Scotton.", "year": 2002}, {"title": "Spanish-english code-switching in email communication", "author": ["R.G. Negr\u00f3n."], "venue": "Language@Internet, 6(3).", "citeRegEx": "Negr\u00f3n.,? 2009", "shortCiteRegEx": "Negr\u00f3n.", "year": 2009}, {"title": "Understanding code-switching & word borrowing from a pluralistic approach of multilingualism", "author": ["V.E. Nzai", "Y-L. Feng", "M.R. Medina-Jim\u00e9nez", "J. Ekiaka-Oblazamengo"], "venue": null, "citeRegEx": "Nzai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nzai et al\\.", "year": 2014}, {"title": "The automatic identification of lexical variation between language varieties", "author": ["Yves Peirsman", "Dirk Geeraerts", "Dirk Speelman."], "venue": "Natural Language Engineering, 16(4):469\u2013491.", "citeRegEx": "Peirsman et al\\.,? 2010", "shortCiteRegEx": "Peirsman et al\\.", "year": 2010}, {"title": "Estimating code-switching on twitter with a novel generalized word-level language detection technique", "author": ["Shruti Rijhwani", "Royal Sequiera", "Monojit Choudhury", "Kalika Bali", "Chandra Shekhar Maddila."], "venue": "ACL 2017.", "citeRegEx": "Rijhwani et al\\.,? 2017", "shortCiteRegEx": "Rijhwani et al\\.", "year": 2017}, {"title": "Understanding language preference for expression of opinion and sentiment: What do hindienglish speakers do on twitter", "author": ["Koustav Rudra", "Shruti Rijhwani", "Rafiya Begum", "Kalika Bali", "Monojit Choudhury", "Niloy Ganguly"], "venue": null, "citeRegEx": "Rudra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rudra et al\\.", "year": 2016}, {"title": "Semantic density analysis: Comparing word meaning across time and phonetic space", "author": ["Eyal Sagi", "Stefan Kaufmann", "Brady Clark."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 104\u2013111. Associa-", "citeRegEx": "Sagi et al\\.,? 2009", "shortCiteRegEx": "Sagi et al\\.", "year": 2009}, {"title": "Chinese-english code-switching in blogs by macao young people", "author": ["H.K. San."], "venue": "MSc. thesis. University of Edinburgh.", "citeRegEx": "San.,? 2009", "shortCiteRegEx": "San.", "year": 2009}, {"title": "The case of the nonce loan in tamil", "author": ["D. Sankoff", "S. Poplack", "S. Vanniarajan."], "venue": "Language variation and change, 2(01):71\u2013101.", "citeRegEx": "Sankoff et al\\.,? 1990", "shortCiteRegEx": "Sankoff et al\\.", "year": 1990}, {"title": "Code-switching or lexical borrowing: Numerals in chasu language of rural tanzania", "author": ["R.Y. Sebonde."], "venue": "Journal of Arts and Humanities, 3(3):67.", "citeRegEx": "Sebonde.,? 2014", "shortCiteRegEx": "Sebonde.", "year": 2014}, {"title": "Borrowings or code mixes: The presence of lone english nouns in mixed discourse", "author": ["C D.W. Senaratne"], "venue": null, "citeRegEx": "Senaratne.,? \\Q2013\\E", "shortCiteRegEx": "Senaratne.", "year": 2013}, {"title": "Overview for the first shared task on language identification in code-switched data", "author": ["T. Solorio", "E. Blair", "S. Maharjan", "S. Bethard", "M. Diab", "M. Gohneim", "A. Hawwari", "F. AlGhamdi", "J. Hirschberg", "A. Chang", "P. Fung."], "venue": "EMNLP First", "citeRegEx": "Solorio et al\\.,? 2014", "shortCiteRegEx": "Solorio et al\\.", "year": 2014}, {"title": "Part-of-speech tagging for english-spanish code-switched text", "author": ["T. Solorio", "Y. Liu."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1051\u20131060. Association for Computational Linguistics.", "citeRegEx": "Solorio and Liu.,? 2008", "shortCiteRegEx": "Solorio and Liu.", "year": 2008}, {"title": "Ehhhh utede hacen plane sin mi???:@ im feeling left out :( form, function and type of code switching in sms texting", "author": ["S. Sotillo."], "venue": "ICAME, pages 309\u2013 310.", "citeRegEx": "Sotillo.,? 2012", "shortCiteRegEx": "Sotillo.", "year": 2012}, {"title": "Contact as a source of language change", "author": ["S.G. Thomason."], "venue": "The handbook of historical linguistics, pages 687\u2013712.", "citeRegEx": "Thomason.,? 2003", "shortCiteRegEx": "Thomason.", "year": 2003}, {"title": "POS tagging of english-hindi codemixed social media content", "author": ["Y. Vyas", "S. Gella", "J. Sharma", "K. Bali", "M. Choudhury."], "venue": "EMNLP, volume 14, pages 974\u2013979.", "citeRegEx": "Vyas et al\\.,? 2014", "shortCiteRegEx": "Vyas et al\\.", "year": 2014}, {"title": "Significance testing of the spearman\u2019s rank correlation coefficient", "author": ["J.H. Zar."], "venue": "Journal of the American Statistical Association, 67(339):578\u2013580.", "citeRegEx": "Zar.,? 1972", "shortCiteRegEx": "Zar.", "year": 1972}], "referenceMentions": [{"referenceID": 0, "context": "In social media communication, multilingual people often switch between languages, a phenomenon known as code-switching or codemixing (Auer, 1984).", "startOffset": 134, "endOffset": 146}, {"referenceID": 3, "context": "This makes language identification and tagging, which is perhaps a pre-requisite for almost all other language processing tasks that follow, a challenging problem (Barman et al., 2014).", "startOffset": 163, "endOffset": 184}, {"referenceID": 1, "context": "While it is difficult in general to ascertain whether a foreign word or phrase used in an utterance is borrowed or just an instance of code-mixing (Bali et al., 2014), one tell tale sign is that only proficient multilinguals can code-mix, while even monolingual speakers can use borrowed words because, by definition, these are part of the vocabulary of a language.", "startOffset": 147, "endOffset": 166}, {"referenceID": 18, "context": "A borrowed word from L2 initially appears frequently in speech, then gradually in print media like newspaper and finally it loses its origin\u2019s identity and is used in L1 resulting in an inclusion in the dictionary of L1 (Myers-Scotton, 2002; Thomason, 2003).", "startOffset": 220, "endOffset": 257}, {"referenceID": 32, "context": "A borrowed word from L2 initially appears frequently in speech, then gradually in print media like newspaper and finally it loses its origin\u2019s identity and is used in L1 resulting in an inclusion in the dictionary of L1 (Myers-Scotton, 2002; Thomason, 2003).", "startOffset": 220, "endOffset": 257}, {"referenceID": 5, "context": "The above reasons motivate us to resort to the social media (in particular, Twitter), where a large population of bilingual/multilingual speakers are known to often tweet in code-mixed colloquial languages (Carter et al., 2013; Solorio et al., 2014; Vyas et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017).", "startOffset": 206, "endOffset": 313}, {"referenceID": 29, "context": "The above reasons motivate us to resort to the social media (in particular, Twitter), where a large population of bilingual/multilingual speakers are known to often tweet in code-mixed colloquial languages (Carter et al., 2013; Solorio et al., 2014; Vyas et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017).", "startOffset": 206, "endOffset": 313}, {"referenceID": 33, "context": "The above reasons motivate us to resort to the social media (in particular, Twitter), where a large population of bilingual/multilingual speakers are known to often tweet in code-mixed colloquial languages (Carter et al., 2013; Solorio et al., 2014; Vyas et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017).", "startOffset": 206, "endOffset": 313}, {"referenceID": 14, "context": "The above reasons motivate us to resort to the social media (in particular, Twitter), where a large population of bilingual/multilingual speakers are known to often tweet in code-mixed colloquial languages (Carter et al., 2013; Solorio et al., 2014; Vyas et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017).", "startOffset": 206, "endOffset": 313}, {"referenceID": 22, "context": "The above reasons motivate us to resort to the social media (in particular, Twitter), where a large population of bilingual/multilingual speakers are known to often tweet in code-mixed colloquial languages (Carter et al., 2013; Solorio et al., 2014; Vyas et al., 2014; Jurgens et al., 2017; Rijhwani et al., 2017).", "startOffset": 206, "endOffset": 313}, {"referenceID": 1, "context": "26) if we use the most competitive baseline (Bali et al., 2014) available in the literature.", "startOffset": 44, "endOffset": 63}, {"referenceID": 20, "context": "More recently, (Nzai et al., 2014) analyzed the formal conversation of Spanish-English multilingual people and found that code mixing/borrowing is not only restricted to daily speech but is also prevalent in formal conversations.", "startOffset": 15, "endOffset": 34}, {"referenceID": 12, "context": "(Hadei, 2016) showed that phonological integration could be evaluated to understand the phenomenon of word borrowing.", "startOffset": 0, "endOffset": 13}, {"referenceID": 27, "context": "Along similar lines, (Sebonde, 2014) showed morphological and syntactic features could be good indicators for numerical borrowings.", "startOffset": 21, "endOffset": 36}, {"referenceID": 28, "context": "(Senaratne, 2013) reported that in many languages English words are likely to be borrowed in both formal and semi-formal text.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "Linguists have for a long time focused on the sociological and the conversational necessity of borrowing and mixing in multilingual communities (see Auer (1984) and Muysken (1996) for a review).", "startOffset": 149, "endOffset": 161}, {"referenceID": 0, "context": "Linguists have for a long time focused on the sociological and the conversational necessity of borrowing and mixing in multilingual communities (see Auer (1984) and Muysken (1996) for a review).", "startOffset": 149, "endOffset": 180}, {"referenceID": 0, "context": "Linguists have for a long time focused on the sociological and the conversational necessity of borrowing and mixing in multilingual communities (see Auer (1984) and Muysken (1996) for a review). In particular, Sankoff et al. (1990) describes the complexity of choosing features that are indicative of borrowing.", "startOffset": 149, "endOffset": 232}, {"referenceID": 31, "context": "tion and social media: (Sotillo, 2012) investi-", "startOffset": 23, "endOffset": 38}, {"referenceID": 4, "context": "Similar observations about chat and email messages have been reported in (Bock, 2013; Negr\u00f3n, 2009).", "startOffset": 73, "endOffset": 99}, {"referenceID": 19, "context": "Similar observations about chat and email messages have been reported in (Bock, 2013; Negr\u00f3n, 2009).", "startOffset": 73, "endOffset": 99}, {"referenceID": 15, "context": "However, studies of code-mixing with Chinese-English bilinguals from Hong Kong (Li, 2009) and Macao (San, 2009) brings forth results that contrast the aforementioned findings and indicate that in these societies code-mixing is driven more by linguistic than social motivations.", "startOffset": 79, "endOffset": 89}, {"referenceID": 25, "context": "However, studies of code-mixing with Chinese-English bilinguals from Hong Kong (Li, 2009) and Macao (San, 2009) brings forth results that contrast the aforementioned findings and indicate that in these societies code-mixing is driven more by linguistic than social motivations.", "startOffset": 100, "endOffset": 111}, {"referenceID": 13, "context": "(Hidayat, 2012) noted that in Facebook, users mostly preferred inter-sentential mix-", "startOffset": 0, "endOffset": 15}, {"referenceID": 8, "context": "In contrast, (Das and Gamb\u00e4ck, 2014) showed that in case of Facebook messages, intra-sentential mixing accounted for more than half of the cases while inter-sentential mixing accounted only for about one-third of the cases.", "startOffset": 13, "endOffset": 36}, {"referenceID": 29, "context": "In fact, in the First Workshop on Computational Approaches to Code Switching a shared task on code-mixing in tweets was launched and four different codemixed corpora were collected from Twitter as a part of the shared task (Solorio et al., 2014).", "startOffset": 223, "endOffset": 245}, {"referenceID": 7, "context": "Language identification task has also been handled for English-Hindi and English-Bengali code-mixed tweets in (Das and Gamb\u00e4ck, 2013).", "startOffset": 110, "endOffset": 133}, {"referenceID": 30, "context": "Part-ofspeech tagging have been recently done for codemixed English-Hindi tweets (Solorio and Liu, 2008; Vyas et al., 2014).", "startOffset": 81, "endOffset": 123}, {"referenceID": 33, "context": "Part-ofspeech tagging have been recently done for codemixed English-Hindi tweets (Solorio and Liu, 2008; Vyas et al., 2014).", "startOffset": 81, "endOffset": 123}, {"referenceID": 24, "context": "For instance, (Sagi et al., 2009) uses latent semantic analysis for detection and tracking of changes in word meaning, whereas (Frermann and Lapata, 2016) presents a Bayesian approach for the same problem.", "startOffset": 14, "endOffset": 33}, {"referenceID": 10, "context": ", 2009) uses latent semantic analysis for detection and tracking of changes in word meaning, whereas (Frermann and Lapata, 2016) presents a Bayesian approach for the same problem.", "startOffset": 101, "endOffset": 128}, {"referenceID": 21, "context": "(Peirsman et al., 2010) presents a distributed model for automatic identification of lexical variation between language varieties.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "(Bamman and Crane, 2011) discusses a method for automatically identifying word sense variation in a dated collection of historical books .", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "(Mitra et al., 2014) presents a computational method for automatic identification of change in word senses across different timescales.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "(Cook et al., 2014) presents a method for novel sense identification of words over time.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "The only work that makes an attempt in this direction is (Bali et al., 2014), which is described in detail in Sec 3.", "startOffset": 57, "endOffset": 76}, {"referenceID": 1, "context": "proposed in (Bali et al., 2014) as the baseline metric.", "startOffset": 12, "endOffset": 31}, {"referenceID": 1, "context": "The authors in (Bali et al., 2014) claim that the more positive the value of this metric is for a word w, the higher is the likeliness of its being borrowed.", "startOffset": 15, "endOffset": 34}, {"referenceID": 23, "context": "To bootstrap the data collection process, we used the language tagged tweets presented in (Rudra et al., 2016).", "startOffset": 90, "endOffset": 110}, {"referenceID": 11, "context": "Language tagging: We tag each word in a tweet with the language of its origin using the method outlined in (Gella et al., 2013).", "startOffset": 107, "endOffset": 127}, {"referenceID": 1, "context": "in (Bali et al., 2014).", "startOffset": 3, "endOffset": 22}, {"referenceID": 34, "context": "Rank correlation: We measure the standard Spearman\u2019s rank correlation (\u03c1) (Zar, 1972) pairwise between rank lists generated by (i) UUR (ii) UTR (iii) UPR (iv) baseline metric and the ground truth.", "startOffset": 74, "endOffset": 85}, {"referenceID": 9, "context": "The average inter-annotator agreement (Fleiss, 1971) for our experiments was found to be as high as 0.", "startOffset": 38, "endOffset": 52}, {"referenceID": 23, "context": "This assumption has also been made in earlier studies (Rudra et al., 2016).", "startOffset": 54, "endOffset": 74}], "year": 2017, "abstractText": "In this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from social media. In terms of Spearman\u2019s correlation values, our methods perform more than two times better (\u223c 0.62) in predicting the borrowing likeliness compared to the best performing baseline (\u223c 0.26) reported in literature. Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts. In 88% of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems.", "creator": "LaTeX with hyperref package"}}}