{"id": "1707.08866", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2017", "title": "Deep Residual Learning for Weakly-Supervised Relation Extraction", "abstract": "deep digital residual learning ( resnet ) is a new method for training very deep noise neural brain networks using identity map - ping for shortcut speech connections. resnet has won the imagenet ilsvrc 2015 classification task task, and achieved state - of - the - art performances in many computer graphics vision development tasks. however, so the effect of noisy residual learning on challenging noisy natural language processing tasks is still not well understood. in this paper, we design a partly novel convolutional neural network ( cnn ) with residual learning, monitor and investigate its impacts on the task of distantly supervised noisy relation extraction. in reasons contradictory to popular beliefs that use resnet only works well for very deep networks, we found that even even with 9 layers of cnns, using identity mapping could significantly improve the performance for distantly - supervised relation extraction.", "histories": [["v1", "Thu, 27 Jul 2017 13:56:36 GMT  (244kb,D)", "http://arxiv.org/abs/1707.08866v1", "Accepted by EMNLP 2017"]], "COMMENTS": "Accepted by EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yi yao huang", "william yang wang"], "accepted": true, "id": "1707.08866"}, "pdf": {"name": "1707.08866.pdf", "metadata": {"source": "CRF", "title": "Deep Residual Learning for Weakly-Supervised Relation Extraction", "authors": ["Yi Yao Huang", "William Yang Wang"], "emails": ["b02901042@ntu.edu.tw", "william@cs.ucsb.edu"], "sections": [{"heading": "1 Introduction", "text": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017). For example, given a sentence \u201cBarack Obama was born in Honolulu, Hawaii.\u201d, a relation classifier aims at predicting the relation of \u201cbornInCity\u201d. Relation extraction is the key component for building relation knowledge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question answering, and summarization.\nA major issue for relation extraction is the lack of labeled training data. In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al.,\n2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooling strategy to improve the CNNs. Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results. However, most of these neural relation extraction models are relatively shallow CNNs\u2014typically only one convolutional layer and one fully connected layer were involved, and it was not clear whether deeper models could have benefits on distilling signals from noisy inputs in this task.\nIn this paper, we investigate the effects of training deeper CNNs for distantly-supervised relation extraction. More specifically, we designed a convolutional neural network based on residual learning (He et al., 2016)\u2014we show how one can incorporate word embeddings and position embeddings into a deep residual network, while feeding identity feedback to convolutional layers for this noisy relation prediction task. Empirically, we evaluate on the NYT-Freebase dataset (Riedel et al., 2010), and demonstrate the state-of-the-art performance using deep CNNs with identify mapping and shortcuts. In contrast to popular beliefs in vision that deep residual network only works for very deep CNNs, we show that even with a moderately deep CNNs, there are substantial improvements over vanilla CNNs for relation extraction. Our contributions are three-fold:\n\u2022 We are the first to consider deeper convolutional neural networks for weakly-supervised\nar X\niv :1\n70 7.\n08 86\n6v 1\n[ cs\n.C L\n] 2\n7 Ju\nl 2 01\n7\nrelation extraction using residual learning;\n\u2022 We show that our deep residual network model outperforms CNNs by a large margin empirically, obtaining state-of-the-art performances;\n\u2022 Our identity mapping with shortcut feedback approach can be easily applicable to any variants of CNNs for relation extraction."}, {"heading": "2 Deep Residual Networks for Relation Extraction", "text": "In this section, we describe a novel deep residual learning architecture for distantly supervised relation extraction. Figure 1 describes the architecture of our model."}, {"heading": "2.1 Vector Representation", "text": "Let xi be the i-th word in the sentence and e1, e2 be the two corresponding entities. Each word will access two embedding look-up tables to get the word embedding WFi and the position embedding PFi. Then, we concatenate the two embeddings and denote each word as a vector of vi = [WFi,PFi]."}, {"heading": "2.1.1 Word Embeddings", "text": "Each representation vi corresponding to xi is a real-valued vector. All of the vectors are encoded in an embeddings matrix Vw \u2208 Rdw\u00d7|V | where V is a fixed-sized vocabulary."}, {"heading": "2.1.2 Position Embeddings", "text": "In relation classification, we focus on finding a relation for entity pairs. Following (Zeng et al., 2014), a PF is the combination of the relative distances of the current word to the first entity e1 and the second entity e2. For instance, in the sentence \u201dSteve Jobs is the founder of Apple.\u201d, the relative distances from founder to e1 (Steve Job) and e2 are 3 and -2, respectively. We then transform the relative distances into real valued vectors by looking up one randomly initialized position embedding matrices Vp \u2208 Rdp\u00d7\u2016P\u2016 where P is fixed-sized distance set. It should be noted that if a word is too far from entities, it may be not related to the relation. Therefore, we choose maximum value emax and minimum value emin for the relative distance.\nIn the example shown in Figure 1, it is assumed that dw is 4 and dp is 1. There are two position embeddings: one for e1, the other for e2. Finally, we concatenate the word embeddings and position\nembeddings of all words and denote a sentence of length n (padded where necessary) as a vector\nv = v1 \u2295 v2 \u2295 ...\u2295 vn where\u2295 is the concatenation operator and vi \u2208 Rd (d = dw + dp \u00d7 2)."}, {"heading": "2.2 Convolution", "text": "Let vi:i+j refer to the concatenation of words vi, vi+1, ..., vi+j . A convolution operation involves a filter w \u2208 Rhd, which is applied to a window of h words to produce a new feature. A feature ci is generated from a window of word vi:i+h\u22121 by\nci = f(w \u00b7 xi:i+h\u22121 + b)\nHere b \u2208 R is a bias term and f is a non-linear function. This filter is applied to each possible window of words from v1 to vn to produce feature c = [c1, c2, ..., cn\u2212h+1] with c \u2208 Rs(s = n\u2212 h + 1)."}, {"heading": "2.3 Residual Convolution Block", "text": "Residual learning connects low-level to high-level representations directly, and tackles the vanishing gradient problem in deep networks. In our model, we design the residual convolution block by applying shortcut connections. Each residual convolutional block is a sequence of two convolutional layers, each one followed by an ReLU activation. The kernel size of all convolutions is h, with padding such that the new feature will have the same size as the original one. Here are two convolutional filter w1, w2 \u2208 Rh\u00d71. For the first convolutional layer:\nc\u0303i = f(w1 \u00b7 ci:i+h\u22121 + b1)\nFor the second convolutional layer:\nc\u0301i = f(w2 \u00b7 c\u0303i:i+h\u22121 + b2)\nHere b1, b2 are bias terms. For the residual learning operation: c = c + c\u0301 Conveniently, the notation of c on the left is changed to define as the output vectors of the block. This operation is performed by a shortcut connection and element-wise addition. This block will be multiply concatenated in our architecture."}, {"heading": "2.4 Max Pooling, Softmax Output", "text": "We then apply a max-pooling operation over the feature and take the maximum value c\u0302 = max{c}.\nWe have described the process by which one feature is extracted from one filter. Take all features into one high level extracted feature z = [c\u03021, c\u03022, ..., c\u0302m](note that here we have m filters). Then, these features are passed to a fully connected softmax layer whose output is the probability distribution over relations. Instead of using y = w \u00b7 z + b for output unit y in forward propagation, dropout uses y = w \u00b7 (z \u25e6 r) + b where \u25e6 is the element-wise multiplication operation and r \u2208 Rm is a \u2019masking\u2019 vector of Bernoulli random variables with probability p of being 1. In the test procedure, the learned weight vectors are scaled by p such that w\u0302 = pw and used (without dropout) to score unseen instances."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental Settings", "text": "In this paper, we use the word embeddings released by (Lin et al., 2016) which are trained on the NYT-Freebase corpus (Riedel et al., 2010). We fine tune our model using validation on the training data. The word embedding is of size 50. The input text is padded to a fixed size of 100. Training is performed with tensorflow adam optimizer, using a mini-batch of size 64, an initial learning rate of 0.001. We initialize our convolutional layers following (Glorot and Bengio, 2010). The implementation is done using Tensorflow 0.11. All experiments are performed on a single NVidia Titan X (Pascal) GPU. In Table 1 we show all parameters used in the experiments. We experiment with several state-of-the-art baselines and variants of our model.\n\u2022 CNN-B: Our implementation of the CNN baseline (Zeng et al., 2014) which contains one convolutional layer, and one fully connected layer.\n\u2022 CNN+ATT: CNN-B with attention over instance learning (Lin et al., 2016).\n\u2022 PCNN+ATT: Piecewise CNN-B with attention over instance learning (Lin et al., 2016).\n\u2022 CNN: Our CNN model which includes one convolutional layer and three fully connected layers.\n\u2022 CNN-x: Deeper CNN model which has x convolutional layers. For example, CNN-9 is a model constructed with 9 convolutional layers (1 + 4 residual cnn block without identity shortcut) and three fully connected layers.\n\u2022 ResCNN-x: Our proposed CNN-x model with residual identity shortcuts.\nWe evaluate our models on the widely used NYT freebase larger dataset (Riedel et al., 2010). Note that ImageNet dataset used by the original ResNet paper (He et al., 2016) has 1.28 million training instances. NYT freebase dataset includes 522K training sentences, which is the largest dataset in relation extraction, and it is the only suitable dataset to train deeper CNNs."}, {"heading": "3.2 NYT-Freebase Dataset Performance", "text": "The advantage of this dataset is that there are 522,611 sentences in training data and 172,448 sentences in testing data and this size can support\nus to train a deep network. Similar to previous work (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using the held-out evaluation. We report both the aggregate curves precision/recall curves and Precision@N (P@N).\nIn Figure 2, we compare the proposed ResCNN model with various CNNs. First, CNNs with multiple fully-connected layers obtained very good results, which is a novel finding. Second, the results also suggest that deeper CNNs with residual learning help extracting signals from noisy distant supervision data. We observe that overfitting happened when we try to add more layers and the performance of CNN-9 is much worse than CNN. We find that ResNet can solve this problem and ResCNN-9 obtains better performance as compared to CNN-B and CNN and dominates the precision/recall curve overall.\nWe show the effect of depth in residual networks in Figure 3. We observe that ResCNN-5 is worse than CNN-5 because the ResNet does not work well for shallow CNNs, and this is consis-\ntent with the original ResNet paper. As we increase the network depth, we see that CNN-9 does overfit to the training data. With residual learning, both ResCNN-9 and ResCNN-13 provide significant improvements over CNN-5 and ResCNN-5 models. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance learning in a noisy input setting.\nThe intuition of ResNet help this task in two aspect. First, if the lower, middle, and higher levels learn hidden lexical, syntactic, and semantic representations respectively, sometimes it helps to bypass the syntax to connect lexical and semantic space directly. Second, ResNet tackles the vanishing gradient problem which will decrease the effect of noise in distant supervision data.\nIn Table 2, we compare the performance of our models to state-of-the-art baselines. We show that our ResCNN-9 outperforms all models that do not select training instances. And even without piecewise max-pooling and instance-based attention, our model is on par with the PCNN+ATT model.\nFor the more practical evaluation, we compare the results for precision@N where N is small (1, 5, 10, 20, 50) in Table 3. We observe that our ResCNN-9 model dominates the performance when we predict the relation in the range of higher probability. ResNet helps CNNs to focus on the highly possible candidate and mitigate the noise effect of distant supervision. We believe that residual connections actually can be seen as a form of renormalizing the gradients, which prevents the model from overfitting to the noisy distant supervision data.\nIn our distant-supervised relation extraction experience, we have two important observations: (1) We get significant improvements with CNNs adding multiple fully-connected layers. (2) Resid-\nual learning could significantly improve the performance for deeper CNNs."}, {"heading": "4 Conclusion", "text": "In this paper, we introduce a deep residual learning method for distantly-supervised relation extraction. We show that deeper convolutional models help distill signals from noisy inputs. With shortcut connections and identify mapping, the performances are significantly improved. These results aligned with a recent study (Conneau et al., 2017), suggesting that deeper CNNs do have positive effects on noisy NLP problems."}], "references": [{"title": "Subsequence kernels for relation extraction", "author": ["Razvan Bunescu", "Raymond J Mooney."], "venue": "NIPS, pages 171\u2013178.", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "Very deep convolutional networks for natural language processing", "author": ["Alexis Conneau", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Yann Lecun"], "venue": null, "citeRegEx": "Conneau et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Conneau et al\\.", "year": 2017}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou GuoDong", "Su Jian", "Zhang Jie", "Zhang Min."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 427\u2013434. Association for Computational Linguis-", "citeRegEx": "GuoDong et al\\.,? 2005", "shortCiteRegEx": "GuoDong et al\\.", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Knowledgebased weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computa-", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun."], "venue": "ACL 2016.", "citeRegEx": "Lin et al\\.,? 2016", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 148\u2013163. Springer.", "citeRegEx": "Riedel et al\\.,? 2010", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Attentionbased convolutional neural network for semantic relation extraction", "author": ["Yatian Shen", "Xuanjing Huang."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers.", "citeRegEx": "Shen and Huang.,? 2016", "shortCiteRegEx": "Shen and Huang.", "year": 2016}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Surdeanu et al\\.,? 2012", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Improved neural relation detection for knowledge base question answering. CoRR, abs/1704.06194", "author": ["Mo Yu", "Wenpeng Yin", "Kazi Saidul Hasan", "C\u0131\u0301cero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2017}, {"title": "Kernel methods for relation extraction", "author": ["Dmitry Zelenko", "Chinatsu Aone", "Anthony Richardella."], "venue": "Journal of machine learning research, 3(Feb):1083\u20131106.", "citeRegEx": "Zelenko et al\\.,? 2003", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), Lis-", "citeRegEx": "Zeng et al\\.,? 2015", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Deep residual learning (ResNet) (He et al., 2016) is a new method for training very deep neural networks using identity mapping for shortcut connections.", "startOffset": 32, "endOffset": 49}, {"referenceID": 12, "context": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017).", "startOffset": 98, "endOffset": 185}, {"referenceID": 0, "context": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017).", "startOffset": 98, "endOffset": 185}, {"referenceID": 3, "context": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017).", "startOffset": 98, "endOffset": 185}, {"referenceID": 11, "context": "Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005; Yu et al., 2017).", "startOffset": 98, "endOffset": 185}, {"referenceID": 7, "context": "In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data.", "startOffset": 37, "endOffset": 103}, {"referenceID": 5, "context": "In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data.", "startOffset": 37, "endOffset": 103}, {"referenceID": 10, "context": "In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data.", "startOffset": 37, "endOffset": 103}, {"referenceID": 14, "context": "Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance.", "startOffset": 132, "endOffset": 151}, {"referenceID": 6, "context": "Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results.", "startOffset": 29, "endOffset": 69}, {"referenceID": 9, "context": "Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results.", "startOffset": 29, "endOffset": 69}, {"referenceID": 5, "context": ", 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction\u2014 it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooling strategy to improve the CNNs.", "startOffset": 8, "endOffset": 438}, {"referenceID": 4, "context": "More specifically, we designed a convolutional neural network based on residual learning (He et al., 2016)\u2014we show how one can incorporate word embeddings and position embeddings into a deep residual network, while feeding identity feedback to convolutional layers for this noisy relation prediction task.", "startOffset": 89, "endOffset": 106}, {"referenceID": 8, "context": "Empirically, we evaluate on the NYT-Freebase dataset (Riedel et al., 2010), and demonstrate the state-of-the-art performance using deep CNNs with identify mapping and shortcuts.", "startOffset": 53, "endOffset": 74}, {"referenceID": 14, "context": "Following (Zeng et al., 2014), a PF is the combination of the relative distances of the current word to the first entity e1 and the second entity e2.", "startOffset": 10, "endOffset": 29}, {"referenceID": 6, "context": "In this paper, we use the word embeddings released by (Lin et al., 2016) which are trained on the NYT-Freebase corpus (Riedel et al.", "startOffset": 54, "endOffset": 72}, {"referenceID": 8, "context": ", 2016) which are trained on the NYT-Freebase corpus (Riedel et al., 2010).", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": "We initialize our convolutional layers following (Glorot and Bengio, 2010).", "startOffset": 49, "endOffset": 74}, {"referenceID": 14, "context": "\u2022 CNN-B: Our implementation of the CNN baseline (Zeng et al., 2014) which contains one convolutional layer, and one fully connected layer.", "startOffset": 48, "endOffset": 67}, {"referenceID": 6, "context": "\u2022 CNN+ATT: CNN-B with attention over instance learning (Lin et al., 2016).", "startOffset": 55, "endOffset": 73}, {"referenceID": 6, "context": "\u2022 PCNN+ATT: Piecewise CNN-B with attention over instance learning (Lin et al., 2016).", "startOffset": 66, "endOffset": 84}, {"referenceID": 8, "context": "We evaluate our models on the widely used NYT freebase larger dataset (Riedel et al., 2010).", "startOffset": 70, "endOffset": 91}, {"referenceID": 4, "context": "Note that ImageNet dataset used by the original ResNet paper (He et al., 2016) has 1.", "startOffset": 61, "endOffset": 78}, {"referenceID": 13, "context": "Similar to previous work (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using the held-out evaluation.", "startOffset": 25, "endOffset": 62}, {"referenceID": 6, "context": "Similar to previous work (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using the held-out evaluation.", "startOffset": 25, "endOffset": 62}, {"referenceID": 1, "context": "These results aligned with a recent study (Conneau et al., 2017), suggesting that deeper CNNs do have positive effects on noisy NLP problems.", "startOffset": 42, "endOffset": 64}], "year": 2017, "abstractText": "Deep residual learning (ResNet) (He et al., 2016) is a new method for training very deep neural networks using identity mapping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-theart performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.", "creator": "LaTeX with hyperref package"}}}