{"id": "1707.09168", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jul-2017", "title": "Learning to Predict Charges for Criminal Cases with Legal Basis", "abstract": "the charge prediction task is straightforward to determine appropriate charges for a seemingly given case, which is helpful for studying legal assistant systems where the user input is termed fact description. we argue that relevant law articles help play an important role in this identification task, and therefore propose an attention - based neural network method to jointly model the charge prediction inference task and the relevant article extraction task in a unified framework. the experimental results show that, besides providing legal basis, the relevant articles can effectively also clearly improve the charge prediction results, since and our full model can effectively always predict appropriate charges for contrasting cases with different expression styles.", "histories": [["v1", "Fri, 28 Jul 2017 09:46:29 GMT  (1455kb,D)", "http://arxiv.org/abs/1707.09168v1", "10 pages, accepted by EMNLP 2017"]], "COMMENTS": "10 pages, accepted by EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bingfeng luo", "yansong feng", "jianbo xu", "xiang zhang", "dongyan zhao"], "accepted": true, "id": "1707.09168"}, "pdf": {"name": "1707.09168.pdf", "metadata": {"source": "CRF", "title": "Learning to Predict Charges for Criminal Cases with Legal Basis", "authors": ["Bingfeng Luo", "Yansong Feng", "Jianbo Xu", "Xiang Zhang", "Dongyan Zhao"], "emails": ["zhaody}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The task of automatic charge prediction is to determine appropriate charges, such as fraud, larceny or homicide, for a case by analyzing its textual fact description. Such techniques are crucial for legal assistant systems, where users could find similar cases or possible penalties by describing a case with their own words. This is helpful for non-legal professionals to get to know the legal basis of their interested cases, e.g., cases they or their friends are involved in, since the massive legal materials and the lack of knowledge of legal jargons make it hard for outsiders to do it on their own.\nHowever, predicting appropriate charges based on fact descriptions is not trivial: (1) The differences between two charges can be subtle, for example, in the context of criminal cases in China, distinguishing intentional homicide from intentional injury would require to determine, from the fact description, whether the defendant intended to kill the victim, or just intended to hurt the vic-\ntim, who, unfortunately died of severe injury. (2) Multiple crimes may be involved in a single case, which means we need to conduct charge prediction in the multi-label classification paradigm. (3) Although we can expect an off-the-shelf classification model to learn to label a case with corresponding charges through massive training data, it is always more convincing to make the prediction with its involved law articles explicitly shown to the users, as legal basis to support the prediction. This issue is crucial in countries using the civil law system, e.g., China (except Hong Kong), where judgements are made based on statutory laws only. For example, in Fig. 1, a judgement document in China always includes relevant law articles (in the court view part) to support the decision. Even in countries using the common law system, e.g., the United States (except Louisiana), where the judgement is based mainly on decisions of previous cases, there are still some statutory laws that need to be followed when making decisions.\nExisting attempts formulate the task of automatic charge prediction as a single-label classification problem, by either adopting a k-Nearest Neighbor (KNN) (Liu et al., 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al., 2012), which make those works hard to scale to more types of charges. There are also works addressing a related task, finding the law articles that are involved in a given case. A simple solution is to convert this multi-label problem into a multiclass classification task by only considering a fixed set of article combinations (Liu and Liao, 2005; Liu and Hsieh, 2006), which can only be applied to a small set of articles and does not fit to real applications. Recent improvement takes a twostep approach by performing a preliminary classification first and then re-ranking the results with ar X iv :1 70 7.\n09 16\n8v 1\n[ cs\n.C L\n] 2\n8 Ju\nl 2 01\n7\nword-level and article-level features (Liu et al., 2015). These efforts advance the applications of machine learning and natural language processing methods into legal assistance services, however, they are still in an early stage, e.g., relying on expert knowledge, using relatively simple classification paradigms, and shallow textual analysis. More importantly, related tasks, e.g., charge prediction and relevant article extraction, are treated independently, ignoring the fact that they could benefit from each other.\nRecent advances in neural networks enable us to jointly model charge prediction and relevant article extraction in a unified framework, where the latent correspondence from the fact description about a case to its related law articles and further to its charges can be explicitly addressed by a two-stack attention mechanism. Specifically, we use a sentence-level and a documentlevel Bi-directional Gated Recurrent Units (BiGRU) (Bahdanau et al., 2015) with a stack of factside attention components to model the correlations among words and sentences, in order to capture the whole story as well as important details of the case. Given the analysis of the fact description, we accordingly learn a stack of article-side attention components to attentively select the most supportive law articles from the statutory laws to support our charge prediction, which is investigated in the multi-label paradigm.\nWe evaluate our model in the context of predicting charges for criminal cases in China. We collect publicly available judgement documents from China\u2019s government website, from which we can automatically extract fact descriptions, relevant law articles and the charges using simple rules, as shown in Figure 1. Experimental results show that our neural network method can effectively predict appropriate charges for a given case, and also provide relevant law articles as legal basis to support the prediction. Our experiments also provide quantitive analysis about the effect of factside and article-side information on charge predicion, and confirm that, apart from providing legal basis, relevant articles also contain useful information that can help to improve charge prediction in the civil law system. We also examine our model on the news reports about criminal cases. Although trained on judgement documents, our model can still achieve promising performance on news data, showing a reasonable generalization\nability over different expression styles."}, {"heading": "2 Related Work", "text": "The charge prediction task aims at finding appropriate charges based on the facts of a case. Previous works consider this task in a multi-class classification framework, which takes the fact description as input and outputs a charge label. (Liu et al., 2004; Liu and Hsieh, 2006) use KNN to classify 12 and 6 criminal charges in Taiwan. However, except for the inferior scalability of the KNN method, their word-level and phrase-level features are too shallow to capture sufficient evidence to distinguish similar charges with subtle differences. (Lin et al., 2012) propose to make deeper understanding of a case by identifying charge-specific factors that are manually designed for 2 charges. This method also suffers from the scalability issue due to the human efforts required to design and annotate these factors for each pair of charges. Our method, however, employs Bi-GRU and a twostack attention mechanism to make comprehensive understanding of a case without relying on explicit human annotations.\nWithin the civil law system, there are some works focusing on identifying applicable law articles for a given case. (Liu and Liao, 2005; Liu and Hsieh, 2006) convert this multi-label problem into a multi-class classification problem by only considering a fixed set of article combinations, which cannot scale well since the number of possible combinations will grow exponentially when a larger set of law articles are considered. (Liu et al., 2015) instead design a scalable two-step approach by first using Support Vector Machine (SVM) for preliminary article classification, and then re-ranking the results using word level features and co-occurence tendency among articles. We also use SVM to extract top k candidate articles, but further adopt Bi-GRU and article-side attention to better understand the texts and exploring the correlation among articles. More importantly, we optimize the article extraction task within our charge prediction framework, which not only provides another view to understand the facts, but also serves as legal basis to support the final decision.\nAnother related thread of work is to predict the overall outcome of a case. The target can be which party will the outcome side with (Aletras et al., 2016), or whether the present court will affirm or reverse the decision of a lower court (Katz et al.,\n2016). Our work differs from them in that, instead of binary outcome (the latter one also contains an other class), we step further to focus on the detailed results of a case, i.e., the charges, where the output may contain multiple labels.\nWe also share similar spirit with the legal question answering task (Kim et al., 2014a), which aims at answering the yes/no questions in the Japanese legal bar exams, that we all believe that relevant law articles are important for decisions in the civil law system. Different from ours, this task requires participants to extract relevant Japanese Civil Code articles first, and then use them to answer the yes/no questions. The former phase is often treated as an information retrieval task, and the latter phase is considered as a textual entailment task (Kim et al., 2014b; Carvalho et al., 2016).\nIn the field of artificial intelligence and law, there are also works trying to find relevant cases given the input query (Raghav et al., 2016; Chen et al., 2013), which is crutial for decision making in the common law system. Rather than finding relevant cases, our work focuses on predicting specific charges, and we also emphasize the importance of law articles in decision making, which is important in the civil law system where the decisions are made based solely on statutory laws.\nOur work is also related to the task of document classification, but mainly differs in that we also need to automatically identify applicable law articles to support and improve the charge prediction. Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification. (Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and an-\nother RNN for document embedding. (Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically generated for each instance according to the fact-side clues as extra guidance. Another difference is the multi-label nature of our task, where, rather than optimizing as multiple binary classification tasks (Nam et al., 2014), we convert the multi-label target to label distribution during training with cross entropy as loss function (Kurata et al., 2016), and use a threshold tuned on validation set to produce the final prediction, which performs better in our pilot experiments."}, {"heading": "3 Data Preparation", "text": "Our data are collected from China Judgements Online1, where the Chinese government has been publishing judgement documents since 2013. We randomly choose 50,000 documents for training, 5,000 for validation and 5,000 for testing. To ensure enough training data for each charge, we only classify the charges that appear more than 80 times in the training data, and treat documents with other charges as negative data. As for law articles, we consider those in the Criminal Law of the People\u2019s Republic of China. The resulting dataset contains 50 distinct charges, 321 distinct articles, averagely 383 words per fact description, 3.81 articles per case, and 3.56% cases with more than one charges.\nAn example judgement document is shown in Figure 1, where we highlight the indicator clauses that we used to divide a document into three pieces\n1http://wenshu.court.gov.cn\nand extract fact description, articles, and charges from each piece, respectively. We use a manually collected charge list to identify all the charges, and law articles are extracted by regular expressions2. The extracted charges and articles are considered as gold standard charges and articles for the corresponding fact description. We also masked all the charges in fact descriptions, since although rare, charge names sometimes may appear in the fact description part.\nCurrently, it is hard and expensive to match the facts related to different defendants with their corresponding charges. We therefore only consider the cases with one defendant, and leave the challenging multi-defendant cases for future work. Although this simplification may change the realworld charge distribution, it enables us to automatically build large scale high quality dataset without relying on annotations from legal practitioners."}, {"heading": "4 Our Approach", "text": "As depicted in Fig. 2, our approach contains the following steps: (1) The input fact description is fed to a document encoder to generate the fact embedding df , where ufw and ufs are global word-level and sentence-level context vectors used to attentively select informative words and sentences. (2) Concurrently, the fact description is also passed to an article extractor to find top k relevant law articles. (3) These articles are embedded by another document encoder, and passed to an article aggregator to attentively select supportive articles, and produce the aggregated article embedding da. Specifically, three context vectors, i.e., uaw, uas and uad, are dynamically generated from df , to produce attention values within the article document encoder and the article aggregator. (4) Finally, df and da are concatenated and passed to a softmax classifier to predict the charge distribution for the input case."}, {"heading": "4.1 Document Encoder", "text": "Intuitively, a sentence is a sequence of words and a document is a sequence of sentences. The document embedding problem, therefore, can be converted to two sequence embedding problems (Tang et al., 2015; Yang et al., 2016). As shown in Fig. 3, we can first embed each sentence using a sentence-level sequence encoder, and then\n2The regular expression used to extract law articles: \u201c\u7b2c[\u3001\u96f6\u25cb\u4e00\u4e8c\u4e24\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341\u767e\u53430-9]+\u6761(\u4e4b[\u4e00 \u4e8c\u4e24\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341])?)\u201d\naggregate them with a document-level sequence encoder to produce the document embedding d. While these two encoders can have different architectures, we use the same here for simplicity.\nBi-GRU Sequence Encoder A challenge in building a sequence encoder is how to take the correlation among different elements into consideration. A promising solution is Bi-directional Gated Recurrent Units (Bi-GRU) (Bahdanau et al., 2015), which encodes the context of each element by using a gating mechanism to track the state of sequence. Specifically, Bi-GRU first uses a forward and a backward GRU (Cho et al., 2014), which is a kind of RNN, to encode the sequence in two opposite directions, and then concatenates the states of both GRUs to form its own states.\nGiven a sequence [x1,x2, ...,xT ] where xt is\nthe input embedding of element t, the state of BiGRU at position t is:\nht = [hft,hbt] (1)\nwhere hft and hbt are the states of the forward and backward GRU at position t. The final sequence embedding is either the concatenation of hfT and hb1 or simply the average of ht.\nAttentive Sequence Encoder However, directly using [hfT ,hb1] for sequence encoding often fails to capture all the information when the sequence is long, while using the average of ht also has the drawback of treating useless elements equally with informative ones. Inspired by (Yang et al., 2016), we use a context vector to attentively aggregate the elements, but instead of using a global context vector, we allow the context vector to be dynamically generated when extra guidance is available (see Sec. 4.2).\nAs shown in Fig. 4, given the Bi-GRU state sequence [h1,h2, ...,hT ], our attentive sequence encoder calculates a sequence of attention values [\u03b11, \u03b12, ..., \u03b1T ], where \u03b1t \u2208 [0, 1] and \u2211 t \u03b1t = 1. The final sequence embedding g is calculated by:\ng = T\u2211 t=1 \u03b1tht; \u03b1t = exp(tanh(Wht) Tu)\u2211 t exp(tanh(Wht) Tu)\n(2) where W is a weight matrix, and u is the context vector to distinguish informative elements from non-informative ones.\nBy using this sequence encoder for fact embedding, the fact-side attention module actually contains two components, i.e., the word-level and sentence-level, using ufw and ufs as their global context vectors, respectively."}, {"heading": "4.2 Using Law Articles", "text": "One of the challenges of using law articles to support charge prediction lies in the fact that statutory laws contain a large number of articles, which makes applying complex models to these articles directly time-consuming, and thus hard to scale. The multi-label nature of relevant article extraction also requires a model that can output multiple articles. We thus adopt a two-step approach, specifically, we first build a fast and easy-to-scale classifier to filter out a large fraction of irrelevant articles, and retain the top k articles. Then, we use neural networks to make comprehensive understanding of the top k articles, and further use\nthe article-side attention module to select the most supportive ones for charge prediction.\nTop k Article Extractor We treat the relevant article extraction task as multiple binary classifications. Specifically, we build a binary classifier for each article, focusing on its relevance to the input case, which results in 321 binary classifiers corresponding to the 321 distinct law articles appearing in our dataset. When more articles are considered, we can simply add more binary classifiers accordingly, with the existing classifiers untouched.\nSimilar to the preliminary classification phase of (Liu et al., 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012). Specifically, we use bag-ofwords TF-IDF features, chi-square for feature selection and linear kernel for binary classification.\nArticle Encoder Since each law article may contain multiple sentences, as shown in Fig. 2, we also use the document encoder described in Sec. 4.1 to produce an embedding aj , j \u2208 [1, k], for each article in the top k extracted articles. While using similar architecture, this article encoder differs from the fact encoder that, instead of using global context vectors, its word-level context vector uaw and sentence-level context vector uas are dynamically generated for each case according to its corresponding fact embedding df :\nuaw = Wwdf + bw; uas = Wsdf + bs (3)\nwhere W\u2217 is the weight matrix and b\u2217 is the bias. The context vectors, uaw and uas, are used to produce the word-level and sentence-level attention values, respectively. Through the dynamic context vectors, the fact embedding df actually guides our model to attend to informative words or sentences with respect to the facts of each case, rather than just selecting generally informative ones.\nAttentive Article Aggregator The article aggregator aims to find supportive articles for charge prediction from the top k extractions, and accordingly produce an aggregated article embedding. Although the order of the top k extracted articles is not fully reliable, (Vinyals et al., 2016) suggests that it is still beneficial to use a bi-directional RNN to embed the context of each element even in a set, where the order does not exist. In our\ntask, bi-directional RNN can help to utilize the cooccurrence tendency of relevant articles.\nSpecifically, we use the attentive sequence encoder in Sec. 4.1 to produce the aggregated article embedding da. Again, to guide the attention with fact descriptions, we dynamically generate the article-level context vector uad by:\nuad = Wddf + bd (4)\nThe attention values produced by the attentive sequence encoder can be seen as the relevance of each article to the input case, which can be used to rank and filter the top k articles. The results can be shown to users as legal basis for charge prediction."}, {"heading": "4.3 The Output", "text": "To make the final charge prediction, we first concatenate the document embedding df and the aggregated article embedding da, and feed them to two consecutive full connection layers to generate a new vector d\u2032, which is then passed to a softmax classifier to produce the predicted charge distribution. We use the validation set to determine a threshold \u03c4 , and consider all the charges with output probability higher than \u03c4 as positive predictions. The input to the first full connection layer can also be only df or da, which means we use only fact or article to make the prediction.\nThe loss function for training is cross entropy:\nLoss = \u2212 N\u2211 i=1 L\u2211 l=1 yillog(oil) (5)\nwhere N is the number of training data, L is the number of charges, yil and oil are the target and predicted probability of charge l for case i. The target charge distribution yi is produced by setting positive labels to 1mi and negative ones to 0, where mi is the number of positive labels in case i.\nSupervised Article Attention We can also utilize the gold-standard law articles naturally in the judgement documents to supervise the article attention during training. Specifically, given the top k articles, we want the article attention distribution \u03b1 \u2208 Rk to simulate the target article distribution t \u2208 Rk, where tj = 1k\u2032 if article j belongs to the gold-standard articles and tj = 0 otherwise. Here k\u2032 is the number of gold-standard articles in the top k extractions. We, again, use cross entropy,\nand the loss function is: Loss = \u2212 N\u2211 i=1 ( L\u2211 l=1 yillog(oil)+\u03b2 k\u2211 j=1 tijlog(\u03b1ij)) (6) where \u03b2 is the weight of the article attention loss."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "We use HanLP3 for Chinese word segmentation and POS tagging. Word embeddings are trained using word2vec (Mikolov et al., 2013) on judgement documents, web pages from several legal forums and Baidu Encyclopedia. The resulting word embeddings contain 573,353 words, with 100 dimension. We randomly initialize a 50-d vector for each POS tag, which is concatenated with the word embedding as the final input. Each GRU in the Bi-GRU is of size 75, the two full connection layers are of size 200 and 150. The relevant article extractor generates top 20 articles, the weight of the article attention loss (\u03b2 in Eq. 6) is 0.1, and prediction threshold \u03c4 is 0.4. We use Stochastic Gradient Descent (SGD) for training, with learning rate 0.1, and batch size 8.\nWe compare our full model with two variations: without article attention supervision and only using facts for charge prediction. The latter one is similar to the state-of-art document classification model (Yang et al., 2016), but adapted to the multilabel nature of our problem. We also implement an SVM model, which is effective and scales well in many fact-description-related tasks in the field of artificial intelligence and law (Liu et al., 2015; Aletras et al., 2016). Specifically, the SVM model takes bag-of-words TF-IDF features as input, and uses chi-square to select top 2,000 features."}, {"heading": "5.2 Charge Prediction Results", "text": "The charge distribution is imbalanced, and the top 5 charges take more than 60% of the cases. Therefore, we evaluate the charge prediction task using precision, recall and F1, in both micro- and macro-level. The macro-precision/recall are calculated by averaging the precision and recall of each charge, and the micro-precision/recall are averaged over each prediction.\nAs shown in Table 1, the basic SVM fact model, which only takes fact descriptions as input, indeed proves to be a strong baseline. By\n3https://github.com/hankcs/HanLP\ncontrast, our corresponding neural network model (NN fact), which also only uses facts for prediction, outperforms SVM fact by about 4% in micro-F1. Since NN fact benefits from the pretrained word embeddings, the two-level Bi-GRU architecture, and the fact-side attention module, it can attentively recognize informative expressions from the description and better capture the underlying correspondence from fact descriptions to appropriate charges, even when there is less overlap in the words used among cases with the same charge, or when there are limited data (i.e., infrequent charges). This may explain that NN models have more balanced performance over different charges, leading to more prominent improvements over SVM ones in macro metrics, which usually have a strong bias towards frequent charges.\nWhen we use both facts and extracted relevant law articles (that are admittedly noisy), the SVM version (SVM fact art) drops by around 5% than SVM fact, showing that the SVM model cannot benefit from the extracted, thus noisy, relevant articles in such a straightforward way. However, our NN version (NN fact article) can still learn from the noisy article extractions through attentively aggregating those extracted articles even without direct guidance, thus improves NN fact by around 0.4%. Furthermore, if we use the gold standard articles during training as supervision for the article attention (our full model, NN fact supv art), the results can be further improved, achieving 90.21% and 80.48% in micro- and macro-F1, respectively. The improvements made by using relevant law articles actually indicates the nature of the civil law system that\njudgements are made based on statutory laws. However, if we only use the extracted relevant articles to make prediction (SVM art and NN art4), the performance becomes worse. Even with the proved-helpful attentive aggregator, the model performs worst among all NN variants (though still better than SVM fact). This indicates that it is necessary to consider both facts and relevant law articles for charge prediction, and, the fact that NN fact outperforms NN art also indicates that although the judgments are made based on the statutory laws in the civil law system, the logic employed by the court when making decisions, to some extent, may be implicitly captured through massive fact-charges paris.\nNow the question is: how much improvement can we have if we can make full use of the relevant law articles within the civil law system? Let us consider an ideal situation where we can access both fact descriptions and gold standard law articles during testing, which could be considered as an upper bound scenario. The SVM version (SVM fact gold art) significantly outperforms SVM fact art by more than 30% in macro-F1. And the NN version (NN fact gold art) outperforms NN fact supv art by over 8%. These comparisons confirm again that law articles play an important role for automatic judgement prediction, but the extracted relevant articles inevitably contain noise, which should be properly handled, e.g., using an attentively aggregation mechanism to distill valuable evidence to support charge prediction.\nCase Study We study the model outputs and find certain star-like confusion patterns among the charges. For example, intentional injury is often confused with multiple charges like intentional homicide (when the victim is dead, the difference is whether the defendant intends to kill or just hurt the victim) and picking quarrels and provoking troubles (there may also exist injuries here). These charges usually share some similar fact descriptions, e.g., how the injuries are caused, and since intentional injury appears more frequently than the others, SVM fact thus outputs intentional injury in most situations, and fails to distinguish these charges. However, by using Bi-GRU and the attention mechanism, NN fact can at-\n4 NN art uses fact embeddings to attentively aggregate relevant articles, but only use the aggregated article embedding da, without fact embedding df , for charge prediction.\ntend to important details of the facts and significantly improves the performance on these charges. When the direct supervision for articles is available, NN fact supv art can enhance the interaction between certain pairs of fact descriptions and law articles, which helps to capture the subtle differences among similar charges, and further improves the performance on these situations."}, {"heading": "5.3 Article Extraction Results", "text": "We also evaluate our SVM article extractor, which achieves 77.60%, 88.96%, 94.21% and 96.53% recall regarding the top 5, 10, 20 and 30 articles, respectively. Although simple, the SVM extractor can obtain over 94% recall for top 20 articles, which is good enough for further refinement. However, the micro-F1 of the extractor is only 61.08% in the test set, which will lead to severe error propagation problem if we use the prediction results of the extractor directly. Therefore, we design the article attention mechanism to handle the noise in the top 20 articles.\nTable 2 shows the re-ranking results of our article attention module (column 2-3) and the corresponding charge prediction performances (column 4), under different weights for article attention (\u03b2 in Eq. 6). Prec@1 refers to top 1 precision, and MAP refers to mean average precision. We can see that, even if there is no supervision over the article attention (\u03b2 = 0), our model still has reasonable performance on re-ranking the k articles. When the attention supervision is employed, the extraction quality improves significantly, and keeps increasing as \u03b2 goes up. However, the charge prediction performance does not always increase with the article extraction quality, and the best performance is achieved when \u03b2 = 0.1. This is not surprising, since there exists a tradeoff between the benefits of more accurate article extraction and the less model capacity left for charge classification due to the increased emphasis on the article extraction performance. The promising article extraction results also confirm the ability of our model to provide legal basis for the charge prediction."}, {"heading": "5.4 Performance on News Data", "text": "There are usually clear differences between the expressions used by legal practitioners and people without legal background, thus it is important to see how our model will perform on fact descriptions written by non-legal professionals.\nWe create a news dataset by asking 3 law school students to annotate the appropriate charges for 100 social news reports about criminal cases from two news websites5, with 262 words on average and 25 distinct charges. The \u03ba value is 0.83, indicating good consistency. The annotators are asked to have a disscussion to achieve an aggreement on inconsistent annotations. The results are shown in Table 3, where we only report micro statistics due to the relatively small size of the dataset compared with the number of distinct charges.\nWe can see that, SVM fact suffers a significant drop in F1 on the news data, confirming the gap between the expressions used by legal practitioners and non-legal professionals, given the BOW nature of SVM fact. Although SVM fact cannot generalize well, the patterns learned by SVM fact are reliable in themselves, leading to a high precision. It is not surprising that our NN models also suffer from the expression differences, but due to the effectiveness of our NN architecture, with about 10%\u223c15% less absolute drop in F1, and NN fact supv art can still achieve 79.12% in F1. For example, the word\u66b4\u6253 (beat up) is seldom used in judgement documents, making it hard for SVM fact to correctly utilize \u66b4 \u6253 as an indicator for injury related charges, but, our NN models can associate it with its nearsynonymy\u6bb4\u6253 (hit), which is a formal expression in judgement documents. Furthermore, the clear improvements from NN fact to NN fact art, and further to NN fact supv art prove again the importance of relevant law articles in supporting the charge prediction, even in news domain. The reasonable performance on news data also shows that our method do have the ability to help non-legal professionals.\n5http://news.cn and http://people.com.cn"}, {"heading": "6 Conclusion", "text": "In this paper, we propose an attention-based neural network framework that can jointly model the charge prediction task and the relevant article extraction task, where the weighted relevant articles can serve as legal basis to support the charge prediction. The experimental results on judgement documents of criminal cases in China show the effectiveness of our model on both charge prediction and relevant article extraction. The comparison of different variants of our model also indicates the importance of law articles in making judicial decisions in the civil law system. By experimenting on news data, we show that, although trained on judgement documents, our model also has reasonable generalization ability on fact descriptions written by non-legal professionals. While promising, our model still cannot explicitly handle multidefendant cases, and there is also a clear gap between our model and the upper bound improvement that relevant articles can achieve. We will leave these challenges for future work."}, {"heading": "Acknowledgement", "text": "This work is supported by the National High Technology R&D Program of China (2015AA015403); the National Natural Science Foundation of China (61672057, 61672058); KLSTSPI Key Lab. of Intelligent Press Media Technology."}], "references": [{"title": "Predicting judicial decisions of the european court of human rights: A natural language processing perspective", "author": ["Nikolaos Aletras", "Dimitrios Tsarapatsanis", "Daniel Preo\u0163iuc-Pietro", "Vasileios Lampos."], "venue": "PeerJ Computer Science, 2:e93.", "citeRegEx": "Aletras et al\\.,? 2016", "shortCiteRegEx": "Aletras et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Lexicalmorphological modeling for legal text analysis", "author": ["Danilo S Carvalho", "Minh-Tien Nguyen", "Tran Xuan Chien", "Minh Le Nguyen."], "venue": "arXiv preprint arXiv:1609.00799.", "citeRegEx": "Carvalho et al\\.,? 2016", "shortCiteRegEx": "Carvalho et al\\.", "year": 2016}, {"title": "A text mining approach to assist the general public in the retrieval of legal documents", "author": ["Yen-Liang Chen", "Yi-Hung Liu", "Wu-Liang Ho."], "venue": "Journal of the American Society for Information Science and Technology, 64(2):280\u2013290.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Schwenk", "Yoshua Bengio."], "venue": "Proceedings of EMNLP, pages 1724\u20131734.", "citeRegEx": "Schwenk and Bengio.,? 2014", "shortCiteRegEx": "Schwenk and Bengio.", "year": 2014}, {"title": "Learning to classify text using support vector machines: Methods, theory and algorithms", "author": ["Thorsten Joachims."], "venue": "Kluwer Academic Publishers.", "citeRegEx": "Joachims.,? 2002", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "A general approach for predicting the behavior of the supreme court of the united states", "author": ["Daniel Martin Katz", "II Bommarito", "J Michael", "Josh Blackman."], "venue": "arXiv preprint arXiv:1612.03473.", "citeRegEx": "Katz et al\\.,? 2016", "shortCiteRegEx": "Katz et al\\.", "year": 2016}, {"title": "COLIEE-14", "author": ["Mi-Young Kim", "Randy Goebe", "Ken Satoh."], "venue": "http://webdocs. cs.ualberta.ca/ \u0303miyoung2/jurisin_ task/index.html.", "citeRegEx": "Kim et al\\.,? 2014a", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Legal question answering using ranking svm and syntactic/semantic similarity", "author": ["Mi-Young Kim", "Ying Xu", "Randy Goebel."], "venue": "JSAI International Symposium on Artificial Intelligence, pages 244\u2013 258. Springer.", "citeRegEx": "Kim et al\\.,? 2014b", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings EMNLP, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Improved neural network-based multi-label classification with better initialization leveraging label cooccurrence", "author": ["Gakuto Kurata", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of NAACL-HLT, pages 521\u2013526.", "citeRegEx": "Kurata et al\\.,? 2016", "shortCiteRegEx": "Kurata et al\\.", "year": 2016}, {"title": "Exploiting machine learning models for chinese legal documents labeling, case classification, and sentencing prediction", "author": ["Wan-Chen Lin", "Tsung-Ting Kuo", "Tung-Jia Chang."], "venue": "ROCLING XXIV (2012), page 140.", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Case instance generation and refinement for case-based criminal summary judgments in chinese", "author": ["Chao-Lin Liu", "Cheng-Tsung Chang", "Jim-How Ho."], "venue": "Journal of Information Science and Engineering, 20(4):783\u2013800.", "citeRegEx": "Liu et al\\.,? 2004", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Exploring phrase-based classification of judicial documents for criminal charges in chinese", "author": ["Chao-Lin Liu", "Chwen-Dar Hsieh."], "venue": "International Symposium on Methodologies for Intelligent Systems, pages 681\u2013690. Springer.", "citeRegEx": "Liu and Hsieh.,? 2006", "shortCiteRegEx": "Liu and Hsieh.", "year": 2006}, {"title": "Classifying criminal charges in chinese for web-based legal services", "author": ["Chao-Lin Liu", "Ting-Ming Liao."], "venue": "Asia-Pacific Web Conference, pages 64\u201375. Springer.", "citeRegEx": "Liu and Liao.,? 2005", "shortCiteRegEx": "Liu and Liao.", "year": 2005}, {"title": "Predicting associated statutes for legal problems", "author": ["Yi-Hung Liu", "Yen-Liang Chen", "Wu-Liang Ho."], "venue": "Information Processing & Management, 51(1):194\u2013211.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of NIPS, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Largescale multi-label text classification\u2014revisiting neural networks", "author": ["Jinseok Nam", "Jungi Kim", "Eneldo Loza Menc\u0131\u0301a", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery", "citeRegEx": "Nam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2014}, {"title": "Analyzing the extraction of relevant legal judgments using paragraph-level and citation information", "author": ["K Raghav", "P Krishna Reddy", "V Balakista Reddy."], "venue": "AI4J\u2013Artificial Intelligence for Justice, page 30.", "citeRegEx": "Raghav et al\\.,? 2016", "shortCiteRegEx": "Raghav et al\\.", "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["Oriol Vinyals", "Charles Blundell", "Tim Lillicrap", "Daan Wierstra"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90\u201394. As-", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Existing attempts formulate the task of automatic charge prediction as a single-label classification problem, by either adopting a k-Nearest Neighbor (KNN) (Liu et al., 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al.", "startOffset": 156, "endOffset": 195}, {"referenceID": 13, "context": "Existing attempts formulate the task of automatic charge prediction as a single-label classification problem, by either adopting a k-Nearest Neighbor (KNN) (Liu et al., 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al.", "startOffset": 156, "endOffset": 195}, {"referenceID": 11, "context": ", 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al., 2012), which make those works hard to scale to more types of charges.", "startOffset": 160, "endOffset": 178}, {"referenceID": 14, "context": "A simple solution is to convert this multi-label problem into a multiclass classification task by only considering a fixed set of article combinations (Liu and Liao, 2005; Liu and Hsieh, 2006), which can only be applied to a small set of articles and does not fit to real applications.", "startOffset": 151, "endOffset": 192}, {"referenceID": 13, "context": "A simple solution is to convert this multi-label problem into a multiclass classification task by only considering a fixed set of article combinations (Liu and Liao, 2005; Liu and Hsieh, 2006), which can only be applied to a small set of articles and does not fit to real applications.", "startOffset": 151, "endOffset": 192}, {"referenceID": 1, "context": "GRU) (Bahdanau et al., 2015) with a stack of factside attention components to model the correlations among words and sentences, in order to capture the whole story as well as important details of the case.", "startOffset": 5, "endOffset": 28}, {"referenceID": 12, "context": "(Liu et al., 2004; Liu and Hsieh, 2006) use KNN to classify 12 and 6 criminal charges in Taiwan.", "startOffset": 0, "endOffset": 39}, {"referenceID": 13, "context": "(Liu et al., 2004; Liu and Hsieh, 2006) use KNN to classify 12 and 6 criminal charges in Taiwan.", "startOffset": 0, "endOffset": 39}, {"referenceID": 11, "context": "(Lin et al., 2012) propose to make deeper understanding of a case by identifying charge-specific factors that are manually designed for 2 charges.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "(Liu et al., 2015) instead design a scalable two-step approach by first using Support Vector Machine (SVM) for preliminary article classification, and then re-ranking the results using word level features and co-occurence tendency among articles.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "The target can be which party will the outcome side with (Aletras et al., 2016), or whether the present court will affirm or reverse the decision of a lower court (Katz et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 7, "context": "We also share similar spirit with the legal question answering task (Kim et al., 2014a), which aims at answering the yes/no questions in the Japanese legal bar exams, that we all believe that relevant law articles are important for decisions in", "startOffset": 68, "endOffset": 87}, {"referenceID": 8, "context": "latter phase is considered as a textual entailment task (Kim et al., 2014b; Carvalho et al., 2016).", "startOffset": 56, "endOffset": 98}, {"referenceID": 2, "context": "latter phase is considered as a textual entailment task (Kim et al., 2014b; Carvalho et al., 2016).", "startOffset": 56, "endOffset": 98}, {"referenceID": 9, "context": "Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification.", "startOffset": 95, "endOffset": 106}, {"referenceID": 19, "context": "(Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and another RNN for document embedding.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "(Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embed-", "startOffset": 0, "endOffset": 19}, {"referenceID": 17, "context": "difference is the multi-label nature of our task, where, rather than optimizing as multiple binary classification tasks (Nam et al., 2014), we convert the multi-label target to label distribution during training with cross entropy as loss function (Ku-", "startOffset": 120, "endOffset": 138}, {"referenceID": 19, "context": "The document embedding problem, therefore, can be converted to two sequence embedding problems (Tang et al., 2015; Yang et al., 2016).", "startOffset": 95, "endOffset": 133}, {"referenceID": 22, "context": "The document embedding problem, therefore, can be converted to two sequence embedding problems (Tang et al., 2015; Yang et al., 2016).", "startOffset": 95, "endOffset": 133}, {"referenceID": 1, "context": "A promising solution is Bi-directional Gated Recurrent Units (Bi-GRU) (Bahdanau et al., 2015), which encodes the context of each element by using a gating mechanism to track the state of sequence.", "startOffset": 70, "endOffset": 93}, {"referenceID": 22, "context": "Inspired by (Yang et al., 2016), we use a context vector to attentively aggregate the elements, but instead of using a global context vector, we allow the context vector to be dynamically generated when extra guidance is", "startOffset": 12, "endOffset": 31}, {"referenceID": 15, "context": "Similar to the preliminary classification phase of (Liu et al., 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012).", "startOffset": 51, "endOffset": 69}, {"referenceID": 5, "context": ", 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012).", "startOffset": 117, "endOffset": 157}, {"referenceID": 21, "context": ", 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012).", "startOffset": 117, "endOffset": 157}, {"referenceID": 20, "context": "Although the order of the top k extracted articles is not fully reliable, (Vinyals et al., 2016) suggests that it is still beneficial to use a bi-directional RNN to embed the context of each element even in a set, where the order does not exist.", "startOffset": 74, "endOffset": 96}, {"referenceID": 16, "context": "Word embeddings are trained using word2vec (Mikolov et al., 2013) on judgement documents, web pages from several legal forums and Baidu Encyclopedia.", "startOffset": 43, "endOffset": 65}, {"referenceID": 22, "context": "The latter one is similar to the state-of-art document classification model (Yang et al., 2016), but adapted to the multi-", "startOffset": 76, "endOffset": 95}, {"referenceID": 15, "context": "We also implement an SVM model, which is effective and scales well in many fact-description-related tasks in the field of artificial intelligence and law (Liu et al., 2015; Aletras et al., 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 0, "context": "We also implement an SVM model, which is effective and scales well in many fact-description-related tasks in the field of artificial intelligence and law (Liu et al., 2015; Aletras et al., 2016).", "startOffset": 154, "endOffset": 194}], "year": 2017, "abstractText": "The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.", "creator": "LaTeX with hyperref package"}}}