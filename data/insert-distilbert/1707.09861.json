{"id": "1707.09861", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2017", "title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging", "abstract": "in this paper we show below that reporting a single outcome performance score is insufficient to compare non - deterministic approaches. we demonstrate for common sequence tagging tasks \u2013 that the seed value for the random number generator can result in similar statistically significant ( p & lt ; 10 ^ - 4 ) differences for state - of - the - art filtering systems. for two recent systems for ner, separately we observe an absolute difference prediction of one percentage point f1 - r score depending on the selected seed value, making these systems perceived either as state - of - the - system art resources or mediocre. instead of publishing and avoiding reporting single performance target scores, we propose to actually compare score sampling distributions based on multiple executions. based on on collecting the evaluation of 50. 000 lstm - networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as margins are more stable with respect to the remaining hyperparameters.", "histories": [["v1", "Mon, 31 Jul 2017 14:25:24 GMT  (541kb,D)", "http://arxiv.org/abs/1707.09861v1", "Accepted at EMNLP 2017"]], "COMMENTS": "Accepted at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["nils reimers", "iryna gurevych"], "accepted": true, "id": "1707.09861"}, "pdf": {"name": "1707.09861.pdf", "metadata": {"source": "CRF", "title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging", "authors": ["Nils Reimers", "Iryna Gurevych"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Large efforts are spent in our community on developing new state-of-the-art approaches. To document that those approaches are better, they are applied to unseen data and the obtained performance score is compared to previous approaches. In order to make results comparable, a provided split\nThis paper is published in the proceedings of EMNLP 2017.\n1https://arxiv.org/abs/1707.06799 2https://github.com/UKPLab/\nemnlp2017-bilstm-cnn-crf\nbetween train, development and test data is often used, for example from a former shared task.\nIn recent years, deep neural networks were shown to achieve state-of-the-art performance for a wide range of NLP tasks, including many sequence tagging tasks (Ma and Hovy, 2016), dependency parsing (Andor et al., 2016), and machine translation (Wu et al., 2016). The training process for neural networks is highly non-deterministic as it usually depends on a random weight initialization, a random shuffling of the training data for each epoch, and repeatedly applying random dropout masks. The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010). Depending on the seed value for the pseudo-random number generator, the network will converge to a different local minimum.\nOur experiments show that these different local minima have vastly different characteristics on unseen data. For the recent NER system by Ma and Hovy (2016) we observed that, depending on the random seed value, the performance on unseen data varies between 89.99% and 91.00% F1-score. The difference between the best and worst performance is statistically significant (p < 10\u22124) using a randomization test3. In conclusion, whether this newly developed approach is perceived as state-of-the-art or as mediocre, largely depends on which random seed value is selected. This issue is not limited to this specific approach, but potentially applies to all approaches with non-deterministic training processes.\nThis large dependence on the random seed value creates several challenges when evaluating new\n31 Million iterations. p-value adapted using the Bonferroni correction to take the 86 tested seed values into account.\nar X\niv :1\n70 7.\n09 86\n1v 1\n[ cs\n.C L\n] 3\n1 Ju\nl 2 01\n7\napproaches:\n\u2022 Observing a (statistically significant) improvement through a new non-deterministic approach might not be the result of a superior approach, but the result of having a more favorable sequence of random numbers.\n\u2022 Promising approaches might be rejected too early, as they fail to deliver an outperformance simply due to a less favorable sequence of random numbers.\n\u2022 Reproducing results is difficult.\nTo study the impact of the random seed value on the performance we will focus on five linguistic sequence tagging tasks: POS-tagging, Chunking, Named Entity Recognition, Entity Recognition4, and Event Detection. Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al., 2016; S\u00f8gaard and Goldberg, 2016).\nFixing the random seed value would solve the issue with the reproducibility, however, there is no justification for choosing one seed value over another seed value. Hence, instead of reporting and comparing a single performance, we show that comparing score distributions can lead to new insights into the functioning of algorithms.\nOur main contributions are:\n1. Showing the implications of non-deterministic approaches on the evaluation of approaches and the requirement to compare score distributions instead of single performance scores.\n2. Comparison of two recent, state-of-the-art systems for NER and showing that reporting a single performance score can be misleading.\n3. In-depth analysis of different LSTMarchitectures for five sequence tagging tasks with respect to: superior performance, stability of results, and importance of tuning parameters.\n4Entity Recognition labels all tokens that refer to an entity in a sentence, also generic phrases like U.S. president."}, {"heading": "2 Background", "text": "Validating and reproducing results is an important activity in science to manifest the correctness of previous conclusions and to gain new insights into the presented approaches. Fokkens et al. (2013) show that reproducing results is not always straightforward, as factors like preprocessing (e.g. tokenization), experimental setup (e.g. splitting data), the version of components, the exact implementation of features, and the treatment of ties can have a major impact on the achieved performance and sometimes on the drawn conclusions.\nFor approaches with non-deterministic training procedures, like neural networks, reproducing exact results becomes even more difficult, as randomness can play a major role in the outcome of experiments. The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010). The sequence of random numbers plays a major role to which minima the network converges during the training process. However, not all minima generalize equally well to unseen data. Erhan et al. (2010) showed for the MNIST handwritten digit recognition task that different random seeds result in largely varying performances. They noted further that with increasing depth of the neural network, the probability of finding poor local minima increases.\nAs (informally) defined by Hochreiter and Schmidhuber (1997a), a minimum can be flat, where the error function remains approximately constant for a large connected region in weight-space, or it can be sharp, where the error function increases rapidly in\na small neighborhood of the minimum. A conceptual sketch is given in Figure 1. The error functions for training and testing are typically not perfectly synced, i.e. the local minima on the train or development set are not the local minima for the held-out test set. A sharp minimum usually depicts poorer generalization capabilities, as a slight variation results in a rapid increase of the error function. On the other hand, flat minima generalize better on new data (Keskar et al., 2016). Keskar et al. observe for the MNIST, TIMIT, and CIFAR dataset, that the generalization gap is not due to over-fitting or over-training, but due to different generalization capabilities of the local minima the networks converge to.\nA priori it is unknown to which type of local minimum a neural network will converge. Some methods like the weight initialization (Erhan et al., 2010; Glorot and Bengio, 2010) or small-batch training (Keskar et al., 2016) help to avoid bad (e.g. sharp) minima. Nonetheless, the non-deterministic behavior of approaches must be considered when they are evaluated."}, {"heading": "3 Impact of Randomness in the Evaluation of Neural Networks", "text": "Two recent, state-of-the-art systems for NER are proposed by Ma and Hovy (2016)5 and by Lample et al. (2016)6. Lample et al. report an F1-score of 90.94% and Ma and Hovy report an F1-score of 91.21%. Ma and Hovy draw the conclusion that their system achieves a significant improvement over the system by Lample et al.\nWe re-ran both implementations multiple times, each time only changing the seed value of the random number generator. We ran the Ma and Hovy system 86 times and the Lample et al. system, due to its high computational requirement, for 41 times. The score distribution is depicted as a violin plot in Figure 2. Using a Kolmogorov-Smirnov significance test (Massey, 1951), we observe a statistically significant difference between these two distributions (p < 0.01). The plot reveals that the quartiles for the Lample et al. system are above those of the Ma and Hovy system. Further it reveals a smaller standard deviation \u03c3 of the F1-\n5https://github.com/XuezheMax/ LasagneNLP\n6https://github.com/glample/tagger\nscores for the Lample et al. system. Using a BrownForsythe test, the standard deviations are different with p < 0.05. Table 1 shows the minimum, the maximum, and the median performance for the test performances.\nBased on this observation, we draw the conclusion that the system by Lample et al. outperforms the system by Ma and Hovy, as their implementation achieves a higher score distribution and shows a lower standard deviation.\nIn a usual setup, approaches would be compared on a development set and the run with the highest development score would be used for unseen data, i.e. be used to report the test performance. For the Lample et al. system we observe a Spearman\u2019s rank correlation between the development and the test score of \u03c1 = 0.229. This indicates a weak correlation and that the performance on the development set is not a reliable indicator. Using the run with the best development score (94.44%) would yield a test performance of mere 90.31%. Using the second best run on development set (94.28%), would yield state-of-the-art performance with 91.00%. This difference is statistically significant (p < 0.002). In conclusion, a development set will not necessarily solve the issue with bad local minima.\nThe main difference between these two approaches is in the generation of character-based representations: Ma and Hovy uses a Convolutional Neural Network (CNN) (LeCun et al., 1989), while Lample et al. uses an LSTM-network. As our experiments in section 6.4 show, both approaches perform comparably if all other parameters were kept the same. Further, we could only observe a\nstatistically significant improvement for the tasks POS, Chunking and Event Detection. For NER and Entity Recognition, the difference was statistically not significant given the number of tested hyperparameters.\nIn the next step, we evaluated the impact of the random seed value for the five sequence tagging tasks described in section 4. We sampled randomly 1830 different configurations, for example different numbers of recurrent units, and ran the network twice, each time with a different seed value. The results are depicted in Table 2.\nThe largest difference was observed for the ACE 2005 Entities dataset: Using one seed value, the network achieved an F1 performance of 82.5% while using another seed value, the network achieved a performance of only 74.3%. Even though this is a rare extreme case, the median difference between different weight initializations is still large. For example for the CoNLL 2003 NER dataset, the median difference is at 0.38% and the 95th percentile is at 1.08%.\nIn conclusion, if the fact of different local minima is not taken care of and single performance scores are compared, there is a high chance of drawing false conclusions and either rejecting promising approaches or selecting weaker approaches due to a more or less favorable sequence of random numbers."}, {"heading": "4 Experimental Setup", "text": "In order to find LSTM-network architectures that perform robustly on different tasks, we selected five classical NLP tasks as benchmark tasks: Partof-Speech tagging (POS), Chunking, Named Entity Recognition (NER), Entity Recognition (Entities) and Event Detection (Events).\nFor Part-of-Speech tagging, we use the benchmark setup described by Toutanova et al. (2003). Using the full training set for POS tagging would hinder our ability to detect design choices that are consistently better than others. The error rate for this dataset is approximately 3% (Marcus et al., 1993), making all improvements above 97% accuracy likely the result of chance. A 97.24% accuracy was achieved by Toutanova et al. (2003). Hence, we reduced the training set size from over 38.000 sentences to the first 500 sentences. This decreased the accuracy to about 95%.\nFor Chunking, we use the CoNLL 2000 shared task setup. For Named Entity Recognition (NER), we use the CoNLL 2003 setup. The ACE 2005 entity recognition task annotated not only named entities, but all words referring to an entity, e.g. the phrase U.S. president. We use the same data split as Li et al. (2013). For the Event Detection task, we use the TempEval3 Task B setup. There, the smallest extent of text, usually a single word, that expresses the occurrence of an event, is annotated.\nFor the POS-task, we report accuracy and for the other tasks we report the F1-score."}, {"heading": "4.1 Model", "text": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). To be able to evaluate a large number of different network configurations, we optimized our implementation for efficiency, reducing by a factor of 6 the time required per epoch compared to Ma and Hovy (2016)."}, {"heading": "4.2 Evaluated Parameters", "text": "We evaluate the following design choices and hyperparameters: Pre-trained Word Embeddings. We evaluate the Google News embeddings (G. News)7 from Mikolov et al. (2013), the Bag of Words (Le. BoW) as well as the dependency based embeddings (Le. Dep.)8 by Levy and Goldberg (2014), three different GloVe embeddings9 from Pennington et al. (2014) trained either on Wikipedia 2014 + Gigaword 5 (GloVe1 with 100 dimensions and GloVe2 with 300 dimensions) or on Common Crawl (GloVe3), and the Komninos and Manandhar (2016) embeddings (Komn.)10. We also evaluate the approach of Bojanowski et al. (2016) (FastText), which trains embeddings for n-grams with length 3 to 6. The embedding for a word is defined as the sum of the embeddings of the ngrams.\nCharacter Representation. We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al. (2016) using LSTMnetworks to derive character-based representations.\nOptimizer. Besides Stochastic Gradient Descent (SGD), we evaluate Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.\nGradient Clipping and Normalization. Two common strategies to deal with the exploding gradi-\n7https://code.google.com/archive/p/ word2vec/\n8https://levyomer.wordpress.com/2014/ 04/25/dependency-based-word-embeddings/\n9http://nlp.stanford.edu/projects/ glove/\n10https://www.cs.york.ac.uk/nlp/extvec/\nent problem are gradient clipping (Mikolov, 2012) and gradient normalization (Pascanu et al., 2013). Gradient clipping involves clipping the gradient\u2019s components element-wise if it exceeds a defined threshold. Gradient normalization has a better theoretical justification and rescales the gradient whenever the norm goes over a threshold.\nTagging schemes. We evaluate the BIO and IOBES schemes for tagging segments.\nDropout. We compare no dropout, naive dropout, and variational dropout (Gal and Ghahramani, 2016). Naive dropout applies a new dropout mask at every time step of the LSTM-layers. Variational dropout applies the same dropout mask for all time steps in the same sentence. Further, it applies dropout to the recurrent units. We evaluate the dropout rates {0.05, 0.1, 0.25, 0.5}.\nClassifier. We evaluate a Softmax classifier as well as a CRF classifier as the last layer of the network.\nNumber of LSTM-layers. We evaluated 1, 2, and 3 stacked BiLSTM-layers.\nNumber of recurrent units. For each LSTM-layer, we selected independently a number of recurrent units from the set {25, 50, 75, 100, 125}.\nMini-batch sizes. We evaluate the mini-batch sizes 1, 8, 16, 32, and 64."}, {"heading": "5 Robust Model Evaluation", "text": "We have shown in section 3 that re-running nondeterministic approaches multiple times and comparing score distributions is essential to draw correct conclusions. However, to truly understand the capabilities of an approach, it is interesting to test the approach with different sets of hyperparameters for the complete network.\nTraining and tuning a neural network can be time consuming, sometimes taking multiple days to train a single instance of a network. A priori it is hard to know which hyperparameters will yield the best performance and the selection of the parameters often makes the difference between mediocre and state-of-the-art performance (Hutter et al., 2014). If an approach yields good performance only for a narrow set of parameters, it might be difficult to adapt the approach to new tasks, new domains\nor new languages, as a large range of possible parameters must be evaluated, each time requiring a significant amount of training time. Hence it is desirable, that the approach yields stable results for a wide range of parameters.\nIn order to find approaches that result in high performance and are robust against the remaining parameters, we decided to randomly sample several hundred network configurations from the set described in section 4.2. For each sampled configuration, we compare different options, e.g. different options for the last layer of the network. For example, we sampled in total 975 configurations and each configuration was trained with a Softmax classifier as well as with a CRF classifier, totaling to 1950 trained networks.\nOur results are presented in Table 3. The table shows that for the NER task 232 configurations were sampled randomly and for 210 of the 232 configurations (90.5%), the CRF setup achieved a better test performance than the setup with a Softmax classifier. To measure the difference between these two options, we compute the median of the absolute differences: Let Si be the test performance (F1-measure) for the Softmax setup for configuration i and Ci the test performance for the CRF setup. We then compute \u2206F1 = median(S1 \u2212 C1, S2 \u2212C2, . . . , S232 \u2212C232). For the NER task, the median difference was \u2206F1 = \u22120.66%, i.e. the setup with a Softmax classifier achieved on average an F1-score of 0.66 percentage points below that of the CRF setup.\nWe also evaluated the standard deviation of the F1-\nscores to detect approaches that are less dependent on the remaining hyperparameters and the random number generator. The standard deviation \u03c3 for the CRF-classifier is with 0.0060 significantly lower (p < 10\u22123 using Brown-Forsythe test) than for the Softmax classifier with \u03c3 = 0.0082."}, {"heading": "6 Results", "text": "This section highlights our main insights in the evaluation of different design choices for BiLSTM architectures. We limit the number of results we present for reasons of brevity. Detailed information can be found in (Reimers and Gurevych, 2017).11"}, {"heading": "6.1 Classifier", "text": "Table 3 shows a comparison between using a Softmax classifier as a last layer and using a CRF classifier. The BiLSTM-CRF architecture by Huang et al. (2015) achieves a better performance on 4 out of 5 tasks. For the NER task it further achieves a 27% lower standard deviation (statistically significant with p < 10\u22123), indicating that it is less sensitive to the remaining configuration of the network.\nThe CRF classifier only fails for the Event Detection task. This task has nearly no dependency between tags, as often only a single token is annotated as an event trigger in a sentence.\nWe studied the differences between these two classifiers in terms of number of LSTM-layers. As Figure 3 shows, a Softmax classifier profits from a deep LSTM-network with multiple stacked layers. On the other hand, if a CRF classifier is used, the effect of additional LSTM-layers is much smaller."}, {"heading": "6.2 Optimizer", "text": "We evaluated six optimizers with the suggested default configuration from their respective papers. We observed that SGD is quite sensitive towards the selection of the learning rate and it failed in many instances to converge. For the optimizers SGD, Adagrad and Adadelta we observed a large standard deviation in terms of test performance,\n11https://public.ukp.informatik. tu-darmstadt.de/reimers/Optimal_ Hyperparameters_for_Deep_LSTM-Networks. pdf\nwhich was for the NER task at 0.1328 for SGD, 0.0139 for Adagrad, and 0.0138 for Adadelta. The optimizers RMSProp, Adam, and Nadam on the other hand produced much more stable results. Not only were the medians for these three optimizers higher, but also the standard deviation was with 0.0096, 0.0091, and 0.0092 roughly 35% smaller in comparison to Adagrad. A large standard deviation indicates that the optimizer is sensitive to the hyperparameters as well as to the random initialization and bears the risk that the optimizer produces subpar results.\nThe best result was achieved by Nadam. For 453 out of 882 configurations (51.4%), it yielded the highest performance out of the six tested optimizers. For the NER task, it produced on average a 0.82 percentage points better performance than Adagrad.\nBesides test performance, the convergence speed is important in order to reduce training time. Here, Nadam had the best convergence speed. For the NER dataset, Nadam converged on average after 9 epochs, whereas SGD required 42 epochs."}, {"heading": "6.3 Word Embeddings", "text": "The pre-trained word embeddings had a large impact on the performance as shown in Table 4. The embeddings by Komninos and Manandhar (2016) resulted in the best performance for the POS, the Entities and the Events task. For the Chunking task, the dependency-based embeddings of Levy and Goldberg (2014) are slightly ahead of the Komninos embeddings, the significance level\nis at p = 0.025. For NER, the GloVe embeddings trained on common crawl perform on par with the Komninos embeddings (p = 0.391).\nWe observe that the underlying word embeddings have a large impact on the performance for all tasks. Well suited word embeddings are especially critical for datasets with small training sets. For the POS task we observe a median difference of 4.97% between the Komninos embeddings and the GloVe2 embeddings.\nNote we only evaluated the pre-trained embeddings provided by different authors, but not the underlying algorithms to generate these embeddings. The quality of word embeddings depends on many factors, including the size, the quality, and the preprocessing of the data corpus. As the corpora are not comparable, our results do not allow concluding that one approach is superior for generating word embeddings."}, {"heading": "6.4 Character Representation", "text": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al. (2016) using LSTM-networks to derive character-based representations.\nTable 5 shows that character-based representations yield a statistically significant difference only for the POS, the Chunking, and the Events task. For NER and Entity Recognition, the difference to not using a character-based representation is not significant (p > 0.01).\nThe difference between the CNN approach by Ma and Hovy (2016) and the LSTM approach by Lample et al. (2016) to derive a character-based representations is statistically insignificant for all tasks. This is quite surprising, as both approaches have fundamentally different properties: The CNN approach from Ma and Hovy (2016) takes only trigrams into account. It is also position independent, i.e. the network will not be able to distinguish between trigrams at the beginning, in the middle, or at the end of a word, which can be crucial information for some tasks. The BiLSTM approach from Lample et al. (2016) takes all characters of the word into account. Further, it is position aware, i.e. it can distinguish between characters at the start and at the end of the word. Intuitively, one would think that the LSTM approach by Lample et al. would be superior."}, {"heading": "6.5 Gradient Clipping and Normalization", "text": "For gradient clipping (Mikolov, 2012) we couldn\u2019t observe any improvement for the thresholds of 1, 3, 5, and 10 for any of the five tasks.\nGradient normalization has a better theoretical justification (Pascanu et al., 2013) and we can confirm with our experiments that it performs better. Not normalizing the gradient was the best option only for 5.6% of the 492 evaluated configurations (under null-hypothesis we would expect 20%). Which threshold to choose, as long as it is not too small or too large, is of lower importance. In most cases, a threshold of 1 was the best option (30.5% of the\ncases).\nWe observed a large performance increase compared to not normalizing the gradient. The median increase was between 0.29 percentage points F1score for the Chunking task and 0.82 percentage points for the POS task."}, {"heading": "6.6 Dropout", "text": "Dropout is a popular method to deal with overfitting for neural networks (Srivastava et al., 2014). We could observe that variational dropout (Gal and Ghahramani, 2016) clearly outperforms naive dropout and not using dropout. It was the best op-\ntion in 83.5% of the 479 evaluated configurations. The median performance increase in comparison to not using dropout was between 0.31 percentage points for the POS-task and 1.98 for the Entities task. We also observed a large improvement in comparison to naive dropout between 0.19 percentage points for the POS task and 1.32 percentage points for the Entities task. Variational dropout showed the smallest standard deviation, indicating that it is less dependent on the remaining hyperparameters and the random number sequence.\nWe further evaluated whether variational dropout should be applied to the output units of the LSTMnetwork, to the recurrent units, or to both. We observed that applying dropout to both dimensions gave in most cases (62.6%) the best results. The median performance increase was between 0.05 percentage points and 0.82 percentage points."}, {"heading": "6.7 Further Evaluated Parameters", "text": "The tagging schemes BIO and IOBES performed on par for 4 out of 5 tasks. For the Entities task, the BIO scheme significantly outperformed the IOBES scheme for 88.7% of the tested configurations. The median difference was \u2206F1 = \u22121.01%.\nFor the evaluated tasks, 2 stacked LSTM-layers achieved the best performance. For the POStagging task, 1 and 2 layers performed on par. For flat networks with a single LSTM-layer, around 150 recurrent units yielded the best performance. For networks with 2 or 3 layers, around 100 recurrent units per network yielded the best performance. However, the impact of the number of recurrent units was extremely small.\nFor tasks with small training sets, smaller minibatch sizes of 1 up to 16 appears to be a good choice. For larger training sets sizes of 8 - 32 appears to be a good choice. Mini-batch sizes of 64 usually performed worst."}, {"heading": "7 Conclusion", "text": "In this paper, we demonstrated that the sequence of random numbers has a statistically significant impact on the test performance and that wrong conclusions can be made if performance scores based on single runs are compared. We demonstrated this for the two recent state-of-the-art NER systems by Ma\nand Hovy (2016) and Lample et al. (2016). Based on the published performance scores, Ma and Hovy draw the conclusion of a significant improvement over the approach of Lample et al. Re-executing the provided implementations with different seed values however showed that the implementation of Lample et al. results in a superior score distribution generalizing better to unseen data.\nComparing score distributions reduces the risk of rejecting promising approaches or falsely accepting weaker approaches. Further it can lead to new insights on the properties of an approach. We demonstrated this for ten design choices and hyperparameters of LSTM-networks for five tasks.\nBy studying the standard deviation of scores, we estimated the dependence on hyperparameters and on the random seed value for different approaches. We showed that SGD, Adagrad and Adadelta have a higher dependence than RMSProp, Adam or Nadam. We have shown that variational dropout also reduces the dependence on the hyperparameters and on the random seed value. As future work, we will investigate if those methods are either less dependent on the hyperparameters or are less dependent on the random seed value, e.g. if they avoid converging to bad local minima.\nBy testing a large number of configurations, we showed that some choices consistently lead to superior performance and are less dependent on the remaining configuration of the network. Thus, there is a good chance that these configurations require less tuning when applied to new tasks or domains."}, {"heading": "Acknowledgements", "text": "This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1. Calculations for this research were conducted on the Lichtenberg high performance computer of the TU Darmstadt."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "CoRR, abs/1603.06042.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Enriching Word Vectors with Subword Information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606.", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Incorporating Nesterov Momentum into Adam", "author": ["Timothy Dozat"], "venue": null, "citeRegEx": "Dozat.,? \\Q2015\\E", "shortCiteRegEx": "Dozat.", "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "J. Mach. Learn. Res., 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Offspring from Reproduction Problems: What Replication Failure Teaches Us", "author": ["Antske Fokkens", "Marieke van Erp", "Marten Postma", "Ted Pedersen", "Piek Vossen", "Nuno Freire."], "venue": "ACL (1), pages 1691\u20131701. The Association for Computer Linguistics.", "citeRegEx": "Fokkens et al\\.,? 2013", "shortCiteRegEx": "Fokkens et al\\.", "year": 2013}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10,", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statis-", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Neural Networks for Machine Learning - Lecture 6a - Overview of mini-batch gradient descent", "author": ["Geoffrey Hinton"], "venue": null, "citeRegEx": "Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "Flat Minima", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(1):1\u201342.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997a", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997b", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "CoRR, abs/1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "An Efficient Approach for Assessing Hyperparameter Importance", "author": ["Frank Hutter", "Holger Hoos", "Kevin Leyton-Brown."], "venue": "Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, ICML\u201914, pages I\u2013", "citeRegEx": "Hutter et al\\.,? 2014", "shortCiteRegEx": "Hutter et al\\.", "year": 2014}, {"title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang."], "venue": "CoRR, abs/1609.04836.", "citeRegEx": "Keskar et al\\.,? 2016", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Dependency based embeddings for sentence classification tasks", "author": ["Alexandros Komninos", "Suresh Manandhar."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Komninos and Manandhar.,? 2016", "shortCiteRegEx": "Komninos and Manandhar.", "year": 2016}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "CoRR, abs/1603.01360.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Backpropagation Applied to Handwritten Zip Code Recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel."], "venue": "Neural Computation, 1(4):541\u2013551.", "citeRegEx": "LeCun et al\\.,? 1989", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Efficient BackProp", "author": ["Yann LeCun", "L\u00e9on Bottou", "Genevieve B. Orr", "Klaus-Robert M\u00fcller."], "venue": "Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 9\u201350, London, UK, UK. Springer-Verlag.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "DependencyBased Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Joint Event Extraction via Structured Prediction with Global Features", "author": ["Qi Li", "Heng Ji", "Liang Huang."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73\u201382, Sofia, Bulgaria. Association for", "citeRegEx": "Li et al\\.,? 2013", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "author": ["Xuezhe Ma", "Eduard H. Hovy."], "venue": "CoRR, abs/1603.01354.", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Comput. Linguist., 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "The kolmogorov-smirnov test for goodness of fit", "author": ["Frank J. Massey."], "venue": "Journal of the American Statistical Association, 46(253):68\u201378.", "citeRegEx": "Massey.,? 1951", "shortCiteRegEx": "Massey.", "year": 1951}, {"title": "Statistical language models based on neural networks", "author": ["Tom\u00e1\u0161 Mikolov."], "venue": "Ph.D. thesis, Brno University of Technology.", "citeRegEx": "Mikolov.,? 2012", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A method of solving a convex programming problem with convergence rate O(1/sqr(k))", "author": ["Yurii Nesterov."], "venue": "Soviet Mathematics Doklady, 27:372\u2013 376. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.", "citeRegEx": "Nesterov.,? 1983", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543. Nils Reimers and Iryna Gurevych. 2017. Optimal Hy-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics (Volume 2: Short Papers), pages 231\u2013235, Berlin, Germany", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Toutanova et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 1929}], "referenceMentions": [{"referenceID": 21, "context": "to achieve state-of-the-art performance for a wide range of NLP tasks, including many sequence tagging tasks (Ma and Hovy, 2016), dependency parsing (Andor et al.", "startOffset": 109, "endOffset": 128}, {"referenceID": 0, "context": "to achieve state-of-the-art performance for a wide range of NLP tasks, including many sequence tagging tasks (Ma and Hovy, 2016), dependency parsing (Andor et al., 2016), and machine translation (Wu et al.", "startOffset": 149, "endOffset": 169}, {"referenceID": 18, "context": "non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010).", "startOffset": 88, "endOffset": 128}, {"referenceID": 4, "context": "non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010).", "startOffset": 88, "endOffset": 128}, {"referenceID": 21, "context": "For the recent NER system by Ma and Hovy (2016) we observed that, depending on the random seed value, the performance on unseen data varies between 89.", "startOffset": 29, "endOffset": 48}, {"referenceID": 10, "context": "Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al.", "startOffset": 64, "endOffset": 99}, {"referenceID": 21, "context": "Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al., 2016; S\u00f8gaard and Goldberg, 2016).", "startOffset": 197, "endOffset": 265}, {"referenceID": 16, "context": "Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al., 2016; S\u00f8gaard and Goldberg, 2016).", "startOffset": 197, "endOffset": 265}, {"referenceID": 5, "context": "Fokkens et al. (2013) show that reproducing results is not always straightforward, as factors like preprocessing (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 18, "context": "The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010).", "startOffset": 139, "endOffset": 179}, {"referenceID": 4, "context": "The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010).", "startOffset": 139, "endOffset": 179}, {"referenceID": 4, "context": "Erhan et al. (2010) showed for the MNIST handwritten digit recognition task that different random seeds result in largely varying performances.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Figure 1: A conceptual sketch of flat and sharp minima from Keskar et al. (2016). The Y-axis indicates values of the error function and the Xaxis the weight-space.", "startOffset": 60, "endOffset": 81}, {"referenceID": 9, "context": "As (informally) defined by Hochreiter and Schmidhuber (1997a), a minimum can be flat, where the error function remains approximately constant for a large connected region in weight-space, or it can be sharp, where the error function increases rapidly in", "startOffset": 27, "endOffset": 62}, {"referenceID": 13, "context": "On the other hand, flat minima generalize better on new data (Keskar et al., 2016).", "startOffset": 61, "endOffset": 82}, {"referenceID": 4, "context": "Some methods like the weight initialization (Erhan et al., 2010; Glorot and Bengio, 2010) or small-batch training (Keskar et al.", "startOffset": 44, "endOffset": 89}, {"referenceID": 7, "context": "Some methods like the weight initialization (Erhan et al., 2010; Glorot and Bengio, 2010) or small-batch training (Keskar et al.", "startOffset": 44, "endOffset": 89}, {"referenceID": 13, "context": ", 2010; Glorot and Bengio, 2010) or small-batch training (Keskar et al., 2016) help to avoid bad (e.", "startOffset": 57, "endOffset": 78}, {"referenceID": 20, "context": "Two recent, state-of-the-art systems for NER are proposed by Ma and Hovy (2016)5 and by Lample et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 16, "context": "Two recent, state-of-the-art systems for NER are proposed by Ma and Hovy (2016)5 and by Lample et al. (2016)6.", "startOffset": 88, "endOffset": 109}, {"referenceID": 23, "context": "Using a Kolmogorov-Smirnov significance test (Massey, 1951), we observe a statistically significant difference between these two distributions (p < 0.", "startOffset": 45, "endOffset": 59}, {"referenceID": 17, "context": "The main difference between these two approaches is in the generation of character-based representations: Ma and Hovy uses a Convolutional Neural Network (CNN) (LeCun et al., 1989), while Lample et al.", "startOffset": 160, "endOffset": 180}, {"referenceID": 20, "context": "Table 1: The system by Ma and Hovy (2016) and Lample et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 16, "context": "Table 1: The system by Ma and Hovy (2016) and Lample et al. (2016) were run multiple times with different seed values.", "startOffset": 46, "endOffset": 67}, {"referenceID": 28, "context": "For Part-of-Speech tagging, we use the benchmark setup described by Toutanova et al. (2003). Using the full training set for POS tagging would hinder our ability to detect design choices that are consistently better than others.", "startOffset": 68, "endOffset": 92}, {"referenceID": 29, "context": "24% accuracy was achieved by Toutanova et al. (2003). Hence, we reduced the training set size from over 38.", "startOffset": 29, "endOffset": 53}, {"referenceID": 20, "context": "We use the same data split as Li et al. (2013). For the Event Detection task, we use the TempEval3 Task B setup.", "startOffset": 30, "endOffset": 47}, {"referenceID": 11, "context": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).", "startOffset": 61, "endOffset": 121}, {"referenceID": 21, "context": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).", "startOffset": 61, "endOffset": 121}, {"referenceID": 16, "context": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).", "startOffset": 61, "endOffset": 121}, {"referenceID": 11, "context": "We use a BiLSTM-network for sequence tagging as described in (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). To be able to evaluate a large number of different network configurations, we optimized our implementation for efficiency, reducing by a factor of 6 the time required per epoch compared to Ma and Hovy (2016).", "startOffset": 62, "endOffset": 331}, {"referenceID": 21, "context": "News)7 from Mikolov et al. (2013), the Bag of Words (Le.", "startOffset": 12, "endOffset": 34}, {"referenceID": 17, "context": ")8 by Levy and Goldberg (2014), three different GloVe embeddings9 from Pennington et al.", "startOffset": 6, "endOffset": 31}, {"referenceID": 17, "context": ")8 by Levy and Goldberg (2014), three different GloVe embeddings9 from Pennington et al. (2014) trained either on Wikipedia 2014 + Gigaword 5 (GloVe1 with 100 dimensions and GloVe2 with 300 dimensions) or on Common Crawl (GloVe3), and the Komninos and Manandhar (2016) embeddings (Komn.", "startOffset": 6, "endOffset": 96}, {"referenceID": 14, "context": "(2014) trained either on Wikipedia 2014 + Gigaword 5 (GloVe1 with 100 dimensions and GloVe2 with 300 dimensions) or on Common Crawl (GloVe3), and the Komninos and Manandhar (2016) embeddings (Komn.", "startOffset": 150, "endOffset": 180}, {"referenceID": 1, "context": "We also evaluate the approach of Bojanowski et al. (2016) (FastText), which trains embeddings for n-grams with length 3 to 6.", "startOffset": 33, "endOffset": 58}, {"referenceID": 20, "context": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 16, "context": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al. (2016) using LSTMnetworks to derive character-based representations.", "startOffset": 118, "endOffset": 139}, {"referenceID": 3, "context": "Besides Stochastic Gradient Descent (SGD), we evaluate Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 63, "endOffset": 83}, {"referenceID": 8, "context": ", 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 42, "endOffset": 56}, {"referenceID": 14, "context": ", 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 63, "endOffset": 84}, {"referenceID": 2, "context": ", 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 96, "endOffset": 109}, {"referenceID": 26, "context": ", 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton, 2012), Adam (Kingma and Ba, 2014), and Nadam (Dozat, 2015), an Adam variant that incorporates Nesterov momentum (Nesterov, 1983) as optimizers.", "startOffset": 163, "endOffset": 179}, {"referenceID": 24, "context": "uk/nlp/extvec/ ent problem are gradient clipping (Mikolov, 2012) and gradient normalization (Pascanu et al.", "startOffset": 49, "endOffset": 64}, {"referenceID": 6, "context": "We compare no dropout, naive dropout, and variational dropout (Gal and Ghahramani, 2016).", "startOffset": 62, "endOffset": 88}, {"referenceID": 12, "context": "A priori it is hard to know which hyperparameters will yield the best performance and the selection of the parameters often makes the difference between mediocre and state-of-the-art performance (Hutter et al., 2014).", "startOffset": 195, "endOffset": 216}, {"referenceID": 11, "context": "The BiLSTM-CRF architecture by Huang et al. (2015) achieves a better performance on 4 out of 5 tasks.", "startOffset": 31, "endOffset": 51}, {"referenceID": 15, "context": "The embeddings by Komninos and Manandhar (2016) resulted in the best performance for the POS, the Entities and the Events task.", "startOffset": 18, "endOffset": 48}, {"referenceID": 15, "context": "The embeddings by Komninos and Manandhar (2016) resulted in the best performance for the POS, the Entities and the Events task. For the Chunking task, the dependency-based embeddings of Levy and Goldberg (2014) are slightly ahead of the Komninos embeddings, the significance level is at p = 0.", "startOffset": 18, "endOffset": 211}, {"referenceID": 20, "context": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 16, "context": "We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the approach of Lample et al. (2016) using LSTM-networks to derive character-based representations.", "startOffset": 118, "endOffset": 139}, {"referenceID": 20, "context": "The difference between the CNN approach by Ma and Hovy (2016) and the LSTM approach by Lample et al.", "startOffset": 43, "endOffset": 62}, {"referenceID": 16, "context": "The difference between the CNN approach by Ma and Hovy (2016) and the LSTM approach by Lample et al. (2016) to derive a character-based representations is statistically insignificant for all tasks.", "startOffset": 87, "endOffset": 108}, {"referenceID": 21, "context": "This is quite surprising, as both approaches have fundamentally different properties: The CNN approach from Ma and Hovy (2016) takes only trigrams into account.", "startOffset": 108, "endOffset": 127}, {"referenceID": 16, "context": "The BiLSTM approach from Lample et al. (2016) takes all characters of the word into account.", "startOffset": 25, "endOffset": 46}, {"referenceID": 24, "context": "For gradient clipping (Mikolov, 2012) we couldn\u2019t observe any improvement for the thresholds of 1, 3, 5, and 10 for any of the five tasks.", "startOffset": 22, "endOffset": 37}, {"referenceID": 16, "context": "2016) or LSTMs (Lample et al., 2016) to derive character-based representations.", "startOffset": 15, "endOffset": 36}, {"referenceID": 28, "context": "Dropout is a popular method to deal with overfitting for neural networks (Srivastava et al., 2014).", "startOffset": 73, "endOffset": 98}, {"referenceID": 6, "context": "We could observe that variational dropout (Gal and Ghahramani, 2016) clearly outperforms naive dropout and not using dropout.", "startOffset": 42, "endOffset": 68}, {"referenceID": 20, "context": "We demonstrated this for the two recent state-of-the-art NER systems by Ma and Hovy (2016) and Lample et al.", "startOffset": 72, "endOffset": 91}, {"referenceID": 16, "context": "We demonstrated this for the two recent state-of-the-art NER systems by Ma and Hovy (2016) and Lample et al. (2016). Based on the published performance scores, Ma and Hovy draw the conclusion of a significant improvement over the approach of Lample et al.", "startOffset": 95, "endOffset": 116}], "year": 2017, "abstractText": "In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant (p < 10\u22124) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F1-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTMnetworks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters. The full experimental results are published in (Reimers and Gurevych, 2017).1 The implementation of our network is publicly available.2", "creator": "LaTeX with hyperref package"}}}