{"id": "1707.09920", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2017", "title": "Regularization techniques for fine-tuning in neural machine translation", "abstract": "we investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out - of - domain dataset is adapted manually to a small in - domain dataset. in this scenario, overfitting is a major challenge. we investigate a number of alternative techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout translation and l2 - regularization towards an out - of - domain prior. in addition, we introduce tuneout, a novel regularization technique inspired further by dropout. we apply four these techniques, run alone and paired in combination, to employ neural machine translation, obtaining improvements on iwslt datasets for english - & gt ; german and english - & gt ; russian. we also investigate the amounts of in - domain training data needed for domain adaptation in nmt, solve and find a logarithmic relationship between the amount of training data and gain in determining bleu score.", "histories": [["v1", "Mon, 31 Jul 2017 15:31:12 GMT  (34kb,D)", "http://arxiv.org/abs/1707.09920v1", "EMNLP 2017 short paper; for bibtex, seethis http URL"]], "COMMENTS": "EMNLP 2017 short paper; for bibtex, seethis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["antonio valerio miceli barone", "barry haddow", "ulrich germann", "rico sennrich"], "accepted": true, "id": "1707.09920"}, "pdf": {"name": "1707.09920.pdf", "metadata": {"source": "CRF", "title": "Regularization techniques for fine-tuning in neural machine translation", "authors": ["Antonio Valerio Miceli Barone", "Barry Haddow", "Ulrich Germann", "Rico Sennrich"], "emails": ["ugermann}@inf.ed.ac.uk", "rico.sennrich@ed.ac.uk"], "sections": [{"heading": null, "text": "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset.\nIn this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English\u2192German and English\u2192Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score."}, {"heading": "1 Introduction", "text": "Neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014) has established itself as the new state of the art at recent shared translation tasks (Bojar et al., 2016; Cettolo et al., 2016). In order to achieve good generalization accuracy, neural machine translation, like most other large machine learning systems, requires large amounts of training examples sampled from a distribution as close as possible to the distribution of the inputs seen during execution. However, in many applications, only a small amount of parallel text is available for the specific application domain, and it is\ntherefore desirable to leverage larger out-domain datasets.\nOwing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data. This strategy was also found to be very effective for neural machine translation (Luong and Manning, 2015; Sennrich et al., 2016b).\nSince the amount of in-domain data is typically small, overfitting is a concern. A common solution is early stopping on a small held-out in-domain validation dataset, but this reduces the amount of in-domain data available for training.\nIn this paper, we show that we can make finetuning strategies for neural machine translation more robust by using several regularization techniques. We consider fine-tuning with varying amounts of in-domain training data, showing that improvements are logarithmic in the amount of indomain data.\nWe investigate techniques where domain adaptation starts from a pre-trained out-domain model, and only needs to process the in-domain corpus. Since we do not need to process the large out-domain corpus during adaptation, this is suitable for scenarios where adaptation must be performed quickly or where the original outdomain corpus is not available. Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features (Daume III, 2007; Finkel and Manning, 2009; Wuebker et al., 2015). These techniques are largely orthogonal to\nar X\niv :1\n70 7.\n09 92\n0v 1\n[ cs\n.C L\n] 3\n1 Ju\nl 2 01\n7\nours1 and can be used in combination. In fact, Chu et al. (2017) successfully apply fine-tuning in combination with joint training."}, {"heading": "2 Regularization Techniques for Transfer Learning", "text": "Overfitting to the small amount of in-domain training data that may be available is a major challenge in transfer learning for domain adaptation. We investigate the effect of different regularization techniques to reduce overfitting, and improve the quality of transfer learning."}, {"heading": "2.1 Dropout", "text": "The first variant that we consider is fine-tuning with dropout. Dropout (Srivastava et al., 2014) is a stochastic regularization technique for neural networks. In particular, we consider \"Bayesian\" dropout for recurrent neural networks (Gal and Ghahramani, 2016).\nIn this technique, during training, the columns of the weight matrices of the neural network are randomly set to zero, independently for each example and each epoch, but with the caveat that when the same weight matrix appears multiple times in the unrolled computational graph of a given example, the same columns are zeroed.\nFor an arbitrary layer that takes an input vector h and computes the pre-activation vector v (ignoring the bias parameter),\nvi,j = W \u00b7MW,i,j \u00b7 hi,j (1)\nwhere MW,i,j = 1pdiag(Bernoulli \u2297n(p)) is the dropout mask for matrix W and training example i seen in epoch j. This mask is a diagonal matrix whose entries are drawn from independent Bernoulli random variables with probability p and then scaled by 1/p. Gal and Ghahramani (2016) have shown that this corresponds to approximate variational Bayesian inference over the weight matrices considered as model-wise random variables, where the individual weights have a Gaussian prior with zero mean and small diagonal covariance. During execution we simply set the dropout masks to identity matrices, as in the standard approximation scheme.\nSince dropout is not a specific transfer learning technique per se, we can apply it during finetuning, irrespective of whether or not the orig1 although in the special case of linear models, they are related\nto MAP-L2 fine-tuning.\ninal out-of-domain model was also trained with dropout."}, {"heading": "2.2 MAP-L2", "text": "L2-norm regularization is widely used for machine learning and statistical models. For linear models, it corresponds to imposing a diagonal Gaussian prior with zero mean on the weights. Chelba and Acero (2006) extended this technique to transfer learning by penalizing the weights of the in-domain model by their L2-distance from the weights of the previously trained out-of-domain model.\nFor each parameter matrix W , the penalty term is LW = \u03bb \u00b7 \u2225\u2225\u2225W \u2212 W\u0302\u2225\u2225\u22252\n2 (2)\nwhere W is the in-domain parameter matrix to be learned and W\u0302 is the corresponding fixed out-ofdomain parameter matrix. Bias parameters may be regularized as well. For linear models, this corresponds to maximum a posteriori inference w.r.t. a diagonal Gaussian prior with mean equal to the out-of-domain parameters and 1/\u03bb variance.\nTo our knowledge this method has not been applied to neural networks, except for a recent work by Kirkpatrick et al. (2017) which investigates a variant of it for continual learning (learning a new task while preserving performance on previously learned task) rather than domain adaptation. In this work we investigate L2-distance from out-ofdomain penalization (MAP-L2) as a domain adaptation technique for neural machine translation."}, {"heading": "2.3 Tuneout", "text": "We also propose a novel transfer learning technique which we call tuneout. Like Bayesian dropout, we randomly drop columns of the weight matrices during training, but instead of setting them to zero, we set them to the corresponding columns of the out-of-domain parameter matrices.\nThis can be alternatively seen as learning matrices of parameter differences between in-domain and out-of-domain models with standard dropout, starting from a zero initialization at the beginning of fine-tuning. Therefore, equation 2 becomes\nvi,j = (W\u0302 + \u2206W \u00b7M\u2206W,i,j) \u00b7 hi,j (3)\nwhere W\u0302 is the fixed out-of-domain parameter matrix and \u2206W is the parameter difference matrix to be learned and M\u2206W,i,j is a Bayesian dropout mask."}, {"heading": "3 Evaluation", "text": "We evaluate transfer learning on test sets from the IWSLT shared translation task (Cettolo et al., 2012)."}, {"heading": "3.1 Data and Methods", "text": "Test sets consist of transcripts of TED talks and their translations; small amounts of in-domain training data are also provided. For English-toGerman we use IWSLT 2015 training data, while for English-to-Russian we use IWSLT 2014 training data. For the out-of-domain systems, we use training data from the WMT shared translation task,2 which is considered permissible for IWSLT tasks, including back-translations of monolingual training data (Sennrich et al., 2016b), i.e., automatic translations of data available only in target language \u201cback\u201d into the source language.3.\nWe train out-of-domain systems following tools and hyperparameters reported by Sennrich et al. (2016a), using Nematus (Sennrich et al., 2017) as the neural machine translation toolkit. We differ from their setup only in that we use Adam (Kingma and Ba, 2015) for optimization. Our baseline fine-tuning models use the same hyperparameters, except that the learning rate is 4 times smaller and the validation frequency for early stopping 4 times higher. Early stopping serves an important function as the only form of regularization in the baseline fine-tuning model. We also use this configuration for the in-domain only baselines.\nAfter some exploratory experiments for English-to-German, we set dropout retention probabilities to 0.9 for word-dropout and 0.8 for all the other parameter matrices. Tuneout retention probabilities are set to 0.6 (word-dropout) and 0.2 (other parameters). For MAP-L2 regularization, we found that a penalty of 10\u22123 per mini-batch performs best. For English-to-Russian, retention probabilities of 0.95 (word-dropout) 0.89 (other parameters) for both dropout and tuneout performed best.\nThe out-of-domain training data consists of about 7.92M sentence pairs for English-toGerman and 4.06M sentence pairs for English-toRussian. In-domain training data is about 206k sentence pairs for English-to-German and 181k sentence pairs for English-to-Russian. Training 2 http://www.statmt.org/wmt16/ 3 http://data.statmt.org/rsennrich/wmt16_backtranslations/\ndata is tokenized, truecased and segmented into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016c).\nFor replicability and ease of adoption, we include our implementation of dropout and MAP-L2 in the master branch of Nematus. Tuneout regularization is available in a separate code branch of Nematus.4"}, {"heading": "3.2 Results", "text": "We report the translation quality in terms of NISTBLEU scores of our models in Table 1 for Englishto-German and Table 2 for English-to-Russian. Statistical significance on the concatenated test sets scores is determined via bootstrap resampling (Koehn, 2004).\nDropout and MAP-L2 improve translation quality when fine-tuning both separately and in combination. When the two methods are used in combination, the improvements are significant at 5% for both language pairs, while in isolation dropout is non-significant and MAP-L2 is only significant for English-to-Russian. Tuneout does not yield improvements for English-to-German, in fact it is significantly worse, but yields a small, nonsignificant improvement for English-to-Russian.\nIn order to obtain a better picture of the training dynamics, we plot training curves5 for several of our English-to-German models in Figure 1.\n4 https://github.com/EdinburghNLP/nematus/tree/ tuneout-branch 5 These BLEU scores are computed using Moses multi-bleu.perl which gives slightly different results than NIST mteval-v13a.pl that is used for Table 1.\nBaseline fine-tuning starts to noticeably overfit between the second and third epoch (1 epoch \u2248 104 mini-batches), while dropout, MAP-L2 and tuneout seem to converge without displaying noticeable overfitting.\nIn our experiments, all forms of regularization, including early stopping, have shown to be successful at mitigating the effect of overfitting. Still, our results suggest that there is value in not relying only on early stopping:\n\u2022 our results suggest that multiple regularizers outperform a single one.\n\u2022 if the amount of in-domain data is very small, we may want to use all of it for fine-tuning, and not hold out any for early stopping.\nTo evaluate different fine-tuning streategies on varying amounts of in-domain data, we tested finetuning with random samples of in-domain data, ranging from 10 sentence pairs to the full data set of 206k sentence pairs. Fine-tuning with low amounts of training data is of special interest for online adaptation scenarios where a system is fed back post-edited translation.6 Results are shown 6 We expect even bigger gains in that scenario because we\nwould not train on a random sample, but on translations that are conceivably from the same document.\nin Figure 2. The results show an approximately logarithmic relation between the size of the in-domain training set and BLEU. We consider three baseline approaches: fine-tuning for a fixed number of epochs (1 or 5), or early stopping. All three baseline approaches have their disadvantages. Fine-tuning for 1 epoch shows underfitting on small amounts of data (less than 1,000 sentence pairs); fine-tuning for 5 epochs overfits on 500-200,000 sentence pairs. Early stopping is generally a good strategy, but it requires an in-domain held-out dataset.\nOn the same amount of data, regularization (dropout+MAP-L2) leads to performance that is better (or no worse) than the baseline with only early stopping. Fine-tuning with regularization is also more stable, and if we have no access to a in-domain valdiation set for early stopping, can be run for a fixed number of epochs with little or no accuracy loss."}, {"heading": "4 Conclusion", "text": "We investigated fine-tuning for domain adaptation in neural machine translation with different amounts of in-domain training data, and strategies to avoid overfitting. We found that our baseline that relies only on early stopping has a strong performance, but fine-tuning with recurrent dropout and with MAP-L2 regularization yield additional small improvements of the order of 0.3 BLEU points for both English-to-German and English-toRussian, while the improvements in terms of final translation accuracy of tuneout appear to be less consistent.\nFurthermore, we found that regularization techniques that we considered make training more robust to overfitting, which is particularly helpful in scenarios where only small amounts of in-domain data is available, making early-stopping impractical as it relies on a sufficiently large in-domain validation set. Given the results of our experiments, we recommend using both dropout and MAP-L2 regularization for fine-tuning tasks, since they are easy to implement, efficient, and yield improvements while stabilizing training. We also present a learning curve that shows a logarithmic relationship between the amount of in-domain training data and the quality of the adapted system.\nOur techniques are not specific to neural machine translation, and we propose that they could be also tried for other neural network architectures\nand other tasks."}, {"heading": "Acknowledgments", "text": "This project has received funding from the European Union\u2019s Horizon 2020 re-\nsearch and innovation programme under grant agreements 644333 (TraMOOC) and 645487 (ModernMT). We also thank Booking.com for their support."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Findings of the 2016 Conference on Machine Translation (WMT16)", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Machine Translation, Vol-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT), pages 261\u2013268, Trento,", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Report on the 13th IWSLT Evaluation Campaign", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico."], "venue": "IWSLT 2016, Seattle, USA.", "citeRegEx": "Cettolo et al\\.,? 2016", "shortCiteRegEx": "Cettolo et al\\.", "year": 2016}, {"title": "Adaptation of maximum entropy capitalizer: Little data can help a lot", "author": ["Ciprian Chelba", "Alex Acero."], "venue": "Computer Speech & Language, 20(4):382\u2013399.", "citeRegEx": "Chelba and Acero.,? 2006", "shortCiteRegEx": "Chelba and Acero.", "year": 2006}, {"title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation", "author": ["Chenhui Chu", "Raj Dabre", "Sadao Kurohashi."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Vancouver,", "citeRegEx": "Chu et al\\.,? 2017", "shortCiteRegEx": "Chu et al\\.", "year": 2017}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["Hal Daume III."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256\u2013263, Prague, Czech Republic. Association for Computational Linguistics.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Hierarchical Bayesian Domain Adaptation", "author": ["Jenny Rose Finkel", "Christopher D. Manning."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Finkel and Manning.,? 2009", "shortCiteRegEx": "Finkel and Manning.", "year": 2009}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29 (NIPS).", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov."], "venue": "Science, 313(5786):504\u2013507.", "citeRegEx": "Hinton and Salakhutdinov.,? 2006", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations, San Diego, California, USA.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["Raia Hadsell."], "venue": "Proceedings of the National Academy of Sciences, 114(13):3521\u20133526.", "citeRegEx": "Hadsell.,? 2017", "shortCiteRegEx": "Hadsell.", "year": 2017}, {"title": "Statistical Significance Tests for Machine Translation Evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of EMNLP 2004, pages 388\u2013395, Barcelona, Spain.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Stanford Neural Machine Translation Systems for Spoken Language Domains", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the International Workshop on Spoken Language Translation 2015, Da Nang, Vietnam.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Unsupervised and Transfer Learning Challenge: a Deep Learning Approach", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Xavier Glorot", "Salah Rifai", "Yoshua Bengio", "Ian J Goodfellow", "Erick Lavoie", "Xavier Muller", "Guillaume Desjardins", "David Warde-Farley"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2012}, {"title": "Nematus: a Toolkit for Neural Machine", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Edinburgh Neural Machine Translation Systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation, pages 371\u2013 376, Berlin, Germany. Association for Computa-", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013", "citeRegEx": "Sennrich et al\\.,? 2016c", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, pages 3104\u20133112,", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Hierarchical Incremental Adaptation for Statistical Machine Translation", "author": ["Joern Wuebker", "Spence Green", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1059\u20131065, Lisbon,", "citeRegEx": "Wuebker et al\\.,? 2015", "shortCiteRegEx": "Wuebker et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014) has established itself as the new state of the art at recent shared translation tasks (Bojar et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 20, "context": "Neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014) has established itself as the new state of the art at recent shared translation tasks (Bojar et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 3, "context": ", 2014) has established itself as the new state of the art at recent shared translation tasks (Bojar et al., 2016; Cettolo et al., 2016).", "startOffset": 94, "endOffset": 136}, {"referenceID": 9, "context": "Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data.", "startOffset": 170, "endOffset": 246}, {"referenceID": 14, "context": "Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data.", "startOffset": 170, "endOffset": 246}, {"referenceID": 22, "context": "Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data.", "startOffset": 170, "endOffset": 246}, {"referenceID": 13, "context": "This strategy was also found to be very effective for neural machine translation (Luong and Manning, 2015; Sennrich et al., 2016b).", "startOffset": 81, "endOffset": 130}, {"referenceID": 17, "context": "This strategy was also found to be very effective for neural machine translation (Luong and Manning, 2015; Sennrich et al., 2016b).", "startOffset": 81, "endOffset": 130}, {"referenceID": 7, "context": "Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features (Daume III, 2007; Finkel and Manning, 2009; Wuebker et al., 2015).", "startOffset": 141, "endOffset": 206}, {"referenceID": 21, "context": "Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features (Daume III, 2007; Finkel and Manning, 2009; Wuebker et al., 2015).", "startOffset": 141, "endOffset": 206}, {"referenceID": 5, "context": "In fact, Chu et al. (2017) successfully apply fine-tuning in combination with joint training.", "startOffset": 9, "endOffset": 27}, {"referenceID": 19, "context": "Dropout (Srivastava et al., 2014) is a stochastic regularization technique for neural networks.", "startOffset": 8, "endOffset": 33}, {"referenceID": 8, "context": "In particular, we consider \"Bayesian\" dropout for recurrent neural networks (Gal and Ghahramani, 2016).", "startOffset": 76, "endOffset": 102}, {"referenceID": 8, "context": "Gal and Ghahramani (2016) have shown that this corresponds to approximate variational Bayesian inference over the weight matrices considered as model-wise random variables, where the individual weights have a Gaussian prior with zero mean and small diagonal covariance.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Chelba and Acero (2006) extended this technique to transfer learning by penalizing the weights of the in-domain model by their L2-distance from the weights of the previously trained out-of-domain model.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "We evaluate transfer learning on test sets from the IWSLT shared translation task (Cettolo et al., 2012).", "startOffset": 82, "endOffset": 104}, {"referenceID": 17, "context": "For the out-of-domain systems, we use training data from the WMT shared translation task,2 which is considered permissible for IWSLT tasks, including back-translations of monolingual training data (Sennrich et al., 2016b), i.", "startOffset": 197, "endOffset": 221}, {"referenceID": 15, "context": "(2016a), using Nematus (Sennrich et al., 2017) as the neural machine translation toolkit.", "startOffset": 23, "endOffset": 46}, {"referenceID": 10, "context": "We differ from their setup only in that we use Adam (Kingma and Ba, 2015) for optimization.", "startOffset": 52, "endOffset": 73}, {"referenceID": 14, "context": "We train out-of-domain systems following tools and hyperparameters reported by Sennrich et al. (2016a), using Nematus (Sennrich et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 18, "context": "data is tokenized, truecased and segmented into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016c).", "startOffset": 93, "endOffset": 117}, {"referenceID": 12, "context": "Statistical significance on the concatenated test sets scores is determined via bootstrap resampling (Koehn, 2004).", "startOffset": 101, "endOffset": 114}], "year": 2017, "abstractText": "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English\u2192German and English\u2192Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.", "creator": "LaTeX with hyperref package"}}}