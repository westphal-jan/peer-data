{"id": "1708.00075", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2017", "title": "Efficient Regret Minimization in Non-Convex Games", "abstract": "we consider regret minimization in repeated games with distinct non - convex loss functions. minimizing the standard notion of regret is computationally intractable. thus, we define a natural notion of regret which permits efficient optimization and effectively generalizes polynomial offline guarantees for convergence parallel to an approximate local optimum. likewise we then give gradient - slice based methods that achieve optimal regret, which in turn guarantee perfect convergence to equilibrium in this framework.", "histories": [["v1", "Mon, 31 Jul 2017 21:23:29 GMT  (176kb,D)", "http://arxiv.org/abs/1708.00075v1", "Published as a conference paper at ICML 2017"]], "COMMENTS": "Published as a conference paper at ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.GT stat.ML", "authors": ["elad hazan", "karan singh", "cyril zhang"], "accepted": true, "id": "1708.00075"}, "pdf": {"name": "1708.00075.pdf", "metadata": {"source": "CRF", "title": "Efficient Regret Minimization in Non-Convex Games", "authors": ["Elad Hazan", "Karan Singh", "Cyril Zhang"], "emails": ["ehazan@cs.princeton.edu,", "karans@cs.princeton.edu,", "cyril.zhang@princeton.edu,"], "sections": [{"heading": null, "text": "standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework."}, {"heading": "1 Introduction", "text": "Repeated games with non-convex utility functions serve to model many natural settings, such as multiplayer games with risk-averse players and adversarial (e.g. GAN) training. However, standard regret minimization and equilibria computation with general non-convex losses are computationally hard. This paper studies computationally tractable notions of regret minimization and equilibria in non-convex repeated games.\nRegret minimization in games typically amounts to repeated play in which the decision maker accumulates an average loss proportional to that of the best fixed decision in hindsight. This is a global notion with respect to the decision set of the player. If the loss functions are convex (or, as often considered, linear) restricted to the actions of the other players, then this notion of global optimization is computationally tractable. It can be shown that under certain conditions, players that minimize regret converge in various notions to standard notions of equilibrium, such as Nash equilibrium, correlated equilibrium, and coarse correlated equilibrium. This convergence crucially relies on the global optimality guaranteed by regret.\nIn contrast, it is NP-hard to compute the global minimum of a non-convex function over a convex domain. Rather, efficient non-convex continuous optimization algorithms focus on finding a local minimum. We thus consider notions of equilibrium that can be obtained from local optimality conditions of the players with respect to each-others\u2019 strategies. This requires a different notion of regret whose minimization guarantees convergence to a local minimum.\nThe rest of the paper is organized as follows. After briefly discussing why standard regret is not a suitable metric of performance, we introduce and motivate local regret, a surrogate for regret to the non-convex world. We then proceed to give efficient algorithms for non-convex online learning with optimal guarantees for this new objective. In analogy with the convex setting, we discuss the way our framework captures the offline and stochastic cases. In the final section, we describe a game-theoretic solution concept which is intuitively appealing, and, in contrast to other equilibria, efficiently attainable in the non-convex setting by simple algorithms."}, {"heading": "1.1 Related work", "text": "The field of online learning is by now rich with a diverse set of algorithms for extremely general scenarios, see e.g. [CBL06]. For bounded cost functions over a bounded domain, it is well known that versions of the multiplicative weights method gives near-optimal regret bounds [Cov91, Vov90, AHK12].\n\u2217ehazan@cs.princeton.edu, Department of Computer Science, Princeton University \u2020karans@cs.princeton.edu, Department of Computer Science, Princeton University \u2021cyril.zhang@princeton.edu, Department of Computer Science, Princeton University\nar X\niv :1\n70 8.\n00 07\n5v 1\n[ cs\n.L G\n] 3\n1 Ju\nl 2 01\n7\nDespite the tremendous generality in terms of prediction, the multiplicative weights method in its various forms yields only exponential-time algorithms for these general scenarios. This is inevitable, since regret minimization implies optimization, and general non-convex optimization is NP-hard. Convex forms of regret minimization have dominated the learning literature in recent years due to the fact that they allow for efficient optimization, see e.g. [Haz16, SS11].\nNon-convex mathematical optimization algorithms typically find a local optimum. For smooth optimization, gradient-based methods are known to find a point with gradient of squared norm at most \u03b5 in O( 1\u03b5 ) iterations [Nes04].1 A rate of O( 1\u03b52 ) is known for stochastic gradient descent [GL13]. Further accelerations in terms of the dimension are possible via adaptive regularization [DHS11].\nRecently, stochastic second-order methods have been considered, which enable even better guarantees for non-convex optimization: not only is the gradient at the point returned small, but the Hessian is also guaranteed to be close to positive semidefinite (i.e. the objective function is locally almost-convex), see e.g. [EM15, CDHS16, AAZB+16, ABH16].\nThe relationship between regret minimization and learning in games has been considered in both the machine learning literature, starting with [FS97], and the game theory literature by [HMC00]. Motivated by [HMC00], [BM05] study reductions from internal to external regret, and [HK07] relate the computational efficiency of these reductions to fixed point computations."}, {"heading": "2 Setting", "text": "We begin by introducing the setting of online non-convex optimization, which is modeled as a game between a learner and an adversary. During each iteration t, the learner is tasked with predicting xt from K \u2286 Rn, a convex decision set. Concurrently, the adversary chooses a loss function ft : K \u2192 R; the learner then observes ft(x) (via access to a first-order oracle) and suffers a loss of ft(xt). This procedure of play is repeated across T rounds.\nThe performance of the learner is measured through its regret, which is defined as a function of the loss sequence f1, . . . , fT and the sequence of online decisions x1, . . . , xT made by the learner. We discuss our choice of regret measure at length in Section 2.2.\nThroughout this paper, we assume the following standard regularity conditions:\nAssumption 2.1. We assume the following is true for each loss function ft:\n(i) ft is bounded: |ft(x)| \u2264M.\n(ii) ft is L-Lipschitz: |ft(x)\u2212 ft(y)| \u2264 L\u2016x\u2212 y\u2016.\n(iii) ft is \u03b2-smooth (has a \u03b2-Lipschitz gradient):\n\u2016\u2207ft(x)\u2212\u2207ft(y)\u2016 \u2264 \u03b2\u2016x\u2212 y\u2016."}, {"heading": "2.1 Projected gradients and constrained non-convex optimization", "text": "In constrained non-convex optimization, minimizing the gradient presents difficult computational challenges. In general, even when objective functions are smooth and bounded, local information may provide no information about the location of a stationary point. This motivates us to refine our search criteria.\nConsider, for example, the function sketched in Figure 1. In this construction, defined on the hypercube in Rn, the unique point with a vanishing gradient is a hidden valley, and gradients outside this valley are all identical. Clearly, it is hopeless in an information-theoretic sense to find this point efficiently: the number of value or gradient evaluations of this function must be exp(\u2126(n)) to discover the valley.\n1We note here that we measure the squared norm of the gradient, since it is more compatible with convex optimization. The mathematical optimization literature sometimes measures the norm of the gradient without squaring it.\nTo circumvent such inherently difficult and degenerate cases, we relax our conditions, and try to find a vanishing projected gradient. In this section, we introduce this notion formally, and motivate it as a natural quantity of interest to capture the search for local minima in constrained non-convex optimization.\nDefinition 2.2 (Projected gradient). Let f : K \u2192 R be a differentiable function on a closed (but not necessarily bounded) convex set K \u2286 Rn. Let \u03b7 > 0. We define \u2207K,\u03b7f : K \u2192 Rn, the (K, \u03b7)-projected gradient of f , by\n\u2207K,\u03b7f(x) def =\n1 \u03b7 (x\u2212\u03a0K [x\u2212 \u03b7\u2207f(x)]) ,\nwhere \u03a0K[\u00b7] denotes the orthogonal projection onto K.\nThis can be viewed as a surrogate for the gradient which ensures that the gradient descent step always lies within K, by transforming it into a projected gradient descent step. Indeed, one can verify by definition that\nx\u2212 \u03b7\u2207K,\u03b7(x) = \u03a0K [x\u2212 \u03b7\u2207f(x)] .\nIn particular, when K = Rn,\n\u2207K,\u03b7f(x) = 1\n\u03b7 (x\u2212 x+ \u03b7\u2207f(x)) = \u2207f(x),\nand we retrieve the usual gradient at all x. We first note that there always exists a point with vanishing projected gradient.\nProposition 2.3. Let K be a compact convex set, and suppose f : K \u2192 R satisfies Assumption 2.1. Then, there exists some point x\u2217 \u2208 K for which\n\u2207K,\u03b7f(x\u2217) = 0.\nProof. Consider the map g : K \u2192 K, defined by\ng(x) def = x\u2212 \u03b7\u2207K,\u03b7f(x) = \u03a0K [x\u2212 \u03b7\u2207f(x)] .\nThis is a composition of continuous functions (noting that the smoothness assumption implies that \u2207f is continuous), and is therefore continuous. Thus g satisfies the conditions for Brouwer\u2019s fixed point theorem, implying that there exists some x\u2217 \u2208 K for which g(x\u2217) = x\u2217. At this point, the projected gradient vanishes.\nIn the limit where \u03b7\u2016\u2207f(x)\u2016 is infinitesimally small, the projected gradient is equal to the gradient in the interior of K; on the boundary of K, it is the gradient with its outward-facing component removed. This exactly captures the first-order condition for a local minimum.\nThe final property that we note here is that an approximate local minimum, as measured by a small projected gradient, is robust with respect to small perturbations.\nProposition 2.4. Let x be any point in K \u2286 Rn, and let f, g be differentiable functions K \u2192 R. Then, for any \u03b7 > 0,\n\u2016\u2207K,\u03b7[f + g](x)\u2016 \u2264 \u2016\u2207K,\u03b7f(x)\u2016+ \u2016\u2207g(x)\u2016.\nProof. Let u = x+\u03b7\u2207f(x), and v = u+\u03b7\u2207g(x). Define their respective projections u\u2032 = \u03a0K [u] , v\u2032 = \u03a0K [v], so that u\u2032 = x\u2212 \u03b7\u2207K,\u03b7f(x) and v\u2032 = x\u2212 \u03b7\u2207K,\u03b7[f + g](x). We first show that \u2016u\u2032 \u2212 v\u2032\u2016 \u2264 \u2016u\u2212 v\u2016.\nBy the generalized Pythagorean theorem for convex sets, we have both \u3008u\u2032 \u2212 v\u2032, v \u2212 v\u2032\u3009 \u2264 0 and \u3008v\u2032 \u2212 u\u2032, u\u2212 u\u2032\u3009 \u2264 0. Summing these, we get\n\u3008u\u2032 \u2212 v\u2032, u\u2032 \u2212 v\u2032 \u2212 (u\u2212 v)\u3009 \u2264 0 =\u21d2 \u2016u\u2032 \u2212 v\u2032\u20162 \u2264 \u3008u\u2032 \u2212 v\u2032, u\u2212 v\u3009\n\u2264 \u2016u\u2032 \u2212 v\u2032\u2016 \u00b7 \u2016u\u2212 v\u2016,\nas claimed. Finally, by the triangle inequality, we have\n\u2016\u2207K,\u03b7[f + g](x)\u2016 \u2212 \u2016\u2207K,\u03b7f(x)\u2016 \u2264 \u2016\u2207K,\u03b7[f + g](x)\u2212\u2207K,\u03b7f(x)\u2016\n= 1\n\u03b7 \u2016u\u2032 \u2212 v\u2032\u2016\n\u2264 1 \u03b7 \u2016u\u2212 v\u2016 = \u2016\u2207g(x)\u2016,\nas required.\nIn particular, this fact immediately implies that \u2016\u2207K,\u03b7f(x)\u2016 \u2264 \u2016\u2207f(x)\u2016. As we demonstrate later, looking for a small projected gradient becomes a feasible task. In Figure 1\nabove, such a point exists on the boundary of K, even when there is no \u201chidden valley\u201d at all."}, {"heading": "2.2 A local regret measure", "text": "In the well-established framework of online convex optimization, numerous algorithms can efficiently achieve optimal regret, in the sense of converging in terms of average loss towards the best fixed decision in hindsight. That is, for any u \u2208 K, one can play iterates x1, . . . , xT such that\n1\nT T\u2211 i=1 [ft(xt)\u2212 ft(u)] = o(1).\nUnfortunately, even in the offline case, it is too ambitious to converge towards a global minimizer in hindsight. In the existing literature, it is usual to state convergence guarantees towards an \u03b5-approximate stationary point \u2013 that is, there exists some iterate xt for which \u2016\u2207f(xt)\u20162 \u2264 \u03b5. As discussed in the previous section, the projected gradient is a natural analogue for the constrained case.\nIn light of the computational intractability of direct analogues of convex regret, we introduce local regret, a new notion of regret which quantifies the objective of predicting points with small gradients on average. The remainder of this paper discusses the motivating roles of this quantity.\nThroughout this paper, for convenience, we will use the following notation to denote the sliding-window time average of functions f , parametrized by some window size 1 \u2264 w \u2264 T :\nFt,w(x) def =\n1\nw w\u22121\u2211 i=0 ft\u2212i(x).\nFor simplicity of notation, we define ft(x) to be identically zero for all t \u2264 0. We define local regret below:\nDefinition 2.5 (Local regret). Fix some \u03b7 > 0. Define the w-local regret of an online algorithm as\nRw(T ) def = T\u2211 t=1 \u2016\u2207K,\u03b7Ft,w(xt)\u20162,\nWhen the window size w is understood by context, we omit the parameter, writing simply local regret as well as Ft(x).\nWe turn to the first motivating perspective on local regret. When an algorithm incurs local regret sublinear in T , a randomly selected iterate has a small time-averaged gradient in expectation:\nProposition 2.6. Let x1, . . . , xT be the iterates produced by an algorithm for online non-convex optimization which incurs a local regret of Rw(T ). Then,\nE t\u223cUnif([T ])\n[ \u2016\u2207K,\u03b7Ft,w(xt)\u20162 ] \u2264 Rw(T )\nT .\nThis generalizes typical convergence results for the gradient in offline non-convex optimization; we discuss concrete reductions in Section 4."}, {"heading": "2.3 Why smoothing is necessary", "text": "In this section, we show that for any online algorithm, an adversarial sequence of loss functions can force the local regret incurred to scale with T as \u2126 ( T w2 ) . This demonstrates the need for a time-smoothed performance measure in our setting, and justifies our choice of larger values of the window size w in the sections that follow.\nTheorem 2.7. Define K = [\u22121, 1]. For any T \u2265 1, 1 \u2264 w \u2264 T , and \u03b7 \u2264 1, there exists a distribution D on 0-smooth, 1-bounded cost functions f1, . . . , fT on K such that for any online algorithm, when run on this sequence of functions,\nE D\n[Rw(T )] \u2265 1\n4w\n\u230a T\n2w\n\u230b .\nProof. We begin by partitioning the T rounds of play into b T2w c repeated segments, each of length 2w. For the first half of the first segment (t = 1, . . . , w), the adversary declares that\n\u2022 For odd t, select ft(x) i.i.d. as follows:\nft(x) := { \u2212x, with probability 12 x, with probability 12\n\u2022 For even t, ft(x) := \u2212ft\u22121(x).\nDuring the second half (t = w+1, . . . , 2w), the adversary sets all ft(x) = 0. This construction is repeated b T2w c times, padding the final T mod 2w costs arbitrarily with ft(x) = 0.\nBy this construction, at each round t at which ft(x) is drawn randomly, we have Ft,w(x) = ft(x)/w. Furthermore, for any xt played by the algorithm, |\u2207K,\u03b7ft(xt)| = 1 with probability at least 12 . so that E [ \u2016\u2207K,\u03b7Ft,w(xt)\u20162 ] \u2265 12w2 . The claim now follows from the fact that there are at least w 2 of these rounds\nper segment, and exactly \u230a T 2w \u230b segments in total.\nWe further note that the notion of time-smoothing captures non-convex online optimization under limited concept drift : in online learning problems where Ft,w(x) \u2248 ft(x), a bound on local regret truly captures a guarantee of playing points with small gradients."}, {"heading": "3 An efficient non-convex regret minimization algorithm", "text": "Our approach, as given in Algorithm 1, is to play follow-the-leader iterates, approximated to a suitable tolerance using projected gradient descent. We show that this method efficiently achieves an optimal local regret bound of O ( T w2 ) , taking O (Tw) iterations of the inner loop.\nAlgorithm 1 Time-smoothed online gradient descent\n1: Input: window size w \u2265 1, learning rate 0 < \u03b7 < \u03b22 , tolerance \u03b4 > 0, a convex body K \u2286 R n. 2: Set x1 \u2208 K arbitrarily. 3: for t = 1, . . . , T do 4: Predict xt. Observe the cost function ft : K \u2192 R. 5: Initialize xt+1 := xt. 6: while \u2016\u2207K,\u03b7Ft,w(xt+1)\u2016 > \u03b4/w do 7: Update xt+1 := xt+1 \u2212 \u03b7\u2207K,\u03b7Ft,w(xt+1). 8: end while 9: end for\nTheorem 3.1. Let f1, . . . , fT be the sequence of loss functions presented to Algorithm 1, satisfying Assumption 2.1. Then:\n(i) The w-local regret incurred satisfies\nRw(T ) \u2264 (\u03b4 + 2L)2 T\nw2 .\n(ii) The total number of gradient steps \u03c4 taken by Algorithm 1 satisfies\n\u03c4 \u2264 M \u03b42 ( \u03b7 \u2212 \u03b2\u03b722 ) \u00b7 (2Tw + w2) . Proof of (i). We note that Algorithm 1 will only play an iterate xt if \u2016\u2207K,\u03b7Ft\u22121,w\u2016 \u2264 \u03b4/w. (Note that at t = 1, Ft\u22121,w is zero.) Let ht(x) = 1 w (ft(x)\u2212 ft\u2212w(x)), which is 2L w -Lipschitz. Then, for each 1 \u2264 t \u2264 T we have a bound on each cost\n\u2016\u2207K,\u03b7Ft,w(xt)\u20162 = \u2016\u2207K,\u03b7 [Ft,w\u22121 + ht(x)] (xt)\u20162\n\u2264 (\u2016\u2207K,\u03b7Ft,w\u22121\u2016+ \u2016\u2207ht(xt)\u2016)2 \u2264 ( \u03b4\nw +\n2L\nw\n)2 = (\u03b4 + 2L)2\nw2 ,\nwhere the first inequality follows from Proposition 2.4. Summing over all t gives the desired result.\nProof of (ii). First, we require an additional property of the projected gradient.\nLemma 3.2. Let K \u2208 Rn be a closed convex set, and let \u03b7 > 0. Suppose f : K \u2192 R is differentiable. Then, for any x \u2208 R,\n\u3008\u2207f(x),\u2207K,\u03b7f(x)\u3009 \u2265 \u2016\u2207K,\u03b7f(x)\u20162.\nProof. Let u = x\u2212 \u03b7\u2207f(x) and u\u2032 = \u03a0K [u]. Then,\n\u3008\u2207f(x),\u2207K,\u03b7f(x)\u3009 \u03b72 \u2212 \u2016\u2207K,\u03b7f(x)\u2016 2 \u03b72 = \u3008u\u2212 x, u\u2032 \u2212 x\u3009 \u2212 \u3008u\u2032 \u2212 x, u\u2032 \u2212 x\u3009 = \u3008u\u2212 u\u2032, u\u2032 \u2212 x\u3009 \u2265 0,\nwhere the last inequality follows by the generalized Pythagorean theorem.\nFor 2 \u2264 t \u2264 T , let \u03c4t be the number of gradient steps taken in the outer loop at iteration t\u22121, in order to compute the iterate xt. For convenience, define \u03c41 = 0. We establish a progress lemma during each gradient descent epoch:\nLemma 3.3. For any 2 \u2264 t \u2264 T , Ft\u22121(xt)\u2212 Ft\u22121(xt\u22121) \u2264 \u2212\u03c4t ( \u03b7 \u2212 \u03b2\u03b7 2\n2\n) \u03b42\nw2 .\nProof. Consider a single iterate z of the inner loop, and the next iterate z\u2032 := z \u2212 \u03b7\u2207K,\u03b7Ft\u22121(z). We have, by \u03b2-smoothness of Ft\u22121,\nFt\u22121(z \u2032)\u2212 Ft\u22121(z) \u2264 \u3008\u2207Ft\u22121(z), z\u2032 \u2212 z\u3009+\n\u03b2 2 \u2016z\u2032 \u2212 z\u20162\n= \u2212\u03b7 \u3008\u2207Ft\u22121(z),\u2207K,\u03b7Ft\u22121(z)\u3009+ \u03b2\u03b72\n2 \u2016\u2207K,\u03b7Ft\u22121(z)\u20162.\nThus, by Lemma 3.2,\nFt\u22121(z \u2032)\u2212 Ft\u22121(z) \u2264 \u2212 ( \u03b7 \u2212 \u03b2\u03b7 2\n2\n) \u2016\u2207K,\u03b7Ft\u22121(z)\u20162.\nThe algorithm only takes projected gradient steps when \u2016\u2207K,\u03b7Ft\u22121(z)\u2016 \u2265 \u03b4/w. Summing across all \u03c4t consecutive iterations in the epoch yields the claim.\nTo complete the proof of the theorem, we write the telescopic sum (understanding F0(x0) = 0):\nFT (xT ) = T\u2211 t=1 Ft(xt)\u2212 Ft\u22121(xt\u22121)\n= T\u2211 t=1 Ft\u22121(xt)\u2212 Ft\u22121(xt\u22121) + ft(xt)\u2212 ft\u2212w(xt)\n\u2264 T\u2211 t=2 [Ft\u22121(xt)\u2212 Ft\u22121(xt\u22121)] + 2MT w .\nUsing Lemma 3.3, we have\nFT (xT ) \u2264 2MT\nw \u2212\n( \u03b7 \u2212 \u03b2\u03b7 2\n2\n) \u03b42\nw2 \u00b7 T\u2211 t=1 \u03c4t,\nwhence\n\u03c4 = T\u2211 t=1 \u03c4t \u2264 w2 \u03b42 ( \u03b7 \u2212 \u03b2\u03b722 ) \u00b7 (2MT w \u2212 FT (xT ) )\n\u2264 M \u03b42 ( \u03b7 \u2212 \u03b2\u03b722 ) \u00b7 (2Tw + w2) , as claimed.\nSetting \u03b7 = 1/\u03b2 and \u03b4 = L gives the asymptotically optimal local regret bound, with O(Tw) timeaveraged gradient steps (and thus O(Tw2) individual gradient oracle calls). We further note that in the case where K = Rn, one can replace the gradient descent subroutine (the inner loop) with non-convex SVRG [AZH16], achieving a complexity of O(Tw5/3) gradient oracle calls."}, {"heading": "4 Implications for offline and stochastic non-convex optimization", "text": "In this section, we discuss the ways in which our online framework generalizes the offline and stochastic versions of non-convex optimization \u2013 that any algorithm achieving a small value of Rw(T ) efficiently finds a point with small gradient in these settings. For convenience, for 1 \u2264 t \u2264 t\u2032 \u2264 T , we denote by D[t,t\u2032] the uniform distribution on time steps t through t\u2032 inclusive."}, {"heading": "4.1 Offline non-convex optimization", "text": "For offline optimization on a fixed non-convex function f : K \u2192 R, we demonstrate that a bound on local regret translates to convergence. In particular, using Algorithm 1 one finds a point x \u2208 K with \u2016\u2207K,\u03b7f(x)\u20162 \u2264 \u03b5 while making O ( 1 \u03b5 ) calls to the gradient oracle, matching the best known result for the convergence of gradient-based methods.\nCorollary 4.1. Let f : K \u2192 R satisfy Assumption 2.1. When online algorithm A is run on a sequence of T identical loss functions f(x), it holds that for any 1 \u2264 w < T ,\nEt\u223cD[w,T ]\u2016\u2207K,\u03b7f(xt)\u2016 2 \u2264 Rw(A)\nT \u2212 w .\nIn particular, Algorithm 1, with parameter choices T = 2w, \u03b7 = 1\u03b2 , \u03b4 = L, and w = (\u03b4 + 2L) \u221a 2 \u03b5 , yields\nEt\u223cDw,T \u2016\u2207K,\u03b7f(xt)\u20162 \u2264 \u03b5. Furthermore, the algorithm makes O ( 1 \u03b5 ) calls to the gradient oracle in total.\nProof. Since ft(x) = f(x) for all t, it follows that Ft,w(x) = f(x) for all t \u2265 w. As a consequence, we have\nEt\u223cDw,t\u2016\u2207K,\u03b7f(xt)\u20162 \u2264 1\nT \u2212 w T\u2211 t=1 \u2016\u2207K,\u03b7f(xt)\u20162\n\u2264 Rw(A) T \u2212 w .\nWith the stated choice of parameters, Theorem 3.1 guarantees that\nEt\u223cDw,t\u2016\u2207K,\u03b7f(xt)\u20162 \u2264 \u03b5 2 \u00b7 T T \u2212 w = \u03b5.\nAlso, since the loss functions are identical, the execution of line 7 of Algorithm 1 requires exactly one call to the gradient oracle at each iteration. This entails that the total number of gradient oracle calls made in the execution is O(Tw + w2) = O( 1\u03b5 )."}, {"heading": "4.2 Stochastic non-convex optimization", "text": "We examine the way in which our online framework captures stochastic non-convex optimization of a fixed function f : Rn \u2192 R, in which an algorithm has access to a noisy stochastic gradient oracle \u2207\u0303f(x). We note that the reduction here will only apply in the unconstrained case; it becomes challenging to reason about the projected gradient under noisy information. From a local regret bound, we recover a stochastic algorithm\nwith oracle complexity O ( \u03c34\n\u03b52\n) . We note that this black-box reduction recovers an optimal convergence rate\nin terms of \u03b5, but not \u03c32. In the setting, the algorithm must operate on the noisy estimates of the gradient as the feedback. In particular, for any ft that the adversary chooses, the learning algorithm is supplied with a stochastic gradient oracle for ft. The discussion in the preceding sections may be viewed as a special case of this setting with \u03c3 = 0. We list the assumptions we make on the stochastic gradient oracle, which are standard:\nAssumption 4.2. We assume that each call to the stochastic gradient oracle yields an i.i.d. random vector \u2207\u0303f(x) with the following properties:\n(i) Unbiased: E [ \u2207\u0303f(x) ] = \u2207f(x).\n(ii) Bounded variance: E [ \u2016\u2207\u0303f(x)\u2212\u2207f(x)\u20162 ] \u2264 \u03c32.\nWhen an online algorithm incurs small local regret in expectation, it has a convergence guarantee in offline stochastic non-convex optimization:\nProposition 4.3. Let 1 \u2264 w < T . Suppose that online algorithm A is run on a sequence of T identical loss functions f(x) satisfying Assumption 2.1, with identical stochastic gradient oracles satisfying Assumption 4.2. Sample t \u223c D[w,T ]. Then, over the randomness of t and the oracles,\nE [ \u2016\u2207f(xt)\u20162 ] \u2264 E [Rw(A)]\nT \u2212 w .\nProof. Observe that\nEt\u223cD[w,T ] [ \u2016\u2207f(xt)\u20162 ] \u2264 \u2211T t=1 \u2016\u2207f(xt)\u20162 T \u2212 w \u2264 Rw(A) T \u2212 w .\nThe claim follows by taking the expectation of both sides, over the randomness of the oracles.\nFor a concrete online-to-stochastic reduction, we consider Algorithm 2, which exhibits such a bound on expected local regret.\nAlgorithm 2 Time-smoothed online gradient descent with stochastic gradient oracles\n1: Input: learning rate \u03b7 > 0, window size w \u2265 1. 2: Set x1 = 0 \u2208 Rn arbitrarily. 3: for t = 1, . . . , T do 4: Predict xt. Observe the cost function ft : Rn \u2192 R. 5: Update xt+1 := xt \u2212 \u03b7w \u2211w\u22121 i=0 \u2207\u0303f t\u2212i(xt). 6: end for\nTheorem 4.4. Let f1, . . . , ft satisfy Assumption 2.1. Then, Algorithm 2, with access to stochastic gradient oracles {\u2207\u0303f t(xt)} satisfying Assumption 4.2, and a choice of \u03b7 = 1\u03b2 , guarantees\nE [Rw(T )] \u2264 ( 8\u03b2M + \u03c32 ) T w .\nFurthermore, Algorithm 2 makes a total of O(Tw) calls to the stochastic gradient oracles.\nUsing this expected local regret bound in Proposition 4.3, we obtain the reduction claimed at the beginning of the section:\nCorollary 4.5. Algorithm 2, with parameter choices w = 12M\u03b2+2\u03c3 2\n\u03b5 , T = 2w, and \u03b7 = 1 \u03b2 , yields\nE [ \u2016\u2207f(xt)\u20162 ] \u2264 \u03b5.\nFurthermore, the algorithm makes O ( \u03c34\n\u03b52\n) stochastic gradient oracle calls in total."}, {"heading": "5 An efficient algorithm with second-order guarantees", "text": "We note that by modifying Algorithm 1 to exploit second-order information, our online algorithm can be improved to play approximate first-order critical points which are also locally almost convex. This entails replacing the gradient descent epochs with a cubic-regularized Newton method [NP06, AAZB+16].\nIn this setting, we assume that we have access to each ft through a value, gradient, and Hessian oracle. That is, once we have observed ft, we can obtain ft(x), \u2207ft(x), and \u22072ft(x) for any x. Let MinEig(A) be the minimum (eigenvalue, eigenvector) pair for matrix A. As is standard for offline second-order algorithms, we must add the following additional smoothness restriction:\nAssumption 5.1. ft is twice differentiable and has an L2-Lipschitz Hessian:\n\u2016\u22072f(x)\u2212\u22072f(y)\u2016 \u2264 L2\u2016x\u2212 y\u2016.\nAdditionally, we consider only the unconstrained case where K = Rn; the second-order optimality condition is irrelevant when the gradient does not vanish at the boundary of K.\nThe second-order Algorithm 3 uses the same approach as in Algorithm 1, but terminates each epoch under a stronger approximate-optimality condition. We define\n\u03a6t(x) := max { \u2016\u2207Ft(x)\u20162,\u2212 4\u03b2\n3L22 \u00b7 \u03bbmin(\u22072Ft(x))3\n} ,\nso that the quantity \u2211T t=1 \u03a6t(xt) is termwise lower bounded by the costs in Rw(T ), but penalizes local concavity.\nAlgorithm 3 Time-smoothed online Newton method\n1: Input: window size w \u2265 1, tolerance \u03b4 > 0. 2: Set x1 \u2208 K arbitrarily. 3: for t = 1, . . . , T do 4: Predict xt. Observe the cost function ft : Rn \u2192 R. 5: Initialize xt+1 := xt. 6: while \u03a6t(xt+1) > \u03b4\n3/w3 do 7: Update xt+1 := xt+1 \u2212 1\u03b2\u2207Ft,w(xt+1). 8: Let (\u03bb, v) := MinEig ( \u22072Ft,w(xt+1) ) .\n9: if \u03bb < 0 then 10: Flip the sign of v so that \u3008v,\u2207Ft,w(xt+1)\u3009 \u2264 0. 11: Compute yt+1 := xt +\n2\u03bb L2 v.\n12: if Ft,w(yt+1) < Ft,w(xt+1) then 13: Set xt+1 := yt+1. 14: end if 15: end if 16: end while 17: end for\nWe characterize the convergence and oracle complexity properties of this algorithm:\nTheorem 5.2. Let f1, . . . , fT be the sequence of loss functions presented to Algorithm 3, satisfying Assumptions 2.1 and 5.1. Choose \u03b4 = \u03b2. Then, for some constants C1, C2 in terms of M,L, \u03b2, L2:\n(i) The iterates {xt} produced by Algorithm 3 satisfy\nT\u2211 t=1 \u03a6t(xt) \u2264 C1 \u00b7 T w2 .\n(ii) The total number of iterations \u03c4 of the inner loop taken by Algorithm 3 satisfies\n\u03c4 \u2264 C2 \u00b7 Tw2.\nProof of (i). For each 1 \u2264 t \u2264 T , we have\n\u03a6t\u22121(xt) \u2264 \u03b43\nw3 .\nLet ht(x) := 1 w (ft(x)\u2212 ft\u2212w(x)). Then, since ht(x) is 2L w -Lipschitz and 2\u03b2 w -smooth,\n\u03a6t(xt) = max { \u2016\u2207Ft\u22121(xt) +\u2207ht(xt)\u20162,\n\u2212 4\u03b2 3L22 \u00b7 \u03bbmin(\u22072Ft(xt) +\u22072ht(xt))3 } \u2264 max {( \u03b43/2\nw3/2 +\n2L\nw\n)2 , 4\u03b2 3L22 \u00b7 ( \u03b4 w + 2\u03b2 w )3} ,\nwhich is bounded by C1/w 2, for some C1(M,L, \u03b2, L 2 2). The claim follows by summing this inequality across all 1 \u2264 t \u2264 T .\nProof of (ii). We first show the following progress lemma:\nLemma 5.3. Let z, z\u2032 be two consecutive iterates of the inner loop in Algorithm 3 during round t. Then,\nFt(z \u2032)\u2212 Ft(z) \u2264 \u2212\n\u03a6t(z)\n2\u03b2 .\nProof. Let u denote the step z\u2032 \u2212 z. Let g := \u2207Ft(z), H := \u22072Ft(z), and (\u03bb, v) := MinEig(H). Suppose that at time t, the algorithm takes a gradient step, so that u = g/\u03b2. Then, by second-order smoothness of Ft, we have\nFt(z \u2032)\u2212 Ft(z) \u2264 \u3008g, u\u3009+\n\u03b2 2 \u2016u\u20162 = \u2212 1 2\u03b2 \u2016g\u20162.\nSupposing instead that the algorithm takes a second-order step, so that u = \u00b1 2\u03bbL2 v (whichever sign makes \u3008g, u\u3009 \u2264 0), the third-order smoothness of Ft implies\nFt(z \u2032)\u2212 Ft(z) \u2264 \u3008g, u\u3009+\n1 2 uTHtu+ L2 6 \u2016u\u20163\n= \u3008g, u\u3009+ \u03bb 2 \u2016u\u20162 + L2 6 \u2016u\u20163 \u2264 2\u03bb 3\n3L22 =\n1 2\u03b2 \u00b7 4\u03b2\u03bb\n3\n3L22 .\nThe lemma follows due to the fact that the algorithm takes the step that gives a smaller value of Ft(z \u2032).\nFollowing the technique from Theorem 3.1, for 2 \u2264 t \u2264 T , let \u03c4t be the number of iterations of the inner loop during the execution of Algorithm 3 during round t\u2212 1 (in order to generate the iterate xt). Then, we have the following lemma:\nLemma 5.4. For any 2 \u2264 t \u2264 T ,\nFt\u22121(xt)\u2212 Ft\u22121(xt\u22121) \u2264 \u2212\u03c4t \u00b7 \u03b43\n2\u03b2w3 .\nProof. This follows by summing the inequality Lemma 5.3 for across all pairs of consecutive iterates of the inner loop within the same epoch, and noting that each term \u03a6(z) is at least \u03b4 3\nw3 before the inner loop has terminated.\nFinally, we write (understanding F0(x0) := 0):\nFT (xT ) = T\u2211 t=1 Ft(xt)\u2212 Ft\u22121(xt\u22121)\n= T\u2211 t=1 Ft\u22121(xt)\u2212 Ft\u22121(xt\u22121) + ft(xt)\u2212 ft\u2212w(xt)\n\u2264 T\u2211 t=2 [Ft\u22121(xt)\u2212 Ft\u22121(xt\u22121)] + 2MT w .\nUsing Lemma B.1, we have\nFT (xT ) \u2264 2MT w \u2212 \u03b4\n3 2\u03b2w3 \u00b7 T\u2211 t=1 \u03c4t,\nwhence\n\u03c4 = T\u2211 t=1 \u03c4t \u2264 2\u03b2w3 \u03b43 \u00b7 ( 2MT w \u2212 FT (xT ) ) \u2264 2\u03b2M\n\u03b43 \u00b7 ( 2Tw2 + w3 ) \u2264 6M\n\u03b22 \u00b7 Tw2,\nas claimed (recalling that we chose \u03b4 = \u03b2 for this analysis)."}, {"heading": "6 A solution concept for non-convex games", "text": "Finally, we discuss an application of our regret minimization framework to learning in k-player T -round iterated games with smooth, non-convex payoff functions. Suppose that each player i \u2208 [k] has a fixed decision set Ki \u2282 Rn, and a fixed payoff function fi : K \u2192 R satisfies Assumption 2.1 as before. Here, K denotes the Cartesian product of the decision sets Ki: each payoff function is defined in terms of the choices made by every player.\nIn such a game, it is natural to consider the setting where players will only consider small local deviations from their strategies. This is a natural setting, which models risk aversion. This setting lends itself to the notion of a local equilibrium, to replace the stronger condition of Nash equilibrium: a joint strategy in which no player encounters a large gradient on her utility. However, finding an approximate local equilibrium in this sense remains computationally intractable when the utility functions are non-convex.\nUsing the idea of time-smoothing, we formulate a tractable relaxed notion of local equilibrium, defined over some time window w. Intuitively, this definition captures a state of an iterated game in which each player examines the past w actions played, and no player can make small deviations to improve the average performance of her play against her opponents\u2019 historical play. We formulate this solution concept as follows:\nDefinition 6.1 (Smoothed local equilibrium). Fix some \u03b7 > 0, w \u2265 1. Let { fi(x 1, . . . , xk) : K \u2192 R }k i=1 be the payoff functions for a k-player iterated game. A joint strategy (x1t , . . . , x k t ) is an \u03b5-approximate (\u03b7, w)-\nsmoothed local equilibrium with respect to past iterates {\n(x1t\u2212j , . . . , x k t\u2212j) }w\u22121 j=0\nif, for every player i \u2208 [k],\u2225\u2225\u2225\u2225\u2225\u2207K,\u03b7 [\u2211w\u22121 j=0 f\u0303i,t\u2212j w ] (xit) \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b5, where\nf\u0303i,t\u2032(x) def = fi(x 1 t\u2032 , . . . , x i\u22121 t\u2032 , x, x i+1 t\u2032 , . . . , x k t\u2032).\nTo achieve such an equilibrium efficiently, we use Algorithm 4, which runs k copies of any online algorithm that achieves a w-local regret bound for some \u03b7 > 0.\nAlgorithm 4 Time-smoothed game simulation\n1: Input: convex decision sets K1, . . . ,Kk \u2286 Rn, payoff functions fi : (K1, . . . ,Kk) \u2192 R, online algorithm A, window size 1 \u2264 w < T . 2: Initialize k copies (A1, . . . ,Ak) of A with window size w, where each Ai plays on decision set Ki. 3: for t = 1, . . . , T do 4: Each Ai outputs xit. 5: Show each Ai the online loss function\nfi,t(x) := \u2212fi(x1t , . . . , xi\u22121t , x, xi+1t , . . . , xkt ).\n6: end for\nWe show this meta-algorithm yields a subsequence of iterates that satisfy our solution concept, with error parameter dependent on the local regret guarantees of each player:\nTheorem 6.2. For some t such that w \u2264 t \u2264 T , the joint strategy (x1t , . . . , xkt ) produced by Algorithm 4 is an \u03b5-approximate (\u03b7, w)-smoothed local equilibrium with respect to { (x1t\u2212j , . . . , x k t\u2212j) }t\u22121 j=0 , where\n\u03b5 = \u221a\u221a\u221a\u221a k\u2211 i=1 Rw,Ai(T ) T \u2212 w .\nProof. Summing up the definitions of w-regret bounds achieved by each A, and truncating the first w \u2212 1 terms, we get\nk\u2211 i=1 T\u2211 t=w \u2016\u2207K,\u03b7F it (xit)\u20162 \u2264 k\u2211 i=1 Rw,Ai(T ).\nThus, for some t between w and T inclusive, it holds that\nk\u2211 i=1 \u2225\u2225\u2225\u2225\u2225\u2207K,\u03b7 [\u2211w\u22121 j=0 f\u0303i,t\u2212j w ] (xit) \u2225\u2225\u2225\u2225\u2225 2 = k\u2211 i=1 \u2016\u2207K,\u03b7F it (xit)\u20162\n\u2264 k\u2211 i=1 Rw,Ai(T ) T \u2212 w .\nThus, for the same t we have\nmax i\u2208[k] \u2225\u2225\u2225\u2225\u2225\u2207K,\u03b7 [\u2211w\u22121 j=0 f\u0303i,t\u2212j w ] (xit) \u2225\u2225\u2225\u2225\u2225 \u2264 \u221a\u221a\u221a\u221a k\u2211\ni=1\nRw,Ai(T )\nT \u2212 w ,\nas claimed."}, {"heading": "6.1 Experience replay for GAN training", "text": "The training of generative adversarial networks (GANs), a popular generative model, can be viewed as a symmetric game with a non-convex payoff function. In this section, we apply and contextualize our framework of smoothed local equilibrium for GANs.\nIn the seminal setting of [GPAM+14], there are two players: a generator who wants to imitate samples from a \u201ctrue\u201d distribution D on Rn, and a discriminator who wants to distinguish true samples from D and fake samples produced by the generator. The generator chooses some function G : Rm \u2192 Rn, which maps input randomness z \u223c D\u2032 to fake samples. The discriminator chooses a function D : Rn \u2192 [0, 1], a guess for the likelihood that a data point is real. D and G are chosen from some function classes parameterized by \u03b8D and \u03b8G, often both neural networks. The generator and discriminator play an iterated game with the objective function\nL(\u03b8D, \u03b8G) := E x\u223cD,z\u223cD\u2032 [logD(x) + log (1\u2212D(G(z)))] .\nIn the language of our model, the discriminator\u2019s payoff is L(\u03b8D, \u03b8G), while the generator\u2019s payoff is\u2212L(\u03b8D, \u03b8G), and the players access L via stochastic gradient oracles, as in Section 4.2. Indeed, in GAN training, it is a standard technique to update \u03b8D and \u03b8G incrementally in lockstep, via stochastic gradient descent steps. This is very similar to using Algorithm 2 as the local regret minimization algorithm driving Algorithm 4 with w = 1; the only difference is that the players\u2019 updates are alternating rather than simultaneous.\nInstability is a major challenge for GAN training, and improving training stability is a highly active research area in deep learning. To this end, our local regret framework provides a meaningful yet attainable theoretical goal, which is met by our time-smoothed gradient-based algorithms. For this game, a smoothed local equilibrium with window parameter w guarantees that the generator and discriminator simultaneously encounter small averaged gradients on each other\u2019s past w choices of functions. This is a particularly appealing notion of equilibrium in the setting of GAN training, as it implies that a gradient-based training process becomes approximately stationary.\nIndeed, maintaining a buffer of past discriminators (running Algorithm 2) is a known technique for stabilizing GAN training. [MPPSD16] In reinforcement learning, this corresponds to a form of experience replay. [PV16]"}, {"heading": "7 Concluding remarks", "text": "We have described how to extend the theory of online learning to non-convex loss functions, while permitting efficient algorithms. Our definitions give rise to efficient online and stochastic non-convex optimization algorithms that converge to local optima of first and second order. We give a game-theoretic solution concept which we call local equilibrium, which, in contrast to existing solution concepts such as Nash equilibrium, is efficiently attainable in any non-convex game."}, {"heading": "Acknowledgments", "text": "We thank Naman Agarwal, Brian Bullins, Matt Weinberg, and Yi Zhang for helpful discussions."}, {"heading": "A Proof of Theorem 4.4", "text": "Since each ft is \u03b2-smooth, it follows that each Ft is \u03b2-smooth. Define \u2207\u0302ft = xt\u2212xt+1\u03b7 . Note that since the iterates (xt : t \u2208 [T ]) depend on the gradient estimates, the iterates are stochastic variables, as are \u2207\u0302ft. By \u03b2-smoothness of Ft, we have\nFt,w(xt+1)\u2212 Ft,w(xt)\n\u2264\u3008\u2207Ft,w(xt), xt+1 \u2212 xt\u3009+ \u03b2\n2 \u2016xt+1 \u2212 xt\u20162 =\u2212 \u03b7 \u2329 \u2207Ft,w(xt), \u2207\u0302ft \u232a + \u03b72 \u03b2\n2 \u2016\u2207\u0302ft\u20162 =\u2212 \u03b7\u2016\u2207Ft,w(xt)\u20162 \u2212 \u03b7 \u2329 \u2207Ft,w(xt), \u2207\u0302ft \u2212\u2207Ft,w(xt) \u232a + \u03b72 \u03b2\n2\n( \u2016\u2207Ft,w(xt)\u20162 ) + \u03b72 \u03b2\n2\n( 2 \u2329 \u2207Ft,w(xt), \u2207\u0302ft \u2212\u2207Ft,w(xt) \u232a) + \u03b72 \u03b2\n2\n( \u2016\u2207\u0302ft \u2212\u2207Ft,w(xt)\u20162 ) =\u2212 ( \u03b7 \u2212 \u03b2 2 \u03b72 ) \u2016\u2207Ft,w(xt)\u20162\n\u2212 (\u03b7 \u2212 \u03b2\u03b72) \u2329 \u2207Ft,w(xt), \u2207\u0302ft \u2212\u2207Ft,w(xt) \u232a + \u03b72 \u03b2\n2 \u2016\u2207\u0302ft \u2212\u2207f(xt)\u20162.\nAdditionally, we each observe that \u2207\u0302ft is an average of w independently sampled unbiased gradient estimates of variance \u03c32 each. It follows as a consequence that\nE [ \u2207\u0302ft \u2223\u2223xt] = \u2207Ft,w(xt) E [ \u2016\u2207\u0302ft \u2212\u2207Ft,w(xt)\u20162 \u2223\u2223xt] \u2264 \u03c32 w\nNow, applying E [\u00b7|xt] on both sides, it follows that( \u03b7 \u2212 \u03b2 2 \u03b72 ) \u00b7 E\u2016\u2207Ft,w(xt)\u20162\n\u2264 E [Ft,w(xt)\u2212 Ft,w(xt+1)] + \u03b72 \u03b2\n2\n\u03c32 w .\nAlso, we note that\nFt+1,w(xt+1)\u2212 Ft,w(xt+1)\n= 1\nw w\u22121\u2211 i=0 ft+1\u2212i(xt+1)\u2212 1 w w\u22121\u2211 i=0 ft\u2212i(xt+1)\n= 1\nw w\u22122\u2211 i=\u22121 ft\u2212i(xt+1)\u2212 1 w w\u22121\u2211 i=0 ft\u2212i(xt+1)\n= ft+1(xt+1)\u2212 ft\u2212w+1(xt+1) w \u2264 2M w\nAdding the last two inequalities, we proceed to sum the above inequality over all time steps:\nE [ T\u2211 t=1 \u2016\u2207Ft,w(xt)\u20162 ] \u2264 2M + 2MTw + T\u03b2\u03b72 2w \u03c3 2 \u03b7 \u2212 \u03b2\u03b722 .\nSetting \u03b7 = 1/\u03b2 yields the claim from the theorem. Finally, note that for each round the number of stochastic gradient oracle calls required is w. Therefore,\nacross all T rounds, the number of noisy oracle calls is Tw."}, {"heading": "B Proof of Theorem 5.1 (ii)", "text": "Following the technique from Theorem 3.1, for 2 \u2264 t \u2264 T , let \u03c4t be the number of iterations of the inner loop during the execution of Algorithm 3 during round t\u2212 1 (in order to generate the iterate xt). Then, we have the following lemma:\nLemma B.1. For any 2 \u2264 t \u2264 T ,\nFt\u22121(xt)\u2212 Ft\u22121(xt\u22121) \u2264 \u2212\u03c4t \u00b7 \u03b43\n2\u03b2w3 .\nProof. This follows by summing the inequality Lemma 5.3 for across all pairs of consecutive iterates of the inner loop within the same epoch, and noting that each term \u03a6(z) is at least \u03b4 3\nw3 before the inner loop has terminated.\nFinally, we write (understanding F0(x0) := 0):\nFT (xT ) = T\u2211 t=1 Ft(xt)\u2212 Ft\u22121(xt\u22121)\n= T\u2211 t=1 Ft\u22121(xt)\u2212 Ft\u22121(xt\u22121) + ft(xt)\u2212 ft\u2212w(xt)\n\u2264 T\u2211 t=2 [Ft\u22121(xt)\u2212 Ft\u22121(xt\u22121)] + 2MT w .\nUsing Lemma B.1, we have\nFT (xT ) \u2264 2MT w \u2212 \u03b4\n3 2\u03b2w3 \u00b7 T\u2211 t=1 \u03c4t,\nwhence\n\u03c4 = T\u2211 t=1 \u03c4t \u2264 2\u03b2w3 \u03b43 \u00b7 ( 2MT w \u2212 FT (xT ) ) \u2264 2\u03b2M\n\u03b43 \u00b7 ( 2Tw2 + w3 ) \u2264 6M\n\u03b22 \u00b7 Tw2,\nas claimed (recalling that we chose \u03b4 = \u03b2 for this analysis)."}, {"heading": "C Proof of Theorem 6.2", "text": "Summing up the definitions of w-regret bounds achieved by each A, and truncating the first w \u2212 1 terms, we get\nk\u2211 i=1 T\u2211 t=w \u2016\u2207K,\u03b7F it (xit)\u20162 \u2264 k\u2211 i=1 Rw,Ai(T ).\nThus, for some t between w and T inclusive, it holds that\nk\u2211 i=1 \u2225\u2225\u2225\u2225\u2225\u2207K,\u03b7 [\u2211w\u22121 j=0 f\u0303i,t\u2212j w ] (xit) \u2225\u2225\u2225\u2225\u2225 2 = k\u2211 i=1 \u2016\u2207K,\u03b7F it (xit)\u20162\n\u2264 k\u2211 i=1 Rw,Ai(T ) T \u2212 w .\nThus, for the same t we have\nmax i\u2208[k] \u2225\u2225\u2225\u2225\u2225\u2207K,\u03b7 [\u2211w\u22121 j=0 f\u0303i,t\u2212j w ] (xit) \u2225\u2225\u2225\u2225\u2225 \u2264 \u221a\u221a\u221a\u221a k\u2211\ni=1\nRw,Ai(T )\nT \u2212 w ,\nas claimed."}], "references": [{"title": "Finding approximate local minima for nonconvex optimization in linear time", "author": ["Naman Agarwal", "Zeyuan Allen-Zhu", "Brian Bullins", "Elad Hazan", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1611.01146,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Second order stochastic optimization for machine learning in linear time", "author": ["Naman Agarwal", "Brian Bullins", "Elad Hazan"], "venue": "arXiv preprint arXiv:1602.03943,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Variance reduction for faster non-convex optimization", "author": ["Zeyuan Allen-Zhu", "Elad Hazan"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Allen.Zhu and Hazan.,? \\Q2016\\E", "shortCiteRegEx": "Allen.Zhu and Hazan.", "year": 2016}, {"title": "From external to internal regret", "author": ["A. Blum", "Y. Mansour"], "venue": "In COLT,", "citeRegEx": "Blum and Mansour.,? \\Q2005\\E", "shortCiteRegEx": "Blum and Mansour.", "year": 2005}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Accelerated methods for non-convex optimization", "author": ["Yair Carmon", "John C. Duchi", "Oliver Hinder", "Aaron Sidford"], "venue": "arXiv preprint 1611.00756,", "citeRegEx": "Carmon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carmon et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["Murat A Erdogdu", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Erdogdu and Montanari.,? \\Q2015\\E", "shortCiteRegEx": "Erdogdu and Montanari.", "year": 2015}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ghadimi and Lan.,? \\Q2013\\E", "shortCiteRegEx": "Ghadimi and Lan.", "year": 2013}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Introduction to online convex optimization", "author": ["Elad Hazan"], "venue": "Foundations and Trends in Optimization,", "citeRegEx": "Hazan.,? \\Q2016\\E", "shortCiteRegEx": "Hazan.", "year": 2016}, {"title": "Computational equivalence of fixed points and no regret algorithms, and convergence to equilibria", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hazan and Kale.,? \\Q2007\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2007}, {"title": "A simple adaptive procedure leading to correlated", "author": ["Sergiu Hart", "Andreu Mas-Colell"], "venue": "equilibrium. Econometrica,", "citeRegEx": "Hart and Mas.Colell.,? \\Q2000\\E", "shortCiteRegEx": "Hart and Mas.Colell.", "year": 2000}, {"title": "Unrolled generative adversarial networks", "author": ["Luke Metz", "Ben Poole", "David Pfau", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1611.02163,", "citeRegEx": "Metz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Metz et al\\.", "year": 2016}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Cubic regularization of newton method and its global performance", "author": ["Yurii Nesterov", "Boris T Polyak"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov and Polyak.,? \\Q2006\\E", "shortCiteRegEx": "Nesterov and Polyak.", "year": 2006}, {"title": "Connecting generative adversarial networks and actor-critic methods", "author": ["David Pfau", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1610.01945,", "citeRegEx": "Pfau and Vinyals.,? \\Q2016\\E", "shortCiteRegEx": "Pfau and Vinyals.", "year": 2016}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Aggregating strategies", "author": ["Volodimir G. Vovk"], "venue": "In Proceedings of the Third Annual Workshop on Computational Learning Theory, COLT", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}], "referenceMentions": [], "year": 2017, "abstractText": "We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.", "creator": "LaTeX with hyperref package"}}}