{"id": "1708.00712", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Dynamic Data Selection for Neural Machine Translation", "abstract": "intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase - based machine translation ( pbmt ). with the recent and increase in popularity of neural machine translation ( computational nmt ), we confidently explore models in this paper to what extent and see how nmt can also benefit from data selection. here while state - of - the - art data selection ( axelrod et al., 2011 ) consistently predicted performs unexpectedly well significantly for pbmt, we show that expected gains are substantially lower for nmt. essentially next, we systematically introduce dynamic data selection for nmt, a method in memory which we vary the selected subset times of training data between different training epochs. our experiments show that the best diagnostic results are achieved when applying a tailored technique we call gradual fine - tuning, ranging with improvements up to + 2. 6 squared bleu over the original data selection approach and up to + 3. 1 bleu over a general baseline.", "histories": [["v1", "Wed, 2 Aug 2017 11:55:57 GMT  (99kb,D)", "http://arxiv.org/abs/1708.00712v1", "Accepted at EMNLP2017"]], "COMMENTS": "Accepted at EMNLP2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marlies van der wees", "arianna bisazza", "christof monz"], "accepted": true, "id": "1708.00712"}, "pdf": {"name": "1708.00712.pdf", "metadata": {"source": "CRF", "title": "Dynamic Data Selection for Neural Machine Translation", "authors": ["Marlies van der Wees", "Arianna Bisazza", "Christof Monz"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) as the most common machine translation paradigm. With large quantities of parallel data, NMT outperforms PBMT for an increasing number of language pairs (Bojar et al., 2016). Unfortunately, training an NMT model is often a time-consuming task, with training times of several weeks not being unusual.\nDespite its training inefficiency, most work in NMT greedily uses all available training data for a given language pair. However, it is unlikely\n\u2217Work done while at University of Amsterdam\nthat all data is equally helpful to create the bestperforming system. In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasco\u0301 et al., 2012). Instead, for a given translation task, the training bitext likely contains sentences that are irrelevant or even harmful, making it beneficial to keep only the most relevant subset of the data while discarding the rest, with the additional benefit of smaller models and faster training.\nMotivated by the success of data selection in PBMT, we investigate in this paper to what extent and how NMT can benefit from data selection as well. While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translation quality have not been investigated. Intuitively, and confirmed by our exploratory experiments in Section 5.1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (Zoph et al., 2016; Fadaee et al., 2017), and do not have a separate large-scale target-side language model to compensate for smaller parallel training data.\nTo alleviate the negative effect of small training data on NMT, we introduce dynamic data selection. Following conventional data selection, we still dramatically reduce the training data size, favoring parts of the data which are most relevant to the translation task at hand. However, we exploit the fact that the NMT training process iterates over the training corpus in multiple epochs, and we alter the quantity or the composition of the training data between epochs. The proposed method requires no modifications to the NMT architecture or parameters, and substantially speeds up training times while simultaneously improving translation quality with respect to a complete-bitext baseline. ar X\niv :1\n70 8.\n00 71\n2v 1\n[ cs\n.C L\n] 2\nA ug\n2 01\n7\nIn summary, our contributions are as follows: (i) We compare the effects of a commonly used data selection approach (Axelrod et al., 2011) on PBMT and NMT using four different test sets. We find that this method is much less effective for NMT than for PBMT, while using the exact same training data subsets.\n(ii) We introduce dynamic data selection as a way to make data selection profitable for NMT. We explore two techniques to alter the selected data subsets, and find that our method called gradual fine-tuning improves over conventional static data selection (up to +2.6 BLEU) and over a high-resource general baseline (up to +3.1 BLEU). Moreover, gradual fine-tuning approximates indomain fine-tuning in \u223c20% of the training time, even when no parallel in-domain data is available."}, {"heading": "2 Static data selection", "text": "As a first step towards dynamic data selection for NMT, we compare the effects of a commonly used, state-of-the-art data selection method (Axelrod et al., 2011) on both neural and phrase-based MT. Briefly, this approach ranks sentence pairs in a large training bitext according to their difference in cross-entropy with respect to an in-domain corpus (i.e., a corpus representing the test data) and a general corpus. Next, the top n sentence pairs with the highest rank\u2014thus lowest cross-entropy\u2014are selected and used for training an MT system.\nFormally, given an in-domain corpus I , we first create language models from the source side f of I (LMI,f ) and the target side e of I (LMI,e). We then draw a random sample (similar in size to I) of the large general corpus G and create language models from the source and target sides of G: LMG,f and LMG,e, respectively. Note that the data for creating these LMs need not be parallel but can be independent corpora in both languages.\nNext, we compute for each sentence pair s in G four cross-entropy scores, defined as:\nHC,sb = \u2212 \u2211 p (sb) log ( LMC,b (sb) ) , (1)\nwhere C \u2208 {I,G} is the corpus, b \u2208 {f, e} refers to the bitext side, and sb is the bitext side b of sentence pair s in the parallel training corpus.\nTo find sentences that are similar to the indomain corpus, i.e., have low HI , and at the same time dissimilar to the general corpus, i.e., have high HG, we compute for each sentence pair s\nthe bilingual cross-entropy difference CEDs following Axelrod et al. (2011):\nCEDs = (HI,sf \u2212HG,sf )+(HI,se\u2212HG,se). (2)\nFinally, we rank all sentence pairs s \u2208 G according to their CEDs, and then select only the top n sentence pairs with the lowest CEDs.\nFollowing related work by Moore and Lewis (2010), we restrict the vocabulary of the LMs to the words occurring at least twice in the in-domain corpus. To analyze the quality of the selected data subsets, we also run experiments on random selections, all performed in threefold. Finally, we always use the exact same selection of sentence pairs in equivalent PBMT and NMT experiments.\nLSTM versus n-gram The described data selection method uses n-gram LMs to determine the domain-relevance of sentence pairs. We adhere to this setting for our comparative experiments on PBMT and NMT (Section 5.1). However, when applying data selection to NMT, we examine the potential benefit of replacing the conventional ngram LMs with LSTMs1. These have the advantage to remember longer histories, and do not have to back off to shorter histories when encountering out-of-vocabulary words. In this neural variant to rank sentences, the score for each sentence pair in G is still computed as the bilingual cross-entropy difference in Equation (2). In addition, we use the same in-domain and general corpora as with the ngram method, and we again restrict the vocabulary to the most frequent words."}, {"heading": "3 Dynamic data selection", "text": "While data selection aims to discard irrelevant data, it can also exacerbate the problem of low vocabulary coverage and unreliable statistics for rarer words in the \u2018long tail\u2019, which are major issues in NMT (Luong et al., 2015b; Sennrich et al., 2016b). In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017).\nTo overcome this problem, we introduce dynamic data selection, in which we vary the selected data subsets during training. Unlike other MT paradigms, which require training data to be fixed during the entire training process, NMT iterates over the training corpus in several epochs,\n1We use four-layer LSTMs with embedding and hidden sizes of 1,024, which we train for 30 epochs.\nallowing to use a different subset of the training data in every epoch.\nDynamic data selection starts from a relevanceranked bitext, which we create using CED scores as computed in Equation (2). Given this ranking, we investigate two dynamic data selection techniques2 that vary per epoch the composition or the size of the selected training data. Both techniques aim to favor highly relevant sentences over less relevant sentences while not completely discarding the latter. In all experiments, we use a fixed vocabulary created from the complete bitext.\nWhile we use in this work a domain-relevance ranking of the bitext following Axelrod et al. (2011), dynamic data selection can also be applied using other ranking criteria, for example limiting redundancy in the training data (Lewis and Eetemadi, 2013) or complementing similarity with diversity (Ruder and Plank, 2017).\nSampling sentence pairs In the first technique, illustrated in Figure 1a, we sample for every epoch n sentence pairs from G, using a distribution computed from the domain-specific CEDs scores. Concretely, this is done as follows:\nFirst, since higher ranked sentence pairs have lower CEDs scores, and they can be either negative or positive, we scale and invert CEDs scores such that 0 \u2264 CED\u2032s \u2264 1 for each sentence pair s \u2208 G:\nCED\u2032s = 1\u2212 CEDs \u2212min(CEDG)\nmax(CEDG)\u2212min(CEDG) , (3)\n2Code for bitext ranking and both selection techniques: github.com/marliesvanderwees/dds-nmt.\nwhere CEDG refers to the set of CEDs scores for bitext G.\nNext, we convert CED\u2032s scores to relative weights, such that \u2211 s\u2208Gw(s) = 1:\nw(s) = CED\u2032s\u2211\nsi\u2208G CED \u2032 si\n. (4)\nWe then use {w(s) : s \u2208 G} to perform weighted sampling, drawing for each epoch n sentence pairs without replacement. While all selection weights are very close to zero, higher ranked sentences have a noticeably higher probability of being selected than lower-ranked sentences; in practice we find that top-ranked sentences get selected in nearly each epoch, while bottom-ranked sentence pairs get selected at most once. Note that the sampled selection for any epoch is independent of selections for all other epochs.\nGradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data. However, rather than training a full model on the complete bitext G, we gradually decrease the training data size, starting from G and keeping only the top n sentence pairs for the duration of \u03b7 epochs, where the top n pairs are defined by their CEDs scores. Given its resemblance to fine-tuning, we refer to this variant as gradual fine-tuning.\nDuring gradual fine-tuning, the selection size n is a function of epoch i:\nn(i) = \u03b1 \u00b7 |G| \u00b7 \u03b2b(i\u22121)/\u03b7c. (5)\nHere 0 \u2264 \u03b1 \u2264 1 is the relative start size, i.e., the fraction of general bitext G used for the first selection, 0 \u2264 \u03b2 \u2264 1 is the retention rate, i.e., the fraction of data to be kept in each new selection, and \u03b7 \u2265 1 is the number of consecutive epochs each selected subset is used. Note that bi/\u03b7 + 1c indicates rounding down i/\u03b7+1 to the nearest integer. For example, if we start with the complete bitext (\u03b1 = 1), select the top 60% (\u03b2 = 0.6) every second epoch (\u03b7 = 2), then we run epochs 1 and 2 with a subset of size |G|, epochs 3 and 4 with a subset of size 0.6 \u00b7 |G|, epochs 5 and 6 with a subset of size 0.36 \u00b7 |G|, and so on. For every size n, the actual selection contains the top n sentences pairs of G."}, {"heading": "4 Experimental settings", "text": "We evaluate static and dynamic data selection on a German\u2192English translation task comprising four test sets. Below we describe the MT systems and data specifications."}, {"heading": "4.1 Machine translation systems", "text": "While the main aim of this paper is to improve data selection for NMT, we also perform comparative experiments using PBMT. Our PBMT system is an in-house system similar to Moses (Koehn et al., 2007). To create optimal PBMT systems given the available resources, we apply test-set-specific parameter tuning using PRO (Hopkins and May, 2011). In addition, we use a linearly interpolated target-side language model trained with KneserNey smoothing on 480M tokens of data in various domains. LM interpolation weights are also optimized per test set. Consistent with Axelrod et al. (2011), we do not vary the target-side LM between different experiments on the same test set. All ngram models in our work are 5-gram.\nFor our NMT experiments we use an in-house encoder-decoder3 model with global attention as described in Luong et al. (2015a). This choice comes at the cost of optimal translation quality but allows for a relatively fast realization of largescale experiments given our available resources. Both the encoder and decoder are four-layer unidirectional LSTMs, with embedding and layer sizes\n3github.com/ketranm/tardis\nof 1,000. We uniformly initialize all parameters, and use SGD with a mini-batch size of 64 and an initial learning rate of 1, which is decayed by a factor two every epoch after the fifth epoch. We use dropout with probability 0.3, and a beam size of 12. We train for 16 epochs and test on the model from the last epoch. All NMT experiments are run on a single NVIDIA Titan X GPU."}, {"heading": "4.2 Training and evaluation data", "text": "We evaluate all experiments on four domains: (i) EMEA medical guidelines (Tiedemann, 2009), (ii) movie dialogues (van der Wees et al., 2016) constructed from OpenSubtitles (Lison and Tiedemann, 2016), (iii) TED talks (Cettolo et al., 2012), and (iv) WMT news. For TED, we use IWSLT2010 as development set and IWSLT20112014 as test set, and for WMT we use newstest2013 as development set and newstest2016 as test set. We train our systems on a mixture of domains, comprising Commoncrawl, Europarl, News Commentary, EMEA, Movies, and TED. Corpus specifications are listed in Table 1.\nThe in-domain LMs used to rank training sentences for data selection are trained on small portions of in-domain parallel data whenever available (3.3M, 1.2M and 3.3M German tokens for EMEA, Movies and TED, respectively). Since no sizeable in-domain parallel text is available for WMT, we independently sample 200K sentences from the WMT monolingual News Crawl corpora (3.3M German tokens or 3.5M English tokens). This demonstrates the applicability of data selection techniques even in cases where one lacks parallel in-domain data.\nBefore running data selection, we preprocess our data by tokenizing, lowercasing and remov-\n0 10\n20\n30\n40 50 B LE U\nEMEA medical\n0\n5\n10\n15\n20\n25\n30\nB LE\nU\nMovie dialogues\n0 5 10 15 20 25 30 35 40 45 50\nSelection size (% of complete bitext)\n0\n5\n10\n15\n20\n25\n30\nB LE\nU\nTED talks\n0 5 10 15 20 25 30 35 40 45 50\nSelection size (% of complete bitext)\n0\n5\n10\n15\n20\n25\n30\nB LE\nU\nWMT news\nSize of in-domain bitext PBMT complete baseline PBMT Axelrod selection PBMT random selection NMT complete baseline NMT Axelrod selection NMT random selection\nFigure 2: PBMT (purple) and NMT (green) German\u2192English results of Axelrod data selection and random data selection (average of three runs) for four domains. Purple and green stars indicate BLEU scores when only the available in-domain data is used. We use selections of the in-domain size |I|, and 5%, 10%, 20%, and 50% of the complete bitext, which are exactly the same for PBMT and NMT.\ning sentences that are longer than 50 tokens or that are identified as a different language. After selection, we apply Byte-pair encoding (BPE, Sennrich et al. (2016b)) with 40K merge operations on either side of the complete mix-of-domains training bitext. For our NMT experiments we use BPEprocessed corpora on both bitext sides, while for PBMT we only apply BPE to the German side. Our NMT systems use a vocabulary size of 40K on both the source and target side."}, {"heading": "5 Results", "text": "Below we discuss the results of our translation experiments using static and dynamic data selection, measuring translation quality with case-insensitive untokenized BLEU (Papineni et al., 2002)."}, {"heading": "5.1 Static data selection for PBMT and NMT", "text": "We first compare the effects of static data selection with n-gram LMs on both NMT and PBMT using various selection sizes. Concretely, we select the top n sentence pairs such that the number of selected tokens t \u2208 { 5%, 10%, 20%, 50% } of G, or t = |I| (the in-domain corpus size). Figure 2 shows German\u2192English translation perfor-\nmance in BLEU for our four test sets. The benefits of n-gram-based data selection for PBMT (purple circles) are confirmed: In all test sets, the selection of size |I| (dotted vertical line) yields better performance than using only the in-domain data of the exact same size (purple star), and at least one of the selected subsets\u2014often using only 5% of the complete bitext\u2014outperforms using the complete bitext (light purple line). We also show that the informed selections are superior to random selections of the same size (purple diamonds).\nIn NMT, results of n-gram-based data selection (green triangles) vary: While for Movies a selection of only 10% outperforms the complete bitext (light green line), none of the selected subsets for other test sets is noticeably better than the full bitext.4 Interestingly, the same selections of size |I| that proved useful in PBMT, never beat the system that uses exactly the available in-domain data (green star), indicating that the current selections can be further improved for NMT. In all scenarios we see that NMT suffers much more from smalldata settings than PBMT. Finally, the random se-\n4Validation cross-entropy converges after 10\u201312 epochs, never reaching the scores of the complete bitext.\nlections (green squares) show that NMT not only needs large quantities of data, but it is also affected when the selected data is of low quality. In PBMT, both low-quantity and low-quality scenarios appear to be compensated for by the large monolingual LM on the target side.\nWhen comparing the different test sets, we observe that the impact of domain mismatch in NMT with respect to PBMT is largest for the two domains that are most distinct from the general bitext, EMEA and Movies. For WMT, both MT systems achieve very similar baseline results, but translation quality deteriorates considerably in data selection experiments, which is likely caused by the lack of in-domain data in the general bitext.\nLSTM versus n-gram Before proceeding with dynamic data selection for NMT, we test whether bitext ranking for NMT can be improved using LSTMs rather than conventional n-gram LMs. Table 2 shows NMT BLEU scores of a few different sizes of selected subsets created using n-gram LMs or LSTMs. While results vary among test sets and selection sizes, we observe an average improvement of 0.4 BLEU when using LSTMs instead of n-gram LMs. For PBMT, similar results have been reported when replacing n-gram LMs with recurrent neural LMs (Duh et al., 2013). In all subsequent experiments we use relevance rankings computed with LSTMs instead of n-gram LMs."}, {"heading": "5.2 Dynamic data selection for NMT", "text": "Equipped with a relevance ranking of sentence pairs in bitext G, we now examine two variants of dynamic data selection as described in Section 3.\nWe are interested in reducing training time while limiting the negative effect on BLEU for various domains. Therefore we report BLEU as\nwell as the relative training time of each experiment. Since wall-clock times depend on other factors such as the NMT architecture and memory speed, we define training time as the total number of tokens observed while training the NMT system, i.e., the sum of tokens in the selected subsets of all epochs. We report all training times relative to the training time of our complete-bitext baseline (i.e., 4.3M tokens \u00d7 16 epochs). Note that this measure of training time corresponds closely but not exactly to the number of model updates, as the latter relies on the number of sentences, which vary in length, rather than the number of tokens in the training data. For completeness: Training the 100% baseline takes 106 hours, while our fastest dynamic selection variant takes 19\u201321 hours. Computing CED scores takes \u223c15 minutes when using n-gram LMs and 5\u20136 hours when using LSTMs.\nFigure 3 shows BLEU scores of some selected experiments as a function of relative training time. Compared to static data selection (blue lines), our weighted sampling technique (orange triangles) yields variable results. When sampling a subset of 20% of |G| from the top 50% of the ranked bitext, we obtain small improvements for TED and WMT, but small drops for EMEA and Movies. Other selection sizes (30% and 40%, not shown) give similar results lacking a consistent pattern.\nBy contrast, our gradual fine-tuning method performs consistently better than static selection, and even beats the general baseline in three out of four test sets. The displayed version uses settings (\u03b1 = 0.5, \u03b2 = 0.7, \u03b7 = 2) and is at least as fast as static selection using 20% of the bitext, yielding up to +2.6 BLEU improvement (for WMT news) over this static version. Compared to the complete baseline, this gradual fine-tuning method improves up to +3.1 BLEU (for TED talks).\nTable 3 provides detailed information on additional experiments using other settings. For all three test domains which are covered in the parallel data\u2014EMEA, Movies and TED\u2014 improvements are highest when starting gradual fine-tuning with only the top 50% of the ranked bitext, which are also the fastest approaches. For WMT, which is not covered in the general bitext, adding more data clearly benefits translation quality. These findings are consistent with the static data selection patterns; Using low-ranked sentences on top of the most relevant selection\ndoes not improve translation performance for any domain except WMT news.\nFinally, we compare our data selection experiments to domain-specific fine-tuning (light blue stars in Figure 3), which is the current state-of-theart for domain adaptation in NMT. To this end, we first train a model on the complete bitext, and then train for twelve additional epochs on available indomain data, using an initial learning rate of 1 which halves every epoch. Depending on the test\nset, this approach yields +2.5\u20134.4 BLEU improvements over our baselines, however it does not speed up training and requires a parallel in-domain text which may not be available (e.g., for WMT). While none of our data selection experiments outperforms domain-specific fine-tuning, we obtain competitive translation quality in only 20% of the training time. In additional experiments we found that in-domain fine-tuning on top of our selection approaches does not yield improvements."}, {"heading": "6 Further analysis", "text": "In this section we conduct a few additional experiments and analyses. We restrict to one parameter setting per selection approach: Static selection and sampling with 20% of the data, and gradual finetuning using (\u03b1 = 0.5, \u03b2 = 0.7, \u03b7 = 2). All have very similar training times.\nFirst, we hypothesize that dynamic data selection works well because more different sentence pairs are observed during training, and it therefore increases coverage with respect to static data selection. To verify this, we measure for each test set the number of unseen source word types in the training data of different selection methods. Figure 4 shows indeed that the average number of unseen word types is reduced noticeably in both of our dynamic selection techniques, being much closer to the complete bitext baseline than to static selection. Note that all methods use the same vocabulary during training.\nNext, following the static data selection experiments in Section 5.1, we examine how well dynamic data selection performs using random selections. To this end, we repeat all techniques using a bitext which is ranked randomly rather than by its relevance to the test sets. The results in Table 4 show that the bitext ranking plays a crucial role in the success of data selection. However, the results also show that even in the absence of an appropriate bitext ranking, dynamic data selection\u2014and in particular gradual fine-tuning\u2014is still superior to static data selection. We explain this result as follows: Compared to static selection, both sampling and gradual fine-tuning have better coverage due to their improved exploration of the data. However, sampling also suffers from a surprise effect of observing new data in every epoch. Gradual fine-tuning on the other hand gradually improves\nlearning on a subset of the selected data, suggesting that repetition across epochs has a positive effect on translation quality.\nOne could expect that changing the data during training results in volatile training behavior. To test this, we inspect cross-entropy of our development sets after every training epoch. Figure 5 shows these results for TED. Clearly, static data selection converges most steadily. However, both dynamic selection techniques eventually converge to a lower cross-entropy value which is reflected by higher translation quality of the test set. We observe very similar behavior for the other test sets.\nBy its nature, our gradual fine-tuning technique uses training epochs of different sizes, and therefore also implicitly differs from other methods in its parameter optimization behavior. Since we decrease both the training data size and the SGD learning rate after finishing complete training epochs, we automatically decay the learning rate at decreasing time intervals. We therefore study how this approach is affected when we (i)\ndecay the learning rate after a fixed number of updates (i.e., the same as in static data selection) rather than per epoch, or (ii) keep the learning rate fixed. In the first scenario, we observe that translation performance drops with \u20131.1\u20132.0 BLEU. When keeping a fixed learning rate, BLEU scores hardly change or even improve, indicating that the implicit change in search behavior may contribute to the success of gradual fine-tuning."}, {"heading": "7 Related work", "text": "A few research topics are related to our work. Regarding data selection for SMT, previous work has targeted two goals; to reduce model sizes and training times, or to adapt to new domains. Data selection methods for domain adaptation mostly employ information theory metrics to rank training sentences by their relevance to the domain at hand. This has been applied monolingually (Gao et al., 2002) as well as bilingually (Yasuda et al., 2008). In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively.\nData selection with the aim of model size and training time reduction has the objective to use the minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasco\u0301 et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively similar. A comprehensive survey on data selection for SMT is provided by Eetemadi et al. (2015). While in this work we have used a similarity objective to rank our bitext, one could also apply dynamic data selection using a coverage objective.\nIn NMT, data selection can serve similar goals as in PBMT; increasing training efficiency or domain adaptation. Domain adaptation in NMT typically involves training a model on the complete bitext, followed by fine-tuning the parameters on a smaller in-domain corpus (Luong and Manning,\n2015; Zoph et al., 2016). Other work combines fine-tuning with model ensembles (Freitag and AlOnaizan, 2016) or with domain-specific tags in the training corpus (Chu et al., 2017). Finally, Sennrich et al. (2016a) adapt their systems by backtranslating in-domain data, which is then added to the training data and used for fine-tuning.\nSome other previous work has addressed training efficiency for NMT, for example by parallelizing models or data (Wu et al., 2016), modifying the NMT network structure (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are \u2018challenging\u2019 to the NMT system (Zhang et al., 2016). The latter is most related to our work since training data is also adjusted during training, however we reduce the training data size much more aggressively and study different techniques of data selection.\nFinally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and Sa\u0301nchezCartagena, 2017; Koehn and Knowles, 2017)."}, {"heading": "8 Conclusions", "text": "With the recent increase in popularity of neural machine translation (NMT), we explored in this paper to what extent and how NMT can benefit from data selection. We first showed that a stateof-the-art data selection method yields unreliable results for NMT while consistently performing well for PBMT. Next, we have introduced dynamic data selection for NMT, which entails varying the selected subset of training data between different training epochs. We explored two techniques of dynamic data selection and found that our gradual fine-tuning technique, in which we gradually reduce training size, improves consistently over conventional static data selection (up to +2.6 BLEU) and over a high-resource general baseline (up to +3.1 BLEU). Moreover, gradual finetuning approximates in-domain fine-tuning using only\u223c20% of the training time, even when no parallel in-domain data is available."}, {"heading": "Acknowledgments", "text": "This research was funded in part by NWO under project numbers 639.022.213 and 639.021.646. We thank Ke Tran for providing the NMT system, and the reviewers for their valuable comments."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355\u2013362.", "citeRegEx": "Axelrod et al\\.,? 2011", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Data selection with fewer words", "author": ["Amittai Axelrod", "Philip Resnik", "Xiaodong He", "Mari Ostendorf."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 58\u201365.", "citeRegEx": "Axelrod et al\\.,? 2015", "shortCiteRegEx": "Axelrod et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Findings of the 2016 conference on machine translation (WMT16)", "author": ["Ondrej Bojar", "Rajen Chatterjee", "Christian Federmann", "Yvette Graham", "Barry Haddow", "Matthias Huck", "Antonio Jimeno Yepes", "Philipp Koehn", "Varvara Logacheva", "Christof Monz"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2016}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT), pages 261\u2013268.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Semi-supervised convolutional networks for translation adaptation with tiny amount of in-domain data", "author": ["Boxing Chen", "Fei Huang."], "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages", "citeRegEx": "Chen and Huang.,? 2016", "shortCiteRegEx": "Chen and Huang.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "An empirical comparison of simple domain adaptation methods for neural machine translation", "author": ["Chenhui Chu", "Raj Dabre", "Sadao Kurohashi."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Chu et al\\.,? 2017", "shortCiteRegEx": "Chu et al\\.", "year": 2017}, {"title": "SYSTRAN\u2019s pure neural machine translation systems", "author": ["Josep Crego", "Jungi Kim", "Guillaume Klein", "Anabel Rebollo", "Kathy Yang", "Jean Senellart", "Egor Akhanov", "Patrice Brunelle", "Aurelien Coquard", "Yongchao Deng"], "venue": null, "citeRegEx": "Crego et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Crego et al\\.", "year": 2016}, {"title": "Adaptation data selection using neural language models: Experiments in machine translation", "author": ["Kevin Duh", "Graham Neubig", "Katsuhito Sudoh", "Hajime Tsukada."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Duh et al\\.,? 2013", "shortCiteRegEx": "Duh et al\\.", "year": 2013}, {"title": "Low cost portability for statistical machine translation based on n-gram frequency and TF-IDF", "author": ["Matthias Eck", "Stephan Vogel", "Alex Waibel."], "venue": "Proceedings of the 2005 International Workshop on Spoken Language Translation, pages 61\u201367.", "citeRegEx": "Eck et al\\.,? 2005", "shortCiteRegEx": "Eck et al\\.", "year": 2005}, {"title": "Survey of data-selection methods in statistical machine translation", "author": ["Sauleh Eetemadi", "William Lewis", "Kristina Toutanova", "Hayder Radha."], "venue": "Machine Translation, 29(3-4):189\u2013223.", "citeRegEx": "Eetemadi et al\\.,? 2015", "shortCiteRegEx": "Eetemadi et al\\.", "year": 2015}, {"title": "Data augmentation for low-resource neural machine translation", "author": ["Marzieh Fadaee", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Fadaee et al\\.,? 2017", "shortCiteRegEx": "Fadaee et al\\.", "year": 2017}, {"title": "Neural vs", "author": ["M. Amin Farajian", "Marco Turchi", "Matteo Negri", "Nicola Bertoldi", "Marcello Federico."], "venue": "phrase-based machine translation in a multi-domain scenario. In Proceedings of the 15th Conference of the European Chapter of the Association for Com-", "citeRegEx": "Farajian et al\\.,? 2017", "shortCiteRegEx": "Farajian et al\\.", "year": 2017}, {"title": "Fast domain adaptation for neural machine translation", "author": ["Markus Freitag", "Yaser Al-Onaizan."], "venue": "arXiv preprint arXiv:1612.06897.", "citeRegEx": "Freitag and Al.Onaizan.,? 2016", "shortCiteRegEx": "Freitag and Al.Onaizan.", "year": 2016}, {"title": "Toward a unified approach to statistical language modeling for chinese", "author": ["Jianfeng Gao", "Joshua Goodman", "Mingjing Li", "KaiFu Lee."], "venue": "ACM Transactions on Asian Language Information Processing (TALIP), 1(1):3\u201333.", "citeRegEx": "Gao et al\\.,? 2002", "shortCiteRegEx": "Gao et al\\.", "year": 2002}, {"title": "Does more data always yield better translations", "author": ["Guillem Gasc\u00f3", "Martha-Alicia Rocha", "Germ\u00e1n Sanchis-Trilles", "Jes\u00fas Andr\u00e9s-Ferrer", "Francisco Casacuberta"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association", "citeRegEx": "Gasc\u00f3 et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gasc\u00f3 et al\\.", "year": 2012}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352\u20131362.", "citeRegEx": "Hopkins and May.,? 2011", "shortCiteRegEx": "Hopkins and May.", "year": 2011}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu."], "venue": "arXiv preprint arXiv:1610.10099.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Sequencelevel knowledge distillation", "author": ["Yoon Kim", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327.", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Demo and", "citeRegEx": "Zens et al\\.,? 2007", "shortCiteRegEx": "Zens et al\\.", "year": 2007}, {"title": "Six challenges for neural machine translation", "author": ["Philipp Koehn", "Rebecca Knowles."], "venue": "arXiv preprint arXiv:1706.03872.", "citeRegEx": "Koehn and Knowles.,? 2017", "shortCiteRegEx": "Koehn and Knowles.", "year": 2017}, {"title": "Dramatically reducing training data size through vocabulary saturation", "author": ["William D. Lewis", "Sauleh Eetemadi."], "venue": "Proceedings of the 8th Workshop on Statistical Machine Translation, pages 281\u2013291.", "citeRegEx": "Lewis and Eetemadi.,? 2013", "shortCiteRegEx": "Lewis and Eetemadi.", "year": 2013}, {"title": "Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles", "author": ["Pierre Lison", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016).", "citeRegEx": "Lison and Tiedemann.,? 2016", "shortCiteRegEx": "Lison and Tiedemann.", "year": 2016}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Minh-Thang Luong", "Christopher D Manning."], "venue": "Proceedings of the 12th International Workshop on Spoken Language Translation, pages 76\u201379.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Data selection for compact adapted SMT models", "author": ["Shachar Mirkin", "Laurent Besacier."], "venue": "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas, pages 301\u2013314.", "citeRegEx": "Mirkin and Besacier.,? 2014", "shortCiteRegEx": "Mirkin and Besacier.", "year": 2014}, {"title": "Intelligent selection of language model training data", "author": ["Robert C. Moore", "William Lewis."], "venue": "Proceedings of the ACL 2010 Conference Short Papers, pages 220\u2013224.", "citeRegEx": "Moore and Lewis.,? 2010", "shortCiteRegEx": "Moore and Lewis.", "year": 2010}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning to select data for transfer learning with bayesian optimization", "author": ["Sebastian Ruder", "Barbara Plank."], "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Ruder and Plank.,? 2017", "shortCiteRegEx": "Ruder and Plank.", "year": 2017}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 86\u201396.", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "News from OPUS-a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent advances in natural language processing, volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "A multifaceted evaluation of neural versus phrasebased machine translation for 9 language directions", "author": ["Antonio Toral", "V\u0131\u0301ctor M. S\u00e1nchez-Cartagena"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association", "citeRegEx": "Toral and S\u00e1nchez.Cartagena.,? \\Q2017\\E", "shortCiteRegEx": "Toral and S\u00e1nchez.Cartagena.", "year": 2017}, {"title": "Measuring the effect of conversational aspects on machine translation quality", "author": ["Marlies van der Wees", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics, pages 2571\u2013", "citeRegEx": "Wees et al\\.,? 2016", "shortCiteRegEx": "Wees et al\\.", "year": 2016}, {"title": "Method of selecting training data to build a compact and efficient translation model", "author": ["Keiji Yasuda", "Ruiqiang Zhang", "Hirofumi Yamamoto", "Eiichiro Sumit."], "venue": "International Joint Conference on Natural Language Processing (IJCNLP), pages 655\u2013660.", "citeRegEx": "Yasuda et al\\.,? 2008", "shortCiteRegEx": "Yasuda et al\\.", "year": 2008}, {"title": "Boosting neural machine translation", "author": ["Dakun Zhang", "Jungi Kim", "Joseph Crego", "Jean Senellart."], "venue": "arXiv preprint arXiv:1612.06138.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Transfer learning for lowresource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "arXiv preprint arXiv:1604.02201.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT.", "startOffset": 38, "endOffset": 60}, {"referenceID": 34, "context": "Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) as the most common machine translation paradigm.", "startOffset": 99, "endOffset": 164}, {"referenceID": 7, "context": "Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) as the most common machine translation paradigm.", "startOffset": 99, "endOffset": 164}, {"referenceID": 2, "context": "Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) as the most common machine translation paradigm.", "startOffset": 99, "endOffset": 164}, {"referenceID": 4, "context": "With large quantities of parallel data, NMT outperforms PBMT for an increasing number of language pairs (Bojar et al., 2016).", "startOffset": 104, "endOffset": 124}, {"referenceID": 29, "context": "In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasc\u00f3 et al., 2012).", "startOffset": 179, "endOffset": 244}, {"referenceID": 0, "context": "In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasc\u00f3 et al., 2012).", "startOffset": 179, "endOffset": 244}, {"referenceID": 17, "context": "In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasc\u00f3 et al., 2012).", "startOffset": 179, "endOffset": 244}, {"referenceID": 7, "context": "While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translation quality have not been investigated.", "startOffset": 76, "endOffset": 115}, {"referenceID": 27, "context": "While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translation quality have not been investigated.", "startOffset": 76, "endOffset": 115}, {"referenceID": 40, "context": "1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (Zoph et al., 2016; Fadaee et al., 2017), and do not have a separate large-scale target-side language model to compensate for smaller parallel training data.", "startOffset": 108, "endOffset": 148}, {"referenceID": 13, "context": "1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (Zoph et al., 2016; Fadaee et al., 2017), and do not have a separate large-scale target-side language model to compensate for smaller parallel training data.", "startOffset": 108, "endOffset": 148}, {"referenceID": 0, "context": "(i) We compare the effects of a commonly used data selection approach (Axelrod et al., 2011) on PBMT and NMT using four different test sets.", "startOffset": 70, "endOffset": 92}, {"referenceID": 0, "context": "As a first step towards dynamic data selection for NMT, we compare the effects of a commonly used, state-of-the-art data selection method (Axelrod et al., 2011) on both neural and phrase-based MT.", "startOffset": 138, "endOffset": 160}, {"referenceID": 0, "context": ", have high HG, we compute for each sentence pair s the bilingual cross-entropy difference CEDs following Axelrod et al. (2011):", "startOffset": 106, "endOffset": 128}, {"referenceID": 29, "context": "Following related work by Moore and Lewis (2010), we restrict the vocabulary of the LMs to the words occurring at least twice in the in-domain corpus.", "startOffset": 26, "endOffset": 49}, {"referenceID": 27, "context": "While data selection aims to discard irrelevant data, it can also exacerbate the problem of low vocabulary coverage and unreliable statistics for rarer words in the \u2018long tail\u2019, which are major issues in NMT (Luong et al., 2015b; Sennrich et al., 2016b).", "startOffset": 208, "endOffset": 253}, {"referenceID": 33, "context": "While data selection aims to discard irrelevant data, it can also exacerbate the problem of low vocabulary coverage and unreliable statistics for rarer words in the \u2018long tail\u2019, which are major issues in NMT (Luong et al., 2015b; Sennrich et al., 2016b).", "startOffset": 208, "endOffset": 253}, {"referenceID": 40, "context": "In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017).", "startOffset": 97, "endOffset": 162}, {"referenceID": 13, "context": "In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017).", "startOffset": 97, "endOffset": 162}, {"referenceID": 22, "context": "In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017).", "startOffset": 97, "endOffset": 162}, {"referenceID": 23, "context": "(2011), dynamic data selection can also be applied using other ranking criteria, for example limiting redundancy in the training data (Lewis and Eetemadi, 2013) or complementing similarity with diversity (Ruder and Plank, 2017).", "startOffset": 134, "endOffset": 160}, {"referenceID": 31, "context": "(2011), dynamic data selection can also be applied using other ranking criteria, for example limiting redundancy in the training data (Lewis and Eetemadi, 2013) or complementing similarity with diversity (Ruder and Plank, 2017).", "startOffset": 204, "endOffset": 227}, {"referenceID": 0, "context": "While we use in this work a domain-relevance ranking of the bitext following Axelrod et al. (2011), dynamic data selection can also be applied using other ranking criteria, for example limiting redundancy in the training data (Lewis and Eetemadi, 2013) or complementing similarity with diversity (Ruder and Plank, 2017).", "startOffset": 77, "endOffset": 99}, {"referenceID": 25, "context": "Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data.", "startOffset": 138, "endOffset": 236}, {"referenceID": 40, "context": "Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data.", "startOffset": 138, "endOffset": 236}, {"referenceID": 32, "context": "Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data.", "startOffset": 138, "endOffset": 236}, {"referenceID": 15, "context": "Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data.", "startOffset": 138, "endOffset": 236}, {"referenceID": 18, "context": "To create optimal PBMT systems given the available resources, we apply test-set-specific parameter tuning using PRO (Hopkins and May, 2011).", "startOffset": 116, "endOffset": 139}, {"referenceID": 0, "context": "Consistent with Axelrod et al. (2011), we do not vary the target-side LM between different experiments on the same test set.", "startOffset": 16, "endOffset": 38}, {"referenceID": 0, "context": "Consistent with Axelrod et al. (2011), we do not vary the target-side LM between different experiments on the same test set. All ngram models in our work are 5-gram. For our NMT experiments we use an in-house encoder-decoder3 model with global attention as described in Luong et al. (2015a). This choice comes at the cost of optimal translation quality but allows for a relatively fast realization of largescale experiments given our available resources.", "startOffset": 16, "endOffset": 291}, {"referenceID": 35, "context": "We evaluate all experiments on four domains: (i) EMEA medical guidelines (Tiedemann, 2009), (ii) movie dialogues (van der Wees et al.", "startOffset": 73, "endOffset": 90}, {"referenceID": 24, "context": ", 2016) constructed from OpenSubtitles (Lison and Tiedemann, 2016), (iii) TED talks (Cettolo et al.", "startOffset": 39, "endOffset": 66}, {"referenceID": 5, "context": ", 2016) constructed from OpenSubtitles (Lison and Tiedemann, 2016), (iii) TED talks (Cettolo et al., 2012), and (iv) WMT news.", "startOffset": 84, "endOffset": 106}, {"referenceID": 32, "context": "After selection, we apply Byte-pair encoding (BPE, Sennrich et al. (2016b)) with 40K merge operations on either side of the complete mix-of-domains training bitext.", "startOffset": 51, "endOffset": 75}, {"referenceID": 30, "context": "Below we discuss the results of our translation experiments using static and dynamic data selection, measuring translation quality with case-insensitive untokenized BLEU (Papineni et al., 2002).", "startOffset": 170, "endOffset": 193}, {"referenceID": 10, "context": "For PBMT, similar results have been reported when replacing n-gram LMs with recurrent neural LMs (Duh et al., 2013).", "startOffset": 97, "endOffset": 115}, {"referenceID": 16, "context": "This has been applied monolingually (Gao et al., 2002) as well as bilingually (Yasuda et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 38, "context": ", 2002) as well as bilingually (Yasuda et al., 2008).", "startOffset": 31, "endOffset": 52}, {"referenceID": 29, "context": "In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain.", "startOffset": 147, "endOffset": 198}, {"referenceID": 0, "context": "In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively.", "startOffset": 171, "endOffset": 330}, {"referenceID": 0, "context": "In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively.", "startOffset": 171, "endOffset": 356}, {"referenceID": 11, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013).", "startOffset": 72, "endOffset": 136}, {"referenceID": 17, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013).", "startOffset": 72, "endOffset": 136}, {"referenceID": 23, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013).", "startOffset": 72, "endOffset": 136}, {"referenceID": 11, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively similar.", "startOffset": 73, "endOffset": 189}, {"referenceID": 11, "context": "minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc\u00f3 et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively similar. A comprehensive survey on data selection for SMT is provided by Eetemadi et al. (2015). While in this work we have used a similarity objective to rank our bitext, one could also apply dynamic data selection using a coverage objective.", "startOffset": 73, "endOffset": 481}, {"referenceID": 25, "context": "Domain adaptation in NMT typically involves training a model on the complete bitext, followed by fine-tuning the parameters on a smaller in-domain corpus (Luong and Manning, 2015; Zoph et al., 2016).", "startOffset": 154, "endOffset": 198}, {"referenceID": 40, "context": "Domain adaptation in NMT typically involves training a model on the complete bitext, followed by fine-tuning the parameters on a smaller in-domain corpus (Luong and Manning, 2015; Zoph et al., 2016).", "startOffset": 154, "endOffset": 198}, {"referenceID": 8, "context": "Other work combines fine-tuning with model ensembles (Freitag and AlOnaizan, 2016) or with domain-specific tags in the training corpus (Chu et al., 2017).", "startOffset": 135, "endOffset": 153}, {"referenceID": 8, "context": "Other work combines fine-tuning with model ensembles (Freitag and AlOnaizan, 2016) or with domain-specific tags in the training corpus (Chu et al., 2017). Finally, Sennrich et al. (2016a) adapt their systems by backtranslating in-domain data, which is then added to the training data and used for fine-tuning.", "startOffset": 136, "endOffset": 188}, {"referenceID": 19, "context": ", 2016), modifying the NMT network structure (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 9, "context": ", 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are \u2018challenging\u2019 to the NMT system (Zhang et al.", "startOffset": 76, "endOffset": 116}, {"referenceID": 20, "context": ", 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are \u2018challenging\u2019 to the NMT system (Zhang et al.", "startOffset": 76, "endOffset": 116}, {"referenceID": 39, "context": ", 2016; Kim and Rush, 2016), or by boosting parts of the data that are \u2018challenging\u2019 to the NMT system (Zhang et al., 2016).", "startOffset": 103, "endOffset": 123}, {"referenceID": 3, "context": "Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S\u00e1nchezCartagena, 2017; Koehn and Knowles, 2017).", "startOffset": 73, "endOffset": 180}, {"referenceID": 14, "context": "Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S\u00e1nchezCartagena, 2017; Koehn and Knowles, 2017).", "startOffset": 73, "endOffset": 180}, {"referenceID": 22, "context": "Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S\u00e1nchezCartagena, 2017; Koehn and Knowles, 2017).", "startOffset": 73, "endOffset": 180}], "year": 2017, "abstractText": "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.", "creator": "LaTeX with hyperref package"}}}