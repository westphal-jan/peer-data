{"id": "1708.02977", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Aug-2017", "title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "abstract": "we address the problem encountered of end - to - end visual storytelling. given a photo album, our model first selects the most representative ( summary ) photos, and it then composes a consensus natural language story for the word album. for this assessment task, we regularly make use of the current visual storytelling dataset and a model composed of three hierarchically - attentive recurrent neural nets ( rnns ) leading to : encode the album photos, select representative ( summary ) photos, and compose the story. automatic and human evaluations show our model approach achieves better performance rely on selection, generation, and retrieval than baselines.", "histories": [["v1", "Wed, 9 Aug 2017 19:26:47 GMT  (6201kb,D)", "http://arxiv.org/abs/1708.02977v1", "To appear at EMNLP-2017 (7 pages)"]], "COMMENTS": "To appear at EMNLP-2017 (7 pages)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["licheng yu", "mohit bansal", "tamara l berg"], "accepted": true, "id": "1708.02977"}, "pdf": {"name": "1708.02977.pdf", "metadata": {"source": "CRF", "title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "authors": ["Licheng Yu", "Mohit Bansal", "Tamara L. Berg"], "emails": ["tlberg}@cs.unc.edu"], "sections": [{"heading": "1 Introduction", "text": "Since we first developed language, humans have always told stories. Fashioning a good story is an act of creativity and developing algorithms to replicate this has been a long running challenge. Adding pictures as input can provide information for guiding story construction by offering visual illustrations of the storyline. In the related task of image captioning, most methods try to generate descriptions only for individual images or for short videos depicting a single activity. Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).\nThe type of data we consider in this paper provides input illustrations for story generation in the form of photo albums, sampled over a few minutes to a few days of time. For this type of data, generating textual descriptions involves telling a temporally consistent story about the depicted visual information, where stories must be coherent and take into account the temporal context of the im-\nages. Applications of this include constructing visual and textual summaries of albums, or even enabling search through personal photo collections to find photos of life events.\nPrevious visual storytelling works can be classified into two types, vision-based and languagebased, where image or language stories are constructed respectively. Among the vision-based approaches, unsupervised learning is commonly applied: e.g., (Sigurdsson et al., 2016) learns the latent temporal dynamics given a large amount of albums, and (Kim and Xing, 2014) formulate the photo selection as a sparse time-varying directed graph. However, these visual summaries tend to be difficult to evaluate and selected photos may not agree with human selections. For languagebased approaches, a sequence of natural language sentences are generated to describe a set of photos. To drive this work (Park and Kim, 2015) collected a dataset mined from Blog Posts. However, this kind of data often contains contextual information or loosely related language. A more direct dataset was recently released (Huang et al., 2016), where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.\nIn this paper, we make use of the Visual Storytelling Dataset (Huang et al., 2016). While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5- representative (summary) photos hand-selected by people from an album. Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums. This requires us to either generate a story from all of the album\u2019s photos or to learn selection mechanisms to identify representative photos and then generate stories from those summary photos. We evaluate each type of approach.\nUltimately, we propose a model of hierarchically-attentive recurrent neural nets,\nar X\niv :1\n70 8.\n02 97\n7v 1\n[ cs\n.C L\n] 9\nA ug\n2 01\n7\nconsisting of three RNN stages. The first RNN encodes the whole album context and each photo\u2019s content, the second RNN provides weights for photo selection, and the third RNN takes the weighted representation and decodes to the resulting sentences. Note that during training, we are only given the full input albums and the output stories, and our model needs to learn the summary photo selections latently.\nWe show that our model achieves better performance over baselines under both automatic metrics and human evaluations. As a side product, we show that the latent photo selection also reasonably mimics human selections. Additionally, we propose an album retrieval task that can reliably pick the correct photo album given a sequence of sentences, and find that our model also outperforms the baselines on this task."}, {"heading": "2 Related work", "text": "Recent years have witnessed an explosion of interest in vision and language tasks, reviewed below. Visual Captioning: Most recent approaches to image captioning (Vinyals et al., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions. For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description. Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information. Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots. While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014). Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling (Kim and Xing, 2014), unsupervised mining (Sigurdsson et al., 2016), blog-photo\nalignment (Kim et al., 2015), and language retelling (Huang et al., 2016; Park and Kim, 2015). While (Park and Kim, 2015) collects data by mining Blog Posts, (Huang et al., 2016) collects stories using Mechanical Turk, providing more directly relevant stories."}, {"heading": "3 Model", "text": "Our model (Fig. 1) is composed of three modules: Album Encoder, Photo Selector, and Story Generator, jointly learned during training."}, {"heading": "3.1 Album Encoder", "text": "Given an album A = {a1, a2, ..., an}, composed of a set of photos, we use a bi-directional RNN to encode the local album context for each photo. We first extract the 2048-dimensional visual representation fi \u2208 Rk for each photo using ResNet101 (He et al., 2016), then a bi-directional RNN is applied to encode the full album. Following (Huang et al., 2016), we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence. The sequence output at each time step encodes the local album context for each photo (from both directions). Fused with the visual representation followed by ReLU, our final photo representation is (top module in Fig. 1):\nfi = ResNet(ai) ~hi = ~GRUalbum(fi,~hi\u22121) ~hi = ~GRUalbum(fi, ~hi+1)\nvi = ReLU([~hi, ~hi] + fi)."}, {"heading": "3.2 Photo Selector", "text": "The Photo Selector (illustrated in the middle yellow part of Fig. 1) identifies representative photos to summarize an album\u2019s content. As discussed, we do not assume that we are given the ground-truth album summaries during training, instead regarding selection as a latent variable in the end-to-end learning. Inspired by Pointer Networks (Vinyals et al., 2015a), we use another GRU-RNN to perform this task 1.\nGiven the album representation V n\u00d7k, the photo selector outputs probabilities pt \u2208 Rn (likelihood of selection as t-th summary image) for all photos using soft attention.\nh\u0304t = GRUselect(pt\u22121, h\u0304t\u22121),\np(yai(t) = 1) = \u03c3(MLP([h\u0304t, vi])),\n1While the pointer network requires grounding labels, we regard the labels as latent variables\nAt each summarization step, t, the GRU takes the previous pt\u22121 and previous hidden state as input, and outputs the next hidden state h\u0304t. h\u0304t is fused with each photo representation vi to compute the ith photo\u2019s attention pit = p(yai(t) = 1). At test time, we simply pick the photo with the highest probability to be the summary photo at step t."}, {"heading": "3.3 Story Generator", "text": "To generate an album\u2019s story, given the album representation matrix V and photo summary probabilities pt from the first two modules, we compute the visual summary representation gt \u2208 Rk (for the t-th summary step). This is a weighted sum of the album representations, i.e., gt = pTt V . Each of these 5 gt embeddings (for t = 1 to 5) is then used to decode 1 of the 5 story sentences respectively, as shown in the blue part of Fig. 1.\nGiven a story S = {st}, where st is t-th summary sentence. Following Donahue et al. (2015), the l-th word probability of the t-th sentence is:\nwt,l\u22121 = West,l\u22121,\nh\u0303t,l = GRUstory(wt,l\u22121, gt, h\u0303t,l\u22121),\np(st,l) = softmax(MLP(h\u0303t,l)),\n(1)\nwhere We is the word embedding. The GRU takes the joint input of visual summarization gt, the previous word embedding wt,l, and the previous hidden state, then outputs the next hidden state. The generation loss is then the sum of the negative log likelihoods of the correct words: Lgen(S) = \u2212 \u2211T t=1 \u2211Lt l=1 log pt,l(st,l).\nTo further exploit the notion of temporal coherence in a story, we add an order-preserving con-\nstraint to order the sequence of sentences within a story (related to the story-sorting idea in Agrawal et al. (2016)). For each story S we randomly shuffle its 5 sentences to generate negative story instances S\u2032. We then apply a max-margin ranking loss to encourage correctly-ordered stories: Lrank(S, S\n\u2032) = max(0,m\u2212log p(S\u2032)+log p(S)). The final loss is then a combination of the generation and ranking losses:\nL = Lgen(S) + \u03bbLrank(S, S \u2032). (2)"}, {"heading": "4 Experiments", "text": "We use the Visual Storytelling Dataset (Huang et al., 2016), consisting of 10,000 albums with 200,000 photos. Each album contains 10-50 photos taken within a 48-hour span with two annotations: 1) 2 album summarizations, each with 5 selected representative photos, and 2) 5 stories describing the selected photos."}, {"heading": "4.1 Story Generation", "text": "This task is to generate a 5-sentence story describing an album. We compare our model with two sequence-to-sequence baselines: 1) an encoderdecoder model (enc-dec), where the sequence of album photos is encoded and the last hidden state is fed into the decoder for story generation, 2) an encoder-attention-decoder model (Xu et al., 2015) (enc-attn-dec) with weights computed using a soft-attention mechanism. At each decoding time step, a weighted sum of hidden states from the encoder is decoded. For fair comparison, we\nuse the same album representation (Sec. 3.1) for the baselines.\nWe test two variants of our model trained with and without ranking regularization by controlling \u03bb in our loss function, denoted as h-attn (without ranking), and h-attn-rank (with ranking). Evaluations of each model are shown in Table 1. The h-attn outperforms both baselines, and h-attnrank achieves the best performance for all metrics. Note, we use beam-search with beam size=3 during generation for a reasonable performancespeed trade-off (we observe similar improvement trends with beam size = 1).2 To test performance under optimal image selection, we use one of the two ground-truth human-selected 5-photo-sets as an oracle to hard-code the photo selection, denoted as h-(gd)attn-rank. This achieves only a slightly higher Meteor compared to our end-to-end model.\nAdditionally, we also run human evaluations in a forced-choice task where people choose between stories generated by different methods. For this evaluation, we select 400 albums, each evaluated by 3 Turkers. Results are shown in Table 2. Experiments find significant preference for our model over both baselines. As a simple Turing test, we also compare our results with human written stories (last row of Table 2), indicating room for improvement of methods."}, {"heading": "4.2 Album Summarization", "text": "We evaluate the precision and recall of our generated summaries (output by the photo selector) compared to human selections (the combined set\n2We also compute the p-value of Meteor on 100K samples via the bootstrap test (Efron and Tibshirani, 1994), as Meteor has better agreement with human judgments than Bleu/Rouge (Huang et al., 2016). Our h-attn-rank model has strong statistical significance (p = 0.01) over the enc-dec and enc-attn-dec models (and is similar to the h-attn model).\nof both human-selected 5-photo stories). For comparison, we evaluate enc-attn-dec on the same task by aggregating predicted attention and selecting the 5 photos with highest accumulated attention. Additionally, we also run DPP-based video summarization (Kulesza et al., 2012) using the same album features. Our models have higher performance compared to baselines as shown in Table 3 (though DPP also achieves strong results, indicating that there is still room to improve the pointer network)."}, {"heading": "4.3 Output Example Analysis", "text": "Fig. 2 and Fig. 3 shows several output examples of the joint album summarization and storytelling generation. We compare our full model h-attnrank with the baseline enc-attn-dec, as both models are able to do the album summarization and story generation tasks jointly. In Fig. 2 and Fig. 3, we use blue dashed box and red box to indicate the album summarization by the two models respectively. As reference, we also show the groundtruth album summaries by randomly selecting 1 out of 2 human album summaries, which are highlighted with green box. Below each album are their generated stories."}, {"heading": "4.4 Album Retrieval", "text": "Given a human-written story, we introduce a task to retrieve the album described by that story. We randomly select 1000 albums and one groundtruth story from each for evaluation. Using the generation loss, we compute the likelihood of each album Am given the query story S and retrieve the album with the highest generation likelihood, A = argmaxAmp(S|Am). We use Recall@k and Median Rank for evaluation. As shown in Table 4), we find that our models outperform the baselines, but the ranking term in Eqn.2 does not improve performance significantly."}, {"heading": "5 Conclusion", "text": "Our proposed hierarchically-attentive RNN based models for end-to-end visual storytelling can\njointly summarize and generate relevant stories from full input photo albums effectively. Automatic and human evaluations show that our\nmethod outperforms strong sequence-to-sequence baselines on selection, generation, and retrieval tasks."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments. This research is supported by NSF Awards #1633295, 1444234, 1445409, 1562098."}], "references": [{"title": "Sort story: Sorting jumbled images and captions into stories", "author": ["Harsh Agrawal", "Arjun Chandrasekaran", "Dhruv Batra", "Devi Parikh", "Mohit Bansal."], "venue": "EMNLP.", "citeRegEx": "Agrawal et al\\.,? 2016", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata."], "venue": "ACL.", "citeRegEx": "Cheng and Lapata.,? 2016", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Textually customized video summaries", "author": ["Jinsoo Choi", "Tae-Hyun Oh", "In So Kweon."], "venue": "arXiv preprint arXiv:1702.01528.", "citeRegEx": "Choi et al\\.,? 2017", "shortCiteRegEx": "Choi et al\\.", "year": 2017}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "CVPR.", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Diverse sequential subset selection for supervised video summarization", "author": ["Boqing Gong", "Wei-Lun Chao", "Kristen Grauman", "Fei Sha."], "venue": "NIPS.", "citeRegEx": "Gong et al\\.,? 2014", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Video summarization by learning submodular mixtures of objectives", "author": ["Michael Gygli", "Helmut Grabner", "Luc Van Gool."], "venue": "CVPR.", "citeRegEx": "Gygli et al\\.,? 2015", "shortCiteRegEx": "Gygli et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "CVPR.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Visual storytelling. In NACCL", "author": ["Ting-Hao Kenneth Huang", "Francis Ferraro", "Nasrin Mostafazadeh", "Ishan Misra", "Aishwarya Agrawal", "Jacob Devlin", "Ross Girshick", "Xiaodong He", "Pushmeet Kohli", "Dhruv Batra"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Large-scale video summarization using web-image priors", "author": ["Aditya Khosla", "Raffay Hamid", "Chih-Jen Lin", "Neel Sundaresan."], "venue": "CVPR.", "citeRegEx": "Khosla et al\\.,? 2013", "shortCiteRegEx": "Khosla et al\\.", "year": 2013}, {"title": "Joint photo stream and blog post summarization and exploration", "author": ["Gunhee Kim", "Seungwhan Moon", "Leonid Sigal."], "venue": "CVPR.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Reconstructing storyline graphs for image recommendation from web community photos", "author": ["Gunhee Kim", "Eric P Xing."], "venue": "CVPR.", "citeRegEx": "Kim and Xing.,? 2014", "shortCiteRegEx": "Kim and Xing.", "year": 2014}, {"title": "Determinantal point processes for machine learning. Foundations and Trends R", "author": ["Alex Kulesza", "Ben Taskar"], "venue": null, "citeRegEx": "Kulesza and Taskar,? \\Q2012\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2012}, {"title": "Story-driven summarization for egocentric video", "author": ["Zheng Lu", "Kristen Grauman."], "venue": "CVPR.", "citeRegEx": "Lu and Grauman.,? 2013", "shortCiteRegEx": "Lu and Grauman.", "year": 2013}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "NAACL.", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui."], "venue": "CVPR.", "citeRegEx": "Pan et al\\.,? 2016", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Expressing an image stream with a sequence of natural sentences", "author": ["Cesc C Park", "Gunhee Kim."], "venue": "NIPS.", "citeRegEx": "Park and Kim.,? 2015", "shortCiteRegEx": "Park and Kim.", "year": 2015}, {"title": "Movie description", "author": ["Anna Rohrbach", "Atousa Torabi", "Marcus Rohrbach", "Niket Tandon", "Christopher Pal", "Hugo Larochelle", "Aaron Courville", "Bernt Schiele."], "venue": "IJCV.", "citeRegEx": "Rohrbach et al\\.,? 2016", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "EMNLP.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Learning visual storylines with skipping recurrent neural networks", "author": ["Gunnar A Sigurdsson", "Xinlei Chen", "Abhinav Gupta."], "venue": "ECCV.", "citeRegEx": "Sigurdsson et al\\.,? 2016", "shortCiteRegEx": "Sigurdsson et al\\.", "year": 2016}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko."], "venue": "ICCV.", "citeRegEx": "Venugopalan et al\\.,? 2015", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "CVPR.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Automatic generation of story highlights", "author": ["Kristian Woodsend", "Mirella Lapata."], "venue": "ACL.", "citeRegEx": "Woodsend and Lapata.,? 2010", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "ICCV.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu."], "venue": "CVPR.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Summary transfer: Exemplar-based subset selection for video summarization", "author": ["Ke Zhang", "Wei-Lun Chao", "Fei Sha", "Kristen Grauman."], "venue": "CVPR.", "citeRegEx": "Zhang et al\\.,? 2016a", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Video summarization with long shortterm memory", "author": ["Ke Zhang", "Wei-Lun Chao", "Fei Sha", "Kristen Grauman."], "venue": "ECCV.", "citeRegEx": "Zhang et al\\.,? 2016b", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).", "startOffset": 127, "endOffset": 210}, {"referenceID": 15, "context": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).", "startOffset": 127, "endOffset": 210}, {"referenceID": 13, "context": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).", "startOffset": 127, "endOffset": 210}, {"referenceID": 8, "context": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016).", "startOffset": 127, "endOffset": 210}, {"referenceID": 19, "context": ", (Sigurdsson et al., 2016) learns the latent temporal dynamics given a large amount of albums, and (Kim and Xing, 2014) formulate the photo selection as a sparse time-varying directed graph.", "startOffset": 2, "endOffset": 27}, {"referenceID": 11, "context": ", 2016) learns the latent temporal dynamics given a large amount of albums, and (Kim and Xing, 2014) formulate the photo selection as a sparse time-varying directed graph.", "startOffset": 80, "endOffset": 100}, {"referenceID": 16, "context": "To drive this work (Park and Kim, 2015) collected a dataset mined from Blog Posts.", "startOffset": 19, "endOffset": 39}, {"referenceID": 8, "context": "A more direct dataset was recently released (Huang et al., 2016), where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.", "startOffset": 44, "endOffset": 64}, {"referenceID": 8, "context": "In this paper, we make use of the Visual Storytelling Dataset (Huang et al., 2016).", "startOffset": 62, "endOffset": 82}, {"referenceID": 22, "context": "Visual Captioning: Most recent approaches to image captioning (Vinyals et al., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions.", "startOffset": 62, "endOffset": 102}, {"referenceID": 24, "context": "Visual Captioning: Most recent approaches to image captioning (Vinyals et al., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions.", "startOffset": 62, "endOffset": 102}, {"referenceID": 20, "context": "For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description.", "startOffset": 38, "endOffset": 82}, {"referenceID": 15, "context": "For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description.", "startOffset": 38, "endOffset": 82}, {"referenceID": 24, "context": "Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information.", "startOffset": 21, "endOffset": 73}, {"referenceID": 26, "context": "Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information.", "startOffset": 21, "endOffset": 73}, {"referenceID": 25, "context": "Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information.", "startOffset": 21, "endOffset": 73}, {"referenceID": 18, "context": "Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots.", "startOffset": 60, "endOffset": 148}, {"referenceID": 1, "context": "Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots.", "startOffset": 60, "endOffset": 148}, {"referenceID": 14, "context": "Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots.", "startOffset": 60, "endOffset": 148}, {"referenceID": 23, "context": "Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots.", "startOffset": 60, "endOffset": 148}, {"referenceID": 13, "context": "While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al.", "startOffset": 48, "endOffset": 91}, {"referenceID": 9, "context": "While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al.", "startOffset": 48, "endOffset": 91}, {"referenceID": 6, "context": ", 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014).", "startOffset": 103, "endOffset": 165}, {"referenceID": 5, "context": ", 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014).", "startOffset": 103, "endOffset": 165}, {"referenceID": 2, "context": "Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries.", "startOffset": 39, "endOffset": 58}, {"referenceID": 11, "context": "Previous works include storyline graph modeling (Kim and Xing, 2014), unsupervised mining (Sigurdsson et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 19, "context": "Previous works include storyline graph modeling (Kim and Xing, 2014), unsupervised mining (Sigurdsson et al., 2016), blog-photo alignment (Kim et al.", "startOffset": 90, "endOffset": 115}, {"referenceID": 10, "context": ", 2016), blog-photo alignment (Kim et al., 2015), and language retelling (Huang et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 8, "context": ", 2015), and language retelling (Huang et al., 2016; Park and Kim, 2015).", "startOffset": 32, "endOffset": 72}, {"referenceID": 16, "context": ", 2015), and language retelling (Huang et al., 2016; Park and Kim, 2015).", "startOffset": 32, "endOffset": 72}, {"referenceID": 16, "context": "While (Park and Kim, 2015) collects data by mining Blog Posts, (Huang et al.", "startOffset": 6, "endOffset": 26}, {"referenceID": 8, "context": "While (Park and Kim, 2015) collects data by mining Blog Posts, (Huang et al., 2016) collects stories using Mechanical Turk, providing more directly relevant stories.", "startOffset": 63, "endOffset": 83}, {"referenceID": 7, "context": "We first extract the 2048-dimensional visual representation fi \u2208 Rk for each photo using ResNet101 (He et al., 2016), then a bi-directional RNN is applied to encode the full album.", "startOffset": 99, "endOffset": 116}, {"referenceID": 8, "context": "Following (Huang et al., 2016), we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence.", "startOffset": 10, "endOffset": 30}, {"referenceID": 21, "context": "Inspired by Pointer Networks (Vinyals et al., 2015a), we use another GRU-RNN to perform this task 1.", "startOffset": 29, "endOffset": 52}, {"referenceID": 3, "context": "Following Donahue et al. (2015), the l-th word probability of the t-th sentence is:", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "To further exploit the notion of temporal coherence in a story, we add an order-preserving constraint to order the sequence of sentences within a story (related to the story-sorting idea in Agrawal et al. (2016)).", "startOffset": 190, "endOffset": 212}, {"referenceID": 8, "context": "We use the Visual Storytelling Dataset (Huang et al., 2016), consisting of 10,000 albums with 200,000 photos.", "startOffset": 39, "endOffset": 59}, {"referenceID": 24, "context": "We compare our model with two sequence-to-sequence baselines: 1) an encoderdecoder model (enc-dec), where the sequence of album photos is encoded and the last hidden state is fed into the decoder for story generation, 2) an encoder-attention-decoder model (Xu et al., 2015) (enc-attn-dec) with weights computed using a soft-attention mechanism.", "startOffset": 256, "endOffset": 273}, {"referenceID": 4, "context": "We also compute the p-value of Meteor on 100K samples via the bootstrap test (Efron and Tibshirani, 1994), as Meteor has better agreement with human judgments than Bleu/Rouge (Huang et al.", "startOffset": 77, "endOffset": 105}, {"referenceID": 8, "context": "We also compute the p-value of Meteor on 100K samples via the bootstrap test (Efron and Tibshirani, 1994), as Meteor has better agreement with human judgments than Bleu/Rouge (Huang et al., 2016).", "startOffset": 175, "endOffset": 195}], "year": 2017, "abstractText": "We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.", "creator": "LaTeX with hyperref package"}}}