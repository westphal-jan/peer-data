{"id": "1708.04236", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Motion Planning under Partial Observability using Game-Based Abstraction", "abstract": "we study motion planning problems where agents move inside environments that are not fully observable and subject to uncertainties. the goal is to compute a systematic strategy for an agent that is guaranteed to satisfy certain high safety and performance specifications. such problems are naturally naturally modelled by partially observable markov decision processes ( pomdps ). because of the potentially huge or even infinite belief space of pomdps, verification and strategy synthesis is in general computationally intractable. we tackle this difficulty by exploiting exactly typical structural properties of such scenarios ; for instance, we assume that agents have the ability to illegally observe inside their optimal own positions inside an environment. ambiguity in the state of the environment matrix is abstracted into non - deterministic choices over the possible states of the environment. technically, this abstraction transforms pomdps into probabilistic two - player games ( pgs ). for these pgs, efficient verification tools are able to determine strategies that approximate certain measures on the pomdp. if an approximation is too coarse to successfully provide guarantees, an abstraction refinement scheme further resolves the belief space of the pomdp. we demonstrate that our method improves the state of the art by orders of magnitude independently compared to a direct solution of the pomdp.", "histories": [["v1", "Mon, 14 Aug 2017 15:49:21 GMT  (102kb)", "http://arxiv.org/abs/1708.04236v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["leonore winterer", "sebastian junges", "ralf wimmer", "nils jansen", "ufuk topcu", "joost-pieter katoen", "bernd becker"], "accepted": false, "id": "1708.04236"}, "pdf": {"name": "1708.04236.pdf", "metadata": {"source": "CRF", "title": "Motion Planning under Partial Observability using Game-Based Abstraction", "authors": ["Leonore Winterer", "Sebastian Junges", "Ralf Wimmer", "Nils Jansen", "Ufuk Topcu", "Bernd Becker"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n04 23\n6v 1\n[ cs\n.R O\n] 1\n4 A\nug 2\n01 7\nI. INTRODUCTION\nOffline motion planning for dynamical systems with uncertainties aims at finding a strategy for an agent that ensures certain desired behavior [1]. Planning scenarios that exhibit stochastic behavior are naturally modeled by Markov decision processes (MDPs). An MDP is a non-deterministic model in which the agent chooses to perform an action under full knowledge of the environment it is operating in. The outcome of the action is a distribution over the states. For many robotic applications, however, information about the current state of the environment is not observable [2], [3], [4]. In such scenarios, where the actual state of the environment is not exactly known, the model is a partially observable Markov decision process (POMDP). By tracking the observations made while it moves, an agent can infer the likelihood of the environment being in a certain state. This likelihood is called the belief state of the agent. Executing an action leads to an update of the belief state because new observations are made. The belief state together with an update function form a (possibly infinite) MDP, commonly referred to as the underlying belief MDP [5].\nThis work was partly supported by the CDZ project CAP (GZ 1023) and by the German Research Foundation (DFG) as part of the Cluster of Excellence BrainLinks/BrainTools (EXC1086) and by the awards ONR # N000141612051, NASA # NNX17AD04G and DARPA # W911NF-16-10001.\n1Albert-Ludwigs-Universita\u0308t Freiburg, Freiburg im Breisgau, Germany 2RWTH Aachen University, Aachen, Germany 3The University of Texas at Austin, Austin, TX, USA 4Radboud University, Nijmegen, The Netherlands\nAs an example, take a scenario where a controllable agent needs to traverse a room while avoiding static obstacles and randomly moving opponents whose positions cannot always be observed by the agent. The goal is to determine a strategy for the agent, that (provably) ensures safe traversal with a certain high probability.\nQuantitative verification techniques like probabilistic model checking [6] provide comprehensive guarantees on such a strategy. For finite MDPs, tools like PRISM [7] or storm [8] employ efficient model checking algorithms to asses the probability to reach a certain set of states. However, POMDP verification suffers from the large, potentially infinite belief space, and is intractable even for rather small instances.\nApproach: We outline the approach and the structure of the paper in Fig. 1, details in the figure will be discussed in the respective sections. Starting from a problem description, we propose to use an encoding of the problem as a POMDP. We observe that motion planning scenarios as described above naturally induce certain structural properties in the POMDP. In particular, we assume that the agent can observe its own position while the exact position of the opponents is observable only if they are nearby according to a given distance metric. We propose an abstraction method that, intuitively, lumps states inducing the same observations. Since it is not exactly known in which state of the environment a certain action is executed, a non-deterministic choice over these lumped states is introduced. Resolving this choice induces a new level of non-determinism into the system in addition to the choices of the agent: The POMDP abstraction results in a probabilistic two-player game (PG) [10]. The agent is the first player choosing an action while the second player chooses in which of the possible (concrete) states the action is executed. Model checking computes an optimal strategy for the agent on this PG.\nThe automated abstraction procedure is inspired by gamebased abstraction [10], [11] of potentially infinite MDPs, where states are lumped in a similar fashion. We show that our approach is sound in the sense that a strategy for the agent in the PG defines a strategy for the original POMDP. Guarantees for the strategy carry over to POMDPs, as it induces bounds on probabilities. As we target an undecidable problem [12], our approach is not complete in the sense that it does not always obtain a strategy which yields the required optimal probability. However, we define a scheme to refine the abstraction and extend the observability.\nWe implemented a Python tool-chain taking a graph formulation of the motion planning as input and applying the\nproposed abstraction-refinement procedure. The tool-chain uses PRISM-games [9] as a model checker for PGs. For the motion planning scenario considered, our preliminary results indicate an improvement of orders in magnitude over the state of the art in POMDP verification [13].\nRelated work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17]. An overview on point-based value iteration for POMDPs is given in [5]. Other methods employ control techniques to synthesize strategies with safety considerations under observation and dynamics noise [2], [18], [19]. Preprocessing of POMDPs in motion planning problems for robotics is suggested in [20].\nGeneral verification problems for POMDPs and their decidability have been studied in [21], [22]. A recent survey about decidability results and algorithms for \u03c9-regular properties is given in [12], [23]. The probabilistic model checker PRISM has recently been extended to support POMDPs [13]. Partly based on the methods from [24], it produces lower and upper bounds for a variety of queries. Reachability can be analyzed for POMDPs for up to 30,000 states. In [25], an iterative refinement is proposed to solve POMDPs: Starting with total information, strategies that depend on unobservable information are excluded. In [26], a compositional framework for reasoning about POMDPs is introduced. Refinement based on counterexamples is considered in [27]. Partially observable probabilistic games have been considered in [28]. Finally, an overview of applications for PGs is given in [29]."}, {"heading": "II. FORMAL FOUNDATIONS", "text": ""}, {"heading": "A. Probabilistic games", "text": "For a finite or countably infinite set X , let \u00b5 : X \u2192 [0, 1] such that \u2211\nx\u2208X \u00b5(x) = 1 denote a probability distribution over X and Dist(X) the set of all probability distributions over X . The Dirac distribution \u03b4x \u2208 Dist(X) on x \u2208 X is given by \u03b4x(x) = 1 and \u03b4x(y) = 0 for y 6= x.\nDefinition 1 (Probabilistic game) A probabilistic game (PG) is a tuple G = (S1, S2, sinit,Act , P ) where S = S1 \u222a\u0307S2 is a finite set of states, S1 the states of Player 1, S2 the states of Player 2, sinit \u2208 S the initial state, Act a finite set of actions, and P : S \u00d7 Act 7\u2192 Dist(S) a (partial) probabilistic transition function.\nLet Act(s) = {\u03b1 \u2208 Act | (s, \u03b1) \u2208 dom(P )} denote the available actions in s \u2208 S. We assume that PG G does not contain any deadlock states, i. e., Act(s) 6= \u2205 for all s \u2208 S. A Markov decision process (MDP) M is a PG with S2 = \u2205. We write M = (S, sinit,Act , P ). A discrete-time Markov chain (MC) is an MDP with |Act(s)| = 1 for all s \u2208 S.\nThe game is played as follows: In each step, the game is in a unique state s \u2208 S. If it is a Player 1 state (i. e., s \u2208 S1), then Player 1 chooses an available action \u03b1 \u2208 Act(s) nondeterministically; otherwise Player 2 chooses. The successor state of s is determined probabilistically according to the probability distribution P (s, \u03b1): The probability of s\u2032 being the next state is P (s, \u03b1)(s\u2032). The game is then in state s\u2032. A path through G is a finite or infinite sequence \u03c0 = s0 \u03b10\u2212\u2192 s1\n\u03b11\u2212\u2192 \u00b7 \u00b7 \u00b7 , where s0 = sinit, si \u2208 S, \u03b1i \u2208 Act(si), and P (si, \u03b1i)(si+1) > 0 for all i \u2208 N. The (i+1)-th state si on \u03c0 is \u03c0(i), and last(\u03c0) denotes the last state of \u03c0 if \u03c0 is finite. The set of (in)finite paths is PathsfinG (Paths inf G ).\nTo define a probability measure over the paths of a PG G,\nthe non-determinism needs to be resolved by strategies.\nDefinition 2 (PG strategy) A strategy \u03c3 for G is a pair \u03c3 = (\u03c31, \u03c32) of functions \u03c3i : {\u03c0 \u2208 Paths fin G\n| last(\u03c0) \u2208 Si} \u2192 Dist(Act) such that for all \u03c0 \u2208 PathsfinG , {\u03b1 | \u03c3i(\u03c0)(\u03b1) > 0} \u2286 Act ( last(\u03c0) )\n. \u03a3G denotes the set of all strategies of G and \u03a3i\nG all Player-i strategies of G.\nFor MDPs, the strategy consists of a Player-1 strategy only. A Player-i strategy \u03c3i is memoryless if last(\u03c0) = last(\u03c0\n\u2032) implies \u03c3i(\u03c0) = \u03c3i(\u03c0\n\u2032) for all \u03c0, \u03c0\u2032 \u2208 dom(\u03c3i). It is deterministic if \u03c3i(\u03c0) is a Dirac distribution for all \u03c0 \u2208 dom(\u03c3i). A memoryless deterministic strategy is of the form \u03c3i : Si \u2192 Act . A strategy \u03c3 for a PG resolves all non-deterministic choices, yielding an induced MC, for which a probability measure over the set of infinite paths is defined by the standard cylinder set construction [30]. These notions are analogous for MDPs."}, {"heading": "B. Partial observability", "text": "For many applications, not all system states are observable [2]. For instance, an agent may only have an estimate on the current state of its environment. In that case, the underlying model is a partially observable Markov decision process.\nDefinition 3 (POMDP) A partially observable Markov decision process (POMDP) is a tuple D = (M,O, \u03bb) such that M = (S, sinit,Act , P ) is the underlying MDP of D, O a finite set of observations, and \u03bb : S \u2192 O the observation function.\nW. l. o. g. we require that states with the same observations have the same set of enabled actions, i. e., \u03bb(s) = \u03bb(s\u2032) implies Act(s) = Act(s\u2032) for all s, s\u2032 \u2208 S. More general observation functions \u03bb have been considered in the literature, taking into account the last action and providing a\ndistribution over O. There is a polynomial transformation of the general case to the POMDP definition used here [23].\nThe notions of paths and probability measures directly transfer from MDPs to POMDPs. We lift the observation function to paths: For a POMDP D and a path \u03c0 = s0 \u03b10\u2212\u2192 s1 \u03b11\u2212\u2192 \u00b7 \u00b7 \u00b7 sn \u2208 Paths fin D , the associated observation sequence is \u03bb(\u03c0) = \u03bb(s0) \u03b10\u2212\u2192 \u03bb(s1)\n\u03b11\u2212\u2192 \u00b7 \u00b7 \u00b7\u03bb(sn). Note that several paths in the underlying MDP M can give rise to the same observation sequence. Strategies have to take this restricted observability into account.\nDefinition 4 (Observation-based strategy) An observation-based strategy of POMDP D is a function \u03c3 : Pathsfin D\n\u2192 Dist(Act) such that \u03c3 is a strategy for the underlying MDP and for all paths \u03c0, \u03c0\u2032 \u2208 Pathsfin\nD with\n\u03bb(\u03c0) = \u03bb(\u03c0\u2032) we have \u03c3(\u03c0) = \u03c3(\u03c0\u2032). \u03a3o D denotes the set of such strategies.\nThat means an observation-based strategy selects based on the observations and actions made along the current path.\nThe semantics of a POMDP can be described using a belief MDP with an uncountable state space. The idea is that each state of the belief MDP corresponds to a distribution over the states in the POMDP. This distribution is expected to correspond to the probability to be in a specific state based on the observations made so far. Initially, the belief is a Dirac distribution on the initial state. A formal treatment of belief MDPs is beyond the scope of this paper, for details see [5]."}, {"heading": "C. Specifications", "text": "Given a set G \u2286 S of goal states and a set B \u2286 S of bad states, we consider quantitative reach-avoid properties of the form \u03d5 = P>p(\u00acB U G). The specification \u03d5 is satisfied by a PG if Player 1 has a strategy such that for all strategies of Player 2 the probability is at least p to reach a goal state without entering a bad state in between. For POMDPs, \u03d5 is satisfied if the agent has an observation-based strategy which leads to a probability of at least p.\nFor MDPs and PGs, memoryless deterministic strategies suffice to prove or disprove satisfaction of such specifications [31]. For POMDPs, observation-based strategies in their full generality are necessary [32]."}, {"heading": "III. METHODOLOGY", "text": "We first intuitively describe the problem and list the assumptions we make. After formalizing the setting, we present a formal problem statement. We present the intuition behind the concept of game-based abstraction for MDPs, how to apply it to POMDPs, and state the correctness of our method."}, {"heading": "A. Problem Statement", "text": "We consider a motion planning problem that involves n+1 moving agents inside a world such as a landscape or a room. One agent is controllable (Agent 0), the other agents (also called opponents) move stochastically according to a fixed randomized strategy which is based on their own location and the location of Agent 0. We assume that all agents move in an alternating manner. A position of an agent defines\nthe physical location inside the world as well as additional properties such as the agent\u2019s orientation. A graph models all possible movements of an agent between positions, referred to as the world graph of an agent. Therefore, nodes in the graph uniquely refer to positions while multiple nodes may refer to the same physical location in the world. We require that the graph does not contain any deadlocks: For every position, there is at least one edge in the graph corresponding to a possible action an agent can execute.\nA collision occurs, if Agent 0 shares its location with another agent. The set of goal nodes (goal positions) in the graph is uniquely defined by a set of physical goal locations in the world. The target is to move Agent 0 towards a goal node without colliding with other agents. Technically, we need to synthesize a strategy for Agent 0 that maximizes the probability to achieve the target. Additionally, we assume:\n\u2022 The strategies of all opponents are known to Agent 0. \u2022 Agent 0 is able to observe its own position and knows the goal positions it has to reach. \u2022 The positions of opponents are observable for Agent 0 from its current position, if they are visible with respect\nto a certain distance metric.\nGeneralizing the problem statement is discussed in Sect. VI."}, {"heading": "B. Formal setting", "text": "We first define an individual world graph for each Agent i with 0 \u2264 i \u2264 n over a fixed set Loc of (physical) locations.\nDefinition 5 (World graph of Agent i) The world graph Gi for Agent i over Loc is a tuple Gi = (Vi, v 0 i ,Mov i, Ei, \u2113i) such that Vi is the set of positions and v 0 i \u2208 Vi is the initial position of Agent i. Mov i is the set of movements 1; the edges Ei : Vi\u00d7Mov i 7\u2192 Vi are the movement effects. The function \u2113i : Vi \u2192 Loc maps a position to the corresponding location.\nThe enabled movements for Agent i in position v are Mov i(v) = { m \u2208 Mov i | (v,m) \u2208 dom(Ei) } .\nFor Agent 0 we need the possibility to restrict its viewing range. This is done by a function \u03bd0 : V0 \u2192 2Loc which assigns to each position of Agent 0 the set of visible locations. According to our assumptions, for all v \u2208 V0 it holds that \u2113(v) \u2208 \u03bd0(v) and Mov i(v) 6= \u2205. Each Agent i with i > 0 has a randomized strategy \u03c3i : V0\u00d7Vi \u2192 Dist(Mov i), which maps positions of Agent 0 and Agent i to a distribution over enabled movements of Agent i. The world graphs for all agents with randomized strategies for the opponents are subsumed by a single world POMDP. We first define the underlying world MDP modeling the possible behavior of all agents based on their associated world graphs.\nDefinition 6 (World MDP) Given world graphs G0, . . . , Gn, the induced world MDP M = (S, sinit,Act , P ) is defined by S = V0 \u00d7 V1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Vn \u00d7 {0, . . . n},\n1We use movements to avoid confusion with actions in PGs.\nsinit = (v 0 0 , v 0 1 , . . . , v 0 n, 0), and Act = Mov0 \u222a\u0307 {\u22a5}. P is defined by:\n\u2022 For \u03b1 \u2208 Mov0(v0) and v\u0306 \u2208 V1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Vn, we have P ( (v0, v\u0306, 0), \u03b1 )\n= \u03b4(E0(v0,\u03b1),v\u0306,1). \u2022 P ( (v0, v\u0302, vi, v\u0306), i),\u22a5 )( (v0, v\u0302, v \u2032 i, v\u0306, i + 1\nmod n + 1) )\n= q, with v\u0302 \u2208 V1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Vi\u22121, v\u0306 \u2208 Vi+1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Vn, 1 \u2264 i \u2264 n and q = \u2211\n{m\u2208Movi |Ei(vi,m)=v\u2032i} \u03c3i(v0, vi)(m).\n\u2022 0 in all other cases.\nThe first item in the definition of P translates each movement in the world graph of Agent 0 into an action in the MDP that connects states with probability one, i. e., a Dirac distribution is attached to each action. Upon taking the action, the position of Agent 0 changes and Agent 1 has to move next. The second item defines movements of the opponents. In each state where Agent i is moving next, the action \u22a5 reflecting this move is added. The outcome of \u22a5 is determined by \u03c3i and the fact that Agent i+1 moves next.\nDefinition 7 (World POMDP) Let M be a world MDP for world graphs G0, . . . , Gn. The world POMDP D = (M,O, \u03bb) with O = V0 \u00d7\u00d71\u2264i\u2264n(Vi \u222a\u0307 {\u2213}) and \u03bb defined by:\n\u03bb((v0, . . . , vn))i =\n{\nvi, if \u2113(vi) \u2208 \u03bd0(v0),\n\u2213, otherwise.\nThus, the position of Agent i is observed iff the location of Agent i is visible from the position of Agent 0, and otherwise a dummy value \u2213, which is referred to as far away, is observed.\nGiven a set GoalLocations \u2286 Loc, the mappings \u2113i : Vi \u2192 Loc are used to define the states corresponding to collisions and goal locations. In particular, we have Collision = {\n((v0, . . . , vn), j) \u2208 S \u2223 \u2223\u22031 \u2264 i \u2264 n. \u21130(v0) = \u2113i(vi) } and\nGoals = { ((v0, . . . , vn), j) \u2223 \u2223 \u21130(v0) \u2208 GoalLocations } .\nFormal problem statement: Given a world POMDP D for a set of world graphs G0, . . . , Gn, a set of collision states Collision, and a set of goal states Goals, an observationbased strategy \u03c3 \u2208 \u03a3o D\nfor D is p-safe for p \u2208 [0, 1], if P \u03c3 >p(\u00acCollision U Goals) holds. We want to compute a psafe strategy for a given p \u2208 [0, 1]."}, {"heading": "C. Abstraction", "text": "We propose an abstraction method for world POMDPs that builds on game-based abstraction (GBAR), originally defined for MDPs [10], [11]. GBAR for MDPs: For an MDP M = (S, sinit,Act , P ), we assume a partition \u03a0 = {B1, . . . , Bk} of S, i. e., a set of non-empty, pairwise disjoint, subsets (called blocks) Bi \u2286 S with \u22c3k\ni=1 Bi = S. GBAR takes the partition \u03a0 and turns each block into an abstract state Bi; these blocks form the Player 1 states. Then Act(Bi) = \u22c3\ns\u2208Bi Act(s). To\ndetermine the outcome of selecting \u03b1 \u2208 Act(Bi), we add intermediate selector-states \u3008Bi, \u03b1\u3009 as Player 2 states. In the selector state \u3008Bi, \u03b1\u3009, emanating actions reflect the choice of the actual state the system is in at Bi, i. e., Act(\u3008Bi, \u03b1\u3009) =\nBi. For taking an action s \u2208 Bi in \u3008Bi, \u03b1\u3009, the distribution P (s, \u03b1) is lifted to a distribution over abstract states:\nP ( \u3008Bi, \u03b1\u3009, s )( Bj ) = \u2211\ns\u2032\u2208Bj\nP (s, \u03b1)(s\u2032).\nThe semantics of this PG is as follows: For an abstract state Bi, Player 1 (controllable) selects an action to execute. In selector-states, the Player 2 (adversary) selects the worstcase state from Bi where the selection was executed.\nApplying GBAR to POMDPs: The key idea in GBAR\nfor POMDPs is to merge states with equal observations.\nDefinition 8 (Abstract PG) The abstract PG of POMDP D = ( (S, sinit,Act , P ),O, \u03bb )\nis G = (S1, S2, s \u2032 init,Act \u2032, P \u2032, R\u2032) with S1 = {\n{s \u2208 S |\u03bb(s) = \u03bb(s\u2032)} \u2223 \u2223 s\u2032 \u2208 S } , S2 = { \u3008B,\u03b1\u3009 \u2223 \u2223B \u2208 S1 \u2227 \u03b1 \u2208 Act(B) } , s\u2032init = B s. t. sinit \u2208 B, and Act \u2032 = S \u222a\u0307 Act .\nThe transition probabilities P \u2032 are defined as follows:\n\u2022 P \u2032(B,\u03b1) = \u03b4\u3008B,\u03b1\u3009 for B \u2208 S1 and \u03b1 \u2208 Act(B), \u2022 P \u2032 ( \u3008B,\u03b1\u3009, s )( B\u2032 ) = \u2211\ns\u2032\u2208B\u2032 P (s, \u03b1)(s \u2032) for \u3008B,\u03b1\u3009 \u2208\nS2, s \u2208 B, and B \u2032 \u2208 S1, \u2022 and 0 in all other cases.\nBy construction, Player 1 has to select the same action for all states in an abstract state. As the abstract states coincide with the observations, this means that we obtain an observationbased strategy for the POMDP. For the classes of properties we consider, a memoryless deterministic strategy suffices for PGs to achieve the maximal probability of reaching a goal state without collision [31]. We thus obtain an optimal strategy \u03c3 : S1 \u2192 Act \u2032 for Player 1 in the PG which maps every abstract state to an action. As abstract states are constructed such that they coincide with all possible observations in the POMDP (see Def. 8), this means that \u03c3 maps every observation to an action.\nAbstract world PG: We now connect the abstraction to our setting. For ease of presentation, we assume in the rest of this section that there is only one opponent agent, i. e., we have Agent 0 and Agent 1. Therefore, if Agent 0 sees an agent and moves, no additional agent will appear. Moreover, Agent 0 either knows the exact state, or does not know where the opponent is.\nFirst, we call the abstract PG of the world POMDP the abstract world PG. In particular, the abstract states Bk in the world PG are either of the form Bk = (v0, v1, i) or of the form Bk = (v0,\u2213, i). with i \u2208 {0, 1}. In the former, the opponent is visible and the agent has full knowledge, in the latter only the own position is known. Recall that \u2213 is a dummy value for the distance referred to as far away. Furthermore, all states in an abstract state correspond to the same position of Agent 0. For abstract states with full knowledge, there is no non-determinism of Player 2 involved as these states correspond to a single state in the world POMDP.\nCorrectness: We show that a safe strategy for Player 1 induces a safe strategy for Agent 0. Consider therefore a path B0 \u03b10\u2212\u2192 \u3008B0, \u03b10\u3009 s\u2208B0\u2212\u2212\u2212\u2192 B1 \u03b11\u2212\u2192 . . . Bn in the PG. This path is projected to the blocks: B0 \u03b10\u2212\u2192 B1\n\u03b11\u2212\u2192 . . . Bn. The location of Agent 0 encoded in the blocks is independent of the choices by Player 2. The sequence of actions \u03b10\u03b11 . . . thus yields a unique path of positions of Agent 0 in its world graph. Thus, if the path in the PG reaches a goal state, the path induces a path in the POMDP which also reaches a goal state. Moreover, the worst-case behavior over-approximates the probability for the opponent to be in any location, and any collision is observable. Thus if there is a collision in the POMDP, then there is a collision in the PG.\nFormally, for a deterministic memoryless strategy \u03c3\u2032 in the abstract world PG the corresponding strategy \u03c3 in the POMDP is defined as \u03c3(s) = \u03c3\u2032(B) for s \u2208 B.\nTheorem 1 Given a p-safe strategy in an abstract world PG, the corresponding strategy in the world POMDP is p-safe.\nThe assessment of the strategy is conservative: A p-safe strategy in the abstract world PG may induce a corresponding strategy in the POMDP which is p+\u03c4 -safe for some \u03c4 > 0. In particular, applying the corresponding strategy to the original POMDP yields a discrete-time Markov chain (MC). This MC can be efficiently analyzed by, e. g., probabilistic model checking to determine the value of p+\u03c4 . Naturally, the optimal scheduler obtained for the PG does not need to be optimal in the POMDP.\nAll positions where Agent 1 is visible yield Dirac distributions in the belief MDP, i. e., the successor states in the MDP depend solely on the action choice. These beliefs are represented as single states in the abstract world PG. The abstraction lumps for each position of Agent 0 all (uncountably many) other belief states together."}, {"heading": "D. Refinement of the PG", "text": "In the GBAR approach described above, we remove relevant information for an optimal strategy. In particular, the behavior of Agent 1 (the opponent) is strengthened (overapproximated):\n\u2022 We abstract probabilistic movements of Agent 1 outside of the visible area into non-determinism. \u2022 We allow jumps in Agent 1\u2019s movements, i.e., Agent 1 may change position in the PG. This is impossible in\nthe POMDP; these movements are called spurious.\nIf, due to the lack of this information, no safe strategy can be found, the abstraction needs to be refined. In GBAR for MDPs [11], abstract states are split heuristically, yielding a finer over-approximation. In our construction, we cannot split abstract states arbitrarily: This would destroy the one-to-one correspondence between abstract states and observations. We would thus obtain a partially observable PG, or equivalently, for a strategy in the PG the corresponding strategy in the original POMDP is no longer observation-based.\nHowever, we can restrict the spurious movements of Agent 1 by taking the history of observations made along\na path into account. We present three types of history-based refinements.\na) One-step history refinement: If Agent 0 moves to state s from where Agent 1 is no longer visible, we have \u03bb(s) = \u2213. Upon the next move, Agent 1 could thus appear anywhere. However, until Agent 1 moves, the belief MDP is still in a Dirac distribution; the positions where Agent 1 can appear are thus restricted. Similarly, if Agent 1 disappears, upon a turn of Agent 0 in the same direction, Agent 1 will be visible again. The (one-step history) refined world PG extends the original PG by additional states (v0, v1, i) where v1 6\u2208 \u03bd0(v0), i. e., v1 is not visible for Agent 0. These \u201cfar away\u201d states are only reached from states with full information. Intuitively, although Agent 1 is invisible, its position is remembered for one step.\nb) Multi-step history refinement: Further refinement is possible by considering longer paths. If we first observe Agent 1 at location x, then loose visibility for one turn, and then observe Agent 1 again at position y, then we know that either x and y are at most two moves apart or that such a movement is spurious. To encode the observational history into the states of the abstraction, we store the last known position of Agent 1, as well as the number m of moves made since then. We then only allow Agent 1 to appear in positions which are at most m moves away from the last known position. We can cap m by the diameter of the graph. c) Region-based multi-step history refinement: As the refinement above blows up the state space drastically, we utilize a technique called magnifying lens abstraction [33]. Instead of single locations, we define regions of locations together with the information if Agent 1 could be present. After each move, we extend the possible regions by all neighbor regions.\nMore formally, the (multi-step history) refined world PG has a refined far-away value \u2213: Given a partition of the positions of Agent 1, e. g., extracted from the graph structure, into sets X = {X1, . . . , Xl} with \u22c3\nX\u2208X = V1 and Xi \u2229 Xj = \u2205 for all 1 \u2264 i < j \u2264 l. We define \u2213\u2032 : X \u2192 {0, 1}. Abstract states now are either of the form (v0, v1, i) as before, or (v0,\u2213\u2032, i). For singleton regions, this coincides with the method proposed above. Notice that this approach also offers some flexibility: If for instance two regions are connected only by the visible area, Agent 0 can assure wether Agent 1 enters the other region.\nCorrectness: First, a deterministic memoryless strategy \u03c3\u2032 on a refined abstract world PG needs to be translated to a strategy \u03c3 for the original POMDP while p-safety is conserved. Intuitively, as the proposed refinement steps encode history into the abstract world PG, the strategy \u03c3 is not memoryless anymore but has a finite memory at most m according to the maximum number of moves that are observed.\nTheorem 2 A p-safe strategy in a refined abstract world PG has a corresponding p-safe strategy in the world POMDP.\nThe proposed refinements eliminate spurious movements of Agent 1 from the original abstract world PG. Intuitively, the\nnumber of states where Player 2 may select states with belief zero (in the underlying belief MDP) is reduced. We thus only prevent paths that have probability zero in the POMDP. Vice versa, the refinement does not restrict the movement of Agent 0 and any path leading to a goal state still leads to one in the refinement. However, the behavior of Agent 1 is restricted, therefore, the probability of a collision drops. Intuitively, for the refined PG strategies can be computed that are at least as good as for the original PG.\nTheorem 3 If an abstract world PG has a p-safe strategy, then its refined abstract world PG has a p\u2032-safe strategy with p\u2032 \u2265 p."}, {"heading": "E. Refinement of the Graph", "text": "The proposed approach cannot solve every scenario \u2014 the problem is undecidable [12]. Therefore, if the method fails to find a p-safe scheduler, we do not know whether there exists such a scheduler. With increased visibility, however, the maximal level of safety does not decrease in both the POMDP and the PG. To determine good spots for increased visibility, we can use the analysis results: Locations in which a collision occurs are most likely good candidates."}, {"heading": "IV. CASE STUDY AND IMPLEMENTATION", "text": ""}, {"heading": "A. Description", "text": "For our experiments, we choose the following scenario: A (controllable) Robot R and a Vacuum Cleaner VC are moving around in a two-dimensional grid world with static opaque obstacles. Neither R nor VC may leave the grid or visit grid cells occupied by a static obstacle. The position of R contains the cell CR (the location) and a wind direction. R can move one grid cell forward, or turn by 90\u25e6 in either direction without changing its location. The position of VC is determined solely by its cell CVC. In each step, VC can move one cell in any wind direction. We assume that VC moves to all available successor cells with equal probability.\nThe sensors on R only sense VC within a viewing range r around CR. More precisely, VC is visible iff \u2016CR\u2212CVC\u2016\u221e \u2264 r and there is no grid cell with a static obstacle on the straight line from CR\u2019s center to CVC\u2019s center. That means, R can observe the position of the VC if VC is in the viewing range and VC is not hidden behind an obstacle. A refinement of the world is realized by adding additional cameras, which make cells visible independent of the location of R."}, {"heading": "B. Tool-Chain", "text": "To synthesize strategies for the scenario described above, we implemented a tool-chain in Python. The input consists of the grid with the locations of all obstacles, the location of cameras, and the viewing range. As output, two PRISM files are created: A PG formulation of the abstraction including one-step history refinement, to be analyzed using PRISM-games [9], and the original POMDP for PRISM-pomdp [13]. For multi-step history refinement, additional regions can be defined.\nThe encoding of the PG contains a precomputed lookup-\ntable for the visibility relation. The PG is described by two parallel processes running interleaved: One for Player 1 and one for Player 2. As only R can make choices, they are listed in Player 1 actions, while VC\u2019s moves are stored in Player 2 actions. More precisely, the process for R contains its location, and the process for VC either contains its location or a far-away value. Then, Player 1 makes its decision, afterwards the outcome of the move and the outcomes of the subsequent move of VC are compressed into one step of Player 2."}, {"heading": "V. EXPERIMENTS", "text": ""}, {"heading": "A. Experimental Setup", "text": "All experiments were run on a machine with a 3.6 GHz Intel R\u00a9 CoreTM i7-4790 CPU and 16 GB RAM, running Ubuntu Linux 16.04. We denote experiments taking over 5400 s CPU time as time-out and taking over 10 GB memory as mem-out (MO). We considered several variants of the scenario described in IV-A. The Robot always started in the upper-left corner and had the lower-right corner as target. The VC started in the lower-right corner. In all variants, the view range was 3. We evaluated the following five scenarios: SC1 Rooms of varying size without obstacles. SC2 Differently sized rooms with a cross-shaped obstacle in the center, which scales with increasing grid size. SC3 A 25 \u00d7 25 room with up to 70 randomly placed obstacles. SC4 Two rooms (together 10 \u00d7 20) as depicted in Fig. 2. The doorway connecting the two rooms is a potential point of failure, as R cannot see to the other side. To improve reachability, we added cameras to improve visibility. SC5 Corridors of the format 4\u00d7x \u2013 long, narrow grids that the Robot has to traverse from top to bottom, passing the VC on its way down."}, {"heading": "B. Results", "text": "Table I shows the direct comparison between the POMDP description and the abstraction for SC1. The first column gives the grid size. Then, first for the POMDP and afterwards for the PG, the table lists the number of states, non-deterministic choices, and transitions of the model. The results include the safety probability induced by the optimal scheduler (\u201cResult\u201d), the run times (all in seconds) PRISM takes for constructing the state space from the symbolic description (\u201cModel Time\u201d), and finally the time to solve the\nPOMDP / PG (\u201cSol. Time\u201d). The last column shows the safety probability as computed using the fully observable MDP; it is an upper bound on the probability that is achievable for each grid. Note that optimal schedulers from this MDP are in general not observation-based and therefore not admissible for the POMDP. The time for creating the PRISM files was < 0.1 s in all cases.\nTable II lists data for the PG constructed from SC2 (first block of rows) and SC5 (without additional refinement in the second block, with region-based multi-step history refinement in the third block), analogous to Table I. Additionally the runtime for creating the symbolic description is given (\u201cRun times / Create\u201d). On the fully observable MDP, the resulting probability is 1.0 for all SC2- and 0.999 for all SC5 instances.\nTable III shows the results for SC3. The first column (\u201c#O\u201d) corresponds to the number of obstacles, while the remaining entries are analogous to Table II. The data for SC4 is shown in Table IV. Its structure is identical to that of Table III, with the first column (\u201c#C\u201d) corresponding to the number of cameras added for the graph refinement as in III-E."}, {"heading": "C. Evaluation", "text": "Consider SC1: While for very small examples, PRISM-pomdp delivers results within reasonable time, already the 6\u00d7 6 grid yields a mem-out. On the other hand,"}, {"heading": "10 297686 581135 1093201 0.9976 2.10 89.7 285.0 0.9999", "text": ""}, {"heading": "40 234012 454652 823410 0.9706 2.74 87.3 179.1 0.9999", "text": ""}, {"heading": "60 198927 385803 679321 0.6476 3.12 59.4 201.5 0.9999", "text": ""}, {"heading": "70 187515 363401 633884 0.6210 3.30 59.4 116.1 0.9896", "text": "our abstraction handles grids up to 30 \u00d7 30 within minutes, while still providing schedulers with a solid performance. The safety probability is lower for small grids, as there is less room for R to avoid VC, and there are proportionally more situations in which R is trapped in a corner or against a wall. Notice that for the MDP, the state space for an n\u00d7n grid is in O(n4) compared to a state space in O(r2n2) for the PG, where r is the viewing range r. As a consequence, no upper bound could be computed for the 50\u00d7 50 grid, as constructing the state space yielded a mem-out.\nIn Table II, for the SC5 benchmarks, we see that the safety probability goes down for grids with a longer corridor. This is because in the abstraction, the Robot can meet the VC multiple times when traveling down the corridor. To avoid this unrealistic behavior, we used the region-based multi-step history refinement as described in Sect. III-D. Although we only look at histories of one step of the VC in length, this is enough to keep the safety probability at a value much closer to the upper bound, regardless of the length of the corridor.\nTable II, SC2, indicates that the pre-computation of the visibility-lookup (see Sect. IV) for large grids with many obstacles eventually takes significant time, yet the model construction time increases on a faster pace. In comparison with SC1, we see that adding obstacles decreases the number of reachable states and thus also reduces the number of choices and transitions. Eventually, model construction takes longer than the actual model checking procedure.\nTable III indicates that the model checking time is not significantly influenced by the number of obstacles. Further-\nmore, we observe that the first 50 obstacles behave benevolent and only marginally influence the safety probability, while at over 60 obstacles, the probability dips significantly compared to the upper bound. This is because the added obstacles provide blind spots, in which the Robot can no longer observe the movement of the VC.\nThe same blind spot behavior can also be observed in Table IV (SC4). Here we add cameras to aid the robot by providing improved visibility around the blind spot, resulting in a near-perfect safety property. This doubles state space size and increases the model checking time by about 40 seconds."}, {"heading": "VI. DISCUSSION", "text": "Game-based abstraction successfully prunes the state space of MDPs by merging similar states. By adding an adversary that assumes the worst-case state, a PG is obtained. In general, this turns the POMDP at hand into a partially observable PG, which remains intractable. However, splitting according to observational equivalence leads to a fully observable PG. PGs can be analyzed by black-box algorithms as implemented, e. g., in PRISM-games, which also returns an optimal scheduler. The strategy from the PG can be applied to the POMDP, which yields the actual (higher) safety level.\nIn general, the abstraction can be too coarse; however, in the examples above, we have shown successfully that the game-based abstraction is not too coarse if one makes some assumptions about the POMDP. These assumptions are often naturally fulfilled by motion planning scenarios.\nThe assumptions from Sect. III-A can be relaxed in several respects: Our method naturally extends to multiple opponents. We restricted the method to a single controllable agent, but if information is shared among multiple agents, the method is applicable also to this setting. If information sharing is restricted, special care has to be taken to prevent information leakage. Richer classes of behavior for the opponents, including non-deterministic choices, are an important area for future research. This would lead to partially observable PGs, and game-based abstraction would yield three-player games. As two sources of non-determinism are uncontrollable, both the opponents and the abstraction could be controlled by Player 2, thus yielding a PG again.\nSupporting a richer class of temporal specifications is another option: PRISM-games supports a probabilistic variant of alternating (linear-) time logic extended by rewards and trade-off analysis. Using the same abstraction technique as we have presented, a larger class of properties thus can be analyzed. However, care has to be taken when combining invariants and reachability criteria arbitrarily, as they involve under- and over-approximations.\nOur method can be generalized to POMDPs for other settings. We use the original problem statement on the graph only to motivate the correctness. The abstraction can be lifted (as indicated by Def. 8), for refinement, however, a more refined argument for correctness is necessary.\nThe proposed construction of the PG is straightforward and currently realized without constructing the POMDP first. This simplifies the implementation of the refinement, but\nmapping the scheduler on the POMDP is currently not supported. Improved tool support thus should yield better results (cf. the (p+\u03c4)-safety in Fig. 1) without changing the method."}, {"heading": "VII. CONCLUSION", "text": "We utilized the successful technique of game-based abstraction to synthesize strategies for a class of POMDPs. Experiments show that this approach is promising. In future work, we will lift our approach to a broader class of POMDPs and improve the refinement steps, including an automatic refinement loop."}], "references": [{"title": "Dynamic Programming and Markov Processes, 1st ed", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1960}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "vol. 101, no. 1, pp. 99\u2013134, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Control of probabilistic systems under dynamic, partially known environments with temporal logic specifications.", "author": ["T. Wongpiromsarn", "E. Frazzoli"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A survey of point-based POMDP solvers", "author": ["G. Shani", "J. Pineau", "R. Kaplow"], "venue": "Autonomous Agents and Multi-Agent Systems, vol. 27, no. 1, pp. 1\u201351, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "The probabilistic model checking landscape.", "author": ["J.-P. Katoen"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "PRISM 4.0: Verification of probabilistic real-time systems", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "vol. 6806, 2011, pp. 585\u2013591.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "A storm is coming: A modern probabilistic model checker", "author": ["C. Dehnert", "S. Junges", "J.-P. Katoen", "M. Volk"], "venue": "CoRR, vol. abs/1702.04311, 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "PRISM-games: A model checker for stochastic multi-player games", "author": ["T. Chen", "V. Forejt", "M.Z. Kwiatkowska", "D. Parker", "A. Simaitis"], "venue": "vol. 7795, 2013, pp. 185\u2013191.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Verification and refutation of probabilistic specifications via games", "author": ["M. Kattenbelt", "M. Huth"], "venue": "ser. LIPIcs, vol. 4. Schloss Dagstuhl, 2009, pp. 251\u2013262.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A game-based abstraction-refinement framework for Markov decision processes", "author": ["M. Kattenbelt", "M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "vol. 36, no. 3, pp. 246\u2013280, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "What is decidable about partially observable Markov decision processes with \u03c9-regular objectives", "author": ["K. Chatterjee", "M. Chmel\u0131\u0301k", "M. Tracol"], "venue": "vol. 82, no. 5, pp. 878\u2013911, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Verification and control of partially observable probabilistic systems", "author": ["G. Norman", "D. Parker", "X. Zou"], "venue": "Real-Time Systems, vol. 53, no. 3, pp. 354\u2013402, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Scaling up Gaussian belief space planning through covariance-free trajectory optimization and automatic differentiation", "author": ["S. Patil", "G. Kahn", "M. Laskey", "J. Schulman", "K. Goldberg", "P. Abbeel"], "venue": "Algorithmic Foundations of Robotics XI, ser. Springer Tracts in Advanced Robotics, vol. 107, 2014, pp. 515\u2013533.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Sampling-based motion planning with sensing uncertainty.", "author": ["B. Burns", "O. Brock"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Rapidly-exploring random belief trees for motion planning under uncertainty.", "author": ["A. Bry", "N. Roy"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Control in belief space with temporal logic specifications", "author": ["C.-I. Vasile", "K. Leahy", "E. Cristofalo", "A. Jones", "M. Schwager", "C. Belta"], "venue": "2016, pp. 7419\u20137424.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Randomized belief-space replanning in partiallyobservable continuous spaces", "author": ["K. Hauser"], "venue": "Algorithmic Foundations of Robotics IX, 2010, pp. 193\u2013209.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Closed-loop belief space planning for linear, Gaussian systems.", "author": ["M.P. Vitus", "C.J. Tomlin"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Extending the applicability of POMDP solutions to robotic tasks", "author": ["D.K. Grady", "M. Moll", "L.E. Kavraki"], "venue": "IEEE Trans. Robotics, vol. 31, no. 4, pp. 948\u2013961, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "The verification of probabilistic systems under memoryless partial-information policies is hard", "author": ["L. de Alfaro"], "venue": "DTIC Document, Tech. Rep., 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Qualitative analysis of POMDPs with temporal logic specifications for robotics applications", "author": ["K. Chatterjee", "M. Chmel\u0131\u0301k", "R. Gupta", "A. Kanodia"], "venue": "2015, pp. 325\u2013330.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal cost almost-sure reachability in POMDPs", "author": ["\u2014\u2014"], "venue": "vol. 234, pp. 26\u201348, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Discretized approximations for POMDP with average cost", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "UAI. AUAI Press, 2004, p. 519.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Verification of partial-information probabilistic systems using counterexample-guided refinements", "author": ["S. Giro", "M.N. Rabe"], "venue": "vol. 7561, 2012, pp. 333\u2013348.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Assume-guarantee reasoning framework for MDP-POMDP.", "author": ["X. Zhang", "B. Wu", "H. Lin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Counterexample-guided abstraction refinement for POMDPs", "author": ["\u2014\u2014"], "venue": "CoRR, vol. abs/1701.06209, 2017.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "Partial-observation stochastic games: How to win when belief fails", "author": ["K. Chatterjee", "L. Doyen"], "venue": "vol. 15, no. 2, pp. 16:1\u201316:44, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantitative verification and strategy synthesis for stochastic games", "author": ["M. Svorenov\u00e1", "M. Kwiatkowska"], "venue": "Eur. J. Control, vol. 30, pp. 15\u201330, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Principles of Model Checking", "author": ["C. Baier", "J.-P. Katoen"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "The complexity of stochastic games", "author": ["A. Condon"], "venue": "Inf. Comput., vol. 96, no. 2, pp. 203\u2013224, 1992.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1992}, {"title": "Introduction to Stochastic Dynamic Programming", "author": ["S.M. Ross"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1983}, {"title": "Magnifying-lens abstraction for Markov decision processes", "author": ["L. de Alfaro", "P. Roy"], "venue": "vol. 4590. Springer, 2007, pp. 325\u2013338.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Offline motion planning for dynamical systems with uncertainties aims at finding a strategy for an agent that ensures certain desired behavior [1].", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "For many robotic applications, however, information about the current state of the environment is not observable [2], [3], [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "For many robotic applications, however, information about the current state of the environment is not observable [2], [3], [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "The belief state together with an update function form a (possibly infinite) MDP, commonly referred to as the underlying belief MDP [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Quantitative verification techniques like probabilistic model checking [6] provide comprehensive guarantees on such a strategy.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "For finite MDPs, tools like PRISM [7] or storm [8] employ efficient model checking algorithms to asses the probability to reach a certain set of states.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "For finite MDPs, tools like PRISM [7] or storm [8] employ efficient model checking algorithms to asses the probability to reach a certain set of states.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "addition to the choices of the agent: The POMDP abstraction results in a probabilistic two-player game (PG) [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "The automated abstraction procedure is inspired by gamebased abstraction [10], [11] of potentially infinite MDPs, where states are lumped in a similar fashion.", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "The automated abstraction procedure is inspired by gamebased abstraction [10], [11] of potentially infinite MDPs, where states are lumped in a similar fashion.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "As we target an undecidable problem [12], our approach is not complete in the sense that it does not always obtain a strategy which yields the required optimal probability.", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "model checking using [9] on PG on POMDP", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "The tool-chain uses PRISM-games [9] as a model checker for PGs.", "startOffset": 32, "endOffset": 35}, {"referenceID": 11, "context": "For the motion planning scenario considered, our preliminary results indicate an improvement of orders in magnitude over the state of the art in POMDP verification [13].", "startOffset": 164, "endOffset": 168}, {"referenceID": 12, "context": "Related work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Related work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Related work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "Related work: Sampling-based methods for motion planning in POMDP scenarios are considered in [14], [15], [16], [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "An overview on point-based value iteration for POMDPs is given in [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Other methods employ control techniques to synthesize strategies with safety considerations under observation and dynamics noise [2], [18], [19].", "startOffset": 129, "endOffset": 132}, {"referenceID": 16, "context": "Other methods employ control techniques to synthesize strategies with safety considerations under observation and dynamics noise [2], [18], [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "Other methods employ control techniques to synthesize strategies with safety considerations under observation and dynamics noise [2], [18], [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "Preprocessing of POMDPs in motion planning problems for robotics is suggested in [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "General verification problems for POMDPs and their decidability have been studied in [21], [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "General verification problems for POMDPs and their decidability have been studied in [21], [22].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "A recent survey about decidability results and algorithms for \u03c9-regular properties is given in [12], [23].", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "A recent survey about decidability results and algorithms for \u03c9-regular properties is given in [12], [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "The probabilistic model checker PRISM has recently been extended to support POMDPs [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "Partly based on the methods from [24], it produces lower and upper bounds for a variety of queries.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "In [25], an iterative refinement is proposed to solve POMDPs: Starting with total information, strategies that depend on unobservable information are excluded.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [26], a compositional framework for reasoning about POMDPs is introduced.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "Refinement based on counterexamples is considered in [27].", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "Partially observable probabilistic games have been considered in [28].", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "Finally, an overview of applications for PGs is given in [29].", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "For a finite or countably infinite set X , let \u03bc : X \u2192 [0, 1] such that \u2211", "startOffset": 55, "endOffset": 61}, {"referenceID": 28, "context": "A strategy \u03c3 for a PG resolves all non-deterministic choices, yielding an induced MC, for which a probability measure over the set of infinite paths is defined by the standard cylinder set construction [30].", "startOffset": 202, "endOffset": 206}, {"referenceID": 1, "context": "For many applications, not all system states are observable [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 21, "context": "There is a polynomial transformation of the general case to the POMDP definition used here [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "A formal treatment of belief MDPs is beyond the scope of this paper, for details see [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 29, "context": "For MDPs and PGs, memoryless deterministic strategies suffice to prove or disprove satisfaction of such specifications [31].", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "For POMDPs, observation-based strategies in their full generality are necessary [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": ", Gn, a set of collision states Collision, and a set of goal states Goals, an observationbased strategy \u03c3 \u2208 \u03a3 D for D is p-safe for p \u2208 [0, 1], if", "startOffset": 136, "endOffset": 142}, {"referenceID": 0, "context": "We want to compute a psafe strategy for a given p \u2208 [0, 1].", "startOffset": 52, "endOffset": 58}, {"referenceID": 8, "context": "We propose an abstraction method for world POMDPs that builds on game-based abstraction (GBAR), originally defined for MDPs [10], [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "We propose an abstraction method for world POMDPs that builds on game-based abstraction (GBAR), originally defined for MDPs [10], [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "For the classes of properties we consider, a memoryless deterministic strategy suffices for PGs to achieve the maximal probability of reaching a goal state without collision [31].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "In GBAR for MDPs [11], abstract states are split heuristically, yielding a finer over-approximation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 31, "context": "c) Region-based multi-step history refinement: As the refinement above blows up the state space drastically, we utilize a technique called magnifying lens abstraction [33].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "The proposed approach cannot solve every scenario \u2014 the problem is undecidable [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "As output, two PRISM files are created: A PG formulation of the abstraction including one-step history refinement, to be analyzed using PRISM-games [9], and the original POMDP for PRISM-pomdp [13].", "startOffset": 148, "endOffset": 151}, {"referenceID": 11, "context": "As output, two PRISM files are created: A PG formulation of the abstraction including one-step history refinement, to be analyzed using PRISM-games [9], and the original POMDP for PRISM-pomdp [13].", "startOffset": 192, "endOffset": 196}], "year": 2017, "abstractText": "We study motion planning problems where agents move inside environments that are not fully observable and subject to uncertainties. The goal is to compute a strategy for an agent that is guaranteed to satisfy certain safety and performance specifications. Such problems are naturally modeled by partially observable Markov decision processes (POMDPs). Because of the potentially huge or even infinite belief space of POMDPs, verification and strategy synthesis is in general computationally intractable. We tackle this difficulty by exploiting typical structural properties of such scenarios; for instance, we assume that agents have the ability to observe their own positions inside an environment. Ambiguity in the state of the environment is abstracted into non-deterministic choices over the possible states of the environment. Technically, this abstraction transforms POMDPs into probabilistic two-player games (PGs). For these PGs, efficient verification tools are able to determine strategies that approximate certain measures on the POMDP. If an approximation is too coarse to provide guarantees, an abstraction refinement scheme further resolves the belief space of the POMDP. We demonstrate that our method improves the state of the art by orders of magnitude compared to a direct solution of the POMDP.", "creator": "LaTeX with hyperref package"}}}