{"id": "1708.04278", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Sampling High Throughput Data for Anomaly Detection of Data-Base Activity", "abstract": "data leakage and risk theft from databases is a dangerous threat to organizations. data security and data privacy protection systems ( dsdp ) increasingly monitor data access and usage to identify leakage or suspicious traffic activities that should be investigated. because of the high velocity nature of database systems, such systems intentionally audit only a portion of the vast significant number of transactions that take place. anomalies are investigated by a security officer ( so ) in order to choose according the proper response. in this paper we investigate the effect of sampling methods not based systematically on the risk the transaction poses and propose a new method describing for \" combined sampling \" approaches for capturing a more varied sample.", "histories": [["v1", "Mon, 14 Aug 2017 19:05:20 GMT  (436kb)", "http://arxiv.org/abs/1708.04278v1", "Proceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016"]], "COMMENTS": "Proceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["hagit grushka-cohen", "oded sofer", "ofer biller", "michael dymshits", "lior rokach", "bracha shapira"], "accepted": false, "id": "1708.04278"}, "pdf": {"name": "1708.04278.pdf", "metadata": {"source": "CRF", "title": "Sampling High Throughput Data for Anomaly Detection of Data-Base Activity", "authors": ["Hagit Grushka-Cohen", "Oded Sofer", "Ofer Biller", "Michael Dymshits", "Lior Rokach"], "emails": ["{hgrushka@post.bgu.ac.il}", "{odedso@il.ibm.com}", "{ofer.biller@il.ibm.com}", "{m.dmshts@gmail.com}", "@bgu.ac.il}", "{bshapira@bgu.ac.il}"], "sections": [{"heading": null, "text": "Proceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016\nData leakage and theft from databases is a dangerous threat to organizations. Data Security and Data Privacy protection systems (DSDP) monitor data access and usage to identify leakage or suspicious activities that should be investigated. Because of the high velocity nature of database systems, such systems audit only a portion of the vast number of transactions that take place. Anomalies are investigated by a Security Officer (SO) in order to choose the proper response. In this paper we investigate the effect of sampling methods based on the risk the transaction poses and propose a new method for \"combined sampling\" for capturing a more varied sample. Keywords sampling; anomaly detection; risk assessment; insider threats; intrusion detection; high throughput; cyber\nProceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016\n1. INTRODUCTION\nDatabases lie at the heart of IT organizational infrastructure. Organizations monitor database operations in real time to prevent data leakage. Data security and data privacy protection (DSDP) systems are widely used to help implement security policies and detect attacks and data abuse. DSDP systems monitor database (DB) activity and enforce a predefined policy in order to issue alerts about policy violations and discover vulnerabilities such as weak passwords or out-of-date software. DSDP systems also apply anomaly detection algorithms in an attempt to detect data misuse, data leakage, impersonation, and attacks on database systems [1, 2]. These systems generate alerts when policy rules are violated or anomalous activities are performed [3,4,5]. Each alert demands the attention of a security officer (SO) [6] who must decide whether an alert represents a threat which should be investigated or dismissed. For investigation purposes, the security officer (SO) needs the original log data to be as informative as possible as she needs to assemble the passel in order to evaluate the whole picture. Organizations also save this data for future investigation in case a breach is discovered later. To address these needs some industrial DSDP systems maintain an archive of log data describing past transactions, and these logs are then passed on to an anomaly detection system to identify suspicious activity. Unlike the network domain, in the data-base security domain organizations prefer not to admit the attacks hence there is no indication to which or what portion of the anomalies are \u201ctrue positives\u201d. The databases monitored by DSDP systems serve thousands of users making up to a hundred thousand transactions per second. Storage is limited and can become a prohibitive cost for these systems, yet the amount of data that is logged affects the quality of anomaly detection and the ability to investigate historical data when a breach is discovered. Current solutions for the limited storage space are based on defining a policy to govern which portion of the transactions will be saved and audited.\nProceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016\nvarious techniques of dimensionality reduction such as PCA, deep learning / auto encoding approximation methods (including sketching), and Bloom filters [7,8]. These methods allow extracting features without consuming excess memory and disk space. However, saving compressed data does not provide SOs with the information required to decide how to respond to anomalies and does not facilitate the investigation of past behavior. Another approach to reducing the amount of data saved while preserving the full attributes is to sample transactions or activities. Techniques for sampling and their effects on anomaly detection have been studied in the domain of network traffic flow [9,10,11]. This domain is quite different from the domain of database transaction as the data is richer, containing more features, and the damage from a single transaction can be greater than the damage from a network packet. We propose using a sampling strategy based on the perceived risk posed by each transaction to the organization. The risk can be estimated using a manually calibrated policy or estimated using a machine learning ranking algorithm such as CyberRank [12]. We seek a smart monitoring policy based on information theory sampling that will be economical (storage-wise), without compromising our ability to discover the same anomalies, and also enable us to investigate an anomaly once discovered. The main contribution of our work is introducing new feature representing \u201csubject matter expert knowledge\u201d for storage reduction and improving performance of DSDP systems while maintaining the anomaly detection results and the investigative capabilities as required by the SO and the regulator. We present a sampling strategy for incorporating the risk associated with transactions \\ events and preliminary results on anomaly detection using simulated data.\n2. DATA EXPLORATION\nWe collected 24 hours of user activity data from a production DSDP system. The system\u2019s data is made of aggregated transactions details such as the IP address, time of the day and description of the\nProceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016\nCyberRank work [12], the user is an important entity whose behavior and activity is useful for identifying risk and controlling database transactions. When a transaction occurs, it is compared to the user\u2019s history to detect anomalies. The data collected consists of 24 hours of monitored DB transactions made by 1,901 unique users. We identified three different user behaviors: most users have very little activity with just a few queries (less than a thousand queries per user). The second group includes users with a lot of activity and thousands of queries. User activity is described in the histogram in Figure 1.\nis present for each user.\n3. EXPERIMENTAL SETTING\nOur objective was to compare different strategies for sampling transaction data. We look at three sampling methods:\n(i) vanilla: keeping a specific portion of the transactions for sampling (ii) risky only: sampling only from the transactions labeled as risky (iii) combination sampling: sampling from both risky and non-risky transactions using the\nGibbs sampling approach to define the portions of each transaction class in the resulting data\nThe DSDP system we investigated recorded the information per transaction of 20 features. Detecting anomalies with such complex features is not a trivial task, especially as it is difficult to determine\nProceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016\nunlabeled). Since we want to investigate the impact of sampling strategy based on the new risk feature on the results of anomaly detection we concentrate on producing low-complexity data to simulate the data sampled for audit. Anomalies are introduced randomly into the data, so the anomaly detection system can be evaluated in a controlled environment (the experiment data is labeled)."}, {"heading": "3.1 Data creation", "text": "We represent the user as a Gaussian distribution which produces a series of observations. An anomaly is created by sampling a data point obtained from a distribution different from that of the user. We generated 1,000 users, and for each user a series of observations. The generated observations contained 1% anomalies, an observation generated from a different Gaussian. We added a new feature to the data - A risk label, generated randomly in an independent manner for each observation, and 30% of the observations were labeled as risky."}, {"heading": "3.2 Anomaly detection", "text": "We simulated the following anomaly detection system for each user: (1) Each observation may be sampled for audit. (2) If it is sampled, it is compared to the previous series of sampled observations. (3) The user's Gaussian is estimated using the mean and standard deviation. (4) An observation is marked as an anomaly by the mock system if the observation value difference from the user's mean is greater than three standard deviations. Each sampling method was applied three times with different random seeds, and the results were averaged. We sampled at four proportions: 35%, 30%, 25%, and 20%. The sampling posteriors (proportion of each class in the sample) for the \"combination sampling\" method were set at 80% for the risky class and 20% for the non-risky class\n4. RESULTS\nProceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016\nsystem detected 57% of the anomalies with precision of 64%, with no significant difference between the observations labeled as high-risk or low-risk. We compare the recall of the generated anomalies between the sampling methods, and the results are divided by class (high-risk / low-risk). The vanilla sampling method produced the same recall for both classes - as the high-risk class is less frequent, the recall for this class is low. The \"risky only\" sampling method achieved high recall for the high-risk class at the price of zero recall for the lowrisk class. The \"combination sampling\" method achieves recall slightly lower than \"risk only\" on the high-risk class, while still capturing some of the anomalies in the low-risk class. See Figure 2 for a comparison of the recall for the two classes.\n5. DISCUSSION\nChoosing the right sampling strategy is an important decision when designing and implementing a DSDP system. We have shown that introducing transaction risk into the anomaly detection sampling process can significantly affect the results. In our experiment there was no underlying difference between the high and low-risk classes, we expect these transactions to behave differently in real life, amplifying the effect of the sampling algorithm on the observed anomalies. The risk captures the likelihood that an SO would investigate the anomaly [12], however the investigation will be more thorough if low-risk transactions are also captured for the suspect user. The \"risky only\" method would not provide this level of resolution, while the na\u00efve vanilla sampling would not detect most of the high-risk anomalies. Using a Gibbs sampling approach to provide\nFigure 2(a) recall for low-risk anomalies \u2013 vanilla sampling produces the best result (b) recall for high-risk anomalies \u2013 \"risky only\" has the higher recall.\nProceedings of the 11th Pre-ICIS Workshop on Information Security and Privacy, Dublin, Ireland December 10, 2016\nground. The proportion of each class can be tuned by the SO to fit the organizational needs (the \"risky only\" method is simply \"combination sampling\" with the proportion of high-risk samples set to one). We are working on applying a similar type of analysis to a sample of real data, as well as more complex anomaly detection methods. Other future work should include a usage study of the effect of different sampling strategies on the adoption and use by security officers. Another venue for future work is to combine data compression methods with sampling for improving both anomaly detection and the ability to investigate afterwards.\n6. REFERENCES\n[1] Veeramachaneni, K. and Arnaldo, I., AI2: Training a big data machine to defend. [2] Harel, A., Shabtai, A., Rokach, L. and Elovici, Y., 2012. M-score: A misuseability weight measure. Dependable and\nSecure Computing, IEEE Transactions on, 9(3), pp.414-428.\n[3] Sallam, A., Bertino, E., Hussain, S.R., Landers, D., Lefler, R.M. and Steiner, D., 2015. DBSAFE\u2014An Anomaly\nDetection System to Protect Databases From Exfiltration Attempts. IEEE SYSTEMS JOURNAL.\n[4] Kim, G., Lee, S. and Kim, S., 2014. A novel hybrid intrusion detection method integrating anomaly detection with\nmisuse detection. Expert Systems with Applications, 41(4), pp.1690-1700.\n[5] Chandola, V., Banerjee, A. and Kumar, V., 2009. Anomaly detection: A survey. In ACM computing surveys\n(CSUR), 41(3), p.15.\n[6] Veeramachaneni, K. and Arnaldo, I., AI2: Training a big data machine to defend. [7] Lall, A., Ogihara, M. and Xu, J., 2009, April. An efficient algorithm for measuring medium-to large-sized flows in\nnetwork traffic. In INFOCOM 2009, IEEE (pp. 2711-2715). IEEE.\n[8] Feldman, D., Schmidt, M. and Sohler, C., 2013, January. Turning big data into tiny data: Constant-size coresets for\nk-means, pca and projective clustering. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms (pp. 1434-1453). Society for Industrial and Applied Mathematics.\n[9] Jadidi, Z., Muthukkumarasamy, V., Sithirasenan, E. and Singh, K., 2016. Intelligent Sampling Using an Optimized\nNeural Network. Journal of Networks, 11(01), pp.16-27.\n[10] Mai, J., Chuah, C.N., Sridharan, A., Ye, T. and Zang, H., 2006, October. Is sampled data sufficient for anomaly\ndetection?. In Proceedings of the 6th ACM SIGCOMM conference on Internet measurement (pp. 165-176). ACM.\n[11] Juba, B., Musco, C., Long, F., Sidiroglou-Douskos, S. and Rinard, M.C., 2015, February. Principled Sampling for\nAnomaly Detection. In NDSS.\n[12] Grushka-Cohen, H., Sofer, O., Biller, O., Shapira, B. and Rokach, L., 2016, October. CyberRank: Knowledge\nElicitation for Risk Assessment of Database Security. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (pp. 2009-2012). ACM."}], "references": [{"title": "M-score: A misuseability weight measure", "author": ["A. Harel", "A. Shabtai", "L. Rokach", "Y. Elovici"], "venue": "Dependable and Secure Computing, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "DBSAFE\u2014An Anomaly Detection System to Protect Databases From Exfiltration Attempts", "author": ["A. Sallam", "E. Bertino", "S.R. Hussain", "D. Landers", "R.M. Lefler", "D. Steiner"], "venue": "IEEE SYSTEMS JOURNAL", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A novel hybrid intrusion detection method integrating anomaly detection with misuse detection", "author": ["G. Kim", "S. Lee", "S. Kim"], "venue": "Expert Systems with Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Anomaly detection: A survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "In ACM computing surveys (CSUR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "An efficient algorithm for measuring medium-to large-sized flows in network traffic", "author": ["A. Lall", "M. Ogihara", "J. Xu", "April"], "venue": "IEEE (pp. 2711-2715)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "January. Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["D. Feldman", "M. Schmidt", "C. Sohler"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms (pp. 1434-1453)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Intelligent Sampling Using an Optimized Neural Network", "author": ["Z. Jadidi", "V. Muthukkumarasamy", "E. Sithirasenan", "K. Singh"], "venue": "Journal of Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "October. Is sampled data sufficient for anomaly detection", "author": ["J. Mai", "C.N. Chuah", "A. Sridharan", "T. Ye", "H. Zang"], "venue": "In Proceedings of the 6th ACM SIGCOMM conference on Internet measurement (pp. 165-176)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Principled Sampling for Anomaly Detection", "author": ["B. Juba", "C. Musco", "F. Long", "S. Sidiroglou-Douskos", "M.C. Rinard", "February"], "venue": "In NDSS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "CyberRank: Knowledge Elicitation for Risk Assessment of Database Security", "author": ["H. Grushka-Cohen", "O. Sofer", "O. Biller", "B. Shapira", "L. Rokach", "October"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (pp. 2009-2012)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "DSDP systems also apply anomaly detection algorithms in an attempt to detect data misuse, data leakage, impersonation, and attacks on database systems [1, 2].", "startOffset": 151, "endOffset": 157}, {"referenceID": 1, "context": "These systems generate alerts when policy rules are violated or anomalous activities are performed [3,4,5].", "startOffset": 99, "endOffset": 106}, {"referenceID": 2, "context": "These systems generate alerts when policy rules are violated or anomalous activities are performed [3,4,5].", "startOffset": 99, "endOffset": 106}, {"referenceID": 3, "context": "These systems generate alerts when policy rules are violated or anomalous activities are performed [3,4,5].", "startOffset": 99, "endOffset": 106}, {"referenceID": 4, "context": "These include various techniques of dimensionality reduction such as PCA, deep learning / auto encoding approximation methods (including sketching), and Bloom filters [7,8].", "startOffset": 167, "endOffset": 172}, {"referenceID": 5, "context": "These include various techniques of dimensionality reduction such as PCA, deep learning / auto encoding approximation methods (including sketching), and Bloom filters [7,8].", "startOffset": 167, "endOffset": 172}, {"referenceID": 6, "context": "Techniques for sampling and their effects on anomaly detection have been studied in the domain of network traffic flow [9,10,11].", "startOffset": 119, "endOffset": 128}, {"referenceID": 7, "context": "Techniques for sampling and their effects on anomaly detection have been studied in the domain of network traffic flow [9,10,11].", "startOffset": 119, "endOffset": 128}, {"referenceID": 8, "context": "Techniques for sampling and their effects on anomaly detection have been studied in the domain of network traffic flow [9,10,11].", "startOffset": 119, "endOffset": 128}, {"referenceID": 9, "context": "The risk can be estimated using a manually calibrated policy or estimated using a machine learning ranking algorithm such as CyberRank [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 9, "context": "As described in our CyberRank work [12], the user is an important entity whose behavior and activity is useful for identifying risk and controlling database transactions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "The risk captures the likelihood that an SO would investigate the anomaly [12], however the investigation will be more thorough if low-risk transactions are also captured for the suspect user.", "startOffset": 74, "endOffset": 78}], "year": 2017, "abstractText": "Data leakage and theft from databases is a dangerous threat to organizations. Data Security and Data Privacy protection systems (DSDP) monitor data access and usage to identify leakage or suspicious activities that should be investigated. Because of the high velocity nature of database systems, such systems audit only a portion of the vast number of transactions that take place. Anomalies are investigated by a Security Officer (SO) in order to choose the proper response. In this paper we investigate the effect of sampling methods based on the risk the transaction poses and propose a new method for \"combined sampling\" for capturing a more varied sample.", "creator": "Microsoft\u00ae Word 2013"}}}