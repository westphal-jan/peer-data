{"id": "1708.06075", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "Scientific Information Extraction with Semi-supervised Neural Tagging", "abstract": "following this paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, element or material. we cast the core problem as sequence tagging data and introduce semi - supervised methods to a neural crest tagging model, which builds stronger on recent advances in named entity recognition. as since annotated training data is scarce in this domain, we introduce a graph - based semi - analytical supervised algorithm together with installing a data selection scheme to leverage unannotated articles. potentially both inductive and transductive digital semi - random supervised learning strategies outperform state - of - the - art information extraction performance on the 2017 semeval sciences task 10 scienceie task.", "histories": [["v1", "Mon, 21 Aug 2017 03:33:58 GMT  (1762kb,D)", "http://arxiv.org/abs/1708.06075v1", "accepted by EMNLP 2017"]], "COMMENTS": "accepted by EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yi luan", "mari ostendorf", "hannaneh hajishirzi"], "accepted": true, "id": "1708.06075"}, "pdf": {"name": "1708.06075.pdf", "metadata": {"source": "CRF", "title": "Scientific Information Extraction with Semi-supervised Neural Tagging", "authors": ["Yi Luan", "Mari Ostendorf", "Hannaneh Hajishirzi"], "emails": ["hannaneh}@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "As a research community grows, more and more papers are published each year. As a result there is increasing demand for improved methods for finding relevant papers and automatically understanding the key ideas in those papers. However, due to the large variety of domains and extremely limited annotated resources, there has been relatively little work on scientific information extraction. Previous research has focused on unsupervised approaches such as bootstrapping (Gupta and Manning, 2011; Tsai et al., 2013), where hand-designed templates are used to extract scientific keyphrases, and more templates are added through bootstrapping.\nVery recently a new challenge on Scientific Information Extraction (ScienceIE) (Augenstein et al., 2017)1 provides a dataset consisting of 500\n1SemEval (Task 10)https://scienceie.github. io/index.html\nscientific paragraphs with keyphrase annotations for three categories: TASK, PROCESS, MATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in Figure 1. This dataset enables the use of more advanced approaches such as neural network (NN) models. To that end, we cast the keyphrase extraction task as a sequence tagging problem, and build on recent progress in another information extraction task: Named Entity Recognition (NER) (Lample et al., 2016; Peng and Dredze, 2015). Like named entities, keyphrases can be identified by their linguistic context, e.g. researchers \u201duse\u201d methods. In addition, keyphrases can be associated with different categories in different contexts. For example, \u2018semantic parsing\u2019 can be labeled as a TASK in one article and as a PROCESS in another. Scientific keyphrases differ in that they can include both noun phrases and verb phrases and in that non-standard \u201cwords\u201d (equations, chemical compounds, references) can provide important cues.\nSince the scale of the data is still small for supervised training of neural systems, we introduce semi-supervised methods to the neural tagging\nar X\niv :1\n70 8.\n06 07\n5v 1\n[ cs\n.C L\n] 2\n1 A\nug 2\n01 7\nmodel in order to take advantage of the large quantity of unlabeled scientific articles. This is particularly important because of the differences in keyphrases across domains. Our semi-supervised learning algorithm uses a graph-based label propagation scheme to estimate the posterior probabilities of unlabeled data. It additionally extends the training objective to leverage the confidence of the estimated posteriors. The new training treats low confidence tokens as missing labels and computes the sentence-level score by marginalizing over them.\nOur experiments show that our neural tagging model achieves state-of-the-art results in the SemEval Science IE task. We further show that both inductive and transductive semi-supervised strategies significantly improve the performance. Finally, we provide in-depth analysis of domain differences as well as analysis of failure cases.\nThe key contributions of our work include: i) achieving state of the art in scientific information extraction SEMEVAL Task 10 by extending recent advances in neural tagging models; ii) introducing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training."}, {"heading": "2 Related Work", "text": "There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based\non this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and S\u00f8gaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data.\nNeural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles.\nPrevious work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone."}, {"heading": "3 Problem Definition and Data", "text": "The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What PROCESS or method has the paper used or compared to? What MATERIALS has the paper utilized in experiments? While these fundamental concepts are important in a wide variety of scientific disciplines, the terms that are used in specific disciplines can be substantially differ-\nent. For example, MATERIALS in computer science might be a text corpus, while they would be physical materials in physics or materials science.\nData We use the SemEval 2017 Task 10 ScienceIE dataset. Fig. 1 provides examples that illustrate the variation in domains, but also show that there are common cues such as \u201cthe task of\u201d, \u201cusing\u201d, \u201ctechnique,\u201d etc. A challenge with this dataset is that the size of the training data is very small. It is built from ScienceDirect open access publications and consists of 500 journal articles, but only one paragraph of each article is manually labeled. Therefore, we use a large amount of external data to leverage the continuous-space representation of language in neural network model. We explore the effect of pre-training word embedding with two different external resources: i) a data set of Wikipedia articles as a general English resource, and ii) a data set of 50k Computer Science papers from ACM.2\nTagging Problem Formulation The task requires detecting the exact span of a keyphrase. In order to be able to distinguish spans of two consecutive keyphrases of the same type, we assign labels to every word in a sentence, indicating position in the phrase and the type of phrase. We formulate the problem as an IOBES (Inside, Outside, Beginning, End and Singleton) tagging problem where every token is labeled either as: B, if it is at the beginning of a keyphrase; E, if it ends the phrase; I, if it is inside a keyphrase but not the first or last token; S, if it is a single-word keyphrase; or O, otherwise. For example, \u201cnamed entity recognition\u201d in first sentence of Fig. 1 is labeled as \u201cB-Task Itask E-task\u201d."}, {"heading": "4 Neural Architecture Model", "text": "We introduce an end-to-end model to categorize scientific keyphrases, building on a neural named entity recognition model (Lample et al., 2016) and adding a feature-based embedding."}, {"heading": "4.1 Model", "text": "We develop a 3-layer hierarchical neural model to tag tokens of the documents (details of the tokenization is in Sec. 6). (1) The token representation layer concatenates three components for\n2Due to the difficulty of data collection, experiments with external data from the other two domains is left to future work.\neach token: a bi-directional character-based embedding, a word embedding, and an embedding associated with orthographic and part-of-speech features. (2) The token LSTM layer uses a bidirectional LSTM to incorporate contextual cues from surrounding tokens to derive intermediate token embeddings. (3) The CRF tagging layer models token-level tagging decisions jointly using a CRF objective function to incorporate dependencies between tags. Character-Based Embedding. The embedding for a token is derived from its characters as the concatenation of forward and backward representations from a bidirectional LSTM. The character lookup table is initialized at random. The advantage of building a character-based embedding layer is that it can handle out-of-vocabulary words and equations, which are frequent in this data, all of which are mapped to \u201cUNK\u201d tokens in the Word Embedding Layer. Word Embedding. Words from a fixed vocabulary (plus the unknown word token) are mapped to a vector space, initialized using Word2vec pretraining with different combinations of corpora. Feature Embedding. We map features to a vector space: capitalization (all capital, first capital, all lower, any capital but first letter) and Part-ofSpeech tags.3 We randomly initialize feature vectors and train them together as other parameters. Token LSTM Layer We apply a bidirectional LSTM at the token level taking the concatenated character-word-feature embedding as input. The token representation obtained by stacking the forward and backward LSTM hidden states is passed as input to a linear layer that project the dimension to the size of label type space and is used as input to CRF layer. CRF Layer Keyphrase categorization is a task where there is strong dependencies across output labels (e.g., I-TASK cannot follow B-Process). Therefore, instead of making independent tagging decisions for each output, we model them jointly using conditional random field (Lafferty et al., 2001). For an input sentence x = (x1, x2, x3, . . . , xn), we consider P to be the matrix of scores output by the bidirectional LSTM network. P is of size n\u00d7m, where n is the number of tokens in a sentence, and m is the number of distinct tags. Pt,i corresponds to the score of\n3Dependency features were investigated but did not lead to performance gains.\nthe i-th tag of the t-th word in a sentence. We use a first-order Markov Model and define a transition matrix T where Ti,j represents the score from tag i to tag j. We also add y0 and yn as the start and end tags of a sentence. Therefore T becomes a square matrix of dimension m+ 2. Given one possible output y, and neural network parameters \u03b8 we define the score as\n\u03c6(y;x, \u03b8) = n\u2211 t=0 Tyt,yt+1 + n\u2211 t=1 Pt,yt (1)\nThe probability of sequence y is obtained by applying a softmax over all possible tag sequences\np\u03b8(y|x) = exp(\u03c6(y;x, \u03b8))\u2211\ny\u2032\u2208Y exp(\u03c6(y \u2032;x, \u03b8))\n(2)\nwhere Y denotes all possible tag sequences. The normalization term is efficiently computed using the forward algorithm.\nSupervised Training During training, we maximize the log-probability L(Y ;X, \u03b8) of the correct tag sequence given the corpus {X,Y }. Backpropagation is done based on a gradient computed using sentence-level scores."}, {"heading": "5 Semi-supervised Learning", "text": "We develop a semi-supervised algorithm that extends self-training by estimating the labels of unlabeled data and then using those labels for retraining. Specifically, we use a graph-based algorithm to estimate the posterior probabilities of unlabeled data and develop a new CRF training to take the uncertainty of the estimated labels into account while optimizing the objective function."}, {"heading": "5.1 Graph-based Posterior Estimates", "text": "Our semi-supervised algorithm uses the following steps to estimate the posterior. It first constructs a graph of tokens based on their semantic similarity, then uses the CRF marginal as a regularization term to do label propagation on the graph. The smoothed posterior is then used to either interpolate with the CRF marginal or as an additional feature to the neural network. Graph Construction Vertices in the graph correspond to tokens, and edges are distance between token features which capture semantic similarity. The total size of the graph is equal to the number of tokens in both labeled data Vl and unlabeled data Vu. The tokens are modelled with a concatenation of pre-trained word embeddings (with dimension d) of 5-gram centered by the current token, the word embedding of the closest verb, and a set of discrete features including part-of-speech tags and capitalization (43 and 4 dimension onehot features). The resulting feature vector with dimension of 5d + d + 43 + 4 is then projected down to 100 dimensions using PCA. We define the weight wuv of the edge between nodes u and v as follows: wuv = de(u, v) if v \u2208 K(u) or u \u2208 K(v), where K(u) is the set of k-nearest neighbors of u and de(u, v) is the Euclidean distance between any two nodes u and v in the graph. An example of our graph is in Fig. 2.\nFor every node i in the graph, we compute the marginal probabilities {qi} using the forwardbackward algorithm. Let \u03b8i represent the estimate of the CRF parameters after the n-th iteration, we compute the marginal probabilities p\u0303(j,t) = p(yjt |x; \u03b8i) over IOBES tags for every token position t in sentence j in labeled and unlabeled data. Label Propagation We use prior-regularized measure propagation (Liu and Kirchhoff, 2014; Subramanya and Bilmes, 2011) to propagate labels from the annotated data to their neighbors in the graph. The algorithm aims for the label distribution between neighboring nodes to be as similar to each other as possible by optimizing an objective function that minimizes the Kullback-Leibler distances between: i) the empirical distribution ru of labeled data and the predicted label distribution qu for all labeled nodes in the graph; ii) the distributions qu and qv for all nodes u in the graph and their neighbors v; iii) the distributions qu and the CRF marginals p\u0303u for all nodes. The third term regularizes the predicted distribution toward the\nCRF prediction if the node is not connected to a labeled vertex, ensuring the algorithm performs at least as well as standard self-training.\nPosterior Estimates We develop two strategies to estimate the new posteriors p\u0302(yt|x; \u03b8), which can then be used in the CRF training.\nThe first strategy (called GRAPHINTERP) is the commonly used approach (Subramanya et al., 2010; Aliannejadi et al., 2014) that interpolates the smoothed posterior {q} with CRF marginals p:\np\u0302(yt|x; \u03b8) = \u03b1p(yt|x; \u03b8) + (1\u2212 \u03b1)q(y) (3)\nwhere \u03b1 is a mixing coefficient. A second strategy introduced here (called GRAPHFEAT) uses the smoothed posterior {q} as features and learns it with other parameters in the neural network. Given a sentence {x1, . . . , xn}, let Q = {q1, . . . , qn} be the predicted label distribution from the graph. We then use Q as a feature input to neural network as P\u0303 = P +MQ where P is the n \u00d7m matrix output by the bidirectional LSTM network as in Eq. 1, and M is m\u00d7m matrix and is learned together with other parameters of neural network. We modify Eq. 1 by replacing Pt,yt with P\u0303t,yt . Note that GRAPHFEAT can only be done in a transductive way since it requires output Q from the graph at test time."}, {"heading": "5.2 CRF training with Uncertain Labels", "text": "A standard approach to self-training is to make hard decisions for labeling tokens based on the estimated posteriors and retrain the model. However, the estimated posteriors in our task are noisy due to the difficulty and variety of the ScienceIE task. Instead, we extend the CRF training to leverage the confidence of the estimated posteriors. The new CRF training (called Uncertain Label\nMarginalizing (ULM)) treats low confidence tokens as missing labels and computes the sentencelevel score by marginalizing over them. A similar idea has been previously used in treating partially labeled data (Kim et al., 2015).\nSpecifically, given a sentence x we define a constrained lattice Y(x), where at each position t the allowed label types Y(xt) are:\nY(xt) = { {yt}, if p(yt|x; \u03b8) > \u03b7 All label types, otherwise\n(4)\nwhere \u03b7 is the confidence threshold, yt is the prediction of posterior decoding and p(yt|x; \u03b8) is its CRF token marginal. The new neural network parameters \u03b8 are estimated by maximizing the loglikelihood of p\u03b8(Y(xk)|xk) for every input sentence xk, where\np\u03b8(Y(xk)|xk) = \u2211 yk\u2208Y(xk) exp(\u03c6(y k;xk, \u03b8))\u2211\ny\u2032\u2208Y exp(\u03c6(y \u2032;x, \u03b8))\nwhere yk is an instance sequence of lattice Y(x), and k is the sentence index in the training set. Extreme cases are when all tokens are uncertain then the likelihood would be equal to 1, when all tokens of a sequence are confident, it would be equal to Eq. 2 where only one possible sequence, as in Fig. 3. Inductive and Transductive Learning The semi-supervised training process is summarized as follow: It first computes marginals over the unlabeled data given a set of CRF parameters. It then uses the marginals as a regularization term for label propagation. The smoothed posteriors from the graph are then interpolated with the CRF marginal in GRAPHINTERP or used as an additional feature in GRAPHFEAT. It then uses the estimated labels for the unlabeled data combined with the labeled data to retrain the CRF using either the hard decision CRF training objective as Eq. 2 or the ULM data selection objective.\nIn the inductive setting, we only use the unlabeled data from the development set for the semisupervision. In the transductive setting we also use the unlabeled data of the test set to construct the graph. In both cases, the parameters are tuned only on the dev set."}, {"heading": "6 Experimental Setup", "text": "Data The SemEval ScienceIE (SE) corpus consists of 500 journal articles; one paragraph of each\narticle is randomly selected and annotated. The complete unlabeled articles and their metadata are provided together with the labeled data. The training data consists of 350 documents; 50 are kept for development and 100 for testing. The 500 articles come from 82 different journals evenly distributed in three domains. We manually labeled 82 journal names in the dataset into the three domains and do analysis based on the domain partitions. The 500 full articles contains 2M words and is 30 times the size of the annotated data.\nAdditionally, we use two external resources for pretraining word embeddings: i) WIKI, as for Wikipedia articles, specifically a full Wikipedia dump from 2012 containing 46M words, and ii) ACM, a collection of CS papers, containing 108M words.\nComparisons We compare our system with two template matching baselines and the state-of-theart on the SemEval Science IE task. The first baseline (Gupta and Manning, 2011) is an unsupervised method to extract keyphrases by initially using seed patterns in a dependency tree, and then adding to seed patterns through bootstrapping. The second baseline (Tsai et al., 2013) improves the work of Gupta and Manning (2011) by adding Named Entity Features and use different set of seed patterns.\nImplementation details All parameters are tuned on the dev set performance, the best parameters are selected and fixed for model switching and semi-supervised systems. The word embedding dimension is 250; the token-level hidden dimension is 100; the character-level hidden dimension is 25; and the optimization algorithm is SGD with a learning rate of 0.05. For building the graph, the best pre-trained embeddings for the supervised system (Sec. 7.2) are used in each domain. Two special tokens BOS and EOS are added when pre-training, indicating the begin and end of a sentence. The number of the graph vertices is 2M in tranductive setting and 1.4M in inductive setting. The ULM parameter \u03b7 in Eq. 4 is tuned from 0.1 to 0.9, the best \u03b7 is 0.4. The best parameters of label propagation are \u00b5 = 10\u22126 and \u03bd = 10\u22125. The interpolation parameter \u03b1 in Eq. 3 is tuned from 0.1 to 0.9, the best \u03b1 is 0.3. We do iteration of semi-supervised learning until we obtain the best result on the dev set, which is mostly achieved in the second round.\nWe use Stanford CoreNLP (Manning et al., 2014) tokenizer to tokenize words. The tokenizer is augmented with a few hand-designed rules to handle equations (e.g. \u201cfs(B,t)=Spel(t)S\u201d is a single token) and other non-standard word phenomena (Cu40Zn, 20MW/m2) in scientific literature. We use Approximate Nearest Neighbor Searching (ANN)4 to calculate the k-nearest neighbors. For all experiments in this paper, k = 10. Setup We evaluate our system in both inductive and transductive settings. The systems with a \u2217 superscript in the table are transductive. The inductive setting uses 400 full articles in ScienceIE training and dev sets, while the transductive setting uses 500 full articles including the test set. In both settings parameters are tuned over the dev set.\n4https://www.cs.umd.edu/\u02dcmount/ANN/"}, {"heading": "7 Experimental Results", "text": "We evaluate our NN-CRF model in both supervised and semi-supervised settings. We also perform ablations and try different variants to best understand our model."}, {"heading": "7.1 Best Case System Performance", "text": "Table 1 reports the results of our neural sequence tagging model NN-CRF in both supervised and semi-supervised learning (ULM and graph-based), and compares them with the baselines and the state-of-the-art (best SemEval System (Augenstein et al., 2017)).\nAugenstein and S\u00f8gaard (2017) use a multi-task learning strategy to improve the performance of supervised keyphrase classification, but they only report dev set performance on SemEval Task 10, we also include their result here and refer it as MULTITASK. We report results for both span identification (SemEval SubTask A) and span classification into TASK, PROCESS and MATERIAL (SemEval Subtask B).5\nThe results show that our neural sequence tagging models significantly outperforms the state of the art and both baselines. It confirms that our neural tagging model outperforms other nonneural and neural models for the SemEval ScienceIE challenge6. It further shows that our system achieves significant boost from semi-supervised learning using unlabeled data. Table 5 shows the detailed analysis of the system across different categories."}, {"heading": "7.2 Supervised Learning", "text": "Impact of Neural Model Components Table 2 provides the results of an ablation study on the dev set showing the impact of different components of our NN-CRF on the Scientific IE task. For the basic model, the word embeddings are initialized by word2vec trained on the 350 full journal articles in the SE training set together with Wikipedia and ScienceIE data. The feature layer, character layer, and bi-LSTM word layers all improves the performance. Moreover, we observe a large improvement (20.6% relative) in the scientific IE task by adding the CRF layer. Initialization Table 3 reports our NN-CRF performance when pretrained on different do-\n5The evaluation script is provided by the challenge, with a modification to report 3 decimal precision results.\n6Best SemEval Numbers from https://scienceie.github.io/\nmains. We explore different word embedding pretraining with ScienceIE training set alone (SE), and adding other external resources including Wikipedia (wiki) and Computer Science articles (ACM). All alternatives use word2vec. Compared with using SE alone, introduction of all external data sources improve performance. Moreover, we observe that with the introduction of the ACM dataset, the performance on the CS domain is increased significantly in both the dev and test sets. Adding Wikipedia data benefits all three domains, with more significant improvement on the MS and Physics domains.\nBased on these observations, we select the best model on each domain according to the dev set and use the combined result as our best suprevised system (called NN-CRF(supervised)). The F1 score improves from 39.4 to 40.2 when applying model switching strategy. The best model on the dev set is used for each domain: for MS and physics domain, we pretrain word embeddings with the SE and Wiki, and for the CS domain, we pretrain with the SE and ACM."}, {"heading": "7.3 Semi-Supervision Learning", "text": "Table 4 reports the results of the semi-supervised learning algorithms in different settings. In particular we ablate incorporating the graph-based methods of computing the posterior and CRF training (ULM vs. hard decision). The table shows incorporating graph-based methods for computing posterior and ULM for CRF training outperforms their counterparts.\nFor computing the posterior, we explore two different strategies of the graph-based methods: i) GRAPHINTERP that interpolates the smoothed posterior from label propagation with CRF marginals; For inductive setting, GRAPHINTERP only uses un-annotated data from the dev set and uses the best model for decoding at test time. For transductive setting, GRAPHINTERP\u2217 uses unannoated data from test set to build the graph as\nwell, and tune the parameters on the dev set. ii) GRAPHFEAT uses the smoothed posterior from label propagation as additional feature to neural network and only has transductive setting.\nAs expected, the transductive approaches consistently outperform inductive approaches on the test set. With around the same performance on dev set, GRAPHINTERP* seems to generalize better on test set with 1.6% relative improvement over GRAPHINTERP. We observe higher improvement with GRAPHFEAT* compared to GRAPHINTERP. This is mainly because automatically learning the weight matrix M between neural network scores and graph outputs adds more flexibility compared to tuning an interpolation weight \u03b1. The performance is further improved by applying data selection through modifying the objective to ULM. The best inductive system is ULM+GRAPHINTERP with 5.6% relative improvement over pure SelfTraining that makes hard decisions, and the best transductive system is ULM+GRAPHFEAT* with 8.6% relative improvement."}, {"heading": "7.4 Category and Span Analysis", "text": "Table 5 details the performance of our method on the three categories at the span and token level. We observe significant improvement by using\nULM+GRAPHINTERP and ULM+GRAPHFEAT over best SemEval and our best supervised system on all three categories at both token and span levels. We further observe that systems\u2019 performance on TASK classification is much lower than PROCESS and MATERIAL. This is in part because TASK is much less frequent than the other types. In addition, TASK keyphrases often include verb phrases while the other two domains mainly consists of noun phrases. An analysis of confusion patterns show that the most frequent type confusions are between PROCESS and MATERIAL. However, we observe that ULM+GRAPHFEAT* can greatly reduce the confusion, with 3.5% relative improvement of PROCESS and 3.6% relative improvement of PROCESS over ULM+GRAPHINTERP on token level."}, {"heading": "7.5 Error Analysis", "text": "We provide examples of typical errors that our system makes in Table 6. As described in the previous subsection, TASK is the hardest type to identify with our system. Row 1 shows a failure to detect the verb phrase following \u2018to\u2019 as part of the TASK, but detect \u2018enantiopure products\u2019 as MATERIAL. The system prefers to predict PROCESS or MATERIAL since those classes have more samples than TASK. Row 2 illustrates the problem of identifying general terms as keyphrases due to similar context, such as \u2018receptors\u2019 and \u2018drug action\u2019. A third common error involves incorrectly labeling adjectives, such as \u2018neighbouring\u2019 in Row 3, which leads to span errors. Another common cause of error is insufficient context: in the last example, a larger context is needed to determine whether \u2018SWE\u2019 is a PROCESS or MATERIAL."}, {"heading": "8 Conclusion", "text": "This paper casts the scientific information extraction task as a sequence tagging problem and introduces a hierarchical LSTM-CRF neural tagging model for this task, building on recent results in NER. We introduced a semi-supervised learning algorithm that incorporates graph-based label propagation and confidence-aware data selection. We show the introduction of semi-supervision significantly outperforms the performance of the supervised LSTM-CRF tagging model. We additionally show that external resources are useful for initializing word embeddings. Both inductive and transductive semi-supervised strategies\nachieve state-of-the-art performance in SemEval 2017 ScienceIE task. We also conducted a detailed analysis of the system and point out common error cases.\nIn our experiments, we observe that including in-domain data only for semi-supervised learning has slightly better performance than using crossdomain data. Reducing the amount of in-domain data hurts performance. Therefore, adding more in-domain unlabeled data may help when combined with selection schemes such as the ULM algorithms proposed here. It would be useful to assess the impact of matched unlabeled data for the physics and material science domain. Other future work includes leveraging global context, information of citation network."}, {"heading": "9 Acknowledgments", "text": "This research was supported by the NSF (IIS 1616112), Allen Institute for AI (66-9175), Allen Distinguished Investigator Award, and gifts from Google, Samsung, and Bloomberg. We thank the anonymous reviewers for their helpful comments."}], "references": [{"title": "Coherent citation-based summarization of scientific papers", "author": ["Amjad Abu-Jbara", "Dragomir Radev."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association", "citeRegEx": "Abu.Jbara and Radev.,? 2011", "shortCiteRegEx": "Abu.Jbara and Radev.", "year": 2011}, {"title": "Graphbased semi-supervised conditional random fields for spoken language understanding using unaligned data", "author": ["Mohammad Aliannejadi", "Masoud Kiaeeha", "Shahram Khadivi", "Saeed Shiry Ghidary."], "venue": "Australasian Language Technology Associ-", "citeRegEx": "Aliannejadi et al\\.,? 2014", "shortCiteRegEx": "Aliannejadi et al\\.", "year": 2014}, {"title": "Towards a computational history of the ACL", "author": ["Ashton Anderson", "Dan McFarland", "Dan Jurafsky"], "venue": null, "citeRegEx": "Anderson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2012}, {"title": "Contextenhanced citation sentiment detection", "author": ["Awais Athar", "Simone Teufel."], "venue": "Proceedings of the 2012 conference of the North American chapter of the Association for Computational Linguistics: Human language technologies. Associa-", "citeRegEx": "Athar and Teufel.,? 2012a", "shortCiteRegEx": "Athar and Teufel.", "year": 2012}, {"title": "Detection of implicit citations for sentiment detection", "author": ["Awais Athar", "Simone Teufel."], "venue": "Proceedings of the Workshop on Detecting Structure in Scholarly Discourse. Association for Computational Linguistics, pages 18\u201326.", "citeRegEx": "Athar and Teufel.,? 2012b", "shortCiteRegEx": "Athar and Teufel.", "year": 2012}, {"title": "Semeval 2017 task 10: ScienceIE - extracting keyphrases and relations from scientific publications", "author": ["Isabelle Augenstein", "Mrinal Das", "Sebastian Riedel", "Lakshmi Vikraman", "Andrew McCallum."], "venue": "Proceedings of SemEval .", "citeRegEx": "Augenstein et al\\.,? 2017", "shortCiteRegEx": "Augenstein et al\\.", "year": 2017}, {"title": "Multitask learning of keyphrase boundary classification", "author": ["Isabelle Augenstein", "Anders S\u00f8gaard."], "venue": "arXiv preprint arXiv:1704.00514.", "citeRegEx": "Augenstein and S\u00f8gaard.,? 2017", "shortCiteRegEx": "Augenstein and S\u00f8gaard.", "year": 2017}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A Smith."], "venue": "EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "The ACL anthology reference corpus: A reference dataset for bibliographic research", "author": ["Steven Bird", "Robert Dale", "Bonnie J Dorr", "Bryan R Gibson", "Mark Thomas Joseph", "Min-Yen Kan", "Dongwon Lee", "Brett Powley", "Dragomir R Radev", "Yee Fan Tan"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2008}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Jason PC Chiu", "Eric Nichols."], "venue": "TACL.", "citeRegEx": "Chiu and Nichols.,? 2016", "shortCiteRegEx": "Chiu and Nichols.", "year": 2016}, {"title": "Unsupervised models for named entity classification", "author": ["Michael Collins", "Yoram Singer."], "venue": "Proceedings of the joint SIGDAT conference on empirical methods in natural language processing and very large corpora. Citeseer, pages 100\u2013110.", "citeRegEx": "Collins and Singer.,? 1999", "shortCiteRegEx": "Collins and Singer.", "year": 1999}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems. pages 3079\u20133087.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Extracting and matching authors and affiliations in scholarly documents", "author": ["Huy Hoang Nhat Do", "Muthu Kumar Chandrasekaran", "Philip S Cho", "Min Yen Kan."], "venue": "Proceedings of the 13th ACM/IEEECS joint conference on Digital libraries. ACM,", "citeRegEx": "Do et al\\.,? 2013", "shortCiteRegEx": "Do et al\\.", "year": 2013}, {"title": "Semantic annotation of the ACL anthology corpus for the automatic analysis of scientific literature", "author": ["Kata Gabor", "Haifa Zargayouna", "Davide Buscaldi", "Isabelle Tellier", "Thierry Charnois."], "venue": "Proceedings of the Tenth International Conference on", "citeRegEx": "Gabor et al\\.,? 2016", "shortCiteRegEx": "Gabor et al\\.", "year": 2016}, {"title": "Analyzing the dynamics of research by extracting key aspects of scientific papers", "author": ["Sonal Gupta", "Christopher D Manning."], "venue": "IJCNLP. pages 1\u20139.", "citeRegEx": "Gupta and Manning.,? 2011", "shortCiteRegEx": "Gupta and Manning.", "year": 2011}, {"title": "Driver prediction to improve interaction with in-vehicle hmi", "author": ["Bret Harsham", "Shinji Watanabe", "Alan Esenther", "John Hershey", "Jonathan Le Roux", "Yi Luan", "Daniel Nikovski", "Vamsi Potluru."], "venue": "Proc. Workshop on Digital Signal Processing for In-", "citeRegEx": "Harsham et al\\.,? 2015", "shortCiteRegEx": "Harsham et al\\.", "year": 2015}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv preprint arXiv:1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "The computational linguistics summarization pilot", "author": ["Kokil Jaidka", "Muthu Kumar Chandrasekaran", "Beatriz Fisas Elizalde", "Rahul Jha", "Christopher Jones", "Min-Yen Kan", "Ankur Khanna", "Diego Molla-Aliod", "Dragomir R Radev", "Francesco Ronzano"], "venue": null, "citeRegEx": "Jaidka et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaidka et al\\.", "year": 2014}, {"title": "Structures and statistics of citation networks", "author": ["Miray Kas."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Kas.,? 2011", "shortCiteRegEx": "Kas.", "year": 2011}, {"title": "Weakly supervised slot tagging with partially labeled sequences from web search click logs", "author": ["Young-Bum Kim", "Minwoo Jeong", "Karl Stratos", "Ruhi Sarikaya."], "venue": "HLT-NAACL. pages 84\u201392.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the eighteenth international conference on machine learning, ICML", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "NAACL.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Recognition of stance strength and polarity in spontaneous speech", "author": ["Gina-Anne Levow", "Valerie Freeman", "Alena Hrynkevich", "Mari Ostendorf", "Richard Wright", "Julian Chan", "Yi Luan", "Trang Tran."], "venue": "Spoken Language Technology Workshop (SLT),", "citeRegEx": "Levow et al\\.,? 2014", "shortCiteRegEx": "Levow et al\\.", "year": 2014}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "ACL. pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Graph-based semi-supervised learning for phone and segment classification", "author": ["Yuzong Liu", "Katrin Kirchhoff."], "venue": "Proceedings of Annual Conference of the International Speech Communication Association (Interspeech).", "citeRegEx": "Liu and Kirchhoff.,? 2013", "shortCiteRegEx": "Liu and Kirchhoff.", "year": 2013}, {"title": "Graph-based semi-supervised acoustic modeling in DNN-based speech recognition", "author": ["Yuzong Liu", "Katrin Kirchhoff."], "venue": "IEEE SLT .", "citeRegEx": "Liu and Kirchhoff.,? 2014", "shortCiteRegEx": "Liu and Kirchhoff.", "year": 2014}, {"title": "Acoustic modeling with neural graph embeddings", "author": ["Yuzong Liu", "Katrin Kirchhoff."], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).", "citeRegEx": "Liu and Kirchhoff.,? 2015", "shortCiteRegEx": "Liu and Kirchhoff.", "year": 2015}, {"title": "Graph-based semisupervised learning for acoustic modeling in automatic speech recognition", "author": ["Yuzong Liu", "Katrin Kirchhoff."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing 24(11):1946\u20131956.", "citeRegEx": "Liu and Kirchhoff.,? 2016a", "shortCiteRegEx": "Liu and Kirchhoff.", "year": 2016}, {"title": "Novel frontend features based on neural graph embeddings for DNN-HMM and LSTM-CTC acoustic modeling", "author": ["Yuzong Liu", "Katrin Kirchhoff."], "venue": "Proceedings of Annual Conference of the International Speech Communication Association (Inter-", "citeRegEx": "Liu and Kirchhoff.,? 2016b", "shortCiteRegEx": "Liu and Kirchhoff.", "year": 2016}, {"title": "Multiplicative representations for unsupervised semantic role induction", "author": ["Yi Luan", "Yangfeng Ji", "Hannaneh Hajishirzi", "Boyang Li."], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. page 118.", "citeRegEx": "Luan et al\\.,? 2016a", "shortCiteRegEx": "Luan et al\\.", "year": 2016}, {"title": "Lstm based conversation models", "author": ["Yi Luan", "Yangfeng Ji", "Mari Ostendorf."], "venue": "arXiv preprint arXiv:1603.09457.", "citeRegEx": "Luan et al\\.,? 2016b", "shortCiteRegEx": "Luan et al\\.", "year": 2016}, {"title": "Semisupervised noise dictionary adaptation for exemplarbased noise robust speech recognition", "author": ["Yi Luan", "Daisuke Saito", "Yosuke Kashiwagi", "Nobuaki Minematsu", "Keikichi Hirose."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE", "citeRegEx": "Luan et al\\.,? 2014a", "shortCiteRegEx": "Luan et al\\.", "year": 2014}, {"title": "Performance improvement of automatic pronunciation assessment in a noisy classroom", "author": ["Yi Luan", "Masayuki Suzuki", "Yutaka Yamauchi", "Nobuaki Minematsu", "Shuhei Kato", "Keikichi Hirose."], "venue": "Spoken Language Technology Workshop (SLT), 2012", "citeRegEx": "Luan et al\\.,? 2012", "shortCiteRegEx": "Luan et al\\.", "year": 2012}, {"title": "Efficient learning for spoken language understanding tasks with word embedding based pre-training", "author": ["Yi Luan", "Shinji Watanabe", "Bret Harsham."], "venue": "INTERSPEECH. Citeseer, pages 1398\u20131402.", "citeRegEx": "Luan et al\\.,? 2015", "shortCiteRegEx": "Luan et al\\.", "year": 2015}, {"title": "Relating automatic vowel space estimates to talker intelligibility", "author": ["Yi Luan", "Richard Wright", "Mari Ostendorf", "GinaAnne Levow."], "venue": "Fifteenth Annual Conference of the International Speech Communication Association.", "citeRegEx": "Luan et al\\.,? 2014b", "shortCiteRegEx": "Luan et al\\.", "year": 2014}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "ACL.", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "The stanford CoreNLP natural language processing toolkit", "author": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky."], "venue": "ACL (System Demonstrations). pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Named entity recognition for chinese social media with jointly trained embeddings", "author": ["Nanyun Peng", "Mark Dredze."], "venue": "EMNLP. pages 548\u2013554.", "citeRegEx": "Peng and Dredze.,? 2015", "shortCiteRegEx": "Peng and Dredze.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The acl rd-tec 2.0: A language resource for evaluating term extraction and entity recognition methods", "author": ["Behrang QasemiZadeh", "Anne-Kathrin Schumann"], "venue": null, "citeRegEx": "QasemiZadeh and Schumann.,? \\Q2012\\E", "shortCiteRegEx": "QasemiZadeh and Schumann.", "year": 2012}, {"title": "Discovering factions in the computational linguistics community", "author": ["Yanchuan Sim", "Noah A Smith", "David A Smith."], "venue": "Proceedings of the ACL2012 Special Workshop on Rediscovering 50 Years of Discoveries. Association for Computational Lin-", "citeRegEx": "Sim et al\\.,? 2012", "shortCiteRegEx": "Sim et al\\.", "year": 2012}, {"title": "Semi-supervised learning with measure propagation", "author": ["Amarnag Subramanya", "Jeff Bilmes."], "venue": "Journal of Machine Learning Research 12(Nov):3311\u20133370.", "citeRegEx": "Subramanya and Bilmes.,? 2011", "shortCiteRegEx": "Subramanya and Bilmes.", "year": 2011}, {"title": "Efficient graph-based semisupervised learning of structured tagging models", "author": ["Amarnag Subramanya", "Slav Petrov", "Fernando Pereira."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Subramanya et al\\.,? 2010", "shortCiteRegEx": "Subramanya et al\\.", "year": 2010}, {"title": "Concept-based analysis of scientific literature", "author": ["Chen-Tse Tsai", "Gourab Kundu", "Dan Roth."], "venue": "Proceedings of the 22nd ACM international conference on Conference on information & knowledge management. ACM, pages 1733\u20131738.", "citeRegEx": "Tsai et al\\.,? 2013", "shortCiteRegEx": "Tsai et al\\.", "year": 2013}, {"title": "He said, she said: Gender in the ACL anthology", "author": ["Adam Vogel", "Dan Jurafsky."], "venue": "Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries. Association for Computational Linguistics, pages 33\u201341.", "citeRegEx": "Vogel and Jurafsky.,? 2012", "shortCiteRegEx": "Vogel and Jurafsky.", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "Previous research has focused on unsupervised approaches such as bootstrapping (Gupta and Manning, 2011; Tsai et al., 2013), where hand-designed templates are used to extract scientific keyphrases, and more templates are added through bootstrapping.", "startOffset": 79, "endOffset": 123}, {"referenceID": 45, "context": "Previous research has focused on unsupervised approaches such as bootstrapping (Gupta and Manning, 2011; Tsai et al., 2013), where hand-designed templates are used to extract scientific keyphrases, and more templates are added through bootstrapping.", "startOffset": 79, "endOffset": 123}, {"referenceID": 5, "context": "Very recently a new challenge on Scientific Information Extraction (ScienceIE) (Augenstein et al., 2017)1 provides a dataset consisting of 500", "startOffset": 79, "endOffset": 104}, {"referenceID": 19, "context": "Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al.", "startOffset": 94, "endOffset": 181}, {"referenceID": 14, "context": "Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al.", "startOffset": 94, "endOffset": 181}, {"referenceID": 42, "context": "Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al.", "startOffset": 94, "endOffset": 181}, {"referenceID": 13, "context": "Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al.", "startOffset": 94, "endOffset": 181}, {"referenceID": 18, "context": "Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al.", "startOffset": 94, "endOffset": 181}, {"referenceID": 0, "context": ", 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al.", "startOffset": 23, "endOffset": 50}, {"referenceID": 46, "context": ", 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014).", "startOffset": 91, "endOffset": 186}, {"referenceID": 2, "context": ", 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014).", "startOffset": 91, "endOffset": 186}, {"referenceID": 23, "context": ", 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014).", "startOffset": 91, "endOffset": 186}, {"referenceID": 8, "context": "Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem.", "startOffset": 119, "endOffset": 138}, {"referenceID": 10, "context": "(2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework.", "startOffset": 78, "endOffset": 104}, {"referenceID": 0, "context": ", 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al.", "startOffset": 24, "endOffset": 355}, {"referenceID": 0, "context": ", 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework.", "startOffset": 24, "endOffset": 618}, {"referenceID": 0, "context": ", 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology.", "startOffset": 24, "endOffset": 780}, {"referenceID": 0, "context": ", 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and S\u00f8gaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach.", "startOffset": 24, "endOffset": 951}, {"referenceID": 11, "context": "For example, Collobert et al. (2011) use a CNN over", "startOffset": 13, "endOffset": 37}, {"referenceID": 17, "context": "Huang et al. (2015) use hand-crafted features with LSTMs to improve performance.", "startOffset": 0, "endOffset": 20}, {"referenceID": 38, "context": ", 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a).", "startOffset": 67, "endOffset": 172}, {"referenceID": 40, "context": ", 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a).", "startOffset": 67, "endOffset": 172}, {"referenceID": 24, "context": ", 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a).", "startOffset": 67, "endOffset": 172}, {"referenceID": 20, "context": "powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015).", "startOffset": 176, "endOffset": 194}, {"referenceID": 22, "context": "We introduce an end-to-end model to categorize scientific keyphrases, building on a neural named entity recognition model (Lample et al., 2016) and adding a feature-based embedding.", "startOffset": 122, "endOffset": 143}, {"referenceID": 21, "context": "Therefore, instead of making independent tagging decisions for each output, we model them jointly using conditional random field (Lafferty et al., 2001).", "startOffset": 129, "endOffset": 152}, {"referenceID": 26, "context": "Label Propagation We use prior-regularized measure propagation (Liu and Kirchhoff, 2014; Subramanya and Bilmes, 2011) to propagate labels from the annotated data to their neighbors in the graph.", "startOffset": 63, "endOffset": 117}, {"referenceID": 43, "context": "Label Propagation We use prior-regularized measure propagation (Liu and Kirchhoff, 2014; Subramanya and Bilmes, 2011) to propagate labels from the annotated data to their neighbors in the graph.", "startOffset": 63, "endOffset": 117}, {"referenceID": 44, "context": "The first strategy (called GRAPHINTERP) is the commonly used approach (Subramanya et al., 2010; Aliannejadi et al., 2014) that interpolates the smoothed posterior {q} with CRF marginals p:", "startOffset": 70, "endOffset": 121}, {"referenceID": 1, "context": "The first strategy (called GRAPHINTERP) is the commonly used approach (Subramanya et al., 2010; Aliannejadi et al., 2014) that interpolates the smoothed posterior {q} with CRF marginals p:", "startOffset": 70, "endOffset": 121}, {"referenceID": 20, "context": "A similar idea has been previously used in treating partially labeled data (Kim et al., 2015).", "startOffset": 75, "endOffset": 93}, {"referenceID": 15, "context": "The first baseline (Gupta and Manning, 2011) is an unsupervised method to extract keyphrases by initially using seed patterns in a dependency tree, and then adding to seed patterns through bootstrap-", "startOffset": 19, "endOffset": 44}, {"referenceID": 45, "context": "The second baseline (Tsai et al., 2013) improves the work of Gupta and Manning (2011) by adding Named Entity Features and use different set of seed patterns.", "startOffset": 20, "endOffset": 39}, {"referenceID": 15, "context": ", 2013) improves the work of Gupta and Manning (2011) by adding Named Entity Features and use different set of seed patterns.", "startOffset": 29, "endOffset": 54}, {"referenceID": 37, "context": "We use Stanford CoreNLP (Manning et al., 2014) tokenizer to tokenize words.", "startOffset": 24, "endOffset": 46}, {"referenceID": 5, "context": "Table 1 reports the results of our neural sequence tagging model NN-CRF in both supervised and semi-supervised learning (ULM and graph-based), and compares them with the baselines and the state-of-the-art (best SemEval System (Augenstein et al., 2017)).", "startOffset": 226, "endOffset": 251}], "year": 2017, "abstractText": "This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.", "creator": "LaTeX with hyperref package"}}}