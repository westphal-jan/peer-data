{"id": "1708.06131", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "Evasion Attacks against Machine Learning at Test Time", "abstract": "in security - sensitive analysis applications, knowing the success of machine learning depends on a thorough vetting of their resistance attributes to adversarial data. in one accurately pertinent, well - motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating incoming attack avoidance samples. in this work, we present readers a simple but effective gradient - based approach that can consistently be exploited to systematically assess the security of several, mostly widely - used model classification algorithms defending against evasion attacks. following all a recently proposed framework for security evaluation, we simulate attack prevention scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate other attack samples. this gives the classifier designer a better picture of the classifier performance under evasion risk attacks, and thereby allows him to perform a more informed model selection ( or parameter setting ). we evaluate our approach on the relevant security task targets of malware detection in pdf database files, and show that such systems can be easily evaded. we also sketch illustrate some countermeasures essentially suggested by our analysis.", "histories": [["v1", "Mon, 21 Aug 2017 09:55:37 GMT  (177kb,D)", "http://arxiv.org/abs/1708.06131v1", "In this paper, in 2013, we were the first to introduce the notion of evasion attacks (adversarial examples) created with high confidence (instead of minimum-distance misclassifications), and the notion of surrogate learners (substitute models). These two concepts are now widely re-used in developing attacks against deep networks (even if not always referring to the ideas reported in this work). arXiv admin note: text overlap witharXiv:1401.7727"]], "COMMENTS": "In this paper, in 2013, we were the first to introduce the notion of evasion attacks (adversarial examples) created with high confidence (instead of minimum-distance misclassifications), and the notion of surrogate learners (substitute models). These two concepts are now widely re-used in developing attacks against deep networks (even if not always referring to the ideas reported in this work). arXiv admin note: text overlap witharXiv:1401.7727", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["battista biggio", "igino corona", "davide maiorca", "blaine nelson", "nedim srndic", "pavel laskov", "giorgio giacinto", "fabio roli"], "accepted": false, "id": "1708.06131"}, "pdf": {"name": "1708.06131.pdf", "metadata": {"source": "CRF", "title": "Evasion attacks against machine learning at test time", "authors": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim \u0160rndi\u0107", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "emails": ["roli}@diee.unica.it,", "bnelson@cs.uni-potsdam.de", "pavel.laskov}@uni-tuebingen.de"], "sections": [{"heading": null, "text": "Keywords: adversarial machine learning, evasion attacks, support vector machines, neural networks"}, {"heading": "1 Introduction", "text": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21]. Due to their intrinsic adversarial nature, these applications differ from the classical machine learning setting in which the underlying data distribution is assumed to be stationary. To the contrary, in security-sensitive applications, samples (and,\nar X\niv :1\n70 8.\n06 13\n1v 1\n[ cs\n.C R\nthus, their distribution) can be actively manipulated by an intelligent, adaptive adversary to confound learning; e.g., to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19]. This has led to an arms race between the designers of learning systems and their adversaries, which is evidenced by the increasing complexity of modern attacks and countermeasures. For these reasons, classical performance evaluation techniques are not suitable to reliably assess the security of learning algorithms, i.e., the performance degradation caused by carefully crafted attacks [5].\nTo better understand the security properties of machine learning systems in adversarial settings, paradigms from security engineering and cryptography have been adapted to the machine learning field [2,5,14]. Following common security protocols, the learning system designer should use proactive protection mechanisms that anticipate and prevent the adversarial impact. This requires (i) finding potential vulnerabilities of learning before they are exploited by the adversary; (ii) investigating the impact of the corresponding attacks (i.e., evaluating classifier security); and (iii) devising appropriate countermeasures if an attack is found to significantly degrade the classifier\u2019s performance.\nTwo approaches have previously addressed security issues in learning. The min-max approach assumes the learner and attacker\u2019s loss functions are antagonistic, which yields relatively simple optimization problems [10,12]. A more general game-theoretic approach applies for non-antagonistic losses; e.g., a spam filter wants to accurately identify legitimate email while a spammer seeks to boost his spam\u2019s appeal. Under certain conditions, such problems can be solved using a Nash equilibrium approach [7,8]. Both approaches provide a secure counterpart to their respective learning problems; i.e., an optimal anticipatory classifier.\nRealistic constraints, however, are too complex and multi-faceted to be incorporated into existing game-theoretic approaches. Instead, we investigate the vulnerabilities of classification algorithms by deriving evasion attacks in which the adversary aims to avoid detection by manipulating malicious test samples.4 We systematically assess classifier security in attack scenarios that exhibit increasing risk levels, simulated by increasing the attacker\u2019s knowledge of the system and her ability to manipulate attack samples. Our analysis allows a classifier designer to understand how the classification performance of each considered model degrades under attack, and thus, to make more informed design choices.\nThe problem of evasion at test time was addressed in prior work, but limited to linear and convex-inducing classifiers [9,19,22]. In contrast, the methods presented in Sections 2 and 3 can generally evade linear or non-linear classifiers using a gradient-descent approach inspired by Golland\u2019s discriminative directions technique [13]. Although we focus our analysis on widely-used classifiers such as Support Vector Machines (SVMs) and neural networks, our approach is applicable to any classifier with a differentiable discriminant function.\n4 Note that other kinds of attacks are possible, e.g., if the adversary can manipulate the training data. A comprehensive taxonomy of attacks can be found in [2,14].\nThis paper is organized as follows. We present the evasion problem in Section 2 and our gradient-descent approach in Section 3. In Section 4 we first visually demonstrate our attack on the task of handwritten digit recognition, and then show its effectiveness on a realistic application related to the detection of PDF malware. Finally in Section 5, we summarize our contributions, discuss possibilities for improving security, and suggest future extensions of this work."}, {"heading": "2 Optimal evasion at test time", "text": "We consider a classification algorithm f : X 7\u2192 Y that assigns samples represented in some feature space x \u2208 X to a label in the set of predefined classes y \u2208 Y = {\u22121,+1}, where \u22121 (+1) represents the legitimate (malicious) class. The classifier f is trained on a dataset D = {xi, yi}ni=1 sampled from an underlying distribution p(X, Y ). The label yc = f(x) given by a classifier is typically obtained by thresholding a continuous discriminant function g : X 7\u2192 R. In the sequel, we use yc to refer to the label assigned by the classifier as opposed to the true label y. We further assume that f(x) = \u22121 if g(x) < 0, and +1 otherwise."}, {"heading": "2.1 Adversary model", "text": "To motivate the optimal attack strategy for evasion, it is necessary to disclose one\u2019s assumptions of the adversary\u2019s knowledge and ability to manipulate the data. To this end, we exploit a general model of the adversary that elucidates specific assumptions about adversary\u2019s goal, knowledge of the system, and capability to modify the underlying data distribution. The considered model is part of a more general framework investigated in our recent work [5], which subsumes evasion and other attack scenarios. This model can incorporate application-specific constraints in the definition of the adversary\u2019s capability, and can thus be exploited to derive practical guidelines for developing the optimal attack strategy.\nAdversary\u2019s goal. As suggested by Laskov and Kloft [17], the adversary\u2019s goal should be defined in terms of a utility (loss) function that the adversary seeks to maximize (minimize). In the evasion setting, the attacker\u2019s goal is to manipulate a single (without loss of generality, positive) sample that should be misclassified. Strictly speaking, it would suffice to find a sample x such that g(x) < \u2212 for any > 0; i.e., the attack sample only just crosses the decision boundary.5 Such attacks, however, are easily thwarted by slightly adjusting the decision threshold. A better strategy for an attacker would thus be to create a sample that is misclassified with high confidence; i.e., a sample minimizing the value of the classifier\u2019s discriminant function, g(x), subject to some feasibility constraints.\nAdversary\u2019s knowledge. The adversary\u2019s knowledge about her targeted learning system may vary significantly. Such knowledge may include:\n\u2013 the training set or part of it;\n5 This is also the setting adopted in previous work [9,19,22].\n\u2013 the feature representation of each sample; i.e., how real objects such as emails, network packets are mapped into the classifier\u2019s feature space; \u2013 the type of a learning algorithm and the form of its decision function; \u2013 the (trained) classifier model; e.g., weights of a linear classifier; \u2013 or feedback from the classifier; e.g., classifier labels for samples chosen by\nthe adversary.\nAdversary\u2019s capability. In the evasion scenario, the adversary\u2019s capability is limited to modifications of test data; i.e.altering the training data is not allowed. However, under this restriction, variations in attacker\u2019s power may include:\n\u2013 modifications to the input data (limited or unlimited); \u2013 modifications to the feature vectors (limited or unlimited); \u2013 or independent modifications to specific features (the semantics of the input\ndata may dictate that certain features are interdependent).\nMost of the previous work on evasion attacks assumes that the attacker can arbitrarily change every feature [8,10,12], but they constrain the degree of manipulation, e.g., limiting the number of modifications, or their total cost. However, many real domains impose stricter restrictions. For example, in the task of PDF malware detection [20,24,25], removal of content is not feasible, and content addition may cause correlated changes in the feature vectors."}, {"heading": "2.2 Attack scenarios", "text": "In the sequel, we consider two attack scenarios characterized by different levels of adversary\u2019s knowledge of the attacked system discussed below.\nPerfect knowledge (PK). In this setting, we assume that the adversary\u2019s goal is to minimize g(x), and that she has perfect knowledge of the targeted classifier; i.e., the adversary knows the feature space, the type of the classifier, and the trained model. The adversary can transform attack points in the test data but must remain within a maximum distance of dmax from the original attack sample. We use dmax as parameter in our evaluation to simulate increasingly pessimistic attack scenarios by giving the adversary greater freedom to alter the data.\nThe choice of a suitable distance measure d : X \u00d7 X 7\u2192 R+ is application specific [9,19,22]. Such a distance measure should reflect the adversary\u2019s effort required to manipulate samples or the cost of these manipulations. For example, in spam filtering, the attacker may be bounded by a certain number of words she can manipulate, so as not to lose the semantics of the spam message.\nLimited knowledge (LK). Here, we again assume that the adversary aims to minimize the discriminant function g(x) under the same constraint that each transformed attack point must remain within a maximum distance of dmax from the corresponding original attack sample. We further assume that the attacker knows the feature representation and the type of the classifier, but does not know either the learned classifier f or its training data D, and hence can not directly\ncompute g(x). However, we assume that she can collect a surrogate dataset D\u2032 = {x\u0302i, y\u0302i} nq i=1 of nq samples drawn from the same underlying distribution p(X, Y ) from which D was drawn. This data may be collected by an adversary in several ways; e.g., by sniffing some network traffic during the classifier operation, or by collecting legitimate and spam emails from an alternate source.\nUnder this scenario, the adversary proceeds by approximating the discriminant function g(x) as g\u0302(x), where g\u0302(x) is the discriminant function of a surrogate classifier f\u0302 learnt on D\u2032. The amount of the surrogate data, nq, is an attack parameter in our experiments. Since the adversary wants her surrogate f\u0302 to closely approximate the targeted classifier f , it stands to reason that she should learn f\u0302 using the labels assigned by the targeted classifier f , when such feedback is available. In this case, instead of using the true class labels y\u0302i to train f\u0302 , the adversary can query f with the samples of D\u2032 and subsequently learn using the labels y\u0302ci = f(x\u0302i) for each xi."}, {"heading": "2.3 Attack strategy", "text": "Under the above assumptions, for any target malicious sample x0 (the adversary\u2019s desired instance), an optimal attack strategy finds a sample x\u2217 to minimize g(\u00b7) or its estimate g\u0302(\u00b7), subject to a bound on its distance6 from x0:\nx\u2217 = arg min x g\u0302(x) (1)\ns.t. d(x,x0) \u2264 dmax.\nGenerally, this is a non-linear optimization problem. One may approach it with many well-known techniques, like gradient descent, or quadratic techniques such as Newton\u2019s method, BFGS, or L-BFGS. We choose a gradient-descent procedure. However, g\u0302(x) may be non-convex and descent approaches may not achieve a global optima. Instead, the descent path may lead to a flat region (local minimum) outside of the samples\u2019 support (i.e., where p(x) \u2248 0) where the attack sample may or may not evade depending on the behavior of g in this unsupported region (see left and middle plots in Figure 1).\nLocally optimizing g\u0302(x) with gradient descent is particularly susceptible to failure due to the nature of a discriminant function. Besides its shape, for many classifiers, g(x) is equivalent to a posterior estimate p(yc = \u22121|x); e.g., for neural networks, and SVMs [23]. The discriminant function does not incorporate the evidence we have about the data distribution, p(x), and thus, using gradient descent to optimize Eq. 1 may lead into unsupported regions (p(x) \u2248 0). Because of the insignificance of these regions, the value of g is relatively unconstrained by criteria such as risk minimization. This problem is compounded by our finite (and possibly small) training set, since it provides little evidence in these regions\n6 One can also incorporate additional application-specific constraints on the attack samples. For instance, the box constraint 0 \u2264 xf \u2264 1 can be imposed if the f th feature is normalized in [0, 1], or x0f \u2264 xf can be used if the f th feature of the target x0 can be only incremented.\nto constrain the shape of g. Thus, when our gradient descent procedure produces an evasion example in these regions, the attacker cannot be confident that this sample will actually evade the corresponding classifier. Therefore, to increase the probability of successful evasion, the attacker should favor attack points from densely populated regions of legitimate points, where the estimate g\u0302(x) is more reliable (closer to the real g(x)), and tends to become negative in value.\nTo overcome this shortcoming, we introduce an additional component into our attack objective, which estimates p(x|yc = \u22121) using a density estimator. This term acts as a penalizer for x in low density regions and is weighted by a parameter \u03bb \u2265 0 yielding the following modified optimization problem:\narg min x F (x) = g\u0302(x)\u2212 \u03bb n \u2211 i|yci=\u22121 k ( x\u2212xi h ) (2)\ns.t. d(x,x0) \u2264 dmax , (3)\nwhere h is a bandwidth parameter for a kernel density estimator (KDE), and n is the number of benign samples (yc = \u22121) available to the adversary. This\nAlgorithm 1 Gradient-descent evasion attack Input: x0, the initial attack point; t, the step size; \u03bb, the trade-off parameter; > 0 a small constant. Output: x\u2217, the final attack point.\n1: m\u2190 0. 2: repeat 3: m\u2190 m+ 1 4: Set \u2207F (xm\u22121) to a unit vector aligned with \u2207g(xm\u22121)\u2212 \u03bb\u2207p(xm\u22121|yc = \u22121). 5: xm \u2190 xm\u22121 \u2212 t\u2207F (xm\u22121) 6: if d(xm,x0) > dmax then 7: Project xm onto the boundary of the feasible region. 8: end if 9: until F (xm)\u2212 F ( xm\u22121 ) <\n10: return: x\u2217 = xm\nalternate objective trades off between minimizing g\u0302(x) (or p(yc = \u22121|x)) and maximizing the estimated density p(x|yc = \u22121). The extra component favors attack points that imitate features of known legitimate samples. In doing so, it reshapes the objective function and thereby biases the resulting gradient descent towards regions where the negative class is concentrated (see the bottom plot in Fig. 1). This produces a similar effect to that shown by mimicry attacks in network intrusion detection [11].7 For this reason, although our setting is rather different, in the sequel we refer to this extra term as the mimicry component.\nFinally, we point out that, when mimicry is used (\u03bb > 0), our gradient descent clearly follows a suboptimal path compared to the case when only g(x) is minimized (\u03bb = 0). Therefore, more modifications may be required to reach the same value of g(x) attained when \u03bb = 0. However, as previously discussed, when \u03bb = 0, our descent approach may terminate at a local minimum where g(x) > 0, without successfully evading detection. This behavior can thus be qualitatively regarded as a trade-off between the probability of evading the targeted classifier and the number of times that the adversary must modify her samples."}, {"heading": "3 Gradient descent attacks", "text": "Algorithm 1 solves the optimization problem in Eq. 2 via gradient descent. We assume g(x) to be differentiable almost everywhere (subgradients may be used at discontinuities). However, note that if g is non-differentiable or insufficiently smooth, one may still use the mimicry / KDE term of Eq. (2) as a search heuristic. This investigation is left to future work.\n7 Mimicry attacks [11] consist of camouflaging malicious network packets to evade anomaly-based intrusion detection systems by mimicking the characteristics of the legitimate traffic distribution."}, {"heading": "3.1 Gradients of discriminant functions", "text": "Linear classifiers. Linear discriminant functions are g(x) = \u3008w,x\u3009 + b where w \u2208 Rd is the feature weights and b \u2208 R is the bias. Its gradient is \u2207g(x) = w. Support vector machines. For SVMs, g(x) = \u2211 i \u03b1iyik(x,xi) + b. The gradi-\nent is thus\u2207g(x) = \u2211 i \u03b1iyi\u2207k(x,xi). In this case, the feasibility of our approach depends on whether the kernel gradient \u2207k(x,xi) is computable as it is for many numeric kernels. For instance, the gradient of the RBF kernel, k(x,xi) = exp{\u2212\u03b3\u2016x \u2212 xi\u20162}, is \u2207k(x,xi) = \u22122\u03b3 exp{\u2212\u03b3\u2016x \u2212 xi\u20162}(x \u2212 xi), and for the polynomial kernel, k(x,xi) = (\u3008x,xi\u3009+ c)p, it is \u2207k(x,xi) = p(\u3008x,xi\u3009+ c)p\u22121xi.\nNeural networks. For a multi-layer perceptron with a single hidden layer of m neurons and a sigmoidal activation function, we decompose its discriminant function g as follows (see Fig. 2): g(x) = (1+e\u2212h(x))\u22121, h(x) = \u2211m k=1 wk\u03b4k(x)+b, \u03b4k(x) = (1 + e \u2212hk(x))\u22121, hk(x) = \u2211d j=1 vkjxj + bk. From the chain rule, the i th component of \u2207g(x) is thus given by: \u2202g \u2202xi = \u2202g\u2202h \u2211m k=1 \u2202h \u2202\u03b4k \u2202\u03b4k \u2202hk \u2202hk \u2202xi = g(x)(1\u2212 g(x)) \u2211m k=1 wk\u03b4k(x)(1\u2212 \u03b4k(x))vki ."}, {"heading": "3.2 Gradients of kernel density estimators", "text": "Similarly to SVMs, the gradient of kernel density estimators depends on the kernel gradient. We consider generalized RBF kernels of the form k ( x\u2212xi h ) =\nexp ( \u2212d(x,xi)h ) , where d(\u00b7, \u00b7) is any suitable distance function. Here we use the same distance d(\u00b7, \u00b7) defined in Eq. (3), but, in general, they can be different. For `2- and `1-norms (i.e., RBF and Laplacian kernels), the KDE (sub)gradients are respectively given by:\n\u2212 2nh \u2211 i|yci=\u22121 exp ( \u2212\u2016x\u2212xi\u2016 2 2 h ) (x\u2212 xi) ,\n\u2212 1nh \u2211 i|yci=\u22121 exp ( \u2212\u2016x\u2212xi\u20161h ) (x\u2212 xi) .\nNote that the scaling factor here is proportional to O( 1nh ). Therefore, to influence gradient descent with a significant mimicking effect, the value of \u03bb in the objective function should be chosen such that the value of \u03bbnh is comparable with (or higher than) the range of values of the discriminant function g\u0302(x)."}, {"heading": "3.3 Descent in discrete spaces", "text": "In discrete spaces, gradient approaches travel through infeasible portions of the feature space. In such cases, we need to find a feasible neighbor x that maximally decrease F (x). A simple approach to this problem is to probe F at every point in a small neighborhood of x, which would however require a large number of queries. For classifiers with a differentiable decision function, we can instead select the neighbor whose change best aligns with \u2207F (x) and decreases the objective function; i.e., to prevent overshooting a minimum."}, {"heading": "4 Experiments", "text": "In this section, we first report a toy example from the MNIST handwritten digit classification task [18] to visually demonstrate how the proposed algorithm modifies digits to mislead classification. We then show the effectiveness of the proposed attack on a more realistic and practical scenario: the detection of malware in PDF files."}, {"heading": "4.1 A toy example on handwritten digits", "text": "Similar to Globerson and Roweis [12], we consider discriminating between two distinct digits from the MNIST dataset [18]. Each digit example is represented as a gray-scale image of 28 \u00d7 28 pixels arranged in raster-scan-order to give feature vectors of d = 28\u00d7 28 = 784 values. We normalized each feature (pixel) x \u2208 [0, 1]d by dividing its value by 255, and we constrained the attack samples to this range. Accordingly, we optimized Eq. (2) subject to 0 \u2264 xf \u2264 1 for all f .\nWe only consider the perfect knowledge (PK) attack scenario. We used the Manhattan distance (`1-norm), d, both for the kernel density estimator (i.e., a Laplacian kernel) and for the constraint d(x,x0) \u2264 dmax in Eq. (3), which bounds the total difference between the gray level values of the original image x0 and the attack image x. We used dmax = 5000 255 to limit the total gray-level change to 5000. At each iteration, we increased the `1-norm value of x\u2212 x0 by 10 255 , or equivalently, we changed the total gray level by 10. This is effectively the gradient step size. The targeted classifier was an SVM with the linear kernel and C = 1. We randomly chose 100 training samples and applied the attacks to a correctly-classified positive sample.\nIn Fig. 3 we illustrate gradient attacks in which a \u201c3\u201d is to be misclassified as a \u201c7\u201d. The left image shows the initial attack point, the middle image shows the first attack image misclassified as legitimate, and the right image shows the attack point after 500 iterations. When \u03bb = 0, the attack images exhibit only\na weak resemblance to the target class \u201c7\u201d but are, nevertheless, reliably misclassified. This is the same effect demonstrated in the top-left plot of Fig. 1: the classifier is evaded by making the attack sample sufficiently dissimilar from the malicious class. Conversely, when \u03bb = 10, the attack images strongly resemble the target class because the mimicry term favors samples that are more similar to the target class. This is the same effect seen in the bottom plot of Fig. 1.\nFinally note that, as expected, g(x) tends to decrease more gracefully when mimicry is used, as we follow a suboptimal descent path. Since the targeted classifier can be easily evaded when \u03bb = 0, exploiting the mimicry component would not be the optimal choice in this case. However, in the case of limited knowledge, as discussed at the end of Section 2.3, mimicry may allow us to trade for a higher probability of evading the targeted classifier, at the expense of a higher number of modifications."}, {"heading": "4.2 Malware detection in PDF files", "text": "We now focus on the task of discriminating between legitimate and malicious PDF files, a popular medium for disseminating malware [26]. PDF files are excellent vectors for malicious-code, due to their flexible logical structure, which can described by a hierarchy of interconnected objects. As a result, an attack can be easily hidden in a PDF to circumvent file-type filtering. The PDF format further allows a wide variety of resources to be embedded in the document including JavaScript, Flash, and even binary programs. The type of the embedded object is specified by keywords, and its content is in a data stream. Several recent\nworks proposed machine-learning techniques for detecting malicious PDFs using the file\u2019s logical structure to accurately identify the malware [20,24,25]. In this case study, we use the feature representation of Maiorca et al. [20] in which each feature corresponds to the tally of occurrences of a given keyword.\nThe PDF structure imposes natural constraints on attacks. Although it is difficult to remove an embedded object (and its keywords) from a PDF without corrupting the PDF\u2019s file structure, it is rather easy to insert new objects (and, thus, keywords) through the addition of a new version to the PDF file [1]. In our feature representation, this is equivalent to allowing only feature increments, i.e., requiring x0 \u2264 x as an additional constraint in the optimization problem given by Eq. (2). Further, the total difference in keyword counts between two samples is their Manhattan distance, which we again use for the kernel density estimator and the constraint in Eq. (3). Accordingly, dmax is the maximum number of additional keywords that an attacker can add to the original x0.\nExperimental setup. For experiments, we used a PDF corpus with 500 malicious samples from the Contagio dataset8 and 500 benign samples collected from the web. We randomly split the data into five pairs of training and testing sets with 500 samples each to average the final results. The features (keywords) were extracted from each training set as described in [20]. On average, 100 keywords were found in each run. Further, we also bounded the maximum value of each feature to 100, as this value was found to be close to the 95th percentile for each feature. This limited the influence of outlying samples.\nWe simulated the perfect knowledge (PK) and the limited knowledge (LK) scenarios described in Section 2.1. In the LK case, we set the number of samples used to learn the surrogate classifier to ng = 100. The reason is to demonstrate that even with a dataset as small as the 20% of the original training set size, the adversary may be able to evade the targeted classifier with high reliability. Further, we assumed that the adversary uses feedback from the targeted classifier f ; i.e., the labels y\u0302ci = f(x\u0302i) for each surrogate sample x\u0302i \u2208 D\u2032.9\nAs discussed in Section 3.2, the value of \u03bb is chosen according to the scale of the discriminant function g(x), the bandwidth parameter h of the kernel density estimator, and the number of legitimate samples n in the surrogate training set. For computational reasons, to estimate the value of the KDE at x, we only consider the 50 nearest (legitimate) training samples to x; therefore, n \u2264 50 in our case. The bandwidth parameter was set to h = 10, as this value provided a proper rescaling of the Manhattan distances observed in our dataset for the KDE. We thus set \u03bb = 500 to be comparable with O(nh).\nFor each targeted classifier and training/testing pair, we learned five surrogate classifiers by randomly selecting ng samples from the test set, and we averaged their results. For SVMs, we sought a surrogate classifier that would correctly match the labels from the targeted classifier; thus, we used parameters C = 100, and \u03b3 = 0.1 (for the RBF kernel) to heavily penalize training errors.\n8 http://contagiodump.blogspot.it 9 Similar results were also obtained using the true labels (without relabeling), since\nthe targeted classifiers correctly classified almost all samples in the test set.\nExperimental results. We report our results in Figure 4, in terms of the false negative (FN) rate attained by the targeted classifiers as a function of the maximum allowable number of modifications, dmax \u2208 [0, 50]. We compute the FN rate corresponding to a fixed false positive (FP) rate of FP= 0.5%. For dmax = 0, the FN rate corresponds to a standard performance evaluation using unmodified PDFs. As expected, the FN rate increases with dmax as the PDF is increasingly modified. Accordingly, a more secure classifier will exhibit a more graceful increase of the FN rate.\nResults for \u03bb = 0. We first investigate the effect of the proposed attack in the PK case, without considering the mimicry component (Figure 4, first column), for varying parameters of the considered classifiers. The linear SVM (Figure 4, top-left plot) is almost always evaded with as few as 5 to 10 modifications, independent of the regularization parameter C. It is worth noting that attacking a linear classifier amounts to always incrementing the value of the same highestweighted feature (corresponding to the /Linearized keyword in the majority of the cases) until it reaches its upper bound. This continues with the next highest weighted non-bounded feature until termination. This occurs simply because the gradient of g(x) does not depend on x for a linear classifier (see Section 3.1). With the RBF kernel (Figure 4, middle-left plot), SVMs exhibit a similar behavior with C = 1 and various values of its \u03b3 parameter,10 and the RBF SVM provides a higher degree of security compared to linear SVMs (cf. top-left plot and middle-left plot in Figure 4). Interestingly, compared to SVMs, neural networks (Figure 4, bottom-left plot) seem to be much more robust against the proposed evasion attack. This behavior can be explained by observing that the decision function of neural networks may be characterized by flat regions (i.e., regions where the gradient of g(x) is close to zero). Hence, the gradient descent algorithm based solely on g(x) essentially stops after few attack iterations for most of the malicious samples, without being able to find a suitable attack.\nIn the LK case, without mimicry, classifiers are evaded with a probability only slightly lower than that found in the PK case, even when only ng = 100 surrogate samples are used to learn the surrogate classifier. This aspect highlights the threat posed by a skilled adversary with incomplete knowledge: only a small set of samples may be required to successfully attack the target classifier using the proposed algorithm.\nResults for \u03bb = 500. When mimicry is used (Figure 4, second column), the success of the evasion of linear SVMs (with C = 1) decreases both in the PK (e.g., compare the blue curve in the top-left plot with the solid blue curve in the top-right plot) and LK case (e.g., compare the dashed red curve in the top-left plot with the dashed blue curve in the top-right plot). The reason is that the computed direction tends to lead to a slower descent; i.e., a less direct path that often requires more modifications to evade the classifier. In the non-linear case (Figure 4, middle-right and bottom-right plot), instead, mimicking exhibits some beneficial aspects for the attacker, although the constraint on feature addition\n10 We also conducted experiments using C = 0.1 and C = 100, but did not find significant differences compared to the presented results using C = 1.\nmay make it difficult to properly mimic legitimate samples. In particular, note how the targeted SVMs with RBF kernel (with C = 1 and \u03b3 = 1) in the PK case (e.g., compare the solid blue curve in the middle-left plot with the solid blue curve in the middle-right plot) is evaded with a significantly higher probability than in the case of \u03bb = 0. The reason is that, as explained at the end of Section 2.3, a pure descent strategy on g(x) may find local minima (i.e., attack samples) that do not evade detection, while the mimicry component biases the descent towards regions of the feature space more densely populated by legitimate samples, where g(x) eventually attains lower values. For neural networks, this aspect is even more evident, in both the PK and LK settings (compare the dashed/solid curves in the bottom-left plot with those in the bottom-right plot), since g(x) is essentially flat far from the decision boundary, and thus pure gradient descent on g can not even commence for many malicious samples, as previously mentioned. In this case, the mimicry term is thus critical for finding a reasonable descent path to evasion.\nDiscussion. Our attacks raise questions about the feasibility of detecting malicious PDFs solely based on logical structure. We found that /Linearized, /OpenAction, /Comment, /Root and /PageLayout were among the most commonly manipulated keywords. They indeed are found mainly in legitimate PDFs, but can be easily added to malicious PDFs by the versioning mechanism. The attacker can simply insert comments inside the malicious PDF file to augment its /Comment count. Similarly, she can embed legitimate OpenAction code to add /OpenAction keywords or add new pages to insert /PageLayout keywords."}, {"heading": "5 Conclusions, limitations and future work", "text": "In this work we proposed a simple algorithm for evasion of classifiers with differentiable discriminant functions. We investigated the attack effectiveness in the case of perfect and limited knowledge of the attacked system, and empirically showed that very popular classification algorithms (in particular, SVMs and neural networks) can still be evaded with high probability even if the adversary can only learn a copy of the classifier from a small surrogate dataset. Thus, our investigation raises important questions on whether such algorithms can be reliably employed in security-sensitive applications.\nWe believe that the proposed attack formulation can be extended to classifiers with non-differentiable discriminant functions as well, such as decision trees and k-nearest neighbors; e.g., by defining suitable search heuristics similar to our mimicry term to minimize g(x).\nInterestingly our analysis also suggests improvements for classifier security. From Fig. 1, it is clear that a tighter enclosure of the legitimate samples increasingly forces the adversary to mimic the legitimate class, which may not always be possible; e.g., malicious network packets or PDF files must contain a valid exploit for the attack to be successful. Accordingly, more secure classifiers can be designed by employing regularization terms that promote enclosure of the legitimate class; e.g., by penalizing \u201cblind spots\u201d - regions with low p(x) - classified\nas legitimate. Alternatively, one may explicitly model the attack distribution, as in [4]; or add the generated attack samples to the training set. Nevertheless, improving security probably must be balanced with a higher FP rate.\nIn our example applications, the feature representations could be inverted to obtain a corresponding real-world objects (e.g., spam emails, or PDF files); i.e., it is straightforward to manipulate the given real-world object to obtain the desired feature vector x\u2217 of the optimal attack. However, in practice some complex feature mappings can not be easily inverted; e.g., n-gram features [11]. Another idea would be to modify the real-world object at each step of the gradient descent to obtain a sample in the feature space which is as close as possible to the sample that would be obtained at the next attack iteration. A similar technique has been already exploited by [6] to overcome the pre-image problem.\nOther interesting extensions of our work may be to (i) consider more effective strategies such as those proposed by [19,22] to build a small but representative set of surrogate data; and (ii) improve the classifier estimate g\u0302(x). To this end, one may exploit ensemble techniques such as bagging or the random subspace method to train several classifiers and then average their output.\nAcknowledgments. This work has been partly supported by the project CRP18293, L.R. 7/2007, Bando 2009, and by the project \u201cAdvanced and secure sharing of multimedia data over social networks in the future Internet\u201d (CUP F71J11000690002), both funded by Regione Autonoma della Sardegna. Davide Maiorca gratefully acknowledges Regione Autonoma della Sardegna for the financial support of his PhD scholarship. Blaine Nelson thanks the Alexander von Humboldt Foundation for providing additional financial support. The opinions expressed in this paper are solely those of the authors and do not necessarily reflect the opinions of any sponsor."}], "references": [{"title": "Can machine learning be secure", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "ASIACCS \u201906: Proc. of the 2006 ACM Symp. on Information, computer and comm. security. pp. 16\u201325", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Int\u2019l J. of Machine Learning and Cybernetics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Design of robust classifiers for adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics (SMC). pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Trans. on Knowl. and Data Eng. 99(PrePrints),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "J. (eds.) 29th Int\u2019l Conf. on Mach. Learn", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["M. Br\u00fcckner", "T. Scheffer"], "venue": "In: Knowl. Disc. and D. Mining (KDD). pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "J. Mach. Learn. Res", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "ACM SIGKDD Int\u2019l Conf. on Knowl. Discovery and Data Mining (KDD). pp", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Learning to classify with missing and corrupted features", "author": ["O. Dekel", "O. Shamir", "L. Xiao"], "venue": "Mach. Learn", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "Proc. 15th Conf. on USENIX Sec. Symp. USENIX", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S.T. Roweis"], "venue": "Proc. of the 23rd Int\u2019l Conf. on Mach. Learn", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Discriminative direction for kernel classifiers", "author": ["P. Golland"], "venue": "Neu. Inf. Proc. Syst. (NIPS). pp", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "ACM Workshop on Art. Int. and Sec. (AISec", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "Proc. of the 13th Int\u2019l Conf. on Art. Int. and Stats. (AISTATS). pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Feature weighting for improved classifier robustness", "author": ["A. Kolcz", "C.H. Teo"], "venue": "Sixth Conf. on Email and Anti-Spam (CEAS)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "A framework for quantitative security analysis of machine learning", "author": ["P. Laskov", "M. Kloft"], "venue": "AISec \u201909: Proc. of the 2nd ACM works. on Sec. and art. int. pp. 1\u20134. ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. M\u00fcller", "E. S\u00e4ckinger", "P. Simard", "V. Vapnik"], "venue": "In: Int\u2019l Conf. on Art. Neu. Net. pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proc. of the Eleventh ACM SIGKDD Int\u2019l Conf. on Knowl. Disc. and D. Mining (KDD)", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "A pattern recognition system for malicious pdf files detection", "author": ["D. Maiorca", "G. Giacinto", "I. Corona"], "venue": "In: MLDM. pp", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Exploiting machine learning to subvert your spam", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C. Sutton", "J.D. Tygar", "K. Xia"], "venue": "filter. In: LEET\u201908: Proc. of the 1st Usenix Work. on L.-S. Exp. and Emerg. Threats", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J.D. Tygar"], "venue": "J. Mach. Learn. Res", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods", "author": ["J. Platt"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "Malicious pdf detection using metadata and structural features", "author": ["C. Smutz", "A. Stavrou"], "venue": "Proc. of the 28th Annual Comp. Sec. App. Conf.. pp", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Detection of malicious pdf files based on hierarchical document structure", "author": ["N. \u0160rndi\u0107", "P. Laskov"], "venue": "Proc. 20th Annual Net. & Dist. Sys. Sec. Symp", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "IBM X-force mid-year trend & risk report", "author": ["R. Young"], "venue": "Tech. rep.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 3, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 7, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 9, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 12, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 13, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 14, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 17, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 19, "context": "Machine learning is being increasingly used in security-sensitive applications such as spam filtering, malware detection, and network intrusion detection [3,5,9,11,14,15,16,19,21].", "startOffset": 154, "endOffset": 179}, {"referenceID": 1, "context": ", to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19].", "startOffset": 139, "endOffset": 150}, {"referenceID": 7, "context": ", to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19].", "startOffset": 139, "endOffset": 150}, {"referenceID": 14, "context": ", to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19].", "startOffset": 139, "endOffset": 150}, {"referenceID": 17, "context": ", to avoid detection, spam emails are often modified by obfuscating common spam words or inserting words associated with legitimate emails [3,9,16,19].", "startOffset": 139, "endOffset": 150}, {"referenceID": 3, "context": ", the performance degradation caused by carefully crafted attacks [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "To better understand the security properties of machine learning systems in adversarial settings, paradigms from security engineering and cryptography have been adapted to the machine learning field [2,5,14].", "startOffset": 199, "endOffset": 207}, {"referenceID": 3, "context": "To better understand the security properties of machine learning systems in adversarial settings, paradigms from security engineering and cryptography have been adapted to the machine learning field [2,5,14].", "startOffset": 199, "endOffset": 207}, {"referenceID": 12, "context": "To better understand the security properties of machine learning systems in adversarial settings, paradigms from security engineering and cryptography have been adapted to the machine learning field [2,5,14].", "startOffset": 199, "endOffset": 207}, {"referenceID": 8, "context": "The min-max approach assumes the learner and attacker\u2019s loss functions are antagonistic, which yields relatively simple optimization problems [10,12].", "startOffset": 142, "endOffset": 149}, {"referenceID": 10, "context": "The min-max approach assumes the learner and attacker\u2019s loss functions are antagonistic, which yields relatively simple optimization problems [10,12].", "startOffset": 142, "endOffset": 149}, {"referenceID": 5, "context": "Under certain conditions, such problems can be solved using a Nash equilibrium approach [7,8].", "startOffset": 88, "endOffset": 93}, {"referenceID": 6, "context": "Under certain conditions, such problems can be solved using a Nash equilibrium approach [7,8].", "startOffset": 88, "endOffset": 93}, {"referenceID": 7, "context": "The problem of evasion at test time was addressed in prior work, but limited to linear and convex-inducing classifiers [9,19,22].", "startOffset": 119, "endOffset": 128}, {"referenceID": 17, "context": "The problem of evasion at test time was addressed in prior work, but limited to linear and convex-inducing classifiers [9,19,22].", "startOffset": 119, "endOffset": 128}, {"referenceID": 20, "context": "The problem of evasion at test time was addressed in prior work, but limited to linear and convex-inducing classifiers [9,19,22].", "startOffset": 119, "endOffset": 128}, {"referenceID": 11, "context": "In contrast, the methods presented in Sections 2 and 3 can generally evade linear or non-linear classifiers using a gradient-descent approach inspired by Golland\u2019s discriminative directions technique [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "A comprehensive taxonomy of attacks can be found in [2,14].", "startOffset": 52, "endOffset": 58}, {"referenceID": 12, "context": "A comprehensive taxonomy of attacks can be found in [2,14].", "startOffset": 52, "endOffset": 58}, {"referenceID": 3, "context": "The considered model is part of a more general framework investigated in our recent work [5], which subsumes evasion and other attack scenarios.", "startOffset": 89, "endOffset": 92}, {"referenceID": 15, "context": "As suggested by Laskov and Kloft [17], the adversary\u2019s goal should be defined in terms of a utility (loss) function that the adversary seeks to maximize (minimize).", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "5 This is also the setting adopted in previous work [9,19,22].", "startOffset": 52, "endOffset": 61}, {"referenceID": 17, "context": "5 This is also the setting adopted in previous work [9,19,22].", "startOffset": 52, "endOffset": 61}, {"referenceID": 20, "context": "5 This is also the setting adopted in previous work [9,19,22].", "startOffset": 52, "endOffset": 61}, {"referenceID": 6, "context": "Most of the previous work on evasion attacks assumes that the attacker can arbitrarily change every feature [8,10,12], but they constrain the degree of manipulation, e.", "startOffset": 108, "endOffset": 117}, {"referenceID": 8, "context": "Most of the previous work on evasion attacks assumes that the attacker can arbitrarily change every feature [8,10,12], but they constrain the degree of manipulation, e.", "startOffset": 108, "endOffset": 117}, {"referenceID": 10, "context": "Most of the previous work on evasion attacks assumes that the attacker can arbitrarily change every feature [8,10,12], but they constrain the degree of manipulation, e.", "startOffset": 108, "endOffset": 117}, {"referenceID": 18, "context": "For example, in the task of PDF malware detection [20,24,25], removal of content is not feasible, and content addition may cause correlated changes in the feature vectors.", "startOffset": 50, "endOffset": 60}, {"referenceID": 22, "context": "For example, in the task of PDF malware detection [20,24,25], removal of content is not feasible, and content addition may cause correlated changes in the feature vectors.", "startOffset": 50, "endOffset": 60}, {"referenceID": 23, "context": "For example, in the task of PDF malware detection [20,24,25], removal of content is not feasible, and content addition may cause correlated changes in the feature vectors.", "startOffset": 50, "endOffset": 60}, {"referenceID": 7, "context": "The choice of a suitable distance measure d : X \u00d7 X 7\u2192 R is application specific [9,19,22].", "startOffset": 81, "endOffset": 90}, {"referenceID": 17, "context": "The choice of a suitable distance measure d : X \u00d7 X 7\u2192 R is application specific [9,19,22].", "startOffset": 81, "endOffset": 90}, {"referenceID": 20, "context": "The choice of a suitable distance measure d : X \u00d7 X 7\u2192 R is application specific [9,19,22].", "startOffset": 81, "endOffset": 90}, {"referenceID": 21, "context": ", for neural networks, and SVMs [23].", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "This produces a similar effect to that shown by mimicry attacks in network intrusion detection [11].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "7 Mimicry attacks [11] consist of camouflaging malicious network packets to evade anomaly-based intrusion detection systems by mimicking the characteristics of the legitimate traffic distribution.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "In this section, we first report a toy example from the MNIST handwritten digit classification task [18] to visually demonstrate how the proposed algorithm modifies digits to mislead classification.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "Similar to Globerson and Roweis [12], we consider discriminating between two distinct digits from the MNIST dataset [18].", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "Similar to Globerson and Roweis [12], we consider discriminating between two distinct digits from the MNIST dataset [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "We now focus on the task of discriminating between legitimate and malicious PDF files, a popular medium for disseminating malware [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "works proposed machine-learning techniques for detecting malicious PDFs using the file\u2019s logical structure to accurately identify the malware [20,24,25].", "startOffset": 142, "endOffset": 152}, {"referenceID": 22, "context": "works proposed machine-learning techniques for detecting malicious PDFs using the file\u2019s logical structure to accurately identify the malware [20,24,25].", "startOffset": 142, "endOffset": 152}, {"referenceID": 23, "context": "works proposed machine-learning techniques for detecting malicious PDFs using the file\u2019s logical structure to accurately identify the malware [20,24,25].", "startOffset": 142, "endOffset": 152}, {"referenceID": 18, "context": "[20] in which each feature corresponds to the tally of occurrences of a given keyword.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The features (keywords) were extracted from each training set as described in [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Alternatively, one may explicitly model the attack distribution, as in [4]; or add the generated attack samples to the training set.", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": ", n-gram features [11].", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "A similar technique has been already exploited by [6] to overcome the pre-image problem.", "startOffset": 50, "endOffset": 53}, {"referenceID": 17, "context": "Other interesting extensions of our work may be to (i) consider more effective strategies such as those proposed by [19,22] to build a small but representative set of surrogate data; and (ii) improve the classifier estimate \u011d(x).", "startOffset": 116, "endOffset": 123}, {"referenceID": 20, "context": "Other interesting extensions of our work may be to (i) consider more effective strategies such as those proposed by [19,22] to build a small but representative set of surrogate data; and (ii) improve the classifier estimate \u011d(x).", "startOffset": 116, "endOffset": 123}], "year": 2017, "abstractText": "In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker\u2019s knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.", "creator": "LaTeX with hyperref package"}}}