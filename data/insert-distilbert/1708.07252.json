{"id": "1708.07252", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "A Study on Neural Network Language Modeling", "abstract": "mainly an exhaustive study on neural network language modeling ( nnlm ) is performed in this paper. different architectures of basic neural network language models are described broadly and examined. a number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional simultaneous recurrent neural network ( birnn ), are studied separately, and the advantages and disadvantages of every technique are evaluated. then, the limits of neural network language modeling are explored from by the aspects of model architecture and knowledge representation. part of the total statistical information from a word sequence will loss when it is processed word by word in a certain order, and alternatively the mechanism of training neural network analyses by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of nnlm. for knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by training word sequences in modifying a natural language. finally, some strategic directions for improving neural network language modeling further is discussed.", "histories": [["v1", "Thu, 24 Aug 2017 02:14:50 GMT  (125kb,D)", "http://arxiv.org/abs/1708.07252v1", "20 pages, 6 figures"]], "COMMENTS": "20 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["dengliang shi"], "accepted": false, "id": "1708.07252"}, "pdf": {"name": "1708.07252.pdf", "metadata": {"source": "CRF", "title": "A Study on Neural Network Language Modeling", "authors": ["Dengliang Shi", "D. Shi"], "emails": ["dengliang.shi@yahoo.com"], "sections": [{"heading": null, "text": "Keywords: neural network language modeling, optimization techniques, limits, improvement scheme"}, {"heading": "1. Introduction", "text": "Generally, a well-designed language model makes a critical difference in various natural language processing (NLP) tasks, like speech recognition (Hinton et al., 2012; Graves et al., 2013a), machine translation (Cho et al., 2014a; Wu et al., 2016), semantic extraction (Collobert and Weston, 2007, 2008) and etc. Language modeling (LM), therefore, has been the research focus in NLP field all the time, and a large number of sound research results have been published in the past decades. N-gram based LM (Goodman, 2001a), a non-parametric approach, is used to be state of the art, but now a parametric method - neural network language modeling (NNLM) is considered to show better performance and more potential over other LM techniques, and becomes the most commonly used LM technique in multiple NLP tasks.\nAlthough some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al. (2003) and did not show prominent advantages over other techniques of LM until recurrent neural network (RNN) was investigated for NNLM (Mikolov et al., 2010, 2011). After more than a decade\u2019s research, numerous improvements, marginal or critical, over basic NNLM have been proposed. However, the existing experimental results of these techniques are not com-\nc\u00a9 Dengliang Shi.\nar X\niv :1\n70 8.\n07 25\n2v 1\n[ cs\n.C L\n] 2\n4 A\nug 2\n01 7\nparable because they were obtained under different experimental setups and, sometimes, these techniques were evaluated combined with other different techniques. Another significant problem is that most researchers focus on achieving a state of the art language model, but the limits of NNLM are rarely studied. In a few works (Jozefowicz et al., 2016) on exploring the limits of NNLM, only some practical issues, like computational complexity, corpus, vocabulary size, and etc., were dealt with, and no attention was spared on the effectiveness of modeling a natural language using NNLM.\nSince this study focuses on NNLM itself and does not aim at raising a state of the art language model, the techniques of combining neural network language models with other kind of language models, like N-gram based language models, maximum entropy (ME) language models and etc., will not be included. The rest of this paper is organized as follows: In next section, the basic neural network language models - feed-forward neural network language model (FNNLM), recurrent neural network language model (RNNLM) and long-short term memory (LSTM) RNNLM, will be introduced, including the training and evaluation of these models. In the third section, the details of some important NNLM techniques, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), will be described, and experiments will be performed on them to examine their advantages and disadvantages separately. The limits of NNLM, mainly about the aspects of model architecture and knowledge representation, will be explored in the fourth section. A further work section will also be given to represent some further researches on NNLM. In last section, a conclusion about the findings in this paper will be made."}, {"heading": "2. Basic Neural Network Language Models", "text": "The goal of statistical language models is to estimate the probability of a word sequence w1w2...wT in a natural language, and the probability can be represented by the production of the conditional probability of every word given all the previous ones:\nP (wT1 ) = T\u220f t=1 P (wt|wt\u221211 )\nwhere, wji = wiwi+1 . . . wj\u22121wj . This chain rule is established on the assumption that words in a word sequence only statistically depend on their previous context and forms the foundation of all statistical language modeling. NNLM is a kind of statistical language modeling, so it is also termed as neural probabilistic language modeling or neural statistical language modeling. According to the architecture of used ANN, neural network language models can be classified as: FNNLM, RNNLM and LSTM-RNNLM."}, {"heading": "2.1 Feed-forward Neural Network Language Model, FNNLM", "text": "As mentioned above, the objective of FNNLM is to evaluate the conditional probability P (wt|wt\u221211 ), but feed-forward neural network (FNN) lacks of an effective way to represent history context. Hence, the idea of n-gram based LM is adopted in FNNLM that words in a word sequence more statistically depend on the words closer to them, and only the n\u2212 1\ndirect predecessor words are considered when evaluating the conditional probability, this is:\nP (wt|wt\u221211 ) \u2248 P (wt|w t\u22121 t\u2212n+1)\nThe architecture of the original FNNLM proposed by Bengio et al. (2003) is showed in Figure 1, and w0, wT+1 are the start and end marks of a word sequence respectively. In this model, a vocabulary is pre-built from a training data set, and every word in this vocabulary is assigned with a unique index. To evaluate the conditional probability of word wt, its n\u22121 direct previous words wt\u2212n+1, ..., wt\u22121 are projected linearly into feature vectors using a shared matrix C \u2208 Rk\u00d7m according to their index in the vocabulary, where k is the size of the vocabulary and m is the feature vectors\u2019 dimension. In fact, every row of projection matrix C is a feature vector of a word in the vocabulary. The input x \u2208 Rni of FNN is formed by concatenating the feature vectors of words wt\u2212n+1, ..., wt\u22121, where ni = m\u00d7 (n\u2212 1) is the size of FNN\u2019s input layer. FNN can be generally represented as:\ny = V \u00b7 f(U \u00b7 x + b) + M \u00b7 x + d\nWhere, U \u2208 Rnh\u00d7ni , V \u2208 Rno\u00d7nh are weight matrixes, nh is the size of hidden layer, no = k is the size of output layer, weight matrix M \u2208 Rno\u00d7ni is for the direct connections between input layer and output layer, b \u2208 Rnh and d \u2208 Rno are vectors for bias terms in hidden layer and output layer respectively, y \u2208 Rno is output vector, and f(\u00b7) is activation function.\nThe i-th element of output vector y is the unnormalized conditional probability of the word with index i in the vocabulary. In order to guarantee all the conditional probabilities of words positive and summing to one, a softmax layer is always adopted following the output layer of FNN:\nP (vi|wt\u221211 ) \u2248 P (vi|w t\u22121 t\u2212n+1) = ey(vi,w t\u22121 t\u2212n+1)\u2211k\nj=1 e y(vj ,w\nt\u22121 t\u2212n+1)\n, i = 1, 2, ..., k\nwhere y(vi, w t\u22121 t\u2212n+1) (i = 1, 2, ..., k) is the i-th element of output vector y , and vi is the i-th word in the vocabulary.\nTraining of neural network language models is usually achieved by maximizing the penalized log-likelihood of the training data:\nL = 1\nT T\u2211 t=1 log(P (wt|wt\u221211 ; \u03b8)) + R(\u03b8)\nwhere, \u03b8 is the set of model\u2019s parameters to be trained, R(\u03b8) is a regularization term. The recommended learning algorithm for neural network language models is stochastic gradient descent (SGD) method using backpropagation (BP) algorithm. A common choice for the loss function is the cross entroy loss which equals to negative log-likelihood here. The parameters are usually updated as:\n\u03b8 = \u03b8 + \u03b1 \u2202L\n\u2202\u03b8 \u2212 \u03b2\u03b8\nwhere, \u03b1 is learning rate and \u03b2 is regularization parameter. The performance of neural network language models is usually measured using perplexity (PPL) which can be defined as:\nPPL = T \u221a\u221a\u221a\u221a T\u220f i=1\n1\nP (wi|wi\u221211 ) = 2\u2212 1 T\n\u2211T i=1 log2P (wi|w i\u22121 1 )\nPerplexity can be defined as the exponential of the average number of bits required to encode the test data using a language model and lower perplexity indicates that the language model is closer to the true model which generates the test data."}, {"heading": "2.2 Recurrent Neural Network Language Model, RNNLM", "text": "The idea of applying RNN in LM was proposed much earlier (Bengio et al., 2003; Castro and Prat, 2003), but the first serious attempt to build a RNNLM was made by Mikolov et al. (2010, 2011). RNNs are fundamentally different from feed-forward architectures in the sense that they operate on not only an input space but also an internal state space, and the state space enables the representation of sequentially extended dependencies. Therefore, arbitrary length of word sequence can be dealt with using RNNLM, and all previous context can be taken into account when predicting next word. As showed in Figure 2, the representation of words in RNNLM is the same as that of FNNLM, but the input of RNN at every step is the feature vector of a direct previous word instead of the concatenation of the n \u2212 1 previous words\u2019 feature vectors and all other previous words are taken into account by the internal state of previous step. At step t, RNN can be described as:\nst = f(U \u00b7 x t + W \u00b7 st\u22121 + b), y t = V \u00b7 st + M \u00b7 x t + d\nwhere, weight matrix W \u2208 Rnh\u00d7nh , and the input layer\u2019s size of RNN ni = m. The outputs of RNN are also unnormalized probabilities and should be regularized using a softmax layer.\nBecause of the involvement of previous internal state at every step, back-propagation through time (BPTT) algorithm (Rumelhart et al., 1986) is preferred for better performance\nwhen training RNNLMs. If data set is treated as a single long word sequence, truncated BPTT should be used and back-propagating error gradient through 5 steps is enough, at least for small corpus (Mikolov, 2012). In this paper, neural network language models will all be trained on data set sentence by sentence, and the error gradient will be back-propagated trough every whole sentence without any truncation."}, {"heading": "2.3 Long Short Term Memory RNNLM, LSTM-RNNLM", "text": "Although RNNLM can take all predecessor words into account when predicting next word in a word sequence, but it is quite difficult to be trained over long term dependencies because of the vanishing or exploring problem (Hochreiter and Schmidhuber, 1997). LSTM-RNN was designed aiming at solving this problem, and better performance can be expected by replacing RNN with LSTM-RNN. LSTM-RNNLM was first proposed by Sundermeyer et al. (2012), and the whole architecture is almost the same as RNNLM except the part of neural network. LSTM-RNN was proposed by Hochreiter and Schmidhuber (1997) and was refined and popularized in following works (Gers and Schmidhuber, 2000; Cho et al., 2014b). The general architecture of LSTM-RNN is:\ni t = \u03c3(U i \u00b7 x t + W i \u00b7 st\u22121 + V i \u00b7 ct\u22121 + bi) f t = \u03c3(U f \u00b7 x t + W f \u00b7 st\u22121 + V f \u00b7 ct\u22121 + bf ) g t = f(U \u00b7 x t + W \u00b7 st\u22121 + V \u00b7 ct\u22121 + b) ct = f t \u2217 ct\u22121 + i t \u2217 g t ot = \u03c3(U o \u00b7 x t + W o \u00b7 st\u22121 + V o \u00b7 ct + bo) st = ot \u2217 f(ct) y t = V \u00b7 st + M \u00b7 x t + d\nWhere, i t, f t, ot \u2208 Rnh are input gate, forget gate and output gate, respectively. ct \u2208 Rnh is the internal memory of unit. U i, U f , U o, U \u2208 Rnh\u00d7ni , W i, W f , W o, W \u2208 Rnh\u00d7nh , V i, V f , V o, V \u2208 Rnh\u00d7nh are all weight matrixes. bi, bf , bo, b \u2208 Rnh , and d \u2208 Rno\nare vectors for bias terms. f(\u00b7) is the activation function in hidden layer and \u03c3(\u00b7) is the activation function for gates."}, {"heading": "2.4 Comparison of Neural Network Language Models", "text": "Comparisons among neural network language models with different architectures have already been made on both small and large corpus (Mikolov, 2012; Sundermeyer et al., 2013). The results show that, generally, RNNLMs outperform FNNLMs and the best performance is achieved using LSTM-NNLMs. However, the neural network language models used in these comparisons are optimized using various techniques, and even combined with other kind of language models, let alone the different experimental setups and implementation details, which make the comparison results fail to illustrate the fundamental discrepancy in the performance of neural network language models with different architecture and cannot be taken as baseline for the studies in this paper.\nComparative experiments on neural network language models with different architecture were repeated here. The models in these experiments were all implemented plainly, and only a class-based speed-up technique was used which will be introduced later. Experiments were performed on the Brown Corpus, and the experimental setup for Brown corpus is the same as that in (Bengio et al., 2003), the first 800000 words (ca01\u223ccj54) were used for training, the following 200000 words (cj55\u223ccm06) for validation and the rest (cn01\u223ccr09) for test.\nThe experiment results are showed in Table 1 which suggest that, on a small corpus likes the Brown Corpus, RNNLM and LSTM-RNN did not show a remarkable advantage over FNNLM, instead a bit higher perplexity was achieved by LSTM-RNNLM. Maybe more data is needed to train RNNLM and LSTM-RNNLM because longer dependencies are taken into account by RNNLM and LSTM-RNNLM when predicting next word. LSTMRNNLM with bias terms or direct connections was also evaluated here. When the direct connections between input layer and output layer of LSTM-RNN are enabled, a slightly higher perplexity but shorter training time were obtained. An explanation given for this phenomenon by Bengio et al. (2003) is that direct connections provide a bit more capacity and faster learning of the \u201dlinear\u201d part of mapping from inputs to outputs but impose a negative effect on generalization. For bias terms, no significant improvement on performance was gained by adding bias terms which was also observed on RNNLM by Mikolov (2012). In the rest of this paper, all studies will be performed on LSTM-RNNLM with neither\ndirect connections nor bias terms, and the result of this model in Table 1 will be used as the baseline for the rest studies."}, {"heading": "3. Optimization Techniques", "text": ""}, {"heading": "3.1 Importance Sampling", "text": "Inspired by the contrastive divergence model (Hinton, 2002), Bengio and Senecal (2003b) proposed a sampling-based method to speed up the training of neural network language models. In order to apply this method, the outputs of neural network should be normalized in following way instead of using a softmax function:\nP (vi|wt\u221211 ) = e\u2212y(vi,w t\u22121 1 )\u2211k\nj=1 e \u2212y(vj ,wt\u221211 )\n, i = 1, 2, . . . , k; t = 1, 2, . . . , T\nthen, neural network language models can be treated as a special case of energy-based probability models.\nThe main idea of sampling based method is to approximate the average of log-likelihood gradient with respect to the parameters \u03b8 by samples rather than computing the gradient explicitly. The log-likelihood gradient for the parameters set \u03b8 can be generally represented as the sum of two parts: positive reinforcement for target word wt and negative reinforcement for all word vi, weighted by P (vi|wt\u221211 ):\n\u2202logP (wt|wt\u221211 ) \u2202\u03b8 = \u2212\u2202y(wt, w t\u22121 1 ) \u2202\u03b8 + k\u2211 i=1 P (vi|wt\u221211 ) \u2202y(vi, w t\u22121 1 ) \u2202\u03b8\nThree sampling approximation algorithms were presented by Bengio and Senecal (2003b): Monte-Carlo Algorithm, Independent Metropolis-Hastings Algorithm and Importance Sampling Algorithm. However, only importance sampling worked well with neural network language model. In fact, 19-fold speed-up was achieved during training while no degradation of the perplexities was observed on both training and test data (Bengio and Senecal, 2003b).\nImportance sampling is a Monte-Carlo scheme using an existing proposal distribution, and its estimator can be represented as:\nE[ \u2211 x\u223cP P (x)g(x)] = 1 N \u2211 x\u2032\u2208\u0393 g(x \u2032 ) P (x \u2032 ) Q(x\u2032)\nwhere, Q is an existing proposal distribution, N is the number of samples from Q, \u0393 is the set of samples from Q. Appling importance sampling to the average log-likelihood gradient of negative samples and the denominator of P (vi|wt\u221211 ), then the overall estimator for example (wt, w t\u22121 1 ) using N samples from distribution Q is:\nE[ \u2202logP (wt|wt\u221211 )\n\u2202\u03b8 ] = \u2212\u2202y(wt, w\nt\u22121 1 )\n\u2202\u03b8 +\n\u2211 w\u2032\u2208\u0393 \u2202y(w \u2032 |wt\u221211 ) \u2202\u03b8 e\n\u2212y(w\u2032 ,wt\u221211 )/Q(w \u2032 |wt\u221211 )\u2211\nw\u2032\u2208\u0393 e \u2212y(w\u2032 ,wt\u221211 )/Q(w\u2032 |wt\u221211 )\nIn order to avoid divergence, the sample size N should be increased as training processes which is measured by the effective sample size of importance sampling:\nS = ( \u2211N j=1 rj) 2\u2211N\nj=1 r 2 j\n, rj \u2248 e\u2212y(w\n\u2032 j ,w t\u22121 1 )/Q(w\n\u2032 j |w t\u22121 1 )\u2211\nw\u2032\u2208\u0393 e \u2212y(w\u2032 ,wt\u221211 )/Q(w\u2032 |wt\u221211 )\nAt every iteration, sampling is done block by block with a constant size until the effective sample size S becomes greater than a minimum value, and a full back-propagation will be performed when the sampling size N is greater than a certain threshold.\nThe introduction of importance sampling is just posted here for completeness and no further studies will be performed on it. Because a quick statistical language model which is well trained, like n-gram based language model, is needed to implement importance sampling. In addition, it cannot be applied into RNNLM or LSTM-RNNLM directly and other simpler and more efficient speed-up techniques have been proposed now."}, {"heading": "3.2 Word Classes", "text": "Before the idea of word classes was introduced to NNLM, it had been used in LM extensively for improving perplexities or increasing speed (Brown et al., 1992; Goodman, 2001b). With word classes, every word in vocabulary is assigned to a unique class, and the conditional probability of a word given its history can be decomposed into the probability of the word\u2019s class given its history and the probability of the word given its class and history, this is:\nP (wt|wt\u221211 ) = P (wt|c(wt), w t\u22121 1 )P (c(wt)|w t\u22121 1 )\nwhere c(wt) is the class of word wt. The architecture of class based LSTM-RNNLM is illustrated in Figure 3, and p, q are the lower and upper index of words in a class respectively.\nMorin and Bengio (2005) extended word classes to a hierarchical binary clustering of words and built a hierarchical neural network language model. In hierarchical neural network language model, instead of assigning every word in vocabulary with a unique class, a hierarchical binary tree of words is built according to the word similarity information extracted from WordNet (Fellbaum, 1998), and every word in vocabulary is assigned with a\nbit vector b = [b1(vi), b2(vi), . . . , bl(vi)], i = 1, 2, . . . , k. When b1(vi), b2(vi), . . . , bj\u22121(vi) are given, bj(vi) = 0, j = 1, 2, . . . , l indicates that word vi belongs to the sub-group 0 of current node and bj(vi) = 1 indicates it belongs to the other one. The conditional probability of every word is represented as:\nP (vi|wt\u221211 ) = l\u220f\nj=1\nP (bj(wt)|b1(wt), . . . , bj\u22121(wt), wt\u221211 )\nTheoretically, an exponential speed-up, on the order of k/log2k, can be achieved with this hierarchical architecture. In Morin and Bengio (2005), impressive speed-up during both training and test, which were less than the theoretical one, were obtained but an obvious increase in PPL was also observed. One possible explanation for this phenomenon is that the introduction of hierarchical architecture or word classes impose negative influence on the word classification by neural network language models. As is well known, a distribution representation for words, which can be used to represent the similarities between words, is formed by neural network language models during training. When words are clustered into classes, the similarities between words from different classes cannot be recognized directly. For a hierarchical clustering of words, words are clustered more finely which might lead to worse performance, i.e., higher perplexity, and deeper the hierarchical architecture is, worse the performance would be.\nTo explore this point further, hierarchical LSTM-NNLMs with different number of hierarchical layers were built. In these hierarchical LSTM-NNLMs, words were clustered randomly and uniformly instead of according to any word similarity information. The results of experiment on these models are showed in Table 2 which strongly support the above hypothesis. When words are clustered into hierarchical word classes, the speed of both training and test increase, but the effect of speed-up decreases and the performance declines dramatically as the number of hierarchical layers increases. Lower perplexity can be expected if some similarity information of words is used when clustering words into classes. However, because of the ambiguity of words, the degradation of performance is unavoidable by assigning every word with a unique class or path. On the other hand, the similarities among words recognized by neural network is hard to defined, but it is sure that they are not confined to linguistical ones.\nThere is a simpler way to speed up neural network language models using word classes which was proposed by Mikolov et al. (2011). Words in vocabulary are arranged in descent\norder according to their frequencies in training data set, and are assigned to classes one by one using following rule:\ni r < z\u2211 j=1 fj F \u2264 i+ 1 r , 0 \u2264 i \u2264 r \u2212 1\nwhere, r is the target number of word classes, fj is the frequency of the j-th word in vocabulary, the sum of all words\u2019 frequencies F = \u2211k\nj=1 fj . If the above rule is satisfied, the z-th word in vocabulary will be assigned to i-th class. In this way, the word classes are not uniform, and the first classes hold less words with high frequency and the last ones contain more low-frequency words. This strategy was further optimized by (Mikolov, 2012) using following criterion:\ni r < z\u2211 j=1\n\u221a fj/F\ndF \u2264 i+ 1 r\nwhere, the sum of all words\u2019 sqrt frequencies dF = \u2211k\nj=1\n\u221a fj/F .\nThe experiment results (Table 2) indicate that higher perplexity and a little more training time were obtained when the words in vocabulary were classified according to their frequencies than classified randomly and uniformly. When words are clustered into word classed using their frequency, words with high frequency, which contribute more to final perplexity, are clustered into very small word classes, and this leads to higher perplexity. On the other hand, word classes consist of words with low frequency are much bigger which causes more training time. However, as the experiment results show, both perplexity and training time were improved when words were classified according to their sqrt frequency, because word classes were more uniform when built in this way. All other models in this paper were speeded up using word classes, and words were clustered according to their sqrt frequencies."}, {"heading": "3.3 Caching", "text": "Like word classes, caching is also a common used optimization technique in LM. The cache language models are based on the assumption that the word in recent history are more likely to appear again. In cache language model, the conditional probability of a word is calculated by interpolating the output of standard language model and the probability evaluated by caching, like:\nP (wt|wt\u221210 ) = \u03bbPo(wt|w t\u22121 0 ) + (1\u2212 \u03bb)Pc(wt|w t\u22121 0 )\nwhere, Po(wt|wt\u221210 ) is the output of standard language model, Pc(wt|w t\u22121 0 ) is the probability evaluated using caching, and \u03bb is a constant, 0 \u2264 \u03bb \u2264 1. Soutner et al. (2012) combined FNNLM with cache model to enhance the performance of FNNLM in speech recognition, and the cache model was formed based on the previous context as following:\nPc(wt|wt\u22121t\u2212N ) = 1\nN N\u2211 j=1 \u03c1\u03b4(wt, wt\u2212j)\nwhere, \u03b4(\u00b7) means Kronecker delta, N is the cache length, i.e., the number of previous words taken as cache, \u03c1 is a coefficient depends on j which is the distance between previous word and target word. A cache model with forgetting can be obtained by lowering \u03c1 linearly or exponentially respecting to j. A class cache model was also proposed by Soutner et al. (2012) for the case in which words are clustered into word classes. In class cache model, the probability of target word given the last recent word classes is determined. However, both word based cache model and class one can be defined as a kind of unigram language model built from previous context, and this caching technique is an approach to combine neural network language model with a unigram model.\nAnother type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014). The main idea of this approach is to store the outputs and states of language models for future prediction given the same contextual history. In Huang et al. (2014), four caches were proposed, and they were all achieved by hash lookup tables to store key and value pairs: probability P (wt|wt\u221210 ) and word sequence wt0; history w t\u22121 0 and its corresponding hidden state vector; history w t\u22121 0 and the denominator of the softmax function for classes; history wt\u221210 , class index c(wt) and the denominator of the softmax function for words. In Huang et al. (2014), around 50-fold speed-up was reported with this caching technique in speech recognition but, unfortunately, it only works for prediction and cannot be applied during training.\nInspired by the first caching technique, if the previous context can be taken into account through the internal states of RNN, the perplexity is expected to decrease. In this paper, all language models are trained sentence by sentence, and the initial states of RNN are initialized using a constant vector. This caching technique can be implemented by simply initializing the initial states using the last states of direct previous sentence in the same article. However, the experiment result (Table 3) shows this caching technique did not work as excepted and the perplexity even increased slightly. Maybe, the Brown Corpus is too small and more data is needed to evaluated this caching technique, as more context is taken into account with this caching technique."}, {"heading": "3.4 Bidirectional Recurrent Neural Network", "text": "In Sutskever et al. (2014), significant improvement on neural machine translation (NMT) for an English to French translation task was achieved by reversing the order of input word sequence, and the possible explanation given for this phenomenon was that smaller \u201dminimal time lag\u201d was obtained in this way. In my opinion, another possible explanation is that a word in word sequence may more statistically depend on the following context than previous one. After all, a number of words are determined by its following words instead of previous ones in some natural languages. Take the articles in English as examples, indefinite article\n\u201dan\u201d is used when the first syllable of next word is a vowel while \u201da\u201d is preposed before words starting with consonant. What\u2019s more, if a noun is qualified by an attributive clause, definite article \u201dthe\u201d should be used before the noun. These examples illustrate that words in a word sequence depends on their following words sometimes. To verify this hypothesis further, an experiment is performed here in which the word order of every input sentence is reversed, and the probability of word sequence w1w2. . .wT is evaluated as following:\nP (wT1 ) = T\u220f t=1 P (wt|wTt+1)\nHowever, the experiment result (Table 4) shows that almost the same perplexity was achieved by reversing the order of words. This indicates that the same amount statistical information, but not exactly the same statistical information, for a word in a word sequence can be obtained from its following context as from its previous context, at least for English.\nAs a word in word sequence statistically depends on its both previous and following context, it is better to predict a word using context from its both side. Bidirectional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) was designed to process data in both directions with two separate hidden layers, so better performance can be expected by using BiRNN. BiRNN was introduced to speech recognition by Graves et al. (2013b), and then was evaluated in other NLP tasks, like NMT (Bahdanau et al., 2015; Wu et al., 2016). In these studies, BiRNN showed more excellent performance than unidirectional\nRNN. Nevertheless, BiRNN cannot be evaluated in LM directly as unidirectional RNN, because statistical language modeling is based on the chain rule which assumes that word in a word sequence only statistically depends on one side context. BiRNN can be applied in NLP tasks, like speech recognition and machine translation, because the input word sequences in these tasks are treated as a whole and usually encoded as a single vector. The architecture for encoding input word sequences using BiRNN is showed in Figure 4. The facts that better performance can be achieved using BiRNN in speech recognition or machine translation indicate that a word in a word sequence is statistically determined by the words of its both side, and it is not a suitable way to deal with word sequence in a natural language word by word in an order."}, {"heading": "4. Limits of Neural Network Language Modeling", "text": "NNLM is state of the art, and has been introduced as a promising approach to various NLP tasks. Numerous researchers from different areas of NLP attempt to improve NNLM, expecting to get better performance in their areas, like lower perplexity on test data, less word error rate (WER) in speech recognition, higher Bilingual Evaluation Understudy (BLEU) score in machine translation and etc. However, few of them spares attention on the limits of NNLM. Without a thorough understanding of NNLM\u2019s limits, the applicable scope of NNLM and directions for improving NNLM in different NLP tasks cannot be defined clearly. In this section, the limits of NNLM will be studied from two aspects: model architecture and knowledge representation."}, {"heading": "4.1 Model Architecture", "text": "In most language models including neural network language models, words are predicated one by one according to their previous context or following one which is believed to simulate the way human deal with natural languages, and, according to common sense, human actually speak or write word by word in a certain order. However, the intrinsic mechanism in human mind of processing natural languages cannot like this way. As mentioned above, it is not always true that words in a word sequence only depend on their previous or following context. In fact, before human speaking or writing, they know what they want to express and map their ideas into word sequence, and the word sequence is already cached in memory\nwhen human speaking or writing. In most case, the cached word sequence may be not a complete sentence but at least most part of it. On the other hand, for reading or listening, it is better to know both side context of a word when predicting the meaning of the word or define the grammar properties of the word. Therefore, it is not a good strategy to deal with word sequences in a natural language word by word in a certain order which has also been questioned by the success application of BiRNN in some NLP tasks.\nAnother limit of NNLM caused by model architecture is original from the monotonous architecture of ANN. In ANN, models are trained by updating weight matrixes and vectors which distribute among all nodes. Training will become much more difficult or even unfeasible when increasing the size of model or the variety of connections among nodes, but it is a much efficient way to enhance the performance of ANN. As is well known, ANN is designed by imitating biological neural system, but biological neural system does not share the same limit with ANN. In fact, the strong power of biological neural system is original from the enormous number of neurons and various connections among neurons, including gathering, scattering, lateral and recurrent connections (Nicholls et al., 2011). In biological neural system, the features of signals are detected by different receptors, and encoded by low-level central neural system (CNS) which is changeless. The encoded signals are integrated by high-level CNS. Inspired by this, an improvement scheme for the architecture of ANN is proposed, as illustrated in Figure 5. The features of signal are extracted according to the knowledge in certain field, and every feature is encoded using changeless neural network with careful designed structure. Then, the encoded features are integrated using a trainable neural network which may share the same architecture as existing ones. Because the model for encoding does not need to be trained, the size of this model can be much huge and the structure can be very complexity. If all the parameters of encoding model are designed using binary, it is possible to implement this model using hardware and higher efficiency can be expected."}, {"heading": "4.2 Knowledge Representation", "text": "The word \u201dlearn\u201d appears frequently with NNLM, but what neural network language models learn from training data set is rarely analyzed carefully. The common statement about the knowledge learned by neural network language models is the probabilistic distribution of word sequences in a natural language. Strictly speaking, it is the probabilistic distribution of word sequences from a certain training data set in a natural language, rather than the general one. Hence, the neural network language model trained on data set from a certain field will perform well on data set from the same field, and neural network language model\ntrained on a general data set may show worse performance when tested on data set from a special field. In order to verify this, one million words reviews on electronics and books were extracted from Amazon reviews (He and J.Mcauley, 2016; Mcauley et al., 2015) respectively as data sets from different fields, and 800000 words for training, 100000 words for validation, and the rest for test. In this experiment, two models were trained on training data from electronics reviews and books reviews respectively, and the other one was trained on both. Then, all three models were tested on the two test data sets.\nThe lowest perplexity on each test data set was gained by the model trained on corresponding training data set, instead of the model trained on both training data set (Table 6). The results show that the knowledge represented by a neural network language model is the probabilistic distribution of word sequences from training data set which varies from field to field. Except for the probabilistic distribution of word sequences, the feature vectors of words in vocabulary are also formed by neural network during training. Because of the classification function of neural network, the similarities between words can be observed using these feature vectors. However, the similarities between words are evaluated in a multiple dimensional space by feature vectors and it is hard to know which features of words are taken into account when these vectors are formed, which means words cannot be grouped according to any single feature by the feature vectors. In summary, the knowledge represented by neural network language model is the probabilistic distribution of word sequences from certain training data set and feature vectors for words in vocabulary formed in multiple dimensional space. Neither the knowledge of language itself, like grammar, nor the knowledge conveyed by a language can be gained from neural network language models. Therefore, NNLM can be a good choice for NLP tasks in some special fields where language understanding is not necessary. Language understanding cannot be achieved just with the probabilistic distribution of word sequences in a natural language, and new kind of knowledge representation should be raised for language understanding.\nSince the training of neural network language model is really expensive, it is important for a well-trained neural network language model to keep learning during test or be improved on other training data set separately. However, the neural network language models built so far do not show this capacity. Lower perplexity can be obtained when the parameters of a trained neural network language model are tuned dynamically during test, as showed in Table 5, but this does not mean neural network language model can learn dynamically during test. ANN is just a numerical approximation method in nature, and it approximate the target function, the probabilistic distribution of word sequences for LM, by tuning parameters when trained on data set. The learned knowledge is saved as weight matrixes and vectors. When a trained neural network language model is expected to adaptive to new data set, it should be retrained on both previous training data set and new one. This is another limit of NNLM because of knowledge representation, i.e., neural network language models cannot learn dynamically from new data set."}, {"heading": "5. Future Work", "text": "Various architectures of neural network language models are described and a number of improvement techniques are evaluated in this paper, but there are still something more should be included, like gate recurrent unit (GRU) RNNLM, dropout strategy for address-\ning overfitting, character level neural network language model and ect. In addition, the experiments in this paper are all performed on Brown Corpus which is a small corpus, and different results may be obtained when the size of corpus becomes larger. Therefore, all the experiments in this paper should be repeated on a much larger corpus.\nSeveral limits of NNLM has been explored, and, in order to achieve language understanding, these limits must be overcome. I have not come up with a complete solution yet but some ideas which will be explored further next. First, the architecture showed in Figure 5 can be used as a general improvement scheme for ANN, and I will try to figure out the structure of changeless neural network for encoder. What\u2019s more, word sequences are commonly taken as signals for LM, and it is easy to take linguistical properties of words or sentences as the features of signals. However, it maybe not a proper way to deal with natural languages. Natural languages are not natural but man-made, and linguistical knowledge are also created by human long after natural language appeared. Liguistical knowledge only covers the \u201dright\u201d word sequences in a natural language, but it is common to deal with \u201dwrong\u201d ones in real world. In nature, every natural language is a mechanism of linking voices or signs with objects, both concrete and abstract. Therefore, the proper way to deal with natural languages is to find the relations between special voices or signs and objects, and the features of voices or signs can be defined easier than a natural language itself. Every voice or sign can be encoded as a unique code, vector or matrix, according to its features, and the similarities among voices or signs are indeed can be recognized from their codes. It is really difficult to model the relation between voices or signs and objects at once, and this work should be split into several steps. The first step is to covert voice or sign into characters, i.e., speech recognition or image recognition, but it is achieved using the architecture described in Figure 5."}, {"heading": "6. Conclusion", "text": "In this paper, different architectures of neural network language models were described, and the results of comparative experiment suggest RNNLM and LSTM-RNNLM do not show any advantages over FNNLM on small corpus. The improvements over these models, including importance sampling, word classes, caching and BiRNN, were also introduced and evaluated separately, and some interesting findings were proposed which can help us have a better understanding of NNLM.\nAnother significant contribution in this paper is the exploration on the limits of NNLM from the aspects of model architecture and knowledge representation. Although state of the art performance has been achieved using NNLM in various NLP tasks, the power of NNLM has been exaggerated all the time. The main idea of NNLM is to approximate the\nprobabilistic distribution of word sequences in a natural language using ANN. NNLM can be successfully applied in some NLP tasks where the goal is to map input sequences into output sequences, like speech recognition, machine translation, tagging and ect. However, language understanding is another story. For language understanding, word sequences must be linked with any concrete or abstract objects in real world which cannot be achieved just with this probabilistic distribution.\nAll nodes of neural network in a neural network language model have parameters needed to be tunning during training, so the training of the model will become very difficult or even impossible if the model\u2019s size is too large. However, an efficient way to enhance the performance of a neural network language model is to increase the size of model. One possible way to address this problem is to implement special functions, like encoding, using changeless neural network with special struture. Not only the size of the changeless neural network can be very large, but also the structure can be very complexity. The performance of NNLM, both perplexity and training time, is expected to be improved dramatically in this way."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J.S. Senecal"], "venue": "In AISTATS,", "citeRegEx": "Bengio and Senecal.,? \\Q2003\\E", "shortCiteRegEx": "Bengio and Senecal.", "year": 2003}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "V.J.D. Pietra", "P.V. DeSouza", "J.C. Lai", "R.L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "New directions in connectionist language modeling", "author": ["M.J. Castro", "F. Prat"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Castro and Prat.,? \\Q2003\\E", "shortCiteRegEx": "Castro and Prat.", "year": 2003}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Computer Science,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.M. Van", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Fast semantic extraction using a novel neural network architecture", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the Meeting of the Association for Computational Linguistics,", "citeRegEx": "Collobert and Weston.,? \\Q2007\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2007}, {"title": "A unified architecture for natural language processing - deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Recurrent nets that time and count", "author": ["F.A. Gers", "J. Schmidhuber"], "venue": "In Proceedings of the IEEE-INNS-ENNS International Joint Conference on,", "citeRegEx": "Gers and Schmidhuber.,? \\Q2000\\E", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Classes for fast maximum entropy training", "author": ["J. Goodman"], "venue": "In International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "A bit of progress in language modeling", "author": ["J.D. Goodman"], "venue": "Computer Speech and Langauge,", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. r. Mohamed", "G. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["A. Graves", "N. Jaitly", "A.R. Mohamed"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Ups and downs: modeling the visual evolution of fashion trends with one-class collaborative filtering", "author": ["R. He", "J.Mcauley"], "venue": "In International World Wide Web Conferences Steering Committee,", "citeRegEx": "He and J.Mcauley.,? \\Q2016\\E", "shortCiteRegEx": "He and J.Mcauley.", "year": 2016}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton.,? \\Q2002\\E", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Cache based recurrent neural network language model inference for first pass speech recognition", "author": ["Z. Huang", "G. Zweig", "B. Dumoulin"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "Neural Computation,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Recurrent neural network based language modeling in meeting recognization", "author": ["S. Kombrink", "T. Mikolov", "M. Karafiat", "L. Burget"], "venue": "In INTERSPEECH,", "citeRegEx": "Kombrink et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kombrink et al\\.", "year": 2011}, {"title": "Image-based recommendations on styles and substitutes", "author": ["J. Mcauley", "C. Targett", "Q. Shi", "A.V.D. Hengel"], "venue": "In International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Mcauley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mcauley et al\\.", "year": 2015}, {"title": "Natural language processing with modular pdp neural networks and distributed lexicon", "author": ["R. Miikkulainen", "M.G. Dyer"], "venue": "Cognitive Science,", "citeRegEx": "Miikkulainen and Dyer.,? \\Q1991\\E", "shortCiteRegEx": "Miikkulainen and Dyer.", "year": 1991}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J.H. Cernocky", "S. Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network based language model", "author": ["T. Mikolov", "S. Kombrink", "J.H. Cernocky", "S. Khudanpur"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In Aistats,", "citeRegEx": "Morin and Bengio.,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "From neuron to brain", "author": ["J.G. Nicholls", "A.R. Martin", "P.A. Brown", "M.E. Diamond", "D.A. Weisblat"], "venue": "Sinauer Associates, Inc,", "citeRegEx": "Nicholls et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nicholls et al\\.", "year": 2011}, {"title": "Learning internal representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323:533\u2013536,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Sequential neural neural text compression", "author": ["J. Schmidhuber"], "venue": "IEEE Transactions on Neural Network,", "citeRegEx": "Schmidhuber.,? \\Q1996\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1996}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Prefix tree based n-best list rescoring for recurrent neural network language model used in speech recognition system", "author": ["Y. Si", "Q. Zhang", "T. Li", "J. Pan", "Y. Yan"], "venue": "In INTERSPEECH,", "citeRegEx": "Si et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Si et al\\.", "year": 2013}, {"title": "Neural network language model with cache", "author": ["D. Soutner", "Z. Loose", "L. Muller", "A. Prazak"], "venue": "In International Conference on Text, Speech and Dialogue,", "citeRegEx": "Soutner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soutner et al\\.", "year": 2012}, {"title": "Lstm nerual networks for language modeling", "author": ["M. Sundermeyer", "R. Schluter", "H. Ney"], "venue": "In Interspeech,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Comparison of feedforward and recurrent nerual network language models", "author": ["M. Sundermeyer", "I. Oparin", "J.L. Gauvain", "B. Freiberg", "R. Schluter", "H. Ney"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sundermeyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Google\u2019s neural machine translation system: bridging the gap between human and machine translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "venue": "Computer Science,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Can artificial neural network learn language models", "author": ["W. Xu", "A. Rudnicky"], "venue": "In Proceedings of International Conference on Speech and Language Processing,", "citeRegEx": "Xu and Rudnicky.,? \\Q2000\\E", "shortCiteRegEx": "Xu and Rudnicky.", "year": 2000}], "referenceMentions": [{"referenceID": 18, "context": "Generally, a well-designed language model makes a critical difference in various natural language processing (NLP) tasks, like speech recognition (Hinton et al., 2012; Graves et al., 2013a), machine translation (Cho et al.", "startOffset": 146, "endOffset": 189}, {"referenceID": 38, "context": ", 2013a), machine translation (Cho et al., 2014a; Wu et al., 2016), semantic extraction (Collobert and Weston, 2007, 2008) and etc.", "startOffset": 30, "endOffset": 66}, {"referenceID": 24, "context": "Although some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al.", "startOffset": 32, "endOffset": 103}, {"referenceID": 31, "context": "Although some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al.", "startOffset": 32, "endOffset": 103}, {"referenceID": 39, "context": "Although some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al.", "startOffset": 32, "endOffset": 103}, {"referenceID": 2, "context": "Although some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996; Xu and Rudnicky, 2000) had been made to introduce artificial neural network (ANN) into LM, NNLM began to attract researches\u2019 attentions only after Bengio et al. (2003) and did not show prominent advantages over other techniques of LM until recurrent neural network (RNN) was investigated for NNLM (Mikolov et al.", "startOffset": 228, "endOffset": 249}, {"referenceID": 21, "context": "In a few works (Jozefowicz et al., 2016) on exploring the limits of NNLM, only some practical issues, like computational complexity, corpus, vocabulary size, and etc.", "startOffset": 15, "endOffset": 40}, {"referenceID": 2, "context": "P (wt|w 1 ) \u2248 P (wt|w t\u22121 t\u2212n+1) The architecture of the original FNNLM proposed by Bengio et al. (2003) is showed in Figure 1, and w0, wT+1 are the start and end marks of a word sequence respectively.", "startOffset": 84, "endOffset": 105}, {"referenceID": 3, "context": "2 Recurrent Neural Network Language Model, RNNLM The idea of applying RNN in LM was proposed much earlier (Bengio et al., 2003; Castro and Prat, 2003), but the first serious attempt to build a RNNLM was made by Mikolov et al.", "startOffset": 106, "endOffset": 150}, {"referenceID": 5, "context": "2 Recurrent Neural Network Language Model, RNNLM The idea of applying RNN in LM was proposed much earlier (Bengio et al., 2003; Castro and Prat, 2003), but the first serious attempt to build a RNNLM was made by Mikolov et al.", "startOffset": 106, "endOffset": 150}, {"referenceID": 30, "context": "Because of the involvement of previous internal state at every step, back-propagation through time (BPTT) algorithm (Rumelhart et al., 1986) is preferred for better performance", "startOffset": 116, "endOffset": 140}, {"referenceID": 25, "context": "If data set is treated as a single long word sequence, truncated BPTT should be used and back-propagating error gradient through 5 steps is enough, at least for small corpus (Mikolov, 2012).", "startOffset": 174, "endOffset": 189}, {"referenceID": 19, "context": "3 Long Short Term Memory RNNLM, LSTM-RNNLM Although RNNLM can take all predecessor words into account when predicting next word in a word sequence, but it is quite difficult to be trained over long term dependencies because of the vanishing or exploring problem (Hochreiter and Schmidhuber, 1997).", "startOffset": 262, "endOffset": 296}, {"referenceID": 11, "context": "LSTM-RNN was proposed by Hochreiter and Schmidhuber (1997) and was refined and popularized in following works (Gers and Schmidhuber, 2000; Cho et al., 2014b).", "startOffset": 110, "endOffset": 157}, {"referenceID": 16, "context": "3 Long Short Term Memory RNNLM, LSTM-RNNLM Although RNNLM can take all predecessor words into account when predicting next word in a word sequence, but it is quite difficult to be trained over long term dependencies because of the vanishing or exploring problem (Hochreiter and Schmidhuber, 1997). LSTM-RNN was designed aiming at solving this problem, and better performance can be expected by replacing RNN with LSTM-RNN. LSTM-RNNLM was first proposed by Sundermeyer et al. (2012), and the whole architecture is almost the same as RNNLM except the part of neural network.", "startOffset": 263, "endOffset": 482}, {"referenceID": 16, "context": "3 Long Short Term Memory RNNLM, LSTM-RNNLM Although RNNLM can take all predecessor words into account when predicting next word in a word sequence, but it is quite difficult to be trained over long term dependencies because of the vanishing or exploring problem (Hochreiter and Schmidhuber, 1997). LSTM-RNN was designed aiming at solving this problem, and better performance can be expected by replacing RNN with LSTM-RNN. LSTM-RNNLM was first proposed by Sundermeyer et al. (2012), and the whole architecture is almost the same as RNNLM except the part of neural network. LSTM-RNN was proposed by Hochreiter and Schmidhuber (1997) and was refined and popularized in following works (Gers and Schmidhuber, 2000; Cho et al.", "startOffset": 263, "endOffset": 632}, {"referenceID": 25, "context": "4 Comparison of Neural Network Language Models Comparisons among neural network language models with different architectures have already been made on both small and large corpus (Mikolov, 2012; Sundermeyer et al., 2013).", "startOffset": 179, "endOffset": 220}, {"referenceID": 36, "context": "4 Comparison of Neural Network Language Models Comparisons among neural network language models with different architectures have already been made on both small and large corpus (Mikolov, 2012; Sundermeyer et al., 2013).", "startOffset": 179, "endOffset": 220}, {"referenceID": 3, "context": "Experiments were performed on the Brown Corpus, and the experimental setup for Brown corpus is the same as that in (Bengio et al., 2003), the first 800000 words (ca01\u223ccj54) were used for training, the following 200000 words (cj55\u223ccm06) for validation and the rest (cn01\u223ccr09) for test.", "startOffset": 115, "endOffset": 136}, {"referenceID": 2, "context": "An explanation given for this phenomenon by Bengio et al. (2003) is that direct connections provide a bit more capacity and faster learning of the \u201dlinear\u201d part of mapping from inputs to outputs but impose a negative effect on generalization.", "startOffset": 44, "endOffset": 65}, {"referenceID": 2, "context": "An explanation given for this phenomenon by Bengio et al. (2003) is that direct connections provide a bit more capacity and faster learning of the \u201dlinear\u201d part of mapping from inputs to outputs but impose a negative effect on generalization. For bias terms, no significant improvement on performance was gained by adding bias terms which was also observed on RNNLM by Mikolov (2012). In the rest of this paper, all studies will be performed on LSTM-RNNLM with neither", "startOffset": 44, "endOffset": 384}, {"referenceID": 17, "context": "1 Importance Sampling Inspired by the contrastive divergence model (Hinton, 2002), Bengio and Senecal (2003b) proposed a sampling-based method to speed up the training of neural network language models.", "startOffset": 67, "endOffset": 81}, {"referenceID": 1, "context": "1 Importance Sampling Inspired by the contrastive divergence model (Hinton, 2002), Bengio and Senecal (2003b) proposed a sampling-based method to speed up the training of neural network language models.", "startOffset": 83, "endOffset": 110}, {"referenceID": 1, "context": "Three sampling approximation algorithms were presented by Bengio and Senecal (2003b): Monte-Carlo Algorithm, Independent Metropolis-Hastings Algorithm and Importance Sampling Algorithm.", "startOffset": 58, "endOffset": 85}, {"referenceID": 4, "context": "2 Word Classes Before the idea of word classes was introduced to NNLM, it had been used in LM extensively for improving perplexities or increasing speed (Brown et al., 1992; Goodman, 2001b).", "startOffset": 153, "endOffset": 189}, {"referenceID": 10, "context": "In hierarchical neural network language model, instead of assigning every word in vocabulary with a unique class, a hierarchical binary tree of words is built according to the word similarity information extracted from WordNet (Fellbaum, 1998), and every word in vocabulary is assigned with a", "startOffset": 227, "endOffset": 243}, {"referenceID": 27, "context": "Morin and Bengio (2005) extended word classes to a hierarchical binary clustering of words and built a hierarchical neural network language model.", "startOffset": 0, "endOffset": 24}, {"referenceID": 28, "context": "In Morin and Bengio (2005), impressive speed-up during both training and test, which were less than the theoretical one, were obtained but an obvious increase in PPL was also observed.", "startOffset": 3, "endOffset": 27}, {"referenceID": 25, "context": "There is a simpler way to speed up neural network language models using word classes which was proposed by Mikolov et al. (2011). Words in vocabulary are arranged in descent", "startOffset": 107, "endOffset": 129}, {"referenceID": 25, "context": "This strategy was further optimized by (Mikolov, 2012) using following criterion: i r < z \u2211", "startOffset": 39, "endOffset": 54}, {"referenceID": 34, "context": "Soutner et al. (2012) combined FNNLM with cache model to enhance the performance of FNNLM in speech recognition, and the cache model was formed based on the previous context as following: Pc(wt|w t\u2212N ) = 1 N N \u2211", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014).", "startOffset": 77, "endOffset": 158}, {"referenceID": 22, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014).", "startOffset": 77, "endOffset": 158}, {"referenceID": 33, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014).", "startOffset": 77, "endOffset": 158}, {"referenceID": 20, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014).", "startOffset": 77, "endOffset": 158}, {"referenceID": 29, "context": "A class cache model was also proposed by Soutner et al. (2012) for the case in which words are clustered into word classes.", "startOffset": 41, "endOffset": 63}, {"referenceID": 2, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014). The main idea of this approach is to store the outputs and states of language models for future prediction given the same contextual history. In Huang et al. (2014), four caches were proposed, and they were all achieved by hash lookup tables to store key and value pairs: probability P (wt|w 0 ) and word sequence wt 0; history w t\u22121 0 and its corresponding hidden state vector; history w t\u22121 0 and the denominator of the softmax function for classes; history wt\u22121 0 , class index c(wt) and the denominator of the softmax function for words.", "startOffset": 78, "endOffset": 325}, {"referenceID": 2, "context": "Another type of caching has been proposed as a speed-up technique for RNNLMs (Bengio et al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014). The main idea of this approach is to store the outputs and states of language models for future prediction given the same contextual history. In Huang et al. (2014), four caches were proposed, and they were all achieved by hash lookup tables to store key and value pairs: probability P (wt|w 0 ) and word sequence wt 0; history w t\u22121 0 and its corresponding hidden state vector; history w t\u22121 0 and the denominator of the softmax function for classes; history wt\u22121 0 , class index c(wt) and the denominator of the softmax function for words. In Huang et al. (2014), around 50-fold speed-up was reported with this caching technique in speech recognition but, unfortunately, it only works for prediction and cannot be applied during training.", "startOffset": 78, "endOffset": 725}, {"referenceID": 37, "context": "4 Bidirectional Recurrent Neural Network In Sutskever et al. (2014), significant improvement on neural machine translation (NMT) for an English to French translation task was achieved by reversing the order of input word sequence, and the possible explanation given for this phenomenon was that smaller \u201dminimal time lag\u201d was obtained in this way.", "startOffset": 44, "endOffset": 68}, {"referenceID": 32, "context": "Bidirectional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) was designed to process data in both directions with two separate hidden layers, so better performance can be expected by using BiRNN.", "startOffset": 47, "endOffset": 75}, {"referenceID": 0, "context": "(2013b), and then was evaluated in other NLP tasks, like NMT (Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 61, "endOffset": 101}, {"referenceID": 38, "context": "(2013b), and then was evaluated in other NLP tasks, like NMT (Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 61, "endOffset": 101}, {"referenceID": 13, "context": "BiRNN was introduced to speech recognition by Graves et al. (2013b), and then was evaluated in other NLP tasks, like NMT (Bahdanau et al.", "startOffset": 46, "endOffset": 68}, {"referenceID": 29, "context": "In fact, the strong power of biological neural system is original from the enormous number of neurons and various connections among neurons, including gathering, scattering, lateral and recurrent connections (Nicholls et al., 2011).", "startOffset": 208, "endOffset": 231}, {"referenceID": 16, "context": "In order to verify this, one million words reviews on electronics and books were extracted from Amazon reviews (He and J.Mcauley, 2016; Mcauley et al., 2015) respectively as data sets from different fields, and 800000 words for training, 100000 words for validation, and the rest for test.", "startOffset": 111, "endOffset": 157}, {"referenceID": 23, "context": "In order to verify this, one million words reviews on electronics and books were extracted from Amazon reviews (He and J.Mcauley, 2016; Mcauley et al., 2015) respectively as data sets from different fields, and 800000 words for training, 100000 words for validation, and the rest for test.", "startOffset": 111, "endOffset": 157}], "year": 2017, "abstractText": "An exhaustive study on neural network language modeling (NNLM) is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed.", "creator": "LaTeX with hyperref package"}}}