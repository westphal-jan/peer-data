{"id": "1708.07940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2017", "title": "Navigation Objects Extraction for Better Content Structure Understanding", "abstract": "existing works meant for extracting navigation objects from webpages focus constantly on navigation menus, so special as to reveal the core information architecture of the geographic site. however, web 2. 0 sites such as social networks, e - commerce user portals etc. developments are making the understanding of the content structure in a web site increasingly difficult. dynamic links and personalized elements such as top stories, recommended discussion list sheets in a webpage are vital to the understanding understand of the dynamic nature of web 2. 0 graphic sites. to better understand the desired content structure depicted in web 2. 0 sites, in this experimental paper we propose doing a new extraction method for navigation objects in a webpage. our method will extract not only the static navigation menus, but also the dynamic and personalized page - specific navigation instrument lists. since the navigation objects in a designated webpage naturally come drawn in blocks, we carefully first cluster hyperlinks into different blocks by exploiting spatial locations of hyperlinks, the hierarchical structure of the dom - tree and the hyperlink density.. then we identify navigation objects from those blocks using the svm classifier with novel features such as anchor text lengths etc etc. experiments on real - world data sets with webpages from various domains strengths and styles verified and the effectiveness of our method.", "histories": [["v1", "Sat, 26 Aug 2017 06:59:24 GMT  (943kb,D)", "http://arxiv.org/abs/1708.07940v1", "2017 IEEE/WIC/ACM International Conference on Web Intelligence (WI)"]], "COMMENTS": "2017 IEEE/WIC/ACM International Conference on Web Intelligence (WI)", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["kui zhao", "bangpeng li", "zilun peng", "jiajun bu", "can wang"], "accepted": false, "id": "1708.07940"}, "pdf": {"name": "1708.07940.pdf", "metadata": {"source": "CRF", "title": "Navigation Objects Extraction for Be\u0082er Content Structure Understanding", "authors": ["Kui Zhao", "Bangpeng Li", "Zilun Peng", "Jiajun Bu", "Can Wang"], "emails": ["zhaokui@zju.edu.cn", "bondlee@zju.edu.cn", "zilunpeng@gmail.com", "bjj@zju.edu.cn", "wcan@zju.edu.cn", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Information systems \u2192 Data mining; Data extraction and integration; \u2022Computing methodologies\u2192 Cluster analysis;\nKEYWORDS Web Structure Mining; Information Extraction; Navigation Objects\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. WI \u201917, Leipzig, Germany \u00a9 2017 ACM. 978-1-4503-4951-2/17/08. . .$15.00 DOI: 10.1145/3106426.3106437\nACM Reference format: Kui Zhao, Bangpeng Li, Zilun Peng, Jiajun Bu, and Can Wang. 2017. Navigation Objects Extraction for Be er Content Structure Understanding. In Proceedings of WI \u201917, Leipzig, Germany, August 23-26, 2017, 8 pages. DOI: 10.1145/3106426.3106437"}, {"heading": "1 INTRODUCTION", "text": "e explosive growth of the World Wide Web generates tremendous amount of web data and consequently web data mining has become an important technique for discovering useful information and knowledge. Among many popular topics in web data mining, extracting information architecture or content structures for a web site has a racted many research a ention in recent years. Existing works mainly extract navigation menus from webpages to reveal the content structure of the site [11]. Many applications can be derived from the extracted content structure, including generating site map to improve information accessibility for disabled users, or providing content hierarchy in search results [10] etc.\nHowever, the increasing number of web 2.0 sites such as social networks, e-commerce portals etc. are turning the web from a static information repository into a dynamic platform for information sharing and interactions. As shown in Figure 1, the information architecture on these sites are characterized not only by the traditional static directory structure of the site, but also by the dynamic elements such as the top reading list, recommended items etc. In fact, the dynamic nature of web 2.0 sites are be er captured by these dynamic and personalized elements. But their importance are neglected in existing works of web structure extraction, which mainly focus on extracting static web site structures such as the navigation menus[11], headings[17] etc.\nIn this paper, we propose a new extraction method for navigation objects in a webpage to capture both the static directory structures and the dynamic content structures in a web site. It is a non-trivial task mainly because of the great diversities in webpage structures. Webpages come with various layouts, thus navigation objects in di erent webpages varies greatly in their presentation. Moreover,\nar X\niv :1\n70 8.\n07 94\n0v 1\n[ cs\n.A I]\n2 6\nmany navigation elements in a webpage nowadays are generated dynamically, or customized for speci c users.\nTo overcome these di culties, we a empt to develop a pagedependent extractor for navigation objects in a webpage. Our method is based on following observations for navigation objects, in a typical webpage: 1) the navigation objects are naturally grouped in di erent hyperlink blocks, in which few other contents other than these hyperlinks exist; 2) the anchor text for these hyperlinks are usually short and well aligned. With these observations, the rst step of our method is to cluster hyperlinks in a webpage into multiple blocks by exploiting features such as spatial locations of hyperlinks, the hierarchical structure of the DOM-tree and the hyperlink density etc.\nen we identify navigation objects using the SVM classi er. Generally, the hyperlink blocks in a webpage can be divided into the following four categories:\n\u2022 Navigation Menu. Hyperlinks provide site-level navigation. ey stay relatively invariant and can be directly mapped to the static directory structure in a website. \u2022 Navigation List. Hyperlinks provide page-dependent navigation and capture the dynamic and personalized content structures, such as recommended list etc. \u2022 Content Hyperlink. Hyperlinks appears in the main content. \u2022 Others. Hyperlinks include Ads, copyright information etc.\nObviously, we intend to extract Navigation Menu and Navigation List in a webpage. e SVM classi er is trained with some well de ned features, such as the number of hyperlinks, the mean and the variance of anchor text lengths etc. Experimental results in multiple real-world datasets verify the e ectiveness of our method.\ne rest of the paper is organized as follows. We brie y review related works in section 2. We describe our method in section 3 and 4, the part of clustering hyperlinks into blocks is in section 3 and the part of classifying hyperlink blocks is in section 4. en in section 5 we show our experimental setup and results followed by discussing the results. Finally, we present our conclusions and plans to future research in section 6."}, {"heading": "2 RELATEDWORK", "text": "Our work is related to areas of web structure mining and web information extraction.\nWeb structure mining. Web structure mining aims to study the hyperlink structure of the web. Some early works studied the structure of the web at large [4][12] and uncover the major connected components of the web. Others analyzed the generally\nproperties related with the web graph, such as its diameter [1], size and accessibility of information on the web [14] etc. PageRank [18] exploits the linkage information to learn the importance of webpages and becomes widely used in modern search engines.\nRecent works on web structure mining focused more on the local structures of the web graph. Ravi et al. [13] used the hierarchical structure of URLs to generate hierarchical web site segmentation. ough the hierarchical structure of URLs was also used in many other works, such as [31], the hierarchical structure of URLs does not re ect the web site organization accurately. Eduarda Mendes et al. [23] noticed that and thought navigation objects could re ect the web site structure be er. ey applied frequent item-set algorithms on the outgoing hyperlinks of webpages to detect repeated navigation menus and then used them to represent web sites. Keller et al. [11] also tried to use navigation menus to reveal the information architecture of web sites, but they extracted navigation menus in a very di erent way. ey extracted navigation menus by analyzing maximal cliques on the web graph. Some works do not extract navigation objects directly, but they take into account the structural information navigation objects provide. For instance, when Cindy Xide et al. [15] clustered webpages, they considered parallel links which are siblings in the DOM-tree of a webpage and usually in the same navigation objects. However, these works only focus on the static structure of a web site represented by navigation menus etc. and neglect the dynamic structure represented by personalized page-speci c navigation lists. ese navigation elements is vital to understand the dynamic nature of web 2.0 sites.\nWeb information extraction. Information extraction from webpages has many applications. Most of the existing works focus on main content extraction from webpages and the early work about that can be traced back to Rahman et al. [21]. ey segment the webpages into zones based on its HTML structure and extract important contents by analyzing zone a ributes.\nAmong many di erent categories of extraction methods, templatebased ones are popular because they are highly accurate and easy to implement. ey extracted content from pages with a common template by looking for special HTML cues using regular expressions. A di erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures. e major problem with template-based extractors is that di erent extractors must be developed for di erent templates. What\u2019s more, once the template updates, as frequently happens in many web site, the extractor will be invalidated.\nTo overcome the limitations of template-based methods, many researchers a empted to extract content from webpages in a templateindependent way. Cai et al. [5] proposed a vision-based webpage segmentation algorithm named VIPS to divide a webpage into several blocks by its visual presentation. Zheng et al. [33] presented a template-independent news extraction method based on visual consistency. Wang et al. [28] exploited more features about the relation between the news title and body by rstly extracting the title block and then extracting the body block. Shanchan et al. [30] trained a machine learning model with multiple features generated by utilizing DOM-tree node properties and extracted content using this model. Although these methods extract webpage content in\na template-independent way, they still have to rely on some particular HTML cues (e.g., < table >, < td >, color and font etc) in their extraction, and thus are more easily a ected by the underlying web development technologies. Two recent works, CETR [29] and CETD [25] address this issue by identifying regions with high text density, i.e., regions including many words and few tags are more likely to be main content.\nAs can be seen, most existing works of information extraction from webpages focus on main content extraction and they can not be applied to extracting navigation objects directly. Even the template-based methods cannot be used directly to extract navigation objects because navigation lists in webpages are usually generated dynamically and page-dependent."}, {"heading": "3 CLUSTERING HYPERLINKS", "text": "Our work is motivated by the observation that the navigation objects are naturally grouped in di erent hyperlink blocks according to their purposes. To be er illustrate our idea, we use a typical webpage, the home page of Techweb1 as an example. As shown in Figure 2, the hyperlinks in the webpage are obviously grouped in di erent blocks with their di erent visual presentation features."}, {"heading": "3.1 DOM-tree", "text": "Before clustering hyperlinks in a webpage into blocks, we parse the webpage into a DOM-tree. Each webpage corresponds to a DOM-tree where detailed text, images and hyperlinks etc. are leaf nodes. An example of the DOM-tree is shown in Figure 3. e DOM-tree at the bo om of Figure 3 is derived from the HTML code at the top right, whose webpage layout is at the top le .\ne DOM-tree is a hierarchical structure and it has three useful properties as follows. First, the relation between child node and parent node re ects their relation in the webpage layout, e.g., in Figure 3 the node < p > and < im\u0434 > are child nodes of node < div > re ects that text and image are included in the block corresponding to < div > in the webpage layout. Second, the relative positions of sibling nodes are preserved when they are displaying in the webpage. More speci cally, if node a and node b 1h p://www.techweb.com\nare sibling nodes and a is at the le side of b on the DOM-tree, the displaying element corresponding to a must stay at the le side or the top of the displaying element corresponding tob in the webpage layout. ird, hyperlinks in the same block must have the same ancestor, which is the root node of the smallest sub-tree including that block. e above three properties are very useful when we cluster hyperlinks into blocks on the DOM-tree of a webpage."}, {"heading": "3.2 DOM-tree Distance", "text": "e central problem in clustering hyperlinks is to de ne a reasonable distance between them that well conforms to their visual presentation. e most intuitive choice is the Euclid distance between their locations on the webpage as rendered by browsers. However, obtaining these locations requires expensive computation cost. Moreover, locations for many hyperlinks can not be obtained without user interactions, e.g., in multilevel menus, the displaying locations of hyperlinks in the second or third level menus are only available a er clicking their parent menus.\nTo address this issue, we analyze the structure of the HTML code and use the DOM-tree distance to approximate the distance between two hyperlinks. We rst traverse the DOM-tree of a given webpage with depth- rst search order and index each node we encounter, starting from 1. en we calculate the DOM-tree Distance (DD) between hyperlinks l1 and l2 as follow:\nDD(l1, l2) = |index(l1) \u2212 index(l2)|, (1)\nwhere index(li ) means the index of hyperlink li . For two given\nhyperlink blocks B1 and B2, we de ne the gap between them as the minimum distance between hyperlinks in B1 and B2:\ngap(B1,B2) = min i, j DD(li , lj ), (2)\nwhere li \u2208 B1, lj \u2208 B2. We can use the internal node to represent a hyperlink block, which includes all hyperlink nodes in the corresponding sub-tree. In Figure 4, the node indexed with 2 can represent the hyperlink block including hyperlinks indexed with 6, 8 and the node indexed with 11 can represent the hyperlink block including hyperlink indexed with 12. e gap between these two hyperlink blocks is min{4, 6} = 4."}, {"heading": "3.3 Hyperlink Density", "text": "Another important observation is that a hyperlink block usually includes few text except the text in hyperlinks. We consequently de ne theHyperlink DensityHD(S) for a given layout block S , which consists of one or more sub-trees of a DOM-tree:\nHD(S) = #{anchor text in S} + \u03f5 #{all text in S} + \u03f5 , (3)\nwhere #{anchor text in S} means the word number of the anchor text in all hyperlinks in S , #{all text in S} means the word number of all text in S and \u03f5 is the smoothing parameter to avoid dividing zero. We set \u03f5 = 10\u221210 in all our experiments."}, {"heading": "3.4 Clustering on DOM-tree", "text": "In the process of clustering hyperlinks into blocks, we make good use of the hierarchical structure of the DOM-tree and its properties. e complete algorithm of clustering hyperlinks on the DOM-tree is shown in Algorithm 1 with details.\ne core of our algorithm is a recursive process. For two given hyperlink blocks B1 and B2, in which the hyperlinks have been ensured in the same block respectively. If these two hyperlink blocks have the same parent and are neighbors, we try to merge them. When the gap between hyperlink blocks B1 and B2 is no larger than a given threshold \u0434t and the Hyperlink Density of the potential hyperlink block consisting of B1 and B2 is no smaller than a given threshold hdt , we merge them into one hyperlink block. We only try to merge hyperlink blocks which have the same parent because hyperlinks in the same block should have the same ancestor. We only try to merge hyperlink blocks which are neighbors because the relative positions of sibling nodes are preserved when displaying in the webpage layout.\ne whole process executes from bo om to top on the whole DOM-tree and from le to right on each level of the DOM-tree. We have avoided a lot of useless comparison by making good use of the hierarchical structure and properties of the DOM-tree."}, {"heading": "3.5 reshold", "text": "We use the gap threshold (denoted by\u0434t ) and the Hyperlink Density threshold (denoted by hdt ) to control the results of clustering. Due to the variety of webpages, \u0434t and hdt vary greatly for di erent webpages. So we need an e ective method to learn proper \u0434t and hdt for each webpage.\n3.5.1 Gap threshold. As we have explained in the previous subsection, we only try to merge hyperlink blocks which are neighbors.\nAlgorithm 1 Clustering Hyperlinks on DOM-tree Input: DOM-tree T , hyperlink nodes set H , Gap threshold \u0434t , Hy-\nperlink Density threshold hdt Output: Cluster set C Initialization: C = ;\n1: function Cluster(root ) 2: if (leaf nodes of root )\u2229H is then 3: return TRUE 4: end if 5: cList = []; jList = [] 6: for all child of root from le to right do 7: j = Cluster(child) 8: Append j to jList ; Append child to cList 9: end for\n10: cluster = []; tcList = []; isOne =TRUE 11: cNum \u2190 the length of cList ; s = 1 12: for i = 1\u2192 cNum do 13: Add cList[i] into tcList 14: if (leaf nodes of cList[i])\u2229H is not then 15: if jList[i] is FALSE then 16: Add cluster into C 17: isOne =FALSE; cluster = []; tcList = [] 18: Continue 19: end if 20: if cluster is not empty then 21: \u0434 = gap(cList[s], cList[i]) 22: hd = HD(tcList); s = i 23: if \u0434 > \u0434t or hd < hdt then 24: Add cluster into C 25: isOne =FALSE; cluster = []; tcList = [] 26: end if 27: end if 28: Add (leaf nodes of cList[i])\u2229H into cluster 29: Add cList[i] into tcList 30: end if 31: end for 32: Add cluster into C 33: return isOne 34: end function 35: 36: if Cluster(the root node of T) is TRUE then 37: Add H to C 38: end if\nSo the proper value of \u0434t is among the gaps between all neighbor hyperlink blocks with an additional 0. ough we cannot directly get the set Sb of all gaps between neighbor hyperlink blocks, we can easily get the set Sh of all distances between neighbor hyperlinks and we now prove that Sb = Sh . Firstly, each hyperlink is a hyperlink block which only contains one hyperlink, so Sh \u2282 Sb . Secondly, as de ned in equation (4), the gap between two hyperlink blocks is the minimum distance between hyperlinks in those two hyperlink blocks, which must be the distance between two neighbor hyperlinks, so Sb \u2282 Sh . Above all, Sb = Sh is proved.\nLet DL denote Sh with an additional 0, the problem of calculating \u0434t becomes choosing a proper value from DL:\n(1) e \u0434t should not be too large to avoid clustering all hyperlinks into very few big blocks;\n(2) e \u0434t should not be too small to avoid clustering all hyperlinks into too many small blocks.\nWe choose the following i-th value in DL as \u0434t a er sorting DL in decreasing order:\nargi min ( DLi DL1 + \u03b2 i length(DL) ) (4)\nwhere the DL1 is the maximum value in DL, the length(DL) is the number of values in DL, 1 \u2264 i \u2264 length(DL). ey are used to normalize the value of distance and the number of potential blocks. \u03b2 is a tradeo parameter and we set \u03b2 = 1 in all our experiments.\n3.5.2 Hyperlink density threshold. A hyperlink block includes few text except the text in hyperlinks. Intuitively, since the node with < body > tag is the root node of the DOM-tree and it contains no less other text than each hyperlink block. Let HDB denote the Hyperlink Density of the whole webpage, then\nhdt = \u03b3HDB (5)\nperform the lower bound of Hyperlink Density of hyperlink blocks. \u03b3 \u2265 0 is a tuning parameter and we set \u03b3 = 1 in our experiments."}, {"heading": "4 CLASSIFYING HYPERLINK BLOCKS", "text": "We train a SVM classi er using RBF kernel with some well de ned features to identify navigation objects."}, {"heading": "4.1 Features", "text": "4.1.1 The number of hyperlinks. From our observation, the navigation object usually contains many hyperlinks, while other hyperlink blocks contain less hyperlinks. So the number of hyperlinks is a very useful feature to distinguish navigation object from nonnavigation object. For a given hyperlink block Bi , we denote the number of hyperlinks in it as #Bi .\n4.1.2 Text length in hyperlinks. e length of anchor text is also very useful. On one hand, anchor texts in a navigation object are usually short, while hyperlinks in main content usually have relatively longer texts and hyperlinks in Ads etc. usually contain images without any text. So the mean of text length in a navigation object is usually small but not zero. On the other hand, the text in a navigation object is usually neat and the variance of these text lengths is small. For a given hyperlink block Bi , we denote the mean and variance of the text length in its hyperlinks as Btmi and B tv i respectively. We apply the re-implemented Gaussian smoothing [29] to the text lengths of hyperlinks in a DOM-tree to avoid sudden changes in the text lengths.\nAbove all, for a given hyperlink block Bi , the feature vector of Bi is [#Bi ,Btmi ,B tv i ]. en the SVM classi er with RBF kernel is applied to classify Bi as navigation object or non-navigation object."}, {"heading": "4.2 SVM Classi er", "text": "Support Vector Machine (SVM) is a famous supervised learning model. In order to perform non-linear classi cation, we use the SVM classi er with RBF kernel [6].\nWhen using SVM classi ers, we need to calculate the distance between two points. Since the ranges of di erent features are\nsigni cantly widely di erent, the features are normalized so that each feature contributes approximately in an equal proportion to the nal distance. What\u2019s more, the normalization can also reduce the training time of SVM classi ers [26]."}, {"heading": "5 EXPERIMENT", "text": "Experiments on real world dataset demonstrate the e ectiveness of our method."}, {"heading": "5.1 Date Set", "text": "In our experiments we use data from two sources: (1) dataset from CleanEval[3]; (2) news site data from MSS[19].\nCleanEval: CleanEval is a shared competitive evaluation on the topic of cleaning arbitrary webpages 2. It is a diverse dataset, only a few webpages are used from each site and the sites use various styles and structures. Moreover, this data set has many webpages including dynamic and page-dependent navigation elements.\nMSS: e dataset can be retrieved from Pasternak and Roth\u2019s repository3. is data set contains 45 individual websites which are further separated into two non-overlapping sets. 1) the Big 5: Tribune, Freep, Ny Post, Suntimes and Techweb; 2) the Myriad 40: the webpages which were chosen randomly from the Yahoo! Directory. e Big 5 includes ve most popular news sites and the Myriad 40 contains an international mix of 40 English-language sites of widely varying size and sophistication."}, {"heading": "5.2 Performance Metrics", "text": "5.2.1 Clustering hyperlinks. e results of clustering hyperlinks are identi cations of several hyperlink blocks, and we compared them with the hand-labeled ground truth.\ne rst metric is the Adjusted Rand Index (ARI) [9]. Rand Index (RI) is used to measure the agreement between the output results of clustering and the ground truth[22]. ARI is a adjusted-for-chance version of the Rand Index, which equals 0 on average for random partitions and 1 for two identical partitions. So the larger ARI value means the be er performance.\ne second metric is the Adjusted Mutual Information (AMI) [27]. Mutual Information (MI) is a symmetric measurement for quantifying the statistical information shared between the output results of clustering and the ground truth [8]. AMI is an adjustment of the MI to account chances, it ranges from 0 to 1 and larger value indicates be er performance.\n5.2.2 Classifying hyperlink blocks. e performance of classifying hyperlink blocks is measured by standard metrics. Speci cally, precision, recall and F1-score are calculated by comparing the output of our method against a hand-labeled gold standard.\nPerformances on each dataset are calculated by averaging the scores of above metrics over all webpages. Note that every hyperlink in the webpage is considered as a distinct hyperlink even if some hyperlinks appear multiple times in a webpage.\n2h p://cleaneval.sigwac.org.uk 3h p://cogcomp.cs.illinois.edu/Data/MSS/"}, {"heading": "5.3 Implementation Details", "text": "All programs were implemented in Python language with the help of scikit-learn [20]. A er parsing the HTML le of a webpage into a DOM-tree, we treated all elements with the tag < a > as hyperlinks, including some bu ons and drop-down lists. We kept everything in a webpage without any preprocess, in order to show that our method can handle most noise in the webpage.\n5.3.1 Clustering hyperlinks. In order to properly evaluate the performance of our method on clustering hyperlinks, we compared our method\u2019s performance with several common clustering algorithms, including Agglomeration, DBSCAN, K-Means and Spectral Clustering [24]. All algorithms use equation (1) to measure the distance between two hyperlinks. e Agglomeration initializes every hyperlink to a singleton cluster at the beginning. At each of the N \u2212 1 steps, the two closest clusters are merged into one singleton cluster. We implement this algorithm by ourselves and use single-linkage to measure the intergroup dissimilarity and use \u0434t as the threshold to jump out of its iteration. e DBSCAN algorithm regards clusters as areas with high density separated by areas with low density. We use the implementation in scikit-learn by setting eps = \u0434t ,min samples = 1, where eps means the maximum distance between two samples for them to be considered as in the same neighborhood and min samples is the minimum number of samples in a neighborhood for a point to be considered as a core point. e K-Means algorithm clusters data by trying to split samples into K groups. We use the implementation in scikit-learn by se ing parameter K with the number of blocks in the ground truth. For Spectral Clustering ,we use the implementation in scikit-learn by se ing parameter K with the number of blocks in the ground truth and use the one nearest neighbor method to construct the a nity matrix for Spectral Clustering.\nere are two versions of our method. CHD is the version of clustering hyperlink on DOM-tree without considering Hyperlink\nDensity by se ing \u03b3 = 0 in equation (5) and CHD-HD is the version considering Hyperlink Density by se ing \u03b3 = 1.\n5.3.2 Classifying hyperlink blocks. e standard deviation is \u03c3 = 2 in the re-implemented Gaussian smoothing algorithm. We classify each hyperlink block as navigation object or non-navigation object by using SVM with RBF kernel implemented in scikit-learn. e parameters in this SVM classi er are set as C = 1.0and\u03b3 = 0.1, where C is the penalty parameter of the error term and \u03b3 is the kernel coe cient for RBF."}, {"heading": "5.4 Results", "text": "For each data set, we randomly select 50% webpages as the training set and the remaining webpages as the testing set.\n5.4.1 Clustering hyperlinks. Table 1 and Table 2 present the hyperlink clustering performance of di erent algorithms on the CleanEval, Myriad 40 and Big 5 data sets in the ARI metric and AMI metric respectively. e Big 5 has been broken down into it\u2019s individual sources.\nComparing the average ARI values and AMI values over all data sets, our methods (including both CHD and CHD-HD) outperform all comparison methods. Actually, our methods have a be er performance than most comparison methods when comparing ARI values and AMI values on individual data set. Moreover, our method is more reliable than the comparison methods since our method has a stable performances while comparison methods may collapse on some particular data sets. It is because that our method makes good use of the hierarchical structure of the DOM-tree as well as the distance information on DOM-tree. Finally, CHD-HD always performs be er than CHD, especially on the dataset CleanEval, which has the greatest diversity and most dynamic and page-dependent navigation elements. at means besides the hierarchical structure of the DOM-tree, Hyperlink Density is also very helpful.\nBesides our method, the Agglomeration has the best performance, except the collapse of performance on Suntimes data set. Although it makes no use of any information from the hierarchical structure, it uses the fact that hyperlinks in a block are gathering together. e DBSCAN also uses this fact, so its performances are quite similar with the performance of Agglomeration. For instance, on the Myriad 40 and four sources in Big 5, the performance of DBSCAN is the same as Agglomeration. For K-Means and Spectral clustering, the performance is very poor, even though they have \u201ccheatet\u201d by using K obtained from the ground truth. Actually, nding the best K is very di cult. e average cumulative percentage of webpages for which the clustering performance of a particular method is less than a certain ARI value is plo ed in Figure 5. e corresponding gure for AMI is in Figure 6. e slower the curve goes up from le to right, the be er performance the corresponding method has. ese two gures provide a more obvious illustration than Table 1 and Table 2, in terms of the be er performance that our method achieved on each ARI and AMI value relative to comparison methods. e majority of webpages that our method processed have a larger AMI or ARI value. Taking Figure 5 as an example, for Agglomeration, DBSCAN, K-Means and Spectral clustering, the average percentages of ARI value lower than 0.6 are about 26%, 27%, 84% and 89%. At the same time, for CHD and CHD-HD, such percentage are only 13% and 10% respectively.\n5.4.2 Classifying hyperlink blocks. Table 3 and Table 4 present the results of classifying hyperlink blocks. It clearly shows that our method performs very well, not only on datasets with webpages from a single site (such as Tribune and Freep etc.) but also on\ndatasets with webpages from various sites (such as CleanEval and Myriad 40). e results on CleanEval data set are less competitive because this data set has the greatest diversity. Moreover, the result in which CHD-HD is used for clustering is be er than the result under clustering using CHD. at is very reasonable because CHDHD can achieve be er clustering results than CHD."}, {"heading": "5.5 Discussion", "text": "To show the generalization ability of our method, we continuously increase the percentage of hyperlinks in training set to be used from 1% to 100%, and plot the corresponding F1-scores. e incremental value is 1% when the percentage is less than 10%, and the incremental value is 10% otherwise. We used CHD-HD to cluster hyperlinks in this experiment. We can observe that even using very few hyperlinks as the training data, 5% hyperlinks of whole training set for an example, the performance of our method is very impressive. is means our method has a strong generalization ability because it needs very few training data to perform very well. at brings great practicability to our method."}, {"heading": "6 CONCLUSIONS", "text": "In this paper we propose a new extracting method for navigation objects in a webpage to capture both the static directory structures and the dynamic content structures in a website. Our method will extract not only the static navigation menus, but also the dynamic and personalized page-speci c navigation lists, including top stories and recommended list etc. Based on the observation that hyperlinks in a webpage are naturally arranged in di erent blocks, we use a two-step process to extract navigation objects in a webpage by rst clustering hyperlinks in a webpage into multiple blocks and then\nidentify navigation object blocks from the clustering results using the SVM classi er. e e ectiveness of our method is veri ed with experiments on real-world data sets.\nIn addition to its e ectiveness, the greatest strengths of our method are the simplicity of its implementation and its great practicability. Firstly, it has a very strong ability of generalization and needs very few training data to perform well, which gives it great practicability. Secondly, our method only requires the HTML le of a webpage and does not need any preprocess to handle noises in the webpage. irdly, our method does not rely on any special HTML cues (e.g., < table >, < td >, color and font etc.), which brings great stabilization over time.\nere are several interesting problems to be investigated in our future work: (1) we will consider using more features in clustering hyperlinks and classifying hyperlink blocks without injuring the simplicity of our method; (2) we may try to achieve similar performance without any training data, which makes the method much easier to use; (3) we can incorporate additional information in our method, such as the cliques in the web graph, to further improve the understanding of content structures in websites."}, {"heading": "ACKNOWLEDGMENTS", "text": "is work is supported by Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Zhejiang Provincial So Science Project (Grant no. 2015C25053), Zhejiang Provincial Natural Science Foundation of China (Grant no. LZ13F020001), National Science Foundation of China (Grant no. 61173185)."}], "references": [{"title": "Internet: Diameter of the world-wide web", "author": ["R\u00e9ka Albert", "Hawoong Jeong", "Albert-L\u00e1szl\u00f3 Barab\u00e1si"], "venue": "Nature 401,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Template detection via data mining and its applications", "author": ["Ziv Bar-Yossef", "Sridhar Rajagopalan"], "venue": "In Proceedings of the 11th international conference on World Wide Web", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Cleaneval: a Competition for Cleaning Web Pages", "author": ["Marco Baroni", "Francis Chantree", "Adam Kilgarri", "Serge Sharo"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Graph structure in the web", "author": ["Andrei Broder", "Ravi Kumar", "Farzin Maghoul", "Prabhakar Raghavan", "Sridhar Rajagopalan", "Raymie Stata", "Andrew Tomkins", "Janet Wiener"], "venue": "Computer networks 33,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "VIPS: A visionbased page segmentation algorithm", "author": ["Deng Cai", "Shipeng Yu", "Ji-Rong Wen", "Wei-Ying Ma"], "venue": "Technical Report. Microso\u0089 technical report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Support vector machines for histogram-based image classi\u0080cation", "author": ["Olivier Chapelle", "Patrick Ha\u0082ner", "Vladimir N Vapnik"], "venue": "Neural Networks, IEEE Transactions on 10,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Template detection for large scale search engines", "author": ["Liang Chen", "Shaozhi Ye", "Xing Li"], "venue": "In Proceedings of the 2006 ACM symposium on Applied computing", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Comparing partitions", "author": ["Lawrence Hubert", "Phipps Arabie"], "venue": "Journal of classi\u0080cation 2,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Search result presentation: Supporting post-search navigation by integration of taxonomy data", "author": ["Ma\u008ahias Keller", "Patrick M\u00fchlschlegel", "Hannes Hartenstein"], "venue": "In Proceedings of the 22nd international conference on World Wide Web companion. International World Wide Web Conferences Steering Commi\u008aee,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "MenuMiner: revealing the information architecture of large web sites by analyzing maximal cliques", "author": ["Ma\u008ahias Keller", "Martin Nussbaumer"], "venue": "In Proceedings of the 21st international conference companion on World Wide Web", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Hierarchical topic segmentation of websites", "author": ["Ravi Kumar", "Kunal Punera", "Andrew Tomkins"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Accessibility of information on the web", "author": ["Steve Lawrence", "C Lee Giles"], "venue": "Nature 400,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Hierarchical web-page clustering via in-page and cross-page link structures. In Advances in Knowledge Discovery and Data", "author": ["Cindy Xide Lin", "Yintao Yu", "Jiawei Han", "Bing Liu"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Discovering informative content blocks from Web documents", "author": ["Shian-Hua Lin", "Jan-Ming Ho"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Extracting logical hierarchical structure of HTML documents based on headings", "author": ["Tomohiro Manabe", "Keishi Tajima"], "venue": "Proceedings of the VLDB Endowment 8,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "\u008ce PageRank citation ranking: bringing order to the Web", "author": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Extracting article text from the web with maximum subsequence segmentation", "author": ["Je\u0082 Pasternack", "Dan Roth"], "venue": "In Proceedings of the 18th international conference on World wide web", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Content extraction from html documents", "author": ["AFR Rahman", "H Alam", "R Hartono"], "venue": "In 1st Int. Workshop on Web Document Analysis", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["William M Rand"], "venue": "Journal of the American Statistical association", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1971}, {"title": "Link Structure Graphs for Representing and Analyzing Web Sites", "author": ["Eduarda Mendes Rodrigues", "Natasa Milic-Frayling", "Martin Hicks", "Gavin Smyth"], "venue": "Technical Report", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A survey of clustering algorithms. In Data mining and knowledge discovery handbook", "author": ["Lior Rokach"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Dom based content extraction via text density", "author": ["Fei Sun", "Dandan Song", "Lejian Liao"], "venue": "In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Feature scaling in support vector data descriptions", "author": ["David MJ Tax", "Robert PW Duin"], "venue": "Technical Report", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Nguyen Xuan Vinh", "Julien Epps", "James Bailey"], "venue": "\u008ae Journal of Machine Learning Research", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Can we learn a template-independent wrapper for news article extraction from a single training site", "author": ["J. Wang", "C. Chen", "C. Wang", "J. Pei", "J. Bu", "Z. Guan", "W.V. Zhang"], "venue": "In Proceedings of the 15th ACM SIGKDD", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "CETR: content extraction via tag ratios", "author": ["Tim Weninger", "William H Hsu", "Jiawei Han"], "venue": "In Proceedings of the 19th international conference on World wide web", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Automatic Web Content Extraction by Combination of Learning and Grouping", "author": ["Shanchan Wu", "Jerry Liu", "Jian Fan"], "venue": "In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi\u008aee,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Web site topic-hierarchy generation based on link structure", "author": ["Christopher C Yang", "Nan Liu"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Eliminating noisy information in web pages for data mining", "author": ["Lan Yi", "Bing Liu", "Xiaoli Li"], "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Template-independent news extraction based on visual consistency", "author": ["Shuyi Zheng", "Ruihua Song", "Ji-Rong Wen"], "venue": "In AAAI,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "Existing works mainly extract navigation menus from webpages to reveal the content structure of the site [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "Many applications can be derived from the extracted content structure, including generating site map to improve information accessibility for disabled users, or providing content hierarchy in search results [10] etc.", "startOffset": 207, "endOffset": 211}, {"referenceID": 9, "context": "But their importance are neglected in existing works of web structure extraction, which mainly focus on extracting static web site structures such as the navigation menus[11], headings[17] etc.", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "But their importance are neglected in existing works of web structure extraction, which mainly focus on extracting static web site structures such as the navigation menus[11], headings[17] etc.", "startOffset": 184, "endOffset": 188}, {"referenceID": 3, "context": "Some early works studied the structure of the web at large [4][12] and uncover the major connected components of the web.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "Others analyzed the generally properties related with the web graph, such as its diameter [1], size and accessibility of information on the web [14] etc.", "startOffset": 90, "endOffset": 93}, {"referenceID": 11, "context": "Others analyzed the generally properties related with the web graph, such as its diameter [1], size and accessibility of information on the web [14] etc.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "PageRank [18] exploits the linkage information to learn the importance of webpages and becomes widely used in modern search engines.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "[13] used the hierarchical structure of URLs to generate hierarchical web site segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "\u008cough the hierarchical structure of URLs was also used in many other works, such as [31], the hierarchical structure of URLs does not re\u0083ect the web site organization accurately.", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "[23] noticed that and thought navigation objects could re\u0083ect the web site structure be\u008aer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] also tried to use navigation menus to reveal the information architecture of web sites, but they extracted navigation menus in a very di\u0082erent way.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] clustered webpages, they considered parallel links which are siblings in the DOM-tree of a webpage and usually in the same navigation objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "A di\u0082erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures.", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "A di\u0082erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures.", "startOffset": 84, "endOffset": 88}, {"referenceID": 28, "context": "A di\u0082erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures.", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "A di\u0082erent category of template-based methods used template detection algorithms [2][16][32][7], in which webpages with the same template are collected and used to learn common structures.", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "[5] proposed a vision-based webpage segmentation algorithm named VIPS to divide a webpage into several blocks by its visual presentation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[33] presented a template-independent news extraction method based on visual consistency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28] exploited more features about the relation between the news title and body by \u0080rstly extracting the title block and then extracting the body block.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[30] trained a machine learning model with multiple features generated by utilizing DOM-tree node properties and extracted content using this model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Two recent works, CETR [29] and CETD [25] address this issue by identifying regions with high text density, i.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Two recent works, CETR [29] and CETD [25] address this issue by identifying regions with high text density, i.", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "We apply the re-implemented Gaussian smoothing [29] to the text lengths of hyperlinks in a DOM-tree to avoid sudden changes in the text lengths.", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "In order to perform non-linear classi\u0080cation, we use the SVM classi\u0080er with RBF kernel [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 22, "context": "What\u2019s more, the normalization can also reduce the training time of SVM classi\u0080ers [26].", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "In our experiments we use data from two sources: (1) dataset from CleanEval[3]; (2) news site data from MSS[19].", "startOffset": 75, "endOffset": 78}, {"referenceID": 16, "context": "In our experiments we use data from two sources: (1) dataset from CleanEval[3]; (2) news site data from MSS[19].", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "\u008ce \u0080rst metric is the Adjusted Rand Index (ARI) [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 18, "context": "Rand Index (RI) is used to measure the agreement between the output results of clustering and the ground truth[22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "\u008ce second metric is the Adjusted Mutual Information (AMI) [27].", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "In order to properly evaluate the performance of our method on clustering hyperlinks, we compared our method\u2019s performance with several common clustering algorithms, including Agglomeration, DBSCAN, K-Means and Spectral Clustering [24].", "startOffset": 231, "endOffset": 235}], "year": 2017, "abstractText": "Existing works for extracting navigation objects from webpages focus on navigation menus, so as to reveal the information architecture of the site. However, web 2.0 sites such as social networks, e-commerce portals etc. are making the understanding of the content structure in a web site increasingly di\u0081cult. Dynamic and personalized elements such as top stories, recommended list in a webpage are vital to the understanding of the dynamic nature of web 2.0 sites. To be\u008aer understand the content structure in web 2.0 sites, in this paper we propose a new extraction method for navigation objects in a webpage. Our method will extract not only the static navigation menus, but also the dynamic and personalized page-speci\u0080c navigation lists. Since the navigation objects in a webpage naturally come in blocks, we \u0080rst cluster hyperlinks into di\u0082erent blocks by exploiting spatial locations of hyperlinks, the hierarchical structure of the DOM-tree and the hyperlink density. \u008cen we identify navigation objects from those blocks using the SVM classi\u0080er with novel features such as anchor text lengths etc. Experiments on real-world data sets with webpages from various domains and styles veri\u0080ed the e\u0082ectiveness of our method.", "creator": "LaTeX with hyperref package"}}}