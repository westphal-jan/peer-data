{"id": "1708.09151", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Paradigm Completion for Derivational Morphology", "abstract": "the generation of complex derived word forms has been an overlooked problem in nlp ; we shall fill this gap by applying neural sequence - to - sequence models to the task. we overview the theoretical motivation matrix for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a model parallel to sequential inflectional paradigm completion. various state - of - the - art neural models, adapted from the inflection task, are able to automatically learn a range of derivation patterns, and hopefully outperform a non - neural verbal baseline by 16. 4 %. however, due to semantic, historical, and lexical considerations involved in testing derivational morphology, future work will hopefully be needed to achieve performance parity with inflection - generating systems.", "histories": [["v1", "Wed, 30 Aug 2017 07:55:57 GMT  (21kb)", "http://arxiv.org/abs/1708.09151v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ryan cotterell", "ekaterina vylomova", "huda khayrallah", "christo kirov", "david yarowsky"], "accepted": true, "id": "1708.09151"}, "pdf": {"name": "1708.09151.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ryan Cotterell", "Ekaterina Vylomova", "Huda Khayrallah", "Christo Kirov", "David Yarowsky"], "emails": ["ryan.cotterell@jhu.edu", "evylomova@student.unimelb.edu.au", "huda@cs.jhu.edu", "ckirov@cs.jhu.edu", "yarowsky@cs.jhu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n09 15\n1v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\n01 7\nforms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models, adapted from the inflection task, are able to learn a range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems."}, {"heading": "1 Introduction", "text": "Unlike inflectional morphology, which produces grammatical variants of the same core lexical item (e.g., take7\u2192takes), derivational morphology is one of the key processes by which new lemmata are created. For example, the English verb corrode can evolve into the noun corrosion, the adjective corrodent, and numerous other complex derived forms such as anticorrosive. Derivational morphology is often highly productive, leading to the ready creation of neologisms such as RaoBlackwellize and Rao-Blackwellization, both originating from the Rao-Blackwell theorem. Despite the prevalence of productive derivational morphology, however, there has been little work on its generation. Commonly used derivational resources such as NomBank (Meyers et al., 2004) are still finite. Moreover, the complex phonological and historical changes (e.g., the adjectiviza-\ntion corrode7\u2192corrosive) and affix selection (e.g., choosing between English deverbal suffixes -ment and -tion) make generation of derived forms an interesting and challenging problem for NLP.\nIn this work, we show that viewing derivational morphological processes as paradigmatic may be fruitful for generation. This means that there are a number of well-defined form-function pairs associated with a core word. For example, a typical English verb may have five forms in its inflectional paradigm, corresponding to its base (take), past tense (took), past participle (taken), progressive (taking) and third-person singular (takes) forms. These forms are related by a consistent set of relations, such as affixation. Similarly, a verb may have several slots in its derivational paradigm: The form take has the agentive nominalization taker, and the abilitative adjectivization takable. Note there are also consistent patterns associated with each derivational slot, e.g., the -er suffix regularly produces the agentive.\nExploiting this paradigmatic characterization of derivational morphology allows us to create a statistical model capable of generating derivationally complex forms. We apply state-of-the-art models for inflection generation, which learn mappings from fixed paradigm slots to derived forms. Empirically, we compare results for two models on the new task of derivational paradigm completion: a neural sequence-to-sequence model and a standard non-neural baseline. Our best neural model for derivation achieves 71.7% accuracy, beating the non-neural baseline by 16.4 points. Nevertheless, we note this is about 25 points lower than the equivalent model on the English inflection task (and even 20 points lower than the model\u2019s performance on the harder Finnish inflection generation). These results point to additional complications in derivation that require more elaborate models or data annotation to overcome. While\ninflection generation is becoming a solved problem (Cotterell et al., 2017), derivation generation is still very much open."}, {"heading": "2 Derivational Morphology", "text": "The generation of derived forms is structurally similar to the generation of inflectional variants, but presents additional challenges for NLP. Here, we provide linguistic background comparing the two types of morphological processes.\nInflection and Derivation. Inflectional morphology primarily marks semantic features that are necessary for syntax, e.g., gender, tense and aspect. Thus, it follows that in most languages inflection never changes the part of speech of the word and often does not change its basic meaning. The set of inflectional forms for a given lexeme is said to form a paradigm, e.g., the full paradigm for the verb to take is \u3008take, taking, takes, took, taken\u3009. Each entry in an inflectional paradigm is termed a slot and is indexed by a syntacto-semantic category, e.g., the PAST form of take is took. We may reasonably expect that all English verbs\u2014including neologisms\u2014have these five forms.1 Furthermore, there is typically a fairly regular relationship between a paradigm slot and its form (e.g., add -s for the third person singular form). Derivational morphology, on the other hand, often changes the core part of speech of a word and makes more radical changes in meaning. In fact, derivational processes are often subcategorized by the part-of-speech change they engender, e.g., corrode7\u2192corrosion is a deverbal nominalization.\n1Only a handful of English irregulars distinguish between the past tense and the past participle, e.g., took and taken, and thus have five unique forms in their verbal paradigms; most English verbs have four unique forms.\nDerivational Paradigms. Much like inflection, derivational processes may be organized into paradigms, with slots corresponding to more abstract lexico-semantic categories for an associated part of speech (Corbin, 1987; Booij, 2008; S\u030ctekauer, 2014). Lieber (2004) presents one of the first theoretical frameworks to enumerate a set of derivational paradigm slots, motivated by previous studies of semantic primitives by Wierzbicka (1988). A partial listing of possible derivational paradigm slots for base English adjectives, nouns, and verbs is given in Table 1. The list contains several productive cases. A key difficulty comes from the the fact that the mapping between semantics and suffixes is not always clean; Lieber (2004) points out the category AGENT could be expressed by the suffix -er (as in runner) or by -ee (as in escapee). However, both -er and -ee may have the PATIENT role; consider burner (\u201ca cheap phone intended to be disposed of, i.e. burned\u201d) and employee (\u201cone being employed\u201d), respectively. We flesh out partial derivational paradigms for several English verbs in Table 2.\nUnlike in inflectional paradigms, where we expect most cells to be filled for any given base form, derivational paradigms often contain baseslot combinations that are not semantically compatible, leading to the gaps in Table 2.2 We also observe increased paradigm irregularity due to some derived forms becoming lexicalized at different points in history, differences in the language from which the base word entered the target language (e.g., English roots of Germanic and Latinate origin behave differently (Bauer, 1983)), as well as other factors that are not obvious from the characters in the base word (e.g., gender or number of the resulting noun).\nAs an example of how difficult these factors can make derivation, consider the wide variety of potential nominalizations corresponding to the RESULT of a verb, e.g., -ion, -al and -ment, (Jackendoff, 1975). While any particular English verb will almost exclusively employ exactly one of these suffixes (e.g., we have refuse7\u2192refusal and other candidates \u2217refusion and \u2217refusement are illicit),3 the information required to choose the cor-\n2For instance, if suffix -ee marks a PATIENT it is semantically not compatible with intransitive verbs, i.e., \u2217sneezee cannot be derived from intransitive sneeze.\n3Note some forms appear to have multiple nominalizations, e.g., deport 7\u2192{deportation,deportment}, but closer inspection shows there is one regular semantic transformation\nrect suffix may be both arbitrary or not easily available.\nProductivity. There is a general agreement in linguistics that frequently used complex words become part of the lexicon as wholes, while most others are likely to be constructed from constituents (Bauer, 2001; Aronoff and Lindsay, 2014); the latter ones typically follow derivational patterns, or rules, such as adding -able to express potential or ability or applying -ly to convert adjectives into adverbs. These patterns typically present two essential properties: productivity and restrictedness. Productivity relates to the ability of a pattern to be applied to any novel base form to create a new word, potentially on-the-fly. One example of such a productive transformation is adding -less (privative construction), which may attach to almost any noun to form an adjective. Moreover, the resulting form\u2019s meaning is compositional and predictable. Many derivational suffixes in English are of this type. On the other hand, some patterns are subject to semantic, pragmatic, morphological or phonological restrictions. Consider the English patient suffix -ee, which cannot be attached to a base ending in /i(:)/, e.g., it cannot be attached to the verb free to form freeee. Restrictedness is closely related to productivity, i.e., highly productive rules are less restricted. A parsimonious model of derivational morphology would describe forms using productive rules when possible, but may store forms with highly restricted patterns directly as full lexical items.\nper word sense: deportation is eviction, but deportment is behavior.\nA Note On Terminology. We would like to make a subtle, but important point regarding terminology: the phrase morphologically rich in the NLP community almost exclusively refers to inflectional, rather than derivational morphology. For example, English is labeled as morphologically impoverished, whereas German and Russian are considered morphologically rich, e.g., see the introduction of Tsarfaty et al. (2010). As regards derivation, English is quite complex and even similar in richness to German or Russian as it contains productive formations from two substrata: Germanic and Latinate. From this perspective, English is very much a morphologically rich language. Indeed, a corpus study on the Brown Corpus showed that the majority of English words are morphologically complex when derivation is considered (Light, 1996). Note that there many languages that have exhibit neither rich inflection, nor rich derivational morphology, e.g., Chinese, which most commonly employs compounding for word word formation (Chung et al., 2014)."}, {"heading": "3 Task and Models", "text": "We discuss our two systems for derivational paradigm completion and the results they achieve."}, {"heading": "3.1 Data", "text": "We experiment on English derivational triples extracted from NomBank (Meyers et al., 2004).4 Each triple consists of a base form, the semantics of the derivation and a corresponding derived form e.g., \u3008ameliorate, RESULT, amelioration\u3009. Note that in this task we do not predict whether a slot ex-\n4There are few resources annotated for derivation in nonEnglish languages, making wider experimentation difficult.\nists, merely what form it would take given the base and the slot. In terms of current study, we consider the following derivational types: verb nominalization such as RESULT, AGENT and PATIENT, adverbalization and adjective-noun transformations. We intentionally avoid zero-derivations. We also exclude overly orthographically distant pairs by filtering out those for which the Levenshtein distance exceeds half the sum of their lengths, which appear to be misannotations in NomBank. The final dataset includes 6,029 derivational samples, which we split into train (70%), development (15%), and test (15%).5 We also note that NomBank annotations are often semantically more coarse-grained."}, {"heading": "3.2 Evaluation Metrics", "text": "We evaluate on 3 metrics: accuracy, average edit distance, and F1. Accuracy measures how often system output exactly matches the gold string. Edit distance, by comparison, measures the Levenshtein distance between system output and the gold string. Finally, we calculate affix F1 scores for individual derivational affixes. E.g., for - ment precision is the number of words where the model correctly predicted -ment (out of total predictions) and recall is the number of words where the model correctly predicted out of the number of true words."}, {"heading": "3.3 Baseline Transducer", "text": "We train a simple transducer for each base-toparadigm slot mapping in the training set, identical to the baseline described in Cotterell et al. (2016). This uses an averaged perceptron classifier to greedily apply an output transformation (substitution, deletion, or insertion) to each input charac-\n5The dataset is available at http://github.com/ryancotterell/derviational-paradigms.\nter given the surrounding characters and previous decisions."}, {"heading": "3.4 RNN Encoder-Decoder", "text": "Following Kann and Schu\u0308tze (2016) on the morphological inflection task, we use an encoder-decoder gated recurrent neural network (Bahdanau et al., 2015). First, an encoder network encodes a sequence: the concatenation of the characters of the input word and a tag describing the desired transformation\u2014both represented by embeddings. This encoder is bidirectional and consists of two gated RNNs (Cho et al., 2014), one encoding the input in the forward direction, the other one encoding in the backward direction. The output of the two RNNs is the resulting hidden vectors \u2212\u2192 hi and \u2190\u2212 hi . The hidden state is a concatenation of the forward and backward hidden vectors, i.e., hi = [ \u2212\u2192 hi \u2190\u2212 hi ].\nThe decoder also consists of an RNN, but is additionally equipped with an attention mechanism. The latter computes a weight for each of the encoder hidden vectors for each character or subtag, which can be roughly understood as giving a certain importance to each of the inputs. The probability of the target sequence y = (y1, . . . , y|y|) given the input sequence x = (x1, . . . , x|x|) is modeled by\np(y | x1, . . . , x|x|) =\n|y|\u220f\nt=1\np(yt | {y1, . . . , yt\u22121}, ct)\n=\n|y|\u220f\nt=1\ng(yt\u22121, st, ct), (1)\nwhere g is a multi-layer perceptron, st is the hidden state of the decoder and ct is the sum of the encoder states hi, scored by attention weights \u03b1i(st\u22121) that depend on the decoder state: ct =\u2211 i \u03b1i(st\u22121)hi.\nInput Encoding. We model this problem as a character translation problem, with special encodings for the transformation tags that indicate the type of derivation. For example, we treat the triple: \u3008ameliorate, RESULT, amelioration\u3009 as the source string a m e l i o r a t e RESULT and target string a m e l i o r a t i o n. This is similar to the encoding in Kann and Schu\u0308tze (2016).\nTraining. We use the Nematus toolkit (Sennrich et al., 2017).6 We exactly follow\n6 https://github.com/rsennrich/nematus/\nthe recipe in Kann and Schu\u0308tze (2016), the winning submission on the 2016 SIGMORPHON shared task for inflectional morphology. Accordingly, we use a character embedding size of 300, 100 hidden units in both the encoder and decoder, Adadelta (Zeiler, 2012) with a minibatch size of 20, and a beam size of 12. We train for 300 epochs and select the test model based on the performance on the development set."}, {"heading": "4 Experimental Results", "text": "Table 3 compares the accuracy of our baseline system with the accuracy of our sequence-tosequence neural network using the data splits discussed in \u00a73.1. In all cases, the network outperforms the baseline. While 1-best performance is not nearly as high as that expected from a stateof-the-art inflectional generation system, the key point is that performance significantly increases when considering the 10-best outputs. This suggests that the network is indeed learning the correct set of possible nominalization patterns. However, the information needed to correctly choose among these patterns for a given input is not necessarily available to the network. In particular, the network is only aware of important disambiguating historical (e.g., is the input of Latin or Greek origin) and lexical-semantic (e.g., is the input verb transitive or intransitive) factors to the extent that they are implicitly encoded in the input character sequence. We speculate that making these additional pieces of information directly available as input features will significantly improve 1-best accuracy.\nUnfortunately, NomBank does not provide the necessary annotations in most cases. For instance, there is no way to differentiate actor and actress without gender. It also does not distinguish the semantics of some adjective nominalizations, e.g., activism and activity. Future work will reannotate NomBank to make these finer-grained distinctions.\nError Analysis. We observe mistakes on less frequent suffixes, e.g., -age\u2014we predict \u2217draination instead of drainage. Also, there are several cases where NomBank only lists one available form, e.g., complexity, and our model predicts complexness. We also see mistakes on irregular adverbs, e.g., we generate advancely from advance, rather than in-advance, as well as in PATIENT nominalizations, e.g., the model\nproduces containee in place of content\u2014this last distinction is unpredictable."}, {"heading": "5 Related Work", "text": "Previous work in unsupervised morphological segmentation and has implicitly incorporated derivational morphology. Such systems attempt to segment words into all constituent morphs, treating inflectional and derivational affixes as equivalent. The popular Morfessor tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al. (2015) inter alia. Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided pre-segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Schu\u0308tze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-tosequence architecture is quite similar."}, {"heading": "6 Conclusion", "text": "We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigmatic, where slots refer to semantic categories, e.g., corrode+RESULT 7\u2192corrosion, we draw upon recent advances in the generation of inflectional morphology. Applying this method works well, achieving an overall accuracy of 71.71%, and beat-\ning a non-neural baseline. Performance, however, is lower than on the task of paradigm completion for inflectional morphology, indicating that paradigm completion for derivational morphology is more challenging than its inflectional counterpart."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Jason Eisner and Colin Wilson for helpful discussions about derivation. The first author acknowledges funding from an NDSEG graduate fellowship. This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-15-C-0113. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA."}], "references": [{"title": "Paradigm classification in supervised learning of morphology", "author": ["Markus Forsberg", "Mans Hulden."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Forsberg and Hulden.,? 2015", "shortCiteRegEx": "Forsberg and Hulden.", "year": 2015}, {"title": "Productivity, blocking and lexicalization", "author": ["Mark Aronoff", "Mark Lindsay."], "venue": "The Oxford Handbook of Derivational Morphology, pages 67\u201383.", "citeRegEx": "Aronoff and Lindsay.,? 2014", "shortCiteRegEx": "Aronoff and Lindsay.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "English Word-formation", "author": ["Laurie Bauer."], "venue": "Cambridge University Press.", "citeRegEx": "Bauer.,? 1983", "shortCiteRegEx": "Bauer.", "year": 1983}, {"title": "Morphological Productivity, volume 95", "author": ["Laurie Bauer."], "venue": "Cambridge University Press.", "citeRegEx": "Bauer.,? 2001", "shortCiteRegEx": "Bauer.", "year": 2001}, {"title": "Paradigmatic morphology", "author": ["Geert Booij."], "venue": "La Raison Morphologique. Hommage \u00e0 la M\u00e9moire de Danielle Corbin, pages 29\u201338.", "citeRegEx": "Booij.,? 2008", "shortCiteRegEx": "Booij.", "year": 2008}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Trans-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Sino-Tibetan", "author": ["Karen Steffen Chung", "Nathan W. Hill", "Sun Jackson T.-S."], "venue": "Rochelle Lieber and Pavol \u0160tekauer, editors, The Oxford Handbook of Derivational Morphology, chapter 34, pages 609\u2013 650. Oxford University Press, Oxford.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Morphologie D\u00e9rivationnelle et Structuration du Lexique, volume 193", "author": ["Danielle Corbin."], "venue": "Walter de Gruyter.", "citeRegEx": "Corbin.,? 1987", "shortCiteRegEx": "Corbin.", "year": 1987}, {"title": "The CoNLL-SIGMORPHON 2017 shared task: Universal morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "G\u00e9raldine Walther", "Ekaterina Vylomova", "Patrick Xia", "Manaal Faruqui", "Sandra K\u00fcbler", "David Yarowsky", "Jason Eisner", "Mans Hulden"], "venue": null, "citeRegEx": "Cotterell et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cotterell et al\\.", "year": 2017}, {"title": "The SIGMORPHON 2016 shared task\u2014morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "Proceedings of the 14th SIGMORPHON Work-", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Labeled morphological segmentation with semi-Markov models", "author": ["Ryan Cotterell", "Thomas M\u00fcller", "Alexander Fraser", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning", "citeRegEx": "Cotterell et al\\.,? 2015", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "Joint semantic synthesis and morphological analysis of the derived word", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "Transactions of the Association for Computational Linguistics (TACL), 5:147\u2013161.", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2017", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2017}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "ACMTransactions on Speech and Language Processing (TSLP), 4(1):3.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Morphological and semantic regularities in the lexicon", "author": ["Ray Jackendoff."], "venue": "Language, pages 639\u2013 671.", "citeRegEx": "Jackendoff.,? 1975", "shortCiteRegEx": "Jackendoff.", "year": 1975}, {"title": "Single-model encoder-decoder with explicit morphological representation for reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 555\u2013560, Berlin, Germany. Association for", "citeRegEx": "Kann and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Morphology and Lexical Semantics, volume 104", "author": ["Rochelle Lieber."], "venue": "Cambridge University Press.", "citeRegEx": "Lieber.,? 2004", "shortCiteRegEx": "Lieber.", "year": 2004}, {"title": "Morphological Cues for Lexical Semantics", "author": ["Marc Light."], "venue": "Ph.D. thesis, University of Rochester.", "citeRegEx": "Light.,? 1996", "shortCiteRegEx": "Light.", "year": 1996}, {"title": "The nombank project: An interim report", "author": ["A. Meyers", "R. Reeves", "C. Macleod", "R. Szekely", "V. Zielinska", "B. Young", "R. Grishman."], "venue": "HLTNAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24\u201331, Boston, Massachusetts, USA.", "citeRegEx": "Meyers et al\\.,? 2004", "shortCiteRegEx": "Meyers et al\\.", "year": 2004}, {"title": "An unsupervised method for uncovering morphological chains", "author": ["Karthik Narasimhan", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Transactions of the Association for Computational Linguistics (TACL), 3:157\u2013167.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Inflection generation as discriminative string transduction", "author": ["Colin Cherry", "Grzegorz Kondrak."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Cherry and Kondrak.,? 2015", "shortCiteRegEx": "Cherry and Kondrak.", "year": 2015}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["Colin Cherry", "Kristina Toutanova."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American", "citeRegEx": "Cherry and Toutanova.,? 2009", "shortCiteRegEx": "Cherry and Toutanova.", "year": 2009}, {"title": "Weighting finite-state transductions with neural context", "author": ["Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Cotterell and Eisner.,? 2016", "shortCiteRegEx": "Cotterell and Eisner.", "year": 2016}, {"title": "Supervised morphological segmentation in a low-resource learning setting using conditional random fields", "author": ["Teemu Ruokolainen", "Oskar Kohonen", "Sami Virpioja", "Mikko Kurimo."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning", "citeRegEx": "Ruokolainen et al\\.,? 2013", "shortCiteRegEx": "Ruokolainen et al\\.", "year": 2013}, {"title": "Statistical parsing of morphologically rich languages (SPMRL) what, how and whither", "author": ["Reut Tsarfaty", "Djam\u00e9 Seddah", "Yoav Goldberg", "Sandra Kuebler", "Yannick Versley", "Marie Candito", "Jennifer Foster", "Ines Rehbein", "Lamia Tounsi."], "venue": "Proceedings of the NAACL HLT 2010 First Work-", "citeRegEx": "Tsarfaty et al\\.,? 2010", "shortCiteRegEx": "Tsarfaty et al\\.", "year": 2010}, {"title": "Derivational paradigms", "author": ["Pavol \u0160tekauer."], "venue": "Rochelle Lieber and Pavol \u0160tekauer, editors, The Oxford Handbook of Derivational Morphology, chapter 12, pages 354\u2013369. Oxford University Press, Oxford.", "citeRegEx": "\u0160tekauer.,? 2014", "shortCiteRegEx": "\u0160tekauer.", "year": 2014}, {"title": "Context-aware prediction of derivational word-forms", "author": ["Ekaterina Vylomova", "Ryan Cotterell", "Timothy Baldwin", "Trevor Cohn."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational", "citeRegEx": "Vylomova et al\\.,? 2017", "shortCiteRegEx": "Vylomova et al\\.", "year": 2017}, {"title": "The Semantics of Grammar, volume 18", "author": ["Anna Wierzbicka."], "venue": "John Benjamins Publishing.", "citeRegEx": "Wierzbicka.,? 1988", "shortCiteRegEx": "Wierzbicka.", "year": 1988}], "referenceMentions": [{"referenceID": 20, "context": "Commonly used derivational resources such as NomBank (Meyers et al., 2004) are still finite.", "startOffset": 53, "endOffset": 74}, {"referenceID": 9, "context": "inflection generation is becoming a solved problem (Cotterell et al., 2017), derivation generation is still very much open.", "startOffset": 51, "endOffset": 75}, {"referenceID": 8, "context": "Much like inflection, derivational processes may be organized into paradigms, with slots corresponding to more abstract lexico-semantic categories for an associated part of speech (Corbin, 1987; Booij, 2008; \u0160tekauer, 2014).", "startOffset": 180, "endOffset": 223}, {"referenceID": 5, "context": "Much like inflection, derivational processes may be organized into paradigms, with slots corresponding to more abstract lexico-semantic categories for an associated part of speech (Corbin, 1987; Booij, 2008; \u0160tekauer, 2014).", "startOffset": 180, "endOffset": 223}, {"referenceID": 27, "context": "Much like inflection, derivational processes may be organized into paradigms, with slots corresponding to more abstract lexico-semantic categories for an associated part of speech (Corbin, 1987; Booij, 2008; \u0160tekauer, 2014).", "startOffset": 180, "endOffset": 223}, {"referenceID": 5, "context": "Much like inflection, derivational processes may be organized into paradigms, with slots corresponding to more abstract lexico-semantic categories for an associated part of speech (Corbin, 1987; Booij, 2008; \u0160tekauer, 2014). Lieber (2004) presents one of the first theoretical frameworks to enumerate a set of derivational paradigm slots, motivated by previous studies of semantic primitives by Wierzbicka (1988).", "startOffset": 195, "endOffset": 239}, {"referenceID": 5, "context": "Much like inflection, derivational processes may be organized into paradigms, with slots corresponding to more abstract lexico-semantic categories for an associated part of speech (Corbin, 1987; Booij, 2008; \u0160tekauer, 2014). Lieber (2004) presents one of the first theoretical frameworks to enumerate a set of derivational paradigm slots, motivated by previous studies of semantic primitives by Wierzbicka (1988). A partial listing of possible derivational paradigm slots for base English adjectives, nouns, and verbs is given in Table 1.", "startOffset": 195, "endOffset": 413}, {"referenceID": 5, "context": "Much like inflection, derivational processes may be organized into paradigms, with slots corresponding to more abstract lexico-semantic categories for an associated part of speech (Corbin, 1987; Booij, 2008; \u0160tekauer, 2014). Lieber (2004) presents one of the first theoretical frameworks to enumerate a set of derivational paradigm slots, motivated by previous studies of semantic primitives by Wierzbicka (1988). A partial listing of possible derivational paradigm slots for base English adjectives, nouns, and verbs is given in Table 1. The list contains several productive cases. A key difficulty comes from the the fact that the mapping between semantics and suffixes is not always clean; Lieber (2004) points out the category AGENT could be expressed by the suffix -er (as in runner) or by -ee (as in escapee).", "startOffset": 195, "endOffset": 707}, {"referenceID": 3, "context": ", English roots of Germanic and Latinate origin behave differently (Bauer, 1983)), as well as other factors that are not obvious from the charac-", "startOffset": 67, "endOffset": 80}, {"referenceID": 16, "context": ", -ion, -al and -ment, (Jackendoff, 1975).", "startOffset": 23, "endOffset": 41}, {"referenceID": 4, "context": "There is a general agreement in linguistics that frequently used complex words become part of the lexicon as wholes, while most others are likely to be constructed from constituents (Bauer, 2001; Aronoff and Lindsay, 2014); the latter ones typically follow derivational patterns, or rules, such as adding -able to express potential or ability or applying -ly to convert adjectives into adverbs.", "startOffset": 182, "endOffset": 222}, {"referenceID": 1, "context": "There is a general agreement in linguistics that frequently used complex words become part of the lexicon as wholes, while most others are likely to be constructed from constituents (Bauer, 2001; Aronoff and Lindsay, 2014); the latter ones typically follow derivational patterns, or rules, such as adding -able to express potential or ability or applying -ly to convert adjectives into adverbs.", "startOffset": 182, "endOffset": 222}, {"referenceID": 19, "context": "Indeed, a corpus study on the Brown Corpus showed that the majority of English words are morphologically complex when derivation is considered (Light, 1996).", "startOffset": 143, "endOffset": 156}, {"referenceID": 7, "context": ", Chinese, which most commonly employs compounding for word word formation (Chung et al., 2014).", "startOffset": 75, "endOffset": 95}, {"referenceID": 24, "context": ", see the introduction of Tsarfaty et al. (2010). As regards derivation, English is quite complex and even similar in richness to German or Russian as it contains productive formations from two substrata: Germanic and Latinate.", "startOffset": 26, "endOffset": 49}, {"referenceID": 20, "context": "We experiment on English derivational triples extracted from NomBank (Meyers et al., 2004).", "startOffset": 69, "endOffset": 90}, {"referenceID": 9, "context": "We train a simple transducer for each base-toparadigm slot mapping in the training set, identical to the baseline described in Cotterell et al. (2016). This uses an averaged perceptron classifier to greedily apply an output transformation (substitution, deletion, or insertion) to each input charac-", "startOffset": 127, "endOffset": 151}, {"referenceID": 2, "context": "Following Kann and Sch\u00fctze (2016) on the morphological inflection task, we use an encoder-decoder gated recurrent neural network (Bahdanau et al., 2015).", "startOffset": 129, "endOffset": 152}, {"referenceID": 6, "context": "This encoder is bidirectional and consists of two gated RNNs (Cho et al., 2014), one encoding the input in the forward direction, the other one encoding in the backward direction.", "startOffset": 61, "endOffset": 79}, {"referenceID": 15, "context": "Following Kann and Sch\u00fctze (2016) on the morphological inflection task, we use an encoder-decoder gated recurrent neural network (Bahdanau et al.", "startOffset": 10, "endOffset": 34}, {"referenceID": 17, "context": "This is similar to the encoding in Kann and Sch\u00fctze (2016).", "startOffset": 35, "endOffset": 59}, {"referenceID": 17, "context": "the recipe in Kann and Sch\u00fctze (2016), the winning submission on the 2016 SIGMORPHON shared task for inflectional morphology.", "startOffset": 14, "endOffset": 38}, {"referenceID": 13, "context": "The popular Morfessor tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.", "startOffset": 27, "endOffset": 51}, {"referenceID": 25, "context": "Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided pre-segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch\u00fctze, 2017).", "startOffset": 196, "endOffset": 275}, {"referenceID": 11, "context": "Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided pre-segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch\u00fctze, 2017).", "startOffset": 196, "endOffset": 275}, {"referenceID": 12, "context": "Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided pre-segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch\u00fctze, 2017).", "startOffset": 196, "endOffset": 275}, {"referenceID": 14, "context": "Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016).", "startOffset": 100, "endOffset": 214}, {"referenceID": 15, "context": "Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016).", "startOffset": 100, "endOffset": 214}, {"referenceID": 9, "context": "The popular Morfessor tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al.", "startOffset": 28, "endOffset": 160}, {"referenceID": 9, "context": "The popular Morfessor tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al. (2015) inter alia.", "startOffset": 28, "endOffset": 186}, {"referenceID": 28, "context": "In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context.", "startOffset": 32, "endOffset": 55}], "year": 2017, "abstractText": "The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models, adapted from the inflection task, are able to learn a range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems.", "creator": "LaTeX with hyperref package"}}}