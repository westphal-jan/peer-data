{"id": "1708.09163", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "An Empirical Study of Discriminative Sequence Labeling Models for Vietnamese Text Processing", "abstract": "this paper presents an empirical study of two widely - independently used sequence prediction models, broad conditional random fields ( crfs ) and long short - term memory networks ( lstms ), on two fundamental tasks for vietnamese text processor processing, including part - of - speech tagging and named entity recognition. we show that a strong lower bound for labeling accuracy can be obtained by relying only on simple word - based features with minimal hand - crafted precision feature engineering, of 90. 65 \\ % and 86. 03 \\ % performance scores on the standard test sets for the two tasks respectively. in both particular, we demonstrate empirically the surprising improvement efficiency of word embeddings in both of the two tasks, with both of the two models. we point out that the additive state - of - under the - art lstms model does not always outperform significantly the traditional crfs model, especially on moderate - sized data sets. finally, we give some suggestions and discussions for efficient use there of sequential sequence labeling models in practical applications.", "histories": [["v1", "Wed, 30 Aug 2017 08:32:32 GMT  (66kb)", "http://arxiv.org/abs/1708.09163v1", "To appear in the Proceedings of the 9th International Conference on Knowledge and Systems Engineering (KSE) 2017"]], "COMMENTS": "To appear in the Proceedings of the 9th International Conference on Knowledge and Systems Engineering (KSE) 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["phuong le-hong", "minh pham quang nhat", "thai-hoang pham", "tuan-anh tran", "dang-minh nguyen"], "accepted": false, "id": "1708.09163"}, "pdf": {"name": "1708.09163.pdf", "metadata": {"source": "CRF", "title": "An Empirical Study of Discriminative Sequence Labeling Models for Vietnamese Text Processing", "authors": ["Phuong Le-Hong", "Minh Pham Quang Nhat", "Thai-Hoang Pham", "Tuan-Anh Tran", "Dang-Minh Nguyen"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n09 16\n3v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\nI. INTRODUCTION\nMany datasets, such as text collections and genetic databases, consist of sequences of distinct values. For applications that use such datasets, we often need to predict the sequence of labels given an observation sequence. In sequence prediction problems, we attempt to predict elements of a sequence on the basis of the preceding elements. Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5]. These are all powerful probabilistic tools for modeling sequential data and have been applied to many text-related tasks such as partof-speech tagging, named entity recognition, text segmentation and information extraction. These models also support applications in bioinformatics such as expressed sequence tag finding and gene discovery.\nIn this paper, we present an empirical study of two prevalent discriminative sequence labeling models, CRFs and LSTMs, on two fundamental problems of text processing, namely part-of-speech tagging and named entity recognition. Experiments are carefully designed, carried out and analyzed on standard Vietnamese data sets. The main findings of this work are as follows. First, we show that we can obtain a strong performance lower bound for both part-of-speech (POS) tagging and named entity recognition (NER) by using only simple and word-based features, both with CRFs and LSTMs. For POS tagging, we achieve a test accuracy of 90.65% by using only word identities, word shapes and word embedding features on sentences less than 25 tokens. For NER with the\nsame feature set and sentence length, we obtain about 86.03% of F1-score. Second, we show that word embeddings are very effective and beneficial for both of the two tasks. They help improve POS tagging accuracy significantly by about 4.0% when using with LSTMs or 1.35% when using with CRFs. Word embeddings are even more beneficial for NER \u2013 they help improve recognition performance by more than 5% in both of the models. Third, we show that although the LSTMs model slightly outperforms CRFs in terms of accuracy, the gap is relatively small, especially on moderate-sized data sets, with the cost of much longer training time. Finally, this paper gives some suggestions for efficient use of sequence labeling models in practical applications.\nThe remainder of this paper is structured as follows. Section II presents the adopted methodology. Section III describes detailed settings and experimental results. Section VI gives discussions and findings. Section V presents related work. Finally, Section VI concludes the paper."}, {"heading": "II. METHODOLOGY", "text": ""}, {"heading": "A. Fundamental Tasks", "text": "This subsection gives a brief description of two fundamental sequence learning tasks investigated in this study, part-ofspeech tagging and named entity recognition.\n1) Part-of-Speech Tagging: POS tagging is a typical sequence prediction task, where we are interested in building a model that reads text in some languages and assigns a part-ofspeech to each token (word), such as noun, verb, adjective. In general, POS taggers in computational applications use more fine-grained POS tags like common noun or proper noun. For example, each word of the following English sentence is tagged with its most likely correct part-of-speech:\nProfits/N soared/V at/P Boeing/N Co./N ,/, easily/ADV topping/V forecasts/N on/P Wall/N Street/N ,/, as/P their/POSS CEO/N Alan/N Mulally/N announced/V first/ADJ quarter/N results/N ./.\nwhere the tags N, V, P, ADV, ADJ denotes a noun, a verb, a preposition, an adverb, an adjective, respectively.\n2) Named Entity Recognition: Named entity recognition, also known as entity identification is a subtask of information extraction that aims to locate and classify elements in texts into pre-defined categories such as the names of persons, organizations, locations and so on.\nFor example, the named entities extracted from the same English sentence above are as follows:\nProfits soared at [Organization Boeing Co.], easily topping forecasts on [Location Wall Street], as their CEO [Person Alan Mulally] announced first quarter results.\nIn this example, an organization name, a location and a person name have been detected and classified. Actually, NER can be formalized as a sequence tagging problem, where each token is tagged with a specific tag, for example:\nProfits/O soared/O at/O Boeing/B-ORG Co./I-ORG ,/O easily/O topping/O forecasts/O on/O Wall/B-LOC Street/ILOC ,/O as/O their/O CEO/O Alan/B-PER Mulally/I-PER announced/O first/O quarter/O results/O ./O\nHere, the tag O means \u201cno entity\u201d, the tags B-ORG and I-ORG mean \u201cbegin organization\u201d and \u201cin organization\u201d respectively; similarly, the tags B-LOC and I-LOC mean \u201cbegin location\u201d and \u201cin location\u201d respectively, and so on."}, {"heading": "B. Discriminative Models", "text": "In this subsection, we give a brief description of two discriminative sequence models used in this study, including Conditional Random Fields (CRFs), and Long Short-Term Memory Recurrent Neural Networks (LSTMs).\n1) Conditional Random Fields: Conditional Random Fields (CRF) [4] is a discriminative probabilistic framework, which directly model conditional probabilities of a tag sequence given a word sequence. Formally, in CRF, the conditional probability of a tag sequence y = (y1, y2, . . . , yT ), given a word sequence x = (x1, x2, . . . , xT ) is defined as follow.\nP (y |x) \u221d exp( \u2211\nj\n\u03bbjtj(yi\u22121, yi,x, i) + \u2211\nk\n\u00b5ksk(yi,x, i))\nwhere tj(yi\u22121, yi,x, i) is a transition feature function of the entire observation sequence and the labels at the position i and i\u22121 in the label sequence; sk(yi,x, i) is a state feature function of the label at the position i and the observation sequence; \u03bbj and \u00b5k are parameters to be estimated from training data.\nWe can simplify the notations by writing s(yi,x, i) = s(yi\u22121,x, i) and\nFj(y,x) =\nn\u2211\ni=1\nfj(yi\u22121, yi,x, i)\nwhere each fj(yi\u22121, yi,x, i) is either a state function s(yi\u22121, yi,x) or a transition function t(yi\u22121, yi,x, i). By using this notation, we can write the conditional probability as follows:\nP (y |x, \u03bb) = 1\nZ(x) exp(\n\u2211\nj\n\u03bbjFj(y,x))\nZ(x) is a normalization factor.\nThe parameters in CRF can be estimated by maximizing log-likelihood objective function:\nL(\u03bb) = \u2211\nk\n[log 1 Z(x(k)) + \u2211\nj\n\u03bbjFj(y (k),x(k))]\nParameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].\n2) Long Short-Term Memory Networks: Recurrent Neural Networks (RNNs) have recently been widely used for sequence labelling because they can directly represent sequential structures such as word sequences, sounds and time series data. For this reason, there is a rapidly growing interest in using RNNs for practical applications as an efficient method to map input sequences to output sequences. They are computationally more powerful and biologically more plausible than other adaptive approaches such as Hidden Markov Models (no continuous internal states), Feed-Forward Neural Networks (FFNN) and Support Vector Machines (no internal states at all).1 Traditional RNNs of the 1990s could not learn to look far back into the past because of the vanishing or exploding gradient problems. A feedback network called Long Short-Term Memory (LSTM) [9] was proposed to overcome these problems.\nWe represent the word sequence of a sentence with a bidirectional LSTM [10]. The LSTM unit at the t-th word consists of a collection of multi-dimensional vectors, including an input gate it, a forget gate ft, an output gate ot, a memory cell ct, and a hidden state ht. The unit takes as input a ddimensional input vector xt, the previous hidden state ht\u22121, the previous memory cell ct\u22121, and calculates the new vectors using the following six equations:\nit = \u03c3(W ixt + U iht\u22121 + b i) ft = \u03c3(W fxt + U fht\u22121 + b f ) ot = \u03c3(W oxt + U oht\u22121 + b o) ut = tanh(W uxt + U uht\u22121 + b u)\nct = it \u00b7 ut + ft \u00b7 ct\u22121 ht = ot \u00b7 tanh(ct),\nwhere \u03c3 denotes the logistic function, the dot product denotes the element-wise multiplication of vectors, W and U are weight matrices and b are bias vectors. The LSTM unit at tth word receives the corresponding word embedding as input vector xt. Since the LSTM is bidirectional, we concatenate the hidden state vectors of the two directions\u2019 LSTM units corresponding to each word as its output vector and pass it to the subsequent layer."}, {"heading": "III. EXPERIMENTS", "text": ""}, {"heading": "A. Datasets", "text": "1) Part-of-Speech Tagging: We perform experiments on Vietnamese part-of-speech tagging using the standard partof-speech tagged corpus of the VLSP project.2. This corpus contains 10,165 manually tagged sentences where the training set contains 9,000 sentences and the test set contains 1,165 sentences. The tagset has 21 different tags. Further details of the corpus are described in [11].\n2) Named Entity Recognition: For experiments on named entity recognition, we use the standard NER corpus developed by the Vietnamese Language and Speech Processing3 community in late 2016. Similar to the CoNLL 2003 NER corpus for English, four named entity types are considered, including\n1Many interesting details of RNNs are available online at http://people.idsia.ch/~juergen/rnn.html\n2https://vlsp.hpda.vn/demo/?page=home 3http://vlsp.org.vn/\npersons (PER), organizations (ORG), locations (LOC), and miscellaneous entities (MISC). The data are collected from electronic newspapers published on the web. Table I shows the quantity of named entity annotated in the training set and the test set."}, {"heading": "B. Feature Sets", "text": "1) Word Identities: The first basic feature set contains only word occurrence information extracted from the training set. All words having an occurrence frequency above a minimum threshold are kept in a vocabulary V . Each word can be represented by an one-hot sparse vector of size |V|.\n2) Word Shapes: In addition to word identities, word shapes have been shown to be important features for improving prediction ability, especially for unknown or rare words. Common word shape features used in our experiments are shown in Table II. We used regular expressions to extract those word shape features.\n3) Word Embeddings: Word embeddings are lowdimensional distributed representation of words. Each word embedding is a real-valued vector of d dimensions where d is much smaller than |V| of its one-hot sparse representation. Distributed word representations have been shown very useful for many natural language processing tasks. Many state-ofthe-art language processing models are now employing word or character embeddings. In particular, some previous works have also integrated Vietnamese word embeddings to improve performance [12], [13].\nTo create distributed word representations, we use a dataset consisting of 7.3GB of text from 2 million articles collected through a Vietnamese news portal.4 The text is first normalized to lower case and all special characters are removed except these common symbols: the comma, the semicolon, the colon, the full stop and the percentage sign. All numeral sequences are replaced with the special token <number>, so that correlations between certain words and numbers are correctly recognized by the neural network or the log-bilinear regression model.\n4http://www.baomoi.com\nEach word in the Vietnamese language may consist of more than one syllables with spaces in between, which could be regarded as multiple words by the unsupervised models. Hence it is necessary to replace the spaces within each word with underscores to create full word tokens. The tokenization process follows the method described in [14]. After removal of special characters and tokenization, the articles add up to 969 million word tokens, spanning a vocabulary of 1.5 million unique tokens. We train the unsupervised models with the full vocabulary to obtain the representation vectors, and then prune the collection of word vectors to the 65, 000 most frequent words, excluding special symbols and the token <number> representing numeral sequences. We train the Mikolov\u2019s continuous Skip-gram model using the neural network and source code introduced in [15]. The continuous skip-gram model itself is described in details in [16]. Each word is represented by a real-valued vector of 25 dimensions.\nIn both CRF and LSTM models, we use three kinds of features mentioned above. In the CRF model, we represent word identities, word shapes as binary features. Each dimension of a word-embedding vector is a feature and its value is the feature value. In the LSTM model, we use word shape and word embedding features as additional dimensions in vector representation for each word. Thus, each word in the LSTM model is represented by a vector of size |V| + 34 (|V| is the vocabulary size; we use 9 word shape features and 25- dimension word-embedding vectors)."}, {"heading": "C. Evaluation Method", "text": "For the POS task, our system is evaluated by the tagging accuracy on the corresponding data sets. The accuracy is the ratio of number of tokens which are correctly tagged divided by the total number of tokens in the test set. For the NER task, the performance of our system is measured with F1 score: F1 = 2 \u2217 P \u2217 R/(P + R). Precision (P ) is the percentage of named entities found by the learning system, which are correct predictions. Recall (R) is the percentage of named entities present in the corpus that are found by the system. A named entity is correct only if it is an exact match of the corresponding entity in the data file. The performance of our system is evaluated by the automatic evaluation script of the CoNLL 2003 shared task.5"}, {"heading": "D. Experimental Settings", "text": "In the experiments, we fix the minimum frequency threshold for features as 5. In other words, all words or tags which do not appear at least 5 times in the training corpus are considered unknown. In our experiments with the CRF model, we adopted CRFsuite [17], an implementation of linear-chain (first-order Markov) CRF. That toolkit allows us to easily incorporate both binary and numeric features such as word embedding features. We use default setting of CRFsuite in which the training algorithm is L-BFGS [18] and L2 regularization is used. The coefficient for L2 regularization is 1.0.\nThe recurrent neural networks all have one bidirectional recurrent layer of different numbers of units whose activation function is tanh. The output layer uses softmax activation function as usual. The multiclass cross entropy loss function\n5http://www.cnts.ua.ac.be/conll2003/ner/\nis selected. The network is trained by using the stochastic gradient descent optimization algorithm with learning rate fixed at 0.01. The Xavier initilizer is used for parameter initialization [19]. We use early stopping when training the network to help avoid overfitting and remove the need to manually set the number of training epoch. The training terminates either if the training score does not improve for three consecutive epoches or if the number of epoches reaches 400."}, {"heading": "E. Results", "text": "This subsections presents experimental results of the models on the two tasks. We first present results of part-of-speech tagging, and then those of named entity recognition.\n1) Part-of-Speech Tagging: In the first experiment, we train and compare performance of sequence models on sentences of length not greater than 20 tokens. There are 4,879 training sentences and 570 test sentences. The vocabulary size is 1,630.\nWe train different LSTMs with varying number hidden units in the range from 32 to 200. Table III shows their performance on the feature set {word identity, word shapes}. We see that the larger number of hidden units is, the better result the tagger can achieve on the test set. The LSTMs tagger achieves 85.98% of accuracy on the test set when the network has 200 hidden units.\nIn the second experiment, we add word embeddings as features to the LSTMs model to see whether they are helpful or not. Table IV shows their performance on the feature set {word identity, word shapes, word embeddings}.\nIt is surprising that word embeddings helps improve the accuracy of the tagger significantly. With the same training parameters, we are able to boost the accuracy on the test set from 85.98% to 89.92%. This result demonstrates that in LSTMs, it is beneficial to combine both discrete features and continuous features to build a better tagger.\nTable V shows the results of the CRF model with two feature sets: 1) {word identity, word shapes}; and 2) {word identity, word shapes, word embeddings}. The table indicates that incorporating word embedding features helps to improves the accuracy of the CRF model 1.35% from 87.62% to 88.97%. The CRF model outperformed LSTMs when we do not use word embedding features. However, its accuracy is lower than that of LSTMs when word embedding features are incorporated.\nIn the third experiment, we enlarge the data set by considering longer sentences. We train and compare sequence models on sentences not longer than 25 tokens. With this length, there are 6,221 training sentences and 737 test sentences in the standard data set. The vocabulary now contains 2,197 different words. The LSTM tagger achieves an accuracy of 90.65% on the test set, significantly better than its performance on the 20- token data set. Similar to the LSTM models, the performances of CRFs model are better than those on shorter sentences. This can be explained by the fact that the more training data are available, the greater number of patterns the models can learn. These results also confirm the effectiveness of word embedding features for the CRF model.\nWe observe that performance of the CRF model is slightly worse than that of LSTMs model, both on the test sets and on the training sets. In addition, the LSTMs model has a very good memorization capacity \u2013 its accuracy on the training set is nearly perfect on long sentences, especially when the number of hidden units in use is large enough.\n2) Named Entity Recognition: Similar to PoS tagging experiments, we evaluate NER methods on sentences of length not greater than 20 and on sentences of length not greater than 25. In the former experiment, there are 8,968 training sentences and 1,355 test sentences; the vocabulary size is 2,525. In the latter experiment, there are 11,436 training sentences and 1,787 test sentences; the vocabulary size is 3,368.\nTable VI and Table VII shows the performance of the LSTM models on sentences not longer than 20 and 25 tokens, respectively. Table VIII and Table IX shows the experimental results of the CRF model on the same data sets and feature sets as those of LSTM experiments. The results indicated the effectiveness of PoS tag and word embedding features. While using PoS tag features mainly improved recall using word embedding features helped to improve both precision and\nrecall. It can be explained that PoS features and especially word embedding features can better capture semantic relationship between words.\nWe see that the LSTM model is slightly better than the CRF model on short sentences; while the two models perform similarly on longer sentences. The best F-score of the LSTM model is about 86.03%, which is not very far below the stateof-the-art NER result on this data set, despite of the minimal simplicity of the features in use."}, {"heading": "IV. DISCUSSION", "text": "Discriminative sequence labeling models such as CRF or LSTMs models have been used for Vietnamese text processing tasks such as PoS tagging or named-entity recognition (NER). In this work, we compare the LSTMs model and the CRF model in two Vietnamese text processing tasks. In our understanding, our work is the first empirical work that compares these two discriminative sequence labeling models for Vietnamese text processing tasks in a systematic way. We found that the LSTMs model obtained slightly better test accuracies and had much better memorization capacity than the CRF model in PoS tagging and NER tasks. We also showed the effectiveness of word embedding features in sequence labeling models.\nBecause of the space limitation, we did not include maximum-entropy Markov models (MEMM) [3] and hidden Markov models (HMM) [1] in our comparison. It has been shown that the CRF model overcomes limitations of MEMM and HMM and outperforms MEMM and HMM in many sequence labeling tasks. For the comparison between MEMM, HMM and CRF, we can refer to the work [4]. In particular, the work [20] investigated and compared MEMM and CRF.\nWe also did not include bidirectional LSTM-CNNsCRF [21], the state-of-the-art end-to-end sequence labeling model, which combine bidirectional LSTM, CNN and CRF. That work used both word- and character-level representations in the neural network. Actually, in our paper, we do not aim to obtain state-of-the-art results but to compare discriminative sequence labeling models in a basic setting.\nWord representations which are learned from raw text corpora, have been shown to be effective in sequence labeling models. In [22], Turian et al. intensively evaluated features\nderived from unsupervised word representations such as Brown clusters and word vectors on NER and chunk tasks with the CRF model. They used near state-of-the-art supervised baselines, and showed that word representation features improved those baselines. Our work confirmed the benefit of using word representation features for Vietnamese language processing tasks. In this paper, although we did not use word-clusterbased features, we obtained significant improvements in both two tasks.\nIn our work, we limit the maximal length of sentences to 25 tokens. The reason is that the LSTMs model has very high computational cost, especially for long sentences. We need about 8 hours just to train an experiment with the LSTMs model.6 Considering that the main purpose of the paper is to compare two sequence labeling models in experiments with simple settings, we decided to limit the maximized length of sentences. With the same reason, we decided to just use simple unigram features in the two sequence labeling models.\nWe found that the LSTMs model did not really outperform the CRF model in our experiments. We suspect that the training data size we used in experiments is not large, and it affected the generalization capacity of the LSTMs model. Improving generalization capacity of deep learning models on small data is still a challenging problem in the deep learning research community. In contrast, the CRF model worked quite well even with moderate-sized training data. The lesson we leaned from the results is that in an application domain that we could not obtain large data, we may use fast sequence labeling models such as CRF and spend time designing good features that are specific and beneficial for that domain."}, {"heading": "V. RELATED WORK", "text": "This section briefly reviews related works on Vietnamese part-of-speech tagging and named entity recognition using discriminative sequence models. In [23], the authors give an empirical study of MEMM for Vietnamese part-of-speech tagging with diffferent feature sets. Their best model has a tagging accuracy of about 93.5% when all the VLSP treebank is used. We see that despite using a smaller data set with short sentences and a very simple feature set with minimal handcrafted word shapes, we are able to achieve a tagging accuracy of more than 90%. This is a strong lower bound for this task when only raw text is available for tagging.\nIn VLSP 2016 workshop, several different systems have been proposed for Vietnamese NER. The F-score of the best participating system is 88.78% [24] in that shared task. That system used many hand-crafted features to improve the performance of MEMM. Most approaches in VLSP 2016 used the CRF and maximum entropy models, whose performance is\n6On an IBM server with 32 GB RAM and 8-core CPU.\nheavily dependent on feature engineering. Table X shows those models and their performance. We observe that although the models studied in this work only rely on word features, their performance is very competitive.\nMost recently, a more advanced end-to-end system for Vietnamese NER using LSTMs was proposed [28], which achieved an F1 score of 88.59%."}, {"heading": "VI. CONCLUSION", "text": "We have presented an empirical and comparative study of two discriminative sequence prediction models CRFs and LSTMs on two fundamental tasks of Vietnamese text processing. We have demonstrated the great benefit of integrating word embeddings which are trained by an unsupervised learning method into both of the two models. These word embeddings are able to capture semantic similarities which help improve the prediction ability of the models, thereby increase the part-of-speech tagging and named entity recognition accuracy by about 4.0% and 5%, respectively. The LSTMs model is slight better than the CRFs model in terms of accuracy but the gap is not always significant in moderate-sized data sets, with the cost of much longer training time. We have also shown for the first time that a strong accuracy lower bound for both part-of-speech tagging and named entity recognition can be obtained by relying on only simple, word-based features with a minimal hand-crafted features. Using a feature set of word identities, word shapes and word embeddings, we can achieve 90.65% of tagging performance and 86.03% of recognition performance on sentence not longer than 25 tokens. One practical implication of this work is that in an application domain where large data is not readily available, we should use fast sequence labeling models such as CRFs and spend time designing good features that are specific and beneficial for that domain instead of relying on complicated LSTMs models."}], "references": [{"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257 \u2013286, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Hidden Markov models and the Baum-Welch algorithm", "author": ["L.R. Welch"], "venue": "IEEE Information Theory Society Newsletter, vol. 53, no. 4, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximum entropy Markov models for information extraction and segmentation", "author": ["A. McCallum", "D. Freitag", "F. Pereira"], "venue": "Proceedings of ICML, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "ICML, 2001, pp. 282\u2013289.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Generalized iterative scaling for loglinear models", "author": ["J.N. Darroch", "D. Ratcliff"], "venue": "Annals of Mathematical Statistics, vol. 43, no. 5, pp. 1470\u20131480, 1972.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1972}, {"title": "Sequential conditional generalized iterative scaling", "author": ["J. Goodman"], "venue": "Proceedings of ACL, 2002, pp. 9\u201316.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Numerical Optimization, 2nd ed", "author": ["J. Nocedal", "S.J. Wright"], "venue": "New York: Springer,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Proceedings of ICASSP. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Building a large syntactically-annotated corpus of Vietnamese", "author": ["P.T. Nguyen", "L.V. Xuan", "T.M.H. Nguyen", "V.H. Nguyen", "P. Le- Hong"], "venue": "Proceedings of the 3rd Linguistic Annotation Workshop, ACL- IJCNLP, Suntec City, Singapore, 2009, pp. 182\u2013185.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast dependency parsing using distributed word representations", "author": ["P. Le-Hong", "T.-M.-H. Nguyen", "T.-L. Nguyen", "M.-L. Ha"], "venue": "Trends and Applications in Knowledge Discovery and Data Mining, ser. Lecture Notes in Artificial Intelligence. Springer, 2015, vol. 9441.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving Vietnamese dependency parsing using distributed word representations", "author": ["C. Vu-Manh", "A.-T. Luong", "P. Le-Hong"], "venue": "Proceedings of SoICT. ACM, 2015, pp. 54\u201360.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid approach to word segmentation of Vietnamese texts", "author": ["P. Le-Hong", "T.M.H. Nguyen", "A. Roussanaly", "T.V. Ho"], "venue": "Language and Automata Theory and Applications, ser. Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2008, vol. 5196, pp. 240\u2013249.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26, C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, Eds. Curran Associates, Inc., 2013, pp. 3111\u20133119.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of Workshop at ICLR, Scottsdale, Arizona, USA, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Crfsuite: a fast implementation of conditional random fields (crfs)", "author": ["N. Okazaki"], "venue": "2007. [Online]. Available: http://www.chokkan.org/software/crfsuite/", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Updating quasi-newton matrices with limited storage", "author": ["J. Nocedal"], "venue": "Mathematics of computation, vol. 35, no. 151, pp. 773\u2013782, 1980.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1980}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, vol. 9, Sardinia, Italy, 2010, pp. 249\u2013256.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "On the effect of the label bias problem in part-of-speech tagging", "author": ["P. Le-Hong", "X.-H. Phan", "T.T. Tran"], "venue": "The 10th IEEE RIVF. Hanoi, Vietnam: IEEE, 2013, pp. 103\u2013108.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF", "author": ["X. Ma", "E. Hovy"], "venue": "Proceedings of the ACL, Berlin, Germany, August 2016, pp. 1064\u20131074.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of ACL, Uppsala, Sweden, 2010, pp. 384\u2013394.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical study of maximum entropy approach for part-of-speech tagging of Vietnamese texts", "author": ["P. Le-Hong", "A. Roussanaly", "T.M.H. Nguyen", "M. Rossignol"], "venue": "Actes de Traitement Automatique des Langues, Montreal, Canada, 2010, pp. 50\u201361.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Vietnamese named entity recognition using token regular expressions and bidirectional inference", "author": ["P. Le-Hong"], "venue": "Proceedings of VLSP, Hanoi, Vietnam, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "DSKTLAB-NER: Nested named entity recognition in Vietnamese text", "author": ["T.C.V. Nguyen", "T.S. Pham", "T.H. Vuong", "N.V. Nguyen", "M.V. Tran"], "venue": "Proceedings of VLSP, Hanoi, Vietnam, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Vietnamese named entity recognition at VLSP 2016 evaluation campaign", "author": ["T.S. Nguyen", "L.M. Nguyen", "X.C. Tran"], "venue": "Proceedings of VLSP, Hanoi, Vietnam, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Named entity recognition in Vietnamese text", "author": ["T.H. Le", "T.T.T. Nguyen", "T.H. Do", "X.T. Nguyen"], "venue": "Proceedings of VLSP, Hanoi, Vietnam, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end recurrent neural network models for Vietnamese named entity recognition: Word-level vs. characterlevel", "author": ["T.-H. Pham", "P. Le-Hong"], "venue": "Proceedings of PACLING, Yangon, Myanmar, 2017.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "Many statistical sequence models have been developed for sequence prediction, for example hidden Markov models (HMM) [1], [2], maximum entropy Markov models (MEMMs) [3], conditional random fields (CRFs) [4] or recurrent neural nets (RNNs) [5].", "startOffset": 239, "endOffset": 242}, {"referenceID": 3, "context": "1) Conditional Random Fields: Conditional Random Fields (CRF) [4] is a discriminative probabilistic framework, which directly model conditional probabilities of a tag sequence given a word sequence.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "Parameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "Parameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "Parameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "Parameter estimation in CRF can be done by using iterative scaling algorithms [6], [4], [7] or gradient-based methods [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "A feedback network called Long Short-Term Memory (LSTM) [9] was proposed to overcome these problems.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "We represent the word sequence of a sentence with a bidirectional LSTM [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Further details of the corpus are described in [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "In particular, some previous works have also integrated Vietnamese word embeddings to improve performance [12], [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "In particular, some previous works have also integrated Vietnamese word embeddings to improve performance [12], [13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "The tokenization process follows the method described in [14].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "We train the Mikolov\u2019s continuous Skip-gram model using the neural network and source code introduced in [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "The continuous skip-gram model itself is described in details in [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "In our experiments with the CRF model, we adopted CRFsuite [17], an implementation of linear-chain (first-order Markov) CRF.", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "We use default setting of CRFsuite in which the training algorithm is L-BFGS [18] and L2 regularization is used.", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "The Xavier initilizer is used for parameter initialization [19].", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "Because of the space limitation, we did not include maximum-entropy Markov models (MEMM) [3] and hidden Markov models (HMM) [1] in our comparison.", "startOffset": 89, "endOffset": 92}, {"referenceID": 0, "context": "Because of the space limitation, we did not include maximum-entropy Markov models (MEMM) [3] and hidden Markov models (HMM) [1] in our comparison.", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "For the comparison between MEMM, HMM and CRF, we can refer to the work [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 19, "context": "In particular, the work [20] investigated and compared MEMM and CRF.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "We also did not include bidirectional LSTM-CNNsCRF [21], the state-of-the-art end-to-end sequence labeling model, which combine bidirectional LSTM, CNN and CRF.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "In [22], Turian et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [23], the authors give an empirical study of MEMM for Vietnamese part-of-speech tagging with diffferent feature sets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "78% [24] in that shared task.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "Le-Hong [24] ME 88.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "[25] ME 84.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] LSTM 83.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] CRF 78.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Most recently, a more advanced end-to-end system for Vietnamese NER using LSTMs was proposed [28], which achieved an F1 score of 88.", "startOffset": 93, "endOffset": 97}], "year": 2017, "abstractText": "This paper presents an empirical study of two widely-used sequence prediction models, Conditional Random Fields (CRFs) and Long Short-Term Memory Networks (LSTMs), on two fundamental tasks for Vietnamese text processing, including part-of-speech tagging and named entity recognition. We show that a strong lower bound for labeling accuracy can be obtained by relying only on simple word-based features with minimal handcrafted feature engineering, of 90.65% and 86.03% performance scores on the standard test sets for the two tasks respectively. In particular, we demonstrate empirically the surprising efficiency of word embeddings in both of the two tasks, with both of the two models. We point out that the state-of-the-art LSTMs model does not always outperform significantly the traditional CRFs model, especially on moderate-sized data sets. Finally, we give some suggestions and discussions for efficient use of sequence labeling models in practical applications.", "creator": "LaTeX with hyperref package"}}}