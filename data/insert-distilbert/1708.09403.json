{"id": "1708.09403", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set", "abstract": "we first present a viable minimal feature set for transition - based dependency parsing, continuing a recent trend trend started by kiperwasser and geoffrey goldberg ( 2016a ) yang and cross and chang huang ( md 2016a ) of using bi - directional lstm features. yet we plug our minimal feature set into the dynamic - programming performance framework of huang and sagae ( 2010 ) \u2014 and kuhlmann et al. ( 2011 ) to produce the first implementation of worst - case o ( n ^'3 ) exact decoders for arc - hybrid and arc - eager transition systems. with our minimal variable features, we also present o ( n ^ 3 ) global training methods. finally, using ensembles including collectively our new parsers, we \" achieve \" the best unlabeled attachment score sequence reported ( to our knowledge ) on the chinese treebank and the \" second - best - in - class \" result on the english journal penn jersey treebank.", "histories": [["v1", "Wed, 30 Aug 2017 18:01:08 GMT  (97kb,D)", "http://arxiv.org/abs/1708.09403v1", "Proceedings of EMNLP, 2017. 12 pages"]], "COMMENTS": "Proceedings of EMNLP, 2017. 12 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tianze shi", "liang huang", "lillian lee"], "accepted": true, "id": "1708.09403"}, "pdf": {"name": "1708.09403.pdf", "metadata": {"source": "CRF", "title": "Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set", "authors": ["Tianze Shi", "Liang Huang", "Lillian Lee"], "emails": ["tianze@cs.cornell.edu", "liang.huang.sh@gmail.com", "llee@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding. But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration. But consulting many positions means that although polynomial-time exact-decoding algo-\nrithms do exist, having been introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6q (originally reported as Opn7q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming.\nRecently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1\nInspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to larger sets, and out-performs a single position. (Details regarding the situation with arc-standard can be found in \u00a72.)\nOur minimal feature set plugs into Huang and Sagae\u2019s and Kuhlmann et al.\u2019s dynamic program-\n1We note that K&G were not focused on minimizing positions, although they explicitly noted the implications of doing so: \u201cWhile not explored in this work, [fewer positions] results in very compact state signatures, [which is] very appealing for use in transition-based parsers that employ dynamicprogramming search\u201d (pg. 319). C&H also noted in their follow-up (Cross and Huang, 2016b) the possibility of future work using dynamic programming thanks to simple features.\nar X\niv :1\n70 8.\n09 40\n3v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\n01 7\nming framework to produce the first implementation of Opn3q exact decoders for arc-hybrid and arc-eager parsers. We also enable and implement Opn3q global training methods. Empirically, ensembles containing our minimal-feature, globallytrained and exactly-decoded models produce the best unlabeled attachment score (UAS) reported (to our knowledge) on the Chinese Treebank and the \u201csecond-best-in-class\u201d result on the English Penn Treebank.2\nAdditionally, we provide a slight update to the theoretical connections previously drawn by Go\u0301mez-Rodr\u0131\u0301guez, Carroll, and Weir (2008, 2011) between TBDPs and the graph-based dependency parsing algorithms of Eisner (1996) and Eisner and Satta (1999), including results regarding the arc-eager parsing system."}, {"heading": "2 A Minimal Feature Set", "text": "TBDPs incrementally process a sentence by making transitions through search states representing parser configurations. Three of the main transition systems in use today (formal introduction in \u00a73.1) all maintain the following two data structures in their configurations: (1) a stack of partially parsed subtrees and (2) a buffer (mostly) of unprocessed sentence tokens.\nTo featurize configurations for use in a scoring function, it is common to have features that extract information about the first several elements on the stack and the buffer, such as their word forms and part-of-speech (POS) tags. We refer to these as positional features, as each feature relates to a particular position in the stack or buffer. Typically, millions of sparse indicator features (often developed via manual engineering) are used.\nIn contrast, Chen and Manning (2014) introduce a feature set consisting of dense word-, POS-, and dependency-label embeddings. While dense, these features are for the same 18 positions that have been typically used in prior work. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) adopt bi-directional LSTMs, which have nice expressiveness and context-sensitivity properties, to reduce the number of positions considered down to four and three,\n2Our ideas were subsequently adapted to the labeled setting by Shi, Wu, Chen, and Cheng (2017) in their submission to the CoNLL 2017 shared task on Universal Dependencies parsing. Their team achieved the second-highest labeled attachment score in general and had the top average performance on the surprise languages.\nfor different transition systems, respectively. This naturally begs the question, what is the lower limit on the number of positional features necessary for a parser to perform well? Kiperwasser and Goldberg (2016a) reason that for the arc-hybrid system, the first and second items on the stack and the first buffer item \u2014 denoted by s0, s1, and b0, respectively \u2014 are required; they additionally include the third stack item, s2, because it may not be adjacent to the others in the original sentence. For arc-standard, Cross and Huang (2016a) argue for the necessity of s0, s1, and b0.\nWe address the lower-limit question empirically, and find that, surprisingly, two positions suffice for the greedy arc-eager and arc-hybrid parsers. We also provide empirical support for Cross and Huang\u2019s argument for the necessity of three features for arc-standard. In the rest of this section, we explain our experiments, run only on an English development set, that support this conclusion; the results are depicted in Table 1. We later explore the implementation implications in \u00a73-4 and then test-set parsing-accuracy in \u00a76.\nWe employ the same model architecture as Kiperwasser and Goldberg (2016a). Specifically, we first use a bi-LSTM to encode an n-token sentence, treated as a sequence of per-token concatenations of word- and POS-tag embeddings, into a sequence of vectors r \u00d1\u00d0 w1, . . . , \u00d1\u00d0 wns, where each \u00d1\u00d0 wi\nis the output of the bi-LSTM at time step i. (The double-arrow notation for these vectors emphasizes the bi-directionality of their origin). Then, for a given parser configuration, stack positions are represented by \u00d1\u00d0 s j , defined as \u00d1\u00d0 wipsjq where ipsjq gives the position in the sentence of the token that is the head of the tree in sj . Similarly, buffer positions are represented by \u00d1\u00d0 b j , defined as \u00d1\u00d0 wipbjq for the token at buffer position j. Finally, as in Chen and Manning (2014), we use a multilayer perceptron to score possible transitions from the given configuration, where the input is the concatenation of some selection of the \u00d1\u00d0 s js and \u00d1\u00d0 b ks. We use greedy decoders, and train the models with dynamic oracles (Goldberg and Nivre, 2013).\nTable 1 reports the parsing accuracy that results for feature sets of size four, three, two, and one for three commonly-used transition systems. The data is the development section of the English Penn Treebank (PTB), and experimental settings are as described in our other experimental section, \u00a76. We see that we can go down to three or, in the arc-hybrid and arc-eager transition systems, even two positions with very little loss in performance, but not further. We therefore call t \u00d1\u00d0 s0, \u00d1\u00d0 b 0u our minimal feature set with respect to arc-hybrid and arc-eager, and empirically confirm that Cross and Huang\u2019s t \u00d1\u00d0 s0, \u00d1\u00d0 s 1, \u00d1\u00d0 b 0u is minimal for arc-standard; see Table 1 for a summary.3"}, {"heading": "3 Dynamic Programming for TBDPs", "text": "As stated in the introduction, our minimal feature set from \u00a72 plugs into Huang and Sagae and Kuhlmann et al.\u2019s dynamic programming (DP) framework. To help explain the connection, this section provides an overview of the DP framework. We draw heavily from the presentation of Kuhlmann et al. (2011)."}, {"heading": "3.1 Three Transition Systems", "text": "Transition-based parsing (Nivre, 2008; Ku\u0308bler et al., 2009) is an incremental parsing framework based on transitions between parser configura-\n3We tentatively conjecture that the following might explain the observed phenomena, but stress that we don\u2019t currently see a concrete way to test the following hypothesis. With t \u00d1\u00d0 s 0, \u00d1\u00d0 b 0u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing information. In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, re\u00f1, introduced in \u00a73.1); perhaps \u00d1\u00d0 s 1 is therefore not so crucial for arc-hybrid in practice?\ntions. For a sentence to be parsed, the system starts from a corresponding initial configuration, and attempts to sequentially apply transitions until a configuration corresponding to a full parse is produced. Formally, a transition system is defined as S \u201c pC, T, cs, C\u03c4 q, where C is a nonempty set of configurations, each t P T : C \u00e1 C is a transition function between configurations, cs is an initialization function that maps an input sentence to an initial configuration, and C\u03c4 \u010e C is a set of terminal configurations.\nAll systems we consider share a common tripartite representation for configurations: when we write c \u201c p\u03c3, \u03b2,Aq for some c P C, we are referring to a stack \u03c3 of partially parsed subtrees; a buffer \u03b2 of unprocessed tokens and, optionally, at its beginning, a subtree with only left descendants; and a set A of elements ph,mq, each of which is an attachment (dependency arc) with head h and modifier m.4 We write m\u00f0h to indicate that m left-modifies h, and h\u00f1m to indicate that m rightmodifies h. For a sentence w \u201c w1, ..., wn, the initial configuration is p\u03c30, \u03b20, A0q, where \u03c30 and A0 are empty and \u03b20 \u201c rROOT|w1, ..., wns; ROOT is a special node denoting the root of the parse tree5 (vertical bars are a notational convenience for indicating different parts of the buffer or stack; our convention is to depict the buffer first element leftmost, and to depict the stack first element rightmost). All terminal configurations have an empty buffer and a stack containing only ROOT.\nArc-Standard The arc-standard system (Nivre, 2004) is motivated by bottom-up parsing: each dependent has to be complete before being attached. The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (re\u00f1, reduce and attach a right modifier), and left-reduce (re\u00f0, reduce and attach a left modifier), are defined as:\nshrp\u03c3, b0|\u03b2,Aqs \u201c p\u03c3|b0, \u03b2, Aq re\u00f1rp\u03c3|s1|s0, \u03b2, Aqs \u201c p\u03c3|s1, \u03b2, AY tps1, s0quq re\u00f0rp\u03c3|s1|s0, \u03b2, Aqs \u201c p\u03c3|s0, \u03b2, AY tps0, s1quq\nArc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; Go\u0301mez-Rodr\u0131\u0301guez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and re\u00f1 as arc-standard, but forces\n4For simplicity, we only present unlabeled parsing here. See Shi et al. (2017) for labeled-parsing results.\n5Other presentations place ROOT at the end of the buffer or omit it entirely (Ballesteros and Nivre, 2013).\nthe collection of left modifiers before right modifiers via its b0-modifier re\u00f0 transition. This contrasts with arc-standard, where the attachment of left and right modifiers can be interleaved on the stack.\nshrp\u03c3, b0|\u03b2,Aqs \u201c p\u03c3|b0, \u03b2, Aq re\u00f1rp\u03c3|s1|s0, \u03b2, Aqs \u201c p\u03c3|s1, \u03b2, AY tps1, s0quq re\u00f0rp\u03c3|s0, b0|\u03b2,Aqs \u201c p\u03c3, b0|\u03b2,AY tpb0, s0quq\nArc-Eager In contrast to the former two systems, the arc-eager system (Nivre, 2003) makes attachments as early as possible \u2014 even if a modifier has not yet received all of its own modifiers. This behavior is accomplished by decomposing the right-reduce transition into two independent transitions, one making the attachment (ra) and one reducing the right-attached child (re).\nshrp\u03c3, b0|\u03b2,Aqs \u201c p\u03c3|b0, \u03b2, Aq re\u00f0rp\u03c3|s0, b0|\u03b2,Aqs \u201c p\u03c3, b0|\u03b2,AY tpb0, s0quq\n(precondition: s0 not attached to any word)\nrarp\u03c3|s0, b0|\u03b2,Aqs \u201c p\u03c3|s0|b0, \u03b2, AY tps0, b0quq rerp\u03c3|s0, \u03b2, Aqs \u201c p\u03c3, \u03b2,Aq\n(precondition: s0 has been attached to its head)"}, {"heading": "3.2 Deduction and Dynamic Programming", "text": "Kuhlmann et al. (2011) reformulate the three transition systems just discussed as deduction systems (Pereira and Warren, 1983; Shieber et al., 1995), wherein transitions serve as inference rules; these are given as the lefthand sides of the first three subfigures in Figure 1. For a given w \u201c w1, ..., wn, assertions take the form ri, j, ks (or, when applicable, a two-index shorthand to be discussed soon), meaning that there exists a sequence of transitions that, starting from a configuration wherein headps0q \u201c wi, results in an ending configuration wherein headps0q \u201c wj and headpb0q \u201c wk. If we define w0 as ROOT and wn`1 as an endof-sentence marker, then the goal theorem can be stated as r0, 0, n` 1s.\nFor arc-standard, we depict an assertion ri, h, ks as a subtree whose root (head) is the token at h. Assertions of the form ri, i, ks play an important role for arc-hybrid and arc-eager, and we employ the special shorthand ri, ks for them in Figure 1. In that figure, we also graphically depict such situations as two consecutive half-trees with roots wi and wk, where all tokens between i and k are already attached. The superscript b in an arc-eager\nassertion rib, js is an indicator variable for whether wi has been attached to its head (b \u201c 1) or not (b \u201c 0) after the transition sequence is applied.\nKuhlmann et al. (2011) show that all three deduction systems can be directly \u201ctabularized\u201d and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incorporating complex features (we return to this later), time and space needs are low-order polynomial. Specifically, as the two-index shorthand ri, js suggests, arc-eager and arc-hybrid systems can be implemented to take Opn2q space and Opn3q time; the arc-standard system requires Opn3q space and Opn4q time (if one applies the so-called hook trick (Eisner and Satta, 1999)).\nSince an Opn4q running time is not sufficiently practical even in the simple-feature case, in the remainder of this paper we consider only the archybrid and arc-eager systems, not arc-standard."}, {"heading": "4 Practical Optimal Algorithms Enabled", "text": "By Our Minimal Feature Set\nUntil now, no one had suggested a set of positional features that was both information-rich enough for accurate parsing and small enough to obtain the Opn3q running-time promised above. Fortunately, our bi-LSTM-based t \u00d1\u00d0 s0, \u00d1\u00d0 b 0u feature set qualifies, and enables the fast optimal procedures described in this section."}, {"heading": "4.1 Exact Decoding", "text": "Given an input sentence, a TBDP must choose among a potentially exponential number of corresponding transition sequences. We assume access to functions ft that score individual configurations, where these functions are indexed by the transition functions t P T . For a fixed transition sequence t \u201c t1, t2, . . ., we use ci to denote the configuration that results after applying ti.\nTypically, for efficiency reasons, greedy left-toright decoding is employed: the next transition t\u02dai out of ci\u00b41 is arg maxt ftpci\u00b41q, so that past and future decisions are not taken into account. The score F ptq for the transition sequence is induced by summing the relevant ftipci\u00b41q values.\nHowever, our use of minimal feature sets enables direct computation of an argmax over the entire space of transition sequences, arg maxt F ptq, via dynamic programming, because our positions don\u2019t rely on any information \u201coutside\u201d the deduction rule indices, thus eliminating the need for ad-\nditional state-keeping. We show how to integrate the scoring functions for the arc-eager system; the arc-hybrid system is handled similarly. The score-annotated rules are as follows: rib, js : v\nrj0, j ` 1s : 0 pshq rkb, is : v1 ri0, js : v2 rkb, js : v1 ` v2 `\u2206 pre\u00f0q\nwhere \u2206 \u201c fshp \u00d1\u00d0 wk, \u00d1\u00d0 wiq ` fre\u00f0p \u00d1\u00d0 wi, \u00d1\u00d0 wjq \u2014 abusing notation by referring to configurations by their features. The left-reduce rule says that we can first take the sequence of transitions asserted by rkb, is, which has a score of v1, and then a shift transition moving wi from b0 to s0. This means that the initial condition for ri0, js is met, so we can take the sequence of transitions asserted by ri0, js\u2014 say it has score v2 \u2014 and finally a left-reduce transition to finish composing the larger transition sequence. Notice that the scores for sh and ra are 0, as the scoring of these transitions is accounted for by reduce rules elsewhere in the sequence."}, {"heading": "4.2 Global Training", "text": "We employ large-margin training that considers each transition sequence globally. Formally, for a training sentence w \u201c w1, . . . , wn with gold transition sequence tgold, our loss function is\nmax t\n\u00b4 F ptq ` costptgold, tq \u00b4 F ptgoldq \u00af\nwhere costptgold, tq is a custom margin for taking t instead of tgold \u2014 specifically, the number of mis-attached nodes. Computing this max can again be done efficiently with a slight modification to the scoring of reduce transitions:\nrkb, is : v1 ri0, js : v2 rkb, js : v1 ` v2 `\u22061 pre\u00f0q\nwhere \u22061 \u201c \u2206 ` 1 pheadpwiq \u2030 wjq. This lossaugmented inference or cost-augmented decoding (Taskar et al., 2005; Smith, 2011) technique has previously been applied to graph-based parsing by Kiperwasser and Goldberg (2016a).\nEfficiency Note The computation decomposes into two parts: scoring all feature combinations, and using DP to find a proof for the goal theorem in the deduction system. Time-complexity analysis is usually given in terms of the latter, but the former might have a large constant factor, such as 104 or worse for neural-network-based scoring\nfunctions. As a result, in practice, with a small n, scoring with the feature set t \u00d1\u00d0 s 0, \u00d1\u00d0 b 0u (Opn2q) can be as time-consuming as the decoding steps (Opn3q) for the arc-hybrid and arc-eager systems."}, {"heading": "5 Theoretical Connections", "text": "Our minimal feature set brings implementation of practical optimal algorithms to TBDPs, whereas previously only graph-based dependency parsers (GBDPs) \u2014 a radically different, non-incremental paradigm \u2014 enjoyed the ability to deploy them. Interestingly, for both the transition- and graphbased paradigms, the optimal algorithms build dependency trees bottom-up from local structures. It is thus natural to wonder if there are deeper, more formal connections between the two.\nIn previous work, Kuhlmann et al. (2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach. Go\u0301mez-Rodr\u0131\u0301guez et al. (2008, 2011) formally prove that sequences of steps in the edgefactored GBDP (Eisner, 1996) can be used to emulate any individual step in the arc-hybrid system (Yamada and Matsumoto, 2003) and the Eisner and Satta (1999, Figure 1d) version. However, they did not draw an explicitly direct connection between Eisner and Satta (1999) and TBDPs.\nHere, we provide an update to these previous findings, stated in terms of the expressiveness of scoring functions, considered as parameterization.\nFor the edge-factored GBDP, we write the score for an edge as fGp \u00d1\u00d0 h, \u00d1\u00d0 mq, where h is the head and m the modifier. A tree\u2019s score is the sum of its edge scores. We say that a parameterized dependency parsing model A contains model B if for every instance of parameterization in model B, there exists an instance of model A such that the two models assign the same score to every parse tree. We claim:\nLemma 1. The arc-eager model presented in \u00a74.1 contains the edge-factored model.\nProof Sketch. Consider a given edge-factored GBDP parameterized by fG. For any parse tree, every edge i\u00f0j involves two deduction rules, and their contribution to the score of the final proof is fsh( \u00d1\u00d0 wk, \u00d1\u00d0 wi) ` fre\u00f0p \u00d1\u00d0 wi, \u00d1\u00d0 wjq. We set fsh( \u00d1\u00d0 wk, \u00d1\u00d0 wi) \u201c 0 and fre\u00f0p \u00d1\u00d0 wi, \u00d1\u00d0 wjq \u201c fGp \u00d1\u00d0 wj , \u00d1\u00d0 wiq. Similarly, for edges k\u00f1i in the other direction, we set\nfra( \u00d1\u00d0 wk, \u00d1\u00d0 wi) \u201c fGp \u00d1\u00d0 wk, \u00d1\u00d0 wiq and frep \u00d1\u00d0 wi, \u00d1\u00d0 wjq \u201c 0. The parameterization we arrive at emulates exactly the scoring model of fG.\nWe further claim that the arc-eager model is more expressive than not only the edge-factored GBDP, but also the arc-hybrid model in our paper.\nLemma 2. The arc-eager model contains the archybrid model.\nProof Sketch. We leverage the fact that the arceager model divides the sh transition in the archybrid model into two separate transitions, sh and ra. When we constrain the parameters fsh \u201c fra in the arc-eager model, the model hypothesis space becomes exactly the same as arc-hybrid\u2019s.\nThe extra expressiveness of the arc-eager model comes from the scoring functions fsh and fre that capture structural contexts other than headmodifier relations. Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions."}, {"heading": "6 Experiments", "text": "Data and Evaluation We experimented with English and Chinese. For English, we used the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (via the Stanford parser 3.3.0) of the Penn Treebank (Marcus et al., 1993, PTB). As is standard, we used \u00a72-21 of the Wall Street Journal for training, \u00a722 for development,\nand \u00a723 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003). For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008). We adopted the CTB\u2019s goldstandard tokenization and POS tags. We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM). Following prior work, all punctuation is excluded from evaluation. For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation.\nImplementation Details Our model structures reproduce those of Kiperwasser and Goldberg (2016a). We use 2-layer bi-directional LSTMs with 256 hidden cell units. Inputs are concatenations of 28-dimensional randomly-initialized partof-speech embeddings and 100-dimensional word vectors initialized from GloVe vectors (Pennington et al., 2014) (English) and pre-trained skipgram-model vectors (Mikolov et al., 2013) (Chinese). The concatenation of the bi-LSTM feature vectors is passed through a multi-layer perceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh. We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.6 All parameters except the word embed-\n6For bi-LSTM input and recurrent connections, we consider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u.\ndings are initialized uniformly (Glorot and Bengio, 2010). Approximately 1,000 tokens form a mini-batch for sub-gradient computation. We train each model for 20 epochs and perform model selection based on development UAS. The proposed structured loss function is optimized via Adam (Kingma and Ba, 2015). The neural network computation is based on the python interface to DyNet (Neubig et al., 2017), and the exact decoding algorithms are implemented in Cython.7\nMain Results We implement exact decoders for the arc-hybrid and arc-eager systems, and present the test performance of different model configurations in Table 2, comparing global models with local models. All models use the same decoder for testing as during the training process. Though no global decoder for the arc-standard system has been explored in this paper, its local models are listed for comparison. We also include an edgefactored graph-based model, which is conventionally trained globally. The edge-factored model scores bi-LSTM features for each head-modifier pair; a maximum spanning tree algorithm is used to find the tree with the highest sum of edge scores. For this model, we use Dozat and Man-\n7See https://github.com/tzshi/dp-parser-emnlp17 .\nning\u2019s (2017) biaffine scoring model, although in our case the model size is smaller.8\nAnalogously to the dev-set results given in \u00a72, on the test data, the minimal feature sets perform as well as larger ones in locally-trained models. And there exists a clear trend of global models outperforming local models for the two different transition systems on both datasets. This illustrates the effectiveness of exact decoding and global training. Of the three types of global models, the arceager arguably has the edge, an empirical finding resonating with our theoretical comparison of their model expressiveness.\nComparison with State-of-the-Art Models Figure 2 compares our algorithms\u2019 results with those of the state-of-the-art.9 Our models are competitive and an ensemble of 15 globallytrained models (5 models each for arc-eager DP, arc-hybrid DP and edge-factored) achieves 95.33 and 90.22 on PTB and CTB, respectively, reach-\n8The same architecture and model size as other transitionbased global models is used for fair comparison.\n9We exclude Choe and Charniak (2016), Kuncoro et al. (2017) and Liu and Zhang (2017), which convert constituentbased parses to dependency parses. They produce higher PTB UAS, but access more training information and do not directly apply to datasets without constituency annotation.\ning the highest reported UAS on the CTB dataset, and the second highest reported on the PTB dataset among dependency-based approaches."}, {"heading": "7 Related Work Not Yet Mentioned", "text": "Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment.\nNeural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power. Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing.\nRecurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large."}, {"heading": "8 Concluding Remarks", "text": "In this paper, we have shown the following.\n\u2022 The bi-LSTM-powered feature set t \u00d1\u00d0 s 0, \u00d1\u00d0 b 0u\nis minimal yet highly effective for arc-hybrid and arc-eager transition-based parsing.\n\u2022 Since DP algorithms for exact decoding (Huang and Sagae, 2010; Kuhlmann et al.,\n2011) have a run-time dependence on the number of positional features, using our mere two effective positional features results in a running time of Opn3q, feasible for practice.\n\u2022 Combining exact decoding with global training \u2014 which is also enabled by our minimal feature set \u2014 with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among \u201cpurely\u201d dependency-based approaches.\nThere are many directions for further exploration. Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems. It would also be interesting to further investigate relationships between graph-based and dependency-based parsing. In \u00a75 we have mentioned important earlier work in this regard, and provided an update to those formal findings.\nIn our work, we have brought exact decoding, which was formerly the province solely of graphbased parsing, to the transition-based paradigm. We hope that the future will bring more inspiration from an integration of the two perspectives.\nAcknowledgments: an author-reviewer success story We sincerely thank all the reviewers for their extraordinarily careful and helpful comments. Indeed, this paper originated as a short paper submission by TS&LL to ACL 2017, where an anonymous reviewer explained in the review comments how, among other things, the DP runtime could be improved from Opn4q to Opn3q. In their author response, TS&LL invited the reviewer to co-author, suggesting that they ask the conference organizers to make the connection between anonymous reviewer and anonymous authors. All three of us are truly grateful to PC co-chair Regina Barzilay for implementing this idea, bringing us together!\nWe also thank Kai Sun for help with Chinese word vectors, and Xilun Chen, Yao Cheng, Dezhong Deng, Juneki Hong, Jon Kleinberg, Ryan McDonald, Ashudeep Singh, and Kai Zhao for discussions and suggestions. TS and LL were supported in part by a Google focused research grant to Cornell University. LH was supported in part by NSF IIS-1656051, DARPA N66001-17-24030, and a Google Faculty Research Award."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the Annual Meeting of the Association", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Training with exploration improves a greedy stack LSTM parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 2005\u20132010.", "citeRegEx": "Ballesteros et al\\.,? 2016", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "Going to the roots of dependency parsing", "author": ["Miguel Ballesteros", "Joakim Nivre."], "venue": "Computational Linguistics, 39(1):5\u201313.", "citeRegEx": "Ballesteros and Nivre.,? 2013", "shortCiteRegEx": "Ballesteros and Nivre.", "year": 2013}, {"title": "Experiments with a higherorder projective dependency parser", "author": ["Xavier Carreras."], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 957\u2013961.", "citeRegEx": "Carreras.,? 2007", "shortCiteRegEx": "Carreras.", "year": 2007}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 740\u2013750, Doha, Qatar.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Bi-directional attention with agreement for dependency parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 2204\u20132214.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 2331\u20132336, Austin, Texas.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Programming languages and their compilers: Preliminary notes", "author": ["John Cocke."], "venue": "Technical report, Courant Institute of Mathematical Sciences, New York University.", "citeRegEx": "Cocke.,? 1969", "shortCiteRegEx": "Cocke.", "year": 1969}, {"title": "Incremental parsing with minimal features using bi-directional LSTM", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 32\u201337.", "citeRegEx": "Cross and Huang.,? 2016a", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1\u201311.", "citeRegEx": "Cross and Huang.,? 2016b", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "Proceedings of the 5th International Conference on Learning Representations.", "citeRegEx": "Dozat and Manning.,? 2017", "shortCiteRegEx": "Dozat and Manning.", "year": 2017}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason Eisner."], "venue": "Proceedings of the 16th International Conference on Computational Linguistics, pages 340\u2013345.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Efficient parsing for bilexical context-free grammars and head automaton grammars", "author": ["Jason Eisner", "Giorgio Satta."], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 457\u2013464.", "citeRegEx": "Eisner and Satta.,? 1999", "shortCiteRegEx": "Eisner and Satta.", "year": 1999}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems, pages 1019\u20131027.", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "A dynamic oracle for arc-eager dependency parsing", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Proceedings of the 24th International Conference on Computational Linguistics, pages 959\u2013976.", "citeRegEx": "Goldberg and Nivre.,? 2012", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2012}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Transactions of the Association for Computational Linguistics, 1:403\u2013414.", "citeRegEx": "Goldberg and Nivre.,? 2013", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2013}, {"title": "A deductive approach to dependency parsing", "author": ["Carlos G\u00f3mez-Rodr\u0131\u0301guez", "John Carroll", "David Weir"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technology,", "citeRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.", "year": 2008}, {"title": "Dependency parsing schemata and mildly non-projective dependency parsing", "author": ["Carlos G\u00f3mez-Rodr\u0131\u0301guez", "John Carroll", "David Weir"], "venue": "Computational Linguistics,", "citeRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.", "year": 2011}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5-6):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["James Henderson."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 24\u201331.", "citeRegEx": "Henderson.,? 2003", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson."], "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 95\u2013102.", "citeRegEx": "Henderson.,? 2004", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Liang Huang", "Kenji Sagae."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077\u2013 1086.", "citeRegEx": "Huang and Sagae.,? 2010", "shortCiteRegEx": "Huang and Sagae.", "year": 2010}, {"title": "An efficient recognition and syntax-analysis algorithm for context-free languages", "author": ["Tadao Kasami."], "venue": "Technical report, Hawaii University Honolulu Department of Electrical Engineering.", "citeRegEx": "Kasami.,? 1965", "shortCiteRegEx": "Kasami.", "year": 1965}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the 4th International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics, 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016a", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Easyfirst dependency parsing with hierarchical tree LSTMs", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics, 4:445\u2013461.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016b", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423\u2013430.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Dependency parsing, volume 2 of Synthesis Lectures on Human Language Technologies", "author": ["Sandra K\u00fcbler", "Ryan McDonald", "Joakim Nivre."], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "K\u00fcbler et al\\.,? 2009", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2009}, {"title": "Dynamic programming algorithms for transition-based dependency parsers", "author": ["Marco Kuhlmann", "Carlos G\u00f3mez-Rodr\u0131\u0301guez", "Giorgio Satta"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Kuhlmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kuhlmann et al\\.", "year": 2011}, {"title": "What do recurrent neural network grammars learn about syntax", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": "In Proceedings of the 15th Conference of the European Chapter", "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Distilling an ensemble of greedy dependency parsers into one MST parser", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Kuncoro et al\\.,? 2016", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2016}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 729\u2013739.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Global neural CCG parsing with optimality guarantees", "author": ["Kenton Lee", "Mike Lewis", "Luke Zettlemoyer."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 2366\u20132376.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "In-order transition-based constituent parsing", "author": ["Jiangming Liu", "Yue Zhang."], "venue": "Transactions of the Association for Computational Linguistics. To appear.", "citeRegEx": "Liu and Zhang.,? 2017", "shortCiteRegEx": "Liu and Zhang.", "year": 2017}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Stanford typed dependencies manual", "author": ["Marie-Catherine de Marneffe", "Christopher D. Manning."], "venue": "Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 81\u201388.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "DyNet: The dynamic neural network toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv preprint", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the 8th International Workshop on Parsing Technologies, pages 149\u2013160.", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50\u201357.", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics, 34(4):513\u2013553.", "citeRegEx": "Nivre.,? 2008", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Parsing as deduction", "author": ["Fernando C.N. Pereira", "David H.D. Warren."], "venue": "Proceedings of the 21st Annual Meeting on Association for Computational Linguistics, pages 137\u2013144.", "citeRegEx": "Pereira and Warren.,? 1983", "shortCiteRegEx": "Pereira and Warren.", "year": 1983}, {"title": "A best-first probabilistic shift-reduce parser", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 691\u2013698.", "citeRegEx": "Sagae and Lavie.,? 2006", "shortCiteRegEx": "Sagae and Lavie.", "year": 2006}, {"title": "Dependency parsing and domain adaptation with LR models and parser ensembles", "author": ["Kenji Sagae", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Sagae and Tsujii.,? \\Q2007\\E", "shortCiteRegEx": "Sagae and Tsujii.", "year": 2007}, {"title": "Combining global models for parsing Universal Dependencies", "author": ["Tianze Shi", "Felix G. Wu", "Xilun Chen", "Yao Cheng."], "venue": "Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 31\u201339, Van-", "citeRegEx": "Shi et al\\.,? 2017", "shortCiteRegEx": "Shi et al\\.", "year": 2017}, {"title": "Principles and implementation of deductive parsing", "author": ["Stuart M. Shieber", "Yves Schabes", "Fernando C.N. Pereira."], "venue": "The Journal of Logic Programming, 24(1):3\u201336.", "citeRegEx": "Shieber et al\\.,? 1995", "shortCiteRegEx": "Shieber et al\\.", "year": 1995}, {"title": "Linguistic Structure Prediction, volume 13 of Synthesis Lectures on Human Language Technologies", "author": ["Noah A. Smith."], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "Smith.,? 2011", "shortCiteRegEx": "Smith.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A minimal span-based neural constituent parser", "author": ["Mitchell Stern", "Jacob Andreas", "Dan Klein."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. To appear.", "citeRegEx": "Stern et al\\.,? 2017", "shortCiteRegEx": "Stern et al\\.", "year": 2017}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin."], "venue": "Proceedings of the 22nd International Conference on Machine Learning, pages 896\u2013903.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Optimal shift-reduce constituent parsing with structured perceptron", "author": ["Quang Le Thang", "Hiroshi Noji", "Yusuke Miyao."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Thang et al\\.,? 2015", "shortCiteRegEx": "Thang et al\\.", "year": 2015}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Efficient structured inference for transition-based parsing with neural networks and error states", "author": ["Ashish Vaswani", "Kenji Sagae."], "venue": "Transactions of the Association for Computational Linguistics, 4:183\u2013 196.", "citeRegEx": "Vaswani and Sagae.,? 2016", "shortCiteRegEx": "Vaswani and Sagae.", "year": 2016}, {"title": "Graph-based dependency parsing with bidirectional LSTM", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2306\u20132315.", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Building a large-scale annotated Chinese corpus", "author": ["Nianwen Xue", "Fu-Dong Chiou", "Martha Palmer."], "venue": "Proceedings of the 19th International Conference on Computational Linguistics.", "citeRegEx": "Xue et al\\.,? 2002", "shortCiteRegEx": "Xue et al\\.", "year": 2002}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proceedings of the 8th International Workshop on Parsing Technologies, pages 195\u2013206.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "Recognition and parsing of context-free languages in time n", "author": ["Daniel H. Younger."], "venue": "Information and Control, 10(2):189 \u2013 208.", "citeRegEx": "Younger.,? 1967", "shortCiteRegEx": "Younger.", "year": 1967}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 562\u2013571.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Computational Linguistics, 37(1):105\u2013151.", "citeRegEx": "Zhang and Clark.,? 2011", "shortCiteRegEx": "Zhang and Clark.", "year": 2011}, {"title": "Optimal incremental parsing via best-first dynamic programming", "author": ["Kai Zhao", "James Cross", "Liang Huang."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 758\u2013768.", "citeRegEx": "Zhao et al\\.,? 2013", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features.", "startOffset": 117, "endOffset": 150}, {"referenceID": 8, "context": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features.", "startOffset": 154, "endOffset": 178}, {"referenceID": 8, "context": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al.", "startOffset": 154, "endOffset": 314}, {"referenceID": 8, "context": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case Opn3q exact decoders for arc-hybrid and arceager transition systems.", "startOffset": 154, "endOffset": 341}, {"referenceID": 58, "context": "But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016).", "startOffset": 285, "endOffset": 344}, {"referenceID": 11, "context": "But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016).", "startOffset": 285, "endOffset": 344}, {"referenceID": 0, "context": "But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016).", "startOffset": 285, "endOffset": 344}, {"referenceID": 0, "context": ", 2015; Andor et al., 2016). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration. But consulting many positions means that although polynomial-time exact-decoding algorithms do exist, having been introduced by Huang and Sagae (2010) and Kuhlmann et al.", "startOffset": 8, "endOffset": 471}, {"referenceID": 0, "context": ", 2015; Andor et al., 2016). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration. But consulting many positions means that although polynomial-time exact-decoding algorithms do exist, having been introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time.", "startOffset": 8, "endOffset": 498}, {"referenceID": 0, "context": ", 2015; Andor et al., 2016). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration. But consulting many positions means that although polynomial-time exact-decoding algorithms do exist, having been introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6q (originally reported as Opn7q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming.", "startOffset": 8, "endOffset": 911}, {"referenceID": 23, "context": "Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well.", "startOffset": 10, "endOffset": 43}, {"referenceID": 8, "context": "Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well.", "startOffset": 47, "endOffset": 71}, {"referenceID": 9, "context": "C&H also noted in their follow-up (Cross and Huang, 2016b) the possibility of future work using dynamic programming thanks to simple features.", "startOffset": 34, "endOffset": 58}, {"referenceID": 12, "context": "Additionally, we provide a slight update to the theoretical connections previously drawn by G\u00f3mez-Rodr\u0131\u0301guez, Carroll, and Weir (2008, 2011) between TBDPs and the graph-based dependency parsing algorithms of Eisner (1996) and Eisner and Satta (1999), including results regarding the arc-eager parsing system.", "startOffset": 208, "endOffset": 222}, {"referenceID": 12, "context": "Additionally, we provide a slight update to the theoretical connections previously drawn by G\u00f3mez-Rodr\u0131\u0301guez, Carroll, and Weir (2008, 2011) between TBDPs and the graph-based dependency parsing algorithms of Eisner (1996) and Eisner and Satta (1999), including results regarding the arc-eager parsing system.", "startOffset": 208, "endOffset": 250}, {"referenceID": 4, "context": "In contrast, Chen and Manning (2014) introduce a feature set consisting of dense word-, POS-, and dependency-label embeddings.", "startOffset": 13, "endOffset": 37}, {"referenceID": 4, "context": "In contrast, Chen and Manning (2014) introduce a feature set consisting of dense word-, POS-, and dependency-label embeddings. While dense, these features are for the same 18 positions that have been typically used in prior work. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) adopt bi-directional LSTMs, which have nice expressiveness and context-sensitivity properties, to reduce the number of positions considered down to four and three,", "startOffset": 13, "endOffset": 273}, {"referenceID": 4, "context": "In contrast, Chen and Manning (2014) introduce a feature set consisting of dense word-, POS-, and dependency-label embeddings. While dense, these features are for the same 18 positions that have been typically used in prior work. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) adopt bi-directional LSTMs, which have nice expressiveness and context-sensitivity properties, to reduce the number of positions considered down to four and three,", "startOffset": 13, "endOffset": 301}, {"referenceID": 24, "context": "Bottom: sizes of the minimal feature sets in Kiperwasser and Goldberg (2016a), Cross and Huang (2016a), and our work.", "startOffset": 45, "endOffset": 78}, {"referenceID": 8, "context": "Bottom: sizes of the minimal feature sets in Kiperwasser and Goldberg (2016a), Cross and Huang (2016a), and our work.", "startOffset": 79, "endOffset": 103}, {"referenceID": 24, "context": "This naturally begs the question, what is the lower limit on the number of positional features necessary for a parser to perform well? Kiperwasser and Goldberg (2016a) reason that for the arc-hybrid system, the first and second items on the stack and the first buffer item \u2014 denoted by s0, s1, and b0, respectively \u2014 are required; they additionally include the third stack item, s2, because it may not be adjacent to the others in the original sentence.", "startOffset": 135, "endOffset": 168}, {"referenceID": 8, "context": "For arc-standard, Cross and Huang (2016a) argue for the necessity of s0, s1, and b0.", "startOffset": 18, "endOffset": 42}, {"referenceID": 26, "context": "We employ the same model architecture as Kiperwasser and Goldberg (2016a). Specifically, we first use a bi-LSTM to encode an n-token sentence, treated as a sequence of per-token concatenations of word- and POS-tag embeddings, into a", "startOffset": 41, "endOffset": 74}, {"referenceID": 4, "context": "Finally, as in Chen and Manning (2014), we use a multilayer perceptron to score possible transitions from the given configuration, where the input is the con-", "startOffset": 15, "endOffset": 39}, {"referenceID": 17, "context": "We use greedy decoders, and train the models with dynamic oracles (Goldberg and Nivre, 2013).", "startOffset": 66, "endOffset": 92}, {"referenceID": 23, "context": "As stated in the introduction, our minimal feature set from \u00a72 plugs into Huang and Sagae and Kuhlmann et al.\u2019s dynamic programming (DP) framework. To help explain the connection, this section provides an overview of the DP framework. We draw heavily from the presentation of Kuhlmann et al. (2011).", "startOffset": 74, "endOffset": 299}, {"referenceID": 43, "context": "Transition-based parsing (Nivre, 2008; K\u00fcbler et al., 2009) is an incremental parsing framework based on transitions between parser configura-", "startOffset": 25, "endOffset": 59}, {"referenceID": 29, "context": "Transition-based parsing (Nivre, 2008; K\u00fcbler et al., 2009) is an incremental parsing framework based on transitions between parser configura-", "startOffset": 25, "endOffset": 59}, {"referenceID": 42, "context": "Arc-Standard The arc-standard system (Nivre, 2004) is motivated by bottom-up parsing: each dependent has to be complete before being attached.", "startOffset": 37, "endOffset": 50}, {"referenceID": 60, "context": "Arc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; G\u00f3mez-Rodr\u0131\u0301guez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and re\u00f1 as arc-standard, but forces", "startOffset": 33, "endOffset": 115}, {"referenceID": 18, "context": "Arc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; G\u00f3mez-Rodr\u0131\u0301guez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and re\u00f1 as arc-standard, but forces", "startOffset": 33, "endOffset": 115}, {"referenceID": 30, "context": "Arc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; G\u00f3mez-Rodr\u0131\u0301guez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and re\u00f1 as arc-standard, but forces", "startOffset": 33, "endOffset": 115}, {"referenceID": 2, "context": "Other presentations place ROOT at the end of the buffer or omit it entirely (Ballesteros and Nivre, 2013).", "startOffset": 76, "endOffset": 105}, {"referenceID": 44, "context": "See Shi et al. (2017) for labeled-parsing results.", "startOffset": 4, "endOffset": 22}, {"referenceID": 41, "context": "Arc-Eager In contrast to the former two systems, the arc-eager system (Nivre, 2003) makes attachments as early as possible \u2014 even if a modifier has not yet received all of its own modifiers.", "startOffset": 70, "endOffset": 83}, {"referenceID": 45, "context": "(2011) reformulate the three transition systems just discussed as deduction systems (Pereira and Warren, 1983; Shieber et al., 1995), wherein transitions serve as inference rules; these are given as the lefthand sides of the first three subfigures in Figure 1.", "startOffset": 84, "endOffset": 132}, {"referenceID": 49, "context": "(2011) reformulate the three transition systems just discussed as deduction systems (Pereira and Warren, 1983; Shieber et al., 1995), wherein transitions serve as inference rules; these are given as the lefthand sides of the first three subfigures in Figure 1.", "startOffset": 84, "endOffset": 132}, {"referenceID": 13, "context": "Specifically, as the two-index shorthand ri, js suggests, arc-eager and arc-hybrid systems can be implemented to take Opn2q space and Opn3q time; the arc-standard system requires Opn3q space and Opn4q time (if one applies the so-called hook trick (Eisner and Satta, 1999)).", "startOffset": 247, "endOffset": 271}, {"referenceID": 13, "context": "1d: the edge-factored graph-based parsing algorithm (Eisner and Satta, 1999) discussed in \u00a75.", "startOffset": 52, "endOffset": 76}, {"referenceID": 53, "context": "This lossaugmented inference or cost-augmented decoding (Taskar et al., 2005; Smith, 2011) technique has previously been applied to graph-based parsing by Kiperwasser and Goldberg (2016a).", "startOffset": 56, "endOffset": 90}, {"referenceID": 50, "context": "This lossaugmented inference or cost-augmented decoding (Taskar et al., 2005; Smith, 2011) technique has previously been applied to graph-based parsing by Kiperwasser and Goldberg (2016a).", "startOffset": 56, "endOffset": 90}, {"referenceID": 26, "context": ", 2005; Smith, 2011) technique has previously been applied to graph-based parsing by Kiperwasser and Goldberg (2016a).", "startOffset": 85, "endOffset": 118}, {"referenceID": 7, "context": "(2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach.", "startOffset": 68, "endOffset": 110}, {"referenceID": 24, "context": "(2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach.", "startOffset": 68, "endOffset": 110}, {"referenceID": 61, "context": "(2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach.", "startOffset": 68, "endOffset": 110}, {"referenceID": 12, "context": "(2008, 2011) formally prove that sequences of steps in the edgefactored GBDP (Eisner, 1996) can be used to emulate any individual step in the arc-hybrid system (Yamada and Matsumoto, 2003) and the Eisner and Satta (1999, Figure 1d) version.", "startOffset": 77, "endOffset": 91}, {"referenceID": 60, "context": "(2008, 2011) formally prove that sequences of steps in the edgefactored GBDP (Eisner, 1996) can be used to emulate any individual step in the arc-hybrid system (Yamada and Matsumoto, 2003) and the Eisner and Satta (1999, Figure 1d) version.", "startOffset": 160, "endOffset": 188}, {"referenceID": 24, "context": "In previous work, Kuhlmann et al. (2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach.", "startOffset": 18, "endOffset": 41}, {"referenceID": 7, "context": "(2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach. G\u00f3mez-Rodr\u0131\u0301guez et al. (2008, 2011) formally prove that sequences of steps in the edgefactored GBDP (Eisner, 1996) can be used to emulate any individual step in the arc-hybrid system (Yamada and Matsumoto, 2003) and the Eisner and Satta (1999, Figure 1d) version. However, they did not draw an explicitly direct connection between Eisner and Satta (1999) and TBDPs.", "startOffset": 69, "endOffset": 568}, {"referenceID": 38, "context": "Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions.", "startOffset": 103, "endOffset": 131}, {"referenceID": 3, "context": "Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions.", "startOffset": 151, "endOffset": 167}, {"referenceID": 55, "context": "As is standard, we used \u00a72-21 of the Wall Street Journal for training, \u00a722 for development, and \u00a723 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003).", "startOffset": 199, "endOffset": 223}, {"referenceID": 36, "context": "0) of the Penn Treebank (Marcus et al., 1993, PTB). As is standard, we used \u00a72-21 of the Wall Street Journal for training, \u00a722 for development, and \u00a723 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003). For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008). We adopted the CTB\u2019s goldstandard tokenization and POS tags.", "startOffset": 25, "endOffset": 454}, {"referenceID": 44, "context": "Inputs are concatenations of 28-dimensional randomly-initialized partof-speech embeddings and 100-dimensional word vectors initialized from GloVe vectors (Pennington et al., 2014) (English) and pre-trained skipgram-model vectors (Mikolov et al.", "startOffset": 154, "endOffset": 179}, {"referenceID": 39, "context": ", 2014) (English) and pre-trained skipgram-model vectors (Mikolov et al., 2013) (Chinese).", "startOffset": 57, "endOffset": 79}, {"referenceID": 14, "context": "We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 51, "context": "We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.", "startOffset": 75, "endOffset": 100}, {"referenceID": 25, "context": "Implementation Details Our model structures reproduce those of Kiperwasser and Goldberg (2016a). We use 2-layer bi-directional LSTMs with 256 hidden cell units.", "startOffset": 63, "endOffset": 96}, {"referenceID": 57, "context": "Weiss et al. (2015) and Andor et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "(2015) and Andor et al. (2016) achieve UAS of 94.", "startOffset": 11, "endOffset": 31}, {"referenceID": 15, "context": "dings are initialized uniformly (Glorot and Bengio, 2010).", "startOffset": 32, "endOffset": 57}, {"referenceID": 25, "context": "The proposed structured loss function is optimized via Adam (Kingma and Ba, 2015).", "startOffset": 60, "endOffset": 81}, {"referenceID": 6, "context": "We exclude Choe and Charniak (2016), Kuncoro et al.", "startOffset": 11, "endOffset": 36}, {"referenceID": 6, "context": "We exclude Choe and Charniak (2016), Kuncoro et al. (2017) and Liu and Zhang (2017), which convert constituentbased parses to dependency parses.", "startOffset": 11, "endOffset": 59}, {"referenceID": 6, "context": "We exclude Choe and Charniak (2016), Kuncoro et al. (2017) and Liu and Zhang (2017), which convert constituentbased parses to dependency parses.", "startOffset": 11, "endOffset": 84}, {"referenceID": 23, "context": "Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding.", "startOffset": 66, "endOffset": 112}, {"referenceID": 30, "context": "Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding.", "startOffset": 66, "endOffset": 112}, {"referenceID": 28, "context": "Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework.", "startOffset": 25, "endOffset": 154}, {"referenceID": 46, "context": "Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework.", "startOffset": 25, "endOffset": 154}, {"referenceID": 47, "context": "Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework.", "startOffset": 25, "endOffset": 154}, {"referenceID": 64, "context": "Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework.", "startOffset": 25, "endOffset": 154}, {"referenceID": 54, "context": "Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework.", "startOffset": 25, "endOffset": 154}, {"referenceID": 34, "context": "Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework.", "startOffset": 25, "endOffset": 154}, {"referenceID": 63, "context": "Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016).", "startOffset": 94, "endOffset": 117}, {"referenceID": 9, "context": "Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016).", "startOffset": 135, "endOffset": 191}, {"referenceID": 56, "context": "Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016).", "startOffset": 209, "endOffset": 234}, {"referenceID": 4, "context": "Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power.", "startOffset": 98, "endOffset": 210}, {"referenceID": 58, "context": "Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power.", "startOffset": 98, "endOffset": 210}, {"referenceID": 0, "context": "Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power.", "startOffset": 98, "endOffset": 210}, {"referenceID": 10, "context": "Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power.", "startOffset": 98, "endOffset": 210}, {"referenceID": 33, "context": "Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large.", "startOffset": 149, "endOffset": 223}, {"referenceID": 11, "context": "Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large.", "startOffset": 149, "endOffset": 223}, {"referenceID": 27, "context": "Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large.", "startOffset": 149, "endOffset": 223}, {"referenceID": 0, "context": ", 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power. Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing.", "startOffset": 8, "endOffset": 133}, {"referenceID": 23, "context": "\u2022 Since DP algorithms for exact decoding (Huang and Sagae, 2010; Kuhlmann et al., 2011) have a run-time dependence on the number of positional features, using our mere two effective positional features results in a running time of Opn3q, feasible for practice.", "startOffset": 41, "endOffset": 87}, {"referenceID": 30, "context": "\u2022 Since DP algorithms for exact decoding (Huang and Sagae, 2010; Kuhlmann et al., 2011) have a run-time dependence on the number of positional features, using our mere two effective positional features results in a running time of Opn3q, feasible for practice.", "startOffset": 41, "endOffset": 87}], "year": 2017, "abstractText": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case Opn3q exact decoders for arc-hybrid and arceager transition systems. With our minimal features, we also presentOpn3q global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the \u201csecond-best-in-class\u201d result on the English Penn Treebank.", "creator": "LaTeX with hyperref package"}}}