{"id": "1709.00224", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Variational Inference for Logical Inference", "abstract": "functional data distributional tree semantics is a mental framework that aims to learn, from text, semantic matrix representations which can be interpreted in using terms logic of truth. building here we make two contributions to this framework. the first is to show how a type of logical inference can be actually performed by evaluating conditional probabilities. the second is to make these approximate calculations tractable by means of a variational approximation. this approximation also enables faster convergence during training, allowing us to close the gap gaps with state - of - the - art vector space models when evaluating on semantic similarity. we demonstrate promising performance on two tasks.", "histories": [["v1", "Fri, 1 Sep 2017 09:55:44 GMT  (31kb,D)", "http://arxiv.org/abs/1709.00224v1", "Conference on Logic and Machine Learning in Natural Language (LaML)"]], "COMMENTS": "Conference on Logic and Machine Learning in Natural Language (LaML)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guy emerson", "ann copestake"], "accepted": false, "id": "1709.00224"}, "pdf": {"name": "1709.00224.pdf", "metadata": {"source": "CRF", "title": "Variational Inference for Logical Inference", "authors": ["Guy Emerson"], "emails": ["gete2@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction and Background", "text": "Standard approaches to distributional semantics represent meanings as vectors, whether this is done using the more traditional count vectors (Turney and Pantel, 2010), or using embedding vectors trained with a neural network (Mikolov et al., 2013). While vector space models have advanced the state of the art in many tasks, they raise questions when it comes to representing larger phrases. Ideally, we would like to learn representations that naturally have logical interpretations.\nThere have been several attempts to incorporate vectors into logical representations, and while we do not have space for a full literature review here, we will mention two prominent lines of research. Coecke et al. (2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed,\nGrefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus. Garrette et al. (2011) and Beltagy et al. (2016) incorporate a vector space model into a Markov Logic Network, in the form of weighted inference rules (the truth of one predicate implying the truth of another). This approach requires existing vectors, and assumes we can interpret similarity in terms of inference.\nIn contrast to the above, Emerson and Copestake (2016) (henceforth E&C) introduced the framework of Functional Distributional Semantics, which represents the meaning of a predicate not as a vector, but as a function.\nTo define these functions, we assume a semantic space X , each point representing the features of a possible individual. We refer to points in X as \u2018pixies\u2019, intuitively \u2018pixels\u2019 of the space, to make clear they are not individuals \u2013 two individuals may be represented by the same pixie. Further discussion of model theory will be given in forthcoming work (Emerson and Copestake, 2017) (henceforth E&C-forth). We take X to be a vector space, each dimension intuitively representing a feature.\nA semantic function maps from the space X to the range [0, 1]. This can be interpreted both in the machine-learning sense of a classifier, and in the logical sense of a truth-conditional function.1 In the machine learning view, a semantic function is a probabilistic classifier for a binary classification task \u2013 each input x \u2208 X is either an instance of the predicate\u2019s class, or it is not. In the logical view, a semantic function specifies what features a pixie needs to have in order for the predicate to be true of it \u2013 that is, the predicate\u2019s truth conditions.\n1We take a probabilistic approach, where a predicate has a probability of truth for any pixie. We believe this is a strength of the model, as it can model fuzzy boundaries of concepts. However, we could also use semantic functions in a more traditional logic, by assigning truth when the function\u2019s value is above 0.5, and falsehood otherwise. This is equivalent to turning the probabilistic classifier into a \u2018hard\u2019 classifier.\nar X\niv :1\n70 9.\n00 22\n4v 1\n[ cs\n.C L\n] 1\nS ep\n2 01\n7\nThis is related to probabilistic type judgements in the framework of Probabilistic Type Theory with Records (TTR) (Cooper, 2005; Cooper et al., 2015). Working within TTR, Larsson (2013) argues in favour of representing perceptual concepts as classifiers of perceptual input. While TTR represents situations in terms of situation types, a semantic function model defines a semantic space without reference to any types or predicates.\nSchlangen et al. (2016) take a similar view, representing meanings as image classifiers. Zarrie\u00df and Schlangen (2017) use a distributional model to help train such classifiers, but do not directly learn logical representations from distributional data.\nOur approach to logical inference is related to the work of Bergmair (2010) and Clarke and Keller (2015), who use fuzzy truth values and probabilistic truth values, respectively. However, neither incorporate distributional data.\nIn contrast to the above, a semantic function model can be trained on a parsed corpus. By defining a generative model, we can apply unsupervised learning: optimising the model parameters to maximise the probability of generating the corpus data.\nOur model can be trained on a corpus annotated with Dependency Minimal Recursion Semantics (DMRS) (Copestake et al., 2005; Copestake, 2009). This represents the meaning of a sentence as a semantic dependency graph that has a logical interpretation. An example DMRS graph is shown in Fig. 1 (ignoring quantifiers, and ignoring properties such number and tense). Note that, as these are semantic dependencies, not syntactic dependencies, active and passive voice sentences can be represented with the same graph. This dependency graph could be generated by the probabilistic graphical model shown in Fig. 2. The generated predicates are at the bottom: q might correspond to a verb, p its subject, and r its object. The dependency links (ARG1 and ARG2) are at the top.\nRather than generating the predicates directly, we assume that each predicate is true of a latent, unobserved pixie. For example, the DMRS graph for the dog chased the cat has three pixies, corresponding to the dog, the chasing event, and the cat. We can define a generative model for such sets of pixies (the top row of Fig. 2), assuming each link corresponds to a probabilistic dependence; intuitively, different kinds of events occur with different kinds of arguments. In machine learning terms, this forms an undirected graphical model.\ndog chase cat ARG2ARG1\nFigure 1: A simplified DMRS graph, illustrating the type of data observed during training. This graph would correspond to sentences like The dog chased the cat, or Cats are chased by dogs.\ny zx ARG2ARG1\n\u2208 X\ntc, x tc, y tc, z\n\u2208 {\u22a5,>} |V |\np q r\n\u2208 V\nFigure 2: A probabilistic graphical model for Functional Distributional Semantics (E&C Fig. 3). Each node denotes a random variable, but only the bottom row is observed. The plate (middle row) denotes repeating variables across the vocabulary. Top row: latent pixies x, y, and z, lying in a semantic space X . Their joint distribution is determined by the DMRS links ARG1 and ARG2. Middle row: each predicate c in the vocabulary V is probabilistically true or false for each pixie. Bottom row: for each pixie, we observe exactly one predicate, probabilistically chosen out of all predicates that are true of the pixie.\nWe generate the predicates in three stages (from top to bottom in Fig. 2). First, we generate a set of pixies, with DMRS links specifying probabilistic dependence. Second, we use the semantic functions for all predicates to generate a truth value for each predicate applied to each pixie. Third, for each pixie, we generate a single predicate out of all true predicates. The separation of pixies and truth values gives us a connection with logical inference, as we will see in \u00a72.1.\nFor a DMRS graph with a different structure, we can define a similar graphical model. For example, for an intransitive sentence, with just a verb and its subject, we can remove the right-hand column of Fig. 2. The model parameters are shared across all such graphs, so we can train our model on a heterogenous set of DMRS graphs.\nWe follow E&C, and implement this model as shown in Fig. 3. The semantic space X consists of\nsparse binary-valued vectors, where a small fixed number of dimensions are 1, and the rest 0. Intuitively, each dimension is a \u2018feature\u2019 of a pixie, and only a small number are present. The joint distribution over pixies is given by a Cardinality Restricted Boltzmann Machine (CaRBM) (Swersky et al., 2012). The semantic functions are one-layer feedforward networks, with a sigmoid activation so the output is in the range [0, 1]. The probability of generating a predicate (bottom row) is weighted by the observed frequency of the predicate."}, {"heading": "2 Theoretical Contributions", "text": ""}, {"heading": "2.1 Logical Inference", "text": "The model in Fig. 2 contains, in the middle row, a node for the truth of each predicate for each pixie. Using these nodes, we can convert certain logical propositions into statements about probabilities.\nFor example, we might be interested in whether one predicate implies another. For simplicity, consider a single pixie x, as shown in Fig. 4. Then, the logical proposition \u2200x \u2208 X , a(x)\u21d2 b(x) is equivalent2 to the statement P (tb,x|ta,x) = 1.\n2More precisely, the equivalence requires the logic to have \u2018existential import\u2019: Every A implies that some A exists. This follows from the definition of conditional probability P (B|A) = P (A \u2227B)/P (A), only defined if P (A) 6= 0\nIntuitively, conditioning on ta,x means restricting to those pixies x for which the predicate a is true. If the probability of tb,x being true is 1, then the predicate b is true for all of those x. Similarly, \u2203x \u2208 X , a(x) \u2227 b(x) is equivalent to P (tb,x|ta,x) > 0. Furthermore, classical rules of inference hold under this equivalence. For example, from P (tb,x|ta,x) = 1 and P (tc,x|tb,x) = 1, we can deduce that P (tc,x|ta,x) = 1. This is precisely the classical Barbara syllogism. A proof is given in Appendix A.\nIn practice, when training on distributional data, the conditional probability P (tb,x|ta,x) will never be exactly 0 or 1, because the model only implements soft constraints. Nonetheless, this quantity can be very informative: if it is 0.999, then we know that if a(x) is true, it is almost always the case that b(x) is also true. So, it represents the degree to which a implies b, in an intuitive sense.\nSeparate from this notion of inference, we can also consider the similarity of two latent pixies \u2013 if a is true of x, and b is true of y, how many features do x and y share? If a and b are antonyms, the truth of one will not imply the truth of the other, but the pixies may share many features.\nAs Copestake and Herbelot (2012) note, distinguishing synonyms and antonyms requires checking whether expressions are mutually exclusive. We do not have access to such information in our training data, and such cases are inconsistently annotated in our test data (see \u00a73.1). Nonetheless, the model allows us to make such a distinction, which is an advantage over vector space models. Exploiting this distinction (perhaps by using coreference information) would be a task for future work.\nTo calculate P (tb,x|ta,x), we must marginalise out x, because the model actually defines the joint probability P (x, tb,x, ta,x). This is analogous to removing bound variables when calculating the truth of quantified expressions in classical logic. Quantifiers will be discussed further by E&C-forth. However, marginalising out x requires summing over the semantic space X , which is intractable when X has a large number of dimensions. In \u00a72.2, we introduce a variational approximation to make this calculation tractable.\nIn the general case, there are multiple pixie variables. This opens up the possibility of inferring what is true of one pixie, given what is true of another. For example, we might be interested in what is true of a verb\u2019s arguments, which we could ex-\nplore with the three-pixie graph in Fig. 5. We can ask questions such as: if the predicate paint is true of an event, what predicates are true of its arguments? A good model might answer that for the ARG1 pixie, artist and person are likely true, while democracy and aubergine are likely false.\nJust as with the one-pixie case, there is an equivalence between logical propositions and statements about probabilities. For example, \u2203x, y \u2208 X , a(y) \u2227 b(x) \u2227 ARG1(y, x) is equivalent to P (tb,x|ta,y) > 0. Note that ARG1 does not correspond to a random variable \u2013 it is instead represented directly by the structure of the graphical model (the edges in the top row of Fig. 2 and the middle row of Fig. 5). As before, this conditional probability is never going to be exactly 0 or 1, but it is nonetheless a useful quantity when performing approximate inference, as we will see in \u00a73.2."}, {"heading": "2.2 Variational Inference", "text": "As explained in the previous section, we can express certain logical propositions as conditional probabilities, but calculating these probabilities exactly is intractable, as it involves summing over the semantic space, which grows exponentially with the number of dimensions. Furthermore, we need to calculate similar conditional probabilities when training the model in the first place.\nInstead of summing over the entire space, E&C proposed summing over a small number of carefully chosen pixies, using a Markov Chain Monte Carlo method. However, this algorithm is slow for two reasons. Firstly, many iterations of the Markov chain are required before the samples are useful. Secondly, even if we are not summing over the entire space, many samples are still needed, because the discrete values lead to high variance.\nIn this section, we introduce a variational inference algorithm, where we directly approximate the distribution over pixies that we need to calculate, and then optimise this approximation. This makes the calculations in the previous section tractable, and also makes training more efficient.\nThe distribution we need to approximate is P (x|tc,x), the probability that a latent pixie x has particular features, given the truth of some predicate c. We use a mean-field approximation: we assume that each dimension has an independent probability qi of being active, as shown in (1). The approximate probability Q(x) is simply the product of the probabilities of each dimension. Furthermore, we assume that each of these probabilities depends on the average activation of all other dimensions (i.e. the mean field activation).\nP (x|tc,x) \u2248 Q(x) = \u220f\ni|xi=1\nqi \u220f\ni|xi=0\n(1\u2212 qi) (1)\nFor Q to be a good approximation, it needs to be close to P . We can measure this using the Kullback-Leibler divergence from Q to P .3 Minimising this quantity is also done in the Expectation Propagation algorithm (Minka, 2001). However, a semantic function model is not in the exponential family, making it difficult to apply Expectation Propagation directly.\nGiven this mean-field approximation Q(x), we have a a mean-field vector qi. This vector is not in X , because each dimension is now a value in the range [0, 1], rather than taking one of the values 0 or 1. It represents a \u2018typical\u2019 pixie for these truth values. Furthermore, we have implemented semantic functions as one-layer neural networks, and each weight in the network can be multiplied\n3Variational Bayes minimises the KL-divergence in the opposite direction \u2013 that is, the KL-divergence from P to Q. However, for the above approximation, this is infinite: if the number of active units is not equal to the fixed cardinality, then P (x|tc,x) = 0 but Q(x) 6= 0, giving infinite Q(x) logP (x|tc,x). Furthermore, while Variational Bayes prefers \u2018high precision\u2019 approximations (areas of high Q are accurate), we will prefer \u2018high recall\u2019 approximations (areas of high P are accurate). This is appropriate for two reasons. Firstly, in areas where the number of active units is wrong, Q is bound to be too high, but if we want to sample from Q, we can avoid these areas by using belief propagation, as explained by Swersky et al. (2012). Secondly, in areas where the number of active units is correct, Q will be much higher than P only if there is a dependence between dimensions that Q cannot capture, such as if P is a multi-modal distribution. Because of the definition of an RBM, such a dependence is impossible within one pixie, and combined with the simple form of our semantic functions, such a dependence will be rare between pixies.\nby a value in the range [0, 1] just as easily as it can be multiplied by 0 or 1. Since a mean-field vector defines a distribution over pixies, applying a semantic function to a mean-field vector lets us approximately calculate the probability that a predicate is true of a pixie drawn from this distribution.\nDifferentiating the KL-divergence with respect to qi, and using the above idea that we can apply semantic functions to mean-field vectors, we can derive the update rule given in (2), with a full derivation given in Appendix B. This updates the value of qi, while holding the rest fixed. Here, x(+i) is the mean-field vector where unit i is fixed to be on, x(\u2212i) is the mean-field vector where unit i is fixed to be off, tc is the semantic function for the predicate c, D is the number of dimensions of X , and C is the number of active units. Optimising Q can then be done by repeatedly applying this update rule across all dimensions.\nqi =\n( 1 +\nD \u2212 C C\ntc ( x(\u2212i) ) tc ( x(+i) ))\u22121 (2) This update rule looks at how likely the predicate c is to be true when the dimension xi is active, and when it is not. If c is much more likely to be true when xi is active, then qi will be close to 1. If c is much more likely to be true when xi is inactive, then qi will be close to 0. If there is no difference at all, then qi will be C/D, the expected probability if all dimensions are equally likely.\nWe can apply this to logical inference, to calculate P (tb,x|ta,x), as shown in Fig. 6. We first find the mean-field vector for x, conditioning on the truth of a. This approximates P (x|ta,x). Then, we evaluate the semantic function for b on this meanfield vector. This approximates P (tb,x|ta,x).\nFor multiple pixies, the process is similar, as shown in Fig. 7. We have one mean-field vector for each pixie, and we optimise these together. The only difference to the update rule is that, as well as considering how activating one dimension changes the probability of a predicate being true, we also have to consider how likely this dimension is to be active, given the other pixies in the graph. This leads to an extra term in the update rule, as exemplified in (3), where there is a link from x to y. The link has weights Wij which control how likely it is that xi and yj are both active.\nqi =\n( 1 +\nD \u2212 C C\ntc ( x(\u2212i) ) tc ( x(+i) )e\u2212\u03a3jWijyj)\u22121 (3)\n3 Experimental Results4\nFinding a good evaluation task is difficult. Lexical similarity tasks do not require logical inference, while tasks like textual entailment require a level of coverage beyond the scope of this paper. We consider two tasks: lexical similarity, as a simple benchmark, and the RELPRON dataset, which lets us explore a controlled kind of inference.\nWe trained our model on subject-verb-object (SVO) triples extracted from WikiWoods5, a parsed version of the July 2008 dump of the English Wikipedia, distributed by DELPH-IN. This resource was produced by Flickinger et al. (2010), using the English Resource Grammar (Flickinger, 2000, 2011), and the PET parser (Callmeier, 2001; Toutanova et al., 2005), with parse ranking trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al., 2009).\nOur source code is available online.6 The WikiWoods corpus was pre-processed using the Python packages pydelphin7 (developed by Michael Goodman), and pydmrs8 (Copestake et al., 2016).\nTo speed up training, we initialised our model using random positive-only projections, a simple method for producing reduced-dimension count vectors (QasemiZadeh and Kallmeyer, 2016). Rather than counting each context separately, every context is randomly mapped to a dimension, so each dimension corresponds to the total count of several contexts. These counts can then be trans-\n4A fuller set of results, with further discussion, will be given by E&C-forth.\n5http://moin.delph-in.net/WikiWoods 6http://github.com/guyemerson/sem-func 7http://github.com/delph-in/pydelphin 8http://github.com/delph-in/pydmrs\nformed into PPMI scores. As with normal PPMIbased count vectors, there are several hyperparameters that can be tuned (Levy et al., 2015) \u2013 however, as we are using these vectors as parameters for semantic functions, it should be noted that the optimal hyperparameter settings are not the same.\nWe compare our model to two vector space models, also trained on Wikipedia. Both use Mikolov et al. (2013)\u2019s skipgram algorithm with negative sampling. \u201cWord2Vec\u201d was trained on raw text, while \u201cSVO Word2Vec\u201d was trained on the same SVO triples used to train our model. We tested these models using cosine similarity."}, {"heading": "3.1 Lexical Semantic Similarity", "text": "To measure the similarity of two predicates a and b, we use the conditional probability described in \u00a72.1, and illustrated in Figs. 4 and 6. Since this is an asymmetric measure, we multiply the conditional probabilities in both directions, i.e. we calculate P (ta,x|tb,x)P (tb,x|ta,x).\nWe evaluated on two datasets which aim to capture similarity, rather than relatedness: SimLex999 (Hill et al., 2015), and WordSim-353 (Finkelstein et al., 2001), which Agirre et al. (2009) split into similarity and relatedness subsets. Results are shown in Table 1.9 For each dataset, hyperparameters were tuned on the remaining datasets. As WordSim-353 is a noun-based dataset, it is possible that performance on SimLex-999 verbs could be improved by optimising hyperparameters on a more appropriate development set.\nNote that we would like a low correlation on the\n9Performance of Word2Vec on SimLex-999 is higher than reported by Hill et al. (2015). Despite correspondence with the authors, it is not clear why their figures are so low.\nrelatedness subset of WordSim-353. In the real world, related predicates are unlikely to be true of the same pixies (and the pixies they are true of are unlikely to even share features). For predicates which are true of similar but disjoint sets of pixies, annotations in these datasets are inconsistent. For example, SimLex-999 gives a low score to (aunt, uncle), but a high score to (cat, dog). The semantic function model achieves the lowest correlation on the relatedness subset.\nCompared to E&C, the gap between the semantic function model and the vector space models has essentially been closed. Which model performs best is inconsistent across the evaluation datasets. This shows that the previously reported lower performance was not due to a problem with the model itself, but rather with an inefficient training algorithm and with poor choice of hyperparameters."}, {"heading": "3.2 RELPRON", "text": "The RELPRON dataset was produced by Rimell et al. (2016). It consists of \u2018terms\u2019 (all nouns), each paired with up to ten \u2018properties\u2019. For example, a telescope is a device that astronomers use, and a saw is a device that cuts wood. All properties are of this form: a hypernym of the term, modified by a relative clause with a transitive verb. For each term, the task is to identify the properties which apply to this term. Since every property follows one of only two patterns, this dataset lets us focus on semantics, rather than parsing.\nA model that captures relatedness can achieve good performance on this dataset \u2013 Rimell et al. found that the other argument of the verb was the best predictor of the term (e.g. astronomer predicts telescope). Logically speaking, these predicates do not imply each other. However, Rimell et al. included confounders for a model relying on relatedness \u2013 e.g. a document that has a balance is an account, not the quality of balance. In all of their models, this was the top-ranked property for balance. By combining a vector model with our model, we hoped to improve performance.\nWe tested our model using the method described in \u00a72 and illustrated in Figs. 5 and 7: for each term and property, we find the probability of the term being true, conditioned on all predicates in the property. Results are given in Table 2. As noted in \u00a73.1, our model does not capture relatedness, and it performs below vector addition. However, the ensemble outperforms the vector space\nmodel alone. This improvement is not simply due to increasing the capacity of the model \u2013 increasing the dimensionality of the vector space did not yield this improvement.\nFurthermore, inspecting the differences in predictions between the vector space model and the ensemble, it appears that there is particular improvement on the confounders included in the dataset, which require some kind of logical inference. In our ensemble model, for the term balance, the top-ranked property is no longer the confounder document that has a balance, but instead the correct property quality that an ear maintains."}, {"heading": "4 Conclusion", "text": "We can define probabilistic logical inference in Functional Distributional Semantics, and efficiently calculate it using variational inference. We can use this to improve performance on the RELPRON dataset, suggesting our model can learn structure not captured by vector space models."}, {"heading": "Acknowledgements", "text": "We would like to thank Emily Bender, for helpful discussion and feedback on an earlier draft.\nThis work was supported by a Schiff Foundation Studentship."}, {"heading": "A Logical equivalence", "text": "A.1 Proof of general case Syllogisms are classically expressed in settheoretic terms. A quantified proposition of the form Q A are B, where Q is some quantifier, gives constraints on the sizes of the sets A \u2229B and A \\B, and says nothing about the size of B.\nFor the quantifier \u2203, we have:\n|A \u2229B| > 0\nFor the quantifier \u2200, we have the following, where the second constraint assumes existential import:\n|A \\B| = 0 |A \u2229B| > 0\nFrom these definitions, we can use standard set theory to prove all and only the valid syllogisms. To show equivalence with our probabilistic framework, we first note that sizes of sets form a measure (the \u2018counting measure\u2019), and probabilities also form a measure. The above conditions are all\nconstraints on sizes of sets being zero or nonzero, so it suffices to show that the sizes and probabilities are equivalent in the measure-theoretic sense: they agree on which sets have measure zero.\nFirst, we note that P (B|A) = P (A\u2229B)P (A) is defined only when P (A) > 0, which will give us existential import.\nFor \u2203, we have:\nP (B|A) = P (A \u2229B) P (A) > 0\nP (A \u2229B) > 0\nWe can say nothing further about the probability P (A \\B) = P (A)\u2212 P (A \u2229B), which may be zero or nonzero, just as in the classical case.\nFor \u2200, we have:\nP (A \u2229B) P (A) = 1\nP (A \u2229B) = P (A) P (A \u2229B) = P (A \u2229B) + P (A \\B) P (A \\B) = 0\nAnd we also have:\nP (A \\B) + P (A \u2229B) = P (A) > 0 P (A \u2229B) > 0\nThis demonstrates the equivalence.\nA.2 Example\nWe can prove the Barbara syllogism as follows:\nP (B|A) = 1 =\u21d2 P (A \\B) = 0, P (A) > 0\nP (C|B) = 1 =\u21d2 P (B \\ C) = 0\nP (A \\ C) = P (A \u2229B \\ C) + P (A \\B \\ C) \u2264 P (B \\ C) + P (A \\B) = 0\nP (A \u2229 C) = P (A)\u2212 P (A \\ C) = P (A) > 0\n=\u21d2 P (C|A) = P (A \u2229 C) P (A) = 1"}, {"heading": "B Derivation of update rule", "text": "We are trying to optimise Q to minimise the KLdivergence from Q(x) to P (x|tc,x):\nDKL(P ||Q) = \u2211 x P (x|tc,x) log P (x|tc,x) Q(x)\n= \u2211 x P (x|tc,x) ( logP (x|tc,x)\u2212 logQ(x) ) Note that the first term is independent of Q. To iteratively optimise one parameter qi at a time, we take the derivative:\n\u2202\n\u2202qi DKL(P ||Q) = \u2212\n\u2202\n\u2202qi \u2211 x P (x|tc,x) logQ(x)\n= \u2211\nx|xi=1\nP (x|tc,x) 1 qi \u2212 \u2211\nx|xi=0\nP (x|tc,x) 1\n1\u2212 qi\nNow we can rewrite P (x|tc,x) as the following. If there is just one pixie, then we can assume a uniform prior over x. For D dimensions, of which C are active, there are ( D C ) different vectors.\nP (x|tc,x) = P (x)P (tc,x|x)\nP (tc,x)\n= tc(x)(\nD C ) P (tc,x)\nNote that ( D C ) P (tc,x) is constant in x. Setting\nthe derivative to 0, we have:\u2211 x|xi=1 tc(x) 1 qi = \u2211 x|xi=0 tc(x) 1 1\u2212 qi\nSumming over all x is intractable, but we can approximate this sum using mean-field vectors for x. For most values of x, tc(x) will be close to 0, and the regions of interest will be near the mean-field vectors. Let x(+i) denote the meanfield vector when xi = 1 and the total activation of the remaining dimensions is C \u2212 1, and let x(\u2212i) denote the mean-field vector when xi = 0 and the total activation of the remaining dimensions is C. Both of these vectors can be approximated using the values of qj for j 6= i, scaled so that their sum is correct. Then we have:( D\u22121 C\u22121 ) tc(x (+i)) 1 qi \u2248 ( D\u22121 C ) tc(x (\u2212i)) 1 1\u2212 qi\ntc(x (+i))\n1 qi \u2248 D\u2212C C tc(x (\u2212i)) 1 1\u2212 qi\nRe-arranging for qi yields the following, which is the optimal value for qi, given the other dimensions qj , and given the above approximations:\nqi \u2248 ( 1 +\nD \u2212 C C\ntc ( x(\u2212i) ) tc ( x(+i) ))\u22121 In the above derivation, we assumed a uniform prior over x, which meant that P (x|tc,x) \u221d tc(x). If there are links between pixies, then this no longer holds, and we instead have P (x) being determined by the RBM weights, which gives the following, where we sum over all links x l\u2212\u2192 y, from the pixie x to another pixie y with label l. Each link type l has weights W (l)jk (and for incoming links, we simply take the transpose of this matrix). For clarity, we do not write bias terms.\nP (x|tc,x) \u221d tc(x) exp \u2211 x l\u2212\u2192 y \u2211 j,k W (l) jk xjyk\nSo to amend the update rule, we replace tc(x) with the above expression, which gives:1 + D \u2212 C\nC\ntc ( x(\u2212i) ) exp \u2211 W (l) jk x (\u2212i) j yk\ntc ( x(+i) ) exp \u2211 W (l) jk x (+i) j yk\n\u22121\nNow note that this ratio of exponentials can be rewritten as:\nexp \u2211\nW (l) jk\n( x\n(\u2212i) j \u2212 x (+i) j ) yk\nFor dimensions j 6= i, the difference between the two mean-field vectors will be small, so if\u2211\nk W (l) jk yk is on average close to 0, the above expression will be dominated by the value at j = i. So, we can approximate it as:\nexp\u2212 \u2211 x l\u2212\u2192 y \u2211 k W (l) ik yk\nThis gives the following update rule, which reduces to (3) in the case of a single link:1 + D\u2212C\nC\ntc ( x(\u2212i) ) tc ( x(+i) ) exp\u2212\u2211 x l\u2212\u2192 y \u2211 k W (l) ik yk  \u22121"}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa."], "venue": "Proceedings of the 2009 Conference of the North American", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Frege in space: A program of compositional distributional semantics", "author": ["Marco Baroni", "Raffaela Bernardi", "Roberto Zamparelli."], "venue": "Linguistic Issues in Language Technology 9.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Representing meaning with a combination of logical and distributional models", "author": ["Islam Beltagy", "Stephen Roller", "Pengxiang Cheng", "Katrin Erk", "Raymond J Mooney."], "venue": "Computational Linguistics 42(4):763\u2013808.", "citeRegEx": "Beltagy et al\\.,? 2016", "shortCiteRegEx": "Beltagy et al\\.", "year": 2016}, {"title": "Monte Carlo Semantics: Robust inference and logical pattern processing with Natural Language text", "author": ["Richard Bergmair."], "venue": "Ph.D. thesis, University of Cambridge.", "citeRegEx": "Bergmair.,? 2010", "shortCiteRegEx": "Bergmair.", "year": 2010}, {"title": "Efficient parsing with largescale unification grammars", "author": ["Ulrich Callmeier."], "venue": "Master\u2019s thesis, Universit\u00e4t des Saarlandes, Saarbr\u00fccken, Germany.", "citeRegEx": "Callmeier.,? 2001", "shortCiteRegEx": "Callmeier.", "year": 2001}, {"title": "Efficiency in ambiguity: Two models of probabilistic semantics for natural language", "author": ["Daoud Clarke", "Bill Keller."], "venue": "Proceedings of the 11th International Conference on Computational Semantics (IWCS). pages 129\u2013139.", "citeRegEx": "Clarke and Keller.,? 2015", "shortCiteRegEx": "Clarke and Keller.", "year": 2015}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "Linguistic Analysis 36:345\u2013384.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Austinian truth, attitudes and type theory", "author": ["Robin Cooper."], "venue": "Research on Language and Computation 3(2-3):333\u2013362.", "citeRegEx": "Cooper.,? 2005", "shortCiteRegEx": "Cooper.", "year": 2005}, {"title": "Probabilistic type theory and natural language semantics", "author": ["Robin Cooper", "Simon Dobnik", "Staffan Larsson", "Shalom Lappin."], "venue": "LiLT (Linguistic Issues in Language Technology) 10.", "citeRegEx": "Cooper et al\\.,? 2015", "shortCiteRegEx": "Cooper et al\\.", "year": 2015}, {"title": "Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go", "author": ["Ann Copestake."], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. pages 1\u20139.", "citeRegEx": "Copestake.,? 2009", "shortCiteRegEx": "Copestake.", "year": 2009}, {"title": "Resources for building applications with Dependency Minimal Recursion Semantics", "author": ["Ann Copestake", "Guy Emerson", "Michael Wayne Goodman", "Matic Horvat", "Alexander Kuhnle", "Ewa Muszy\u0144ska."], "venue": "Proceedings of the 10th International", "citeRegEx": "Copestake et al\\.,? 2016", "shortCiteRegEx": "Copestake et al\\.", "year": 2016}, {"title": "Minimal Recursion Semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag."], "venue": "Research on Language and Computation 3(2-3):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Lexicalised compositionality", "author": ["Ann Copestake", "Aur\u00e9lie Herbelot."], "venue": "Unpublished draft.", "citeRegEx": "Copestake and Herbelot.,? 2012", "shortCiteRegEx": "Copestake and Herbelot.", "year": 2012}, {"title": "Functional Distributional Semantics", "author": ["Guy Emerson", "Ann Copestake."], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP (RepL4NLP). Association for Computational Linguistics, pages 40\u201352.", "citeRegEx": "Emerson and Copestake.,? 2016", "shortCiteRegEx": "Emerson and Copestake.", "year": 2016}, {"title": "Semantic composition via probabilistic model theory", "author": ["Guy Emerson", "Ann Copestake."], "venue": "Proceedings of the 12th International Conference on Computational Semantics (IWCS). Association for Computational Linguistics.", "citeRegEx": "Emerson and Copestake.,? 2017", "shortCiteRegEx": "Emerson and Copestake.", "year": 2017}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th International Conference on the World Wide Web. Asso-", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "On building a more efficient grammar by exploiting types", "author": ["Dan Flickinger."], "venue": "Natural Language Engineering 6(1):15\u201328.", "citeRegEx": "Flickinger.,? 2000", "shortCiteRegEx": "Flickinger.", "year": 2000}, {"title": "Accuracy vs", "author": ["Dan Flickinger."], "venue": "robustness in grammar engineering. In Emily M Bender and Jennifer E Arnold, editors, Language from a cognitive perspective: Grammar, usage, and processing, CSLI Publications, pages 31\u201350.", "citeRegEx": "Flickinger.,? 2011", "shortCiteRegEx": "Flickinger.", "year": 2011}, {"title": "WikiWoods: Syntacto-semantic annotation for English Wikipedia", "author": ["Dan Flickinger", "Stephan Oepen", "Gisle Ytrest\u00f8l."], "venue": "Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC). European Language Resources", "citeRegEx": "Flickinger et al\\.,? 2010", "shortCiteRegEx": "Flickinger et al\\.", "year": 2010}, {"title": "Integrating logical representations with probabilistic information using Markov logic", "author": ["Dan Garrette", "Katrin Erk", "Raymond Mooney."], "venue": "Proceedings of the 9th International Conference on Computational Semantics (IWCS). Association for Computational", "citeRegEx": "Garrette et al\\.,? 2011", "shortCiteRegEx": "Garrette et al\\.", "year": 2011}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette."], "venue": "Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics (*SEM). pages 1\u201310.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics 41(4):665\u2013695.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Formal semantics for perceptual classification", "author": ["Staffan Larsson."], "venue": "Journal of Logic and Computation 25(2):335\u2013369.", "citeRegEx": "Larsson.,? 2013", "shortCiteRegEx": "Larsson.", "year": 2013}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 3:211\u2013 225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the 1st International Conference on Learning Representations.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["Thomas P Minka."], "venue": "Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence. pages 362\u2013369.", "citeRegEx": "Minka.,? 2001", "shortCiteRegEx": "Minka.", "year": 2001}, {"title": "Random positive-only projections: PPMI-enabled incremental semantic space construction", "author": ["Behrang QasemiZadeh", "Laura Kallmeyer."], "venue": "Proceedings of the 5th Joint Conference on Lexical and Computational Semantics (*SEM). pages 189\u2013198.", "citeRegEx": "QasemiZadeh and Kallmeyer.,? 2016", "shortCiteRegEx": "QasemiZadeh and Kallmeyer.", "year": 2016}, {"title": "RELPRON: A relative clause evaluation dataset for compositional distributional semantics", "author": ["Laura Rimell", "Jean Maillard", "Tamara Polajnar", "Stephen Clark."], "venue": "Computational Linguistics 42(4):661\u2013 701.", "citeRegEx": "Rimell et al\\.,? 2016", "shortCiteRegEx": "Rimell et al\\.", "year": 2016}, {"title": "Resolving references to objects in photographs using the words-as-classifiers model", "author": ["David Schlangen", "Sina Zarrie\u00df", "Casey Kennington."], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. pages 1213\u20131223.", "citeRegEx": "Schlangen et al\\.,? 2016", "shortCiteRegEx": "Schlangen et al\\.", "year": 2016}, {"title": "Cardinality Restricted Boltzmann Machines", "author": ["Kevin Swersky", "Ilya Sutskever", "Daniel Tarlow", "Richard S Zemel", "Ruslan R Salakhutdinov", "Ryan P Adams."], "venue": "Advances in Neural Information Processing Systems 25 (NIPS). pages 3293\u20133301.", "citeRegEx": "Swersky et al\\.,? 2012", "shortCiteRegEx": "Swersky et al\\.", "year": 2012}, {"title": "Stochastic HPSG parse selection using the Redwoods corpus", "author": ["Kristina Toutanova", "Christopher D Manning", "Stephan Oepen."], "venue": "Journal of Research on Language and Computation 3(1):83\u2013105.", "citeRegEx": "Toutanova et al\\.,? 2005", "shortCiteRegEx": "Toutanova et al\\.", "year": 2005}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Extracting and annotating Wikipedia subdomains", "author": ["Gisle Ytrest\u00f8l", "Stephan Oepen", "Dan Flickinger"], "venue": "In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories", "citeRegEx": "Ytrest\u00f8l et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ytrest\u00f8l et al\\.", "year": 2009}, {"title": "Is this a child, a girl or a car? Exploring the contribution of distributional similarity to learning referential word meanings", "author": ["Sina Zarrie\u00df", "David Schlangen."], "venue": "Proceedings of the 15th Annual Conference of the European Chapter of the Association", "citeRegEx": "Zarrie\u00df and Schlangen.,? 2017", "shortCiteRegEx": "Zarrie\u00df and Schlangen.", "year": 2017}], "referenceMentions": [{"referenceID": 31, "context": "Standard approaches to distributional semantics represent meanings as vectors, whether this is done using the more traditional count vectors (Turney and Pantel, 2010), or using embedding vectors trained with a neural network (Mikolov et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 24, "context": "Standard approaches to distributional semantics represent meanings as vectors, whether this is done using the more traditional count vectors (Turney and Pantel, 2010), or using embedding vectors trained with a neural network (Mikolov et al., 2013).", "startOffset": 225, "endOffset": 247}, {"referenceID": 14, "context": "Further discussion of model theory will be given in forthcoming work (Emerson and Copestake, 2017) (henceforth E&C-forth).", "startOffset": 69, "endOffset": 98}, {"referenceID": 4, "context": "Coecke et al. (2010) and Baroni et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure.", "startOffset": 11, "endOffset": 32}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed, Grefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus.", "startOffset": 11, "endOffset": 293}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed, Grefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus. Garrette et al. (2011) and Beltagy et al.", "startOffset": 11, "endOffset": 383}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed, Grefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus. Garrette et al. (2011) and Beltagy et al. (2016) incorporate a vector space model into a Markov Logic Network, in the form of weighted inference rules (the truth of one predicate implying the truth of another).", "startOffset": 11, "endOffset": 409}, {"referenceID": 1, "context": "(2010) and Baroni et al. (2014) propose a tensor-based approach, where vectors are combined according to argument structure. However, this leaves open the question of how to perform logical inference, as vector spaces do not provide a natural notion of entailment. Indeed, Grefenstette (2013) proved that quantifiers cannot be expressed using tensor calculus. Garrette et al. (2011) and Beltagy et al. (2016) incorporate a vector space model into a Markov Logic Network, in the form of weighted inference rules (the truth of one predicate implying the truth of another). This approach requires existing vectors, and assumes we can interpret similarity in terms of inference. In contrast to the above, Emerson and Copestake (2016) (henceforth E&C) introduced the framework of Functional Distributional Semantics, which represents the meaning of a predicate not as a vector, but as a function.", "startOffset": 11, "endOffset": 730}, {"referenceID": 7, "context": "This is related to probabilistic type judgements in the framework of Probabilistic Type Theory with Records (TTR) (Cooper, 2005; Cooper et al., 2015).", "startOffset": 114, "endOffset": 149}, {"referenceID": 8, "context": "This is related to probabilistic type judgements in the framework of Probabilistic Type Theory with Records (TTR) (Cooper, 2005; Cooper et al., 2015).", "startOffset": 114, "endOffset": 149}, {"referenceID": 7, "context": "This is related to probabilistic type judgements in the framework of Probabilistic Type Theory with Records (TTR) (Cooper, 2005; Cooper et al., 2015). Working within TTR, Larsson (2013) argues in favour of representing perceptual concepts as classifiers of perceptual input.", "startOffset": 115, "endOffset": 186}, {"referenceID": 3, "context": "Our approach to logical inference is related to the work of Bergmair (2010) and Clarke and Keller (2015), who use fuzzy truth values and probabilistic truth values, respectively.", "startOffset": 60, "endOffset": 76}, {"referenceID": 3, "context": "Our approach to logical inference is related to the work of Bergmair (2010) and Clarke and Keller (2015), who use fuzzy truth values and probabilistic truth values, respectively.", "startOffset": 60, "endOffset": 105}, {"referenceID": 11, "context": "Our model can be trained on a corpus annotated with Dependency Minimal Recursion Semantics (DMRS) (Copestake et al., 2005; Copestake, 2009).", "startOffset": 98, "endOffset": 139}, {"referenceID": 9, "context": "Our model can be trained on a corpus annotated with Dependency Minimal Recursion Semantics (DMRS) (Copestake et al., 2005; Copestake, 2009).", "startOffset": 98, "endOffset": 139}, {"referenceID": 29, "context": "The joint distribution over pixies is given by a Cardinality Restricted Boltzmann Machine (CaRBM) (Swersky et al., 2012).", "startOffset": 98, "endOffset": 120}, {"referenceID": 9, "context": "As Copestake and Herbelot (2012) note, distinguishing synonyms and antonyms requires checking whether expressions are mutually exclusive.", "startOffset": 3, "endOffset": 33}, {"referenceID": 25, "context": "3 Minimising this quantity is also done in the Expectation Propagation algorithm (Minka, 2001).", "startOffset": 81, "endOffset": 94}, {"referenceID": 29, "context": "Firstly, in areas where the number of active units is wrong, Q is bound to be too high, but if we want to sample from Q, we can avoid these areas by using belief propagation, as explained by Swersky et al. (2012). Secondly, in areas where the number of active units is correct, Q will be much higher than P only if there is a dependence between dimensions that Q cannot capture, such as if P is a multi-modal distribution.", "startOffset": 191, "endOffset": 213}, {"referenceID": 4, "context": "(2010), using the English Resource Grammar (Flickinger, 2000, 2011), and the PET parser (Callmeier, 2001; Toutanova et al., 2005), with parse ranking trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al.", "startOffset": 88, "endOffset": 129}, {"referenceID": 30, "context": "(2010), using the English Resource Grammar (Flickinger, 2000, 2011), and the PET parser (Callmeier, 2001; Toutanova et al., 2005), with parse ranking trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al.", "startOffset": 88, "endOffset": 129}, {"referenceID": 32, "context": ", 2005), with parse ranking trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al., 2009).", "startOffset": 83, "endOffset": 106}, {"referenceID": 10, "context": "6 The WikiWoods corpus was pre-processed using the Python packages pydelphin7 (developed by Michael Goodman), and pydmrs8 (Copestake et al., 2016).", "startOffset": 122, "endOffset": 146}, {"referenceID": 12, "context": "This resource was produced by Flickinger et al. (2010), using the English Resource Grammar (Flickinger, 2000, 2011), and the PET parser (Callmeier, 2001; Toutanova et al.", "startOffset": 30, "endOffset": 55}, {"referenceID": 26, "context": "To speed up training, we initialised our model using random positive-only projections, a simple method for producing reduced-dimension count vectors (QasemiZadeh and Kallmeyer, 2016).", "startOffset": 149, "endOffset": 182}, {"referenceID": 23, "context": "As with normal PPMIbased count vectors, there are several hyperparameters that can be tuned (Levy et al., 2015) \u2013 however, as we are using these vectors as parameters for semantic functions, it should be noted that the optimal hyperparameter settings are not the same.", "startOffset": 92, "endOffset": 111}, {"referenceID": 24, "context": "Both use Mikolov et al. (2013)\u2019s skipgram algorithm with negative sampling.", "startOffset": 9, "endOffset": 31}, {"referenceID": 21, "context": "We evaluated on two datasets which aim to capture similarity, rather than relatedness: SimLex999 (Hill et al., 2015), and WordSim-353 (Finkelstein et al.", "startOffset": 97, "endOffset": 116}, {"referenceID": 15, "context": ", 2015), and WordSim-353 (Finkelstein et al., 2001), which Agirre et al.", "startOffset": 25, "endOffset": 51}, {"referenceID": 0, "context": ", 2001), which Agirre et al. (2009) split into similarity and relatedness subsets.", "startOffset": 15, "endOffset": 36}, {"referenceID": 21, "context": "Performance of Word2Vec on SimLex-999 is higher than reported by Hill et al. (2015). Despite correspondence with the authors, it is not clear why their figures are so low.", "startOffset": 65, "endOffset": 84}, {"referenceID": 27, "context": "The RELPRON dataset was produced by Rimell et al. (2016). It consists of \u2018terms\u2019 (all nouns), each paired with up to ten \u2018properties\u2019.", "startOffset": 36, "endOffset": 57}], "year": 2017, "abstractText": "Functional Distributional Semantics is a framework that aims to learn, from text, semantic representations which can be interpreted in terms of truth. Here we make two contributions to this framework. The first is to show how a type of logical inference can be performed by evaluating conditional probabilities. The second is to make these calculations tractable by means of a variational approximation. This approximation also enables faster convergence during training, allowing us to close the gap with state-of-the-art vector space models when evaluating on semantic similarity. We demonstrate promising performance on two tasks.", "creator": "LaTeX with hyperref package"}}}