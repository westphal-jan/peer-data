{"id": "1709.00226", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Semantic Composition via Probabilistic Model Theory", "abstract": "semantic composition remains an open problem for vector space models of semantics. in this paper, we explain how the probabilistic graphical model used in the framework of functional distributional semantics can be interpreted as a probabilistic version of model theory. building on this, we explain how various semantic phenomena used can be recast in terms of conditional probabilities in the graphical model. this connection between formal vector semantics and machine learning is helpful in both directions : it gives us an technically explicit mechanism for modelling context - dependent meanings ( a challenge for formal relational semantics ), and both also strongly gives us well - motivated techniques constructed for composing distributed representations ( a challenge for distributional semantics ). we present results on two candidate datasets that go beyond mere word similarity, showing how these semantically - closely motivated techniques improve on the performance of vector models.", "histories": [["v1", "Fri, 1 Sep 2017 10:01:52 GMT  (26kb,D)", "http://arxiv.org/abs/1709.00226v1", "International Conference on Computational Semantics (IWCS)"]], "COMMENTS": "International Conference on Computational Semantics (IWCS)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guy emerson", "ann copestake"], "accepted": false, "id": "1709.00226"}, "pdf": {"name": "1709.00226.pdf", "metadata": {"source": "CRF", "title": "Semantic Composition via Probabilistic Model Theory", "authors": ["Guy Emerson"], "emails": ["gete2@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Vector space models of semantics are popular in NLP, as they are easy to work with, can be trained on unannotated corpora, and are useful in many tasks. They can be trained in multiple ways, including count methods (Turney and Pantel, 2010) and neural embedding methods (Mikolov et al., 2013). Furthermore, they allow a natural and computationally efficient measure of similarity, in the form of cosine similarity.\nHowever, even if we can train models that produce good similarity scores, a vector space does not provide natural operations for other aspects of meaning. How can vectors be composed to form semantic representations for larger phrases? Can we say that one vector implies another? How do we capture how meanings vary according to context? An overview of existing approaches to these questions is given in \u00a72, but these issues do not have clear solutions.\nIn contrast, the framework of Functional Distributional Semantics (Emerson and Copestake, 2016) (henceforth E&C) aims to overcome such issues, not by extending a vector space model, but by learning a different kind of representation. Each predicate is represented not by a vector, but by a function, which forms part of a probabilistic graphical model. In \u00a73, we build on the description given by E&C, and explain how this graphical model can in fact be viewed as encapsulating a probabilistic version of model theory. With this connection, we can naturally transfer concepts in formal semantics to this probabilistic framework, and we culminate in \u00a73.5 by showing how generalised quantifiers can be interpreted in our probabilistic model. In \u00a74, we look at how to naturally derive context-dependent representations, and further, how these representations can be used for certain kinds of inference and semantic composition.\nIn \u00a75, we turn to using the model in practice, and evaluate on three tasks. Firstly, we look at lexical similarity, to show it is competitive with vector-based models. Secondly, we consider the dataset produced by Grefenstette and Sadrzadeh (2011), which measures the similarity of verbs in the context of a specific subject and object. Finally, we consider the RELPRON dataset produced by Rimell et al. (2016), which requires matching individual nouns to short phrases including relative clauses. Our aim is to show that, not only does the connection with formal semantics give us well-motivated techniques to tackle these disparate datasets, but this also leads to improvements in performance.\nar X\niv :1\n70 9.\n00 22\n6v 1\n[ cs\n.C L\n] 1\nS ep\n2 01\n7"}, {"heading": "2 Related Work", "text": "One approach to compositionality in a vector space model is to find a composition function that maps a pair of vectors to a new vector in the same space. Mitchell and Lapata (2010) compare a variety of such functions, but they find that componentwise addition and multiplication are in fact competitive with the best functions they consider, despite being symmetric and hence insensitive to syntax.\nAnother approach is to use a recurrent neural network, which processes text one token at a time, updating a hidden state vector at each token. The final hidden state can be seen as a representation of the whole sequence. However, the state cannot be directly compared to the word vectors \u2013 indeed, they may have different numbers of dimensions. Other architectures have been proposed, aiming to use syntactic structure, such as recursive neural networks (Socher et al., 2010). However, this still does not use semantic structure \u2013 for example, there is no connection between active and passive voice sentences.\nCoecke et al. (2010) and Baroni et al. (2014) introduce a tensor-based approach, where words are represented not just by vectors, but also by higher-order tensors, which combine according to argument structure: nouns are vectors, intransitive verbs are matrices (mapping noun vectors to sentence vectors), transitive verbs are third-order tensors (mapping pairs of noun vectors to sentence vectors), and so on. However, Grefenstette (2013) showed that quantifiers cannot be expressed in this framework.\nFurthermore, in all the above methods, it is unclear how to perform inference, While we can use the representations as input features for another system, they do not have an inherent logical interpretation. Balkir et al. (2016) extend the tensor-based framework to allow inference, but rely on existing vectors, and must assume the dimensions have logical interpretations. Lewis and Steedman (2013) use distributional information to cluster predicates, but this leaves no graded notion of similarity. Garrette et al. (2011) and Beltagy et al. (2016) incorporate a vector space model into a Markov Logic Network, in the form of weighted inference rules (the truth of one predicate implying the truth of another). However, this assumes we can interpret similarity in terms of inference (a position defended by Erk (2016)), and requires existing vectors, rather than directly learning logical representations from distributional data.\nMany proposals exist for contextualising vectors. Erk and Pado\u0301 (2008) and Thater et al. (2011) modify a vector according to syntactic dependencies. However, by proposing new operations, they make assumptions about the properties of the space, which may not apply to all models. Erk and Pado\u0301 (2010) build a context-specific vector, by combining the most similar contexts in a corpus. However, this reduces the amount of training data. Lui et al. (2012)\u2019s \u201cper-lemma\u201d model uses Latent Dirichlet Allocation to model contextual meaning as a mixture of senses, but this requires training a separate LDA model for each word. Furthermore, all of these methods focus on a specific kind of context, making it nontrivial to generalise them to arbitrary contexts.\nOur notion of probabilistic truth values is similar to the Austinian truth values in the framework of probabilistic Type Theory with Records (TTR) (Cooper, 2005; Cooper et al., 2015). Sutton (2015, 2017) takes a similar probabilistic approach to truth values to deal with philosophical problems concerning gradable predicates. Our stochastic generation of situations is also similar to the approach taken by Goodman and Lassiter (2015), who represent semantics with the stochastic lambda calculus, using handwritten probabilistic models to show how semantics and world knowledge can interact. While these approaches are in principle compatible with our work, they do not provide an approach to distributional semantics. We use Minimal Recursion Semantics (Copestake et al., 2005), as it can be represented using dependency graphs \u2013 this allows a more natural connection with probabilistic graphical models, as explained in \u00a73.4.\nOthers have also proposed representing the meaning of a predicate as a classifier. Larsson (2013) represents the meaning of a perceptual concept as a classifier of perceptual input, in the TTR framework. Schlangen et al. (2016) train image classifiers using captioned images, and Zarrie\u00df and Schlangen (2017a,b) build on this, using distributional similarity to help train such classifiers. However, they do not learn an interpretable representation directly from text; rather, they use similarity scores to generalise from one label of an image to other similar labels. McMahan and Stone (2015) represent the meaning of a colour term as a probabilistic region of colour space, which could also be interpreted as a probabilistic classifier. However, this model was not intended to be a general-purpose distributional model."}, {"heading": "3 From Model Theory to Probability Theory", "text": "In this section, we show how model theory can be recast in a probabilistic setting. The aim is not to detail a full probabilistic logic, but rather to show how we can define a family of probability distributions that capture traditional model structures as a special case, while also allowing structured representations of the kind used in machine learning. In this way, we will be able to view Functional Distributional Semantics as a generalisation of model theory."}, {"heading": "3.1 Background: Model Theory, Neo-Davidsonian Events, and Situations", "text": "A standard approach to formal semantics is to use an extensional model structure (Cann, 1993; Allan, 2001; Kamp and Reyle, 2013). We first define a set of \u2018individuals\u2019 (or \u2018entities\u2019) in the model. We then define the meaning of a predicate to be its extension \u2013 the subset of individuals for which the predicate is true. The extension can also be characterised in terms of a truth-conditional function \u2013 a function mapping from individuals to truth-values. Individuals in the extension of the predicate are mapped to true, and all other individuals to false.\nWe take a neo-Davidsonian approach to event semantics (Davidson, 1967; Parsons, 1990). This treats events as also being individuals, and verbal predicates are one-place relations, which can be true of event individuals. Other participants in an an event are indicated by two-place relations, linking the event to the argument individual. For example, a sentence like pictures tell stories would be represented with three individuals and five relations: picture(x), tell(y), story(z), ARG1(y, x), ARG2(y, z). Here, the ARG1 and ARG2 relations express the argument structure of the telling event.\nFinally, we take an approach in the spirit of situation semantics (Barwise and Perry, 1983), and assume that the model contains a set of situations. Each situation consists of a small number of individuals (unlike a possible world, which would consist of many individuals), and the relations that stand between them. As we are taking a neo-Davidsonian approach, this means that we take a situation to be set of individuals, where each predicate assigns a truth value to each individual, and where there are two-place relations between individuals, to express argument structure."}, {"heading": "3.2 Model Structures as Probability Distributions", "text": "In this section, we generalise this notion of a model structure in two ways. Firstly, rather than a set of situations, we will consider a probability distribution over a set of situations. Secondly, rather than deterministic truth-conditional functions, we will consider probabilistic truth-conditional functions.\nA probability distribution over a set of situations is naturally more general than the set itself, since it provides more information \u2013 as well as knowing that a situation is in the set, we additionally know its probability. From the formal linguistic point of view, this might seem irrelevant to the notion of truth. However, from the machine learning point of view (and perhaps also from the acquisition point of view), it is very helpful \u2013 if our aim is not just to represent what is true, but also to learn what is true, we do not know in advance what situations should be part of the model structure. By using probability distributions, we can smoothly change between different models. This lets us use continuous optimisation algorithms, such as methods based on gradient descent, which are generally more efficient than discrete optimisation algorithms. Intuitively, as we learn about what kinds of situations exist, we can update the model appropriately.\nA truth-conditional function can be defined as a function mapping from a set of individuals to the set {0, 1}, where 0 denotes falsehood, and 1 truth. We can generalise this to a function mapping from a set of individuals to the range [0, 1]. This allows us to naturally model the fuzzy boundaries of concepts, by using intermediate values between 0 and 1. This idea was used by Labov (1973) to model the fuzzy boundaries between concepts like cup, mug, and bowl. For an unusual object that is intermediate between a typical cup and a typical bowl, we can say that the predicates for cup and mug both have an intermediate probability of being true of the object. As with our previous generalisation step, allowing a continuous range of values is also helpful during learning, since we can smoothly change a function between assigning truth or falsehood to a particular individual.\ny zx ARG2ARG1\n\u2208 X\ntc, x tc, y tc, z\n\u2208 {\u22a5,>} |V |\np q r\n\u2208 V"}, {"heading": "3.3 Denotations versus Truth-Conditional Functions", "text": "If individuals are atomic elements, without any further structure, then denotations and truth-conditional functions have almost identical representations. A denotation is a subset of the set of individuals, while a truth-conditional function is the indicator function for this subset: individuals in the denotation are mapped to 1, and other individuals to 0. Converting between these two representations is trivial.\nHowever, if individuals are structured objects, denotations and truth-conditional functions may have rather different representations. To represent the structure of individuals, we assume we have a semantic space, where each point in the space represents a possible individual, including information about all its features. We will use the term \u2018pixie\u2019 to refer to a point in the semantic space, as it is intuitively a \u2018pixel\u2019 of the space. Note that E&C use the term \u2018entity\u2019 to refer to both individuals and pixies.\nFor example, consider a model with five individuals: two black cats, a white cat, a bowl of rice, and a carrot. If we use a semantic space, and represent these individuals with the features COLOUR (black, white, or orange) and ANIMACY (+ or \u2212), then the denotation of the predicate for cat is a set of three individuals, whose pixies are {COLOUR: black, ANIMACY: +} (appearing twice) and {COLOUR: white, ANIMACY: +} (appearing once).1 As a probability distribution, the denotation assigns a probability of 2\u20443 to the first pixie, 1\u20443 to the second, and 0 to all others. However, the truth-conditional function can be represented much more simply \u2013 it takes the value 1 if and only if the pixie is animate.\nAs can be seen in this example, we may have multiple individuals represented by the same pixie. This can be accounted for in the probabilistic model structure, by assigning higher probabilities to pixies that correspond to more individuals. Note that, technically, this means that we are not working directly with distributions over situations, but rather with distributions over equivalence classes of situations, where situations are equivalent if their individuals are indistinguishable in terms of their features."}, {"heading": "3.4 Functional Distributional Semantics as Model-Theoretic Semantics", "text": "Now we have described the above probabilistic generalisation of a model structure, we explain how Functional Distributional Semantics can be seen as implementing such a generalised model structure.\nE&C define a probabilistic graphical model to generate semantic dependency graphs like that in\ninterpretation of the dependency graphs: each node represents a predicate, and the ARG links represent argument structure. The graphical model in Fig. 2 generates dependency graphs corresponding to transitive sentences \u2013 the predicates (p, q, r) can be seen at the bottom, and the dependency links (ARG1, ARG2) can be seen at the top. For example, p, q, and r might correspond to pictures, tell, and stories.\nRather than generating a dependency graph directly, the semantic function model assumes that it generated based on latent structure. We assume that for each observed predicate, there is an unobserved, latent pixie which the predicate is true of. These pixies are the orange nodes at the top of Fig. 2. Each pixie node is a random variable, taking values in the semantic space X of all possible pixies. We also assume that every predicate is either true or false of each pixie. These truth values are the purple nodes in the middle row of Fig. 2 (note that each node is repeated |V | times, once for each predicate). They are random variables, with two possible values: true or false. While we know each observed predicate is true of its pixie, the truth values for all other predicates are latent variables.\nThe generative model proceeds from the top to the bottom of Fig. 2. First, we define a joint distribution over pixies, as an undirected graphical model \u2013 whenever a pair of pixie nodes is linked, the model determines how likely it is for specific values of those nodes to co-occur. This allows us to generate tuples of pixies. E&C implement this with a Cardinality Restricted Boltzmann Machine (CaRBM) (Swersky et al., 2012): pixies are sparse binary-valued vectors, with each dimension representing a different feature. Each dependency link determines how likely it is for specific features of the linked pixies to co-occur; this is encoded using one trainable parameter for each pair of dimensions.\nNext, we define a semantic function for each predicate \u2013 this maps each pixie to the probability that the predicate is true of it. So, given a set of generated pixies, we can generate truth values for each pixie. E&C implement these functions with one-layer feedforward networks \u2013 by using a sigmoid activation, the output is in the range [0, 1], so it can be interpreted as a probability. Finally, given the truth values for all predicates, we generate one predicate for each pixie, by choosing from the true predicates.\nThe above generative process was given by E&C. However, we can see the first two stages in this process as an instance of the probabilistic model structure discussed in \u00a73.2. The linked pixies can together be viewed as a situation. The joint distribution over pixies then gives us a distribution over situations, which can be seen as our probabilistic generalisation of a set of situations in a model structure. Furthermore, as the semantic functions map from pixies to probabilities, they can be seen as generalised truth-conditional functions. So, we can view the semantic function model as generating dependency graphs based on a probabilistic model structure. In this model, a denotation can be represented by a probability distribution over the semantic space, while a truth-conditional function can be represented by a semantic function, mapping from the semantic space to [0, 1].\nHowever, we should note that this model only implements soft constraints on semantics \u2013 indeed, it would be difficult to learn hard constraints from corpus data alone. This means that, all our distributions over pixies have a non-zero probability for every pixie, and all our semantic functions assign a non-zero probability of truth to every pixie. By analogy with a traditional model structure, we might want to have zero values, to indicate that a certain pixie or situation is impossible, or that a certain predicate is definitely false. However, from a Bayesian point of view, zero probabilities are problematic \u2013 they would imply that, no matter what new evidence we observe, we cannot change our mind.\nIn practice, some probabilities will be vanishingly small. In fact, to make interesting predictions, this is necessary \u2013 for high-dimensional spaces, an interesting subspace (perhaps representing a domain, like rock-climbing or ballroom dancing) may be small. For example, suppose we have 1000 binary-valued dimensions, with only 40 active at once. This gives 1072 pixies. A subspace only using 200 dimensions has 1042 pixies, or one part in 1030 of the whole space! To define a distribution with most probability mass in this subspace, pixies in the subspace must be at least 1030 times more likely than outside.\nSuppose a predicate is probably true in this subspace, and probably false outside. Given a uniform prior over the space, and observing the predicate to be true, we may expect the posterior to assign most probability mass to the subspace. For this to happen, the probability of truth in the subspace must be 1030 times larger than outside. So, for a semantic function to be useful, it must be close to a step function. This makes it look more like a traditional truth-conditional function with only 0 and 1 as values."}, {"heading": "3.5 Interpretation of Quantifiers", "text": "Interpreting the semantic function model as a probabilistic model structure, we can define quantification in a natural way. Unlike Herbelot and Vecchi (2015), we are not mapping from a distributional space to a model structure, but directly interpreting quantifiers in our distributional model.\nTo assign a truth value to DMRS graph, we must first convert it to a fully scoped representation, such as in Fig. 3. In cases of scope ambiguity, a single DMRS graph allows several scoped representations, and this conversion must resolve the ambiguity.3\nIn a complete structure, there is one quantifier for each MRS variable, and hence also for each pixie-valued random variable, as there is a one-to-one mapping between them. For each quantifier, we define a binary-valued random variable, representing whether the quantified expression is true (given any remaining free variables). We define distributions for these random variables recursively, bottom-up through the scope tree. At each stage, we marginalise out the quantified pixie variable. This is analogous to semantic composition in traditional models \u2013 truth values are calculated bottom-up, and evaluating a quantifier removes a free variable. At the root of the tree, we have a single probabilistic truth value.\nEach quantifier depends on its restriction and body, each of which may be either4 a predicate or a quantified expression \u2013 both are binary-valued random variables. In the classical theory of generalised quantifiers, the truth of a quantified expression depends on the cardinality of the restriction set, and the cardinality of the intersection of the restriction and body sets (Barwise and Cooper, 1981; Van Benthem, 1984). As explained in \u00a73.3, our probabilistic model structure uses probabilities in place of cardinalities. Let the probabilistic truth values for the quantified expression, restriction, and body be Q, R, and B, respectively. It makes sense to consider the conditional probability P (B|R), which naturally uses both of the classical sets, since P (B|R) = P (R\u2229B)P (R) . Intuitively, the truth of Q depends on how likely B is to be true, given that R is true.5\nIn a traditional logic, the truth value of a quantified expression is a function of all free variables. Analogously, each quantifier\u2019s random variable is conditionally dependent on all free variables. More precisely, if the set of free variables is V , let us define the function q(V ) = P (B|R, V ). We can now consider different quantifiers, and define the probability of the quantified expression Q being true, in terms of the value of q: every is true iff q = 1, some is true iff q > 0, most is true iff q > 1/2, and so on. In these cases, the probability of truth is exactly 0 or 1, but by using intermediate probabilities, we can also naturally model \u2018fuzzy\u2019 quantifiers such as few and many, which do not have a sharp cutoff.\n3 A DMRS graph includes scopal constraints, specifying that nodes are the same place in the scope tree, or that one dominates another. These constraints were not used in the generative model in \u00a73.4, akin to other simplified MRS-based dependency structures such as EDS (Oepen and L\u00f8nning, 2006), but they are necessary to specify the correct set of scope readings.\n4 More generally, we may have a set of predicates and quantified expressions. In this case, we can condition on all truth values in the set. We consider a single random truth value, for ease of exposition.\n5 This account does not cover cardinal quantifiers. However, the English Resource Grammar (ERG) represents numbers not as quantifiers, but as additional predicates. This is compatible with Link (2002)\u2019s lattice-theoretic approach, which allows reference to plural individuals without quantification. For more information on the semantic analyses in the ERG, see the documentation produced by Flickinger et al. (2014), which is available here: http://www.delph-in.net/esd"}, {"heading": "4 From Conditional Dependence to Context Dependence", "text": "In the previous section, we saw how a model structure can be generalised using probability distributions. In this section, we show how this approach allows us to capture context-dependent meanings using conditional probabilities, in a natural way."}, {"heading": "4.1 Occasion Meaning versus Standing Meaning", "text": "When discussing context dependence (a challenge for both formal semantics and vector-based semantics), it is helpful to distinguish two kinds of meaning, following Quine (1960): standing meaning refers to the fixed, unchanging meaning that a linguistic expression has in general; occasion meaning refers to the particular meaning that a linguistic expression has in a given context.\nSearle (1980) discusses an interesting set of examples, noting how a gardener cutting grass involves a very different kind of cutting from a child cutting a cake. There is something common to both events, but they involve different tools and different physical motions. However, Searle also notes how there are also less obvious interpretations of these expressions. For a gardener who sells turf to people who need ready-grown lawns, cutting grass could also refer to cutting out an area of grass, including the soil.6 This kind of cutting would more closely resemble cutting a cake. We can see from this example that while an expression may refer to quite different situations, some situations may be more likely than others.\nThis state of affairs can be modelled in our probabilistic version of model theory. We can say that an expression like cut grass has fixed truth conditions \u2013 it is true of both mowing a lawn and preparing a section of turf. However, we can also say that the former is much more probable than the latter. More precisely, our prior distribution over situations assigns a much higher probability to lawn-mowing situations than to turf-slicing situations; but the probabilistic truth-conditional functions for cut and grass assign high probabilities to their respective individuals in both of these types of situation.\nWhile the expression cut grass could refer to different types of situation, in most contexts we can infer that it is likely to refer to a lawn-mowing situation. We can view this as performing Bayesian inference. We begin with a prior probability distribution over situations, where the situation includes (at least) two pixies y and z, with an ARG2 link from y to z (i.e. the situations with enough structure for cut grass, since grass is the ARG2 of cut). This prior distribution is given to us by our probabilistic model structure. Importantly, these situations are jointly distributed with truth values for the predicates for cut and grass (as well as all other predicates). On observing that the cut predicate is true of the pixie y, and the grass predicate is true of the pixie z (the ARG2 of y), we can form a posterior distribution over situations (i.e. a joint posterior over the pixies). This posterior should assign a high probability to lawn-mowing situations, a low probability to turf-slicing situations, and an extremely low probability to unrelated situations like baking a cake. If we receive information that makes a turf-slicing situation more likely (e.g. hearing about a person selling turf), we can update our posterior again, and assign a higher probability to such a situation. However, in the absence of such information, we effectively \u2018default\u2019 to the higher-probability lawn-mowing interpretation.\nIn summary, we can view truth-conditional functions as representing context-invariant standing meanings, and posterior distributions over situations as representing context-dependent occasion meanings."}, {"heading": "4.2 Context Dependence in Functional Distributional Semantics", "text": "In Functional Distributional Semantics, the standing meaning of a predicate is its semantic function \u2013 a mapping from the semantic space to probabilities of truth. These are the arrows in Fig. 2 from the orange pixie nodes (top row) to the purple truth value nodes (middle row). These functions are implemented as feedforward neural networks \u2013 note that the meaning is not the input or output of a network, but rather the network itself.\nThe occasion meaning of a predicate is the posterior distribution over the semantic space, for the pixie the predicate is true of. The pixies are the orange nodes in Fig. 2, but these nodes do not directly\n6 There are yet other interpretations of cut grass, such as adulterate marijuana, but we focus on the two discussed by Searle.\nstand for meanings \u2013 an occasion meaning is the posterior distribution of such a node, when conditioned on the truth of one or more predicates.\nWe should also note that, while we only consider specific kinds of linguistic contexts in this paper, this approach generalises to arbitrary contexts. As an occasion meaning is simply a posterior distribution, we could in principle condition on any kind of observation. For example, if we are dealing with a specific domain, and we know the kinds of pixies that are likely to appear in this domain, we can produce a domain-specific meaning, which we could then further condition on a linguistic context.\nTo calculate occasion meanings, we need to calculate posterior distributions over the semantic space, given some observed truth values. However, exactly calculating the posterior is generally intractable, as this requires summing over the entire semantic space. For a large number of dimensions, the space is simply too big. Sampling from the space using a Markov Chain Monte Carlo method, as described by E&C, is also computationally expensive.\nTo make calculating the posterior tractable, we can use a variational approximation \u2013 this involves specifying a restricted class of distributions which is easier to work with, and then finding the optimal distribution in this restricted class that approximates the posterior. In particular, we can use a mean field approximation, following Emerson and Copestake (2017) (henceforth E&C2) \u2013 we assume that each dimension (intuitively, each feature) has an independent probability of being active, and we optimise each of these probabilities based on the mean activations of all other dimensions. Under this approximation, an occasion meaning is represented by a mean field vector. Intuitively, we assign high probabilities to a dimension for two possible reasons: either it\u2019s connected with high weights to highly probable dimensions in other pixies, or activating this dimension makes it much more likely for an observed predicate to be true. If neither of these facts hold, we will assign a low probability \u2013 because we are enforcing sparsity on the pixie vectors, the dimensions are effectively competing with each other.\nThis mean field approximation gives us a tractable way to approximately calculate a posterior distribution over pixies. This allows us to construct vectors representing context-dependent meanings, which we can use as the basis for further calculations, as illustrated in the following sections."}, {"heading": "4.3 Semantic Composition using Context-Dependent Meanings", "text": "Semantic composition involves taking semantic representations for multiple expressions, and combining them into a single representation for the whole expression. In vector space models, this involves mapping two or more vectors to a single vector. Intuitively, with a fixed number of dimensions, this loses information. As Mooney (2014) colourfully put it, \u201cYou can\u2019t cram the meaning of a whole %&!$# sentence into a single $&!#* vector!\u201d More precisely, if nearby vectors represent similar meanings, only the first few significant digits of each dimension are important, which limits how much information a vector can contain. Even if we use sparse vectors, at some point we will have \u2018used up\u2019 all of the available dimensions. So, composing vectors is not viable in the general case \u2013 even proponents of vector space models would not suggest composing vectors to produce an accurate representation of an entire book.\nUnlike vectors, the representations used in formal semantics are not bounded in size \u2013 logical formulae and semantic dependency graphs can be arbitrarily large. While it can be useful to summarise a large representation with a smaller one, we do not believe that a semantic theory should force composition to involve summarisation. Full and detailed semantic representations should also have their place.\nIn a semantic function model, we can use DMRS composition. Individual lexical items are associated with predicates, and these are composed to form a DMRS graph. However, the probabilistic framework gives us a new interpretation of the DMRS graphs. If we start from two DMRS graphs, we can consider the two posterior distributions over situations defined by those graphs. Once we compose these two graphs, we have a new posterior distribution, over larger situations. However, this posterior is not the same as naively combining the posteriors of the two subgraphs. As the pixie nodes of the two subgraphs are now linked together, we have a joint distribution for all the pixie nodes, which depends on all the observed predicates. This means that, as we build a composed DMRS graph, we modify the posterior distributions at every step. In this way, we can see semantic composition as simultaneously composing the logical structure and refining the context-dependent meanings."}, {"heading": "4.4 Inference using Context-Dependent Meanings", "text": "A semantic function model includes a random variable for the truth of each predicate for each pixie. As noted by E&C2, these random variables allow us to convert certain logical propositions into statements about conditional probabilities. For example, we might be interested in whether one predicate implies another. For simplicity, we can first consider a situation containing only a single pixie x, as shown in Fig. 4a. Then, the proposition \u2200x \u2208 X , p(x)\u21d2 a(x) is equivalent to the statement P (ta,x|tp,x) = 1. Conditioning on tp,x means restricting to those pixies x for which the predicate p is true, and if the probability of ta,x being true is 1, then it is always true. Similarly, \u2203x \u2208 X , a(x) \u2227 b(x) is equivalent to P (ta,x|tp,x) > 0. This equivalence is discussed in more detail by E&C2.\nIn practice, the conditional probability P (ta,x|tp,x) will never be exactly 0 or 1, as discussed in \u00a73.4. Nonetheless, this quantity represents the degree to which a implies b, in an intuitive sense: the higher the value, the closer we are to every; and the lower the value, the closer we are to no. We will use this quantity in \u00a75.1 to measure semantic similarity.\nTo calculate P (ta,x|tp,x), we need to marginalise out x, because the model defines the joint probability P (x, tp,x, ta,x). This is analogous to the process of removing bound variables in \u00a73.5, but note that here we do not have quantifiers to evaluate. Rather, we know certain facts about a situation, so we want to consider just those situations where those facts are true. Exactly marginalising out a pixie would require summing over the entire semantic space X , which is intractable for a large number of dimensions. As explained in \u00a74.2, the posterior for x given tp,x can be approximated using a mean field vector. This gives us a probability for each dimension of x, representing a \u2018typical\u2019 pixie for the observed truth values. Applying the semantic function for a to this mean field vector lets us approximately calculate P (ta,x|tp,x).\nIn the general case, we have more than one pixie in a situation, as shown in Fig. 4b. For example, if we know that a person is cutting grass, we could ask how likely it is that the person is also a gardener (likely), an artist (less likely), or a flowerpot (very unlikely). As before, we can answer this question by calculating a conditional probability: P (ta, x|tp, x, tq, y, tr, z). Because the truth values are connected via the latent pixies, the truth of one predicate depends on all the others. Inference requires marginalising out all pixies in the situation, so we first find the joint mean field distribution for all pixies, and then apply the semantic function for gardener to the mean field vector for the person pixie. Note how the contextdependent meaning of person (the mean field vector) is crucial to this calculation \u2013 although we are only applying applying the gardener function to the person vector, this vector depends on all predicates in the context."}, {"heading": "5 Experimental Results", "text": "We trained our model using WikiWoods7, a corpus providing DMRS graphs for 55m sentences of English (900m tokens). WikiWoods was produced by Flickinger et al. (2010) and Solberg (2012) from the July 2008 dump of the full English Wikipedia, using the English Resource Grammar (Flickinger, 2000, 2011) and the PET parser (Callmeier, 2001; Toutanova et al., 2005), with parse ranking trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al., 2009). It is distributed by DELPH-IN.\nWe extracted SVO triples (in a slight abuse of terminology), by which we mean DMRS subgraphs comprising a verbal predicate and nominal ARG1 and/or ARG2, discarding pronouns and named entities. This gives 10m full SVO triples, and a further 21m where one of the two arguments is missing. For further details, see E&C. To preprocess the corpus, we used the python packages pydelphin8 (developed by Michael Goodman), and pydmrs9 (Copestake et al., 2016). Our source code is available online.10\nCarefully initialising the model parameters allows us to drastically reduce the necessary training time. We initialised the parameters of the semantic functions using random positive-only projections, a simple random-indexing technique introduced by QasemiZadeh and Kallmeyer (2016). The total number of dimensions is fixed, and each context predicate is randomly assigned to a context dimension (which means that many contexts will be randomly assigned to the same dimension). For each target predicate, we count how many times each context dimension appears. With these counts, we can calculate a standard PPMI vector. This method lets us initialise vectors in very little time, and we can use the same hyperparameters discussed by Levy et al. (2015). However, it should be noted that, because we are not using the vectors in the same way, the ideal hyperparameters are not the same. In particular, we found that, unlike for normal word vectors, it was unhelpful to use a negative offset for PPMI scores.\nOnce the semantic function parameters have been initialised, the CaRBM parameters can be initialised based on mean field vectors. Each semantic function defines a no-context mean field vector, as described in \u00a74.4 for Fig. 4a. For each SVO triple in the training data, we can take the mean field vectors for the observed predicates, and for each link, we can calculate the mean field activation of each pair of dimensions of the linked pixies \u2013 this is simply the outer product of the mean field vectors for the linked pixies. We can then average these mean field activations across the whole training set, and calculate PPMI scores, which we can use to initialise the link\u2019s parameters. For an average mean field activation of f , the PPMI is log(f)\u2212 2 log(CD ), where D is the dimensionality and C the cardinality, since the expected activation of a pair of dimensions of two random vectors is (CD )\n2. We compare our model to two vector baselines. The first is a standard Word2Vec model (Mikolov et al., 2013), trained on the plain text version of the WikiWoods corpus. The second is the same Word2Vec algorithm, trained on the SVO triples we used to train our model: each triple was used to produce one \u2018sentence\u2019, where each \u2018token\u2019 is a predicate.\nFinding a good evaluation task is far from obvious. Simple similarity tasks do not require semantic structure, while tasks like textual entailment require a level of coverage beyond the scope of this paper. We consider the SVO similarity and RELPRON datasets, described below, because they provide restricted tasks in which we can explore approaches to semantic composition. The results on RELPRON were also reported by E&C2, but we give further error analysis here. In future work, we plan to use the datasets produced by Herbelot and Vecchi (2016) and Herbelot (2013), where pairs of \u2018concepts\u2019 (such as tricycle) and \u2018features\u2019 (such as is small) are annotated with suitable quantifiers (out of these options: all, most, some, few, no). One challenge posed by these datasets is the syntactic variation in the features, such as has 3 wheels and lives on coasts. These datasets can be seen as a further stepping stone between this paper and general textual entailment.\n7 http://moin.delph-in.net/WikiWoods 8 https://github.com/delph-in/pydelphin 9 https://github.com/delph-in/pydmrs\n10 https://github.com/guyemerson/sem-func"}, {"heading": "5.1 Lexical Similarity", "text": "We evaluated our model on several lexical similarity datasets. Our aim is firstly to show that the performance of our model is competitive with state-of-the-art vector space models, and secondly to show that our model can specifically target similarity rather than relatedness. For example, while the predicates painter and painting are related, they are true of very different individuals.\nWe used SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016), which both aim to measure similarity, not relatedness; MEN (Bruni et al., 2014); and WordSim-353 (Finkelstein et al., 2001), which Agirre et al. (2009) split into similarity and relatedness subsets.\nTo calculate a similarity score in our model, we can use the conditional probability of one predicate being true, given that another predicate is true, as shown in Fig. 4a. To make this into a symmetric score, we can multiply the conditional probabilities in both directions. Results are shown in Table 1.11\nWe can see that the semantic function model is competitive with Word2Vec, but has qualitatively different behaviour, as it has very low correlation for the relatedness subset of WordSim-353. It has lower performance on MEN and the similarity subset of WordSim-353, but these two datasets were not annotated to target similarity, in the sense given above. For SimLex-999 and SimVerb-3500, which do target similarity, performance is higher than Word2Vec.\nWe note also that the performance of our model is higher than that reported in our previous work. This is due to better hyperparameter tuning. Using the initialisation method described above allowed for faster experiments and hence a greater exploration of the hyperparameter space. Using more datasets also allowed for more targeted tuning: hyperparameters for each dataset were tuned on the remaining datasets, except for SimVerb-3500, which has its own development set. Compared to E&C2\u2019s results, performance is much improved on the verb subset of SimLex-999, which was previously tuned on noun datasets only, indicating that the optimal settings for nouns and verbs differ considerably."}, {"heading": "5.2 Similarity in Context", "text": "Grefenstette and Sadrzadeh (2011) produced a dataset of pairs of SVO triples, where only the verb varies in the pair. Each pair was annotated for similarity. For example, annotators had to judge the similarity\n11 Performance of Word2Vec on SimLex-999 is higher than reported by Hill et al. (2015). Despite correspondence with the authors, it is not clear why their figures are so low.\nof the triples (table, show, result) and (table, express, result). In line with lexical similarity datasets, a system can be evaluated using the Spearman rank correlation between the system\u2019s scores and the average annotations.\nFor each triple, we calculated the mean field vector for the verb, conditioned on all three predicates. We then calculated the probability that the other verb\u2019s predicate is true of this mean field vector, similarly to Fig. 4b (the only difference being that we are interested in pixie y, not pixie x). To get a symmetric score, we multiplied the probabilities in both directions.\nResults are given in the \u201cGS2011\u201d column of Table 2. The performance of our model (.25) matches the best model Grefenstette and Sadrzadeh consider. The performance of our ensemble (.32) matches the improved model of Grefenstette et al. (2013), despite using less training data. Furthermore, the fact that the ensemble outperforms both the semantic function model and the vector space model shows that the two models have learnt different kinds of information. This is not simply due to the combined model having a larger capacity \u2013 increasing the size of the individual models did not give this improvement."}, {"heading": "5.3 Composition of Relative Clauses", "text": "The RELPRON dataset was produced by Rimell et al. (2016). It consists of a set of \u2018terms\u2019, each paired with up to ten \u2018properties\u2019. Each property is a short phrase, consisting of a hypernym of the term, modified by a relative clause with a transitive verb. For example, a telescope is a device that astronomers use, and a saw is a device that cuts wood. The task is to identify the properties which apply to each term, construed as a retrieval task: given a single term, and the full set of properties, the aim is to rank the properties, with the correct properties at the top of the list. There are 65 terms and 518 properties in the development set, and 73 terms and 569 properties in the test set.\nSince every property follows one of only two patterns (subject or object relative clause), this dataset lets us focus on evaluating semantics, rather than parsing. A model that uses relatedness can perform fairly well on this dataset \u2013 for example, astronomer can predict telescope, without knowing what relation there is between them. However, the dataset also includes lexical confounders \u2013 for example, a document that has a balance is a financial account, not the quality of balance (not falling over). The textual overlap means that a vector addition model is easily fooled by such confounders, and indeed the best three models that Rimell et al. tested all ranked this confounding property at the top.\nWe can represent each property as a situation of three pixies, as in Fig. 4b. Although they are syntactically noun phrases, the argument structure is the same as a transitive clause. For each property, we calculated the contextual mean field vectors, conditioned on all three predicates. To find the probability that the term\u2019s predicate is true, we apply the term\u2019s semantic function to the hypernym\u2019s mean-field vector. The difference between subject and object relative clauses is captured by whether this vector corresponds to the ARG1 pixie or the ARG2 pixie.\nResults are given in the last two columns of Table 2. Our model performs worse than vector addition, perhaps as expected, since it does not capture relatedness, as explained in \u00a75.1. However, the ensemble performs better than either model alone \u2013 just as argued in \u00a75.2, this shows that our model has learnt different information from the vector space model. In particular, the ensemble improves performance on the lexical confounders. of which there are 27 in the test set. The vector space model places 17 of them in the top rank, and all of them in the top 4 ranks. The ensemble model, however, succeeds in moving 9 confounders out of the top 10 ranks. To our knowledge, this is the first system that manages to improve both overall performance as well as performance on the confounders."}, {"heading": "6 Conclusion", "text": "We can interpret Functional Distributional Semantics as learning a probabilistic model structure, which gives us natural operations for composition, inference, and context dependence, with applications in both computational and formal semantics. Our experiments show that the additional structure of the model allows it to learn and use information that is not captured by vector space models."}, {"heading": "Acknowledgements", "text": "We would like to thank Emily Bender, for helpful discussion and detailed feedback on an earlier draft. This work was supported by a Schiff Foundation studentship."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Semantic composition remains an open problem for vector space models of semantics. In this paper, we explain how the probabilistic graphical model used in the framework of Functional Distributional Semantics can be interpreted as a probabilistic version of model theory. Building on this, we explain how various semantic phenomena can be recast in terms of conditional probabilities in the graphical model. This connection between formal semantics and machine learning is helpful in both directions: it gives us an explicit mechanism for modelling context-dependent meanings (a challenge for formal semantics), and also gives us well-motivated techniques for composing distributed representations (a challenge for distributional semantics). We present results on two datasets that go beyond word similarity, showing how these semantically-motivated techniques improve on the performance of vector models.", "creator": "LaTeX with hyperref package"}}}