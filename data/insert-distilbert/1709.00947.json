{"id": "1709.00947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "abstract": "this paper describes a preliminary study package for producing and distributing a large - scale database of embeddings from the portuguese twitter stream. we start by directly experimenting with a relatively small sample and focusing on three challenges : volume of training data, objective vocabulary size and intrinsic evaluation metrics. using a single gpu, instead we were able to scale up vocabulary size from 2048 words embedded and 500k actual training examples to 32768 words over 10m training examples while keeping a stable validation loss and approximately linear trend graph on iq training time per epoch. we also observed that using less than 50 \\ % of api the available training examples for each vocabulary size might result in overfitting. results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words. nevertheless, intrinsic evaluation metrics suffer from over - sensitivity to their corresponding cosine similarity thresholds, indicating that within a wider range of metrics need patience to be developed to track progress.", "histories": [["v1", "Mon, 4 Sep 2017 13:30:23 GMT  (178kb,D)", "http://arxiv.org/abs/1709.00947v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pedro saleiro", "lu\\'is sarmento", "eduarda mendes rodrigues", "carlos soares", "eug\\'enio oliveira"], "accepted": false, "id": "1709.00947"}, "pdf": {"name": "1709.00947.pdf", "metadata": {"source": "CRF", "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "authors": ["Pedro Saleiro", "L\u00fa\u0131s Sarmento", "Eduarda Mendes Rodrigues", "Carlos Soares", "Eug\u00e9nio Oliveira"], "emails": ["pssc@fe.up.pt"], "sections": [{"heading": "1 Introduction", "text": "Word embeddings have great practical importance since they can be used as pre-computed high-density features to ML models, significantly reducing the amount of training data required in a variety of NLP tasks. However, there are several inter-related challenges with computing and consistently distributing word embeddings concerning the:\n\u2013 intrinsic properties of the embeddings. How many dimensions do we actually need to store all the \u201cuseful\u201d semantic information? How big should the embedded vocabulary be to have practical value? How do these two factors interplay? \u2013 type of model used for generating the embeddings. There are multiple possible models and it is not obvious which one is the \u201cbest\u201d, both in general or in the context of a specific type of applications. \u2013 the size and properties of training data: What is the minimum amount of training data needed? Should we include out of vocabulary words in the training?\nar X\niv :1\n70 9.\n00 94\n7v 1\n[ cs\n.C L\n] 4\nS ep\n\u2013 optimization techniques to be used, model hyperparameter and training parameters.\nNot only the space of possibilities for each of these aspects is large, there are also challenges in performing a consistent large-scale evaluation of the resulting embeddings [1]. This makes systematic experimentation of alternative wordembedding configurations extremely difficult.\nIn this work, we make progress in trying to find good combinations of some of the previous parameters. We focus specifically in the task of computing word embeddings for processing the Portuguese Twitter stream. User-generated content (such as twitter messages) tends to be populated by words that are specific to the medium, and that are constantly being added by users. These dynamics pose challenges to NLP systems, which have difficulties in dealing with out of vocabulary words. Therefore, learning a semantic representation for those words directly from the user-generated stream - and as the words arise - would allow us to keep up with the dynamics of the medium and reduce the cases for which we have no information about the words.\nStarting from our own implementation of a neural word embedding model, which should be seen as a flexible baseline model for further experimentation, our research tries to answer the following practical questions:\n\u2013 how large is the vocabulary the one can realistically embed given the level of resources that most organizations can afford to buy and to manage (as opposed to large clusters of GPU\u2019s only available to a few organizations)? \u2013 how much data, as a function of the size of the vocabulary we wish to embed, is enough for training meaningful embeddings? \u2013 how can we evaluate embeddings in automatic and consistent way so that a reasonably detailed systematic exploration of the previously describe space of possibilities can be performed?\nBy answering these questions based on a reasonably small sample of Twitter data (5M), we hope to find the best way to proceed and train embeddings for Twitter vocabulary using the much larger amount of Twitter data available (300M), but for which parameter experimentation would be unfeasible. This work can thus be seen as a preparatory study for a subsequent attempt to produce and distribute a large-scale database of embeddings for processing Portuguese Twitter data."}, {"heading": "2 Related Work", "text": "There are several approaches to generating word embeddings. One can build models that explicitly aim at generating word embeddings, such as Word2Vec or GloVe [2,3], or one can extract such embeddings as by-products of more general models, which implicitly compute such word embeddings in the process of solving other language tasks.\nWord embeddings methods aim to represent words as real valued continuous vectors in a much lower dimensional space when compared to traditional bagof-words models. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the fundamental information that allows creating such representations. Two approaches exist for building word embeddings. One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3]. The other approach consists in extracting internal representations from neural network models of text [2,5, 6]. Levy and Goldberg [7] showed that the two approaches are closely related.\nAlthough, word embeddings research go back several decades, it was the recent developments of Deep Learning and the word2vec framework [2] that captured the attention of the NLP community. Moreover, Mikolov et al. [8] showed that embeddings trained using word2vec models (CBOW and Skip-gram) exhibit linear structure, allowing analogy questions of the form \u201cman:woman::king:??.\u201d and can boost performance of several text classification tasks.\nOne of the issues of recent work in training word embeddings is the variability of experimental setups reported. For instance, in the paper describing GloVe [3] authors trained their model on five corpora of different sizes and built a vocabulary of 400K most frequent words. Mikolov et al. [8] trained with 82K vocabulary while Mikolov et al. [2] was trained with 3M vocabulary. Recently, Arora et al. [9] proposed a generative model for learning embeddings that tries to explain some theoretical justification for nonlinear models (e.g. word2vec and GloVe) and some hyper parameter choices. Authors evaluated their model using 68K vocabulary.\nSemEval 2016-Task 4: Sentiment Analysis in Twitter organizers report that participants either used general purpose pre-trained word embeddings, or trained from Tweet 2016 dataset or \u201cfrom some sort of dataset\u201d [10]. However, participants neither report the size of vocabulary used neither the possible effect it might have on the task specific results.\nRecently, Rodrigues et al. [11] created and distributed the first general purpose embeddings for Portuguese. Word2vec gensim implementation was used and authors report results with different values for the parameters of the framework. Furthermore, authors used experts to translate well established word embeddings test sets for Portuguese language, which they also made publicly available and we use some of those in this work."}, {"heading": "3 Our Neural Word Embedding Model", "text": "The neural word embedding model we use in our experiments is heavily inspired in the one described in [5], but ours is one layer deeper and is set to solve a slightly different word prediction task. Given a sequence of 5 words - wi\u22122 wi\u22121 wi wi+1 wi+2, the task the model tries to perform is that of predicting the middle word, wi, based on the two words on the left - wi\u22122 wi\u22121 - and the two words on the right - wi+1 wi+2: P (wi|wi\u22122, wi\u22121, wi+1, wi+2). This should\nproduce embeddings that closely capture distributional similarity, so that words that belong to the same semantic class, or which are synonyms and antonyms of each other, will be embedded in \u201cclose\u201d regions of the embedding hyper-space.\nOur neural model is composed of the following layers:\n\u2013 a Input Word Embedding Layer, that maps each of the 4 input words represented by a 1-hot vectors with |V | dimensions (e.g. 32k) into a low dimension space (64 bits). The projections matrix - Winput - is shared across the 4 inputs. This is not be the embedding matrix that we wish to produce.\n\u2013 a Merge Layer that concatenates the 4 previous embeddings into a single vector holding all the context information. The concatenation operation ensures that the rest of the model has explicit information about the relative position of the input words. Using an additive merge operation instead would preserve information onlu about the presence of the words, not their sequence.\n\u2013 a Intermediate Context Embedding Dense Layer that maps the preceding representation of 4 words into a lower dimension space, still representing the entire context. We have fixed this context representation to 64 dimensions. This ultimately determines the dimension of the resulting embeddings. This intermediate layer is important from the point of view of performance because it isolates the still relatively high-dimensional input space (4 x 64 bits input word embeddings) from the very high-dimensional output space.\n\u2013 a final Output Dense Layer that maps the takes the previous 64-bit representation of the entire input context and produces a vector with the dimensionality of the word output space (|V | dimensions). This matrix - Woutput - is the one that stores the word embeddings we are interested in.\n\u2013 A Softmax Activation Layer to produces the final prediction over the word space, that is the P (wi|wi\u22122, wi\u22121, wi+1, wi+2) distribution\nAll neural activations in the model are sigmoid functions. The model was implemented using the Syntagma1 library which relies on Keras [12] for model development, and we train the model using the built-in ADAM [13] optimizer with the default parameters."}, {"heading": "4 Experimental Setup", "text": "We are interested in assessing two aspects of the word embedding process. On one hand, we wish to evaluate the semantic quality of the produced embeddings. On the other, we want to quantify how much computational power and training data are required to train the embedding model as a function of the size of the vocabulary |V | we try to embed. These aspects have fundamental practical importance for deciding how we should attempt to produce the large-scale database\n1 https://github.com/sarmento/syntagma\nof embeddings we will provide in the future. All resources developed in this work are publicly available2.\nApart from the size of the vocabulary to be processed (|V |), the hyperparamaters of the model that we could potentially explore are i) the dimensionality of the input word embeddings and ii) the dimensionality of the output word embeddings. As mentioned before, we set both to 64 bits after performing some quick manual experimentation. Full hyperparameter exploration is left for future work.\nOur experimental testbed comprises a desktop with a nvidia TITAN X (Pascal), Intel Core Quad i7 3770K 3.5Ghz, 32 GB DDR3 RAM and a 180GB SSD drive."}, {"heading": "4.1 Training Data", "text": "We randomly sampled 5M tweets from a corpus of 300M tweets collected from the Portuguese Twitter community [14]. The 5M comprise a total of 61.4M words (approx. 12 words per tweets in average). From those 5M tweets we generated a database containing 18.9M distinct 5-grams, along with their frequency counts. In this process, all text was down-cased. To help anonymizing the ngram information, we substituted all the twitter handles by an artificial token \u201cT HANDLE\u201d. We also substituted all HTTP links by the token \u201cLINK\u201d. We prepended two special tokens to complete the 5-grams generated from the first two words of the tweet, and we correspondingly appended two other special tokens to complete 5-grams centered around the two last tokens of the tweet.\nTokenization was perform by trivially separating tokens by blank space. No linguistic pre-processing, such as for example separating punctuation from words, was made. We opted for not doing any pre-processing for not introducing any linguistic bias from another tool (tokenization of user generated content is not a trivial problem). The most direct consequence of not performing any linguistic pre-processing is that of increasing the vocabulary size and diluting token counts. However, in principle, and given enough data, the embedding model should be able to learn the correct embeddings for both actual words (e.g. \u201cronaldo\u201d) and the words that have punctuation attached (e.g. \u201cronaldo!\u201d). In practice, we believe that this can actually be an advantage for the downstream consumers of the embeddings, since they can also relax the requirements of their own tokenization stage. Overall, the dictionary thus produced contains approximately 1.3M distinct entries. Our dictionary was sorted by frequency, so the words with lowest index correspond to the most common words in the corpus.\nWe used the information from the 5-gram database to generate all training data used in the experiments. For a fixed size |V | of the target vocabulary to be embedded (e.g. |V | = 2048), we scanned the database to obtain all possible 5-grams for which all tokens were among the top |V | words of the dictionary (i.e. the top |V | most frequent words in the corpus). Depending on |V |, different numbers of valid training 5-grams were found in the database: the larger |V | the 2 https://github.com/saleiro/embedpt\nmore valid 5-grams would pass the filter. The number of examples collected for each of the values of |V | is shown in Table 1.\nSince one of the goals of our experiments is to understand the impact of using different amounts of training data, for each size of vocabulary to be embedded |V | we will run experiments training the models using 25%, 50%, 75% and 100% of the data available."}, {"heading": "4.2 Metrics related with the Learning Process", "text": "We tracked metrics related to the learning process itself, as a function of the vocabulary size to be embedded |V | and of the fraction of training data used (25%, 50%, 75% and 100%). For all possible configurations, we recorded the values of the training and validation loss (cross entropy) after each epoch. Tracking these metrics serves as a minimalistic sanity check: if the model is not able to solve the word prediction task with some degree of success (e.g. if we observe no substantial decay in the losses) then one should not expect the embeddings to capture any of the distributional information they are supposed to capture."}, {"heading": "4.3 Tests and Gold-Standard Data for Intrinsic Evaluation", "text": "Using the gold standard data (described below), we performed three types of tests:\n\u2013 Class Membership Tests: embeddings corresponding two member of the same semantic class (e.g. \u201cMonths of the Year\u201d, \u201cPortuguese Cities\u201d, \u201cSmileys\u201d) should be close, since they are supposed to be found in mostly the same contexts. \u2013 Class Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts. \u2013 Word Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. \u201cporque\u201d abbreviated by \u201cpq\u201d) and partial references (e.g. \u201cslb and benfica\u201d) should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning).\nTherefore, in our tests, two words are considered:\n\u2013 distinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80). \u2013 to belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80). \u2013 equivalent if the cosine of the embeddings is higher that 0.85 (or 0.95).\nWe report results using different thresholds of cosine similarity as we noticed that cosine similarity is skewed to higher values in the embedding space, as observed in related work [15, 16]. We used the following sources of data for testing Class Membership:\n\u2013 AP+Battig data. This data was collected from the evaluation data provided by [11]. These correspond to 29 semantic classes. \u2013 Twitter-Class - collected manually by the authors by checking top most frequent words in the dictionary and then expanding the classes. These include the following 6 sets (number of elements in brackets): smileys (13), months (12), countries (6), names (19), surnames (14) Portuguese cities (9).\nFor the Class Distinction test, we pair each element of each of the gold standard classes, with all the other elements from other classes (removing duplicate pairs since ordering does not matter), and we generate pairs of words which are supposed belong to different classes. For Word Equivalence test, we manually collected equivalente pairs, focusing on abbreviations that are popular in Twitters (e.g. \u201cqt\u201d ' \u201cquanto\u201d or \u201clx\u201d ' \u201clisboa\u201d and on frequent acronyms (e.g. \u201cslb\u201d ' \u201cbenfica\u201d). In total, we compiled 48 equivalence pairs.\nFor all these tests we computed a coverage metric. Our embeddings do not necessarily contain information for all the words contained in each of these tests. So, for all tests, we compute a coverage metric that measures the fraction of the gold-standard pairs that could actually be tested using the different embeddings produced. Then, for all the test pairs actually covered, we obtain the success metrics for each of the 3 tests by computing the ratio of pairs we were able to correctly classified as i) being distinct (cosine < 0.7 or 0.8), ii) belonging to the same class (cosine > 0.7 or 0.8), and iii) being equivalent (cosine > 0.85 or 0.95).\nIt is worth making a final comment about the gold standard data. Although we do not expect this gold standard data to be sufficient for a wide-spectrum evaluation of the resulting embeddings, it should be enough for providing us clues regarding areas where the embedding process is capturing enough semantics, and where it is not. These should still provide valuable indications for planning how to produce the much larger database of word embeddings."}, {"heading": "5 Results and Analysis", "text": "We run the training process and performed the corresponding evaluation for 12 combinations of size of vocabulary to be embedded, and the volume of training data available that has been used. Table 2 presents some overall statistics after training for 40 epochs.\nThe average time per epoch increases first with the size of the vocabulary to embed |V | (because the model will have more parameters), and then, for each |V |, with the volume of training data. Using our testbed (Section 4), the total time of learning in our experiments varied from a minimum of 160 seconds, with |V | = 2048 and 25% of data, to a maximum of 22.5 hours, with |V | = 32768 and using 100% of the training data available (extracted from 5M tweets). These numbers give us an approximate figure of how time consuming it would be to train embeddings from the complete Twitter corpus we have, consisting of 300M tweets.\nWe now analyze the learning process itself. We plot the training set loss and validation set loss for the different values of |V | (Figure 5 left) with 40 epochs and using all the available data. As expected, the loss is reducing after each epoch, with validation loss, although being slightly higher, following the same\ntrend. When using 100% we see no model overfitting. We can also observe that the higher is |V | the higher are the absolute values of the loss sets. This is not surprising because as the number of words to predict becomes higher the problem will tend to become harder. Also, because we keep the dimensionality of the embedding space constant (64 dimensions), it becomes increasingly hard to represent and differentiate larger vocabularies in the same hyper-volume. We believe this is a specially valuable indication for future experiments and for deciding the dimensionality of the final embeddings to distribute.\nOn the right side of Figure 5 we show how the number of training (and validation) examples affects the loss. For a fixed |V | = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure 5). This suggests that for the future we should not try any drastic reduction of the training data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs. We observed no phase-transition effects (the model seems simple enough for not showing that type of behavior). This indicates we have a practical way of safely deciding when to stop training the model."}, {"heading": "5.1 Intrinsic Evaluation", "text": "Table 3 presents results for the three different tests described in Section 4. The first (expected) result is that the coverage metrics increase with the size of the vocabulary being embedded, i.e., |V |. Because the Word Equivalence test set was specifically created for evaluating Twitter-based embedding, when embedding |V | = 32768 words we achieve almost 90% test coverage. On the other hand, for the Class Distinction test set - which was created by doing the cross product of the test cases of each class in Class Membership test set - we obtain very low coverage figures. This indicates that it is not always possible to re-use previously compiled gold-standard data, and that it will be important to compile goldstandard data directly from Twitter content if we want to perform a more precise evaluation.\nThe effect of varying the cosine similarity decision threshold from 0.70 to 0.80 for Class Membership test shows that the percentage of classified as correct test cases drops significantly. However, the drop is more accentuated when training with only a portion of the available data. The differences of using two alternative thresholds values is even higher in the Word Equivalence test.\nThe Word Equivalence test, in which we consider two words equivalent word if the cosine of the embedding vectors is higher than 0.95, revealed to be an extremely demanding test. Nevertheless, for |V | = 32768 the results are far superior, and for a much larger coverage, than for lower |V |. The same happens with the Class Membership test.\nOn the other hand, the Class Distinction test shows a different trend for larger values of |V | = 32768 but the coverage for other values of |V | is so low that becomes difficult to hypothesize about the reduced values of True Negatives (TN) percentage obtained for the largest |V |. It would be necessary to confirm this behavior with even larger values of |V |. One might hypothesize that the ability to distinguish between classes requires larger thresholds when |V | is large. Also, we can speculate about the need of increasing the number of dimensions to be able to encapsulate different semantic information for so many words."}, {"heading": "5.2 Further Analysis regarding Evaluation Metrics", "text": "Despite already providing interesting practical clues for our goal of trying to embed a larger vocabulary using more of the training data we have available, these results also revealed that the intrinsic evaluation metrics we are using are overly sensitive to their corresponding cosine similarity thresholds. This sensitivity poses serious challenges for further systematic exploration of word embedding architectures and their corresponding hyper-parameters, which was also observed in other recent works [16].\nBy using these absolute thresholds as criteria for deciding similarity of words, we create a dependency between the evaluation metrics and the geometry of the embedded data. If we see the embedding data as a graph, this means that metrics will change if we apply scaling operations to certain parts of the graph, even if its structure (i.e. relative position of the embedded words) does not change.\nFor most practical purposes (including training downstream ML models) absolute distances have little meaning. What is fundamental is that the resulting embeddings are able to capture topological information: similar words should be closer to each other than they are to words that are dissimilar to them (under\nthe various criteria of similarity we care about), independently of the absolute distances involved.\nIt is now clear that a key aspect for future work will be developing additional performance metrics based on topological properties. We are in line with recent work [17], proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine. Future work will necessarily include developing this type of metrics."}, {"heading": "6 Conclusions", "text": "Producing word embeddings from tweets is challenging due to the specificities of the vocabulary in the medium. We implemented a neural word embedding model that embeds words based on n-gram information extracted from a sample of the Portuguese Twitter stream, and which can be seen as a flexible baseline for further experiments in the field. Work reported in this paper is a preliminary study of trying to find parameters for training word embeddings from Twitter and adequate evaluation tests and gold-standard data.\nResults show that using less than 50% of the available training examples for each vocabulary size might result in overfitting. The resulting embeddings obtain an interesting performance on intrinsic evaluation tests when trained a vocabulary containing the 32768 most frequent words in a Twitter sample of relatively small size. Nevertheless, results exhibit a skewness in the cosine similarity scores that should be further explored in future work. More specifically, the Class Distinction test set revealed to be challenging and opens the door to evaluation of not only similarity between words but also dissimilarities between words of different semantic classes without using absolute score values.\nTherefore, a key area of future exploration has to do with better evaluation resources and metrics. We made some initial effort in this front. However, we believe that developing new intrinsic tests, agnostic to absolute values of metrics and concerned with topological aspects of the embedding space, and expanding gold-standard data with cases tailored for user-generated content, is of fundamental importance for the progress of this line of work.\nFurthermore, we plan to make public available word embeddings trained from a large sample of 300M tweets collected from the Portuguese Twitter stream. This will require experimenting producing embeddings with higher dimensionality (to avoid the cosine skewness effect) and training with even larger vocabularies. Also, there is room for experimenting with some of the hyper-parameters of the model itself (e.g. activation functions, dimensions of the layers), which we know have impact on final results.\nAcknowledgements We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research."}], "references": [{"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of machine learning research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Hlt-naacl,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Randwalk: A latent variable model approach to word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1502.03520,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Semeval-2016 task 4: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Fabrizio Sebastiani", "Veselin Stoyanov"], "venue": "Proceedings of SemEval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Lx-dsemvectors: Distributional semantics models for portuguese", "author": ["Jo\u00e3o Rodrigues", "Ant\u00f3nio Branco", "Steven Neale", "Jo\u00e3o Silva"], "venue": "In International Conference on Computational Processing of the Portuguese Language,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Twitterecho: a distributed focused crawler to support open research with twitter data", "author": ["Matko Bo\u0161njak", "Eduardo Oliveira", "Jos\u00e9 Martins", "Eduarda Mendes Rodrigues", "L\u00fa\u0131s Sarmento"], "venue": "In Proceedings of the 21st International Conference on World Wide Web,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Improving zero-shot learning by mitigating the hubness problem", "author": ["Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni"], "venue": "arXiv preprint arXiv:1412.6568,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Problems with evaluation of word embeddings using word similarity tasks", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer"], "venue": "ACL 2016,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Intrinsic evaluations of word embeddings: What can we do better", "author": ["Anna Gladkova", "Aleksandr Drozd", "Computing Center"], "venue": "ACL 2016,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Not only the space of possibilities for each of these aspects is large, there are also challenges in performing a consistent large-scale evaluation of the resulting embeddings [1].", "startOffset": 176, "endOffset": 179}, {"referenceID": 1, "context": "One can build models that explicitly aim at generating word embeddings, such as Word2Vec or GloVe [2,3], or one can extract such embeddings as by-products of more general models, which implicitly compute such word embeddings in the process of solving other language tasks.", "startOffset": 98, "endOffset": 103}, {"referenceID": 2, "context": "One can build models that explicitly aim at generating word embeddings, such as Word2Vec or GloVe [2,3], or one can extract such embeddings as by-products of more general models, which implicitly compute such word embeddings in the process of solving other language tasks.", "startOffset": 98, "endOffset": 103}, {"referenceID": 3, "context": "One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 2, "context": "One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "The other approach consists in extracting internal representations from neural network models of text [2,5, 6].", "startOffset": 102, "endOffset": 110}, {"referenceID": 4, "context": "The other approach consists in extracting internal representations from neural network models of text [2,5, 6].", "startOffset": 102, "endOffset": 110}, {"referenceID": 5, "context": "The other approach consists in extracting internal representations from neural network models of text [2,5, 6].", "startOffset": 102, "endOffset": 110}, {"referenceID": 6, "context": "Levy and Goldberg [7] showed that the two approaches are closely related.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Although, word embeddings research go back several decades, it was the recent developments of Deep Learning and the word2vec framework [2] that captured the attention of the NLP community.", "startOffset": 135, "endOffset": 138}, {"referenceID": 7, "context": "[8] showed that embeddings trained using word2vec models (CBOW and Skip-gram) exhibit linear structure, allowing analogy questions of the form \u201cman:woman::king:??.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "For instance, in the paper describing GloVe [3] authors trained their model on five corpora of different sizes and built a vocabulary of 400K most frequent words.", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "[8] trained with 82K vocabulary while Mikolov et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] was trained with 3M vocabulary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed a generative model for learning embeddings that tries to explain some theoretical justification for nonlinear models (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "SemEval 2016-Task 4: Sentiment Analysis in Twitter organizers report that participants either used general purpose pre-trained word embeddings, or trained from Tweet 2016 dataset or \u201cfrom some sort of dataset\u201d [10].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "[11] created and distributed the first general purpose embeddings for Portuguese.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The neural word embedding model we use in our experiments is heavily inspired in the one described in [5], but ours is one layer deeper and is set to solve a slightly different word prediction task.", "startOffset": 102, "endOffset": 105}, {"referenceID": 11, "context": "The model was implemented using the Syntagma library which relies on Keras [12] for model development, and we train the model using the built-in ADAM [13] optimizer with the default parameters.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "We randomly sampled 5M tweets from a corpus of 300M tweets collected from the Portuguese Twitter community [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "We report results using different thresholds of cosine similarity as we noticed that cosine similarity is skewed to higher values in the embedding space, as observed in related work [15, 16].", "startOffset": 182, "endOffset": 190}, {"referenceID": 14, "context": "We report results using different thresholds of cosine similarity as we noticed that cosine similarity is skewed to higher values in the embedding space, as observed in related work [15, 16].", "startOffset": 182, "endOffset": 190}, {"referenceID": 10, "context": "This data was collected from the evaluation data provided by [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "This sensitivity poses serious challenges for further systematic exploration of word embedding architectures and their corresponding hyper-parameters, which was also observed in other recent works [16].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "We are in line with recent work [17], proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores.", "startOffset": 32, "endOffset": 36}], "year": 2017, "abstractText": "This paper describes a preliminary study for producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream. We start by experimenting with a relatively small sample and focusing on three challenges: volume of training data, vocabulary size and intrinsic evaluation metrics. Using a single GPU, we were able to scale up vocabulary size from 2048 words embedded and 500K training examples to 32768 words over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch. We also observed that using less than 50% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics suffer from oversensitivity to their corresponding cosine similarity thresholds, indicating that a wider range of metrics need to be developed to track progress.", "creator": "LaTeX with hyperref package"}}}