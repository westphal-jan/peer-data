{"id": "1709.01188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Storytelling Agents with Personality and Adaptivity", "abstract": "we explore the expression of personality capacities and adaptivity through the gestures of familiar virtual agents in a diverse storytelling task. we conduct two experiments using four different dialogic statistical stories. we manipulate agent personality on the extraversion scale, whether the agents adapt to one another in their gestural performance and agent gender. our results show that more subjects are more able to perceive the intended variation in extraversion between different virtual agents, independently of deciding the story they are specifically telling and the gender of the agent. a second study shows responses that subjects also prefer adaptive to emotionally nonadaptive virtual agents.", "histories": [["v1", "Mon, 4 Sep 2017 23:06:05 GMT  (1857kb,D)", "http://arxiv.org/abs/1709.01188v1", "Related dataset:this https URL"]], "COMMENTS": "Related dataset:this https URL", "reviews": [], "SUBJECTS": "cs.HC cs.CL", "authors": ["zhichao hu", "marilyn a walker", "michael neff", "jean e fox tree"], "accepted": false, "id": "1709.01188"}, "pdf": {"name": "1709.01188.pdf", "metadata": {"source": "CRF", "title": "Storytelling Agents with Personality and Adaptivity", "authors": ["A. Walker", "Zhichao Hu", "Marilyn A. Walker", "Michael Neff"], "emails": ["zhu@ucsc.edu", "mawalker@ucsc.edu", "foxtree@ucsc.edu", "mpneff@ucdavis.edu"], "sections": [{"heading": null, "text": "Keywords: personality, gesture generation and variation, gestural adaptation, story telling, collaborative story telling"}, {"heading": "1 Introduction", "text": "It is a truism that every person is a unique individual. However, when interacting with or observing others, people make inferences that generalize from specific, observed behaviors to explanations for those behaviors in terms of dispositional traits [1]. One theory that attempts to account for such inferences is the Big Five theory of personality, which posits that consistent patterns in the way individuals behave, feel, and think across different situations, can be described in terms of trait adjectives, such as sociable, shy, trustworthy, disorganized or imaginative [2,3].\nPrevious work suggests both that personality traits are real, and that they are useful as a basis for models for Intelligent Virtual Agents (IVAs) for a range of applications [4,5,6,7,8]. Many findings about how people perceive other humans appear to carry over to their perceptions of IVAs [9,10,11,12,13]. Research suggests that human users are more engaged and thus learn more when interacting with characters endowed with personality and emotions, and that a character\u2019s personality, surprisingly, affects users\u2019 perceptions of the system\u2019s competence [14,15]. Recent experiments show that the Big Five theory is a useful basis for multi-modal integration of nonverbal and linguistic behavior, and that automatically generated variations in personality are perceived as intended [16,17,18,19].\nHowever, personality is not expressed in a void. Conversants dynamically adapt to their conversational partner, both in conversation and when telling stories, and using both verbal and nonverbal features [20,21,22,23,24], inter alia. There is also evidence that people prefer IVAs that align with human behavior, such as by mimicking head movements [22] or speech style [11]. A human\u2019s attraction to an IVA is increased when the IVA adapts its personality to the human over time rather than maintaining a consistently similar personality [11]. Inspired by previous work, this paper:\n\u2013 Introduces a novel task of two IVAs co-telling a story;\nar X\niv :1\n70 9.\n01 18\n8v 1\n[ cs\n.H C\n] 4\nS ep\n2 01\n7\n\u2013 Varies IVA personality through gestural parameters of gesture rate, speed, expanse and form. \u2013 Varies whether the IVAs adapt to one another\u2019s gestures in gesture rate, speed, expanse and form and use of specific gestures. \u2013 Tests the effect of, and interaction between, these variations with human perceptual experiments and report our results. Protest Story\nA1: Hey, do you remember that day? It was a work day, I remember there was some big event going on. B1: Yeah, that day was the start of the G20 summit. It\u2019s an event that happens every year. A2: Oh yeah, right, it\u2019s that meeting where 20 of the leaders of the world come together. They talk about how to run their governments effectively. B2: Yeah, exactly. There were many leaders coming together. They had some pretty different ideas about what\u2019s the best way to run a government. A3: And the people who follow the governments also have different ideas. Whenever world leaders meet, there will be protesters expressing different opinions. I remember the protest that happened just along the street where we work. B3: It looked peaceful at the beginning.... A4: Right, until a bunch of people started rebelling and creating\na riot. B4: Oh my gosh, it was such a riot, police cars were burned, and\nthings were thrown at cops. A5: Police were in full riot gear to stop the violence. B5: Yeah, they were. When things got worse, the protesters\nsmashed the windows of stores. A6: Uh huh. And then police fired tear gas and bean bag bullets. B6: That\u2019s right, tear gas and bean bag bullets... It all happened\nright in front of our store. A7: That\u2019s so scary. B7: It was kind of scary, but I had never seen a riot before, so it\nwas kind of interesting for me.\nFig. 1. Protest Dialogue, with fixed level of linguistic adaptation.\nOur stories come from weblogs of personal narratives [25] whose content has been regenerated as dialogues to support story co-telling. Example dialogs from the four we use in our experiments are in Fig. 1 and Fig. 5 in Sec. 3. These dialogs have a fixed linguistic representation and use oral language, discourse markers, shorter sentences, and repetitions and confirmations between speakers, as well as techniques to make the story sound like the two speakers experienced the event together. Our aim is to mimic the finding that storytelling in the wild is naturally conversational [24], and that the style of oral storytelling among friends varies depending on their personalities [24].\nWe carry out two experiments. In the personality experiment, we elicit\nsubjects\u2019 perceptions of two virtual agents designed to have different personalities. In the gestural adaptation experiment, we ask whether subjects prefer adaptive vs. non adaptive agents. Our results show that agents intended to be extroverted or introverted are perceived as such, and that subjects prefer adaptive stories. Sec. 2 describes our story dialog corpus. Sec. 3 and 4 presents our experimental design and results. In order to compare more concisely with our work, we delay discussion of related work until Sec. 5, where we discuss our results and describe future work."}, {"heading": "2 Story Dialog Corpus", "text": "We first annotate dialogs with a general underspecified gesture representation, then we prepare several versions of each dialog by varying experimental parameters such as agent extraversion and adaptivity.\nGesture Annotation We build on Neff et al.\u2019s work on the impact of extraversion on gesture in IVAs [17], as shown in Table 1, and select parameters to depict both introverted and extraverted IVAs by varying gesture amplitude, direction, rate and speed. We test user perceptions of IVA personality during story co-telling without adaptation, and then test whether we can achieve effects on personality perception when IVAs adapt to one another.\nWe construct the dialogs for the two IVAs manually from the monolog webblogs. We then generate audio for the utterances of each IVA using the AT&T Text to Speech engine (female voice Crystal and male voice Mike). We annotate the dialogs with a general, underspecified gestural representation, specifying both potential gestures and gesture placements. This representation allows us to procedurally generate hundreds of possible combinations of story co-tellings varying both gestural performance (personality) and adaptation. Annotators can insert a gesture when the dialog introduces new concepts, and add gesture adaptation (mimicry) when there are repetitions or confirmations in the dialog. The decisions of where to insert a gesture and which gesture to insert are mainly subjective. We use gestures from a database of 271 motion cap-\ntured gestures, including metaphoric, iconic, deictic and beat gestures. Fig. 2 illustrates how every gesture can be generated to include up to 4 phases [26]:\n\u2013 prep: move arms from default resting position or the end point of the last gesture to the start position of the stroke \u2013 stroke: perform the movement that conveys most of the gesture\u2019s meaning \u2013 hold: remain at the final position in the stroke \u2013 retract: move arms from the previous position to a default resting position\nFig. 3 shows the first 5 turns of the protest story annotated with gestures. The timing information of the gestures comes from the TTS audio timeline. Each gesture annotation\ncontains information in the following format: [gesture stroke begin time](gesture name, hand use, stroke duration). For example, in the first gesture \u201c[1.90s](Cup, RH 0.46s)\u201d, gesture stroke begins at 1.9 seconds of the dialog audio, it is a Cup gesture, uses the right hand, and the gesture stroke lasts 0.46 seconds. Research has shown that people prefer gestures occurring earlier than the accompanying speech [27]. Thus in this annotation, a gesture stroke is positioned 0.2 seconds before the beginning of the gesture\u2019s following word. For example, the first word after gesture \u201cCup\u201d is \u201cHey\u201d, it begins at 2.1 seconds, then the stroke of gesture \u201cCup\u201d begins at 1.9 seconds.\nOur gesture annotation does not specify features associated with particular gestures (i.e. gesture amplitude, direction and speed). But these features can be easily adjusted in our animation software, which can vary the amplitude, direction and speed. The default gesture annotation frequency is designed for extraverts, with a gesture rate of 1 - 2 gestures per sentence. For an introverted agent, a lower gesture rate is achieved by removing some of the gestures. In this way, both speakers\u2019 gestural performance can vary from introverted to extraverted using the whole scale of parameter values for every parameter.\nIn addition, we can also vary gestural adaptation in the annotation. In extravert & extravert gestural adaptation (based on the model described in [28]), two extraverts move together towards a more extraverted personality. Gesture rate is increased by adding extra gestures (marked with an asterisk \u201c*\u201d). Specific gestures are copied as part of adaptation, especially when the co-telling involves repetition and confirmation. Gestures in bold indicate copying of gesture form (adaptation), gestures after the slash \u201c/\u201d are non-adapted. Combined with personality variations for gestures described in the previous paragraph, it is possible to produce combinations of two agents with any level of extraversion engaged in a conversation with or without gestural adaptation.\nStimulus Construction We currently have 50 annotated story dialogs. In this experiment, we use four stories with different subject matter: protest, pet, storm and gardening, as illustrated in Fig. 1 and Fig. 5. Fig. 4 shows a screenshot of the stimuli. We use our own animation software to generate the stimuli based on the specified gesture script. This software uses motion captured data for the wrist path, hand shape and hand orienta-\ntion for each gesture stroke, motion captured data for body movement, and spline based interpolation for preparation and retractions. It also uses simplified physical simulation to add nuance to the motion. A gesture contains up to 4 phases: prep, stroke, hold and retract: we insert a hold and connecting prep between two strokes if they are less than 2.5 seconds away from each other. Otherwise, we insert a retraction.\nThe animation software takes as input scripts specifying gesture sequences, along with modifying edits (specifying features such as gesture amplitude, direction and speed), and produces an animation meeting the constraints as output. This is exported as a bvh file, that is then imported into Maya for rendering on the final model. In the video, two IVAs stand almost face-to-face, but each has an 55\u25e6 angle \u201ccheat\u201d towards the audience, as is commonly used in stage performances. We also add background body\nmovements [29] and head rotation movements for both agents. Both of these are kept constant for each stimuli pair.\n3 Experiment Method Pet Story\nA1: I have always felt like I was a dog person but our two cats are great. They are much more low maintenance than dogs are. B1: Yeah, I\u2019m really glad we got our first one at a no-kill shelter. A2: I had wanted a little kitty, but the only baby kitten they had\nscratched the crap out of me the minute I picked it up so that was a big \u201cNO\u201d. B2: Well, the no-kill shelter also had what they called \u201cteenagers\u201d, which were cats around four to six months old...a bit bigger than the little kitties. A3: Oh yeah, I saw those \u201cteenagers\u201d. They weren\u2019t exactly adults, but they were a bit bigger than the little kittens. B3: Yeah one of them really stood out to me then\u2013 mostly because she jumped up on a shelf behind us and smacked me in the head with her paw. A4: Yeah, we definitely had a winner! B4: I had no idea how much personality a cat can have. Our first\nkitty loves playing. She will play until she is out of breath. A5: Yeah, and then after playing for a long time she likes to look\nat you like she\u2019s saying, \u201cJust give me a minute, I\u2019ll get my breath back and be good to go.\u201d B5: Sometimes I wish I had that much enthusiasm for anything in my life. A6: Yeah, me too. Man, she has so much enthusiasm for chasing string too! To her it\u2019s the best thing ever. Well ok, maybe it runs a close second to hair scrunchies. B6: Oh I love playing fetch with her with hair scrunchies! A7: Yeah, you can just throw the scrunchies down the stairs and\nshe runs at top speed to fetch them. And she always does this until she\u2019s out of breath! B7: If only I could work out that hard before I was out of breath... I\u2019d probably be thinner.\nFig. 5. Pet Dialogue, with fixed level of linguistic adaptation.\nWe conduct two separate experiments, one on personality variation during cotelling a story, and the second using the same personalities but with and without adaptation.\nExperiment 1: Personality Variation. We prepared two versions of the video of the story co-telling for each of the four stories, one where the female is extraverted (higher values for gesture rate, gesture expanse, height, outwardness, speed and scale) and the male is introverted (lower values for those gesture features) and one where only the genders (virtual agent model and voice) of the agents are switched. The dialogue scripts and corresponding gesture forms do not vary from one co-telling to another. This results in 8 video stimuli for four stories.\nWe conducted a between-subjects experiment on Mechanical Turk where we first ask Turkers to answer the TIPI [30] personality survey for themselves, and then answer it for only one of the agents in the video, after watching the video as many times as they like. Thus for each video stimulus, there are two surveys. We ran our 16 surveys as 16 HITs (Human Intelligence Tasks) on Mechanical Turk, requesting 20 subjects per HIT (each worker can only do one of the tasks), which results in 320 judgements. The average completion time for the 8 HITs on Mechanical Turk was 5 minutes 15 seconds. The average stimulus length was 1 minute 32 seconds. Since the survey is hosted outside Mechanical Turk, sometimes we get more than 20 subjects for each HIT.\nExperiment 2: Gestural Adaptation. For the adaptive experiment, both agents are designed to be extroverted. We chose to use two extraverted agents because we have foundations from previous work showing the adaptation model between two extraverted speakers [28] (where both agents become more extraverted). We use only a part of each story for one experimental task. The stimuli for one task has two variations: adapted and non-adapted. Both stimuli use the same audio, contain 2 to 4 dialog turns with the same gestures as an introduction to the story (which we refer to as context), and the next (and last) dialog turn with gesture adaptation or without gesture adaptation (which we refer to as response). Adaptation only begins to occur in the last dialog turn. In this way, subjects can get to know the story through the context, and compare the responses to decide whether they like the adapted or non-adapted version.\n\u2013 Non-adapted: In the last dialog turn, the extraverted agent maintains his or her gesture rate (1 - 2 gestures per sentence), expanse, height, outwardness, speed and scale. There is no copying of specific gestures. \u2013 Adapted: In the last dialog turn, the extraverted agent increases the gesture rate (1 - 3 gestures per sentence), expanse (18 cm further from center), height (10 cm higher), outwardness (10 cm more outward), speed (1.25 times faster) and scale (1.5 times larger). Fig. 6 shows the same gesture with different expanses and heights. In the adapted version, specific gestures are copied (e.g. gestures in bold font in Fig.3).\nThus every story has two versions. One version ends with the female agent\u2019s response, another ends with the male agent\u2019s response. For example, Garden ABA has three turns, ending with the female agent adapting to the male, and\nGarden ABAB has four turns, ending with the male agent adapting to the female. Every version consists of two conditions (adapted and non-adapted versions) and a short survey. The order of the two conditions is random for every participant. But there is a letter mark assigned to every video for easy reference (see Fig. 4).\nSubjects are asked to watch the two stimuli first, and then finish the survey. Subjects are told that the audio of the two videos is the same, but only the last few gestures of the female/male agent are different. Subjects are also advised to watch the video as many times as they want. The survey has two questions: (1) Which video is a better story co-telling based on the gestures? (2) Please explain the reason behind your choice to the previous question (which we refer to as the \u201cwhy\u201d question). Our primary aim is to determine whether people perceive the adaptation and whether it makes a better story.\nWe ran our 8 tasks for 4 stories as 8 HITs on Mechanical Turk, requesting 25 subjects per task. The average completion time for the 8 tasks on Mechanical Turk was 2 minutes 53 seconds. The average stimulus length was 35.3 seconds. This means that, on average, a subject spent 1 minute 43 seconds answering the questions. We removed subjects who failed to state their reasons of preference in the \u201dwhy\u201d question."}, {"heading": "4 Experimental Results", "text": "4.1 Personality Results Story IntroAgent ExtraAgent\nGarden 4.2 5.4 Pet 4.7 5.0 Protest 4.2 5.3 Storm 3.7 5.7\nTable 2. Experiment results: participant evaluated extraversion scores (range from 1 - 7, with 1 being the most introverted and 7 being the most extraverted).\nWe conducted a three-way ANOVA with agent intended personality, agent gender and story as independent variables and perceived agent personality as the dependent variable. See Table 2. The results show that subjects clearly perceive the intended extraverted or intended introverted personality of the two agents (F = 67.1, p < .001). There is no main effect for story (as intended in our design), but there is an interaction effect between story and intended personality, with the introverted agent in the storm story being seen as much more introverted\nthan in the other stories (F= 7.5, p < .001). There is no significant variation by agent gender (F = 2.3, p = .14).\nSince previous work suggests that personality is perceived for an agent along all Big Five dimensions whether it is designed to be manifest or not [16,31], we also conducted a two-way ANOVA by story and agent intended personality for the other 4 traits. There are no significant differences for Conscientiousness, or Openness. However Introverted agents are seen as more agreeable (p = .008) and more emotionally stable (p = .016). There were no significant differences by story except that both agents in the Storm story were seen as less open, presumably because the content of the story is about how scary the storm is.\n4.2 Adaption Results Story Version #A #NA %A %NA Garden ABA 11 9 55% 45% Garden ABAB 20 2 91% 9% Pet ABABA 10 13 43% 57% Pet ABABAB 19 5 79% 21% Protest ABAB 8 11 42% 58% Protest ABABA 11 11 50% 50% Storm ABABA 16 4 80% 20% Storm ABABAB 14 5 74% 26% Total 109 60 64% 36%\nTable 3. Experiment results: number and percentage of subjects who preferred the adapted (A) stimulus and the non-adapted (NA) stimulus. The letters in the story version refer to dialog turns by speaker A or B. For example, ABA means A takes dialog turns 1 and 3 in the stimuli, while B takes dialog turn 2.\nThe results in Table 3 show that across all the videos, the mean percentage of people who preferred the adapted version was 64% (19% standard deviation), which is marginally better than a predicted preference of 50%, t(7) = 2.15, p = .07. Analysis of participants\u2019 descriptions of why they preferred one video over another shows 4 distinct categories of reasons of why people made their choices (see Table 4).\nSubjects who preferred the adapted versions said that the gestures fit the dialog better (\u201cadapted good gestures\u201d in\nTable 4): the subjects stated that the adapted versions had gestures that \u201cflowed better with the words\u201d, were \u201cmore natural\u201d, \u201cmore appropriate to what he said\u201d, and \u201crelevant to the dialog\u201d, and that they \u201ccould imagine a friend making various hand gestures similar\u201d to the ones in the story. Another reason was that gestures were \u201cmore animated\u201d (\u201cadapted animated\u201d): the adapted version had \u201cmore hand gestures\u201d, and the agent \u201cused his arms more\u201d, \u201cgestured more\u201d, and \u201cwas much more alive\u201d. In contrast, in the nonadapted version, the agent \u201cseemed very bored\u201d and \u201cwanted to end the conversation\u201d. This indicates that the subjects preferred agents with a higher gesture rate. Ten subjects commented on the expanse, height, scale and speed of the gestures: they chose the adapted version because the agent \u201cgestured higher in the air\u201d, \u201cmaking wider, grander gestures\u201d that were \u201cmore expansive\u201d and \u201cbigger\u201d. And in the non-adapted version, the gestures were \u201ctoo slow\u201d. However, there was no comment about the copying of gestures, possibly because copying was less obvious when the expanse and height of the gestures changed in the adapted version.\nAmong those who preferred the non-adapted versions of the stories, one reason was that the gestures fit the dialog better (\u201cnon-adapted good gestures\u201d in Table 4): the subjects stated that the gestures in the non-adapted version \u201cwent a lot better with what she was saying\u201d and were \u201cmore appropriate\u201d. Another reason is that the gestures were \u201cmore realistic\u201d (\u201cnon-adapted realistic\u201d) : subjects didn\u2019t like the gestures being \u201ctoo animated\u201d, or \u201ctoo busy\u201d, nor did they like the agents \u201cshowing way too much\nemotions\u201d or \u201clooking like she is exercising\u201d. That is, too much animation can be seen as unrealistic.\nThe percentages of the subjects that had comments related to those 4 categories are in Table 4. In 7 out of 8 tasks, there were more subjects who preferred the adapted version because it was animated at the right level (e.g. animated enough, but not too animated). If we only consider the \u201canimated\u201d factor in deciding which is a better stimulus, 84% of the subjects preferred the adapted version."}, {"heading": "5 Discussion and Future Work", "text": "To our knowledge this is the first time that it has been shown that subjects perceive differences in agent personal-\nity during a storytelling task, and that adaptive gestural behavior during storytelling is positively perceived. We re-use natural personal narratives that are rendered dialogically, so that two IVAs co-tell the story.\nIt is obvious that being able to adapt is a key part of being more human-like. There are attempts to integrate language adaptation within natural language generation [32] and research has shown that human bystanders perceive linguistic adaptation positively [33]. However, this is the first experiment to demonstrate a positive effect for gestural adaptation.\nRecent work on gesture generation has focused largely on iconic gesture generation. For example, Bergmann and Kopp [34] present a model that allows virtual agents to automatically select the content and derive the form of coordinated language and iconic gestures. Luo et al. [29] also presents an effective algorithm for adding full body postural movement to animation sequences of arm gestures. More generally, current systems generally select gestures using either a text-to-gesture or concept-to-gesture mapping. Text-to-gesture systems, such as VHP [13], may have a limited number of gestures (only 7 in this case) and limited gesture placement options, but the alignment of speech content and gestures are more accurate. Concept-to-gesture systems such as PPP [35], AC and BEAT [36] defines general rules for gesture insertion based on linguistic components. For example, iconic gestures are triggered by words with spatial or concrete context (e.g. \u201ccheck\u201d). These kind of systems have more gestures, but the gesture placement largely depends on general rules derived from literature, thus the accuracy is not guaranteed. An alternative approach learns a personalized statistical model that predicts a gesture given the text to be spoken and a model that captures an individual\u2019s gesturing preferences [37]. None of these models adequately address the production of gesture for dialogues, where a process of co-adaptation will modulate both the type of gesture chosen and the specific form of that gesture (e.g. its size). This current work aims to provide a basis for developing such models.\nGratch investigates creating rapport with virtual agents using gesture adaptation mainly focused on head gestures and posture shifts (while ours focused on hand gestures), and used real human movements as control [38]. Our adaptation stimuli are more similar to Endrass et al. [12]. To investigate culture-related aspects of behavior for virtual characters, they chose prototypical body postures from corpora for German and Japanese cultural background, embodied those postures in a two-agent dialogs, and asked subjects from German and Japanese cultural background to evaluate the dialogs.\nIn future work, we aim to test the expression of personality and adaptivity with different personality combinations. Our ultimate goal is to automatically convert monologic blog stories to dialogs with both linguistic and gestural adaptation. Experimental exploration, such as undertaken here, is crucial for formulating models of gesture generation that correctly incorporate personality and adaptation."}], "references": [{"title": "The trait construct in lay and professional psychology", "author": ["R.E. Nisbett"], "venue": "Retrospections on social psychology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1980}, {"title": "Personality in its natural habitat: Manifestations and implicit folk theories of personality in daily life", "author": ["M.R. Mehl", "S.D. Gosling", "J.W. Pennebaker"], "venue": "Journal of Personality and Social Psychology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Toward an adequate taxonomy of personality attributes: Replicated factor structure in peer nomination personality rating", "author": ["W.T. Norman"], "venue": "Journal of Abnormal and Social Psychology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1963}, {"title": "The comforting presence of relational agents", "author": ["T. Bickmore", "D. Schulman"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Implementing expressive gesture synthesis for embodied conversational agents", "author": ["B. Hartmann", "M. Mancini", "C. Pelachaud"], "venue": "In Proc. Gesture Workshop 2005,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Synthesizing multimodal utterances for conversational agents", "author": ["S. Kopp", "I. Wachsmuth"], "venue": "Computer Animation and Virtual Worlds,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Smartbody: Behavior realization for embodied conversational agents", "author": ["M. Thiebaux", "A. Marshall", "S. Marsella", "M. Kallman"], "venue": "In Proc. of 7th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "EMBR\u2013A Realtime Animation Engine for Interactive Embodied Agents", "author": ["A. Heloir", "M. Kipp"], "venue": "In Intelligent Virtual Agents", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Integrating models of personality and emotions into lifelike characters", "author": ["E. Andr\u00e9", "M. Klesen", "P. Gebhard", "S. Allen", "T. Rist"], "venue": "In Proc. of the Workshop on Affect in Interactions - Towards a new Generation of Interfaces,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Embodied conversational agents on a common ground. From brows to trust: evaluating embodied conversational agents, chapter 2, pages 27\u201366", "author": ["Z. Ruttkay", "C. Dormann", "H. Noot"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "How \u201creal\u201d are computer personalities?: Psychological responses to personality types in human-computer interaction", "author": ["Y. Moon", "C. Nass"], "venue": "Communication Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Investigating culture-related aspects of behavior for virtual characters", "author": ["B. Endra\u00df", "E. Andr\u00e9", "M. Rehm", "Y.I. Nakano"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Design of a virtual human presenter. Center for Human Modeling and Simulation, page", "author": ["T. Noma", "N.I. Badler", "L. Zhao"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "User robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy", "author": ["A. Tapus", "C. Tapus", "M.J. Mataric"], "venue": "Intelligent Service Robotics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "The politeness effect: Pedagogical agents and learning gains", "author": ["N. Wang", "W.L. Johnson", "R.E. Mayer", "P. Rizzo", "E. Shaw", "H. Collins"], "venue": "Frontiers in AI and Applications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Controlling user perceptions of linguistic style: Trainable generation of personality traits", "author": ["F. Mairesse", "M.A. Walker"], "venue": "Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Evaluating the effect of gesture and language on personality perception in conversational agents", "author": ["M. Neff", "Y. Wang", "R. Abbott", "M. Walker"], "venue": "In IVA,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Bossy or wimpy: expressing social dominance by combining gaze and linguistic behaviors", "author": ["N. Bee", "C. Pollock", "E. Andr\u00e9", "M. Walker"], "venue": "In Intelligent Virtual Agents,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Dont scratch! selfadaptors reflect emotional stability", "author": ["M. Neff", "N. Toothman", "R. Bowmani", "J.E. Fox Tree", "Walker M. A"], "venue": "In IVA,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Listening in on monologues and dialogues", "author": ["J.E. Fox Tree"], "venue": "Discourse Processes,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Seeing and hearing double: The influence of mimicry in speech and gesture on observers", "author": ["F. Parrill", "I. Kimbara"], "venue": "Journal of Nonverbal Behavior,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Digital chameleons: Automatic assimilation of nonverbal gestures in immersive virtual environments", "author": ["J.N. Bailenson", "N. Yee"], "venue": "Psychological Science,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Addressee backchannels steer narrative development", "author": ["J. Tolins", "J.E. Fox Tree"], "venue": "Journal of Pragmatics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Channeling identity: A study of storytelling in conversations between introverted and extraverted friends", "author": ["A. Thorne", "N. Korobov", "E.M. Morgan"], "venue": "Journal of research in personality,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Identifying personal stories in millions of weblog entries", "author": ["A. Gordon", "R. Swanson"], "venue": "In 3rd Int. Conference on Weblogs and Social Media,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Movement phase in signs and co-speech gestures, and their transcriptions by human coders", "author": ["S. KIita", "I. Van Gijn", "H. Van Der Hulst"], "venue": "In Proc. of the Int. Gesture Workshop on Gesture and Sign Language in Human-Computer Interaction, Springer-Verlag,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "The influence of prosody on the requirements for gesture-text alignment", "author": ["Y. Wang", "M. Neff"], "venue": "In IVA,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Augmenting gesture animation with motion capture data to provide full-body engagement", "author": ["P. Luo", "M. Kipp", "M. Neff"], "venue": "In IVA,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "A very brief measure of the big five personality domains", "author": ["S.D. Gosling", "P.J. Rentfrow", "W.B. Swann"], "venue": "Journal of Research in Personality,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "Judging iva personality using an open-ended question", "author": ["K. Liu", "J. Tolins", "J.E. Fox Tree", "M. Walker", "M. Neff"], "venue": "In IVA,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "An alignment-capable microplanner for natural language generation", "author": ["H. Buschmeier", "K. Bergmann", "S. Kopp"], "venue": "In Proc. of the 12th European Workshop on Natural Language Generation,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Entrainment in pedestrian direction giving: How many kinds of entrainment", "author": ["Z. Hu", "G. Halberg", "C. Jimenez", "M. Walker"], "venue": "IWSDS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Increasing the expressiveness of virtual agents: autonomous generation of speech and gesture for spatial description tasks", "author": ["K. Bergmann", "S. Kopp"], "venue": "In Proc. of The 8th Int. Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Wip/ppp: Automatic generation of personalized multimedia presentations", "author": ["E. Andr\u00e9", "J. M\u00fcller", "T. Rist"], "venue": "In Proc. of the fourth ACM international conference on Multimedia,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1997}, {"title": "Beat: the behavior expression animation toolkit", "author": ["J. Cassell", "H.H. Vilhj\u00e1lmsson", "T. Bickmore"], "venue": "In Life-Like Characters,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "Gesture modeling and animation based on a probabilistic re-creation of speaker style", "author": ["M. Neff", "M. Kipp", "I. Albrecht", "H.P. Seidel"], "venue": "In ACM Transactions on Graphics (TOG). ACM,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Creating rapport with virtual agents", "author": ["J. Gratch", "N. Wang", "J. Gerten", "E. Fast", "R. Duffy"], "venue": "In IVA,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "However, when interacting with or observing others, people make inferences that generalize from specific, observed behaviors to explanations for those behaviors in terms of dispositional traits [1].", "startOffset": 194, "endOffset": 197}, {"referenceID": 1, "context": "One theory that attempts to account for such inferences is the Big Five theory of personality, which posits that consistent patterns in the way individuals behave, feel, and think across different situations, can be described in terms of trait adjectives, such as sociable, shy, trustworthy, disorganized or imaginative [2,3].", "startOffset": 320, "endOffset": 325}, {"referenceID": 2, "context": "One theory that attempts to account for such inferences is the Big Five theory of personality, which posits that consistent patterns in the way individuals behave, feel, and think across different situations, can be described in terms of trait adjectives, such as sociable, shy, trustworthy, disorganized or imaginative [2,3].", "startOffset": 320, "endOffset": 325}, {"referenceID": 3, "context": "Previous work suggests both that personality traits are real, and that they are useful as a basis for models for Intelligent Virtual Agents (IVAs) for a range of applications [4,5,6,7,8].", "startOffset": 175, "endOffset": 186}, {"referenceID": 4, "context": "Previous work suggests both that personality traits are real, and that they are useful as a basis for models for Intelligent Virtual Agents (IVAs) for a range of applications [4,5,6,7,8].", "startOffset": 175, "endOffset": 186}, {"referenceID": 5, "context": "Previous work suggests both that personality traits are real, and that they are useful as a basis for models for Intelligent Virtual Agents (IVAs) for a range of applications [4,5,6,7,8].", "startOffset": 175, "endOffset": 186}, {"referenceID": 6, "context": "Previous work suggests both that personality traits are real, and that they are useful as a basis for models for Intelligent Virtual Agents (IVAs) for a range of applications [4,5,6,7,8].", "startOffset": 175, "endOffset": 186}, {"referenceID": 7, "context": "Previous work suggests both that personality traits are real, and that they are useful as a basis for models for Intelligent Virtual Agents (IVAs) for a range of applications [4,5,6,7,8].", "startOffset": 175, "endOffset": 186}, {"referenceID": 8, "context": "Many findings about how people perceive other humans appear to carry over to their perceptions of IVAs [9,10,11,12,13].", "startOffset": 103, "endOffset": 118}, {"referenceID": 9, "context": "Many findings about how people perceive other humans appear to carry over to their perceptions of IVAs [9,10,11,12,13].", "startOffset": 103, "endOffset": 118}, {"referenceID": 10, "context": "Many findings about how people perceive other humans appear to carry over to their perceptions of IVAs [9,10,11,12,13].", "startOffset": 103, "endOffset": 118}, {"referenceID": 11, "context": "Many findings about how people perceive other humans appear to carry over to their perceptions of IVAs [9,10,11,12,13].", "startOffset": 103, "endOffset": 118}, {"referenceID": 12, "context": "Many findings about how people perceive other humans appear to carry over to their perceptions of IVAs [9,10,11,12,13].", "startOffset": 103, "endOffset": 118}, {"referenceID": 13, "context": "Research suggests that human users are more engaged and thus learn more when interacting with characters endowed with personality and emotions, and that a character\u2019s personality, surprisingly, affects users\u2019 perceptions of the system\u2019s competence [14,15].", "startOffset": 248, "endOffset": 255}, {"referenceID": 14, "context": "Research suggests that human users are more engaged and thus learn more when interacting with characters endowed with personality and emotions, and that a character\u2019s personality, surprisingly, affects users\u2019 perceptions of the system\u2019s competence [14,15].", "startOffset": 248, "endOffset": 255}, {"referenceID": 15, "context": "Recent experiments show that the Big Five theory is a useful basis for multi-modal integration of nonverbal and linguistic behavior, and that automatically generated variations in personality are perceived as intended [16,17,18,19].", "startOffset": 218, "endOffset": 231}, {"referenceID": 16, "context": "Recent experiments show that the Big Five theory is a useful basis for multi-modal integration of nonverbal and linguistic behavior, and that automatically generated variations in personality are perceived as intended [16,17,18,19].", "startOffset": 218, "endOffset": 231}, {"referenceID": 17, "context": "Recent experiments show that the Big Five theory is a useful basis for multi-modal integration of nonverbal and linguistic behavior, and that automatically generated variations in personality are perceived as intended [16,17,18,19].", "startOffset": 218, "endOffset": 231}, {"referenceID": 18, "context": "Recent experiments show that the Big Five theory is a useful basis for multi-modal integration of nonverbal and linguistic behavior, and that automatically generated variations in personality are perceived as intended [16,17,18,19].", "startOffset": 218, "endOffset": 231}, {"referenceID": 19, "context": "Conversants dynamically adapt to their conversational partner, both in conversation and when telling stories, and using both verbal and nonverbal features [20,21,22,23,24], inter alia.", "startOffset": 155, "endOffset": 171}, {"referenceID": 20, "context": "Conversants dynamically adapt to their conversational partner, both in conversation and when telling stories, and using both verbal and nonverbal features [20,21,22,23,24], inter alia.", "startOffset": 155, "endOffset": 171}, {"referenceID": 21, "context": "Conversants dynamically adapt to their conversational partner, both in conversation and when telling stories, and using both verbal and nonverbal features [20,21,22,23,24], inter alia.", "startOffset": 155, "endOffset": 171}, {"referenceID": 22, "context": "Conversants dynamically adapt to their conversational partner, both in conversation and when telling stories, and using both verbal and nonverbal features [20,21,22,23,24], inter alia.", "startOffset": 155, "endOffset": 171}, {"referenceID": 23, "context": "Conversants dynamically adapt to their conversational partner, both in conversation and when telling stories, and using both verbal and nonverbal features [20,21,22,23,24], inter alia.", "startOffset": 155, "endOffset": 171}, {"referenceID": 21, "context": "There is also evidence that people prefer IVAs that align with human behavior, such as by mimicking head movements [22] or speech style [11].", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "There is also evidence that people prefer IVAs that align with human behavior, such as by mimicking head movements [22] or speech style [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "A human\u2019s attraction to an IVA is increased when the IVA adapts its personality to the human over time rather than maintaining a consistently similar personality [11].", "startOffset": 162, "endOffset": 166}, {"referenceID": 24, "context": "Our stories come from weblogs of personal narratives [25] whose content has been regenerated as dialogues to support story co-telling.", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "Our aim is to mimic the finding that storytelling in the wild is naturally conversational [24], and that the style of oral storytelling among friends varies depending on their personalities [24].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Our aim is to mimic the finding that storytelling in the wild is naturally conversational [24], and that the style of oral storytelling among friends varies depending on their personalities [24].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "\u2019s work on the impact of extraversion on gesture in IVAs [17], as shown in Table 1, and select parameters to depict both introverted and extraverted IVAs by varying gesture amplitude, direction, rate and speed.", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "2 illustrates how every gesture can be generated to include up to 4 phases [26]:", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "Research has shown that people prefer gestures occurring earlier than the accompanying speech [27].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "movements [29] and head rotation movements for both agents.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "We conducted a between-subjects experiment on Mechanical Turk where we first ask Turkers to answer the TIPI [30] personality survey for themselves, and then answer it for only one of the agents in the video, after watching the video as many times as they like.", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "Since previous work suggests that personality is perceived for an agent along all Big Five dimensions whether it is designed to be manifest or not [16,31], we also conducted a two-way ANOVA by story and agent intended personality for the other 4 traits.", "startOffset": 147, "endOffset": 154}, {"referenceID": 29, "context": "Since previous work suggests that personality is perceived for an agent along all Big Five dimensions whether it is designed to be manifest or not [16,31], we also conducted a two-way ANOVA by story and agent intended personality for the other 4 traits.", "startOffset": 147, "endOffset": 154}, {"referenceID": 30, "context": "There are attempts to integrate language adaptation within natural language generation [32] and research has shown that human bystanders perceive linguistic adaptation positively [33].", "startOffset": 87, "endOffset": 91}, {"referenceID": 31, "context": "There are attempts to integrate language adaptation within natural language generation [32] and research has shown that human bystanders perceive linguistic adaptation positively [33].", "startOffset": 179, "endOffset": 183}, {"referenceID": 32, "context": "For example, Bergmann and Kopp [34] present a model that allows virtual agents to automatically select the content and derive the form of coordinated language and iconic gestures.", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "[29] also presents an effective algorithm for adding full body postural movement to animation sequences of arm gestures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Text-to-gesture systems, such as VHP [13], may have a limited number of gestures (only 7 in this case) and limited gesture placement options, but the alignment of speech content and gestures are more accurate.", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "Concept-to-gesture systems such as PPP [35], AC and BEAT [36] defines general rules for gesture insertion based on linguistic components.", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "Concept-to-gesture systems such as PPP [35], AC and BEAT [36] defines general rules for gesture insertion based on linguistic components.", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "An alternative approach learns a personalized statistical model that predicts a gesture given the text to be spoken and a model that captures an individual\u2019s gesturing preferences [37].", "startOffset": 180, "endOffset": 184}, {"referenceID": 36, "context": "Gratch investigates creating rapport with virtual agents using gesture adaptation mainly focused on head gestures and posture shifts (while ours focused on hand gestures), and used real human movements as control [38].", "startOffset": 213, "endOffset": 217}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We explore the expression of personality and adaptivity through the gestures of virtual agents in a storytelling task. We conduct two experiments using four different dialogic stories. We manipulate agent personality on the extraversion scale, whether the agents adapt to one another in their gestural performance and agent gender. Our results show that subjects are able to perceive the intended variation in extraversion between different virtual agents, independently of the story they are telling and the gender of the agent. A second study shows that subjects also prefer adaptive to nonadaptive virtual agents.", "creator": "LaTeX with hyperref package"}}}