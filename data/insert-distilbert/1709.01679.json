{"id": "1709.01679", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse", "abstract": "this study addresses the intuitive problem of identifying the explicit meaning of unknown unknown words or entities in a single discourse with respect to the word embedding approaches used in neural language models. we proposed a method alternative for on - the - fly construction, and its exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. this extends the model dynamic entity representation used in kobayashi et al. ( 2016 ) and formally incorporates a copy mechanism proposed independently examined by gu et al. ( 2016 ) \u2022 and gulcehre et al. ( 2016 ). in addition, we construct a potential new task and dataset called anonymized language modeling for evaluating them the ability to capture word meanings while reading. experiments conducted using our novel dataset show that the proposed variant of rnn language vocabulary model outperformed the baseline model. furthermore, the experiments also demonstrate suggests that certain dynamic updates of finding an output layer help a model predict reappearing entities, likewise whereas those of an input content layer are effective to predict words following reappearing entities.", "histories": [["v1", "Wed, 6 Sep 2017 05:23:37 GMT  (601kb)", "http://arxiv.org/abs/1709.01679v1", "11 pages. To appear in IJCNLP 2017"], ["v2", "Tue, 17 Oct 2017 12:35:33 GMT  (601kb)", "http://arxiv.org/abs/1709.01679v2", "11 pages. To appear in IJCNLP 2017"]], "COMMENTS": "11 pages. To appear in IJCNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sosuke kobayashi", "naoaki okazaki", "kentaro inui"], "accepted": false, "id": "1709.01679"}, "pdf": {"name": "1709.01679.pdf", "metadata": {"source": "CRF", "title": "A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse", "authors": ["Sosuke Kobayashi"], "emails": ["sosk@preferred.jp", "okazaki@c.titech.ac.jp", "inui@ecei.tohoku.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n01 67\n9v 1\n[ cs\n.C L\n] 6\nS ep\n2 01\n7"}, {"heading": "1 Introduction", "text": "Language models that use probability distributions over sequences of words are found in many natural language processing applications, including speech recognition, machine translation, text summarization, and dialogue utterance generation. Recent studies have demonstrated that language models trained using neural network (Bengio et al., 2003; Mikolov et al., 2010) such as recurrent neural network (RNN) (Jozefowicz et al., 2016) and\nconvolutional neural network (Dauphin et al., 2016) achieve the best performance across a range of corpora (Mikolov et al., 2010; Chelba et al., 2014; Merity et al., 2017; Grave et al., 2017). However, current neural language models have a major drawback: the language model works only when applied to a closed vocabulary of fixed size (usually comprising high-frequency words from the given training corpus). All occurrences of outof-vocabulary words are replaced with a single dummy token \u201c<unk>\u201d, showing that the word is unknown. For example, the word sequence, Pikotaro sings PPAP on YouTube is treated as <unk> sings <unk> on <unk> assuming that the words Pikotaro, PPAP, and YouTube are out of the vocabulary. The model therefore assumes that these words have the same meaning, which is clearly incorrect. The derivation of meanings of unknown words remains a persistent and nontrivial challenge when using word embeddings. In addition, existing language models further assume that the meaning of a word is the same\nd[4],2d[4],1\n... [ 1 ] killed [ 2 ] with bombs \u2026 ... police suspects [ 1 ] attacked ...\nd[3],1\nMerge Merge\nd\u2019[1],1 d\u2019[2],1\nd[2],1\nd[1],1d[1],0\nd[2],0\nd[4],0 d[3],0\nd[2],2\nd[1],2\nd[3],2\nMerge\nd\u2019[1],2\n= \u2212\u2212\u2212\u2192 RNN( = \u2190\u2212\u2212\u2212 RNN(\nFigure 2: Dynamic Neural Text Modeling: the meaning representation of each unknown word, denoted by a coreference index \u201c[ k ]\u201d, is inferred from the local contexts in which it occurs.\nand universal across different documents. Neural language models also make this assumption and represent all occurrences of a word with a single word vector across all documents. However, the assumption of a universal meaning is also unlikely correct. For example, the name John is likely to refer to different individuals in different documents. In one story, John may be a pianist while another John denoted in a second story may be an infant. A model that represents all occurrences of John with the same vector fails to capture the very different behavior expected from John as a pianist and John as an infant.\nIn this study, we address these issues and propose a novel neural language model that can build and dynamically change distributed representations of words based on the multi-sentential discourse. The idea of incorporating dynamic meaning representations into neural networks is not new. In the context of reading comprehension, Kobayashi et al. (2016) proposed a model that dynamically computes the representation of a named entity mention from the local context given by its prior occurrences in the text. In neural machine translation, the copy mechanism was proposed as a way of improving the handling of outof-vocabulary words (e.g., named entities) in a source sentence (Gu et al., 2016; Gulcehre et al., 2016). We use a variant of recurrent neural language model (RNLM), that combines dynamic representation and the copy mechanism. The resulting novel model, Dynamic Neural Text Model, uses the dynamic word embeddings that are constructed from the context in the output and input layers of an RNLM, as shown in Figures 1 and 2.\nThe contributions of this paper are three-fold. First, we propose a novel neural language model, which we named the Dynamic Neural Text Model. Second, we introduce a new evaluation task and dataset called Anonymized Language Modeling.\nThis dataset can be used to evaluate the ability of a language model to capture word meanings from contextual information (Figure 3). This task involves a kind of one-shot learning tasks, in which the meanings of entities are inferred from their limited prior occurrences. Third, our experimental results indicate that the proposed model outperforms baseline models that use only global and static word embeddings in the input and/or output layers of an RNLM. Dynamic updates of the output layer helps the RNLM predict reappearing entities, whereas those of the input layer are effective to predict words following reappearing entities. A more detailed analysis showed that the method was able to successfully capture the meanings of words across large contexts, and to accumulate multiple context information."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 RNN Language Model", "text": "Given a sequence of N tokens of a document D = (w1, w2, ..., wN ), an RNN language model computes the probability p(D) =\u220fN\nt=1 p(wt|w1, ..., wt\u22121). The computation of each factorized probability p(wt|w1, ..., wt\u22121) can also be viewed as the task of predicting a following word wt from the preceding words (w1, ..., wt\u22121). Typically, RNNs recurrently compute the probability of the following word wt by using a hidden state ht\u22121 at time step t\u2212 1,\np(wt|w1, ..., wt\u22121) = exp(~h\u22bat\u22121ywt + bwt)\n\u2211 w\u2208V exp( ~h \u22ba t\u22121yw + bw) ,\n(1)\n~ht = \u2212\u2212\u2212\u2192 RNN(xwt , ~ht\u22121). (2)\nHere, xwt and ywt denote the input and output word embeddings of wt respectively, V represents the set of words in the vocabulary, and bw is a bias value applied when predicting the\nword w. The function \u2212\u2212\u2212\u2192 RNN is often replaced with LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) to improve performance."}, {"heading": "2.2 Dynamic Entity Representation", "text": "RNN-based models have been reported to achieve better results on the CNN QA reading comprehension dataset (Hermann et al., 2015; Kobayashi et al., 2016). In the CNN QA dataset, every named entity in each document is anonymized. This is done to allow the ability to comprehend a document using neither prior nor external knowledge to be evaluated. To capture the meanings of such anonymized entities, Kobayashi et al. (2016) proposed a new model that they named dynamic entity representation. This encodes the local contexts of an entity and uses the resulting context vector as the word embedding of a subsequent occurrence of that entity in the input layer of the RNN. This model: (1) constructs context vectors d\u2032e,i from the local contexts of an entity e at the i-th sentence; (2) merges multiple contexts of the entity e through max pooling and produces the dynamic representation de,i; and (3) replaces the embedding of the entity e in the (i + 1)-th sentence with the dynamic embedding xe,i+1 produced from de,i. More formally,\nxe,i+1 = Wdcde,i + be, (3)\nde,i = maxpooling(d \u2032 e,i,de,i\u22121), (4) d\u2032e,i = ContextEncoder(e, i). (5)\nHere, be denotes a bias vector, maxpooling is a function that yields the largest value from the elementwise inputs, and ContextEncoder is an encoding function. Figure 2 gives an example of the process of encoding and merging contexts from sentences. An arbitrary encoder can be used for ContextEncoder; Kobayashi et al. (2016) used bidirectional RNNs, encoding the words surrounding the entity e of a sentence in both directions. If the entity e fails to appear in the i-th sentence, the embedding is not updated, i.e., de,i = de,i\u22121."}, {"heading": "3 Proposed Method: Dynamic Neural Text Modeling", "text": "In this section, we introduce the extension of dynamic entity representation to language modeling. From Equations 1 and 2, RNLM uses a set of word\nembeddings in the input layer to encode the preceding contextual words, and another set of word embeddings in the output layer to predict a word from the encoded context. Therefore, we consider incorporating the idea of dynamic representation into the word embeddings in the output layer (yw in Equation 1) as well as in the input layer (xw in Equation 2; refer to Figure 1). The novel extension of dynamic representation to the output layer affects predictions made for entities that appear repeatedly, whereas that in the input layer is expected to affect the prediction of words that follow the entities. The procedure for constructing dynamic representations of e, de,i is the same as that introduced in Section 2.2. Before reading the (i + 1)-th sentence, the model constructs the context vectors [d\u2032e,1, ...,d \u2032\ne,i] from the local contexts of e in every preceding sentence. Here, d\u2032e,j denotes the context vector of e in the j-th sentence. ContextEncoder in the model produces a context vector d\u2032e for e at the t-th position in a sentence, using a bidirectional RNN1 as follows:\nd\u2032e = ReLU(Whd[ ~ht\u22121, ~ht+1]+bd), (6) ~ht = \u2212\u2212\u2212\u2192 RNN(xwt , ~ht\u22121), (7) ~ht = \u2190\u2212\u2212\u2212 RNN(xwt , ~ht+1). (8)\nHere, ReLU denotes the ReLU activation function (Nair and Hinton, 2010), while Wdc and Whd correspond to learnable matrices; bd is a bias vector. As in the RNN language model, ~ht\u22121 and ~ht+1 as well as their composition d\u2032e can capture information necessary to predict the features of the target e at the t-th word. Following context encoding, the model merges the multiple context vectors, [d\u2032e,1, ...,d \u2032\ne,i], into the dynamic representation de,i using a merging function. A range of functions are abailable for merging multiple vectors, while Kobayashi et al. (2016) used only max pooling (Equation 4). In this study, we explored three further functions: GRU, GRU followed by ReLU (de,i = ReLU(GRU(d\u2032e,i,de,i\u22121))) and a function that selects only the latest context, i.e., de,i = d\u2032e,i. This comparison clarifies the effect of the accumulation of contexts as the experiments proceeded2 .\n1Equations 2 and 7 are identical but do not share internal parameters.\n2Note that merging functions are not restricted to considering two arguments (a new context and a merged past\nThe merging function produces the dynamic representation de,i of e. In language modeling, to read the (i + 1)-th sentence, the model uses two dynamic word embeddings of e in the input and output layers. The input embedding xe, used to encode contexts (Equation 2), and the output embedding ye, used to predict the occurrence of e (Equation 1), are replaced with dynamic versions:\nxe = Wdxde,i + b x e , (9) ye = Wdyde,i + b y e , (10)\nwhere Wdx and Wdy denote learnable matrices, and bxe and b y e denote learnable vectors tied to e. We can observe that a conventional RNN language model is a variant that removes the dynamic terms (Wdxde,i and Wdyde,i) using only the static terms (bxe and b y e ) to represent e. The initial dynamic representation de,0 is defined as a zero vector, so that the initial word embeddings (xe and ye) are identical to the static terms (bxe and b y e ) until the point at which the first context of the target word e is observed. All parameters in the end-to-end model are learned entirely by backpropagation, maximizing the log-likelihood in the same way as a conventional RNN language model.\nWe can view the approach in Kobayashi et al. (2016) as a variant on the proposed method, but using the dynamic terms only in the input layer (for xe). We can also view the copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) as a variant on the proposed method, in which specific embeddings in the output layer are replaced with special dynamic vectors.\ncontext) recurrently but can consider all vectors over the whole history [d\u2032e,1, ...,d \u2032\ne,i] (e.g., by using attention mechanism (Bahdanau et al., 2015)). However, for simplicity, this research focuses only on the case of a function with two arguments."}, {"heading": "4 Anonymized Language Modeling", "text": "This study explores methods for on-the-fly capture and exploitation of the meanings of unknown words or entities in a discourse. To do this, we introduce a novel evaluation task and dataset that we called Anonymized Language Modeling. Figure 3 gives an example from the dataset. Briefly, the dataset anonymizes certain noun phrases, treating them as unknown words and retaining their coreference relations. This allows a language model to track the context of every noun phrase in the discourse. Other words are left unchanged, allowing the language model to preserve the context of the anonymized (unknown) words, and to infer their meanings from the known words. The process was inspired by Hermann et al. (2015), whose approach has been explored by the research on reading comprehension. More precisely, we used the OntoNotes (Pradhan et al., 2012) corpus, which includes documents with coreferences and named entity tags manually annotated. We assigned an anonymous identifier to every coreference chain in the corpus3 in order of first appearance4 , and replaced mentions of a coreference chain with its identifier. In our experiments, each coreference chain was given a dynamic representation. Following Mikolov et al. (2010), we limited the vocabulary to 10,000 words appearing frequently in the corpus. Finally, we inserted \u201c<bos>\u201d and \u201c<eos>\u201d tokens to mark the beginning and end of each sentence. An important difference between this dataset and the one presented in Hermann et al. (2015) is in the way that coreferences are treated.\n3We used documents with no more than 50 clusters, which covered more than 97% of the corpus.\n4Following the study of Luong et al. (2015), we assigned \u201c<unk1>\u201d, \u201c<unk2>\u201d, ... to coreference clusters in order of first appearance.\nHermann et al. (2015) used automatic resolusion of coreferences, whereas our study made use of the manual annotations in the OntoNotes. Thus, the process of Hermann et al. (2015) introduced (intentional and unintentional) errors into the dataset. Additionally, the dataset did not assign an entity identifier to a pronoun. In contrast, as our dataset has access to the manual annotations of coreferences, we are able to investigate the ability of the language model to capture meanings from contexts. Dynamic updating could be applied to words in all lexical categories, including verbs, adjectives, and nouns without requiring additional extensions. However, verbs and adjectives were excluded from targets of dynamic updates in the experiments, for two reasons. First, proper nouns and nouns accounted for the majority (70%) of the low-frequency (unknown) words, followed by verbs (10%) and adjectives (9%). Second, we assumed that the meaning of a verb or adjective would shift less over the course of a discourse than that of a noun. When semantic information of unknown verbs and adjectives is required, their embeddings may be extracted from ad-hoc training on a different larger corpus. This, however, was beyond the scope of this study."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Setting", "text": "An experiment was conducted to investigate the effect of Dynamic Neural Text Model on the Anonymized Language Modeling dataset. The split of dataset followed that of the original corpus (Pradhan et al., 2012). Table 1 summarizes the statistics of the dataset. The baseline model was a typical LSTM RNN language model with 512 units. We compared three variants of the proposed model, using different applications of dynamic embedding: in the input layer only (as in Kobayashi et al. (2016)), in\nthe output layer only, and in both the input and output layers. The context encoders were bidirectional LSTMs with 512 units, the parameters of which were not the same as those in the LSTM RNN language models. All models were trained by maximizing the likelihood of correct tokens, to achieve best perplexity on the validation dataset5. Most hyper-parameters were tuned and fixed by the baseline model on the validation dataset6.\nIt is difficult to adequately train the all parts of a model using only the small dataset of Anonymized Language Modeling. We therefore pretrained word embeddings and ContextEncoder (the bidirectional RNNs and matrices in Equations 6\u2013 8) on a sentence completion task in which clozes were predicted from the surrounding words in a large corpus (Melamud et al., 2016)7. We used the objective function with negative sampling (Mikolov et al., 2013): \u2211 e(log \u03c3(x\u0302 \u22ba\nexe) +\u2211 v\u2208Neg(log \u03c3(\u2212x\u0302 \u22ba\nexv))). Here, x\u0302e is a context vector predicted by ContextEncoder, xe denotes the word embedding of a target word e appearing in the corpus, and Neg represents randomly sampled words. These pretrained parameters of ContextEncoder were fixed when the whole language model was trained on the Anonymized Language Modeling dataset. We implemented models in Python using the Chainer neural network library (Tokui et al., 2015). The code and the constructed dataset are publicly available8.\n5We performed a validation at the end of every half epoch out of five epochs.\n6Batchsize was 8. Adam (Kingma and Ba, 2015) with learning rate 10\u22123. Gradients were normalized so that their norm was smaller than 1. Truncation of backpropagation and updating was performed after every 20 sentences and at the end of document.\n7We pretrained a model on the Gigaword Corpus, excluding sentences with more than 32 tokens. We performed training for 50000 iterations with a batch size of 128 and five negative samples. Only words that occurred no fewer than 500 times are used; other words were treated as unknown tokens. Melamud et al. (2016) used three different sets of word embeddings for the two inputs with respect to the encoders ( \u2212\u2212\u2212\u2192 RNN and \u2190\u2212\u2212\u2212 RNN) and the output (target). However, we forced the sets of word embeddings to share a single set of word embeddings in pretraining. We initialized the word embeddings in both the input layer (xw) and the output layer (yw) of the novel models, including the baseline model, with this single set. The word embeddings of all anonymized tokens were initialized as unknown words with the word embedding of \u201c<unk>\u201d.\n8 https://github.com/soskek/dynamic_neural_text_model"}, {"heading": "5.2 Results and Analysis", "text": ""}, {"heading": "5.2.1 Perplexity", "text": "Table 2 shows performance of the baseline model and the three variants of the proposed method in terms of perplexity. The table reports the mean and standard error of three perplexity values after training using three different randomly chosen initializations (we used the same convention throughout this paper). Here, we discuss the proposed method using GRU followed by ReLU as the merging function, as this achieved the best perplexity (see Section 5.2.2 for a comparison of functions). We also show perplexitiy values when evaluating words of specific categories: (1) all words; (2) reappearing entity words; (3) words following entities; and (4) non-entity words. All variants of the proposed method outperformed the baseline model. Focusing on the categories (2) and (3) highlights the roles of dynamic updates of the input and output layers. Dynamic updates of the input layer (B) had a larger improvement for predicting words following entities (3) than those of the output layer (C). In contrast, dynamic updates of the output layer (C) were quite effective for predicting reappearing entities (2) whereas those of the input layer (B) were not. These facts confirm that: dynamic updates of the input layer help a model predict words following entities by supplying on-the-fly context information; and those of the output layer are effective to predict entity words appearing multiple times. In addition, dynamic updates of both the input and output layers (D) further improved the performance from those of either the output (C) or input (B) layer. Thus, the proposed dynamic output was shown to be compatible with dynamic input, and vice versa. These results demonstrated the positive effect of capturing and exploiting the contextsensitive meanings of entities. In order to examine whether dynamic updates of\nthe input and output embeddings capture contextsensitive meanings of entities, we present Figures 4, 5 and 6. Figure 4 depicts the perplexity of words with different positions in a document9. The figure confirms that the advantage of the proposed method over the baseline is more evident especially in the latter part of documents, where repeated words are more likely to occur.\nFigure 5 shows the perplexity with respect to the frequency of words t within documents. Note that the word embedding at the first occurrence of an entity is static. This figure indicates that entities appearing many times enjoy the benefit of the dynamic language model. Figure 6 visualizes the perplexity of entities with respect to the numbers of their antecedent candidates. It is clear from this figure that the proposed method is better at memorizing the semantic information of entities appearing repeatedly in documents than the baseline. These results also demonstrated the contribution of dynamic updates of word embeddings.\n9It is more difficult to predict tokens appearing latter in a document because the number of new (unknown) tokens increases as a model reads the document."}, {"heading": "5.2.2 Comparison of Merging functions", "text": "Table 3 compares models with different merging functions; GRU-ReLU, GRU, max pooling, and the use of the latest context. The use of the latest context had the worst performance for all variants of the proposed method. Thus, a proper accumulation of multiple contexts is indispensable for dynamic updates of word embeddings. Although Kobayashi et al. (2016) used only max pooling as the merging function, GRU and GRU-ReLU were shown to be comparable in performance and superior to max pooling when predicting tokens related to entities (2) and (3)."}, {"heading": "5.2.3 Predicting Entities by Likelihood of a Sentence", "text": "In order to examine contribution of the dynamic language models on a downstream task, we conducted cloze tests for comprehension of a sentence with reappearing entities in a discourse. Given multiple preceding entities E = {e+, e1, e2, ...}\nfollowed by a cloze sentence, the models were required to predict the true antecedent e+ which allowed the cloze to be correctly filled, among the other alternatives E\u2212 = {e1, e2, ...}. Language models solve this task by comparing the likelihoods of sentences filled with antecedent candidates in E and returning the entity with the highest likelihood of the sentence. In this experiment, the performance of a model was represented by the Mean Quantile (MQ) (Guu et al., 2015). The MQ computes the mean ratio at which the model predicts a correct antecedent e+ more likely than negative antecedents in E\u2212,\nMQ = |{e\u2212 \u2208 E\u2212 : p(e\u2212) < p(e+)}|\n|E\u2212| . (11)\nHere, p(e) denotes the likelihood of a sentence whose cloze is filled with e. If the correct antecedent e+ yields highest likelihood, MQ gets 1. Table 4 reports MQs for the three variants and merging functions. Dynamic updates of the in-\nput layer greatly boosted the performance by approximately 10%, while using both dynamic input and output improved it further. In this experiment, the merging functions with GRUs outperform the others. These results demonstrated that Dynamic Neural Text Models can accumulate a new information in word embeddings and contribute to modeling the semantic changes of entities in a discourse."}, {"heading": "6 Related Work", "text": "An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings. Li and Jurafsky (2015) applied word disambiguation and use a sense embedding to the target word. Choi et al. (2017) captured the contextsensitive meanings of common words using word embeddings, applied through a gating function controlled by history words, in the context of machine translation. In future work, we will explore a wider range of models, to integrate our dynamic text modeling with methods that estimate the meaning of unknown words or entities from their constituents. When addressing well-known entities such as Obama and Trump, it makes sense to learn their embeddings from external resources, as well as dynamically from the preceding context in a given discourse (as in our Dynamic Neural Text Model). The integration of these two sources of information is an intriguing challenge in language modeling. A key aspect of our model is its incorporation of the copy mechanism (Gu et al., 2016;\nGulcehre et al., 2016), using dynamic word embeddings in the output layer. Independently of this study, several research groups have explored the use of variants of the copy mechanisms in language modeling (Merity et al., 2017; Grave et al., 2017; Peng and Roth, 2016). These studies, however, did not incorporate dynamic representations in the input layer. In contrast, our proposal incorporates the copy mechanism through the use of dynamic representations in the output layer, integrating them with dynamic mechanisms in both the input and output layers by applying dynamic entity-wise representation. Our experiments have demonstrated the benefits of such integration.\nAnother related trend in recent studies is the use of neural network to capture the information flow of a discourse. One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017). Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entity-wise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation.\nWe summarize and compare works for entitycentric neural networks that read a document. Kobayashi et al. (2016) pioneered entity-centric neural models tracking states in a discourse. They proposed Dynamic Entity Representation, which encodes contexts of entities and updates the states using entity-wise memories. Wiseman et al. (2016) also managed such entity-wise features on neural networks and improved a coreference resolution model. Clark and Manning (2016b,a) pursued such entity-wise representations in mentionranking coreference models. Our paper follows the Kobayashi et al. (2016) and exploits dynamic entity reprensetion in neural language models, which are also used as neural decoders for various sequence generation tasks, e.g., machine translation and dialog response generation. Simultaneously with our paper, Ji et al. (2017) use dynamic entity representation in a neural language model for reranking outputs of a coreference res-\nolution system. Yang et al. (2017) experiment language modeling with referring internal contexts or external data. Henaff et al. (2017) focus on neural networks tracking contexts of entities, achieving the state-of-the-art result in bAbI, a reading comprehension task. They encode the contexts of each entity by an attention-like gated RNN instead of using coreference links directly. Dhingra et al. (2017) also try to improve a reading comprehension model using coreference links. As a similar work of dynamic entity representation, Bahdanau et al. (2017) construct on-the-fly word embeddings of rare words from dictionary definitions.\nThe fisrt key component of dynamic entity representation is a function to merge more than one contexts about an entity into a consistent representation of the entity. Various choices for the function exist, e.g., max or average-pooling (Kobayashi et al., 2016; Clark and Manning, 2016b), RNN (GRU, LSTM (Wiseman et al., 2016; Yang et al., 2017) or other gated RNNs (Henaff et al., 2017; Ji et al., 2017)), or using the latest context only (without any merging) (Yang et al., 2017). This paper is the first work comparing the effects of those choices (see Section 5.2.2).\nThe second component is a function to encode contexts from text, e.g., bidirectional RNN encoding surrounding context (Kobayashi et al., 2016), unidirectional RNN used in a language model (Ji et al., 2017; Yang et al., 2017), feedforward neural network with a sentence vector and an entity\u2019s word vector (Henaff et al., 2017) or hand-crafted features with word embeddings (Wiseman et al., 2016; Clark and Manning, 2016b). This paper employs bi-RNN as well as Kobayashi et al. (2016), which can access full context with powerful learnable units.\nIn the task setting proposed in this study, a model must capture meanings of specific words from a small number of contexts in a given discourse. The task could also be applied to novel one-shot learning (Fei-Fei et al., 2006) of word meanings. With the exception of a single study by Vinyals et al. (2016), which used a task in which the context of a target word was matched with a different context of the same word, this has been little studied."}, {"heading": "7 Conclusion", "text": "This study addressed the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input layer and output layer of a neural model by tracking contexts. This extended the dynamic entity representation presented in Kobayashi et al. (2016), and incorporated a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In the course of the study, we also constructed a new task and dataset, called Anonymized Language Modeling, for evaluating the ability of a model to capture word meanings while reading. Experiments conducted using our novel dataset demonstrated that the RNN language model variants proposed in this study outperformed the baseline model. More detailed analysis indicated that the proposed method was particularly successful in capturing the meaning of an unknown words from texts containing few instances."}, {"heading": "Acknowledgments", "text": "This work was supported by JSPS KAKENHI Grant Number 15H01702 and JSPS KAKENHI Grant Number 15H05318. We thank members of Preferred Networks, Inc., Makoto Miwa and Daichi Mochihashi for suggestive discussions."}], "references": [{"title": "Learning to compute word embeddings on the fly", "author": ["Dzmitry Bahdanau", "Tom Bosc", "Stanis\u0142aw Jastrz\u0119bski", "Edward Grefenstette", "Pascal Vincent", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1706.00286 .", "citeRegEx": "Bahdanau et al\\.,? 2017", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "JOURNAL OF MACHINE LEARNING RESEARCH 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "TomasMikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "In Proceedings of INTERSPEECH", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Context-dependent word representation for neural machine translation", "author": ["Heeyoul Choi", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Computer Speech & Language 45:149\u2013160.", "citeRegEx": "Choi et al\\.,? 2017", "shortCiteRegEx": "Choi et al\\.", "year": 2017}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of EMNLP. pages 2256\u20132262.", "citeRegEx": "Clark and Manning.,? 2016a", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Improving coreference resolution by learning entitylevel distributed representations", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of ACL. pages 643\u2013653.", "citeRegEx": "Clark and Manning.,? 2016b", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Language Modeling with Gated Convolutional Networks", "author": ["Yann N. Dauphin", "Angela Fan", "Michael Auli", "David Grangier."], "venue": "arXiv preprint arXiv:1612.08083 .", "citeRegEx": "Dauphin et al\\.,? 2016", "shortCiteRegEx": "Dauphin et al\\.", "year": 2016}, {"title": "Linguistic knowledge as memory for recurrent neural networks", "author": ["Bhuwan Dhingra", "Zhilin Yang", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1703.02620 .", "citeRegEx": "Dhingra et al\\.,? 2017", "shortCiteRegEx": "Dhingra et al\\.", "year": 2017}, {"title": "Oneshot learning of object categories", "author": ["Li Fei-Fei", "Rob Fergus", "Pietro Perona."], "venue": "IEEE transactions on TPAMI 28(4):594\u2013611.", "citeRegEx": "Fei.Fei et al\\.,? 2006", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2006}, {"title": "Improving neural language models with a continuous cache", "author": ["Edouard Grave", "Armand Joulin", "Nicolas Usunier."], "venue": "Proceedings of ICLR.", "citeRegEx": "Grave et al\\.,? 2017", "shortCiteRegEx": "Grave et al\\.", "year": 2017}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "O.K. Victor Li."], "venue": "Proceedings of ACL. pages 1631\u20131640.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of ACL. pages 140\u2013149.", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Traversing knowledge graphs in vector space", "author": ["Kelvin Guu", "John Miller", "Percy Liang."], "venue": "Proceedings of EMNLP. pages 318\u2013327.", "citeRegEx": "Guu et al\\.,? 2015", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Tracking the world state with recurrent entity networks", "author": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun."], "venue": "Proceedings of ICLR.", "citeRegEx": "Henaff et al\\.,? 2017", "shortCiteRegEx": "Henaff et al\\.", "year": 2017}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Proceedings of NIPS. pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Dynamic entity representations in neural language models", "author": ["Yangfeng Ji", "Chenhao Tan", "Sebastian Martschat", "Yejin Choi", "Noah A. Smith."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Ji et al\\.,? 2017", "shortCiteRegEx": "Ji et al\\.", "year": 2017}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proceedings of AAAI. pages 2741\u20132749.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Dynamic entity representation with max-pooling improves machine reading", "author": ["Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui."], "venue": "Proceedings of NAACL-HLT. pages 850\u2013855.", "citeRegEx": "Kobayashi et al\\.,? 2016", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2016}, {"title": "Do multi-sense embeddings improve natural language understanding? In Proceedings of EMNLP", "author": ["Jiwei Li", "Dan Jurafsky."], "venue": "pages 1722\u20131732.", "citeRegEx": "Li and Jurafsky.,? 2015", "shortCiteRegEx": "Li and Jurafsky.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "D. Christopher Manning."], "venue": "Proceedings of ACL. pages 1054\u20131063.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL. pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "context2vec: Learning generic context embedding with bidirectional lstm", "author": ["Oren Melamud", "Jacob Goldberger", "Ido Dagan."], "venue": "Proceedings of CoNLL. pages 51\u201361.", "citeRegEx": "Melamud et al\\.,? 2016", "shortCiteRegEx": "Melamud et al\\.", "year": 2016}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."], "venue": "Proceedings of ICLR.", "citeRegEx": "Merity et al\\.,? 2017", "shortCiteRegEx": "Merity et al\\.", "year": 2017}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of INTERSPEECH. pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proceedings of NIPS. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton."], "venue": "Johannes F\u00c3ijrnkranz and Thorsten Joachims, editors, Proceedings of ICML. Omnipress, pages 807\u2013 814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Two discourse driven language models for semantics", "author": ["Haoruo Peng", "Dan Roth."], "venue": "Proceedings of ACL. pages 290\u2013300.", "citeRegEx": "Peng and Roth.,? 2016", "shortCiteRegEx": "Peng and Roth.", "year": 2016}, {"title": "CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "author": ["Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Pradhan et al\\.,? 2012", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Japanese and korean voice search", "author": ["Mike Schuster", "Kaisuke Nakajima."], "venue": "Proceedings of ICASSP. pages 5149\u20135152.", "citeRegEx": "Schuster and Nakajima.,? 2012", "shortCiteRegEx": "Schuster and Nakajima.", "year": 2012}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of ACL. pages 1715\u2013 1725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of AAAI. pages 3776\u20133783.", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "Proceedings of NIPS. pages 2440\u20132448.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton."], "venue": "Proceedings of Workshop on LearningSys in NIPS 28.", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Recurrent memory networks for languagemodeling", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of NAACL-HLT. pages 321\u2013331.", "citeRegEx": "Tran et al\\.,? 2016", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Matching networks for one shot learning", "author": ["Oriol Vinyals", "Charles Blundell", "Tim Lillicrap", "koray kavukcuoglu", "Daan Wierstra"], "venue": "In Proceedings of NIPS", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Larger-context language modelling with recurrent neural network", "author": ["Tian Wang", "Kyunghyun Cho."], "venue": "Proceedings of ACL. pages 1319\u20131329.", "citeRegEx": "Wang and Cho.,? 2016", "shortCiteRegEx": "Wang and Cho.", "year": 2016}, {"title": "Learning global features for coreference resolution", "author": ["Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber."], "venue": "Proceedings of NAACL-HLT. pages 994\u20131004.", "citeRegEx": "Wiseman et al\\.,? 2016", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "Reference-aware language models", "author": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 20, "context": "This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al.", "startOffset": 55, "endOffset": 79}, {"referenceID": 12, "context": "(2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al.", "startOffset": 67, "endOffset": 84}, {"referenceID": 12, "context": "(2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading.", "startOffset": 67, "endOffset": 111}, {"referenceID": 2, "context": "Recent studies have demonstrated that language models trained using neural network (Bengio et al., 2003; Mikolov et al., 2010) such as recurrent neural network (RNN) (Jozefowicz et al.", "startOffset": 83, "endOffset": 126}, {"referenceID": 28, "context": "Recent studies have demonstrated that language models trained using neural network (Bengio et al., 2003; Mikolov et al., 2010) such as recurrent neural network (RNN) (Jozefowicz et al.", "startOffset": 83, "endOffset": 126}, {"referenceID": 19, "context": ", 2010) such as recurrent neural network (RNN) (Jozefowicz et al., 2016) and .", "startOffset": 47, "endOffset": 72}, {"referenceID": 8, "context": "convolutional neural network (Dauphin et al., 2016) achieve the best performance across a range of corpora (Mikolov et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 28, "context": ", 2016) achieve the best performance across a range of corpora (Mikolov et al., 2010; Chelba et al., 2014; Merity et al., 2017; Grave et al., 2017).", "startOffset": 63, "endOffset": 147}, {"referenceID": 3, "context": ", 2016) achieve the best performance across a range of corpora (Mikolov et al., 2010; Chelba et al., 2014; Merity et al., 2017; Grave et al., 2017).", "startOffset": 63, "endOffset": 147}, {"referenceID": 27, "context": ", 2016) achieve the best performance across a range of corpora (Mikolov et al., 2010; Chelba et al., 2014; Merity et al., 2017; Grave et al., 2017).", "startOffset": 63, "endOffset": 147}, {"referenceID": 11, "context": ", 2016) achieve the best performance across a range of corpora (Mikolov et al., 2010; Chelba et al., 2014; Merity et al., 2017; Grave et al., 2017).", "startOffset": 63, "endOffset": 147}, {"referenceID": 12, "context": ", named entities) in a source sentence (Gu et al., 2016; Gulcehre et al., 2016).", "startOffset": 39, "endOffset": 79}, {"referenceID": 13, "context": ", named entities) in a source sentence (Gu et al., 2016; Gulcehre et al., 2016).", "startOffset": 39, "endOffset": 79}, {"referenceID": 17, "context": "The function \u2212\u2212\u2212\u2192 RNN is often replaced with LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al.", "startOffset": 50, "endOffset": 84}, {"referenceID": 4, "context": "The function \u2212\u2212\u2212\u2192 RNN is often replaced with LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) to improve performance.", "startOffset": 92, "endOffset": 110}, {"referenceID": 16, "context": "RNN-based models have been reported to achieve better results on the CNN QA reading comprehension dataset (Hermann et al., 2015; Kobayashi et al., 2016).", "startOffset": 106, "endOffset": 152}, {"referenceID": 22, "context": "RNN-based models have been reported to achieve better results on the CNN QA reading comprehension dataset (Hermann et al., 2015; Kobayashi et al., 2016).", "startOffset": 106, "endOffset": 152}, {"referenceID": 16, "context": "RNN-based models have been reported to achieve better results on the CNN QA reading comprehension dataset (Hermann et al., 2015; Kobayashi et al., 2016). In the CNN QA dataset, every named entity in each document is anonymized. This is done to allow the ability to comprehend a document using neither prior nor external knowledge to be evaluated. To capture the meanings of such anonymized entities, Kobayashi et al. (2016) proposed a new model that they named dynamic entity representation.", "startOffset": 107, "endOffset": 424}, {"referenceID": 22, "context": "An arbitrary encoder can be used for ContextEncoder; Kobayashi et al. (2016) used", "startOffset": 53, "endOffset": 77}, {"referenceID": 30, "context": "Here, ReLU denotes the ReLU activation function (Nair and Hinton, 2010), while Wdc and Whd correspond to learnable matrices; bd is a bias vector.", "startOffset": 48, "endOffset": 71}, {"referenceID": 22, "context": "A range of functions are abailable for merging multiple vectors, while Kobayashi et al. (2016) used only max pooling (Equation 4).", "startOffset": 71, "endOffset": 95}, {"referenceID": 12, "context": "We can also view the copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) as a variant on the proposed method, in which specific embeddings in the output layer are replaced with special dynamic vectors.", "startOffset": 36, "endOffset": 76}, {"referenceID": 13, "context": "We can also view the copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) as a variant on the proposed method, in which specific embeddings in the output layer are replaced with special dynamic vectors.", "startOffset": 36, "endOffset": 76}, {"referenceID": 20, "context": "We can view the approach in Kobayashi et al. (2016) as a variant on the proposed method, but using the dynamic terms only in the input layer (for xe).", "startOffset": 28, "endOffset": 52}, {"referenceID": 1, "context": ", by using attention mechanism (Bahdanau et al., 2015)).", "startOffset": 31, "endOffset": 54}, {"referenceID": 32, "context": "More precisely, we used the OntoNotes (Pradhan et al., 2012) corpus, which includes documents with coreferences and named", "startOffset": 38, "endOffset": 60}, {"referenceID": 16, "context": "The process was inspired by Hermann et al. (2015), whose approach has been explored by the research on reading comprehension.", "startOffset": 28, "endOffset": 50}, {"referenceID": 28, "context": "Following Mikolov et al. (2010), we limited the vocabulary to 10,000 words appearing frequently in the corpus.", "startOffset": 10, "endOffset": 32}, {"referenceID": 16, "context": "and the one presented in Hermann et al. (2015) is in the way that coreferences are treated.", "startOffset": 25, "endOffset": 47}, {"referenceID": 25, "context": "Following the study of Luong et al. (2015), we assigned \u201c<unk1>\u201d, \u201c<unk2>\u201d, .", "startOffset": 23, "endOffset": 43}, {"referenceID": 32, "context": "The split of dataset followed that of the original corpus (Pradhan et al., 2012).", "startOffset": 58, "endOffset": 80}, {"referenceID": 22, "context": "We compared three variants of the proposed model, using different applications of dynamic embedding: in the input layer only (as in Kobayashi et al. (2016)), in the output layer only, and in both the input and output layers.", "startOffset": 132, "endOffset": 156}, {"referenceID": 26, "context": "We therefore pretrained word embeddings and ContextEncoder (the bidirectional RNNs and matrices in Equations 6\u2013 8) on a sentence completion task in which clozes were predicted from the surrounding words in a large corpus (Melamud et al., 2016)7.", "startOffset": 221, "endOffset": 243}, {"referenceID": 29, "context": "We used the objective function with negative sampling (Mikolov et al., 2013): \u2211 e(log \u03c3(x\u0302 \u22ba exe) + \u2211 v\u2208Neg(log \u03c3(\u2212x\u0302 \u22ba exv))).", "startOffset": 54, "endOffset": 76}, {"referenceID": 37, "context": "We implemented models in Python using the Chainer neural network library (Tokui et al., 2015).", "startOffset": 73, "endOffset": 93}, {"referenceID": 21, "context": "Adam (Kingma and Ba, 2015) with learning rate 10.", "startOffset": 5, "endOffset": 26}, {"referenceID": 21, "context": "Adam (Kingma and Ba, 2015) with learning rate 10. Gradients were normalized so that their norm was smaller than 1. Truncation of backpropagation and updating was performed after every 20 sentences and at the end of document. We pretrained a model on the Gigaword Corpus, excluding sentences with more than 32 tokens. We performed training for 50000 iterations with a batch size of 128 and five negative samples. Only words that occurred no fewer than 500 times are used; other words were treated as unknown tokens. Melamud et al. (2016) used three different sets of word embeddings for the two inputs with respect to the en-", "startOffset": 6, "endOffset": 537}, {"referenceID": 14, "context": "In this experiment, the performance of a model was represented by the Mean Quantile (MQ) (Guu et al., 2015).", "startOffset": 89, "endOffset": 107}, {"referenceID": 20, "context": "An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings.", "startOffset": 74, "endOffset": 169}, {"referenceID": 34, "context": "An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings.", "startOffset": 74, "endOffset": 169}, {"referenceID": 24, "context": "An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings.", "startOffset": 74, "endOffset": 169}, {"referenceID": 33, "context": "An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings.", "startOffset": 74, "endOffset": 169}, {"referenceID": 20, "context": "An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings. Li and Jurafsky (2015) applied word disambigua-", "startOffset": 75, "endOffset": 284}, {"referenceID": 5, "context": "Choi et al. (2017) captured the contextsensitive meanings of common words using word embeddings, applied through a gating function", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "A key aspect of our model is its incorporation of the copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), using dynamic word embeddings in the output layer.", "startOffset": 69, "endOffset": 109}, {"referenceID": 13, "context": "A key aspect of our model is its incorporation of the copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), using dynamic word embeddings in the output layer.", "startOffset": 69, "endOffset": 109}, {"referenceID": 27, "context": "Independently of this study, several research groups have explored the use of variants of the copy mechanisms in language modeling (Merity et al., 2017; Grave et al., 2017; Peng and Roth, 2016).", "startOffset": 131, "endOffset": 193}, {"referenceID": 11, "context": "Independently of this study, several research groups have explored the use of variants of the copy mechanisms in language modeling (Merity et al., 2017; Grave et al., 2017; Peng and Roth, 2016).", "startOffset": 131, "endOffset": 193}, {"referenceID": 31, "context": "Independently of this study, several research groups have explored the use of variants of the copy mechanisms in language modeling (Merity et al., 2017; Grave et al., 2017; Peng and Roth, 2016).", "startOffset": 131, "endOffset": 193}, {"referenceID": 40, "context": "One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al.", "startOffset": 52, "endOffset": 93}, {"referenceID": 35, "context": "One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al.", "startOffset": 52, "endOffset": 93}, {"referenceID": 36, "context": ", 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017).", "startOffset": 102, "endOffset": 167}, {"referenceID": 38, "context": ", 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017).", "startOffset": 102, "endOffset": 167}, {"referenceID": 27, "context": ", 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017).", "startOffset": 102, "endOffset": 167}, {"referenceID": 22, "context": "Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al.", "startOffset": 34, "endOffset": 79}, {"referenceID": 15, "context": "Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al.", "startOffset": 34, "endOffset": 79}, {"referenceID": 39, "context": "Wiseman et al. (2016) also managed such entity-wise features on neural networks and improved a coreference resolution model.", "startOffset": 0, "endOffset": 22}, {"referenceID": 21, "context": "Our paper follows the Kobayashi et al. (2016) and exploits dynamic entity reprensetion in neural language models, which are also used as neural decoders for various sequence generation tasks, e.", "startOffset": 22, "endOffset": 46}, {"referenceID": 18, "context": "Simultaneously with our paper, Ji et al. (2017) use dynamic entity representation in a neural language model for reranking outputs of a coreference res-", "startOffset": 31, "endOffset": 48}, {"referenceID": 41, "context": "Yang et al. (2017) experiment language modeling with referring internal contexts or external data.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "Henaff et al. (2017) focus on neural networks tracking contexts of entities, achieving the state-of-the-art result in bAbI, a reading comprehension task.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "As a similar work of dynamic entity representation, Bahdanau et al. (2017) construct on-the-fly word embeddings of rare words from dictionary definitions.", "startOffset": 52, "endOffset": 75}, {"referenceID": 22, "context": ", max or average-pooling (Kobayashi et al., 2016; Clark and Manning, 2016b), RNN (GRU, LSTM (Wiseman et al.", "startOffset": 25, "endOffset": 75}, {"referenceID": 7, "context": ", max or average-pooling (Kobayashi et al., 2016; Clark and Manning, 2016b), RNN (GRU, LSTM (Wiseman et al.", "startOffset": 25, "endOffset": 75}, {"referenceID": 41, "context": ", 2016; Clark and Manning, 2016b), RNN (GRU, LSTM (Wiseman et al., 2016; Yang et al., 2017) or other gated RNNs (Henaff et al.", "startOffset": 50, "endOffset": 91}, {"referenceID": 42, "context": ", 2016; Clark and Manning, 2016b), RNN (GRU, LSTM (Wiseman et al., 2016; Yang et al., 2017) or other gated RNNs (Henaff et al.", "startOffset": 50, "endOffset": 91}, {"referenceID": 15, "context": ", 2017) or other gated RNNs (Henaff et al., 2017; Ji et al., 2017)), or using the latest context only (without any merging) (Yang et al.", "startOffset": 28, "endOffset": 66}, {"referenceID": 18, "context": ", 2017) or other gated RNNs (Henaff et al., 2017; Ji et al., 2017)), or using the latest context only (without any merging) (Yang et al.", "startOffset": 28, "endOffset": 66}, {"referenceID": 42, "context": ", 2017)), or using the latest context only (without any merging) (Yang et al., 2017).", "startOffset": 65, "endOffset": 84}, {"referenceID": 22, "context": ", bidirectional RNN encoding surrounding context (Kobayashi et al., 2016), unidirectional RNN used in a language model (Ji et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 18, "context": ", 2016), unidirectional RNN used in a language model (Ji et al., 2017; Yang et al., 2017), feedforward neural network with a sentence vector and an entity\u2019s word vector (Henaff et al.", "startOffset": 53, "endOffset": 89}, {"referenceID": 42, "context": ", 2016), unidirectional RNN used in a language model (Ji et al., 2017; Yang et al., 2017), feedforward neural network with a sentence vector and an entity\u2019s word vector (Henaff et al.", "startOffset": 53, "endOffset": 89}, {"referenceID": 15, "context": ", 2017), feedforward neural network with a sentence vector and an entity\u2019s word vector (Henaff et al., 2017) or hand-crafted features with word embeddings (Wiseman et al.", "startOffset": 87, "endOffset": 108}, {"referenceID": 41, "context": ", 2017) or hand-crafted features with word embeddings (Wiseman et al., 2016; Clark and Manning, 2016b).", "startOffset": 54, "endOffset": 102}, {"referenceID": 7, "context": ", 2017) or hand-crafted features with word embeddings (Wiseman et al., 2016; Clark and Manning, 2016b).", "startOffset": 54, "endOffset": 102}, {"referenceID": 6, "context": ", 2016; Clark and Manning, 2016b). This paper employs bi-RNN as well as Kobayashi et al. (2016), which can access full context with powerful learnable units.", "startOffset": 8, "endOffset": 96}, {"referenceID": 10, "context": "The task could also be applied to novel one-shot learning (Fei-Fei et al., 2006) of word meanings.", "startOffset": 58, "endOffset": 80}, {"referenceID": 10, "context": "The task could also be applied to novel one-shot learning (Fei-Fei et al., 2006) of word meanings. With the exception of a single study by Vinyals et al. (2016), which used a task in which the context of a target word was matched with a different context of the same word, this has been little studied.", "startOffset": 59, "endOffset": 161}, {"referenceID": 20, "context": "This extended the dynamic entity representation presented in Kobayashi et al. (2016), and incorporated a copy mechanism proposed independently by Gu et al.", "startOffset": 61, "endOffset": 85}, {"referenceID": 12, "context": "(2016), and incorporated a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al.", "startOffset": 68, "endOffset": 85}, {"referenceID": 12, "context": "(2016), and incorporated a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In the course of the study, we also constructed a new task and dataset, called Anonymized Language Modeling, for evaluating the ability of a model to capture word meanings while reading.", "startOffset": 68, "endOffset": 112}], "year": 2017, "abstractText": "This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities.", "creator": "LaTeX with hyperref package"}}}