{"id": "1709.01887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "Measuring the Similarity of Sentential Arguments in Dialog", "abstract": "when people converse about social or political discussions topics, similar internal arguments are often paraphrased by different speakers, across many decidedly different conversations. debate websites produce curated summaries of arguments on such topics ; these summaries typically consist of linear lists of sentences that represent frequently paraphrased propositions, or labels capturing the essence together of one particular aspect of an argument, e. g. morality or second amendment. we call these frequently paraphrased propositions argument facets. like these curated sites, our goal is to induce and identify argument evidence facets across multiple conversations, and produce summaries. however, we aim to do this automatically. we frame approaching the problem as seemingly consisting of two steps : we first extract sentences that express an argument from raw social media dialogs, and then rank the extracted arguments in terms of their similarity rates to one another. sets of similar arguments are used to similarly represent argument facets. we show here that we can predict argument facet similarity with a correlation averaging 0. 63 compared to a human topline averaging 0. 68 over three debate topics, easily beating several reasonable baselines.", "histories": [["v1", "Wed, 6 Sep 2017 17:15:49 GMT  (343kb,D)", "http://arxiv.org/abs/1709.01887v1", "Measuring the Similarity of Sentential Arguments in Dialog, by Misra, Amita and Ecker, Brian and Walker, Marilyn A, 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages={276}, year={2016} The dataset is available atthis https URL"]], "COMMENTS": "Measuring the Similarity of Sentential Arguments in Dialog, by Misra, Amita and Ecker, Brian and Walker, Marilyn A, 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages={276}, year={2016} The dataset is available atthis https URL", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["amita misra", "brian ecker", "marilyn a walker"], "accepted": false, "id": "1709.01887"}, "pdf": {"name": "1709.01887.pdf", "metadata": {"source": "META", "title": "Measuring the Similarity of Sentential Arguments in Dialog", "authors": ["Amita Misra", "Brian Ecker", "Marilyn A. Walker"], "emails": ["amitamisra@soe.ucsc.edu", "becker@soe.ucsc.edu", "maw@soe.ucsc.edu"], "sections": [{"heading": null, "text": "Proceedings of the SIGDIAL 2016 Conference, pages 276\u2013287, Los Angeles, USA, 13-15 September 2016. c\u00a92016 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. For example, consider the dialog excerpts in Fig. 1 from the 89K sentences about gun control in the IAC 2.0 corpus of online dialogs (Abbott et al., 2016). Each of the sentences S1 to S6 provide\ndifferent linguistic realizations of the same proposition namely that Criminals will have guns even if gun ownership is illegal.\nDebate websites, such as Idebate and ProCon produce curated summaries of arguments on the gun control topic, as well as many other topics.12 These summaries typically consist of lists, e.g. Fig. 2 lists eight different aspects of the gun control argument from Idebate. Such manually curated summaries identify different linguistic realizations of the same argument to induce a set of common, repeated, aspects of arguments, what we call ARGUMENT FACETS. For example, a curator might identify sentences S1 to S6 in Fig. 1 with a label to represent the facet that Criminals will have guns even if gun ownership is illegal.\nLike these curated sites, we also aim to induce and identify facets of an argument across multiple conversations, and produce summaries of all the different facets. However our aim is to do this automatically, and over time. In order to simplify the problem, we focus on SENTENTIAL ARGUMENTS, single sentences that clearly express\n1See http://debatepedia.idebate.org/en/ index.php/Debate: Gun control,\n2See http://gun-control.procon.org/\n276\nar X\niv :1\n70 9.\n01 88\n7v 1\n[ cs\n.C L\n] 6\nS ep\n2 01\n7\na particular argument facet in dialog. We aim to use SENTENTIAL ARGUMENTS to produce extractive summaries of online dialogs about current social and political topics. This paper extends our previous work which frames our goal as consisting of two tasks (Misra et al., 2015; Swanson et al., 2015).\n\u2022 Task1: Argument Extraction: How can we extract sentences from dialog that clearly express a particular argument facet? \u2022 Task2: Argument Facet Similarity: How can we recognize that two sentential arguments are semantically similar, i.e. that they are different linguistic realizations of the same facet of the argument?\nTask1 is needed because social media dialogs consist of many sentences that either do not express an argument, or cannot be understood out of context. Thus sentences that are useful for inducing argument facets must first be automatically identified. Our previous work on Argument Extraction achieved good results, (Swanson et al., 2015), and is extended here (Sec. 2).\nTask2 takes pairs of sentences from Task1 as input and then learns a regressor that can predict Argument Facet Similarity (henceforth AFS). Related work on argument mining (discussed in more detail in Sec. 4) defines a finite set of facets for each topic, similar to those from Idebate in\nFig. 2.3 Previous work then labels posts or sentences using these facets, and trains a classifier to return a facet label (Conrad et al., 2012; Hasan and Ng, 2014; Boltuzic and S\u030cnajder, 2014; Naderi and Hirst, 2015), inter alia. However, this simplification may not work in the long term, both because the sentential realizations of argument facets are propositional, and hence graded, and because\n3See also the facets in Fig. 3 below from ProCon.org.\nfacets evolve over time, and hence cannot be represented by a finite list.\nIn our previous work on AFS, we developed an AFS regressor for predicting the similarity of human-generated labels for summaries of dialogic arguments (Misra et al., 2015). We collected 5 human summaries of each dialog, and then used the Pyramid tool and scheme to annotate sentences from these summaries as contributors to (paraphrases of) a particular facet (Nenkova and Passonneau, 2004). The Pyramid tool requires the annotator to provide a human readable label for a collection of contributors that realize the same propositional content. The AFS regressor operated on pairs of human-generated labels from Pyramid summaries of different dialogs about the same topic. In this case, facet identification is done by the human summarizers, and collections of similar labels represent an argument facet. We believe this is a much easier task than the one we attempt here of training an AFS regressor on automatically extracted raw sentences from social media dialogs. The contributions of this paper are:\n\u2022 We develop a new corpus of sentential arguments with gold-standard labels for AFS. \u2022 We analyze and improve our argument extractor, by testing it on a much larger dataset. We develop a larger gold standard corpus for ARGUMENT QUALITY (AQ). \u2022 We develop a regressor that can predict AFS on extracted sentential arguments with a correlation averaging 0.63 compared to a human topline of 0.68 for three debate topics.4"}, {"heading": "2 Corpora and Problem Definition", "text": "Many existing websites summarize the frequent, and repeated, facets of arguments about current topics, that are linguistically realized in different ways, across many different social media and debate forums. For example, Fig. 2 illustrates the eight facets for gun control on IDebate. Fig. 3 illustrates a different type of summary, for the death penalty topic, from ProCon, where the argument facets are called out as the \u201cTop Ten Pros and Cons\u201d and given labels such as Morality, Constitutionality and Race. See the top of Fig. 3. The bottom of Fig. 3 shows how each facet is then elaborated by a paragraph for both its Pro and Con side: due to space we only show the summary for the Morality facet here.\nThese summaries are curated, thus one would 4Both the AQ and the AFS pair corpora are available at\nnlds.soe.ucsc.edu.\nnot expect that different sites would call out the exact same facets, or even that the same type of labels would be used for a specific facet. As we can see, ProCon (Fig. 3) uses one word or phrasal labels, while IDebate (Fig. 2) describes each facet with a sentence. Moreover, these curated summaries are not produced for a particular topic once-and-for-all: the curators often reorganize their summaries, drawing out different facets, or combining previously distinct facets under a single new heading. We hypothesize that this happens because new facets arise over time. For example, it is plausible that for the gay marriage topic, the facet that Gay marriage is a civil rights issue came to the fore only in the last ten years.\nOur long-term aim is to produce summaries similar to these curated summaries, but automatically, and over time, so that as new argument facets arise for a particular topic, we can identify them. We begin with three debate topics, gun control (38102 posts), gay marriage (22425 posts) and death penalty (5283 posts), from the Internet Argument Corpus 2.0 (Abbott et al., 2016). We first need to create a large sample of high quality sentential arguments (Task1 above) and then create a large sample of paired sentential arguments in order to train the model for AFS (Task2 above)."}, {"heading": "2.1 Argument Quality Data", "text": "We extracted all the sentences for all of the posts in each topic to first create a large corpus of topicsorted sentences. See Table 1.\nWe started with the Argument Quality (AQ) re-\ngressor from Swanson et al. (2015), which gives a score to each sentence. The AQ score is intended to reflect how easily the speaker\u2019s argument can be understood from the sentence without any context. Easily understandable sentences are assumed to be prime candidates for producing extractive summaries. In Swanson et al. (2015), the annotators rated AQ using a continuous slider ranging from hard (0.0) to easy to interpret (1.0). We refined the Mechanical Turk task to elicit new training data for AQ as summarized in Table 1. Fig. 8 in the appendix shows the HIT we used to collect new AQ labels for sentences, as described below.\nWe expected to to apply Swanson\u2019s AQ regressor to our sample completely \u201cout of the box\u201d. However, we first discovered that many sentences given high AQ scores were very similar, while we need a sample that realizes many diverse facets. We then discovered that some extracted sentential arguments were not actually high quality. We hypothesized that the diversity issue arose primarily because Swanson\u2019s dataset was filtered using high PMI n-grams. We also hypothesized that the quality issue had not surfaced because Swanson\u2019s sample was primarily selected from sentences marked with the discourse connectives but, first, if, and so. Our sample (Original column of Table 1) is much larger and was not similarly filtered.\nFig. 4 plots the distribution of word counts for sentences from our sample that were given an AQ score > 0.91 by Swanson\u2019s trained AQ regressor. The first bin shows that many sentences with\nless than 10 words are predicted to be high quality, but many of these sentences in our data consisted of only a few elongated words (e.g. HAHAHAHA...). The upper part of the distribution shows a large number of sentences with more than 70 words with a predicted AQ > 0.91. We discovered that most of these long sentences are multiple sentences without punctuation. We thus refined the AQ model by removing duplicate sentences, and rescoring sentences without a verb and with less than 4 dictionary words to AQ = 0. We then restricted our sampling to sentences between 10 and 40 tokens, to eliminate run-on sentences and sentences without much propositional content. We did not retrain the regressor, rather we resampled and rescored the corpus. See the Rescored column of Table 1. After removing the two tails in Fig. 4, the distribution of word counts is almost uniform across bins of sentences from length 10 to 40.\nAs noted above, the sample in Swanson et al. (2015) was filtered using PMI, and PMI contributes to AQ. Thus, to end up with a diverse set of sentences representing many facets of each topic, we decided to sample sentences with lower AQ scores than Swanson had used. We binned the sentences based on predicted AQ score and extracted random samples across bins ranging from .55\u20131.0, in increments of .10. Then we extracted a smaller sample and collected new AQ annotations for gay marriage and death penalty on Mechanical Turk, using the definitions in Fig. 8 (in the appendix). See the Sampled column of Table 1. We pre-selected three annotators using a qualifier that included detailed instructions and sample annotations. A score of 3 was mapped to a yes and scores of 1 or 2 mapped to a no. We simplified the task slightly in the HIT for gun control, where five annotators were instructed to select a yes label if the sentence clearly expressed an argument (score 3), or a no label otherwise (score 1 or 2).\nWe then calculated the probability that the sentences in each bin were high quality arguments using the resulting AQ gold standard labels, and found that a threshhold of predicted AQ > 0.55 maintained both diversity and quality. See Fig. 9 in the appendix. Table 1 summarizes the results of each stage of the process of producing the new AQ corpus of 6188 sentences (Sampled and then annotated). The last column of Table 1 shows that gold standard labels agree with the rescored AQ regressor between 77% and 88% of the time."}, {"heading": "2.2 Argument Facet Similarity Data", "text": "The goal of Task2 is to define a similarity metric and train a regression model that takes as input two sentential arguments and returns a scalar value that predicts their similarity(AFS). The model must reflect the fact that similarity is graded, e.g. the same argument facet may be repeated with different levels of explicit detail. For example, sentence A1 in Fig. 2 is similar to the more complete argument, Given the fact that guns are weapons\u2014things designed to kill\u2014they should not be in the hands of the public, which expresses both the premise and conclusion. Sentence A1 leaves it up to the reader to infer the (obvious) conclusion.\nOur approach to Task2 draws strongly on recent work on semantic textual similarity (STS) (Agirre et al., 2013; Dolan and Brockett, 2005; Mihalcea et al., 2006). STS measures the degree of semantic similarity between a pair of sentences with values that range from 0 to 5. Inspired by the scale used for STS, we first define what a facet is, and then define the values of the AFS scale as shown in Fig. 10 in the appendix (repeated from Misra et al. (2015) for convenience). We distinguish AFS from STS because: (1) our data are so different: STS data consists of descriptive sentences whereas our sentences are argumentative excerpts from dialogs; and (2) our definition of facet allows for sentences that express opposite stance to be realizations of the same facet (AFS = 3) in Fig. 10.\nRelated work has primarily used entailment or semantic equivalence to define argument similarity (Habernal and Gurevych, 2015; Boltuzic and S\u030cnajder, 2015; Boltuzic and S\u030cnajder, 2015; Habernal et al., 2014). We believe the definition of AFS given in Fig. 10 will be more useful in the long run than semantic equivalence or entailment, because two arguments can only be contradictory if they are about the same facet. For example, consider that sentential argument S7 in Fig. 5 is anti gun-control, while sentences S8 and S9 are pro gun-control. Our annotation guidelines label them with the same facet, in a similar way to how the\ncurated summaries on ProCon provides both a Pro and Con side for each facet. See Fig. 3.\nIn order to efficiently collect annotations for AFS, we want to produce training data pairs that are more likely than chance to be the same facet (scores 3 and above as defined in Fig. 10). Similar arguments are rare with an all-pairs matching protocol, e.g. in ComArg approximately 67% of the annotations are \u201cnot a match\u201d (Boltuzic and S\u030cnajder, 2014). Also, we found that Turkers are confused when asked to annotate similarity and then given a set of sentence pairs that are almost all highly dissimilar. Annotations also cost money. We therefore used UMBC STS (Han et al., 2013) to score all potential pairs.5 To foreshadow, the plot in Fig. 6 shows that this pre-scoring works: (1) the lower quadrant of the plot shows that STS < .20 corresponds to the lower range of scores for AFS; and (2) the lower half of the left hand side shows that we still get many arguments that are low AFS (values below 3) in our training data.\nWe selected 2000 pairs in each topic, based on their UMBC similarity scores, which resulted in lowest UMBC scores of 0.58 for GM, 0.56 for GC and 0.58 for DP. To ensure a pool of diverse arguments, a particular sentence can appear in at most ten pairs. MT workers took a qualification test with definitions and instructions as shown in Fig. 10. Sentential arguments with sample AFS annotations were part of the qualifier. The 6000 pairs were made available to our three most reliable pre-qualified workers. The last row of Table 3 reports the human topline for the task, i.e. the average pairwise r across all three workers. Interestingly, the Gay marriage topic (r = 0.60) is more difficult for human annotators than either Death Penalty (r = 0.74) or Gun Control (r = 0.69).\n5This is an off-the-shelf STS tool from University of Maryland Baltimore County available at swoogle.umbc.edu/SimService/."}, {"heading": "3 Argument Facet Similarity", "text": "Given the data collected above, we defined a supervised machine learning experiment with AFS as our dependent variable. We developed a number of baselines using off the shelf tools. Features are grouped into sets and discussed in detail below."}, {"heading": "3.1 Feature Sets", "text": "NGRAM cosine. Our primary baseline is an ngram overlap feature. For each argument, we extract the unigrams, bigrams and trigrams, and then calculate the cosine similarity between two texts represented as vectors of their ngram counts. Rouge. Rouge is a family of metrics for comparing the similarity of two summaries (Lin, 2004), which measures overlapping units such as continuous and skip ngrams, common subsequences, and word pairs. We use all the rouge f-scores from the pyrouge package. Our analysis shows that rouge s* f score correlates most highly with AFS.6 UMBC STS. We consider STS, a measure of the semantic similarity of two texts (Agirre et al., 2012), as another baseline, using the UMBC STS tool. Fig. 6 illustrates that in general, STS is rough approximation of AFS. It is possible that our selection of data for pairs for annotation using UMBC STS either improves or reduces its performance. Google Word2Vec. Word embeddings from word2vec (Mikolov et al., 2013) are popular for expressing semantic relationships between words, but using word embeddings to express entire sentences often requires some compromises. In particular, averaging word2vec embeddings for each word may lose too much information in long sentences. Previous work on argument mining has developed methods using word2vec that are effective for clustering similar arguments (Habernal and Gurevych, 2015; Boltuzic and S\u030cnajder, 2015) Other research creates embeddings at the sentence level using more advanced techniques such as Paragraph Vectors (Le and Mikolov, 2014).\nWe take a more direct approach in which we use the word embeddings directly as features. For each sentential argument in the pair, we create a 300-dimensional vector by filtering for stopwords and punctuation and then averaging the word embeddings from Google\u2019s word2vec model for the remaining words.7 Each dimension of the 600 dimensional concatenated averaged vector is used directly as a feature. In our experiments, this\n6https://pypi.python.org/pypi/pyrouge/ 7https://code.google.com/archive/p/\nword2vec/\nconcatenation method greatly outperforms cosine similarity (Table 2, Table 3). Sec. 3.3 discusses properties of word embeddings that may yield these performance differences. Custom Word2Vec. We also create our own 300-dimensional embeddings for our dialogic domain using the Gensim library (R\u030cehu\u030ar\u030cek and Sojka, 2010), with default settings, and a very large corpus of user-generated dialogic content. This includes the corpus described in Sec. 2 (929, 206 forum posts), an internal corpus of 1, 688, 639 tweets on various topics, and a corpus of 53, 851, 542 posts from Reddit.8 LIWC category and Dependency Overlap. Both dependency structures and the Linguistics Inquiry Word Count (LIWC) tool have been useful in previous work (Pennebaker et al., 2001; Somasundaran and Wiebe, 2009; Hasan and Ng, 2013). We develop a novel feature set that combines LIWC category and dependency overlap, aiming to capture a generalized notion of concept overlap between two arguments, i.e. to capture the hypothesis that classes of content words such as affective processes or emotion types are indicative of a shared facet across pairs of arguments.\nWe create partially generalized LIWC dependency features and count overlap normalized by sentence length across pairs, building on previous work (Joshi and Penstein-Rose\u0301, 2009). Stanford dependency features (Manning et al., 2014) are generalized by leaving one dependency element lexicalized, replacing the other word in the dependency relation with its LIWC category and by removing the actual dependency type (nsubj, dobj, etc.) from the triple. This creates a tuple of (\u201cgovernor token\u201d, LIWC category of dependent token). We call these simplified LIWC dependencies.\nFig. 7 illustrates the generalization process for three LIWC simplified dependencies, (\u201ddeter\u201d, \u201dfear), (\u201ddeter\u201d, \u201dpunishment\u201d), and (\u201ddeter\u201d, \u201dlove\u201d). Because LIWC is a hierarchical lexicon,\n8One month sample https://www.reddit.com/ r/datasets/comments/3bxlg7/i_have_every_ publicly_available_reddit_comment\ntwo dependencies may share many generalizations or only a few. Here, the tuples with dependent tokens fear and punishment are more closely related because their shared generalization include both Negative Emotion and Affective Processes, but the tuples with dependent tokens fear and love have a less similar relationship, because they only share the Affective Processes generalization."}, {"heading": "3.2 Machine Learning Regression Results", "text": "We randomly selected 90% of our annotated pairs to use for nested 10-fold cross-validation, setting aside 10% for qualitative analysis of predicted vs. gold-standard scores. We use Ridge Regression (RR) with l2-norm regularization and Support Vector Regression (SVR) with an RBF kernel from scikit-learn (Pedregosa et al., 2011). Performance evaluation uses two standard measures, Correlation Coefficient (r) and Root Mean Squared Error (RMSE). A separate inner crossvalidation within each fold of the outer crossvalidation is used to perform a grid search to determine the hyperparameters for that outer fold. The outer cross-validation reports the scoring metrics. Simple Ablation Models. We first evaluate simple models based on a single feature using both RR and SVR. Table 2, Rows 1, 2, and 3 show the baseline results: UMBC Semantic Textual Similarity (STS), Ngram Cosine, and Rouge. Surprisingly, the UMBC STS measure does not perform as well as Ngram Cosine for Death Penalty and Gay Marriage. LIWC dependencies (Row 4) perform similarly to Rouge (Row 3) across topics. Cosine similarity for the custom word2vec model (Row 5) performs about as well or better than ngrams across topics, but cosine similarity using the Google model (Row 6) performs worse than ngrams for all topics except Death Penalty. Interestingly our custom Word2Vec models perform significantly better than the Google word2vec models for Gun Control and Gay Marriage, with both much higher r and lower RMSE, while performing identically for Death Penalty. Feature Combination Models. Table 3 shows the results of testing feature combinations to learn which ones are complementary. Since SVR consistently performs better than RR, we use SVR only. Significance is calculated using paired t-tests between the RMSE values across folds. We paired Ngrams separately with LIWC and ROUGE to evaluate if the combination is significant. Ngram+Rouge (Row 1) is significantly better than Ngram for Gun Control and Death Penalty (p < .01), and Gay Marriage (p = .03).\nNgram+LIWC (Row 2) is significantly better than Ngram for Gun Control, and Death Penalty (p < .01). Thus both Rouge and LIWC provide complementary information to Ngrams.\nOur best result using our hand-engineered features is a combination of LIWC, Rouge, and Ngrams (Row 3). Interestingly, adding UMBC STS (Row 4) gives a small but significant improvement (p < 0.01 for gun control; p = 0.07 for gay marriage). Thus we take Ngrams, LIWC, Rouge, and UMBC STS (Row 4) as our best handengineered model across all topics with a correlation of 0.65 for gun control, 0.50 for death penalty and 0.40 for gay marriage. This combination is significantly better than the baselines for Ngram baseline (p < .01), UMBC STS (p <= .02) and Rouge (p < .01) for all three topics.\nWe then further combine the hand-engineered features (Row 4) with the Google Word2Vec features (Row 6), creating the model in Row 8. A paired t-test between RMSE values from each cross-validation fold for each model (Row 4 vs. Row 8 and Row 6 vs. Row 8) shows that the our hand-engineered features are complementary to Word2Vec, and their combination yields a model significantly better than either model alone (p <\n.01). We note that although the custom word2vec model performs much better for gun control and gay marriage when using cosine, it actually performs slightly but significantly (p = .05) worse when using concatenation with hand-engineered features. This may simply be due to the size of the training data, i.e. the Google model used nearly twice as much training data, while our domain-specific word2vec model achieves comparable performance to the Google model with much less training data."}, {"heading": "3.3 Analysis and Discussion", "text": "Although it is common to translate word embeddings into single features or reduced feature sets for similarity through the use of clustering (Habernal and Gurevych, 2015) or cosine similarity (Boltuzic and S\u030cnajder, 2015), we show that it is possible to improve results by directly combining word embeddings with hand-engineered features. In our task, sentences were limited to a maximum of 40 tokens in order to encourage singlefacet sentences, but this may have provided an additional benefit by allowing us to average word embeddings while still preserving useful signal.\nOur results also demonstrate that using concate-\nnation for learning similarity with vector representations works much better than the common practice of reducing a pair of vectors to a single score using cosine similarity. Previous work (Li et al., 2015; Pennington et al., 2014) also shows that all dimensions are not equally useful predictors for a specific task. For sentiment classification, Li et al. (2015) find that \u201ctoo large a dimensionality leads many dimensions to be non-functional ... causing two sentences of opposite sentiment to differ only in a few dimensions.\u201d This may also be the situation for the 300-dimensional embeddings used for AFS. Hence, when using concatenation, single dimensions can be weighted to adjust for non-functional dimensions, but using cosine makes this per-dimension weighting impossible. This might explain why our custom word2vec model outperforms the Google model when using cosine as compared to concatenation, i.e. more dimensions are informative in the custom model, but overall, the Google model provides more complementary information when non-functional dimensions are accounted for. More analysis is needed to fully support this claim.\nTo qualitatively illustrate some of the differences between our final AFS regressor model (Row 8 of Table 3) and several baselines, we apply the model to a set-aside 200 pairs per topic. Table 4 shows examples selected to highlight the strengths of AFS prediction for different models as compared to the AFS gold standard scores.\nMT AFS values near 1 indicate same topic but no similarity. Rows GC1 and DP2 talk about totally different facets and only share the same topic (AFS = 1). Rouge and Ngram features based on\nword overlap predict scores that are too high. In contrast, LIWC dependencies and word2vec based on concept and semantic overlap are more accurate. MT values near 3 indicate same facet but somewhat different arguments. Arguments in row GM4 talk about marriage rights to all, and there is some overlap in these arguments beyond simply being the same topic, however the speakers are on opposite stance sides. Both of the arguments in row GM5 (MT AFS of 3.3) reference the same facet of the financial and legal benefits available to married couples, but Arg2 is more specific. Both Word2vec and our trained AFS model can recognize the similarity in the concepts in the two arguments and make good predictions.\nMT values above 4 indicate two arguments that are the same facet and very similar. Row DP6 gets a high Rouge overlap score and Word2vec relates \u2018lower crime rate\u2019 as semantically similar to \u2018deter murder rates\u2019 thus yielding an accurately high AFS score. DP7 is an example where LIWC dependencies perform better as compared to other features, because it focuses in on the dependency between the death penalty and cost, but none of the models do well at predicting the MT AFS score. One issue here may be that, despite our attempts to sample pairs with more representatives of high AFS, there is just less training data available for this part of the distribution. Hence all the regressors will be conservative at predicting the highest values. We hope in future work to improve our AFS regressor by finding additional methods for populating the training data with more highly similar pairs."}, {"heading": "4 Related Work", "text": "There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task. Social media arguments are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012).\nMoreover, in social media, segments of text that are argumentative must first be identified, as in our Task1. Habernal and Gurevych (2016) train a classifier to recognize text segments that are argumentative, but much previous work does Task1 manually. Goudas et al. (2014) annotate 16,000 sentences from social media documents and consider 760 of them to be argumentative. Hasan and Ng (2014) also manually identify argumentative sentences, while Boltuzic and S\u030cnajder (2014) treat the whole post as argumentative, after manually removing \u201cspam\u201d posts. Biran and Rambow (2011) automatically identify justifications as a structural component of an argument.\nOther work groups semantically-similar classes of reasons or frames that underlie a particular speaker\u2019s stance, what we call ARGUMENT FACETS. One approach categorizes sentences or posts using topic-specific argument labels, which are functionally similar to our facets as discussed above (Conrad et al., 2012; Hasan and Ng, 2014; Boltuzic and S\u030cnajder, 2014; Naderi and Hirst, 2015). For example, Fig. 2 lists facets A1 to A8 for Gun Control from the IDebate website; Boltuzic and S\u030cnajder (2015) use this list to label posts. They apply unsupervised clustering using a semantic textual similarity tool, but evaluate clusters using their hand-labelled argument tags. Our method instead explicitly models graded similarity of sentential arguments."}, {"heading": "5 Conclusion and Future Work", "text": "We present a method for scoring argument facet similarity in online debates using a combination of hand-engineered and unsupervised features with a correlation averaging 0.63 compared to a human top line averaging 0.68. Our approach differs from similar work that finds and groups the \u201creasons\u201d underlying a speakers stance, because our models are based on the belief that it is not possible to define a finite set of discrete facets for a topic. A qualitative analysis of our results, illustrated by\nTable 4, suggests that treating facet discovery as a similarity problem is productive, i.e. examination of particular pairs suggests facets about legal and financial benefits for same-sex couples, the claim that the death penalty does not actually affect murder rates, and an assertion that \u201cthey\u201d, implying \u201ccongress\u201d, do not have the express, enumerated power to pass legislation restricting guns.\nPrevious work shows that metrics used for evaluating machine translation quality perform well on paraphrase recognition tasks (Madnani et al., 2012). In our experiments, ROUGE performed very well, suggesting that other machine translation metrics such as Terp and Meteor may be useful (Snover et al., 2009; Lavie and Denkowski, 2009). We will explore this in future work.\nIn future, we will use our AFS regressor to cluster and group similar arguments and produce argument facet summaries as a final output of our pipeline. Habernal and Gurevych (2015) apply clustering in argument mining by averaging word embeddings from posts and sentences from debate portals, clustering the resulting averaged vectors, and then computing distance measures from clusters to unseen sentences (\u201cclassification units\u201d) as features. Cosine similarity between weighted and summed vector representations is also a common approach, and Boltuzic and S\u030cnajder (2015) show word2vec cosine similarity beats bag-ofwords and STS baselines when used with clustering for argument identification.\nFinally, our AQ extractor treats all posts on a topic equally, operating on a set of concatenated posts. We will explore other sampling methods to ensure that the AQ extractor does not eliminate arguments made by less articulate citizens, by e.g. enforcing that \u201cEvery speaker in a debate contributes at least one argument\u201d. We will also sample by stance-side, so that summaries can be organized using \u201cPro\u201d and \u201cCon\u201d, as in curated summaries. Our final goal is to combine quality-based argument extraction, our AFS model, stance, post and author level information, so that our summaries represent the diversity of views on a topic, a quality not always guaranteed by summarization techniques, human or machine."}, {"heading": "Acknowledgments", "text": "This work was supported by NSF CISE RI 1302668. Thanks to Keshav Mathur and the three anonymous reviewers for helpful comments."}, {"heading": "A Appendix", "text": "Figure 8 shows the definitions used in our Argument Quality HIT.\nFigure 9 shows the relation between predicted AQ score and gold-standard argument quality annotations.\nFigure 10 provides our definition of FACET and instructions for AFS annotation. This is repeated here from (Misra et al., 2015) for the reader\u2019s convenience."}], "references": [{"title": "Internet argument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it", "author": ["Robert Abbott", "Brian Ecker", "Pranav Anand", "Marilyn Walker"], "venue": "In Language Resources and Evaluation Conference,", "citeRegEx": "Abbott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abbott et al\\.", "year": 2016}, {"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."], "venue": "Proc. of the First Joint Conference on Lexical and Computational Semantics, volume 1, pages 385\u2013393.", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo."], "venue": "In* SEM 2013: The Second Joint Conference on Lexical and Computational Se-", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "Identifying justifications in written dialogs", "author": ["Oram Biran", "Owen Rambow."], "venue": "2011 Fifth IEEE International Conference on Semantic Computing (ICSC), pages 162\u2013168.", "citeRegEx": "Biran and Rambow.,? 2011", "shortCiteRegEx": "Biran and Rambow.", "year": 2011}, {"title": "Back up your stance: Recognizing arguments in online discussions", "author": ["Filip Boltuzic", "Jan \u0160najder."], "venue": "Proc. of the First Workshop on Argumentation Mining, pages 49\u201358.", "citeRegEx": "Boltuzic and \u0160najder.,? 2014", "shortCiteRegEx": "Boltuzic and \u0160najder.", "year": 2014}, {"title": "Identifying prominent arguments in online debates using semantic textual similarity", "author": ["Filip Boltuzic", "Jan \u0160najder."], "venue": "Proc. of the Second Workshop on Argumentation Mining.", "citeRegEx": "Boltuzic and \u0160najder.,? 2015", "shortCiteRegEx": "Boltuzic and \u0160najder.", "year": 2015}, {"title": "Combining textual entailment and argumentation theory for supporting online debates interactions", "author": ["Elena Cabrio", "Serena Villata."], "venue": "Proc. of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages", "citeRegEx": "Cabrio and Villata.,? 2012", "shortCiteRegEx": "Cabrio and Villata.", "year": 2012}, {"title": "Recognizing arguing subjectivity and argument tags", "author": ["Alexander Conrad", "Janyce Wiebe"], "venue": "In Proc. of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,", "citeRegEx": "Conrad and Wiebe,? \\Q2012\\E", "shortCiteRegEx": "Conrad and Wiebe", "year": 2012}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["William B Dolan", "Chris Brockett."], "venue": "Proc. of IWP.", "citeRegEx": "Dolan and Brockett.,? 2005", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games* 1", "author": ["P.M. Dung."], "venue": "Artificial intelligence, 77(2):321\u2013357.", "citeRegEx": "Dung.,? 1995", "shortCiteRegEx": "Dung.", "year": 1995}, {"title": "Analyzing argumentative discourse units in online interactions", "author": ["Debanjan Ghosh", "Smaranda Muresan", "Nina Wacholder", "Mark Aakhus", "Matthew Mitsui."], "venue": "ACL 2014, page 39.", "citeRegEx": "Ghosh et al\\.,? 2014", "shortCiteRegEx": "Ghosh et al\\.", "year": 2014}, {"title": "Coalescent argumentation", "author": ["Michael A. Gilbert"], "venue": null, "citeRegEx": "Gilbert.,? \\Q1997\\E", "shortCiteRegEx": "Gilbert.", "year": 1997}, {"title": "Argument extraction from news, blogs, and social media", "author": ["Theodosis Goudas", "Christos Louizos", "Georgios Petasis", "Vangelis Karkaletsis."], "venue": "Artificial Intelligence: Methods and Applications, pages 287\u2013299. Springer.", "citeRegEx": "Goudas et al\\.,? 2014", "shortCiteRegEx": "Goudas et al\\.", "year": 2014}, {"title": "Exploiting debate portals for semi-supervised argumentation mining in user-generated web discourse", "author": ["Ivan Habernal", "Iryna Gurevych."], "venue": "Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages", "citeRegEx": "Habernal and Gurevych.,? 2015", "shortCiteRegEx": "Habernal and Gurevych.", "year": 2015}, {"title": "Argumentation mining in user-generated web discourse", "author": ["Ivan Habernal", "Iryna Gurevych."], "venue": "CoRR, abs/1601.02403.", "citeRegEx": "Habernal and Gurevych.,? 2016", "shortCiteRegEx": "Habernal and Gurevych.", "year": 2016}, {"title": "Argumentation mining on the web from information seeking perspective", "author": ["Ivan Habernal", "Judith Eckle-Kohler", "Iryna Gurevych."], "venue": "Proc. of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Pro-", "citeRegEx": "Habernal et al\\.,? 2014", "shortCiteRegEx": "Habernal et al\\.", "year": 2014}, {"title": "Umbc ebiquitycore: Semantic textual similarity systems", "author": ["Lushan Han", "Abhay Kashyap", "Tim Finin", "James Mayfield", "Jonathan Weese."], "venue": "Proceedings of the Second Joint Conference on Lexical and Computational Semantics, pages 44\u201352.", "citeRegEx": "Han et al\\.,? 2013", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Frame semantics for stance classification", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "CoNLL, pages 124\u2013132.", "citeRegEx": "Hasan and Ng.,? 2013", "shortCiteRegEx": "Hasan and Ng.", "year": 2013}, {"title": "Why are you taking this stance? identifying and classifying reasons in ideological debates", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Hasan and Ng.,? 2014", "shortCiteRegEx": "Hasan and Ng.", "year": 2014}, {"title": "Structure of conversational argument: Pragmatic bases for the enthymeme", "author": ["Sally Jackson", "Scott Jacobs."], "venue": "Quarterly Journal of Speech, 66(3):251\u2013265.", "citeRegEx": "Jackson and Jacobs.,? 1980", "shortCiteRegEx": "Jackson and Jacobs.", "year": 1980}, {"title": "Generalizing dependency features for opinion mining", "author": ["M. Joshi", "C. Penstein-Ros\u00e9."], "venue": "Proc. of the ACL-IJCNLP 2009 Conference Short Papers, pages 313\u2013316.", "citeRegEx": "Joshi and Penstein.Ros\u00e9.,? 2009", "shortCiteRegEx": "Joshi and Penstein.Ros\u00e9.", "year": 2009}, {"title": "The meteor metric for automatic evaluation of machine translation", "author": ["Alon Lavie", "Michael J. Denkowski."], "venue": "Machine Translation, 23(2-3):105\u2013115, September.", "citeRegEx": "Lavie and Denkowski.,? 2009", "shortCiteRegEx": "Lavie and Denkowski.", "year": 2009}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Tony Jebara and Eric P. Xing, editors, Proc. of the 31st International Conference on Machine Learning (ICML14), pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1506.01066.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Rouge: A package for automatic evaluation of summaries rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin."], "venue": "Proc. of the Workshop on Text Summarization Branches Out (WAS 2004).", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Nitin Madnani", "Joel Tetreault", "Martin Chodorow."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Madnani et al\\.,? 2012", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proc. of 52nd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava."], "venue": "AAAI, volume 6, pages 775\u2013780.", "citeRegEx": "Mihalcea et al\\.,? 2006", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Using summarization to discover argument facets in dialog", "author": ["Amita Misra", "Pranav Anand", "Jean E. Fox Tree", "Marilyn Walker."], "venue": "Proc. of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Misra et al\\.,? 2015", "shortCiteRegEx": "Misra et al\\.", "year": 2015}, {"title": "Argumentation mining in parliamentary discourse", "author": ["Nona Naderi", "Graeme Hirst."], "venue": "CMNA 2015: 15th workshop on Computational Models of Natural Argument.", "citeRegEx": "Naderi and Hirst.,? 2015", "shortCiteRegEx": "Naderi and Hirst.", "year": 2015}, {"title": "Evaluating content selection in summarization: The pyramid method", "author": ["Ani Nenkova", "Rebecca Passonneau."], "venue": "Proc. of Joint Annual Meeting of Human Language Technology and the North American chapter of the Association for Computational", "citeRegEx": "Nenkova and Passonneau.,? 2004", "shortCiteRegEx": "Nenkova and Passonneau.", "year": 2004}, {"title": "Ranking the annotators: An agreement study on argumentation structure", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Proc. of the 7th linguistic annotation workshop and interoperability with discourse, pages 196\u2013204.", "citeRegEx": "Peldszus and Stede.,? 2013", "shortCiteRegEx": "Peldszus and Stede.", "year": 2013}, {"title": "LIWC: Linguistic Inquiry and Word Count", "author": ["James W. Pennebaker", "L.E. Francis", "R.J. Booth"], "venue": null, "citeRegEx": "Pennebaker et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Pennebaker et al\\.", "year": 2001}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Araucaria: Software for argument analysis, diagramming and representation", "author": ["Chris Reed", "Glenn Rowe."], "venue": "International Journal on Artificial Intelligence Tools, 13(04):961\u2013979.", "citeRegEx": "Reed and Rowe.,? 2004", "shortCiteRegEx": "Reed and Rowe.", "year": 2004}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proc. of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric", "author": ["Matthew Snover", "Nitin Madnani", "Bonnie Dorr", "Richard Schwartz."], "venue": "Proceedings of the Fourth Workshop on Statistical Machine Translation, pages", "citeRegEx": "Snover et al\\.,? 2009", "shortCiteRegEx": "Snover et al\\.", "year": 2009}, {"title": "Recognizing stances in online debates", "author": ["Swapna Somasundaran", "Janyce Wiebe."], "venue": "Proc. of the 47th Annual Meeting of the ACL, pages 226\u2013234.", "citeRegEx": "Somasundaran and Wiebe.,? 2009", "shortCiteRegEx": "Somasundaran and Wiebe.", "year": 2009}, {"title": "Annotating argument components and relations in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych."], "venue": "Proc. of the 25th International Conference on Computational Linguistics (COLING 2014), pages 1501\u20131510.", "citeRegEx": "Stab and Gurevych.,? 2014", "shortCiteRegEx": "Stab and Gurevych.", "year": 2014}, {"title": "Argument mining: Extracting arguments from online dialogue", "author": ["Reid Swanson", "Brian Ecker", "Marilyn Walker."], "venue": "Proc. of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 217\u2013226.", "citeRegEx": "Swanson et al\\.,? 2015", "shortCiteRegEx": "Swanson et al\\.", "year": 2015}, {"title": "The Uses of Argument", "author": ["Stephen E. Toulmin."], "venue": "Cambridge University Press.", "citeRegEx": "Toulmin.,? 1958", "shortCiteRegEx": "Toulmin.", "year": 1958}, {"title": "Argumentation Schemes", "author": ["Douglas Walton", "Chris Reed", "Fabrizio Macagno."], "venue": "Cambridge University Press.", "citeRegEx": "Walton et al\\.,? 2008", "shortCiteRegEx": "Walton et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "0 corpus of online dialogs (Abbott et al., 2016).", "startOffset": 27, "endOffset": 48}, {"referenceID": 29, "context": "extends our previous work which frames our goal as consisting of two tasks (Misra et al., 2015; Swanson et al., 2015).", "startOffset": 75, "endOffset": 117}, {"referenceID": 40, "context": "extends our previous work which frames our goal as consisting of two tasks (Misra et al., 2015; Swanson et al., 2015).", "startOffset": 75, "endOffset": 117}, {"referenceID": 40, "context": "Our previous work on Argument Extraction achieved good results, (Swanson et al., 2015), and is extended here (Sec.", "startOffset": 64, "endOffset": 86}, {"referenceID": 18, "context": "tences using these facets, and trains a classifier to return a facet label (Conrad et al., 2012; Hasan and Ng, 2014; Boltuzic and \u0160najder, 2014; Naderi and Hirst, 2015), inter alia.", "startOffset": 75, "endOffset": 168}, {"referenceID": 4, "context": "tences using these facets, and trains a classifier to return a facet label (Conrad et al., 2012; Hasan and Ng, 2014; Boltuzic and \u0160najder, 2014; Naderi and Hirst, 2015), inter alia.", "startOffset": 75, "endOffset": 168}, {"referenceID": 30, "context": "tences using these facets, and trains a classifier to return a facet label (Conrad et al., 2012; Hasan and Ng, 2014; Boltuzic and \u0160najder, 2014; Naderi and Hirst, 2015), inter alia.", "startOffset": 75, "endOffset": 168}, {"referenceID": 29, "context": "In our previous work on AFS, we developed an AFS regressor for predicting the similarity of human-generated labels for summaries of dialogic arguments (Misra et al., 2015).", "startOffset": 151, "endOffset": 171}, {"referenceID": 31, "context": "We collected 5 human summaries of each dialog, and then used the Pyramid tool and scheme to annotate sentences from these summaries as contributors to (paraphrases of) a particular facet (Nenkova and Passonneau, 2004).", "startOffset": 187, "endOffset": 217}, {"referenceID": 0, "context": "0 (Abbott et al., 2016).", "startOffset": 2, "endOffset": 23}, {"referenceID": 40, "context": "gressor from Swanson et al. (2015), which gives a score to each sentence.", "startOffset": 13, "endOffset": 35}, {"referenceID": 40, "context": "gressor from Swanson et al. (2015), which gives a score to each sentence. The AQ score is intended to reflect how easily the speaker\u2019s argument can be understood from the sentence without any context. Easily understandable sentences are assumed to be prime candidates for producing extractive summaries. In Swanson et al. (2015), the annotators rated AQ using a continuous slider ranging from hard (0.", "startOffset": 13, "endOffset": 329}, {"referenceID": 40, "context": "As noted above, the sample in Swanson et al. (2015) was filtered using PMI, and PMI contributes to AQ.", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": ", 2013; Dolan and Brockett, 2005; Mihalcea et al., 2006). STS measures the degree of semantic similarity between a pair of sentences with values that range from 0 to 5. Inspired by the scale used for STS, we first define what a facet is, and then define the values of the AFS scale as shown in Fig. 10 in the appendix (repeated from Misra et al. (2015) for convenience).", "startOffset": 8, "endOffset": 353}, {"referenceID": 13, "context": "Related work has primarily used entailment or semantic equivalence to define argument similarity (Habernal and Gurevych, 2015; Boltuzic and \u0160najder, 2015; Boltuzic and \u0160najder, 2015; Habernal et al., 2014).", "startOffset": 97, "endOffset": 205}, {"referenceID": 5, "context": "Related work has primarily used entailment or semantic equivalence to define argument similarity (Habernal and Gurevych, 2015; Boltuzic and \u0160najder, 2015; Boltuzic and \u0160najder, 2015; Habernal et al., 2014).", "startOffset": 97, "endOffset": 205}, {"referenceID": 5, "context": "Related work has primarily used entailment or semantic equivalence to define argument similarity (Habernal and Gurevych, 2015; Boltuzic and \u0160najder, 2015; Boltuzic and \u0160najder, 2015; Habernal et al., 2014).", "startOffset": 97, "endOffset": 205}, {"referenceID": 15, "context": "Related work has primarily used entailment or semantic equivalence to define argument similarity (Habernal and Gurevych, 2015; Boltuzic and \u0160najder, 2015; Boltuzic and \u0160najder, 2015; Habernal et al., 2014).", "startOffset": 97, "endOffset": 205}, {"referenceID": 16, "context": "We therefore used UMBC STS (Han et al., 2013) to score all potential pairs.", "startOffset": 27, "endOffset": 45}, {"referenceID": 24, "context": "Rouge is a family of metrics for comparing the similarity of two summaries (Lin, 2004), which measures overlapping units such as continuous and skip ngrams, common subsequences, and word pairs.", "startOffset": 75, "endOffset": 86}, {"referenceID": 1, "context": "We consider STS, a measure of the semantic similarity of two texts (Agirre et al., 2012), as another baseline, using the UMBC STS tool.", "startOffset": 67, "endOffset": 88}, {"referenceID": 28, "context": "Word embeddings from word2vec (Mikolov et al., 2013) are popular for expressing semantic relationships between words, but using word embeddings to express entire sentences often requires some compromises.", "startOffset": 30, "endOffset": 52}, {"referenceID": 13, "context": "Previous work on argument mining has developed methods using word2vec that are effective for clustering similar arguments (Habernal and Gurevych, 2015; Boltuzic and \u0160najder, 2015) Other research creates embeddings at the sen-", "startOffset": 122, "endOffset": 179}, {"referenceID": 5, "context": "Previous work on argument mining has developed methods using word2vec that are effective for clustering similar arguments (Habernal and Gurevych, 2015; Boltuzic and \u0160najder, 2015) Other research creates embeddings at the sen-", "startOffset": 122, "endOffset": 179}, {"referenceID": 22, "context": "tence level using more advanced techniques such as Paragraph Vectors (Le and Mikolov, 2014).", "startOffset": 69, "endOffset": 91}, {"referenceID": 36, "context": "We also create our own 300-dimensional embeddings for our dialogic domain using the Gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010), with default settings, and a very large corpus of user-generated dialogic content.", "startOffset": 99, "endOffset": 124}, {"referenceID": 33, "context": "ful in previous work (Pennebaker et al., 2001; Somasundaran and Wiebe, 2009; Hasan and Ng, 2013).", "startOffset": 21, "endOffset": 96}, {"referenceID": 38, "context": "ful in previous work (Pennebaker et al., 2001; Somasundaran and Wiebe, 2009; Hasan and Ng, 2013).", "startOffset": 21, "endOffset": 96}, {"referenceID": 17, "context": "ful in previous work (Pennebaker et al., 2001; Somasundaran and Wiebe, 2009; Hasan and Ng, 2013).", "startOffset": 21, "endOffset": 96}, {"referenceID": 20, "context": "We create partially generalized LIWC dependency features and count overlap normalized by sentence length across pairs, building on previous work (Joshi and Penstein-Ros\u00e9, 2009).", "startOffset": 145, "endOffset": 176}, {"referenceID": 26, "context": "Stanford dependency features (Manning et al., 2014) are generalized by leaving one dependency element lexicalized, replacing the other word in the dependency relation with its LIWC category and by removing the actual dependency type (nsubj, dobj,", "startOffset": 29, "endOffset": 51}, {"referenceID": 13, "context": "Although it is common to translate word embeddings into single features or reduced feature sets for similarity through the use of clustering (Habernal and Gurevych, 2015) or cosine similarity (Boltuzic and \u0160najder, 2015), we show that it", "startOffset": 141, "endOffset": 170}, {"referenceID": 5, "context": "Although it is common to translate word embeddings into single features or reduced feature sets for similarity through the use of clustering (Habernal and Gurevych, 2015) or cosine similarity (Boltuzic and \u0160najder, 2015), we show that it", "startOffset": 192, "endOffset": 220}, {"referenceID": 23, "context": "For sentiment classification, Li et al. (2015) find that \u201ctoo large a dimensionality leads many dimensions to be non-functional .", "startOffset": 30, "endOffset": 47}, {"referenceID": 19, "context": "There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task.", "startOffset": 79, "endOffset": 189}, {"referenceID": 35, "context": "There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task.", "startOffset": 79, "endOffset": 189}, {"referenceID": 42, "context": "There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task.", "startOffset": 79, "endOffset": 189}, {"referenceID": 11, "context": "There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task.", "startOffset": 79, "endOffset": 189}, {"referenceID": 41, "context": "There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task.", "startOffset": 79, "endOffset": 189}, {"referenceID": 9, "context": "There are many theories of argumentation that might be applicable for our task (Jackson and Jacobs, 1980; Reed and Rowe, 2004; Walton et al., 2008; Gilbert, 1997; Toulmin, 1958; Dung, 1995), but one definition of argument structure may not work for every NLP task.", "startOffset": 79, "endOffset": 189}, {"referenceID": 39, "context": "are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012).", "startOffset": 92, "endOffset": 233}, {"referenceID": 32, "context": "are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012).", "startOffset": 92, "endOffset": 233}, {"referenceID": 10, "context": "are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012).", "startOffset": 92, "endOffset": 233}, {"referenceID": 15, "context": "are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012).", "startOffset": 92, "endOffset": 233}, {"referenceID": 12, "context": "are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012).", "startOffset": 92, "endOffset": 233}, {"referenceID": 6, "context": "are often informal, and do not necessarily follow logical rules or schemas of argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2013; Ghosh et al., 2014; Habernal et al., 2014; Goudas et al., 2014; Cabrio and Villata, 2012).", "startOffset": 92, "endOffset": 233}, {"referenceID": 6, "context": ", 2014; Cabrio and Villata, 2012). Moreover, in social media, segments of text that are argumentative must first be identified, as in our Task1. Habernal and Gurevych (2016) train a classifier to recognize text segments that are argumentative, but much previous work does Task1 manually.", "startOffset": 8, "endOffset": 174}, {"referenceID": 6, "context": ", 2014; Cabrio and Villata, 2012). Moreover, in social media, segments of text that are argumentative must first be identified, as in our Task1. Habernal and Gurevych (2016) train a classifier to recognize text segments that are argumentative, but much previous work does Task1 manually. Goudas et al. (2014) annotate 16,000 sentences from social media documents and con-", "startOffset": 8, "endOffset": 309}, {"referenceID": 14, "context": "Hasan and Ng (2014) also manually identify argumentative sentences, while Boltuzic and \u0160najder (2014) treat the whole post as argumentative, after manually removing \u201cspam\u201d posts.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Hasan and Ng (2014) also manually identify argumentative sentences, while Boltuzic and \u0160najder (2014) treat the whole post as argumentative, after manually removing \u201cspam\u201d posts.", "startOffset": 74, "endOffset": 102}, {"referenceID": 3, "context": "Biran and Rambow (2011) automatically identify justifications as a structural component of an argument.", "startOffset": 0, "endOffset": 24}, {"referenceID": 18, "context": "are functionally similar to our facets as discussed above (Conrad et al., 2012; Hasan and Ng, 2014; Boltuzic and \u0160najder, 2014; Naderi and Hirst, 2015).", "startOffset": 58, "endOffset": 151}, {"referenceID": 4, "context": "are functionally similar to our facets as discussed above (Conrad et al., 2012; Hasan and Ng, 2014; Boltuzic and \u0160najder, 2014; Naderi and Hirst, 2015).", "startOffset": 58, "endOffset": 151}, {"referenceID": 30, "context": "are functionally similar to our facets as discussed above (Conrad et al., 2012; Hasan and Ng, 2014; Boltuzic and \u0160najder, 2014; Naderi and Hirst, 2015).", "startOffset": 58, "endOffset": 151}, {"referenceID": 4, "context": ", 2012; Hasan and Ng, 2014; Boltuzic and \u0160najder, 2014; Naderi and Hirst, 2015). For example, Fig. 2 lists facets A1 to A8 for Gun Control from the IDebate website; Boltuzic and \u0160najder (2015) use this list to label posts.", "startOffset": 28, "endOffset": 193}, {"referenceID": 25, "context": "Previous work shows that metrics used for evaluating machine translation quality perform well on paraphrase recognition tasks (Madnani et al., 2012).", "startOffset": 126, "endOffset": 148}, {"referenceID": 37, "context": "In our experiments, ROUGE performed very well, suggesting that other machine translation metrics such as Terp and Meteor may be useful (Snover et al., 2009; Lavie and Denkowski, 2009).", "startOffset": 135, "endOffset": 183}, {"referenceID": 21, "context": "In our experiments, ROUGE performed very well, suggesting that other machine translation metrics such as Terp and Meteor may be useful (Snover et al., 2009; Lavie and Denkowski, 2009).", "startOffset": 135, "endOffset": 183}, {"referenceID": 13, "context": "Habernal and Gurevych (2015) apply clustering in argument mining by averaging word embeddings from posts and sentences from debate portals, clustering the resulting averaged vectors, and then computing distance measures from clusters to unseen sentences (\u201cclassification units\u201d)", "startOffset": 0, "endOffset": 29}, {"referenceID": 4, "context": "Cosine similarity between weighted and summed vector representations is also a common approach, and Boltuzic and \u0160najder (2015) show word2vec cosine similarity beats bag-ofwords and STS baselines when used with clustering for argument identification.", "startOffset": 100, "endOffset": 128}], "year": 2017, "abstractText": "When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. Debate websites produce curated summaries of arguments on such topics; these summaries typically consist of lists of sentences that represent frequently paraphrased propositions, or labels capturing the essence of one particular aspect of an argument, e.g. Morality or Second Amendment. We call these frequently paraphrased propositions ARGUMENT FACETS. Like these curated sites, our goal is to induce and identify argument facets across multiple conversations, and produce summaries. However, we aim to do this automatically. We frame the problem as consisting of two steps: we first extract sentences that express an argument from raw social media dialogs, and then rank the extracted arguments in terms of their similarity to one another. Sets of similar arguments are used to represent argument facets. We show here that we can predict ARGUMENT FACET SIMILARITY with a correlation averaging 0.63 compared to a human topline averaging 0.68 over three debate topics, easily beating several reasonable baselines.", "creator": "LaTeX with hyperref package"}}}