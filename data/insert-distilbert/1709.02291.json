{"id": "1709.02291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Basic Filters for Convolutional Neural Networks: Training or Design?", "abstract": "when noisy convolutional neural networks are used to tackle learning problems based on learning time series, e. g., audio data, raw one - dimensional data are commonly pre - processed accordingly to obtain spectrogram variables or mel - spectrogram coefficients, which are then used as input to the actual neural network. in this contribution, we investigate, both theoretically and experimentally, the influence of this complex pre - processing step on the network's performance and pose the question, whether independently replacing it by applying adaptive or learned filters directly adds to the raw stimulus data, than can improve learning success. the promising theoretical results show that approximately reproducing mel - spectrogram coefficients by applying adaptive filters and subsequent time - consistent averaging solutions is in principle possible. on the other hand, extensive, experimental work leads strictly to the conclusion, that recognizing the adaptive invariance induced typically by designing mel - focused spectrogram coefficients is both desirable and hard to infer by the learning process. thus, the results achieved by adaptive end - to - end learning approaches are close adequate to but slightly worse than results achieved by state - of - + the - art reference architectures using standard input bandwidth coefficients derived from the spectrogram.", "histories": [["v1", "Thu, 7 Sep 2017 14:51:37 GMT  (282kb,D)", "http://arxiv.org/abs/1709.02291v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["monika doerfler", "thomas grill", "roswitha bammer", "arthur flexer"], "accepted": false, "id": "1709.02291"}, "pdf": {"name": "1709.02291.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["monika.doerfler@univie.ac.at"], "sections": [{"heading": "1 Introduction", "text": "Convolutional neural networks, first introduced in learning tasks for image data [15], have revolutionized state-of-the-art results in many machine learning (ML) problems. In convolutional neural networks (CNNs), when applied to image data, all filter coefficients are usually learned. For applications to time series, such as audio data, on the other hand, it is common practice to first apply a fixed filter bank to the raw, one-dimensional data in order to generate a feature representation. In traditional audio signal processing methods, used, e.g., in music information retrieval (MIR) or speech processing, FFTbased features such as mel-spectrograms are typically used as such inputs. These first level features are two-dimensional arrays, derived from some kind of windowed Fourier transform with subsequent mel-scale averaging.\nRecently, the natural question arose, what kind of filters a network would learn if it was given the raw audio input. To date, encouraging results are scarce and so far, a true end-to-end approach for music signals, i.e., acting on raw audio without any pre-processing, has not been able to outperform models based on linear-frequency spectrogram or mel-spectrogram input [5]. It has been argued that these two ubiquitous representations automatically capture invariances which are of importance for all audio signals, in particular, a kind of translation invariance in time (guaranteed by introducing the non-linear magnitude operation) and a certain stability, introduced by the mel-averaging, to frequency shifts and time-warping (cp. [1]).\nIn this contribution, we give a formal description of the action of mel-scale averaging on spectrogram coefficients. We show that the resulting mel-spectrogram coefficients can indeed be mimicked by applying frequency-adaptive filters, however, followed\n1This work has been supported by the Vienna Science and Technology Fund (WWTF) through project MA14-018.\nar X\niv :1\n70 9.\n02 29\n1v 1\n[ cs\n.L G\n] 7\nS ep\n2 01\n7\nby time-averaging of each filter\u2019s output. In order to obtain a close approximation to mel-spectrogram coefficients, the frequency adaptive filter bank\u2019s output signals must each undergo a time-averaging operation and the time-averaging window is different for each channel. Note that the similarity of mel-spectrogram coefficients to the result of time-averaging wavelet coefficients has already been observed in [1], without giving a precise formulation of the connection.2\nWe derive the necessary conditions on the filters, a different one for each bin in the mel-scale, by using the theory of Gabor multipliers and their spreading function, cf. [7]. Considering the description of an operator by means of its spreading function gives interesting insight in the nature of the correlations invoked by the application of the corresponding operator on the signal coefficients. In the case of mel-spectrogram coefficients it turns out that applying wide triangular windows in the high frequency regions actually corresponds to the application of an operator with little spreading in time. This seems to be the intuitively correct choice for audio signals such as music and speech. While a similar effect can be realized by applying wavelet or constant-Q type filters, the subsequent time-averaging alleviates the significant frequency-spreading effect introduced by rather narrow filtering windows. The observation gained from investigating the classical mel-spectrogram coefficients is thus, that time- and frequency-averaging spectrogram coefficients provide invariances which are useful in most audio classification tasks. On the other hand, relaxing the strict averaging performed by computing melspectrogram coefficients may intuitively open the opportunity to keep information on details which may be necessary in certain learning tasks.\nIn our numerical experiments we thus strive to understand how time and frequency averaging influence CNN prediction performance on realistic data sets. The observations drawn from the experiments on learning filters can be summarized as follows:\n\u2022 Allowing the net to learn time-averaging leads to comparable, but not improved, prediction results compared to learning from mel-spectrogram inputs.\n\u2022 The adapted time-averaging windows do not coincide with the general mathematical theory but are dependent on the specific structure of the training data.\n\u2022 Tricks are required to make the CNNs adapt the feature processing stage at all. Otherwise, the classification part of the network takes over the adaptation required to minimize the target loss.\nThis paper is organized as follows. In the next section we introduce necessary concepts from time-frequency analysis. In Section 3, we give a formal description of the network architecture, since we haven\u2019t found any concise exposition in the literature. Section 4 then gives the formal result linking mel-spectrogram coefficients with adaptive filterbanks. We also give a detailed analytical example based on Gaussian windows and present a numerical evaluation of the resulting approximation of mel-spectrogram coefficients. In Section 5 we report on the experiments with a real-world data set for the problem\n2This observation seems to have served as one motivation to introduce the so-called scattering transform, which consists of repeated composition of convolution, a nonlinearity in the form of taking the absolute value and time-averaging. In that framework, mel-spectrogram coefficients are interpreted as first order scattering coefficients.\nof singing voice detection. Finally we conclude with a discussion and perspectives in Section 6."}, {"heading": "2 Time-Frequency concepts", "text": "The Fourier transformation of a function f \u2208 H, for some Hilbertspace H, will be denoted by F(f). We use the normalization F(f)(\u03c9) = \u222b R f(t)e\n2\u03c0i\u03c9tdt and denote its inverse by F\u22121(f)(t) = \u222b R f(\u03c9)e\n2\u03c0i\u03c9td\u03c9. For x, \u03c9 \u2208 R, the translation or time shift operator of a function f is defined as\nTxf(t) = f(t\u2212 x).\nand the modulation or frequency shift operator of a function f is defined as\nM\u03c9f(t) = e 2\u03c0it\u03c9f(t).\nThe operators of the form TxM\u03c9 or M\u03c9Tx are called time-frequency shifts. To obtain local information about the frequency spectrum we define the short-time Fourier transform (STFT) of a function f with respect to a window g 6= 0, where f, g \u2208 H, as\nVgf(b, k) = \u222b t f(t)g(t\u2212 b)e\u22122\u03c0iktdt = F(f \u00b7 Tbg)(k). (1)\nMoreover this can be written as an inner product combining the above introduced operators\nVgf(b, k) = \u3008f,MkTbg\u3009.\nTaking the absolute value squared we obtain the spectrogram as S0(b, k) = |Vgf(b, k)|2."}, {"heading": "3 The structure of CNNs", "text": "The basic, modular structure of CNNs has often been described, see e.g., [10]. Here, we will give a formal statement of the specific architecture used in the experiments in this paper. This architecture has been successfully applied to several MIR tasks and seems to have a prototypical character for audio applications, cf. [11]. The most basic building block in a general neural network may be written as\nxn+1 = \u03c3(Anxn + bn)\nwhere xn is the data vector, or array, in the n-th layer, An represents a linear operator, bn is a vector of biases in the n-th layer and the nonlinearity \u03c3 is applied componentwise. Note that in each layer the array xn may have a different dimension. Now, in the case of convolutional layers of CNNs, the matrix A has a particular structure for the convolutional layers, namely, it is a block-Toeplitz matrix, or, depending on the implementation of the filters, a concatenation of circular matrices, each representing one convolution kernel. There may be an arbitrarily high number of convolutional layers, followed by a certain number of so-called dense layers, for which An is again an arbitrary linear operator. In this paper, the chosen architecture comprises up to four convolutional and two or three dense layers.\nRemark 3.1. Note that is has been observed in [16, 19] that in the context of scattering networks, most of the input signal\u2019s energy is contained in the output of the first two convolutional layers. While the context and the filters here are different, this observation might be interesting also as a background for the usual choice of architecture of CNNs for audio processing."}, {"heading": "3.1 The CNN with Spectrogram Input", "text": "The standard input in learning methods for audio signal is based on a spectrogram, either in its raw form, or after some pre-processing such as the computation of mel-spectrogram, cf. Equation (4) in Section 4, which we will consider in detail in Section 4. In any case, the input to the CNN is a matrix of size M \u00d7N .\nRemark 3.2. In most MIR tasks, the inputs are derived from rather short snippets, that is, about 2 to 4 seconds of sound. considering a sampling rate of 22050 Hz, a window size of 2048 samples and a time shift parameter of 512 samples, i.e., 23 ms, the resulting spectrogram (containing positive frequencies only) is of size M \u00d7N = 1024\u00d7 130, where the latter is the time dimension. Hence, the frequency dimension is, in some sense, over-sampled. In particular, individual bins in the higher frequency regions contain less energy and thus information than in lower regions. Computing the mel-spectrogram is a convenient and straight-forward method of reducing the information to typically 80 frequency channels by averaging over increasingly many frequency bins.\nWe now define the following building blocks of a typical CNN: \u2022 Convolution: S \u2217 w(m,n) := \u2211 m\u2032 \u2211 n\u2032 S(m \u2032, n\u2032)w(m\u2212m\u2032, n\u2212 n\u2032)\n\u2022 Pooling: For 1 \u2264 p \u2264 \u221e, we define A \u00d7 B pooling as the operator mapping an M \u00d7N matrix S0 to a M/A\u00d7N/B matrix S1 by\nS1(m,n) = P A,B p (m,n) = \u2016v m,n S0 \u2016p\nwhere vm,nS0 is the vector consisting of the matrix entries S0((m\u2212 1) \u00b7A+ 1, . . . ,m \u00b7 A; (n \u2212 1) \u00b7 B + 1, . . . , n \u00b7 B) for m = 1, . . . ,M/A, n = 1, . . . , N/B. In this work, we use max-pooling, which has been the most successful choice, corresponding to p =\u221e in the above formula.\n\u2022 A nonlinearity \u03c3 : R 7\u2192 R, whose action is always to be understood componentwise. In all but the last layer we use leaky rectified linear units, which allow for a small, non-zero gradient when the unit is not active:\n\u03c3(x) =\n{ x if x > 0\ncx otherwise\nfor some c 1.\nThe output layer\u2019s nonlinearity \u03c3o is a sigmoid function. Given the above definitions, we can now write the output of a convolutional layer with an input array S kn\u22121 n \u2208\nRMn\u00d7Nn\u00d7Kn\u22121 :\nSknn+1=P An,Bn \u221e \u03c3( Kn\u22121\u2211 kn\u22121=1 Skn\u22121n \u2217 w kn\u22121 kn\ufe38 \ufe37\ufe37 \ufe38\nMn\u00d7Nn\u00d7Kn\u22121\u00d7Kn\n+bkn \u2297 1)\n\ufe38 \ufe37\ufe37 \ufe38 Mn/An\u00d7Nn/Bn\u00d7Kn\n(2)\nwhere 1 is an all-ones matrix of size Mn\u00d7Nn, bkn \u2208 RKn and Sknn+1 \u2208 RMn/An\u00d7Nn/Bn for kn = 1, . . . ,Kn. Letting Dc denote the number of convolutional layers and S kDc\u22121 Dc\n\u2208 RMDc\u00d7NDc\u00d7KDc\u22121 the output of the last convolutional layer, the over-all action of a CNN with two dense layers, and considering, for simplicity, a single output unit emitting xout, can be written as\nxout = \u03c3o(A2 \u00b7 [\u03c3(A1 \u00b7 S kDc\u22121 Dc + bDc+1)] + bDc+2) (3)\nHere, A1 and A2 are weight-matrices of size Nd\u00d7MDcNDcKDc\u22121 and 1\u00d7Nd, respectively, where Nd is the number of hidden units in the first dense layer, b\nDc+1 \u2208 RNd and bDc+2 \u2208 R."}, {"heading": "3.2 Modifying the input matrix", "text": "As mentioned in the previous section, the spectrogram of audio is often pre-processed in order to reduce the dimensionality on the one hand, and in order to obtain a spectral representation that better fits both human perception and properties of speech and music on the other hand. Additionally, the authors in [1] pointed out that using mel-spectrogram instead of the spectrogram guarantees improved stability with respect to frequency shifts or, more generally, deformations of the original audio signals, than the usage of spectrograms. However, given appropriate choice of network architecture, comparable results can usually be achieved using either the spectrogram or the mel-spectrogram, i.e., the invariance introduced by the mel-averaging can also be learned. In other respects, omitting the frequency-averaging provided by the mel-spectrogram leads to an increase in the number of weights to be learned. On the other hand, this observation raises the question, whether using filters learned directly in the time-domain, would improve the net\u2019s ability to achieve the amount of invariance most appropriate for a particular ML task and thus increase stability. The corresponding approach then implies learning time-domain filters already in a layer prior to the first 2D-convolution. To put this remark into perspective, we note that the spectrogram may easily be interpreted as the combined (and possibly sub-sampled) output of several convolutions, since, setting h\u030c(n) = h(\u2212n), we can write\nS0(m,n) = | \u2211 n\u2032 f(n\u2032)h(n\u2032 \u2212 n)e\u22122\u03c0imn\u2032 |2 = |f \u2217 h\u030cm(n)|2\nIn the two following sections we thus raise and answer two questions.\n(i) Is it possible to obtain coefficients which are approximately equivalent to the wellestablished mel-spectrogram coefficients simply by using the \u2018correct\u2019 filters directly on the audio signal?\n(ii) Can adaptivity in frequency- and time-averaging improve prediction accuracy? In particular, for a given set of frequency-adaptive filters precisely mimicking the melscale, can a time-averaging layer with learned averaging width improve learning performance?"}, {"heading": "4 The mel-spectrogram and basic filters", "text": "In this section, we take a detailed look at the mel-spectrogram. This representation is derived from the classical spectrogram by weighted averaging of the absolute values squared of the STFT and can undoubtedly be referred to as the most important feature set used in speech and audio processing, together with MFCCs which are directly derived from it. The number of mel-filters used varies between 80 filters between 80 Hz and 16 kHz [11] and 128 [5] or more. In order to better understand the relation between the result of mel-averaging and FFT-based analysis with flexible windows, we observe the following: denote the input signal by f \u2208 CN , the window function for generating the spectrogram by g \u2208 CN and the mel-filters by \u039b\u03bd \u2208 CN for \u03bd = 1 . . .K, where K is the chosen number of filters. We can then write the mel-spectrogram as\nMSg(f)(b, \u03bd) = \u2211 k |F(f \u00b7 Tbg)(k)|2 \u00b7 \u039b\u03bd(k). (4)\nAnde\u0301n and Mallat showed in [1], that the mel-spectrogram can be approximated by time-averaging the absolute values squared of a wavelet transform. Here, we make their considerations precise by showing that we can get a close approximation of the melspectrogram coefficients if we use adaptive filters.\nRemark 4.1. Note that the resulting transform may be interpreted as a nonstationary Gabor transform, compare [2, 3, 13, 6].\nRecall that the STFT can be understood as\nVgf(b, k) = F(f \u00b7 Tbg)(k)\nProposition 4.2. Let an analysis window g and mel-filters \u039b\u03bd be given, for \u03bd \u2208 I. If, for each \u03bd, the windows h\u03bd and time-averaging functions $\u03bd are chosen such that\nVh\u03bdh\u03bd(x, \u03be) \u00b7 F($\u03bd)(\u03be) = Vgg(x, \u03be) \u00b7 F\u22121(\u039b\u03bd)(x), (5)\nthen the mel-spectrogram coefficients MSg(f)(b, \u03bd) can be obtained by time-averaging the filtered signal\u2019s absolute value squared as follows:\nMSg(f)(b, \u03bd) = \u2211 l |(f \u2217 h\u03bd)(l)|2 \u00b7$\u03bd(l \u2212 b) =: TAh(f)(b, \u03bd). (6)\nThe proof is given in Appendix A. While it is in general tedious to derive conditions for the optimal filters h\u03bd and the timeaveraging windows $\u03bd explicitly, we obtain a more accessible situation if we restrict the choice of windows to dilated Gaussians. In this case we set g(t) = \u03d5\u03c3(t) = ( 2 \u03c3 ) 1 4 e\u2212\u03c0 t2\n\u03c3 . Then we have, by straight-forward computation,\nV\u03d5\u03c3\u03d5\u03c3(x, \u03be) = e\u2212 \u03c0 2 x2 \u03c3 e\u2212 \u03c0 2 \u03c3\u03be2e\u2212\u03c0ix\u03be,\nwhich leads to the following result:\nCorollary 4.3. Fix g = \u03d5\u03c3 for some scaling factor \u03c3, and let the filters \u039b\u03bd be given as shifted and possibly dilated versions of a basic shape (e.g., in the case of mel-filters, asymmetric triangular functions), i.e., \u039b\u03bd(\u03be) = T\u03bdDa(\u03bd)\u039b(\u03be), for \u03bd \u2208 I. Assuming that each filter h\u03bd is a Gaussian window \u03d5\u03c1(\u03bd) with a frequency-dependent dilation factor \u03c1(\u03bd) and frequency-shifted to position \u03bd, i.e., h\u03bd(t) = e\n2\u03c0i\u03bdt\u03d5\u03c1(\u03bd)(t). Then condition (5) is equivalent to the following conditions in separate variables:\ne \u2212\u03c0 2 x2( 1 \u03c1(\u03bd) \u2212 1 \u03c3 ) = F\u22121(Da(\u03bd)\u039b)(x) (7)\ne\u2212 \u03c0 2 \u03be2(\u03c3\u2212\u03c1(\u03bd)) = F($\u03bd)(\u03be). (8)\nProof : Noting that Vh\u03bdh\u03bd(x, \u03be) = e 2\u03c0i\u03bdxV\u03d5\u03c1(\u03bd)\u03d5\u03c1(\u03bd)(x, \u03be) and that F\u22121(\u039b\u03bd)(x) = e2\u03c0i\u03bdxF\u22121(Da(\u03bd)\u039b)(x), the result is straight-forward.\nRemark 4.4. From the above corollary we see that a precise recovery of the mel-spectrogram coefficients by means of time-averaging of the spectrogram coefficients with respect to any dilated Gaussian window is only possible if the averaging windows Da(\u03bd)\u039b are themselves dilated Gaussians. However, we can easily deduce the following pointwise error estimate:\n|MSg(f)(b, \u03bd)\u2212 TAh(f)(b, \u03bd)| \u2264 (9) \u2016Vh\u03bdh\u03bd \u00b7 F($\u03bd)\u2212 Vgg \u00b7 F\u22121(\u039b\u03bd)\u20162\nSo, even if the mel-spectrogram coefficients can not precisely be recovered by adaptive filtering followed by time-averaging, a close approximation can be achieved for reasonable windows g. The experiments in the next section show that the deviations resulting from adaptive filtering followed by time-averaging as opposed to mel-coefficients are small and probably negligible in the setting of CNN-based learning. However, excellent precision of approximation is only achieved in the case of full spectrograms, i.e., if no subsampling is applied and declines with increasingly coarse subsampling strategies. To see this, note that both F($\u03bd) and F\u22121(\u039b\u03bd) are periodic with period 1\u03b1 and 1 \u03b2 , respectively, if the time- and frequency variable in (1) are sub-sampled by \u03b1 and \u03b2, respectively. Obviously, the resulting aliases deteriorate the estimate in (9). In the realistic scenarios addressed in Section 5, which always involve subsampled spectrograms, the differences between mel-spectrogram coefficients and their approximation by adaptive filtering become relevant for the performance of CNN-learning."}, {"heading": "4.1 Examples", "text": "In this section, we give some examples of filters h\u03bd computed to obtain mel-coefficients MSg(f) by time-averaging |(f \u2217h\u03bd)(l)|2 as in (6). We consider Hann windows, which is the standard choice in audio processing, also applied in the computation of mel-spectrogram coefficients and their approximation in Section 5. Starting from a Hann window g, we compute adaptive filters h\u03bd for the first 50 bins of the mel-scale. Figure 1 shows the ambiguity functions Vgg, Vh\u03bdh\u03bd , and the weighted ambiguity functions Vgg \u00b7 F\u22121(\u039b\u03bd), Vh\u03bdh\u03bd \u00b7 F($\u03bd)(\u03be), for \u03bd = 49, which corresponds to 2587.6 Hz.\nIn Figure 2, the upper plot shows the original Hann and three adapted windows, narrowing with increasing mel-number to realize the mel-averaging by adaptivity in the frequency domain. The lower plot shows the average error per bin obtained from computing the mel-spectrogram coefficients and their approximations for 200 (normally distributed) random signals. We only show relevant bins, i.e., those bins, where mel-averaging concerns more than 1 original frequency bin since the error is zero otherwise. For illustration, in Figure 3, the absolute value squared of the output of filtering a random signal f with a modulated version of g and h14, i.e., one row of |Vgf |2 and |Vh14f |2, respectively, as well as the frequency-averaged (Mel) and time-averaged (Approx.) versions thereof, are shown. Finally, the lower plots in Figure 3 show the full mel-spectrogram of a synthetic signal and its approximation by using the adapted h\u03bd per bin with subsequent time-averaging."}, {"heading": "5 Experiments on Singing Voice Detection", "text": "In Proposition 4.2 it is shown that coefficients with mel-characteristics (and other related nonlinear scales) can be closely approximated by applying appropriately chosen filters directly to raw audio data and allowing for a subsequent time-averaging step. Now, we are interested in investigating if the theoretical findings translate to typical realworld problems that have already been successfully treated with CNNs. Thereby, we are motivated by the fact that state-of-the-art results for several MIR problems are based on mel-spectrogram coefficients which show certain desirable invariance and stability properties. In particular, due to the modulus, they are invariant to translation and, due to the frequency averaging, they exhibit stability to certain deformations such as timewarping, cf. [1]. However, in general, the required invariance and stability with respect to deformations will depend on data characteristics and the learning task, cf. [17]. Hence, we start from our construction encompassing adaptive filtering followed by time-averaging and allow the time-averaging width to be learned by the network, posing the question whether adaptivity in this step can improve the network\u2019s performance.\nWe need to note that, when trained on a specific problem, both the feature layers (including the adaptive time-averaging step) and the classification part of a CNN will concurrently adapt their parameters towards optimally predicting the given targets. We will discuss the implications of this behavior for our experiments in Section 5.4.\nOur hypothesis is that a CNN with an architecture that is adapted to a given learning task will learn filters \u2014 in this case their time-averaging components \u2014 which alleviate the extraction of stabilities and invariance properties and are thus beneficial in the given context."}, {"heading": "5.1 Data", "text": "We investigate the effects of learning filters directly on raw audio by revisiting the problem of singing voice detection [18] we have studied before. In the referenced publication, a CNN was tuned for maximum prediction accuracy both in the absence or presence of various forms of data augmentation.\nThe experiments were performed on a non-public dataset of 188 30-second audio snippets from an online music store (dataset \u2018In-House A\u2019), covering a very wide range\nof genres and origins. We used a five-fold cross-validation, for each iteration 150 files for training, the remaining 38 for evaluation. The audio was subsampled to a sampling rate of 22.05 kHz and down-mixed to mono. The precomputed mel-spectrograms were calculated using an STFT with Hann windows, a frame length of 1024 and hop size of 315 samples (yielding 70 frames per second). We discarded the phases and applied a filterbank with 80 triangular mel-scaled filters from 27.5 Hz to 8 kHz, then logarithmized the magnitudes (after clipping values below 10\u22127). Finally, each mel-band was normalized to zero mean and unit variance over the entire training set."}, {"heading": "5.2 CNN training procedure and architecture", "text": "The training procedure used in our experiments is slightly different than in the reference publication [18]. The networks are trained on mel-spectrogram excerpts of 115 spectrogram frames (\u223c1.6 seconds) paired with a label denoting the presence or absence of human voice in the central frame. Training is performed using stochastic gradient descent on cross-entropy error based on mini-batches of 64 randomly chosen examples. Updates to the network weights are computed using the ADAM update rule [14] with an initial learning rate of 0.001 and an adaptive scheme reducing the learning rate twice by a factor of 10 whenever the training error does not improve over three consecutive episodes of 1000 updates.\nEvaluation is performed running a complete five-fold cross-validation run to obtain predictions for the whole set of training data, with this procedure repeated five times. The mean classification error (1\u2212accuracy) obtained with this architecture in the absence of augmentation on the \u2018In-House dataset A\u2019 is 11.6%, insignificantly below the value of 12.0% reported in the reference paper.\nAs described in Section 3.1, the applied CNN architecture employs three types of feed-forward neural network layers: convolutional feature processing layers convolving a stack of 2D inputs with a set of learned 2D kernels, pooling layers subsampling a stack of 2D inputs by taking the maximum over small groups of neighboring pixels, and dense classification layers flattening the input to a vector and applying a dot product with a learned weight matrix Aj .\nThe architecture used in [18] has a total number of 1.41 million weights, with the dense connections of the classification layers taking up the major share (1.28 million, or 91%). It can be expected that the actual output of the convolutional feature stage is of subordinate importance when the classification stage with its high explanatory power dominates the network.\nIf no data augmentation is applied, the network size \u2014 especially the classification part \u2014 can be drastically reduced while largely preserving its performance. This size reduction is possible, since, as a general rule, the necessary number of parameters determining the network is correspondent to the complexity of the training data set. Since we are interested in the impact of the convolutional feature stage\u2019s properties, we reduce the architecture for our experiments as follows: We use four convolutional layers, two 3 \u00d7 3 convolutions of 32 and 16 kernels, respectively, followed by 3 \u00d7 3 non-overlapping max-pooling and two more 3 \u00d7 3 convolutions of 32 and 16 kernels, respectively, and another 3\u00d7 3 pooling stage. In the notation of (2), this corresponds to\n\u2022 K0 = K2 = 32, K1 = K3 = 16\n\u2022 wk0 \u2208 R3\u00d73, k0 = 1, . . . , 32;\n\u2022 wk0k1 \u2208 R 3\u00d73\u00d732, k1 = 1, . . . , 16\n\u2022 A1 = B1 = 1, A2 = B2 = 3\n\u2022 wk1k2 \u2208 R 3\u00d73\u00d716, k2 = 1, . . . , 32;\n\u2022 wk2k3 \u2208 R 3\u00d73\u00d732, k3 = 1, . . . , 16\n\u2022 A3 = B3 = 1, A4 = B4 = 3\nFor the classification part, we experimented with two variants: One with two dense layers of 64 and 16 units, and the other one with just one dense layer of 32 units. In both cases, the final dense layer is a single sigmoidal output unit. For the first variant, the total number of weights is 94337, with the classification stage taking up 79969 units, or 85%. The second variant features a considerably smaller classification network: the total number of weights is 34113, with the classification stage taking up 19745 units, or 58%. The different network sizes, especially the ratio of feature to classification stage allows us to analyze the influence of the different parts."}, {"heading": "5.3 Experimental setup", "text": "In the following, we will compare the behavior of the CNNs applied to the STFT spectrogram features to features computed ad hoc from the audio signal. For that, we use Gabor filter banks with subsequent magnitude computation, time-averaging and logarithmization. The maximum kernel sizes of the Gabor filter banks are set to 1024, identical to the frame length of the previously used STFT. The kernel size equals the time support for the lowest frequency band (50 Hz) and reduces, according to the band-width requirements of the mel frequency scale, down to 94 samples for the highest band at 7740 Hz. The ad-hoc feature computation also necessitates a shift from global normalization of spectral features (unit mean and standard deviation on the entire training set) to batch-wise normalization, computed on the fly. We could verify in side experiments with the reference architecture that the use of batch normalization does not change the scores. The training examples are snippets of the audio signal with a length of 115\u00d7 315 + 1024 = 37249 samples each with a hop size of 315 samples.\nTo judge the influence of adaptivity, two different approaches have been compared:\n1. Fixed-size time-averaging after the Gabor filter banks (architecture \u2018Gabor fixedwidth\u2019)\n2. Adaptive time-averaging after the Gabor filter banks, learned alongside the CNN weights either with a uniform averaging length over all frequency bins (\u2018Gabor variable-width uniform\u2019), or with individual adaptation per frequency bin (\u2018Gabor variable-width individual\u2019).\nFor reasons of computational cost it is not feasible to perform a full sample-bysample convolution for the Gabor filter bank. For our experiments, we have chosen a convolution stride for the Gabor filters of 21 samples, that is, the resulting spectrum is down-sampled along the time axis by a factor of 21. The subsequent non-overlapping averaging is computed on 15 frames each, in order to stay comparable with the STFT hop size of 315 = 21 \u00d7 15 samples. Preliminary tests have shown that the chosen subsampling does not significantly degrade the results. Note that the stride is a factor of about 4.5 lower than the shortest kernel support (21 vs. 94).\nFor the fixed-size time-averaging variant standard average-pooling is used, implemented as a 15 \u00d7 1 2D-pooling layer acting on the Gabor magnitude spectrum. In this case the temporal averaging size is uniform over the frequency axis which is an approximation of the mathematical findings. The variable-size case is implemented using Hann windows with adaptive width, acting on the magnitude spectrum. The maximum time support of this Hann window is 8 times the STFT hop size, equivalent to 2520 samples. The choice of Hann in contrast to Boxcar windows (as in the fixed-size case) is motivated by its smoothness which enables adaptivity for the CNN training process. Two variants were tried: Firstly, a uniform width over all frequency bands, corresponding to the fixed-size case, and, secondly, individually adaptive sizes per band."}, {"heading": "5.4 Experimental results", "text": "Figure 4 shows the results of our CNN experiments for the problem of singing voice detection. For our evaluations, we have switched from the simple error measure with the \u2018optimal\u2019 (in the sense of maximum accuracy) threshold per experiment to the more informative \u2018area over the ROC curve\u2019 measure (AOC), fusing classification errors for all possible thresholds into one measure. A lower measure indicates a better result. Leftmost, the reference architecture with the large network is displayed. Right from it, there are two groups of experiments: for the \u2018small-two\u2019 and \u2018small-one\u2019 networks, respectively. Each group contains one experiment for the STFT baseline (\u2018STFT ad-hoc\u2019), one for a fixed-width time averaging (\u2018Gabor fixed-width\u2019) and variable-width experiments with either uniform or individual averaging widths for the spectral bands.\nThe reference implementation in [18] uses pre-computed spectrograms, with a normalization globally on the the training set and eventual padding performed also on the spectrogram. End-to-end learning as performed in our experiments demands on-line normalization (using a batch normalization layer) and padding directly on the audio time signal. We could verify that this yields a performance equivalent with the reference experiments (independent t-test probability level 79% for equal means, df = 8) which is shown in Figure 4 in the first two columns (\u2018STFT precomputed\u2019 and \u2018STFT ad-hoc\u2019).\nWe can also confirm that the results of our smaller networks (\u2018small-two\u2019 and \u2018smallone\u2019, for the two- and one-classification-layer variants) are comparable to the large baseline architecture. For AOC (\u2018STFT ad-hoc\u2019 case), the smaller networks score 5.22% (\u2018small-two\u2019) and 5.29% (\u2018small-one\u2019), respectively, compared to 4.96% of the original architecture. Although the STFT results for the two small networks are almost identical (independent t-test probability level 70%, df = 8), there is a general tendency for the smaller networks to show slightly greater AOC measures, as expected.\nIn the course of experimentation it has become apparent that the averaging widths of\nthe Gabor-filter models hardly adapt at all. They rather stay at the initial values, while the CNN weights adapt instead. As a trick we have boosted the widths\u2019 gradients for the back-propagation by a factor 3 to force the width parameters to adapt at a higher rate. Higher factors have proven unfeasible, causing the adaptation to run out of bounds. Since the adaptation process is intricate, the choice of a starting value for the variable averaging length (time support of the Hann window) is important. We have tried values of 0.1, 0.2, 0.3, 0.5 of the maximum time support, with 0.2 leading to the best results.\nThe performance measures obtained using Gabor filters with subsequent time-averaging is significantly lower than with STFT-computed mel-spectrograms, with a t-test probability level of p < 0.5% for equal means (again df = 8). The different variations of Gabor-filter-based experiments perform equivalently for the \u2018small-two\u2019 and \u2018small-one\u2019 architectures, respectively. There is no observable trend or statistically significant performance difference between the \u2018Gabor fixed-width\u2019, \u2018Gabor variable-width uniform\u2019 and \u2018Gabor variable-width individual\u2019 models.\nFigure 5 shows the adapted averaging widths per frequency band calculated in the CNN training process of the \u2018small-two\u2019 for the five cross-validation folds with 5 experiments each. Obviously, the widths vary considerably across experiments. The mean Pearson correlations between pairs of curves amount to 0.49, 0.45, 0.38, 0.44, 0.29, for the different folds, all representing significant correlations at a level of p = 5%. Some common patterns can be discerned, e.g., the bump upwards at about 600 Hz, or the hillvalley-hill shape towards the high end of the spectrum. In fact, when taking the means of the adapted averaging widths for the different architectures in our study, it becomes clear that there is a very strong correlation of the adaptations of the two models, shown in Figure 6. The Pearson correlation coefficients between pairs of widths for the architectures lie in the range of r = 0.86 . . . 0.91. The adapted time-averaging widths range from 390 to 655 samples, or 18 to 30 ms. Note that the time-averaging is performed with a Hann window, resulting in a somewhat smaller effective length, in contrast to the Boxcar window of the fixed-width case with an averaging length of 315 samples. The results of the adaptive case tend to be slightly better than the fixed averaging, but again, not significantly.\nFrom the mathematical derivation above, one may expect the time-averaging width to increase with frequency in order to mimic the, supposedly optimal, behavior of the mel-spectrogram. It can be observed that this is not the case in practical experiments, where the structure adapts to the characteristics of the training data."}, {"heading": "5.4.1 Adapting the mel-filters", "text": "As discussed in the previous section, experiments showed that coefficients computed by using filters which directly reflect the frequency resolution of the mel-scale are not able to improve or even reach the state-of-the art results obtained by using mel-spectrogram coefficients as input to the networks. Allowing for adaptivity in the frequency resolution itself deteriorated the results even further. We therefore assume that computing frequencyaveraged coefficients from pre-computed spectrograms is advantageous at least for the learning task we focus on and thus, instead of a true end-to-end learning approach, allowed the net to adapt the frequency-filters used in the averaging step as described in (4). Indeed, using this approach, the reference results could be reproduced and a small but\nsignificant performance improvement is achieved when adaptivity is introduced. We can observe, that the adaptation process is again consistent over different architectures, i.e., the amount of adaptivity per mel-frequency band is highly correlated (pairwise Pearson correlation > 0.99) over the three architectures studied. The deviation from the (mel-scale based) initialization is shown in Figure 7."}, {"heading": "6 Discussion and Perspectives", "text": "In this article we posed and answered two questions concerning the application of alternative time-frequency representations for learning problems in music information retrieval. First, it has been analytically shown, under which conditions mel-spectrogram coefficients can be reproduced by applying frequency-adaptive filters followed by a time-averaging step. In practice, this procedure will always lead to approximate values. We expected the resulting approximation errors to be out-balanced by subsequent convolutional layers of the neural network and thus at least equal or, due to possible advantageous effects of adaptivity, improved performance in comparison to standard settings. However, and this answers the second question, extensive experimental studies concerning the problem of singing voice detection have not led to advantageous effects of the usage of adaptive filters. Coefficients obtained by averaging pre-computed spectrogram coefficients over frequency bins (as in mel-spectrogram coefficients, cf. (4)), consistently lead to the best results, albeit by a small margin. The reasons for this observation require further investigations and may be related to the following facts: First, the setting involving only one basic analysis window is well-established and in particular, the original window length may be optimal in the sense that the subsequent CNN architecture\u2019s parameters, e.g., the size of the subsequent filters, is properly adapted to its characteristics. Second, the classical method to obtain mel-spectrogram coefficients, (4), leads to a block-structure of the time-frequency resolution of the resulting representation. This block-structure depends on the length, in samples, of the original analysis window, since this length determines the FFT-length in (1). Hence, the number of available frequency-bins the mel-spectrogram coefficients can be based on, depends on the analysis window\u2019s length. It seems possible, that subsequent convolutional layers adapt more easily to this blockstructure. Third, the impact of the approximation errors occurring in the Gabor-filter based approximation of mel-spectrogram coefficients is unclear and needs to be further studied analytically.\nFuture work on the problem of learned basic filters in MIR tasks will involve the study of the influence of the analysis window used to compute the spectrogram, cp. preliminary work in [6], both regarding the network\u2019s expressivity and the performance of the learning process. Further, the questions whether the block-structure corresponding to mel-spectrogram coefficients is beneficial will be investigated both analytically and experimentally. Finally, the propagation and alleviation of approximation errors will be investigated relying on existing results on stability of CNNs, compare [20, 4, 12].\nA Invariance Properties induced by filters: Technical Details\nA.1 Proof of Proposition 1\nThe proof of Proposition 1 is based on the observation that the mel-spectrogram can be written via the operation of so-called STFT- or Gabor-multipliers, cf. [9], on any given function in the sense of a bilinear form. Before deriving the involved correspondence, we thus introduce this important class of operators. Given a window function g, time- and frequency-sub-sampling parameters \u03b1, \u03b2, respectively, and a function m : Z\u00d7Z 7\u2192 C, the corresponding Gabor multipler G\ng\u03b2\u03b1,m is defined\nas G g\u03b2\u03b1,m f = \u2211 k \u2211 l m(k, l)\u3008f,Mk\u03b2Tl\u03b1g\u3009Mk\u03b2Tl\u03b1g.\nWe next derive the expression of a mel-spectrogram by an appropriately chosen Gabor multiplier. Using sub-sampling factors \u03b1 in time and \u03b2 in frequency as before, we start from (4) and reformulate as follows:\nMSg(f)(b, \u03bd) = \u2211 k |F(f \u00b7 Tbg)(\u03b2k)|2 \u00b7 \u039b\u03bd(k)\n= \u2211 k \u3008f,M\u03b2kTbg\u3009\u3008f,M\u03b2kTbg\u3009\u039b\u03bd(k)\n=\u3008 \u2211 k \u039b\u03bd(k)\u3008f,M\u03b2kTbg\u3009M\u03b2kTbg, f\u3009\n=\u3008 \u2211 k \u2211 l m(k, l)\u3008f,M\u03b2kT\u03b1lg\u3009M\u03b2kT\u03b1lg, f\u3009\nwith m(k, l) = \u03b4(\u03b1l \u2212 b)\u039b\u03bd(k), we see that the mel-coefficients can thus be interpreted via a Gabor multiplier as defined above, i.e., MSg(f)(b, \u03bd) = \u3008Gg\u03b2\u03b1,mf, f\u3009.\nThe next step is to switch to an alternative operator representation. Indeed, as shown in [8], every operator H can equally be written by means of its spreading function \u03b7H as\nHf(t) = \u222b x \u222b \u03be \u03b7H(x, \u03be)f(t\u2212 x)e2\u03c0it\u03bed\u03bedx. (10)\nWe note that two operators H1, H2 are equal if and only if their spreading functions coincide, see [7, 8] for details. As shown in [7], a Gabor multiplier\u2019s spreading function \u03b7\ng\u03b2\u03b1,m is given by\n\u03b7 g\u03b2\u03b1,m\n(x, \u03be) =M(x, \u03be)Vgg(x, \u03be), (11)\nwhere M(x, \u03be) denotes the (\u03b2\u22121, \u03b1\u22121)-periodic symplectic Fourier transform of m, i.e., M(x, \u03be) = Fs(m)(x, \u03be) = \u2211 k \u2211 l m(k, l)e\u22122\u03c0i(l\u03b1\u03be\u2212k\u03b2x).\nWe now equally rewrite the time-averaging operation applied to a filtered signal, as defined in (6), as a Gabor multiplier. We set h\u0303(t) = h(\u2212t) and have\u2211\nl\n|(f \u2217 h\u03bd)(\u03b1l)|2 \u00b7$\u03bd(\u03b1l \u2212 b) =\n\u2211 l | \u2211 n f(n)h\u0303\u03bd(n\u2212 \u03b1l)|2 \u00b7$\u03bd(\u03b1l \u2212 b) (12)\n= \u2211 k \u2211 l |\u3008f,M\u03b2kT\u03b1lh\u0303\u03bd\u3009|2 \u00b7$\u03bd(\u03b1l \u2212 b)\u03b4(k)\n= \u3008Gh\u0303\u03b1\u03bd\u03b2mT f, f\u3009.\nwith mT (k, l) = Tb$\u03bd(l)\u03b4(k). In order to compare the two operators G\ng\u03b2\u03b1,m and Gh\u0303\u03b1\u03bd\u03b2mT , we compute their respective\nspreading functions by applying the symplectic Fourier transform to m = Tb\u03b4 \u00b7 \u039b\u03bd and mT = Tb$\u03bd \u00b7 \u03b4, respectively, to find:\nM(x, \u03be) = Fs(m)(x, \u03be) = e\u22122\u03c0ib\u03be \u00b7 F\u22121(\u039b\u03bd)(x) and MT (x, \u03be) = Fs(mT )(x, \u03be) = e\u22122\u03c0ib\u03be \u00b7 F($\u03bd)(\u03be) Plugging these expressions into (11) yields the condition in Proposition 1 and thus concludes its proof.\nRemark A.1. It is interesting to interpret the action of an operator in terms of its spreading function. In view of (10), we see that the spreading function determines the amount of shift in time and frequency, which the action of the operator imposes on a function. For Gabor multipliers, if well-concentrated window functions are used, it is immediately obvious that the amount of shifting is moderate as well as determined by the window\u2019s eccentricity. At the same time, the aliasing effects introduced by coarse sub-sampling are reflected in the periodic nature of M. Since, for F\u22121(\u039b\u03bd) the subsampling density in frequency, determined by \u03b2, and for F($\u03bd) the sub-sampling density in time, determined by \u03b1, determine the amount of aliasing, the over-all approximation quality deteriorates with increasing sub-sampling factors."}], "references": [{"title": "Deep scattering spectrum", "author": ["J. And\u00e9n", "S. Mallat"], "venue": "IEEE Transactions on Signal Processing, 62(16):4114\u20134128", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Theory", "author": ["P. Balazs", "M. D\u00f6rfler", "F. Jaillet", "N. Holighaus", "G.A. Velasco"], "venue": "implementation and applications of nonstationary Gabor frames. J. Comput. Appl. Math., 236(6):1481\u20131496", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Adapted and adaptive linear time-frequency representations: a synthesis point of view", "author": ["P. Balazs", "M. D\u00f6rfler", "M. Kowalski", "B. Torr\u00e9sani"], "venue": "IEEE Signal Processing Magazine, 30(6):20\u201331", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Invariance and Stability of Gabor Scattering for Music Signals", "author": ["R. Bammer", "M. D\u00f6rfler"], "venue": "Proceedings of SAMPTA 2017, http://arxiv.org/abs/1706.08818", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Inside the Spectrogram: Convolutional Neural Networks in Audio Processing", "author": ["M. D\u00f6rfler", "R. Bammer", "T. Grill"], "venue": "Proceedings of SAMPTA 2017. SALSA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Representation of operators in the time-frequency domain and generalized Gabor multipliers", "author": ["M. D\u00f6rfler", "B. Torr\u00e9sani"], "venue": "J. Fourier Anal. Appl., 16(2):261\u2013293", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Quantization of TF lattice-invariant operators on elementary LCA groups", "author": ["H.G. Feichtinger", "W. Kozek"], "venue": "H. G. Feichtinger and T. Strohmer, editors, Gabor analysis and algorithms, Appl. Numer. Harmon. Anal., pages 233\u2013266. Birkh\u00e4user Boston", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "A first survey of Gabor multipliers", "author": ["H.G. Feichtinger", "K. Nowak"], "venue": "H. G. Feichtinger and T. Strohmer, editors, Advances in Gabor Analysis, Appl. Numer. Harmon. Anal., pages 99\u2013128. Birkh\u00e4user", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Music Boundary Detection Using Neural Networks on Combined Features and Two-Level Annotations", "author": ["T. Grill", "J. Schl\u00fcter"], "venue": "Proceedings of the 16th International Society for Music Information Retrieval Conference ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional neural networks on cartoon functions", "author": ["P. Grohs", "T. Wiatowski", "H. B\u00f6lcskei"], "venue": "In Proc. of IEEE International Symposium on Information Theory (ISIT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "A framework for invertible", "author": ["N. Holighaus", "M. D\u00f6rfler", "G.A. Velasco", "T. Grill"], "venue": "real-time constant-Q transforms. IEEE Trans. Audio Speech Lang. Process., 21(4):775 \u2013785", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278 \u2013 2324", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Group Invariant Scattering", "author": ["S. Mallat"], "venue": "Comm. Pure Appl. Math., 65(10):1331\u20131398", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding deep convolutional networks", "author": ["S. Mallat"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 374", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2065}, {"title": "Exploring Data Augmentation for Improved Singing Voice Detection with Neural Networks", "author": ["J. Schl\u00fcter", "T. Grill"], "venue": "Proceedings of the 16th International Society for Music Information Retrieval Conference ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Energy propagation in deep convolutional neural networks", "author": ["T. Wiatowski", "P. Grohs", "H. B\u00f6lcskei"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Discrete deep feature extraction: A theory and new architectures", "author": ["T. Wiatowski", "M. Tschannen", "A. Stani", "P. Grohs", "H. B\u00f6lcskei"], "venue": "Proc. of International Conference on Machine Learning ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Convolutional neural networks, first introduced in learning tasks for image data [15], have revolutionized state-of-the-art results in many machine learning (ML) problems.", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": ", acting on raw audio without any pre-processing, has not been able to outperform models based on linear-frequency spectrogram or mel-spectrogram input [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Note that the similarity of mel-spectrogram coefficients to the result of time-averaging wavelet coefficients has already been observed in [1], without giving a precise formulation of the connection.", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": ", [10].", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Note that is has been observed in [16, 19] that in the context of scattering networks, most of the input signal\u2019s energy is contained in the output of the first two convolutional layers.", "startOffset": 34, "endOffset": 42}, {"referenceID": 18, "context": "Note that is has been observed in [16, 19] that in the context of scattering networks, most of the input signal\u2019s energy is contained in the output of the first two convolutional layers.", "startOffset": 34, "endOffset": 42}, {"referenceID": 0, "context": "Additionally, the authors in [1] pointed out that using mel-spectrogram instead of the spectrogram guarantees improved stability with respect to frequency shifts or, more generally, deformations of the original audio signals, than the usage of spectrograms.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "The number of mel-filters used varies between 80 filters between 80 Hz and 16 kHz [11] and 128 [5] or more.", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "The number of mel-filters used varies between 80 filters between 80 Hz and 16 kHz [11] and 128 [5] or more.", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "And\u00e9n and Mallat showed in [1], that the mel-spectrogram can be approximated by time-averaging the absolute values squared of a wavelet transform.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Note that the resulting transform may be interpreted as a nonstationary Gabor transform, compare [2, 3, 13, 6].", "startOffset": 97, "endOffset": 110}, {"referenceID": 2, "context": "Note that the resulting transform may be interpreted as a nonstationary Gabor transform, compare [2, 3, 13, 6].", "startOffset": 97, "endOffset": 110}, {"referenceID": 12, "context": "Note that the resulting transform may be interpreted as a nonstationary Gabor transform, compare [2, 3, 13, 6].", "startOffset": 97, "endOffset": 110}, {"referenceID": 5, "context": "Note that the resulting transform may be interpreted as a nonstationary Gabor transform, compare [2, 3, 13, 6].", "startOffset": 97, "endOffset": 110}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "1 Data We investigate the effects of learning filters directly on raw audio by revisiting the problem of singing voice detection [18] we have studied before.", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "2 CNN training procedure and architecture The training procedure used in our experiments is slightly different than in the reference publication [18].", "startOffset": 145, "endOffset": 149}, {"referenceID": 13, "context": "Updates to the network weights are computed using the ADAM update rule [14] with an initial learning rate of 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "The architecture used in [18] has a total number of 1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "The reference implementation in [18] uses pre-computed spectrograms, with a normalization globally on the the training set and eventual padding performed also on the spectrogram.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "preliminary work in [6], both regarding the network\u2019s expressivity and the performance of the learning process.", "startOffset": 20, "endOffset": 23}, {"referenceID": 19, "context": "Finally, the propagation and alleviation of approximation errors will be investigated relying on existing results on stability of CNNs, compare [20, 4, 12].", "startOffset": 144, "endOffset": 155}, {"referenceID": 3, "context": "Finally, the propagation and alleviation of approximation errors will be investigated relying on existing results on stability of CNNs, compare [20, 4, 12].", "startOffset": 144, "endOffset": 155}, {"referenceID": 11, "context": "Finally, the propagation and alleviation of approximation errors will be investigated relying on existing results on stability of CNNs, compare [20, 4, 12].", "startOffset": 144, "endOffset": 155}, {"referenceID": 8, "context": "[9], on any given function in the sense of a bilinear form.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Indeed, as shown in [8], every operator H can equally be written by means of its spreading function \u03b7H as", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "We note that two operators H1, H2 are equal if and only if their spreading functions coincide, see [7, 8] for details.", "startOffset": 99, "endOffset": 105}, {"referenceID": 7, "context": "We note that two operators H1, H2 are equal if and only if their spreading functions coincide, see [7, 8] for details.", "startOffset": 99, "endOffset": 105}, {"referenceID": 6, "context": "As shown in [7], a Gabor multiplier\u2019s spreading function \u03b7 g \u03b1,m is given by", "startOffset": 12, "endOffset": 15}], "year": 2017, "abstractText": "When convolutional neural networks are used to tackle learning problems based on time series, e.g., audio data, raw one-dimensional data are commonly preprocessed to obtain spectrogram or mel-spectrogram coefficients, which are then used as input to the actual neural network. In this contribution, we investigate, both theoretically and experimentally, the influence of this pre-processing step on the network\u2019s performance and pose the question, whether replacing it by applying adaptive or learned filters directly to the raw data, can improve learning success. The theoretical results show that approximately reproducing mel-spectrogram coefficients by applying adaptive filters and subsequent time-averaging is in principle possible. On the other hand, extensive experimental work leads to the conclusion, that the invariance induced by mel-spectrogram coefficients is both desirable and hard to infer by the learning process. Thus, the results achieved by adaptive end-to-end learning approaches are close to but slightly worse than results achieved by state-of-the-art reference architectures using standard input coefficients derived from the spectrogram.", "creator": "LaTeX with hyperref package"}}}