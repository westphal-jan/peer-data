{"id": "1709.02759", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Semantic Preserving Embeddings for Generalized Graphs", "abstract": "a new approach to the study of generalized graphs as semantic data structures purely using machine learning techniques is presented. tomorrow we show how vector representations maintaining many semantic characteristics of the original data can be directly obtained from a given derived graph using neural encoding architectures and considering the topological properties of the graph. semantic features of these new representations systems are tested purely by using understanding some machine learning tasks and new directions depending on efficient link discovery, entitity retrieval model and long distance query methodologies on large relational datasets are investigated using real datasets.", "histories": [["v1", "Thu, 7 Sep 2017 10:58:37 GMT  (1118kb,D)", "http://arxiv.org/abs/1709.02759v1", "Multi-lingual Paper. Main language: English. Additional Language: Spanish. 15 Figures. English: 28 pages. Spanish: 32 pages"]], "COMMENTS": "Multi-lingual Paper. Main language: English. Additional Language: Spanish. 15 Figures. English: 28 pages. Spanish: 32 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["pedro almagro-blanco", "fernando sancho-caparrini"], "accepted": false, "id": "1709.02759"}, "pdf": {"name": "1709.02759.pdf", "metadata": {"source": "CRF", "title": "Semantic Preserving Embeddings for Generalized Graphs", "authors": ["Fernando Sancho-Caparrini"], "emails": [], "sections": [{"heading": null, "text": "Semantic Preserving Embeddings for Generalized\nGraphs\nPedro Almagro-Blanco and Fernando Sancho-Caparrini\nSeptember 11, 2017"}, {"heading": "1 Introduction", "text": "In this work we present a new approach to the treatment of property graphs using neural encoding techniques derived from machine learning. Specifically, we will deal with the problem of embedding property graphs in vector spaces.\nThroughout this paper we will use the term embedding as an operation that allows to consider a mathematical structure, X, inside another structure Y , through a function, f : X \u2192 Y . We are interested on embeddings capable of capturing, within the characteristics of a vector space (distance, linearity, clustering, etc.), the interesting features of the graph.\nFor example, it would be interesting to get embeddings that, when projecting the nodes of the graph into points of a vector space, keep edges with the same type of the graph into the same vectors. In this way, we can interpret that the semantic associated to the relation has been captured by the embedding. Another option is to check if the embedding verifies clustering properties with respect to the types of nodes, types of edges, properties, or some of the metrics that can be measured on the graph.\nSubsequently, we will use these good embedding features to try to obtain prediction / classification / discovery tools on the original graph.\nThis paper is structured as follows: we will start by giving some preliminary definitions necessary for the presentation of our proposal and a brief introduction to the use of artificial neural networks as encoding machines. After this review, we will present our embedding proposal based on neural encoders, and we will verify if the topological and semantic characteristics of the original graph have been maintained in the new representation. After evaluating the properties of the new representation, it will be used to carry out machine learning and discovery tasks on real databases. Finally, we will present some conclusions and future work proposals that have arisen during the implementation of this work."}, {"heading": "2 Previous Definitions", "text": ""}, {"heading": "2.1 Generalized Graphs", "text": "The definition of Generalized Graph that we present below unifies different graph definitions that can be found in the literature, and allows to have a general framework to support the data structures necessary for our proposal. More information about generalized graphs can be found in [1].\nar X\niv :1\n70 9.\n02 75\n9v 1\n[ cs\n.A I]\n7 S\nep 2\n01 7\nDefinition 1. A Generalized Graph is a tuple G = (V,E, \u00b5) where:\n\u2022 V and E are sets, called, respectively, set of nodes and set of edges of G. \u2022 \u00b5 is a relation (usually functional, but not necessarily) that associates each\nnode or edge in the graph with a set of properties, that is, \u00b5 : (V \u222aE)\u00d7R\u2192 S, where R represents the set of possible keys for available properties, and S the set of possible values associated.\nUsually, for each \u03b1 \u2208 R and x \u2208 V \u222a E, we write \u03b1(x) = \u00b5(x, \u03b1). In addition, we require the existence of a special key for the edges of the graph, called incidences and denoted by \u03b3, which associates to each edge of the graph a tuple, ordered or not, of vertices of the graph.\nAlthough the definition that we have presented here is more general than those from related literature, we will also call them Property Graphs, since they are a natural extension of this type of graphs.\nIt should be noted that in generalized graphs, unlike traditional definitions, the elements in E are symbols representing the edges, and not pairs of elements of V and \u03b3 is the function that associates to each edge the set of vertices that it relates. Generalized graphs represent a simple and powerful generalization for most existing graph definitions and allow for working with broader concepts, such as hypergraphs, in a natural way."}, {"heading": "2.2 Encoding Neural Networks", "text": "Prediction-related tasks represent the most common application of feedforward neural networks. In this section we present this kind of networks from a diferent perspective, using them in a way that will be (and has been) of fundamental importance for the new results that have been obtained with them.\nA neural encoder is a neural network used for learning codings for datasets. Note that when a feedforward neural network has hidden layers, all the communication that occurs between input and output layer passes through each of the hidden layers. Thus, if we are trying to approximate a function by means of a feedforward network, after setting the parameters of the network we can interpret that a given hidden layer keeps the information required from the input data for the calculation of the function. Therefore, always from the point of view of the function that calculates the network, we can say that the section of the network from the input layer to the hidden layer encodes the input data, and the weights (and bias) of this section of the network define the encoding function between both spaces [13]. Similarly, we can understand that the part of the original network that goes from this hidden layer to the output layer defines a decoding function (see Figure 1).\nIf we leave aside the posterior layers (including the original output layer) and the associated parameters, we obtain a new neural network that produces as output a representation of the input space into a specific dimension vector space (the number of neurons in the hidden layer). We must remember that this representation is achieved as partial application of a complete feedforward network that approximates a prefixed function and, consequently, this coding is relative to this function (and, of course, to the approximation process).\nAn autoencoder is a specific neural encoder where the function to learn is the identity function and, consequently, the input and output layers have the\nsame number of neurons. As with normal encoders, when the network reaches an acceptable state (it is able to show an output similar enough to the input), the activations in the units of the hidden layers represent the encoding of the original data presented in the input layer [2].\nIf the number of units in the hidden layer differs from the number of units in the input layer (and output) we are also making a dimensional change when performing the encoding. In fact, this is one of the available methods to perform dimensionality changes by maintaining the structural characteristics (eg, proximity or similarity relations) of the training sets.\nIn this work we use neural encoders as a tool to perform generalized graph embeddings into vector spaces. Neural networks trained on adequate functions are used in order to verify to what extent semantic structures of the graph are conserved in the new vector space representation."}, {"heading": "3 Related Works", "text": "The application of neural encoders to texts has provided very interesting results. In 2013, T. Mikolov et al. [16] presented two new architectures, under the generic name of word2vec, to learn vector representations of words trying to minimize computational complexity while maintaining the grammatical properties present in the texts from which they are extracted: Continuous bag-of-words (CBOW) and Skip-gram. In this task the context of a word in a text is defined as the set of words that appear in adjacent positions to it. The two architectures presented in [16] consist of feedforward artificial neural networks with 3 layers: an input layer, a hidden layer (encoding layer) and an output layer, but they differ in the objective function they try to approximate. On the one hand, neural encoders with CBOW architecture receive the context of a given word as input and try to predict the word in its output. By contrast, encoders with Skip-gram architecture receive the word as input and try to predict the context associated with it. The main objective of the work of Mikolov et al. is to reduce the complexity in the neural model allowing the system to learn from a large volume of textual data. Until the arrival of word2vec, none of the available architectures had been able to train with more than a few million words. Through the relationship established between vocabulary words and their contexts, the model captures different types of similarity [17], both functional and structural, and\nprovides an embedding of words in vector space that reflects these similarities. In recent years different methods that try to learn vector representations of entities and relations in knowledge databases have been developed [11, 4, 21]. All of them represent the entities as elements of a given vector space and the relationships as a combination of the representations of the entities that participate in it.\nIn [4], embeddings of multi-relational data in vector space trying to verify some additional properties are proposed. Specifically, they look for a projection of nodes and types of edges, \u03c0, in vector space with two goals:\n1. To minimize the distance d(\u03c0(s)+\u03c0(l), \u03c0(t)) from each existing (observed)\nrelation (s l\u2212\u2192 t) in the dataset, where s represents the source element of the relation, l represents the relationship type, and t represents the target element of the relation.\n2. To maximize the distances d(\u03c0(s\u2032) + \u03c0(l), \u03c0(t)), d(\u03c0(s) + \u03c0(l\u2032), \u03c0(t)) and d(\u03c0(s) + \u03c0(l), \u03c0(t\u2032)), where s\u2032 and t\u2032 represent graph nodes, and l\u2032 repre-\nsents a type of relationship of the graph, for which the relations s\u2032 l\u2212\u2192 t, s l\u2212\u2192 t\u2032 and s l \u2032 \u2212\u2192 t do not exist (unobserved relationships) in the graph.\nTo improve the efficiency of the algorithm, the authors randomly sample the original graph for both existing and non-existing relationships. In [11], and in order to achieve better results in the projection, the authors follow a similar procedure but making use of a Siamese Neural Network 1. In [23] some of these techniques are grouped together on the same general theoretical framework making it possible to compare the complexity of the models and the obtained results.\nDespite the relationship between these works and our approach, the requirement to maximize unobserved relationships (essential for the results they obtain) works against one of the objectives we pursue, since we do not assume that the original graph has complete information and, in our context, random unobserved links creation is not a good idea. Moreover, the prediction of this kind of links is one of the task that we pursue.\nIn other cases, works that perform vector embedding of entities and relations in knowledge databases learns representation of the entities using the prediction of unobserved links (Link Prediction) as objective function, conditioning the embedding with a supervised learning. In our case, the encoding is only conditioned by the similarity of the contexts in which the entities are immersed, opening the possibility of using these representations to a wider range of tasks.\nDeepWalk [18] is a recent methodology that uses neural encoders to obtain vector representations nodes in uni-relational graphs using a very similar idea to word2vec. In this work the uni-relational graph is linearized using truncated random paths, interpreting the obtained paths as sentences. Subsequently, and completely equivalent to word2vec, they use these paths to train a neural encoder with Skip-gram architecture and to obtain an embedding of the nodes of the uni-relational graph in a vector space. This method does not allow to work with large uni-relational graphs efficiently, nor with multi-relational graphs.\n1A Siamese Neural Network is a type of comparative neural network composed by two networks that share weights and architecture (each one receives a data to be compared) and whose outputs are compared by a distance function.\nLINE [22] is able to learn d-dimensional representations of uni-relational graph nodes through two phases: first it learns d/2 dimensions generating random paths in Breath-First Search mode, then it learns the remaining d/2 dimensions by sampling the nodes that are strictly at distance 2 from the embedding node.\nNode2vec [12] groups and extends the ideas presented in DeepWalk and LINE. Specifically, the authors develop a flexible algorithm that, through two hyperparameters, allows to modify the generation of random paths that explore the environment of the nodes and that become their contexts. Based on two standard search strategies, Breath-First Sampling (BFS) and Depth-First Sampling (DFS), the two hyperparameters allow to control whether the random paths tend to a BFS or DFS strategy. In particular, they assert that a sampling guided by a BFS strategy results in embeddings reflecting structural equivalence between entities and that a sampling driven by a DFS strategy results in a embedding in which the homophilia is reflected. In this sense, DeepWalk is a specific case of this model. Node2vec have been evaluated in Multi-Label Classification and Link Prediction tasks.\nIn recent years, some works that use convolutional neural networks (CNN) to create vector representations of the nodes of uni-relational graphs have been published. In [15], the objective is to learn a function that encodes nodes by constructing descriptions of them. Outputs that represent the complete graph can also be obtained by applying some pooling operation [9]. In [8], the authors work with the convolution operator in the Fourier field, and generalize the convolutional networks to go from their original definition in low-dimensional regular euclidean spaces (where we work with images, videos or audio) to be able to work with high-dimensional irregular domains (multi-relational graphs obtained from social networks or biological phenomena). In [20] an extension of the Graph Convolutional Network [15] called the Relational Graph Convolutional Network is presented, which allows learning through the convolution and pooling operations typical of convolutional networks on multi-relational graphs.\nIn [19], the Graph Neural Network Model is defined, which converts the data graph into a recurrent neural network, and each node in a multi-layer feedforward network. The combination of these structures allows supervised learning where many of the weights of the network are shared, reducing the learning cost.\nIn Heterogeneous Network Embedding [7], a framework to perform network embeddings connecting data of different types in low-dimensional spaces is presented. As the model perform unsupervised learning, the new representation is adequate for any prediction task since it has not be conditioned. Using these representations, in [14] they face the task of automatically assign tags to nodes of different types in a heterogeneous network that has no types in the edges. The algorithm is designed to learn dependencies between node tags and to infer them exploiting the properties of the global graph and the characteristics of the neighbours of nodes. They impose two objectives: (1) grouping nodes of the same type that are connected (with less intensity as longer is the path connecting them), and (2) grouping nodes of different types if they share contexts. When working with property graphs, properties are represented as nodes.\nAs we have shown, there are numerous methodologies that allows to perform vector embedding of graphs, some of them are limited to working with unirelational graphs, others are conditioned through the generation of unobserved\nrelationships or do not capture the global semantic characteristics of a property graph. Our proposal, which we detail below, aims to obtain embeddings that are not affected by these limitations."}, {"heading": "4 Generalized Graph Embeddings", "text": "The selected architecture for our neural encoder is CBOW because, despite its simplicity and the low computational training cost, it obtains good results capturing both syntactical and semantic relations [16].\nIn a first approximation, and in order to evaluate to what extent the semantic structure given by the edges is maintained, we will make a projection on the vector space using only the set of nodes, V . In this way, following the analogy offered by the word2vec algorithm, our vocabulary will be the set of nodes of the graph and their associated properties values, S.\nA context, C, associated to a node n \u2208 V is obtained by randomly selecting a number of neighbouring nodes to n and their properties (selecting elements from N (n)\u222a\u00b5(n, \u00b7)), regardless of the edges that connects them and of the type of property. The number of selected nodes/properties determines the selection window size.\nFollowing a similar methodology to those from the previous section, we will generate a training set consisting of pairs (n,C), where n \u2208 V and C is one of its associated contexts. The neural encoder is trained using this training set and then the activations of the hidden layer of the neural network are used as vector representation of each node. Algorithm 1, GG2Vec, shows the followed procedure.\nAlgorithm 1 GG2Vec(G,N ,ws,D)\n1: training set = {} 2: for each 0 < i \u2264 N do 3: n = randomly selected element from VG 4: C = {} 5: for each 0 < i \u2264 ws do 6: e = randomly selected element from N (n) \u222a \u00b5G(n) 7: C = C \u222a {e} 8: end for 9: training set = training set \u222a {(v, C)}\n10: end for 11: Train a CBOW -like architecture with D neurons in hidden layer using\ntraining set 12: return The resulting encoding for each element in V\nWe will use these vector representations trying to solve some classification and discovery tasks in the original graph. The results of these tasks will provide a measure of reliability on the achieved embedding (Fig. 2).\nIn the embedding procedure the free parameters of the model, which will have to be adjusted in the various experiments to analyze its effectiveness and viability, are:\n\u2022 D, number of neurons in the hidden layer, determines the dimension of the vector space where we will embed the elements of the graph.\n\u2022 N , size of the training set, number of pairs (n,C) used to train the encoder.\n\u2022 ws, selection window size, number of neighbours and properties considered to construct the contexts of nodes in V .\nIn what follows, we will note by \u03c0 : V \u2192 RD the embedding obtained from the trained neural encoder.\nDespite generalized graphs allow hypergraph definition, next we present the embedding procedure for binary links because the databases that we use in the experiments represent binary relational data. After obtaining an embedding of the nodes of the graph, an embedding of the edges in the same vector space is induced (which will also be noted by \u03c0) in the following way:\nDefinition 2. If G = (V,E, \u03c4, \u00b5) is a binary Generalized Graph, and \u03c0 : V \u2192 RD is a node embedding, we extend \u03c0 : E \u2192 RD by:\ne \u2208 E, s e\u2192 t, then \u03c0(e) = \u2212\u2212\u2212\u2212\u2212\u2192\u03c0(s)\u03c0(t)\nSince the usual operations in vector spaces are widely used in current computation units (processors and GPUs), this new representation can be used to analyze, repair and extract information from multi-relational datasets efficiently. Some tasks that can improve with this type of embeddings are:\n\u2022 Clusters formed by nodes/edges in the new vector space can be used to induce missing properties in the elements of the graph (making use of distance, linearity or clustering relationships, for example).\n\u2022 Vector representations of the elements of a graph can help to obtain measures of similarity between them.\n\u2022 Analysis of vectors associated with the different families of relations (those sharing a common type, or verifying similar properties, for example) can help to detect missing relationships in the original dataset that in the new representation become evident. If the position of two nodes complies\nwith the representative vector of some type of relation, maybe those nodes should be connected by an edge with this type although that relation does not appear in the graph.\n\u2022 The representation of graph paths in the new space can help to develop more efficient ways to perform multi-relational queries."}, {"heading": "5 Empirical Evaluation", "text": "Let us perform some empirical evaluations of our method with two differentiated objectives:\n1. To analyse that the obtained vectorial representations maintain semantic characteristics of the original graphs.\n2. To perform classification and discovery tasks making use of the resulting embeddings.\nWe will say that an embedding respects the semantics of a property graph if, from the new representation, it is possible to obtain the types associated with nodes and edges despite them have not being present during the embedding process. The type of each node or edge will be determined by a key \u03c4 \u2208 R. In order to perform this verification, the several embeddings we will calculate will not receive information about types of nodes or edges in the original the graph (formally, they will not receive information about \u03c4). Hence, contexts associated with nodes of the graph, which are used to create the training set, are generated by randomly selecting a number (ws) of neighbouring nodes and values of their different properties in \u00b5 excluding \u03c4 ."}, {"heading": "5.1 Implementation details and experiments", "text": "Python has been chosen as programming language to perform the experimental evaluation 2. CBOW architecture implementation of Gensim toolkit 3 (version 0.12.4) has been used. In addition, Neo4j 4 has been used as a persistence system.\nEach embedding experiment, has been repeated 10 times, obtaining a standard deviation smaller than 2% in type prediction experiments. In the case of tasks related to Entity Retrieval these deviation is bounded by 8% and in the case of obtaining the target nodes of a typed path is bounded by 8.9%.\nMachine learning models used to learn from the new data representations arek-NN, Random Forest and Neural Networks. For the general classification tests, and unless otherwise is indicated, k-NN with k = 3 has been used as base comparison model."}, {"heading": "5.2 Datasets", "text": "The experiments were carried out in 3 different graph databases, two of them widely known by the data analysis community: WordNet and TheMovieDB. The third is a data set about the ecuadorian intangible cultural heritage.\n2https://github.com/palmagro/gg2vec 3https://radimrehurek.com/gensim 4http://neo4j.com\nDatasets have been partially manipulated to reduce their size and complexity. Below we give some details about each of these graphs in order to contextualize the characteristics that we will find in the results.\nWordNet R\u00a9 [10] is a database of english nouns, verbs, adjectives and adverbs. It is one of the most important resources in the area of computational linguistics, and has been built as a combination of dictionary and thesaurus, designed to be intuitive. Each element in the database represents a list of synonymous words (which they call synset), and the relationships that are established between the elements occur both at lexical and semantic level. In this work we have used a section of the 3.0 version, considering only entities and relations that are shown in Figure 3 (in a similar way to [11]), obtaining a graph with 97,593 nodes and 240,485 relations, with the distribution of types shown in Figure 4.\nTheMovieDB (TMDb) 5 is a dataset with information about actors, movies and television content. For our experiments we have considered all the TMDb entities that are connected by relations belonging to the types acts in, directed,\n5Available at https://www.themoviedb.org\ngenre and studio, obtaining a graph with 66,020 nodes and 125,624 relations. Figure 5 shows a graphical representation of the data schema, and Figure 6 shows the distribution by types of nodes and edges of this dataset.\nIt should be noted that Actor and Director types are overlapping, specifically in our dataset there are 44,097 nodes associated only to Actor type, 5,191 nodes associated only to Director type, and 846 nodes with both types simultaneously (multi-type nodes).\nEcuadorian Intangible Cultural Heritage (EICH Database of Ecuador) corresponds to a section of the National Institute of Ecuadorian Cultural Heritage database 6 which contains 38,990 nodes and 55,358 relations distributed through 11 types of nodes and 10 types of edges, with information about the intangible cultural heritage of Ecuador. This database is the most heterogeneous of the three analysed, presenting more typology in both nodes and edges, and also its elements have more properties than the other two considered datasets. Figures 7 and 8 show the schema and distribution of nodes and edges in this graph, respectively.\nIf we define the semantic richness of a node as the sum of the number of relations it participates in and the number of properties it possesses, we can construct the histogram of semantic richness for each dataset (Fig. 9). In the case of WordNet, the average semantic richness is 5.56, in the case of TMDb it is 3.21, and in the case of EICH it is 7.86. The different behaviour of this distribution in the studied cases may be help to understand the results.\n6Accessible from http://www.inpc.gob.ec"}, {"heading": "5.3 Node Types Prediction", "text": "Our first experiment aims to predict \u03c4 function. In a similar way, we could try to predict any property of \u00b5, always being careful that it is not used during training.\nA first intuition about how the achieved embeddings maintain the semantic structures can be obtained by analysing how the various types are distributed in the vector space.\nFigure 10 shows two projections of a section of TMDb graph embedding. The representation on the left shows a random selection of Movie and Actor nodes using a embedding in a space of 200 dimensions (which has been projected later on a two-dimensional space using Multi-Dimensional Scaling [5]), while the representation on the right shows the same section of the graph making use of an embedding on a 2 dimension space. Although the dimensionality reduction considered is clearly excessive (but necessary to visualize the data in these pages), both representations show that the TMDb nodes are not ran-\ndomly distributed with respect to the types, which shows that, the embedding maintains information relative to the type of nodes, and we consider that the embedding process captures the semantic associated to node types.\nIn addition to the freedom of choice over the parameters involved in the encoding, we find some additional degrees of freedom when deciding which machine learning methiod will be used to learn from the vector representation of the nodes in a graph. As a first approximation, an exhaustive study of the free parameters of the model has been made using the classification method kNN. The reason for choosing this model focuses on two fundamental aspects: it depends only on its own parameter (the value of k, which is known to work relatively well for k = 3) and, in spite of its simplicity, provides robust results that serve as a comparative basis for other more sophisticated classification models.\nTable 1 shows optimal values of free parameters of the embedding using kNN as a later learning model (with k = 3). Figure 11 shows the results of this\nanalysis for the three considered graphs, where prediction rates above 70 % are obtained for all of them.\nIn all cases, the optimal training set size to perform the automatic prediction of node types is proportional to the number of nodes in the graph. Both EICH and TMDb (for WordNet we do not know the optimal value, because it is increasing in the analysed range) show a reduction in the prediction rate from the optimum value, this can be due to an overfitting related to the existence of nodes of different types with the same label.\nRegarding the vector space dimension, no big changes can be observed when we increase D above 10-15, a relatively low dimension, but it is almost imperceptible. We can interpret that around 10-15 dimensions are enough to capture the complexity of the analysed graphs.\nThe study of the influence of the selection window size (ws) shows that small values of this parameter are required to obtain good results in the prediction of node types. It is important to note that the best prediction is not achieved in any case with ws = 1, since this would mean that the system does not need to receive node-context pairs as elements of the training set but would suffice to show instances of the relations present in each node."}, {"heading": "5.3.1 Using other prediction models", "text": "Once the parameters of the embedding shown in Table 1 are set, we proceed to compare the predictive capacity with some other classification methods on the same task. Specifically, we will compare the results of k-NN with those obtained through Random Forest and Feedforward Neural Networks.\nFigure 12 shows obtained results. (a) Shows the variation of the results provided by k-NN depending on k. In (b), results obtained varying the number of trees using Random Forest as a later machine learning are shown. Finally, in (c) the results of the neural network are shown when the number of neurons in\nthe hidden layer is modified. We also present averaged confusion matrices after performing 10 experiments using the optimal parameters indicated above and k-NN: WordNet (Table 2), TMDb (Table 3), and EICH (Table 7). These matrices capture the semantic similarities between node types. In EICH, for example, Canton, Parroquia and Provincia show overlapping behaviour because they all represent highly correlated geospatial information. In TMDb something similar occurs, ACTOR and DIRECTOR nodes appear related because, as we mentioned, there are numerous nodes in this database having both types simultaneously."}, {"heading": "5.4 Edge Types Prediction", "text": "The second experiment aims to determine the goodness of the embedding in edge types prediction task.\nFigure 13 shows a two-dimensional projection from a randomly selected set of edges of the TMDb dataset. It can be observed that Genre edges do not form a single cluster, but a collection of peripheral ones that corresponding to\nvalues Action, Comedy, Drama, Documentary, Horror and Crime, showing a different semantic behaviour respect other edge types. This might indicate that Genre may not form a semantically unique edge type, and that its behaviour reflects some heterogeneity in design decisions when constructing the original database model. It is here where this type of analysis shows unique characteristics that can make it suitable to be used as an additional normalizer to databases covering also semantic and not only structural information.\nIn Figure 14, results of edge type prediction using k-NN method are shown. Taking into account that the percentage of correctness is above 80 % in all the studied datasets (even over 95 % in some of them), we can conclude that our embedding methodology maintains the semantic properties of the edges.\nIn general, we can observe that the training set size necessary to obtain good results when predicting edge types is superior to that required to make a good prediction of the node types for the three analysed graphs. In addition, although WordNet is the dataset with best results when predicting node types, in the case of edge types the best results are obtained for EICH.\nAs in the previous case, embeddings require a relatively low dimension, D ' 15.\nThe behaviour of edge types prediction tasks according to ws shows values higher than those required for nodes. In any case, it is important to note that, again, the best prediction is not achieved in any case with ws = 1."}, {"heading": "5.4.1 Using other prediction models", "text": "Following the same methodology as for node types, Figure 15 shows results obtained by the three same automatic classification methods in the edge types prediction case. These classification tasks were performed with embeddings using the parameters presented in Table 1.\nConfusion matrices after averaging 10 experiments and using k are shown in tables 4, 6, and 8. Results show that the embeddings capture similarities between different types of edges. In the case of EICH, edge types related to geospatial information show an overlapping behaviour with edges of type LENGUA, because there is a correlation between the languages and the territories where they are spoken. WordNet shows similar behaviour between hypernym and hyponim types. In the case of TMDb, as expected, DIRECTED edges are confused with ACTED IN edges due to the overlapping between ACTOR and DIRECTOR nodes.\nExperimental results show that for the analysed datasets and with the proposed methodology, the obtained embedding preserves the semantics associated with edge types, and it is able to detect semantic similarities between them.\nIt should be noted that, since there may be edges of different types between\nthe same pair of nodes, the results in edge types prediction may have been affected. This fact has not been taken into account in our experiments, so the results in edge types prediction could be improved. Undoubtedly, using this method we can never obtain absolute reliability about the results, but it can be taken into account for additional data normalization tasks, aproximate solution method or as a filtering method for other options."}, {"heading": "5.5 Entity Retrieval", "text": "In order to show the goodness and usefulness of our embeddings in other prediction tasks, we will try to recover missing relations. Let us consider a subset of edges, E\u2032 \u2208 E, that belong to the original graph G and consider the subgraph G\u2032 = (V,E \\ E\u2032, \u03c4, \u00b5), that will be used to learn an embedding, \u03c0. We will try to obtain the target node associated with each edge in E\u2032 using only its source node and \u03c0.\nFormally, given an edge e = (s, t) \u2208 E\u2032 of type \u03c4(e), which has been eliminated before the embedding process, we will try to obtain t from \u03c0, \u03c4(e) and s. This task is known as Entity Retrieval [6].\nTo obtain the target node of the missing relations we will use the representative vector of edge type which we define as:\nDefinition 3. Given a property graph G = (V,E, \u03c4, \u00b5), the representative vector, \u03c0(\u03c9), associated to an edge type \u03c9 \u2208 \u03c4(E), is the mean vector of all vectors representing edges of type \u03c9.\nIf we denote E\u03c9 = \u03c4 \u22121({\u03c9}) = {e \u2208 E : \u03c4(e) = \u03c9}, then:\n\u03c0(\u03c9) = 1\n#(E\u03c9)\n\u2211\ne\u2208E\u03c9 \u03c0(e)\nA candidate of the target of a relation e from the source node can be obtained through the representative vector by:\n\u03c0(te) = \u03c0(s) + \u03c0(\u03c4(e))\nThen, we obtain a ranking of the nodes of the graph by using the distances to \u03c0(te).\nTable 5 shows the first ten ranked results after applying this method to query nodes related by hypernym type to different source nodes in WordNet graph. The results are filtered to show only nodes of type NOUN.\nIt is possible that in some cases the source node of the relation has not been sampled furing the embedding process and, therefore, we can not construct its vector representation. In these cases the edge can not be evaluated and it will not be considered.\nTo evaluate the goodness of the embedding with respect to this task we will use the Mean Reciprocal Rank metric, a usual metric in Information Retrieval [6, 24].\nDefinition 4. The Reciprocal Rank associated with a particular result with a list of possible answers given a query, is the inverse of the position that the correct result occupies in that list. The Mean Reciprocal Rank (MRR) is the\naverage of the Reciprocal Ranks for a set of queries, Q:\nMRR = 1\n|Q|\n|Q|\u2211\ni=1\n1\nranki\nwhere ranki is the position of the correct answer in each ranking.\nIn Figure 16 results obtained using MRR metric on EICH, TMDb and WordNet are shown (after removing from the ranking the nodes with wrong type). As it can be seen, our method produces excellent results that are improved when we increase the size of the training set used to perform the embedding."}, {"heading": "5.6 Typed Paths Prediction", "text": "Finally, to show the possibilities offered by a generalized graph embedding, we present a technique to obtain the target node of a given typed path, using the type of the path and the source node of the same.\nA typed path is a sequence of node and edge types that correspond to one path in the graph (in some context, those typed paths are known as traversals):\nDefinition 5. A typed path of a generalized graph G = (V,E, \u03c4, \u00b5) is a sequence\nT = t1 r1\u2192 t2 r2\u2192 . . . rq\u2192 tq+1 where ti \u2208 \u03c4(V ) and ri \u2208 \u03c4(E). We denote Tp(G) the set of typed paths in G.\nDefinition 6. We define an application, Tp, that associates to each possible typed path in G the set of paths that verify it, such that if T = t1 r1\u2192 t2 r2\u2192 . . .\nrq\u2192 tq+1, then for each \u03c1 \u2208 Tp(T ), (t1, . . . , tq+1) is the ordered sequence of node types in \u03c1, and (r1 . . . , rq) is the ordered sequence of edge types in \u03c1.\nOur goal is to obtain the target node of a path, just from the source node representation and the representative vector of the type that such path verifies. In this case we are not removing the paths before performing the embedding because we only pursue a new way to perform long distance queries in graph databases (not prediction tasks). It should be noted that this kind of queries in the current persistence systems are computationally expensive and that, with methods like the presented here, better performance can be reached by sacrificing optimality.\nWe will define the representative vector of a path as we did before for edges (indeed, edges can be seen as a particular case of types paths with length 1).\nDefinition 7. The representative vector of a path, n1 \u03c1 nk, is the vector separating the representations of the source node of the path, \u03c0(n1), and the target node of the path, \u03c0(nk). Then:\n\u03c0(\u03c1) = \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 \u03c0(n1)\u03c0(nk) = \u03c0(nk)\u2212 \u03c0(n1)\nThe representative vector of a typed path, T , is the average vector of all representative vectors of paths of type T :\n\u03c0(T ) = 1 |Tp(T )| \u2211\n\u03c1\u2208Tp(T ) \u03c0(\u03c1)\nSimilarly as in the case of edge types, it is possible that some source nodes have not been sampled by the embedding process and thus their representation do not exist. In those cases, such path will not be evaluated.\nWe have performed typed path entity retrieval experiments using a similar approach as in the edge case. We have filtered target nodes depending on the type of the last element in the node sequence of the path and we have used the MRR metric again. Experiments have been performed on EICH dataset since this dataset represents a more complex schema and allows more complex typed paths queries.\nFigure 17 shows some results obtained using the following typed paths (edge types are omitted as they can be directly inferred from the schema in Figure 7):\n1. T1 = (Inmaterial r1\u2192 DetSubambito r2\u2192 Subambito r3\u2192 Ambito)\nIt is associated to paths of length 3 and contains information about Ambito nodes (there are 5 of this type in EICH) associated with Inmaterial nodes in the graph.\n2. T2 = (Inmaterial r1\u2192 Parroquia r2\u2192 Canton r3\u2192 Provincia)\nIt is associated to paths of length 3 and contains information about Provincia nodes (there are 24 of this type in EICH) associated with Inmaterial nodes in the graph.\nAs Figure 17 shows, this methodology performs well in obtaining the target node of T1 paths, obtaining results near to 70% in MRR metric for a training set with more than 3 million pairs. In the case of T2, its results tends to be worst when we increase the training set size over a million pairs. The problem associated to T2 is more complex, because of the number of Provincia nodes in the graph and the big confusion with this kind of nodes.\nWe need to perform more tests to validate it, but this application shows that it can be used to approximate long distance queries in databases, a task specially inefficient in classic persistence systems."}, {"heading": "6 Conclusions and Future Work", "text": "The main goal of this work has been to allow traditional machine learning algorithms to learn from multi-relational data through vector space embedding of generealized graphs, performing automatic feature extraction, and maintaining semantic structures in the obtained representation. We have analysed the different options that a semantic preserving embedding for property graphs (generalized graphs) in vector spaces allows.\nIf there exists an element (a subgraph) that is immersed in a graph database (a generalized graph), the task of manual feature extraction to characterize it can be very complex. The approximation presented in this work automatically obtains a vectorial representation of such relational data through a sampling of the network information. In this way, this work aliviate the process of feature extraction in multi-relational data and takes into account the global information available during the embedding process.\nThere are not many works that use neural encoders to perform multi-relational graph embeddings in vector spaces. Our methodology uses simple architectures to obtain vector representations of nodes and edges that maintain (to some extend) the structural and semantic characteristics of the original graph. In addition, it has been experimentally demonstrated that unobserved semantic connections in the original graph (due to lack of information or inconsistency) can be recovered and that it is possible to perform normalization tasks in graph databases or to optimize some kind of queries using our methodology.\nEvaluation tests have shown that the accuracy and precision of machine\nlearning algorithms on the new vector representation can inform about the quality of the semantic structure of the dataset. For example, confusion between some nodes / edges in classification tasks can inform about some adjustments to be done in the dataset in order to reflect the semantic characteristics correctly (and to improve classification or prediction tasks). A detailed report about how different node and edge types clusters are overlapping in the new space would be very useful when normalizing graph database data schemas.\nThe training set and selection window size positively influence the applicability of the obtained embedding, but these facts must be studied in greater depth, since they can be crucial in the automatic tunning of the encoding paremeters.\nWe have explored how vector structures can be used to retrieve information from generalized graphs, as shown by Entity Retrieval and typed paths experiments. It is likely that looking for complex structures in the projected space will be simpler than in the original one. In fact, the use of a second layer of learning models after neural encoding can improve the results of various tasks related to information retrieval tasks in semantic graphs. Results show that it is worth considering this line of research. Although not enough experiments have been carried out on typed path queries, the results obtained show that executing query time can be reduced dramatically by sacrificing optimality. This type of queries are very expensive in classical databases, and although graph databases help to reduce their computational cost they still present hard efficiency problems when the path to query is longer than 3.\nCompared to other approaches in the same direction, this paper presents the novelty of working with more general semantic contexts, and not only with random paths, which suppose a linearisation of the original graph structure. But these are not the only options to carry out generalized graph encodings through neural networks, as future work, we could achieve vectorial encodings by using neural autoencoders, so that the neural encoder will learn the identity function for the elements of the graph, avoiding the bias imposed by the function that relates the elements to their context.\nIt should be noted that during the revision of this document new tools optimizing word2vec related learning procedures have been published [3]. In spite of the probable improvement that these tools would suppose in our methodology, we have decided not to take them into account since they do not modify the essence of our proposal, although it would alleviate the computational costs associated to the performed experiments.\nEfficiency improvements in long-distance queries shown in section 5.6 deserve to be evaluated in greater depth and compared with other similar methods. Some results related to the semantic analysis of generalized graphs have not been presented in this paper although they are expected to be in later works. Options such as sampling the context of edges, performing their embedding and infer from it an embedding for nodes have not been taken into account and can offer interesting results. Even embeddings of the both sets simultaneously should be considered.\nAlso as a future line of work, to analyse the characteristics of the embeddings should be considered. A first step is about how to construct the training set to be consumed by the neural encoder. In a first approximation the construction of the training set has been totally random, ie, all nodes have the same probability of being sampled, as well as all their properties and neighbours. This may not be the most appropriate way depending on the type of activity to be performed\nwith the obtained embedding. For example, it may be beneficial that nodes with a greater semantic richness are more likely to be in the training set, this option may contribute to explore regions initially less likely to be considered.\nIt should also be noted that the possibility of working with continuous properties in nodes and edges is open, and should be considered to expand the capacity of our methodology. There are direct mechanisms to include the presence of continuous properties, it remains as work to begin by testing them and to measure later to what extent other approaches can be taken into account.\nSimilarly, it would be interesting to think about neural encoders that make use of recurrent neural networks to analyse the behaviour of dynamic relational information, an area practically unexplored today."}, {"heading": "Acknowledgement", "text": "We thank the \u201dInstituto Nacional de Patrimonio Cultural\u201d of Ecuador for the information related to the Intangible Cultural Heritage of Ecuador. This work has been partially supported by TIC-6064 Excellence Project of the Junta de Andaluc\u0301\u0131a and TIN2013-41086-P from Spanish Ministry of Economy and Competitiveness (cofinanced with FEDER funds) and by Research and Graduate Studies Head Department of Central University of Ecuador."}, {"heading": "2. Definiciones Previas", "text": ""}, {"heading": "2.1. Grafos Generalizados", "text": "La definicio\u0301n de Grafo Generalizado que presentamos a continuacio\u0301n unifica diferentes variantes de grafo que se pueden encontrar en la literatura, y nos permite disponer de un marco lo suficiente general para dar soporte a las estructuras de datos necesarias para nuestra propuesta. Se puede encontrar ma\u0301s informacio\u0301n acerca de grafos generalizados en [1].\nDefinicio\u0301n 1. Un Grafo Generalizado es una tupla G = (V,E, \u00b5) donde:\nV y E son conjuntos, que llamaremos, respectivamente, conjunto de nodos y conjunto de aristas de G.\n\u00b5 es una relacio\u0301n (habitualmente la consideraremos funcional, pero no es necesario) que asocia a cada nodo o arista en el grafo su conjunto de propiedades, es decir, \u00b5 : (V \u222a E) \u00d7 R \u2192 S, donde R representa el conjunto de posibles claves para dichas propiedades, y S el conjunto de posibles valores asociados a las mismas.\nHabitualmente, para cada \u03b1 \u2208 R y x \u2208 V \u222a E, escribiremos \u03b1(x) = \u00b5(x, \u03b1). Adema\u0301s, exigiremos la existencia de una clave destacada para las aristas del grafo, que llamaremos incidencias y denotaremos por \u03b3, que asocia a cada arista del grafo una tupla, ordenada o no, de ve\u0301rtices del grafo.\nAunque la definicio\u0301n que hemos presentado aqu\u0301\u0131 es ma\u0301s general que las que se pueden encontrar en la literatura relacionada, tambie\u0301n los denominaremos Grafos con Propiedades, como hacen muchos de esos trabajos, ya que suponen una extensio\u0301n natural de este tipo de grafos.\nCabe indicar que en los grafos generalizados que acabamos de mostrar, y a diferencia de las definiciones tradicionales, los elementos en E son s\u0301\u0131mbolos que representan a las aristas, y no pares de elementos de V , y es \u03b3 la funcio\u0301n que asocia a cada arista el conjunto de ve\u0301rtices que relaciona, pudiendo trabajar con conceptos ma\u0301s amplios, como el de hipergafo, de forma natural."}, {"heading": "2.2. Redes Neuronales Codificadoras", "text": "El uso ma\u0301s habitual de las redes neuronales feedforward ha sido como ma\u0301quinas de ca\u0301lculo, pero en esta seccio\u0301n presentamos un uso que sera\u0301 (y ha sido) de fundamental importancia para los nuevos resultados que se han obtenido con ellas.\nObse\u0301rvese que cuando una red feedforward tiene capas ocultas toda la comunicacio\u0301n que se produce entre la capa de entrada y la de salida pasa por cada una de las capas ocultas. De esta forma, si estamos intentando aproximar una funcio\u0301n por medio de una red feedforward que tiene una capa oculta, tras el\najuste de los para\u0301metros de la red (se haga por el procedimiento que se haga) podemos interpretar que la capa oculta mantiene la informacio\u0301n necesaria de los datos de entrada que son imprescindibles para el ca\u0301lculo de la funcio\u0301n. Por ello, siempre desde el punto de vista de la funcio\u0301n que calcula la red, podemos decir que la capa oculta codifica los datos de entrada, y los pesos (y bias) que se han usado definen la funcio\u0301n de codificacio\u0301n entre ambos espacios [13]. De igual forma, podemos entender que la parte de la red original que va desde la capa oculta que consideremos hasta la capa de salida define una decodificacio\u0301n hacia el espacio de llegada (ver Figura 1).\nFigura 1: Codificador neuronal.\nEl objetivo de los codificadores neuronales es aprender una codificacio\u0301n a partir de un conjunto de datos. Si prescindimos de las capas posteriores (incluida la capa de salida original) a partir de una capa interior dada, obtenemos una nueva red neuronal que produce como salida una representacio\u0301n del espacio de entrada en un espacio de dimensio\u0301n concreta (el nu\u0301mero de neuronas en la capa oculta que se ha considerado). Debemos recordar que esta representacio\u0301n se consigue como aplicacio\u0301n parcial de una funcio\u0301n que se ha obtenido a partir de una red feedforward completa que aproxima una funcio\u0301n prefijada y, consecuentemente, la codificacio\u0301n obtenida es relativa a esta funcio\u0301n (y, por supuesto, al proceso de aproximacio\u0301n).\nSi el nu\u0301mero de unidades usado en la capa oculta para la codificacio\u0301n difiere del nu\u0301mero de unidades en la capa de entrada (y salida) estaremos adema\u0301s haciendo un cambio dimensional al realizar la codificacio\u0301n. De hecho, es uno de los me\u0301todos disponibles para realizar cambios de dimensionalidad manteniendo las caracter\u0301\u0131sticas estructurales presentes en los conjuntos de entrenamiento (por ejemplo, las relaciones de proximidad o similitud).\nUn autocodificador es un caso concreto de codificador neuronal en el que se ha intentado aprender la funcio\u0301n identidad y, en consecuencia, las capas de entrada y salida poseen el mismo nu\u0301mero de neuronas. Es decir, el conjunto de muestras de entrenamiento ser\u0301\u0131a S = {(~x1, ~x1), . . . , (~xN , ~xN )}. Al igual que ocurre cuando trabajamos con codificadores, cuando la red alcanza un estado aceptable (es capaz de mostrar una salida suficientemente parecida a la entrada para cada uno de los ejemplos), las activaciones en las unidades de las capas ocultas capturan informacio\u0301n del dato original ~x presentado en la capa de entrada [2].\nEn este trabajo usaremos codificadores neuronales como medio de inmersio\u0301n de Grafos con Propiedades en espacios vectoriales. Para ello, haremos uso\nde redes neuronales entrenadas con funciones adecuadas con el fin de comprobar hasta que\u0301 punto las estructuras sema\u0301nticas del grafo se conservan en las propiedades vectoriales de la inmersio\u0301n."}, {"heading": "3. Trabajos Relacionados", "text": "El uso de codificadores neurales para la inmersio\u0301n de palabras en espacios vectoriales ha proporcionado resultados muy interesantes. En 2013, T. Mikolov et al. [16] presentaron dos nuevas arquitecturas, bajo el nombre gene\u0301rico de word2vec, para aprender representaciones vectoriales de palabras tratando de minimizar la complejidad computacional y manteniendo propiedades sema\u0301nticas y gramaticales presentes en los textos de los que se extraen: Continuous bag-of-words (CBOW) y Skip-gram. En este modelo, el contexto de una palabra en un texto se define como el conjunto de palabras que aparecen en posiciones adyacentes a e\u0301sta. Las dos arquitecturas presentadas en [16] consisten en redes neuronales artificiales feedforward con 3 capas: una capa de entrada, una capa oculta (capa de codificacio\u0301n) y una capa de salida, pero difieren en la funcio\u0301n objetivo que intentan aproximar. Por un lado, los codificadores neuronales con arquitectura CBOW toman el contexto de una palabra como entrada y tratan de predecir la palabra en su salida. Por el contrario, los codificadores con la arquitectura Skip-gram reciben la palabra como entrada y tratan de predecir el contexto asociado a ella. El objetivo principal del trabajo de Mikolov et al. es reducir la complejidad del modelo neuronal para permitir al sistema aprender de un gran volumen de datos textuales. Hasta la llegada de word2vec, ninguna de las arquitecturas disponibles hab\u0301\u0131a podido entrenar con ma\u0301s de unos pocos millones de palabras. A trave\u0301s de la relacio\u0301n establecida entre las palabras del vocabulario y sus contextos, el modelo captura diferentes tipos de similitud, tanto funcional como estructural, y proporciona una inmersio\u0301n vectorial de palabras que refleja estas similitudes.\nEn los u\u0301ltimos an\u0303os se han desarrollado diferentes me\u0301todos que tratan de aprender representaciones vectoriales de entidades y relaciones en bases de conocimiento [11, 4, 20]. Todas ellas representan las entidades como elementos de un espacio vectorial determinado y las relaciones como combinacio\u0301n de las representaciones de las entidades que participan en ella.\nEn [4] se propone una inmersio\u0301n de datos multi-relacionales en un espacio vectorial intentando verificar algunas propiedades adicionales. Concretamente, se busca una proyeccio\u0301n de nodos y tipos de aristas, \u03c0, en el espacio vectorial con el objetivo de:\n1. Minimizar la distancia d(\u03c0(s) + \u03c0(l), \u03c0(t)) de cada relacio\u0301n (s l\u2212\u2192 t) exis-\ntente (observada) en el conjunto de datos, donde s representa el elemento origen de la relacio\u0301n, l representa el tipo de relacio\u0301n, y t representa el elemento destino de la relacio\u0301n.\n2. Maximizar las distancias d(\u03c0(s\u2032)+\u03c0(l), \u03c0(t)), d(\u03c0(s)+\u03c0(l\u2032), \u03c0(t)) y d(\u03c0(s)+ \u03c0(l), \u03c0(t\u2032)), donde s\u2032 y t\u2032 representan nodos del grafo, y l\u2032 representa un\ntipo de relacio\u0301n del grafo, para los que las relaciones s\u2032 l\u2212\u2192 t, s l\u2212\u2192 t\u2032 y s l\u2032\u2212\u2192 t no existen (relaciones no observadas) en el grafo.\nPara mejorar la eficiencia del algoritmo, los autores hacen un muestreo aleatorio del grafo original, tanto para las relaciones existentes como para las no existentes. En [11], y con el fin de conseguir mejores resultados en la proyeccio\u0301n, los autores siguen un procedimiento similar pero haciendo uso de una red neuronal siamesa1 en vez de una red neuronal esta\u0301ndar. En [22] se agrupan algunas de estas te\u0301cnicas sobre un mismo marco teo\u0301rico general que permite comparar la complejidad de los modelos obtenidos y de los resultados.\nPese a la relacio\u0301n existente entre estos trabajos y nuestra aproximacio\u0301n, el segundo requerimiento que imponen para maximizar las distancias en las relaciones no observadas (y que es imprescindible para los resultados que obtienen) va en contra de uno de los objetivos que perseguimos, ya que no suponemos que el grafo con propiedades original tenga informacio\u0301n completa y, en consecuencia, las relaciones no observadas pueden deberse a una carencia informativa y no a una inexistencia real. Au\u0301n ma\u0301s, precisamente la prediccio\u0301n de este tipo de relaciones no observadas es una de las razones por las que buscamos una inmersio\u0301n en un espacio que ofrezca una capacidad de ana\u0301lisis adicional.\nAdema\u0301s, la mayor\u0301\u0131a de los trabajos que realizan aprendizaje de representaciones vectoriales de entidades y relaciones en bases de conocimiento tienen como objetivo evaluar la posibilidad de existencia de determinadas relaciones (Link Prediction), condicionando con un aprendizaje supervisado la representacio\u0301n de las entidades. En nuestro caso, la codificacio\u0301n que trataremos de aprender estara\u0301 so\u0301lo condicionada por conseguir representaciones vectoriales que capturen la similitud de los contextos en los que se encuentran inmersas las entidades que representan, abriendo as\u0301\u0131 la posibilidad de usar estas representaciones a un abanico ma\u0301s amplio de tareas.\nDeepWalk [17] es una metodolog\u0301\u0131a reciente que utiliza codificadores neuronales para representar los nodos de un grafo uni-relacional haciendo uso de una idea muy similar a la que Mikolov et al. presentaron para la inmersio\u0301n de grandes conjuntos de textos en espacios vectoriales [16]. En concreto, en el citado trabajo el grafo uni-relacional es linealizado a partir de la generacio\u0301n de caminos aleatorios truncados, interpretando los caminos obtenidos como frases y considerando a partir de ellos un conjunto de entrenamiento con la forma\nS = {(n1, C1), . . . , (nN , CN )}\ndonde ni representa un nodo concreto del grafo y Ci un contexto generado a partir de dichos caminos aleatorios truncados, es decir, el conjunto de nodos que aparecen en el mismo camino segu\u0301n el orden de recorrido. Posteriormente, y de forma completamente equivalente a word2vec, se usa S para entrenar un codificador neuronal con arquitectura Skip-gram y obtener as\u0301\u0131 una inmersio\u0301n de los nodos del grafo uni-relacional en un espacio vectorial. El me\u0301todo propuesto no permite trabajar con grafos uni-relacionales grandes de manera eficiente, y tampoco con grafos multi-relacionales.\nEn [21] se presenta la metodolog\u0301\u0131a LINE para aprender una representacio\u0301n d-dimensional de los elementos inmersos en un grafo uni-relacional a trave\u0301s de dos fases: primero se aprenden d/2 dimensiones generando caminos aleatorios en modo Breath-First Search, posteriormente, se aprenden las d/2 dimensiones\n1Una red neuronal siamesa es un tipo de red neuronal comparativa compuesta por dos redes que comparten pesos y arquitectura (cada una recibe un dato a ser comparado) y cuyas salidas son comparadas mediante una funcio\u0301n de distancia.\nrestantes haciendo un muestreo de los nodos que esta\u0301n estrictamente a distancia 2 del nodo origen.\nNode2vec [12] agrupa y extiende las ideas presentadas en DeepWalk y LINE ampliando las posibilidades a la hora de construir los caminos aleatorios en el grafo. Concretamente, los autores desarrollan un algoritmo flexible que, a trave\u0301s de dos hiperpara\u0301metros, permite modificar la generacio\u0301n de los caminos aleatorios que exploran el entorno de los nodos y dan lugar a su contexto. A partir de dos estrategias esta\u0301ndar de bu\u0301squeda, Breath-First Sampling (BFS) y Depth-First Sampling (DFS), los dos para\u0301metros permiten controlar si el camino aleatorio tiende a una estrategia BFS o DFS. En particular, afirman que un muestreo guiado por una estrategia BFS da lugar a inmersiones que reflejan la equivalencia estructural entre las entidades y que un muestreo guiado por una estrategia DFS da lugar a una inmersio\u0301n en la que se refleja la homofilia. Por medio de experimentos los autores evalu\u0301an su capacidad para llevar a cabo Multi-Label Classification y Link Prediction. En este sentido, DeepWalk ser\u0301\u0131a un caso concreto en el que el valor de ambos para\u0301metros es el mismo.\nEn ninguno de los trabajos anteriores ([17, 21, 12]) se trabaja con grafos multi-relacionales, y se limitan a detectar la homofilia y la equivalencia estructural en las representaciones vectoriales.\nEn los u\u0301ltimos an\u0303os se han publicado algunos trabajos que hacen uso de Redes Neuronales Convolucionales (CNN) para crear representaciones vectoriales de los nodos de un grafo uni-relacional. En [15], el objetivo es aprender una funcio\u0301n que codifique las caracter\u0301\u0131sticas de los nodos de un grafo a partir de una descripcio\u0301n de las mismas (almacenadas en una matriz de descripciones) y de la matriz de adyacencias del grafo. Tambie\u0301n se pueden obtener salidas que representen el grafo completo aplicando alguna operacio\u0301n de tipo pooling [9]. En [8] sin embargo, se trabaja con el operador de convolucio\u0301n en el campo de Fourier, y generalizan las redes convolucionales para pasar de su definicio\u0301n original en espacios eucl\u0301\u0131deos regulares de baja dimensio\u0301n (donde se trabaja naturalmente con ima\u0301genes, v\u0301\u0131deos o audios) a poder trabajar con dominios irregulares de alta dimensio\u0301n (grafos multi-relacionales obtenidos de redes sociales o de feno\u0301menos biolo\u0301gicos). En [19] se presenta una extensio\u0301n del modelo Graph Convolutional Network [15] denominado Relational Graph Convolutional Network, que permite realizar aprendizaje a trave\u0301s de las operaciones de convolucio\u0301n y pooling t\u0301\u0131picas de las redes convolucionales sobre grafos multi-relacionales.\nEn [18] se define el modelo Graph Neural Network Model, que convierte el grafo de datos en una red neuronal recurrente, y cada nodo del mismo en una red feedforward multi-capa. La combinacio\u0301n de estas estructuras permite llevar a cabo un aprendizaje supervisado en el que muchos de los pesos de la red son compartidos, reduciendo as\u0301\u0131 el coste en el aprendizaje.\nEn [7] se presenta Heterogeneous Network Embedding, un framework para hacer inmersiones de redes que conectan datos de diferentes tipos en espacios de baja dimensio\u0301n utilizando una red neuronal profunda. Como el aprendizaje que se lleva a cabo es no supervisado, la nueva representacio\u0301n es adecuada para aplicar cualquier algoritmo de aprendizaje automa\u0301tico ya que no se ha condicionado el aprendizaje a una tarea determinada. En [14] se enfrentan a la tarea de asignacio\u0301n automa\u0301tica de etiquetas a nodos de diferentes tipos en una red heteroge\u0301nea que no tiene tipos en las aristas. El algoritmo que presentan esta\u0301 disen\u0303ado para aprender las dependencias existentes entre los conjuntos de etiquetas asociadas a los diferentes nodos y para inferir las etiquetas asociadas\na un nodo explotando las propiedades del grafo global y las caraceristicas de los nodos vecinos. Para ello, imponen dos objetivos: (1) intentar agrupar nodos del mismo tipo que este\u0301n conectados (con menos intensidad cuanto ma\u0301s largo sea el camino que los conecta), y (2) intentar agrupar nodos de diferentes tipos si comparten contextos. Para trabajar con grafos con propiedades, representan cada propiedad como un nodo nuevo.\nComo hemos mostrado, existen numerosas metodolog\u0301\u0131as para realizar inmersiones de grafos en espacios vectoriales, algunas de ellas esta\u0301n limitadas a trabajar con grafos uni-relacionales, otras condicionan la codificacio\u0301n a trave\u0301s de la generacio\u0301n de relaciones no observadas o no capturan las caracter\u0301\u0131sticas sema\u0301nticas t\u0301\u0131picas de un grafo con propiedades. Nuestra propuesta, que pasamos a detallar a continuacio\u0301n, tiene como objetivo obtener inmersiones que no este\u0301n afectadas por estas limitaciones."}, {"heading": "4. Inmersiones de Grafos con Propiedades", "text": "Entre las opciones barajadas, la arquitectura seleccionada para el codificador neuronal fue la arquitectura CBOW debido a que, a pesar de su simplicidad y el bajo coste computacional en su entrenamiento, obtiene buenos resultados en la tarea de capturar relaciones tanto sinta\u0301cticas como sema\u0301nticas entre los elementos codificados [16].\nAs\u0301\u0131 pues, la metodolog\u0301\u0131a que presentamos a continuacio\u0301n hace uso de un codificador neuronal, similar al usado en la arquitectura CBOW, para codificar los elementos de un grafo con propiedades en un espacio vectorial adecuado.\nAunque un grafo con propiedades tiene muchos elementos constitutivos, en una primera aproximacio\u0301n, y con el fin de evaluar hasta que\u0301 punto se mantiene la estructura sema\u0301ntica dada por las aristas, haremos una proyeccio\u0301n usando u\u0301nicamente el conjunto de nodos sobre el espacio vectorial. De esta forma, siguiendo con la analog\u0301\u0131a que nos ofrece el algoritmo word2vec, nuestro vocabulario sera\u0301 el conjunto de nodos del grafo (y tambie\u0301n sus propiedades asociadas).\nUn contexto, C, asociado a un nodo n \u2208 V se obtiene seleccionando, aleatoriamente y con repeticio\u0301n, un nu\u0301mero determinado de nodos vecinos a n y propiedades suyas, independientemente del tipo de relacio\u0301n que los conecta y del tipo de propiedad. El nu\u0301mero de nodos/propiedades seleccionados determina el taman\u0303o de la ventana de seleccio\u0301n.\nSiguiendo una metodolog\u0301\u0131a similar a las vistas en el apartado anterior, generaremos un conjunto de entrenamiento formado por pares (n,C), donde n \u2208 V y C es uno de sus contextos asociados. El conjunto de muestras es utilizado para entrenar el codificador neuronal y, a continuacio\u0301n, las activaciones de la capa oculta de la red neuronal se utilizan como representacio\u0301n vectorial de cada uno de los nodos.\nUna vez entrenado el codificador, usaremos estas representaciones vectoriales para intentar resolver algunas tareas de clasificacio\u0301n y descubrimiento en el grafo original. Los resultados de estas tareas proporcionara\u0301n una medida de fiabilidad sobre las inmersiones conseguidas (Fig. 2). El Algoritmo 1 muestra el procedimiento seguido.\nEn el procedimiento de inmersio\u0301n los para\u0301metros libres del modelo, que habra\u0301 que ajustar en los diversos experimentos para analizar su eficacia y viabilidad, son:\nAlgorithm 1 GG2Vec(G,N ,ws,D)\n1: training set = {} 2: for each 0 < i \u2264 N do 3: n = randomly selected element from VG 4: C = {} 5: for each 0 < i \u2264 ws do 6: e = randomly selected element from N (n) \u222a \u00b5G(n) 7: C = C \u222a {e} 8: end for 9: training set = training set \u222a {(v, C)}\n10: end for 11: Train a CBOW -like architecture with D neurons in hidden layer using\ntraining set 12: return The resulting encoding for each element in V\nFigura 2: Representacio\u0301n esquema\u0301tica de la metodolog\u0301\u0131a propuesta.\nD, taman\u0303o de la capa oculta, determina la dimensio\u0301n del espacio vectorial en el que haremos la inmersio\u0301n de los elementos del grafo.\nN , taman\u0303o del conjunto de entrenamiento, nu\u0301mero de pares (n,C) utilizados para entrenar el codificador.\nws, taman\u0303o de la ventana de seleccio\u0301n, nu\u0301mero de vecinos y propiedades considerados para construir los contextos de nodos de V .\nEn lo que sigue, notaremos por \u03c0 : V \u2192 RD la inmersio\u0301n que hemos obtenido a partir del codificador neuronal entrenado.\nDebido a que en la implementacio\u0301n y experimentos sobre bases de datos reales vamos a trabajar con grafos binarios con propiedades (no hipergrafos), tras haber obtenido una proyeccio\u0301n sobre los nodos del grafo, se induce una proyeccio\u0301n de las aristas en el mismo espacio vectorial (que notaremos tambie\u0301n por \u03c0):\nDefinicio\u0301n 2. Si G = (V,E, \u03c4, \u00b5) es un Grafo con Propiedades, y \u03c0 : V \u2192 RD es una inmersio\u0301n de los nodos del grafo, damos una extensio\u0301n de la inmersio\u0301n al conjunto de aristas, \u03c0 : E \u2192 RD de la siguiente forma:\ne \u2208 E, s e\u2192 t, entonces \u03c0(e) = \u2212\u2212\u2212\u2212\u2212\u2192\u03c0(s)\u03c0(t) Como las operaciones habituales en espacios vectoriales son de uso extendido en las actuales unidades de ca\u0301lculo (procesadores y GPUs), esta nueva representacio\u0301n puede ayudar a desarrollar algoritmos ma\u0301s eficientes para analizar, reparar y extraer informacio\u0301n de conjuntos de datos multi-relacionales, en general, y ma\u0301s concretamente, de bases de datos en grafos. Por ejemplo, algunas tareas que pueden verse mejoradas con este tipo de inmersiones son:\nClu\u0301sters formados por nodos/aristas en el nuevo espacio vectorial pueden ayudar a asignar propiedades faltantes a los elementos de un grafo (haciendo uso de relaciones de distancia, linealidad o clusterizacio\u0301n, por ejemplo).\nLa representacio\u0301n vectorial de los elementos de un grafo puede ayudar a obtener medidas de similitud entre ellos.\nEl ana\u0301lisis de los vectores asociados a las diferentes familias de relaciones (aquellas que comparten un tipo comu\u0301n, o verifican propiedades similares, por ejemplo) puede ayudar a detectar relaciones faltantes en el conjunto de datos original pero que en la nueva representacio\u0301n se hacen evidentes (la disposicio\u0301n de dos nodos cumple con el vector representante de algu\u0301n tipo de relacio\u0301n, a pesar de que esa relacio\u0301n no aparece en el grafo).\nEl ana\u0301lisis de la representacio\u0301n de las aristas que forman caminos en el grafo original puede ayudar a desarrollar formas ma\u0301s eficientes de detectar la existencia de dichos caminos en el grafo original.\n5. Evaluacio\u0301n Emp\u0301\u0131rica\nEs momento ahora de realizar una evaluacio\u0301n emp\u0301\u0131rica de nuestro me\u0301todo con dos objetivos claramente diferenciados:\n1. Analizar que las representaciones vectoriales que se obtienen a partir de grafos con propiedades mantienen caracter\u0301\u0131sticas sema\u0301nticas presentes en los mismos.\n2. Evaluar diferentes aplicaciones que hacen uso de la inmersio\u0301n propuesta para realizar tareas de clasificacio\u0301n y descubrimiento.\nEn nuestro contexto, diremos que una inmersio\u0301n vectorial respeta la sema\u0301ntica de un grafo con propiedades si, a partir de la nueva representacio\u0301n, es posible obtener los tipos asociados a nodos y aristas sin que e\u0301stos hayan estado presentes durante la inmersio\u0301n. El tipo de cada nodo o arista estara\u0301 determinado por la clave \u03c4 \u2208 R. Como hemos comentado, para hacer estas comprobaciones, las diversas inmersiones que vamos a calcular no recibira\u0301n informacio\u0301n acerca de los tipos de nodos o aristas que componen el grafo (formalmente, no recibira\u0301n informacio\u0301n sobre \u03c4). Los contextos asociados a los diferentes nodos del grafo, y que son utilizados para crear el conjunto de entrenamiento, son generados seleccionando aleatoriamente un nu\u0301mero (que viene determinado por el taman\u0303o de la ventana de seleccio\u0301n) de nodos vecinos y de valores de sus diferentes propiedades en \u00b5 (sin \u03c4)."}, {"heading": "5.1. Detalles de la implementacio\u0301n y experimentos", "text": "Se ha elegido Python como lenguaje de programacio\u0301n para llevar a cabo la evaluacio\u0301n experimental sen\u0303alada2. Para la implementacio\u0301n de la arquitectura CBOW se ha utilizado el conjunto de herramientas Gensim3 (versio\u0301n 0.12.4). Adema\u0301s, se ha utilizado Neo4j4 como sistema de persistencia para los grafos con propiedades analizados.\nCada experimento de inmersio\u0301n, con para\u0301metros prefijados, se ha repetido 10 veces, valor que experimentalmente ha mostrado una desviacio\u0301n esta\u0301ndar en los resultados experimentales con respecto a la prediccio\u0301n de los tipos de nodos y aristas inferior al 2 %. En el caso de las tareas relacionadas con Entity Retrieval estas desviaciones suben hasta el 7,8 %, y en el caso de la obtencio\u0301n de los nodos destino de un traversal han quedado acotadas por 8,9 %.\nEn lo relativo a las tareas posteriores que hacen uso de otros modelos de aprendizaje para validar la inmersio\u0301n se han considerado k-NN, Random Forest y Redes Neuronales. Para los test de clasificacio\u0301n generales, y salvo que se indique lo contrario, se ha utilizado como modelo base de comparacio\u0301n k-NN con k = 3."}, {"heading": "5.2. Datasets", "text": "Los experimentos se han llevado a cabo en 3 grafos con propiedades diferentes. Dos de ellos son ampliamente conocidos por la comunidad cient\u0301\u0131fica relacionada con el ana\u0301lisis de datos sema\u0301nticos: WordNet y TheMovieDB. El tercero es un conjunto de datos desconocido para la comunidad denominado Ecuadorian Intangible Cultural Heritage.\nLos conjuntos de datos han sido parcialmente manipulados para reducir su taman\u0303o y complejidad por motivos de eficiencia. A continuacio\u0301n damos algunos detalles acerca de cada uno de estos conjuntos para contextualizar las caracter\u0301\u0131sticas que encontraremos en los resultados obtenidos.\nWordNet R\u00a9 [10] es una base de datos de nombres, verbos, adjetivos y adverbios de la lengua inglesa. Es uno de los recursos ma\u0301s importantes en el a\u0301rea de lingu\u0308\u0301\u0131stica computacional, y se ha construido como una combinacio\u0301n de diccionario y tesauro, ideado para que su uso sea intuitivo. Cada elemento en la base de datos representa una lista de palabras sino\u0301nimas (que denominan synset), y las relaciones que se establecen entre los elementos se dan tanto a nivel le\u0301xico como sema\u0301ntico, razo\u0301n por la cual esta base de datos ha sido ampliamente usada en el ana\u0301lisis sinta\u0301ctico de textos y en entornos de extraccio\u0301n automa\u0301tica de informacio\u0301n sema\u0301ntica.\nPara este trabajo hemos utilizado una seccio\u0301n de la versio\u0301n 3.0, considerando u\u0301nicamente las entidades y relaciones que se muestran en la Figura 3 (de manera similar a [11]), obteniendo de esta forma un grafo con 97.593 nodos y 240.485 relaciones, con una distribucio\u0301n de tipos en nodos y aristas tal y como muestra la Figura 4.\nTheMovieDB (TMDb)5 es un conjunto de datos que contiene informacio\u0301n sobre actores, pel\u0301\u0131culas y contenidos de televisio\u0301n. Para nuestros experimen-\n2https://github.com/palmagro/gg2vec 3https://radimrehurek.com/gensim 4http://neo4j.com 5https://www.themoviedb.org\nFigura 3: Esquema de datos de WordNet.\n(a) Distribucio\u0301n de nodos por tipo. (b) Distribucio\u0301n de aristas por tipo.\nFigura 4: Distribucio\u0301n de nodos y aristas en WordNet.\ntos hemos considerado todas las entidades de TMDb que esta\u0301n conectadas por relaciones pertenecientes a los tipos acts in, directed, genre y studio, obteniendo un grafo con 66.020 nodos y 125.624 relaciones. La Figura 5 muestra una representacio\u0301n gra\u0301fica del esquema de datos presente en este dataset, y en la Figura 6 se muestra la distribucio\u0301n por tipos de nodos y aristas en el subconjunto de TMDb considerado.\nCabe destacar que los tipos Actor y Director esta\u0301n solapados, concretamente, en nuestro conjunto existen 44.097 nodos que so\u0301lo tienen asignado el tipo Actor, 5.191 nodos que so\u0301lo tienen asignado el tipo Director, y 846 nodos que poseen los tipos Actor y Director al mismo tiempo (nodos multi-tipo).\nEcuadorian Intangible Cultural Heritage (EICH o Base de Datos del Patrimonio Cultural Inmaterial del Ecuador) corresponde a una seccio\u0301n de la base de datos del Instituto Nacional de Patrimonio Cultural Ecuatoriano 6 que contiene 38.990 nodos y 55.358 relaciones distribuidas a trave\u0301s de 11 tipos de nodos y 10 tipos de aristas, con informacio\u0301n sobre el patrimonio cultural inmaterial del Ecuador. Esta base de datos es la ma\u0301s heteroge\u0301nea de las 3 analizadas, presentando mayor tipolog\u0301\u0131a tanto en nodos como en aristas, y adema\u0301s sus ele-\n6Accesible desde http://www.inpc.gob.ec\nFigura 5: Esquema de datos de TMDb.\n(a) Distribucio\u0301n de nodos por tipo. (b) Distribucio\u0301n de aristas por tipo.\nFigura 6: Distribucio\u0301n de nodos y aristas en TMDb.\nmentos poseen ma\u0301s propiedades que las de los elementos de las otras dos bases consideradas. Las Figuras 7 y 8 muestran respectivamente el esquema y distribucio\u0301n de nodos y aristas en este grafo.\nSi medimos, de forma muy primitiva, la riqueza sema\u0301ntica de un nodo como la suma del nu\u0301mero de relaciones en las que participa ma\u0301s el nu\u0301mero de propiedades que posee, podemos construir el histograma de riqueza sema\u0301ntica para cada uno de los conjuntos de datos anteriores (Fig. 9). En el caso de WordNet, el promedio de riqueza sema\u0301ntica es 5, 56, en el caso de TMDb es 3, 21 y en el caso de EICH es 7, 86. El distinto comportamiento que muestra esta distribucio\u0301n en los casos estudiados puede ayudarnos a entender e interpretar los resultados obtenidos."}, {"heading": "5.3. Prediccio\u0301n de Tipos de Nodos", "text": "Nuestro primer experimento tiene como objetivo predecir la funcio\u0301n \u03c4 que asocia tipos a los nodos del grafo y que, como comentamos anteriormente, no se proporciona durante el proceso de entrenamiento. Por supuesto, de forma similar, podr\u0301\u0131amos intentar predecir cualquier propiedad de \u00b5, manteniendo siempre la precaucio\u0301n de que la caracter\u0301\u0131stica evaluada no haya sido utilizada durante el entrenamiento.\nUna primera intuicio\u0301n acerca de que las inmersiones conseguidas mantienen las estructuras sema\u0301nticas (concretamente, el tipo de cada nodo) la podemos obtener analizando co\u0301mo se distribuyen los diversos tipos en el espacio vectorial sobre el que se ha hecho la inmersio\u0301n.\nLa figura 10 muestra dos proyecciones de una seccio\u0301n de la inmersio\u0301n obtenida para el grafo TMDb. La representacio\u0301n de la izquierda muestra una seleccio\u0301n\nFigura 7: Esquema de datos de EICH.\n(a) Distribucio\u0301n de nodos por tipo. (b) Distribucio\u0301n de aristas por tipo.\nFigura 8: Distribucio\u0301n de nodos y aristas en EICH.\naleatoria de los vectores asociados a los nodos de tipo Movie y Actor haciendo uso de una inmersio\u0301n en un espacio de dimensio\u0301n 200 (que ha sido proyectada posteriormente sobre un espacio bidimensional haciendo uso de la te\u0301cnica Multi-Dimensional Scaling [5] para facilitar su visualizacio\u0301n), mientras que la representacio\u0301n de la derecha muestra la misma seccio\u0301n del grafo haciendo uso de una inmersio\u0301n sobre un espacio de dimensio\u0301n 2 directamente. A pesar de que la reduccio\u0301n de dimensionalidad considerada es a todas luces excesiva (pero necesaria para poder visualizar estos conjuntos en estas pa\u0301ginas), ambas representaciones muestran que las inmersiones de los nodos del grafo TMDb no se distribuyen aleatoriamente respecto del tipo, sino que siguen un patro\u0301n, lo que evidencia que, en efecto, la inmersio\u0301n obtenida mantiene informacio\u0301n relativa al tipo de los nodos a pesar de que, como comentamos, la funcio\u0301n \u03c4 nunca ha sido utilizada en su construccio\u0301n.\nAdema\u0301s de la libertad de eleccio\u0301n en los para\u0301metros que intervienen en la codificacio\u0301n, encontramos algunos grados de libertad adicionales al decidir que\u0301 ma\u0301quina de aprendizaje se usara\u0301 posteriormente para clasificar la inmersio\u0301n\n(a) WordNet\n(b) TMDb\n(c) EICH\nFigura 9: Riqueza sema\u0301ntica (inferiores a 20).\nde los nodos del grafo. Como primera aproximacio\u0301n, y a pesar de la alta carga computacional que demanda, se ha realizado un estudio exhaustivo de los para\u0301metros libres del modelo haciendo uso del me\u0301todo de clasificacio\u0301n k-NN para explorar la dependencia que muestra la inmersio\u0301n respecto del para\u0301metro k. La razo\u0301n por la que se ha elegido este modelo se centra en dos aspectos fundamentales: solo depende de un para\u0301metro propio (el valor de k, que se sabe que funciona relativamente bien para k = 3 de forma general) y, a pesar de su simplicidad, proporciona resultados robustos que sirven de base comparativa para otros modelos de clasificacio\u0301n ma\u0301s sofisticados.\nLa Tabla 1 muestra valores de los para\u0301metros en los que se han obtenido resultados de clasificacio\u0301n buenos usando k-NN como modelo posterior de aprendizaje (con k = 3). La Figura 11 muestra los resultados de este ana\u0301lisis para los tres datasets considerados, donde se consiguen tasas de prediccio\u0301n superiores al 70 % para todos ellos.\nFigura 10: Representaciones 2D de nodos de tipo Movie y Actor en TMDb.\nTabla 1: Para\u0301metros de inmersio\u0301n N D Tam. ventana Prediccio\u0301n\nTMDb 400.000 150 3 ' 72 % WordNet 1.000.000 50 8 ' 96 % EICH 300.000 20 2 ' 83 %\nEn todos los casos se cumple que el taman\u0303o del conjunto de entrenamiento, N , o\u0301ptimo para realizar la prediccio\u0301n automa\u0301tica de los tipos de nodo es proporcional al nu\u0301mero de nodos en el grafo. Tanto EICH como TMDb (para WordNet no conocemos el valor o\u0301ptimo, porque es creciente en el rango analizado) muestran una reduccio\u0301n en la tasa de prediccio\u0301n a partir del valor o\u0301ptimo, esto puede deberse a un sobreajuste relacionado con la existencia de nodos de diferentes tipos que poseen la misma etiqueta.\nRespecto a la dimensio\u0301n del espacio vectorial, se pueden observar leves cambios cuando aumentamos D por encima de 10-15, una dimensio\u0301n relativamente baja, pero es casi imperceptible.\nEl estudio del para\u0301metro ws muestra que se requieren valores pequen\u0303os de este para\u0301metro para obtener buenos resultados en la prediccio\u0301n del tipo asociado a los nodos. Es importante sen\u0303alar que la mejor prediccio\u0301n no se consigue en ningu\u0301n caso con ws = 1, ya que esto supondr\u0301\u0131a que el sistema no necesita recibir pares nodo-contexto como elementos del conjunto de entrenamiento sino que bastar\u0301\u0131a con mostrarle instancias de las relaciones/propiedades presentes en cada nodo."}, {"heading": "5.3.1. Comparacio\u0301n con otros modelos de prediccio\u0301n", "text": "Una vez fijados los para\u0301metros de la inmersio\u0301n que proporciona la Tabla 1, procedemos a comparar la capacidad predictiva con algunos me\u0301todos automa\u0301ticos de clasificacio\u0301n sobre la misma representacio\u0301n. Concretamente, compararemos estos resultados con los obtenidos a trave\u0301s de redes neuronales feedforward y Random Forest.\nEn la Figura 12 se muestran los resultados obtenidos. La gra\u0301fica (a) muestra la variacio\u0301n de los resultados proporcionados por k-NN cuando var\u0301\u0131a el valor k; en (b) se muestran los resultados arrojados por Random Forest cuando se modifica el nu\u0301mero de a\u0301rboles; finalmente, en (c) se muestran los resultados de la red neuronal cuando se modifica el nu\u0301mero de neuronas en la capa oculta.\nTabla 2: Matriz de confusio\u0301n: Prediccio\u0301n de tipos de nodos (WordNet) adjective verb noun adverb\nadjective 90.01 % 1.75 % 8.2 % 0.05 % verb 0.44 % 88.36 % 11.19 % 0.0 % noun 0.2 % 1.56 % 98.23 % 0.01 % adverb 10.16 % 1.63 % 29.27 % 58.94 %\nTabla 3: Matriz de confusio\u0301n: Prediccio\u0301n de tipos de nodos (TMDb) Director Movie Genre Studio Actor\nDirector 11.45 % 9.54 % 0.02 % 1.48 % 77.51 % Movie 6.51 % 65.8 % 0.02 % 0.29 % 27.37 % Genre 9.66 % 33.79 % 2.07 % 3.45 % 51.03 % Studio 9.66 % 8.49 % 0.01 % 1.23 % 80.61 % Actor 5.77 % 7.87 % 0.0 % 0.7 % 85.66 %\nEn cualquier caso, los valores de los para\u0301metros de estos modelos se mantienen relativamente bajos para la tarea que se esta\u0301 llevando a cabo.\nTambie\u0301n presentamos las matrices de confusio\u0301n promediadas tras realizar 10 experimentos utilizando los para\u0301metros o\u0301ptimos indicados anteriormente: WordNet (Tabla 2), TMDb (Tabla 3), y EICH (Tabla 7). Estas matrices capturan las similitudes sema\u0301nticas entre los tipos de nodo. En EICH, por ejemplo, Canton, Parroquia y Provincia muestran un comportamiento solapado debido a que todos ellos representan informacio\u0301n geoespacial altamente correlacionada. En TMDb ocurre algo similar, ACTOR y DIRECTOR aparecen relacionados debido a que, como comentamos, existen numerosos nodos en esta base de datos que tienen ambos tipos."}, {"heading": "5.4. Prediccio\u0301n de Tipos de Aristas", "text": "El segundo experimento tiene como objetivo determinar la bondad que presentan las inmersiones en la prediccio\u0301n de los tipos de aristas, que tampoco han sido usados en el proceso de entrenamiento del codificador neuronal.\nFigura 13: Representacio\u0301n 2D aristas de TMDb.\nLa figura 13 muestra una proyeccio\u0301n bidimensional (obtenida tambie\u0301n aplicando el me\u0301todo MDS) de un conjunto de aristas seleccionadas aleatoriamente del conjunto de datos TMDb. Se puede observar que las aristas de tipo Genre no forman un u\u0301nico cluster, sino un conjunto de clu\u0301sters perife\u0301ricos que corresponden a los valores Action, Comedy, Drama, Documentary, Horror y Crime, mostrando un comportamiento sema\u0301ntico diferente al del resto de tipos. Esto podr\u0301\u0131a indicarnos que Genre quiza\u0301s no forma un tipo u\u0301nico desde el punto de vista sema\u0301ntico, y que su comportamiento refleja informacio\u0301n acerca de cierta heterogeneidad que ha sido incluida en las decisiones de disen\u0303o al construir la base de datos original. Es aqu\u0301\u0131 donde este tipo de ana\u0301lisis muestra caracter\u0301\u0131sticas que lo pueden hacer adecuado para ser usado como normalizador adicional en bases de datos que cubre informacio\u0301n sema\u0301ntica y no solo estructural.\nEn la Figura 14 se muestran los resultados de la eficiencia de k-NN respecto a cambios en los para\u0301metros de la inmersio\u0301n. Teniendo en cuenta que el porcentaje de acierto esta\u0301 por encima del 80 % en todos los conjuntos de datos estudiados (incluso superando el 95 % en alguno de ellos), podemos concluir que la metodolog\u0301\u0131a seguida para la inmersio\u0301n mantiene las propiedades sema\u0301nticas tambie\u0301n respecto de las aristas.\nEn general, podemos observar que el taman\u0303o del conjunto de entrenamiento necesario para obtener buenos resultados a la hora de predecir los tipos de las aristas es superior al requerido para realizar una buena prediccio\u0301n de los tipos de nodo para los tres datasets analizados. Adema\u0301s, observando los resultados descubrimos que, a pesar de que WordNet es el grafo con propiedades que mejores resultados ofrec\u0301\u0131a en la prediccio\u0301n del tipo de los nodos, en el caso de la prediccio\u0301n de aristas se consiguen mejores resultados para EICH, logrando tasas con un valor ' 97 % para los para\u0301metros de la inmersio\u0301n estudiados.\nAl igual que en el caso anterior, las inmersiones requieren una dimensio\u0301n relativamente baja, con pequen\u0303os cambios a partir de D = 15.\nEl comportamiento en la prediccio\u0301n de tipos de aristas segu\u0301n ws muestra valores ma\u0301s elevados que los requeridos para la prediccio\u0301n en los tipos de nodos (superior a 20). En cualquier caso, es importante sen\u0303alar que, de nuevo, la mejor prediccio\u0301n no se consigue en ningu\u0301n caso con ws = 1, lo que evidencia que muestrear los contextos locales de un nodo consigue mejores resultados que el que se podr\u0301\u0131a conseguir capturando so\u0301lo las relaciones binarias entre los nodos que ofrecen las aristas."}, {"heading": "5.4.1. Comparacio\u0301n con otros modelos de prediccio\u0301n", "text": "Siguiendo la misma metodolog\u0301\u0131a que para los tipos de nodos, la Figura 15 muestra los resultados obtenidos por los tres me\u0301todos de clasificacio\u0301n automa\u0301tica utilizados en el apartado anterior. Estas tareas de clasificacio\u0301n se han realizado con inmersiones que hacen uso de los para\u0301metros presentados en la tabla 1, y se analizan modificando los mismos hiperpara\u0301metros de cada modelo concreto.\nLas matrices de confusio\u0301n tras promediar 10 experimentos para cada grafo se muestran en las Tablas 4, 8, y 6, poniendo en evidencia que las inmersiones capturan similitudes entre diversos tipos de aristas. En el caso de EICH los tipos de aristas relacionados con informacio\u0301n geoespacial muestran un comportamiento solapado con las aristas de tipo LENGUA, debido a que existe una correlacio\u0301n entre las lenguas habladas y los territorios en los que se habla. WordNet muestra un comportamiento similar entre tipos de aristas hypernym y hyponim, debido\nTabla 4: Matriz de confusio\u0301n: Prediccio\u0301n de tipos de aristas (TMDb) GENRE DIRECTED STUDIO ACTS IN\nGENRE 99.51 % 0.02 % 0.21 % 0.26 % DIRECTED 0.01 % 15.28 % 2.04 % 82.67 % STUDIO 0.13 % 7.22 % 62.87 % 29.79 % ACTS IN 0.01 % 4.75 % 0.94 % 94.3 %\na que tienen un comportamiento sema\u0301ntico similar. En el caso de TMDb, como era de esperar, las aristas de tipo DIRECTED se confunden con las aristas de tipo ACTED IN debido al solapamiento entre los tipos de nodo ACTOR y DIRECTOR que intervienen en las mismas.\nLos resultados experimentales correspondientes a la clasificacio\u0301n automa\u0301tica muestran que, para los conjuntos de datos analizados, la inmersio\u0301n obtenida con la metodolog\u0301\u0131a propuesta conserva la sema\u0301ntica asociada a los tipos de aristas y es capaz de detectar similitudes sema\u0301nticas entre los tipos de aristas.\nCabe destacar que, debido a que pueden existir aristas de diferentes tipos entre el mismo par de nodos, el resultado en la prediccio\u0301n en el tipo de arista entre dos nodos puede haberse visto afectado. Si estamos tratando de predecir el tipo de una arista entre dos nodos so\u0301lo a partir de las posiciones vectoriales de los mismos, y entre ellos existen aristas de diferentes tipos, la solucio\u0301n no es u\u0301nica, ya que cualquiera de estos tipos ser\u0301\u0131a una solucio\u0301n va\u0301lida. Este hecho no se ha tenido en cuenta a la hora de llevar a cabo los experimentos, por lo que los resultados en la prediccio\u0301n de tipos de aristas suponen un l\u0301\u0131mite inferior y podr\u0301\u0131an ser mejorados teniendo en cuenta que la respuesta correcta no es u\u0301nica. Sin lugar a dudas, por este me\u0301todo nunca podremos obtener una fiabilidad absoluta acerca de los resultados, y como sistema de prediccio\u0301n puro para aristas presenta limitaciones fundamentales, pero puede ser tenido en cuenta para tareas adicionales de normalizacio\u0301n de datos o como me\u0301todo de filtrado para otras operaciones que trabajen sobre la sema\u0301ntica de las aristas."}, {"heading": "5.5. Entity Retrieval", "text": "Para seguir poniendo a prueba la bondad de las inmersiones que conseguimos respecto a la sema\u0301ntica interna de los grafos, y para poner de manifiesto su utilidad en otras tareas de prediccio\u0301n, vamos a evaluar hasta que\u0301 punto somos capaces de predecir relaciones faltantes haciendo uso de las inmersiones.\nPara ello, consideraremos un subconjunto de aristas, E\u2032 \u2208 E, que pertenecen al grafo original G = (V,E, \u03c4, \u00b5) y que posteriormente eliminaremos, consiguiendo un subgrafo del anterior, G\u2032 = (V,E \\ E\u2032, \u03c4, \u00b5), sobre el que entrenaremos la inmersio\u0301n. Posteriormente, trataremos de obtener el nodo destino asociado a cada arista en E\u2032 usando u\u0301nicamente su nodo origen y \u03c0(G\u2032).\nFormalmente, dada una arista e = (s, t) \u2208 E\u2032 de tipo \u03c4(e), que ha sido eliminada de manera previa a la inmersio\u0301n del grafo, trataremos de obtener t a partir de \u03c0(G\u2032), \u03c4(e) y s. Esta tarea de obtener el target de una relacio\u0301n dado el nodo origen y el tipo de la misma es conocida como Entity Retrieval [6].\nVamos a utilizar el vector representante de los tipos para obtener el nodo destino de las relaciones faltantes, que definimos como:\nDefinicio\u0301n 3. Dado un grafo con propiedades G = (V,E, \u03c4, \u00b5), el vector repre-\nTabla 5: Ranking de Entity Retrieval usando la relacio\u0301n hypernym foam spasm justification neconservatism\n1 hydrazine ejection reading pruritus 2 pasteboard rescue explanation conservatism 3 silicon dioxide putting to death analysis sight 4 humate sexual activity proposition hawkishness 5 cellulose ester behavior modification religious doctrine coma 6 synthetic substance disturbance accusation scientific method 7 silver nitrate mastectomy assay autocracy 8 cast iron sales event confession judiciousness 9 sulfide instruction research reverie 10 antihemorrhagic factor debasement discouragement racism\nsentante, \u03c0(\u03c9), asociado a un tipo de arista \u03c9 \u2208 \u03c4(E), es el vector promedio de todos los vectores que representan a aristas de tipo \u03c9.\nSi denotamos E\u03c9 = \u03c4 \u22121({\u03c9}) = {e \u2208 E : \u03c4(e) = \u03c9}, entonces:\n\u03c0(\u03c9) = 1\n#(E\u03c9)\n\u2211\ne\u2208E\u03c9 \u03c0(e)\nA partir de la extensio\u0301n de \u03c0 que hemos dado para las aristas, si queremos obtener un candidato del destino de una relacio\u0301n e a partir del origen haciendo uso del vector representante de la relacio\u0301n y del vector asociado al nodo origen, basta hacer:\n\u03c0(te) = \u03c0(s) + \u03c0(\u03c4(e))\nEl vector \u03c0(te) representa la posicio\u0301n a la que apunta el vector representante de \u03c4(e) desde el vector que representa el nodo origen \u03c0(s) de la relacio\u0301n e. Una vez obtenido el vector \u03c0(te) podemos obtener un ranking para los nodos del grafo, que se puede construir a partir de las distancias a \u03c0(te) de cada vector asociado a los nodos del grafo original, de tal manera que los nodos que ma\u0301s cerca se encuentren del vector \u03c0(te) ocupara\u0301n las primeras posiciones de dicho ranking.\nEn la tabla 5 se muestran los diez primeros resultados del ranking obtenido tras aplicar Entity Retrieval a trave\u0301s del vector representante, \u03c0(hypernym), de las relaciones de tipo hypernym a diferentes nodos origen del grafo WordNet, los resultados esta\u0301n filtrados de tal manera que so\u0301lo se muestran los nodos de tipo NOUN.\nComo nuestra metodolog\u0301\u0131a para construir la inmersio\u0301n se realiza a partir de muestras aleatorias de diferentes contextos locales del grafo, es posible que en algunos casos el nodo origen de la relacio\u0301n no haya sido considerado en ningu\u0301n momento y, por tanto, no podamos construir su representacio\u0301n vectorial. Para que este hecho no afecte a los resultados, en estos casos la arista no podra\u0301 ser evaluada y no influira\u0301 en el resultado experimental obtenido.\nPara evaluar la bondad de la inmersio\u0301n respecto de esta tarea haremos uso de la me\u0301trica Mean Reciprocal Rank, una me\u0301trica habitual en el a\u0301rea de Information Retrieval, y que ha sido utilizada en varios estudios de este tipo [6, 23].\nDefinicio\u0301n 4. El Reciprocal Rank asociado a un resultado concreto, en una lista de posibles respuestas dada una consulta, es el inverso de la posicio\u0301n que\nocupa ese resultado en dicha lista. El Mean Reciprocal Rank (MRR) es el promedio de los Reciprocal Ranks de una lista de consultas determinada:\nMRR = 1\n|Q|\n|Q|\u2211\ni=1\n1\nranki\ndonde Q representa el conjunto de consultas a evaluar, y ranki la posicio\u0301n que ocupa en cada ranking la respuesta correcta.\nEn la Figura 16 se muestran los resultados obtenidos usando esta me\u0301trica sobre los datasets EICH, TMDb y WordNet en funcio\u0301n del taman\u0303o del conjunto de entrenamiento utilizado para realizar la inmersio\u0301n, y eliminando del ranking aquellos nodos que no son del tipo que indica el nodo destino del tipo de relacio\u0301n evaluada. Como se puede observar en la gra\u0301fica, el me\u0301todo propuesto para llevar a cabo este tipo de tareas produce unos excelentes resultados que mejoran cuanto mayor es el conjunto de entrenamiento (en este caso, es un problema de multiclasificacio\u0301n, por lo que no se pueden esperar resultados que se acerquen al 100 %).\nFigura 16: Ana\u0301lisis MRR en Entity Retrieval.\nEste buen comportamiento nos permite obtener ciertas conclusiones sobre la estructura que los diferentes vectores (asociados a nodos y aristas) forman en la nueva representacio\u0301n: si el vector representante sirve para obtener el nodo destino de una arista significa que existe poca desviacio\u0301n entre las aristas del mismo tipo. Por otro lado, los nodos origen y destino del tipo de arista que se usa deben estar lo suficientemente dispersos para que, utilizando el vector representante, se consigan buenos resultados en cuanto a tareas relacionadas con Entity Retrieval."}, {"heading": "5.6. Inmersio\u0301n de caminos tipados", "text": "Por u\u0301ltimo, y solo a modo de demostracio\u0301n de las posibilidades que abre el tener una buena representacio\u0301n vectorial de los elementos de un grafo con propiedades, presentamos una te\u0301cnica basada en la inmersio\u0301n para obtener el nodo destino de un camino tipado dado el tipo del camino y el nodo origen del mismo.\nUn camino tipado no es ma\u0301s que la sucesio\u0301n de tipos en nodos y aristas que corresponde a un camino dentro del grafo (en algunos contextos a estos caminos\ntipados se les conoce como traversals, pero preferimos no usar esta nomenclatura debido al solapamiento que produce con los traversals como me\u0301todos de consulta en determinados lenguajes de consulta sobre grafos). Formalmente:\nDefinicio\u0301n 5. Un camino tipado de un grafo con propiedades G = (V,E, \u03c4, \u00b5) es una sucesio\u0301n\nT = t1 r1\u2192 t2 r2\u2192 . . . rq\u2192 tq+1 donde ti \u2208 \u03c4(V ) (es un tipo va\u0301lido para los nodos) y ri \u2208 \u03c4(E) (es un tipo va\u0301lido para las aristas). Denotaremos por Tp(G) el conjunto de posibles caminos tipados de G.\nDefinicio\u0301n 6. Podemos definir la aplicacio\u0301n, Tp, que asocia a cada posible camino tipado de G el conjunto de caminos que verifican el patro\u0301n de tipos especificado por e\u0301l, tal que si T = t1 r1\u2192 t2 r2\u2192 . . .\nrq\u2192 tq+1, entonces para cada camino \u03c1 \u2208 Tp(T ) se verifica que \u03c4(sopV (\u03c1)) = (t1, . . . , tq+1) y \u03c4(sopE(\u03c1)) = (r1 . . . , rq) (donde sopV (\u03c1) representa la secuencia ordenada de nodos en \u03c1, y sopE(\u03c1) la secuencia de aristas).\nNuestro objetivo es obtener el nodo destino de un camino existente dado el nodo origen del mismo y el camino tipado que verifica. En este caso no eliminamos los caminos antes de realizar la inmersio\u0301n, pues no tratamos de hacer prediccio\u0301n sino de ofrecer un nuevo mecanismo para la obtencio\u0301n (o, al menos, su estimacio\u0301n) del nodo destino de un camino que permita mejorar los tiempos que requieren este tipo de consultas, ya que en los sistemas actuales tienen un coste computacional muy elevado.\nPara ello, definimos el vector representante de un camino de forma similar a como lo hicimos en la tarea anterior (que realmente se puede considerar un caso particular de camino tipado para caminos de longitud 1).\nDefinicio\u0301n 7. El vector representante de un camino, n1 \u03c1 nk, en un grafo con propiedades, es el vector que separa la representacio\u0301n vectorial del nodo origen del camino, \u03c0(n1), y la representacio\u0301n vectorial del nodo destino del mismo, \u03c0(nk). Es decir:\n\u03c0(\u03c1) = \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 \u03c0(n1)\u03c0(nk) = \u03c0(nk)\u2212 \u03c0(n1)\nEl vector representante asociado a un camino tipado, T , es el vector promedio de todos los vectores que verifican el patro\u0301n de tipos especificado por T , es decir:\n\u03c0(T ) = 1 |Tp(T )| \u2211\n\u03c1\u2208Tp(T ) \u03c0(\u03c1)\nComo ocurr\u0301\u0131a con las aristas, es posible que en algunos casos el nodo origen del camino no haya sido tomado en la muestra de la inmersio\u0301n y, por tanto, no exista su representacio\u0301n vectorial. En estos casos, dicho camino no podra\u0301 ser evaluado y no influira\u0301 en el resultado experimental obtenido.\nA partir de esta definicio\u0301n se han realizado experimentos para evaluar la tarea de obtener el nodo destino de un camino dado el nodo origen y el vector representante del camino tipado asociado. Para ello, hemos filtrado los nodos destino segu\u0301n el tipo indicado por el u\u0301ltimo elemento de la secuencia que define el camino tipado y hemos utilizando de nuevo la me\u0301trica MRR presentada en el apartado anterior. Los experimentos han sido realizados sobre el dataset EICH\ndebido a que presenta una estructura ma\u0301s compleja en sus tipos que el resto de datasets y permite la construccio\u0301n de caminos tipados ma\u0301s complejos.\nEn la figura 17 se muestran los resultados obtenidos en los experimentos haciendo uso de los siguientes caminos tipados (los tipos de los nodos se representan en minu\u0301sculas, y los de las aristas se omiten porque representan el u\u0301nico tipo de arista que permite el esquema mostrado en la figura 7):\n1. T1 = (Inmaterial r1\u2192 DetSubambito r2\u2192 Subambito r3\u2192 Ambito)\nEsta\u0301 asociado a caminos de longitud 3 y contiene informacio\u0301n sobre a que\u0301 Ambito (existen 5 a\u0301mbitos diferentes en EICH) pertenece cada elemento del patrimonio inmaterial almacenado en el grafo.\n2. T2 = (Inmaterial r1\u2192 Parroquia r2\u2192 Canton r3\u2192 Provincia)\nEsta\u0301 asociado a caminos de longitud 3 y contiene informacio\u0301n sobre a que\u0301 Provincia (existen 24 provincias diferentes en EICH) pertenece cada elemento del patrimonio inmaterial almacenado en el grafo.\nFigura 17: Ana\u0301lisis MRR en caminos tipados.\nEn los resultados se aprecia el buen desempen\u0303o de la tarea propuesta para caminos tipados de tipo T1, cercana al 70 %, para un conjunto de entrenamiento de taman\u0303o superior a 3 millones. En el caso del camino tipado T2, sin embargo, su resultado tiende a empeorar cuando aumentamos el taman\u0303o del conjunto de entrenamiento por encima de 1 millo\u0301n, llegando a estar por debajo del 20 %. En cualquier caso, el problema asociado a T2 es considerablemente ma\u0301s complejo, ya que hay ma\u0301s de 20 provincias, frente a los 5 posibles a\u0301mbitos para el primer caso, y adema\u0301s pudimos ver anteriormente que se obten\u0301\u0131a una alta confusio\u0301n entre los tipos de nodos involucrados.\nAunque, por supuesto, har\u0301\u0131an falta ma\u0301s pruebas para validar esta metodolog\u0301\u0131a, esta aplicacio\u0301n muestra que un sistema como e\u0301ste podr\u0301\u0131a utilizarse para aproximar el resultado de consultas a larga distancia en bases de datos, que son especialmente ineficientes en el caso de los sistemas cla\u0301sicos de persistencia, por lo que la inmersio\u0301n se presenta como una alternativa interesante que, aunque reduciendo la fiabilidad del resultado, permite agilizar enormemente la carga computacional requerida en algunas tareas relacionadas."}, {"heading": "6. Conclusiones y Trabajo Futuro", "text": "El objetivo de este trabajo ha sido el de ofrecer la posibilidad de llevar a cabo tareas de aprendizaje automa\u0301tico relacional a trave\u0301s de algoritmos tradicionales haciendo una seleccio\u0301n automa\u0301tica de atributos (feature extraction) a trave\u0301s de inmersiones vectoriales, manteniendo las estructuras sema\u0301nticas mediante la generacio\u0301n de un conjunto de entrenamiento adecuado. De esta forma buscamos analizar que\u0301 opciones ofrecen los algoritmos tradicionales cuando deseamos no perder las estructuras enriquecidas propias de la informacio\u0301n relacional.\nSi existe un elemento (un subgrafo) que esta\u0301 inmerso en una base de datos (un grafo generalizado, o un grafo con propiedades, en nuestro contexto) la tarea de construir atributos para el aprendizaje a partir de las relaciones que presenta en la estructura global puede ser muy complicada. La aproximacio\u0301n que se presenta en este trabajo pasa por construir una representacio\u0301n vectorial de cada elemento en el sistema a partir de un muestreo de la informacio\u0301n presente en la red. De esta manera evitamos, por un lado, el trabajo manual de seleccio\u0301n de los atributos a tener en cuenta y, por otro, conseguimos que el algoritmo de aprendizaje a utilizar se alimente de una representacio\u0301n obtenida a partir de informacio\u0301n global disponible.\nEn comparacio\u0301n con otras tareas de aprendizaje automa\u0301tico, hay pocos trabajos que hayan utilizado codificadores neuronales para realizar inmersiones de grafos con propiedades, o estructuras similares, en espacios vectoriales. Nuestra metodolog\u0301\u0131a ha buscado usar arquitecturas simples para obtener representaciones vectoriales que mantienen las caracter\u0301\u0131sticas sema\u0301nticas y topolo\u0301gicas del grafo original. Adema\u0301s, se ha demostrado experimentalmente que con las inmersiones obtenidas se pueden obtener conexiones sema\u0301nticas que no aparecen expl\u0301\u0131citamente en el grafo original (debido a incompletitud en los datos almacenados, o a incoherencias en los mismos), o incluso ayudar en la optimizacio\u0301n de algunas tareas complejas de consulta en bases de datos.\nHemos comprobado que las caracter\u0301\u0131sticas geome\u0301tricas de las estructuras formadas por las inmersiones de nodos y aristas en el nuevo espacio vectorial pueden ayudar a asignar tipos o propiedades faltantes a los elementos del grafo original (usando medidas relacionadas con distancia, linealidad, o agrupacio\u0301n, entre otras), o pueden incluso ayudar a identificar nuevas relaciones entre elementos que no esta\u0301n presentes expl\u0301\u0131citamente. Esta funcionalidad puede ser de gran utilidad en procesos que trabajan con grandes conjuntos de datos relacionales, donde la incompletitud de los datos es inherente al problema.\nAdema\u0301s, y por encima de la competitividad que ofrece esta metodolog\u0301\u0131a frente a otras existentes, como se ha observado a partir de las pruebas de evaluacio\u0301n, el rendimiento y la precisio\u0301n de las tareas de aprendizaje automa\u0301tico sobre estas representaciones vectoriales pueden proporcionar informacio\u0301n sobre la estructura sema\u0301ntica del conjunto de datos en s\u0301\u0131, y no so\u0301lo sobre los algoritmos en uso. Por ejemplo, la confusio\u0301n de algunos nodos / aristas en tareas de clasificacio\u0301n puede darnos informacio\u0301n sobre la necesidad de realizar un ajuste en el esquema de datos para reflejar las caracter\u0301\u0131sticas sema\u0301nticas correctamente. Un informe detallado sobre co\u0301mo los diferentes tipos, propiedades, y clusters se superponen y confunden en la inmersio\u0301n ser\u0301\u0131a de utilidad para tomar decisiones relacionadas con la normalizacio\u0301n de los esquemas de datos, algo de lo que carecen casi todas las propuestas actuales de ana\u0301lisis y que es pra\u0301cticamente inexistente en los sistemas de datos noSQL.\nEs evidente que el taman\u0303o del conjunto de entrenamiento y de la ventana de seleccio\u0301n influyen positivamente en la capacidad de aplicacio\u0301n de la inmersio\u0301n resultante, pero estas influencias deben ser estudiadas a mayor profundidad, ya que pueden arrojar claves para la automatizacio\u0301n de los para\u0301metros de la inmersio\u0301n.\nAdema\u0301s, en este trabajo se ha explorado co\u0301mo las estructuras vectoriales pueden usarse para recuperar informacio\u0301n de grafos con propiedades, como muestran los experimentos de Entity Retrieval y de caminos tipados. Es probable que buscar estructuras complejas en el espacio proyectado sea ma\u0301s sencillo que en el espacio original. De hecho, el uso de una segunda capa de modelos de aprendizaje tras la codificacio\u0301n neuronal puede mejorar los resultados de varias tareas relacionadas con la recuperacio\u0301n de informacio\u0301n en grafos sema\u0301nticos. Los resultados en este trabajo muestran que esta es una l\u0301\u0131nea de investigacio\u0301n que vale la pena ser considerada. A pesar de que no se han llevado a cabo suficientes experimentos en cuanto a consultas a larga distancia a trave\u0301s de los vectores representantes en el nuevo espacio, los resultados obtenidos muestran que los tiempos de consulta pueden ser reducidos drama\u0301ticamente sacrificando la optimalidad. Este tipo de consultas son muy costosas en las bases de datos, y a pesar de que las bases de datos en grafo han ayudado a reducir su coste computacional siguen presentando grandes problemas de eficiencia cuando el camino de bu\u0301squeda tiene ma\u0301s de 3 aristas.\nFrente a otras aproximaciones en la misma direccio\u0301n, este trabajo presenta la novedad de trabajar con contextos sema\u0301nticos ma\u0301s generales, y no solo con caminos aleatorios, que suponen una linealizacio\u0301n de la estructura del grafo original. Pero estas no son las u\u0301nicas opciones para llevar a cabo codificaciones de grafos con propiedades por medio de redes neuronales. Podemos conseguir codificaciones vectoriales de grafos con propiedades haciendo uso de autocodificadores neuronales, de tal manera que el codificador neuronal aprendera\u0301 la funcio\u0301n identidad para los elementos del grafo, desligando la codificacio\u0301n de la funcio\u0301n que relaciona a los elementos con su contexto.\nCon este trabajo hemos dado un marco inicial para realizar tareas de aprendizaje automa\u0301tico a partir de grafos con propiedades en las que se tiene en cuenta informacio\u0301n del grafo completo para codificar cada elemento. Esta nueva representacio\u0301n de los grafos con propiedades permite trabajar con datos relacionales almacenados en casi cualquier sistema de persistencia de manera vectorial, aprovechando la potencia que tienen actualmente las CPUs y las GPUs para trabajar con este tipo de estructuras.\nDebe sen\u0303alarse que durante la revisio\u0301n de este documento se han publicado nuevas herramientas basadas en las arquitecturas de Word2Vec que optimizan el proceso de aprendizaje de sema\u0301nticas latentes a partir de lenguaje natural [3]. A pesar de la probable mejora que estas herramientas supondr\u0301\u0131an en nuestra metodolog\u0301\u0131a, hemos decidido no tenerlas en cuenta ya que no modifican la esencia de nuestra propuesta, aunque s\u0301\u0131 aligerar\u0301\u0131a, posiblemente, la carga de ca\u0301lculo asociada a los experimentos realizados.\nLas mejoras en eficiencia en consultas a larga distancia planteadas en el apartado 5.6 merecen ser evaluadas a mayor profundidad y comparadas con otros me\u0301todos similares. Algunos resultados relacionados con el ana\u0301lisis sema\u0301ntico de grafos con propiedades no han sido llevados a cabo en profundidad y no se han presentado en este art\u0301\u0131culo aunque se preve\u0301 sean presentados en trabajos posteriores. Opciones como muestrear el contexto de las aristas, realizar una\ninmersio\u0301n de las mismas y a partir de e\u0301sta inferir una inmersio\u0301n para los nodos no han sido tenidas en cuenta y pueden ofrecer resultados interesantes.\nDurante la concepcio\u0301n, implementacio\u0301n y experimentacio\u0301n de este trabajo han ido abrie\u0301ndose nuevas v\u0301\u0131as que pueden ser consideradas para analizar las caracter\u0301\u0131sticas de las inmersiones obtenidas.\nUna primera consideracio\u0301n a tener en cuenta esta\u0301 relacionada con la manera de construir el conjunto de entrenamiento que es consumido por el codificador neuronal. En los experimentos realizados la construccio\u0301n del conjunto de entrenamiento ha sido totalmente aleatoria, es decir, todos los nodos tienen la misma probabilidad de ser muestreados, al igual que todas sus propiedades y vecinos. E\u0301sta puede no ser la manera ma\u0301s adecuada dependiendo del tipo de actividad que se desee realizar con la inmersio\u0301n resultante. Por ejemplo, puede ser favorable construir el conjunto de entrenamiento de manera que aquellos nodos que posean una mayor riqueza sema\u0301ntica tengan ma\u0301s probabilidad de entrar en el conjunto de entrenamiento, lo que puede contribuir a que regiones inicialmente menos probables de ser consideradas compensen este hecho.\nOtra l\u0301\u0131nea a tener en cuenta es la de construir una red neuronal que trabaje con los contextos de un elemento como entrada (en formato one-hot) y aprenda a devolver una propiedad determinada de e\u0301ste como salida de la misma, es decir, conectar el clasificador/regresor neuronal directamente con el codificador, para de esta forma aprender la codificacio\u0301n adecuada y la clasificacio\u0301n/regresio\u0301n a partir de e\u0301sta de manera simulta\u0301nea. De igual forma, ser\u0301\u0131a interesante pensar en codificadores neuronales que hacen uso de redes neuronales recurrentes para poder analizar el comportamiento de informacio\u0301n relacional dina\u0301mica, un terreno pra\u0301cticamente inexplorado en la actualidad.\nTambie\u0301n cabe destacar que queda abierta la posibilidad de trabajar con propiedades continuas en nodos y aristas, una caracter\u0301\u0131stica no presente en los datasets utilizados, pero que debe ser considerada para ampliar la capacidad de la metodolog\u0301\u0131a presentada. En este caso hay mecanismos directos para incluir la presencia de propiedades continuas, queda como trabajo comenzar probando con estos mecanismos ma\u0301s evidentes y medir posteriormente hasta que\u0301 punto se pueden tener en cuenta otras aproximaciones.\nDel mismo modo, ser\u0301\u0131a interesante pensar en codificadores neuronales que hacen uso de redes neuronales recurrentes para analizar el comportamiento de informacio\u0301n relacional dina\u0301mica, un a\u0301rea pra\u0301cticamente inexplorada hoy en d\u0301\u0131a.\nAgradecimientos\nAgradecemos al Instituto Nacional de Patrimonio Cultural (INPC) del Ecuador por la informacio\u0301n relacionada con el Patrimonio Cultural Inmaterial del Ecuador (EICH). Este trabajo ha sido apoyado parcialmente por el Proyecto de Excelencia TIC-6064 de la Junta de Andaluc\u0301\u0131a (Espan\u0303a), por el proyecto TIN2013-41086-P del Ministerio Espan\u0303ol de Econom\u0131\u0301a y Competitividad (cofinanciado con fondos FEDER) y por el Departamento de Investigacio\u0301n y Postgrado de la Universidad Central del Ecuador.\nReferencias\n[1] P. Almagro-Blanco and F. Sancho-Caparrini. Generalized Graph Pattern Matching. arXiv e-prints arXiv:1708.03734.\n[2] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R\u00a9 in Machine Learning, 2(1):1\u2013127, 2009.\n[3] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.\n[4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multirelational data. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2787\u20132795. Curran Associates, Inc., 2013.\n[5] I. Borg and P.J.F. Groenen. Modern Multidimensional Scaling: Theory and Applications. Springer, 2005.\n[6] Kai-Wei Chang, Scott Wen-tau Yih, Bishan Yang, and Chris Meek. Typed tensor decomposition of knowledge bases for relation extraction. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. ACL \u2013 Association for Computational Linguistics, October 2014.\n[7] Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, and Thomas S. Huang. Heterogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 119\u2013128, New York, NY, USA, 2015. ACM.\n[8] Michae\u0308l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. CoRR, abs/1606.09375, 2016.\n[9] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2224\u20132232. Curran Associates, Inc., 2015.\n[10] C. Fellbaum. WordNet: An Electronic Lexical Database. Language, speech, and communication. MIT Press, 1998.\n[11] Xavier Glorot, Antoine Bordes, Jason Weston, and Yoshua Bengio. A semantic matching energy function for learning with multi-relational data. CoRR, abs/1301.3485, 2013.\n[12] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks, 2016. cite arxiv:1607.00653Comment: In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.\n[13] G E Hinton and R R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, July 2006.\n[14] Yann Jacob, Ludovic Denoyer, and Patrick Gallinari. Learning latent representations of nodes for classifying in heterogeneous social networks. In Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM \u201914, pages 373\u2013382, New York, NY, USA, 2014. ACM.\n[15] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\n[16] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013.\n[17] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, pages 701\u2013710, New York, NY, USA, 2014. ACM.\n[18] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Trans. Neural Networks, 20(1):61\u201380, 2009.\n[19] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. arXiv preprint arXiv:1703.06103, 2017.\n[20] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 926\u2013934. Curran Associates, Inc., 2013.\n[21] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, WWW \u201915, pages 1067\u20131077, New York, NY, USA, 2015. ACM.\n[22] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. CoRR, abs/1412.6575, 2014.\n[23] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Learning multi-relational semantics using neural-embedding models. CoRR, abs/1411.4072, 2014.\nT a b\nla 6 :\nM at\nri z\nd e\nco n\nfu si\no\u0301 n\n: P\nre d\nic ci\no\u0301 n\nd e\nti p\no s\nd e\na ri\nst a s\n(W o rd\nN et ) h y p e r d o m r e g p a r t m e r o d o m c a t p a r t h o lo d o m u s a g e a ls o m e m b m e r o in s t h y p o d o m m e m b u s a g e m e m\nb h o lo\nh y p o\nh y p e r\n8 3 .6\n7 %\n0 .0\n1 %\n1 .1\n4 %\n0 .7\n6 %\n0 .4\n3 %\n0 .0\n2 %\n0 .5\n6 %\n1 .9\n6 %\n0 .1\n3 %\n0 .0\n2 %\n2 .7\n7 %\n8 .5\n2 %\nd o m\nr e g\n2 .0\n8 %\n6 5 .5\n8 %\n2 5 .0\n% 0 .3\n3 %\n0 .9\n3 %\n0 .0\n% 0 .2\n1 %\n3 .2\n7 %\n0 .2\n4 %\n0 .0\n% 1 .1\n2 %\n1 .2\n5 %\np a r t\nm e r o\n2 9 .2\n1 %\n0 .7\n4 %\n4 4 .4\n7 %\n1 .1\n% 3 .5\n4 %\n0 .0\n1 %\n0 .7\n9 %\n4 .1\n2 %\n0 .3\n4 %\n0 .0\n% 4 .4\n7 %\n1 1 .2\n%\nd o m\nc a t\n1 4 .8\n8 %\n0 .0\n2 %\n1 .1\n9 %\n7 8 .9\n1 %\n0 .1\n1 %\n0 .0\n1 %\n0 .4\n4 %\n0 .1\n8 %\n0 .0\n4 %\n0 .0\n% 0 .2\n5 %\n3 .9\n7 %\np a r t\nh o lo\n1 5 .5\n3 %\n0 .0\n9 %\n3 .2\n8 %\n0 .3\n4 %\n4 5 .3\n6 %\n0 .0\n1 %\n0 .7\n8 %\n2 .2\n5 %\n1 .2\n4 %\n0 .0\n% 6 .6\n7 %\n2 4 .4\n5 %\nd o m\nu s a g e\n4 .2\n8 %\n0 .0\n% 0 .0\n6 %\n0 .3\n9 %\n0 .0\n3 %\n9 3 .4\n1 %\n0 .9\n9 %\n0 .0\n6 %\n0 .0\n% 0 .0\n% 0 .1\n1 %\n0 .6\n8 %\na ls\no 8 .0\n% 0 .0\n% 0 .3\n8 %\n0 .4\n3 %\n0 .1\n9 %\n0 .0\n3 %\n7 8 .0\n% 1 .9\n2 %\n0 .0\n7 %\n0 .0\n1 %\n4 .6\n4 %\n6 .3\n5 %\nm e m\nb m\ne r o\n1 1 .7\n% 0 .1\n8 %\n1 .7\n6 %\n0 .2\n% 0 .7\n7 %\n0 .0\n% 0 .9\n3 %\n5 0 .7\n2 %\n0 .2\n1 %\n0 .0\n% 2 4 .6\n7 %\n8 .8\n7 %\nin s t\nh y p o\n2 .4\n7 %\n0 .0\n6 %\n0 .8\n7 %\n0 .0\n5 %\n1 .8\n1 %\n0 .0\n% 0 .2\n% 0 .5\n3 %\n8 0 .4\n2 %\n0 .0\n% 1 .5\n6 %\n1 2 .0\n2 %\nd o m\nm e m\nb u s a g e\n1 .1\n% 0 .0\n% 0 .0\n3 %\n0 .0\n3 %\n0 .1\n4 %\n0 .0\n% 1 .1\n3 %\n0 .0\n9 %\n0 .0\n6 %\n9 3 .6\n2 %\n0 .0\n9 %\n3 .7\n3 %\nm e m\nb h o lo\n1 1 .2\n4 %\n0 .0\n5 %\n0 .6\n7 %\n0 .0\n9 %\n1 .8\n3 %\n0 .0\n% 0 .9\n1 %\n1 4 .0\n1 %\n0 .2\n1 %\n0 .0\n% 6 2 .3\n7 %\n8 .6\n1 %\nh y p o\n1 0 .5\n2 %\n0 .0\n1 %\n0 .4\n2 %\n0 .3\n1 %\n1 .1\n8 %\n0 .0\n2 %\n0 .5\n7 %\n1 .2\n3 %\n1 .1\n8 %\n0 .0\n1 %\n3 .6\n7 %\n8 0 .8\n8 %\nT ab\nla 7:\nM at\nri z\nd e\nco n\nfu si\no\u0301n :\nP re\nd ic\nci o\u0301 n\nd e\nti p\no s\nd e\nn o d\no s\n(E IC\nH )\nS u b a m\nb it\no P\nr o v in\nc ia\nC o m\nu n id\na d\nA n e x o s\nH e r r a m\nie n t a\nC a n t o n\nL e n g u a\nI n m\na t e r ia\nl A\nm b it\no P a r r o q u ia\nD e t a ll e S u b a m\nb it\no\nS u b a m\nb it\no 1 4 .5\n3 %\n0 .0\n% 0 .0\n% 2 .5\n6 %\n0 .0\n% 0 .0\n% 0 .0\n% 4 7 .8\n6 %\n0 .0\n% 0 .8\n5 %\n3 4 .1\n9 %\nP r o v in\nc ia\n0 .0\n% 7 .1\n4 %\n2 .0\n4 %\n0 .0\n% 0 .0\n% 6 9 .3\n9 %\n5 .1\n% 1 .0\n2 %\n0 .0\n% 1 5 .3\n1 %\n0 .0\n%\nC o m\nu n id\na d\n0 .0\n% 0 .0\n% 0 .0\n% 7 .9\n1 %\n0 .0\n% 1 .4\n4 %\n0 .0\n% 2 5 .1\n8 %\n0 .0\n% 6 5 .4\n7 %\n0 .0\n%\nA n e x o s\n0 .0\n% 0 .0\n% 0 .0\n% 8 1 .1\n6 %\n0 .0\n% 0 .0\n% 0 .0\n% 1 8 .6\n3 %\n0 .0\n% 0 .2\n1 %\n0 .0\n%\nH e r r a m\nie n t a\n0 .0\n% 0 .0\n% 0 .0\n% 0 .6\n8 %\n3 6 .9\n9 %\n0 .0\n% 0 .0\n% 6 2 .3\n3 %\n0 .0\n% 0 .0\n% 0 .0\n%\nC a n t o n\n0 .0\n% 3 .7\n4 %\n0 .1\n% 5 .2\n7 %\n0 .0\n% 1 2 .1\n8 %\n0 .0\n% 2 4 .2\n6 %\n0 .0\n% 5 4 .2\n7 %\n0 .1\n9 %\nL e n g u a\n0 .0\n% 0 .0\n% 0 .0\n% 6 .2\n5 %\n0 .0\n% 0 .0\n% 0 .0\n% 2 1 .2\n5 %\n0 .0\n% 7 2 .5\n% 0 .0\n%\nI n m\na t e r ia\nl 0 .0\n1 %\n0 .0\n% 0 .0\n% 9 .4\n4 %\n0 .1\n9 %\n0 .0\n% 0 .0\n1 %\n8 9 .7\n7 %\n0 .0\n% 0 .5\n6 %\n0 .0\n1 %\nA m\nb it\no 4 4 .0\n% 0 .0\n% 0 .0\n% 0 .0\n% 0 .0\n% 0 .0\n% 0 .0\n% 2 8 .0\n% 0 .0\n% 0 .0\n% 2 8 .0\n%\nP a r r o q u ia\n0 .0\n2 %\n0 .4\n2 %\n0 .0\n8 %\n2 .3\n4 %\n0 .0\n2 %\n2 .1\n1 %\n0 .3\n5 %\n2 9 .6\n7 %\n0 .0\n% 6 4 .8\n2 %\n0 .1\n8 %\nD e t a ll e S u b a m\nb it\no 1 .6\n3 %\n0 .0\n% 0 .0\n% 5 .4\n2 %\n0 .0\n% 0 .1\n8 %\n0 .0\n% 4 9 .3\n7 %\n0 .0\n% 4 .5\n2 %\n3 8 .8\n8 %\nT a b\nla 8:\nM at\nri z\nd e\nco n\nfu si\no\u0301 n\n: P\nre d\nic ci\no\u0301n d\ne ti\np o s\nd e\nar is\nta s\n(E IC\nH )\nC A\nN T\nO N\nL C\nO M\nL O\nC H\nE R\nR A\nM P A\nR R\nO Q\nL A\nN E X\nO S U\nB A\nM B\nI T\nO P\nL E N\nG U\nA A\nM B\nI T\nO D\nS U\nB A\nM B\nI T\nO P\nC A\nN T\nO N\nL 2 5 .0\n5 %\n1 .9\n4 %\n1 2 .6\n2 %\n0 .0\n% 2 6 .8\n% 6 .9\n9 %\n0 .0\n% 2 2 .9\n1 %\n3 .6\n9 %\n0 .0\n%\nC O\nM 0 .0\n% 9 7 .9\n2 %\n0 .1\n8 %\n0 .0\n% 0 .0\n9 %\n0 .3\n7 %\n0 .0\n% 1 .4\n% 0 .0\n4 %\n0 .0\n%\nL O\nC 0 .0\n% 0 .1\n2 %\n9 6 .7\n7 %\n0 .0\n4 %\n1 .0\n8 %\n1 .5\n1 %\n0 .0\n% 0 .3\n3 %\n0 .1\n5 %\n0 .0\n%\nH E R\nR A\nM 0 .0\n% 0 .0\n% 1 .4\n9 %\n4 4 .0\n3 %\n2 .2\n4 %\n4 8 .5\n1 %\n0 .0\n% 3 .7\n3 %\n0 .0\n% 0 .0\n%\nP A\nR R\nO Q\nL 0 .8\n9 %\n0 .9\n9 %\n1 3 .9\n5 %\n0 .0\n2 %\n5 9 .1\n1 %\n3 .3\n4 %\n0 .0\n% 1 9 .2\n% 2 .4\n5 %\n0 .0\n5 %\nA N\nE X\nO 0 .1\n4 %\n0 .1\n4 %\n0 .4\n9 %\n0 .0\n2 %\n2 .4\n6 %\n9 5 .8\n7 %\n0 .0\n% 0 .7\n3 %\n0 .1\n1 %\n0 .0\n5 %\nS U\nB A\nM B\nI T\nO P\n0 .8\n1 %\n0 .0\n% 1 0 .5\n7 %\n0 .0\n% 8 .1\n3 %\n9 .7\n6 %\n3 0 .8\n9 %\n1 4 .6\n3 %\n5 .6\n9 %\n1 9 .5\n1 %\nL E N\nG U\nA 0 .0\n% 1 .0\n6 %\n0 .0\n9 %\n0 .0\n% 0 .0\n1 %\n0 .3\n9 %\n0 .0\n% 9 8 .3\n4 %\n0 .1\n% 0 .0\n%\nA M\nB I T\nO 0 .0\n1 %\n0 .0\n4 %\n0 .2\n8 %\n0 .0\n1 %\n0 .0\n4 %\n1 .9\n% 0 .0\n% 2 .3\n8 %\n9 5 .3\n3 %\n0 .0\n2 %\nD S U\nB A\nM B\nI T\nO P\n0 .1\n8 %\n0 .1\n8 %\n1 .9\n6 %\n0 .0\n% 2 .6\n7 %\n3 .2\n1 %\n0 .8\n9 %\n2 2 .4\n6 %\n7 .6\n6 %\n6 0 .7\n8 %\n(a) En funcio\u0301n del taman\u0303o del conjunto de entrenamiento.\n(b) En funcio\u0301n del nu\u0301mero de dimensiones.\n(c) En funcio\u0301n del taman\u0303o de la ventana de seleccio\u0301n.\nFigura 11: Ana\u0301lisis de la inmersio\u0301n (prediccio\u0301n de tipos de nodo).\n(a) k-Nearest Neighbor.\n(b) Random Forest.\n(c) Red Neuronal Artificial.\nFigura 12: Ana\u0301lisis clasificacio\u0301n tipos de nodo por diferentes me\u0301todos.\n(a) En funcio\u0301n del taman\u0303o del conjunto de entrenamiento.\n(b) En funcio\u0301n del nu\u0301mero de dimensiones.\n(c) En funcio\u0301n del taman\u0303o de la ventana de seleccio\u0301n.\nFigura 14: Ana\u0301lisis de la inmersio\u0301n (prediccio\u0301n de tipos de arista).\n(a) k-Nearest Neighbor.\n(b) Random Forest.\n(c) Red Neuronal Artificial.\nFigura 15: Ana\u0301lisis clasificacio\u0301n tipos de arista por diferentes me\u0301todos."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this work we present a new approach to the treatment of property graphs using neural encoding techniques derived from machine learning. Specifically, we will deal with the problem of embedding property graphs in vector spaces. Throughout this paper we will use the term embedding as an operation that allows to consider a mathematical structure, X, inside another structure Y , through a function, f : X \u2192 Y . We are interested on embeddings capable of capturing, within the characteristics of a vector space (distance, linearity, clustering, etc.), the interesting features of the graph. For example, it would be interesting to get embeddings that, when projecting the nodes of the graph into points of a vector space, keep edges with the same type of the graph into the same vectors. In this way, we can interpret that the semantic associated to the relation has been captured by the embedding. Another option is to check if the embedding verifies clustering properties with respect to the types of nodes, types of edges, properties, or some of the metrics that can be measured on the graph. Subsequently, we will use these good embedding features to try to obtain prediction / classification / discovery tools on the original graph. This paper is structured as follows: we will start by giving some preliminary definitions necessary for the presentation of our proposal and a brief introduction to the use of artificial neural networks as encoding machines. After this review, we will present our embedding proposal based on neural encoders, and we will verify if the topological and semantic characteristics of the original graph have been maintained in the new representation. After evaluating the properties of the new representation, it will be used to carry out machine learning and discovery tasks on real databases. Finally, we will present some conclusions and future work proposals that have arisen during the implementation of this work.", "creator": "LaTeX with hyperref package"}}}