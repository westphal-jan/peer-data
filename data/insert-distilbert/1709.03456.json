{"id": "1709.03456", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "CLAD: A Complex and Long Activities Dataset with Rich Crowdsourced Annotations", "abstract": "this paper introduces a novel activity dataset which generally exhibits real - life functions and diverse scenarios of complex, temporally - extended cellular human activities and normal actions. the dataset presents a set of realistic videos of actors performing everyday activities both in a natural and unscripted manner. the dataset was recorded using a static kinect 2 sensor which is moderately commonly used on many robotic computing platforms. the dataset comprises of rgb - d images, point cloud pixel data, utilizing automatically generated skeleton tracks in addition to crowdsourced annotations. furthermore, we also describe from the methodology here used to acquire annotations through crowdsourcing. unfortunately finally some activity recognition benchmarks will are presented using current state - of - when the - art techniques. we believe that this dataset is particularly suitable as a testbed for activity recognition research but it can also be applicable to for other common tasks in robotics / computer vision research such phenomena as object detection and human skeleton tracking.", "histories": [["v1", "Mon, 11 Sep 2017 16:01:17 GMT  (681kb,D)", "http://arxiv.org/abs/1709.03456v1", null], ["v2", "Thu, 21 Sep 2017 16:52:04 GMT  (682kb,D)", "http://arxiv.org/abs/1709.03456v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["jawad tayyub", "majd hawasly", "david c hogg", "anthony g cohn"], "accepted": false, "id": "1709.03456"}, "pdf": {"name": "1709.03456.pdf", "metadata": {"source": "CRF", "title": "CLAD: A Complex and Long Activities Dataset with Rich Crowdsourced Annotations", "authors": ["Jawad Tayyub", "Majd Hawasly", "David C. Hogg", "Anthony G. Cohn"], "emails": [], "sections": [{"heading": null, "text": "Keywords Activity Dataset, Crowdsourcing"}, {"heading": "1 Introduction", "text": "An artificial intelligence system embedded in a human environment, such as a robot, is required to perform activity recognition in order to fully comprehend its environment so it could take appropriate steps and make decisions to gain utility. Recognisable/human activities largely vary in complexity, length and expressiveness from short primitive actions lasting for a few seconds, such as \u2018pick\u2019, \u2018wave\u2019, \u2018approach\u2019 etc., to longer activities lasting minutes, hours or days, such as \u2018dinner at a restaurant\u2019, \u2018surgery in a hospital\u2019, \u2018studying for exams\u2019, etc. Currently, most research in human activity recognition has been focused on recognising relatively short activities[]. Though there has been a steady shift towards slightly longer activity recognition, there is still a need for standardised datasets that present longer and more complex activities. In the service robotics domain, this requirement has now become imperative to tackle. A fully autonomous system capable of autonomously running for days or months amongst humans, for example the Strands project Hawes et al. (2016), needs to be equipped with the capability of recognising long-term activities that last hours or days using embedded sensors. However, popular activity datasets, e.g. OPPORTUNITY Activity Dataset Roggen et al. (2010), offer sensor data from a multitude of inertial and other sensors that are either body worn or installed in a perspective point, such as on the ceiling or corners of rooms, whose outputs are not normally available to an embedded robot in general environments. It is more natural to assume that the robot has to rely primarily on video data from onboard cameras for activity analysis and recognition in these settings.\nIn this paper, we propose a dataset called Complex and Long Activities Dataset (CLAD). The dataset can be accessed at this web address : https://doi.org/10.\n5518/249. This dataset is i) recorded from the perspective of a mobile robot using a video RGBD camera which is commonly found sensor on a robot and ii) aimed to promote research on long-term complex activities that span longer periods of time than previous datasets. One of the biggest challenges in gathering a datasets consisting of long activity videos comes with the challenge of accurate annotation in a timely and cost effective manner. We therefore further provide a mechanism for obtaining annotations effectively through the use of crowdsourcing. Crowdsourced annotations are achieved through a worldwide pool of non-expert users who independently annotate samples of the videos in an objective and unbiased manner. This generates annotations which reflect aspects of true human understanding of the activities within the video, but adds to the complexity from the use of unconstrained natural language in the annotations which exhibit large variability when describing the videos.\nThe remainder of this paper is organised as follows: in Section 2 we describe some related datasets that are presently available. Then, in Section 3 we provide a detailed description of the presented CLAD dataset. In Section 4 we describe the ground-truth collection process and finally we conclude in Section 5.\n1University of Leeds, UK 2Five AI, UK\nCorresponding author: Jawad Tayyub, University of Leeds Woodhouse Ln., Leeds, LS2 9JT, UK. Email: sc12jbmt@leeds.ac.uk\nPrepared using sagej.cls [Version: 2016/06/24 v1.10]\nar X\niv :1\n70 9.\n03 45\n6v 1\n[ cs\n.C V\n] 1\n1 Se\np 20\n17"}, {"heading": "2 Related Datasets", "text": "Activity recognition has been a popular research direction in robotics. Many activity datasets are openly available for activity recognition research ranging from datasets consisting of simple repetitive action such as jumping, walking etc. (e.g., []) to more complex activities such as cooking food, cleaning microwave etc. (e.g., []). Most available datasets are can be categorised into three categories: heterogeneous actions, specific actions and others Chaquet et al. (2013). Our dataset falls in the \u2019activities of daily living\u2019 subcategory of the \u2019specific actions category\u2019. Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc. from a large number of naturally recorded instances. These datasets were mostly generated from movies or YouTube videos. However, videos in these datasets, though longer than previous datasets, were still short activities where each video only comprised of one basic activity thus temporal segmentation was assumed. Duckworth et al. (2017) collected a dataset comprising of long videos of human activities recorded by a mobile robot in a kitchen setting. Videos were continuous and comprised of multiple activities. However, the annotations of this dataset were for segmented short actions such as pick up, put down, pour etc. The Cornell Activity Datasets (CAD-120 and CAD60) Koppula et al. (2013); Sung et al. (2012) presented challenging and datasets consisting of longer videos than before and with complex activities such as stacking boxes, taking medicine etc. Though this was a benchmarking dataset, it did have some shortcomings. The videos in this dataset only consisted of a single actor and the annotations provided were limited to two levels of granularity: highlevel activities per video and sub-level actions within each high-level activity video. In all above mentioned datasets, annotations for activity labels were consistent, extracted from pre-set label lists and collected with a set of welldefined rules in place. The CLAD dataset presented in this paper is created with particular focus to address some of these shortcomings. The dataset presented here consists of much longer activities for example service at a restaurant, consisting of multiple actors, contains deep hierarchy of activities and has crowd-sourced freely descriptive annotation i.e. annotators were given the freedom to describe the activities using their own words. Given the advancement towards long-running autonomous robots gathering longer videos of activity data, using crowdsourcing to elicit annotations is becoming popular and therefore such annotations are included in our dataset. To our knowledge, this is the only dataset that presents multiple new challenges to the research community namely, modelling longer naturally occurring and variable activities with many levels of granularity as well as semantic analysis and extraction of information from crowdsourced annotations."}, {"heading": "3 Dataset Description", "text": "The dataset presented in this paper can be accessed from [Dataset URL DOI]. The full size of the dataset available\nPrepared using sagej.cls\non-line is 222 GB. This is the entire dataset except the point clouds data of the scenes which are available upon request."}, {"heading": "3.1 Recording Setup", "text": "This dataset comprises of 62 videos recorded at a high resolution of 1920x1080 pixels using a Microsoft Kinect 2 sensor. The sensor was installed at a height of approximately 4.5 feet from the ground. The aim of this view point was to simulate one that a commonly used robot such as a PR2, Tiago, Scitos G5 etc. would normally have when patrolling in an environment. Approximately 8-10 objects relevant to the activity were available to the subjects to use. Five subjects were then employed to act out the scenes. The subjects were told of the top-level activity to act out, after which they were free to use the objects available to act out the task. There were 1-2 subjects involved in each scene. These subjects were carefully selected from outside the faculty having no computer science research experience in order to capture natural acting of activities which is unbiased due to any research knowledge. Minimal to no instructions were given to the subjects as to how to act out the requested scenes; in fact, they were encouraged to be variable and random in performing the task during different recordings. This allowed for a natural and unbiased data capture of activities exhibiting high variation. In natural setting such as an actual restaurant for a variety of practical and ethical reasons, all the videos were recorded in our research lab; this is a short coming of the dataset which otherwise tries to capture activities in as natural way as possible. Every effort was made to minimise the influence of the environment on the actors by clearing and neutralising any irrelevant items where possible."}, {"heading": "3.2 Dataset Content", "text": "This dataset comprises of 62 videos of everyday activities ranging from 3 to 10 minutes in length. Only one toplevel activity occurs within each of the full videos however numerous sub-level activities occur during the entire video. The top-level activities comprise of three natural scenes of having breakfast, lunch in a restaurant and working in an office. These scenarios were chosen as normally occurring in a at home, restaurant or an office locations likely for a robot to be deployed in. Sample images from the dataset are shown in figure 1. Each of the 62 recordings consists of the following data and meta-data:\n\u2022 Video: A low-resolution compressed .mp4 video of the recording. \u2022 Images: Uncompressed images of the recording\nwhere each image is labelled Kinect {frame number}.{format} and has a resolution of 1920x1080 pixels. {} denotes a wildcard for variables in the file names. The images are provided in .png or .jpg formats. \u2022 Skeletons: Two state-of-the-art skeleton trackers are\nused to generate skeleton tracks for all subjects in the recording Wei et al. (2016); Rafi et al. (2016). These are provided in the skeletons files for each recording. Each skeleton file contains two files corresponding to the two different skeleton trackers used namely cpm and aachen. Each of the skeleton tracker file consists\nof files labelled person {person Id}. Depending on the number of subjects in the video, each person would have a dedicated file of their skeleton tracks identified by the person Id. In each of the person {person Id} file, there are files labelled track {frame number}.txt. These files are generated per frame and contain a list of (x,y) coordinates that define the location of all joints, of the skeleton onto the image frame in the following order: head, neck, r-shoulder, r-elbow, rhand, l-shoulder, l-elbow, l-hand, r-hip, r-knee, r-foot, l-hip, l-hand, l-foot, torso where r and l refer to right and left. \u2022 Point Cloud: This folder consists of the point cloud\nfiles labelled pc {frame number}. These are not available on-line but can be acquired upon request. These are not uploaded due to their large file sizes. \u2022 Annotations: This folder contains annotations of the recording acquired through crowdsourcing. Since there are multiple annotators employed for each video, the annotation files are split by annotator id. The Annotations folder therefor contains files labelled annotation {annotator id}. Each of the files then contains a list of annotations where each row corresponds to the format: {subject performing the activity}, {activity label/description}, {starting frame}, {ending frame}. These files are simple CSV files which are easy and quick to parse and use. More details on how these annotations are gathered is provided in section 4.\nThe file structure of the entire dataset is shown in figure 2.\nPrepared using sagej.cls"}, {"heading": "4 Dataset Annotations", "text": "Annotation of videos involves the identification of activities occurring within the video along with their temporal boundary (their start and end times). Traditionally this task has been performed manually, usually by experts who hold some domain knowledge. The task is expensive, timeconsuming and, in case of having domain knowledgeable experts, can result in biased annotations (Roggen et al. 2010; Nguyen-Dinh et al. 2013). In order to reduce cost and time taken to annotate activities, crowdsourcing platforms are increasingly gaining popularity. A crowdsourcing platform offer a large pool of world-wide workers that are able to perform a human intelligent task (HIT), such as annotation of a video, for a small financial incentive. For large datasets and long videos, as is the case for our dataset, this is a suitable option to attain annotations in a cost-effective and efficient manner. Furthermore, since many annotators are employed to annotate each single video, this helped ensure that a rich and varied perspective of the latent activities were reflected in the annotations. A single person usually tends to annotate videos sequentially one activity after the other in a flat temporal sequence. With multiple annotators, we increased the degree to which we obtained annotations at multiple level of temporal granualirty. Combining these annotations from different workers provides a richer annotation of the video. For our dataset, we make use of a popular crowdsourcing platform called Amazon Mechanical Turk(AMT) 2.\nThere are however many challenges and design decisions to be taken when developing a system to elicit annotations using crowdsourcing such as deciding on the payment amount, making a clear interface for workers to perform the task, detection and removal of spam or non-diligent workers etc. Overcoming these challenges factors greatly affects the ability to obtain accurate annotations as shown by NguyenDinh et al. (2013). We will describe our design process next."}, {"heading": "4.1 Interface Design", "text": "For the success of a crowd-sourced annotation system, it is crucial that an easy-to-use interface is provided for workers to submit their annotations through. An image of our interface is shown in figure 3. The interface can be divided into the following parts for ease of description:\n\u2022 Instructions: This part simply provides instructions for worker on what is required from them and how to use the interface. The instructions provided in our interface are simply worded since some annotators may not be native English speakers. Also, only the top-level scenario of the activity video is mentioned, no other instructions are given to worker in order to obtain a maximally unbiased set of annotations. \u2022 Video Player: An easy to use video player with frame\nskipping control is provided along with the current frame number display. This allows the worker to fine tune the temporal boundaries of identified activities. \u2022 Verification Questions: An important aspect of the\ninterface is the qualification questions. In order to filter out workers who are inputting spam data or tend to fill in the form with random character inputs, we included two verification questions. These questions\nare objective, randomly placed and have been preanswered. A boolean question and a numeric question was used. The aim of the questions was to enforce the user to watch the entire video before answering. For example, a sample boolean question is \u2019Did the subject use the teapot?\u2019 and a sample numeric question is \u2019How many times did the subject flip the newspaper pages?\u2019. Each numeric question had answers within a range pre-specified? If the worker failed to answer these correctly, they would be disqualified from continuing the task. Through experimenting, we found that inclusion of verification questions helped in discouraging non-diligent user from trying to cheat the task. \u2022 Annotation Submission Form: This section allows the user to input the start and end frames of the activity, the activity label and the actor/s performing the activity. The workers are not allowed to type in frame numbers but rather use a button to automatically insert them in the fields. This is done to minimise erroneous input in the frame number\u2019s numeric fields. An important design decision made here was to allow the user to freely enter the activity label with minor restrictions such as alphabetical input only and 4 words maximum per activity label to disallow long essay-like descriptions and sentences but produce concise short labels of activities. For example \u2019consuming coffee\u2019 instead of \u2019the subject is now drinking a cup of coffee\u2019. It is common to preset a list of labels for workers to choose from to avoid ambiguity and variability in natural language but at the same time limits the descriptive power of the worker. We have chosen to allow the workers full control over the choice of words to describe the identified activity to ensure that annotations captured are as expressive without being excessively lengthy. This also removes the requirement of generating a pre-set list of labels\nPrepared using sagej.cls\nfor workers to choose from, which can be a timeconsuming process in itself. \u2022 Annotations Table: In this section the worker can\nobserve the annotations they have identified so far, edit them or delete them. We further impose a restriction of a minimum number of annotations required for each task. Longer tasks would require a higher number of minimum annotations and shorter would require a smaller number. The worker is unaware of this number and is prompted with an error if he/she tries to submit the task without having met the minimum activities identified. Repeated attempts to submit the task with insufficient number of identified activities results in a disqualification warning followed by disqualification. \u2022 Annotation Time-line: The annotation time-line is\nsimply a graphical representation of the annotations identified along the time-line of the video in frames. This tool is intended to encourage workers to identify parallel and overlapping activities as two overlapping activities will appear on different rows. We are, however, unable to quantify the utility of this feature as there seemed to be no significant change in the quality of annotations received with and without this feature included. Further investigation of the time-line remains as future work."}, {"heading": "4.2 Parameters", "text": "Besides the design decisions that were made regarding the interface used, Amazon Mechanical Turk allows the setting of various crucial parameters that affect the quality of annotations produced. We will briefly describe the important parameters and our choices of their values in this section. An overview of the chosen parameters is shown in table 1.\nIt can be seen from the table that there are multiple parameters that require careful thought and experimentation in order optimise the quality of annotations gathered. We will further elaborate some of our choices for the parameters. The \u2019Assignment Duration\u2019 defines maximum time allotted to the worker to finish the task. A pilot study with 6 volunteers was performed to estimate the average maximum needed time (15 minutes) for a task of annotating a 1000 frames video. Given this, any task of f number of frames is give a maximum time of (f \u2217 15)/1000 minutes following a linear relationship. The \u2019Reward\u2019 amount is computed at a\nmaximum of $3 for a task of 1000 frames. This amount is split into three parts: i) $1.5 for passing the verification test and submitting minimum number of required annotations, ii) up to $1 for quality of labels, start and end times accuracy which were manually checked for a few randomly selected labels and iii) up to $0.5 for identification of parallel or overlapping activities. Furthermore, bonuses of up to $1 for a 1000 frame long video were awarded for exceptional work as additional motivation for workers to perform well. Similar to the assignment duration, the reward was linearly adjusted depending on the length of the video in frames. Given the above price model, we ensured that workers were sufficiently motivated to perform the task effectively and within a reasonable amount of time."}, {"heading": "5 Conclusion", "text": "In this paper, we have presented an activity dataset of naturally occurring daily activities as might be observed by mobile robots. The dataset can be accessed at https:// doi.org/10.5518/249. We further presented the activity annotations gathered through the use of crowdsourcing. We believe this dataset will be useful in robotics and computer vision research. The dataset presents new challenges for long-term autonomous robots systems to comprehend activities they observe. In future, we plan to augment the dataset with object tracks as well as add other activities that involve more subjects interacting and boasts higher complexity. We also plan to record videos in a real-life setting such as a real restaurant or a real office."}, {"heading": "Acknowledgements", "text": "We acknowledge our colleagues in the School of Computing Robotics Lab, other schools in the university, and in the STRANDS project consortium (http://strands-project.eu) for their contributions. We further acknowledge the financial support provided by EU FP7 project 600623 (STRANDS).\nNotes\n1. http://homepages.inf.ed.ac.uk/rbf/CAVIAR/caviar.htm 2. https://www.mturk.com/mturk/welcome"}], "references": [{"title": "Actions as space-time shapes", "author": ["M Blank", "L Gorelick", "E Shechtman", "M Irani", "R Basri"], "venue": "Computer Vision,", "citeRegEx": "Blank et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blank et al\\.", "year": 2005}, {"title": "A (2013) A survey of video datasets for human action and activity recognition", "author": ["JM Chaquet", "EJ Carmona", "Fern\u00e1ndez-Caballero"], "venue": "Computer Vision and Image Understanding", "citeRegEx": "Chaquet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chaquet et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation for unsupervised activity analysis on an autonomous mobile robot", "author": ["P Duckworth", "M Al-Omari", "J Charles", "DC Hogg", "AG Cohn"], "venue": null, "citeRegEx": "Duckworth et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Duckworth et al\\.", "year": 2017}, {"title": "The STRANDS project: Long-term autonomy in everyday environments", "author": ["N Hawes", "C Burbridge", "F Jovan", "L Kunze", "B Lacerda", "L Mudrov\u00e1", "J Young", "J Wyatt", "D Hebesberger", "T K\u00f6rtner"], "venue": null, "citeRegEx": "Hawes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hawes et al\\.", "year": 2016}, {"title": "Learning human activities and object affordances from RGB-D videos", "author": ["HS Koppula", "R Gupta", "A Saxena"], "venue": "The International Journal of Robotics Research", "citeRegEx": "Koppula et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Koppula et al\\.", "year": 2013}, {"title": "Recognizing realistic actions from videos in the wild", "author": ["J Liu", "J Luo", "M Shah"], "venue": "Computer vision and pattern recognition,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Actions in context", "author": ["M Marsza\u0142ek", "I Laptev", "C Schmid"], "venue": "IEEE Conference on Computer Vision & Pattern Recognition", "citeRegEx": "Marsza\u0142ek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Marsza\u0142ek et al\\.", "year": 2009}, {"title": "Tagging human activities in video by crowdsourcing", "author": ["LV Nguyen-Dinh", "C Waldburger", "D Roggen", "G Tr\u00f6ster"], "venue": "Proceedings of the 3rd ACM conference on International conference on multimedia retrieval", "citeRegEx": "Nguyen.Dinh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen.Dinh et al\\.", "year": 2013}, {"title": "Collecting complex activity datasets in highly rich networked sensor environments", "author": ["D Roggen", "A Calatroni", "M Rossi", "T Holleczek", "K F\u00f6rster", "G Tr\u00f6ster", "P Lukowicz", "D Bannach", "G Pirkl", "A Ferscha"], "venue": "Networked Sensing Systems (INSS),", "citeRegEx": "Roggen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Roggen et al\\.", "year": 2010}, {"title": "Recognizing human actions: a local svm approach", "author": ["C Schuldt", "I Laptev", "B Caputo"], "venue": "Pattern Recognition,", "citeRegEx": "Schuldt et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schuldt et al\\.", "year": 2004}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["K Soomro", "AR Zamir", "M Shah"], "venue": null, "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Unstructured human activity detection from RGBD images", "author": ["J Sung", "C Ponce", "B Selman", "A Saxena"], "venue": "Robotics and Automation (ICRA),", "citeRegEx": "Sung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2012}, {"title": "Convolutional pose machines", "author": ["SE Wei", "V Ramakrishna", "T Kanade", "Y Sheikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Wei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2016}, {"title": "Event-based analysis of video", "author": ["L Zelnik-Manor", "M Irani"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "Zelnik.Manor and Irani,? \\Q2001\\E", "shortCiteRegEx": "Zelnik.Manor and Irani", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "A fully autonomous system capable of autonomously running for days or months amongst humans, for example the Strands project Hawes et al. (2016), needs to be equipped with the capability of recognising long-term activities that last hours or days using embedded sensors.", "startOffset": 125, "endOffset": 145}, {"referenceID": 3, "context": "A fully autonomous system capable of autonomously running for days or months amongst humans, for example the Strands project Hawes et al. (2016), needs to be equipped with the capability of recognising long-term activities that last hours or days using embedded sensors. However, popular activity datasets, e.g. OPPORTUNITY Activity Dataset Roggen et al. (2010), offer sensor data from a multitude of inertial and other sensors that are either body worn or installed in a perspective point, such as on the ceiling or corners of rooms, whose outputs are not normally available to an embedded robot in general environments.", "startOffset": 125, "endOffset": 362}, {"referenceID": 0, "context": "Most available datasets are can be categorised into three categories: heterogeneous actions, specific actions and others Chaquet et al. (2013). Our dataset falls in the \u2019activities of daily living\u2019 subcategory of the \u2019specific actions category\u2019.", "startOffset": 121, "endOffset": 143}, {"referenceID": 0, "context": "Most available datasets are can be categorised into three categories: heterogeneous actions, specific actions and others Chaquet et al. (2013). Our dataset falls in the \u2019activities of daily living\u2019 subcategory of the \u2019specific actions category\u2019. Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al.", "startOffset": 121, "endOffset": 331}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally.", "startOffset": 86, "endOffset": 106}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc.", "startOffset": 86, "endOffset": 333}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al.", "startOffset": 86, "endOffset": 622}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al.", "startOffset": 86, "endOffset": 641}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc.", "startOffset": 86, "endOffset": 689}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc. from a large number of naturally recorded instances. These datasets were mostly generated from movies or YouTube videos. However, videos in these datasets, though longer than previous datasets, were still short activities where each video only comprised of one basic activity thus temporal segmentation was assumed. Duckworth et al. (2017) collected a dataset comprising of long videos of human activities recorded by a mobile robot in a kitchen setting.", "startOffset": 86, "endOffset": 1155}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc. from a large number of naturally recorded instances. These datasets were mostly generated from movies or YouTube videos. However, videos in these datasets, though longer than previous datasets, were still short activities where each video only comprised of one basic activity thus temporal segmentation was assumed. Duckworth et al. (2017) collected a dataset comprising of long videos of human activities recorded by a mobile robot in a kitchen setting. Videos were continuous and comprised of multiple activities. However, the annotations of this dataset were for segmented short actions such as pick up, put down, pour etc. The Cornell Activity Datasets (CAD-120 and CAD60) Koppula et al. (2013); Sung et al.", "startOffset": 86, "endOffset": 1514}, {"referenceID": 0, "context": "Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marsza\u0142ek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc. from a large number of naturally recorded instances. These datasets were mostly generated from movies or YouTube videos. However, videos in these datasets, though longer than previous datasets, were still short activities where each video only comprised of one basic activity thus temporal segmentation was assumed. Duckworth et al. (2017) collected a dataset comprising of long videos of human activities recorded by a mobile robot in a kitchen setting. Videos were continuous and comprised of multiple activities. However, the annotations of this dataset were for segmented short actions such as pick up, put down, pour etc. The Cornell Activity Datasets (CAD-120 and CAD60) Koppula et al. (2013); Sung et al. (2012) presented challenging and datasets consisting of longer videos than before and with complex activities such as stacking boxes, taking medicine etc.", "startOffset": 86, "endOffset": 1534}, {"referenceID": 12, "context": "\u2022 Skeletons: Two state-of-the-art skeleton trackers are used to generate skeleton tracks for all subjects in the recording Wei et al. (2016); Rafi et al.", "startOffset": 123, "endOffset": 141}, {"referenceID": 12, "context": "\u2022 Skeletons: Two state-of-the-art skeleton trackers are used to generate skeleton tracks for all subjects in the recording Wei et al. (2016); Rafi et al. (2016). These are provided in the skeletons files for each recording.", "startOffset": 123, "endOffset": 161}, {"referenceID": 8, "context": "The task is expensive, timeconsuming and, in case of having domain knowledgeable experts, can result in biased annotations (Roggen et al. 2010; Nguyen-Dinh et al. 2013).", "startOffset": 123, "endOffset": 168}, {"referenceID": 7, "context": "The task is expensive, timeconsuming and, in case of having domain knowledgeable experts, can result in biased annotations (Roggen et al. 2010; Nguyen-Dinh et al. 2013).", "startOffset": 123, "endOffset": 168}, {"referenceID": 7, "context": "2010; Nguyen-Dinh et al. 2013). In order to reduce cost and time taken to annotate activities, crowdsourcing platforms are increasingly gaining popularity. A crowdsourcing platform offer a large pool of world-wide workers that are able to perform a human intelligent task (HIT), such as annotation of a video, for a small financial incentive. For large datasets and long videos, as is the case for our dataset, this is a suitable option to attain annotations in a cost-effective and efficient manner. Furthermore, since many annotators are employed to annotate each single video, this helped ensure that a rich and varied perspective of the latent activities were reflected in the annotations. A single person usually tends to annotate videos sequentially one activity after the other in a flat temporal sequence. With multiple annotators, we increased the degree to which we obtained annotations at multiple level of temporal granualirty. Combining these annotations from different workers provides a richer annotation of the video. For our dataset, we make use of a popular crowdsourcing platform called Amazon Mechanical Turk(AMT) 2. There are however many challenges and design decisions to be taken when developing a system to elicit annotations using crowdsourcing such as deciding on the payment amount, making a clear interface for workers to perform the task, detection and removal of spam or non-diligent workers etc. Overcoming these challenges factors greatly affects the ability to obtain accurate annotations as shown by NguyenDinh et al. (2013). We will describe our design process next.", "startOffset": 6, "endOffset": 1560}], "year": 2017, "abstractText": "This paper introduces a novel activity dataset which exhibits real-life and diverse scenarios of complex, temporallyextended human activities and actions. The dataset presents a set of videos of actors performing everyday activities in a natural and unscripted manner. The dataset was recorded using a static Kinect 2 sensor which is commonly used on many robotic platforms. The dataset comprises of RGB-D images, point cloud data, automatically generated skeleton tracks in addition to crowdsourced annotations. Furthermore, we also describe the methodology used to acquire annotations through crowdsourcing. Finally some activity recognition benchmarks are presented using current state-of-the-art techniques. We believe that this dataset is particularly suitable as a testbed for activity recognition research but it can also be applicable for other common tasks in robotics/computer vision research such as object detection and human skeleton tracking.", "creator": "LaTeX with hyperref package"}}}