{"id": "1709.03544", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "KnowNER: Incremental Multilingual Knowledge in Named Entity Recognition", "abstract": "knowner is a multilingual named entity recognition ( ner ) system implementation that leverages different degrees of external knowledge. these a novel modular framework divides the knowledge into three four categories according to the depth of knowledge they consistently convey. each category consists of a set of features automatically generated uniquely from different information sources ( such as a knowledge - base, a list instance of names or document - specific semantic annotations ) and is used to train a conditional random field ( crf ). since those information sources are usually multilingual, knowner projects can be easily trained for constructing a wide range of languages. in this paper, we show that introducing the incorporation of deeper knowledge systematically boosts accuracy and compare knowner with state - of - the - art ner approaches across three languages ( i. e., english, german european and spanish ) performing amongst state - of - the art systems in all of them.", "histories": [["v1", "Mon, 11 Sep 2017 18:54:15 GMT  (929kb,D)", "http://arxiv.org/abs/1709.03544v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dominic seyler", "tatiana dembelova", "luciano del corro", "johannes hoffart", "gerhard weikum"], "accepted": false, "id": "1709.03544"}, "pdf": {"name": "1709.03544.pdf", "metadata": {"source": "META", "title": "KnowNER: Incremental Multilingual Knowledge in Named Entity Recognition", "authors": ["Dominic Seyler", "Tatiana Dembelova", "Luciano Del Corro", "Johannes Hoffart", "Gerhard Weikum"], "emails": ["dseyler2@illinois.edu", "tdembelo@mpi-inf.mpg.de", "corrogg@mpi-inf.mpg.de", "jhoffart@mpi-inf.mpg.de", "weikum@mpi-inf.mpg.de"], "sections": [{"heading": "1 INTRODUCTION", "text": "Named Entity Recognition (NER) is the task of detecting named entity mentions in text and assigning them to their corresponding coarse-grained type (e.g., person, location, organization, miscellaneous). For instance, given the sentence \u201cJimmy Page played in New York\u201d, the goal is to recognize \u201cJimmy Page\u201d and \u201cNew York\u201d as named entities and classify them as person and location. NER is a key component in a wide range of natural language understanding tasks such as named entity disambiguation (NED), information extraction, question answering, machine translation, knowledge graph construction, etc.\nHere we present KnowNER, a multilingual NER system which incorporates different degrees of external knowledge through language agnostic features, designed to exploit existing multilingual knowledge resources.\nIn contrast to previous approaches, KnowNER is implemented as a modular framework, drawing on different sources of external knowledge. We divide the information sources into four different categories according to the depth of knowledge they convey.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). \u00a9 2017 Copyright held by the owner/author(s).\nEach one carries more information than the previous. This additional knowledge boosts accuracy but also increases the processing overhead, establishing a clear accuracy-speed trade-off that can be exploited according to processing requirements and the availability of computational and knowledge resources.\nThis work has three main goals: (i) present a high performance knowledge intensive NER framework that can be used for a wide range of languages, (ii) understand to which extent external knowledge improves NER performance, and (iii) present a novel set of knowledge intensive features that can be used in a multilingual setting.\nKnowNER implements a linear chain CRF, which was proven to work well for the NER task [5]. We divide the features according to the knowledge categories defined below: Agnostic.These features correspond to the standard lexico-syntactic features extensively used in literature [5]. They are usually called local features since they are directly extracted from text and do not use any external knowledge. For instance, part-of-speech (POS) tags are good indicators of named entities (e.g., In \u201cJimmy Page plays guitar\u201d, \u201cJimmy\u201d and \u201cPage\u201d are proper nouns). Name-based. This set is extracted from a list with millions of named entity names. They unveil common patterns or attributes that indicate the presence of named entities. For example, the word\u201cJimmy\u201d is usually associated with named entities. KB-based. This group is generated from a knowledge base (KB) or an entity annotated corpus. The aim is to go beyond the surface forms exposing particular semantics of the named entities (e.g., their types). Following Ratinov and Roth. [19], we use gazetteers that associate named entity names with types. We generate them in an automatic way from YAGO [21], a multilingual KB. We also use it to extract richer information like the probability of a single token having a given type and appear in a specific position (e.g., the probability of \u201cJimmy\u201d being a person and appearing at the beginning of the name). Additionally, we exploit an annotated corpus to estimate the likelihood of a token referring to a named entity (e.g., The number of times \u201cPage\u201d is linked in Wikipedia articles). Entity-based. These features exploit information from a particular document. The idea is that if some entities in the document are identified in advance, it is easier to spot more difficult cases later. For instance, if we know that the European Union is mentioned in a text, we can assume that the token \u201cEU\u201d will most probably refer to it. Previous work [18] builds on this idea, using disambiguated entities ar X iv :1 70 9. 03 54 4v 1 [ cs\n.C L\n] 1\n1 Se\np 20\nfrom ground truth data to extract document specific features. We follow this approach but, in addition, we evaluate our system in a real world scenario using AIDA [8], a state-of-the-art entity-linking system.\nWhen KnowNER includes all knowledge categories, it performs among the best NER systems across all the evaluated languages (i.e., English, German and Spanish) on four standard datasets. In the experimental section, we also present an extensive study showing that the degree of knowledge correlates positively with task accuracy and negatively with processing time. We also show that external knowledge is particularly important for types like organizations, persons, and locations, reaching in the last two cases human-level accuracy (more than 95 F1 points) for the English language. Apart from the traditional NER metrics (class label plus text span) we additionally report the span recognition accuracy (named entity without the type tag), essential for certain tasks (e.g., NED).\nNow we summarize our central contributions: \u2022 A high performance multilingual NER system based on a modular framework for incorporating different types of external knowledge.\n\u2022 A comprehensive study to verify the impact of external knowledge into NER, including ablation and timing experiments. \u2022 Amultilingual set of knowledge intensive automatically generated features derived from large list of names, or a multilingual KB. \u2022 Real world scenario experiments to test the specific effects of NED into NER."}, {"heading": "2 THE NAMED ENTITY RECOGNITION TASK", "text": ""}, {"heading": "2.1 Task Definition", "text": "The goal of NER is to find named entity mentions in text and map them to pre-defined types (e.g., person, location, etc.). For instance, in the sentence \u201cJimmy Page plays guitar.\u201d the goal is to recognize that the text span \u201cJimmy Page\u201d refers to a named entity that can be categorized as person.\nThe task implies two challenges: (i) Find the text span of a named entity name and (ii) Annotate each named entity with a type. The first challenge requires identifying tokens that refer to named entities. A named entity may be composed by more than one token (\u201cUnited States\u201d), and a named entity may be embedded in another named entity (\u201cSupreme Court of the United States\u201d). The second challenge requires deeper semantic understanding (e.g. understand that \u201cJimmy Page\u201d is not only a named entity but specifically a person).\nAlthough NER commonly refers to both tasks, some applications may rely only on the first one (e.g. NED). In Sec. 4 we present results for the named entity mention span detection separately."}, {"heading": "2.2 A linear chain CRF model", "text": "Previous work [5, 9, 14, 16, 18, 19] proved the effectiveness of CRFs [11] for the NER task. We implemented KnowNER as a linear chain CRF similar to [5]. The underlying idea is to cast NER as a sequence model with a bidirectional flow. The CRF represents the probability of a hidden state sequence (i.e., token labels) given a set of observations. In a linear chain CRF, the probability of a token\nbeing a named entity depends on a set of observations including the label of its adjacent neighbors. For a more in-depth description of the model refer to Finkel et al., 2005."}, {"heading": "3 KNOWLEDGE AUGMENTED NER", "text": "Here, we describe the knowledge categories, which function as modules in our system. We define four: agnostic (A), name-based, KB-based and entity-based, each containing an increasing amount of external knowledge. A category consists of the set of features, sumarized in Tab. 1."}, {"heading": "3.1 Knowledge Agnostic", "text": "This category contains the so-called \u201clocal\u201d features. Their distinctive characteristic is that they can be extracted directly from text without any external knowledge. These features are mostly of a lexical, syntactic or linguistic nature and have been well-studied in literature. We implement most of the features described in Finkel et al. [5] and Zhang and Johnson. [24], namely:\n(1) The current word and words in a window of size 2 ; (2) Word shapes of the current word and words in a window of size 2; (3) POS tags in a window of size 2; (4) Prefixes (length three and four) and Suffixes (length one to four); (5) Presence of the current word in a window of size 4; (6) Beginning of sentence."}, {"heading": "3.2 Name-Based Knowledge", "text": "In this category, the knowledge is extracted from a list of named entity names. This list does not carry any additional information apart from the names themselves. The intuition is that names tend to follow patterns and even the set of possible names is limited. To the best of our knowledge, these features have not been previously used. We extracted a list of all names from YAGO [21] (30.85M for the languages we trained on) and created the following features:\nFrequent mention tokens. Reflects the frequency of a given token in a list of entity names. We tokenized the list to compute the frequencies. The feature assigns a weight to each token in the text corresponding to their normalized frequency. The intuition is that some words like \u201cJohn\u201d or \u201cOrganization\u201d may be indicative of a named entity and thus carry a high weight. For instance, the top-5 tokens we found in English were \u201ccounty\u201d, \u201cjohn\u201d, \u201cschool\u201d, \u201cstation\u201d and \u201cdistrict\u201d. All tokens without occurences are assigned 0 weight.\nFrequent POS Tag Sequences. This feature intends to identify POS sequences common to named entities. For example, person names tend to be described as a series of proper nouns, while organizations may have richer patterns. For instance, both \u201cOrganization of American States\u201d and \u201cUnion for Ethical Biotrade\u201d share the pattern NNP-IN-NNP-NNP, where NNP is a proper noun and IN a preposition. To generate these patterns, we construct a simple artificial sentence for each name in our list and run a POS-tagger. We then compute and rank the entity POS tag sequences and keep the top 100. The feature is implemented by finding the longest matching POS sequences in the input text and marking whether the current token belongs to a frequent sequence or not. We search the sequences from left to right and, in case of overlap, annotate only the leftmost sequence. This might need to be done differently for languages that read right to left."}, {"heading": "3.3 Knowledge-Base-Based Knowledge", "text": "This category groups features that are extracted from a KB or an entity annotated corpus. They encode knowledge about named entities themselves or their usages. Conceptually, we aim to incorporate the likelihood of a particular token being linked to an entity of a specific type. We implemented three features:\nType-infused Gazetteer Match. It finds the longest occurring token sequence in a type specific gazetteer. It adds a binary indicator to each token, depending on whether the token is part of a sequence. We use 30 dictionaries distributed by Ratinov and Roth, 2009 containing type-name information for English. For instance, \u201cNew York\u201d is a place and \u201cMcDonald\u2019s\u201d a corporation. These dictionaries have been successfully used in the past [14, 16, 18]. For the rest of the languages we generated the dictionaries automatically by mapping each dictionary to a set of YAGO types and extracting the corresponding names. For the dictionary containing corporations, for example, we incorporated all the names in the specific language corresponding to types company and enterprise.\nWikipedia Link Probability. This feature measures the likelihood of a token being linked to a named entity Wikipedia page. The intuition is that tokens linked to named entity pages tend to be indicative of named entities. For instance, the token \u201cObama\u201d is usually linked while the term \u201cbox\u201d is not. The list of pages referring to named entities is extracted from YAGO. Given a token in the text, it is assigned the probability of being linked according to Eq. 1, where linkd (t) equals 1, if token t in documentd is linked to another Wikipedia document. presentd equals 1 if t occurs in d .\nPW iki (t) = \u2211 d \u2208D linkd (t)\u2211\nd \u2208D presentd (t) (1)\nSince usually in Wikipedia only the first occurrence of a named entity is linked, we count a word on a page as linked if it links to a named entity page at least once.\nType Probability. Intended to discriminate between types, it encodes the likelihood of a token belonging to a given type. The idea is to capture the fact that, for instance, the token \u201cObama\u201d is more\nlikely a person than a location. Since YAGO contains types and names for each entity, we can calculate the conditional probability.\nGiven a set of entities E with mentionsMe and tokens Tem we calculate the probability of a class c \u2208 C given a token t as\nP(c |t) = \u2211E e \u2211Me me \u2211Tem tem c(e)\u2211E\ne \u2211Me me \u2211Tem tem \u2211C ci ci (e)\n(2)\nwhere c(e) = 1 if entity e belongs to class c and c(e) = 0 otherwise. For each token in the text, we create one feature per type with the respective probability as its value.\nToken Type Position. Attempts to reflect that tokens may appear in different positions according to the entity type. For instance, \u201cSupremeCourt of the United States\u201d, is an organization and \u201cUnited\u201d occurs at the end. In \u201cUnited States\u201d, a location, occurs at the beginning. This helps with named entities inside other named entities.\nThis idea is implemented using the BILOU (Begin, Inside, Last, Outside, Unit) encoding [19], which tags each token with respect to the position in which it occurs (e.g., \u201cO-The B-Supreme I-Court I-of I-the I-United L-States\u201d). The number of features depends on the number of types in the dataset (4 BILU positions times n classes + O position). For each token, each feature receives the probability of a class given the token and position. The class probabilities are calculated as in Equation 2, incorporating also the token position. This strategy gives us the possibility to combine the class type probabilities with the token positions.\nTo the best of our knowledge, the last three features (Token Type Position, Type Probability and Wikipedia Link Probability) have not been used in previous work."}, {"heading": "3.4 Entity-Based Knowledge", "text": "This category encodes document specific knowledge about the entities found in text. The idea is to exploit the inherent association between NER and NED. Previous work showed that the flow of information between the two tasks generates significant improvements in NER performance [14, 18].\nComparatively, this module requires more (computational and knowledge) resources than the previous ones. It requires a first run of NED to generate document specific features, based on the disambiguated named entities. The generated features are used in a second run of NER.\nFollowing Radford et al. [18], after the first run of NED, we create a set of document-specific gazetteers derived from the named entities found. The idea is that this information will help in the second round to find new named entities missed in the first one. Take the sentence \u201cThree-quarters of citizens of the European Union working in the United Kingdom would not meet current visa requirements for non-EU overseas workers if the uk left the bloc\u201d. We can imagine that in the first round of NED European Union and United Kingdom can be easily identified. However, \u201cEU\u201d or the wrongly capitalized \u201cuk\u201d might be missed. After the disambiguation, we know that both disambiguated entities are organizations and have the aliases EU and UK respectively. The idea is that if we introduce this information in a second NER run, they are easier to spot.\nFor each documentwe gather all entities that were disambiguated in the first NED run. Then we extract all surface forms of the identified entities from YAGO. The surface forms are tokenized and assigned the type of the corresponding entity plus its BILOU position. For example, the surface form \u201cBarack Obama\u201d will result in the two tokens \u201cBarack\u201d and \u201cObama\u201d, which will be assigned to \u201cB-Person\u201d and \u201cL-Person\u201d respectively. In KnowNER this feature is incorporated as 17 binary features (BILU tags multiplied by 4 coarse grained types + O tag), which fire when a token is encountered that is part of a list that contains the mappings from tokens to type\u2013BILOU pairs."}, {"heading": "4 EVALUATION", "text": "In this section, we analyze the effect of external knowledge (Sec. 4.2) and compare KnowNER with state-of-the-art approaches (Sec. 4.1) for three languages: English, German and Spanish."}, {"heading": "4.1 Experimental Setup", "text": "KnowNER. The CRF was trained using CRF-suite [15] with the Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm and L1 regularization (coeff. = 1), which performed best on the English CoNLL2003 dev. set. We provide two settings for the system: (i) KnowNER\u0434old , which as Radford et al. [18] uses the gold standard named entity annotations for the Entity-based features and was used to analyze the impact of knowledge into the NER task, and (ii) KnowNERaida which runs AIDA to produce the Entity-based features and was used across all languages to compare with other available NER systems.\nDatasets. We evaluated KnowNER on four well established datasets that provide annotated named entity mentions and types.\nCoNLL2003e. By Sang and Meulder [20], it is a collection of English Reuter\u2019s newswireswith named entitymentions annotatedwith types (i.e., persons, locations, organizations and miscellaneous). CoNLL2003e-dev does not include the developing set in training while CoNLL2003e-test does.\nMUC-7.A set of New York Times articles (in English) [1] designed for NED and NER. It annotates named entities and their types\n(i.e., organizations, persons, locations), dates, times, and quantities (monetary values, percentages). We only focused on the named entity types. MUC-7-dev does not include the developing set in training while MUC-7-test does.\nCoNLL2003g. A German dataset also by Sang and Meulder [20], similar to CoNLL2003e the named entities are classified according to four types (i.e., persons, locations, organizations andmiscellaneous). It consists of a collection of news articles from the Frankfurter Rundschau.\nCoNLL2002. By Tjong Kim Sang [22], it is a collection of news wire articles in Spanish made available by the Spanish EFE News Agency. The named entities are classified into persons, organizations, locations, times and quantities. We only focus on the first three.\nMetrics. We report F1-score for all systems and two evaluation methodologies for our system and the other methods, when available.\nMention-based. It considers a named entity prediction as correct, if and only if the mention boundaries and predicted types are exact matches with the gold standard.\nSpan-based. Measures the correctness of mention boundaries ignoring type labels. This measure is important for applications which do not necessarily require type annotations such as NED.\nKnowledge depth. To demonstrate the impact of increasing knowledge onNER performancewe tested four variations of KnowNER, equivalent to the categories introduced in Sec. 3. Each variation contains the features corresponding to a category name plus all those from the lighter categories.\nAgnostic: KnowNERA uses only local lexico-syntactic features without any external knowledge resources (Sec. 3.1).\nName-based: KnowNERName uses features based on a list of names (Sec. 3.2) plus those in KnowNERA.\nKB-based:KnowNERKB utilizes features derived from a knowledgebase (Sec. 3.3) plus KnowNERName features.\nEntity-based: KnowNEREntity requires the execution of NED to generate document based features (Sec. 3.4) in addition to all the features in KnowNERKB ."}, {"heading": "4.2 Incremental Knowledge", "text": "Here we analyze the impact of external knowledge on the system. Our detailed analysis is specific for the English language on the CoNLL2003e and MUC-7 datasets. Results show a clear improvement when deeper knowledge is used across datasets and entity types.\nFig. 1a shows the effect of the span detection in each category. Although it drops slightly for the name-based category, it quickly recovers as deeper knowledge is added. The effect is similar for both datasets.\nRegarding the mention-based metric, Fig. 1b shows the effect of different knowledge categories for the Mention-based metric. In all cases adding knowledge generates a boost in performance. The effect is particularly strong for MUC-7-test which registered an overall increment of almost 10 F1 points. In both cases, the biggest boost is registered when the KB-based features are added.\nHowever, the ablation study in Tab. 2 suggests that some KBbased features may be subsumed by the Entity-based ones which\ngenerates the most significant boost. This is somehow expected as the entity specific information is extracted from the same KB and strongly relies on the entity types. The Entity-based component is also the most expensive concerning timing performance. Fig. 2 shows the time required by each setting, establishing a trade-off between accuracy and runtime. The Stanford agnostic system was faster than our implementation as it took 158.55 ms per document on average.\nFig. 3a and Fig. 3b show the performance for each specific entity type for both CoNLL2003e-test and CoNLL2003e-dev. KnowNER achieves human-level performance for labelling persons (F1 96.03 and F1 95.86) and locations (F1 92.13 and F1 96.39). The positive effect of external knowledge is quite significant for organizations (F1 80.86 to F1 89.32 on test; F1 83.94 to F1 89.75 on dev) while it is relatively moderate for miscellaneous. In the case of MUC-7 (Fig. 3c and Fig. 3d), the effect is similar except for locations in MUC-7test which tend to slightly drop when the entity-based category is used. The positive impact on persons for MUC-7-test is especially significative as it generates a change in ranking performance with respect to the other types. It jumps from the second and third position on MUC-7 test and MUC-7 dev with agnostic features to the very first position in MUC-7-test and the second on MUC-7-dev.\nFinally, Tab. 3 and Tab. 4 display mention-based results on CoNLL2003e and MUC-7 for all knowledge categories and entity types. They also display the span-based performance for each\nknowledge category. The numbers suggest that adding knowledge improves task performance."}, {"heading": "4.3 Comparative Performance", "text": "English. KnowNER performs amongst state-of-the-art NER English systems on both datasets. Tab. 5 reports the results for mentionbased performance on CoNLL2003e-test compared to the bestknown systems. The results for KnowNER correspond to a setting using all the knowledge categories when using the gold standard for the entity-based step (as in Radford at al. [18]) or using the AIDA system for the entity-based knowledge category.\nTab. 6 displays detailed results for one of the latest versions of Finkel et al.[5] (Stanford NER 3.6.0), probably the most widely used NER system to date, which KnowNER outperforms.\nGerman. To the best of our knowledge, KnowNER is one of the best performing systems to date for the German language on CoNLL2003g. Tab. 7 presents the results for KnowNER compared with state-of-the-art systems. Tab. 8 presents detailed results for each named entity type. The biggest boost in Germany is generated by the entity-based features, which generate an increment of more than 7 points in recall with respect to the previous knowledge category (i.e., kb-based).\nSpanish. Tab. 9 presents the results for KnowNER compared with state-of-the-art systems for Spanish. Tab. 10 presents detailed results for each named entity type."}, {"heading": "5 RELATEDWORK", "text": "NER is a widely studied problem in the natural language understanding community. Well developed work has established a clear direction towards the use of CRFs [11] with systems achieving high\nrelative performance [5, 9, 14, 16, 18, 19]. A new line, focused on neural networks methods [2, 3, 7, 12, 23, 23]. Chiu and Nichols [2], for instance, the best NER system for English to date implemented a hybrid bidirectional LSTM-CNNwhose inputs are tokens, word and character embeddings, and a set of gazetteers with type encodings.\nAmong the CRF methods, early work has focused on purely agnostic systems [5, 10]. Klein et al. [10] presents a system addressing the importance of substring features, an idea that we also capture in our agnostic model via prefixes and suffixes. Finkel et al. [5] is one of the most popular agnostic systems. Following this work, our agnostic category implements most of the features described in the\npaper plus prefixes and suffixes used in Zhang and Johnson [24]. We do not make use of feature type statistics from the dataset which may explain a small drop in performance in our agnostic setting compared to Finkel et al. [5] in our English experiments. In contrast to agnostic approaches, our system strongly relies on background knowledge to improve performance.\nPrevious work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24]. Most of these works incorporate background knowledge in the form of entity-type gazetteers [6, 9, 16, 19, 24]. In fact, dictionaries were already provided for the early CoNLL2003 shared-task encouraging the use of external knowledge. Ratinov and Roth [19] used 30 gazetteers mostly extracted from Wikipedia, thereby generating big boosts in performance. These gazetteers have been successfully reused by other systems [14, 16, 18]. In particular, Luo et al. [14] used a total of 655 gazetteers including those from Ratinov and Roth [19]. We also incorporate gazetteers in our knowledge-based features. Finally, Kazama and Torisawa [9] was one of the first works to extract type information from Wikipedia. Their approach extracts category labels from the first sentence of the Wikipedia entity pages. In our method we also explicitly incorporate type information in the KB-based and the entity-based categories but in a cleaner way as they are derived from a high precision knowledge base like YAGO.\nCompared to previous approaches using external knowledge, our method is more modular in the way the knowledge is incorporated. Our framework allows us to classify and easily derive more features. We have both more light-weight and knowledge-intensive features from different sources: entity names, knowledge bases and NED. Our distinctive features include frequent mention tokens, frequent mention shapes, frequent POS mention patterns, Wikipedia token probability and the class type probability, among others (Sec. 3).\nThe association between NER and NED has been successfully exploited by recent work [4, 14, 18] as a means to boost NER performance. Radford et al. [18] uses a two-step approach. They showed that local features derived from an initial NED run improve the performance on a second NER step. Specifically, alternative entity names and types tend to be important. We follow a similar two-step approach but, in contrast, we also run NED using a real world setting. Luo et al. [14] present a joint model for named entity recognition and disambiguation, as a CRF with a topology for joint optimization. NER and NED tasks are inherently associated so performing these tasks jointly poses natural advantages. However, NER has multiple applications apart from NED, which tends to be a computationally expensive task. Our modular approach, on top of a simpler and more tractable model, avoids expensive joint optimisation and permits to easily decouple the NED module for settings with small computational requirements. It also benefits from the mutual dependency between NER and NED when heavy computation is not an issue.\nRegarding multilinguality, recent work has focused on methods to handle NER across a wide set of languages [7, 12, 23]. Yang et al. [23], one of the best systems across languages, implements a hierarchical recurrent neural network for joint POS tagging, chunking and NER, implemented on top of a CRF layer to do the labelling."}, {"heading": "6 CONCLUSION", "text": "We presented KnowNER, a multilingual system that explicitly encodes different degrees of external knowledge for NER. KnowNER\u2019s framework defines four knowledge categories, each containing deeper external knowledge. Our experimental study shows that KnowNER performs among state-of-the-art NER systems across languages. It also shows that increasing the degree of external knowledge encoded in the system significantly boosts NER performance."}], "references": [{"title": "MUC-7 named entity task definition", "author": ["Nancy Chinchor", "Patricia Robinson"], "venue": "In Proceedings of MUC-7", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Named Entity Recognition with Bidirectional LSTM-CNNs. TACL", "author": ["Jason Chiu", "Eric Nichols"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Boosting Named Entity Recognition with Neural Character Embeddings", "author": ["C\u00edcero Nogueira dos Santos", "Victor Guimar\u00e3es"], "venue": "Proceedings of the Fifth Named Entity Workshop", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking", "author": ["Greg Durrett", "Dan Klein"], "venue": "TACL", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher D. Manning"], "venue": "In Proceedings of ACL", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Named Entity Recognition through Classifier Combination", "author": ["Radu Florian", "Abraham Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of CoNLL", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Multilingual Language Processing From Bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "In Proceedings of NAACL", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Robust Disambiguation of Named Entities in Text", "author": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": "Proceedings of EMNLP", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Exploiting Wikipedia as External Knowledge for Named Entity Recognition", "author": ["Kazama Jun\u2019ichi", "Kentaro Torisawa"], "venue": "In Proceedings of EMNLP-CoNLL", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Named Entity Recognition with Character-Level Models", "author": ["Dan Klein", "Joseph Smarr", "HuyNguyen", "Christopher D.Manning"], "venue": "In Proceedings of CoNLL", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira"], "venue": "In Proceedings of ICML", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In Proceedings of NAACL", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Phrase Clustering for Discriminative Learning", "author": ["Dekang Lin", "Xiaoyun Wu"], "venue": "In Proceedings of ACL", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Joint Entity Recognition and Disambiguation", "author": ["Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie"], "venue": "In Proceedings of EMNLP", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "CRFsuite: a fast implementation of Conditional Random Fields (CRFs)", "author": ["Naoaki Okazaki"], "venue": "http://www.chokkan.org/software/crfsuite/", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution", "author": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum"], "venue": "In Proceedings of CoNLL", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Combining labeled and unlabeled data with word-class distribution learning", "author": ["Yanjun Qi", "Ronan Collobert", "Pavel P. Kuksa", "Koray Kavukcuoglu", "Jason Weston"], "venue": "In Proceedings of CIKM", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Named entity recognition with document-specific KB tag gazetteers", "author": ["Will Radford", "Xavier Carreras", "James Henderson"], "venue": "In Proceedings of EMNLP", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Design Challenges and Misconceptions in Named Entity Recognition", "author": ["Lev-Arie Ratinov", "Dan Roth"], "venue": "In Proceedings of CoNLL", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Introduction to the CoNLL- 2003 Shared Task: Language-Independent Named Entity Recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of CoNLL", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Yago: a core of semantic knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of WWW", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Introduction to the CoNLL-2002 Shared Task: Language-independent Named Entity Recognition", "author": ["Erik F. Tjong Kim Sang"], "venue": "In Proceedings of CoNLL", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "author": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "A Robust Risk Minimization based Named Entity Recognition System", "author": ["Tong Zhang", "David Johnson"], "venue": "In Proceedings of CoNLL", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}], "referenceMentions": [{"referenceID": 4, "context": "KnowNER implements a linear chain CRF, which was proven to work well for the NER task [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "These features correspond to the standard lexico-syntactic features extensively used in literature [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 18, "context": "[19], we use gazetteers that associate named entity names with types.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We generate them in an automatic way from YAGO [21], a multilingual KB.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "Previous work [18] builds on this idea, using disambiguated entities ar X iv :1 70 9.", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "We follow this approach but, in addition, we evaluate our system in a real world scenario using AIDA [8], a state-of-the-art entity-linking system.", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Previous work [5, 9, 14, 16, 18, 19] proved the effectiveness of CRFs [11] for the NER task.", "startOffset": 14, "endOffset": 36}, {"referenceID": 8, "context": "Previous work [5, 9, 14, 16, 18, 19] proved the effectiveness of CRFs [11] for the NER task.", "startOffset": 14, "endOffset": 36}, {"referenceID": 13, "context": "Previous work [5, 9, 14, 16, 18, 19] proved the effectiveness of CRFs [11] for the NER task.", "startOffset": 14, "endOffset": 36}, {"referenceID": 15, "context": "Previous work [5, 9, 14, 16, 18, 19] proved the effectiveness of CRFs [11] for the NER task.", "startOffset": 14, "endOffset": 36}, {"referenceID": 17, "context": "Previous work [5, 9, 14, 16, 18, 19] proved the effectiveness of CRFs [11] for the NER task.", "startOffset": 14, "endOffset": 36}, {"referenceID": 18, "context": "Previous work [5, 9, 14, 16, 18, 19] proved the effectiveness of CRFs [11] for the NER task.", "startOffset": 14, "endOffset": 36}, {"referenceID": 10, "context": "Previous work [5, 9, 14, 16, 18, 19] proved the effectiveness of CRFs [11] for the NER task.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "We implemented KnowNER as a linear chain CRF similar to [5].", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "[5] and Zhang and Johnson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[24], namely: (1) The current word and words in a window of size 2 ; (2) Word shapes of the current word and words in a window of size 2; (3) POS tags in a window of size 2; (4) Prefixes (length three and four) and Suffixes (length one to four); (5) Presence of the current word in a window of size 4; (6) Beginning of sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We extracted a list of all names from YAGO [21] (30.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "These dictionaries have been successfully used in the past [14, 16, 18].", "startOffset": 59, "endOffset": 71}, {"referenceID": 15, "context": "These dictionaries have been successfully used in the past [14, 16, 18].", "startOffset": 59, "endOffset": 71}, {"referenceID": 17, "context": "These dictionaries have been successfully used in the past [14, 16, 18].", "startOffset": 59, "endOffset": 71}, {"referenceID": 18, "context": "This idea is implemented using the BILOU (Begin, Inside, Last, Outside, Unit) encoding [19], which tags each token with respect to the position in which it occurs (e.", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "Previous work showed that the flow of information between the two tasks generates significant improvements in NER performance [14, 18].", "startOffset": 126, "endOffset": 134}, {"referenceID": 17, "context": "Previous work showed that the flow of information between the two tasks generates significant improvements in NER performance [14, 18].", "startOffset": 126, "endOffset": 134}, {"referenceID": 17, "context": "[18], after the first run of NED, we create a set of document-specific gazetteers derived from the named entities found.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The CRF was trained using CRF-suite [15] with the Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm and L1 regularization (coeff.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "[18] uses the gold standard named entity annotations for the Entity-based features and was used to analyze the impact of knowledge into the NER task, and (ii) KnowNERaida which runs AIDA to produce the Entity-based features and was used across all languages to compare with other available NER systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "By Sang and Meulder [20], it is a collection of English Reuter\u2019s newswireswith named entitymentions annotatedwith types (i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "A set of New York Times articles (in English) [1] designed for NED and NER.", "startOffset": 46, "endOffset": 49}, {"referenceID": 19, "context": "A German dataset also by Sang and Meulder [20], similar to CoNLL2003e the named entities are classified according to four types (i.", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "By Tjong Kim Sang [22], it is a collection of news wire articles in Spanish made available by the Spanish EFE News Agency.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "[18]) or using the AIDA system for the entity-based knowledge category.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] (Stanford NER 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Well developed work has established a clear direction towards the use of CRFs [11] with systems achieving high", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Chiu and Nichols [2] 91.", "startOffset": 17, "endOffset": 20}, {"referenceID": 13, "context": "[14] 91.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] 91.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] 90.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] 90.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Lin and Wu [13] 90.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "Ratinov and Roth [19] 90.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "[18] 89.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] 86.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] 78.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] 76.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] 75.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] 85.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] 85.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] 82.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "dos Santos and Guimar\u00e3es [3] 82.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "relative performance [5, 9, 14, 16, 18, 19].", "startOffset": 21, "endOffset": 43}, {"referenceID": 8, "context": "relative performance [5, 9, 14, 16, 18, 19].", "startOffset": 21, "endOffset": 43}, {"referenceID": 13, "context": "relative performance [5, 9, 14, 16, 18, 19].", "startOffset": 21, "endOffset": 43}, {"referenceID": 15, "context": "relative performance [5, 9, 14, 16, 18, 19].", "startOffset": 21, "endOffset": 43}, {"referenceID": 17, "context": "relative performance [5, 9, 14, 16, 18, 19].", "startOffset": 21, "endOffset": 43}, {"referenceID": 18, "context": "relative performance [5, 9, 14, 16, 18, 19].", "startOffset": 21, "endOffset": 43}, {"referenceID": 1, "context": "A new line, focused on neural networks methods [2, 3, 7, 12, 23, 23].", "startOffset": 47, "endOffset": 68}, {"referenceID": 2, "context": "A new line, focused on neural networks methods [2, 3, 7, 12, 23, 23].", "startOffset": 47, "endOffset": 68}, {"referenceID": 6, "context": "A new line, focused on neural networks methods [2, 3, 7, 12, 23, 23].", "startOffset": 47, "endOffset": 68}, {"referenceID": 11, "context": "A new line, focused on neural networks methods [2, 3, 7, 12, 23, 23].", "startOffset": 47, "endOffset": 68}, {"referenceID": 22, "context": "A new line, focused on neural networks methods [2, 3, 7, 12, 23, 23].", "startOffset": 47, "endOffset": 68}, {"referenceID": 22, "context": "A new line, focused on neural networks methods [2, 3, 7, 12, 23, 23].", "startOffset": 47, "endOffset": 68}, {"referenceID": 1, "context": "Chiu and Nichols [2], for instance, the best NER system for English to date implemented a hybrid bidirectional LSTM-CNNwhose inputs are tokens, word and character embeddings, and a set of gazetteers with type encodings.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Among the CRF methods, early work has focused on purely agnostic systems [5, 10].", "startOffset": 73, "endOffset": 80}, {"referenceID": 9, "context": "Among the CRF methods, early work has focused on purely agnostic systems [5, 10].", "startOffset": 73, "endOffset": 80}, {"referenceID": 9, "context": "[10] presents a system addressing the importance of substring features, an idea that we also capture in our agnostic model via prefixes and suffixes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] is one of the most popular agnostic systems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "paper plus prefixes and suffixes used in Zhang and Johnson [24].", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "[5] in our English experiments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Previous work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24].", "startOffset": 69, "endOffset": 99}, {"referenceID": 8, "context": "Previous work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24].", "startOffset": 69, "endOffset": 99}, {"referenceID": 12, "context": "Previous work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24].", "startOffset": 69, "endOffset": 99}, {"referenceID": 13, "context": "Previous work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24].", "startOffset": 69, "endOffset": 99}, {"referenceID": 15, "context": "Previous work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24].", "startOffset": 69, "endOffset": 99}, {"referenceID": 17, "context": "Previous work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24].", "startOffset": 69, "endOffset": 99}, {"referenceID": 18, "context": "Previous work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24].", "startOffset": 69, "endOffset": 99}, {"referenceID": 23, "context": "Previous work has already regarded NER as a knowledge intensive task [6, 9, 13, 14, 16, 18, 19, 24].", "startOffset": 69, "endOffset": 99}, {"referenceID": 5, "context": "Most of these works incorporate background knowledge in the form of entity-type gazetteers [6, 9, 16, 19, 24].", "startOffset": 91, "endOffset": 109}, {"referenceID": 8, "context": "Most of these works incorporate background knowledge in the form of entity-type gazetteers [6, 9, 16, 19, 24].", "startOffset": 91, "endOffset": 109}, {"referenceID": 15, "context": "Most of these works incorporate background knowledge in the form of entity-type gazetteers [6, 9, 16, 19, 24].", "startOffset": 91, "endOffset": 109}, {"referenceID": 18, "context": "Most of these works incorporate background knowledge in the form of entity-type gazetteers [6, 9, 16, 19, 24].", "startOffset": 91, "endOffset": 109}, {"referenceID": 23, "context": "Most of these works incorporate background knowledge in the form of entity-type gazetteers [6, 9, 16, 19, 24].", "startOffset": 91, "endOffset": 109}, {"referenceID": 18, "context": "Ratinov and Roth [19] used 30 gazetteers mostly extracted from Wikipedia, thereby generating big boosts in performance.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "These gazetteers have been successfully reused by other systems [14, 16, 18].", "startOffset": 64, "endOffset": 76}, {"referenceID": 15, "context": "These gazetteers have been successfully reused by other systems [14, 16, 18].", "startOffset": 64, "endOffset": 76}, {"referenceID": 17, "context": "These gazetteers have been successfully reused by other systems [14, 16, 18].", "startOffset": 64, "endOffset": 76}, {"referenceID": 13, "context": "[14] used a total of 655 gazetteers including those from Ratinov and Roth [19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[14] used a total of 655 gazetteers including those from Ratinov and Roth [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "Finally, Kazama and Torisawa [9] was one of the first works to extract type information from Wikipedia.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "The association between NER and NED has been successfully exploited by recent work [4, 14, 18] as a means to boost NER performance.", "startOffset": 83, "endOffset": 94}, {"referenceID": 13, "context": "The association between NER and NED has been successfully exploited by recent work [4, 14, 18] as a means to boost NER performance.", "startOffset": 83, "endOffset": 94}, {"referenceID": 17, "context": "The association between NER and NED has been successfully exploited by recent work [4, 14, 18] as a means to boost NER performance.", "startOffset": 83, "endOffset": 94}, {"referenceID": 17, "context": "[18] uses a two-step approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] present a joint model for named entity recognition and disambiguation, as a CRF with a topology for joint optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Regarding multilinguality, recent work has focused on methods to handle NER across a wide set of languages [7, 12, 23].", "startOffset": 107, "endOffset": 118}, {"referenceID": 11, "context": "Regarding multilinguality, recent work has focused on methods to handle NER across a wide set of languages [7, 12, 23].", "startOffset": 107, "endOffset": 118}, {"referenceID": 22, "context": "Regarding multilinguality, recent work has focused on methods to handle NER across a wide set of languages [7, 12, 23].", "startOffset": 107, "endOffset": 118}, {"referenceID": 22, "context": "[23], one of the best systems across languages, implements a hierarchical recurrent neural network for joint POS tagging, chunking and NER, implemented on top of a CRF layer to do the labelling.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "KnowNER is a multilingual Named Entity Recognition (NER) system that leverages different degrees of external knowledge. A novel modular framework divides the knowledge into four categories according to the depth of knowledge they convey. Each category consists of a set of features automatically generated from different information sources (such as a knowledge-base, a list of names or document-specific semantic annotations) and is used to train a conditional random field (CRF). Since those information sources are usually multilingual, KnowNER can be easily trained for a wide range of languages. In this paper, we show that the incorporation of deeper knowledge systematically boosts accuracy and compare KnowNER with state-of-the-art NER approaches across three languages (i.e., English, German and Spanish) performing amongst state-of-the art systems in all of them.", "creator": "LaTeX with hyperref package"}}}