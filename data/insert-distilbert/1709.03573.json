{"id": "1709.03573", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "Anomaly Detection in Hierarchical Data Streams under Unknown Models", "abstract": "we each consider the problem cycle of detecting a few targets among trees a large number of hierarchical data streams. the data streams are modeled as random processes with unknown and potentially heavy - tailed distributions. the objective is an active inference strategy that determines, sequentially, which data stream to collect samples from in order to minimize the sample complexity under a reliability constraint. we propose an appropriate active inference strategy variant that induces a biased random walk on the tree - structured hierarchy based based loosely on intrinsic confidence bounds of sample statistics. we then establish its minimum order toward optimality in terms of both scales the size of the search space ( i. e., the number of data generated streams ) and the reliability requirement. the results find applications in hierarchical noisy heavy hitter detection, noisy group testing, and adaptive sampling for active learning, automatic classification, and explicit stochastic root finding.", "histories": [["v1", "Mon, 11 Sep 2017 20:16:29 GMT  (155kb)", "http://arxiv.org/abs/1709.03573v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sattar vakili", "qing zhao", "chang liu", "chen-nee chuah"], "accepted": false, "id": "1709.03573"}, "pdf": {"name": "1709.03573.pdf", "metadata": {"source": "CRF", "title": "Anomaly Detection in Hierarchical Data Streams under Unknown Models", "authors": ["Sattar Vakili", "Qing Zhao", "Chang Liu"], "emails": ["sv388@cornell.edu", "qz16@cornell.edu", "chuah@ucdavis.edu", "cchliu@ucdavis.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n03 57\n3v 1\n[ cs\n.L G\n] 1\n1 Se\nWe consider the problem of detecting a few targets among a large number of hierarchical data streams. The data streams are modeled as random processes with unknown and potentially heavy-tailed distributions. The objective is an active inference strategy that determines, sequentially, which data stream to collect samples from in order to minimize the sample complexity under a reliability constraint. We propose an active inference strategy that induces a biased random walk on the tree-structured hierarchy based on confidence bounds of sample statistics. We then establish its order optimality in terms of both the size of the search space (i.e., the number of data streams) and the reliability requirement. The results find applications in hierarchical heavy hitter detection, noisy group testing, and adaptive sampling for active learning, classification, and stochastic root finding.\nI. INTRODUCTION\nWe consider the problem of detecting a few targets with abnormally high mean values among\na large number of data streams. Each data stream is modeled as a stochastic process with an\nunknown and potentially heavy-tailed distributions. The stochastic nature of the data streams\nmay be due to the inherent randomness of the underlying phenomenon or the noisy response of\nthe measuring process. The number of targets is unknown. The objective is to identify all targets\n(if any) or declare there is no target to meet a required detection accuracy with a minimum\nnumber of samples.\nInherent in a number of applications (see Sec. I-A) is a tree-structured hierarchy among the\nlarge number of data streams. With each node representing a data stream, the tree structure\nencodes the following relationship: the abnormal mean of a target leads to an abnormal mean\n2 in every ancestor of the target (i.e., every node on the shortest path from this target to the root\nof the tree). We illustrate in Fig. 1 a special case of a binary-tree hierarchy.\nTargets are of two types: leaf-level targets and hierarchical targets. A leaf-level target is a leaf (level 0) of the tree whose mean is above a given threshold. Hierarchical targets are defined\nrecursively in an ascending order of the level of the tree. Specifically, an upper-level node with\nan anomalous mean is a (hierarchical) target if its mean remains above a given threshold after\nexcluding all its target descendants (if any). Otherwise, this upper-level node is only a reflecting\npoint for merely being an ancestor of a target (see Fig. 1).\nThe objective of the problem is to detect all targets quickly and reliably by fully exploiting\nthe hierarchical structure of the data streams. Specifically, we seek an active inference strategy\nthat determines, sequentially, which node on the tree to probe and when to terminate the search\nin order to minimize the sample complexity for a given level of detection reliability. We are\nparticularly interested in strategies that achieve a sublinear scaling of the sample complexity\nwith respect to the number of data streams. In other words, accurate detection can be achieved\nby examining only a diminishing fraction of the search space as the search space grows."}, {"heading": "A. Applications", "text": "The above general problem of detecting abnormal mean values in hierarchical data streams\narises in a number of active inference and learning applications in networking and data analytics.\n3 We give below several representative examples to make the formulation of the problem concrete.\nHeavy hitter and hierarchical heavy hitter detection: In Internet and other communication\nand financial networks, it is a common observation that a small number of flows, referred to as\nheavy hitters (HH), account for the most of the total traffic [1]. Quickly identifying the heavy\nhitters is thus crucial to network stability and security. With limited sampling resources at the\nrouter, however, maintaining a packet count of each individual flow is highly inefficient, if not\ninfeasible. The key to an efficient solution is to consider prefix aggregation based on the source\nor destination IP addresses. This naturally leads to a binary tree structure with all targets (HHs)\nat the leaf level.\nA more complex version of the problem is hierarchical heavy hitter (HHH) detection, in\nwhich the search for flows with abnormal volume extends to aggregated flows. In other words,\nthere exist hierarchical targets. HHH detection is of particular interest in detecting distributed\ndenial-of-service attacks [2].\nNoisy group testing: In group testing, the objective is to identify a few defective items in\na large population by performing tests on subsets of items. Each group test gives a binary\noutcome, indicating whether the tested group contains any defective items. The problem was\nfirst motivated by the blood-test screening of draftees during World War II, for which Robert\nDorfman originated the idea of testing blood samples pooled from a group of people [3]. Since\nthen, the problem has found a wide range of applications, including idle channel detection [4],\nnetwork tomography [5], detecting malicious users and attackers [6], [7], and DNA sequencing\nand screening [8], [9].\nMost work on group testing assumes error-free test outcomes (see Sec. I-C for a more\ndetailed discussion on existing work). The problem studied in this paper includes, as a special\ncase, adaptive group testing under general and unknown noise models for the test outcomes.\nSpecifically, the outcome of a group test is no longer a deterministic binary value, but rather a\nBernoulli random variable with an unknown parameter that represents the false alarm probability\n(when the tested group contains no defective items) or the detection power (i.e., the probability of\ncorrect detection when the test group contains defective items). A data stream thus corresponds\nto the noisy Bernoulli test outcomes of a given subset of the population. The hierarchy of the\ndata streams follows from the \u201csubset\u201d relationship among the corresponding test groups. Under the practical assumption that false alarm and miss detection probabilities are smaller than 1/2, targets are those leaf nodes whose mean value exceeds 1/2. A more detailed mapping of the\n4 noisy group testing problem to the active inference problem studied in this paper is given in\nSec. V.\nAdaptive sampling with noisy response: The problem also applies to adaptive sampling with noisy response for estimating a step function in [0, 1]. Such problems arise in active learning of\nbinary threshold classifiers for document classification [10] and stochastic root finding [11].\nPartitioning the [0, 1] interval into small intervals and sampling the boundary points of each\ninterval, we can map the adaptive sampling problem to the target search problem where the target\nis the small interval containing the location of the step. Examining larger intervals (consisting\nof several smaller intervals) induces a hierarchical structure of the noisy responses. See Sec. V\nfor a detailed discussion."}, {"heading": "B. Main Results", "text": "We develop an active inference strategy for detecting an unknown number of targets among a large number N of data streams with unknown distributions. The performance measure is the number of samples (i.e., detection delay) required for achieving a confidence level above 1\u2212 \u01eb (i.e., the probability that the declared target set does not equal to the true set is bounded by \u01eb). By fully exploiting the tree-structured hierarchy, the proposed active inference strategy has a sample complexity that is order optimal in both the size N of the search space and the reliability constraint \u01eb.\nReferred to as Confidence Bounds based Random Walk (CBRW), the proposed strategy\nconsists of a global random walk on the tree interwoven with a local confidence-bound based test.\nSpecifically, it induces a biased random walk that initiates at the root of the tree and eventually\narrives and terminates at a target with the required reliability. Each move in the random walk is\nguided by the output of a local confidence-bound based sequential test carried on each child of\nthe node currently being visited by the random walk. This local sequential test module ensures\nthat the global random walk is more likely to move toward the target than move away from it\nand that the random walk terminates at a true target with a sufficiently high probability.\nThe sample complexity of CBRW is analyzed using properties of biased random walk on\na tree and large deviation results on the concentration of the sample mean statistic. We show that the sample complexity of CBRW is in the order of O(logN + log 1 \u01eb ) provided that the gap between the mean value of each data stream and the given threshold is bounded away from 0. It is thus order optimal in both N and \u01eb as determined by information-theoretic lower bounds. Of\n5 particular significance is that the effect on the sample complexity from an enlarged search space (increasing N) and an enhanced reliability (decreasing \u01eb) is additive rather than multiplicative.\nThis results from the random walk structure which effectively separates two objectives of moving to the targets with O(logN) samples and declaring the targets at the desired confidence level with O(log 1 \u01eb ) samples. The proposed strategy applies to unknown heavy-tailed distribution models and preserves its order optimality in both N and \u01eb. Comprising of calculating confidence bounds of\nthe mean and performing simple comparisons, the proposed strategy is computationally efficient."}, {"heading": "C. Related Work", "text": "The problem studied here is related to several active learning and sequential inference prob-\nlems. We discuss here representative studies most pertinent to this paper and emphasize the\ndifferences in our approach from these existing studies.\nNoisy group testing: Variations of the classic group testing have been extensively studied in\nthe literature focusing mainly on the noisless case. There are several recent studies that consider\none-sided error (false positive or false negative) in the test outcomes [12]\u2013[14] or symmetric\nerror (with equal false positive and false negative probabilities) [15]\u2013[17] with known error\nprobabilities. To our best knowledge, the result in this paper is the first applicable to noisy\ngroup testing under general and unknown noise models.\nAdaptive sampling with noisy response: The main body of work on adaptive sampling is\nbased on a Bayesian approach with binary noise of a known model. A popular Bayesian strategyis\nthe Probabilistic Bisection Algorithm (PBA), which updates the posterior distribution of the step\nlocation after each sample (based on the known model of the noisy response) and chooses\nthe next sampling point to be the median point of the posterior distribution. Although several\nvariations of the method have been extensively studied in the literature [18]\u2013[20] following the\npioneering work of [21], there is little known about the theoretical guarantees, especially when\nit comes to unknown noise models. In this paper we present a non-Bayesian approach to the\nadaptive sampling problem under general unknown noise models.\nHH and HHH detection: Prior solutions for online detection of HHHes typically involve\nadjusting which prefixes to monitor either at the arrival of each packet [22]\u2013[24], or at periodic\nintervals [25], [26]. A particularly relevant work is the adaptive monitoring algorithm proposed\nby Jose et al. [25], where a fixed number of measurement rules are adjusted at periodic intervals\nbased on the aggregate packet counts matching to each of these rules. At each time interval,\n6 the aggregate count is compared to a heuristically chosen threshold (e.g., a fraction of link\ncapacity), to determine whether it is an HHH, and whether the rules need to be kept in the\nnext interval, or expanded to monitor the children of the prefix, or collapsed and combined with\nupstream nodes. While the proposed CBRW has a similar flavor of moving among parent and\nchildren, which is very much inherent to the HHH detection problem, the decision criteria used\nto adjust the prefix is different. Instead of comparing with a fixed threshold, our decision is\nbased on statistical metric determined by the desired detection error. Different from the heuristic\nstudies in the literature, the proposed strategy offers performance grantee and order optimality.\nWe provide a rigorous framework that succinctly captures the tradeoff between detection time\nand overall detection performance.\nPure-exploration bandit problems: The problem studied here is also related to the so-called\npure-exploration bandit problems [27] where the objective is to search for a subset of bandit arms\nwith certain properties. In particular, in the Thresholding Bandit problem introduced in [28], the\nobjective is to determine the arms with mean above a given threshold. The key difference is that\nthe thresholding bandit problem does not assume any structure in the arms and has a linearly\ngrowing sample complexity in the number of arms. The focus of this paper is on exploiting the\nhierarchical structure inherent to many applications to obtain sublinear sample complexity with\nthe size of the search space.\nActive hypothesis testing for anomaly detection: The active inference problem considered\nhere falls into the general class of sequential design of experiments pioneered by Chernoff in\n1959 [29] with variations and extensions studied in [30]\u2013[33]. These studies assume known\nor parametric models and focus on randomized test strategies. This paper, however, adopts a\nnonparametric setting and proposes a deterministic strategy. A number of studies on anomaly\ndetection within the sequential and active hypothesis testing framework exist in the literature\n(see an excellent survey in [34] and a few recent results in [35]\u2013[38]). These studies in general\nassume known models and do not address hierarchical structure of the search space. A particularly\nrelevant work is [40], in which a test strategy based on a random walk on a binary tree was\ndeveloped. While the CBRW policy proposed here shares a similar structure as the test strategy\ndeveloped in [40], the latter work assumes that the stochastic models of all data streams are\nknown and the testing strategy relies on using the likelihood ratios calculated based on the\nknown distributions. For the unknown model scenario considered in this paper, the confidence-\nbound based statistics used for guiding the biased random walk are fundamentally different from\n7 the likelihood ratio. As a result, the performance analysis also differs."}, {"heading": "II. PROBLEM FORMULATION AND PRELIMINARIES", "text": ""}, {"heading": "A. Problem Formulation", "text": "Consider a set of N data streams conforming to a binary-tree structure with K leaf nodes as illustrated in Fig. 1 (extensions to general tree structures are discussed in Sec. V). Let (l, k) (l = 0, 1, . . . , L, k = 1, . . . , 2L\u2212l) denote the kth node at level l of the tree. Let {Xk,l(t)}\u221et=1 denote the corresponding random process which is independent and identically distributed with an unknown distribution fk,l and an unknown mean \u00b5k,l.\nAssociated with each level l of the tree is a given threshold \u03b7l that defines the targets of interest. Specifically, a leaf-level target is a node at level l = 0 whose mean \u00b5k,0 exceeds \u03b70. Hierarchical targets are defined recursively in terms of l. Specifically, a hierarchical target at level l > 0 is a node whose mean value remains above the threshold \u03b7l after excluding all its target decedents. The tree-structured hierarchy encodes the following relationship among nodes in terms of their mean values: for an arbitrary node (k, l), if \u00b5k,l > \u03b7l, then \u00b5k\u2032,l\u2032 > \u03b7l\u2032 for all (k\u2032, l\u2032) on the shortest path from (k, l) to the root node of the tree.\nAn active inference strategy \u03c0 = ({at}t\u22651, T\u03c0,S\u03c0) consists of a sampling strategy {at}t\u22651, a stopping rule \u03c4\u03c0, and a terminal decision rule S\u03c0 . The sampling strategy {at}t\u22651 is a sequence of functions mapping from past actions and observations to a node of the tree to be sampled at the current time t. The stopping rule T\u03c0 determines when to terminate the search, and the decision rule S\u03c0 declares the detected set of targets at the time of stopping. Let EF and PF denote, respectively, the expectation and the probability measure under distribution model F = {f(k,l)}l=0,1,...,L\nk=1,...,2L\u2212l . The objective is as follows:\nminimize\u03c0 EFT\u03c0, s.t. PF [S\u03c0 6= S] \u2264 \u01eb,\nwhere S is the true set of targets."}, {"heading": "B. Sub-Gaussian, Heavy-Tailed, and Concentration Inequality", "text": "We consider a general distribution fk,l for each process. Due to different concentration behaviors, sub-Gaussian and heavy-tailed distributions are treated separately. Recall that a real-valued\n8 random variable X is called sub-Gaussian [41] if, for all \u03bb \u2208 (\u2212\u221e,\u221e),\nE[e\u03bb(X\u2212E[X])] \u2264 e\u03be\u03bb2/2 (1)\nfor some constant \u03be > 0. We assume (an upper bound on) \u03be is known. For sub-Gaussian random\nvariables, Chernoff-Hoeffding concentration inequalities hold. Specifically [42]:\n   P [ X(s) + \u221a 2\u03be log 1 p s < \u00b5 ] \u2264 p P [ X(s)\u2212 \u221a 2\u03be log 1 p\ns > \u00b5\n] \u2264 p,\n(2)\nwhere X(s) = 1 s \u2211s t=1X(t) is the sample mean of s independent samples of X .\nFor heavy-tailed distributions, the moment generating functions are no longer bounded, and\nthe Chernoff-Hoeffding type of concentration inequalities do not hold. However, the following\ncan be said about a truncated sample mean statistic in place of the sample mean in (2). Assume that a b-th (1 < b < 2) moment of X is bounded:\nE[Xb] \u2264 u, (3)\nfor some u > 0. Define the following truncated sample mean with a parameter p \u2208 (0, 1 2 ]\nX\u0302(s, p) = 1\ns\ns\u2211\nt=1\nX(t)1 { |X(t)| \u2264 ( ut\nlog 1 p\n)1/b } . (4)\nWe then have (Lemma 1 in [43]),    Pr [ X\u0302(s, p)\u2212 4u1/b( log 1 p s ) b\u22121 b > \u00b5 ] \u2264 p Pr [ X\u0302(s, p) + 4u1/b( log 1 p\ns ) b\u22121 b < \u00b5\n] \u2264 p.\n(5)"}, {"heading": "III. AN ACTIVE INFERENCE STRATEGY: CBRW", "text": "In this section, we present the Confidence Bounds based Random Walk (CBRW) policy. We\nfocus on the case of a single target and sub-Gaussian distributions. Extensions to multiple target\ndetection and heavy-tailed distributions are discussed in Sec. V.\n9"}, {"heading": "A. Detecting Leaf-Level Targets", "text": "We first consider applications where it is known that the target is at the leaf level. This\nincludes, for example, HH detection, noisy group testing, and adaptive sampling as discussed in\nSec. I-A.\nThe basic structure of CBRW consists of a global random-walk module interwoven with a\nlocal CB-based sequential test module at each step of the random walk. Specifically, the CBRW\npolicy performs a biased random walk on the tree that eventually arrives and terminates at the\ntarget with the required reliability. Each move in the random walk (i.e., which neighboring node\nto visit next) is guided by the output of the local CB-based sequential test module. This module\nensures that the random walk is more likely to move toward the target than to move away from\nthe target and that the random walk terminates at the true target with high probability.\nConsider first the local CB-based sequential test module. This local sequential test is carried out on a specific node (random process) {X(t)}\u221et=1, where we have omitted the node index (k, l) for simplicity. The goal is to determine whether the mean value of {X(t)}\u221et=1 is below a given threshold \u03b7 at a confidence level of 1\u2212 \u03b2 or above the threshold at a confidence level of 1\u2212\u03b1. If the former is true, the test module outputs 0, indicating this node is unlikely to be an ancestor of a target or the target itself. If the latter is true, the output is 1. Let L(\u03b1, \u03b2, \u03b7) denote this local sequential test with given parameters {\u03b1, \u03b2, \u03b7}. It sequentially collects samples from {X(t)}\u221et=1. After collecting each sample, it determines whether to terminate the test and if yes, which value\nto output based on the following rule:\n\u2022 If X(s)\u2212 \u221a 2\u03be log 2s 3 \u03b1\ns > \u03b7, terminate and output 1.\n\u2022 If X(s) +\n\u221a 2\u03be log 2s 3\n\u03b2\ns < \u03b7, terminate and output 0.\n\u2022 Otherwise, continue taking samples,\nwhere X(s) denotes the sample mean obtained form s observations and \u03be is the distribution\nparameter specified in (1). We now specify the random walk on the tree based on the outputs of the local CB-based tests. Let (k, l) denote the current location of the random walk (which is initially set at the root node). Consider first l > 1. The left child of (k, l) is first probed by the local module L(\u03b1, \u03b2, \u03b7) with parameters set to \u03b1 = \u03b2 = p0 where p0 can be set to any constant in (0, 1 \u2212 1\u221a\n2 ), and \u03b7 being the threshold associated with level l \u2212 1 of the children of (k, l).\nIf the output is 1, the random walk moves to the left child of (k, l), and the procedure repeats. Otherwise, the right child of (k, l) is tested with the same set of parameters, and the random\n10\n11\nwalk moves to the right child if this test outputs 1. If the outputs of the tests on both children are 0, the random walk moves back to the parent of (k, l) (the parent of the root node is defined as itself). The values for {\u03b1, \u03b2, \u03b7} specified above ensure that the random walk moves toward the target with a probability greater than 1/2. When the random walk arrives at a node on level l = 1, the left child of (k, l) is first probed by the local module L(\u03b1, \u03b2, \u03b7) with parameters set to \u03b1 = \u01eb\n2LCp0 , \u03b2 = p0 and \u03b7 = \u03b70 where\nCp0 = 1\n( 1\u2212 exp(\u22122(1\u2212 2(1\u2212 p0)2)2) )2 . (6)\nIf the output is 1, the random walk terminates and the left child of (k, l) is declared as the target. Otherwise, the right child of (k, l) is tested with the same set of parameters, and the random walk terminates with the right child declared as the target if this test outputs 1. If the outputs of the tests on both children are 0, random walk moves back to the parent of (k, l). The values for {\u03b1, \u03b2, \u03b7} specified above ensure that the random walk terminates at and declares the true target with the required confidence level of 1\u2212 \u01eb. A description of CBRW for detecting a single leaf-level target is given in Fig. 2."}, {"heading": "B. Detecting Hierarchical Targets", "text": "We now consider the case where the target may reside at higher levels of the tree. The following\nmodified CBRW policy detects a potentially hierarchical target with the required confidence level of 1\u2212 \u01eb. Let (k, l) denote the current location of the random walk (which is initially set at the root node). Consider first (k, l) is a non-leaf node with l > 0. The node (k, l) is first probed by the local module L(\u03b1, \u03b2, \u03b7) with parameters set to \u03b1 = \u03b2 = p0 where p0 \u2208 (0, 1\u2212 13\u221a2) and \u03b7 = \u03b7l. If the output is 0, the random walk moves to the parent of (k, l). If the output is 1, then the left child of (k, l) is tested by the local module L(\u03b1, \u03b2, \u03b7) with parameters set to \u03b1 = \u03b2 = p0 and \u03b7 = \u03b7l\u22121. If the output is 1, the random walk moves to the left child. Otherwise, the right child of (k, l) is tested with the same set of parameters, and the random walk moves to the right child if this test outputs 1. If the outputs of the tests at (k, l) and its children are 1, 0, and 0, respectively, then (k, l) is likely to be a hierarchical target and the random walk stays at (k, l). When the random walk stays at the same node (k, l), the same tests are repeated on (k, l) and its children with an increased confidence level. We increase the confidence level by dividing \u03b1 and\n12\n13\n\u03b2 by 2 iteratively. When the current value of \u03b1 and \u03b2 becomes smaller than \u01eb 3LCHp0 , the random walk stops and declares (k, l) as the target. The value of\nCHp0 = 1\n( 1\u2212 exp(\u22122(1\u2212 2(1\u2212 p0)3)2) )2 (7)\nensures the desired confidence level of 1\u2212\u01eb at detection of the target. If the random walk moves to a new location the values of \u03b1 and \u03b2 is reset to p0. When the random walk arrives at a leaf node (k, l) with l = 0, the leaf node is tested by the local module L(\u03b1, \u03b2, \u03b7) with parameters set to \u03b1 = \u01eb\n3LCHp0 , \u03b2 = p0 and \u03b7 = \u03b70. If the output is 1, the random walk stops and declares\n(k, l) as the target. Otherwise, the random walk moves to the parent of (k, l). A description of\nCBRW for detecting a single hierarchical target is given in Fig. 3."}, {"heading": "IV. PERFORMANCE ANALYSIS", "text": "In this section, we analyze the sample complexity of CBRW. We again focus on the case\nof a single target and sub-Gaussian distributions and leave extensions to more general cases to\nSec. V."}, {"heading": "A. The Sample Complexity of the CB-based Sequential Test Module", "text": "To analyze the sample complexity of CBRW, we first analyze the sample complexity of the local CB-based sequential test module L(\u03b1, \u03b2, \u03b7) in the lemma below. We then analyze the behavior of the random walk to establish the number of times that the local sequential test is\ncarried out.\nLemma 1. Let \u00b5 denote the expected value of an i.i.d. sub-Gaussian random process {X(t)}\u221et=1. Let \u03c4L be the stopping time of the CB-based sequential test L(\u03b1, \u03b2, \u03b7) applied to {X(t)}\u221et=1. We have, in the case of \u00b5 > \u03b7,\nP[X(T ) +\n\u221a 2\u03be log 2T 3\n\u03b2\nT < \u03b7] \u2264 \u03b2, (8)\nE[T ] \u2264 48 (\u00b5\u2212 \u03b7)2 log\n24 3 \u221a\n2 \u03b1\n(\u00b5\u2212 \u03b7)2 + 2. (9)\nIn the case of \u00b5 < \u03b7,\n14\nP[X(T )\u2212\n\u221a 2\u03be log 2T 3\n\u03b1 2T > \u03b7] \u2264 \u03b1, (10)\nE[T ] \u2264 48 (\u00b5\u2212 \u03b7)2 log\n24 3 \u221a\n2 \u03b2\n(\u00b5\u2212 \u03b7)2 + 2. (11)\nProof. See Appendix A.\nThe inequalities (8) and (10) establish the confidence levels for the local sequential CB-\nbased test. Both results on the confidence levels and the sample complexity are based on the\nconcentration inequalities given in (2)."}, {"heading": "B. The Sample Complexity of CBRW", "text": "In both cases of leaf-level target detection and hierarchical target detection, the sample com-\nplexity of CBRW is order optimal in both N and 1 \u01eb as stated in Theorem 1 below.\nTheorem 1. Assume that there exists \u03b4 > 0 such that \u00b5k,l \u2212 \u03b7l \u2265 \u03b4 for all (k, l) (l = 0, 1, . . . , L, k = 1, . . . , 2L\u2212l). We have1\nEF [TCBRW ] = O(log2 N + log 1\n\u01eb ) (12)\nand\nPF [SCBRW 6= S] \u2264 \u01eb. (13)\nProof. See Appendix B.\nThe gap \u00b5k,l \u2212 \u03b7l in the mean value of a random process at level l and the threshold at the respective level indicates the informativeness of the observations. It is a practical assumption that the gap is bounded away from 0. We might also naturally assume that the higher levels are\nless informative; thus, have smaller gaps. For example, in group testing, tests from larger groups\nof items are less informative about the presence of defective items. Under this assumption we\ncan provide a finite-time upper bound on the sample complexity of CBRW.\nWe first introduce some auxiliary notions which are useful in understanding the trajectory\nof the random walk in CBRW. Under leaf-level target setting, consider a sequence of subtrees\n1The search space for the case of detecting leaf-level target is of size K, the number of leaf nodes. However, since N = 2K\u22121\nis of the same order of K, (12) satisfies for both K and N .\n15\n{T1, T2, ..., TL} of T . Subtree TL is obtained by removing the biggest half-tree containing the target from T . Subtree Tl is iteratively obtained by removing the biggest half-tree containing the target from the half-tree containing the target in the previous iteration. In the example given in Fig. 1, T3 = {(1, 3), (2, 2), (3, 1), (4, 1), (5, 0), (6, 0), (7, 0), (8, 0)}, T2 = {(1, 2), (2, 1), (3, 0), (4, 0)} and T1 = {(1, 1), (2, 0)}. Let (rl, l) denote the child of the root node of Tl. Under hierarchical target setting with a hierarchical target at level l0, let subtrees {Tl0+1, ..., TL} be the same as defined for a target that is a decedent of the hierarchical target. Also, define T \u20321 and T \u20322 as subtrees whose root nodes are children of the hierarchical target. Let (r\u2032l, l) denote the root node of T \u2032l .\nA detailed finite-time upper bound on the sample complexity of CBRW is given in (14) and (15) where (r0, 0) and (rl0 , l0) denote the targets under leaf-level and hierarchical settings, respectively."}, {"heading": "V. EXTENSIONS AND DISCUSSIONS", "text": "In this section, we discuss extensions to more general scenarios and the mapping of various\napplications to the target detection problem at hand.\n16"}, {"heading": "A. Detecting an Unknown Number of Targets", "text": "Detecting |S| > 1 targets with |S| known can be easily implemented by sequentially locating the targets one by one. We assume that each target can be removed after it is located by CBRW2. To ensure that the reliability constraint holds, we replace \u01eb with \u01eb|S| in each search of a single target. The reliability constraint holds by union bound on the error probabilities of the searches\nfor a single target.\nWhen the number of targets is unknown, but an upper bound Smax \u2265 |S| on the number of targets is known, we can similarly detect the targets one by one. To ensure that the reliability constraint holds, we replace \u01eb with \u01eb 2Smax in each search of a single target. The stopping rule for the overall search can be implemented by testing the root node. Specifically, the root node is tested by L(\u01eb0, \u01eb0, \u03b7L) with \u01eb0 = \u01eb2Smax every LCp0 steps in the random walk under leaf target setting and every LCHp0 steps under the hierarchical target setting. The reliability constraint holds by union bound on the error probabilities of the searches for a single target and error in stopping\nthe overall search before finding all targets.\nUnder both leaf-level and hierarchical target settings, with this modification, the sample complexity of finding single targets simply add up to ab O(|S| logK+ |S| log 1 \u01eb ) overall sample complexity."}, {"heading": "B. Heavy-Tailed Distributions", "text": "The extension to more general distribution models can be implemented by only modifying the local CB-based test L in a way that the confidence levels remain the same. As a result, the behavior of the random walk on the tree remains the same.\nSpecifically, for heavy-tailed distributions with existing b\u2019th moment as given in (3), we modify\nthe test L\n\u2022 If X\u0302(s, \u03b1)\u2212 4u1/b( log 2s3 \u03b1\ns ) b\u22121 b > \u03b7, terminate and output 1.\n\u2022 If X\u0302(s, \u03b2) + 4u1/b( log 2s\n3\n\u03b2 s ) b\u22121 b < \u03b7, terminate and output 0.\n\u2022 Otherwise, continue taking samples.\n2For example, in group testing, the detected defective item is no longer tested in any subsequent group tests or in HHH\ndetection, the packet count of each detected HHH can be subtracted from the packet count of the parents.\n17\nThe resulting CBRW achieves the same O(logN + log 1 \u01eb ) sample complexity under both leaflevel and hierarchical target settings. The proofs follow similar to the proofs of Theorem 1 and\nLemma 1, using confidence bounds (5) instead of (2) in the proof of Lemma 1."}, {"heading": "C. General Tree Structures", "text": "Consider a general tree-structured hierarchy as shown in Fig. 5. The CBRW policy can be\nmodified as follows.\nTo have the required confidence level in taking the steps toward the target, the input parameters in the local CB-based sequential test L are modified based on the degree dk,l of each node (k, l) in the tree. In particular, under the leaf-level target setting, we choose p0 \u2208 (1 \u2212 1\n2 \u2212(dk,l\u22121)\n) and\n\u03b1 = \u01eb (D\u22121)LC where L is the maximum distance from the root node to a leaf node, D is the maximum degree of the nodes in the tree and C is a constant independent of K and \u01eb. Under the hierarchical target setting, we choose p0 \u2208 (1 \u2212 1 2 \u2212dk,l ) and when increasing the confidence level iteratively to detect the hierarchical target, we terminate the search when p goes below\n\u01eb DLC . The random walk moves to a child or the parent of the current location according to the outputs of the tests.\nFollowing similar lines as in the proof of Theorem 1, we can show a sample complexity of\nO(LD) +O(log 1 \u01eb ) under both leaf-level and hierarchical target settings.\n18"}, {"heading": "D. Mapping from Various Applications", "text": "HH and HHH detection: The CBRW policy directly applies to leaf-level HH and HHH\ndetection under leaf-level and hierarchical target settings, respectively. In particular, provided\na controllable counter which can be assigned to each IP prefix, we assign the counter to the current location which is desired to be tested according to L. Based on the packet count, the counter is moved on the tree according to CBRW. When there are several counters available the\ntree can be partitioned to smaller subtrees and CBRW is run on each subtree separately to make\nan efficient use of the available counters.\nNoisy group testing: In group testing, the objective is to identify a few defective items in a population of K items performing tests on subsets of items. Each group test gives a binary\noutcome, indicating whether the tested group contains any defective items. We consider the case\nof adaptive group testing under general and unknown noise models for test outcomes. Specifically,\nthe outcome of a group test is a Bernoulli random variable with an unknown parameter that\nrepresents the false alarm or missed detection probability.\nThe CBRW policy under leaf-level target setting directly applies to noisy group testing where\nthe defective items are the leaf-level targets. Each data stream corresponds to the noisy Bernoulli\ntest outcomes of the given subsets of the population. The parent-children relationship on the\ntree represents the subset relationship among the corresponding test groups such that the group\ncorresponding to the parent is the union of the groups corresponding to the children. Under\nthe practical assumption that false alarm and miss detection probabilities are smaller than 1/2, targets are those data streams generated by a singleton subset (i.e., leaf nodes) with a mean value exceeding 1/2. Although group testing problem does not necessarily conform to a\npredetermined hierarchical structure, the proposed solution offers order optimal number of tests\nin both population size and reliability constraint.\nAdaptive sampling with noisy response: Consider the [0, 1] interval as the input space.\nWe limit the input space to be one-dimensional in order to demonstrate the main idea. The hypothesis class, denoted by H, is the set of all step functions on [0,1] interval.\nH = { hz : [0, 1] \u2192 R, hz(x) = 1{(z,1]}(x), z \u2208 (0, 1) } (16)\nEach hypothesis hz assigns a binary label to each element of the input space [0, 1]. There is a true hypothesis hz\u2217 that determines the ground truth labels for the input space.\n19\nThe learner is allowed to make sequential observations by adaptively sampling hz\u2217 . The observations are however noisy. The goal is to design a sequential sampling strategy aiming at minimizing the sample complexity required to obtain a confidence interval of length \u2206 for z\u2217 at a 1 \u2212 \u01eb confidence level. Specifically, the learner chooses the sampling point x at each time t and receives a noisy sample of the true hypothesis.\nWe consider two noise models with unknown distribution. In the first noise model, the learner\nobserves a noisy sample of the threshold function in the form of\nhNz\u2217(x; t) = hz\u2217(x; t) + n(x, t), (17)\nwhere n(x, t) is a zero mean sampling noise that possibly depends on the sampling point x and is generated i.i.d. over t.\nIn the second noise model, the binary samples can flip from zeros to ones and vice versa. Specifically, the learner receives erroneous binary samples with an error probability of p(.) in\nthe form of\nhBz\u2217(x; t) = hz\u2217(x; t)\u2295B(x, t), (18)\nwhere \u2295 is the boolean sum and B(x, t) is a Bernoulli random variable with P[B(x, t) = 1] = p(x) that may depend on the sampling point x and is generated i.i.d. over t.\nWe now present a solution to the adaptive sampling problem based on the results obtained\nfor CBRW strategy. For the simplicity of presentation we assume \u2206 = 1 2L (K = 1 \u2206 ). Let each node on a binary tree T represent an interval [zLk,l, zRk,l] \u2282 [0, 1] with zLk,l = (k2L\u2212l \u2212 1)\u2206 and zUk,l = k2 L\u2212l\u2206. The interval corresponding to each node on the tree is the union of the intervals corresponding to its children.\nWhat remains to be specified is what entails when probing a node/interval (k, l). When (k, l) is probed the boundary points of the interval are tested by L(\u03b1, \u03b2, \u03b7) with parameters set to \u03b7l = 0.5 and \u03b1 = \u03b2 = p0 on a non-leaf node, and \u03b1 =\n\u01eb 2LCp0 , \u03b2 = p0 on a leaf node where p0\ncan be set to any constant in (0, 1\u2212 14\u221a2). The output is 1 (indicating that the interval is likely to contain z\u2217) if and only if the output of L is 0 on the left boundary and 1 on the right boundary. The output is 0 otherwise. From the results on the analysis of CBRW the above solution has a sample complexity of O( 1 c2 logK + 1 c2 log 1 \u01eb ) where c is 0.5 under the first noise model and c is a lower bound on the gap in 0.5\u2212 p(.) under the second noise model.\n20"}, {"heading": "VI. CONCLUSION", "text": "In this paper, we studied the problem of detecting a few targets among a large number of\nhierarchical data streams modeled as random processes with unknown distributions. We designed\na sequential strategy to interactively choose the sampling point aiming at minimizing the sample\ncomplexity subject to a reliability constraint. The proposed sequential sampling strategy detects\nthe targets at the desired confidence level with an order optimal logarithmic sample complexity in\nboth problem size and the parameter of reliability constraint. We further showed the extensions\nof the results to a number of active inference and learning problems in networking and data\nanalytics applications.\nThe results obtained in this work extend to the detection of anomaly in other statistics such as\nvariance. In particular, replacing the sample mean with sample variance in the local CB-based\nsequential test and modifying the second term in the upper and lower confidence bounds in\nthe local CB-based sequential test, both the CBRW policy and its sample complexity analysis\napply to anomaly detection where the anomalies manifest in the variance. Similar results can be\nobtained for other statistics assuming the existence of an efficient estimator."}, {"heading": "APPENDIX A", "text": "Proof of Lemma 1. The proof of Lemma 1 is based on concentration inequalities for Sub-\nGaussian distributions.\n23\nWe prove inequality (9) here. The other case, \u00b5 < \u03b7 (11), can be proven similarly.\nP [ X(T ) +\n\u221a 2\u03be log 2T 3\n\u03b2\nT < \u03b7\n]\n\u2264 P [ sup s X(s) +\n\u221a 2\u03be log 2s 3\n\u03b2\ns < \u03b7\n]\n\u2264 \u221e\u2211\ns=1\nP [ X(s) +\n\u221a 2\u03be log 2s 3\n\u03b2\ns < \u03b7\n]\n\u2264 \u221e\u2211\ns=1\nexp(\u2212 log 2s 3\n\u03b2 ) (19)\n=\n\u221e\u2211\ns=1\np\n2s3\n\u2264 \u03b2.\nInequity (19) is obtained by (2). We next analyze the E[T ] for \u00b5 > \u03b7. Let s0 = min{s \u2208 N : \u221a 2 log 2s 3 \u03b1 s \u2264 \u00b5\u2212\u03b7 2 , s > 1}, for n \u2265 s0:\nP[T \u2265 n] \u2264 P [ sup { s : X(s) +\n\u221a 2\u03be log 2s 3\n\u03b2\ns > \u03b7, and\nX(s)\u2212\n\u221a 2\u03be log 2s 3\n\u03b1\ns < \u03b7\n} \u2265 n ]\n\u2264 P [ sup { s : X(s)\u2212\n\u221a 2\u03be log 2s 3\n\u03b1\ns < \u03b7\n} \u2265 n ]\n\u2264 \u221e\u2211\ns=n\nP [ X(s)\u2212\n\u221a 2\u03be log 2s 3\n\u03b1\ns < \u03b7\n]\n\u2264 \u221e\u2211\ns=n\nP [ Xs \u2212 \u00b5 < \u2212\n\u221a 2 log 2s 3\n\u03b1\ns\n] (20)\n\u2264 \u221e\u2211\ns=n\nexp(\u2212 log 2s 3\n\u03b1 )\n\u2264 \u221e\u2211\ns=n\n\u03b1\n2s3\n\u2264 \u03b1 4(n\u2212 1)2 .\n24\nNotice that (20) holds because n \u2265 s0. We can write E[T ] in terms of P[T \u2265 n] as\nE[T ] =\n\u221e\u2211\nn=0\nP[T \u2265 n] (21)\n= s0 +\n\u221e\u2211\nn=s0\nP[T \u2265 n]\n\u2264 s0 + \u221e\u2211\nn=s0\n\u03b1\n4(n\u2212 1)2\n\u2264 s0 + 1.\nFor the last inequality notice that s0 is defined to be bigger than 1. It remains to find s0. Note that for all x > 0 we have log x < \u221a x so log log x < log \u221a x = 1\n2 log x. For s = 48 (\u00b5\u2212\u03b7)2 log 24 3 \u221a 2 \u03b1 (\u00b5\u2212\u03b7)2\nlog 2s3\n\u03b1 = 3 log\n3\n\u221a 2\n\u03b1 s\n= 3 log 3\n\u221a 2\n\u03b1\n48 (\u00b5\u2212 \u03b7)2 log 24 3\n\u221a 2 \u03b1\n(\u00b5\u2212 \u03b7)2\n= 3 log 3\n\u221a 2\n\u03b1\n24 (\u00b5\u2212 \u03b7)2 + 3 log log ( 24 3\n\u221a 2 \u03b1\n(\u00b5\u2212 \u03b7)2 )2\n\u2264 3 log 3 \u221a 2\n\u03b1\n24\n(\u00b5\u2212 \u03b7)2 + 3 log 3\n\u221a 2\n\u03b1\n24\n(\u00b5\u2212 \u03b7)2\n= 6 log 3\n\u221a 2\n\u03b1\n24\n(\u00b5\u2212 \u03b7)2\n= 6 (\u00b5\u2212 \u03b7)2\n48 s.\nThus, for s = 48 (\u00b5\u2212\u03b7)2 log\n24 3 \u221a\n2 \u03b1\n(\u00b5\u2212\u03b7)2 , \u221a\n2 log 2s 3\n\u03b1 s \u2264 \u00b5\u2212 \u03b7 2 .\nSo, we have the following upper bound for s0\ns0 \u2264 \u2308 48 (\u00b5\u2212 \u03b7)2 log 24 3\n\u221a 2 \u03b1\n(\u00b5\u2212 \u03b7)2 \u2309+ 1. (22)\nThe addition of 1 is because s0 is defined to be bigger than 1. Thus,\nE[T ] \u2264 48 (\u00b5\u2212 \u03b7)2 log\n24 3 \u221a\n2 \u03b1\n(\u00b5\u2212 \u03b7)2 + 2, (23)\n25\nwhich completes the proof."}, {"heading": "APPENDIX B", "text": "Proof of Theorem 1. An upper bound on the sample complexity of test L is provided in Lemma 1. Here, we establish an upper bound on the number of times that test L is called in CBRW. First, consider the case of leaf-level target setting.\nIn order to analyze the trajectory of the random walk, we consider the last passage time Tl of the random walk from each subtree Tl. We prove an upper bound on E[Tl] for each l which gives an upper bound on the total number of times that test L is called. Notice that the total number of times that test L is called is not bigger than 2 \u2211L l=1 E[Tl].\nThe random walk initially starts at the root node at distance L from the target. Define the parameters Wt as the steps of the random walk: Wt = 1 if the random walk moves one step further from the target at time t, Wt = \u22121 if the random walk moves one step closer to the target, and Wt = 0 when the random walk does not move. Clearly, \u2211\u03c4 t=1 Wt = \u2212L where \u03c4 is the stopping time of the random walk. The random walk stops when the policy declares a leaf node as the target. For the mean value of Wt, from Lemma 1, we have\nE[Wt] = P[Wt = 1]\u2212 P[Wt = \u22121]\n\u2264 1\u2212 2(1\u2212 p0)2\n< 0.\nNotice that if the random walk is within the subtree TL at step t, we have t\u2211\ns=1\nWs > 0. (24)\nThus, we can write\nP[TL > n] \u2264 P [ sup{t \u2265 1 : t\u2211\ns=1\nWs > 0} > n ]\n(25)\n\u2264 \u221e\u2211\nt=n\nP\n[ t\u2211\ns=1\nWs > 0\n]\n\u2264 \u221e\u2211\nt=n\nexp ( \u2212 1\n2 t(1\u2212 2(1\u2212 p0)2)2\n) (26)\n= exp(\u22122n(1\u2212 2(1\u2212 p0)2)2)\n1\u2212 exp(\u22122(1\u2212 2(1\u2212 p0)2)2) .\n26\nInequity (26) is obtained by Hoeffding inequality for Bernoulli distributions. We can obtain E[TL] from P[TL > n] based on the sum of tail probabilities as\nE[TL] = \u221e\u2211\nn=0\nP[TL > n]\n\u2264 \u221e\u2211\nn=0\nexp(\u22122n(1\u2212 2(1\u2212 p0)2)2) 1\u2212 exp(\u22122(1\u2212 2(1\u2212 p0)2)2)\n= 1\n( 1\u2212 exp(\u22122(1\u2212 2(1\u2212 p0)2)2) )2 .\nLet us define\nCp0 = 1\n( 1\u2212 exp(\u22122(1\u2212 2(1\u2212 p0)2)2) )2 , (27)\nwhich is a constant independent of K and \u01eb. From the symmetry of binary tree, it can be seen that E[Tl] \u2264 Cp0 for all l and the expected number of points visited by the random walk is upper bounded by 2LCp0 . Under the assumption that the informativeness of observations decreases in higher levels we can replace the sample complexity of test L at the highest level and arrive at the first term in (14). The second term in (14), is obtained by direct application of Lemma 1 on the sample complexity of test L at the target node. It remains to show that CBRW satisfies the reliability constraint. We know that at each visit of a leaf node the probability of declaring a non-target node as the target is lower than \u01eb 2LCp0 by the design of the test at leaf nodes. Thus, from the upper bound on the expected number of\npoints visited by the random walk we have\nP[S\u03b4 6= {(k0, 0)}] \u2264 2LCp0 \u01eb\n2LCp0\n= \u01eb.\nOrder optimality of logK follows from information theoretic lower bound. Order optimality\nof log 1 \u01eb can be established following standard techniques in sequential testing problems.\nAn upper bound on the sample complexity of CBRW under hierarchical target setting can be\nobtained similarly. The trajectory of the random walk can be analyzed by considering the last passage times Tl of the random walk from subtrees Tl for l = l0 + 1, ..., L, as well as the last\n27\npassage times T \u20321 and T \u2032 2 of the random walk from subtrees T \u20321 and T \u20322 which can be shown to not bigger than CHp0 following the similar lines as in the proof of Theorem 1 with\nCHp0 = 1\n( 1\u2212 exp(\u22122(1\u2212 2(1\u2212 p0)3)2) )2 . (28)\nThe analysis under hierarchical target setting differs from the analysis under leaf-level target setting in that the consecutive calls of test L on the same node results in increasing the confidence level. We establish an upper bound on the expected total number Ttot of observations from a node at a series of consecutive calls of test L on the node where the confidence level is divided by 2 iteratively at each time. Let T (k) be the number of samples taken at k\u2019th consecutive call of test L on a node. By design of CBRW strategy under hierarchical target setting the value of p in test L is divided by 2 until the first time k that p0\n2k\u22121 < \u01eb\n3LCHp0 . Thus there are at most\n\u2308log2 3LCHp0p0 \u01eb \u2309 consecutive calls of test L on one node. On a non-target node:\nE[Ttot] \u2264 \u2308log2\n3LCHp0 p0\n\u01eb \u2309\u2211\nk=1\npk\u221210 E[T (k)]\n\u2264 \u221e\u2211\nk=1\npk\u221210\n( 48\n(\u00b5\u2212 \u03b7)2 log 24 3\n\u221a 2k\np0 (\u00b5\u2212 \u03b7)2 + 2 )\n\u2264 \u221e\u2211\nk=1\npk\u221210\n( 48\n(\u00b5\u2212 \u03b7)2 log 24 3\n\u221a 2 p0\n(\u00b5\u2212 \u03b7)2 + 2 )\n+ \u221e\u2211\nk=1\npk\u221210 48 (\u00b5\u2212 \u03b7)2 log 3 \u221a 2k\u22121\n= 1\n1\u2212 p0\n( 48\n(\u00b5\u2212 \u03b7)2 log 24 3\n\u221a 2 p0\n(\u00b5\u2212 \u03b7)2 + 2 )\n+ p0 (1\u2212 p0)2 16 log 2 (\u00b5\u2212 \u03b7)2 . (29)\nUpper bound on E[Ttot] in a conservative upper bound on each single time that the test L is called.\n28\nOn the target node:\nE[Ttot] \u2264 \u2308log2\n3LCHp0 p0\n\u01eb \u2309\u2211\nk=1\nE[T (k)]\n\u2264 \u2308log2 3LCHp0p0 \u01eb \u2309 (\n48 (\u00b5\u2212 \u03b7)2 log 24 3\n\u221a 4 \u01eb\n(\u00b5\u2212 \u03b7)2 + 2 )\n\u2264 log2 6LCHp0p0\n\u01eb\n( 48\n(\u00b5\u2212 \u03b7)2 log 24 3\n\u221a 4 \u01eb\n(\u00b5\u2212 \u03b7)2 + 2 ) . (30)\nFrom the upper bound on E[Ttot], the upper bound on the sample complexity of CBRW can be obtained. The satisfaction of the constraint on error probability can be shown similar to the\nleaf-level target setting."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We consider the problem of detecting a few targets among a large number of hierarchical data<lb>streams. The data streams are modeled as random processes with unknown and potentially heavy-tailed<lb>distributions. The objective is an active inference strategy that determines, sequentially, which data stream<lb>to collect samples from in order to minimize the sample complexity under a reliability constraint. We<lb>propose an active inference strategy that induces a biased random walk on the tree-structured hierarchy<lb>based on confidence bounds of sample statistics. We then establish its order optimality in terms of both<lb>the size of the search space (i.e., the number of data streams) and the reliability requirement. The results<lb>find applications in hierarchical heavy hitter detection, noisy group testing, and adaptive sampling for<lb>active learning, classification, and stochastic root finding.", "creator": "LaTeX with hyperref package"}}}