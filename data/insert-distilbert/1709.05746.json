{"id": "1709.05746", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Sim-to-real Transfer of Visuo-motor Policies for Reaching in Clutter: Domain Randomization and Adaptation with Modular Networks", "abstract": "a modular method variation is proposed hoping to learn and transfer behavioral visuo - motor policies from simulation to the real world in an efficient manner by combining domain behavior randomization and adaptation. the feasibility of the approach is demonstrated in a table - top object scale reaching task where a 7 dof arm is controlled in velocity mode trying to reach a blue square cuboid in clutter through visual observations. the learned visuo - motor policies are returning robust targets to novel ( not seen in training ) objects in clutter and even a moving target, achieving it a 93. 3 % success rate and 2. 2 cm control accuracy.", "histories": [["v1", "Mon, 18 Sep 2017 02:27:02 GMT  (3504kb,D)", "http://arxiv.org/abs/1709.05746v1", "Under review for the IEEE International Conference on Robotics and Automation (ICRA) 2018. arXiv admin note: text overlap witharXiv:1610.06781"]], "COMMENTS": "Under review for the IEEE International Conference on Robotics and Automation (ICRA) 2018. arXiv admin note: text overlap witharXiv:1610.06781", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG cs.SY", "authors": ["fangyi zhang", "j\\\"urgen leitner", "michael milford", "peter corke"], "accepted": false, "id": "1709.05746"}, "pdf": {"name": "1709.05746.pdf", "metadata": {"source": "CRF", "title": "Sim-to-real Transfer of Visuo-motor Policies for Reaching in Clutter: Domain Randomization and Adaptation with Modular Networks*", "authors": ["Fangyi Zhang", "J\u00fcrgen Leitner", "Michael Milford", "Peter Corke"], "emails": ["fangyi.zhang@hdr.qut.edu.au"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe advent of large datasets and sophisticated machine learning models, commonly referred to as deep learning, has in recent years created a trend away from hand-crafted solutions towards more data-driven ones. Learning techniques have shown significant improvements in robustness and performance [1], particularly in the computer vision field. Traditionally robotic reaching approaches have been based on crafted controllers that combine (heuristic) motion planners with the use of hand-crafted features to localize the target visually. Recently learning approaches to tackle this problem have been presented [2]\u2013[5], however a consistent issue faced by most approaches is the reliance on large amounts of data to train these models. For example, Google researchers addressed this problem by developing an \"arm farm\" with 6 to 14 robots collecting data in parallel [3]. Generalization forms another challenge: many current systems are brittle when learned models are applied to robotic configurations that differ from those used in training. This leads to the question: Is there a better way to learn and transfer visuo-motor policies on robots for tasks such as reaching?\nVarious approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].\nThe contribution of this paper is a method that connects these three, usually separately considered, approaches. We propose a modular deep learning approach to efficiently learn and transfer visuo-motor policies from simulated to real\n*This research was conducted by the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016). Computational resources & services used in this work were partially provided by the HPC and Research Support Group, Queensland University of Technology.\n1The authors are with the Australian Centre for Robotic Vision (ACRV), Queensland University of Technology (QUT), Brisbane, Australia fangyi.zhang@hdr.qut.edu.au\nenvironments, and benchmark with a visually-guided tabletop object reaching task for a 7 DoF robotic arm (Fig. 1). Vision and kinematics data is gathered in simulation (cheap) to decrease the amount of real world collection necessary (costly). Significantly, by introducing a modular approach the perception skill and the controller can be transferred individually to a robotic platform, while retaining the ability to fine-tune them in an end-to-end fashion to further improve hand-eye coordination on a real robot (in this research a Baxter).\nThe use of both simulated (with domain randomization) and real (domain adaptation) data makes this approach able to transfer perception to the real world (3.2 cm accuracy) with only 3000 simulated and 186 real images. Benefiting from the modular structure and weighted end-to-end finetuning, the learned visuo-motor policy can achieve a reaching accuracy of 2.2 cm with only 333 trajectories (30225 statevelocity pairs). The learned visuo-motor policy is not only able to reach a target object in clutter with seen distractor objects but also for the cases with novel (not seen in training) distractor objects and even when the target object is moving."}, {"heading": "II. RELATED WORK", "text": "Data-driven learning approaches have become popular in computer vision and are starting to replace hand-crafted so-\nar X\niv :1\n70 9.\n05 74\n6v 1\n[ cs\n.R O\n] 1\n8 Se\np 20\n17\nlutions also in robotic applications. Especially robotic vision tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest. The lack of largescale real-world datasets, which are expensive and slow to acquire, limit the general applicability of the approach and has so far limited the broader application. Collecting the datasets required for deep learning has been sped up by using many robots operating in parallel [3]. With over 800,000 grasp attempts recorded, a deep network was trained to predict the success probability of a sequence of motions aiming at grasping using a 7 DoF robotic manipulator with a 2-finger gripper. Combined with a simple derivative-free optimization algorithm the grasping system achieved a success rate of 80%. Another example of dataset collection for grasping is the approach to self-supervised grasp learning in the real world where force sensors were used to autonomously label samples [11]. After training with 50,000 real-world trials using a staged leaning method, a deep convolutional neural network (CNN) achieved a grasping success rate around 70%. These are impressive results but achieved at high cost in terms of dollars, space and time.\nDeepMind showed that a deep reinforcement learning system is able to synthesize control actions for computer games directly from vision data [14]. While this result is an important and exciting breakthrough it does not transfer directly to real robots with real cameras observing real scenes [2]. In fact very modest image distortions in the simulation environment (small translations, Gaussian noise and scaling of the RGB color channels) caused the performance of the system to fall dramatically. Introducing a real camera observing the game screen was even worse [15].\nThere has been increasing interest to create robust visuomotor policies for robotic applications, especially in reaching and grasping. Levine et al. introduced a CNN-based policy representation architecture with an added guided policy search (GPS) to learn visuo-motor policies (from joint angles and camera images to joint torques) [16], which allows reduction in the number of real world training examples by providing an oracle (or expert\u2019s initial condition to start learning). Impressive results were achieved in complex tasks, such as hanging a coat hanger, inserting a block into a toy, and tightening a bottle cap. Recently it has been proposed to simulated depth images to learn and then transfer grasping skills to real-world robotic arms [17], yet no adaptation in the real-world has been performed.\nTransfer learning attempts to develop methods to transfer knowledge between different tasks (scenarios) [18], [19]. To reduce the amount of data collected in the real world (expensive), transferring skills from simulation to the real world is an attractive alternative. Progressive neural networks are leveraged to improve transfer and avoid catastrophic forgetting when learning complex sequences of tasks [20]. Their effectiveness has been validated on reinforcement learning tasks, such as Atari and 3D maze game playing. Modular reinforcement learning approaches have shown skill transfer capabilities in simulation [21]. However, methods for\nreal-world robotic applications are still scarce and require manually designed mapping information, e.g. similaritybased approach to skill transfer for robots [9]. To reduce the number of real-world images required, a method of adapting visual representations from simulated to real environments was proposed, achieving a success rate of 79.2% in a \u201chook loop\u201d task, with 10 times less real-world images [10]. Domain randomization has also been successfully used to transfer deep neural networks from simulation to the real world [7], [8]."}, {"heading": "III. METHODOLOGY", "text": "Domain randomization has been successfully used to transfer deep neural networks from simulation to the real world for object position recognition [7] and also visuomotor control [8]. In [8], auxiliary tasks such as cube pose estimation are used in training to improve performance in an end-to-end manner. In this paper, we propose a modular approach to take use of the object position estimation task for more efficient learning and transfer of visuo-motor policies."}, {"heading": "A. Modular Deep Networks", "text": "Studies of deep visuo-motor policies indicate that the convolutional layers focus on perception, i.e., extracting useful information from visual inputs, while the fully connected (FC) layers perform control [23]. To make the learning and transfer of visuo-motor policies more efficient, we propose to separate a deep neural network into perception and control modules connected by a bottleneck layer (Fig. 2). The bottleneck forces the network to learn a low-dimensional representation, not unlike Auto-encoders [24]. The difference is that we explicitly equate the bottleneck layer with the object position (x\u2217).\nWith the bottleneck, the perception module learns how to estimate the object position x\u2217 from a raw-pixel image I; the control module learns to determine the most appropriate joint velocities v given the object position and joint angles q (defined as scene configuration \u0398 = [x\u2217,q]). The values of x\u2217 and q are normalized to the interval [0, 1]."}, {"heading": "B. Training Method", "text": "1) Perception: The perception module is trained using supervised learning \u2013 first using simulated images, then finetuned with a small number of real samples for skill transfer \u2013 with the quadratic loss function\nLp = 1\n2m m\u2211 j=1 \u2225\u2225yp(Ij)\u2212 x\u2217j\u2225\u22252 , (1) where yp(Ij) is the prediction of x\u2217j for Ij ; m is the number of samples. The physical meaning of x\u2217 guarantees the convenience of collecting labelled training data.\n2) Control: The control module is also trained using supervised learning with only simulated data:\nLc = 1\n2m m\u2211 j=1 \u2016yc(sj)\u2212 vj\u20162 , (2)\nwhere yc(sj) is the prediction of vj for state sj . Here, s = \u0398.\n3) End-to-end fine-tuning using weighted losses: To further improve hand-eye coordination, an end-to-end finetuning is conducted for the combined network (perception + control) after their separate training, using weighted control (Lc) and perception (Lp) losses. Note that s = I in Equation 2 for the end-to-end fine-tuning, rather than \u0398. The control module is updated using only Lc, while the perception module is updated using the weighted loss\nL = \u03b2Lp + (1\u2212 \u03b2)LBNc , (3)\nwhere LBNc is a pseudo-loss which reflects the loss of Lc in the bottleneck; \u03b2 \u2208 [0, 1] is a balancing weight. From the backpropagation algorithm [25], we can infer that \u03b4L = \u03b2\u03b4Lp + (1 \u2212 \u03b2)\u03b4LBNc , where \u03b4L is the gradients resulting from L; \u03b4Lp and \u03b4LBNc are the gradients resulting respectively from Lp and LBNc (equivalent to that resulting from Lc in the perception module)."}, {"heading": "IV. BENCHMARK: ROBOTIC REACHING", "text": "We use a canonical target reaching task as a benchmark to evaluate the feasibility of the proposed approach. The task is defined as controlling a robot arm so that its end-effector position x in operational space moves to the position of a target x\u2217 \u2208 Rm (object position introduced in Section IIIA). The robot\u2019s joint configuration is represented by its joint angles q \u2208 Rn. The two spaces are related by the forward kinematics, i.e., x = K(q). The reaching controller adjusts the robot configuration in velocity mode to minimize the error between the robot\u2019s current and target position, i.e., \u2016x\u2212 x\u2217\u2016. We consider a 7 DoF robotic arm (Fig. 1), i.e., q \u2208 R7 steering its end-effector position in 3D, i.e., x \u2208 R3 \u2013 ignoring orientation."}, {"heading": "A. Task Setup", "text": "The real-world task employs a Baxter robot\u2019s left arm (7 DoF) to reach a blue cuboid in clutter. All objects are arbitrarily placed in the operation area (50\u00d760 cm) as shown in Fig. 3A. The blue cuboid has a side length of 6.5 cm. The robot observes environments through a monocular camera in its right hand (Fig. 1A), providing RGB images with a resolution of 256\u00d7256. The left arm is controlled in velocity mode. A reach is deemed successful, if the Euclidean\ndistance between the the top centre of the target cuboid and the bottom center of the suction gripper (\u201cTop Centre\u201d and \u201cBottom Centre\u201d in Fig. 3) is smaller than 4.6 cm (half of the diagonal length of any side of the cuboid). In the task, the left arm is randomly initialized to a configuration with a normal distribution around the reference configuration shown in Fig. 3B."}, {"heading": "B. Network Architecture", "text": "In this work, we used a network with the architecture shown in Fig. 2. The perception module has an architecture customized from VGG16 [22] for lower computational cost but without losing too much performance for this task. It consists of twelve convolutional layers with 3\u00d73 filters and seven 2\u00d72 max pooling layers, followed by three fully connected layers. Simulated or real RGB images are cropped and down-sampled to 256\u00d7 256 as inputs to the perception module. The control module consists of 3 fully connected layers, with 400 and 300 units in the two hidden layers. Input to the control module is the scene configuration \u0398 (target position and joint angles), its outputs are the estimates for joint velocities v (7 DoF). Networks with a first convolutional layer initialized with weights from pretrained VGG16 [22] are observed to converge faster and\nachieve higher accuracy, particularly when the training sets are relatively small. The other parts of the networks are initialized with random weights."}, {"heading": "C. Datasets Collection", "text": "Perception datasets contain a number of image-position (I-x\u2217) pairs. In this work, we label the position of the target cuboid top centre as the target position x\u2217 rather than its mass-centre. Aiming to get a good balance between domain randomization and adaptation, we collected both simulated and real data as shown in Fig. 4 for investigation. The simulated data was collected using V-REP [26] (a robotic simulation platform) through domain randomization [7] in the following aspects:\n\u2022 number of distractor objects in clutter: random in [0, 9]; \u2022 shape of distractor objects in clutter: random in 9\nprimitive shapes with different geometries (5 cuboids, 2 spheres, 2 cylinders); \u2022 pose of distractor objects: random position in the operation area and random orientation of the axis pointing upwards; \u2022 color of distractor objects: random RGB values; \u2022 left arm configuration: random in joint space, excluding\nthe ones with self-collision; \u2022 color of the table, floor, robot body and target cuboid:\nrandom changes based on reference colors (\u00b110%); \u2022 camera pose: random changes of each right arm joint\nangle based on reference angles (\u00b11%); \u2022 camera field of view (FoV): random changes based on\na reference FoV (\u00b12%); \u2022 table pose: random changes based on a reference posi-\ntion ([\u00b11.5%,\u00b15%,\u00b11%]) and a reference orientation of the axis pointing upwards (\u00b17%);\nAll the above randomization is uniformly distributed. The reference colors , FoV and table pose were tuned manually to approximate the real scene. The reference joint angles were tuned in the real world, making sure the in-hand camera can see the entire operation area. The parameters for the aspects randomized based on references were manually tuned to simulate possible variations in the real scene.\nThe real images shown in Fig. 4 were collected on a real Baxter robot (Fig. 1B) with random objects and left arm configurations. The ground-truth position of the target blue cuboid was collected by putting the end-effector bottom centre on the cuboid top centre and recording the left arm configuration (target configuration q\u2217) for forward kinematics, i.e., x\u2217 = K(q\u2217). The ground-truth position collected in this way is accurate enough for the benchmark task, although some errors might be caused by manually matching the end-effector with the cuboid. This ground-truth position collection method was also used in the control performance evaluation in Section V.\nIn training, to increase the training data diversity, data augmentation is done on-the-fly for both simulated and real images by varying image brightness (\u00b180% for simulated images and \u00b140% for real images) and white balance (\u00b12.5%) in a post-processing manner. These augmentation parameters were empirically determined.\nControl datasets contain a number of sceneconfiguration-velocity (\u0398-v) pairs (i.e., trajectories) as well as image-velocity (I-v) and image-position (I-x\u2217) pairs. \u0398-v pairs are for training control modules separately (Section III-B.2); I-v and I-x\u2217 pairs are for end-to-end fine-tuning to obtain \u03b4Lc and \u03b4Lp (Section III-B.3).\nWe collected control datasets in simulation using V-REP. Trajectories were generated to control the left arm with a random initial configuration (excluding the ones with selfcollision) to reach a target arbitrarily placed in the operation area, regardless of obstacle avoidance. As introduced in Section IV-A, the random initial configuration has a normal distribution around the reference configuration shown in Fig. 3B; the random targets are uniformly distributed in the operation area. When generating the trajectories, the pseudo inverse method (V-REP internal implementation) was used to calculate the desired arm configuration to reach a target, i.e., q\u2217 = K\u22121(x\u2217). Then a simple proportional controller was used to control the left arm to reach the desired configuration from its initial configuration with a control frequency of 20Hz. In the process, the target cuboid position, joint angles and velocity commands were recorded, accompanying with the images from the camera in the right hand. Experiments (Section V-B) show that simulated control training data is sufficient to achieve good performance alone \u2013 there is no need to collect real control datasets."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": "We evaluated the performance of the proposed approach in three aspects: perception accuracy, control performance\nand hand-eye coordination. The performance was evaluated in the real world using the metrics of:\n\u2022 Perception Error: the Euclidean distance between the estimated and ground-truth object positions; \u2022 Control Error: the Euclidean distance between the target cuboid top centre and end-effector bottom centre (\u201cTop Centre\u201d and \u201cBottom Centre\u201d in Fig. 3A); \u2022 Success Rate: the percentage of successful reaching among all trials, where a reach is deemed successful if the final Euclidean distance between the target and end-effector is smaller than 4.6 cm as defined in Section IV-A; \u2022 Time Cost: the number of control steps used in an entire reaching process."}, {"heading": "A. Perception Accuracy", "text": "To investigate the influence of the numbers of simulated and real images on perception accuracy, we evaluated 15 different perception modules. They were trained with different combinations of images:\n\u2022 the number of simulated images from 0 to 3000; \u2022 the number of real images from 0 to 279.\nAs introduced in Section III-B.1, all the 15 perception modules were first trained using simulated images then finetuned with real images. The training was from scratch, except that the first convolutional layer was initialized with weights from pre-trained VGG16 [22]. In training, we used a minibatch size of 32 and a learning rate of 0.01. RmsProp [27] was adopted, the same in the training of control modules (Section V-B) and end-to-end fine-tuning (Section V-C). The pixel values in images were normalized to [\u22121, 1] per image. The mean and standard deviation of their perception errors for a test set are shown in Fig. 5. The test set has 48 real images where the target is uniformly distributed in the operation area, with random distractor objects that have appeared in training.\nFrom Fig. 5, we can see that the perception modules trained with only simulated (the bottom row) or real images (the left most column) have very large errors. For the ones trained with both simulated and real images, increasing the number of either simulated or real images helped reduce the error. Fine-tuning (adaptation) with as few as 93 real images can make a perception module work in the real world with an average error of 6.9 cm. The one trained with 3000 simulated and 279 real images (the top right one) achieved the smallest average error (2.9 cm). However, trading off the accuracy and the used number of real images, we pick the one trained with 3000 simulated and 186 real images for further experiments, labelled as PP. It has an average error of 3.2 cm which is slightly larger than the best one, but needs only 67% of the real images.\nTo study how much the on-the-fly data augmentation method (Section IV-C) can help improve the perception accuracy. We trained a perception module using 3000 simulated and 186 real images without data augmentation. It achieved an average error of 4.1 cm (\u00b13.2 cm), which is 28% larger\n2.9 '1.7 3.2 '1.3 3.7 '1.8 32.5 '16.1\n3.2 '1.7 3.7 '1.9 4.5 '4.0 31.2 '16.2\n6.9 '8.4 32.8 '23.9 33.3 '22.4 35.1 '21.8\n50.2 '23.0 44.6 '17.4 44.5 '18.9 NaN\nPP\nNumber of Real Images 0 340 750 3000\nN um\nbe r\nof S\nim ul\nat ed\nI m\nag es\n279\n186\n93\n0\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55 [cm]\nFig. 5. Object position estimation error map. The numbers in the map show the mean and standard deviation of the Euclidean distances between predicted and ground-truth positions. \u201cNaN\u201d means no result for that case.\nNumber of Trajectories 10 2 10 2.5 10 3 10 3.5E\nuc lid\nea n\nD is\nta nc\nE rr\nor d\n[ cm\n]\n0\n1\n2\n3\n4\n5\n6\n3.1 '2.6 80.0% (118)\nCC\n1.2 '0.8 100.0% (333)\n0.9 '0.6 100.0% (2964)\nFig. 6. Control performance curve which shows the mean and standard deviation of the Euclidean distances between the target and end-effector. Three control modules were evaluated in 15 trials. They were trained with different numbers of trajectories (the numbers in the brackets).\nthan PP. This shows that the data augmentation method did help improve the perception accuracy."}, {"heading": "B. Control Performance", "text": "To investigate how many trajectories are sufficient for training a control module, we evaluated three control modules trained with different control datasets which have varying numbers of trajectories: 118, 333, and 2964. As introduced in Section IV-C, the trajectories in each dataset were all collected in simulation (therefore cheap) for targets uniformly distributed in the operation area. In training, we used a mini-batch size of 64 and a learning rate decreasing from 0.01 to 0.001. The metrics of control error and success rate were used in the evaluation. Their performance in 15 real-world reaching trials are shown in Fig. 6. The 15 trials were for targets uniformly distributed in the operation area, with random initial left arm configurations (normally distributed around the reference configuration in Fig. 3B).\nFrom Fig. 6, we can see that a control module trained with more trajectories is able to achieve a better control performance in terms of both control error and success rate. The one trained with 118 trajectories has a success rate of 80%; the others are 100%. It also has a much larger control error than the other two. This indicates that 118 trajectories\nTABLE I END-TO-END CONTROL PERFORMANCE\nTest Condition Network Control Error [cm] Success Rate [%] Average Time Cost [step]\nSingle object (Fig. 7A) EE0: PP + CC 3.0\u00b11.5 80.0 39.6\nEE1: naively fine-tuned EE0 6.1\u00b13.8 33.3 61.2 EE2: EE0 fine-tuned using our approach 2.2\u00b11.2 93.3 29.1\nClutter with novel objects (Fig. 7B) EE2 2.4\u00b11.1 93.3 34.9With occlusions (Fig. 7C) 5.4\u00b13.8 46.7 43.1\nA B C DOcclusionNovel Objects\nFig. 7. Test cases for end-to-end performance. A: reaching the blue cuboid without distractor objects; B: reaching with seen and novel (not seen in training) objects as distractors; C: reaching with occlusion(s); D: reaching when the target is moving.\nare too few to get a good control module. The one trained with 2964 trajectories just achieved a slightly smaller control error (0.9 cm) than the one (1.2 cm) trained with much fewer trajectories (333). This shows that 333 trajectories are sufficient to get a reasonably good control module. Trading off the performance and number of trajectories, we pick the one trained with 333 trajectories to compose the network for end-to-end reaching in Section V-C, labelled as CC."}, {"heading": "C. Hand-eye Coordination", "text": "To further improve hand-eye coordination, we proposed an end-to-end fine-tuning approach using weighted losses. To evaluate the feasibility of the approach, we compare three combined networks:\n\u2022 EE0: composed by PP and CC, directly connected after separate training without end-to-end fine-tuning; \u2022 EE1: EE0 end-to-end fine-tuned naively, only using the control loss Lc; \u2022 EE2: EE0 fine-tuned using the proposed approach with weighted losses.\nIn the weighted end-to-end fine-tuning for EE2, \u03b2 = 0.9 (empirically determined), we used a learning rate of 0.01 and a mini-batch size of 8 and 64 for control and perception losses respectively. 30225 image-velocity pairs in the control dataset (333 trajectories) for CC were used in the fine-tuning to obtain \u03b4Lc ; its image-position pairs were used to obtain \u03b4Lp . To make sure that the perception module remembers the adaptation to the real scene, the 186 real images for PP were also used to obtain \u03b4Lp . In a mini-batch for \u03b4Lp , 87.5% samples were real ones, i.e., at each weight updating step, 56 real and 8 simulated images were used.\nIn the naive fine-tuning for EE1, we used the same learning rate and a mini-batch size of 64 for Lc. Similarly, 87.5% samples in a mini-batch were real ones, labelled with velocities obtained using the same method in Section IV-C.\nThe evaluation of fine-tuning was first done in the real world without distractor objects on the table as shown in Fig. 7A, using the metrics of control error, success rate and time cost. Their results in 15 real-world reaching trials are listed in Table I. The 15 trials were for the same targets and initial left arm configurations used in Section V-B.\nFrom the results for the single object case (Fig. 7A), we can see that, after the weighted end-to-end fine-tuning, EE2 achieved a better performance compared to EE0: 27% smaller control error; 17% higher success rate; 27% fewer average steps. In comparison, EE1 has a much worse performance than EE0. This shows that the proposed approach of end-to-end fine-tuning with weighted losses is able to significantly improve the performance of a combined network, but a naive approach could make the performance even worse.\nIn addition, EE2 even has a smaller control error than the perception error of PP. If we individually evaluate the perception module in EE2, its perception error increased from 3.2\u00b11.7 cm to 4.2\u00b13.5 cm. This indicates that the endto-end fine-tuning did improve the coordination between the perception and control modules (hand-eye coordination) in EE2, rather than improving them individually.\nTo further evaluate the performance of EE2 in more challenging cases with novel distractor objects in clutter and the target cuboid partially occluded, we conducted more experiments in settings as shown in Fig. 7B-C. From the results in Table I, we can see that EE2 achieved a comparable performance to that in Case A (Fig. 7A) even when there were multiple novel distractor objects in clutter (Fig. 7B). For the case with the target cuboid partially occluded (Fig. 7C), we observed a performance drop, but it was still able to reach half of the targets. We also tested EE2 in the case when the target cuboid was moving (Fig. 7D). It was able to adapt to target position changes in real time and performed well in most cases as shown in the attached video1."}, {"heading": "VI. DISCUSSION", "text": "This work leads us to the following observations: a) Value of a modular structure and end-to-end finetuning: The significant performance improvement between EE2 and EE0 after end-to-end fine-tuning with weighted losses shows the feasibility of the modular approach. Benefiting from the modular structure as well as domain randomization and adaptation, visuo-motor policies for a table-top reaching task can be learned and transferred from simulation to the real world with just 33225 simulated (including the 30225 ones for end-to-end fine-tuning) and 186 real samples, achieving a comparable performance to [8] (the reaching stage of the multi-stage task) but with fewer training data.\nThe modular approach can be used in more general ways. Although we explicitly equated the bottleneck layer with the target object position in this work, the bottleneck in general could be any explicit or latent low-dimensional features, like Auto-encoders. The perception and control modules can\n1The video is also on the project page https://goo.gl/P1c354.\nalso be trained with other methods such as unsupervised learning and reinforcement learning. The feasibility of the modular approach for reinforcement learning (DQN) has been validated in a planar reaching task [23], [28].\nb) Domain randomization and adaptation: In Section V-A, the perception module trained with 3000 simulated images has a large error (50.2\u00b123.0 cm), much higher than expected from [7]. Apart from the experiments in Section VA, we also trained a number of perception modules using simulated images with random RGB values for the table, floor and robot body rather than \u00b110% changes around the reference colors. However, this did not bring obvious accuracy improvement. Possible reasons include: too simple textures (only random RGB values); too simple randomization for light conditions; no simulated shadows; or sensitivity to domain randomization parameters and tuning.\nNevertheless, the adaptation with just a few real images (as few as 93) is able to transfer a network from simulation to the real world, and needs fewer simulated images to reach a comparable accuracy to [7]. The combination of domain randomization and adaptation is promising for more efficient deep neural network transfer."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we proposed a modular approach to learn and transfer visuo-motor policies from simulation to the real world through domain randomization and adaptation. The feasibility was demonstrated in the task of reaching a table-top object amongst clutter with a 7 DoF robotic arm in velocity mode. By using weighted losses to fine-tune a combined network in an end-to-end fashion, its performance was significantly improved (27%), achieving a success rate of 93.3% with an average control error of 2.2 cm. The learned policies are robust to novel distractor objects in clutter and even a moving target. The modular approach is promising for more efficient transfer of visuo-motor policies from simulation to the real world."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": "Australasian Conference on Robotics and Automation (ACRA), December 2015. [Online]. Available: https://arxiv.org/abs/1511.03791", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P.P. Sampedro", "A. Krizhevsky", "D. Quillen"], "venue": "International Symposium on Experimental Robotics (ISER), 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual servoing from deep neural networks", "author": ["Q. Bateux", "E. Marchand", "J. Leitner", "F. Chaumette", "P. Corke"], "venue": "New Frontiers for Deep Learning in Robotics Workshop at Robotics: Science and Systems Conference (RSS), 2017. [Online]. Available: https: //arxiv.org/abs/1705.08940", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Leveraging deep reinforcement learning for reaching robotic tasks", "author": ["K. Katyal", "I.-J. Wang", "P. Burli"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Bridging between computer and robot vision through data augmentation: a case study on object recognition", "author": ["A. D\u2019Innocente", "F.M. Carlucci", "M. Colosi", "B. Caputo"], "venue": "International Conference on Computer Vision Systems (ICVS), 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task", "author": ["S. James", "A.J. Davison", "E. Johns"], "venue": "1st Annual Conference on Robot Learning (CoRL), 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "A similarity-based approach to skill transfer", "author": ["T. Fitzgerald", "A. Goel", "A. Thomaz"], "venue": "Women in Robotics Workshop at Robotics: Science and Systems Conference (RSS), 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["E. Tzeng", "C. Devin", "J. Hoffman", "C. Finn", "P. Abbeel", "S. Levine", "K. Saenko", "T. Darrell"], "venue": "Workshop on the Algorithmic Foundations of Robotics (WAFR), 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "A deep-network solution towards modelless obstacle avoidance", "author": ["L. Tai", "S. Li", "M. Liu"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["I. Lenz", "R. Knepper", "A. Saxena"], "venue": "Robotics: Science and Systems (RSS), 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A robustness analysis of deep q networks", "author": ["A.W. Tow", "S. Shirazi", "J. Leitner", "N. S\u00fcnderhauf", "M. Milford", "B. Upcroft"], "venue": "Australasian Conference on Robotics and Automation (ACRA), 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research, vol. 17, no. 39, pp. 1\u201340, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a visuomotor controller for real world robotic grasping using simulated depth images", "author": ["U. Viereck", "A. t. Pas", "K. Saenko", "R. Platt"], "venue": "1st Annual Conference on Robot Learning (CoRL), 2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1633\u20131685, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "1st Annual Conference on Robot Learning (CoRL), 2017.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["C. Devin", "A. Gupta", "T. Darrell", "P. Abbeel", "S. Levine"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2017.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Modular deep q networks for sim-to-real transfer of visuo-motor policies", "author": ["F. Zhang", "J. Leitner", "M. Milford", "P. Corke"], "venue": "Queensland University of Technology, Tech. Rep., 2017. [Online]. Available: https://arxiv.org/abs/1610.06781", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "science, vol. 313, no. 5786, pp. 504\u2013 507, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "A theoretical framework for back-propagation", "author": ["Y. LeCun"], "venue": "Proceedings of the 1988 Connectionist Models Summer School, D. Touretzky, G. Hinton, and T. Sejnowski, Eds. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988, pp. 21\u201328.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1988}, {"title": "V-rep: A versatile and scalable robot simulation framework", "author": ["E. Rohmer", "S.P. Singh", "M. Freese"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 1321\u20131326.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Tuning modular networks with weighted losses for hand-eye coordination", "author": ["F. Zhang", "J. Leitner", "M. Milford", "P.I. Corke"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017. [Online]. Available: https://arxiv.org/abs/1705.05116", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Learning techniques have shown significant improvements in robustness and performance [1], particularly in the computer vision field.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Recently learning approaches to tackle this problem have been presented [2]\u2013[5], however a consistent issue faced by most approaches is the reliance on large amounts of data to train these models.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "Recently learning approaches to tackle this problem have been presented [2]\u2013[5], however a consistent issue faced by most approaches is the reliance on large amounts of data to train these models.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "For example, Google researchers addressed this problem by developing an \"arm farm\" with 6 to 14 robots collecting data in parallel [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 222, "endOffset": 225}, {"referenceID": 9, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 227, "endOffset": 231}, {"referenceID": 2, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 310, "endOffset": 313}, {"referenceID": 10, "context": "Various approaches have been proposed to address these problems in a robot learning context: (i) the use of simulators and synthetic data [4], [6]\u2013[8]; (ii) methods that transfer the learned models to real-world scenarios [9], [10]; (iii) directly learning real-world tasks by collecting large amounts of data [3], [11].", "startOffset": 315, "endOffset": 319}, {"referenceID": 11, "context": "tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest.", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest.", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "tasks \u2013 robotic tasks based directly on real image data \u2013 such as navigation [12], object grasping and manipulation [3], [11], [13] have seen increased interest.", "startOffset": 127, "endOffset": 131}, {"referenceID": 2, "context": "required for deep learning has been sped up by using many robots operating in parallel [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "Another example of dataset collection for grasping is the approach to self-supervised grasp learning in the real world where force sensors were used to autonomously label samples [11].", "startOffset": 179, "endOffset": 183}, {"referenceID": 13, "context": "DeepMind showed that a deep reinforcement learning system is able to synthesize control actions for computer games directly from vision data [14].", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "While this result is an important and exciting breakthrough it does not transfer directly to real robots with real cameras observing real scenes [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 14, "context": "Introducing a real camera observing the game screen was even worse [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "introduced a CNN-based policy representation architecture with an added guided policy search (GPS) to learn visuo-motor policies (from joint angles and camera images to joint torques) [16], which allows reduction in the number of real world training examples by providing an oracle (or expert\u2019s initial condition to start learning).", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "Recently it has been proposed to simulated depth images to learn and then transfer grasping skills to real-world robotic arms [17], yet no adaptation in the real-world has been performed.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "knowledge between different tasks (scenarios) [18], [19].", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": "knowledge between different tasks (scenarios) [18], [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Progressive neural networks are leveraged to improve transfer and avoid catastrophic forgetting when learning complex sequences of tasks [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "Modular reinforcement learning approaches have shown skill transfer capabilities in simulation [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "similaritybased approach to skill transfer for robots [9].", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "2% in a \u201chook loop\u201d task, with 10 times less real-world images [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Domain randomization has also been successfully used to transfer deep neural networks from simulation to the real world [7], [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "Domain randomization has also been successfully used to transfer deep neural networks from simulation to the real world [7], [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "Domain randomization has been successfully used to transfer deep neural networks from simulation to the real world for object position recognition [7] and also visuomotor control [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "Domain randomization has been successfully used to transfer deep neural networks from simulation to the real world for object position recognition [7] and also visuomotor control [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 7, "context": "In [8], auxiliary tasks such as cube pose estimation are used in training to improve performance in an end-to-end manner.", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": ", extracting useful information from visual inputs, while the fully connected (FC) layers perform control [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "The bottleneck forces the network to learn a low-dimensional representation, not unlike Auto-encoders [24].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "The values of x\u2217 and q are normalized to the interval [0, 1].", "startOffset": 54, "endOffset": 60}, {"referenceID": 21, "context": "The perception module architecture is customized from VGG16 [22] with its first convolutional layer initialized with weights from pre-trained VGG16.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "where L c is a pseudo-loss which reflects the loss of Lc in the bottleneck; \u03b2 \u2208 [0, 1] is a balancing weight.", "startOffset": 80, "endOffset": 86}, {"referenceID": 24, "context": "From the backpropagation algorithm [25], we can infer that \u03b4L = \u03b2\u03b4Lp + (1 \u2212 \u03b2)\u03b4LBN c , where \u03b4L is the gradients resulting from L; \u03b4Lp and \u03b4LBN c are the gradients resulting respectively from Lp and L c (equivalent to that resulting from Lc in the perception module).", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "The perception module has an architecture customized from VGG16 [22] for lower computational cost but without losing too much performance for this task.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "Networks with a first convolutional layer initialized with weights from pretrained VGG16 [22] are observed to converge faster and", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "Simulated images were collected from a V-REP simulator using domain randomization [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 25, "context": "The simulated data was collected using V-REP [26] (a robotic simulation platform) through domain randomization [7] in the following aspects:", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "The simulated data was collected using V-REP [26] (a robotic simulation platform) through domain randomization [7] in the following aspects:", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "\u2022 number of distractor objects in clutter: random in [0, 9]; \u2022 shape of distractor objects in clutter: random in 9 primitive shapes with different geometries (5 cuboids, 2 spheres, 2 cylinders); \u2022 pose of distractor objects: random position in the operation area and random orientation of the axis pointing upwards; \u2022 color of distractor objects: random RGB values; \u2022 left arm configuration: random in joint space, excluding the ones with self-collision; \u2022 color of the table, floor, robot body and target cuboid: random changes based on reference colors (\u00b110%); \u2022 camera pose: random changes of each right arm joint angle based on reference angles (\u00b11%);", "startOffset": 53, "endOffset": 59}, {"referenceID": 21, "context": "The training was from scratch, except that the first convolutional layer was initialized with weights from pre-trained VGG16 [22].", "startOffset": 125, "endOffset": 129}, {"referenceID": 26, "context": "RmsProp [27] was adopted, the same in the training of control modules (Section V-B) and end-to-end fine-tuning (Section V-C).", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "to the real world with just 33225 simulated (including the 30225 ones for end-to-end fine-tuning) and 186 real samples, achieving a comparable performance to [8] (the reaching stage of the multi-stage task) but with fewer training data.", "startOffset": 158, "endOffset": 161}, {"referenceID": 22, "context": "The feasibility of the modular approach for reinforcement learning (DQN) has been validated in a planar reaching task [23], [28].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "The feasibility of the modular approach for reinforcement learning (DQN) has been validated in a planar reaching task [23], [28].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "0 cm), much higher than expected from [7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Nevertheless, the adaptation with just a few real images (as few as 93) is able to transfer a network from simulation to the real world, and needs fewer simulated images to reach a comparable accuracy to [7].", "startOffset": 204, "endOffset": 207}], "year": 2017, "abstractText": "A modular method is proposed to learn and transfer visuo-motor policies from simulation to the real world in an efficient manner by combining domain randomization and adaptation. The feasibility of the approach is demonstrated in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The learned visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 93.3% success rate and 2.2 cm control accuracy.", "creator": "LaTeX with hyperref package"}}}