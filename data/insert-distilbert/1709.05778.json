{"id": "1709.05778", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model for Short Text Multi-class Classification Problems", "abstract": "the bag - of - words model is a standard representation of text for many linear classifier learners. in many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag - sizes of - words text representation can capture sufficient information for linear classifiers to make highly accurate predictions. ) however in settings where there is a large vocabulary, large variance in the dominant frequency of specific terms in the training corpus, many classes and very short text ( e. g., single sentences or document titles ) the bag - of - words representation becomes extremely sparse, and this too can reduce the accuracy of classifiers. a particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class - conditional evidence. in this work we introduce a method for enriching the bag - of - words model by complementing such rare term information with related terms from both general and domain - specific word vector models. by reducing sparseness in the bag - of - words models, our learning enrichment approach achieves improved classification over employing several traditional baseline classifiers in a variety of repetitive text classification problems. our approach is also efficient because it requires no change associated to the linear classifier before or during training, since bag - of - words enrichment applies only to text being classified.", "histories": [["v1", "Mon, 18 Sep 2017 05:00:34 GMT  (32kb,D)", "http://arxiv.org/abs/1709.05778v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["bradford heap", "michael bain", "wayne wobcke", "alfred krzywicki", "susanne schmeidl"], "accepted": false, "id": "1709.05778"}, "pdf": {"name": "1709.05778.pdf", "metadata": {"source": "META", "title": "Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model for Short Text Multi-class Classification Problems", "authors": ["Bradford Heap", "Michael Bain", "Wayne Wobcke", "Alfred Krzywicki", "Susanne Schmeidl"], "emails": ["b.heap@unsw.edu.au", "m.bain@unsw.edu.au", "w.wobcke@unsw.edu.au", "alfredk@unsw.edu.au", "s.schmeidl@unsw.edu.au"], "sections": [{"heading": "Introduction", "text": "The bag-of-words model is used as the standard representation of text input for many linear classification models (such as Multinomial Naive Bayes (McCallum and Nigam 1998) and Support Vector Machines (Joachims 1998)). Linear classifiers have been widely studied for many text classification problems (Sebastiani 2002), such as document classification or sentiment analysis, due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions (Dumais et al. 1998). However, in settings where there is a large vocabulary, a high number of classes (e.g., complex ontologies) and short text (e.g., fragments of text, single sentences or document titles) the bag-of-words representation contains extremely sparse data which reduces the accuracy of the linear classification models (Wang and Manning 2012).\nA particular problem with classification of short texts is low frequency (rare) words, or as an extreme, words that do not occur at all in the text used to train models, but do occur\nin test data. We have found that, despite the large volumes of text available on the Internet, low frequency words present a problem for classification in three different domains that are investigated in this paper: (i) Reuters news article classification, (ii) classification of journal article titles into medical ontologies, and (iii) classification of text snippets into \u201cconflict drivers\u201d for use in social science conflict analysis. These classification problems are also characterized by having a large number of categories (typically around 60 classes at a minimum), and a highly imbalanced distribution of instances over the classes. As there is limited data in the smaller classes, many supervised text classification methods are less accurate for these classes (Joachims 1997; Rennie et al. 2003). In addition, due to the specialized technical vocabulary used in the medical and social science fields, such classification problems are influenced by expert background knowledge not directly expressed in the text itself (Gabrilovich and Markovitch 2006). Moreover, in conflict analysis, the classification of events (to identify their significance in relation to a conflict) from official reports in which details are aggregated over time and location means that superficially similar texts may have very different classifications (Donnay, Gadjanova, and Bhavnani 2014).\nIn this work, we present an approach for increasing the predictive power of low frequency and out of vocabulary words by enriching the bag-of-words model with related terms found in a Word Vector model, trained on either a general or domain-specific corpus. Word Vector models build on the ideas of capturing word co-occurrence information and assume that contextually related words will often occur with a similar set of surrounding words (Bengio et al. 2003). These models are constructed using neural networks and project words into a dense vector space. Two words that are close to each other in this space, either in distance or cosine angle, often have semantic or syntatic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).\nOur method of bag-of-words enrichment assumes that words that occur rarely in the training data may have neighbours in the Word Vector space which are able to aid in the classification of the short text. Using the Word Vector model, we enrich the bag-of-words representation by including neighbouring words found in the Word Vector model. Capturing related terms allows our approach to increase the number of features which a linear model uses to make a clas-\nar X\niv :1\n70 9.\n05 77\n8v 1\n[ cs\n.C L\n] 1\n8 Se\np 20\n17\nsification prediction. Importantly, this approach can handle unseen words efficiently because it does not require retraining of the classification models during testing, as enrichment applies only to the text to be classified.\nOur empirical evaluation shows that this method produces increased classification accuracy in three domains. The first dataset used for evaluation is a subset of the benchmark Reuters-21578 dataset which considers the classification of short financial news articles. The second evaluation is performed on a subset of the benchmark OHSUMED dataset (Hersh et al. 1994) in which we consider the classification of medical articles which contain a title but no abstract. The third evaluation is performence on a dataset which classifies conflict drivers extracted from NGO analyst reports on the Democratic Republic of the Congo (DRC). In the light of the results, we explore the tradeoffs between the use of general and domain-specific corpora for training: general corpora with the advantage of a large amount of training data, but lacking domain-specific technical terms, and domainspecific corpora lacking quantity of data.\nThe paper is organized as follows. Starting with a brief description of the bag-of-words model and its limitations, and presenting various Word Vector models, the main part of the paper is a description of our approach to Word Vector enrichment of the bag-of words model, and the empirical evaluation. We conclude with a discussion of related work."}, {"heading": "Limitations of the Bag-of-Words Model", "text": "We define a text T to be a sequence of individual word tokens T = [t1, t2, . . .]. The bag-of-words model maps each text T to a large vector MT of dimension |V |, where the vocabulary V is the set of all tokens in a corpus of training data. In our setting, this labelled training data is an assignment of a single class label to each instance of a training set.\nThe bag-of-words vector MT contains the term frequency of each token from V that occurs in T , where the value of the vector is 0 for every token in V that does not occur in T (Scott and Matwin 1998; Wang and Domeniconi 2008). Of course, if T contains a token that does not occur in the vocabulary of the training corpus, there is no element of the vector corresponding to that token. Thus the bag-of-words model assumes that the presence and frequencies of tokens are important to the classification of text and the ordering of the words is not.\nAlthough the bag-of-words model is widely used and performs exceptionally well in many problem domains, it contains a number of limitations which are particularly problematic in short text classification problems. In our problem domain there are four key issues:\nVocabulary Size. The dimensionality of the bag-of-words vector is the number of distinct words in the vocabulary. For Support Vector Machines (SVM) this can result in a large training time due to the interaction between training instances and the dimensionality of the bag-of-words vector. In particular, as the number of training instances increases the number of unique words in the vocabulary is likely to increase. This in turn, then increases the dimensionality of the bag-of-words vector and, as the optimization to find the\nSVM hyperplane needs to be calculated across every dimension, the training time increases. In the Multinomial Naive Bayes (MNB) classifier, the size of the vocabulary is used to calculate each term\u2019s conditional probability. This produces very small conditional probabilities and can be particularly problematic in classes with a small class vocabulary or a small number of training examples.\nImbalanced Class Distribution. For the problems of short text classification that we have examined, the class distribution is highly uneven. This in turn biases many classifiers to prefer large classes, performing poorly on (the larger number) of smaller classes.\nRare and Out of Vocabulary Words The frequency distribution of word usage in typical text is extremely skewed (following Zipf\u2019s law). Words that occur only very infrequently do not give a learning algorithm sufficient information to determine their correct influence on classification. Words that do not occur in training data are discarded by the bag-of-words model as there is no knowledge of their classification.\nTerm Overlap Between Classes. In problems such as sentiment analysis or topic classification where there is a clear distinction between terms pertaining to certain classes, linear models using the bag-of-words representation work well. However, in problems with a large overlap between the terms used to describe two or more classes, accuracy can substantially decrease (Scott and Matwin 1998). For example, in social science conflict analysis, the words used in news reports classified as \u201ccondemning an attack\u201d have much common with those labelled \u201cmilitary attack\u201d.\nWord Vector Models Word co-occurrence information can be used to build models that capture the relations between words generally not captured in dictionary-based enrichment systems (Mandala, Takenobu, and Hozumi 1998). For example the proper noun \u201cMay\u201d and the title \u201cPrime Minister\u201d co-occur with high frequency in text related to the United Kingdom, but outside of this domain these two terms may not be as strongly related. Although co-occurrence models are able to capture relations in the same sentence they fail to capture synonymous terms, for example, the words \u201ctumor\u201d and \u201ctumour\u201d (Mandala, Takenobu, and Hozumi 1998).\nWord Vector models are an extension of the ideas of word co-occurrences and build on the idea that contextually related words often occur with a similar set of surrounding words (Bengio et al. 2003). Word Vector models are constructed by projecting all the words from a corpus into a dense vector space, for example, to reduce a bag-of-words vector with a dimension of 100,000 to a 100-dimensional \u201cdistributed feature vector\u201d. Two words that are close to each other in this space, either in distance or cosine angle, often share semantic or syntactic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).\nWord Vector models are constructed using neural networks (Bengio et al. 2003; Collobert and Weston 2008). Construction of the models is unsupervised and generally\ninvolves huge corpora of documents (e.g., Wikipedia (Collobert and Weston 2008) or Google News (Mikolov et al. 2013a)). In the training phase, the problem is typically to predict the next word in the text using the input of the previous n words in a sentence or document (Bengio et al. 2003; Le and Mikolov 2014). Word similarity is achieved in the intermediate layers of the neural network as a by-product. Combining Word Vector models with other machine learning approaches has shown promising results. In particular, Maas et al. (2011) used Word Vector models to discover additional features for sentiment analysis using SVM."}, {"heading": "Bag-of-Words Enrichment with Word Vectors", "text": "In this section we describe our approach for enriching the bag-of-words vector by leveraging information from contextually related words found in an unsupervised Word Vector model. This allows the bag-of-words model to contain more non-zero elements which provides more information to the linear classifiers for making predictions.\nThis process of enriching the bag-of-words model involves a number of steps which are shown in Fig. 1 and described below:\n1. Train or obtain an unsupervised Word Vector model W with dimension d that models the vocabulary and language of the sentences in the labelled or unlabelled data and/or any other domain related text. The Word Vector model must, for a given token t in the vocabulary, output a list of tokens related to t, ranked in order of similarity.\n2. For an unlabelled short text T and each token ti \u2208 T with a frequency in the labelled training data less than n (i.e., ti is a rare word), use the Word Vector model to find the k nearest neighbours of ti which occur with any frequency greater than zero in the labelled training data and construct a new bag-of-words vector Mti with these up to k elements assigned a term frequency of 1 and all other elements in the vector assigned the value 0.\n3. From the original bag-of-words vector MT , create an enriched bag-of-words vector MT+ by adding MT and each individual rare token\u2019s nearest neighbour bag-of-words vector Mti .\nMT+ = MT + \u2211 Mti (1)\nAs a result of this process, T is now classified using the enriched bag-of-words vector MT+ in lieu of the original bag-of-words vector MT .\nWe now work through each of these steps showing how the Word Vector model can be trained to capture contextually similar words and how this is particularly beneficial for\nwords with low frequency in the labelled training data. We are then able to show how enriched bag-of-words vector harnesses this additional information to address the previously outlined shortcomings of the bag-of-words model."}, {"heading": "1. Word Vector Training Algorithms", "text": "In this work, we train Word Vector models using Mikolov et al.\u2019s (2013a) Continuous Skip-gram Model as it addresses our requirement that the Word Vector Model captures both syntactically related and common co-occurring terms. This training algorithm differs from previous word embedding models, instead of using the previous and next n words to predict a missing word, only one word is used as input and the task is to predict the words which occur in sentences around the given input word. This approach captures the relationships between both co-occurring words and syntactically related words. For example the synonymous terms \u201cDaesh\u201d and \u201cISIS\u201d will both produce related words of \u201cfighters\u201d, \u201cterror\u201d and \u201cloyalists\u201d as in the skip-gram training model they will appear in a very similar area of the vector space. Furthermore, in a domain specific model the term \u201cTrump\u201d will produce the common co-occurring word \u201cPresident\u201d. The technical details and empirical evaluations of this approach have been presented in previous works (Mikolov et al. 2013a; 2013b)."}, {"heading": "2. Constructing Nearest Neighbour Bag-of-Words Models using Word Vectors", "text": "Once trained, the distance between individual tokens in a Word Vector model is agnostic to the distribution of word frequencies. This means that any two neighbouring terms in the Word Vector model share similarity with the words which they co-occurred with during the construction of the model independent of the number of training examples for each term. This can result in rare terms having high frequency terms as close neighbours. This close mapping between related terms allows us to find more frequently occurring terms for a given rare term. By capturing these more common related terms we can use their information in the model to aid in the classification of a short text.\nThe construction of a nearest neighbour bag-of-word model requires two hyper-parameters: the rare word frequency threshold n and the number of neighbouring words k to include from the Word Vector model. Suitable values for these parameters needs to be established through empirical evaluations (e.g., a grid search), however, both of these values depend on the labelled training data and not on the Word Vector model, and this allows the Word Vector model\nto be trained in isolation to the supervised model. Furthermore, as the Word Vector model training algorithm has no dependencies between training instances, the Word Vector model can be updated as new test instances are supplied to the model. This allows the Word Vector model to capture the relationships between words which may be out of the vocabulary in the model and map them to other words for which there is training data. This capturing of out of vocabulary words addresses one of the major issues with the classical bag-of-words model."}, {"heading": "3. Aggregation of the Nearest Neighbour", "text": "Bag-of-Words Vector As a final step before classifying the short text, we combine the original bag-of-words vector with the bag-of-word vectors constructed for each of the rare words in the original short text. This aggregation of the bag-of-word vectors has three benefits: 1) reinforcement of the term frequencies of any word which occurs in the original bag-of-words and then is a nearest neighbour of a rare word; 2) an increase in the number of non-zero elements in the enriched bag-of-words vector through the inclusion of neighbouring words; 3) out of vocabulary words (not contained in the labelled text) are able to contribute to the classification through their nearest neighbours found in the Word Vector model (when the Word Vector is trained on a larger corpus).\nThis approach addresses the classical bag-of-words model problems of: a) rare and out vocabulary words, and b) term overlap between classes as the combination of multiple related words should have a much stronger affinity to a narrow set of classes.\nAs an example, consider the classification of the following sentence \u201cOfficials said that supporters of Mullah Haibatullah Akhundzada\u2019s faction and Mullah Rasoul\u2019s supporters clashed in Shindand district of the province\u201d. Without any other context it is hard to determine if this clash is between rival political parties or militant groups (the terms \u201cofficials\u201d, \u201csupporters\u201d, and \u201cfaction\u201d commonly describe both). To illustrate how word vector enrichment can be used to classify this sentence, suppose the name \u201cHaibatullah\u201d is a rare term (it only occurs three times in the training data). Using a domain specific Word Vector model, the neighbouring words of \u201cHaibatullah\u201d are \u201cMullah\u201d and \u201cOmar\u201d. The bag-of-words vector for the original sentence is then enriched with these terms. As \u201cMullah Omar\u201d is a commonly occurring name in the training data,1 the classifier has more information to determine the correct label of the sentence."}, {"heading": "Short Text Classification Models", "text": "Linear classification models are able to use the bag-of-words model to make predictions about the classification of text. In this work we focus on two widely used classification models, Multinomial Naive Bayes (MNB) and Support Vector Machines (SVM). These two models have been widely established as baselines methods for text classification across many domains (Wang and Manning 2012).\n1Haibatullah is the current leader of the Taliban (since May 2016) and Mullah Omar is a former leader.\nMultinomial Naive Bayes. These classifiers apply the Bayes rule over a collection of labelled training examples to estimate the probability of a given text belonging to a particular class. Formally the classifier is defined as:\nCNB = argmax c\u2208C P (c) \u03a0 ti\u2208T P (ti|c) (2)\nwhere P (c), the prior probability, is the probability of a given class out of the set of all classes in the training set, without any other knowledge about the contents of the text to be classified. The conditional probability of a given token for a particular class P (ti|c) is calculated using Laplace smoothing to avoid zero probabilities.\nSupport Vector Machines. These classification models are trained to find a hyperplane separating two classes in a high dimensional space. The classification of an unlabelled text is then decided by which side of the hyperplane the features (tokens) of the text fall. In problem domains with many class labels, a single class classification requires the construction of SVM models for every pair of classes. A text is then classified by every classification pair and the class which the text is classified into most often (Hastie and Tibshirani 1998).\nIt is straightforward to see how both the MNB and SVM classifiers are to use this enriched bag-of-words model to make classification decisions which utilize more of the training information. In particular, the generative MNB model is now able to use more non-zero elements of the bag-of-words vector to calculate a classification probability. The discriminative SVM model is able to use the enriched bag-of-words vector to more confidently decide which side of the hyperplane the short text should be classified on."}, {"heading": "Neural Network Classifiers", "text": "We compare the linear classifiers with two recently developed neural network models for text classification which use data from a pretrained Word Vector as input into a further neural network model.\nParagraph Vectors. Paragraph Vectors (also called doc2vec) are an extension of Mikolov et al.\u2019s (2013a) word2vec word embedding training algorithm. The Paragraph Vectors model is designed to overcome the problems of the loss of word ordering information in bag-of-words models (Le and Mikolov 2014). The primary change is during training class label features (\u201cParagraph IDs\u201d) are inserted into the same dense vector space as the words. After training, classification of text is performed by calculating the mean vector of the words in a text to be classified, and then the text is labelled with the Paragraph ID feature nearest to the mean vector. Recent analysis has shown that replicating the results of the original Paragraph Vectors paper has been difficult due to the nature of tuning the model\u2019s hyperparameters (Lau and Baldwin 2016).\nConvolutional Neural Networks. Kim (2014) uses pretrained Word Vector models as input into a one layer Convolutional Neural Network (CNN). This approach learns sequences of words which is otherwise lost in using the standard bag-of-words model. This work showed that a CNN\ncould be used to classify text in sentiment analysis and question classification benchmark problems. However, subsequent work has shown that tuning the large number of hyperparameters is difficult and often impractical due to the computational cost (Zhang and Wallace 2015).\nFor the neural network based Paragraph Vector model, including additional words during classification will change the mean word vector for a given input sentence. As this change only affects the text to be classified, it is possible that the nearest Paragraph ID to the mean may differ from that of the original text. Because the two dimensional structural patterns identified by the CNN are attuned to word adjacency in the input inserting additional words into the middle of the text to be classified will have a detrimental effect on the recognition of such patterns. Due to the behaviour of the training and classification algorithms of both neural network based classifiers our empirical evaluations show that our enrichment method does not improve the classification accuracy for these neural network models."}, {"heading": "Evaluation", "text": "Our evaluation over the three datasets is setup as a 10x10fold cross validation. For each dataset we train a domain specific Word Vector model and use it for enrichment in each evaluation fold.2 We set the hyper-parameters of each Word Vector model to a dimension d = 100, a training window size of 10, a word frequency of at least two and train for 10 epochs. We train two linear model classifiers, the first is the standard MNB model and the second is the SVM model which we train using Weka\u2019s3 implementation of the SMO training algorithm (with hyper-parameter C = 1). We use DeepLearning4J\u2019s implementation of Paragraph Vectors and Kim\u2019s CNN to train our neural network based classifiers. All classification models are configured to always produce a single classification prediction for a given input.\nOur evaluation setting is a single label, multi-class classification problem where every test instance is assigned one class label. In this setting, recall is a suitable metric to measure the performance of our algorithms. Suppose there are m classes C1, \u00b7 \u00b7 \u00b7 , Cm, and that C = C1 \u222a \u00b7 \u00b7 \u00b7 \u222a Cm is the set of all instances. The recall of a single class Ci (class recall) is defined as the number of correct classifications from Ci (true positives) for this class tpi divided by the number of instances in the class |Ci|, assumed non-zero. Then micro recall is defined as the recall over the aggregated set of classes, or equivalently, as a weighted sum of the class recall measures, as follows:\nmicro recall = \u2211m\ni=1 tpi |C| = m\u2211 i=1 tpi |Ci| |Ci| |C|\n(3)\nNote that recall is equal to precision (and F1) in this setting because there is no need to distinguish the relevant and irrelevant instances over the whole set; more precisely, every instance of Ci that is not classified correctly (a false negative\n2We use DeepLearning4J\u2019s implementation of Mikolov\u2019s Continuous Skip-gram algorithm (https://deeplearning4j.org/).\n3http://www.cs.waikato.ac.nz/ml/weka/\nfor Ci) is a misclassification in another class (a false positive for that class). Similarly, micro recall is equal to accuracy.\nThe reason to focus on recall is that, as our approach is designed to enrich classes with few training examples, we measure this effect using macro recall, defined as the mean of the individual class recall measures:\nmacro recall = 1\nm m\u2211 i=1 tpi |Ci|\n(4)\nIn this metric, each class recall is weighted equally in the calculation; with micro recall, each class recall is weighted by the proportion of instances in that class (as in the second formulation above), hence favours the larger classes.\nFor each dataset and classification model we tune the hyper-parameters for the label frequency n and the number of additional words from the Word Vector model k using a grid search over the first fold of the first evaluation set. Finally, we use the non-parametric Wilcoxon Signed-Ranked test for statistical significance to measure the benefit of our method in the linear classifiers.\nReuters-21578 Dataset This corpus contains news reports from 1987 and is a benchmark dataset widley used in many previous text categorisation evaluations. For our evaluation on short text we extract a subset of the reports where the length of the article body is 100 words or fewer (approximately two sentences). We exclude any article belonging to the \u2018earn\u2019 class as these documents are share ticker information and do not form full sentences. This subset dataset contains 3,003 articles labelled by 93 categories. The largest class \u2018acq\u2019 contains 1,376 documents, followed by the \u2018money-fx\u2019 class with 291 documents, 73 classes have fewer than 30 documents. This subset dataset contains 7,213 unique words and the mean length of each document is 70 words. We train the Word Vector model used for enrichment over the full Reuters-21578 corpus.\nTable 1 shows the results of our evaluation using the full document text without any pre-processing. These results show that at the baseline level the SVM classifier produces the highest recall. The MNB results are the lowest across all four classification models, this is likely a result of the skew in the data having a large influence on the prior probability component of the MNB model and therefore it is more likely to make classifications into the largest classes. These results show that using a Word Vector model to enrich the bag-ofwords representation improves both micro and macro recall results of the MNB and SVM classifiers. For both of these linear classifiers the Wilcoxon Signed-Ranked test shows statistical significance of p < 0.000005 in improving recall.\nIn contrast the neural network classifiers have reduced micro recall and almost no change in macro recall with enrichment. As discussed earlier, the lack of improvement for these classification algorithms through the inclusion of additional words is likely related to the structural representation of the text as a single geometric mean of all the words used in the Paragraph Vector model and word ordering patterns in the CNN model. These approaches assume the structure of the input data is very similar to their training data and their treatment of the sequence of words is very different to the linear\nclassifiers which treat each word independently. Due to the complexity of the neural network models there is a large difference in the time required to train and evaluate each classification algorithm. The Word Vector model used for enrichment across all the models takes 10 minutes to train. The quickest classification model is the MNB model which performs the full 10x10-fold cross validation in under 10 minutes. The SVM model takes three hours to build a model and evaluate each fold. The neural network classifiers are much slower, Paragraph Vectors requires a day to train and evaluate and the CNN model takes three days. Furthermore, a large amount of time is required to tune the hyperparameters of the Paragraph Vectors and CNN models which are both very sensitive to small changes. Although some of this variance in performance is explained by the implementation of each model, due to the computational time and the lack of improvement using Paragraph Vector and CNN models we limit further evaluation to only the linear classifiers.\nIn the above evaluation we trained a domain specific Word Vector model using the full Reuters-21578 dataset. The benefit of the domain specific Word Vector model is that it should be expected that words which have close domain specific similarities are modelled better than a general model trained over a much larger corpus of text. To evaluate this hypothesis we performed a second evaluation of the Reuters dataset which used Mikolov\u2019s Google News pretrained Word Vector model for enrichment. This pre-trained model was trained over 100 billion English words containing three million unique words and phrases. Table 2 presents the results of this evaluation for the linear classifiers. These results show that the large general Google News model can also be used with our algorithm to enrich Bag-of-words vectors and improve the recall of these classifiers. Although these results are statistically significant (p < 0.05), the error reduction is much smaller than the previous results, which is likely a result of the additional words having less relevance to the financial news domain.\nOHSUMED Dataset Our second evaluation considered the evaluation of medical academic article references contained in the OHSUMED dataset (Hersh et al. 1994). This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008). We consider a subset of this\ndataset that contains references which only contain a title and no abstract. This particular problem setting has previously been considered by Gabrilovich and Markovitch (2006) who used Wikipedia to enrich an SVM classifier.\nThe full OHSUMED dataset contains 348,566 references from MEDLINE covering 270 medical journals from the period 1987-1991. Each article reference in this dataset is tagged with multiple human assigned MeSH indexing terms. We extract a subset of this dataset relating to the article references which have been assigned to either the first or second level of 21 MeSH Disease Categories. This produces a dataset of 44,479 references of which 12,752 references only contain a title with no abstract. We perform a 10x10fold cross validation on this set of 12,752 references with the task of predicting which of the 21 Mesh Disease Categories each article reference belongs to. On average each article reference is labelled by 1.1 categories, however, to simplify this problem we configure out classifiers to only produce a single classification label and we consider an article reference to be correctly classified if the linear model correct predicts one of its correct labels. This subset dataset contains 5,899 unique words and the mean length of each title is only 9 words. For our Word Vector enrichment process we train Word Vector models on the larger dataset of 44,479 article references using both the title and the abstract text (where available) under the same hyper-parameter configuration as the Reuters dataset.\nTable 3 shows the mean results of the cross validation where the full text of each title is used as input to the linear classifier. The baseline results for the SVM classifier again shows much higher performance than the baseline MNB result. When we enrich the bag-of-words model, we get a gain in both micro and macro recall for the MNB classifier. For the SVM classifier we get a much smaller gain in micro recall, however, the results are statistically significant (p < 0.0005).\nICG DRC Dataset This dataset contains text snippets extracted from 15 International Crisis Group (ICG) reports on the DRC during the period 2002\u20132006. To construct the dataset, a domain expert read 8,836 sentences across the 15 reports, extracted 2,159 text snippets which were then each given one of 64 class labels. This dataset contains 3,366 unique words and the mean length of each text snippet is 25\nwords. Previous work has shown this dataset is very challenging for state of the art classification algorithms (Heap et al. 2017). We train a domain specific Word Vector model over the full text of all the sentences in the 15 ICG reports.\nTable 4 shows that our Word Vector enrichment approach is able to improve the recall of both the MNB and SVM classifiers. All of these results are statistically significant (p < 0.00005). However, despite the improvement with the enrichment process, the linear classifiers still perform very poorly on this dataset. This is reflective of the difficultly in using this dataset for fully automated machine coding (Heap et al. 2017). An alternative treatment of this dataset is to configure the classification algorithms to produce a ranked top3 predictions of the class label which a human \u201ccoder\u201d can then select from (cf. (Larkey and Croft 1996)). The results of configuring the classifiers in this manner are presented in Table 5 and show that the enrichment method performs very strongly in this type of classification problem.\nOverall, our evaluation across three domain specific datasets has demonstrated that our bag-of-words enrichment with Word Vectors process improves the recall of linear classifiers in multi-class short text classification problems. A key finding in this work has been that using a small domain specific Word Vector model for enrichment is more effective at improving recall than a much larger general model which has been used in many previous works. A further benefit of this process is minimal computation required to enrich texts, this is a consequence of the process only enriching unlabelled texts and therefore requires no retraining of existing classification models."}, {"heading": "Related Work", "text": "A number of approaches for transforming the bag-of-words model or modifying linear models to increase the classification accuracy have been proposed in previous works. These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to\nidentify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al. 1998); 4) Using dictionaries (Mavroeidis et al. 2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms. However, the names of people and organisations are unlikely to occur in dictionaries (Luo, Chen, and Xiong 2011; Mansuy and Hilderman 2006) and may be used rarely in training text.\nThese text transformation techniques can have a large influence over the accuracy of the classification models (Dumais et al. 1998; Yang and Pedersen 1997). However, the effectiveness of each method can vastly vary depending on the classification domain and problem, the dataset and the linear classifier. As such, there is no generally agreed best practice approach for addressing these issues. With regards to short text classification, any approach which reduces the number of words in the dataset or limits classes to their most frequent/relevant words are likely to reduce the classification accuracy as many words will lose any ability to contribute to the classification of a text."}, {"heading": "Conclusion", "text": "In this paper we developed a method of enriching bag-ofwords representations for short text classification. We use a Word Vector model trained on unlabelled domain-related text to capture semantic and syntactic relations between words. With the Word Vector model we locate words in the vector space which may occur rarely in a set of labelled training examples, then use the neighbour words to enrich the bag-of-words vector to be classified. This enriched representation allows linear classifiers to use the class information of more words to enhance classification prediction. A key benefit of the approach is that it does not require any change to the training of linear classification models. Combining the information in the unsupervised Word Vector model with a supervised linear model improves micro and macro recall over baseline on difficult multi-class problems, although the potential of CNNs for these should be investigated further."}, {"heading": "Acknowledgment", "text": "This work was supported by Data to Decisions Cooperative Research Centre. We are grateful to Josie Gardner for labelling the ICG DRC dataset."}], "references": [{"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of Machine Learning Research 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R. Collobert", "J. Weston"], "venue": "International Conference on Machine Learning, 160\u2013167.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Disaggregating Conflict by Actors, Time, and Location", "author": ["K. Donnay", "E. Gadjanova", "R. Bhavnani"], "venue": "Backer, D. amd Huth, P., and Wilkenfeld, J., eds., Peace and Conflict 2014. Boulder, CO: Paradigm. 44 \u2013 56.", "citeRegEx": "Donnay et al\\.,? 2014", "shortCiteRegEx": "Donnay et al\\.", "year": 2014}, {"title": "Inductive Learning Algorithms and Representations for Text Categorization", "author": ["S.T. Dumais", "J.C. Platt", "D. Hecherman", "M. Sahami"], "venue": "International Conference on Information and Knowledge Management, 148\u2013155.", "citeRegEx": "Dumais et al\\.,? 1998", "shortCiteRegEx": "Dumais et al\\.", "year": 1998}, {"title": "Overcoming the Brittleness Bottleneck using Wikipedia: Enhancing Text Categorization with Encyclopedic Knowledge", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "National Conference on Artificial Intelligence, 1301\u20131306.", "citeRegEx": "Gabrilovich and Markovitch,? 2006", "shortCiteRegEx": "Gabrilovich and Markovitch", "year": 2006}, {"title": "Classification by Pairwise Coupling", "author": ["T. Hastie", "R. Tibshirani"], "venue": "The Annals of Statistics 26(2):457\u2013471.", "citeRegEx": "Hastie and Tibshirani,? 1998", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1998}, {"title": "A Joint Human/Machine Process for Coding Events and Conflict Drivers", "author": ["B. Heap", "A. Krzywicki", "S. Schmeidl", "W. Wobcke", "M. Bain"], "venue": "International Conference on Advanced Data Mining and Applications (in press).", "citeRegEx": "Heap et al\\.,? 2017", "shortCiteRegEx": "Heap et al\\.", "year": 2017}, {"title": "OHSUMED: An Interactive Retrieval Evaluation and New Large Test Collection for Research", "author": ["W.R. Hersh", "C. Buckley", "T.J. Leone", "D.H. Hickam"], "venue": "SIGIR, 192\u2013 201.", "citeRegEx": "Hersh et al\\.,? 1994", "shortCiteRegEx": "Hersh et al\\.", "year": 1994}, {"title": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization", "author": ["T. Joachims"], "venue": "International Conference on Machine Learning, 143\u2013151.", "citeRegEx": "Joachims,? 1997", "shortCiteRegEx": "Joachims", "year": 1997}, {"title": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features", "author": ["T. Joachims"], "venue": "ECML-98, 137\u2013142.", "citeRegEx": "Joachims,? 1998", "shortCiteRegEx": "Joachims", "year": 1998}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Y. Kim"], "venue": "EMNLP, 1746\u20131751.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Combining Classifiers in Text Categorization", "author": ["L.S. Larkey", "W.B. Croft"], "venue": "SIGIR, 289\u2013297.", "citeRegEx": "Larkey and Croft,? 1996", "shortCiteRegEx": "Larkey and Croft", "year": 1996}, {"title": "An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation", "author": ["J.H. Lau", "T. Baldwin"], "venue": "Workshop on Representation Learning for NLP, 78\u201386.", "citeRegEx": "Lau and Baldwin,? 2016", "shortCiteRegEx": "Lau and Baldwin", "year": 2016}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "International Conference on Machine Learning, volume 32, 1188\u20131196.", "citeRegEx": "Le and Mikolov,? 2014", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "A semantic term weighting scheme for text categorization", "author": ["Q. Luo", "E. Chen", "H. Xiong"], "venue": "Expert Systems with Applications 38(10):12708\u201312716.", "citeRegEx": "Luo et al\\.,? 2011", "shortCiteRegEx": "Luo et al\\.", "year": 2011}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 142\u2013 150.", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "The Use of WordNet in Information Retrieval", "author": ["R. Mandala", "T. Takenobu", "T. Hozumi"], "venue": "Workshop on Usage of WordNet in Natural Language Processing Systems, 31\u2013", "citeRegEx": "Mandala et al\\.,? 1998", "shortCiteRegEx": "Mandala et al\\.", "year": 1998}, {"title": "A Characterization of WordNet Features in Boolean Models For Text Classification", "author": ["T.N. Mansuy", "R.J. Hilderman"], "venue": "Australasian Data Mining Conference, 103\u2013 109.", "citeRegEx": "Mansuy and Hilderman,? 2006", "shortCiteRegEx": "Mansuy and Hilderman", "year": 2006}, {"title": "Word Sense Disambiguation for Exploiting Hierarchical Thesauri in Text Classification", "author": ["D. Mavroeidis", "G. Tsatsaronis", "M. Vazirgiannis", "M. Theobald", "G. Weikum"], "venue": "PKDD, 181\u2013192.", "citeRegEx": "Mavroeidis et al\\.,? 2005", "shortCiteRegEx": "Mavroeidis et al\\.", "year": 2005}, {"title": "A Comparison of Event Models for Naive Bayes Text Classification", "author": ["A. McCallum", "K. Nigam"], "venue": "AAAI-98 Workshop on Learning for Text Categorization, volume 752, 41\u201348.", "citeRegEx": "McCallum and Nigam,? 1998", "shortCiteRegEx": "McCallum and Nigam", "year": 1998}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, volume 26, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program 14(3):130\u2013137.", "citeRegEx": "Porter,? 1980", "shortCiteRegEx": "Porter", "year": 1980}, {"title": "Tackling the Poor Assumptions of Naive Bayes Text Classifiers", "author": ["J.D. Rennie", "L. Shih", "J. Teevan", "D.R. Karger"], "venue": "International Conference on International Conference on Machine Learning, 616\u2013623.", "citeRegEx": "Rennie et al\\.,? 2003", "shortCiteRegEx": "Rennie et al\\.", "year": 2003}, {"title": "Text Classification Using WordNet Hypernyms", "author": ["S. Scott", "S. Matwin"], "venue": "Workshop on Usage of WordNet in Natural Language Processing Systems, 45\u201352.", "citeRegEx": "Scott and Matwin,? 1998", "shortCiteRegEx": "Scott and Matwin", "year": 1998}, {"title": "Machine Learning in Automated Text Categorization", "author": ["F. Sebastiani"], "venue": "ACM Computing Surveys 34(1):1\u201347.", "citeRegEx": "Sebastiani,? 2002", "shortCiteRegEx": "Sebastiani", "year": 2002}, {"title": "WikiRelate! Computing Semantic Relatedness Using Wikipedia", "author": ["M. Strube", "S.P. Ponzetto"], "venue": "National Conference on Artificial intelligence, 1419\u20131424.", "citeRegEx": "Strube and Ponzetto,? 2006", "shortCiteRegEx": "Strube and Ponzetto", "year": 2006}, {"title": "Building Semantic Kernels for Text Classification using Wikipedia", "author": ["P. Wang", "C. Domeniconi"], "venue": "ACM SIGKDD, 713\u2013721.", "citeRegEx": "Wang and Domeniconi,? 2008", "shortCiteRegEx": "Wang and Domeniconi", "year": 2008}, {"title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, 90\u201394.", "citeRegEx": "Wang and Manning,? 2012", "shortCiteRegEx": "Wang and Manning", "year": 2012}, {"title": "A Comparative Study on Feature Selection in Text Categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "International Conference on Machine Learning, 412\u2013420.", "citeRegEx": "Yang and Pedersen,? 1997", "shortCiteRegEx": "Yang and Pedersen", "year": 1997}, {"title": "A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification", "author": ["Y. Zhang", "B.C. Wallace"], "venue": "CoRR abs/1510.03820.", "citeRegEx": "Zhang and Wallace,? 2015", "shortCiteRegEx": "Zhang and Wallace", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "The bag-of-words model is used as the standard representation of text input for many linear classification models (such as Multinomial Naive Bayes (McCallum and Nigam 1998) and Support Vector Machines (Joachims 1998)).", "startOffset": 147, "endOffset": 172}, {"referenceID": 9, "context": "The bag-of-words model is used as the standard representation of text input for many linear classification models (such as Multinomial Naive Bayes (McCallum and Nigam 1998) and Support Vector Machines (Joachims 1998)).", "startOffset": 201, "endOffset": 216}, {"referenceID": 25, "context": "Linear classifiers have been widely studied for many text classification problems (Sebastiani 2002), such as document classification or sentiment analysis, due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions (Dumais et al.", "startOffset": 82, "endOffset": 99}, {"referenceID": 3, "context": "Linear classifiers have been widely studied for many text classification problems (Sebastiani 2002), such as document classification or sentiment analysis, due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions (Dumais et al. 1998).", "startOffset": 349, "endOffset": 369}, {"referenceID": 28, "context": ", fragments of text, single sentences or document titles) the bag-of-words representation contains extremely sparse data which reduces the accuracy of the linear classification models (Wang and Manning 2012).", "startOffset": 184, "endOffset": 207}, {"referenceID": 8, "context": "As there is limited data in the smaller classes, many supervised text classification methods are less accurate for these classes (Joachims 1997; Rennie et al. 2003).", "startOffset": 129, "endOffset": 164}, {"referenceID": 23, "context": "As there is limited data in the smaller classes, many supervised text classification methods are less accurate for these classes (Joachims 1997; Rennie et al. 2003).", "startOffset": 129, "endOffset": 164}, {"referenceID": 4, "context": "In addition, due to the specialized technical vocabulary used in the medical and social science fields, such classification problems are influenced by expert background knowledge not directly expressed in the text itself (Gabrilovich and Markovitch 2006).", "startOffset": 221, "endOffset": 254}, {"referenceID": 0, "context": "Word Vector models build on the ideas of capturing word co-occurrence information and assume that contextually related words will often occur with a similar set of surrounding words (Bengio et al. 2003).", "startOffset": 182, "endOffset": 202}, {"referenceID": 1, "context": "Two words that are close to each other in this space, either in distance or cosine angle, often have semantic or syntatic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).", "startOffset": 133, "endOffset": 182}, {"referenceID": 20, "context": "Two words that are close to each other in this space, either in distance or cosine angle, often have semantic or syntatic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).", "startOffset": 133, "endOffset": 182}, {"referenceID": 7, "context": "The second evaluation is performed on a subset of the benchmark OHSUMED dataset (Hersh et al. 1994) in which we consider the classification of medical articles which contain a title but no abstract.", "startOffset": 80, "endOffset": 99}, {"referenceID": 24, "context": "The bag-of-words vector MT contains the term frequency of each token from V that occurs in T , where the value of the vector is 0 for every token in V that does not occur in T (Scott and Matwin 1998; Wang and Domeniconi 2008).", "startOffset": 176, "endOffset": 225}, {"referenceID": 27, "context": "The bag-of-words vector MT contains the term frequency of each token from V that occurs in T , where the value of the vector is 0 for every token in V that does not occur in T (Scott and Matwin 1998; Wang and Domeniconi 2008).", "startOffset": 176, "endOffset": 225}, {"referenceID": 24, "context": "However, in problems with a large overlap between the terms used to describe two or more classes, accuracy can substantially decrease (Scott and Matwin 1998).", "startOffset": 134, "endOffset": 157}, {"referenceID": 0, "context": "Word Vector models are an extension of the ideas of word co-occurrences and build on the idea that contextually related words often occur with a similar set of surrounding words (Bengio et al. 2003).", "startOffset": 178, "endOffset": 198}, {"referenceID": 1, "context": "Two words that are close to each other in this space, either in distance or cosine angle, often share semantic or syntactic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).", "startOffset": 135, "endOffset": 184}, {"referenceID": 20, "context": "Two words that are close to each other in this space, either in distance or cosine angle, often share semantic or syntactic similarity (Collobert and Weston 2008; Mikolov et al. 2013a).", "startOffset": 135, "endOffset": 184}, {"referenceID": 0, "context": "Word Vector models are constructed using neural networks (Bengio et al. 2003; Collobert and Weston 2008).", "startOffset": 57, "endOffset": 104}, {"referenceID": 1, "context": "Word Vector models are constructed using neural networks (Bengio et al. 2003; Collobert and Weston 2008).", "startOffset": 57, "endOffset": 104}, {"referenceID": 1, "context": ", Wikipedia (Collobert and Weston 2008) or Google News (Mikolov et al.", "startOffset": 12, "endOffset": 39}, {"referenceID": 20, "context": ", Wikipedia (Collobert and Weston 2008) or Google News (Mikolov et al. 2013a)).", "startOffset": 55, "endOffset": 77}, {"referenceID": 0, "context": "In the training phase, the problem is typically to predict the next word in the text using the input of the previous n words in a sentence or document (Bengio et al. 2003; Le and Mikolov 2014).", "startOffset": 151, "endOffset": 192}, {"referenceID": 13, "context": "In the training phase, the problem is typically to predict the next word in the text using the input of the previous n words in a sentence or document (Bengio et al. 2003; Le and Mikolov 2014).", "startOffset": 151, "endOffset": 192}, {"referenceID": 0, "context": "In the training phase, the problem is typically to predict the next word in the text using the input of the previous n words in a sentence or document (Bengio et al. 2003; Le and Mikolov 2014). Word similarity is achieved in the intermediate layers of the neural network as a by-product. Combining Word Vector models with other machine learning approaches has shown promising results. In particular, Maas et al. (2011) used Word Vector models to discover additional features for sentiment analysis using SVM.", "startOffset": 152, "endOffset": 419}, {"referenceID": 20, "context": "The technical details and empirical evaluations of this approach have been presented in previous works (Mikolov et al. 2013a; 2013b).", "startOffset": 103, "endOffset": 132}, {"referenceID": 20, "context": "In this work, we train Word Vector models using Mikolov et al.\u2019s (2013a) Continuous Skip-gram Model as it addresses our requirement that the Word Vector Model captures both syntactically related and common co-occurring terms.", "startOffset": 48, "endOffset": 73}, {"referenceID": 28, "context": "These two models have been widely established as baselines methods for text classification across many domains (Wang and Manning 2012).", "startOffset": 111, "endOffset": 134}, {"referenceID": 5, "context": "A text is then classified by every classification pair and the class which the text is classified into most often (Hastie and Tibshirani 1998).", "startOffset": 114, "endOffset": 142}, {"referenceID": 13, "context": "The Paragraph Vectors model is designed to overcome the problems of the loss of word ordering information in bag-of-words models (Le and Mikolov 2014).", "startOffset": 129, "endOffset": 150}, {"referenceID": 12, "context": "Recent analysis has shown that replicating the results of the original Paragraph Vectors paper has been difficult due to the nature of tuning the model\u2019s hyperparameters (Lau and Baldwin 2016).", "startOffset": 170, "endOffset": 192}, {"referenceID": 18, "context": "Paragraph Vectors (also called doc2vec) are an extension of Mikolov et al.\u2019s (2013a) word2vec word embedding training algorithm.", "startOffset": 60, "endOffset": 85}, {"referenceID": 10, "context": "Kim (2014) uses pretrained Word Vector models as input into a one layer Convolutional Neural Network (CNN).", "startOffset": 0, "endOffset": 11}, {"referenceID": 30, "context": "However, subsequent work has shown that tuning the large number of hyperparameters is difficult and often impractical due to the computational cost (Zhang and Wallace 2015).", "startOffset": 148, "endOffset": 172}, {"referenceID": 7, "context": "OHSUMED Dataset Our second evaluation considered the evaluation of medical academic article references contained in the OHSUMED dataset (Hersh et al. 1994).", "startOffset": 136, "endOffset": 155}, {"referenceID": 9, "context": "This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008).", "startOffset": 59, "endOffset": 133}, {"referenceID": 4, "context": "This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008).", "startOffset": 59, "endOffset": 133}, {"referenceID": 27, "context": "This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008).", "startOffset": 59, "endOffset": 133}, {"referenceID": 4, "context": "This dataset has been widely used in previous publications (Joachims 1998; Gabrilovich and Markovitch 2006; Wang and Domeniconi 2008). We consider a subset of this dataset that contains references which only contain a title and no abstract. This particular problem setting has previously been considered by Gabrilovich and Markovitch (2006) who used Wikipedia to enrich an SVM classifier.", "startOffset": 75, "endOffset": 341}, {"referenceID": 6, "context": "Previous work has shown this dataset is very challenging for state of the art classification algorithms (Heap et al. 2017).", "startOffset": 104, "endOffset": 122}, {"referenceID": 6, "context": "This is reflective of the difficultly in using this dataset for fully automated machine coding (Heap et al. 2017).", "startOffset": 95, "endOffset": 113}, {"referenceID": 11, "context": "(Larkey and Croft 1996)).", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al.", "startOffset": 100, "endOffset": 127}, {"referenceID": 22, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al.", "startOffset": 144, "endOffset": 157}, {"referenceID": 29, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al.", "startOffset": 196, "endOffset": 220}, {"referenceID": 8, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al.", "startOffset": 332, "endOffset": 347}, {"referenceID": 3, "context": "These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words (Mansuy and Hilderman 2006), stemming terms (Porter 1980) and removing the least frequent words (Yang and Pedersen 1997); 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes (Joachims 1997); 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics (Dumais et al. 1998); 4) Using dictionaries (Mavroeidis et al.", "startOffset": 449, "endOffset": 469}, {"referenceID": 18, "context": "1998); 4) Using dictionaries (Mavroeidis et al. 2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 29, "endOffset": 103}, {"referenceID": 17, "context": "1998); 4) Using dictionaries (Mavroeidis et al. 2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 29, "endOffset": 103}, {"referenceID": 24, "context": "1998); 4) Using dictionaries (Mavroeidis et al. 2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 29, "endOffset": 103}, {"referenceID": 26, "context": "2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 75, "endOffset": 127}, {"referenceID": 27, "context": "2005; Mansuy and Hilderman 2006; Scott and Matwin 1998) and encyclopaedias (Strube and Ponzetto 2006; Wang and Domeniconi 2008) to find synonyms for rare terms.", "startOffset": 75, "endOffset": 127}, {"referenceID": 17, "context": "However, the names of people and organisations are unlikely to occur in dictionaries (Luo, Chen, and Xiong 2011; Mansuy and Hilderman 2006) and may be used rarely in training text.", "startOffset": 85, "endOffset": 139}, {"referenceID": 3, "context": "These text transformation techniques can have a large influence over the accuracy of the classification models (Dumais et al. 1998; Yang and Pedersen 1997).", "startOffset": 111, "endOffset": 155}, {"referenceID": 29, "context": "These text transformation techniques can have a large influence over the accuracy of the classification models (Dumais et al. 1998; Yang and Pedersen 1997).", "startOffset": 111, "endOffset": 155}], "year": 2017, "abstractText": "The bag-of-words model is a standard representation of text for many linear classifier learners. In many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions. However in settings where there is a large vocabulary, large variance in the frequency of terms in the training corpus, many classes and very short text (e.g., single sentences or document titles) the bag-of-words representation becomes extremely sparse, and this can reduce the accuracy of classifiers. A particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence. In this work we introduce a method for enriching the bag-of-words model by complementing such rare term information with related terms from both general and domain-specific Word Vector models. By reducing sparseness in the bag-of-words models, our enrichment approach achieves improved classification over several baseline classifiers in a variety of text classification problems. Our approach is also efficient because it requires no change to the linear classifier before or during training, since bag-ofwords enrichment applies only to text being classified.", "creator": "TeX"}}}