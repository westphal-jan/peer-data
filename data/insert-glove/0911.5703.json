{"id": "0911.5703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2009", "title": "Hierarchies in Dictionary Definition Space", "abstract": "eitan A 244.2 dictionary defines 45.21 words apologetic in leimbach terms of 81.36 other astyages words. vel Definitions niederman can tell chukwuemeka you k\u00fchne the erechtheion meanings bookforum of 11:20 words picturesqueness you don ' arsenio t know, but only if you brusuelas know the meanings slec of aquiline the congreve defining continual words. 2001-2005 How louisiana-monroe many words haizhou do natu you need 85.63 to covens know (kinvara and which bhojpuri ones) in order kosava to be miti\u0107 able gps-based to linky learn all larken the kosdaq rest coas from 11.59 definitions? kuisma We reduced dictionaries to brion their \" stalybridge grounding kernels \" (myeongjong GKs ), gorings about 10% of the dictionary, from cumbal which ionikos all minden the other words could colony be tokachi defined. The GK words turned out to have netrebko psycholinguistic correlates: 35.55 they were odourless learned maolin at e-books an 108.36 earlier age and more stanes concrete caraquet than emap the rhysodidae rest adversely of intar the cystiscus dictionary. But one can compress leuschner still more: cft the GK iiii turns lawson out beyens to have miskitos internal rotating structure, 90-100 with pkf a mariyun strongly kulikova connected \" kernel \u062d\u0633\u0646 core \" (KC) and uterine a surrounding layer, makeen from 17,113 which separators a bayeux hierarchy galleries of debat definitional distances morgentaler can toosi be derived, all osipow the herdsman way bodoe out garston to the supera periphery resurfacing of menologion the full dictionary. These 31.90 definitional black/white distances, 150,000-200 too, run-around are 101.0 correlated aymen with psycholinguistic harghita variables (age johanns of 76s acquisition, concreteness, imageability, oral mandala and bordano written frequency) and auditioner hence perhaps chunklet with the \" fusen mental silvermine lexicon \" veau in ntust each incoherently of our sought-after heads.", "histories": [["v1", "Mon, 30 Nov 2009 18:15:35 GMT  (18kb)", "http://arxiv.org/abs/0911.5703v1", "9 pages, 5 figues, 2 tables, 12 references, 23rd Annual Conference on Neural Information Processing Systems (NIPS): Workshop on Analyzing Networks and Learning With Graphsthis http URL"]], "COMMENTS": "9 pages, 5 figues, 2 tables, 12 references, 23rd Annual Conference on Neural Information Processing Systems (NIPS): Workshop on Analyzing Networks and Learning With Graphsthis http URL", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["olivier picard", "alexandre blondin-masse", "stevan harnad", "odile marcotte", "guillaume chicoisne", "yassine gargouri"], "accepted": false, "id": "0911.5703"}, "pdf": {"name": "0911.5703.pdf", "metadata": {"source": "CRF", "title": "Hierarchies in Dictionary Definition Space", "authors": ["Olivier Picard", "Alexandre Blondin Mass\u00e9", "Stevan Harnad", "Odile Marcotte", "Guillaume Chicoisne", "Yassine Gargouri"], "emails": ["picard.olivier.2@courrier.uqam.ca,", "harnad@uqam.ca,", "chicoisne.guillaume@uqam.ca,", "yassinegargouri@hotmail.com", "alexandre.blondin.masse@gmail.com", "Odile.Marcotte@gerad.ca"], "sections": [{"heading": null, "text": "ar X\niv :0\n91 1.\n57 03"}, {"heading": "1 Introduction", "text": "A category is a kind of thing (object, event, action, trait or state). To categorize is to do the right thing (eat, fight, flee, mate, etc.) with the right kind of thing. All species can acquire categories through trial and error sensorimotor induction. We are the only species that can also acquire and transmit categories through verbal instruction, by naming and defining them. The words in our dictionaries are almost all the names of categories, followed by their definitions. In principle, all\ncategories can be acquired through verbal definition, but we cannot acquire all of them that way: we have to know the meanings of some of the defining words already, by some other means. This is the \u201csymbol grounding problem\u201d [4] and presumably that other means of acquiring categories is sensorimotor induction. But how many words \u2013 and which ones \u2013 need to be grounded directly through sensorimotor induction in order to allow all the rest to be acquired through verbal definition?\nWe have been analyzing dictionaries in order to answer this question. By eliminating all the words that can be reached from other words through definition alone, we have been able to reduce the dictionary to its \u201cgrounding kernel\u201d (GK) \u2013 a set of words (about 10%) \u2013 out of which all the rest of the words can be reached through definition alone [1]. The GK has some striking properties: The words in it are learned at a significantly younger age than the rest of the dictionary and are also more concrete [2], but if the variance correlated with age is removed, the residual GK words are more abstract than the rest of the dictionary. What is the cause of this polarity shift?\nThe GK is unique, and sufficient to ground all the rest of the dictionary, but it is not minimal \u2013 it is not the smallest set of words from which all the rest can be reached via definition alone. That would be a \u201cminimum grounding set\u201d (MGS), which is not in general unique; we have not yet been able to compute a MGS, because this problem (equivalent to finding a \u201cminimum cardinality feedback vertex set\u201d for a general graph) is NP-complete (i.e. too hard to compute in general). We hope to be able to compute MGSs for our special cases, but meanwhile the GKs of our dictionaries \u2013 Cambridge International Dictionary of English (CIDE) [8] and Longman Dictionary of Contemporary English (LDOCE) [7] \u2013 already turn out to have more differentiated internal substructure which we begin analyzing further in this article.\nIn particular two substructures play important roles: the GK itself and a strongly connected subset of the GK that we call the \u201cKernel Core\u201d (KC). The GK words that are acquired earlier, and are more concrete than the rest of the dictionary, tend to be in the KC, whereas the GK words uncorrelated with age of acquisition tend to be in the outer layer surrounding the KC and are more abstract. These correlations between the KC and the rest of the GK, and between the GK and the rest of the dictionary as a whole, are binary (0/1), but one can make more graded comparisons by considering definitional chains of increasing lengths. We have accordingly extracted two hierarchies based on degrees of definitional distance, one based on the GK and one based on strongly connected components, to analyze how definitional distance correlates with age of acquisition, concreteness/abstractness and other psycholinguistic variables."}, {"heading": "2 Definitions and Notations", "text": "This section introduces all necessary definitions of graph-theoretical objects studied in this article. The reader is referred to [9] for complete graph theory and discrete mathematics introductions."}, {"heading": "2.1 Graphs", "text": "A directed graph is a couple G = (V,E), where V is a finite set of elements called vertices and E \u2286 V \u00d7 V is a finite set of couples of vertices called arcs. Given some graph G, we denote its set of vertices and edges by V (G) and E(G) respectively. The density of a directed graph is d(G) = |E(G)|/|V (G)|2.\nA vertex u is a predecessor (or successor) of vertex v if (u, v) \u2208 E (or (v, u) \u2208 E). The sets of predecessors and successors of u are denoted respectively by N\u2212(u) and N+(v). The in-degree and out-degree of u are defined respectively by \u03b4\u2212(u) = |N\u2212(u)| and \u03b4+(u) = |N+(u)|. A vertex of null in-degree (or out-degree) is called a source (or sink).\nA finite path of length n in a graph is a sequence (v0, v1, . . . , vn), where n \u2265 0 is an integer and (vi\u22121, vi) \u2208 E for i = 1, 2, . . . , n. A uv-path is a path starting with u and ending with v. A uv-path is a cycle if u = v. A graph is acyclic if it contains no cycles.\nGiven a graph G = (V,E), we say that G\u2032 = (V \u2032, E\u2032) is a subgraph of G if V \u2032 \u2286 V and E\u2032 \u2286 E. Moreover, if V \u2032 \u2286 V , the subgraph of G induced by V \u2032, denoted by G[V \u2032], is the subgraph G\u2032 = (V \u2032, E\u2032) such that E\u2032 = (V \u2032 \u00d7 V \u2032) \u2229 E."}, {"heading": "2.2 Dictionaries", "text": "Let W be a finite set whose elements are called words, and let 2W denote the collection of all subsets of W . A dictionary is a subset D of W \u00d7 2W such that for every (w, dw) \u2208 D: (i) dw 6= \u2205 (there is no empty definition) and (ii) w /\u2208 dw (a word cannot be used to define itself).\nElements of D are called entries. An entry is therefore a couple (w, dw), where w is a word and dw is a set of words. The element w, the set dw and the elements of dw are called respectively the definiendum (the defined word), the definition of w and the definientes (the defining words).\nThere is a very natural way to derive a graph from a dictionary. The associated graph of a dictionary D \u2286 W \u00d7 2W is the directed graph G = (V,E) where V = W and (u, v) \u2208 E if and only if there exists an entry (v, dv) \u2208 D such that u \u2208 dv . In fact, associated graphs are exactly directed graphs without loops and without sources. An artificially constructed \u201ctoy\u201d dictionary is illustrated in Figure 1."}, {"heading": "2.3 Grounding Kernel (GK)", "text": "Let G = (V,E) be a directed graph. We say that U \u2286 V is a grounding set (also called feedback vertex set) of G if G[V \u2212 U ] is acyclic, i.e. if U covers every cycle of G.\nThe problem of finding grounding sets of minimum size (MGSs) is NP-complete; hence it is unlikely that one will find an efficient algorithm for solving all instances of this problem. We hope to be able to exploit the particular structure of our graphs to get around this difficulty, and will report on our efforts in a forthcoming paper. Here, we are more interested in extracting hierarchies of definitional distance from dictionary-like graphs.\nFor this purpose, let G = (V,E) be a directed graph and SINKS(G) the set of its sinks. We define the operator OUT0 by OUT0(G) = G[V \u2212SINKS(G)]. We define OUT0n(G) as OUT0n\u22121(OUT0(G)) for n \u2265 2, and it is easily verified that there exists some \u2113 such that OUT0n(G) = OUT0\u2113(G) for any n \u2265 \u2113. Then we define OUT0\u221e(G) as OUT0\u2113(G).\nDefinition 1. The grounding kernel of G is given by GK = OUT0\u221e(G).\nNote that the grounding kernel (GK) of G is well defined for any graph, even if it is acyclic, since the process of removing sources recursively must stop after a finite number of steps. Moreover, the GK is unique for every graph G. It is also easy to show that every MGS is included in the GK of G. Hence it is of some interest to study the linguistic and cognitive properties of this special set\nof words as well as its internal graph structure. For instance, we can decompose it according to its strongly connected components (Subsection 2.4)."}, {"heading": "2.4 Kernel Core (KC)", "text": "We recall two classical relations on vertices. Given two vertices u and v of a graph G, we write u \u2192 v if there exists a uv-path in G and we write u \u2194 v if u \u2192 v and v \u2192 u. Note that \u2194 is reflexive, symmetric and transitive so that it is an equivalence relation. Therefore it yields a natural partition of the vertices of G. The equivalence classes of this relation are called the strongly connected components of G.\nLet G be a graph and V1, V2, . . ., Vk the strongly connected components of G. We construct a graph G\u2032 = (V \u2032, E\u2032) as follows : V \u2032 = {V1, V2, . . . , Vk} and (Vi, Vj) \u2208 E\u2032 if and only if Vi 6= Vj and there exist u \u2208 Vi and v \u2208 Vj such that (u, v) \u2208 E. We call this graph the SCC-quotient-graph. The next proposition states a well-known fact about this kind of graph.\nProposition 1. Let G be a graph and G\u2032 be the SCC-quotient-graph of G. Then G\u2032 is acyclic.\nEach acyclic graph induces a partial order on its vertices. In particular, the minimum elements are exactly the sources. They are of particular interest: we define the kernel core (KC) as the set of vertices of G belonging to the sources of the SCC-quotient graph G\u2032. In the dictionaries we have been studying, there turns out to be only one source."}, {"heading": "3 Hierarchies", "text": "The GK of a graph allows us to divide words into two categories: being in the GK or not. However, we would like to refine this division by introducing hierarchies on the vertices, generating more levels. In this article, we consider two hierarchies. The first is induced by the GK and the second is obtained from the strongly connected components.\nLet G = (V,E) be a directed graph associated with some dictionary. Let K be its GK. The GK-level of a vertex v \u2208 V with respect to K is defined by\nLGK(v) =\n{\n0 if v \u2208 K , max{LGK(u) | u \u2208 N\u2212(v)} + 1 otherwise.\n(1)\nWe will call GK-hierarchy the categorization of the vertices of G induced by this level function.\nThe next hierarchy is based on the strongly connected components. Let G be a graph and G\u2032 be its SCC-quotient graph. We define the level LSCC(v\u2032) of a vertex v\u2032 of G\u2032 as follows\nLSCC(v \u2032) =\n{\n0 if v\u2032 is a source of G\u2032, max{LSCC(u\u2032) | u\u2032 \u2208 N\u2212(v\u2032)} + 1 otherwise.\n(2)\nThe level LSCC(v) of a vertex v of G is the level LSCC(v\u2032) of the strongly connected component v\u2032 to which v belongs. This level function is well defined since the graph is acyclic.\nThe SCC-hierarchy is induced by the LSCC level function on the vertices of G. In particular, all elements belonging to the same SCC have the same level.\nIn the next sections, we study three sets of orderings obtained from those two hierarchies: (1) the ordering induced by the GK-hierarchy on the dictionary as a whole; (2) the ordering induced by the SCC-hierarchy on the dictionary as a whole; (3) the ordering induced by the SCC-hierarchy within the GK alone. We include the third hierarchy for a better understanding of the internal structure of the GK.\nExample. Continuing with the graph of Figure 1, Table 1 contains the levels of each word according to both hierarchies. Notice that the words \u201cno\u201d and \u201cnot\u201d have level 0 while all the other words u satisfy LSCC(u) = LGK(u) + 1. This is not always the case: this is explained by the fact that the GK of this very small example consists of the KC (corresponding to one connected component) and two other components that are the successors of the GK. In natural language dictionaries, the two level functions may be quite different."}, {"heading": "4 Natural Language Dictionaries", "text": "We have applied the above hierarchies to the study of two dictionaries: CIDE [8] and LDOCE [7]. As in many natural language processing problems, we are confronted with variation in words\u2019 morphosyntactic form and with polysemy (multiple meanings for the same word-form). Morphosyntactic variation was removed using Porter\u2019s algorithm [6]. To reduce polysemy we applied a common approximation: we kept only the first definition for each word. Finally, we removed loops and words with empty definitions. The number of vertices, the number of edges and the density of the two dictionaries, along with those of their GK and their \u201ckernel core\u201d (KC, the subgraph induced by their larger strongly connected component, defined below), are represented in Table 2.\nLike many networks derived from natural models, dictionary graphs satisfy almost all criteria of small-world graphs [11]. The two graphs are sparse and have density lower than 1%. Both dictionaries turn out to contain one single strongly connected subset. Moreover, their in-degree and out-degree distribution seem to follow a normal law and power-law, respectively. However, they also have a large number of small strongly connected components. The size of CIDE\u2019s biggest SCC is 1453 and LDOCE\u2019s is 1371 (see Table 2), while the remaining SCCs are very small.\nA remarkable observation is that the KC of both dictionaries is obtained from only a single source, and that source corresponds to the biggest strongly connected component. Hence, the heart of the cyclic structure of CIDE and LDOCE is found in the KC."}, {"heading": "5 Psycholinguistic Correlates of Words in the GK, KC, and Definitional Hierarchies", "text": "We analyzed how the structure of dictionary definition space \u2013 in terms of GK, the KC, and the two hierarchies of definitional distance that they induce \u2013 is related to five psycholinguistic variables:\nAOA: Age of Acquisition - the age at which a word is learned C: Concreteness - degree of concreteness/abstractness of word\u2019s referent I: Imageability - how readily one could generate a mental (visual) image of word\u2019s referent BF: Brown Frequency - spoken frequency of word TLF: Thorndike-Lodge Frequency - written frequency of word\nThe psycholinguistic variables (AOA, C, and I) came from the MRC psycholinguistic database [12]. Because MRC only covered 10% of the words in CIDE (and 8% of LDOCE), we merged each with further databases that were highly correlated with the MRC psycholinguistic database and to increase coverage to 33% for CIDE and 26% for LDOCE. [3] [10]\nWe did statistical analyses of the three hierarchies that were induced (1) by the GK across the entire dictionary, (2) by the SCCs (Strongly Connected Components) across the entire dictionary and (3) by the SCCs within the GK only.\nValue\nValue\nWith the GK-induced hierarchy, we did two linear regression analyses. Level 0 words (i.e., those within the GK) were included in the first analysis and excluded from the second. Figure 2 shows that in the first analysis AOA and I are significantly correlated with definitional distance. In the second, no correlation is significant. Hence the only significant effect here is the difference between the GK and the rest of the dictionary: GK words are learned earlier and are more imageable; this effect does not carry over to the higher levels in the induced hierarchy.\nIn contrast, in the analyses for the SCC-induced hierarchy across the entire dictionary, an ANOVA showed that differences in C and in I at higher levels of the hierarchy are significant too; in post-hoc tests, levels 0, 2, 3 and 4 differed from level 1. The means for each level and each variable are shown in Figure 3.\nFinally, for the SCC-induced hierarchy within GK alone, AOA and I are significant in the multiple regression (see Figure 4). Moreover, all variables had significant effects in the ANOVA, with the more specific post-hoc tests showing that C and I change from level to level, and AOA differs for most levels.\nValue\nValue\nAge of acquisition is earlier for level 0 words (the KC) compared to the other levels in the hierarchy induced by the SCCs for the dictionary as a whole. Words in the KC are also more abstract and less imageable than those in the other SCCs. KC words are also used more frequently, both orally and in writing. These results are similar to those reported by [11].\nWithin the GK alone, words become less concrete, imageable and frequent along the SCC-induced levels starting from KC. Age of acquisition is also significantly older than KC, but only for the first level, after which age remains flat. The correlation between concreteness and age of acquisition is also significantly higher for GK words outside than inside the KC.\nThese results cast further light on the prior finding of [2] that GK words are learned earlier and more concrete, but that when AOA\u2019s covariance with C is partialled out, C\u2019s polarity shifts: GK words that are not learned earlier are significantly more abstract than the rest of the dictionary. Our newer finding that the correlation between AOA and C is higher within the GK layer outside the KC suggests that the outer layer may be the locus of this difference. The GK\u2019s KC is more concrete and learned earlier, whereas its outer layer is more abstract, and unrelated to age of acquisition.\nIn the GK-induced hierarchy, only age and imageability were significant. This seemed to contradict our previous findings [2] comparing the GK with the rest of the dictionary, so we reanalyzed this hierarchy excluding its bottom level, the GK. This eliminated all correlations (Figure 2)."}, {"heading": "6 Concluding Remarks", "text": "The GK is a subset of the dictionary with features that differ substantially from the rest of the dictionary. The factors underlying the polarity change in concreteness observed in previous work [2] \u2013 with the GK being more concrete and learned earlier than the rest of the dictionary, but more abstract when the covariance with age is partialled out \u2013 has now been further refined: It turns out that the GK consists of a large, strongly connected KC plus a smaller, less interconnected outer layer. The KC, like the GK, is more concrete and learned earlier than the rest of the dictionary, and it is also more concrete and learned earlier than the outer layer. But with the KC, when the effects of age of acquisition are partialled out, there is no polarity reversal: The KC remains more concrete than its outer layer and also remains more concrete than the rest of the dictionary. So it is the outer layer that is more abstract than the KC , and hence the polarity reversal is related to the difference between the KC and the outer layer of the GK.\nTo further refine the differences between the GK\u2019s KC, the GK\u2019s outer layer and the rest of the dictionary, we induced three orderings in definitional space to produce a graded series of hierarchical levels at an increasing definitional distance from its bottom level or source, to test whether the dichotomous effects based on the GK or the KC extended beyond, along a graded series of definitional\ndistances. One hierarchy was induced from the GK; the second was induced from the entire dictionary\u2019s strongly connected components (SCCs) and the third was induced from the SCCs within the GK alone. The bottom level or \u201csource\u201d of the GK-based hierarchy of definitional distances was the GK itself; the bottom level of both the SCC hierarchies turned out to be the GK\u2019s large strongly connected KC. The successive levels of the SCC-induced hierarchy beyond the KC but within the GK and the levels on beyond the GK into the definitional space of the rest of the dictionary both turned out to be significantly correlated with the psycholinguistic variables (age, concreteness, imageability, oral and written frequency): Concreteness and imageability continue to decrease all the way out to the periphery in definition space; oral and written word frequency likewise continue to decrease; but the correlation with age of acquisition is present only in the contrast between the KC and the next level, which is the outer layer of the GK. The effects of SCC-induction for the entire dictionary were also similar to those of [11]: In their small-world analyses too, the KC was acquired earlier then the rest of the corpus and more frequently used (see Summary Figure 5).\nAll categories, even the most \u201cconcrete\u201d are in fact abstractions, because we must abstract from particular cases, even concrete sensorimotor ones, in order to find the invariant features that distinguish category members from nonmembers and allow us to do the right thing with the right kind of thing. But the more that categories are based on other categories, the more abstract they become, and this is reflected by the distances in our induced definitional space. It is in the nature of words to be amenable to combination and recombination in such a way as to define or describe ever more categories. Defining, like eating, is something we do. Our more concrete categories are answerable to the constraints of the sensorimotor world in which they are grounded, but our more abstract categories are increasingly answerable only to combinations of other categories, as we describe and define them. In abstract mathematics, that constraint, though only formal, is still a rigorous one. In more hermeneutic discourse (e.g., constitutional law or theology) the main constraint on words increasingly becomes just other words. Our mental lexicon must encode the meaning of all the words we use in our thought and discourse. Hierarchies in dictionary space may turn out to have counterparts in cognitive space."}], "references": [{"title": "How is meaning grounded in dictionary definitions", "author": ["A. Blondin Mass\u00e9", "G. Chicoisne", "Y. Gargouri", "S. Harnad", "O. Marcotte", "O. Picard"], "venue": "In TextGraphs \u201908: Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing, Manchester, United Kingdom,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Grounding abstract word definitions in prior concrete experience", "author": ["G. Chicoisne", "A. Blondin Mass\u00e9", "O. Picard", "S. Harnad"], "venue": "6th Int. Conf. on the Mental Lexicon,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Age of acquisition ratings for 3,000 monosyllabic words, Behavior", "author": ["M.J. Cortese", "M.M. Khanna"], "venue": "Research Methods,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "The symbol grounding problem", "author": ["S. Harnad"], "venue": "Physica D 42:335-346,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Reducibility among combinatorial problems", "author": ["R.M. Karp"], "venue": "Complexity of Computer Computations,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1972}, {"title": "An algorithm for suffix stripping, Program", "author": ["M.F. Porter"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1980}, {"title": "Dictionary of Contemporary English (LDOCE)", "author": ["P. Procter", "Longman"], "venue": "Longman Group Ltd.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1978}, {"title": "Cambridge International Dictionary of English (CIDE)", "author": ["P. Procter"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Discrete mathematics and its applications", "author": ["K.H. Rosen"], "venue": "6th ed. McGraw-Hill,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "The Bristol norms for age of acquisition, imageability, and familiarity, Behavior", "author": ["H. Stadthagen-Gonzalez", "C.J. Davis"], "venue": "Research Methods,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "The large-scale structure of semantic networks: statistical analyses and a model of semantic growth", "author": ["M. Steyvers", "J.B. Tenenbaum"], "venue": "Cognitive Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "MRC Psycholinguistic Database: Machine Usable Dictionary, version 2.00", "author": ["M. Wilson", "O.O. Qx", "P. Quinlan"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}], "referenceMentions": [{"referenceID": 3, "context": "This is the \u201csymbol grounding problem\u201d [4] and presumably that other means of acquiring categories is sensorimotor induction.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "By eliminating all the words that can be reached from other words through definition alone, we have been able to reduce the dictionary to its \u201cgrounding kernel\u201d (GK) \u2013 a set of words (about 10%) \u2013 out of which all the rest of the words can be reached through definition alone [1].", "startOffset": 276, "endOffset": 279}, {"referenceID": 1, "context": "The GK has some striking properties: The words in it are learned at a significantly younger age than the rest of the dictionary and are also more concrete [2], but if the variance correlated with age is removed, the residual GK words are more abstract than the rest of the dictionary.", "startOffset": 155, "endOffset": 158}, {"referenceID": 7, "context": "We hope to be able to compute MGSs for our special cases, but meanwhile the GKs of our dictionaries \u2013 Cambridge International Dictionary of English (CIDE) [8] and Longman Dictionary of Contemporary English (LDOCE) [7] \u2013 already turn out to have more differentiated internal substructure which we begin analyzing further in this article.", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "We hope to be able to compute MGSs for our special cases, but meanwhile the GKs of our dictionaries \u2013 Cambridge International Dictionary of English (CIDE) [8] and Longman Dictionary of Contemporary English (LDOCE) [7] \u2013 already turn out to have more differentiated internal substructure which we begin analyzing further in this article.", "startOffset": 214, "endOffset": 217}, {"referenceID": 8, "context": "The reader is referred to [9] for complete graph theory and discrete mathematics introductions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "We have applied the above hierarchies to the study of two dictionaries: CIDE [8] and LDOCE [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "We have applied the above hierarchies to the study of two dictionaries: CIDE [8] and LDOCE [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "Morphosyntactic variation was removed using Porter\u2019s algorithm [6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "Like many networks derived from natural models, dictionary graphs satisfy almost all criteria of small-world graphs [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "We analyzed how the structure of dictionary definition space \u2013 in terms of GK, the KC, and the two hierarchies of definitional distance that they induce \u2013 is related to five psycholinguistic variables: AOA: Age of Acquisition - the age at which a word is learned C: Concreteness - degree of concreteness/abstractness of word\u2019s referent I: Imageability - how readily one could generate a mental (visual) image of word\u2019s referent BF: Brown Frequency - spoken frequency of word TLF: Thorndike-Lodge Frequency - written frequency of word The psycholinguistic variables (AOA, C, and I) came from the MRC psycholinguistic database [12].", "startOffset": 625, "endOffset": 629}, {"referenceID": 2, "context": "[3] [10]", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[3] [10]", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "These results are similar to those reported by [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "These results cast further light on the prior finding of [2] that GK words are learned earlier and more concrete, but that when AOA\u2019s covariance with C is partialled out, C\u2019s polarity shifts: GK words that are not learned earlier are significantly more abstract than the rest of the dictionary.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "This seemed to contradict our previous findings [2] comparing the GK with the rest of the dictionary, so we reanalyzed this hierarchy excluding its bottom level, the GK.", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "The factors underlying the polarity change in concreteness observed in previous work [2] \u2013 with the GK being more concrete and learned earlier than the rest of the dictionary, but more abstract when the covariance with age is partialled out \u2013 has now been further refined: It turns out that the GK consists of a large, strongly connected KC plus a smaller, less interconnected outer layer.", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "The effects of SCC-induction for the entire dictionary were also similar to those of [11]: In their small-world analyses too, the KC was acquired earlier then the rest of the corpus and more frequently used (see Summary Figure 5).", "startOffset": 85, "endOffset": 89}], "year": 2009, "abstractText": "A dictionary defines words in terms of other words. Definitions can tell you the meanings of words you don\u2019t know, but only if you know the meanings of the defining words. How many words do you need to know (and which ones) in order to be able to learn all the rest from definitions? We reduced dictionaries to their \u201cgrounding kernels\u201d (GKs), about 10% of the dictionary, from which all the other words could be defined. The GK words turned out to have psycholinguistic correlates: they were learned at an earlier age and more concrete than the rest of the dictionary. But one can compress still more: the GK turns out to have internal structure, with a strongly connected \u201ckernel core\u201d (KC) and a surrounding layer, from which a hierarchy of definitional distances can be derived, all the way out to the periphery of the full dictionary. These definitional distances, too, are correlated with psycholinguistic variables (age of acquisition, concreteness, imageability, oral and written frequency) and hence perhaps with the \u201cmental lexicon\u201d in each of our heads.", "creator": "LaTeX with hyperref package"}}}