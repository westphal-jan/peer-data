{"id": "1001.0833", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2010", "title": "Random Indexing K-tree", "abstract": "bhasin Random ailerons Indexing (fieldale RI) K - predation tree is lindenau the 782,000 combination of two morwenna algorithms kelefa for uskok clustering. Many large scale problems exist iced in 3,311 document ostiense clustering. farncombe RI K - teched tree scales well directorates with large typifies inputs repository due to 3048 its low complexity. It wallonne also harborplace exhibits paravel features that 27.83 are 31.86 useful anti-psychotic for managing gresson a bonet changing collection. Furthermore, 1.57 it maktoums solves etete previous maio issues nesting with undiscussed sparse akef document 462,500 vectors when exhorted using beinhorn K - 0.066 tree. takeouts The 89.34 algorithms wynants and yumileidi data structures are defined, explained and motivated. vegetarianism Specific modifications to zahava K - tree are made tickled for karekin use with bris\u00e9 RI. Experiments papageorge have maltzan been executed to measure 13.70 quality. sepolcro The buraidah results cronies indicate mukh that bellechasse RI samcheok K - dabaya tree slob improves tesseractic document tollberg cluster clubbiness quality over the original stooping K - tree algorithm.", "histories": [["v1", "Wed, 6 Jan 2010 08:03:20 GMT  (154kb)", "https://arxiv.org/abs/1001.0833v1", "8 pages, ADCS 2009"], ["v2", "Tue, 2 Feb 2010 02:46:22 GMT  (135kb)", "http://arxiv.org/abs/1001.0833v2", "8 pages, ADCS 2009; Hyperref and cleveref LaTeX packages conflicted. Removed cleveref"]], "COMMENTS": "8 pages, ADCS 2009", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.DS", "authors": ["christopher m de vries", "lance de vine", "shlomo geva"], "accepted": false, "id": "1001.0833"}, "pdf": {"name": "1001.0833.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["chris@de-vries.id.au", "l.devine@qut.edu.au", "s.geva@qut.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 1.\n08 33\nv2 [\ncs .I\nR ]\n2 F\neb 2\n01 0\nKeywords Random Indexing, K-tree, Dimensionality Reduction, B-tree, Search Tree, Clustering, Document Clustering, Vector Quantization, k-means"}, {"heading": "1 Introduction", "text": "The purpose of this paper is to present and analyse the combination of Random Indexing (RI) with the K-tree algorithm. Both RI and K-tree adapt to changing data and decrease the cost of computationally intensive vector based applications. This combination is particularly suitable to the representation and clustering of very large document collections. Documents are typically represented in vector space as very sparse high dimensional vectors. RI can reduce the dimensionality and sparsity of this representation. In turn, the condensed representation is highly effective when working with K-tree. The paper is focused on determining the effectiveness of using RI with K-tree through experiments and comparative analysis of results.\nSections 2 to 6 discuss K-tree, Random Indexing, Document Representation, Experimental Setup and Experimental results respectively. The paper ends with a conclusion in Section 7."}, {"heading": "2 K-tree", "text": "K-tree [6, 1] is a height balanced cluster tree. It was first introduced in the context of signal processing by Geva\nProceedings of the 14th Australasian Document Computing Symposium, Sydney, Australia, 4 December 2009. Copyright for this article remains with the authors.\n[10]. The algorithm is particularly suitable to clustering of large collections due to its low complexity. It is a hybrid of the B+-tree and k-means algorithm. The B+-tree algorithm is modified to work with multi dimensional vectors and k-means is used to perform node splits in the tree. K-tree is also related to Tree Structured Vector Quantization (TSVQ) [9]. TSVQ recursively splits the data set, in a top-down fashion, using k-means. TSVQ does not generally produce balanced trees.\nK-tree achieves its efficiency through execution of the high cost k-means step over very small subsets of the data. The number of vectors clustered during any step in the K-tree algorithm is determined by the tree order (usually \u226a 1000) and it is independent of collection size. It is efficient in updating the collection while maintaining clustering properties through the use of a nearest neighbour search tree that directs new vectors to the appropriate leaf node.\nThe K-tree forms a hierarchy of clusters. This hierarchy supports multi-granular clustering where generalisation or specialisation is observed as the tree is traversed from a leaf towards the root or vice versa. The granularity of clusters can be decided at run-time by selecting clusters that meet criteria such as distortion or cluster size."}, {"heading": "2.1 K-tree and Document Clustering", "text": "The K-tree algorithm is well suited to clustering large document collections due to its low time complexity. The time complexity of building K-tree is O(n log n) where n is the number of bytes of data to cluster. This is due to the divide and conquer properties inherent to the search tree. De Vries and Geva [5, 6] investigate the run-time performance and quality of K-tree by comparing results with other INEX submissions and CLUTO [13]. CLUTO is a popular clustering tool kit used in the information retrieval community. K-tree has been compared to k-means, including the CLUTO implementation, and provides comparable quality and a marked increase in run-time performance. However, K-tree forms a hierarchy of clusters and k-means does not. Comparison of the quality of the tree structure will be undertaken in further research. The run-time performance increase of K-tree is most noted when a large number of clusters are required. This is useful in terms of doc-\nument clustering because there are a huge number of topics in a typical collection. The on-line and incremental nature of the algorithm is useful for managing changing document collections. Most clustering algorithms are one shot and must be re-run when new data arrives. K-tree adapts as new data arrives and has the low time complexity of O(log n) for insertion of a single document. Additionally, the tree structure also allows for efficient disk based implementations when the size of data sets exceeds that of main memory."}, {"heading": "2.2 K-tree Definition", "text": "K-tree builds a nearest neighbour search tree over a set of real valued vectors V in d dimensional space.\n\u2200v \u2208 V : v \u2208 Rd (1)\nIt is inspired by the B+-tree where all data records are stored in leaf nodes. Tree nodes, N , consist of a sequence of (vector, child node) pairs of length l. The tree order, m, restricts the number of vectors stored in any node to between one and m.\n1 \u2264 l \u2264 m (2)\nN = \u3008(v1, c1), ..., (vl, cl)\u3009 (3)\nThe tree consists of two types of nodes. Leaf nodes contain the data vectors that were inserted into the tree. Internal nodes contain clusters. A cluster vector is the mean of all data vectors contained in the leaves of all descendant nodes (i.e. the entire cluster sub-tree). This follows the same recursive definition of a B+-tree where each tree is made up of a set of smaller sub-trees. Upon construction of the tree, a nearest neighbour search tree is built in a bottom-up manner by splitting full nodes using k-means [14] where k = 2. As the tree depth increases it forms a hierarchy of \u201cclusters of clusters\u201d from the root to the above-leaf level. The above-leaf level contains the finest granularity cluster vectors. Each leaf node stores the data vectors pointed to by the above-leaf level. The efficiency of K-tree stems from the low complexity of the B+-tree algorithm, combined with only ever executing k-means on a relatively small number of vectors, defined by the tree order, and by using a small value of k."}, {"heading": "2.3 Modifications to K-tree", "text": "The K-tree algorithm was modified for use with RI. This modified version will be referred to as \u201cModified K-tree\u201d and the original K-tree will be referred to as \u201cUnmodified K-tree\u201d.\nAll the document vectors created by RI are of unit length in the modified K-tree. Therefore, all centroids are normalised to unit length at all times. The k-means used for node splits in K-tree was changed to use randomised seeding and restart if it did not converge within six iterations. The process always converged quickly in our experiments; although it is possible to constrain the number of restarts we did not find this to be necessary.\nThe original K-tree algorithm does not modify any of the centroids. They are simply the means of the vectors they represent. The k-means implementation runs to complete convergence and seeds centroids via perturbation of the global mean. To create two seeds the global mean is calculated and then the two seeds are created by moving away from the mean in opposite directions."}, {"heading": "2.4 K-tree Example", "text": "Figures 1 to 3 are K-tree clusters in two dimensions. 1000 points were drawn from a random normal distribution with a mean of 1.0 and standard deviation of 0.3. The order of the K-tree, m, was 11. The grey dots represent the data set, the black dots represent the centroids and the lines represent the Voronoi tessellation of the centroids. Each of the data points contained within each tile of the tessellation are the nearest neighbours of the centroid and belong to the same cluster. It can be seen that the probability distribution is modelled at different granularities. The top level of the tree is level 1. It is the coarsest grained clustering. In this example it splits the distribution in three. Level 2 is more granular and splits the collection into 19 sub-clusters. The individual clusters in level 2 can only be arrived at through a nearest neighbour association with a parent cluster in level 1 of the tree. Level 3 is the deepest level in the tree consisting of cluster centroids. The 4th level is the data set of vectors that were inserted into the tree."}, {"heading": "2.5 Building K-tree", "text": "The K-tree is constructed dynamically as data vectors arrive. Initially the tree contains a single empty root node at the leaf level. Vectors are inserted via a nearest neighbour search, terminating at the leaf level. The root of an empty tree is a leaf, so the first m data vectors are stored in the root, at which point the node becomes full. When the m + 1 vector arrives the root is split using k-means where k = 2, clustering all m + 1 vectors into two clusters. The two centroids that result from k-means are then promoted to become the centroids in a new root. The vectors associated with each centroid\nare placed into a child node. This promotion process has created a new root and two leaf nodes in the tree. The tree is now two levels deep. Insertion of a new data vector follows a nearest neighbour search to find the closest centroid in the root. The vector is inserted into the associated child. When a new vector is inserted the centroids are updated recursively along the nearest neighbour search path, all the way back to the root node. The propagated means are weighted by the number of data vectors contained beneath them. This ensures that any centroid in K-tree is the mean vector of all the data vectors contained in the associated sub tree. This insertion process continues, splitting leaves when they become full, until the root node itself becomes full. K-means is then run on the root node containing centroids. The vectors in the new root node become centroids of centroids. As the tree grows, internal and leaf nodes are split in the same manner. The process of promotion can potentially propagate to cause a full root node at which point the construction of a new root follows and the tree depth is increased by one. At all times the tree is guaranteed to be height balanced. Although the tree is always height balanced nodes can contain as little as one vector. In this case the tree will contain many more levels than a tree where each node is half\nfull. Section 4 shows this construction process for a Ktree of order three (m = 3)."}, {"heading": "2.6 Sparsity and K-tree", "text": "K-tree was originally designed to operate with dense vectors. When a sparse representation is used performance degrades even though there is significantly less data to process. The clusters in the top levels of the tree are means of most of the terms in the collection and are not sparse at all. The algorithm updates cluster centres along the insertion path in the tree. Since document vectors have very high dimensionality this becomes a very expensive process.\nThe medoid K-tree [6] extended the algorithm to use a sparse representation and replace centroids with document examples. This improved run-time performance and decreased memory usage. Unfortunately it decreased quality when using sparse document vectors. The document examples in the root of the tree were almost orthogonal to new documents being inserted. The documents were unlikely to have meaningful overlap in vocabulary.\nThe approach taken by De Vries and Geva at INEX 2008 [5] is a simple approach to dimensionality reduction or feature selection. It is called TF-IDF culling and it is performed by ranking terms. A rank is calculated by summing all weights for each term. The weights are the BM25 weight for each term in each document. This can also be explained as the sum of the column vector in the document by term matrix. The top n terms with the highest rank are selected, where n is the desired dimensionality. This works particularly well with term occurrences due to the Zipf law distribution of terms [19]. The collection frequency of a term is inversely proportional to its rank according to collection frequency. Most of the term weights are contained in the most frequent terms."}, {"heading": "3 Random Indexing", "text": "Random Indexing (RI) [18] is an efficient, scalable and incremental approach to the word space model. Word space models use the distribution of terms to create high dimensional document vectors. The directions of these document vectors represent various semantic meanings and contexts.\nLatent Semantic Analysis (LSA) [7] is a popular word space model. LSA creates context vectors from a document term occurrence matrix by performing Singular Value Decomposition (SVD). Dimensionality reduction is achieved through projection of the document term occurrence vectors onto the subspace spanned by the vectors with the largest Eigen values in the decomposition. This projection is optimal in the sense that it minimises the variance between the original matrix and the projected matrix. In contrast, Random Indexing first creates random context vectors of lower dimensionality, and then combines them to create a term occurrence matrix in the dimensionally reduced space. Each term\nnode vector child l ink k-means performed on enclosed vectors\nin the collection is assigned a random vector, and the document term occurrence vector is then a superposition of all the term random vectors. There is no matrix decomposition and hence the process is efficient.\nThe RI process is conceptually very different from LSA and does not have the same optimality properties. The context vectors used by RI should optimally be orthogonal. Nearly orthogonal vectors can be used and have been found to perform similarly [4]. These vectors can be drawn from a random Gaussian distribution. The Johnson and Linden-Strauss lemma [11] states that if points are projected into a randomly selected subspace of sufficiently high dimensionality, then the distances between the points are approximately preserved. The same topology that exists in the higher dimensional space is reflected in the lower dimensional randomly selected subspace. Consequently, RI offers low complexity dimensionality reduction while still preserving the topological relationships amongst document vectors."}, {"heading": "3.1 Random Indexing Definition", "text": "In RI, each dimension in the original space is given a randomly generated index vector. The index vectors are high dimensional, sparse and ternary. Sparsity is controlled via a seed length that specifies the number of randomly selected non-zero dimensions. Ternary vectors consist of randomly distributed +1 and -1 values in the non-zero dimensions.\nIn the context of document clustering, RI can be viewed as a matrix multiplication of a document by\nterm matrix D and a term by index-vector matrix I . Alternatively, I can be referred to as a random projection matrix. Each row vector in D represents a document, each row vector in I is an index vector, n is the number of documents, t is the number of terms and r is the dimensionality of the reduced spaced. R is the reduced matrix where each row vector represents a document.\nDn\u00d7tIt\u00d7r = Rn\u00d7r (4)\nRI has several advantages. It can be performed incrementally and on-line as data arrives. Any document can be indexed (i.e. encoded as an RI vector) independently from all other documents in the collection. This eliminates the need to build and store the entire document by term matrix. Additionally, newly encountered dimensions (terms) in the document collection are easily accommodated without having to recalculate the projection of previously encoded documents. In contrast, SVD requires global analysis where the number of documents and terms are fixed. The time complexity of RI is also very attractive. It is linear in the number of terms in a document and independent of collection size."}, {"heading": "3.2 Choice of Index Vectors", "text": "The index vectors used in RI were chosen to be sparse and ternary. Ternary index vectors for RI were introduced by Achlioptas [2] as being well suited for database environments. The primary concern of sparse index vectors is reducing time and space complexity. Bingham and Mannila [4] run experiments indicating\nthat sparse index vectors do not affect the quality of results. This is not the only choice when creating index vectors. Kanerva [12] introduces binary spatter codes. Plate [15] explores Holographic Reduced Representations that consist of dense vectors with floating point values."}, {"heading": "3.3 Random Indexing Example", "text": "In practice, to construct a document vector, the document vector is initially set to zero, and then the sparse index vector for each term in the document is added to the document vector. The weight of the added term index vector may be determined by TF-IDF or another weighting scheme. When all terms have been added, the document vector is normalised to unit length. There is no need to explicitly form the random projection matrix in Equation 4 up-front. The random index vectors for each term can be generated and stored as they are first encountered. The fact that each index vector is sparse means that the vectors use less memory to store and are faster to add.\nThe effect of this approach is that each document will have a particular signature that can be compared with other documents via cosine similarity. The document signature is thus a vector on the unit hyper-sphere.\nIn the simple scenario in Figure 5 the index vectors for the four words travel, mars, space and telescope, are added to the document vector as they are encountered in the text of the document. Afterwards, the document should be normalised.\nThe sparse index vectors can be efficiently stored by simply storing the position of the non-zero entries with the sign of the position indicating whether it is one or negative one."}, {"heading": "3.4 Random Indexing K-tree", "text": "The time complexity of K-tree depends on the length of the document vectors. K-tree insertion incurs two costs, finding the appropriate leaf node for insertion and k-means invocation during node splits. It is therefore desirable to operate with lower dimensional vector representation.\nThe combination of RI with K-tree is a good fit. Both algorithms operate in an on-line and incremental mode. This allows it to track the distribution of data as it arrives and changes over time. K-tree insertions and deletions allow flexibility when tracking data in volatile and changing collections. Furthermore, K-tree\nperforms best with dense vectors, such as those produced by RI."}, {"heading": "4 Document Representation", "text": "The INEX 2008 XML Mining collection was used to complete the experiments. It contains 114,366 documents that are a subset of the XML Wikipedia corpus [8]. 15 different categories were provided for the documents.\nDocument content was represented with BM25 [17]. Stop words were removed and the remaining terms were stemmed using the Porter algorithm [16]. BM25 is determined by term distributions within each document and the entire collection. BM25 works with similar concepts as TF-IDF except that is has two tuning parameters. The BM25 tuning parameters were set to the same values as used for TREC [17], K1 = 2 and b = 0.75. K1 influences the effect of term frequency and b influences document length.\nLinks were represented using LF-IDF [5]. This resulted in a document-to-document link matrix. If there is a link between documents i and j then a value of one is added to position i, j and j, i in the matrix. If two documents both link to each other a value of two is recorded in their respective vectors. Each row vector of the matrix represents a document as a vector of link frequencies to and from other documents.\nThe motivation behind this representation is that documents with similar content will link to similar documents. For example, in the current Wikipedia both car manufacturers BMW and Jaguar link to the Automotive Industry document. Link frequencies were weighted with the same Inverse Document Frequency heuristic from TF-IDF. The idea is to decrease the weight of highly frequent links and increase the weight of less frequent links. Links to year documents in the Wikipedia are examples of \u201cstop links\u201d that are weighted down by this heuristic. Unlike term frequencies in TF-IDF the link frequencies in LF-IDF are not normalised. De Vries and Geva [5] found that normalising link frequencies decreased classification performance.\nWhen document and link representations are combined they are both converted to unit vectors and concatenated. Converting each representation to unit vectors ensures that the weights of one representation do not dominate the other. De Vries and Geva [5] found this to be effective for classification."}, {"heading": "5 Experimental Setup", "text": "Experiments have been run to measure the quality difference between various configurations of K-tree. Section 2.3 describes the modifications made to K-tree. Table 1 lists all the configurations tested.\nThe following conditions were used when running the experiments.\n1. Each K-tree configuration was run a total of 20 times.\n2. The documents were inserted in a different random order each time K-tree is built.\n3. If RI was used, the index vectors were generated statistically independently each time K-tree was built.\n4. For each K-tree built, k-means++ [3] was run 20 times on the codebook vectors to create 15 clusters.\n5. All document vectors were unitised after performing dimensionality reduction.\nThe conditions listed above resulted in 400 measurements for each K-tree configuration. For each of the 20 K-trees built, k-means++ was run 20 times. The repetition of the experiments is to measure the variance caused by the random insertion order into K-tree, the randomised seeding process in k-means in the modified K-tree and the randomised seeding process of kmeans++.\nAssessment of clustering quality is based on the INEX XML Mining track. The set of 114,366 documents, belonging to 15 classes were used to evaluate clustering quality of INEX submissions. The cluster labels are taken from the Wikipedia itself. K-tree generates clusters in an unsupervised manner, and it is not necessarily going to produce 15 clusters at a particular level in the tree. In order to re-use the INEX test collection, it was necessary to post process the K-tree and to reduce a cluster level in the tree to 15 clusters by using k-means++. Note that this is a low cost operation involving only a small number of vectors, which is not required in an ordinary application. It is done for the sole purpose of producing comparable results with the INEX benchmark data. The same approach was taken at INEX 2008 by De Vries and Geva [5]. For a comparison of entropy and purity to be meaningful they have to be measured on the same number of clusters.\nMicro averaged purity and entropy are compared. Micro averaging weights the score of a cluster by its size. Purity and entropy are calculated by comparing the clustering solution to the labels provided. A higher purity score indicates a higher quality solution because the clusters are more pure with respect to the ground truth. A lower entropy score indicates a higher quality solution because there is more order with respect to the ground truth."}, {"heading": "6 Experimental Results", "text": "Tables 3 to 7 contain results for the K-tree configurations tested listed in Table 1. Table 2 lists the meaning of the symbols used. Figures 6 and 7 are graphical representations of the average micro purity and entropy.\nThe unmodified K-tree using TF-IDF culling and BM25 had unexpected results as seen in Table 3. The average micro purity and entropy peaked at 400 dimensions. Performing this dimensionality reduction at these lower dimensions had not been performed before. This is an interesting and unexpected result and future experiments will need to determine if the phenomenon occurs in different corpora.\nImprovements in micro purity have been tested for significance via t-tests. The null hypothesis is that both results come from the same distribution with the same mean. In this case they are not significantly different. If the null hypothesis is rejected then the difference is statistically significant.\nThe modifications made to K-tree for use with RI had a significant impact. The unmodified K-tree and modified K-tree were compared. Specifically, configurations B and D, and configurations C and E were tested against each other. All dimensions were compared against each other. The improved performance of the modified K-tree was statistically significant for all dimensions (100 vs 100, 200 vs 200 and so on) with a p-value of 0 or extremely close to 0 (p < 1\u00d7 10\u2212100).\nThe modified K-tree using RI was tested with two representations. Configurations D and E were tested at all dimensions. The null hypothesis was rejected at all dimensions except 10000. This means that BM25 performed significantly better than the BM25 + LF-IDF representation at all dimensions except 10000. At 10000 dimensions the difference was not considered statistically significant with a p-value of 0.3. The increased performance of this representation in classification did not apply to clustering when using RI. The LF-IDF representation may be interfering with the BM25 representation and approaches such as reducing the weight of LF-IDF in the RI process or performing RI separately on each representation and then concatenating the reduced vectors may improve performance. Running k-means on the full sparse vectors will also indicate if RI is responsible for this. Further experimentation is required to provide more evidence for this result.\nThe unexpected results in configuration A were tested against the best RI configuration, E. The highest average at 400 dimensions in configuration A was tested against all dimensions in configuration E (400 vs 100, 400 vs 200, 400 vs 400, 400 vs 1000 and so on). The RI K-tree, configuration E, became statistically more significant at 2000 dimensions with a p-value of 1.48 \u00d7 10\u22126 and thus rejected the null hypothesis. For dimensions 4000 through 10000, the performance difference was statistically significant, with a p-value of 0 in all cases. Thus, RI K-tree improves results, even over the unexpected high results of configuration A, by embedding the original 200,000 dimensional term space into at least a 2000 dimension reduced space."}, {"heading": "6.1 INEX Results", "text": "The INEX XML Mining track is a collaborative evaluation forum where research teams improve approaches in supervised and unsupervised machine learning with XML documents. Participants make submissions and the evaluation results are later released.\nThe RI K-tree in configuration E performs on average at a comparable level to the best results submitted to the INEX 2008 XML Mining track. The top two results from the track had a micro purity of 0.49 and 0.50. These are not average scores for the approaches but the best results participants found. The RI K-tree in configuration E had a maximum micro entropy of 0.55. This is 10% greater than the INEX submissions."}, {"heading": "7 Conclusion", "text": "RI K-tree was introduced as an attractive approach for large scale document clustering. This is the first time RI and K-tree have been combined. The results show that RI K-tree improves quality of clustering results, even over the unexpected results found when using TFIDF culling. Further experiments are required to determine if the unexpected effect of TF-IDF culling at low dimensions is an anomaly or actually exists in many collections. Additionally, RI K-tree is an efficient and high quality approach to overcome previous problems with sparse representations when using K-tree. Unfortunately the combination of BM25 and LF-IDF representations did not improve results in clustering as they did in earlier classification results."}], "references": [{"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences, Volume 66, Number 4, pages 671\u2013687", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA \u201907: Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1027\u20131035, Philadelphia, PA, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "KDD \u201901: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 245\u2013250, New York, NY, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Document clustering with k-tree", "author": ["C.M. De Vries", "S. Geva"], "venue": "Advances in Focused Retrieval: 7th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2008, Dagstuhl Castle, Germany, December 15-18, 2008. Revised and Selected Papers, pages 420\u2013431", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "K-tree: large scale document clustering", "author": ["C.M. De Vries", "S. Geva"], "venue": "SIGIR \u201909: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 718\u2013 719, New York, NY, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science, Volume 41, Number 6, pages 391\u2013407", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "The Wikipedia XML Corpus", "author": ["L. Denoyer", "P. Gallinari"], "venue": "SIGIR Forum", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Vector quantization and signal compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Kluwer Academic Publishers", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "K-tree: a height balanced tree structured vector quantizer", "author": ["S. Geva"], "venue": "Proceedings of the 2000 IEEE Signal Processing Society Workshop Neural Networks for Signal Processing X, 2000., Volume 1, pages 271\u2013280 vol.1", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics, Volume 26, Number 189-206, pages 1\u20131", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1984}, {"title": "The spatter code for encoding concepts at many levels", "author": ["P. Kanerva"], "venue": "ICANN94, Proceedings of the International Conference on Artificial Neural Networks", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "CLUTO-A Clustering Toolkit", "author": ["G. Karypis"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "Information Theory, IEEE Transactions on, Volume 28,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1982}, {"title": "Distributed representations and nested compositional structure", "author": ["T.A. Plate"], "venue": "Ph.D. thesis", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program: Electronic Library and Information Systems, Volume 40, Number 3, pages 211\u2013218", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Simple", "author": ["S.E. Robertson", "K.S. Jones"], "venue": "proven approaches to text retrieval. Update", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "An introduction to random indexing", "author": ["M. Sahlgren"], "venue": "Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE 2005", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Human behavior and the principle of least effort: An introduction to human ecology", "author": ["G.K. Zipf"], "venue": "Addison- Wesley Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1949}], "referenceMentions": [{"referenceID": 4, "context": "K-tree [6, 1] is a height balanced cluster tree.", "startOffset": 7, "endOffset": 13}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "K-tree is also related to Tree Structured Vector Quantization (TSVQ) [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "De Vries and Geva [5, 6] investigate the run-time performance and quality of K-tree by comparing results with other INEX submissions and CLUTO [13].", "startOffset": 18, "endOffset": 24}, {"referenceID": 4, "context": "De Vries and Geva [5, 6] investigate the run-time performance and quality of K-tree by comparing results with other INEX submissions and CLUTO [13].", "startOffset": 18, "endOffset": 24}, {"referenceID": 11, "context": "De Vries and Geva [5, 6] investigate the run-time performance and quality of K-tree by comparing results with other INEX submissions and CLUTO [13].", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "Upon construction of the tree, a nearest neighbour search tree is built in a bottom-up manner by splitting full nodes using k-means [14] where k = 2.", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "The medoid K-tree [6] extended the algorithm to use a sparse representation and replace centroids with document examples.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "The approach taken by De Vries and Geva at INEX 2008 [5] is a simple approach to dimensionality reduction or feature selection.", "startOffset": 53, "endOffset": 56}, {"referenceID": 17, "context": "This works particularly well with term occurrences due to the Zipf law distribution of terms [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "Random Indexing (RI) [18] is an efficient, scalable and incremental approach to the word space model.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "Latent Semantic Analysis (LSA) [7] is a popular word space model.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "Nearly orthogonal vectors can be used and have been found to perform similarly [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "The Johnson and Linden-Strauss lemma [11] states that if points are projected into a randomly selected subspace of sufficiently high dimensionality, then the distances between the points are approximately preserved.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Ternary index vectors for RI were introduced by Achlioptas [2] as being well suited for database environments.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Bingham and Mannila [4] run experiments indicating", "startOffset": 20, "endOffset": 23}, {"referenceID": 10, "context": "Kanerva [12] introduces binary spatter codes.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "Plate [15] explores Holographic Reduced Representations that consist of dense vectors with floating point values.", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "It contains 114,366 documents that are a subset of the XML Wikipedia corpus [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 15, "context": "Document content was represented with BM25 [17].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "Stop words were removed and the remaining terms were stemmed using the Porter algorithm [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "The BM25 tuning parameters were set to the same values as used for TREC [17], K1 = 2 and b = 0.", "startOffset": 72, "endOffset": 76}, {"referenceID": 3, "context": "Links were represented using LF-IDF [5].", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "De Vries and Geva [5] found that normalising link frequencies decreased classification performance.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "De Vries and Geva [5] found this to be effective for classification.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "For each K-tree built, k-means++ [3] was run 20 times on the codebook vectors to create 15 clusters.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "The same approach was taken at INEX 2008 by De Vries and Geva [5].", "startOffset": 62, "endOffset": 65}], "year": 2017, "abstractText": "Random Indexing (RI) K-tree is the combination of two algorithms for clustering. Many large scale problems exist in document clustering. RI K-tree scales well with large inputs due to its low complexity. It also exhibits features that are useful for managing a changing collection. Furthermore, it solves previous issues with sparse document vectors when using Ktree. The algorithms and data structures are defined, explained and motivated. Specific modifications to Ktree are made for use with RI. Experiments have been executed to measure quality. The results indicate that RI K-tree improves document cluster quality over the original K-tree algorithm.", "creator": "LaTeX with hyperref package"}}}