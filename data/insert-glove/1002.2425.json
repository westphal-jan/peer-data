{"id": "1002.2425", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2010", "title": "Application of k Means Clustering algorithm for prediction of Students Academic Performance", "abstract": "The wimbledons ability to monitor the 3,378 progress microcredits of students academic turrids performance bislama is a arch-enemies critical issue to the deshayes academic fuggers community of higher 105.22 learning. pile-dwelling A leigh-on-sea system for balurghat analyzing 266.7 students results based on cluster gregorian analysis cowriter and uses standard statistical algorithms to b\u00fchlmann arrange their heged\u0171s scores long-line data titu according to pradeshiya the inventively level 37.04 of their hammerstein performance yanhua is kingsbury described. In bith this paper, we also deverill implemented k mean clustering algorithm killer7 for 6,772 analyzing 3,446 students crooked result dezong data. The aguardiente model was dakotah combined with sillman the gently deterministic model to cruces analyze naoko the students results 1.4750 of bestbuy.com a kabira private Institution glued in bigoted Nigeria which erewhon is servals a mangala good benchmark to brul monitor husseini the lilyana progression tonseth of wisconsin academic performance mabandla of students mussed in newgate higher concertos Institution for the purpose recolonization of medlock making an wathelet effective decision by the melicope academic planners.", "histories": [["v1", "Thu, 11 Feb 2010 20:41:28 GMT  (819kb)", "http://arxiv.org/abs/1002.2425v1", "IEEE format, International Journal of Computer Science and Information Security, IJCSIS January 2010, ISSN 1947 5500,this http URL"]], "COMMENTS": "IEEE format, International Journal of Computer Science and Information Security, IJCSIS January 2010, ISSN 1947 5500,this http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CY", "authors": ["o j oyelade", "o o oladipupo", "i c obagbuwa"], "accepted": false, "id": "1002.2425"}, "pdf": {"name": "1002.2425.pdf", "metadata": {"source": "CRF", "title": "Application of k-Means Clustering algorithm for prediction of Students\u2019 Academic Performance", "authors": [], "emails": ["Ola2000faith@yahoo.co.uk.", "frajooye@yahoo.com.", "ibidunobagbuwa@yahoo.com"], "sections": [{"heading": null, "text": "community of higher learning. A system for analyzing students\u2019 results based on cluster analysis and uses standard statistical algorithms to arrange their scores data according to the level of their performance is described. In this paper, we also implemented k-mean clustering algorithm for analyzing students\u2019 result data. The model was combined with the deterministic model to analyze the students\u2019 results of a private Institution in %igeria which is a good benchmark to monitor the progression of academic performance of students in higher Institution for the purpose of making an effective decision by the academic planners.\nKeywords- k \u2013 mean, clustering, academic performance, algorithm.\nI. INTRODUCTION\nGraded Point Average (GPA) is a commonly used indicator of academic performance. Many Universities set a minimum GPA that should be maintained in order to continue in the degree program. In some University, the minimum GPA requirement set for the students is 1.5. Nonetheless, for any graduate program, a GPA of 3.0 and above is considered an indicator of good academic performance. Therefore, GPA still remains the most common factor used by the academic planners to evaluate progression in an academic environment [1]. Many factors could act as barriers to students attaining and maintaining a high GPA that reflects their overall academic performance during their tenure in University. These factors could be targeted by the faculty members in developing strategies to improve student learning and improve their academic performance by way of monitoring the progression of their performance.\nTherefore, performance evaluation is one of the bases to monitor the progression of student performance in higher Institution of learning. Base on this critical issue, grouping of students into different categories according to their performance has become a complicated task. With traditional grouping of students based on their average scores, it is difficult to obtain a comprehensive view of the state of the students\u2019 performance and simultaneously discover important details from their time to time performance.\nWith the help of data mining methods, such as clustering algorithm, it is possible to discover the key characteristics from the students\u2019 performance and possibly use those characteristics for future prediction. There have been some promising results from applying k-means clustering algorithm with the Euclidean distance measure, where the distance is computed by finding the square of the distance between each scores, summing the squares and finding the square root of the sum [6].\nThis paper presents k-means clustering algorithm as a simple and efficient tool to monitor the progression of students\u2019 performance in higher institution.\nCluster analysis could be divided into hierarchical clustering and non-hierarchical clustering techniques. Examples of hierarchical techniques are single linkage, complete linkage, average linkage, median, and Ward. Non-hierarchical techniques include k-means, adaptive k-means, k-medoids, and fuzzy clustering. To determine which algorithm is good is a function of the type of data available and the particular purpose of analysis. In more objective way, the stability of clusters can be investigated in simulation studies [4]. The problem of selecting the \u201cbest\u201d algorithm/parameter setting is a difficult one. A good clustering algorithm ideally should produce groups with distinct non-overlapping boundaries, although a perfect separation can not typically be achieved in practice. Figure of merit measures (indices) such as the silhouette width [4] or the homogeneity index [5] can be used to evaluate the quality of separation obtained using a clustering algorithm. The concept of stability of a clustering algorithm was considered in [3]. The idea behind this validation approach is that an algorithm should be rewarded for consistency. In this paper, we implemented traditional kmeans clustering algorithm [6] and Euclidean distance measure of similarity was chosen to be used in the analysis of the students\u2019 scores.\nII. METHODOLOGY"}, {"heading": "A. Development of k-mean clustering algorithm", "text": "Given a dataset of n data points x1, x2, \u2026, xn such that each data point is in R d , the problem of finding the minimum\n292 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nvariance clustering of the dataset into k clusters is that of finding k points {mj} (j=1, 2, \u2026, k) in R d such that\nis minimized, where d(xi, mj) denotes the Euclidean distance between xi and mj. The points {mj} (j=1, 2, \u2026,k) are known as cluster centroids. The problem in Eq.(1) is to find k cluster centroids, such that the average squared Euclidean distance (mean squared error, MSE) between a data point and its nearest cluster centroid is minimized.\nThe k-means algorithm provides an easy method to implement approximate solution to Eq.(1). The reasons for the popularity of k-means are ease and simplicity of implementation, scalability, speed of convergence and adaptability to sparse data.\nThe k-means algorithm can be thought of as a gradient descent procedure, which begins at starting cluster centroids, and iteratively updates these centroids to decrease the objective function in Eq.(1). The k-means always converge to a local minimum. The particular local minimum found depends on the starting cluster centroids. The problem of finding the global minimum is NP-complete. The k-means algorithm updates cluster centroids till local minimum is found. Fig.1 shows the generalized pseudocodes of k-means algorithm; and traditional k-means algorithm is presented in fig. 2 respectively.\nBefore the k-means algorithm converges, distance and centroid calculations are done while loops are executed a number of times, say l, where the positive integer l is known as the number of k-means iterations. The precise value of l varies depending on the initial starting cluster centroids even on the same dataset. So the computational time complexity of the algorithm is O(nkl), where n is the total number of objects in the dataset, k is the required number of clusters we identified and l is the number of iterations, k\u2264n, l\u2264n [6].\n1 MSE = largenumber; 2 Select initial cluster centroids {mj}j\nK = 1;\n3 Do 4 OldMSE = MSE; 5 MSE1 = 0; 6 For j = 1 to k 7 mj = 0; nj = 0; 8 endfor 9 For i = 1 to n 10 For j = 1 to k 11 Compute squared Euclidean\ndistance d 2 (xi, mj);\n12 endfor 13 Find the closest centroid mj to xi; 14 mj = mj + xi; nj = nj+1; 15 MSE1=MSE1+ d 2 (xi, mj); 16 endfor 17 For j = 1 to k 18 nj = max(nj, 1); mj = mj/nj; 19 endfor 20 MSE=MSE1;\nwhile (MSE<OldMSE)\nFig.2: Traditional k-means algorithm [6]\nIII. RESULTS\nWe applied the model on the data set (academic result of one semester) of a university in Nigeria. The result generated is shown in tables 2, 3, and 4, respectively. In table 2, for k = 3; in cluster 1, the cluster size is 25 and the overall performance is 62.22. Also, the cluster sizes and the overall performances for cluster numbers 2 and 3 are 15, 29 and 45.73, and 53.03, respectfully. Similar analyses also hold for tables 3 and 4. The graphs are generated in figures 3, 4 and 5, respectively, where the overall performance is plotted against the cluster size.\nTable 5 shows the dimension of the data set (Student\u2019s scores) in the form N by M matrices, where N is the rows (# of students) and M is the column (# of courses) offered by each student.\nThe overall performance is evaluated by applying deterministic model in Eq. 2 [7] where the group assessment in each of the cluster size is evaluated by summing the average of the individual scores in each cluster.\nWhere\nN = the total number of students in a cluster and n = the dimension of the data\n293 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nIn Figure 3, the overall performance for cluster size 25 is 62.22% while the overall performance for cluster size 15 is 45.73% and cluster size 29 has the overall performance of 53.03%. This analysis showed that, 25 out of 79 students had a \u201cVery Good\u201d performance (62.22%), while 15 out of 79 students had performance in the region of very \u201cFair\u201d performance (45.73%) and the remaining 29 students had a \u201cGood\u201d performance (53.03%) as depicted in the performance index in table 1.\nFigure 4 shows the trends in performance analysis as follows; overall performance for cluster size 24 is 50.08% while the overall performance for cluster size 16 is 65.00%. Cluster size 30 has the overall performance of 58.89%, while cluster size 09 is 43.65%. The trends in this analysis indicated that, 24 students fall in the region of \u201cGood\u201d performance index in table 1 above (50.08%), while 16 students has performance in the region of \u201cVery Good\u201d performance (65.00%). 30 students has a \u201cGood\u201d performance (58.89%) and 9 students had performance of \u201cFair\u201d result (43.65%).\nIn figure 5, the overall performance for cluster size 19 is 49.85%, while the overall performance for cluster size 17 is 60.97%. Cluster size 9 has the overall performance of 43.65%, while the cluster size 14 has overall performance of 64.93% and cluster size 20 has overall performance of 55.79%. This performance analysis indicated that, 19 students crossed over to \u201cGood\u201d performance region (49.85%), while 17 students had \u201cVery Good\u201d performance results (60.97%). 9 students fall in the region of \u201cFair\u201d performance index (43.65%), 14 students were in the region of \u201cVery Good\u201d performance (64.93%) and the remaining 20 students had \u201cGood\u201d performance (55.79%)."}, {"heading": "1 25 62.22", "text": "Table 3: K = 4\nCluster # Cluster size Overall\nPerformance"}, {"heading": "1 24 50.08", "text": ""}, {"heading": "2 16 65.00", "text": ""}, {"heading": "3 30 58.89", "text": ""}, {"heading": "4 9 43.65", "text": "Table 4: K = 5"}, {"heading": "1 19 49.85", "text": ""}, {"heading": "2 17 60.97", "text": ""}, {"heading": "3 9 43.65", "text": ""}, {"heading": "4 14 64.93", "text": ""}, {"heading": "5 20 55.79", "text": "294 http://sites.google.com/site/ijcsis/ ISSN 1947-5500"}, {"heading": "IV. DISCUSSION AND CONCLUSION", "text": "In this paper, we provided a simple and qualitative methodology to compare the predictive power of clustering algorithm and the Euclidean distance as a measure of similarity distance. We demonstrated our technique using kmeans clustering algorithm [6] and combined with the deterministic model in [7] on a data set of private school results with nine courses offered for that semester for each student for total number of 79 students, and produces the numerical interpretation of the results for the performance evaluation. This model improved on some of the limitations of the existing methods, such as model developed by [7] and [8]. These models applied fuzzy model to predict students\u2019 academic performance on two dataset only (English Language and Mathematics) of Secondary Schools results. Also the research work by [9] only provides Data Mining framework for Students\u2019 academic performance. The research by [10] used rough Set theory as a classification approach to analyze student data where the Rosetta toolkit was used to evaluate the student data to describe different dependencies between the attributes and the student status where the discovered patterns are explained in plain English.\nTherefore, this clustering algorithm serves as a good benchmark to monitor the progression of students\u2019 performance in higher institution. It also enhances the decision making by academic planners to monitor the candidates\u2019 performance semester by semester by improving on the future academic results in the subsequence academic session.\nACKNOWLEDGMENT\nThis work was funded by Covenant University Center for Research and Development. We are grateful to Fahim A. M. and Salem A. M. for their useful materials. We also thank Dr. Obembe for his useful assessment which has improved on the quality of this work."}], "references": [{"title": "and K", "author": ["S. Sujit Sansgiry", "M. Bhosle"], "venue": "Sail, \u201cFactors that affect academic performance among pharmacy students,\u201d American Journal of Pharmaceutical Education", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Comparisons and validation of statistical clustering techniques for microarray gene expression data,", "author": ["Susmita Datta", "Somnath Datta"], "venue": "Bioinformatics, vol. 19,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A graphical aid to the interpretation and validation of cluster analysis,", "author": ["J Rousseeuw P"], "venue": "Journal of Computational Appl Math,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Algorithmic approaches to clustering gene expression data,", "author": ["R. Sharmir", "R. Sharan"], "venue": "In current Topics in Computational Molecular Biology MIT Press; pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Adaptive cluster analysis, classification and multivarite graphics,\u201dWeirstrass", "author": ["J. Mucha H"], "venue": "Institute for Applied Analysis and Stochastics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "An efficient enhanced k-means clustering algorithm,", "author": ["M. Fahim A", "M. Salem A", "A. Torkey F", "A. Ramadan M"], "venue": "Journal of Zhejiang University Science A.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "O", "author": ["J.O. Omolehin", "J.O. Oyelade"], "venue": "O. Ojeniyi and K. Rauf, \u201cApplication of Fuzzy logic in decision making on students\u2019 academic performance,\u201d Bulletin of Pure and Applied Sciences, vol. 24E(2), pp. 281-187", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "R", "author": ["J.O. Omolehin", "A.O. Enikuomehin"], "venue": "G. Jimoh and K. Rauf, \u201cProfile of conjugate gradient method algorithm on the performance appraisal for a fuzzy system,\u201d African Journal of Mathematics and Computer Science Research,\u201d vol. 2(3), pp. 030-037", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving Academic Performance of Students by Applying Data Mining Technique,", "author": ["N.V. Anand Kumar", "G.V. Uma"], "venue": "European Journal of Scientific Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Using Rough Set theory for Automatic Data Analysis,", "author": ["P Varapron"], "venue": "Congress on Science and Technology of Thailand,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Therefore, GPA still remains the most common factor used by the academic planners to evaluate progression in an academic environment [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "There have been some promising results from applying k-means clustering algorithm with the Euclidean distance measure, where the distance is computed by finding the square of the distance between each scores, summing the squares and finding the square root of the sum [6].", "startOffset": 268, "endOffset": 271}, {"referenceID": 3, "context": "In more objective way, the stability of clusters can be investigated in simulation studies [4].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Figure of merit measures (indices) such as the silhouette width [4] or the homogeneity index [5] can be used to evaluate the quality of separation obtained using a clustering algorithm.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "Figure of merit measures (indices) such as the silhouette width [4] or the homogeneity index [5] can be used to evaluate the quality of separation obtained using a clustering algorithm.", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "The concept of stability of a clustering algorithm was considered in [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "In this paper, we implemented traditional kmeans clustering algorithm [6] and Euclidean distance measure of similarity was chosen to be used in the analysis of the students\u2019 scores.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "So the computational time complexity of the algorithm is O(nkl), where n is the total number of objects in the dataset, k is the required number of clusters we identified and l is the number of iterations, k\u2264n, l\u2264n [6].", "startOffset": 215, "endOffset": 218}, {"referenceID": 5, "context": "2: Traditional k-means algorithm [6]", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "2 [7] where the group assessment in each of the cluster size is evaluated by summing the average of the individual scores in each cluster.", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "We demonstrated our technique using kmeans clustering algorithm [6] and combined with the deterministic model in [7] on a data set of private school results with nine courses offered for that semester for each student for total number of 79 students, and produces the numerical interpretation of the results for the performance evaluation.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "We demonstrated our technique using kmeans clustering algorithm [6] and combined with the deterministic model in [7] on a data set of private school results with nine courses offered for that semester for each student for total number of 79 students, and produces the numerical interpretation of the results for the performance evaluation.", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "This model improved on some of the limitations of the existing methods, such as model developed by [7] and [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "This model improved on some of the limitations of the existing methods, such as model developed by [7] and [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "Also the research work by [9] only provides Data Mining framework for Students\u2019 academic performance.", "startOffset": 26, "endOffset": 29}, {"referenceID": 9, "context": "The research by [10] used rough Set theory as a classification approach to analyze student data where the Rosetta toolkit was used to evaluate the student data to describe different dependencies between the attributes and the student status where the discovered patterns are explained in plain English.", "startOffset": 16, "endOffset": 20}], "year": 2010, "abstractText": "The ability to monitor the progress of students\u2019 academic performance is a critical issue to the academic community of higher learning. A system for analyzing students\u2019 results based on cluster analysis and uses standard statistical algorithms to arrange their scores data according to the level of their performance is described. In this paper, we also implemented k-mean clustering algorithm for analyzing students\u2019 result data. The model was combined with the deterministic model to analyze the students\u2019 results of a private Institution in %igeria which is a good benchmark to monitor the progression of academic performance of students in higher Institution for the purpose of making an effective decision by the academic planners. Keywordsk \u2013 mean, clustering, academic performance, algorithm.", "creator": "PrimoPDF http://www.primopdf.com/"}}}