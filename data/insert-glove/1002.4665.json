{"id": "1002.4665", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2010", "title": "Syntactic Topic Models", "abstract": "The syntactic topic paicv model (amauri STM) is a occultist Bayesian nonparametric model it.the of language landhi that sarasa discovers downtown latent 109.41 distributions of ponant words (topics) essad that rieders are -62 both semantically 4-page and syntactically chiral coherent. furlough The 295th STM models dependency rahula parsed corpora where sentences then-secretary are kishen grouped into documents. It tecklenburg assumes that sunbiz each non-football word is drawn lectured from a 2,068 latent arabah topic chosen hae-yung by 28.54 combining document - traianus level muhlenbergia features pramote and methandienone the dynham local syntactic context. Each document sawara has fuerteventura a distribution pernell over liberhan latent topics, amateure as in topic models, pastry which provides non-living the semantic consistency. plica Each element ketoacidosis in the meuchner dependency parse olmecs tree cotys also transmits has hej a stretched distribution wilhoite over kearsarge the magassa topics of adduces its nagler children, rei as pegah in kirpal latent - man-of-the-match state syntax historisch models, which floss provides the a-24 syntactic enorme consistency. issara These 1,861 distributions are convolved so zoraya that the topic of each word runway is likely under both bookstall its kakashi document yasi and vukassovich syntactic context. tausen We guangchang derive a fast astrocaryum posterior inference sahag\u00fan algorithm based on euro378 variational methods. We 101.32 report benrubi qualitative sanni and sisters-in-law quantitative studies on both 56-21 synthetic guhl data e-rotic and hand - parsed noyce documents. pentapolis We show meriden that the STM raymonds is raunch a froben more predictive niit model buda of language than current nuuk models beyaz\u0131t based llanbadarn only girlfight on escravos syntax lyotard or ormrod only ba\u010dki on topics.", "histories": [["v1", "Thu, 25 Feb 2010 00:00:47 GMT  (1485kb,DS)", "http://arxiv.org/abs/1002.4665v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI math.ST stat.TH", "authors": ["jordan l boyd-graber", "david m blei"], "accepted": true, "id": "1002.4665"}, "pdf": {"name": "1002.4665.pdf", "metadata": {"source": "CRF", "title": "Syntactic Topic Models", "authors": ["Jordan Boyd-Graber", "David M. Blei"], "emails": ["jbg@umiacs.umd.edu.", "blei@cs.princeton.edu."], "sections": [{"heading": null, "text": "Syntactic Topic Models\nJordan Boyd-Graber\u2217 Institute for Advanced Computer Studies University of Maryland\nDavid M. Blei\u2217\u2217 Computer Science Department Princeton University\nThe syntactic topic model (STM) is a Bayesian nonparametric model of language that discovers latent distributions of words (topics) that are both semantically and syntactically coherent. The STM models dependency parsed corpora where sentences are grouped into documents. It assumes that each word is drawn from a latent topic chosen by combining document-level features and the local syntactic context. Each document has a distribution over latent topics, as in topic models, which provides the semantic consistency. Each element in the dependency parse tree also has a distribution over the topics of its children, as in latent-state syntax models, which provides the syntactic consistency. These distributions are convolved so that the topic of each word is likely under both its document and syntactic context. We derive a fast posterior inference algorithm based on variational methods. We report qualitative and quantitative studies on both synthetic data and hand-parsed documents. We show that the STM is a more predictive model of language than current models based only on syntax or only on topics.\nWhen we read a sentence, we use two kinds of reasoning: one for understanding its syntactic structure and another for integrating its meaning into the wider context of other sentences, other paragraphs, and other documents. Both mental processes are crucial, and psychologists have found that they are distinct. A syntactically correct sentence that is semantically implausible takes longer for people to understand than its semantically plausible counterpart (Rayner et al. 1983). Furthermore, recent brain imaging experiments have localized these processes in different parts of the brain (Dapretto and Bookheimer 1999). Both of these types of reasoning should be accounted for in a probabilistic model of language.\nTo see how these mental processes interact, consider the following sentence from a travel brochure,\nNext weekend, you could be relaxing in ____.\nHow do we reason about filling in the blank? First, because the missing word is the object of a preposition, it should act like a noun, perhaps a location like \u201cbed,\u201d \u201cschool,\u201d or \u201cchurch.\u201d Second, because the document is about travel, we expect travel-related terms. This further restricts the space of possible terms, leaving alternatives like \u201cNepal,\u201d \u201cParis,\u201d or \u201cBermuda\u201d as likely possibilities. Each type of reasoning restricts the likely solution to a subset of words, but the best candidates for the missing word are in their intersection.\nIn this article we develop a probabilistic model of language that mirrors this process. Probabilistic modeling has emerged as a powerful formalism for expressing assumptions\n\u2217 3155 AV Williams, College Park, MD 20742. E-mail: jbg@umiacs.umd.edu. \u2217\u2217 35 Olden Street, Princeton NJ, 08540. E-mail: blei@cs.princeton.edu.\n\u00a9 2006 Association for Computational Linguistics\nar X\niv :1\n00 2.\n46 65\nv1 [\ncs .C\nL ]\n2 5\nFe b\n20 10\nabout natural language and analyzing texts under those assumptions (Manning and Sch\u00fctze 1999). Current models, however, tend to focus on finding and exploiting either syntactic or thematic regularities. On one hand, probabilistic syntax models capture how different words are used in different parts of speech and how those parts of speech are organized into sentences (Charniak 1997; Collins 2003; Klein and Manning 2002). On the other hand, probabilistic topic models find patterns of words that are thematically related in a large collection of documents (Blei et al. 2003; Griffiths et al. 2007).\nEach type of model captures one kind of regularity in language, but ignores the other kind of regularity. Returning to the example, suppose that the correct answer is the noun \u201cBermuda.\u201d A syntax model would fill in the missing word with a noun, but would ignore the semantic distinction between words like \u201cbed\u201d and \u201cBermuda.\u201d1 A topic model would consider travel words to be more likely than others, but would ignore functional differences between words like \u201csailed\u201d and \u201cBermuda.\u201d To arrive at \u201cBermuda\u201d with higher probability requires a model that simultaneously accounts for both syntax and theme.\nThus, our model assumes that language arises from an interaction between syntactic regularities at the sentence level and thematic regularities at the document level. The syntactic component examines the sentence at hand and restricts attention to nouns; the thematic component examines the rest of the document and restricts attention to travel words. Our model makes its ultimate prediction from the intersection of these two restrictions. As we will see, these modeling assumptions lead to a more predictive model of language.\nIn general, hierarchical Bayesian models of language posit that the observed words arise probabilistically via hidden structure, such as syntactic structure or thematic structure. Given a collection of texts, one uses posterior inference to uncover the hidden structure from the observed language. In topic models, one uncovers thematic patterns; in syntax models, one uncovers syntactic patterns.\nBoth topic models and syntax models assume that each word of the data is drawn from a mixture component, a distribution over a vocabulary that represents recurring patterns of words. The central difference between topic models and syntax models is how the component weights are shared: topic models are bag-of-words models where component weights are shared within a document; syntax models share components within a functional category (e.g. the production rules for non-terminals). Components learned from these assumptions reflect either document-level patterns of co-occurrence, which look like themes, or tree-level patterns of co-occurrence, which look like syntactic elements. In both topic models and syntax models, Bayesian non-parametric methods are used to embed the choice of the number of components into the model (Teh et al. 2006; Finkel et al. 2007). These methods further allow for new components to appear with new data.\nIn the syntactic topic model (STM), the components arise from both document-level and sentence-level distributions and therefore reflect both syntactic and thematic patterns in the texts. This captures the two types of understanding described above: the documentlevel distribution over components restricts attention to those that are thematically relevant; the tree-level distribution over components restricts attention to those that are syntactically appropriate. We emphasize that rather than choose between a thematic\n1 A proponent of lexicalized parsers might argue that conditioning on the word might be enough to answer this question completely. However, many of the most frequently used words have such broad meanings (e.g. \u201cgo\u201d) that knowledge of the broader context is necessary.\ncomponent or syntactic component from its appropriate context, as is done in the model of Griffiths et al (2005), components are drawn that are consistent with both sets of weights.\nThis complicates posterior inference algorithms and requires developing new methodology in hierarchical Bayesian modeling of language. However, it leads to a more expressive and predictive model. In Section 1 we review latent variable models for topics and syntax and Bayesian non-parametric methods. In Section 2, building on these formalisms, we present the STM. In Section 2.2 we derive a fast approximate posterior inference algorithm based on variational methods. Finally, in Section 3 we present qualitative and quantitative results on both synthetic text and a large collection of parsed documents."}, {"heading": "1. Background: Topics and Syntax", "text": "Our approach builds on probabilistic topic models, probabilistic syntactic models, and Bayesian non-parametric methods. We review these ideas here."}, {"heading": "1.1 Probabilistic Topic Models", "text": "Probabilistic topic models are hierarchical Bayesian models of text that can be used to automatically discover a hidden thematic structure in a large collection of otherwise unstructured documents. Topic models have emerged as a powerful tool for unsupervised analysis of text (Blei et al. 2003) and have been extended in many ways, e.g., to authorship (Rosen-Zvi et al. 2004), citation (Mimno and McCallum 2007), sentiment analysis (Blei and McAuliffe 2007; Titov and McDonald 2008), corpus exploration (Hall et al. 2008), part-of-speech labeling (Toutanova and Johnson 2008), discourse segmentation (Purver et al. 2006), word sense induction (Brody and Lapata 2009), and word sense disambiguation (Boyd-Graber et al. 2007). Topic models have also been applied to non-language data, such as images (Li Fei-Fei and Perona 2005), population genetics (Pritchard et al. 2000), and music (Hu and Saul 2009). There are several reviews of topic modeling and related literature (Blei and Lafferty 2009; Griffiths et al. 2007).\nHere we will build on latent Dirichlet allocation (LDA) (Blei et al. 2003), which is often used as a building block for other topic models. The modeling assumptions behind LDA are made clear through its generative probabilistic process, the imaginary process by which a document collection is created. LDA posits that there are K topics in a collection, each of which is a distribution over terms. For each document, LDA first draws a vector of topic proportions from a Dirichlet distribution and then draws each word from a topic which is chosen from those proportions. The corpus is associated with a set of topics, and each document as associated with a random mixture of those topics. In statistics, these kinds of assumptions are called mixed-membership assumptions (Erosheva et al. 2007).\nAnalyzing a corpus with LDA amounts to \u201creversing\u201d this process to compute the posterior distribution of the topic proportions, topic assignments, and topics conditioned on the observed documents. Of particular interest are the topics themselves, which reflect corpus-wide patterns of word co-occurrence, and the topic proportions, which describe the documents in terms of their constituent topics.2 Notice that LDA ignores the\n2 The topics tend to correspond to a psychologically plausible interpretation of the themes that pervade the documents (Griffiths et al. 2007). Thus, they are called topics.\norder of words within a document but uses the document context to make inferences about the topics. For example, the term \u201cstock\u201d might have a high probability in both a financial topic and a culinary topic. But if \u201cstock,\u201d \u201csoup,\u201d and \u201cbroth\u201d also appear in the document, the posterior will likely assign appearances of \u201cstock\u201d to the culinary topic.\nTopic models represent a fully probabilistic perspective on techniques like latent semantic analysis (LSA) (Deerwester et al. 1990) and probabilistic latent semantic analysis (pLSA) (Hofmann 1999). LSA and pLSA do not embody fully generative probabilistic processes. By adopting a fully generative model, LDA exhibits better generalization performance and is more easily used as a module in more complicated models. (Blei et al. 2003; Blei and Lafferty 2009)."}, {"heading": "1.2 Probabilistic Syntax Models", "text": "LDA is effective at capturing semantic correlations between words, but it ignores syntactic correlations and connections. The finite tree with independent children model (FTIC) can be seen as the syntactic complement to LDA (Finkel et al. 2007). As in LDA, this model assumes that observed words are generated by latent states. However, rather than considering words in the context of their shared document, the FTIC considers each word in the context of its sentence as determined by its location in a dependency parse.\nThe FTIC embodies a generative process over a collection of sentences with given parses. It is parameterized by a set of \u201csyntactic states,\u201d where each state is associated with three parameters: a distribution over terms, a set of transition probabilities to other states, and a probability of being chosen as the root state. Each sentence is generated by traversing the structure of the parse tree. For each node, draw a syntactic state from\nthe transition probabilities of its parent (or root probabilities) and draw the word from the corresponding distribution over terms. A parse of a sentence with three words is depicted as a graphical model in Figure 1.\nWhile LDA is constructed to analyze a collection of documents, the FTIC is constructed to analyze a collection of parsed sentences. The states discovered through posterior inference correlate with part of speech labels (Finkel et al. 2007). For LDA the components respect the way words co-occur in documents. For FTIC the components respect the way words occur within parse trees."}, {"heading": "1.3 Random Distributions and Bayesian non-parametric methods", "text": "Many recently developed probabilistic models of language, including those described above, employ distributions as random variables. These random distributions are sometimes a prior over a parameter, as in traditional Bayesian statistics, or a latent variable within the model. For example, in LDA the topic proportions and topics are random distributions; in the FTIC, the transition probabilities and term generating distributions are random.\nIn this section, we review the Dirichlet distribution, a commonly used distribution of multinomial parameter vectors, and the stick breaking distribution, a distribution on multinomial parameter vectors with a countably infinite number of components. We will describe the connection between the stick-breaking distribution and the Dirichlet process (DP), which is a distribution over arbitrary discrete distributions and a foundational building block of Bayesian nonparametric methods. These distributions are pivotal to the STM.\nA (K \u2212 1) dimensional Dirichlet distribution is a distribution over finite probability distributions of K elements. Thus its support is the simplex, non-negative vectors that sum to one.3 It is parameterized by a mean value \u03c1, which is a point on the (K \u2212 1) simplex, and a scalar \u03bb, which controls the variance around the mean. A random variable drawn from a Dirichlet is denoted \u03b8 \u223c Dir(\u03c1\u03bb).\nIn LDA, for example, the per-document topic proportions are drawn from a (K \u2212 1) dimensional Dirichlet and the topics themselves are assumed drawn from a (V \u2212 1) dimensional Dirichlet. (Recall that K is the number of topics and V is the number of terms in the vocabulary.) In the FTIC, the syntactic states are assumed drawn from a (V \u2212 1) dimensional Dirichlet, and the transition probabilities between states are drawn from a (K \u2212 1) dimensional Dirichlet.4\nBoth the FTIC and LDA assume that the number of latent components, i.e., topics or syntactic states, is fixed. Choosing this number a priori can be difficult. Recent research has extended Bayesian non-parametric methods to build more flexible models where the number of latent components is unbounded and is determined by the data (Teh et al. 2006; Liang and Klein 2007). The STM will use this methodology.\nWe first describe the stick breaking distribution, a distribution over the infinite simplex. The idea behind this distribution is to draw an infinite number of Beta random variables, i.e., values between zero and one, and then combine them to form a vector\n3 The dimensionality is (K \u2212 1) rather than K because of the constraint that the vector sum to one. 4 The Dirichlet distribution is a convenient distribution for generating multinomials, but there are other\nalternatives that provide different sparsity or correlation patterns. These have proved promising in limited-data frameworks; the logistic normal prior has been applied to grammar induction (Cohen et al. 2008) and integer programming has been applied to unsupervised part of speech tagging (Ravi and Knight 2009).\nwhose infinite sum is one. This can be understood with a stick-breaking metaphor. Consider a unit length stick that is infinitely broken into smaller and smaller pieces. The length of each successive piece is determined by taking a random proportion of the remaining stick. The random proportions are drawn from a Beta distribution,\n\u00b5k \u223c Beta(1, \u03b1),\nand the resulting stick lengths are defined from these breaking points,\n\u03b2k = \u00b5k k\u22121\u220f l=1 (1\u2212 \u00b5l).\nWith this process, the vector \u03b2 is a point on the infinite simplex (Sethuraman 1994). This distribution is notated \u03b2 \u223c GEM(\u03b1).5\nThe stick breaking distribution is a size-biased distribution\u2014the probability tends to concentrate around the initial components. The Beta parameter \u03b1 determines how many components of the probability vector will have high probability. Smaller values of \u03b1 result in a peakier distributions; larger values result in distributions that are more spread out. Regardless of \u03b1, for large enough k, the value of \u03b2k still goes to zero because the vector must sum to one. Figure 2 illustrates draws from the stick breaking distribution for several values of \u03b1.\nThe stick-breaking distribution provides a constructive definition of the Dirichlet process, which is a distribution over arbitrary distributions [Ferguson 1973]. Consider a base distribution G0, which can be any type of distribution, and the following random variables\n\u03b2i \u223c GEM(\u03b1) i \u2208 {1, 2, 3, . . .}\n\u00b5i \u223c G0 i \u2208 {1, 2, 3, . . .}.\nNow define the random distribution\nG = \u221e\u2211 i=1 \u03b2i\u03b4\u00b5i(\u00b7)\nwhich places mass \u03b2i on the point \u00b5i. This is a random distribution because its components are random variables, and note that it is a discrete distribution even if G0 is defined on a continuous space. Marginalizing out \u03b2i and \u00b5i, the distribution ofG is called a Dirichlet process (DP). It is parameterized by the base distribution G0 and a scaling parameter \u03c1. The scaling parameter, as for the finite Dirichlet, determines how close the resulting random distribution is to G0. Smaller \u03c1 yields distributions that are further from G0; larger \u03c1 yields distributions that are closer to G0.6 The base distribution is also called the mean of the DP because E[G |G0, \u03c1] = G0. The Dirichlet process is a commonly\n5 GEM stands for Griffiths, Engen and McCloskey (Pitman 2002). 6 The formal connection between the DP and the finite dimensional Dirichlet is that the finite dimensional\ndistributions of the DP are finite Dirichlet, and the DP was originally defined via the Kolmogorov consistency theorem(Ferguson 1973). The infinite stick breaking distribution was developed for a more constructive definition (Sethuraman 1994). We will not be needing these mathematical details here.\n0 5 15 25\n0. 0\n0. 4\n0. 8\n\u03b1 = 0.1\n5 10 15\n0. 0\n0. 4\n0. 8\n\u03b1 = 0.1\n1 3 5 7\n0. 0\n0. 4\n0. 8\n\u03b1 = 0.1\n1 3 5 7\n0. 0\n0. 4\n0. 8\n\u03b1 = 0.1\n0 5 15 25\n0. 0\n0. 4\n0. 8\n\u03b1 = 1\n0 5 15 25\n0. 0\n0. 4\n0. 8\n\u03b1 = 1\n0 5 15 25\n0. 0\n0. 4\n0. 8\n\u03b1 = 1\n0 20 40 60\n0. 0\n0. 4\n0. 8\n\u03b1 = 1\n0 50 150 250\n0. 0\n0. 4\n0. 8\n\u03b1 = 10\n0 50 150 250\n0. 0\n0. 4\n0. 8\n\u03b1 = 10\n0 50 150 250\n0. 0\n0. 4\n0. 8\n\u03b1 = 10\n0 50 150 250\n0. 0\n0. 4\n0. 8\n\u03b1 = 10\nFigure 2 Draws for three settings of the parameter \u03b1 of a stick-breaking distribution (enough indices are shown to account for 0.95 of the probability). When the parameter is substantially less than one (top row), very low indices are favored. When the parameter is one (middle row), the weight tapers off more slowly. Finally, if the magnitude of the parameter is larger (bottom row), weights are nearer a uniform distribution.\nused prior in Bayesian non-parametric statistics, where we seek a prior over arbitrary distributions (Antoniak 1974; Escobar and West 1995; Neal 2000).\nIn a hierarchical model, the DP can be used to define a topic model with an unbounded number of topics. In such a model, unlike LDA, the data determine the number of topics through the posterior and new documents can ignite previously unseen topics. This extension is an application of a hierarchical Dirichlet process (HDP), a model of grouped data where each group arises from a DP whose base measure is itself a draw from a DP (Teh et al. 2006). In the HDP for topic modeling, the finite dimensional Dirichlet distribution over per-document topic proportions is replaced with a draw from a DP, and the base measure of that DP is drawn once per-corpus from a stick-breaking distribution. The stick-breaking random variable describes the overall prominence of topics in a collection; the draws from the Dirichlet process describe how each document exhibits those topics.\nSimilarly, applying the HDP to the FTIC model of Section 1.2 results in a model where the mean of the Dirichlet process represents the overall prominence of syntactic states. This extension is described as the infinite tree with independent children (ITIC) (Finkel et al. 2007). For each syntactic state, the transition distributions drawn from the Dirichlet process allow each state to prefer certain children states in the parse tree. Other work has applied this non-parametric framework to create language models (Teh 2006), full parsers for Chomsky normal form grammars (Liang et al. 2007), models of lexical acquisition (Goldwater 2007), synchronous grammars (Blunsom et al. 2008), and adaptor grammars for morphological segmentation (Johnson et al. 2006)."}, {"heading": "2. The Syntactic Topic Model", "text": "Topic models like LDA and syntactic models like FTIC find different decompositions of language. Syntactic models ignore document boundaries but account for the order of words within each sentence\u2013thus the components of syntactic models reflect how words are used in sentences. Topic models respect document boundaries but ignore the order of words within a document\u2013thus the components of topic models reflect how words are used in documents. We now develop the syntactic topic model (STM), a hierarchical probabilistic model of language that finds components which reflect both the syntax of the language and the topics of the documents.\nFor the STM, our observed data are documents, each of which is a collection of dependency parse trees. (Note that in LDA, the documents are simply collections of words.) The main idea is that words arise from topics, and that topic occurrence depends on both a document-level variable and parse tree-level variable. We emphasize that, unlike a parser, the STM does not model the tree structure itself and nor does it use any syntactic labeling. Only the words as observed in the tree structure are modeled.\nThe document-level and parse tree-level variables are both distributions over topics, which we call topic weights. These distributions are never drawn from directly. Rather, they are convolved\u2014that is, they are multiplied and renormalized\u2014and the topic assignment for a word is drawn from the convolution. The parse-tree level topic weight enforces syntactic consistency and the document-level topic weight enforces thematic consistency. The resulting set of topics\u2014the distributions over words that the topic weights refer to\u2014will be those that thus reflect both thematic and syntactic constraints. Our model is a Bayesian nonparametric model, so the number of such topics is determined by the data.\nWe now describe this model in more mathematical detail. The STM contains topics (\u03c4 ), transition distributions (\u03c0), per-document topic weights (\u03b8), and top level weights (\u03b2) as hidden random variables. In the STM, topics are multinomial distributions over a fixed vocabulary (\u03c4k). Each topic maintains a transition vector which governs the topics assigned to children of parents assigned a given topic (\u03c0k). Document weights model how much a document is about specific topics. Finally, each word has a topic assignment (zd,n) that decides from which topic the word is drawn. The STM posits a joint distribution using these building blocks and, from the posterior conditioned on the observed documents, we find transitions, per-document topic distributions, and topics.\nAs mentioned, we use Bayesian non-parametric methods to avoid having to set the number of topics. We assume that there is a vector \u03b2 of infinite length which tells us which topics are actually in use (as discussed in Section 1.3). These top-level weights are a random probability distribution drawn from a stick-breaking distribution. Putting this all together, the generative process for the data is as follows:\n1. Choose global weights \u03b2 \u223c GEM(\u03b1) 2. For each topic index k = {1, . . . }:\n(a) Choose topic \u03c4k \u223c Dir(\u03c3\u03c1u) (b) Choose transition distribution \u03c0k \u223c DP(\u03b1T\u03b2)\n3. For each document d = {1, . . .M}: (a) Choose document weights \u03b8d \u223c DP(\u03b1D\u03b2) (b) For each sentence root node with index\n(d, r) \u2208 SENTENCE-ROOTSd: i. Choose topic assignment zd,r \u221d \u03b8d\u03c0start\nii. Choose root word wd,r \u223c mult(1, \u03c4zr )\n(c) For each additional child with index (d, c) and parent with index (d, p):\ni. Choose topic assignment\nzd,c \u221d \u03b8d\u03c0zd,p (1)\nii. Choose word wd,c \u223c mult(1, \u03c4zd,n)\nThis process is illustrated as a probabilistic graphical model in Figure 3. Data analysis with this model amounts to \u201creversing\u201d this process to determine the posterior distribution of the latent variables. The posterior distribution is conditioned on observed words organized into parse trees and documents. It provides a distribution over all of the hidden structure\u2014the topics, the syntactic transition probabilities, the per-document topic weights, and the corpus-wide topic weights.\nBecause both documents and local syntax shape the choice of possible topics for a word, the posterior distribution over topics favors topics that are consistent with both contexts. For example, placing all nouns in a single topic would respect the syntactic constraints but not the thematic, document-level properties, as not all nouns are equally likely to appear in a given document. Instead, the posterior prefers topics which would divide syntactically similar words into different categories based on how frequently they co-occur in documents.\nIn addition to determining what the topics are, i.e., which words appear in a topic with high probability, the posterior also defines a distribution over how those topics are used. It encourages topics to appear in similar documents based on the per-document topic distributions \u03b8 and encourages topics to appear in similar similar local syntactic contexts based on the transition distribution \u03c0. For each word, two different views of its generation are at play. On one hand, a word is part of a document and reflects that document\u2019s themes. On the other hand, a word is part of a local syntactic structure and reflects the likely type of word that is associated with a child of its parent. The posterior balances both these views to determine which topic is associated with each word.\nFinally, through the stick-breaking and DP machinery, the posterior selects the number of topics that are used. This strikes a balance between explaining the data well (e.g. reflecting syntax and document-level properties) and not using too many topics, as governed by the hyperparameter \u03b1 (see Section 1.3).\nAs we will see below, combining document-level properties and syntax (Equation 1) complicates posterior inference (compared to HDP or ITIC) but allows us to simultaneously capture both syntactic and semantic patterns. Under certain limiting assumptions, the STM reduces to the models discussed in Section 1 . The STM reduces to the HDP if we fix \u03c0 to be a vector of ones, thus removing the influence of the tree structure. The STM reduces to the ITIC if we fix \u03b8 to be a vector of ones, removing the influence of the documents."}, {"heading": "2.1 Relationships to Other Work", "text": "The STM attempts to discover patterns of syntax and semantics simultaneously. In this section, we review previous methods to model syntax and semantics simultaneously and the statistical tools that we use to combine syntax and semantics. We also discuss other methodologies from word sense disambiguation, word clustering, and parsers that are similar to the STM.\nWhile the STM combines topics and syntax using a single distribution (Equation 1), an alternative is, for each word, to choose one of the two distributions. In such a model, the topic assignment comes from either the parent\u2019s topic transition \u03c0z(d,p) or document weights \u03b8d, based on a binary selector variable (instead of being drawn from a product of the two distributions). Griffiths et al\u2019s topics and syntax model (2005) did this on the linear order of words in a sentence. A mixture of topics and syntax in a similar manner over parse trees would create different types of topics, individually modeling either topics or syntax. It would not, however, enforce consistency with parent nodes and a document\u2019s themes. A word need only be consistent with either view.\nRather, the STM draws on the idea behind the product of experts (Hinton 1999), multiplying two vectors and renormalizing to obtain a new distribution. Taking the point-wise product can be thought of as viewing one distribution through the \u201clens\u201d of another, effectively choosing only words whose appearance can be explained by both.\nInstead of applying the lens to the selection of the latent classes, the topics, once selected, could be altered based on syntactic features of the text. This is the approach taken by TagLDA (Zhu et al. 2006), where each word is associated with a single tag (such as a part of speech), and the model learns a weighting over the vocabulary terms for each tag. This weighting is combined with the per-topic weighting to emit the words. Unlike the STM, this model does not learn relationships between different syntactic classes and, because the tags are fixed, cannot adjust its understanding of syntax to better reflect the data.\nThere has also been other work that does not seek to model syntax explicitly but nevertheless seeks to use local context to influence topic selection. One example is the hidden topic Markov model (Gruber et al. 2007), which finds chains of homogeneous topics within a document. Like the STM and Griffiths et al, the HTMM sacrifices the exchangibility of a topic model to incorporate local structure. Similarly, Wallach\u2019s bigram\ntopic model (Wallach 2006) assumes a generative model that chooses topics in a fashion identical to LDA but instead chooses words from a distribution based on per-topic bigram probabilities, thus partitioning bigram probabilities across topics.\nA similar vein of research is discourse-based WSD methods. The Yarowsky algorithm, for instance, uses clusters of similar contexts to disambiguate the sense of a word in a given context (Yarowsky 1995; Abney 2004). While the result does not explicitly model syntax, it does have a notion of both document theme (as all senses in a document must have the same sense) and the local context of words (the feature vectors used for clustering mentions). However, the algorithm is only defined on a word-by-word basis and does not build a consistent picture of the corpus for all the words in a document.\nLocal context is better captured by explicitly syntactic models. Work such as Lin similarity (Lin 1998) and semantic space models (Pad\u00f3 and Lapata 2007) build sets of related terms that appear in similar syntactic contexts. However, they cannot distinguish between uses that always appear in different kinds of documents. For instance, the string \u201cfly\u201d is associated with both terms from baseball and entomology.\nThese syntactic models use the output of parsers as input. Some parsing formalisms, such as adaptor grammars (Johnson et al. 2006; Johnson 2009), are broad and expressive enough to also describe topic models. However, there has been no systematic attempt to combine syntax and semantic in such a unified framework. The development of statistical parsers has increasingly turned to methods to refine the latent classes that generate the words and transitions present in a parser. Whether through subcategorization (Klein and Manning 2003) or lexicalization (Collins 2003; Charniak 2000), broad categories are constrained to better model idiosyncrasies of the text. While the STM is not a full parser, it offers an alternate way of constraining the latent classes of terms to be consistent across similar documents."}, {"heading": "2.2 Posterior inference with variational methods", "text": "We have described the modeling assumptions behind the STM. As detailed, the STM assumes a decomposition of the parsed corpus by a hidden semantic and syntactic structure encoded with latent variables. Given a data set, the central computational challenge for the STM is to compute the posterior distribution of that hidden structure given the observed documents, and data analysis proceeds by examining this distribution. Computing the posterior is \u201clearning from data\u201d from the perspective of Bayesian statistics.\nThis posterior distribution, as for many hierarchical Bayesian models, is not tractable to compute exactly and we must appeal to an approximation. (Developing algorithms for approximating posterior distributions of complex hierarchical models is an active research problem in Bayesian statistics and machine learning.) One of the most widely used approximation techniques for such models is Monte Carlo Markov chain (MCMC) sampling, where one samples from a Markov chain whose limiting distribution is the posterior of interest (Neal 1993; Robert and Casella 2004). Gibbs sampling in particular, where the Markov chain is defined by the conditional distribution of each latent variable, has found widespread use in Bayesian non-parametric models and topic models (Neal 1993; Teh 2006; Griffiths and Steyvers 2004; Finkel et al. 2007).\nMCMC is a powerful methodology, but it has drawbacks. Convergence of the sampler to its stationary distribution is difficult to diagnose, and sampling algorithms can be slow to converge in high dimensional models (Robert and Casella 2004). An alternative to MCMC is variational inference. Variational methods, which are based on related techniques from statistical physics, use optimization to find a distribution over the latent\nvariables that is close to the posterior of interest (Jordan et al. 1999; Wainwright and Jordan 2008). Variational methods provide effective approximations in topic models and non-parametric Bayesian models (Blei et al. 2003; Blei and Jordan 2005; Teh et al. 2006; Liang et al. 2007; Kurihara et al. 2007).\nVariational methods enjoy a clear convergence criterion and tend to be faster than MCMC in high-dimensional problems.7 Variational methods provide particular advantages over sampling when latent variable pairs are not conjugate. Gibbs sampling requires conjugacy, and other forms of sampling that can handle non-conjugacy, such as Metropolis-Hastings, are much slower than variational methods. Non-conjugate pairs appear in the dynamic topic model (Blei and Lafferty 2006; Wang et al. 2008), correlated topic model (Blei et al. 2007), and in the STM considered here. Specifically, in the STM the topic assignment is drawn from a renormalized product of two Dirichlet-distributed vectors (Equation 1). The distribution for each word\u2019s topic does not form a conjugate pair with the document or transition topic distributions. In this section, we develop an approximate posterior inference algorithm for the STM that is based on variational methods.\nOur goal is to compute the posterior of topics \u03c4 , topic transitions \u03c0, per-document weights \u03b8, per-word topic assignments z, top-level weights \u03b2 given a collection of documents and the model described in Section 2. The difficulty around this posterior is that the hidden variables are connected through a complex dependency pattern. With a variational method, we begin by positing a family of distributions of the same variables with a simpler dependency pattern. This distribution is called the variational distribution. Here we use the fully-factorized variational distribution,\nq(\u03b2, z,\u03b8,\u03c0, \u03c4 |\u03b2\u2217,\u03c6,\u03b3,\u03bd) = q(\u03b2|\u03b2\u2217) \u220f k q(\u03c0k|\u03bdk) \u220f d\n[ q(\u03b8d|\u03b3d)\n\u220f n q(zd,n|\u03c6d,n)\n] .\nNote that the latent variables are independent and each is governed by its own parameter. The idea behind variational methods is to adjust these parameters to find the member of this family that is close to the true distribution.\nFollowing Liang (2007), q(\u03b2|\u03b2\u2217) is not a full distribution but is a degenerate point estimate truncated so that all weights with index greater thanK are zero in the variational distribution. The variational parameters \u03b3d and \u03bdz index Dirichlet distributions, and \u03c6n is a topic multinomial for the nth word.\nWith this variational family in hand, we optimize the evidence lower bound (ELBO), a lower bound on the marginal probability of the observed data,\nL(\u03b3, \u03bd, \u03c6;\u03b2, \u03b8, \u03c0, \u03c4) ="}, {"heading": "Eq [log p(\u03b2|\u03b1)] + Eq [log p(\u03b8|\u03b1D,\u03b2)] + Eq [log p(\u03c0|\u03b1P ,\u03b2)] + Eq [log p(z|\u03b8,\u03c0)]", "text": "+Eq [log p(w|z, \u03c4 )] + Eq [log p(\u03c4 |\u03c3)]\u2212 Eq [log q(\u03b8) + log q(\u03c0) + log q(z)] . (2)\nVariational inference amounts to fitting the variational parameters to tighten this lower bound. This is equivalent to minimizing the KL divergence between the variational distribution and the posterior. Once fit, the variational distribution is used as an approximation to the posterior.\n7 Understanding the general trade-offs between variational methods and Gibbs sampling is an open research question.\nOptimization of Equation 2 proceeds by coordinate ascent, optimizing each variational parameter while holding the others fixed. Each pass through the variational parameters increases the ELBO, and we iterate this process until reaching a local optimum. When possible, we find the per-parameter maximum value in closed form. When such updates are not possible, we employ gradient-based optimization (Galassi et al. 2003).\nOne can divide the ELBO into document terms and global terms. The document terms reflect the variational parameters of a single document and the global terms reflect variational parameters which are shared across all documents. This can be seen in the plate notion in Figure 4; the variational parameters on the right hand side are specific to individual documents. We expand Equation 2 and divide it into a document component (Equation A.2) and a global component (Equation C.1), which contains a sum of all the document contributions, in the appendix.\nIn coordinate ascent, the global parameters are fixed as we optimize the document level parameters. Thus, we can optimize a single document\u2019s contribution to the ELBO ignoring all other documents. This allows us to parallelize our implementation at the document level; each parallel document-level optimization is followed by an optimization step for the global variational parameters. We iterate these steps until we find a local optimum. In practice, several random starting points are used and we select the variational parameters that reach the best local optimum.\nIn the next sections, we outline the variational updates for the word-specific terms, document-specific terms, and corpus-wide terms. This exposition preserves the parallelization in our implementation and highlights the separate influences of topic modeling and syntactic models.\n2.2.1 Document-specific Terms. We begin with \u03c6d,n, the variational parameter that corresponds to the nth observed word\u2019s assignment to a topic. We can explicitly solve\nfor the value of \u03c6n which maximizes document d\u2019s contribution to the ELBO:\n\u03c6n,i \u221d exp \u03a8 (\u03b3i)\u2212\u03a8(\u2211Kj=1 \u03b3j)+ K\u2211 j=1 \u03c6p(n),j ( \u03a8 (\u03bdj,i)\u2212\u03a8 (\u2211K k=1 \u03bdj,k ))\n\u2212 \u2211 c\u2208c(n) \u03c9\u22121c K\u2211 j \u03b3j\u03bdi,j\u2211 k \u03b3k \u2211 k \u03bdi,k + \u2211 c\u2208c(n) K\u2211 j=1 \u03c6c,j ( \u03a8 (\u03bdi,j)\u2212\u03a8 (\u2211K k=1 \u03bdi,k )) + log \u03c4i,wd,n\n . (3) (Note that we have suppressed the document index d on \u03c6 and \u03b3.)\nThis update reveals the influences on our estimate of the posterior of a single word\u2019s topic assignment. In the first line, the first two terms with the Dirichlet parameter \u03b3 show the influence of the document\u2019s distribution over topics; the term with multinomial parameter \u03c6p(n) and Dirichlet parameter \u03bd reflects the interaction between the topic of the parent and transition probabilities. In the second line, the interaction between the document and transitions forces the document and syntax to be consistent (this is mediated by an additional variational parameter \u03c9c discussed in Appendix 4). In the final line, the influence of the children\u2019s\u2019 topic on the current word\u2019s topic is expressed in the first term, and the probability of a word given a topic in the second.\nThe other document-specific term is the per-document variational Dirichlet over topic proportions \u03b3d. Intuitively, topic proportions should reflect the expected number of words assigned to each topic in a document (the first two terms of equation 4), with the constraint that \u03b3 must be consistent with the syntactic transitions in the document, which is reflected by the \u03bd term (the final term of Equation 4). This interaction prevents us from performing the update directly, so we use the gradient (derived in Appendix 2.2.1)\n\u2202L \u2202\u03b3i = \u03a8\u2032 (\u03b3i)\n( \u03b1D,i\u03b2 \u2217 +\nN\u2211 n=1 \u03c6n,i \u2212 \u03b3i\n) \u2212\u03a8\u2032 (\u2211N j=1 \u03b3j ) K\u2211 j=1 [ \u03b1D,j\u03b2 \u2217 + N\u2211 n=1 \u03c6n,j \u2212 \u03b3j ]\n\u2212 N\u2211 n=1 \u03c9\u22121n K\u2211 j=1 \u03c6p(n),j \u03bdj,i\u2211Nk 6=j \u03b3k \u2212\u2211Nk 6=j \u03bdj,k\u03b3k(\u2211N k=1 \u03b3k )2\u2211N k=1 \u03bdj,k  (4) to optimize a document\u2019s ELBO contribution using numerical methods.\nNow we turn to updates which require input from all documents and cannot be parallelized. Each document optimization, however, produces expected counts which are summed together; this is similar to the how the the E-step of EM algorithms can be parallelized and summed as input to the M-step (Wolfe et al. 2008).\n2.2.2 Global Variational Terms. In this section, we consider optimizing the variational parameters for the transitions between topics and the top-level topic weights. Note that these variational parameters, in contrast with the previous section, are more concerned with the overall syntax, which is shared across all documents. Instead of optimizing a single ELBO term for each document, we now seek to maximize the entirety of Equation 2, expanded in Equation C.1 in the appendix.\nThe non-parametric models in Section 1.3 use a random variable \u03b2 drawn from a stick-breaking distribution to control how many components the model uses. The prior for \u03b2 attempts use as few topics as possible; the ELBO balances this desire against using more topics to better explain the data. We use numerical methods to optimize \u03b2 with respect to the gradient of the global ELBO, which is given in Equation C.2 in the appendix.\nFinally, we optimize the variational distribution \u03bdi. If there were no interaction between \u03b8 and \u03c0, the update for \u03bdi,j would be proportional to the expected number of transitions from parents of topic i to children of topic j (this will set the first two terms of Equation 5 to zero). However, the objective function also encourages \u03bd to be consistent with \u03b3 (the final term of Equation 5); thus, if \u03b3 excludes topics from being observed in a document, the optimization will not allow transitions to those topics. Again, this optimization is done using numerical optimization using the gradient of the ELBO,\n\u2202L\n\u2202\u03bdi,j = \u03a8\u2032 (\u03bdi,j) \u03b1P,j + N\u2211 n=1 \u2211 c\u2208c(n) \u03c6n,i\u03c6c,j \u2212 \u03bdi,j  \u2212\u03a8\u2032 (\u2211K k=1 \u03bdi,k\n) K\u2211 k=1 \u03b1P,k + N\u2211 n=1 \u2211 c\u2208c(n) \u03c6n,i\u03c6c,k \u2212 \u03bdi,k \n\u2212 N\u2211 n \u03c6n,i \u2211 c\u2208c(n) \u03c9\u22121c \u03b3j\u2211Nk 6=j \u03bdi,k \u2212\u2211Nk 6=j \u03bdi,k\u03b3k(\u2211N k=1 \u03bdj,k )2\u2211N k=1 \u03b3k . (5)"}, {"heading": "3. Experiments", "text": "We demonstrate how the STM works on data sets of increasing complexity. First, we show that the STM captures properties of a simple synthetic dataset that elude both topic and syntactic models individually. Next, we use a larger real-word dataset of hand-parsed sentences to show that both thematic and syntactic information is captured by the STM."}, {"heading": "3.1 Topics Learned from Synthetic Data", "text": "We demonstrate the STM on synthetic data that resemble natural language. The data were generated using the grammar specified in Table 1. Each of the parts of speech except for prepositions and determiners was divided into themes, and a document contains a single theme for each part of speech. For example, a document can only contain nouns from a single \u201ceconomic,\u201d \u201cacademic,\u201d or \u201clivestock\u201d theme, verbs from a possibly different theme, etc. Documents had between twenty and fifty sentences. An example of two documents is shown in Figure 5.\nUsing a truncation level of 16, we fit three different non-parametric Bayesian language models to the synthetic data (Figure 6).8 Because the infinite tree model is aware of the tree structure but not documents, it is able to separate all parts of speech\n8 In Figure 6 and Figure 7, we mark topics which represent a single part of speech and are essentially the lone representative of that part of speech in the model. This is a subjective determination of the authors, does not reflect any specialization or special treatment of topics by the model, and is done merely for didactic purposes.\nFixed Syntax S \u2192 VP\nVP \u2192 NP V (PP) (NP) NP \u2192 (Det) (Adj) N (PP) PP \u2192 P NP\nP \u2192 (\u201cabout\u201d, \u201con\u201d, \u201cover\u201d, \u201cwith\u201d) Det \u2192 (\u201ca\u201d, \u201cthat\u201d, \u201cthe\u201d, \u201cthis\u201d)\nDocument-specific Vocabulary V \u2192 (\u201cfalls\u201d, \u201cruns\u201d, \u201csits\u201d) or (\u201cbucks\u201d, \u201cclimbs\u201d, \u201cfalls\u201d, \u201csurges\u201d) . . . N \u2192 (\u201cCOW\u201d, \u201cPONY\u201d, \u201cSHEEP\u201d) or\n(\u201cMUTUAL_FUND\u201d, \u201cSHARE\u201d, \u201cSTOCK\u201d) . . . Adj \u2192 (\u201cAmerican\u201d, \u201cGerman\u201d, \u201cRussian\u201d) or\n(\u201cblue\u201d, \u201cpurple\u201d, \u201cred\u201d, \u201cwhite\u201d) . . .\nTable 1 The procedure for generating synthetic data. Syntax is shared across all documents, but each document chooses one of the thematic terminal distribution for verbs, nouns, and adjectives. This simulates how all documents share syntax and subsets of documents share topical themes. All expansion rules are chosen uniformly at random.\nsurges\nPHD_CANDIDATE\nthat with\nGRAD_STUDENT\nwhite this\nGRAD_STUDENT\npurple about\nPROFESSOR\nabout\nPROFESSOR\npurple that\nsurges\nPROFESSOR\nred\nclimbs\nPROFESSOR\nthe red\nwalks\nabout\nSHEEP\nevil\nfalls\nover\nSHEEP\non\nSHEEP\nevil with\nPONY\non\nSHEEP\nevil evil\nFigure 5 Two synthetic documents with multiple sentences. Nouns are shown in upper case. Each document chooses a theme for each part of speech independently; for example, the document on the left uses motion verbs, academic nouns, and color adjectives. Various models are applied to these data in Figure 6.\nsuccessfully except for adjectives and determiners (Figure 6c). However, it ignores the thematic distinctions that actually divided the terms between documents. The HDP is aware of document groupings and treats the words exchangeably within them and is thus able to recover the thematic topics, but it misses the connections between the parts of speech, and has conflated multiple parts of speech (Figure 6b).\nThe STM is able to capture the the topical themes and recover parts of speech (with the exception of prepositions placed in the same topic as nouns with a self loop). Moreover, it was able to identify the same interconnections between latent classes that\nwere apparent from the infinite tree. Nouns are dominated by verbs and prepositions, and verbs are the root (head) of sentences. Figure 6d shows the two divisions as separate axes; going form left to right, the thematic divisions that the HDP was able to uncover are clear. Going from top to bottom, the syntactic distinctions made by the infinite tree are revealed."}, {"heading": "3.2 Qualitative Description of Topics learned by the STM from Hand-annotated Data", "text": "The same general properties, but with greater variation, are exhibited in real data. We converted the Penn Treebank (Marcus et al. 1994), a corpus of manually curated parse trees, into a dependency parse (Johansson and Nugues 2007). The vocabulary was pruned to terms that appeared in at least ten documents.\nFigure 7 shows a subset of topics learned by the STM with truncation level 32. Many of the resulting topics illustrate both syntactic and thematic consistency. A few non-specific function topics emerged (pronoun, possessive pronoun, general verbs, etc.). Many of the noun categories were more specialized. For instance, Figure 7 shows clusters of nouns relating to media, individuals associated with companies (\u201cmr,\u201d \u201cpresident,\u201d\n\u201cchairman\u201d), and abstract nouns related to stock prices (\u201cshares,\u201d \u201cquarter,\u201d \u201cearnings,\u201d \u201cinterest\u201d), all of which feed into a topic that modifies nouns (\u201chis,\u201d \u201ctheir,\u201d \u201cother,\u201d \u201clast\u201d).\nGriffiths et al (Griffiths et al. 2005) observed that nouns, more than other parts of speech, tend to specialize into distinct topics, and this is also evident here. In Figure 7, the unspecialized syntactic categories (shaded and with rounded edges) serve to connect many different specialized thematic categories, which are predominantly nouns (although the adjectives also showed bifurcation). For example, verbs are mostly found in a single topic, but then have a large number of outgoing transitions to many noun topics. Because of this relationship, verbs look like a syntactic \u201csource\u201d in Figure 7. Many of these noun topics then point to thematically unified topics such as \u201cpersonal pronouns,\u201d which look like syntactic \u201csinks.\u201d\nIt is important to note that Figure 7 only presents half of the process of choosing a topic for a word. While the transition distribution of verb topics allows many different noun topics as possible dependents, because the topic is chosen from a product of \u03b8 and \u03c0, \u03b8 can filter out the noun topics that are inconsistent with a document\u2019s theme.\nThis division between functional and topical uses for the latent classes can also been seen in the values for the per-document multinomial over topics. A number of topics in Figure 7(b), such as 17, 15, 10, and 3, appear to some degree in nearly every document, while other topics are used more sparingly to denote specialized content. With \u03b1 = 0.1, this plot also shows that the non-parametric Bayesian framework is ignoring many later topics."}, {"heading": "3.3 Quantitative Results on Synthetic and Hand-annotated Data", "text": "To study the performance of the STM on new data, we estimated the held out probability of previously unseen documents with an STM trained on a portion of the dataset. For each position in the parse trees, we estimate the probability of the observed word. We compute the perplexity as the exponent of the inverse of the per-word average log probability. The lower the perplexity, the better the model has captured the patterns in the data. We also computed perplexity for individual parts of speech to study the differences in predictive power between content words, such as nouns and verbs, and function words, such as prepositions and determiners. This illustrates how different algorithms better capture aspects of context. We expect function words to be dominated by local context and content words to be determined more by the themes of the document.\nThis trend is seen not only in the synthetic data (Figure 8(a)), where syntactic models better predict functional categories like prepositions, and document-only models fail to account for patterns of verbs and determiners, but also in real data. Figure 8(b) shows that HDP and STM both perform better than syntactic models in capturing the patterns behind nouns, while both STM and the infinite tree have lower perplexity for verbs. Like syntactic models, our model was better able to predict the appearance of prepositions but also remained competitive with HDP on content words. On the whole, STM had lower perplexity than HDP and the infinite tree."}, {"heading": "4. Conclusion", "text": "In this work, we explored the common threads that link syntactic and topic models and created a model that is simultaneously aware of both thematic and syntactic influences in a document. These models are aware of more structure than either model individually.\nMore generally, this work serves as an example of how a mixture model can support two different, simultaneous explanations for how the latent class is chosen. Although this model used discrete observations, the variational inference setup is flexible enough to support other distributions over the output.\nWhile this work\u2019s primary goal was to demonstrate how these two views of context could be simultaneously learned, there are a number of extensions that could lead to more accurate parsers. First, this model could be further extended by integrating a richer syntactic model that does not just model the words that appear in a given structure but one that also models the parse structure itself. This would allow the model to use large, diverse corpora without relying upon an external parser to provide the tree structure.\nRemoving the independence restriction between children also would allow for this model to closer approximate the state of the art syntactic models and to be better distinguish the children of parent nodes (this is especially the problem for head verbs, which often have many children). Finally, this model could also make use of labeled dependency relations and lexicalization.\nWith the ability to adjust to specific document or corpus-based contexts, a parser built using this framework could adapt to handle different domains while still sharing information between them. The classification and clustering implicitly provided by the topic components would allow the parser to specialize its parsing model when necessary, allowing both sentence-level and document-level information to shape the model\u2019s understanding of a document."}, {"heading": "Appendix A: Document Likelihood Bound", "text": "In this section, we seek to fully expand the likelihood lower bound L first introduced in Equation 2 by explicitly computing the expectations with respect to the variational distribution.\nFirst, we expand Eq [log p(z|\u03b8,\u03c0)] from Equation 2. Rather than drawing the topic of a word directly from a multinomial, the topic is chosen from the renormalized pointwise product of two multinomial distributions. In order to handle the expectation of the log sum introduced by the renormalization, we introduce an additional variational parameter \u03c9n for each word via a Taylor approximation of the logarithm to find that Eq [log p(z|\u03b8,\u03c0)] =\nEq [ log\nN\u220f n=1 \u03b8zn\u03c0zp(n),zn\u2211K i \u03b8i\u03c0zp(n),i\n] = Eq [ N\u2211 n=1 log \u03b8zn\u03c0zp(n),zn \u2212 N\u2211 n=1 log K\u2211 i=1 \u03b8i\u03c0zp(n),i ]\n\u2264 N\u2211 n=1 Eq [ log \u03b8zn\u03c0zp(n),zn ] \u2212 N\u2211 n=1 Eq [ \u03c9\u22121n K\u2211 i=1 \u03b8i\u03c0zp(n),i ] + log\u03c9n \u2212 1\n= N\u2211 n=1 K\u2211 i=1 \u03c6n,i ( \u03a8 (\u03b3i)\u2212\u03a8 (\u2211K j=1 \u03b3j )) + N\u2211 n=1 K\u2211 i=1 K\u2211 j=1 \u03c6n,i\u03c6p(n),j ( \u03a8 (\u03bdj,i)\u2212\u03a8 (\u2211K k=1 \u03bdj,k ))\n\u2212  N\u2211 n=1 \u03c9\u22121n \u2211 i=1 \u2211 j=1 \u03c6p(n),j \u03b3i\u03bdj,i\u2211K k=1 \u03b3k \u2211K k=1 \u03bdj,k + log\u03c9n \u2212 1  . (A.1)\nFor the expectations of \u03c0 and \u03b8, multinomials that come from Dirichlet distributions, we employ the fact that differentiating the log normalizer with respect to the natural parameter gives the expectation of the sufficient statistic for exponential family distributions (Blei et al. 2003). Doing this for the Dirichlet distribution introduces the digamma function \u03a8, the derivative of the logarithm of the gamma function, in the above equation.\nThe other terms in the document\u2019s contribution to the overall likelihood bound are more conventional. Expanding the other expectations gives us\nLd = log \u0393 (\u2211K j=1 \u03b1D,j\u03b2 \u2217 ) \u2212 K\u2211 i=1 log \u0393 (\u03b1D,i\u03b2 \u2217) + K\u2211 i=1 (\u03b1D,i\u03b2 \u2217 \u2212 1) ( \u03a8 (\u03b3i)\u2212\u03a8 (\u2211K j=1 \u03b3j ))\n+ N\u2211 n=1 K\u2211 i=1 \u03c6n,i ( \u03a8 (\u03b3i)\u2212\u03a8 (\u2211K j=1 \u03b3j )) + N\u2211 n=1 K\u2211 i=1 K\u2211 j=1 \u03c6n,i\u03c6p(n),j ( \u03a8 (\u03bdj,i)\u2212\u03a8 (\u2211K k=1 \u03bdj,k ))\n\u2212  N\u2211 n=1 \u03c9\u22121n \u2211 i=1 \u2211 j=1 \u03c6p(n),j \u03b3i\u03bdj,i\u2211K k=1 \u03b3k \u2211K k=1 \u03bdj,k + log\u03c9n \u2212 1  +\nN\u2211 n=1 K\u2211 i=1 \u03c6i log \u03c4i,wd,n\n\u2212 log \u0393 (\u2211K\nj=1 \u03b3j\n) + K\u2211 i=1 log \u0393 (\u03b3i)\u2212 K\u2211 i=1 (\u03b3i \u2212 1) ( \u03a8 (\u03b3i)\u2212\u03a8 (\u2211K j=1 \u03b3j ))\n\u2212 N\u2211 n=1 K\u2211 i=1 \u03c6n,i log \u03c6n,i. (A.2)\nApart from the terms derived in Equation A.1, the other terms here are very similar to the objective function for LDA. The expectation of the log of p(\u03b8), q(\u03b8), p(z), q(z), and p(w) all appear in the LDA likelihood bound."}, {"heading": "Appendix B: Document-specific Variational Updates", "text": "In this section, we derive the updates for all document-specific variational parameters other than \u03c6n, which is updated according to Equation 3.\nBecause we cannot assume that the point-wise product of of \u03c0k and \u03b8d sums to one, we introduced a slack term \u03c9n in Equation A.1; its update is\n\u03c9n = \u2211 i=1 \u2211 j=1 \u03c6p(n),j \u03b3i\u03bdj,i\u2211K k=1 \u03b3k \u2211K k=1 \u03bdj,k .\nBecause we couple \u03c0 and \u03b8, the interaction between these terms in the normalizer prevents us from solving the optimization for \u03b3 and \u03bd explicitly. Instead, for each \u03b3d we compute the partial derivative with respect to \u03b3d,i for each component of the vector. We then maximize the likelihood bound for each \u03b3d. In deriving the gradient, the following\nderivative is useful:\nf(x) = N\u2211 i=1 \u03b1i xi\u2211N i=j xj\n\u21d2 \u2202f \u2202xi\n= \u03b1i \u2211N j 6=i xj \u2212 \u2211N j 6=i \u03b1jxj(\u2211N\ni=1 xi )2 . (B.1) This allows us to more easily compute the partial derivative of Equation A.2 with respect to \u03b3i to be\n\u2202L \u2202\u03b3i = \u03a8\u2032 (\u03b3i)\n( \u03b1D,i\u03b2 \u2217 +\nN\u2211 n=1 \u03c6n,i \u2212 \u03b3i\n) \u2212\u03a8\u2032 (\u2211N j=1 \u03b3j ) K\u2211 j=1 [ \u03b1D,j\u03b2 \u2217 + N\u2211 n=1 \u03c6n,j \u2212 \u03b3j ]\n\u2212 N\u2211 n=1 \u03c9\u22121n K\u2211 j=1 \u03c6p(n),j \u03bdj,i\u2211Nk 6=j \u03b3k \u2212\u2211Nk 6=j \u03bdj,k\u03b3k(\u2211N k=1 \u03b3k )2\u2211N k=1 \u03bdj,k "}, {"heading": "Appendix C: Global Updates", "text": "In this section, we expand the terms of Equation 2 that were not expanded in Equation A.2. First, we note that Eq [log GEM(\u03b2;\u03b1)], because the variational distribution only puts weight on \u03b2\u2217, is just log GEM(\u03b2\u2217;\u03b1).\nWe can return to the stick-breaking weights by dividing each \u03b2\u2217z by the sum of all of the indices greater than z (recalling that \u03b2 sums to one), Tz \u2261 1\u2212 \u2211z\u22121 i=1 \u03b2i. Using this reformulation, the total likelihood bound, including Equation A.2 as Ld, is then L = M\u2211 d Ld\n+ (\u03b1\u2212 1) log TK \u2212 K\u22121\u2211 z log Tz + log \u0393 (\u2211K j=1 \u03b1T,j\u03b2 \u2217 ) \u2212\nK\u2211 i=1 log \u0393 (\u03b1T,i\u03b2 \u2217) + K\u2211 i=1 (\u03b1T,i\u03b2 \u2217 \u2212 1) ( \u03a8 (\u03bdi)\u2212\u03a8 (\u2211K j=1 \u03bdj ))\n\u2212 log \u0393 (\u2211K\nj=1 \u03bdj\n) + K\u2211 i=1 log \u0393 (\u03bdi)\u2212 K\u2211 i=1 (\u03bdi \u2212 1) ( \u03a8 (\u03bdi)\u2212\u03a8 (\u2211K j=1 \u03bdj ))\n+ K\u2211 i=1 V\u2211 v=1 \u03c3 log \u03c4i,v. (C.1)\n3.0.1 Variational Dirichlet for Parent-child Transitions. Like the update for \u03b3, the interaction between \u03c0 and \u03b8 in the normalizer prevents us from solving the optimization for each of the \u03bdi explicitly. Differentiating the global likelihood bound, keeping in mind\nEquation B.1, gives\n\u2202L\n\u2202\u03bdi,j = \u03a8\u2032 (\u03bdi,j) \u03b1P,j + N\u2211 n=1 \u2211 c\u2208c(n) \u03c6n,i\u03c6c,j \u2212 \u03bdi,j  \u2212\u03a8\u2032 (\u2211K k=1 \u03bdi,k\n) K\u2211 k=1 \u03b1P,k + N\u2211 n=1 \u2211 c\u2208c(n) \u03c6n,i\u03c6c,k \u2212 \u03bdi,k \n\u2212 N\u2211 n \u03c6n,i \u2211 c\u2208c(n) \u03c9\u22121c \u03b3j\u2211Nk 6=j \u03bdi,k \u2212\u2211Nk 6=j \u03bdi,k\u03b3k(\u2211N k=1 \u03bdj,k )2\u2211N k=1 \u03b3k . Each of the \u03bdi are then maximized individually using conjugate gradient optimization after transforming the vector to assure non-negativity.\n3.0.2 Variational Top-level Weights. The last variational parameter is \u03b2\u2217, which is the variational estimate of the top-level weights \u03b2. Because \u03b2\u2217K is implicitly defined as(\n1\u2212 \u2211K\u22121 i=0 \u03b2 \u2217 i ) , \u03b2\u2217K appears in the partial derivative of \u03b2\n\u2217 with respect to \u03b2\u2217k for k < K. Similarly, we must also use implicit differentiation with respect to the stick breaking proportions Tz , defined above. Taking the derivative and implicitly differentiating \u03b2K gives us\n\u2202L\u03b2\u2217\n\u2202\u03b2\u2217k = ( K\u22121\u2211 z=k+1 1 Tz ) \u2212 \u03b1\u2212 1 TK\n+ \u03b1D M\u2211 d ( \u03a8 (\u03b3d,k)\u2212\u03a8 (\u2211K j=1 \u03b3d,j )) \u2212 \u03b1D M\u2211 d ( \u03a8 (\u03b3d,K)\u2212\u03a8 (\u2211K j=1 \u03b3d,j ))\n+ \u03b1T K\u2211 z ( \u03a8 (\u03bdz,k)\u2212\u03a8 (\u2211K j=1 \u03bdz,j )) \u2212 \u03b1T K\u2211 z ( \u03a8 (\u03bdz,K)\u2212\u03a8 (\u2211K j=1 \u03bdz,j )) \u2212 K [\u03b1T\u03a8 (\u03b1T\u03b2\u2217k)\u2212 \u03b1T\u03a8 (\u03b1T\u03b2\u2217K)]\n\u2212 M [\u03b1D\u03a8 (\u03b1D\u03b2\u2217k)\u2212 \u03b1D\u03a8 (\u03b1D\u03b2\u2217K)] (C.2)\nwhich we use with conjugate gradient optimization after appropriately transforming the variables to ensure non-negativity."}], "references": [{"title": "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems", "author": ["E. Antoniak"], "venue": "The Annals of Statistics,", "citeRegEx": "Antoniak.,? \\Q1974\\E", "shortCiteRegEx": "Antoniak.", "year": 1974}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["Blei", "Jordan2005]David M. Blei", "Michael I. Jordan"], "venue": "Journal of Bayesian Analysis,", "citeRegEx": "Blei et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2005}, {"title": "Dynamic topic models", "author": ["Blei", "Lafferty2006]David M. Blei", "John D. Lafferty"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "Supervised topic models", "author": ["Blei", "McAuliffe2007]David M. Blei", "Jon D. McAuliffe"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Blei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2007}, {"title": "The nested chinese restaurant process and hierarchical topic models, Oct", "author": ["Blei et al.2007]David M. Blei", "Thomas L. Griffiths", "Michael I. Jordan"], "venue": null, "citeRegEx": "Blei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2007}, {"title": "Bayesian synchronous grammar induction", "author": ["Blunsom et al.2008]Phil Blunsom", "Trevor Cohn", "Miles Osborne"], "venue": "In Proc of NIPS", "citeRegEx": "Blunsom et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2008}, {"title": "A topic model for word sense disambiguation", "author": ["David M. Blei", "Xiaojin Zhu"], "venue": "In Proceedings of Emperical Methods in Natural Language Processing", "citeRegEx": "Boyd.Graber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2007}, {"title": "Bayesian word sense induction", "author": ["Brody", "Lapata2009]Samuel Brody", "Mirella Lapata"], "venue": "In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL", "citeRegEx": "Brody et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brody et al\\.", "year": 2009}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Cohen et al.2008]Shay B. Cohen", "Kevin Gimpel", "Noah A. Smith"], "venue": null, "citeRegEx": "Cohen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Form and content: Dissociating syntax and semantics in sentence comprehension", "author": ["Dapretto", "Bookheimer1999]Mirella Dapretto", "Susan Y. Bookheimer"], "venue": null, "citeRegEx": "Dapretto et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dapretto et al\\.", "year": 1999}, {"title": "Indexing by latent semantic analysis", "author": ["Susan Dumais", "Thomas Landauer", "George Furnas", "Richard Harshman"], "venue": "Journal of the American Society of Information", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Describing disability through individual-level mixture models for multivariate binary data", "author": ["Stephen E. Fienberg", "Cyrille Joutard"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Erosheva et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Erosheva et al\\.", "year": 2007}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["Escobar", "West1995]Michael D. Escobar", "Mike West"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Escobar et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Escobar et al\\.", "year": 1995}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["S. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "Ferguson.,? \\Q1973\\E", "shortCiteRegEx": "Ferguson.", "year": 1973}, {"title": "The infinite tree", "author": ["Trond Grenager", "Christopher D. Manning"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "Finkel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2007}, {"title": "Finding scientific topics", "author": ["Griffiths", "Steyvers2004]Thomas L. Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Integrating topics and syntax", "author": ["Mark Steyvers", "David M. Blei", "Joshua B. Tenenbaum"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Griffiths et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2005}, {"title": "Topics in semantic representation", "author": ["Mark Steyvers", "Joshua Tenenbaum"], "venue": "Psychological Review,", "citeRegEx": "Griffiths et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2007}, {"title": "Hidden topic Markov models", "author": ["Gruber et al.2007]Amit Gruber", "Michael Rosen-Zvi", "Yair Weiss"], "venue": "In Artificial Intelligence and Statistics", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Studying the history of ideas using topic models", "author": ["Hall et al.2008]David Hall", "Daniel Jurafsky", "Christopher D. Manning"], "venue": "In Proceedings of Emperical Methods in Natural Language Processing,", "citeRegEx": "Hall et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2008}, {"title": "A probabilistic model of unsupervised learning for musical-key profiles", "author": ["Hu", "Saul2009]Diane Hu", "Lawrence K. Saul"], "venue": "In International Society for Music Information Retrieval Conference", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Extended constituent-to-dependency conversion for English", "author": ["Johansson", "Nugues2007]Richard Johansson", "Pierre Nugues"], "venue": "In Proceedings of the Nordic Conference on Computational Linguistics", "citeRegEx": "Johansson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2007}, {"title": "Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models", "author": ["Johnson et al.2006]Mark Johnson", "Thomas L. Griffiths", "Sharon Goldwater"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Johnson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2006}, {"title": "An introduction to variational methods for graphical models", "author": ["Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Fast exact inference with a factored model for natural language parsing", "author": ["Klein", "Manning2002]Dan Klein", "Christopher D. Manning"], "venue": "In Advances in Neural Information Processing", "citeRegEx": "Klein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2002}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Manning2003]Dan Klein", "Christopher D. Manning"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Accelerated variational Dirichlet process mixtures", "author": ["Max Welling", "Nikos Vlassis"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Kurihara et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kurihara et al\\.", "year": 2007}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["Li Fei-Fei", "Perona2005]Li Fei-Fei", "Pietro Perona"], "venue": "In CVPR \u201905 - Volume", "citeRegEx": "Fei.Fei et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2005}, {"title": "Structured Bayesian nonparametric models with variational inference (tutorial)", "author": ["Liang", "Klein2007]Percy Liang", "Dan Klein"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Liang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2007}, {"title": "The infinite PCFG using hierarchical Dirichlet processes", "author": ["Liang et al.2007]Percy Liang", "Slav Petrov", "Michael Jordan", "Dan Klein"], "venue": "In Proceedings of Emperical Methods in Natural Language Processing,", "citeRegEx": "Liang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2007}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["Beatrice Santorini", "Mary A. Marcinkiewicz"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1994}, {"title": "Mining a digital library for influential authors", "author": ["Mimno", "McCallum2007]David Mimno", "Andrew McCallum"], "venue": "In JCDL \u201907: Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries,", "citeRegEx": "Mimno et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2007}, {"title": "Probabilistic inference using Markov chain Monte Carlo methods", "author": ["M. Neal"], "venue": "Technical Report CRG-TR-93-1,", "citeRegEx": "Neal.,? \\Q1993\\E", "shortCiteRegEx": "Neal.", "year": 1993}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["M. Neal"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Neal.,? \\Q2000\\E", "shortCiteRegEx": "Neal.", "year": 2000}, {"title": "Inference of population structure using multilocus genotype", "author": ["Matthew Stephens", "Peter Donnelly"], "venue": "data. Genetics,", "citeRegEx": "Pritchard et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Pritchard et al\\.", "year": 2000}, {"title": "Unsupervised topic modelling for multi-party spoken discourse", "author": ["Purver et al.2006]Matthew Purver", "Konrad K\u00f6rding", "Thomas L. Griffiths", "Joshua Tenenbaum"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Purver et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Purver et al\\.", "year": 2006}, {"title": "Minimized models for unsupervised part-of-speech tagging", "author": ["Ravi", "Knight2009]Sujith Ravi", "Kevin Knight"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,", "citeRegEx": "Ravi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2009}, {"title": "The interaction of syntax and semantics during sentence processing \u2014 Eye-movements in the analysis of semantically biased sentences", "author": ["Rayner et al.1983]Keith Rayner", "Marcia Carlson", "Lyn Frazier"], "venue": "Journal of Verbal Learning and Verbal Behavior,", "citeRegEx": "Rayner et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Rayner et al\\.", "year": 1983}, {"title": "The author-topic model for authors and documents", "author": ["Thomas L. Griffiths", "Mark Steyvers", "Padhraic Smyth"], "venue": "In Proceedings of Uncertainty in Artificial Intelligence,", "citeRegEx": "Rosen.Zvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Hierarchical Dirichlet processes", "author": ["Teh et al.2006]Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Whye Teh"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Teh.,? \\Q2006\\E", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "A joint model of text and aspect ratings for sentiment summarization", "author": ["Titov", "McDonald2008]Ivan Titov", "Ryan McDonald"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Titov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2008}, {"title": "A Bayesian LDA-based model for semi-supervised part-of-speech tagging", "author": ["Toutanova", "Johnson2008]Kristina Toutanova", "Mark Johnson"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Toutanova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1\u20132):1\u2013305", "author": ["Wainwright", "Jordan2008]Martin J. Wainwright", "Michael I. Jordan"], "venue": null, "citeRegEx": "Wainwright et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2008}, {"title": "Topic modeling: beyond bag-of-words", "author": ["M. Wallach"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "Wallach.,? \\Q2006\\E", "shortCiteRegEx": "Wallach.", "year": 2006}, {"title": "Continuous time dynamic topic models", "author": ["Wang et al.2008]Chong Wang", "David M. Blei", "David Heckerman"], "venue": "In Proceedings of Uncertainty in Artificial Intelligence", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Fully distributed EM for very large datasets", "author": ["Wolfe et al.2008]Jason Wolfe", "Aria Haghighi", "Dan Klein"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Wolfe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wolfe et al\\.", "year": 2008}, {"title": "TagLDA: Bringing document structure knowledge into topic models", "author": ["Zhu et al.2006]Xiaojin Zhu", "David M. Blei", "John Lafferty"], "venue": "Technical Report TR-1553,", "citeRegEx": "Zhu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 38, "context": "A syntactically correct sentence that is semantically implausible takes longer for people to understand than its semantically plausible counterpart (Rayner et al. 1983).", "startOffset": 148, "endOffset": 168}, {"referenceID": 17, "context": "On the other hand, probabilistic topic models find patterns of words that are thematically related in a large collection of documents (Blei et al. 2003; Griffiths et al. 2007).", "startOffset": 134, "endOffset": 175}, {"referenceID": 40, "context": "In both topic models and syntax models, Bayesian non-parametric methods are used to embed the choice of the number of components into the model (Teh et al. 2006; Finkel et al. 2007).", "startOffset": 144, "endOffset": 181}, {"referenceID": 14, "context": "In both topic models and syntax models, Bayesian non-parametric methods are used to embed the choice of the number of components into the model (Teh et al. 2006; Finkel et al. 2007).", "startOffset": 144, "endOffset": 181}, {"referenceID": 39, "context": ", to authorship (Rosen-Zvi et al. 2004), citation (Mimno and McCallum 2007), sentiment analysis (Blei and McAuliffe 2007; Titov and McDonald 2008), corpus exploration (Hall et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 19, "context": "2004), citation (Mimno and McCallum 2007), sentiment analysis (Blei and McAuliffe 2007; Titov and McDonald 2008), corpus exploration (Hall et al. 2008), part-of-speech labeling (Toutanova and Johnson 2008), discourse segmentation (Purver et al.", "startOffset": 133, "endOffset": 151}, {"referenceID": 36, "context": "2008), part-of-speech labeling (Toutanova and Johnson 2008), discourse segmentation (Purver et al. 2006), word sense induction (Brody and Lapata 2009), and word sense disambiguation (Boyd-Graber et al.", "startOffset": 84, "endOffset": 104}, {"referenceID": 6, "context": "2006), word sense induction (Brody and Lapata 2009), and word sense disambiguation (Boyd-Graber et al. 2007).", "startOffset": 83, "endOffset": 108}, {"referenceID": 35, "context": "Topic models have also been applied to non-language data, such as images (Li Fei-Fei and Perona 2005), population genetics (Pritchard et al. 2000), and music (Hu and Saul 2009).", "startOffset": 123, "endOffset": 146}, {"referenceID": 17, "context": "There are several reviews of topic modeling and related literature (Blei and Lafferty 2009; Griffiths et al. 2007).", "startOffset": 67, "endOffset": 114}, {"referenceID": 11, "context": "In statistics, these kinds of assumptions are called mixed-membership assumptions (Erosheva et al. 2007).", "startOffset": 82, "endOffset": 104}, {"referenceID": 17, "context": "2 The topics tend to correspond to a psychologically plausible interpretation of the themes that pervade the documents (Griffiths et al. 2007).", "startOffset": 119, "endOffset": 142}, {"referenceID": 10, "context": "Topic models represent a fully probabilistic perspective on techniques like latent semantic analysis (LSA) (Deerwester et al. 1990) and probabilistic latent semantic analysis (pLSA) (Hofmann 1999).", "startOffset": 107, "endOffset": 131}, {"referenceID": 14, "context": "The finite tree with independent children model (FTIC) can be seen as the syntactic complement to LDA (Finkel et al. 2007).", "startOffset": 102, "endOffset": 122}, {"referenceID": 14, "context": "The states discovered through posterior inference correlate with part of speech labels (Finkel et al. 2007).", "startOffset": 87, "endOffset": 107}, {"referenceID": 40, "context": "Recent research has extended Bayesian non-parametric methods to build more flexible models where the number of latent components is unbounded and is determined by the data (Teh et al. 2006; Liang and Klein 2007).", "startOffset": 172, "endOffset": 211}, {"referenceID": 8, "context": "These have proved promising in limited-data frameworks; the logistic normal prior has been applied to grammar induction (Cohen et al. 2008) and integer programming has been applied to unsupervised part of speech tagging (Ravi and Knight 2009).", "startOffset": 120, "endOffset": 139}, {"referenceID": 40, "context": "This extension is an application of a hierarchical Dirichlet process (HDP), a model of grouped data where each group arises from a DP whose base measure is itself a draw from a DP (Teh et al. 2006).", "startOffset": 180, "endOffset": 197}, {"referenceID": 14, "context": "This extension is described as the infinite tree with independent children (ITIC) (Finkel et al. 2007).", "startOffset": 82, "endOffset": 102}, {"referenceID": 28, "context": "Other work has applied this non-parametric framework to create language models (Teh 2006), full parsers for Chomsky normal form grammars (Liang et al. 2007), models of lexical acquisition (Goldwater 2007), synchronous grammars (Blunsom et al.", "startOffset": 137, "endOffset": 156}, {"referenceID": 5, "context": "2007), models of lexical acquisition (Goldwater 2007), synchronous grammars (Blunsom et al. 2008), and adaptor grammars for morphological segmentation (Johnson et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 22, "context": "2008), and adaptor grammars for morphological segmentation (Johnson et al. 2006).", "startOffset": 59, "endOffset": 80}, {"referenceID": 48, "context": "This is the approach taken by TagLDA (Zhu et al. 2006), where each word is associated with a single tag (such as a part of speech), and the model learns a weighting over the vocabulary terms for each tag.", "startOffset": 37, "endOffset": 54}, {"referenceID": 18, "context": "One example is the hidden topic Markov model (Gruber et al. 2007), which finds chains of homogeneous topics within a document.", "startOffset": 45, "endOffset": 65}, {"referenceID": 22, "context": "Some parsing formalisms, such as adaptor grammars (Johnson et al. 2006; Johnson 2009), are broad and expressive enough to also describe topic models.", "startOffset": 50, "endOffset": 85}, {"referenceID": 14, "context": "Gibbs sampling in particular, where the Markov chain is defined by the conditional distribution of each latent variable, has found widespread use in Bayesian non-parametric models and topic models (Neal 1993; Teh 2006; Griffiths and Steyvers 2004; Finkel et al. 2007).", "startOffset": 197, "endOffset": 267}, {"referenceID": 23, "context": "variables that is close to the posterior of interest (Jordan et al. 1999; Wainwright and Jordan 2008).", "startOffset": 53, "endOffset": 101}, {"referenceID": 40, "context": "Variational methods provide effective approximations in topic models and non-parametric Bayesian models (Blei et al. 2003; Blei and Jordan 2005; Teh et al. 2006; Liang et al. 2007; Kurihara et al. 2007).", "startOffset": 104, "endOffset": 202}, {"referenceID": 28, "context": "Variational methods provide effective approximations in topic models and non-parametric Bayesian models (Blei et al. 2003; Blei and Jordan 2005; Teh et al. 2006; Liang et al. 2007; Kurihara et al. 2007).", "startOffset": 104, "endOffset": 202}, {"referenceID": 26, "context": "Variational methods provide effective approximations in topic models and non-parametric Bayesian models (Blei et al. 2003; Blei and Jordan 2005; Teh et al. 2006; Liang et al. 2007; Kurihara et al. 2007).", "startOffset": 104, "endOffset": 202}, {"referenceID": 46, "context": "Non-conjugate pairs appear in the dynamic topic model (Blei and Lafferty 2006; Wang et al. 2008), correlated topic model (Blei et al.", "startOffset": 54, "endOffset": 96}, {"referenceID": 3, "context": "2008), correlated topic model (Blei et al. 2007), and in the STM considered here.", "startOffset": 30, "endOffset": 48}, {"referenceID": 47, "context": "Each document optimization, however, produces expected counts which are summed together; this is similar to the how the the E-step of EM algorithms can be parallelized and summed as input to the M-step (Wolfe et al. 2008).", "startOffset": 202, "endOffset": 221}, {"referenceID": 31, "context": "We converted the Penn Treebank (Marcus et al. 1994), a corpus of manually curated parse trees, into a dependency parse (Johansson and Nugues 2007).", "startOffset": 31, "endOffset": 51}, {"referenceID": 16, "context": "Griffiths et al (Griffiths et al. 2005) observed that nouns, more than other parts of speech, tend to specialize into distinct topics, and this is also evident here.", "startOffset": 16, "endOffset": 39}], "year": 2010, "abstractText": "The syntactic topic model (STM) is a Bayesian nonparametric model of language that discovers latent distributions of words (topics) that are both semantically and syntactically coherent. The STM models dependency parsed corpora where sentences are grouped into documents. It assumes that each word is drawn from a latent topic chosen by combining document-level features and the local syntactic context. Each document has a distribution over latent topics, as in topic models, which provides the semantic consistency. Each element in the dependency parse tree also has a distribution over the topics of its children, as in latent-state syntax models, which provides the syntactic consistency. These distributions are convolved so that the topic of each word is likely under both its document and syntactic context. We derive a fast posterior inference algorithm based on variational methods. We report qualitative and quantitative studies on both synthetic data and hand-parsed documents. We show that the STM is a more predictive model of language than current models based only on syntax or only on topics.", "creator": "LaTeX with hyperref package"}}}