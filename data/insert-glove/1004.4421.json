{"id": "1004.4421", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2010", "title": "Efficient Learning with Partially Observed Attributes", "abstract": "We hoth describe and analyze ildem efficient algorithms for learning a linear predictor from 98.0 examples manhunter when the \u014dishi learner ostend can jaen only lankford view a few attributes 11-related of each pinklon training example. unabomb This is mayoralty the case, sebu for despotovac instance, christic in raglin medical maleh research, where each sufficiency patient syuri participating minehunters in the experiment aseritis is hellings only hwang willing to stevns go through 13.13 a small procyon number fernandopulle of vidtrac tests. Our usbwa analysis llangattock bounds the number of wanting additional examples birches sufficient to 17.89 compensate floreal for the burndy lack of full information mezera on sass each mannlicher training jolyon example. chmura We sandbar demonstrate steinhausen the williamites efficiency flachau of our algorithms ostrofsky by 3.60 showing antiochus that when running caidic on flux digit recognition arabian data, sniffling they obtain a high super-middleweight prediction dalha accuracy crufts even when the learner gets silencers to crumpets see only four pixels gesto of 52-43 each image.", "histories": [["v1", "Mon, 26 Apr 2010 07:41:50 GMT  (71kb,D)", "https://arxiv.org/abs/1004.4421v1", "This is a full version of the paper appearing in The 27th International Conference on Machine Learning (ICML 2010)"], ["v2", "Wed, 28 Apr 2010 14:38:13 GMT  (71kb,D)", "http://arxiv.org/abs/1004.4421v2", "This is a full version of the paper appearing in The 27th International Conference on Machine Learning (ICML 2010)"]], "COMMENTS": "This is a full version of the paper appearing in The 27th International Conference on Machine Learning (ICML 2010)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nicol\u00f2 cesa-bianchi", "shai shalev-shwartz", "ohad shamir"], "accepted": true, "id": "1004.4421"}, "pdf": {"name": "1004.4421.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning with Partially Observed Attributes", "authors": ["Nicol\u00f2 Cesa-Bianchi", "Shai Shalev-Shwartz"], "emails": ["cesa-bianchi@dsi.unimi.it", "shais@cs.huji.ac.il", "ohadsh@cs.huji.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Suppose we would like to predict if a person has some disease based on medical tests. Theoretically, we may choose a sample of the population, perform a large number of medical tests on each person in the sample and learn from this information. In many situations this is unrealistic, since patients participating in the experiment are not willing to go through a large number of medical tests. The above example motivates the problem studied in this paper, that is learning when there is a hard constraint on the number of attributes the learner may view for each training example.\nWe propose an efficient algorithm for dealing with this partial information problem, and bound the number of additional training examples sufficient to compensate for the lack of full information on each training\nAppearing in Proceedings of the 27 th International Conference on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the author(s)/owner(s).\nexample. Roughly speaking, we actively pick which attributes to observe in a randomized way so as to construct a \u201cnoisy\u201d version of all attributes. Intuitively, we can still learn despite the error of this estimate because instead of receiving the exact value of each individual example in a small set it suffices to get noisy estimations of many examples."}, {"heading": "1.1. Related Work", "text": "Many methods have been proposed for dealing with missing or partial information. Most of the approaches do not come with formal guarantees on the risk of the resulting algorithm, and are not guaranteed to converge in polynomial time. The difficulty stems from the exponential number of ways to complete the missing information. In the framework of generative models, a popular approach is the ExpectationMaximization (EM) procedure (Dempster et al., 1977). The main drawback of the EM approach is that it might find sub-optimal solutions. In contrast, the methods we propose in this paper are provably efficient and come with finite sample guarantees on the risk.\nOur technique for dealing with missing information borrows ideas from algorithms for the adversarial multi-armed bandit problem (Auer et al., 2003; CesaBianchi and Lugosi, 2006). Our learning algorithms actively choose which attributes to observe for each example. This and similar protocols were studied in the context of active learning (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007; 2009; Beygelzimer et al., 2009), where the learner can ask for the target associated with specific examples.\nThe specific learning task we consider in the paper was first proposed in (Ben-David and Dichterman, 1998), where it is called \u201clearning with restricted focus of attention\u201d. Ben-David and Dichterman (1998) considered the classification setting and showed learnability\nar X\niv :1\n00 4.\n44 21\nv2 [\ncs .L\nG ]\n2 8\nA pr\n2 01\nof several hypothesis classes in this model, like k-DNF and axis-aligned rectangles. However, to the best of our knowledge, no efficient algorithm for the class of linear predictors has been proposed.1\nA related setting, called budgeted learning, was recently studied - see for example (Deng et al., 2007; Kapoor and Greiner, 2005) and the references therein. In budgeted learning, the learner purchases attributes at some fixed cost subject to an overall budget. Besides lacking formal guarantees, this setting is different from the one we consider in this paper, because we impose a budget constraint on the number of attributes that can be obtained for every individual example, as opposed to a global budget. In some applications, such as the medical application discussed previously, our constraint leads to a more realistic data acquisition process - the global budget allows to ask for many attributes of some individual patients while our protocol guarantees a constant number of medical tests to all the patients.\nOur technique is reminiscent of methods used in the compressed learning framework (Calderbank et al., 2009; Zhou et al., 2009), where data is accessed via a small set of random linear measurements. Unlike compressed learning, where learners are both trained and evaluated in the compressed domain, our techniques are mainly designed for a scenario in which only the access to training data is restricted.\nThe \u201copposite\u201d setting, in which full information is given at training time and the goal is to train a predictor that depends only on a small number of attributes at test time, was studied in the context of learning sparse predictors - see for example (Tibshirani, 1996) and the wide literature on sparsity properties of `1 regularization. Since our algorithms also enforce low `1 norm, many of those results can be combined with our techniques to yield an algorithm that views only O(1) attributes at training time, and a number of attributes comparable to the achievable sparsity at test time. Since our focus in this work is on constrained information at training time, we do not elaborate on this subject. Furthermore, in some real-world situations, it is reasonable to assume that attributes are very expensive at training time but are more easy to obtain at test time. Returning to the example of medical applications, it is unrealistic to convince patients to participate in a medical experiment in which they need to go through a lot of medical tests, but once the system is trained, at testing time, patients who need\n1Ben-David and Dichterman (1998) do describe learnability results for similar classes but only under the restricted family of product distributions.\nthe prediction of the system will agree to perform as many medical tests as needed.\nA variant of the above setting is the one studied by Greiner et al. (2002), where the learner has all the information at training time and at test time he tries to actively choose a small amount of attributes to form a prediction. Note that active learning at training time, as we do here, may give more learning power than active learning at testing time. For example, we formally prove that while it is possible to learn a consistent predictor accessing at most 2 attributes of each example at training time, it is not possible (even with an infinite amount of training examples) to build an active classifier that uses at most 2 attributes of each example at test time, and whose error will be smaller than a constant."}, {"heading": "2. Main Results", "text": "In this section we outline the main results. We start with a formal description of the learning problem. In linear regression each example is an instance-target pair, (x, y) \u2208 Rd \u00d7 R. We refer to x as a vector of attributes and the goal of the learner is to find a linear predictor x 7\u2192 \u3008w,x\u3009, where we refer to w \u2208 Rd as the predictor. The performance of a predictor w on an instance-target pair, (x, y) \u2208 Rd\u00d7R, is measured by a loss function `(\u3008w,x\u3009, y). For simplicity, we focus on the squared loss function, `(a, b) = (a\u2212b)2, and briefly discuss other loss functions in Section 5. Following the standard framework of statistical learning (Haussler, 1992; Devroye et al., 1996; Vapnik, 1998), we model the environment as a joint distribution D over the set of instance-target pairs, Rd \u00d7 R. The goal of the learner is to find a predictor with low risk, defined as the expected loss: LD(w) def = E(x,y)\u223cD[`(\u3008w,x\u3009, y)]. Since the distribution D is unknown to the learner he learns by relying on a training set of m examples S = (x1, y1), . . . , (xm, ym), which are assumed to be sampled i.i.d. from D. We denote the training loss by LS(w) def = 1m \u2211m i=1(\u3008w,xi\u3009 \u2212 yi)2. We now distinguish between two scenarios:\n\u2022 Full information: The learner receives the entire training set. This is the traditional linear regression setting.\n\u2022 Partial information: For each individual example, (xi, yi), the learner receives the target yi but is only allowed to see k attributes of xi, where k is a parameter of the problem. The learner has the freedom to actively choose which of the attributes will be revealed, as long as at most k of them will be given.\nWhile the full information case was extensively studied, the partial information case is more challenging. Our approach for dealing with the problem of partial information is to rely on algorithms for the full information case and to fill in the missing information in a randomized, data and algorithmic dependent, way. As a simple baseline, we begin by describing a straightforward adaptation of Lasso (Tibshirani, 1996), based on a direct nonadaptive estimate of the loss function. We then turn to describe a more effective approach, which combines a stochastic gradient descent algorithm called Pegasos (Shalev-Shwartz et al., 2007) with the active sampling of attributes in order to estimate the gradient of the loss at each step."}, {"heading": "2.1. Baseline Algorithm", "text": "A popular approach for learning a linear regressor is to minimize the empirical loss on the training set plus a regularization term taking the form of a norm of the predictor. For example, in ridge regression the regularization term is \u2016w\u201622 and in Lasso the regularization term is \u2016w\u20161. Instead of regularization, we can include a constraint of the form \u2016w\u20161 \u2264 B or \u2016w\u20162 \u2264 B. With an adequate tuning of parameters, the regularization form is equivalent to the constraint form. In the constraint form, the predictor is a solution to the following optimization problem:\nmin w\u2208Rd\n1 |S| \u2211 (x,y)\u2208S (\u3008w,x\u3009 \u2212 y)2 s.t. \u2016w\u2016p \u2264 B , (1)\nwhere S = {(x1, y1), . . . , (xm, ym)} is a training set of examples, B is a regularization parameter, and p is 1 for Lasso and 2 for ridge regression. Standard risk bounds for Lasso imply that if w\u0302 is a minimizer of (1) (with p = 1), then with probability greater than 1\u2212 \u03b4 over the choice of a training set of size m we have\nLD(w\u0302) \u2264 min w:\u2016w\u20161\u2264B LD(w)+O\n( B2 \u221a ln(d/\u03b4)\nm\n) . (2)\nTo adapt Lasso to the partial information case, we first rewrite the squared loss as follows:\n(\u3008w,x\u3009 \u2212 y)2 = wT (xxT )w \u2212 2yxTw + y2 ,\nwhere w,x are column vectors and wT ,xT are their corresponding transpose (i.e., row vectors). Next, we estimate the matrix xxT and the vector x using the partial information we have, and then we solve the optimization problem given in (1) with the estimated values of xxT and x. To estimate the vector x we can pick an index i uniformly at random from [d] = {1, . . . , d} and define the estimation to be a vector v\nsuch that\nvr =\n{ d xr if r = i\n0 else . (3)\nIt is easy to verify that v is an unbiased estimate of x, namely, E[v] = x where expectation is with respect to the choice of the index i. When we are allowed to see k > 1 attributes, we simply repeat the above process (without replacement) and set v to be the averaged vector. To estimate the matrix xxT we could pick two indices i, j independently and uniformly at random from [d], and define the estimation to be a matrix with all zeros except d2 xixj in the (i, j) entry. However, this yields a non-symmetric matrix which will make our optimization problem with the estimated matrix non-convex. To overcome this obstacle, we symmetrize the matrix by adding its transpose and dividing by 2. The resulting baseline procedure2 is given in Algorithm 1.\nAlgorithm 1 Baseline(S, k) S \u2014 full information training set with m examples k \u2014 Can view only k elements of each instance in S Parameter: B\nInitialize: A\u0304 = 0 \u2208 Rd,d ; v\u0304 = 0 \u2208 Rd ; y\u0304 = 0 for each (x, y) \u2208 S\nv = 0 \u2208 Rd A = 0 \u2208 Rd,d Choose C uniformly at random from\nall subsets of [d]\u00d7 [d] of size k/2 for each (i, j) \u2208 C vi = vi + (d/k)xi vj = vj + (d/k)xj Ai,j = Ai,j + (d\n2/k)xixj Aj,i = Aj,i + (d\n2/k)xixj end A\u0304 = A\u0304+A/m v\u0304 = v\u0304 + 2 y v/m y\u0304 = y\u0304 + y2/m\nend Let L\u0303S(w) = w T A\u0304w + wT v\u0304 + y\u0304 Output: solution of min w:\u2016w\u20161\u2264B L\u0303S(w)\n2We note that an even simpler approach is to arbitrarily assume that the correlation matrix is the identity matrix and then the solution to the loss minimization problem is simply the averaged vector, w = \u2211 (x,y)\u2208S y x. In that case, we can simply replace x by its estimated vector as defined in (3). While this naive approach can work on very simple classification tasks, it will perform poorly on realistic data sets, in which the correlation matrix is not likely to be identity. Indeed, in our experiments with the MNIST data set, we found out that this approach performed poorly relatively to the algorithms proposed in this paper.\nThe following theorem shows that similar to Lasso, the Baseline algorithm is competitive with the optimal linear predictor with a bounded L1 norm.\nTheorem 1 Let D be a distribution such that P[x \u2208 [\u22121,+1]d \u2227 y \u2208 [\u22121,+1]] = 1. Let w\u0302 be the output of Baseline(S,k), where |S| = m. Then, with probability of at least 1\u2212 \u03b4 over the choice of the training set and the algorithm\u2019s own randomization we have\nLD(w\u0302) \u2264 min w:\u2016w\u20161\u2264B LD(w) +O\n( (dB)2\nk\n\u221a ln(d/\u03b4)\nm\n) .\nThe above theorem tells us that for a sufficiently large training set we can find a very good predictor. Put another way, a large number of examples can compensate for the lack of full information on each individual example. In particular, to overcome the extra factor d2/k in the bound, which does not appear in the full information bound given in (2), we need to increase m by a factor of d4/k2.\nNote that when k = d we do not recover the full information bound. This is because we try to estimate a matrix with d2 entries using only k = d < d2 samples. In the next subsection, we describe a better, adaptive procedure for the partial information case."}, {"heading": "2.2. Gradient-based Attribute Efficient Regression", "text": "In this section, by avoiding the estimation of the matrix xxT , we significantly decrease the number of additional examples sufficient for learning with k attributes per training example. To do so, we do not try to estimate the loss function but rather estimate the gradient \u2207`(w) = 2 (\u3008w,x\u3009 \u2212 y) x, with respect to w, of the squared loss function (\u3008w,x\u3009 \u2212 y)2. Each vector w can define a probability distribution over [d] by letting P[i] = |wi|/\u2016w\u20161. We can estimate the gradient using 2 attributes as follows. First, we randomly pick j from [d] according to the distribution defined by w. Using j we estimate the term \u3008w,x\u3009 by sgn(wj) \u2016w\u20161 xj . It is easy to verify that the expectation of the estimate equals \u3008w,x\u3009. Second, we randomly pick i from [d] according to the uniform distribution over [d]. Based on i, we estimate the vector x as in (3). Overall, we obtain the following unbiased estimation of the gradient:\n\u2207\u0303`(w) = 2 (sgn(wj) \u2016w\u20161 xj \u2212 y) v , (4)\nwhere v is as defined in (3).\nThe advantage of the above approach over the loss based approach we took before is that the magnitude\nof each element of the gradient estimate is order of d \u2016w\u20161. This is in contrast to what we had for the loss based approach, where the magnitude of each element of the matrix A was order of d2. In many situations, the L1 norm of a good predictor is significantly smaller than d and in these cases the gradient based estimate is better than the loss based estimate. However, while in the previous approach our estimation did not depend on a specific w, now the estimation depends on w. We therefore need an iterative learning method in which at each iteration we use the gradient of the loss function on an individual example. Luckily, the stochastic gradient descent approach conveniently fits our needs.\nConcretely, below we describe a variant of the Pegasos algorithm (Shalev-Shwartz et al., 2007) for learning linear regressors. Pegasos tries to minimize the regularized risk\nmin w E (x,y)\u223cD\n[ (\u3008w,x\u3009 \u2212 y)2 ] + \u03bb\u2016w\u201622 . (5)\nOf course, the distribution D is unknown, and therefore we cannot hope to solve the above problem exactly. Instead, Pegasos finds a sequence of weight vectors that (on average) converge to the solution of (5). We start with the all zeros vector w = 0 \u2208 Rd. Then, at each iteration Pegasos picks the next example in the training set (which is equivalent to sampling a fresh example according to D) and calculates the gradient of the loss function on this example with respect to the current weight vector w. In our case, the gradient is simply 2(\u3008w,x\u3009\u2212 y)x. We denote this gradient vector by \u2207. Finally, Pegasos updates the predictor according to the rule: w = (1\u2212 1t ) w \u2212 1 \u03bb t \u2207, where t is the current iteration number.\nTo apply Pegasos in the partial information case we could simply replace the gradient vector \u2207 with its estimation given in (4). However, our analysis shows that it is desirable to maintain an estimation vector \u2207\u0303 with small magnitude. Since the magnitude of \u2207\u0303 is order of d \u2016w\u20161, where w is the current weight vector maintained by the algorithm, we would like to ensure that \u2016w\u20161 is always smaller than some threshold B. We achieve this goal by adding an additional projection step at the end of each Pegasos\u2019s iteration. Formally, after performing the update we set\nw\u2190 argmin u:\u2016u\u20161\u2264B \u2016u\u2212w\u20162 . (6)\nThis projection step can be performed efficiently in time O(d) using the technique described in (Duchi et al., 2008). A pseudo-code of the resulting Attribute Efficient Regression algorithm is given in Algorithm 2.\nAlgorithm 2 AER(S, k) S \u2014 Full information training set with m examples k \u2014 Access only k elements of each instance in S Parameters: \u03bb,B\nw = (0, . . . , 0) ; w\u0304 = w ; t = 1 for each (x, y) \u2208 S\nv = 0 \u2208 Rd Choose C uniformly at random from\nall subsets of [d] of size k/2 for each j \u2208 C vj = vj + 2 k d xj end y\u0302 = 0 for r = 1, . . . , k/2\nsample i from [d] based on P[i] = |wi|/\u2016w\u20161 y\u0302 = y\u0302 + 2k sgn(wi) \u2016w\u20161 xj\nend w = (1\u2212 1t )w \u2212 2 \u03bbt (y\u0302 \u2212 y)v w = argminu:\u2016u\u20161\u2264B \u2016u\u2212w\u20162 w\u0304 = w\u0304 + w/m t = t+ 1\nend Output: w\u0304\nThe following theorem provides convergence guarantees for AER.\nTheorem 2 Let D be a distribution such that P[x \u2208 [\u22121,+1]d \u2227 y \u2208 [\u22121,+1]] = 1. Let w? be any vector such that \u2016w?\u20161 \u2264 B and \u2016w?\u20162 \u2264 B2 Then,\nE[LD(w\u0304)] \u2264 LD(w?)+O ( d (B + 1)B2\u221a\nk\n\u221a ln(m)\nm\n) ,\nwhere |S| = m, w\u0304 is the output of AER(S, k) run with \u03bb = ((B+1)d/B2) \u221a log(m)/(mk), and the expectation is over the choice of S and over the algorithm\u2019s own randomization.\nFor simplicity and readability, in the above theorem we only bounded the expected risk. It is possible to obtain similar guarantees with high probability by relying on Azuma\u2019s inequality \u2014see for example (Cesa-Bianchi et al., 2004).\nNote that \u2016w?\u20162 \u2264 \u2016w?\u20161 \u2264 B, so Theorem 2 implies that\nLD(w\u0304) \u2264 min w:\u2016w\u20161\u2264B LD(w) +O ( dB2\u221a k \u221a ln(m) m ) .\nTherefore, the bound for AER is much better3 than\n3When comparing bounds, we ignore logarithmic terms. Also, in this discussion we assume that B1 and B2 are at least 1.\nthe bound for Baseline: instead of d2/k we have d/ \u221a k.\nIt is interesting to compare the bound for AER to the Lasso bound in the full information case given in (2). As it can be seen, to achieve the same level of risk, AER needs a factor of d2/k more examples than the full information Lasso.4 Since each AER example uses only k attributes while each Lasso example uses all d attributes, the ratio between the total number of attributes AER needs and the number of attributes Lasso needs to achieve the same error is O(d). Intuitively, when having d times total number of attributes, we can fully compensate for the partial information protocol.\nHowever, in some situations even this extra d factor is not needed. Suppose we know that the vector w?, which minimizes the risk, is dense. That is, it satisfies \u2016w?\u20161 \u2248 \u221a d \u2016w?\u20162. In this case, choosing\nB2 = B/ \u221a d, the bound in Theorem 2 becomes order of B2 \u221a d/k \u221a\n1/m. Therefore, the number of examples AER needs in order to achieve the same error as Lasso is only a factor d/k more than the number of examples Lasso uses. But, this implies that both AER and Lasso needs the same number of attributes in order to achieve the same level of error! Crucially, the above holds only if w? is dense. When w? is sparse we have \u2016w?\u20161 \u2248 \u2016w?\u20162 and then AER needs more attributes than Lasso.\nOne might wonder whether a more clever active sampling strategy could attain in the sparse case the performance of Lasso while using the same number of attributes. The next subsection shows that this is not possible in general."}, {"heading": "2.3. Lower bounds and negative results", "text": "We now show (proof in the appendix) that any attribute efficient algorithm needs in general order of d/ examples for learning an -accurate sparse linear predictor. Recall that the upper bound of AER implies that order of d2(B + 1)2B22/\n2 examples are sufficient for learning a predictor with LD(w) \u2212 LD(w?) < . Specializing this sample complexity bound of AER to the w? described in Theorem 3 below, yields that O(d2/ ) examples are sufficient for AER for learning a good predictor in this case. That is, we have a gap of factor d between the lower bound and the upper bound, and it remains open to bridge this gap.\nTheorem 3 For any \u2208 (0, 1/16), k, and d \u2265 4k, 4We note that when d = k we still do not recover the full information bound. However, it is possible to improve the analysis and replace the factor d/ \u221a k with a factor dmaxt \u2016xt\u20162/k.\nthere exists a distribution over examples and a weight vector w?, with \u2016w?\u20160 = 1 and \u2016w?\u20162 = \u2016w?\u20161 = 2 \u221a , such that any attribute efficient regression algorithm accessing at most k attributes per training example must see (in expectation) at least \u2126 ( d k ) examples in order to learn a linear predictor w with LD(w)\u2212 LD(w?) < .\nRecall that in our setting, while at training time the learner can only view k attributes of each example, at test time all attributes can be observed. The setting of Greiner et al. (2002), instead, assumes that at test time the learner cannot observe all the attributes. The following theorem shows that if a learner can view at most 2 attributes at test time then it is impossible to give accurate predictions at test time even when the optimal linear predictor is known.\nTheorem 4 There exists a weight vector w? and a distribution D such that LD(w?) = 0 while any algorithm A that gives predictions A(x) while viewing only 2 attributes of each x must have LD(A) \u2265 1/9.\nThe proof is given in the appendix. This negative result highlights an interesting phenomenon. We can learn an arbitrarily accurate predictor w from partially observed examples. However, even if we know the optimal w?, we might not be able to accurately predict a new partially observed example."}, {"heading": "3. Proof Sketch of Theorem 2", "text": "Here we only sketch the proof of Theorem 2. A complete proof of all our theorems is given in the appendix.\nWe start with a general logarithmic regret bound for strongly convex functions (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008). The regret bound implies the following. Let z1, . . . , zm be a sequence of vectors, each of which has norm bounded by G. Let \u03bb > 0 and consider the sequence of functions g1, . . . , gm such that gt(w) = \u03bb 2 \u2016w\u2016\n2 + \u3008zt,w\u3009. Each gt is \u03bb-strongly convex (meaning, it is not too flat), and therefore regret bounds for strongly convex functions tell us that there is a way to construct a sequence of vectors w1, . . . ,wm such that for any w? that satisfies \u2016w?\u20161 \u2264 B we have\n1\nt m\u2211 t=1 gt(wt)\u2212 1 t m\u2211 t=1 gt(w ?) \u2264 O ( G2 log(m) \u03bbm ) .\nWith an appropriate choice of \u03bb, and with the assumption \u2016w?\u20162 \u2264 B2, the above inequality implies that 1 m \u2211m t=1\u3008zt,wt \u2212w?\u3009 \u2264 \u03b1 where \u03b1 = O ( GB2 log(m)\u221a m ) . This holds for any sequence of z1, . . . , zm, and in particular, we can set zt = 2(y\u0302t \u2212 yt)vt. Note that zt is a\nrandom vector that depends both on the value of wt and on the random bits chosen on round t. Taking conditional expectation of zt w.r.t. the random bits chosen on round t we obtain that E[zt|wt] is exactly the gradient of (\u3008w,xt\u3009 \u2212 yt)2 at wt, which we denote by \u2207t. From the convexity of the squared loss, we can lower bound \u3008\u2207t,wt \u2212 w?\u3009 by (\u3008wt,xt\u3009 \u2212 yt)2 \u2212 (\u3008w?,xt\u3009 \u2212 yt)2. That is, in expectation we have that\nE [ 1 m m\u2211 t=1 ( (\u3008wt,xt\u3009 \u2212 yt)2 \u2212 (\u3008w?,xt\u3009 \u2212 yt)2 )] \u2264 \u03b1 .\nTaking expectation w.r.t. the random choice of the examples from D, denoting w\u0304 = 1m \u2211m t=1, and using Jensen\u2019s inequality we get that E[LD(w\u0304)] \u2264 LD(w?)+ \u03b1. Finally, we need to make sure that \u03b1 is not too large. The only potential danger is that G, the bound on the norms of z1, . . . , zm, will be large. We make sure this cannot happen by restricting each wt to the `1 ball of radius B, which ensures that \u2016zt\u2016 \u2264 O((B+ 1)d) for all t."}, {"heading": "4. Experiments", "text": "We performed some preliminary experiments to test the behavior of our algorithm on the well-known MNIST digit recognition dataset (Cun et al., 1998), which contains 70,000 images (28\u00d7 28 pixels each) of the digits 0 \u2212 9. The advantages of this dataset for our purposes is that it is not a small-scale dataset, has a reasonable dimensionality-to-data-size ratio, and the setting is clearly interpretable graphically. While this dataset is designed for classification (e.g. recognizing the digit in the image), we can still apply our algorithms on it by regressing to the label.\nFirst, to demonstrate the hardness of our settings, we provide in Figure 1 below some examples of images from the dataset, in the full information setting and the partial information setting. The upper row contains six images from the dataset, as available to a full-information algorithm. A partial-information algorithm, however, will have a much more limited access to these images. In particular, if the algorithm may only choose k = 4 pixels from each image, the same six images as available to it might look like the bottom row of Figure 1.\nWe began by looking at a dataset composed of \u201c3 vs. 5\u201d, where all the 3 digits were labeled as \u22121 and all the 5 digits were labeled as +1. We ran four different algorithms on this dataset: the simple Baseline algorithm, AER, as well as ridge regression and Lasso for comparison (for Lasso, we solved (1) with p = 1). Both ridge regression and Lasso were run in the full information setting: Namely, they enjoyed full access to\nall attributes of all examples in the training set. The Baseline algorithm and AER, however, were given access to only 4 attributes from each training example.\nWe randomly split the dataset into a training set and a test set (with the test set being 10% of the original dataset). For each algorithm, parameter tuning was performed using 10-fold cross validation. Then, we ran the algorithm on increasingly long prefixes of the training set, and measured the average regression error (\u3008w,x\u3009 \u2212 y)2 on the test set. The results (averaged over runs on 10 random train-test splits) are presented in Figure 2. In the upper plot, we see how the test regression error improves with the number of examples. The Baseline algorithm is highly unstable at the beginning, probably due to the ill-conditioning of the estimated covariance matrix, although it eventually stabilizes (to prevent a graphical mess at the left hand side of the figure, we removed the error bars from the corresponding plot). Its performance is worse than AER, completely in line with our earlier theoretical analysis.\nThe bottom plot of Figure 2 is similar, only that now the X-axis represents the accumulative number of attributes seen by each algorithm rather than the number of examples. For the partial-information algorithm, the graph ends at approximately 49,000 attributes, which is the total number of attributes accessed by the algorithm after running over all training examples, seeing k = 4 pixels from each example. However, for the full-information algorithm 49,000 attributes are already seen after just 62 examples. When we compare the algorithms in this way, we see that our AER algorithm achieves excellent performance for a given attribute budget, significantly better than the other L1-based algorithms, and even comparable to full-information ridge regression.\nFinally, we tested the algorithms over 45 datasets generated from MNIST, one for each possible pair of dig-\nits. For each dataset and each of 10 random train-test splits, we performed parameter tuning for each algorithm separately, and checked the average squared error on the test set. The median test errors over all datasets are presented in the table below.\nTest Error Full Information Ridge 0.110\nLasso 0.222 Partial Information AER 0.320\nBaseline 0.815\nAs can be seen, the AER algorithm manages to achieve good performance, not much worse than the fullinformation Lasso algorithm. The Baseline algorithm, however, achieves a substantially worse performance, in line with our theoretical analysis above. We also calculated the test classification error of AER, i.e. sign(\u3008w,x\u3009) 6= y, and found out that AER, which can see only 4 pixels per image, usually perform only a little worse than the full-information algorithms (ridge regression and Lasso), which enjoy full access to all 784 pixels in each image. In particular, the median test classification errors of AER, Lasso, and Ridge are\n3.5%, 1.1%, and 1.3% respectively."}, {"heading": "5. Discussion and Extensions", "text": "In this paper, we provided an efficient algorithm for learning when only a few attributes from each training example can be seen. The algorithm comes with formal guarantees, is provably competitive with algorithms which enjoy full access to the data, and seems to perform well in practice. We also presented sample complexity lower bounds, which are only a factor d smaller than the upper bound achieved by our algorithm, and it remains open to bridge this gap.\nOur approach easily extends to other gradient-based algorithms besides Pegasos. For example, generalized additive algorithms such as p-norm Perceptrons and Winnow - see, e.g., (Cesa-Bianchi and Lugosi, 2006).\nAn obvious direction for future research is how to deal with loss functions other than the squared loss. In upcoming work on a related problem, we develop a technique which allows us to deal with arbitrary analytic loss functions, but in the setting of this paper will lead to sample complexity bounds which are exponential in d. Another interesting extension we are considering is connecting our results to the field of privacy-preserving learning (Dwork, 2008), where the goal is to exploit the attribute efficiency property in order to prevent acquisition of information about individual data instances."}, {"heading": "P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire.", "text": "The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32, 2003.\nM-F Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Proceedings of ICML, 2006."}, {"heading": "S. Ben-David and E. Dichterman. Learning with restricted", "text": "focus of attention. Journal of Computer and System Sciences, 56, 1998."}, {"heading": "A. Beygelzimer, S. Dasgupta, and J. Langford. Importance", "text": "weighted active learning. In Proceedings of ICML, 2009."}, {"heading": "R. Calderbank, S. Jafarpour, and R. Schapire. Compressed learning: Universal sparse dimensionality reduction and learning in the measurement domain. Manuscript, 2009.", "text": "N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.\nN. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050\u20132057, September 2004.\nD. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning, 15:201\u2013221, 1994."}, {"heading": "Y. L. Le Cun, L. Bottou, Y. Bengio, and P. Haffner.", "text": "Gradient-based learning applied to document recognition. Proceedings of IEEE, 86(11):2278\u20132324, November 1998."}, {"heading": "A. Dempster, N. Laird, and D. Rubin. Maximum likelihood", "text": "from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Ser. B, 39:1\u201338, 1977."}, {"heading": "K. Deng, C. Bourke, S. Scott, J. Sunderman, and Y. Zheng.", "text": "Bandit-based algorithms for budgeted learning. In Proceedings of ICDM, pages 463\u2013468. IEEE Computer Society, 2007.\nL. Devroye, L. Gyo\u0308rfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996."}, {"heading": "J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.", "text": "Efficient projections onto the `1-ball for learning in high dimensions. In Proceedings of ICML, 2008."}, {"heading": "C. Dwork. Differential privacy: A survey of results. In", "text": "M. Agrawal, D.-Z. Du, Z. Duan, and A. Li, editors, TAMC, volume 4978 of Lecture Notes in Computer Science, pages 1\u201319. Springer, 2008."}, {"heading": "R. Greiner, A. Grove, and D. Roth. Learning cost-sensitive", "text": "active classifiers. Artificial Intelligence, 139(2):137\u2013174, 2002."}, {"heading": "S. Hanneke. A bound on the label complexity of agnostic", "text": "active learning. In Proceedings of ICML, 2007.\nS. Hanneke. Adaptive rates of convergence in active learning. In Proceedings of COLT, 2009."}, {"heading": "D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications.", "text": "Information and Computation, 100(1):78\u2013150, 1992."}, {"heading": "E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In", "text": "Proceedings of ICML, 2006."}, {"heading": "S. Kakade and S. Shalev-Shwartz. Mind the duality gap: Logarithmic regret algorithms for online optimization.", "text": "In Proceedings of NIPS, 2008."}, {"heading": "A. Kapoor and R. Greiner. Learning and classifying under", "text": "hard budgets. In Proceedings of ECML, pages 170\u2013181, 2005.\nS. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM. In Proceedings of ICML, pages 807\u2013814, 2007."}, {"heading": "R. Tibshirani. Regression shrinkage and selection via the", "text": "lasso. J. Royal. Statist. Soc B., 58(1):267\u2013288, 1996.\nV. N. Vapnik. Statistical Learning Theory. Wiley, 1998."}, {"heading": "S. Zhou, J. Lafferty, and L. Wasserman. Compressed and", "text": "privacy-sensitive sparse regression. IEEE Transactions on Information Theory, 55(2):846\u2013866, 2009."}, {"heading": "A. Proofs", "text": "A.1. Proof of Theorem 1\nTo ease our calculations, we first show that sampling k elements without replacements and then averaging the result has the same expectation as sampling just once. In the lemma below, for a set C we denote the uniform distribution over C by U(C).\nLemma 1 Let C be a finite set and let f : C \u2192 R be an arbitrary function. Let Ck = {C \u2032 \u2282 C : |C \u2032| = k}. Then,\nE C\u2032\u223cU(Ck) [ 1k \u2211 c\u2208C\u2032 f(c)] = E c\u223cU(C) [f(c)] .\nProof Denote |C| = n. We have:\nE C\u2032\u223cU(Ck) [ 1k \u2211 c\u2208C\u2032 f(c)] = 1( n k ) \u2211 C\u2032\u2208Ck 1 k \u2211 c\u2208C\u2032 f(c)\n= 1 k ( n k ) \u2211 c\u2208C f(c)|{C \u2032 \u2208 Ck : c \u2208 C \u2032}|\n=\n( n\u22121 k\u22121 )\nk ( n k ) \u2211 c\u2208C f(c)\n= (n\u2212 1)!k!(n\u2212 k)! kn!(k \u2212 1)!(n\u2212 k)! \u2211 c\u2208C f(c) = 1\nn \u2211 c\u2208C f(c)\n= E c\u223cU(C) [f(c)] .\nTo prove Theorem 1 we first show that the estimation matrix constructed by the Baseline algorithm is likely to be close to the true correlation matrix over the training set.\nLemma 2 Let At be the matrix constructed at iteration t of the Baseline algorithm and note that A\u0304 = 1 m \u2211m t=1At.\nLet X = 1m \u2211m t=1 xtx T t . Then, with probability of at least 1 \u2212 \u03b4 over the algorithm\u2019s own randomness we have that\n\u2200r, s |A\u0304r,s \u2212Xr,s| \u2264 d2 k \u00b7 \u221a 2 ln(2d2/\u03b4) m .\nProof Based on Lemma 1, it is easy to verify that E[At] = xTt xt. Additionally, since we sample without replacements, each element of At is in [\u2212d2/k, d2/k] (because we assume \u2016xt\u2016\u221e \u2264 1). Therefore, we can apply Hoeffding\u2019s inequality on each element of A\u0304 and obtain that\nP[|A\u0304r,s \u2212Xr,s| > ] \u2264 2e\u2212mk 2 2/(2d4) .\nCombining the above with the union bound we obtain that\nP[\u2203(r, s) : |A\u0304r,s \u2212Xr,s| > ] \u2264 2d2 e\u2212mk 2 2/(2d4) .\nCalling the right-hand-side of the above \u03b4 and rearranging terms we conclude our proof.\nNext, we show that the estimate of the linear part of the objective function is also likely to be accurate.\nLemma 3 Let vt be the vector constructed at iteration t of the Baseline algorithm and note that v\u0304 = 1 m \u2211m t=1 2ytvt. Let x\u0304 = 1 m \u2211m t=1 2ytxt. Then, with probability of at least 1\u2212 \u03b4 over the algorithm\u2019s own randomness we have that\n\u2016v\u0304 \u2212 x\u0304\u2016\u221e \u2264 d k \u00b7 \u221a 8 ln(2d/\u03b4) m .\nProof Based on Lemma 1, it is easy to verify that E[2ytvt] = 2ytxt. Additionally, since we sample k/2 pairs without replacements, each element of vt is in [\u22122d/k, 2d/k] (because we assume \u2016xt\u2016\u221e \u2264 1) and thus each element of 2ytvt is in [\u22124d/k, 4d/k] (because we assume that |yt| \u2264 1). Therefore, we can apply Hoeffding\u2019s inequality on each element of v\u0304 and obtain that\nP[|v\u0304r \u2212 x\u0304r| > ] \u2264 2e\u2212mk 2 2/(8d2) .\nCombining the above with the union bound we obtain that\nP[\u2203(r, s) : |A\u0304r,s \u2212Xr,s| > ] \u2264 2 d e\u2212mk 2 2/(8d2) .\nCalling the right-hand-side of the above \u03b4 and rearranging terms we conclude our proof.\nWe next show that the estimated training loss found by the Baseline algorithm, L\u0303S(w), is close to the true training loss.\nLemma 4 With probability greater than 1\u2212 \u03b4 over the Baseline Algorithm\u2019s own randomization, for all w such that \u2016w\u20161 \u2264 B we have that\n|L\u0303S(w)\u2212 LS(w)| \u2264 O\n( B2 d2 k \u00b7 \u221a ln(d/\u03b4) m ) .\nProof Combining Lemma 2 with the boundedness of \u2016w\u20161 and using Holder\u2019s inequality twice we easily get that\n|wT (A\u0304\u2212X)w| \u2264 B 2 d2 k \u00b7 \u221a 2 ln(2d2/\u03b4) m .\nSimilarly, using Lemma 3 and Holder\u2019s inequality, |wT (v\u0304 \u2212 x\u0304)| \u2264 B d k \u00b7 \u221a 8 ln(2d/\u03b4) m .\nCombining the above inequalities with the union bound and the triangle inequality we conclude our proof.\nWe are now ready to prove Theorem 1. First, using standard risk bounds (based on Rademacher complexities5) we know that with probability greater than 1\u2212 \u03b4 over the choice of a training set of m examples, for all w s.t. \u2016w\u20161 \u2264 B, we have that\n|LS(w)\u2212 LD(w)| \u2264 O ( B2 \u221a ln(d/\u03b4)\nm\n) .\nCombining the above with Lemma 4 we obtain that for any w s.t. \u2016w\u20161 \u2264 B,\n|LD(w)\u2212 L\u0303S(w)| \u2264 |LD(w)\u2212 LS(w)|+ |LS(w)\u2212 L\u0303S(w)|\n\u2264 O\n( B2 d2 k \u00b7 \u221a ln(d/\u03b4) m ) .\nThe proof of Theorem 1 follows since the Baseline algorithm minimizes L\u0303S(w).\n5To bound the Rademacher complexity, we use the boundedness of \u2016w\u20161, \u2016x\u2016\u221e, |y| to get that the squared loss is O(B) Lipschitz on the domain. Combining this with the contraction principle yields the desired Rademacher bound.\nA.2. Proof of Theorem 2\nWe start with the following lemma.\nLemma 5 Let yt, y\u0302t,vt,wt be the values of y, y\u0302,v,w, respectively, at iteration t of the AER algorithm. Then, for any vector w? s.t. \u2016w?\u20161 \u2264 B we have\nm\u2211 t=1 ( \u03bb 2 \u2016wt\u2016 2 2 + 2(y\u0302t \u2212 yt)\u3008vt,wt\u3009 ) \u2264\nm\u2211 t=1 ( \u03bb 2 \u2016w ?\u201622 + 2(y\u0302t \u2212 yt)\u3008vt,w?\u3009 ) +O ( ((B+1)d)2/k log(m) \u03bb ) .\nProof The proof follows directly from logarithmic regret bounds for strongly convex functions (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008) by noting that according to our construction, maxt 2(y\u0302t \u2212 yt)\u2016vt\u20162 \u2264 O((B + 1) d/ \u221a k). Let B2 be such that \u2016w?\u20162 \u2264 B2 and choose \u03bb = ((B + 1)d/B2) \u221a\nlog(m)/(mk). Since \u03bb\u2016wt\u20162 \u2265 0 we obtain from Lemma 5 that\nm\u2211 t=1 2(y\u0302t \u2212 yt)\u3008vt,wt \u2212w?\u3009 \u2264 m\u03bb\u2016w ?\u201622 2\n+O (\n((B+1)d)2/k log(m) \u03bb\n) = O ( d\u221a k (B + 1)B2 \u221a log(m) m ) \ufe38 \ufe37\ufe37 \ufe38\ndef = \u03b1\n. (7)\nFor each t, let \u2207t = 2(\u3008wt,xt\u3009 \u2212 yt)xt and \u2207\u0303t = 2(y\u0302t \u2212 yt)vt. Taking expectation of (7) with respect to the algorithm\u2019s own randomization, and noting that the conditional expectation of \u2207\u0303t equals \u2207t, we obtain\nE [ m\u2211 t=1 \u3008\u2207t,wt \u2212w?\u3009 ] \u2264 \u03b1 . (8)\nFrom the convexity of the squared loss we know that\n(\u3008wt,xt\u3009 \u2212 yt)2 \u2212 (\u3008w?,xt\u3009 \u2212 yt)2 \u2264 \u3008\u2207t,wt \u2212w?\u3009 .\nCombining with (8) yields\nE [ m\u2211 t=1 (\u3008wt,xt\u3009 \u2212 yt)2 \u2212 (\u3008w?,xt\u3009 \u2212 yt)2 ] \u2264 \u03b1 . (9)\nTaking expectation again, this time with respect to the randomness in choosing the training set, and using the fact that wt only depends on previous examples in the training set, we obtain that\nE [ m\u2211 t=1 LD(wt)\u2212 LD(w?) ] \u2264 \u03b1 . (10)\nFinally, from Jensen\u2019s inequality we know that E[ 1m \u2211m t=1 LD(wt)] \u2265 E[LD(w\u0304)] and this concludes our proof.\nA.3. Proof of Theorem 3\nThe outline of the proof is as follows. We define a specific distribution such that only one \u201cgood\u201d feature is slightly correlated with the label. We then show that if some algorithm learns a linear predictor with an extra risk of at most , then it must know the value of the \u2019good\u2019 feature. Next, we construct a variant of a multi-armed bandit problem out of our distribution and show that a good learner can yield a good prediction strategy. Finally, we adapt a lower bound for the multi-armed bandit problem given in (Auer et al., 2003), to conclude that in our case no learner can be too good.\nThe distribution: We generate a joint distribution over Rd \u00d7 R as follows. Choose some j \u2208 [d]. First, each feature is generated i.i.d. according to P[xi = 1] = P[xi = \u22121] = 12 . Next, given x and j, y is generated according to P[y = xj ] = 12 + p and P[y = \u2212xj ] = 1 2 \u2212 p, where p is set to be \u221a . Denote by Pj the distribution mentioned above assuming the \u201cgood\u201d feature is j. Also denote by Pu the uniform distribution over {\u00b11}d+1. Analogously, we denote by Ej and Eu expectations w.r.t. Pj and Pu.\nA good regressor \u201cknows\u201d j : We now show that if we have a good linear regressor than we can know the value of j. The optimal linear predictor is w? = 2pej and the risk of w? is\nLD(w ?) = E[(\u3008w?,x\u3009 \u2212 y)2] = ( 1 2 + p ) (1\u2212 2p)2 + ( 1 2 \u2212 p ) (1 + 2p)2 = 1 + 4p2 \u2212 8p2 = 1\u2212 4p2 .\nThe risk of an arbitrary weight vector under the aforementioned distribution is:\nLD(w) = E x,y [(\u3008w,x\u3009 \u2212 y)]2 = \u2211 i 6=j w2i + E[(wjxj \u2212 y)2] = \u2211 i 6=j w2i + w 2 j + 1\u2212 4pwj . (11)\nSuppose that LD(w)\u2212 LD(w?) < . This implies that:\n1. For all i 6= j we have w2i < , or equivalently, |wi| < \u221a . 2. 1 + w2j \u2212 4pwj \u2212 (1\u2212 4p2) < and thus |wj \u2212 2p| < \u221a which gives |wj | > 2p\u2212 \u221a\nSince we set p = \u221a , the above implies that we can identify the value of j from any w whose risk is strictly smaller than LD(w ?) + .\nConstructing a variant of a multi-armed bandit problem: We now construct a variant of the multi-armed bandit problem out of the distribution Pj . Each i \u2208 [d] is an arm and the reward of pulling i is 12 |xi+y| \u2208 {0, 1}. Unlike standard multi-armed bandit problems, here at each round the learner chooses K arms at,1, . . . , at,K , which correspond to the K atributes accessed at round t, and his reward is defined to be the average of the rewards of the chosen arms. At the end of each round the learner observes the value of xt at at,1, . . . , at,K , as well as the value of yt. Note that the expected reward is 1 2 + p 1 K \u2211K i=1 1[at,i=j]. Therefore, the total expected reward of an algorithm that runs for T rounds is upper bounded by 12T + pE[Nj ], where Nj is the number of times j \u2208 {at,1, . . . , at,K}.\nA good learner yields a strategy: Suppose that we have a learner that can learn a linear predictor with LD(w)\u2212 LD(w?) < using m examples (on average). Since we have shown that once LD(w)\u2212 LD(w?) < we know the value of j, we can construct a strategy for the multi-armed bandit problem in a straightforward way; Simply use the first m examples to learn w and from then on always pull the arm j, namely, at,1 = . . . = at,K = j. The expected reward of this algorithm is at least\n1 2m+ (T \u2212m) ( 1 2 + p ) = 12T + (T \u2212m)p .\nAn upper bound on the reward of any strategy: Consider an arbitrary prediction algorithm. At round t the algorithm uses the history (and its own random bits, which we can assume are set in advance) to ask for the current K attributes at,1, . . . , at,K . The history is the value of xs at as,1, . . . , as,K as well as the value of ys, for all s < t. That is, we can denote the history at round t to be rt = (r1,1, . . . , r1,K+1), . . . , (rt\u22121,1, . . . , rt\u22121,K+1). Therefore, on round t the algorithm uses a mapping from rt to [d]K . We use r as a shorthand for rT+1. The following lemma shows that any function of the history cannot distinguish too well between the distribution Pj and the uniform distribution.\nLemma 6 Let f : {\u22121, 1}(K+1)T \u2192 [0,M ] be any function defined on a history sequence r = (r1,1, . . . , r1,K+1), . . . , (rT,1, . . . , rT,K+1). Let Nj be the number of times the algorithm calculating f picks action j among the selected arms. Then,\nEj [f(r)] \u2264 Eu[f(r)] +M \u221a \u2212 log(1\u2212 4p2)Eu[Nj ] .\nProof For any two distributions P,Q we let \u2016P \u2212 Q\u20161 = \u2211\nr |P [r] \u2212 Q[r]| be the total variation distance and let KL(P,Q) = \u2211 r P [r] log(P [r]/Q[r]) be the KL divergence. Using Holder inequality we know that Ej [f(r)]\u2212 Eu[f(r)] \u2264M\u2016Pj \u2212 Pu\u20161. Additionally, using Pinsker\u2019s inequality we have 12\u2016Pj \u2212 Pu\u2016 2 1 \u2264 KL(Pu, Pj). Finally, the chain rule and simple calculations yield,\nKL(Pu, Pj) = \u2211 r ( 1 2 )(K+1)T T\u2211 t=1 log ( Pu[rt,\u00b7 | rt\u22121] Pj [rt,\u00b7 | rt\u22121] )\n= \u2211 r ( 1 2 )(K+1)T T\u2211 t=1 log  ( 12)K+1( 1 2 )K+1 + ( 1 2 )K p1[ \u2228K i=1(at,i=j)] sgn(xt,jyt)  = \u2211 r ( 1 2 )(K+1)T T\u2211 t=1 1[ \u2228K i=1(at,i=j)] ( \u2212 log ( 1 + 2 p sgn(xt,jyt)\n)) =\nT\u2211 t=1 Eu [ 1[ \u2228K i=1(at,i=j)] ( \u2212 log ( 1 + 2 p sgn(xt,jyt) ))] =\nT\u2211 t=1 Pu [ K\u2228 i=1 (at,i = j) ] Eu [ \u2212 log ( 1 + 2 p sgn(xt,jyt) )] (since xt,jyt is independent of at,1, . . . , at,K)\n= (\n1 2 (\u2212 log\n( 1 + 2 p) ) + 12 ( \u2212 log(1\u2212 2 p) )) T\u2211 t=1 Pu [ K\u2228 i=1 (at,i = j) ] = \u2212 12 log(1\u2212 4p 2)Eu[Nj ] .\nCombining all the above we conclude our proof.\nWe have shown previously that the expected reward of any algorithm is bounded above by 12T + pEj [Nj ]. Applying Lemma 6 above on f(r) = Nj \u2208 {0, 1, . . . , T} we get that\nEj [Nj ] \u2264 Eu[Nj ] + T \u221a \u2212 log(1\u2212 4p2)Eu[Nj ] .\nTherefore, the expected reward of any algorithm is at most\n1 2T + p ( Eu[Nj ] + T \u221a \u2212 log(1\u2212 4p2)Eu[Nj ] ) .\nSince the adversary will choose j to minimize the above and since the minimum over j is smaller then the expectation over choosing j uniformly at random we have that the reward against an adversarial choice of j is at most\n1 2T + p\n1\nd d\u2211 j=1 ( Eu[Nj ] + T \u221a \u2212 log(1\u2212 4p2)Eu[Nj ] ) . (12)\nNote that\n1\nd d\u2211 j=1 Eu[Nj ] = 1 d Eu[N1 + . . .+Nd] \u2264 KT d .\nCombining this with (12) and using Jensen\u2019s inequality we obtain the following upper bound on the reward\n1 2T + p ( K d T + T \u221a \u2212 log(1\u2212 4p2)Kd T ) .\nAssuming that \u2264 1/16 we have that 4p2 = 4 \u2264 1/4 and thus using the inequality \u2212 log(1 \u2212 q) \u2264 32q, which holds for q \u2208 [0, 1/4], we get the upper bound\n1 2T + p ( K d T + T \u221a 6K d p 2T ) . (13)\nConcluding the proof: Take a learning algorithm that finds an -good predictor using m examples. Since the reward of the strategy based on this learning algorithm cannot exceed the upper bound given in (13) we obtain that:\n1 2T + (T \u2212m)p \u2264 1 2T + p ( K d T + T \u221a 6K d p 2T ) which solved for m gives\nm \u2265 T ( 1\u2212 Kd \u2212 \u221a 6K d p 2T ) .\nSince we assume d \u2265 4K, choosing T = \u230a d / (96Kp2) \u230b , and recalling p2 = , gives\nm \u2265 T 2 = 1 2\n\u230a d\n96K\n\u230b .\nA.4. Proof of Theorem 4\nLet w? = (1/3, 1/3, 1/3). Let x \u2208 {\u00b11}3 be distributed uniformly at random and y is determined deterministically to be \u3008w?,x\u3009. Then, LD(w?) = 0. However, any algorithm that only view 2 attributes have an uncertainty about the label of at least \u00b1 13 , and therefore its expected squared error is at least 1/9. Formally, suppose the algorithm asks for the first two attributes and form its prediction to be y\u0302. Since the generation of attributes is independent, we have that the value of x3 does not depend on x1, x2, and y\u0302, and therefore\nE[(y\u0302\u2212\u3008w?,x\u3009)2] = E[(y\u0302\u2212w?1x1\u2212w?x2\u2212w?3x3)2] = E[(y\u0302\u2212w?1x1\u2212w?x2)2]+E[(w?3x3)2] \u2265 0+(1/3)2 E[x23] = 1/9 ,\nwhich concludes our proof."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Agnostic active learning", "author": ["M-F Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In Proceedings of ICML,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Learning with restricted focus of attention", "author": ["S. Ben-David", "E. Dichterman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Ben.David and Dichterman.,? \\Q1998\\E", "shortCiteRegEx": "Ben.David and Dichterman.", "year": 1998}, {"title": "Importance weighted active learning", "author": ["A. Beygelzimer", "S. Dasgupta", "J. Langford"], "venue": "In Proceedings of ICML,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Compressed learning: Universal sparse dimensionality reduction and learning in the measurement", "author": ["R. Calderbank", "S. Jafarpour", "R. Schapire"], "venue": "domain. Manuscript,", "citeRegEx": "Calderbank et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Calderbank et al\\.", "year": 2009}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y.L. Le Cun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "Cun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1998}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society, Ser. B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Bandit-based algorithms for budgeted learning", "author": ["K. Deng", "C. Bourke", "S. Scott", "J. Sunderman", "Y. Zheng"], "venue": "In Proceedings of ICDM,", "citeRegEx": "Deng et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2007}, {"title": "A Probabilistic Theory of Pattern Recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proceedings of ICML,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Differential privacy: A survey of results", "author": ["C. Dwork"], "venue": null, "citeRegEx": "Dwork.,? \\Q2008\\E", "shortCiteRegEx": "Dwork.", "year": 2008}, {"title": "Learning cost-sensitive active classifiers", "author": ["R. Greiner", "A. Grove", "D. Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "Greiner et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Greiner et al\\.", "year": 2002}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "In Proceedings of ICML,", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Adaptive rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "In Proceedings of COLT,", "citeRegEx": "Hanneke.,? \\Q2009\\E", "shortCiteRegEx": "Hanneke.", "year": 2009}, {"title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "author": ["D. Haussler"], "venue": "Information and Computation,", "citeRegEx": "Haussler.,? \\Q1992\\E", "shortCiteRegEx": "Haussler.", "year": 1992}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal"], "venue": "In Proceedings of ICML,", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "Mind the duality gap: Logarithmic regret algorithms for online optimization", "author": ["S. Kakade", "S. Shalev-Shwartz"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Kakade and Shalev.Shwartz.,? \\Q2008\\E", "shortCiteRegEx": "Kakade and Shalev.Shwartz.", "year": 2008}, {"title": "Learning and classifying under hard budgets", "author": ["A. Kapoor", "R. Greiner"], "venue": "In Proceedings of ECML,", "citeRegEx": "Kapoor and Greiner.,? \\Q2005\\E", "shortCiteRegEx": "Kapoor and Greiner.", "year": 2005}, {"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "In Proceedings of ICML,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal. Statist. Soc B.,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Compressed and privacy-sensitive sparse regression", "author": ["S. Zhou", "J. Lafferty", "L. Wasserman"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "In the framework of generative models, a popular approach is the ExpectationMaximization (EM) procedure (Dempster et al., 1977).", "startOffset": 104, "endOffset": 127}, {"referenceID": 0, "context": "Our technique for dealing with missing information borrows ideas from algorithms for the adversarial multi-armed bandit problem (Auer et al., 2003; CesaBianchi and Lugosi, 2006).", "startOffset": 128, "endOffset": 177}, {"referenceID": 7, "context": "This and similar protocols were studied in the context of active learning (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007; 2009; Beygelzimer et al., 2009), where the learner can ask for the target associated with specific examples.", "startOffset": 74, "endOffset": 161}, {"referenceID": 1, "context": "This and similar protocols were studied in the context of active learning (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007; 2009; Beygelzimer et al., 2009), where the learner can ask for the target associated with specific examples.", "startOffset": 74, "endOffset": 161}, {"referenceID": 15, "context": "This and similar protocols were studied in the context of active learning (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007; 2009; Beygelzimer et al., 2009), where the learner can ask for the target associated with specific examples.", "startOffset": 74, "endOffset": 161}, {"referenceID": 3, "context": "This and similar protocols were studied in the context of active learning (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007; 2009; Beygelzimer et al., 2009), where the learner can ask for the target associated with specific examples.", "startOffset": 74, "endOffset": 161}, {"referenceID": 2, "context": "The specific learning task we consider in the paper was first proposed in (Ben-David and Dichterman, 1998), where it is called \u201clearning with restricted focus of attention\u201d.", "startOffset": 74, "endOffset": 106}, {"referenceID": 2, "context": "The specific learning task we consider in the paper was first proposed in (Ben-David and Dichterman, 1998), where it is called \u201clearning with restricted focus of attention\u201d. Ben-David and Dichterman (1998) considered the classification setting and showed learnability ar X iv :1 00 4.", "startOffset": 75, "endOffset": 206}, {"referenceID": 10, "context": "A related setting, called budgeted learning, was recently studied - see for example (Deng et al., 2007; Kapoor and Greiner, 2005) and the references therein.", "startOffset": 84, "endOffset": 129}, {"referenceID": 20, "context": "A related setting, called budgeted learning, was recently studied - see for example (Deng et al., 2007; Kapoor and Greiner, 2005) and the references therein.", "startOffset": 84, "endOffset": 129}, {"referenceID": 4, "context": "Our technique is reminiscent of methods used in the compressed learning framework (Calderbank et al., 2009; Zhou et al., 2009), where data is accessed via a small set of random linear measurements.", "startOffset": 82, "endOffset": 126}, {"referenceID": 23, "context": "Our technique is reminiscent of methods used in the compressed learning framework (Calderbank et al., 2009; Zhou et al., 2009), where data is accessed via a small set of random linear measurements.", "startOffset": 82, "endOffset": 126}, {"referenceID": 22, "context": "The \u201copposite\u201d setting, in which full information is given at training time and the goal is to train a predictor that depends only on a small number of attributes at test time, was studied in the context of learning sparse predictors - see for example (Tibshirani, 1996) and the wide literature on sparsity properties of `1 regularization.", "startOffset": 252, "endOffset": 270}, {"referenceID": 14, "context": "A variant of the above setting is the one studied by Greiner et al. (2002), where the learner has all the information at training time and at test time he tries to actively choose a small amount of attributes to form a prediction.", "startOffset": 53, "endOffset": 75}, {"referenceID": 17, "context": "Following the standard framework of statistical learning (Haussler, 1992; Devroye et al., 1996; Vapnik, 1998), we model the environment as a joint distribution D over the set of instance-target pairs, R \u00d7 R.", "startOffset": 57, "endOffset": 109}, {"referenceID": 11, "context": "Following the standard framework of statistical learning (Haussler, 1992; Devroye et al., 1996; Vapnik, 1998), we model the environment as a joint distribution D over the set of instance-target pairs, R \u00d7 R.", "startOffset": 57, "endOffset": 109}, {"referenceID": 22, "context": "As a simple baseline, we begin by describing a straightforward adaptation of Lasso (Tibshirani, 1996), based on a direct nonadaptive estimate of the loss function.", "startOffset": 83, "endOffset": 101}, {"referenceID": 21, "context": "We then turn to describe a more effective approach, which combines a stochastic gradient descent algorithm called Pegasos (Shalev-Shwartz et al., 2007) with the active sampling of attributes in order to estimate the gradient of the loss at each step.", "startOffset": 122, "endOffset": 151}, {"referenceID": 21, "context": "Concretely, below we describe a variant of the Pegasos algorithm (Shalev-Shwartz et al., 2007) for learning linear regressors.", "startOffset": 65, "endOffset": 94}, {"referenceID": 12, "context": "This projection step can be performed efficiently in time O(d) using the technique described in (Duchi et al., 2008).", "startOffset": 96, "endOffset": 116}, {"referenceID": 6, "context": "It is possible to obtain similar guarantees with high probability by relying on Azuma\u2019s inequality \u2014see for example (Cesa-Bianchi et al., 2004).", "startOffset": 116, "endOffset": 143}, {"referenceID": 14, "context": "The setting of Greiner et al. (2002), instead, assumes that at test time the learner cannot observe all the attributes.", "startOffset": 15, "endOffset": 37}, {"referenceID": 18, "context": "We start with a general logarithmic regret bound for strongly convex functions (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008).", "startOffset": 79, "endOffset": 132}, {"referenceID": 19, "context": "We start with a general logarithmic regret bound for strongly convex functions (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008).", "startOffset": 79, "endOffset": 132}, {"referenceID": 8, "context": "We performed some preliminary experiments to test the behavior of our algorithm on the well-known MNIST digit recognition dataset (Cun et al., 1998), which contains 70,000 images (28\u00d7 28 pixels each) of the digits 0 \u2212 9.", "startOffset": 130, "endOffset": 148}, {"referenceID": 5, "context": ", (Cesa-Bianchi and Lugosi, 2006).", "startOffset": 2, "endOffset": 33}, {"referenceID": 13, "context": "Another interesting extension we are considering is connecting our results to the field of privacy-preserving learning (Dwork, 2008), where the goal is to exploit the attribute efficiency property in order to prevent acquisition of information about individual data instances.", "startOffset": 119, "endOffset": 132}], "year": 2014, "abstractText": "We describe and analyze efficient algorithms for learning a linear predictor from examples when the learner can only view a few attributes of each training example. This is the case, for instance, in medical research, where each patient participating in the experiment is only willing to go through a small number of tests. Our analysis bounds the number of additional examples sufficient to compensate for the lack of full information on each training example. We demonstrate the efficiency of our algorithms by showing that when running on digit recognition data, they obtain a high prediction accuracy even when the learner gets to see only four pixels of each image.", "creator": "TeX"}}}