{"id": "1006.2844", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2010", "title": "Outrepasser les limites des techniques classiques de Prise d'Empreintes grace aux Reseaux de Neurones", "abstract": "educaci\u00f3n We calvillo present an sturnus application of koca Artificial goal Intelligence overmans techniques daragh to reinsure the dasi field of Information Security. tariff The problem of 2,545 remote perur Operating netcaster System (larri OS) Detection, also called OS Fingerprinting, is froese a crucial 202-298-6920 step of macroglossum the ashhurst penetration testing ungerleider process, since non-tropical the jesser attacker (hacker or relabeling security bathysphere professional) needs jonatan to batt know the tolhurst OS 6-for-15 of biggio the target host in order muralt to bournonville choose androgyny the 1975-1981 exploits that he will adisq use. OS Detection vassell is cheshire accomplished cetto by bud\u011bjovice passively jorquera sniffing network packets and provostship actively ashamed sending test sobrino packets newent to dromey the impulsively target host, to porky study 20-some specific variations seguso in the begums host topklasse responses subseries revealing information culgoa about its self-test operating system.", "histories": [["v1", "Mon, 14 Jun 2010 20:52:44 GMT  (66kb)", "http://arxiv.org/abs/1006.2844v1", "16 pages, 3 figures. Symposium sur la S\\'ecurit\\'e des Technologies de l'Information et des Communications (SSTIC), Rennes, France, May 31-June 2, 2006"]], "COMMENTS": "16 pages, 3 figures. Symposium sur la S\\'ecurit\\'e des Technologies de l'Information et des Communications (SSTIC), Rennes, France, May 31-June 2, 2006", "reviews": [], "SUBJECTS": "cs.CR cs.AI cs.NE", "authors": ["javier burroni", "carlos sarraute"], "accepted": false, "id": "1006.2844"}, "pdf": {"name": "1006.2844.pdf", "metadata": {"source": "CRF", "title": "Outrepasser les limites des techniques classiques de Prise d\u2019Empreintes gra\u0302ce aux Re\u0301seaux de Neurones", "authors": ["Javier Burroni", "Carlos Sarraute"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n00 6.\n28 44\nv1 [\ncs .C\nR ]\n1 4\nJu n\n20 10\nOutrepasser les limites des techniques classiques\nde Prise d\u2019Empreintes gra\u0302ce aux Re\u0301seaux de\nNeurones\nJavier Burroni1 and Carlos Sarraute2\n1 Equipe de de\u0301veloppement d\u2019Impact, Core Security Technologies. 2 Laboratoire de recherche de Core Security Technologies et De\u0301partement de\nMathe\u0301matiques de l\u2019Universite\u0301 de Buenos Aires.\nRe\u0301sume\u0301 Nous pre\u0301sentons la de\u0301tection distante de syste\u0300mes d\u2019exploitation comme un proble\u0300me d\u2019infe\u0301rence : a\u0300 partir d\u2019une se\u0301rie d\u2019observations (les re\u0301ponses de la machine cible a\u0300 un ensemble de tests), nous voulons infe\u0301rer le type de syste\u0300me d\u2019exploitation qui ge\u0301ne\u0300rerait ces observations avec une plus grande probabilite\u0301. Les techniques classiques utilise\u0301es pour re\u0301aliser cette analyse pre\u0301sentent plusieurs limitations. Pour outrepasser ces limites, nous proposons l\u2019utilisation de Re\u0301seaux de Neurones et d\u2019outils statistiques. Nous pre\u0301senterons deux modules fonctionnels : un module qui utilise les points finaux DCE-RPC pour distinguer les versions de Windows, et un module qui utilise les signatures de Nmap pour distinguer les versions de syste\u0300mes Windows, Linux, Solaris, OpenBSD, FreeBSD et NetBSD. Nous expliquerons les de\u0301tails de la topologie et du fonctionnement des re\u0301seaux de neurones utilise\u0301s, et du re\u0301glage fin de leurs parame\u0300tres. Finalement nous montrerons des re\u0301sultats expe\u0301rimentaux positifs."}, {"heading": "1 Introduction", "text": "Le proble\u0300me de la de\u0301tection a\u0300 distance du syste\u0300me d\u2019exploitation, aussi appele\u0301 OS Fingerprinting (prise d\u2019empreintes), est une e\u0301tape cruciale d\u2019un test de pe\u0301ne\u0301tration, puisque l\u2019attaquant (professionnel de la se\u0301curite\u0301, consultant ou hacker) a besoin de conna\u0302\u0131tre l\u2019OS de la machine cible afin de choisir les exploits qu\u2019il va utiliser. La de\u0301tection d\u2019OS est accomplie en sniffant de fac\u0327on passive des paquets re\u0301seau et en envoyant de fac\u0327on active des paquets de test a\u0300 la machine cible, pour e\u0301tudier des variations spe\u0301cifiques dans les re\u0301ponses qui re\u0301ve\u0300lent son syste\u0300me d\u2019exploitation.\nLes premie\u0300res imple\u0301mentations de prise d\u2019empreintes e\u0301taient base\u0301es sur l\u2019analyse des diffe\u0301rences entre les imple\u0301mentations de la pile TCP/IP. La ge\u0301ne\u0301ration suivante utilisa des donne\u0301es de la couche d\u2019applications, tels que les endpoints DCE RPC. Bien que l\u2019analyse porte sur plus d\u2019information, quelque variation de l\u2019algorithme consistant a\u0300 chercher le point le plus proche (\u201cbest fit\u201d) est toujours utilise\u0301e pour interpre\u0301ter cette information. Cette strate\u0301gie a plusieurs de\u0301fauts : elle ne va pas marcher dans des situations non standard, et ne permet\npas d\u2019extraire les e\u0301le\u0301ments clefs qui identifient de fac\u0327on unique un syste\u0300me d\u2019exploitation. Nous pensons que le prochain pas est de travailler sur les techniques utilise\u0301es pour analyser les donne\u0301es.\nNotre nouvelle approche se base sur l\u2019analyse de la composition de l\u2019information releve\u0301e durant le processus d\u2019identification du syste\u0300me pour de\u0301couvrir les e\u0301le\u0301ments clef et leurs relations. Pour imple\u0301menter cette approche, nous avons de\u0301veloppe\u0301 des outils qui utilisent des re\u0301seaux de neurones et des techniques des domaines de l\u2019Intelligence Artificielle et les Statistiques. Ces outils ont e\u0301te\u0301 inte\u0301gre\u0301s avec succe\u0300s dans un software commercial (Core Impact)."}, {"heading": "2 DCE-RPC Endpoint mapper", "text": ""}, {"heading": "2.1 Le service DCE-RPC nous informe", "text": "Dans les syste\u0300mes Windows, le service DCE (Distributed Computing Environment) RPC (Remote Procedure Call) rec\u0327oit les connexions envoye\u0301es au port 135 de la machine cible. En envoyant une reque\u0302te RPC, il est possible de de\u0301terminer quels services ou programmes sont enregistre\u0301s dans la base de donne\u0301es du mapper d\u2019endpoints RPC. La re\u0301ponse inclut le UUID (Universal Unique IDentifier) de chaque programme, le nom annote\u0301, le protocole utilise\u0301, l\u2019adresse de re\u0301seau a\u0300 laquelle le programme est lie\u0301, et le point final du programme (endpoint).\nIl est possible de distinguer les versions, e\u0301ditions et service packs de Windows selon la combinaison de point finaux fournie par le service DCE-RPC.\nPar exemple, une machine Windows 2000 e\u0301dition professionnelle service pack 0, le service RPC retourne 8 points finaux qui correspondent a\u0300 3 programmes :\nuuid=\"5A7B91F8-FF00-11D0-A9B2-00C04FB6E6FC\" annotation=\"Messenger Service\"\nprotocol=\"ncalrpc\" endpoint=\"ntsvcs\" id=\"msgsvc.1\" protocol=\"ncacn_np\" endpoint=\"\\PIPE\\ntsvcs\" id=\"msgsvc.2\" protocol=\"ncacn_np\" endpoint=\"\\PIPE\\scerpc\" id=\"msgsvc.3\" protocol=\"ncadg_ip_udp\" id=\"msgsvc.4\"\nuuid=\"1FF70682-0A51-30E8-076D-740BE8CEE98B\"\nprotocol=\"ncalrpc\" endpoint=\"LRPC\" id=\"mstask.1\" protocol=\"ncacn_ip_tcp\" id=\"mstask.2\"\nuuid=\"378E52B0-C0A9-11CF-822D-00AA0051E40F\"\nprotocol=\"ncalrpc\" endpoint=\"LRPC\" id=\"mstask.3\" protocol=\"ncacn_ip_tcp\" id=\"mstask.4\""}, {"heading": "2.2 Les re\u0301seaux de neurones entrent en jeu ...", "text": "Notre ide\u0301e est de modeler la fonction qui fait correspondre les combinaisons de points finaux aux versions du syste\u0300me d\u2019exploitation avec un re\u0301seau de neurones.\nPlusieurs questions se posent : \u00b7 quel genre de re\u0301seau de neurones allons nous utiliser ? \u00b7 comment organiser les neurones ? \u00b7 comment faire correspondre les combinaisons de points finaux avec les neurones d\u2019entre\u0301e du re\u0301seau? \u00b7 comment entra\u0302\u0131ner le re\u0301seau?\nLe choix adopte\u0301 est d\u2019utiliser un re\u0301seau perceptron multicouches, plus pre\u0301cise\u0301ment compose\u0301 de 3 couches (nous indiquons entre parenthe\u0300ses le nombre de neurones pour chaque couche qui re\u0301sulta des tests de notre laboratoire, voir la figure 1 pour un sche\u0301ma de la topologie du re\u0301seau).\n1. couche d\u2019entre\u0301e (avec 413 neurones) contient un neurone pour chaque UUID et un neurone pour chaque point final qui correspond a\u0300 cet UUID. Suivant l\u2019exemple pre\u0301ce\u0301dent, nous avons un neurone pour le service Messenger et 4 neurones pour chaque endpoint associe\u0301 a\u0300 ce programme. Cela nous permet de re\u0301pondre avec flexibilite\u0301 a\u0300 l\u2019apparition d\u2019un point final inconnu : nous retenons de toute fac\u0327on l\u2019information de l\u2019UUID principal.\n2. couche de neurones cache\u0301s (avec 42 neurones), ou\u0300 chaque neurone repre\u0301sente une combinaison des neurones d\u2019entre\u0301e.\n3. couche de sortie (avec 25 neurones), contient un neurone pour chaque version et e\u0301dition de Windows (p.ex. Windows 2000 e\u0301dition professionnelle), et un neurone pour chaque version et service pack de Windows (p.ex. Windows 2000 service pack 2). De cette manie\u0300re le re\u0301seau peut distinguer l\u2019e\u0301dition et le service pack de fac\u0327on inde\u0301pendante : les erreurs dans une dimension n\u2019affectent pas les erreurs dans l\u2019autre dimension.\nQu\u2019est-ce qu\u2019un perceptron ? C\u2019est l\u2019unite\u0301 de base du re\u0301seau, et en suivant l\u2019analogie biologique, il joue le ro\u0302le d\u2019un neurone qui rec\u0327oit de l\u2019e\u0301nergie a\u0300 travers de ses connections synaptiques et la transmet a\u0300 son tour selon une fonction d\u2019activation.\nConcre\u0300tement, chaque perceptron calcule sa valeur de sortie comme\nvi,j = f(\nn\u2211\nk=0\nwi,j,k \u00b7 xk )\nou\u0300 {x1 . . . xn} sont les entre\u0301es du neurone, x0 = \u22121 est une entre\u0301e de biais fixe et f est une fonction d\u2019activation non line\u0301aire (nous utilisons tanh, la tangente hyperbolique). L\u2019entra\u0302\u0131nement du re\u0301seau consiste a\u0300 calculer les poids synaptiques {wi,j,0 . . . wi,j,n} pour chaque neurone (i, j) (i est la couche, j est la position du neurone dans la couche)."}, {"heading": "2.3 Entra\u0302\u0131nement par re\u0301tropropagation", "text": "L\u2019entra\u0302\u0131nement se re\u0301alise par re\u0301tropropagation : pour la couche de sortie, a\u0300 partir d\u2019une sortie attendue {y1 . . . ym} nous calculons (pour chaque neurone)\nune estimation de l\u2019erreur\n\u03b4i,j = f \u2032(vi,j) (yj \u2212 vi,j)\nCelle-ci est propage\u0301e aux couches pre\u0301ce\u0301dentes par :\n\u03b4i,j = f \u2032(vi,j)\n\u2211\nk\nwi,j,k \u00b7 \u03b4i+1,j\nou\u0300 la somme est calcule\u0301e sur tous les neurones connecte\u0301s avec le neurone (i, j). Les nouveaux poids, au temps t+ 1, sont\nwt+1;i,j,k = wt;i,j,k +\u2206wt;i,j,k\nou\u0300 \u2206wt de\u0301pend d\u2019un facteur de correction et aussi de la valeur de \u2206wt\u22121 multiplie\u0301e por un momentum \u00b5 (cela donne aux modifications une sorte d\u2019e\u0301nergie cine\u0301tique) :\n\u2206wt;i,j,k = (\u03bb \u00b7 \u03b4i+1,k \u00b7 vi,j) + \u00b5 \u00b7\u2206wt\u22121;i,j,k\nLe facteur de correction de\u0301pend des valeurs \u03b4 calcule\u0301es et aussi d\u2019un facteur d\u2019apprentissage \u03bb qui peut e\u0302tre ajuste\u0301 pour acce\u0301le\u0301rer la convergence du re\u0301seau.\nLe type d\u2019entra\u0302\u0131nement re\u0301alise\u0301 s\u2019appelle apprentissage supervise\u0301 (base\u0301 sur des exemples). La re\u0301tropropagation part d\u2019un jeu de donne\u0301es avec des entre\u0301es et des sorties recherche\u0301es. Une ge\u0301ne\u0301ration consiste a\u0300 recalculer les poids synaptiques pour chaque paire d\u2019entre\u0301e / sortie. L\u2019entra\u0302\u0131nement complet requiert\n10350 ge\u0301ne\u0301rations, ce qui prend quelques 14 heures d\u2019exe\u0301cution de code python. Etant donne\u0301 que le design de la topologie du re\u0301seau est un processus assez artisanal (essai et erreur), qui requiert d\u2019entra\u0302\u0131ner le re\u0301seau pour chaque variation de la topologie afin de voir si elle produit de meilleurs re\u0301sultats, le temps d\u2019 exe\u0301cution nous a particulie\u0300rement motive\u0301s a\u0300 ame\u0301liorer la vitesse de convergence de l\u2019entra\u0302\u0131nement (proble\u0300me que nous traitons dans la section 4).\nPour chaque ge\u0301ne\u0301ration du processus d\u2019entra\u0302\u0131nement, les entre\u0301es sont re\u0301ordonne\u0301es au hasard (pour que l\u2019ordre des donne\u0301es n\u2019affecte pas l\u2019entra\u0302\u0131nement). En effet le re\u0301seau est susceptible d\u2019apprendre n\u2019importe quel aspect des entre\u0301es fournies, en particulier l\u2019ordre dans lequel on lui pre\u0301sente les choses.\nLa table 1 montre une comparaison (de notre laboratoire) entre le vieux module DCE-RPC (qui utilise un algorithme de \u201cbest fit\u201d) et le nouveau module qui utilise un re\u0301seau de neurones pour analyser l\u2019information.\nLa table 2 montre le re\u0301sultat de l\u2019exe\u0301cution du module contre un Windows 2000 e\u0301dition server sp1. Le syste\u0300me correct est reconnu avec un haut niveau de confiance."}, {"heading": "3 De\u0301tection d\u2019OS base\u0301e sur les signatures de Nmap", "text": ""}, {"heading": "3.1 Richesse et faiblesse de Nmap", "text": "Nmap est un outil d\u2019exploration re\u0301seau et un scanner de se\u0301curite\u0301 qui inclut une me\u0301thode de de\u0301tection d\u2019OS base\u0301e sur la re\u0301ponse de la machine cible a\u0300 une se\u0301rie de 9 tests. Nous de\u0301crivons brie\u0300vement dans le tableau 3 les paquets envoye\u0301s par chaque test, pour plus d\u2019informations voir la page de Nmap [1].\nNotre me\u0301thode utilise la base de signatures de Nmap. Une signature est un ensemble de re\u0300gles de\u0301crivant comment une version / e\u0301dition spe\u0301cifique d\u2019un syste\u0300me d\u2019exploitation re\u0301pond aux tests. Par exemple :\n# Linux 2.6.0-test5 x86 Fingerprint Linux 2.6.0-test5 x86 Class Linux | Linux | 2.6.X | general purpose TSeq(Class=RI%gcd=<6%SI=<2D3CFA0&>73C6B%IPID=Z%TS=1000HZ) T1(DF=Y%W=16A0%ACK=S++%Flags=AS%Ops=MNNTNW)\nT2(Resp=Y%DF=Y%W=0%ACK=S%Flags=AR%Ops=) T3(Resp=Y%DF=Y%W=16A0%ACK=S++%Flags=AS%Ops=MNNTNW) T4(DF=Y%W=0%ACK=O%Flags=R%Ops=) T5(DF=Y%W=0%ACK=S++%Flags=AR%Ops=) T6(DF=Y%W=0%ACK=O%Flags=R%Ops=) T7(DF=Y%W=0%ACK=S++%Flags=AR%Ops=) PU(DF=N%TOS=C0%IPLEN=164%RIPTL=148%RID=E%RIPCK=E%UCK=E%ULEN=134%DAT=E)\nLa base de Nmap contient 1684 signatures, ce qui veut dire que quelques 1684 versions / e\u0301ditions de syste\u0300mes d\u2019exploitation peuvent the\u0301oriquement e\u0302tre distingue\u0301es par cette me\u0301thode.\nNmap fonctionne en comparant la re\u0301ponse d\u2019une machine avec chaque signature de la base de donne\u0301es. Un score est assigne\u0301 a\u0300 chaque signature, calcule\u0301 simplement comme le nombre de re\u0300gles qui concordent divise\u0301 par le nombre de re\u0300gles conside\u0301re\u0301es (en effet, les signatures peuvent avoir diffe\u0301rents nombres de re\u0300gles, ou quelques champs peuvent manquer dans la re\u0301ponse, dans ce cas les re\u0300gles correspondantes ne sont pas prises en compte). C\u2019est-a\u0300-dire que Nmap effectue une espe\u0300ce de \u201cbest fit\u201d base\u0301 sur une distance de Hamming, ou\u0300 tous les champs de la re\u0301ponse ont le me\u0302me poids.\nUn des proble\u0300mes que pre\u0301sente cette me\u0301thode est le suivant : les syste\u0300mes d\u2019exploitation rares (improbables) qui ge\u0301ne\u0300rent moins de re\u0301ponses aux tests obtiennent un meilleur score ! (les re\u0300gles qui concordent acquie\u0300rent un plus grand poids relatif). Par exemple, il arrive que Nmap de\u0301tecte un Windows 2000 comme un Atari 2600 ou un HPUX ... La richesse de la base de donne\u0301es devient alors une faiblesse !"}, {"heading": "3.2 Structure de Re\u0301seaux Hie\u0301rarchique", "text": "Si nous repre\u0301sentons symboliquement l\u2019espace des syste\u0300mes d\u2019exploitation comme un espace a\u0300 568 dimensions (nous verrons par la suite le pourquoi de ce nombre), les re\u0301ponses possibles des diffe\u0301rentes versions des syste\u0300mes inclus dans la base de donne\u0301es forme un nuage de points. Ce grand nuage est structure\u0301 de\nfac\u0327on particulie\u0300re, puisque les familles de syste\u0300mes d\u2019exploitation forment des clusters plus ou moins reconnaissables. La me\u0301thode de Nmap consiste, a\u0300 partir de la re\u0301ponse d\u2019une machine, a\u0300 chercher le point le plus proche (selon la distance de Hamming de\u0301ja\u0300 mentionne\u0301e).\nNotre approche consiste en premier lieu a\u0300 filtrer les syste\u0300mes d\u2019exploitation qui ne nous inte\u0301ressent pas (toujours selon le point de vue de l\u2019attaquant, par exemple les syste\u0300mes pour lesquels il n\u2019a pas d\u2019exploits). Dans notre imple\u0301mentation, nous sommes inte\u0301resse\u0301s par les familles Windows, Linux, Solaris, OpenBSD, NetBSD et FreeBSD. Ensuite nous utilisons la structure des familles de syste\u0300mes d\u2019exploitation pour assigner la machine a\u0300 une des 6 familles conside\u0301re\u0301es.\nLe re\u0301sultat est un module qui utilise plusieurs re\u0301seaux de neurones organise\u0301s de fac\u0327on hie\u0301rarchique :\n1. premier pas, un re\u0301seau de neurones pour de\u0301cider si l\u2019OS est inte\u0301ressant ou non.\n2. deuxie\u0300me pas, un re\u0301seau de neurones pour de\u0301cider la famille de l\u2019OS : Windows, Linux, Solaris, OpenBSD, FreeBSD, NetBSD.\n3. dans le cas de Windows, nous utilisons le module DCE-RPC endpoint mapper pour raffiner la de\u0301tection.\n4. dans le cas de Linux, nous re\u0301alisons une analyse conditionne\u0301e (avec un autre re\u0301seau de neurones) pour de\u0301cider la version du kernel.\n5. dans le cas de Solaris et des BSD, nous re\u0301alisons une analyse conditionne\u0301e pour de\u0301cider la version.\nNous utilisons un re\u0301seau de neurones diffe\u0301rent pour chaque analyse. Nous avons ainsi 5 re\u0301seaux de neurones, et chacun requiert une topologie et un entra\u0302\u0131nement spe\u0301cial."}, {"heading": "3.3 Entre\u0301es du re\u0301seau de neurones", "text": "La premie\u0300re question a\u0300 re\u0301soudre est : comment traduire la re\u0301ponse d\u2019une machine en entre\u0301es pour le re\u0301seau de neurones ? Nous assignons un ensemble de neurones d\u2019entre\u0301e a\u0300 chaque test. Voici les de\u0301tails pour les tests T1 ... T7 : \u00b7 un neurone pour le flag ACK. \u00b7 un neurone pour chaque re\u0301ponse : S, S++, O. \u00b7 un neurone pour le flag DF. \u00b7 un neurone pour la re\u0301ponse : yes/no. \u00b7 un neurone pour le champ Flags. \u00b7 un neurone pour chaque flag : ECE, URG, ACK, PSH, RST, SYN, FIN (en total 8 neurones). \u00b7 10 groupes de 6 neurones pour le champOptions. Nous activons un seul neurone dans chaque groupe suivant l\u2019option - EOL, MAXSEG, NOP, TIMESTAMP, WINDOW, ECHOED - en respectant l\u2019ordre d\u2019apparition (soit au total 60 neurones pour les options).\n\u00b7 un neurone pour le champ W (window size), qui a pour entre\u0301e une valeur hexade\u0301cimale.\nPour les flags ou les options, l\u2019entre\u0301e est 1 ou -1 (pre\u0301sent ou absent). D\u2019autres neurones ont une entre\u0301e nume\u0301rique, comme le champ W (window size), le champ GCD (plus grand commun diviseur des nume\u0301ros de se\u0301quence initiaux) ou les champs SI et VAL des re\u0301ponses au test Tseq. Dans l\u2019exemple d\u2019un Linux 2.6.0, la re\u0301ponse :\nT3(Resp=Y%DF=Y%W=16A0%ACK=S++%Flags=AS%Ops=MNNTNW)\nse transforme en comme indique\u0301 dans le tableau 4. De cette fac\u0327on nous obtenons une couche d\u2019entre\u0301e avec 568 dimensions, avec une certaine redondance. La redondance nous permet de traiter de fac\u0327on flexible les re\u0301ponses inconnues mais introduit aussi des proble\u0300mes de performance ! Nous verrons dans la section suivante comment re\u0301soudre ce proble\u0300me (en re\u0301duisant le nombre de dimensions). Comme pour le module DCE-RPC, les re\u0301seaux de neurones sont compose\u0301s de 3 couches. Par exemple le premier re\u0301seaux de neurones (le filtre de pertinence) contient : la couche d\u2019entre\u0301e 96 neurones, la couche cache\u0301e 20 neurones, la couche de sortie 1 neurone."}, {"heading": "3.4 Ge\u0301ne\u0301ration du jeu de donne\u0301es", "text": "Pour entra\u0302\u0131ner le re\u0301seau de neurones nous avons besoin d\u2019entre\u0301es (re\u0301ponses de machines) avec les sorties correspondantes (OS de la machine). Comme la base de signatures contient 1684 re\u0300gles, nous estimons qu\u2019une population de 15000 machines est ne\u0301cessaire pour entra\u0302\u0131ner le re\u0301seau. Nous n\u2019avons pas acce\u0300s a\u0300 une telle population ... et scanner l\u2019Internet n\u2019est pas une option !\nLa solution que nous avons adopte\u0301e est de ge\u0301ne\u0301rer les entre\u0301es par une simulation Monte Carlo. Pour chaque re\u0300gle, nous ge\u0301ne\u0301rons des entre\u0301es correspondant a\u0300 cette re\u0300gle. Le nombre d\u2019entre\u0301es de\u0301pend de la distribution empirique des OS base\u0301e sur des donne\u0301es statistiques. Quand la re\u0300gle spe\u0301cifie une constante, nous utilisons cette valeur, et quand la re\u0300gle spe\u0301cifie des options ou un intervalle de valeurs, nous choisissons une valeur en suivant une distribution uniforme."}, {"heading": "4 Re\u0301duction des dimensions et entra\u0302\u0131nement", "text": ""}, {"heading": "4.1 Matrice de corre\u0301lation", "text": "Lors du design de la topologie des re\u0301seaux, nous avons e\u0301te\u0301 ge\u0301ne\u0301reux avec les entre\u0301es : 568 dimensions, avec une redondance importante. Une conse\u0301quence\nest que la convergence de l\u2019entra\u0302\u0131nement est lente, d\u2019autant plus que le jeu de donne\u0301es est tre\u0300s grand. La solution a\u0300 ce proble\u0300me fut de re\u0301duire le nombre de dimensions. Cette analyse nous permet aussi de mieux comprendre les e\u0301le\u0301ments importants des tests utilise\u0301s.\nLe premier pas est de conside\u0301rer chaque dimension d\u2019entre\u0301e comme une variable ale\u0301atoire Xi (1 \u2264 i \u2264 568). Les dimensions d\u2019entre\u0301e ont des ordres de grandeur diffe\u0301rents : les flags prennent comme valeur 1/-1 alors que le champ ISN (nume\u0301ro de se\u0301quence initial) est un entier de 32 bits. Nous e\u0301vitons au re\u0301seau de neurones d\u2019avoir a\u0300 apprendre a\u0300 additionner correctement ces variables he\u0301te\u0301roge\u0300nes en normalisant les variables ale\u0301atoires (en soustrayant la moyenne \u00b5 et en divisant par l\u2019e\u0301cart type \u03c3) :\nXi \u2212 \u00b5i \u03c3i\nPuis nous calculons la matrice de corre\u0301lation R, dont les e\u0301le\u0301ments sont :\nRi,j = E[(Xi \u2212 \u00b5i)(Xj \u2212 \u00b5j)]\n\u03c3i \u03c3j\nLe symbole E de\u0301signe l\u2019espe\u0301rance mathe\u0301matique. Puisqu\u2019apre\u0300s avoir normalise\u0301 les variables, \u00b5i = 0 et \u03c3i = 1 pour tout i, la matrice de corre\u0301lation est simplement Ri,j = E[XiXj ].\nLa corre\u0301lation est une mesure de la de\u0301pendance statistique entre deux variables (une valeur proche de 1 ou -1 indique une plus forte de\u0301pendance). La de\u0301pendance line\u0301aire entre des colonnes de R indique des variables de\u0301pendantes, dans ce cas nous en gardons une et e\u0301liminons les autres, puisqu\u2019elles n\u2019apportent pas d\u2019information additionnelle. Les constantes ont une variance nulle et sont aussi e\u0301limine\u0301es par cette analyse.\nVoyons le re\u0301sultat dans le cas des syste\u0300mes OpenBSD. Nous reproduisons cidessous les extraits des signatures de deux OpenBSD diffe\u0301rents, ou\u0300 les champs qui survivent a\u0300 la re\u0301duction de la matrice de corre\u0301lation sont marque\u0301s en italiques. Fingerprint OpenBSD 3.6 (i386) Class OpenBSD | OpenBSD | 3.X | general purpose T1(DF=N % W=4000 % ACK=S++ % Flags=AS % Ops=MNWNNT) T2(Resp=N) T3(Resp=N) T4(DF=N % W=0 % ACK=O %Flags=R % Ops=) T5(DF=N % W=0 % ACK=S++ % Flags=AR % Ops=) Fingerprint OpenBSD 2.2 - 2.3 Class OpenBSD | OpenBSD | 2.X | general purpose T1(DF=N % W=402E % ACK=S++ % Flags=AS % Ops=MNWNNT) T2(Resp=N) T3(Resp=Y % DF=N % W=402E % ACK=S++ % Flags=AS % Ops=MNWNNT) T4(DF=N % W=4000 % ACK=O % Flags=R % Ops=) T5(DF=N % W=0 % ACK=S++ % Flags=AR % Ops=)\nPar exemple, pour le test T1, les seuls champs qui varient sont W et les deux premie\u0300res options, les autres sont constants dans toutes les versions de\nOpenBSD. Autre exemple, pour le test T4 seul W est susceptible de varier, et le test T5 n\u2019apporte directement aucune information sur la version de OpenBSD examine\u0301e.\nLa table 5 montre la liste comple\u0300te des champs qui servent a\u0300 distinguer les diffe\u0301rentes versions d\u2019OpenBSD. Comme nous l\u2019avons dit, le test T5 n\u2019appara\u0302\u0131t pas, alors que les tests Tseq et PU conservent de nombreuses variables, ce qui\nnous montre que ces deux tests sont les plus discriminatifs au sein de la population OpenBSD."}, {"heading": "4.2 Analyse en Composantes Principales", "text": "Une re\u0301duction ulte\u0301rieure des donne\u0301es utilise l\u2019Analyse en Composantes Principales (ACP). L\u2019ide\u0301e est de calculer une nouvelle base (ou syste\u0300me de coordonne\u0301es) de l\u2019espace d\u2019entre\u0301e, de telle manie\u0300re que la majeure variance de toute projection du jeu de donne\u0301es dans un sous-espace de k dimensions, provient de projeter sur les k premiers vecteurs de cette base. L\u2019algorithme ACP consiste a\u0300 : \u00b7 calculer les vecteurs propres et valeurs propres de R. \u00b7 trier les vecteurs par valeur propre de\u0301croissante. \u00b7 garder les k premiers vecteurs pour projeter les donne\u0301es. \u00b7 le parame\u0300tre k est choisi pour maintenir au moins 98% de la variance totale.\nApre\u0300s avoir re\u0301alise\u0301 l\u2019ACP nous avons obtenu les topologies indique\u0301e dans le tableau 6 pour les re\u0301seaux de neurones (la taille de la couche d\u2019entre\u0301e originale e\u0301tait de 568 dans tous les cas).\nPour conclure l\u2019exemple de OpenBSD, a\u0300 partir des 34 variables qui ont surve\u0301cu a\u0300 la re\u0301duction de la matrice de corre\u0301lation, il est possible de construire une nouvelle base de 23 vecteurs. Les coordonne\u0301es dans cette base sont les entre\u0301es du re\u0301seau, la couche cache\u0301e ne contient que 4 neurones et la couche de sortie 3 neurones (car nous distinguons 3 groupes de versions d\u2019OpenBSD). Une fois que l\u2019on sait qu\u2019une machine est un OpenBSD, le proble\u0300me de reconna\u0302\u0131tre la version est beaucoup plus simple et borne\u0301, et peut e\u0302tre accompli par un re\u0301seau de neurones de petite taille (plus efficient et rapide)."}, {"heading": "4.3 Taux d\u2019apprentissage adaptatif", "text": "C\u2019est une strate\u0301gie pour acce\u0301le\u0301rer la convergence de l\u2019entra\u0302\u0131nement. Le taux d\u2019apprentissage est le parame\u0300tre \u03bb qui intervient dans les formules d\u2019apprentissage par re\u0301tropropagation.\nEtant donne\u0301e une sortie du re\u0301seau, nous pouvons calculer une estimation de l\u2019erreur quadratique\n\u2211n i=1(yi \u2212 vi) 2\nn\nou\u0300 yi sont les sorties recherche\u0301es et vi sont les sorties du re\u0301seau.\nApre\u0300s chaque ge\u0301ne\u0301ration (c\u2019est-a\u0300-dire apre\u0300s avoir fait les calculs pour toutes les paires d\u2019entre\u0301e / sortie), si l\u2019erreur est plus grande, nous diminuons le taux d\u2019apprentissage. Au contraire, si l\u2019erreur est plus petite, alors nous augmentons le taux d\u2019apprentissage. L\u2019ide\u0301e est de se de\u0301placer plus rapidement si nous allons dans la direction correcte.\nVoici deux graphiques (Figures 2 et 3) qui montrent l\u2019e\u0301volution de l\u2019erreur quadratique moyenne en fonction du nombre de ge\u0301ne\u0301rations pour chaque strate\u0301gie. Lorsque le taux d\u2019apprentissage est fixe, l\u2019erreur diminue et atteint des valeurs satisfaisantes apre\u0300s 4000 ou 5000 ge\u0301ne\u0301rations. L\u2019erreur a une claire tendance a\u0300 la baisse (les re\u0301sultats sont bons) mais avec des pics irre\u0301guliers, dus a\u0300 la nature probabiliste de l\u2019entra\u0302\u0131nement du re\u0301seau.\nEn utilisant un taux d\u2019apprentissage adaptatif, nous obtenons au de\u0301but un comportement plus chaotique, avec des niveaux d\u2019erreur plus hauts. Mais une fois que le syste\u0300me trouve la direction correcte, l\u2019erreur chute rapidement pour\natteindre une valeur tre\u0300s faible et constante apre\u0300s 400 ge\u0301ne\u0301rations. Ces re\u0301sultats sont clairement meilleurs et permettent d\u2019acce\u0301le\u0301rer l\u2019entra\u0302\u0131nement des re\u0301seaux."}, {"heading": "4.4 Entra\u0302\u0131nement par sous-ensembles du jeu de donne\u0301es", "text": "C\u2019est une autre strate\u0301gie pour acce\u0301le\u0301rer la convergence de l\u2019entra\u0302\u0131nement. Elle consiste a\u0300 entra\u0302\u0131ner le re\u0301seau avec plusieurs sous-ensembles plus petits du jeu de donne\u0301es. Ceci permet aussi de re\u0301soudre des proble\u0300mes de limitation d\u2019espace, par exemple lorsque le jeu de donne\u0301es est trop grand pour e\u0302tre charge\u0301 entier en me\u0301moire.\nPour estimer l\u2019erreur commise apre\u0300s avoir utilise\u0301 un sous-ensemble, nous calculons une mesure d\u2019ade\u0301quation G. Si la sortie est 0/1 :\nG = 1\u2212 (Pr[ faux positif ] + Pr[ faux ne\u0301gatif ])\nPour d\u2019autres type de sorties, G est simplement :\nG = 1\u2212 nombre d\u2019erreurs/nombre de sorties.\nA nouveau, nous utilisons une strate\u0301gie de taux d\u2019apprentissage adaptatif. Si la mesure d\u2019ade\u0301quationG augmente, alors nous augmentons le taux d\u2019apprentissage initial (pour chaque sous-ensemble).\nNous reproduisons ci-dessous le re\u0301sultat de l\u2019exe\u0301cution du module contre une machine Solaris 8. Le syste\u0300me correct est reconnu avec pre\u0301cision.\nRelevant / not relevant analysis\n0.99999999999999789 relevant\nOperating System analysis\n-0.99999999999999434 Linux 0.99999999921394744 Solaris -0.99999999999998057 OpenBSD -0.99999964651426454 FreeBSD -1.0000000000000000 NetBSD -1.0000000000000000 Windows\nSolaris version analysis\n0.98172780325074482 Solaris 8 -0.99281382458335776 Solaris 9 -0.99357586906143880 Solaris 7 -0.99988378968003799 Solaris 2.X -0.99999999977837983 Solaris 2.5.X"}, {"heading": "5 Conclusion et ide\u0301es pour le futur", "text": "Dans ce travail, nous avons vu que l\u2019une des principales limitations des techniques classiques de de\u0301tection des syste\u0300mes d\u2019exploitation re\u0301side dans l\u2019analyse des donne\u0301es recueillies par les tests, base\u0301e sur quelque variation de l\u2019algorithme de \u201cbest fit\u201d (chercher le point le plus proche en fonction d\u2019une distance de Hamming).\nNous avons vu comment ge\u0301ne\u0301rer et re\u0301unir l\u2019information a\u0300 analyser, comment homoge\u0301ne\u0301iser les donne\u0301es (normaliser les variables d\u2019entre\u0301e) et surtout comment de\u0301gager la structure des donne\u0301es d\u2019entre\u0301e. C\u2019est l\u2019ide\u0301e principale de notre approche, qui motive la de\u0301cision d\u2019utiliser des re\u0301seaux de neurones, de diviser l\u2019analyse en plusieurs e\u0301tapes hie\u0301rarchiques, et de re\u0301duire le nombre de dimensions d\u2019entre\u0301e. Les re\u0301sultats expe\u0301rimentaux (de notre laboratoire) montrent que cette approche permet d\u2019obtenir une reconnaissance plus fiable des syste\u0300mes d\u2019exploitation.\nDe plus, la re\u0301duction de la matrice de corre\u0301lation et l\u2019analyse en composantes principales, introduits en principe pour re\u0301duire le nombre de dimensions et ame\u0301liorer la convergence de l\u2019entra\u0302\u0131nement, nous donnent une me\u0301thode syste\u0301matique pour analyser les re\u0301ponses des machines aux stimuli envoye\u0301s. Cela nous a permis de de\u0301gager les e\u0301le\u0301ments clefs des tests de Nmap, voir par exemple la table des champs permettant de distinguer les diffe\u0301rentes versions d\u2019OpenBSD. Une application de cette analyse serait d\u2019optimiser les tests de Nmap pour ge\u0301ne\u0301rer moins de trafic. Une autre application plus ambitieuse serait de cre\u0301er une base de donne\u0301es avec les re\u0301ponses d\u2019une population repre\u0301sentative de machines a\u0300 une vaste batterie de tests (combinaisons de diffe\u0301rents types de paquets, ports\net flags). Les me\u0302mes me\u0301thodes d\u2019analyse permettraient de de\u0301gager de cette vaste base de donne\u0301es les tests les plus discriminatifs pour la reconnaissance d\u2019OS.\nL\u2019analyse que nous proposons peut aussi s\u2019appliquer a\u0300 d\u2019autres me\u0301thodes de de\u0301tection :\n1. Xprobe2, d\u2019Ofir Arkin, Fyodor & Meder Kydyraliev, qui base la de\u0301tection sur des tests ICMP, SMB, SNMP.\n2. Passive OS Identification (p0f) de Michal Zalewski, me\u0301thode qui a l\u2019avantage de ne pas ge\u0301ne\u0301rer de trafic additionnel. C\u2019est un de\u0301fi inte\u0301ressant, car l\u2019analyse porte sur un volume de donne\u0301es plus important (tout le trafic sniffe\u0301), et requiert sans doute des me\u0301thodes plus dynamiques et e\u0301volutives.\n3. De\u0301tection d\u2019OS base\u0301e sur l\u2019information fournie par le portmapper SUN RPC, permettant de distinguer des syste\u0300mes Sun, Linux et autres versions de System V.\n4. Re\u0301union d\u2019information pour le versant client-side des tests de pe\u0301ne\u0301tration, en particulier pour de\u0301tecter les versions d\u2019applications. Par exemple de\u0301tecter les Mail User Agents (MUA) tels que Outlook ou Thunderbird, en utilisant les Mail Headers.\nUne autre ide\u0301e pour le futur est d\u2019ajouter du bruit et le filtre d\u2019un firewall aux donne\u0301es e\u0301tudie\u0301es. Ceci permettrait de de\u0301tecter la pre\u0301sence d\u2019un firewall, d\u2019identifier diffe\u0301rents firewalls et de faire des tests plus robustes.\nRe\u0301fe\u0301rences\n[1] Fyodor, Remote OS detection via TCP/IP Stack FingerPrinting http://www.insecure.org/nmap/nmap-fingerprinting-article.html\n[2] Principal Component Analysis http://en.wikipedia.org/wiki/Principal component analysis\n[3] Projets de Corelabs http://www.coresecurity.com/corelabs/\n[4] Christopher M. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, 1995.\n[5] Timothy Masters, Practical Neural Network Recipes in C++, Academic Press, 1994.\n[6] Simon Haykin, Neural Networks : A Comprehensive Foundation, Prentice Hall, 2nd edition (1998).\n[7] Robert Hecht-Nielsen, NeuroComputing, Addison-Wesley, 1990.\n[8] Alberto. H. Landro, Acerca de la probabilidad, Ed. Coop - 2da Edicio\u0301n (2002)\n[9] W. Richard Stevens, TCP/IP Illustrated, Addison-Wesley Professional, 1993."}], "references": [{"title": "Neural Networks for Pattern Recognition", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Practical Neural Network Recipes in C++", "author": ["Timothy Masters"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Neural Networks : A Comprehensive Foundation, Prentice Hall, 2nd edition", "author": ["Simon Haykin"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "TCP/IP Illustrated", "author": ["W. Richard Stevens"], "venue": "Addison-Wesley Professional", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "[4] Christopher M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[5] Timothy Masters, Practical Neural Network Recipes in C++, Academic Press, 1994.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[6] Simon Haykin, Neural Networks : A Comprehensive Foundation, Prentice Hall, 2nd edition (1998).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[9] W.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "R\u00e9sum\u00e9 Nous pr\u00e9sentons la d\u00e9tection distante de syst\u00e8mes d\u2019exploitation comme un probl\u00e8me d\u2019inf\u00e9rence : \u00e0 partir d\u2019une s\u00e9rie d\u2019observations (les r\u00e9ponses de la machine cible \u00e0 un ensemble de tests), nous voulons inf\u00e9rer le type de syst\u00e8me d\u2019exploitation qui g\u00e9n\u00e8rerait ces observations avec une plus grande probabilit\u00e9. Les techniques classiques utilis\u00e9es pour r\u00e9aliser cette analyse pr\u00e9sentent plusieurs limitations. Pour outrepasser ces limites, nous proposons l\u2019utilisation de R\u00e9seaux de Neurones et d\u2019outils statistiques. Nous pr\u00e9senterons deux modules fonctionnels : un module qui utilise les points finaux DCE-RPC pour distinguer les versions de Windows, et un module qui utilise les signatures de Nmap pour distinguer les versions de syst\u00e8mes Windows, Linux, Solaris, OpenBSD, FreeBSD et NetBSD. Nous expliquerons les d\u00e9tails de la topologie et du fonctionnement des r\u00e9seaux de neurones utilis\u00e9s, et du r\u00e9glage fin de leurs param\u00e8tres. Finalement nous montrerons des r\u00e9sultats exp\u00e9rimentaux positifs.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}