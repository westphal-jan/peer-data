{"id": "1006.3033", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2010", "title": "Extension of Wirtinger's Calculus to Reproducing Kernel Hilbert Spaces and the Complex Kernel LMS", "abstract": "Over mysteries the saint-christophe last negreanu decade, shangrila kernel seraphim methods pomorski for 7:5 nonlinear chipaya processing have successfully been used donadio in beringen the workplaces machine sub-systems learning community. handedly The chavalit primary yunessi mathematical inscrutably tool serravalle employed audible.com in these methods is the cawthra notion of the yasushi Reproducing seicento Kernel Hilbert cassam Space. However, so ponnelle far, palfreyman the philip emphasis has been on plus-8 batch rusdihardjo techniques. It is semiaquatic only rehavia recently, wdre that 96-minute online techniques usns have mpac been target2 considered rijckaert in the context of rechte adaptive baltics signal malinois processing tasks. bicultural Moreover, these wracking efforts curepipe have wham-o only been focussed on mantova and bec real valued ennistymon data zoledronic sequences. pearce To the uninflected best of self-governing our knowledge, no flore\u015fti kernel - martes based yizheng strategy has pohl been siff developed, mikelic so torridge far, that teide is tyree able armonica to reaper deal filefish with abayudaya complex 6,000-point valued signals. In this paper, stefanou we krah present mwana a turistico general sobraon framework ethem to attack the problem ghailani of 600-pound adaptive basel filtering detroit-area of goradze complex obliterans signals, using hulud either 9th-10th real reproducing kernels, taking ciar advantage of cymothoe a speckled technique called \\ plebiscites textit {lungi complexification} multiple-unit of chandravanshi real minnillo RKHSs, or dimensional complex tyisha reproducing kernels, highlighting the willer use bbp of the complex hongkong gaussian kernel. riffing In madla order to tabatabai derive formula gradients fifty-fifth of operators primarily that cranstoun need mesabi to be defined on theodo the resuscitate associated complex RKHSs, we krips employ the bloviating powerful kayishema tool pancorbo of d'industrie Wirtinger ' s Calculus, which has parz\u0119czew recently attracted jdv much attention in the astyages signal yaqut processing community. Writinger ' anastomoses s significance calculus regularization simplifies elsag computations 21-billion and ctl offers ganyu an undersize elegant hindemith tool trible for treating complex signals. inca To this end, reducible in canonical this workaround paper, 64.42 the foshee notion ollamh of Writinger ' accommodations s calculus indiantown is extended, awry for the first racemes time, to include complex o-2 RKHSs and use co-chairmen it to derive several realizations of hydrocodone the doong Complex Kernel bandwagon Least - Mean - m\u0161k Square (grammies CKLMS) algorithm. buhruz Experiments verify corimon that benishek the axman CKLMS one-car offers perivascular significant http://www.christies.com performance improvements m\u00fcstair over the traditional 24.80 complex shagreen LMS sitz or Widely tear-gas Linear complex LMS (WL - LMS) baro algorithms, when commodores dealing jodeci with mole nonlinearities.", "histories": [["v1", "Tue, 15 Jun 2010 17:09:01 GMT  (498kb)", "http://arxiv.org/abs/1006.3033v1", "30 pages (double spaced), 5 figures, submitted to IEEE Trans. Sig. Proc"], ["v2", "Thu, 30 Sep 2010 15:21:53 GMT  (553kb)", "http://arxiv.org/abs/1006.3033v2", "15 pages (double column), preprint submitted to IEEE Trans. Sig. Proc"], ["v3", "Sat, 27 Nov 2010 09:06:13 GMT  (553kb)", "http://arxiv.org/abs/1006.3033v3", "15 pages (double column), preprint of article accepted in IEEE Trans. Sig. Proc"]], "COMMENTS": "30 pages (double spaced), 5 figures, submitted to IEEE Trans. Sig. Proc", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pantelis bouboulis", "sergios theodoridis"], "accepted": false, "id": "1006.3033"}, "pdf": {"name": "1006.3033.pdf", "metadata": {"source": "CRF", "title": "Extension of Wirtinger\u2019s Calculus in Reproducing Kernel Hilbert Spaces and the Complex Kernel LMS", "authors": ["Pantelis Bouboulis"], "emails": ["bouboulis@di.uoa.gr.", "stheodor@di.uoa.gr."], "sections": [{"heading": null, "text": "ar X\niv :1\n00 6.\n30 33\nv1 [\ncs .L\nG ]\n1 5\nJu n\nOver the last decade, kernel methods for nonlinear processing have successfully been used in the machine learning community. The primary mathematical tool employed in these methods is the notion of the Reproducing Kernel Hilbert Space. However, so far, the emphasis has been on batch techniques. It is only recently, that online techniques have been considered in the context of adaptive signal processing tasks. Moreover, these efforts have only been focussed on and real valued data sequences. To the best of our knowledge, no kernel-based strategy has been developed, so far, that is able to deal with complex valued signals. Furthermore, although the real reproducing kernels are used in an increasing number of machine learning problems, complex kernels have not, yet, been used, in spite of their potential interest in applications that deal with complex signals, with Communications being a typical example. In this paper, we present a general framework to attack the problem of adaptive filtering of complex signals, using either real reproducing kernels, taking advantage of a technique called complexification of real RKHSs, or complex reproducing kernels, highlighting the use of the complex gaussian kernel.\nIn order to derive gradients of operators that need to be defined on the associated complex RKHSs, we employ the powerful tool of Wirtinger\u2019s Calculus, which has recently attracted much attention in the signal processing community. Writinger\u2019s calculus simplifies computations and offers an elegant tool for treating complex signals. To this end, in this paper, the notion of Writinger\u2019s calculus is extended, for the first time, to include complex RKHSs and use it to derive several realizations of the Complex Kernel Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS offers significant performance improvements over the traditional complex LMS or Widely Linear complex LMS (WLLMS) algorithms, when dealing with nonlinearities.\nP. Bouboulis and S. Theodoridis are with the Department of Informatics and Telecommunications, University of Athens,\nGreece, e-mail: {bouboulis,stheodor}@di.uoa.gr.\nExtension of Wirtinger\u2019s Calculus in\nReproducing Kernel Hilbert Spaces and the\nComplex Kernel LMS\nI. INTRODUCTION\nProcessing in Reproducing Kernel Hilbert Spaces (RKHSs), in the context of online adaptive learning, is gaining in popularity within the Machine Learning and Signal Processing communities [1]\u2013[6]. The main advantage of mobilizing the tool of RKHSs is that the original nonlinear task is \u201ctransformed\u201d into a linear one, which can be solved by employing an easier \u201calgebra\u201d. Moreover, different types of nonlinearities can be treated in a unifying way, with no effect on the mathematical derivation of the algorithms, except at the final implementation stage. The main concepts of this procedure can be summarized in the following two steps: 1) Map the finite dimensionality input data from the input space F (usually F \u2282 R\u03bd) into a higher dimensionality (possibly infinite) RKHS H and 2) Perform a linear processing (e.g., adaptive filtering) on the mapped data in H. The procedure is equivalent with a non-linear processing (non-linear filtering) in F .\nAn alternative way of describing this process is through the popular kernel trick [7], [8]: Given an algorithm, which is formulated in terms of dot products, one can construct an alternative algorithm by replacing each one of the dot products with a positive definite kernel \u03ba. The specific choice of kernel implicitly defines a RKHS with an appropriate inner product. Furthermore, the choice of kernel also defines the type of nonlinearity that underlies the model to be used. The main representatives of this class of algorithms are the celebrated support vector machines (SVMs), which have dominated the research in machine learning over the last decade [9]. Besides SVMs and the more recent applications in adaptive filtering, there is a plethora of other scientific domains that have gained from adopting kernel methods (e.g., image processing and denoising [10], [11], principal component analysis [12], clustering [13], e.t.c.).\nIn classification tasks (which have been the dominant applications of kernel methods) the use of complex reproducing kernels is meaningless, since no arrangement can be derived in complex domains and the necessary separating hypersurfaces cannot be defined. Consequently, all known kernel based applications, as they emerged from the specific background, use real-valued kernels and they are able to\ndeal with real valued data sequences only. To our knowledge, no kernel-based strategy has been developed, so far, that is able to effectively deal with complex valued signals.\nIn this paper, we present a general framework to attack the problem of adaptive filtering of complex signals, using either real reproducing kernels, taking advantage of a technique called complexification of real RKHSs, or complex reproducing kernels, highlighting mostly the use of the complex gaussian kernel. Although the real gaussian RBF kernel has become quite popular and it has been used in many applications, the complex gaussian RBF kernel, while known to the mathematicians (especially those working on Reproducing Kernel Hilbert Spaces or Functional Analysis), it has rather remained in obscurity in the Machine Learning and Signal Processing communities. Even though, the presented framework has a broad range and may be applied to generalize a wide variety of kernel methods to the complex domain, this work focuses on the recently developed Kernel LMS (KLMS) [1], [14].\nTo compute the gradients of cost functions defined on the complex RKHSs, the principles of Wirtinger\u2019s calculus are employed. Wirtinger\u2019s calculus [15] has become very popular in the signal processing community, mainly in the context of complex adaptive filtering [16]\u2013[23], as a means of computing, in an elegant way, gradients of real valued cost functions defined on complex domains (C\u03bd). To this end, the main ideas and theorems of Wirtinger\u2019s calculus are generalized to general complex Hilbert spaces for the first time.\nTo summarize, the main contributions of this paper are: a) the development of a wide framework that allows real-valued kernel algorithms to be extended to treat complex data effectively, taking advantage of a technique called complexification of real RKHSs, b) to elevate from obscurity the complex Gaussian kernel as an effective tool for kernel based adaptive processing of complex signals, c) the extension of Wirtinger\u2019s Calculus in complex RKHSs as a means for an elegant and efficient computation of the gradients, which are involved in many adaptive learning algorithms, and d) the development of several realizations of the Complex Kernel LMS (CKLMS) algorithm, by exploiting the extension of Wirtinger\u2019s calculus and the generated complex RKHSs.\nThe paper is organized as follows. We start with an introduction to RKHSs in Section II, which includes real and complex kernels, before we briefly review the KLMS algorithm in Section III. In Section IV, we describe the complexification procedure of a real RKHS, that provides a framework to develop complex kernel methods, based on popular real valued reproducing kernels (e.g., gaussian, polynomial, e.t.c.). A brief introduction on Wirtinger\u2019s Calculus can be found in Section V. The main notions of the extended Wirtinger\u2019s Calculus on general Hilbert spaces are summarized in Section VI and the CKLMS is developed thereafter in Section VII. Finally, experimental results and conclusions are provided in Sections VIII and\nIX. Throughout the paper, we will denote the set of all integers, real and complex numbers by N, R and C respectively. Vector or matrix valued quantities appear in boldfaced symbols."}, {"heading": "II. REPRODUCING KERNEL HILBERT SPACES", "text": "In this section, we briefly describe the theory of Reproducing Kernel Hilbert Spaces. Since we are interested on both real and complex kernels, we recall the basic facts on RKHS associated with a general field F, which it can be either R or C. However, we highlight the basic differences between the two cases. The material presented here may be found with more details in [24] and [25]."}, {"heading": "A. Basic Definitions", "text": "Given a function \u03ba : X \u00d7 X \u2192 F and x1, . . . , xN \u2208 X, the matrix1 K = (Ki,j)N with elements Ki,j = \u03ba(xi, xj), for i, j = 1, . . . , N , is called the Gram matrix (or kernel matrix) of \u03ba with respect to x1, . . . , xN . A Hermitian matrix K = (Ki,j)N satisfying\ncH \u00b7K \u00b7 c = N,N \u2211\ni=1,j=1\nc\u2217i cjKi,j \u2265 0,\nfor all ci \u2208 F, i = 1, . . . , N , where the notation \u2217 denotes the conjugate element, is called Positive Definite. In matrix analysis literature, this is the definition of a positive semidefinite matrix. However, since this is a rather cumbersome term and the distinction between positive definite and positive semidefinite matrices is not important in this paper, we employ the term positive definite in the way presented here. Furthermore, the term positive definite was introduced for the first time by Mercer in the kernel context (see [26]). Let X be a nonempty set. Then a function \u03ba : X \u00d7X \u2192 F, which for all N \u2208 N and all x1, . . . , xN \u2208 X gives rise to a positive definite Gram matrix K, is called a Positive Definite Kernel. In the following, we will frequently refer to a positive definite kernel simply as kernel.\nNext, consider a linear class H of complex valued functions f defined on a set X. Suppose further, that in H we can define an inner product \u3008\u00b7, \u00b7\u3009H with corresponding norm \u2016 \u00b7 \u2016H and that H is complete with respect to that norm, i.e., H is a Hilbert space. We call H a Reproducing Kernel Hilbert Space (RKHS), if for all y \u2208 X the evaluation functional Ty : H \u2192 F : Ty(f) = f(y) is a linear continuous (or, equivalently, bounded) operator. If this is true, then by the Riesz\u2019s representation theorem, for all y \u2208 X there is a function gy \u2208 H such that Ty(f) = f(y) = \u3008f, gy\u3009H. The function \u03ba : X \u00d7X \u2192 F :\n1The term (Ki,j)N denotes a square N \u00d7N matrix.\n\u03ba(x, y) = gy(x) is called a reproducing kernel of H. It can be easily proved that the function \u03ba is a positive definite kernel.\nAlternatively, we can define a RKHS as a Hilbert space H for which there exists a function \u03ba : X \u00d7X \u2192 F with the following two important properties:\n1) For every x \u2208 X, \u03ba(\u00b7, x) belongs to H. 2) \u03ba has the so called reproducing property, i.e.,\nf(x) = \u3008f, \u03ba(\u00b7, x)\u3009H, for all f \u2208 H, (1)\nin particular \u03ba(x, y) = \u3008\u03ba(\u00b7, y), \u03ba(\u00b7, x)\u3009H .\nIt has been proved (see [27]) that to every positive definite kernel \u03ba there corresponds one and only one class of functions H with a uniquely determined inner product in it, forming a Hilbert space and admitting \u03ba as a reproducing kernel. In fact, the kernel \u03ba produces the entire space H, i.e., H = span{\u03ba(x, \u00b7)|x \u2208 X}. The map \u03a6 : X \u2192 H : \u03a6(x) = \u03ba(\u00b7, x) is called the feature map of H. Recall, that in the case of complex Hilbert spaces (i.e., F = C) the inner product is sesqui-linear (i.e. linear in one argument and antilinear in the other) and Hermitian:\n\u3008ax+ by, z\u3009H = a\u3008x, z\u3009H + b\u3008y, z\u3009H,\n\u3008x, ay + bz\u3009H = a\u2217\u3008x, y\u3009H + b\u2217\u3008x, z\u3009H,\n\u3008x, y\u3009\u2217H = \u3008y, x\u3009H,\nfor all x, y, z \u2208 H, and a, b \u2208 C. In the real case, the condition \u03ba(x, y) = \u3008\u03ba(\u00b7, y), \u03ba(\u00b7, x)\u3009H may be replaced by the well known equation \u03ba(x, y) = \u3008\u03ba(\u00b7, x), \u03ba(\u00b7, y)\u3009H . However, since in the complex case the inner product is Hermitian, the aforementioned condition is equivalent to \u03ba(x, y) = (\u3008\u03ba(\u00b7, x), \u03ba(\u00b7, y)\u3009H)\u2217. One of the most important properties of RKHSs is that norm convergence implies pointwise convergence. More precisely, let {fn}n\u2208N \u2282 H be a sequence such that limn \u2016fn \u2212 f\u2016 = 0, for some f \u2208 H. Then, the continuity of Tx gives:\nlim n fn(x) = lim n Tx(fn) = Tx(f) = f(x),\nfor all x \u2208 X. Although, the underlying theory has been developed by the mathematicians for general complex reproducing kernels with associated RKHSs, only the real kernels have been considered by the machine learning community. One of the most widely used is the Gaussian RBF kernel, i.e.,\n\u03ba\u03c3,Rd(x,y) := exp\n(\n\u2212 \u2211d i=1(xi \u2212 yi)2 \u03c32\n)\n, (2)\ndefined for x,y \u2208 Rd, where \u03c3 is a free positive parameter. Another popular kernel is the polynomial kernel:\n\u03bad(x,y) := ( 1 + xTy )d , (3)\nfor d \u2208 N. Many more can be found in the relative literature [7]\u2013[9]. Complex reproducing kernels, that have been extensively studied by the mathematicians, are, among others, the Szego kernels, i.e, \u03ba(z, w) = 11\u2212w\u2217z , for Hardy spaces on the unit disk, and the Bergman kernels, i.e., \u03ba(z, w) = 1(1\u2212w\u2217z)2 , for Bergman spaces on the unit disk, where |z|, |w| < 1 [25]. In the following, we discuss another complex kernel that has remained relatively unknown in the Machine Learning and Signal Processing societies."}, {"heading": "B. The Complex Gaussian Kernel", "text": "Consider the complex valued function\n\u03ba\u03c3,Cd(z,w) := exp\n(\n\u2212 \u2211d i=1(zi \u2212 w\u2217i )2 \u03c32\n)\n, (4)\ndefined on Cd \u00d7 Cd, where z,w \u2208 Cd, zi denotes the i-th component of the complex vector z \u2208 Cd and exp is the extended exponential function in the complex domain. It can be shown that \u03ba\u03c3,Cd is a complex valued kernel, which we call the complex Gaussian kernel with parameter \u03c3. Its restriction \u03ba\u03c3 := ( \u03ba\u03c3,Cd )\n|Rd\u00d7Rd is the well known real Gaussian kernel. An explicit description of the RKHSs of\nthese kernels, together with some important properties can be found in [28]."}, {"heading": "III. KERNEL LEAST MEAN SQUARE ALGORITHM", "text": "In a typical LMS filter the goal is to learn a linear input output mapping f : X \u2192 R : f(x) = wTx, X \u2282 R\u03bd , based on a sequence of examples (x(1), d(1)), (x(2), d(2)), . . . , (x(N), d(N)), so that to minimize the mean square error, E [ |d(n)\u2212wTx(n)|2 ] . To this end, the gradient descent rationale is employed and at each time instant, n = 1, 2, . . . , N , the gradient of the mean square error, i.e., \u22122E[e(n)x(n)], is estimated via its current measurement, i.e., E\u0302[e(n)x(n)] = e(n)x(n), where e(n) = d(n)\u2212w(n\u2212 1)Tx(n) is the error at instance n = 2, . . . , N . It takes a few lines of elementary algebra to deduce that the update of the unknown vector parameter is: w(n) = w(n \u2212 1) + \u00b5e(n)x(n), where \u00b5 is the parameter controlling the step update. If we take the initial value of w as w(0) = 0, then the repeated application of the update equation yields:\nw(n) = \u00b5\nn \u2211\nk=1\ne(k)x(k) (5)\nHence, for the filter output at instance n we have:\nd\u0302(n) = w(n\u2212 1)Tx(n) = \u00b5 n\u22121 \u2211\nk=1\ne(k)x(k)Tx(n), (6)\nfor n = 1, 2, . . . , N . Equation (6) is expressed in terms of inner products only, hence it allows for the application of the kernel trick. Thus, the filter output of the KLMS at instance n is\nd\u0302(n) = \u3008x(n),w(n\u2212 1)\u3009H = \u00b5 n\u22121 \u2211\nk=1\ne(k)\u03ba (x(n),x(k)) , (7)\nwhile w(n) = \u00b5 n \u2211\nk=1\ne(k)\u03ba(\u00b7,x(k)), (8)\nfor n = 1, 2, . . . , N .\nAnother, more formal way of developing the KLMS is the following. First, we transform the input space X to a high dimensional feature space H through the (implicit) mapping \u03a6 : X \u2192 H, \u03a6(x) = \u03ba(\u00b7,x). Thus, the training examples become\n(\u03a6(x(1)), d(1)), . . . , (\u03a6(x(N)), d(N)).\nWe apply the LMS procedure on the transformed data, with the linear filter output d\u0302(n) = \u3008\u03a6(x(n)),w\u3009H. The model \u3008\u03a6(x),w\u3009H is more representative than the simple wTx, since it includes the nonlinear modeling through the presence of the kernel. The objective now becomes to minimize the cost function\nE [ |d(n)\u2212 \u3008\u03a6(x(n)),w\u3009H|2 ]\n(see [29]). Using the notion of the Fre\u0301chet derivative [29]\u2013[31], which it has to be mobilized, since the dimensionality of the RKHS may be infinite, we are able to derive the gradient of the aforementioned cost function with respect to w, if we estimate it by its current measurement |d(n) \u2212 \u3008\u03a6(x(n)),w\u3009|2. Thus the respective gradient is \u22122e(n)\u03a6(x(n)). It has to be emphasized, that now w is not a vector, but a function, i.e., a point in the linear Hilbert space. It turns out that the update of the KLMS is given by w(n) = w(n \u2212 1) + \u00b5e(n)\u03a6(x(n)), where e(n) = d(n) \u2212 d\u0302(n). From this update, following the same procedure as in LMS and applying the reproducing property, we obtain equations (7) and (8), which are at the core of the KLMS algorithm. More details and the algorithmic implementation may be found in [14].\nNote that, in a number of attempts to kernelize known algorithms, that are cast in inner products, the kernel trick is, usually, used in a \u201dblack box\u201d rationale, without consideration of the problem in the RKH space, in which the (implicit) processing is carried out. Such an approach, often, does not allow for a\ndeeper understanding of the problem, especially if a further theoretical analysis is required. Moreover, in our case, such a \u201cblind\u201d application of the kernel trick on a standard complex LMS form, can only lead to spaces defined by complex kernels, as it will become clear soon. Complex RKH spaces, that are built around complexification of real kernels, do not result as a direct application of the standard kernel trick."}, {"heading": "IV. COMPLEXIFICATION OF REAL REPRODUCING KERNEL HILBERT SPACES", "text": "To generalize the kernel adaptive filtering algorithms on complex domains, we need a universal framework regarding complex RKHSs. A first straightforward approach is to use directly a complex RKHS, using one of the complex kernels given in section II. In this section, we present an alternative simple technique called complexification of real RKHSs, which has the advantage of allowing modeling in complex RKHSs using popular well-established and well understood, from a performance point of view, real kernels (e.g., gaussian, polynomial, e.t.c.).\nLet X \u2286 R\u03bd . Define X2 \u2261 X\u00d7X \u2286 R2\u03bd and X = {x+iy,x,y \u2208 X} \u2286 C\u03bd equipped with a complex product structure. Let H be a real RKHS associated with a real kernel \u03ba defined on X2 \u00d7X2 and let \u3008\u00b7, \u00b7\u3009H be its corresponding inner product. Then, every f \u2208 H can be regarded as a function defined on either X2 or X, i.e., f(z) = f(x+ iy) = f(x,y).\nNext, we define H2 = H\u00d7H. It is easy to verify that H2 is also a Hilbert Space with inner product\n\u3008f ,g\u3009H2 = \u3008f1, g1\u3009H + \u3008f2, g2\u3009H, (9)\nfor f = (f1, f2), g = (g1, g2). Our objective is to enrich H2 with a complex structure. We address this problem using the complexification of the real RKHS H. To this end, we define the space H = {f = f1 + if2; f1, f2 \u2208 H} equipped with the complex inner product:\n\u3008f ,g\u3009H =\u3008f1, g1\u3009H + \u3008f2, g2\u3009H + i (\u3008f2, g1\u3009H \u2212 \u3008f1, g2\u3009H) ,\nfor f = f1 + if2, g = g1 + ig2. Hence, f ,g : X \u2286 C\u03bd \u2192 C. It is not difficult to verify that H is a complex RKHS with kernel \u03ba [25]. We call H the complexification of H. It can readily be seen, that, although H is a complex RKHS, its respective kernel is real (i.e., its imaginary part is equal to zero).\nTo complete the presentation of the required framework for working on complex RKHSs using this rationale, we need a technique to implicitly map the samples data from the complex input space to the complexified RKHS H. This can be done using the simple rule:\n\u03a6(z) = \u03a6(x+ iy) = \u03a6(x,y) = \u03a6(x,y) + i\u03a6(x,y), (10)\nwhere \u03a6 is the feature map of the real reproducing kernel \u03ba, i.e., \u03a6(x,y) = \u03ba(\u00b7, (x,y)). It must be emphasized, that \u03a6 is not the feature map associated with the complex RKHS H. Furthermore, the employed kernel is a real one. Therefore, the algorithms derived using this approach cannot be reproduced, if one blindly applies the kernel trick using any complex kernel. However, observe that:\n\u3008\u03a6(z),\u03a6(z\u2032)\u3009H = 2\u3008\u03a6(x,y),\u03a6(x\u2032,y\u2032)\u3009H = 2\u03ba((x,y), (x\u2032,y\u2032)).\nThis relation implies that the complexification procedure is equivalent with the following complexified real kernel trick: Given an algorithm, which is formulated in terms of complex dot products (i.e, wHz, where z = x+ iy, w = w1 + iw2), one can construct an alternative algorithm by replacing each one of the complex dot products with a positive definite real kernel \u03ba, with arguments the extended real vectors of z and w (i.e., \u03ba((x,y), (w2,w2)))."}, {"heading": "V. WIRTINGER\u2019S CALCULUS ON C", "text": "Wirtinger\u2019s calculus [15] is enjoying increasing popularity in the signal processing community mainly in the context of complex adaptive filtering [16]\u2013[23], as a means to compute, in an elegant way, gradients of real valued cost functions that are defined on complex domains (C\u03bd). The Cauchy-Riemann conditions dictate that such functions are not holomorphic (except from the case where the function is a constant) and therefore the complex derivative cannot be used. Instead, if we consider that the cost function is defined on a Euclidean domain with a double dimensionality (R2\u03bd ), then the real derivatives may be employed. The price of this approach is that the computations may become cumbersome and tedious. Wirtinger\u2019s calculus provides an alternative equivalent formulation, that is based on simple rules and principles and which bears a great resemblance to the rules of the standard complex derivative. In this section, we present the main notions of Wirtinger\u2019s calculus for functions defined on complex domains. These ideas are, subsequently, extended in section VI to include the case of general complex Hilbert spaces.\nLet f : C \u2192 C be a complex function defined on C. Obviously, such a function may be regarded as either defined on R2 or C (i.e., f(z) = f(x + iy) = f(x, y)). Furthermore, it may be regarded as either a complex valued function, f(x, y) = u(x, y) + iv(x, y) or as a vector valued function f(x, y) = (u(x, y), v(x, y)). We will say that f is differentiable in the real sense if u and v are differentiable. It turns out that, when the complex structure is considered, the real derivatives may be described using an equivalent and more elegant formulation, which bears a surprising resemblance with the complex derivative. In fact, if the function f is differentiable in the complex sense (i.e. the complex derivative\nexists), the developed derivatives coincide with the complex ones. Although this methodology is known for some time in the German speaking countries and it has been applied to practical applications [32], [33], only recently has become popular in the signal processing community, mostly in the context of works that followed Picinbono\u2019s paper on widely linear estimation filters [16].\nThe Wirtinger\u2019s derivative (or W-derivative for short) of f at a point c is defined as follows\n\u2202f \u2202z (c) = 1 2\n(\n\u2202f \u2202x (c) \u2212 i\u2202f \u2202y (c)\n)\n= 1\n2\n(\n\u2202u \u2202x (c) + \u2202v \u2202y (c)\n)\n+ i\n2\n(\n\u2202v \u2202x (c)\u2212 \u2202u \u2202y (c)\n)\n. (11)\nThe conjugate Wirtinger\u2019s derivative (or CW-derivative for short) of f at c is defined by:\n\u2202f \u2202z\u2217 (c) = 1 2\n(\n\u2202f \u2202x (c) + i \u2202f \u2202y (c)\n)\n= 1\n2\n(\n\u2202u \u2202x (c)\u2212 \u2202v \u2202y (c)\n)\n+ i\n2\n(\n\u2202v \u2202x (c) + \u2202u \u2202y (c)\n)\n. (12)\nThe following properties can be proved [34], [35]: 1) If f has a Taylor series expansion with respect to z (i.e., it is holomorphic) around c, then \u2202f \u2202z\u2217 (c) =\n0.\n2) If f has a Taylor series expansion with respect to z\u2217 around c, then \u2202f \u2202z (c) = 0. 3) (\n\u2202f \u2202z (c)\n)\u2217 = \u2202f \u2217\n\u2202z\u2217 (c).\n4) (\n\u2202f \u2202z\u2217\n(c) )\u2217\n= \u2202f \u2202z\u2217 (c).\n5) Linearity: If f, g are differentiable in the real sense at c and \u03b1, \u03b2 \u2208 C, then \u2202(\u03b1f + \u03b2g)\n\u2202z (c) = \u03b1\n\u2202f \u2202z (c) + \u03b2 \u2202g \u2202z (c), \u2202(\u03b1f + \u03b2g) \u2202z\u2217 (c) = \u03b1 \u2202f \u2202z\u2217 (c) + \u03b2 \u2202g \u2202z\u2217 (c)\n6) Product Rule: If f , g are differentiable in the real sense at c, then\n\u2202(f \u00b7 g) \u2202z (c) = \u2202f \u2202z (c)g(c) + f(c) \u2202g \u2202z (c), \u2202(f \u00b7 g) \u2202z\u2217 (c) = \u2202f \u2202z\u2217 (c)g(c) + f(c) \u2202g \u2202z\u2217 (c).\n7) Division Rule: If f , g are differentiable in the real sense at c and g(c) 6= 0, then\n\u2202(f g )\n\u2202z (c) =\n\u2202f \u2202z (c)g(c) \u2212 f(c)\u2202g \u2202z (c)\ng2(c) ,\n\u2202(f g )\n\u2202z (c) =\n\u2202f \u2202z\u2217 (c)g(c) \u2212 f(c) \u2202g \u2202z\u2217 (c)\ng2(c) .\n8) Chain Rule: If f is differentiable in the real sense at c and g is differentiable in the real sense at\nf(c), then\n\u2202g \u25e6 f \u2202z (c) = \u2202g \u2202z (f(c)) \u2202f \u2202z (c) + \u2202g \u2202z\u2217 (f(c)) \u2202f\u2217 \u2202z (c), \u2202g \u25e6 f \u2202z\u2217 (c) = \u2202g \u2202z (f(c)) \u2202f \u2202z\u2217 (c) + \u2202g \u2202z\u2217 (f(c)) \u2202f\u2217 \u2202z\u2217 (c).\nIn view of the aforementioned properties, one might easily compute the W and CW derivatives of any\ncomplex function f , which is written in terms of z and z\u2217, following the following simple tricks:\n\u2022 To compute the W-derivative of a function f , which is expressed in terms of z and z\u2217,\napply the usual differentiation rules considering z\u2217 as a constant.\n\u2022 To compute the CW-derivative of a function f , which is expressed in terms of z and z\u2217,\napply the usual differentiation rules considering z as a constant.\nNote that any complex function f(z), which is differentiable in the real sense, can be cast in terms of z and z\u2217. For example, if the function f(z) = f(x+ iy) = f(x, y) is given in terms of x and y, replacing x by (z+z\u2217)/2 and y by (z\u2212z\u2217)/2 gives the result. It should be emphasized, that these statements must be regarded as a simple computational trick rather than as a rigorous mathematical rule. This trick works well due to the aforementioned properties. Nonetheless, special care should be considered whenever these tricks are applied. For example, given the function f(z) = |z|2, we might conclude that \u2202f \u2202z\u2217 = 0, since if we consider z as a constant, then f(z) is also a constant. However, one might argue that since there isn\u2019t any rule regarding the complex norm, this rationale leads to an error. Undeniably, if one recasts f as f(z) = zz\u2217, then one concludes that \u2202f \u2202z\u2217 = z and \u2202f \u2202z = z\u2217. Similar rules and principles hold for functions defined on C\u03bd [34]."}, {"heading": "VI. EXTENSION OF WIRTINGER\u2019S CALCULUS TO GENERAL HILBERT SPACES", "text": "To apply minimization algorithms on real valued operators defined on complex RKHSs, we need to compute the associated gradients. To this end, in this section, we generalize the main ideas and results of Wirtinger\u2019s calculus on general Hilbert spaces. We begin with a brief review of the Fre\u0301chet derivative, which generalizes differentiability to Hilbert spaces and which will be the basis for our discussion."}, {"heading": "A. Fre\u0301chet Derivatives", "text": "Since Fre\u0301chet differentiability is not the mainstream of mathematical tools used in the Signal Processing and Machine Learning communities, we give here some basic definitions for the sake of clarity. Consider a Hilbert space H over the field F (typically R or C). The operator T : H \u2192 F \u03bd is said to be Fre\u0301chet differentiable at f0, if there exists a linear continuous operator W = (W1,W2, . . . ,W\u03bd) : H \u2192 F\u03bd such that\nlim \u2016h\u2016H\u21920 \u2016T (f0 + h)\u2212 T (f0)\u2212W (h)\u2016F \u03bd \u2016h\u2016H = 0, (13)\nwhere \u2016 \u00b7 \u2016H = \u221a \u3008\u00b7, \u00b7\u3009H is the induced norm of the corresponding Hilbert Space. Note that F \u03bd is considered as a Banach space under the Euclidean norm. The linear operator W is called the Fre\u0301chet derivative and is usually denoted by dT (f0) : H \u2192 F \u03bd . Observe that this definition is valid not only for\nHilbert spaces, but for general Banach spaces too. However, since we are mainly interested at Hilbert spaces, we present the main ideas in this context. It can be proved that if such a linear continuous operator W can be found, then it is unique (i.e., the derivative is unique) [30]. In the special case where \u03bd = 1 (i.e., the operator T takes values on F ) using the Riesz\u2019s representation theorem, we may replace the linear continuous operator W with an inner product. Therefore, the operator T : H \u2192 F is said to be Fre\u0301chet differentiable at f0, iff there exists a w \u2208 H , such that\nlim \u2016h\u2016H\u21920 T (f0 + h)\u2212 T (f0)\u2212 \u3008h,w\u3009H \u2016h\u2016H = 0, (14)\nwhere \u3008\u00b7, \u00b7\u3009H is the dot product of the Hilbert space H and \u2016 \u00b7 \u2016H is the induced norm. The element w\u2217 is usually called the gradient of T at f0 and it is denoted by w\u2217 = \u2207T (f0). For a general vector valued operator T = (T1, . . . , T\u03bd) : H \u2192 F \u03bd , we may easily derive that if T is differentiable at f0, then T\u03b9 is differentiable at f0, for all \u03b9 = 1, 2, . . . , \u03bd, and that\ndT (f0)(h) =\n\n   \n\u3008h,\u2207T1(f0)\u2217\u3009H ... \u3008h,\u2207T\u03bd(f0)\u2217\u3009H\n\n    . (15)\nTo prove this claim, consider that since T is differentiable, there exists a continuous linear operator W such that\nlim \u2016h\u2016H\u21920 \u2016T (f0 + h)\u2212 T (f0)\u2212W (h)\u2016F \u03bd \u2016h\u2016H = 0 \u21d4\nlim \u2016h\u2016H\u21920\n(\n\u03bd \u2211\n\u03b9=1\n|T\u03b9(f0 + h)\u2212 T\u03b9(f0)\u2212W\u03b9(h)|2F \u2016h\u20162H\n)\n= 0,\nfor all \u03b9 = 1, . . . , \u03bd. Thus,\nlim \u2016h\u2016H\u21920\n(\nT\u03b9(f0 + h)\u2212 T\u03b9(f0)\u2212W\u03b9(h) \u2016h\u2016H\n)\n= 0,\nfor all \u03b9 = 1, 2, \u03bd. The Riesz\u2019s representation theorem dictates that since W\u03b9 is a continuous linear operator, there exists w\u03b9 \u2208 H , such that W\u03b9(h) = \u3008h,w\u03b9\u3009H , for all \u03b9 = 1, . . . , \u03bd. This proves that T\u03b9 is differentiable at f0 and that w\u2217\u03b9 = \u2207T\u03b9(f0), thus equation (15) holds. The converse is proved similarly.\nThe notion of Fre\u0301chet differentiability may be extended to include also partial derivatives. Consider\nthe operator T : H\u00b5 \u2192 F defined on the Hilbert space H\u00b5 with corresponding inner product:\n\u3008f ,g\u3009H\u00b5 = \u00b5 \u2211\n\u03b9=1\n\u3008f\u03b9, g\u03b9\u3009H ,\nwhere f = (f1, f2, . . . f\u00b5), g = (g1, g2, . . . g\u00b5). T (f) is said to be Fre\u0301chet differentiable at f0 with respect to f\u03b9, iff there exists a w \u2208 H , such that\nlim \u2016h\u2016H\u21920 T (f0 + [h]\u03b9)\u2212 T (f0)\u2212 \u3008[h]\u03b9, w\u3009H \u2016h\u2016H = 0, (16)\nwhere [h]\u03b9 = (0, 0, . . . , 0, h, 0, . . . , 0), is the element of H\u00b5 with zero entries everywhere, except at place \u03b9. The element w\u2217 is called the gradient of T at f0 with respect to f\u03b9 and it is denoted by w \u2217 = \u2207\u03b9T (f0). The Fre\u0301chet partial derivative at f0 in respect with f\u03b9 is denoted by \u2202T \u2202f\u03b9 (f0), \u2202T \u2202f\u03b9\n(f0)(h) = \u3008[h]\u03b9, w\u3009H. Although it will not be used here, it is interesting to note that, it is also possible to define Fre\u0301chet derivatives of higher order and a corresponding Taylor\u2019s series expansion. In this context, the n-th Fre\u0301chet derivative of T at f0, denoted as d nT (f 0), is a multilinear 2 map. If T has Fre\u0301chet derivatives of any order, it can be expanded as a Taylor series [36], i.e.,\nT (f0 + h) =\n\u221e \u2211\nn=0\n1\nn! dnT (f0)(h,h, . . . ,h). (17)\nIn relative literature the term dnT (c)(h,h, . . . ,h) is often replaced by dnT (c) \u00b7 hn, which it denotes that the multilinear map dnT (c) is applied to (h,h, . . . ,h)."}, {"heading": "B. Complex Hilbert spaces", "text": "Let H be a real Hilbert space with inner product \u3008\u00b7, \u00b7\u3009H and H2, H the Hilbert spaces defined as shown in section IV. In the following, the complex structure of H will be used to derive derivatives similar to the ones obtained from Wirtinger\u2019s calculus on C.\nConsider the function T : A \u2286 H \u2192 C, T (f) = T (uf + ivf ) = Tr(uf , vf ) + iTi(uf , vf ), defined on an open subset A of H, where uf , vf \u2208 H and Tr, Ti are real valued functions defined on H2. Any such function, T , may be regarded as defined either on a subset of H, or on a subset of H2. Furthermore, T may be regarded either as a complex valued function, or as a vector valued function, which takes values in R2. Therefore, we may equivalently write:\nT (f) = T (uf + ivf ) = Tr(uf , vf ) + iTi(uf , vf ), T (f) = (Tr(uf , vf ), Ti(uf , vf )) .\nIn the following, we will often change the notation according to the specific problem and consider any element of f \u2208 H defined either as f = uf + ivf \u2208 H, or as f = (uf , vf ) \u2208 H2. In a similar manner,\n2A function is called multilinear, if it is linear in each variable.\nany complex number may be regarded as either an element of C, or as an element of R2. We say that T is Fre\u0301chet complex differentiable at c \u2208 H, if there exists w \u2208 H such that:\nlim \u2016h\u2016H\u21920 T (c+ h)\u2212 T (c)\u2212 \u3008h,w\u3009H \u2016h\u2016H = 0. (18)\nThen w\u2217 is called the complex gradient of T at c and it is denoted as w\u2217 \u2261 \u2207T (c). The Fre\u0301chet complex derivative of T at c is denoted as dT (c)(h) = \u3008h,w\u3009H. This definition, although similar with the typical Fre\u0301chet derivative, exploits the complex structure of H. More specifically, the complex inner product that appears in the definition forces a great deal of structure on T . Similarly to the case of ordinary complex functions, it is this simple fact that gives birth to all the important strong properties of the complex derivative. For example, it can be proved, that if dT (c) exists, then so does dnT (c), for n \u2208 N. If T is differentiable at any c \u2208 A, T is called Fre\u0301chet holomorphic in A, or Fre\u0301chet complex analytic in A, in the sense that it can be expanded as a Taylor series, i.e.,\nT (c+ h) =\n\u221e \u2211\nn=0\n1\nn! dnT (c)(h,h, . . . ,h). (19)\nThe proof of this statement is out of the scope of this paper. The interested reader may dig deeper on this subject by referring to [36]. We begin our study, exploring the relations between the complex Fre\u0301chet derivative and the real Fre\u0301chet derivatives. In the following, we will say that T is Fre\u0301chet differentiable in the complex sense, if the complex derivative exists, and that T is Fre\u0301chet differentiable in the real sense, if its real Fre\u0301chet derivative exists (i.e., T is regarded as a vector valued operator T : H2 \u2192 H2). Similarly, the expression \u201cT is Fre\u0301chet complex analytic at c\u201d means that T is Fre\u0301chet complex analytic at a neighborhood around c. We will say that T is Fre\u0301chet real analytic, when both Tr and Ti have a Taylor\u2019s series expansion in the real sense.\nProposition VI.1. Let T : A \u2282 H \u2192 C be an operator such that T (f) = T (uf + ivf ) = T (uf , vf ) = Tr(uf , vf )+ iTi(uf , vf ). If the Fre\u0301chet complex derivative of T at a point c \u2208 A (i.e., dT (c) : H \u2192 C) exists, then Tr and Ti are differentiable at the point c = (c1, c1) = c1+ic2, where c1, c2 \u2208 H. Furthermore,\n\u2207uTr(c1, c2) = \u2207vTi(c1, c2), \u2207vTr(c1, c2) = \u2212\u2207uTi(c1, c2). (20)\nEquations (20) are the Cauchy Riemann conditions with respect to the Fre\u0301chet notion of differentiability. Similar to the simple case of complex valued functions, they provide a necessary and sufficient condition, for a complex operator T defined on H to be differentiable in the complex sense, providing that T is differentiable in the real sense. This is explored in the following proposition.\nProposition VI.2. If the operator T : A \u2286 H \u2192 C, T (f) = Tr(f) + iTi(f), where f = uf + ivf , is Fre\u0301chet differentiable in the real sense at a point (c1, c2) \u2208 H2 and the Fre\u0301chet Cauchy-Riemann conditions hold:\n\u2207uTr(c1, c2) = \u2207vTi(c1, c2), \u2207vTr(c1, c2) = \u2212\u2207uTi(c1, c2), (21)\nthen T is differentiable in the complex sense at the point c = (c1, c2) = c1 + c2i \u2208 H.\nProof: see Appendix A.\nIf the Fre\u0301chet Cauchy Riemann conditions are not satisfied for an operator T , then the Fre\u0301chet complex derivative does not exist and the function cannot be expressed in terms of h, as in the case of Fre\u0301chet complex differentiable functions (see equation 19). Nevertheless, if T is Fre\u0301chet differentiable in the real sense (i.e., Tr and Ti are Fre\u0301chet differentiable), we may still find a form of Taylor\u2019s series expansion by utilizing the extension of Wirtinger\u2019s calculus. It can be shown (see the proof of proposition VI.2 in Appendix A), that:\nT (c+ h) =T (c) + 1\n2 \u3008h, (\u2207uT (c)\u2212 i\u2207vT (c))\u2217\u3009H (22)\n+ 1\n2 \u3008h\u2217, (\u2207uT (c) + i\u2207vT (c))\u2217\u3009H + o(\u2016h\u2016H).\nOne may notice that in the more general case, where T is Fre\u0301chet real-differentiable, the associated Taylor\u2019s expansion is casted in terms of both h and h\u2217. This can be generalized for higher order Taylor\u2019s expansion formulas by following the same rationale. Observe also that, if T is Fre\u0301chet complex differentiable, this relation degenerates (due to the Cauchy Riemann conditions) to the respective Taylor\u2019s expansion formula (i.e., (19)). In this context, the following definitions come naturally.\nWe define the Fre\u0301chet Wirtinger\u2019s gradient (or W-gradient for short) of T at c as\n\u2207fT (c) = 1\n2 (\u22071T (c)\u2212 i\u22072T (c)) =\n1 2 (\u2207uTr(c) +\u2207vTi(c)) + i 2 (\u2207uTi(c)\u2212\u2207vTr(c)) , (23)\nand the Fre\u0301chet Wirtinger\u2019s derivative (or W -derivative) as \u2202T \u2202f (c) : H \u2192 C, such that \u2202T \u2202f (c)(h) = \u3008h,\u2207fT (c)\u2217\u3009H. Consequently, the Fre\u0301chet conjugate Wirtinger\u2019s gradient (or CW-gradient for short) and the Fre\u0301chet conjugate Wirtinger\u2019s derivative (or CW-derivative) of T at c are defined by:\n\u2207f\u2217T (c) = 1\n2 (\u22071T (c) + i\u22072T (c)) =\n1 2 (\u2207uTr(c)\u2212\u2207vTi(c)) + i 2 (\u2207uTi(c) +\u2207vTr(c)) , (24)\nand \u2202T \u2202f\u2217 (c) : H \u2192 C, such that \u2202T \u2202f\u2217 (c)(h) = \u3008h, (\u2207f\u2217T (c))\u2217\u3009H. Note, that both the W-derivative and the CW-derivative exist, if T is Fre\u0301chet differentiable in the real sense. In view of these new definitions,\nequation (22) may now be recasted as follows\nT (c+ h) =T (c) + \u3008h, (\u2207fT (c))\u2217\u3009H + \u3008h \u2217, (\u2207f\u2217T (c))\u2217\u3009H + o(\u2016h\u2016H). (25)\nFrom these definitions, several properties can be derived:\n1) If T (f) is f -holomorphic at c (i.e., it has a Taylor series expansion with respect to f at c), then\nits Fre\u0301chet W-derivative at c degenerates to the standard Fre\u0301chet complex derivative and its Fre\u0301chet\nCW-derivative vanishes, i.e., \u2207f\u2217T (c) = 0. 2) If T (f ) is f\u2217-holomorphic at c (i.e., it has a Taylor series expansion with respect to f\u2217 at c), then\n\u2207fT (c) = 0. 3) The first order Taylor expansion around f \u2208 H is given by\nT (f + h) =T (f ) + \u3008h, (\u2207fT (f))\u2217\u3009H + \u3008h\u2217, (\u2207f\u2217T (f))\u2217\u3009H.\n4) If T (f) = \u3008f ,w\u3009H, then \u2207fT (c) = w\u2217, \u2207f\u2217T (c) = 0, for every c. 5) If T (f) = \u3008f\u2217,w\u3009H, then \u2207fT (c) = 0, \u2207f\u2217T (c) = w\u2217, for every c. 6) Linearity: If T ,S : H \u2192 C are Fre\u0301chet differentiable in the real sense at c \u2208 H and a, b \u2208 C, then\n\u2207f (\u03b1T + \u03b2S)(c) = \u03b1\u2207fT (c) + \u03b2\u2207fS(c)\n\u2207f\u2217(\u03b1T + \u03b2S)(c) = \u03b1\u2207f\u2217T (c) + \u03b2\u2207f\u2217S(c).\nA complete list of the derived properties, together with the proofs of the most important ones, are given in Appendix B.\nAn important consequence of the previous properties is that if T is a real valued operator defined on\nH, then (\u2207fT (c))\u2217 = \u2207f\u2217T (c), and its first order Taylor\u2019s expansion is given by:\nT (f + h) = T (f) + \u3008h, (\u2207fT (f))\u2217\u3009H + \u3008h\u2217, (\u2207f\u2217T (f))\u2217\u3009H\n= T (f) + \u3008h,\u2207f\u2217T (f)\u3009H + (\u3008h,\u2207f\u2217T (f)\u3009H)\u2217 = T (f) + 2 \u00b7 \u211c [\u3008h,\u2207f\u2217T (f)\u3009H] .\nHowever, in view of the Cauchy Riemann inequality we have:\n\u211c [\u3008h,\u2207f\u2217T (f)\u3009H] \u2264 |\u3008h,\u2207f\u2217T (f)\u3009H| \u2264 \u2016h\u2016H \u00b7 \u2016\u2207f\u2217T (f)\u2016H.\nThe equality in the above relationship holds if h \u21c8 \u2207f\u2217T (where the notation \u21c8 denotes that h and \u2207f\u2217T have the same direction, i.e., there is a \u03bb > 0, such that h = \u03bb\u2207f\u2217T ). Hence, the direction of increase of T is \u2207f\u2217T (f). Therefore, any gradient descent based algorithm minimizing T (f) is based on the update scheme:\nfn = fn\u22121 \u2212 \u00b5 \u00b7 \u2207f\u2217T (fn\u22121). (26)\nAssuming differentiability of T , a standard result from Fre\u0301chet real calculus states that a necessary condition for a point c to be an optimum (in the sense that T (f) is minimized or maximized) is that this point is a stationary point of T , i.e. the Fre\u0301chet partial derivatives of T at c vanish. In the context of Wirtinger\u2019s calculus we have the following obvious corresponding result.\nProposition VI.3. If the function T : A \u2286 H \u2192 C is Fre\u0301chet differentiable at c in the real sense, then a necessary condition for a point c to be a local optimum (in the sense that T (c) is minimized or maximized) is that either the Fre\u0301chet W, or the Fre\u0301chet CW derivative vanishes3."}, {"heading": "VII. COMPLEX KERNEL LEAST MEAN SQUARES - CKLMS", "text": "In order to illustrate how the proposed framework may be applied to problems of complex signal processing, we present two realizations of the Kernel Least Mean Squares (KLMS) algorithm for complex data. The first scheme (CKLMS1) employs the complexification of real reproducing kernels (see section IV), while the second one uses pure complex kernels (CKLMS2). Wirtinger\u2019s calculus is exploited in both cases to compute the necessary gradient updates."}, {"heading": "A. Complex KLMS via complexification of real kernels - CKLMS1", "text": "Consider the sequence of examples (z(1), d(1)), (z(2), d(2)), . . . , (z(N), d(N)), where d(n) \u2208 C, z(n) \u2208 V \u2282 C\u03bd , z(n) = x(n) + iy(n), x(n),y(n) \u2208 R\u03bd , for n = 1, . . . , N . Consider, also, a real reproducing kernel \u03ba defined on X \u00d7X, X \u2286 R2\u03bd , and let H be the corresponding RKHS. We map the points z(n) to the RKHS H (H is constructed as explained in section IV) using the mapping \u03a6:\n\u03a6(z(n)) = \u03a6(z(n)) + i\u03a6(z(n)) = \u03ba (\u00b7, (x(n),y(n))) + i \u00b7 \u03ba (\u00b7, (x(n),y(n))) ,\nfor n = 1, . . . , N , where \u03a6 is the feature map of H. The objective of the complex Kernel LMS is to design a filter, w, with desired response d\u0302(n) = \u3008\u03a6(z(n)),w\u3009H, so that to minimize E [Ln(w)], where\nLn(w) = |e(n)|2 = |d(n)\u2212 \u3008\u03a6(z(n)),w\u3009H|2 = (d(n)\u2212 \u3008\u03a6(z(n)),w\u3009H) (d(n)\u2212 \u3008\u03a6(z(n)),w\u3009H)\u2217\n= (d(n)\u2212 \u3008w\u2217,\u03a6\u2217(z(n))\u3009H) (d(n)\u2217 \u2212 \u3008w,\u03a6(z(n))\u3009H) ,\nat each instance n. We then apply the complex LMS to the transformed data, estimating the mean square error by its current measurement E\u0302 [Ln(w)] = Ln(w), using the rules of Wirtinger\u2019s calculus to compute\n3Note, that for real valued functions the W and the CW derivatives constitute a conjugate pair. Thus, if the W derivative\nvanishes, then the CW derivative vanishes too. The converse is also true.\nthe CW gradient, i.e., \u2207w\u2217Ln(w) = \u2212e(n)\u2217 \u00b7\u03a6(z(n)). Therefore the CKLMS1 update rule becomes:\nw(n) = w(n\u2212 1) + \u00b5e(n)\u2217 \u00b7\u03a6(z(n)), (27)\nwhere w(n) denotes the estimate at iteration n.\nAssuming that w(0) = 0, the repeated application of the weight-update equation gives:\nw(n) =w(n\u2212 1) + \u00b5e(n)\u2217\u03a6(z(n)) = w(n \u2212 2) + \u00b5e(n \u2212 1)\u2217\u03a6(z(n\u2212 1)) + \u00b5e(n)\u2217\u03a6(z(n))\n=\u00b5\nn \u2211\nk=1\ne(k)\u2217\u03a6(z(k)). (28)\nThus, the filter output at iteration n becomes:\nd\u0302(n) =\u3008\u03a6(z(n)),w(n\u2212 1)\u3009H = \u00b5 n\u22121 \u2211\nk=1\ne(k)\u3008\u03a6(z(n)),\u03a6(z(k))\u3009H\n=2\u00b5\nn\u22121 \u2211\nk=1\n\u211c[e(k)]\u03ba(z(n),z(k)) + 2\u00b5 \u00b7 i n\u22121 \u2211\nk=1\n\u2111[e(k)]\u03ba(z(n),z(k)), (29)\nwhere the evaluation of the kernel is done by replacing the complex vectors z(n), of C\u03bd with the corresponding real vectors of R2\u03bd , i.e., z(n) = (x(n),y(n)).\nIt can readily be shown that, since the CKLMS1 is the complex LMS in RKHS, the important properties of the LMS (convergence in the mean, misadjustment, e.t.c.) carry over to CKLMS1. Furthermore, we may also define a normalized version, which we call Normalized Complex Kernel LMS (NCKLMS1). The weight-update of the NCKLMS1 is given by:\nw(n) =w(n\u2212 1) + \u00b5 2 \u00b7 \u03ba(z(n),z(n))e(n) \u2217 \u03a6(z(n))\nThe NCKLMS1 algorithm is summarized in Algorithm 1. We should emphasize that this formulation of the complex KLMS cannot be derived following the usual \u201cblack box\u201d rationale of the kernel trick, as it has already been pointed out in section IV. The complexified real kernel trick can be used instead.\nOne might think, that modeling the desired response as d\u0302(n) = \u3008w,\u03a6(z(n))\u3009H, provides an alternative formulation for the CKLMS1 algorithm. In this case, the CW gradient of the instantaneous square error is given by \u2207w\u2217Ln(w) = \u2212e(n)\u03a6(z(n)). Following the same procedure, we conclude that the update rule becomes:\nw(n) = w(n\u2212 1) + \u00b5e(n) \u00b7\u03a6(z(n)),\nand assuming that w(0) = 0, one concludes that:\nw(n) =\u00b5\nn \u2211\nk=1\ne(k)\u03a6(z(k)).\nAlgorithm 1 Normalized Complex Kernel LMS with complexification of real kernels (NCKLMS1) INPUT: (z(1), d(1)), . . . , (z(N), d(N)) OUTPUT: The expansion w = \u2211N\nk=1 a(k)\u03ba(\u00b7,z(k)) + i \u00b7 \u2211N k=1 b(k)\u03ba(\u00b7,z(k)).\nInitialization: Set a = {}, b = {}, Z = {} (i.e., w = 0). Select the step parameter \u00b5 and the kernel \u03ba.\nfor n=1:N do\nCompute the filter output:\nd\u0302(n) =\nn\u22121 \u2211\nk=1\n(a(k) + b(k)) \u00b7 \u03ba(z(n),z(k)) + n\u22121 \u2211\nk=1\n(a(k) \u2212 b(k)) \u00b7 \u03ba(z(n),z(k)).\nCompute the error: e(n) = d(n)\u2212 d\u0302(n). \u03b3 = 2\u03ba(z(n),z(n)). a(n) = \u00b5(\u211c[e(n)] + \u2111[e(n)])/\u03b3. b(n) = \u00b5(\u211c[e(n)]\u2212\u2111[e(n)])/\u03b3. Add the new center z(n) to the list of centers, i.e., add z(n) to the list Z, add a(n) to the list a, add b(n) to the list b.\nend for\nHowever, although this relation is different than equation (28), the filter output at iteration n for this filter turns out to be exactly the same:\nd\u0302(n) =\u3008w(n \u2212 1),\u03a6(z(n))\u3009H = \u00b5 n\u22121 \u2211\nk=1\ne(k)\u3008\u03a6(z(k)),\u03a6(z(n))\u3009H,\nwhich is in line with what we know for the standard complex LMS."}, {"heading": "B. Complex KLMS with pure complex kernels - CKLMS2", "text": "As, in section VII-A, consider the sequence of examples (z(1), d(1)), (z(2), d(2)), . . . , (z(N), d(N)), where d(n) \u2208 C, z(n) \u2208 V \u2282 C\u03bd , z(n) = x(n) + iy(n), x(n),y(n) \u2208 R\u03bd , for n = 1, . . . , N . Consider also a complex reproducing kernel \u03ba defined on X \u00d7 X, X \u2286 C\u03bd and the respective complex RKHS H. Each element f \u2208 H may be cast in the form f = uf + ivf , uf , vf \u2208 H, where H is a real Hilbert space. We map the points z(n) to the complex RKHS H using the feature map \u03a6\u0303 : X \u2192 H :\n\u03a6\u0303(z) = \u3008\u00b7, \u03ba(\u00b7,z)\u3009H, for n = 1, . . . , N . Estimating the filter output by d\u0302(n) = \u3008w, \u03a6\u0303(z(n))\u3009H, the objective of the complex Kernel LMS is to minimize E [Ln(w)], at each instance n. Once more, we apply the complex LMS to the transformed data, using the rules of Wirtinger\u2019s calculus to compute the gradient of Ln(w), i.e., \u2207w\u2217Ln(w) = \u2212e(n)\u2217 \u00b7 \u03a6\u0303(z(n)). Therefore, the CKLMS2 update rule becomes w(n) = w(n\u2212 1) + \u00b5e(n)\u2217 \u00b7 \u03a6\u0303(z(n)), as expected, where w(n) denotes the estimate at iteration n.\nAssuming that w(0) = 0, the repeated application of the weight-update equation gives:\nw(n) =\nn \u2211\nk=1\ne(k)\u2217\u03a6\u0303(z(k)). (30)\nThus, the filter output at iteration n becomes:\nd\u0302(n) =\u3008\u03a6\u0303(z(n)),w(n\u2212 1)\u3009H = \u00b5 n\u22121 \u2211\nk=1\ne(k)\u3008\u03a6\u0303(z(n)), \u03a6\u0303(z(k))\u3009H\n=\u00b5\nn\u22121 \u2211\nk=1\ne(k)\u03ba(z(k),z(n)).\nWe should note, that the CKLMS2 algorithm may be equivalently derived, if one blindly applies the kernel trick on the usual complex LMS. However, such an approach conceals the mathematical framework that lies underneath, which is needed if one seeks a deeper understanding of the problem. The repeated application of the update equation of the CLMS yields:\nw(n) =\nn \u2211\nk=1\ne(k)\u2217z(k),\nwhile the filter output at iteration n is given by:\nd\u0302(n) = \u00b5\nn\u22121 \u2211\nk=1\ne(k)z(n)Hz(k),\nwhere the notation \u00b7H denotes the Hermitian matrix. It is evident that the application of the kernel trick on these equations yields the same results.\nFurthermore, note that, using the complex gaussian kernel, the algorithm is automatically normalized.\nThe CKLMS2 algorithm is summarized in Algorithm 1.\nAnother formulation of the CKLMS2 algorithm may be derived if we estimate the filter output as\nd\u0302(n) = \u3008w, \u03a6\u0303(z(n))\u3009H. Then the update rule becomes\nw(n) = w(n\u2212 1) + \u00b5e(n) \u00b7 \u03a6\u0303(z(n)).\nAssuming that w(0) = 0, the repeated application of the weight-update equation gives:\nw(n) =\nn \u2211\nk=1\ne(k)\u03a6\u0303(z(k)),\nand the filter output at iteration n becomes:\nd\u0302(n) = \u00b5\nn\u22121 \u2211\nk=1\ne(k)\u03ba(z(n),z(k)). (31)\nNote that the two formulations of the CKLMS2 are not identical, as it was the case for CKLMS. However, all the simulated experiments that we performed, using the complex gaussian kernel, exhibited similar performance (in terms of signal to noise ratio - SNR).\nAlgorithm 2 Normalized Complex Kernel LMS2 (NCKLMS2) INPUT: (z(1), d(1)), . . . , (z(N), d(N)) OUTPUT: The expansion w = \u2211N\nk=1 a(k)\u03ba(\u00b7,z(k)).\nInitialization: Set a = {}, Z = {} (i.e., w = 0). Select the step parameter \u00b5 and the parameter \u03c3 of the complex gaussian kernel.\nfor n=1:N do\nCompute the filter output:\nd\u0302(n) =\nn\u22121 \u2211\nk=1\na(k) \u00b7 \u03ba(z(k),z(n)).\nCompute the error: e(n) = d(n)\u2212 d\u0302(n). \u03b3 = 2\u03ba(z(n),z(n)). a(n) = \u00b5e(n)/\u03b3. Add the new center z(n) to the list of centers, i.e., add z(n) to the list Z, add a(n) to the list a.\nend for"}, {"heading": "C. Sparsification", "text": "The main drawback of any kernel based adaptive filtering algorithms is that a growing number of training points, zn, is involved, as it is apparent from (28), (30) in the case of complex KLMS. Hence, increasing memory and computational resources are needed, as time evolves. Several strategies have been proposed to cope with this problem and to produce sparse solutions. In this paper, we employ the well known novelty criterion [14], [37]. In novelty criterion online sparsification, a dictionary of points, C, is formed and updated appropriately. Whenever a new data pair (\u03a6(zn), dn) is considered, a decision\nis immediately made of whether to add the new center, \u03a6(zn), to the dictionary of centers C. The decision is reached following two simple rules. First, the distance of the new center, \u03a6(zn), from the current dictionary is evaluated: dis = minck\u2208C{\u2016\u03a6(zn)\u2212 ck\u2016H}. If this distance is smaller than a given threshold \u03b41 (i.e., the new center is close to the existing dictionary), then the center is not added to C. Otherwise, we compute the prediction error en = dn \u2212 d\u0302n. If |en| is smaller than a predefined threshold \u03b42, then the new center is discarded. Only if |en| \u2265 \u03b42 the new center \u03a6(zn) is added to the dictionary.\nAn alternative method has been considered in [4], which results in an exponential forgetting mechanism of past data. In [6], [38], the sliding window rationale has been considered. In all the implementations of CKLMS that are presented in this paper the novelty criterion was adopted."}, {"heading": "VIII. EXPERIMENTS", "text": "The performance of CKLMS1 and CKLMS2 has been tested in the context of a nonlinear channel\nequalization task (see figure 1). The nonlinear channel consists of a linear filter:\nt(n) = (\u22120.9 + 0.8i) \u00b7 s(n) + (0.6 \u2212 0.7i) \u00b7 s(n\u2212 1)\nand a memoryless nonlinearity\nq(n) = t(n) + (0.1 + 0.15i) \u00b7 t2(n) + (0.06 + 0.05i) \u00b7 t3(n).\nThis is a standard model that has been extensively used in the literature for such tasks. At the receiver end of the channel, the signal is corrupted by white Gaussian noise and then observed as r(n). The input\nsignal that was fed to the channel had the form\ns(n) = 0.70 ( \u221a 1\u2212 \u03c12X(n) + i\u03c1Y (n) ) , (32)\nwhere X(n) and Y (n) are gaussian random variables. This input is circular for \u03c1 = \u221a 2/2 and highly non-circular if \u03c1 approaches 0 or 1 [18]. Note that the issue of circularity is very important in complex adaptive filtering. Circularity is intimately related to rotation in the geometric sense. A complex random variable Z is called circular, if for any angle \u03c6 both Z and Zei\u03c6 (i.e., the rotation of Z by angle \u03c6) follow the same probability distribution [17]. Loosely speaking, non circularity adds some form of nonlinearity to the signal. It can be proved that widely linear estimation (i.e., linear estimation in both z and z\u2217) outperforms standard linear estimation for general (i.e., circular or non-circular) complex signals. For circular signals, the two models produce identical results [16], [39].\nThe aim of a channel equalization task is to construct an inverse filter, which acts on the output r(n) and reproduces the original input signal as close as possible. To this end, we apply the NCKLMS1 and the NCKLMS2 algorithms to the set of samples\n((r(n+D), r(n+D \u2212 1), . . . , r(n+D \u2212 L)), s(n)) ,\nwhere L > 0 is the filter length and D the equalization time delay, which is present to, almost, any equalization set up.\nExperiments were conducted on a set of 5000 samples of the input signal (32) considering both the circular and the non-circular case. The results are compared with the NCLMS and the WL-NCLMS algorithms (i.e., widely linear NCLMS). Note that the WL-NCLMS has been recently used as an alternative to the CLMS, in an attempt to cope with non circularity as well as with soft nonlinearities. In all algorithms, the step update parameter, \u00b5, is tuned for the best possible results. Time delay D was also set for optimality. Figures 2 and 3 show the learning curves of the NCKLMS1 using the real Gaussian kernel \u03ba(x,y) = exp(\u2212\u2016x \u2212 y\u20162/\u03c32) (with \u03c3 = 5) and the NCKLMS2 using the complex Gaussian kernel \u03ba\u03c3,Cd(z,w) := exp ( \u2212 \u2211 d i=1 (zi\u2212w\u2217i ) 2\n\u03c32\n)\n(with \u03c3 = 5), together with those obtained from\nthe NCLMS and the WL-NCLMS algorithms. The novelty criterion was used for the sparsification of the NCKLMS1 with \u03b41 = 0.15 and \u03b42 = 0.2 and of the NCKLMS2 with \u03b41 = 0.1 and \u03b42 = 0.2. In both examples, NCKLMS1 and NCKLMS2 considerably outperform both the NCLMS and the WL-NCLMS algorithms. However, this enhanced behavior comes at a price in computational complexity, since the NCKLMS requires the evaluation of the kernel function. Comparing the NCKLMS1 and the NCKLMS2, the experiments show that the results differ, with the former one leading to an improved performance."}, {"heading": "IX. CONCLUSIONS", "text": "A new framework for kernel adaptive filtering for complex signal processing has been developed. The proposed methodology, besides providing a skeleton for working with pure complex kernels, allows for the construction of complex RKHSs from real ones, through a technique called complexification of RKHSs. Such an approach provides the advantage of working with some popular real kernels in the complex domain. It has to be pointed out, that our method is a general one and can be used on any type of real\nand/or complex kernels that have or can be developed. To the best of our knowledge, this is the first time that a methodology for complex adaptive processing in RKHSs is proposed. Wirtinger\u2019s calculus has been extended to cope with the problem of differentiation in the involved (infinite) dimensional Hilbert spaces. The derived rules and properties of the extended Wirtinger\u2019s calculus on complex RKHS turn out to be similar in structure to the special case of finite dimensional complex spaces. The proposed framework was applied on the complex LMS and two realizations for the complex Kernel LMS algorithm were developed. Experiments, which were performed on the equalization problem of a nonlinear channel, for both circular and non-circular input data, showed a significant decrease in the steady state mean square error, compared with the complex LMS and the widely linear complex LMS."}, {"heading": "APPENDIX A", "text": "PROOF OF PROPOSITION VI.2\nWe start with a lemma that will be used to prove the claim.\nLemma A.1. Consider the Hilbert space H and a, b \u2208 H. The limit\nlim \u2016h\u2016H\u21920 \u3008h\u2217,a\u3009H \u2212 \u3008h, b\u3009H \u2016h\u2016H = 0, (33)\nif and only if a = b = 0.\nConsider the first order Taylor expansions of Tr and Ti at c = c1+ ic2 = (c1, c2), where h = h1+ ih2:\nTr(c+ h) = Tr(c) + \u3008h1,\u2207uTr(c)\u3009H + \u3008h2,\u2207vTr(c)\u3009H + o(\u2016h\u2016H2), Ti(c+ h) = Ti(c) + \u3008h1,\u2207uTi(c)\u3009H + \u3008h2,\u2207vTi(c)\u3009H + o(\u2016h\u2016H2).\nMultiplying the second relation with i and adding it to the first one, we take:\nT (c+ h) = T (c) + \u3008h1,\u2207uTr(c)\u2212 i\u2207uTi(c)\u3009H + \u3008h2,\u2207vTr(c)\u2212 i\u2207vTi(c)\u3009H + o(\u2016h\u2016H).\nTo simplify the notation we may define\n\u2207uT (c) = \u2207uTr(c) + i\u2207uTi(c), \u2207vT (c) = \u2207vTr(c) + i\u2207vTi(c)\nand obtain:\nT (c+ h) = T (c) + \u3008h1, (\u2207uT (c))\u2217\u3009H + \u3008h2, (\u2207vT (c))\u2217\u3009H + o(\u2016h\u2016H2).\nNext, we substitute h1 and h2 using the relations h1 = h+h \u2217 2 and h2 = h\u2212h\u2217 2i and use the sesquilinear property of the inner product of H:\nT (c+ h) = T (c) + 1\n2 \u3008h, (\u2207uT (c)\u2212 i\u2207vT (c))\u2217\u3009H\n+ 1\n2 \u3008h\u2217, (\u2207uT (c) + i\u2207vT (c))\u2217\u3009H + o(\u2016h\u2016H).\nIt has already been shown that equation (22) is essential for the development of Wirtinger\u2019s calculus. To complete the proof of the proposition we compute the fraction that appears in the definition of the complex Fre\u0301chet derivative:\nT (c+ h)\u2212 T (c)\u2212 \u3008h,w\u3009H \u2016h\u2016H = (\n1 2 \u3008h, (\u2207uT (c)\u2212 i\u2207vT (c))\u2217\u3009H + 1 2 \u3008h\u2217, (\u2207uT (c) + i\u2207vT (c))\u2217\u3009H \u2212 \u3008h,w\u3009H\n)\n/ \u2016h\u2016H + o(\u2016h\u2016H) \u2016h\u2016H .\nRecall that, since o(\u2016h\u2016H)/\u2016h\u2016H \u2192 0 as \u2016h\u2016H \u2192 0, for this limit to exist and vanish, it is necessary that \u2207uT (c) + i\u2207v2T (c) = 0 and w\u2217 = \u2207uT (c)(c)\u2212 i\u2207vT (c) (see lemma A.1). However, according to our definition,\n\u2207uT (c) + i\u2207vT (c) = (\u2207uTr(c)\u2212\u2207vTi(c)) + i (\u2207uTi(c) +\u2207vTr(c)) .\nThus, T is differentiable in the Fre\u0301chet complex sense, iff the Cauchy-Riemann conditions hold. Moreover, in this case:\n\u2207T (c) =\u2207uTr(c) + i\u2207uTi(c) = \u2207vTi(c)\u2212 i\u2207vTr(c)."}, {"heading": "APPENDIX B", "text": "PROPERTIES OF WIRTINGER\u2019S DERIVATIVES ON COMPLEX HILBERT SPACES\nBelow we give a complete list of the main properties of the extended Wirtinger\u2019s Calculus in complex Hilbert spaces. A rigorous and detailed presentation of the theory, as well as the proofs of all these properties can be found in [35].\n1) If T (f) is f -holomorphic at c (i.e., it has a Taylor series expansion with respect to f around c),\nthen its Fre\u0301chet W-derivative at c degenerates to the standard Fre\u0301chet complex derivative and its\nFre\u0301chet CW-derivative vanishes, i.e., \u2207f\u2217T (c) = 0. 2) If T (f) is f\u2217-holomorphic at c (i.e., it has a Taylor series expansion with respect to f\u2217 around\nc), then \u2207fT (c) = 0. 3) (\u2207fT (c))\u2217 = \u2207f\u2217T \u2217(c).\n4) (\u2207f\u2217T (c))\u2217 = \u2207fT \u2217(c). 5) If T is real valued, then (\u2207fT (c))\u2217 = \u2207f\u2217T (c). 6) The first order Taylor expansion around f \u2208 H is given by\nT (f + h) =T (f ) + \u3008h, (\u2207fT (f))\u2217\u3009H + \u3008h\u2217, (\u2207f\u2217T (f))\u2217\u3009H.\n7) If T (f) = \u3008f ,w\u3009H, then \u2207fT (c) = w\u2217, \u2207f\u2217T (c) = 0, for every c. 8) If T (f) = \u3008w,f \u3009H, then \u2207fT (c) = 0, \u2207f\u2217T (c) = w, for every c. 9) If T (f) = \u3008f\u2217,w\u3009H, then \u2207fT (c) = 0, \u2207f\u2217T (c) = w\u2217, for every c.\n10) If T (f) = \u3008w,f \u2217\u3009H, then \u2207fT (c) = w, \u2207f\u2217T (c) = 0, for every c. 11) Linearity: If T ,S : H \u2192 C are Fre\u0301chet differentiable in the real sense at c \u2208 H and a, b \u2208 C, then\n\u2207f (\u03b1T + \u03b2S)(c) = \u03b1\u2207fT (c) + \u03b2\u2207fS(c), \u2207f\u2217(\u03b1T + \u03b2S)(c) = \u03b1\u2207f\u2217T (c) + \u03b2\u2207f\u2217S(c).\n12) Product Rule: If T ,S : H \u2192 C are Fre\u0301chet differentiable in the real sense at c \u2208 H, then:\n\u2207f (T \u00b7 S)(c) = \u2207fT (c)S(c) + T (c)\u2207fS(c), \u2207f\u2217(T \u00b7 S)(c) = \u2207f\u2217T (c)S(c) + T (c)\u2207f\u2217S(c).\n13) Division Rule: If T ,S : H \u2192 C are Fre\u0301chet differentiable in the real sense at c \u2208 H and S(c) 6= 0, then:\n\u2207f ( T\nS\n)\n(c) = \u2207fT (c)S(c)\u2212 T (c)\u2207fS(c)\nS2(c) , \u2207f\u2217\n(\nT S\n)\n(c) = \u2207f\u2217T (c)S(c)\u2212 T (c)\u2207f\u2217S(c)\nS2(c) .\n14) Chain Rule: If T : H \u2192 C is Fre\u0301chet differentiable at c \u2208 H, S : C \u2192 C is differentiable in the real sense at T (c) \u2208 C, then:\n\u2207fS \u25e6 T (c) = \u2202S\n\u2202z (T (c))\u2207fT (c) +\n\u2202S \u2202z\u2217 (T (c))\u2207f (T \u2217)(c),\n\u2207f\u2217S \u25e6 T (c) = \u2202S\n\u2202z (T (c))\u2207f\u2217T (c) +\n\u2202S \u2202z\u2217 (T (c))\u2207f\u2217(T \u2217)(c).\nThe proofs of properties 1 and 2 are rather obvious. Here, we give the proofs of properties 3, 7 and\n11, which have been used to derive the main results of this paper. Proof of property 3: The existence of \u2207fT (c) and \u2207f\u2217T (c) is guaranteed by the Fre\u0301chet differentiability of T at c (in the real sense). To take the result, observe that:\n(\u2207fT (c))\u2217 = 1\n2 (\u2207uTr(c) +\u2207vTi(c))\u2212\ni 2 (\u2207uTi(c)\u2212\u2207vTr(c))\n= 1\n2 (\u2207uTr(c)\u2212\u2207v(\u2212Ti)(c)) +\ni 2 (\u2207u(\u2212Ti)(c) +\u2207vTr(c)) = (\u2207f\u2217T \u2217(c)) .\nProperty 4 can be proved similarly.\nProof of property 7: Considering the definition of Fre\u0301chet complex derivative (see equation 18), we\nobserve that:\nT (c+ h)\u2212 T (c)\u2212\u3008h,g\u3009H = \u3008c + h,w\u3009H \u2212 \u3008c,w\u3009H \u2212 \u3008h,g\u3009H = \u3008c,w\u3009H \u2212 \u3008h,g\u3009H.\nThus, T is Fre\u0301chet complex differentiable at c, with \u2207T (c) = w\u2217 and from property 1, \u2207f\u2217(c) = 0 and \u2207f (c) = w.\nProof of property 11: Let T (f) = Tr(uf , vf )+ iTi(uf , vf ), S(f) = S(uf + ivf ) = Sr(uf , vf )+\niSi(uf , vf ) be two complex functions and \u03b1, \u03b2 \u2208 C, such that \u03b1 = \u03b11 + i\u03b12, \u03b2 = \u03b21 + i\u03b22. Then R(f) = \u03b1T (f) + \u03b2S(f) and the Fre\u0301chet W-derivative of R will be given by:\n\u2207fR(c) = 1\n2 (\u2207uRr(c) +\u2207vRi(c)) +\ni 2 (\u2207uRi(c)\u2212\u2207vRr(c)) .\nApplying the linearity property of the ordinary Fre\u0301chet derivative, after some algebra we take the result. For the second part, in view of properties 3, 4 and the linearity property of the Fre\u0301chet W-derivative, the Fre\u0301chet CW-derivative of R at c will be given by:\n\u2207f\u2217R(c) =\u2207f\u2217(\u03b1T + \u03b2S)(c) = (\u2207f (\u03b1T + \u03b2S)\u2217(c))\u2217\n=(\u2207f (\u03b1\u2217T \u2217 + \u03b2\u2217S\u2217)(c))\u2217 = (\u03b1\u2217\u2207fT \u2217(c) + \u03b2\u2217\u2207fS\u2217(c))\u2217 =\u03b1 (\u2207fT \u2217(c))\u2217 + \u03b2 (\u2207fS\u2217(c))\u2217 = \u03b1\u2207f\u2217T (c) + \u03b2\u2207f\u2217S(c),\nwhich completes the proof."}], "references": [{"title": "The kernel least-mean-square algorithm", "author": ["W. Liu", "P. Pokharel", "J.C. Principe"], "venue": "IEEE Trans. Sign. Proc., vol. 56, no. 2, pp. 543\u2013554, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A. Smola", "R.C. Williamson"], "venue": "IEEE Trans. Sign. Proc., vol. 52, no. 8, pp. 2165\u20132176, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "The kernel recursive least-squares algorithm", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "IEEE Trans. Sign. Proc., vol. 52, no. 8, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "On line classification using kernels and projection based adaptive algorithm", "author": ["K. Slavakis", "S. Theodoridis", "I. Yamada"], "venue": "IEEE Trans. Signal Process., vol. 56, no. 7, pp. 2781\u20132797, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Adaptive constrained learning in reproducing kernel hilbert spaces: The robust beamforming case", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 12, pp. 4744\u20134764, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Sliding window generalized kernel affine projection algorithm using projection mappings", "author": ["K. Slavakis", "S. Theodoridis"], "venue": "Eurasip Journal on Advances in Signal Processing, vol. art. no. 735351, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Pattern Recognition, 4th edition", "author": ["S. Theodoridis", "K. Koutroumbas"], "venue": "SUBMITTED TO IEEE TRANSACTIONS ON SIGNAL", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Iterative kernel principal component analysis for image modeling", "author": ["K. Kim", "M.O. Franz", "B. Scholkopf"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 9, pp. 1351\u20131366, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive kernel-based image denoising employing semi-parametric regularization", "author": ["P. Bouboulis", "K. Slavakis", "S. Theodoridis"], "venue": "IEEE Trans. Image Process., vol. 19, no. 6, pp. 1465 \u2013 1479, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Kernel principal component analysis", "author": ["A.J.S.B. Sch\u00f6lkopf", "K.R. Muller"], "venue": "Lecture notes in computer science, vol. 1327, pp. 583\u2013588, 1997.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "A survey of kernel and spectral methods for clustering. pattern recognition", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, no. 1, pp. 176\u2013190, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Zur formalen theorie der functionen von mehr complexen ver\u00e4nderlichen", "author": ["W. Wirtinger"], "venue": "Math. Ann., vol. 97, pp. 357\u2013375, 1927.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1927}, {"title": "Widely linear estimation with complex data", "author": ["B. Picinbono", "P. Chevalier"], "venue": "IEEE Trans. Signal Process., vol. 43, no. 8, pp. 2030\u20132033, 1995.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Complex-valued adaptive signal processing, ser", "author": ["T. Adali", "H. Li"], "venue": "Adaptive Signal Processing: Next Generation Solutions,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Complex ICA using nonlinear functions", "author": ["T. Adali", "H. Li", "M. Novey", "J. Cardoso"], "venue": "IEEE Trans. Signal Process., vol. 56, no. 9, pp. 4536\u20134544, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "On extending the complex fast ICA algorithm to noncircular sources", "author": ["M. Novey", "T. Adali"], "venue": "IEEE Trans. Signal Process., vol. 56, no. 5, pp. 2148\u20132154, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Widely linear decision-feedback equalizer for time-dispersive linear MIMO channels", "author": ["D. Mattera", "L. Paura", "F. Sterle"], "venue": "IEEE Trans. Signal Process., vol. 53, no. 7, pp. 2525\u20132536, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Widely linear versus linear blind multiuser detection with subspacebased channel estimation: Finite sample-size effects", "author": ["A.S. Cacciapuoti", "G. Gelli", "L. Paura", "F. Verde"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 4, pp. 1426\u20131443, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "ARMA prediction of widely linear systems by using the innovations algorithm", "author": ["J. Navarro-Moreno"], "venue": "IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3061\u20133068, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Integral Transforms, Reproducing Kernels and their applications", "author": ["S. Saitoh"], "venue": "Longman Scientific & Technical,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "An introduction to the theory of reproducing kernel hilbert spaces", "author": ["V.I. Paulsen"], "venue": "http://www.math.uh.edu/\u223cvern/rkhs.pdf.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 0}, {"title": "Functions of positive and negative type and their connection with the theory of integral equations", "author": ["J. Mercer"], "venue": "Phil. Trans. Roy. Soc. Ser. A, vol. 209, pp. 415\u2013446, 1909.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1909}, {"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Transactions of the American Mathematical Society, vol. 68, no. 3, pp. 337\u2013404, 1950.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1950}, {"title": "An explicit description of the reproducing kernel hilbert spaces of gaussian rbf kernels", "author": ["I. Steinwart", "D. Hush", "C. Scovel"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 10, pp. 4635\u20134643, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "A complex gradient operator and its application in adaptive array theory", "author": ["D.H. Brandwood"], "venue": "IEE proc. H (Microwaves, optics and Antennas), vol. 130, no. 1, pp. 11\u201316, 1983.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1983}, {"title": "Complex gradient and hessian", "author": ["A.V. de Bos"], "venue": "IEE proc. Visual image signal processing, vol. 141, no. 6, pp. 380\u2013382, 1994.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1994}, {"title": "The complex gradient operator and the CR-calculus", "author": ["K. Kreutz-Delgado"], "venue": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.6515&rep=rep1&type=pdf.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 0}, {"title": "Wirtinger\u2019s calculus in general hilbert spaces", "author": ["P. Bouboulis"], "venue": "http://arxiv.org/abs/1005.5170.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1005}, {"title": "Holomorphy and Calculus in Normed Spaces", "author": ["S.B. Chae"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1985}, {"title": "A resourse allocating network for function interpolation", "author": ["J. Platt"], "venue": "Newral Computation, vol. 3, no. 2, pp. 213\u2013225, 1991.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1991}, {"title": "A sliding-window kernel RLS algorithm and its application to nonlinear channel identification", "author": ["S. van Vaerenbergh", "J. V\u0131\u0301a", "I. Santamar\u0131\u0301a"], "venue": "Proceedings of ICASSP, vol. V. Toulouse, France: IEEE, 2006, pp. 789\u2013792.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Processing in Reproducing Kernel Hilbert Spaces (RKHSs), in the context of online adaptive learning, is gaining in popularity within the Machine Learning and Signal Processing communities [1]\u2013[6].", "startOffset": 201, "endOffset": 204}, {"referenceID": 5, "context": "INTRODUCTION Processing in Reproducing Kernel Hilbert Spaces (RKHSs), in the context of online adaptive learning, is gaining in popularity within the Machine Learning and Signal Processing communities [1]\u2013[6].", "startOffset": 205, "endOffset": 208}, {"referenceID": 6, "context": "An alternative way of describing this process is through the popular kernel trick [7], [8]: Given an algorithm, which is formulated in terms of dot products, one can construct an alternative algorithm by replacing each one of the dot products with a positive definite kernel \u03ba.", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "An alternative way of describing this process is through the popular kernel trick [7], [8]: Given an algorithm, which is formulated in terms of dot products, one can construct an alternative algorithm by replacing each one of the dot products with a positive definite kernel \u03ba.", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "The main representatives of this class of algorithms are the celebrated support vector machines (SVMs), which have dominated the research in machine learning over the last decade [9].", "startOffset": 179, "endOffset": 182}, {"referenceID": 9, "context": ", image processing and denoising [10], [11], principal component analysis [12], clustering [13], e.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": ", image processing and denoising [10], [11], principal component analysis [12], clustering [13], e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": ", image processing and denoising [10], [11], principal component analysis [12], clustering [13], e.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": ", image processing and denoising [10], [11], principal component analysis [12], clustering [13], e.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Even though, the presented framework has a broad range and may be applied to generalize a wide variety of kernel methods to the complex domain, this work focuses on the recently developed Kernel LMS (KLMS) [1], [14].", "startOffset": 206, "endOffset": 209}, {"referenceID": 13, "context": "Wirtinger\u2019s calculus [15] has become very popular in the signal processing community, mainly in the context of complex adaptive filtering [16]\u2013[23], as a means of computing, in an elegant way, gradients of real valued cost functions defined on complex domains (C).", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Wirtinger\u2019s calculus [15] has become very popular in the signal processing community, mainly in the context of complex adaptive filtering [16]\u2013[23], as a means of computing, in an elegant way, gradients of real valued cost functions defined on complex domains (C).", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "Wirtinger\u2019s calculus [15] has become very popular in the signal processing community, mainly in the context of complex adaptive filtering [16]\u2013[23], as a means of computing, in an elegant way, gradients of real valued cost functions defined on complex domains (C).", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "The material presented here may be found with more details in [24] and [25].", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "The material presented here may be found with more details in [24] and [25].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "Furthermore, the term positive definite was introduced for the first time by Mercer in the kernel context (see [26]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "It has been proved (see [27]) that to every positive definite kernel \u03ba there corresponds one and only one class of functions H with a uniquely determined inner product in it, forming a Hilbert space and admitting \u03ba as a reproducing kernel.", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "Many more can be found in the relative literature [7]\u2013[9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Many more can be found in the relative literature [7]\u2013[9].", "startOffset": 54, "endOffset": 57}, {"referenceID": 22, "context": ", \u03ba(z, w) = 1 (1\u2212wz) , for Bergman spaces on the unit disk, where |z|, |w| < 1 [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 25, "context": "An explicit description of the RKHSs of these kernels, together with some important properties can be found in [28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "It is not difficult to verify that H is a complex RKHS with kernel \u03ba [25].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "WIRTINGER\u2019S CALCULUS ON C Wirtinger\u2019s calculus [15] is enjoying increasing popularity in the signal processing community mainly in the context of complex adaptive filtering [16]\u2013[23], as a means to compute, in an elegant way, gradients of real valued cost functions that are defined on complex domains (C).", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "WIRTINGER\u2019S CALCULUS ON C Wirtinger\u2019s calculus [15] is enjoying increasing popularity in the signal processing community mainly in the context of complex adaptive filtering [16]\u2013[23], as a means to compute, in an elegant way, gradients of real valued cost functions that are defined on complex domains (C).", "startOffset": 173, "endOffset": 177}, {"referenceID": 20, "context": "WIRTINGER\u2019S CALCULUS ON C Wirtinger\u2019s calculus [15] is enjoying increasing popularity in the signal processing community mainly in the context of complex adaptive filtering [16]\u2013[23], as a means to compute, in an elegant way, gradients of real valued cost functions that are defined on complex domains (C).", "startOffset": 178, "endOffset": 182}, {"referenceID": 26, "context": "Although this methodology is known for some time in the German speaking countries and it has been applied to practical applications [32], [33], only recently has become popular in the signal processing community, mostly in the context of works that followed Picinbono\u2019s paper on widely linear estimation filters [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 27, "context": "Although this methodology is known for some time in the German speaking countries and it has been applied to practical applications [32], [33], only recently has become popular in the signal processing community, mostly in the context of works that followed Picinbono\u2019s paper on widely linear estimation filters [16].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "Although this methodology is known for some time in the German speaking countries and it has been applied to practical applications [32], [33], only recently has become popular in the signal processing community, mostly in the context of works that followed Picinbono\u2019s paper on widely linear estimation filters [16].", "startOffset": 312, "endOffset": 316}, {"referenceID": 28, "context": "(12) The following properties can be proved [34], [35]: 1) If f has a Taylor series expansion with respect to z (i.", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "(12) The following properties can be proved [34], [35]: 1) If f has a Taylor series expansion with respect to z (i.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "Similar rules and principles hold for functions defined on C [34].", "startOffset": 61, "endOffset": 65}, {"referenceID": 30, "context": "If T has Fr\u00e9chet derivatives of any order, it can be expanded as a Taylor series [36], i.", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "The interested reader may dig deeper on this subject by referring to [36].", "startOffset": 69, "endOffset": 73}, {"referenceID": 31, "context": "In this paper, we employ the well known novelty criterion [14], [37].", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "An alternative method has been considered in [4], which results in an exponential forgetting mechanism of past data.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "In [6], [38], the sliding window rationale has been considered.", "startOffset": 3, "endOffset": 6}, {"referenceID": 32, "context": "In [6], [38], the sliding window rationale has been considered.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "This input is circular for \u03c1 = \u221a 2/2 and highly non-circular if \u03c1 approaches 0 or 1 [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "For circular signals, the two models produce identical results [16], [39].", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "A rigorous and detailed presentation of the theory, as well as the proofs of all these properties can be found in [35].", "startOffset": 114, "endOffset": 118}], "year": 2017, "abstractText": "Over the last decade, kernel methods for nonlinear processing have successfully been used in the machine learning community. The primary mathematical tool employed in these methods is the notion of the Reproducing Kernel Hilbert Space. However, so far, the emphasis has been on batch techniques. It is only recently, that online techniques have been considered in the context of adaptive signal processing tasks. Moreover, these efforts have only been focussed on and real valued data sequences. To the best of our knowledge, no kernel-based strategy has been developed, so far, that is able to deal with complex valued signals. Furthermore, although the real reproducing kernels are used in an increasing number of machine learning problems, complex kernels have not, yet, been used, in spite of their potential interest in applications that deal with complex signals, with Communications being a typical example. In this paper, we present a general framework to attack the problem of adaptive filtering of complex signals, using either real reproducing kernels, taking advantage of a technique called complexification of real RKHSs, or complex reproducing kernels, highlighting the use of the complex gaussian kernel. In order to derive gradients of operators that need to be defined on the associated complex RKHSs, we employ the powerful tool of Wirtinger\u2019s Calculus, which has recently attracted much attention in the signal processing community. Writinger\u2019s calculus simplifies computations and offers an elegant tool for treating complex signals. To this end, in this paper, the notion of Writinger\u2019s calculus is extended, for the first time, to include complex RKHSs and use it to derive several realizations of the Complex Kernel Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS offers significant performance improvements over the traditional complex LMS or Widely Linear complex LMS (WLLMS) algorithms, when dealing with nonlinearities. P. Bouboulis and S. Theodoridis are with the Department of Informatics and Telecommunications, University of Athens, Greece, e-mail: {bouboulis,stheodor}@di.uoa.gr. SUBMITTED TO IEEE TRANSACTIONS ON SIGNAL PROCESSING 1 Extension of Wirtinger\u2019s Calculus in Reproducing Kernel Hilbert Spaces and the Complex Kernel LMS", "creator": "LaTeX with hyperref package"}}}