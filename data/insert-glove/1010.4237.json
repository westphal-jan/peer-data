{"id": "1010.4237", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2010", "title": "Robust PCA via Outlier Pursuit", "abstract": "Singular Value Decomposition (wiencke and Principal landholdings Component Analysis) 27-aug is tubby one of the most perrott widely cranmer used techniques acho for nettavisen dimensionality reduction: gospels successful backstairs and mlm efficiently computable, it 21.38 is dwindles nevertheless plagued kerb by a well - 7-a known, well - kirkley documented poyntzpass sensitivity shimizu to essabar outliers. mombach Recent 45.44 work 19-21 has emulsifiers considered the bitter setting where sunnm\u00f8re each whirry point aprica has a few arbitrarily altgeld corrupted components. bambuco Yet, in deeney applications handcuffs of SVD or PCA ballata such eiendom as veghel robust collaborative filtering or fricker bioinformatics, zaenal malicious 3,069 agents, 1,500-pound defective genes, recreational or 4,625 simply corrupted communique or contaminated haddenham experiments demands may udvar effectively vawa yield entire kottur points that klinghoffer are ready-to-fly completely corrupted.", "histories": [["v1", "Wed, 20 Oct 2010 16:05:28 GMT  (169kb)", "http://arxiv.org/abs/1010.4237v1", "24 pages, appeared in NIPS 2010"], ["v2", "Fri, 31 Dec 2010 18:36:49 GMT  (172kb)", "http://arxiv.org/abs/1010.4237v2", "26 pages, appeared in NIPS 2010. v2 has typos corrected, some re-writing. results essentially unchanged"]], "COMMENTS": "24 pages, appeared in NIPS 2010", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["huan xu", "constantine caramanis", "sujay sanghavi"], "accepted": true, "id": "1010.4237"}, "pdf": {"name": "1010.4237.pdf", "metadata": {"source": "CRF", "title": "Robust PCA via Outlier Pursuit", "authors": ["Huan Xu", "Constantine Caramanis"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n01 0.\n42 37\nv1 [\ncs .L\nG ]\n2 0\nO ct\n2 01\n0 1\nWe present an efficient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identifies the corrupted points. Such identification of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and financial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.\nI. INTRODUCTION\nThis paper is about the following problem: suppose we are given a large data matrix M , and we know it can be decomposed as\nM = L0 + C0,\nwhere L0 is a low-rank matrix, and C0 is non-zero in only a fraction of the columns. Aside from these broad restrictions, both components are arbitrary. In particular we do not know the rank (or the row/column space) of L0, or the number and positions of the non-zero columns of C0. Can we recover the column-space of the low-rank matrix L0, and the identities of the non-zero columns of C0, exactly and efficiently?\nWe are primarily motivated by Principal Component Analysis (PCA), arguably the most widely used technique for dimensionality reduction in statistical data analysis. The canonical PCA problem [1], seeks to find the best (in the least-square-error sense) low-dimensional subspace approximation to high-dimensional points. Using the Singular Value Decomposition (SVD), PCA finds the lower-dimensional approximating subspace by forming a low-rank approximation to the data matrix, formed by considering each point as a column; the output of PCA is the (low-dimensional) column space of this low-rank approximation.\nIt is well known (e.g., [2]\u2013[4]) that standard PCA is extremely fragile to the presence of outliers: even a single corrupted point can arbitrarily alter the quality of the approximation. Such non-probabilistic or persistent data corruption may stem from sensor failures, malicious tampering, or the simple fact that some of the available data may not conform to the presumed low-dimensional source / model. In terms of the data matrix, this means that most of the column vectors will lie in a low-dimensional space \u2013 and hence the corresponding matrix L0 will be low-rank \u2013 while the remaining columns will be outliers \u2013 corresponding to the column-sparse matrix C. The natural question in this setting is to ask if we can still (exactly or near-exactly) recover the column space of the uncorrupted points, and the identities of the outliers. This is precisely our problem.\nRecent years have seen a lot of work on both robust PCA [3], [5]\u2013[12], and on the use of convex optimization for recovering low-dimensional structure [4], [13]\u2013[15]. Our work lies at the intersection of these two fields, but has several significant differences from work in either space. We compare and relate our work to existing literature, and expand on the differences, in Section III-C.\nThe authors are with the Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712 USA email: (huan.xu@mail.utexas.edu; caramanis@mail.utexas.edu; sanghavi@mail.utexas.edu)\n2 II. PROBLEM SETUP\nThe precise PCA with outlier problem that we consider is as follows: we are given n points in pdimensional space. A fraction 1 \u2212 \u03b3 of the points lie on a r-dimensional true subspace of the ambient R\np, while the remaining \u03b3n points are arbitrarily located \u2013 we call these outliers/corrupted points. We do not have any prior information about the true subspace or its dimension r. Given the set of points, we would like to learn (a) the true subspace and (b) the identities of the outliers.\nAs is common practice, we collate the points into a p\u00d7 n data matrix M , each of whose columns is one of the points, and each of whose rows is one of the p coordinates. It is then clear that the data matrix can be decomposed as\nM = L0 + C0.\nHere L0 is the matrix corresponding to the non-outliers; thus rank(L0) = r. Consider its Singular Value Decomposition (SVD)\nL0 = U0\u03a30V \u22a4 0 . (1)\nThe columns of U0 form an orthonormal basis for the r-dimensional subspace we wish to recover. At most (1 \u2212 \u03b3)n of the columns of L0 are non-zero (the rest correspond to the outliers). C0 is the matrix corresponding to the non-outliers; we will denote the set of non-zero columns of C0 by I0, with |I0| = \u03b3n. These non-zero columns are completely arbitrary.\nWith this notation, out intent is to exactly recover the column space of L0, and the set of outliers I0. Clearly, this is not always going to be possible (regardless of the algorithm used) and thus we need to impose a few weak additional assumptions. We develop these in Section II-A below.\nWe are also interested in the noisy case, where\nM = L0 + C0 +W,\nand W corresponds to any additional noise. In this case we are interested in approximate identification of both the true subspace and the outliers.\nA. Incoherence: When does exact recovery make sense?\nIn general, our objective of splitting a low-rank matrix from a column-sparse one is not always a well defined one. As an extreme example, consider the case where the data matrix M is non-zero in only one column. Such a matrix is both low-rank and column-sparse, thus the problem is unidentifiable. To make the problem meaningful, we need to impose that the low-rank matrix L0 cannot itself be column-sparse as well. This is done via the following incoherence condition.\nDefinition: A matrix L \u2208 Rp\u00d7n with SVD L = U\u03a3V \u22a4, and (1\u2212 \u03b3)n of whose columns are non-zero, is said to be column-incoherent with parameter \u00b5 if\nmax i\n\u2016V \u22a4ei\u20162 \u2264 \u00b5r\n(1\u2212 \u03b3)n where {ei} are the coordinate unit vectors.\nThus if V has a column aligned with a coordinate axis, then \u00b5 = (1\u2212\u03b3)n/r. Similarly, if V is perfectly incoherent (e.g. if r = 1 and every non-zero entry of V has magnitude 1/ \u221a\n(1\u2212 \u03b3)n) then \u00b5 = 1. In the standard PCA setup, if the points are generated by some low-dimensional isometric Gaussian distribution, then with high probability, one will have \u00b5 = O(max(1, log(n)/r)) [16]. Alternatively, if the points are generated by a uniform distribution over a bounded set, then \u00b5 = \u0398(1).\nA small incoherence parameter \u00b5 essentially enforces that the matrix L0 will have column support that is spread out. Note that this is quite natural from the application perspective. Indeed, if the left hand side is as big as 1, it essentially means that one of the directions of the column space which we wish to recover, is defined by only a single observation. Given the regime of a constant fraction of arbitrarily chosen and arbitrarily corrupted points, such a setting is not meaningful. Having a small incoherence\n3 \u00b5 is an assumption made in all methods based on nuclear norm minimization up-to-date [4], [15]\u2013[17]. Also unidentifiable is the setting where a corrupted point lies in the true subspace. Thus, in matrix terms, we require that every column of C0 does not lie in the column space of L0.\nThe parameters \u00b5 and \u03b3 are not required for the execution of the algorithm, and do not need to be known a priori. They only arise in the analysis of our algorithm\u2019s performance.\nOther Notation and Preliminaries: Capital letters such as A are used to represent matrices, and accordingly, Ai denotes the ith column vector. Letters U , V , I and their variants (complements, subscripts, etc.) are reserved for column space, row space and column support respectively. There are four associated projection operators we use throughout. The projection onto the column space, U , is denoted by PU and given by PU(A) = UU\u22a4A, and similarly for the row-space PV (A) = AV V \u22a4. The matrix PI(A) is obtained from A by setting column Ai to zero for all i 6\u2208 I. Finally, PT is the projection to the space spanned by U and V , and given by PT (\u00b7) = PU(\u00b7) + PV (\u00b7)\u2212PUPV (\u00b7). Note that PT depends on U and V , and we suppress this notation wherever it is clear which U and V we are using. The complementary operators, PU\u22a5 ,PV \u22a5 , PT\u22a5 and PIc are defined as usual. The same notation is also used to represent a subspace of matrices: e.g., we write A \u2208 PU for any matrix A that satisfies PU(A) = A. Five matrix norms are used: \u2016A\u2016\u2217 is the nuclear norm, \u2016A\u2016 is the spectral norm, \u2016A\u20161,2 is the sum of \u21132 norm of the columns Ai, \u2016A\u2016\u221e,2 is the largest \u21132 norm of the columns, and \u2016A\u2016F is the Frobenius norm. The only vector norm used is \u2016 \u00b7 \u20162, the \u21132 norm. Depending on the context, I is either the unit matrix, or the identity operator; ei is the ith base vector. The SVD of L0 is U0\u03a30V0. The rank of L0 is denoted as r, and we have \u03b3 , |I0|/n, i.e., the fraction of outliers.\nIII. MAIN RESULTS AND CONSEQUENCES\nWhile we do not recover the matrix L0, we show that the goal of PCA can be attained: even under our strong corruption model, with a constant fraction of points corrupted, we show that we can \u2013 under mild assumptions \u2013 exactly recover both the column space of L0 (i.e the low-dimensional space the uncorrupted points lie on) and the column support of C0 (i.e. the identities of the outliers), from M . If there is additional noise corrupting the data matrix, i.e. if we have M = L0 + C0 +W , a natural variant of our approach finds a good approximation."}, {"heading": "A. Algorithm", "text": "Given data matrix M , our algorithm, called Outlier Pursuit, generates (a) a matrix U\u0302 , with orthonormal rows, that spans the low-dimensional true subspace we want to recover, and (b) a set of column indices I\u0302 corresponding to the outlier points.\nAlgorithm 1 Outlier Pursuit\nFind (L\u0302, C\u0302), the optimum of the following convex optimization program\nMinimize: \u2016L\u2016\u2217 + \u03bb\u2016C\u20161,2 Subject to: M = L+ C\n(2)\nCompute SVD L\u0302 = U1\u03a31V \u22a41 and output U\u0302 = U1. Output the set of non-zero columns of C\u0302, i.e. I\u0302 = {j : c\u0302ij 6= 0 for some i}\nWhile in the noiseless case there are simple algorithms with similar performance, the benefit of the algorithm, and of the analysis, is extension to more realistic and interesting situations where in addition to gross corruption of some samples, there is additional noise. Adapting the Outlier Pursuit algorithm, we have the following variant for the noisy case.\nNoisy Outlier Pursuit: Minimize: \u2016L\u2016\u2217 + \u03bb\u2016C\u20161,2 Subject to: \u2016M \u2212 (L+ C)\u2016F \u2264 \u03b5 (3)\n4 Outlier Pursuit (and its noisy variant) is a convex surrogate for the following natural (but combinatorial and intractable) first approach to the recovery problem:\nMinimize: rank(L) + \u03bb\u2016C\u20160,c Subject to: M = L+ C\n(4)\nwhere \u2016 \u00b7 \u20160,c stands for the number of non-zero columns of a matrix."}, {"heading": "B. Performance", "text": "We show that under rather weak assumptions, Outlier Pursuit exactly recovers the column space of the low-rank matrix L0, and the identities of the non-zero columns of outlier matrix C0. The formal statement appears below.\nTheorem 1 (Noiseless Case): Suppose we observe M = L0+C0, where L0 has rank r and incoherence parameter \u00b5. Suppose further that C0 is supported on at most \u03b3n columns. Any output to Outlier Pursuit recovers the column space exactly, and identifies exactly the indices of columns corresponding to outliers not lying in the recovered column space, as long as the fraction of corrupted points, \u03b3, satisfies\n\u03b3 1\u2212 \u03b3 \u2264 c1 \u00b5r , (5)\nwhere c1 = 9121 . This can be achieved by setting the parameter \u03bb in outlier pursuit to be 3 7 \u221a \u03b3n \u2013 in fact it holds for any \u03bb in a specific range which we provide below.\nFor the case where in addition to the corrupted points, we have noisy observations, M\u0303 = M +N , we have the following result.\nTheorem 2 (Noisy Case): Suppose we observe M\u0303 = M +N = L0 + C0 +N , where \u03b3\n1\u2212 \u03b3 \u2264 c2 \u00b5r\n(6)\nwith c2 = 91024 , and \u2016N\u2016F \u2264 \u03b5. Let the output of Noisy Outlier Pursuit be L\u2032, C \u2032. Then there exists L\u0303, C\u0303 such that M = L\u0303+ C\u0303, L\u0303 has the correct column space, and C\u0303 the correct column support, and\n\u2016L\u2032 \u2212 L\u0303\u2016F \u2264 10 \u221a n\u03b5; \u2016C \u2032 \u2212 C\u0303\u2016F \u2264 9 \u221a n\u03b5; .\nThe conditions in this theorem are essentially tight in the following scaling sense (i.e., up to universal constants). If there is no additional structure imposed, beyond what we have stated above, then up to scaling, in the noiseless case, Outlier Pursuit can recover from as many outliers (i.e., the same fraction) as any possible algorithm of arbitrary complexity. In particular, it is easy to see that if the rank of the matrix L0 is r, and the fraction of outliers satisfies \u03b3 \u2265 1/(r + 1), then the problem is not identifiable, i.e., no algorithm can separate authentic and corrupted points. In the presence of stronger assumptions, e.g., isometric distribution, on the authentic points, better recovery guarantees are possible [12]."}, {"heading": "C. Related Work", "text": "Robust PCA has a long history (e.g., [3], [5]\u2013[11]). Each of these algorithms either performs standard PCA on a robust estimate of the covariance matrix, or finds directions that maximize a robust estimate of the variance of the projected data. These algorithms seek to approximately recover the column space, and moreover, no existing approach attempts to identify the set of outliers. This outlier identification, while outside the scope of traditional PCA algorithms, is important in a variety of applications such as finance, bio-informatics, and more.\nMany existing robust PCA algorithms suffer two pitfalls: performance degradation with dimension increase, and computational intractability. To wit, [18] shows several robust PCA algorithms including M-estimator [19], Convex Peeling [20], Ellipsoidal Peeling [21], Classical Outlier Rejection [22], Iterative Deletion [23] and Iterative Trimming [24] have breakdown points proportional to the inverse of dimensionality, and hence are useless in the high dimensional regime we consider.\n5 Algorithms with non-diminishing breakdown point, such as Projection-Pursuit [25] are non-convex or even combinatorial, and hence computationally intractable (NP-hard) as the size of the problem scales. In contrast to these, the performance of Outlier Pursuit does not depend on p, and can be solved in polynomial time.\nAlgorithms based on nuclear norm minimization to recover low rank matrices are now standard, since the seminal paper [14]. Recent work [4], [15] has taken the nuclear norm minimization approach to the decomposition of a low-rank matrix and an overall sparse matrix. At a high level, these papers are close in spirit to ours. However, there are critical differences in the problem setup, the results, and in key analysis techniques. First, these algorithms fail in our setting as they cannot handle outliers \u2013 entire columns where every entry is corrupted. Second, from a technical and proof perspective, all the above works investigate exact signal recovery \u2013 the intended outcome is known ahead of time, and one just needs to investigate the conditions needed for success. In our setting however, the convex optimization cannot recover L0 itself exactly. This requires an auxiliary \u201coracle problem\u201d as well as different analysis techniques on which we elaborate below.\nIV. PROOF OF THEOREM 1\nIn this section and the next section, we prove Theorem 1 and Theorem 2. Past matrix recovery papers, including [4], [15], [16], sought exact recovery. As such, the generic (and successful) roadmap for the proof technique was to identify the first-order necessary and sufficient conditions for a feasible solution to be optimal, and then show that a subgradient certifying optimality of the desired solution exists under the given assumptions. In our setting, the outliers, C0, preclude exact recovery of L0. In fact, the optimum L\u0302 of (2) will be non-zero in every column of C0 that is not orthogonal to L0\u2019s column space \u2013 that is, Outlier Pursuit (2) cannot recover L0 on the columns corresponding to the outliers (intuitively, no method can \u2013 there is nothing left to recover once the entire point is corrupted). Instead, we seek to recover a pair (L\u0302, C\u0302) where L\u0302 has the correct column space and C\u0302 the correct column support. The challenge is that we do not know, a priori, what that pair will be, and hence cannot follow the standard road map to write optimality conditions for a specific pair. The main new ingredient of the proof of correctness and the analysis of the algorithm, is the introduction of an oracle problem with additional side constraints, that produces a solution with the correct column space and support.\nBefore going into technical details, we list some technical preliminaries that we use multiple times in the sequel. The following lemma is well-known, and gives the subgradient of the norms we consider.\nLemma 1: For any column space U , row space V and column support I: 1) Let the SVD of a matrix A be U\u03a3V \u22a4. Then the subgradient to \u2016 \u00b7 \u2016\u2217 at A is {UV \u22a4+W |PT (W ) =\n0, \u2016W\u2016 \u2264 1}. 2) Let the column support of a matrix A be I. Then the subgradient to \u2016\u00b7\u20161,2 at A is {H+Z|PI(H) =\nH,Hi = Ai/\u2016Ai\u20162; PI(Z) = 0, \u2016Z\u2016\u221e,2 \u2264 1}. 3) For any A, B, we have PI(AB) = API(B); for any A, PUPI(A) = PIPU(A). Lemma 2: If a matrix H\u0303 satisfies \u2016H\u0303\u2016\u221e,2 \u2264 1 and is supported on I, then \u2016H\u0303\u2016 \u2264 \u221a\n|I|. Proof: Using the variational form of the operator norm, we have\n\u2016H\u0303\u2016 = max \u2016x\u20162\u22641,\u2016y\u20162\u22641 x\u22a4H\u0303y\n= max \u2016x\u20162\u22641 \u2016x\u22a4H\u0303\u20162 = max \u2016x\u20162\u22641\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n(x\u22a4H\u0303i)2 \u2264 \u221a \u2211\ni\u2208I 1 =\n\u221a\n|I|.\nThe inequality holds because \u2016H\u0303i\u20162 = 1 when i \u2208 I, and equals zero otherwise. Lemma 3: Given a matrix V\u0303 \u2208 Rr\u00d7n, then \u2016UV\u0303 \u22a4\u2016\u221e,2 = maxi \u2016V\u0303 \u22a4ei\u20162.\nProof: By definition we have\n\u2016UV\u0303 \u22a4\u2016\u221e,2 = max i \u2016UV\u0303 \u22a4i \u20162 (a) = max i \u2016V\u0303 \u22a4i \u20162 = max i \u2016V\u0303 \u22a4ei\u20162.\n6 Here (a) holds since U is orthonormal."}, {"heading": "A. Oracle Problem and Optimality Conditions", "text": "As discussed, the true solution (L0, C0) generally cannot be recovered by the Outlier Pursuit algorithm. Instead, our goal is to recover a pair (L\u0302, C\u0302) so that L\u0302 has the correct column space, and C\u0302 the correct column support.\nWe develop our candidate solution (L\u0302, C\u0302) by considering the alternate optimization problem where we add constraints to (2) based on what we hope its optimum should be. In particular, recall the SVD of the true L0 = U0\u03a30V \u22a40 and define for any matrix X the projection onto the space of all matrices with column space contained in U0 as PU0(X) := U0U\u22a40 X . Similarly for the column support I0 of the true C0, define the projection PI0(X) to be the matrix that results when all the columns in Ic0 are set to 0.\nNote that U0 and I0 above correspond to the truth. Thus, with this notation, we would like the optimum of (2) to satisfy PU0(L\u0302) = L\u0302, as this is nothing but the fact that L\u0302 has recovered the true subspace. Similarly, having C\u0302 satisfy PI0(C\u0302) = C\u0302 means that we have succeeded in identifying the outliers. The oracle problem arises by imposing these as additional constraints in (2):\nOracle Problem: Minimize: \u2016L\u2016\u2217 + \u03bb\u2016C\u20161,2 Subject to: M = L+ C; PU0(L) = L; PI0(C) = C.\n(7)\nThe problem is of course bounded (by zero), and is feasible, as (L0, C0) is a feasible solution. Thus, an optimal solution, denoted as L\u0302, C\u0302 exists. We show that (L\u0302, C\u0302) is an optimal solution to Outlier Pursuit.\nThe key is in developing subgradient optimality conditions for a pair that satisfies the additional side constraints of the oracle problem (7). Because of the additional constraints, there are more potential subgradients certifying optimality. The next lemma and definition, are key to the development of our optimality conditions.\nLemma 4: Let the pair (L\u2032, C \u2032) satsify L\u2032 + C \u2032 = M , PU0(L\u2032) = L\u2032, and PI0(C \u2032) = C \u2032. Denote the SVD of L\u2032 as L\u2032 = U \u2032\u03a3V \u2032\u22a4, and the column support of C \u2032 as I \u2032. Then U \u2032U \u2032\u22a4 = U0U\u22a40 , and I \u2032 \u2286 I0. Proof: The only thing we need to prove is that L\u2032 has a rank no smaller than U0. However, since PI0(C \u2032) = C \u2032, we must have PIc0(L\u2032) = PIc0(M), and thus the rank of L\u2032 is at least as large as PIc0(M), hence L\u2032 has a rank no smaller than U0.\nNext we define two operators that are closely related to the subgradient of \u2016L\u2032\u2016\u2217 and \u2016C \u2032\u20161,2. Definition 1: Let (L\u2032, C \u2032) satisfy L\u2032 + C \u2032 = M , PU0(L\u2032) = L\u2032, and PI0(C \u2032) = C \u2032. We define the\nfollowing:\nN(L\u2032) , U \u2032V \u2032\u22a4;\nG(C \u2032) ,\n{ H \u2208 Rm\u00d7n \u2223 \u2223 \u2223\n\u2223 PIc 0 (H) = 0; \u2200i \u2208 I \u2032 : Hi = C \u2032i \u2016C \u2032i\u20162 ; \u2200i \u2208 I0 \u2229 (I \u2032)c : \u2016Hi\u20162 \u2264 1 } ,\nwhere the SVD of L\u2032 is L\u2032 = U \u2032\u03a3V \u2032\u22a4, and the column support of C \u2032 is I \u2032. Further define the operator PT (L\u2032)(\u00b7) : Rm\u00d7n \u2192 Rm\u00d7n as\nPT (L\u2032)(X) = PU \u2032(X) + PV \u2032(X)\u2212PU \u2032PV \u2032(X). Now we present and prove the optimality condition (to Outlier Pursuit) for solutions (L,C) that have the correct column space and support for L and C, respectively. Theorem 3: Let (L\u2032, C \u2032) satisfy L\u2032 + C \u2032 = M , PU0(L\u2032) = L\u2032, and PI0(C \u2032) = C \u2032. Then (L\u2032, C \u2032) is an optimal solution of Outlier Pursuit if there exists Q \u2208 Rm\u00d7n that satisfies (a) PT (L\u2032)(Q) = N(L\u2032); (b) \u2016PT (L\u2032)\u22a5(Q)\u2016 \u2264 1; (c) PI0(Q)/\u03bb \u2208 G(C \u2032); (d) \u2016PIc 0 (Q)\u2016\u221e,2 \u2264 \u03bb. (8)\n7 If both inequalities are strict (dubbed Q strictly satisfies (8)), and PI0 \u2229 PV \u2032 = {0}, then any optimal solution will have the right column space, and column support. Proof: By standard convexity arguments [26], a feasible pair (L\u2032, C \u2032) is an optimal solution of Outlier Pursuit, if there exists a Q\u2032 such that\nQ\u2032 \u2208 \u2202\u2016L\u2032\u2016\u2217; Q\u2032 \u2208 \u03bb\u2202\u2016C \u2032\u20161,2. Note that (a) and (b) imply that Q \u2208 \u2202\u2016L\u2032\u2016\u2217. Furthermore, letting I \u2032 be the support of C \u2032, then by Lemma 4, I \u2032 \u2286 I0. Therefore (c) and (d) imply that\nQi = \u03bbC \u2032i \u2016C \u2032i\u20162 ; \u2200i \u2208 I \u2032;\nand \u2016Qi\u20162 \u2264 \u03bb; \u2200i 6\u2208 I \u2032,\nwhich implies that Q \u2208 \u03bb\u2202\u2016C \u2032\u20161,2. Thus, (L\u2032, C \u2032) is an optimal solution. The rest of the proof establishes that when (b) and (d) are strict, then any optimal solution (L\u2032\u2032, C \u2032\u2032) satisfies PU0(L\u2032\u2032) = L\u2032\u2032, and PI0(C \u2032\u2032) = C \u2032\u2032. We show that for any fixed \u2206 6= 0, (L\u2032+\u2206, C \u2032\u2212\u2206) is strictly worse than (L\u2032, C \u2032), unless \u2206 \u2208 PU0\u2229PI0 . Let W be such that \u2016W\u2016 = 1, \u3008W,PT (L\u2032)\u22a5(\u2206)\u3009 = \u2016PT (L\u2032)\u22a5\u2206\u2016\u2217, and PT (L\u2032)W = 0. Let F be such that\nFi = { \u2212\u2206i \u2016\u2206i\u20162 if i 6\u2208 I0, and \u2206i 6= 0 0 otherwise.\nThen PT (L\u2032)(Q) +W is a subgradient of \u2016L\u2032\u2016\u2217 and PI0(Q)/\u03bb+ F is a subgradient of \u2016C \u2032\u20161,2. Then we have\n\u2016L\u2032 +\u2206\u2016\u2217 + \u03bb\u2016C \u2032 \u2212\u2206\u20161,2 \u2265\u2016L\u2032\u2016\u2217 + \u03bb\u2016C \u2032\u20161,2+ < PT (L\u2032)(Q) +W,\u2206 > \u2212\u03bb < PI0(Q)/\u03bb+ F,\u2206 > =\u2016L\u2032\u2016\u2217 + \u03bb\u2016C \u2032\u20161,2 + \u2016PT (L\u2032)\u22a5(\u2206)\u2016\u2217 + \u03bb\u2016PIc0(\u2206)\u20161,2+ < PT (L\u2032)(Q)\u2212 PI0(Q),\u2206 > =\u2016L\u2032\u2016\u2217 + \u03bb\u2016C \u2032\u20161,2 + \u2016PT (L\u2032)\u22a5(\u2206)\u2016\u2217 + \u03bb\u2016PIc0(\u2206)\u20161,2+ < Q\u2212PT (L\u2032)\u22a5(Q)\u2212 (Q\u2212 PIc0(Q)),\u2206 > =\u2016L\u2032\u2016\u2217 + \u03bb\u2016C \u2032\u20161,2 + \u2016PT (L\u2032)\u22a5(\u2206)\u2016\u2217 + \u03bb\u2016PIc0(\u2206)\u20161,2+ < \u2212PT (L\u2032)\u22a5(Q),\u2206 > + < PIc0(Q),\u2206 > \u2265\u2016L\u2032\u2016\u2217 + \u03bb\u2016C \u2032\u20161,2 + (1\u2212 \u2016PT (L\u2032)\u22a5(Q)\u2016)\u2016PT (L\u2032)\u22a5(\u2206)\u2016\u2217 + (\u03bb\u2212 \u2016PIc0(Q)\u2016\u221e,2)\u2016PIc0(\u2206)\u20161,2 \u2265\u2016L\u2032\u2016\u2217 + \u03bb\u2016C \u2032\u20161,2,\nwhere the last inequality is strict unless\n\u2016PT (L\u2032)\u22a5(\u2206)\u2016\u2217 = \u2016PIc0(\u2206)\u20161,2 = 0. (9) Note that (9) implies that PT (L\u2032)(\u2206) = \u2206 and PI0(\u2206) = \u2206. Furthermore\nPI0(\u2206) = \u2206 = PT (L\u2032)(\u2206) = PU \u2032(\u2206) + PV \u2032PU \u2032\u22a5(\u2206) = PI0PU \u2032(\u2206) + PV \u2032PU \u2032\u22a5(\u2206), where the last equality holds because we can write PI0(\u2206) = \u2206. This leads to\nPI0PU \u2032\u22a5(\u2206) = PV \u2032PU \u2032\u22a5(\u2206). Lemma 4 implies PU \u2032 = PU0 , which means PU\u22a5(\u2206) \u2208 PI0\u2229PV \u2032 , and hence equal 0. Thus, \u2206 \u2208 PI0\u2229PU0 , which completes the proof.\nThus, the oracle problem determines a solution pair, (L\u0302, C\u0302), and then using this, Theorem 3 above, gives the conditions a dual certificate must satisfy. The rest of the proof seeks to build a dual certificate for the pair (L\u0302, C\u0302). To this end, The following two results are quite helpful in what follows. For the remainder of the paper, we use (L\u0302, C\u0302) to denote the dual pair that is the output of the oracle problem, and we assume that the SVD of L\u0302 is given as L\u0302 = U\u0302 \u03a3\u0302V\u0302 \u22a4.\n8 Lemma 5: There exists an orthonormal matrix V \u2208 Rr\u00d7n such that U\u0302 V\u0302 \u22a4 = U0V \u22a4 .\nIn addition, PT\u0302 (\u00b7) , PU\u0302(\u00b7) + PV\u0302 (\u00b7)\u2212 PU\u0302PV\u0302 (\u00b7) = PU0(\u00b7) + PV (\u00b7)\u2212 PU0PV (\u00b7).\nProof: Due to Lemma 4, we have U0U\u22a40 = U\u0302 U\u0302 \u22a4, hence U0 = U\u0302 U\u0302\u22a4U0. Letting V = V\u0302 U\u0302\u22a4U0, we\nhave U\u0302 V\u0302 \u22a4 = U0V \u22a4 , and V V \u22a4 = V\u0302 V\u0302 \u22a4. Note that U0U\u22a40 = U\u0302 U\u0302 \u22a4 leads to PU = PU\u0302 , and V V \u22a4 = V\u0302 V\u0302 \u22a4 leads to PV = PV\u0302 , so the second claim follows. Since L\u0302, C\u0302 is an optimal solution to Oracle Problem (7), there exists Q1, Q2, A\u2032 and B\u2032 such that\nQ1 + PU\u22a5 0 (A\u2032) = Q2 + PIc 0 (B\u2032),\nwhere Q1, Q2 are subgradients to \u2016L\u0302\u2016\u2217 and to \u03bb\u2016C\u0302\u20161,2, respectively. This means that Q1 = U0V \u22a4 +W for some orthonormal V and W such that PT\u0302 (W ) = 0, and Q2 = \u03bb(H\u0302 + Z) for some H\u0302 \u2208 G(C\u0302), and Z such that PI0(Z) = 0. Letting A = W + A\u2032, B = \u03bbZ +B\u2032, we have\nU0V \u22a4 + PU\u22a5\n0\n(A) = \u03bbH\u0302 + PIc 0 (B). (10)\nRecall that H\u0302 \u2208 G(C\u0302) means PI0(H\u0302) = H\u0302 and \u2016H\u0302\u2016\u221e,2 \u2264 1. Lemma 6: We have\nU0PI0(V \u22a4 ) = \u03bbPU0(H\u0302).\nProof: We have\nPU0PI0(U0V \u22a4 + PU\u22a5\n0\n(A)) = PU0PI0(U0V \u22a4 ) + PU0PI0(PU\u22a5\n0\n(A))\n= U0PI0(V \u22a4 ) + PU0PU\u22a5\n0 PI0(A) = U0PI0(V \u22a4 ).\nFurthermore, we have PU0PI0(\u03bbH\u0302 + PIc0(B)) = \u03bbPU0(H\u0302).\nThe lemma follows from (10)."}, {"heading": "B. Obtaining Dual Certificates for Outlier Pursuit", "text": "In this section we complete the proof of Theorem 1 by constructing a dual certificate to (L\u0302, C\u0302), the solution to the oracle problem. The conditions the dual certificate must satisfy are spelled out in Theorem 3. To understand the flow of the proof, it is instructive to consider the simpler case where the corrupted columns are assumed to be orthogonal to the column space of L0 which we seek to recover. Indeed, in that setting, we have V = V\u0302 = V , and moreover, straightforward algebra shows that we automatically satisfy the condition PI0 \u2229 PV = {0}. In the general case, however, we require an addition condition to be satisfied, in order to recover the same property. Moreover, considering that the columns of H are either zero, or defined as normalizations of the columns of matrix C (i.e., normalizations of outliers), that PU(H) = PV (H) = PT (H) = 0, is immediate, as is the condition that PI0(UV \u22a4) = 0. As a result, it is not hard to verify that the dual certificate for the orthogonal case is:\nQ = U0V \u22a4 0 + \u03bbH.\nWhile not required for the proof of our main results, we include the proof of the orthogonal case in Appendix I, as there we get a stronger necessary and sufficient condition for recovery.\nFor the general, non-orthogonal case, however, this certificate does not satisfy the conditions of Theorem 3. For instance, PV0(H) need no longer be zero, and hence checking PT (Q) = U0V \u22a40 may no longer\n9 hold. We correct for the effect of the non-orthogonality by modifying Q with matrices \u22061 and \u22062, which we define below.\nRecalling the definition of V from Lemma 5, define matrix G \u2208 Rr\u00d7r as\nG , PI0(V \u22a4 )(PI0(V \u22a4 ))\u22a4.\nThen we have\nG = \u2211\ni\u2208I0\n[(V \u22a4 )i][(V \u22a4 )i]\n\u22a4 n \u2211\ni=1\n[(V \u22a4 )i][(V \u22a4 )i] \u22a4 = V \u22a4 V = I,\nand hence \u2016G\u2016 \u2264 1. The following lemma bounds \u2016G\u2016 away from 1. Lemma 7: Let c = \u2016G\u2016 and define\n\u00b5 = max i\u2208Ic\n0\n|Ic0| r \u2016PIc 0 (V \u22a4 )ei\u20162.\nWe have the following: (1) c \u2264 \u03bb2\u03b3n; (2) \u00b5 \u2264 \u00b5. Proof: We have\nc = \u2016U0PI0(V \u22a4 )(PI0(V \u22a4 ))\u22a4U\u22a40 \u2016 = \u2016[U0PI0(V \u22a4 )][U0PI0(V \u22a4 )]\u22a4\u2016,\ndue to the fact that U0 is orthonormal. By Lemma 6, this implies\nc = \u2016[\u03bbPU0(H\u0302)][\u03bbPU0(H\u0302)]\u22a4\u2016 = \u03bb2\u2016 \u2211\ni\u2208I0\nPU0(H\u0302i)PU0(H\u0302i)\u22a4\u2016 \u2264 \u03bb2|I0|.\nHere, the inequality holds because \u2016PU0(H\u0302i)\u20162 \u2264 1 implies \u2016PU0(H\u0302i)PU0(H\u0302i)\u22a4\u2016 \u2264 1. Thus, we proved the first claim.\nRecall L0 = U0\u03a30V \u22a40 , and\n\u00b5 = max i\u2208Ic\n0\n|Ic0| r \u2016PIc 0 (V \u22a40 )ei\u20162.\nThus it suffices to show that for fixed i \u2208 I0 the following holds\n\u2016PIc 0 (V \u22a4 )ei\u2016 \u2264 \u2016PIc 0 (V \u22a40 )ei\u2016.\nNote that PIc 0 (V \u22a4 ) and PIc 0 (V \u22a40 ) span the same row space. Thus, due to the fact that PIc0(V \u22a40 ) is orthonormal, we have PIc 0 (V \u22a4 ) is row-wise full rank. Since 0 PIc 0 (V \u22a4 )PIc 0 (V \u22a4 )\u22a4 = I \u2212 G, there exists a symmetric, invertible matrix N \u2208 Rr\u00d7r, such that\n\u2016N\u2016 \u2264 1; and N2 = PIc 0 (V \u22a4 )PIc 0 (V \u22a4 )\u22a4,\nwhich implies that N\u22121PIc 0 (V \u22a4 ) is orthonormal and span the same row space as PIc 0 (V \u22a4 ), and hence span the same row space as PIc 0 (V \u22a40 ). Note that PIc0(V \u22a40 ) is also orthonormal, which implies there exists an orthonormal matrix M \u2208 Rr\u00d7r, such that MN\u22121PIc\n0 (V\n\u22a4 ) = PIc 0 (V \u22a40 ).\nWe have\n\u2016PIc 0 (V \u22a4 )ei\u20162 = \u2016M\u22a4NPIc 0 (V \u22a40 )ei\u20162 \u2264 \u2016M\u22a4\u2016\u2016N\u2016\u2016PIc0(V \u22a40 )ei\u20162 \u2264 \u2016PIc0(V \u22a40 )ei\u20162.\nThe second claim is thus established. Lemma 8: If c < 1, then the following operation PVPIc0PV is an injection from PV to PV , and its inverse operation is I + \u2211\u221e\ni=1(PVPI0PV )i.\n10\nProof: Fix matrix X \u2208 Rm\u00d7n such that \u2016X\u2016 = 1, we have that PVPI0PV (X)\n=PVPI0(XV V \u22a4 ) =PV (XV PI0(V \u22a4 )) =XV PI0(V \u22a4 )V V \u22a4 =XV (PI0(V \u22a4 )V )V \u22a4 =XVGV \u22a4 ,\nwhich leads to \u2016PVPI0PV (X)\u2016 \u2264 c. Since c < 1, [I + \u2211\u221e i=1(PVPI0PV )i](X) is well defined, and has a spectral norm not larger than 1/(1\u2212 c).\nNote that we have PVPIc0PV = PV (I \u2212PV PI0PV ),\nthus for any X \u2208 PV the following holds\nPVPIc0PV [I + \u221e \u2211\ni=1\n(PVPI0PV )i](X)\n=PV (I \u2212 PVPI0PV )[I + \u221e \u2211\ni=1\n(PV PI0PV )i](X)\n=PV (X) = X, which establishes the lemma.\nNow we define the matrices \u22061 and \u22062 used to construct the dual certificate. As the proof reveals, they are designed precisely as \u201ccorrections\u201d to guarantee that the dual certificate satisfies the required constraints of Theorem 3.\nDefine \u22061 and \u22062 as follows:\n\u22061 , \u03bbPU0(H) = U0PI0(V \u22a4 ); (11)\n\u22062 , PU\u22a5 0 PIc 0 PV [I +\n\u221e \u2211\ni=1\n(PVPI0PV )i]PV (\u03bbH\u0302)\n= PIc 0 PV [I +\n\u221e \u2211\ni=1\n(PVPI0PV )i]PV PU\u22a5 0 (\u03bbH\u0302). (12)\nNote here the equality holds since PV ,PI0 ,PIc0 are all right multiplying a matrix, while PU\u22a50 is left multiplying a matrix.\nTheorem 4: Assume c < 1. Let\nQ , U0V \u22a4 + \u03bbH\u0302 \u2212\u22061 \u2212\u22062.\nIf \u03b3\n1\u2212 \u03b3 \u2264 (1\u2212 c)2 (3\u2212 c)2\u00b5r ,\nand (1\u2212 c) \u221a \u00b5r\n1\u2212\u03b3 \u221a n(1\u2212 c\u2212 \u221a \u03b3 1\u2212\u03b3\u00b5r) \u2264 \u03bb \u2264 1\u2212 c (2\u2212 c)\u221an\u03b3 ,\n11\nthen Q satisfies Condition (8) (i.e., it is the dual certificate). If all inequalities hold strictly, then Q strictly satisfies (8). Proof: Note that c < 1 implies PV \u2229 PI0 = {0}. Hence it suffices to show that Q simultaneously satisfies\nPU\u0302(Q) = U\u0302 V\u0302 \u22a4; PV\u0302 (Q) = U\u0302 V\u0302 \u22a4; PI0(Q) = \u03bbH\u0302 ; \u2016PT\u0302\u22a5(Q)\u2016 \u2264 1; \u2016PIc 0 (Q)\u2016\u221e,2 \u2264 \u03bb.\nStep 1: We have\nPU\u0302(Q) = PU0(Q) = PU0(U0V \u22a4 + \u03bbH\u0302 \u2212\u22061 \u2212\u22062)\n= UV \u22a4 + \u03bbPU0(H\u0302)\u2212PU0(\u22061)\u2212PU0(\u22062) = UV \u22a4 = U\u0302 V\u0302 \u22a4.\nStep 2: We have\nPV\u0302 (Q) = PV (Q) = PV (U0V \u22a4 + \u03bbH\u0302 \u2212\u22061 \u2212\u22062)\n= U0V \u22a4 + PV (\u03bbH\u0302)\u2212 PV (\u03bbPU0(H\u0302))\u2212 PV (PV [I +\n\u221e \u2211\ni=1\n(PV PIPV )i]PVPU\u22a5 0 (\u03bbH\u0302))\n= U0V \u22a4 + PV (PU\u22a5\n0\n(\u03bbH\u0302))\u2212PV PIc0PV [I + \u221e \u2211\ni=1\n(PVPIPV )i]PVPU\u22a5 0 (\u03bbH\u0302)\n(a) = U0V \u22a4 + PV (PU\u22a5\n0 (\u03bbH\u0302))\u2212 PV (PU\u22a5 0\n(\u03bbH\u0302)) = U0V \u22a4 = U\u0302 V\u0302 \u22a4.\nHere, (a) holds since on PV , [I + \u2211\u221e i=1(PVPIPV )i] is the inverse operation of PVPIc0PV . Step 3: We have\nPI0(Q) =PI0(U0V \u22a4 + \u03bbH\u0302 \u2212\u22061 \u2212\u22062)\n=U0PI0(V \u22a4 ) + \u03bbH\u0302 \u2212 PI0(U0PI0(V \u22a4 ))\u2212 PI0PIc0PV [I +\n\u221e \u2211\ni=1\n(PVPIPV )i]PVPU\u22a5 0 (\u03bbH\u0302)\n=\u03bbH\u0302.\nStep 4: We need a lemma first. Lemma 9: Given X \u2208 Rm\u00d7n such that \u2016X\u2016 = 1, we have \u2016PIc 0 PV (X)\u2016 \u2264 1.\nProof: By definition PIc 0 PV (X) = XV PIc0(V \u22a4 ).\nFor any z \u2208 Rn such that \u2016z\u20162 = 1, we have\n\u2016XV PIc 0 (V \u22a4 )z\u20162 = \u2016XV V \u22a4PIc 0 (z)\u20162 \u2264 \u2016X\u2016\u2016V V \u22a4\u2016\u2016PIc 0 (z)\u20162 \u2264 1,\nwhere PIc 0 (z) represents that for i \u2208 I set zi to zero, and in the last inequality we used the fact that \u2016X\u2016 = 1. Note that this holds for any z, hence by the definition of spectral norm (as the \u21132 operator norm), we establish the lemma.\n12\nNow we continue on Step 4. We have\nPT\u0302\u22a5(Q) =PT\u0302\u22a5(U0V \u22a4 + \u03bbH\u0302 \u2212\u22061 \u2212\u22062)\n=P V \u22a5PU\u22a5 0 (\u03bbH\u0302)\u2212 P V \u22a5PU\u22a5 0 (PIc 0 PV [I +\n\u221e \u2211\ni=1\n(PVPIPV )i]PVPU\u22a5 0 (\u03bbH\u0302))\n=P V \u22a5PU\u22a5 0 (\u03bbH\u0302)\u2212 PU\u22a5 0 P V \u22a5PIc 0 PV [I +\n\u221e \u2211\ni=1\n(PVPIPV )i]PV (\u03bbH\u0302).\nLet v = \u2016\u03bbH\u0302\u2016. Recall that we have shown v \u2264 \u03bb \u221a |I0|. Thus we have \u2016PV \u22a5PU\u22a50 (\u03bbH\u0302)\u2016 \u2264 v. Furthermore, we have the following\n\u2016PV (\u03bbH\u0302)\u2016 \u2264 v; \u2016[I + \u221e \u2211\ni=1\n(PVPI0PV )i]PV (\u03bbH\u0302)\u2016 \u2264 v/(1\u2212 c);\n\u2016PIc 0 PV [I +\n\u221e \u2211\ni=1\n(PVPI0PV )i]PV (\u03bbH\u0302)\u2016 \u2264 v/(1\u2212 c);\n\u2016PU\u22a5 0 P V \u22a5PIc 0 PV [I +\n\u221e \u2211\ni=1\n(PVPI0PV )i]PV (\u03bbH\u0302)\u2016 \u2264 v/(1\u2212 c);\nThus we have that \u2016PT\u0302\u22a5(Q)\u2016 \u2264 2\u2212 c 1\u2212 c\u03bb \u221a |I0|.\nNote that since \u03bb \u2264 1\u2212 c\n(2\u2212 c)\u221an\u03b3 ,\nwe have \u2016PT\u0302\u22a5(Q)\u2016 \u2264 1.\nThe inequality will be strict if\n\u03bb < 1\u2212 c\n(2\u2212 c)\u221an\u03b3 .\nStep 5: Recall from the proof of Lemma 8 that\nPVPI0PV (X) = XVGV \u22a4 .\nHence note that (PVPI0PV )i = (PVPI0PV )(PVPI0PV )i\u22121 and V \u22a4 V = I , by induction we have\n(PVPI0PV )i(X) = XVGiV \u22a4 .\nNow we expand \u22062:\n\u22062 = PU\u22a5PIc0PV [I + \u221e \u2211\ni=1\n(PVPI0PV )i]PV (\u03bbH\u0302)\n=(I \u2212 U0U\u22a40 )(\u03bbH\u0302)V V \u22a4 [1 +\n\u221e \u2211\ni=1\nV GiV \u22a4 ]V PIc\n0 (V\n\u22a4 ).\n13\nThus, we have\n\u2016\u22062ei\u20162\n\u2264\u2016(I \u2212 U0U\u22a40 )\u2016\u2016(\u03bbH)\u2016\u2016V V \u22a4\u2016\u20161 +\n\u221e \u2211\ni=1\nV GiV \u22a4\u2016\u2016V \u2016\u2016PIc\n0 (V\n\u22a4 )ei\u20162\n\u2264\u2016\u03bbH\u2016 1 1\u2212 c\n\u221a\n\u00b5r\nn\u2212 |I0|\n\u2264 \u03bb \u221a |I0| \u221a \u00b5r n\u2212|I0|\n1\u2212 c ,\nwhich implies\n\u2016\u22062\u2016\u221e,2 \u2264 \u03bb \u221a |I0| \u221a \u00b5r n\u2212|I0|\n1\u2212 c .\nNotice that\n\u2016PIc 0 (Q)\u2016\u221e,2 =\u2016PIc 0 (U0V \u22a4 + \u03bbH\u0302 \u2212\u22061 \u2212\u22062)\u2016\u221e,2\n=\u2016U0PIc 0 (V \u22a4 )\u2212\u22062\u2016\u221e,2 \u2264\u2016U0PIc 0 (V \u22a4 )\u2016\u221e,2 + \u2016\u22062\u2016\u221e,2\n\u2264 \u221a \u00b5r\nn\u2212 |I0| +\n\u03bb \u221a |I0| \u221a \u00b5r\nn\u2212|I0|\n1\u2212 c .\nNote that solving\n\u221a\n\u00b5r\nn\u2212 |I0| +\n\u03bb \u221a |I0| \u221a \u00b5r\nn\u2212|I0|\n1\u2212 c \u2264 \u03bb\n\u21d0\u21d2 \u03bb\n\n1\u2212\n\u221a\n\u03b3\n1\u2212\u03b3\u00b5r\n1\u2212 c\n  \u2265 \u221a\n\u00b5r\nn(1\u2212 \u03b3)\n\u21d0\u21d2 \u03bb \u2265 (1\u2212 c)\n\u221a\n\u00b5r\n1\u2212\u03b3 \u221a n(1\u2212 c\u2212 \u221a \u03b3 1\u2212\u03b3\u00b5r) ,\nas long as 1\u2212c\u2212 \u221a \u03b3\n1\u2212\u03b3\u00b5r > 0 (which will be proved in step 6). Again if the last inequality holds strictly,\nwe then have \u2016PIc 0 (Q)\u2016\u221e,2 < \u03bb.\nStep 6: Finally we show that such \u03bb is possible, which is equivalent to show that\n(1\u2212 c) \u221a \u00b5r\n1\u2212\u03b3 \u221a n(1\u2212 c\u2212 \u221a \u03b3 1\u2212\u03b3\u00b5r) \u2264 1\u2212 c (2\u2212 c)\u221an\u03b3\n\u21d0\u21d2 (2\u2212 c) \u221a \u03b3 1\u2212 \u03b3\u00b5r \u2264 1\u2212 c\u2212 \u221a \u03b3 1\u2212 \u03b3\u00b5r\n\u21d0\u21d2 \u03b3 1\u2212 \u03b3 \u2264 (1\u2212 c)2 (3\u2212 c)2\u00b5r .\n14\nObserve that under this condition, 1\u2212c\u2212 \u221a \u03b3\n1\u2212\u03b3\u00b5r > 0 holds. Note that if the last inequality holds strictly,\nthen so does the first. With the construction of the dual certificate complete, we can establish Theorem 1 from the following corollary. Corollary 1: Let \u03b3 \u2264 \u03b3\u2217, then Outlier Pursuit, with \u03bb = 3\n7 \u221a \u03b3\u2217n , strictly succeeds if\n\u03b3\u2217\n1\u2212 \u03b3\u2217 \u2264 9 121\u00b5r .\nProof: First note that \u03bb = 3 7 \u221a \u03b3\u2217n and \u03b3 \u2264 \u03b3\u2217 imply that\n\u03bb \u2264 3 7 \u221a \u03b3n ,\nwhich by Lemma 7 leads to\nc \u2264 \u03bb2\u03b3n < 1 4 .\nThus, it suffices to check that \u03b3 and \u03bb satisfies the conditions of Theorem 4, namely\n\u03b3 1\u2212 \u03b3 < (1\u2212 c)2 (3\u2212 c)2\u00b5r ,\nand (1\u2212 c) \u221a \u00b5r\n1\u2212\u03b3 \u221a n(1\u2212 c\u2212 \u221a \u03b3 1\u2212\u03b3\u00b5r) < \u03bb < 1\u2212 c (2\u2212 c)\u221an\u03b3 .\nSince c < 1/4, we have\n\u03b3 1\u2212 \u03b3 \u2264 \u03b3\u2217 1\u2212 \u03b3\u2217 \u2264 9 121\u00b5r = (1\u2212 1/4)2 (3\u2212 1/4)2\u00b5r < (1\u2212 c)2 (3\u2212 c)2\u00b5r ,\nwhich proves the first condition.\nNext observe that (1\u2212c)\n\u221a \u00b5r\n1\u2212\u03b3\u221a n(1\u2212c\u2212\n\u221a \u03b3\n1\u2212\u03b3 \u00b5r)\n, as a function of c, \u03b3, (\u00b5r) is strictly increasing in c, (\u00b5r), and \u03b3;\nand \u00b5r \u2264 (1\u2212c)2(1\u2212\u03b3) (3\u2212c)2\u03b3 thus\n(1\u2212 c) \u221a \u00b5r\n1\u2212\u03b3 \u221a n(1\u2212 c\u2212 \u221a \u03b3 1\u2212\u03b3\u00b5r) <\n(1\u2212 c) \u221a (1\u2212c)2 (3\u2212c)2\u03b3\u221a\nn(1\u2212 c\u2212 1\u2212c 3\u2212c)\n= 3 \u221a 1 + \u03b3/(1\u2212 \u03b3) 7 \u221a n \u2264 3 \u221a 1 + \u03b3\u2217/(1\u2212 \u03b3\u2217) 7 \u221a n = \u03bb.\nSimilarly, 1\u2212c (2\u2212c)\u221an\u03b3 is strictly decreasing in c and \u03b3, which implies that\n1\u2212 c (2\u2212 c)\u221an\u03b3 > 1\u2212 1/4 (2\u2212 1/4)\u221an\u03b3\u2217 = \u03bb.\n15\nV. PROOF OF THEOREM 2\nIn practice, the observed matrix may be a noisy copy of M . In this section we investigate this noisy case and show that the proposed method, with minor modification, is robust to noise. Specifically, we observe M \u2032 = M+N for some unknown N , and we want to approximately recover U0 and I0. This leads to the following formulation that replaces the equality constraint M = L+ C with a norm inequality.\nMinimize: \u2016L\u2016\u2217 + \u03bb\u2016C\u20161,2; Subject to: \u2016M \u2032 \u2212 L\u2212 C\u2016 \u2264 \u01eb. (13) In fact, under the essentially equivalent condition as that of the noiseless case, the optimal solution of (13), will be \u201cclose\u201d to a pair that has the correct column space and column support.\nTheorem 5: Let L\u2032, C \u2032 be an optimal solution of (13). Suppose \u2016N\u2016F \u2264 \u01eb, \u03bb < 1, and c < 1/4. Let M = L\u0302+ C\u0302 where PU(L\u0302) = L\u0302 and PI0(C\u0302) = C\u0302. If there exists a Q such that\nPT (L\u0302)(Q) = N(L\u0302); \u2016PT (L\u0302)\u22a5(Q)\u2016 \u2264 1/2; PI0(Q)/\u03bb \u2208 G(C\u0302); \u2016PIc0(Q)\u2016\u221e,2 \u2264 \u03bb/2. (14) then there exists a pair (L\u0303, C\u0303) such that M = L\u0303+ C\u0303, L\u0303 \u2208 PU0 , C\u0303 \u2208 PI0 and\n\u2016L\u2032 \u2212 L\u0303\u2016F \u2264 20 \u221a n\u01eb; \u2016C \u2032 \u2212 C\u0303\u2016F \u2264 18 \u221a n\u01eb.\nProof: Let V be as defined before. We establish the following lemma first. Lemma 10: Recall that c = \u2016G\u2016 where G = PI0(V \u22a4 )PI0(V \u22a4 )\u22a4. We have\n\u2016PI0PVPI0(X)\u2016F \u2264 c\u2016X\u2016F . Proof: Let T \u2208 Rn\u00d7n be such that\nTij =\n{\n1 if i = j, i \u2208 I; 0 otherwise.\nWe then expand PI0PVPI0(X), which equals XTV V \u22a4 T = XTV V \u22a4 T\u22a4 = X(TV )(TV )\u22a4 = XPI0(V \u22a4 )\u22a4PI0(V \u22a4 ). The last equality follows from (TV )\u22a4 = PI0(V \u22a4 ). Since c = \u2016G\u2016 where G = PI0(V \u22a4 )PI0(V \u22a4 )\u22a4, we have \u2016PI0(V \u22a4 )\u22a4PI0(V \u22a4 )\u2016 = \u2016PI0(V \u22a4 )PI0(V \u22a4 )\u22a4\u2016 = c. Now consider the ith row of X , denoted as xi. Since \u2016PI0(V \u22a4 )\u22a4PI0(V \u22a4 )\u2016 = c, we have\n\u2016xiPI0(V \u22a4 )\u22a4PI0(V \u22a4 )\u201622 \u2264 c2\u2016xi\u201622.\nThe lemma holds from the following inequality. \u2016PI0PVPI0(X)\u20162F = \u2016XPI0(V \u22a4 )\u22a4PI0(V \u22a4 )\u20162F = \u2211\ni\n\u2016xiPI0(V \u22a4 )\u22a4PI0(V \u22a4 )\u201622 \u2264 c2 \u2211 \u2016xi\u201622 = c2\u2016X\u20162F .\nLet NL = L\u2032 \u2212 L\u0302, and NC = C \u2032 \u2212 C\u0302. Thus N = NC +NL, and recall that \u2016N\u2016F \u2264 \u01eb. Further, define N+L = NL \u2212 PI0PU0(NL), N+C = NC \u2212 PI0PU0(NC), and N+ = N \u2212 PI0PU0(N). Observe that for any A, \u2016(I \u2212PI0PU0)(A)\u2016F \u2264 \u2016A\u2016F .\nChoosing same W and F as in the proof of Theorem 3, we have\n\u2016L\u0302\u2016\u2217 + \u03bb\u2016C\u0302\u20161,2 \u2265\u2016L\u2032\u2016\u2217 + \u03bb\u2016C \u2032\u20161,2 \u2265\u2016L\u0302\u2016\u2217 + \u03bb\u2016C\u0302\u20161,2 + \u3008PT (L\u0302)(Q) +W,NL\u3009+ \u03bb\u3008PI0(Q)/\u03bb+ F,NC\u3009 =\u2016L\u0302\u2016\u2217 + \u03bb\u2016C\u0302\u20161,2 + \u2016PT (L\u0302)\u22a5(NL)\u2016\u2217 + \u03bb\u2016PIc0(NC)\u20161,2 + \u3008PT (L\u0302)(Q), NL\u3009+ \u3008PI0(Q), NC\u3009 =\u2016L\u0302\u2016\u2217 + \u03bb\u2016C\u0302\u20161,2 + \u2016PT (L\u0302)\u22a5(NL)\u2016\u2217 + \u03bb\u2016PIc0(NC)\u20161,2 \u2212 \u3008PT (L\u0302)\u22a5(Q), NL\u3009 \u2212 \u3008PIc0(Q), NC\u3009+ \u3008Q,NL +NC\u3009 \u2265\u2016L\u0302\u2016\u2217 + \u03bb\u2016C\u0302\u20161,2 + (1\u2212 \u2016PT (L\u0302)\u22a5(Q)\u2016)\u2016PT (L\u0302)\u22a5(NL)\u2016\u2217 + (\u03bb\u2212 \u2016PIc0(Q)\u2016\u221e,2)\u2016PIc0(NC)\u20161,2 + \u3008Q,N\u3009 \u2265\u2016L\u0302\u2016\u2217 + \u03bb\u2016C\u0302\u20161,2 + (1/2)\u2016PT (L\u0302)\u22a5(NL)\u2016\u2217 + (\u03bb/2)\u2016PIc0(NC)\u20161,2 \u2212 \u01eb\u2016Q\u2016F .\n16\nNote that \u2016Q\u2016\u221e,2 \u2264 \u03bb, hence \u2016Q\u2016F \u2264 \u221a n\u03bb. Thus we have\n\u2016P T (L\u0302) \u22a5(NL)\u2016F \u2264 \u2016PT (L\u0302)\u22a5(NL)\u2016\u2217 \u2264 2\u03bb \u221a n\u01eb; \u2016PIc 0 (NC)\u2016F \u2264 \u2016PIc 0 (NC)\u20161,2 \u2264 2 \u221a n\u01eb.\n(15)\nFurthermore\nPI0(N+C ) =PI0(NC)\u2212 PI0PU0PI0(NC) =PI0(N)\u2212PI0PT (L\u0302)\u22a5(NL)\u2212PI0PT (L\u0302)(NL)\u2212PI0PU0PI0(NC) =PI0(N)\u2212PI0PT (L\u0302)\u22a5(NL)\u2212PI0PT (\u2206) + PI0PT (L\u0302)(NC)\u2212 PI0PU0PI0(NC) =PI0(N)\u2212PI0PT (L\u0302)\u22a5(NL)\u2212PI0PT (L\u0302)(\u2206) + PI0PT (L\u0302)PIc0(NC)\n+ PI0PT (L\u0302)PI0(NC)\u2212 PI0PU0PI0(NC) (a) =PI0(N)\u2212PI0PT (L\u0302)\u22a5(NL)\u2212PI0PT (L\u0302)(\u2206) + PI0PT (L\u0302)PIc0(NC) + PI0PT (L\u0302)PI0(N + C )\n(b) =PI0(N)\u2212PI0PT (L\u0302)\u22a5(NL)\u2212PI0PT (L\u0302)(\u2206) + PI0PT (L\u0302)PIc0(NC) + PI0PVPI0(N + C ).\n(16)\nHere (a) holds due to the following\nPI0PT (L\u0302)PI0(N+C ) = PI0PT (L\u0302)PI0(NC)\u2212PI0PT (L\u0302)PI0(PI0PU0(NC)) = PI0PT (L\u0302)PI0(NC)\u2212PI0PU0PI0(NC), and (b) holds since by definition, each column of N+C is orthogonal to U , hence PU0PI0(N+C ) = 0. Thus, Equation (16) leads to\n\u2016PI0(N+C )\u2016F \u2264\u2016PI0(N)\u2212 PI0PT (L\u0302)(N)\u2016F + \u2016PI0PT (L\u0302)\u22a5(NL)\u2016F + \u2016PI0PT (L\u0302)PIc0(NC)\u2016F + \u2016PI0PVPI0(N + C )\u2016F \u2264\u2016N\u2016F + \u2016PT (L\u0302)\u22a5(NL)\u2016F + \u2016PIc0(NC)\u2016F + c\u2016PI0(N + C )\u2016F\n\u2264(1 + 2\u03bb\u221an+ 2\u221an)\u01eb+ c\u2016PI0(N+C )\u2016F . This implies that\n\u2016PI0(N+C )\u2016F \u2264 (1 + 2\u03bb \u221a n + 2 \u221a n)\u01eb/(1\u2212 c).\nNow using the fact that \u03bb < 1, and c < 1/4, we have\n\u2016N+C \u2016F = \u2016PIc0(NC) + PI0(N+C )\u2016F \u2264 \u2016PIc0(NC)\u2016F + \u2016PI0(N+C )\u2016F \u2264 9 \u221a n\u01eb.\nNote that N+C = (I\u2212PI0PU0)(C \u2032\u2212 C\u0302) = C \u2032\u2212 [C\u0302 +PI0PU0(C \u2032\u2212 C\u0302)]. Letting C\u0303 = C\u0302+PI0PU0(C \u2032\u2212 C\u0302), we have C\u0303 \u2208 PI0 and \u2016C\u2212C\u0303\u2016F \u2264 9 \u221a n\u01eb. Let L\u0303 = L\u0302\u2212PI0PU0(C \u2032\u2212C\u0302), we have that L\u0303, C\u0303 is a successful decomposition, and \u2016L\u2032 \u2212 L\u0303\u2016F \u2264 \u2016N\u2016F + \u2016C \u2032 \u2212 C\u0303\u2016F \u2264 10 \u221a n\u01eb.\nRemark: From the proof of Theorem 4, we have that the Condition (14) holds when\n\u03b3 1\u2212 \u03b3 \u2264 (1\u2212 c)2\n(9\u2212 4c)2\u00b50r and\n2(1\u2212 c) \u221a \u00b50r\n1\u2212\u03b3 \u221a n(1\u2212 c\u2212 \u221a \u03b3 1\u2212\u03b3\u00b50r) \u2264 \u03bb \u2264 1\u2212 c 2(2\u2212 c)\u221an\u03b3 .\nFor example, one can take\n\u03bb =\n\u221a 9 + 1024\u00b50r\n14 \u221a n\n,\n17\nand all conditions of Theorem 5 hold when \u03b3\n1\u2212 \u03b3 \u2264 9 1024\u00b50r .\nThis establishes Theorem 2.\nVI. IMPLEMENTATION ISSUE AND NUMERICAL EXPERIMENTS\nWhile minimizing the nuclear norm is known to be a semi-definite program, and can be solved using a general purpose SDP solver such as Yalmip or Sedumi, such a method does not scale well to large data-sets. In fact, the computational time becomes prohibitive even for modest problem sizes as small as hundreds of variables. Recently, a family of optimization algorithms known as proximal gradient algorithms have been proposed to solve optimization problems of the form\nminimize: g(x), subject to: A(x) = b, of which Outlier Pursuit is a special case. It is known that such algorithms converges with a rate of O(k\u22122), and significantly outperform interior point methods for solving SDPs in practice. Following this paradigm, we solve Outlier Pursuit with the following algorithm. The validity of the algorithm follows easily form [27], [28], see also [29].\nInput: M \u2208 Rm\u00d7n, \u03bb, \u03b4 := 10\u22125, \u03b7 := 0.9, \u00b50 := 0.99\u2016M\u2016F . 1) L\u22121, L0 := 0m\u00d7n; C\u22121, C0 := 0m\u00d7n, t\u22121, t0 := 1; \u00b5\u0304 = \u03b4\u00b5; 2) while not converged do 3) Y Lk := Lk + tk\u22121\u22121 tk\n(Lk \u2212 Lk\u22121), Y Ck := Ck + tk\u22121\u22121tk (Ck \u2212 Ck\u22121); 4) GLk := Y L k \u2212 12 ( Y Lk + Y C k \u2212M ) ; GCk := Y C k \u2212 12 ( Y Lk + Y C k \u2212M )\n; 5) (U, S, V ) := svd(GLk ); Lk+1 := UL\u00b5k\n2 (S)V ; 6) Ck+1 := C\u03bb\u00b5k\n2\n(GCk );\n7) tk+1 := 1+ \u221a 4t2 k +1\n2 ; \u00b5k+1 := max(\u03b7\u00b5k.\u00b5\u0304); k ++;\n8) end while Output: L := Lk, C = Ck.\nHere, L\u01eb(S) is the diagonal soft-thresholding operator: i.e., if |Sii| \u2264 \u01eb, then it is set to zero, otherwise, let Sii := Sii \u2212 \u01eb \u00b7 sgn(Sii). Similarly, C\u01eb(C) is the column-wise thresholding operator: i.e., set Ci to zero if \u2016Ci\u20162 \u2264 \u01eb, otherwise let Ci := Ci \u2212 \u01ebCi/\u2016Ci\u20162.\nWe explore the performance of Outlier Pursuit on some synthetic and real-world data, and find that its performance is quite promising.1 Our first experiment investigates the phase-transition property of Outlier Pursuit, using randomly generated synthetic data. Fix n = p = 400. For different r and number of outliers \u03b3n, we generated matrices A \u2208 Rp\u00d7r and B \u2208 R(n\u2212\u03b3n)\u00d7r where each entry is an independent N (0, 1) random variable, and then set L\u2217 := A\u00d7B\u22a4 (the \u201cclean\u201d part of M). Outliers, C\u2217 \u2208 R\u03b3n\u00d7p are generated either neutrally, where each entry of C\u2217 is iid N (0, 1), or adversarial, where every column is an identical copy of a random Gaussian vector. Outlier Pursuit succeeds if C\u0302 \u2208 PI , and L\u0302 \u2208 PU .\nFigure 1 shows the phase transition property. We represent success in gray scale, with white denoting success, and black failure. When outliers are random (easier case) Outlier Pursuit succeeds even when r = 20 with 100 outliers. In the adversarial case, Outlier Pursuit succeeds when r \u00d7 \u03b3 \u2264 c, and fails otherwise, consistent with our theory\u2019s predictions. We then fix r = \u03b3n = 5 and examine the outlier identification ability of Outlier Pursuit with noisy observations. We scale each outlier so that the \u21132 distance of the outlier to the span of true samples equals a pre-determined value s. Each true sample is thus corrupted with a Gaussian random vector with an \u21132 magnitude \u03c3. We perform (noiseless) Outlier\n1We have learned that [30] has also performed some numerical experiments minimizing \u2016 \u00b7 \u2016\u2217 + \u03bb\u2016 \u00b7 \u20161,2, and found promising results.\n18\nPursuit on this noisy observation matrix, and claim that the algorithm successfully identifies outliers if for the resulting C\u0302 matrix, \u2016C\u0302j\u20162 < \u2016C\u0302i\u20162 for all j 6\u2208 I and i \u2208 I, i.e., there exists a threshold value to separate out outliers. Figure 1 (c) shows the result: when \u03c3/s \u2264 0.3 for the identical outlier case, and \u03c3/s \u2264 0.7 for the random outlier case, Outlier Pursuit correctly identifies the outliers.\nWe further study the case of decomposing M under incomplete observation, which is motivated by robust collaborative filtering: we generate M as before, but only observe each entry with a given probability (independently). Letting \u2126 be the set of observed entries, we solve\nMinimize: \u2016L\u2016\u2217 + \u03bb\u2016C\u20161,2; Subject to: P\u2126(L+ C) = P\u2126(M). (17) The same success condition is used. Figure 2 shows a very promising result: the successful decomposition rate under incomplete observation is close the the complete observation case even only 30% of entries are observed. Given this empirical result, a natural direction of future research is to understand theoretical guarantee of (17) in the incomplete observation case.\nNext we report some experimental results on the USPS digit data-set. The goal of this experiment is to show that Outlier Pursuit can be used to identify anomalies within the dataset. We use the data from [31], and construct the observation matrix M as containing the first 220 samples of digit \u201c1\u201d and the last 11 samples of \u201c7\u201d. The learning objective is to correctly identify all the \u201c7\u2019s\u201d. Note that throughout the experiment, label information is unavailable to the algorithm, i.e., there is no training stage. Since the columns of digit \u201c1\u201d are not exactly low rank, an exact decomposition is not possible. Hence, we use the \u21132 norm of each column in the resulting C matrix to identify the outliers: a larger \u21132 norm means that the sample is more likely to be an outlier \u2014 essentially, we apply thresholding after C is obtained.\n19\nFigure 3(a) shows the \u21132 norm of each column of the resulting C matrix. We see that all \u201c7\u2019s\u201d are indeed identified. However, two \u201c1\u201d samples (columns 71 and 137) are also identified as outliers, due to the fact that these two samples are written in a way that is different from the rest \u201c1\u2019s\u201d as showed in Figure 4. Under the same setup, we also simulate the case where only 80% of entries are observed. As Figure 3 (b) and (c) show, similar results as that of the complete observation case are obtained, i.e., all true \u201c7\u2019s\u201d and also \u201c1\u2019s\u201d No 71, No 177 are identified.\n(a) Complete Observation (b) Partial Obs. (one run) (c) Partial Obs. (average)\nVII. CONCLUSION AND FUTURE DIRECTION\nThis paper considers robust PCA from a matrix decomposition approach, and develops the algorithm: Outlier Pursuit. Under some mild conditions, we show that Outlier Pursuit can exactly recover the column support, and exactly identify outliers. This result is new, differing both from results in Robust PCA, and also from results using nuclear-norm approaches for matrix completion and matrix reconstruction. One central innovation we introduce is the use of an oracle problem. Whenever the recovery concept (in this case, column space) does not uniquely correspond to a single matrix (we believe many, if not most cases of interest, fit this description), the use of such a tool will be quite useful. Immediate goals for future work include considering specific applications, in particular, robust collaborative filtering (here, the goal is to decompose a partially observed column-corrupted matrix) and also obtaining tight bounds for outlier identification in the noisy case.\n20\nREFERENCES [1] I. T. Jolliffe. Principal Component Analysis. Springer Series in Statistics, Berlin: Springer, 1986. [2] P. J. Huber. Robust Statistics. John Wiley & Sons, New York, 1981. [3] L. Xu and A. L. Yuille. Robust principal component analysis by self-organizing rules based on statistical physics approach. IEEE\nTransactions on Neural Networks, 6(1):131\u2013143, 1995. [4] E. Cande\u0300s, X. Li, Y. Ma, and J. Wright. Robust pricipal component analysis? ArXiv:0912.3599, 2009. [5] S. J. Devlin, R. Gnanadesikan, and J. R. Kettenring. Robust estimation of dispersion matrices and principal components. Journal of\nthe American Statistical Association, 76(374):354\u2013362, 1981. [6] T. N. Yang and S. D. Wang. Robust algorithms for principal component analysis. Pattern Recognition Letters, 20(9):927\u2013933, 1999. [7] C. Croux and G. Hasebroeck. Principal component analysis based on robust estimators of the covariance or correlation matrix: Influence\nfunctions and efficiencies. Biometrika, 87(3):603\u2013618, 2000. [8] F. De la Torre and M. J. Black. Robust principal component analysis for computer vision. In Proceedings of the Eighth International\nConference on Computer Vision (ICCV\u201901), pages 362\u2013369, 2001. [9] F. De la Torre and M. J. Black. A framework for robust subspace learning. International Journal of Computer Vision, 54(1/2/3):117\u2013142,\n2003. [10] C. Croux, P. Filzmoser, and M. Oliveira. Algorithms for Projection\u2212Pursuit robust principal component analysis. Chemometrics and Intelligent Laboratory Systems, 87(2):218\u2013225, 2007. [11] S. C. Brubaker. Robust PCA and clustering on noisy mixtures. In Proceedings of the Nineteenth Annual ACM -SIAM Symposium on Discrete Algorithms, pages 1078\u20131087, 2009. [12] H. Xu, C. Caramanis, and S. Mannor. Principal component analysis with contaminated data: The high dimensional case. In Proceeding of the Twenty-third Annual Conference on Learning Theory, pages 490\u2013502, 2010. [13] E. J. Cande\u0300s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2):489\u2013509, 2006. [14] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization. To appear in SIAM Review, 2010. [15] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. Willsky. Rank-sparsity incoherence for matrix decomposition. ArXiv:0906.2220, 2009. [16] E. Cande\u0300s and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9:717\u2013772, 2009. [17] E. Cande\u0300s and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(2053-2080), 2010. [18] D. L. Donoho. Breakdown properties of multivariate location estimators. Qualifying paper, Harvard University, 1982. [19] R. Maronna. Robust M-estimators of multivariate location and scatter. The Annals of Statistics, 4:51\u201367, 1976. [20] V. Barnett. The ordering of multivariate data. Journal of Royal Statistics Society Series, A, 138:318\u2013344, 1976. [21] D. Titterington. Estimation of correlation coefficients by ellipsoidal trimming. Applied Statistics, 27:227\u2013234, 1978. [22] V. Barnett and T. Lewis. Outliers in Statistical Data. Wiley, New York, 1978. [23] A. Dempster and M. Gasko-Green. New tools for residual analysis. The Annals of Statistics, 9(5):945\u2013959, 1981. [24] S. J. Devlin, R. Gnanadesikan, and J. R. Kettenring. Robust estimation and outlier detection with correlation coefficients. Biometrika, 62:531\u2013545, 1975. [25] G. Li and Z. Chen. Projection-pursuit approach to robust dispersion matrices and principal components: Primary theory and monte carlo. Journal of the American Statistical Association, 80(391):759\u2013766, 1985. [26] R.T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, N.J., 1970. [27] P. Tsang. On accelerated proximal gradient methods for convex-concave optimization. Submitted to SIAM Journal on Optimizatio, 2008. [28] Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27(372-376), 1983. [29] J-F. Cai, E. Cande\u0300s, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20:1956\u20131982, 2008. [30] M. McCoy and J. Tropp. Personal Correspondence, October 2010. [31] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. the MIT Press, 2006.\nAPPENDIX I ORTHOGONAL CASE\nThis section investigates the special case where each outlier is orthogonal to the span of true samples, as stated in the following assumption.\nAssumption 1: For i \u2208 I0, j 6\u2208 I0, we have M\u22a4i Mj = 0. In the orthogonal case, we are able to derive a necessary and sufficient condition of Outlier Pursuit to succeed. Such condition is of course a necessary condition for Outlier Pursuit to succeed in the more general (non-orthogonal) case. Let\nH0 =\n{ (C0)i \u2016(C0)i\u20162 , if i \u2208 I0;\n0 otherwise.\n21\nTheorem 6: Under Assumption 1, Outlier Pursuit succeeds if and only if\n\u2016H0\u2016 \u2264 1/\u03bb; \u2016U0V \u22a40 \u2016\u221e,2 \u2264 \u03bb. (18) If both inequalities hold strictly, then Outlier Pursuit strictly succeeds.\nCorollary 2: If the outliers are generated adversarial, and Assumption 1 holds, then Outlier Pursuit succeeds (for some \u03bb\u2217) if and only if\n\u03b3 1\u2212 \u03b3 \u2264 1 \u00b5r .\nSpecifically, we can choose \u03bb\u2217 = \u221a\n\u00b5r+1 n ."}, {"heading": "A. Proof of Theorem 6", "text": "The proof consists of three steps. We first show that if Outlier Pursuit succeeds, then (L0, C0) must be an optimal solution to Outlier Pursuit. Then using subgradient condition of optimal solutions to convex programming, we show that the necessary and sufficient condition for (L0, C0) being optimal solution is the existence of a dual certificate Q. Finally, we show that the existence of Q is equivalent to Condition (18) holds. We devote a subsection for each step.\n1) Step 1: We need a technical lemma first. Lemma 11: Given A \u2208 Rm\u00d7n, we have\n\u2016PIc 0 (A)\u2016\u2217 \u2264 \u2016A\u2016\u2217.\nProof: Fix r \u2265 rank(A). It is known that \u2016A\u2016\u2217 has the following variational form (Lemma 5.1 of [14]):\n\u2016A\u2016\u2217 = Minimize:X\u2208Rm\u00d7r ,Y \u2208Rn\u00d7r 1\n2 (\u2016X\u20162F + \u2016Y \u20162F )\nSubject to: XY \u22a4 = A. (19)\nNote that for any XY \u22a4 = A, we have\nXY \u22a4 = X(PIc 0 (Y \u22a4)) = PIc(A),\nwhere Y is the matrix resulted by setting all rows of Y in I to zero. Thus, by variational form of \u2016PIc 0 (A)\u2016\u2217, and note that rank(PIc 0 (A)) \u2264 r, we have\n\u2016PIc 0 (A)\u2016\u2217 \u2264\n1 2 [\u2016X\u20162F + \u2016Y \u20162F ] \u2264 1 2 [\u2016X\u20162F + \u2016Y \u20162F ].\nNote this holds for any X, Y such that XY \u22a4 = A, the lemma follows from (19). Theorem 7: Under Assumption 1, for any L\u2032, C \u2032 such that L\u2032+C \u2032 = M , PI0(C \u2032) = C \u2032, and PU0(L\u2032) = L\u2032, we have \u2016L0\u2016\u2217 + \u03bb\u2016C0\u20161,2 \u2264 \u2016L\u2032\u2016\u2217 + \u03bb\u2016C \u2032\u20161,2,\nwith the equality holds only when L\u2032 = L0 and C \u2032 = C0. Proof: Write L\u2032 = L0 + \u2206 and C \u2032 = C0 \u2212 \u2206. Since PU0(L\u2032) = L\u2032, we have that for i \u2208 I0, PU0\u2206i = \u2206i, which implies that for i \u2208 I0 C\u22a40i\u2206i = (C \u22a4 0iU)U\n\u22a4\u2206i = 0\u00d7 U\u22a4\u2206i, where the last equality holds from Assumption 1 and the definition of C0 (recall that C0i is the ith column of C0). Thus, \u2016C0\u20161,2 = \u2211 i\u2208I \u2016C0i\u20162 \u2264 \u2211 i\u2208I0 \u2016C0i+\u2206i\u20162 \u2264 \u2211n\ni=1 \u2016C0i +\u2206i\u20162 = \u2016C \u2032\u20161,2, with equality only holds when \u2206 = 0.\nFurther note that PI0(C \u2032) = C \u2032 implies that PI0(\u2206) = \u2206, which by definition of L0 leads to L0 = PIc 0 L\u2032.\nThus, Lemma 11 implies \u2016L0\u2016\u2217 \u2264 \u2016L\u2032\u2016\u2217. The theorem thus follows.\n22"}, {"heading": "2) Step 2:", "text": "Theorem 8: Under Assumption 1, (L0, C0) is an optimal solution to Outlier Pursuit if and only if there\nexists Q such that\n(a) PT0(Q) = U0V \u22a40 ; (b) \u2016PT\u22a5\n0 (Q)\u2016 \u2264 1; (c) PI0(Q) = \u03bbH0; (d) \u2016PIc 0 (Q)\u2016\u221e,2 \u2264 \u03bb.\n(20)\nHere PT0(\u00b7) , PT (L0)(\u00b7). In addition, if both inequalities are strict, then (L0, C0) is the unique optimal solution. Proof: Standard convex analysis yields that (L0, C0) is an optimal solution to Outlier Pursuit if and only if there exists a dual matrix Q such that\nQ \u2208 \u2202\u2016L0\u2016\u2217; Q \u2208 \u2202\u03bb\u2016C0\u20161,2. Note that a matrix Q is a subgradient of \u2016 \u00b7 \u2016\u2217 evaluated at L0 if and only if it satisfies\nPT0(Q) = U0V \u22a40 ; and \u2016PT\u22a5 0 (Q)\u2016 \u2264 1. Similarly, Q is a subgradient of \u03bb\u2016 \u00b7 \u20161,2 evaluated at C0 if and only if\nPI0(Q) = \u03bbH0; and \u2016PIc0(Q)\u2016\u221e,2 \u2264 \u03bb. Thus, we proved the first part of the theorem, i.e., the necessary and sufficient condition of (L0, C0) being an optimal solution.\nNext we show that if both inequalities are strict, then (L0, C0) is the unique optimal solution. Fix \u2206 6= 0, we show that (L0 + \u2206, C0 \u2212 \u2206) is strictly worse than (L0, C0). Let W be such that \u2016W\u2016 = 1, \u3008W,PT\u22a5\n0 (\u2206)\u3009 = \u2016PT\u22a5 0 \u2206\u2016\u2217, and PT0W = 0. Let F be such that such that\nFi = { \u2212\u2206i \u2016\u2206i\u20162 if i 6\u2208 I0, and \u2206i 6= 0 0 otherwise.\nThen U0V \u22a40 +W is a subgradient of \u2016 \u00b7 \u2016\u2217 at L0 and H0 + F is a subgradient of \u2016 \u00b7 \u20161,2 at C0. Then we have\n\u2016L0 +\u2206\u2016\u2217 + \u03bb\u2016C0 \u2212\u2206\u20161,2 \u2265 \u2016L0\u2016\u2217 + \u03bb\u2016C0\u20161,2+ < U0V \u22a40 +W,\u2206 > \u2212\u03bb < H0 + F,\u2206 > = \u2016L0\u2016\u2217 + \u03bb\u2016C0\u20161,2 + \u2016PT\u22a5\n0\n(\u2206)\u2016\u2217 + \u03bb\u2016PIc 0 (\u2206)\u20161,2+ < U0V \u22a40 \u2212 \u03bbH0,\u2206 >\n= \u2016L0\u2016\u2217 + \u03bb\u2016C0\u20161,2 + \u2016PT\u22a5 0 (\u2206)\u2016\u2217 + \u03bb\u2016PIc 0 (\u2206)\u20161,2+ < Q\u2212 PT\u22a5 0 (Q)\u2212 (Q\u2212 PIc 0 (Q)),\u2206 > = \u2016L0\u2016\u2217 + \u03bb\u2016C0\u20161,2 + \u2016PT\u22a5 0 (\u2206)\u2016\u2217 + \u03bb\u2016PIc 0 (\u2206)\u20161,2+ < \u2212PT\u22a5 0 (Q),\u2206 > + < PIc 0 (Q),\u2206 > \u2265 \u2016L0\u2016\u2217 + \u03bb\u2016C0\u20161,2 + (1\u2212 \u2016PT\u22a5 0 (Q)\u2016)\u2016PT\u22a5 0 (\u2206)\u2016\u2217 + (\u03bb\u2212 \u2016PIc 0 (Q)\u2016\u221e,2)\u2016PIc 0 (\u2206)\u20161,2\n\u2265 \u2016L0\u2016\u2217 + \u03bb\u2016C0\u20161,2, where the last inequality is strict unless\n\u2016PT\u22a5 0 (\u2206)\u2016\u2217 = \u2016PIc 0 (\u2206)\u20161,2 = 0. (21)\nWe next show that Condition (21) also implies a strict increase of the objective function to complete the proof. Note that Equation (21) is equivalent to \u2206 = PT0(\u2206) = PI0(\u2206), and note that\nPU0(\u2206) = PT0(\u2206)\u2212 PV0(\u2206) + PU0PV0(\u2206) = \u2206\u2212 (I \u2212PU0)PV0\u2206. Since PI0(V \u22a40 ) = 0, PI0(\u2206) = \u2206 implies that PV0(\u2206) = 0, which means \u2206 = PU0(\u2206) = PI0(\u2206). Thus, PU0(L0+\u2206) = L0+\u2206, and PI0(C0\u2212\u2206) = C0\u2212\u2206. By Theorem 7, \u2016L0+\u2206\u2016\u2217+\u03bb\u2016C0\u2212\u2206\u20161,2 > \u2016L0\u2016\u2217 + \u03bb\u2016C0\u20161,2, which completes the proof.\n23"}, {"heading": "3) Step 3:", "text": "Theorem 9: Under Assumption 1, if there exists any matrix Q that satisfies Condition (18), then U0V \u22a40 + \u03bbH0 satisfies (18). Proof: Denote Q0 , U0V \u22a40 + \u03bbH0. We first show that the two equalities of Condition (18) hold. Note that\nPT0(Q0) = PT0(U0V \u22a40 ) + \u03bbPT0(H0) = U0V \u22a40 + \u03bb[PU0(H0) + PV0(H0)\u2212 PU0PV0(H0)]. Further note that PU0(H0) = U0(U\u22a40 H0) = 0 due to Assumption 1, and PV0(H0) = 0 because PI0H0 = H0 and PI0(V \u22a40 ) = 0 lead to H0V0 = 0. Hence\nPT0(Q0) = U0V \u22a40 . Furthermore, PI0(Q0) = PI0(U0V \u22a40 ) + \u03bbPI0(H0) = U0PI0(V \u22a40 ) + \u03bbH0 = \u03bbH0. Here, the last equality holds because PI0(V \u22a40 ) = 0. Note that this also implies that\nPT\u22a5 0 (H0) = H0; PIc 0 (U0V \u22a4 0 ) = U0V \u22a4 0 . (22)\nNow consider any matrix Q that also satisfies the two equalities. Let Q = U0V \u22a40 + \u03bbH0 +\u2206, note that Q satisfies PI0(Q) = \u03bbH0 and PT0(Q) = U0V \u22a40 , which leads to\nPI0(\u2206) = 0; and PT0(\u2206) = 0. Thus,\nPIc 0 (Q) = U0V \u22a4 0 +\u2206; and PT \u22a5\n0\n(Q) = \u03bbH0 +\u2206.\nNote that\n\u2016U0V \u22a40 +\u2206\u2016\u221e,2 = max i \u2016U0(V \u22a40 )i +\u2206i\u20162 \u2265max\ni \u2016U0(V \u22a40 )i\u20162 = \u2016U0V \u22a40 \u2016\u221e,2.\nHere, the inequality holds because PT0(\u2206) = 0 implies that \u2206i are orthogonal to the span of U . Note that the inequality is strict when \u2206 6= 0.\nOn the other hand\n\u2016\u03bbH0\u2016 = max \u2016x\u2016\u22641,\u2016y\u2016\u22641 x\u22a4(\u03bbH0)y (a) = max\n\u2016x\u2016\u22641,\u2016y\u2016\u22641,PIc 0 (y\u22a4)=0\nx\u22a4(\u03bbH0)y\n(b) = max\n\u2016x\u2016\u22641,\u2016y\u2016\u22641,PIc 0 (y\u22a4)=0\nx\u22a4(\u03bbH0 +\u2206)y \u2264 max \u2016x\u2016\u22641,\u2016y\u2016\u22641 x\u22a4(\u03bbH0 +\u2206)y = \u2016\u03bbH0 +\u2206\u2016.\nHere, (a) holds because PI0H0 = H0, thus for any y, set all yi = 0 for i 6\u2208 I0 does not change x\u22a4(\u03bbH0)y; while (b) holds since PIc\n0 \u2206 = \u2206.\nThus, if Q satisfies the two inequalities, then so does Q0, which completes the proof. Note that by Equation 22 we have\nPT\u22a5 0 (H0) = H0; PIc 0 (U0V \u22a4 0 ) = U0V \u22a4 0 .\nThus, Theorem 7, Theorem 8 and Theorem 9 together establish Theorem 6.\n24"}, {"heading": "B. Proof of Corollary 2", "text": "Corollary 2 holds due to the following lemma that tightly bounds \u2016H0\u2016 and \u2016U0V \u22a40 \u2016\u221e,2. Lemma 12: We have (I) \u2016H0\u2016 \u2264 \u221a\u03b3n, and the inequality is tight. (II) \u2016U0V \u22a40 \u2016\u221e,2 = maxi \u2016V \u22a40 ei\u20162 =\n\u221a\n\u00b5r\n(1\u2212\u03b3)n .\nProof: Following the variational form of the operator norm, we have\n\u2016H0\u2016 = max \u2016x\u20162\u22641,\u2016y\u20162\u22641 x\u22a4H0y = max \u2016x\u20162\u22641 \u2016x\u22a4H0\u20162 = max \u2016x\u20162\u22641\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n(x\u22a4Hi)2 \u2264 \u221a \u2211\ni\u2208I0\n1 = \u221a |I0| = \u221a \u03b3n.\nThe inequality holds because \u2016(H0)i\u20162 = 1 when i \u2208 I0, and equals zero otherwise. Note that if we let (H0)i all be the same, such as taking identical outliers, the inequality is tight.\nBy definition we have \u2016U0V \u22a40 \u2016\u221e,2 = maxi \u2016U0(V \u22a40 )i\u20162 (a) = maxi \u2016(V \u22a40 )i\u20162 = maxi \u2016V \u22a40 ei\u20162. Here (a)\nholds since U0 is orthonormal. The second claim hence follows from definition of \u00b5."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques<lb>for dimensionality reduction: successful and efficiently computable, it is nevertheless plagued by a well-known,<lb>well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily<lb>corrupted components. Yet, in applications of SVD or PCA such as robust collaborative filtering or bioinformatics,<lb>malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire<lb>points that are completely corrupted.<lb>We present an efficient convex optimization-based algorithm we call Outlier Pursuit, that under some mild<lb>assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems)<lb>recovers the exact optimal low-dimensional subspace, and identifies the corrupted points. Such identification of<lb>corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinfor-<lb>matics and financial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm<lb>minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of<lb>work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column<lb>space of the uncorrupted matrix, rather than the exact matrix itself.", "creator": "LaTeX with hyperref package"}}}