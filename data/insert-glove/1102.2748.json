{"id": "1102.2748", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2011", "title": "Feature Selection via Sparse Approximation for Face Recognition", "abstract": "Inspired by ganczarski biological transcaucasus vision systems, the passacantando over - hospitable complete local features messily with huge cardinality enumclaw are salzgeber increasingly koffler used gasco for 10,380 face librado recognition during the 27-may last decades. Accordingly, 16-10 feature selection 31.26 has smallpipes become more 825-meter and natesan more important and plays detre a critical role 124.44 for seventy-four face data alberghetti description and obihiro recognition. subgrouping In klem this paper, we propose scripter a mapreduce trainable muldoon feature 1922 selection algorithm tabacco based 5hr on the regularized cotton-top frame 108-foot for 1214 face recognition. meristems By enforcing \u00e5tvidabergs a third-year sparsity penalty time-saving term on 83.32 the minimum squared dzogchen error (MSE) criterion, vallieres we birinus cast the feature humps selection jani problem eight into gothom a combinatorial sparse unclos approximation kakarak problem, coulibaly which can unabridged be nevas solved by greedy methods makdisi or convex relaxation methods. rhyn Moreover, aakhri based contemporaneo on flesher the vratil same 2,269 frame, inter-process we hijacks propose a sparse angleterre Ho - brueghel Kashyap (HK) wearisome procedure 24.93 to obtain vavau simultaneously the optimal sparse solution mangou and the corresponding 1997-8 margin navagrahas vector of coorparoo the digimarc MSE criterion. north-african The strigulation proposed fitzmorris methods todorov are shizue used lacaze for 1978-1989 selecting agradable the most informative Gabor germanic features 13-28 of belperron face jarred images for russborough recognition and the magnetometers experimental orchiectomy results on benchmark smutty face databases demonstrate 19,950 the effectiveness harnden of bukas the ponza proposed methods.", "histories": [["v1", "Mon, 14 Feb 2011 12:05:47 GMT  (102kb)", "http://arxiv.org/abs/1102.2748v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yixiong liang", "lei wang", "yao xiang", "beiji zou"], "accepted": false, "id": "1102.2748"}, "pdf": {"name": "1102.2748.pdf", "metadata": {"source": "CRF", "title": "Feature Selection via Sparse Approximation for Face Recognition", "authors": ["Yixiong Liang", "Lei Wang", "Yao Xiang", "Beiji Zou"], "emails": ["bjzou}@mail.csu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 2.\n27 48\nv1 [\ncs .C\nV ]\n1 4\nFe b\n20 11\nIndex Terms\u2014Face recognition, feature selection, sparse approximation, minimum squared error criterion, Ho-Kashyap procedure.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "W ITHIN the last several decades, facerecognition has received extensive attention due to its wide range of application from identity authentication, access control and surveillance to human-computer interaction and numerous novel face recognition algorithms have been proposed [43], [28]. One of the key issue to successful face recognition systems is the development of effective face representation, namely how to extract and select the discriminative features to represent face image. According to the region from which features are derived, face representation methods can be generally divided into two categories: holistic representation and local representation. The holistic representation extract features from the whole face image while the local representation calculating the features from the local faical regions.\nAfter the introduction of the well-known Eigenfaces [33], the holistic representation meth-\n\u2022 The authors are with the Institute of Information Science and Engineering, Central South University, Changsha, Hunan, 410083, China. E-mail: {yxliang, wanglei, yxiang, bjzou}@mail.csu.edu.cn ods were extensively studied [4], [24], [3], [13], [37], [34]. However, local areas are often more descriptive and more appropriate for dealing with those facial variations due to expression, partial occlusion and illumination, since most variations in appearance only affect a small part of the face region. Local feature analysis (LFA) [25] pioneers the study of local representation for face recognition. Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6]. A lots of local feature descriptors, such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc., have been successfully applied in face recognition. These local features are often over-completed, whereas only a relatively small fraction of them is relevant to the recognition task. Thus feature selection is a crucial and necessary step to select the most discriminant local features to obtain a sparse face representation. While the prior knowledge to the choice of this local feature dictionary of\nlarge cardinality is often limited and a consistent theory is still missing, numerous learned methods are emerging in the empirical practice due to their effectiveness (refer to [12] for an excellent review of feature selection approaches in machine learning). Adaboost-based methods are the most popular and impressive feature selection methods in face recognition Scenario [16], [42], [40], [27], [26], [19], [35]. One possible problem of these methods is very time consuming in the training stage for the need of training and evaluating a classifier for each feature component. An alternative is the regularizedbased method which sparsify with respect to a dictionary of features by the sparsity-enforcing regularization techniques [29], [8]. The main merits of such a regularized approach are its effectiveness even in the presence of a very small number of data coupled with the fact that it is supported by well-grounded theory [8]. Another potential merit is that the regularized methods analyze all feature components together and may be more appropriate to capture groups of correlated features, whereas the Adaboost-based method only consider the relevance of each feature separately, thus may ignore the possible dependencies between features.\nBased on the regularized frame, in this paper, we propose a novel feature selection method based on classical minimum squared error (MSE) criterion [9] which assumes a linear dependence between the feature components and the discriminant functions. We cast the feature selection problem into a combinatorial sparse approximation problem by enforcing a sparsity penalty term on the MSE criterion and the solution can be obtained by greedy methods such as matching pursuit (MP) [21] or the orthogonal matching pursuit (OMP) [30], [31], [32] and convex relaxation methods [31]. We restrict ourselves to the linear models because they are relatively easy to compute and in the absence of information suggesting otherwise, linear models are an attractive candidates. Further, the linear model can be extended to the nonlinear cases by explicitly or implicitly giving some function of the local feature components. The latter is the well-known kernel trick.\nDue to the arbitrary selection of margin vec-\ntor, the MSE procedure cannot guarantee to obtain the optimal separating vector even in the separable case [9]. We impose the sparse constrains on the Ho-Kashyap (HK) procedure [9] and propose a named sparse HK (SHK) procedure to obtain simultaneously the optimal sparse solution and the corresponding margin vector. Similar to the original HK procedure, the proposed SHK procedure is an iterative scheme that alternates between solution of the sparse vector based on the current margin vector and a process of updating the margin vector. It is flexible and can work with any greedy methods or convex relaxation methods. Gabor and LBP are two most representative local features in face recognition. We select Gabor feature as the start representation due to its peculiar ability to model the spatial summation properties of the receptive fields of the so called \u201dbar cells\u201d in the primary visual cortex. Then we apply the proposed feature selection method to select the most informative Gabor features for face recognition. Experimental results on the benchmark face databases demonstrate the effectiveness of the proposed feature selection methods. Our method may be mainly inspired by the work [8] which is also applying the sparse regularized term to the linear model to perform the feature selection. Nevertheless, the linear model in [8] neglects the bias on the one hand and only enforces the linear dependence between the feature components and the class labels on the other hand. In fact, this simple linear dependence is equivalent to set all entries of the margin vector equal to 1 in MSE criterion model. Our method starts off with the MSE criterion and considers simultaneously the bias and the adaptive margin vector and hence can be seemed as a generalization of the method in [8]. Moreover, in [8] the sparse solution is obtained through iterative soft-thresholding method and the convergence relies on the careful normalization of each features component of all training samples at a time, which may destroy the structure of the features. Our method adheres to the original features without any additional normalization and also obtain the convergence. The rest of this paper is organized as follows.\nIn Section 2, we start off with the MSE criterion and propose a novel feature selection method based on sparsity-enforcing regularized techniques. Based on the same frame, in Section 3, we present a sparse extension of the classical HK procedure for feature selection. In Section 4 we first briefly review the Gabor face representation and then illustrate how to apply the proposed feature selection frame to select the most informative Gabor features for face recognition. Experiments and analysis are described in Section 5, whereas Section 6 concludes the paper.\n2 FEATURE SELECTION BASED ON sparse MSE CRITERION In this section, we present a new feature selection algorithm based on the MSE criterion. As mentioned before, we restrict ourselves to the case of a linear discriminant functions that are linear in the components of feature x = [x1, . . . , xd] T :\ng(x) =\nd \u2211\ni=1\n\u03c9ixi + \u03c90 = y Ta, (1)\nwhere g(x) denotes the discriminant function; \u03c90 is the bias or threshold; \u03c9i(i = 1, . . . , d) is the weights; y = [1, x1, . . . , xd]\nT and a = [\u03c90, . . . , \u03c9d]\nT are the augmented feature vector and augmented weight vector, respectively. Since the face recognition can be cast into a classification of the intra-personal and extrapersonal variation [24], we focus on a binary classification problem. As suggested in [9], we substitute all negative samples (i.e. extrapersonal variations) with their negatives to forget the labels and look for a weight vector such that yTi a > 0 for all of the samples. Indeed, this relation is invariant under a positive scaling of a. Thus, we can define a canonical hyperplane such that yTi a = bi where bi is a positive constant called the margin. Now the problem can be reformulated as the following linear system of equations:\nYa = b, (2)\nwhere Y = [y1, . . . ,yn] T \u2208 Rn\u00d7(d+1) is the augmented feature matrix and b = [b1, . . . , bn] T\nis the margin vector. Due to the size of Y, it is infeasible to obtain the exact solution of (2). One classical relaxation is to solve the minimize squared error criterion function\nmin \u2016Ya\u2212 b\u201622 = min n \u2211\ni=1\n(yTi a\u2212 bi)2. (3)\nIt can be solved by a gradient search procedure. However, the MSE solution do not provide feature selection in the sense because it\u2019s typically non-sparse. By enforcing sparse regularization term on the MSE criterion, we can turn the feature selection into solving the following sparsity-enforcing MSE (SMSE) criterion:\nmin \u2016Ya\u2212 b\u201622 + \u03c4 2\u2016a\u20160, (4) where \u2016 \u00b7 \u20160 is the l0 quasi-norm counting the nonzero entries of a vector and \u03c4 is a threshold that quantifies how much improvement in the approximation error is necessary before we admit an additional term into the approximation. It is a classic combinatorial sparse approximation problem and can be solved by greedy techniques such as MP and OMP which construct a sparse approximant one step at a time by selecting the atom most strongly correlated with the residual part of the signal and use it to update the current approximation. An alternative to solving the SMSE criterion (4) is the convex relaxation methods which replace the problem with a relaxed version that can be solved more efficiently. The l1 norm provides a natural convex relaxation of the l0 quasi-norm, and it suggests that we may be able to solve sparse approximation problems by introducing an l1 norm in place of the l0 quasi-norm. From this heuristic, a relaxed version of SMSE (RSMSE) criterion can be derived as follows:\nmin 1\n2 \u2016Ya\u2212 b\u201622 + \u03b3\u2016a\u20161, (5)\nwhich is an unconstrained convex function and thus standard mathematical programming softwares can be used to find a minimizer. The parameter \u03b3 negotiates a compromise between approximation error and sparsity. It has been proved that if the feature matrix Y is incoherent and the threshold parameters are correctly chosen, then the solution to RSMSE criterion (5) identifies every significant atom from the\nsolution to SMSE criterion (4) and no others [31].\nFrom a run-time point of view, we adopt OMP to solve the SMSE criterion (4). Since OMP is iterative, we must supply a criterion for stopping the iteration. Here are two possibilities:\n\u2022 One may halt the procedure when the norm of the residual declines below a specified threshold. \u2022 One may halt the procedure after predefined number of distinct feature components have been selected.\nIn our implementation, the iteration is stopped whenever one of the above conditions is satisfied.\nNotice that in the criterion (3) (4) and (5), the entries of the margin vector b are arbitrary positive constants. Obviously, different choice of b would typically lead to different solutions. As the MSE solution is directly related to the Fisher discriminant vector with a proper choice of the margin vector (i.e. the entries bi corresponding to the same class are equal to the ratio of the sample size of this class to the total sample size) [9], the SMSE solution or RSMSE solution gives a natural sparse generalization of Fisher linear discriminant. Hereafter we refer to the resulting feature selection algorithm as sparse Fisher (SFisher) procedure. Moreover, if we set b = 1n and \u03c90 = 0 (we refer to the resulting algorithm as simplified SMES procedure, or SSMES), in this special case the RSMSE criterion (5) degenerates into the linear model described in [8], and thus our method can also be seemed as a generalization of the method in [8].\n3 FEATURE SELECTION BASED ON sparse HK PROCEDURE Because the objective is minimizing \u2016Ya\u2212b\u201622, as discussed in [9], the MSE procedures yield a solution whether the samples are linearly separable or not, but there is no guarantee that this vector is a separating vector even in the separable case. However, in the separable case, there do exist amargin vector b\u0302with all positive entries such that the corresponding MSE solution is the separating vector. The HK procedure\nextends the MSE procedure to deal with this problem by determining a and b alternately where the components of b cannot decrease. Borrowing from the same ideas, in this section we propose a sparse version the HK procedure to extend our method described in the former section. Specifically speaking, in the proposed SHK procedure there are two stages for each iteration: one for sparse approximating that essentially evaluates a and one for updating the margin vector b. Sparse approximating can be conveniently performed by using greedy or convex relaxation algorithms to solve the SMES criterion (4) with a given b. Similar original HK procedure, the updating rule of b is to start with b > 01 and to refuse to reduce any of its components, namely\n{\nb(1) > 0\nb(t + 1) = b(t) + 2\u03b7(t)e+(t), (6)\nwhere \u03b7(t) is a positive scale factor or learn rate; e(t) = Ya(t)\u2212 b(t) is the error vector and e+(t) = 1\n2 (e(t)+ |e(t)|) is the positive part of the\nerror vector, respectively. Given some stopping rules, our algorithm is: Algorithm SHK. Initialization: Set b(0) > 0, 0 < \u03b7(\u00b7) < 1. Set the iteration index t = 1. Repreat until convergence (stopping criterion):\n\u2022 Sparse approximation stage: Use any greedy algorithms or convex relaxation methods to computer a(k) by approximating the solution of SMES criterion (4). \u2022 Margin vector update stage: b(t + 1) = b(t) + 2\u03b7(k)e+(t). \u2022 Set t = t + 1.\nIn our implementation, the stopping rule is that when \u2016b(t+1)\u2212b(t)\u20162 < \u01eb is reached, the loop is terminated. It is noteworthy that although the convergence of the original HK procedure can be proven theoretically [9], owing to the introduction of the sparse approximation stage, exact analysis of the convergence of the proposed SHK algorithm in a deterministic manner is rather complicated or even impossible. Nevertheless, we can obtain the convergence by\n1. b > 0 means that every component of b is positive.\ncareful selection of \u03b7(t) which decreases with t. Our choice is to set \u03b7(t) = \u03b7(1)\nt ."}, {"heading": "4 GABOR FEATURE SELECTION FOR", "text": "FACE RECOGNITION\nIn this section we describe how we specialize the proposed feature selection frame to the case of face recognition. We first briefly review the Gabor representation of face and then describe how to apply the proposed feature selection methods to select Gabor features for face recognition."}, {"heading": "4.1 Gabor representation", "text": "We start with the widely used Gabor representation because the kernels of Gabor filters are similar to the 2D receptive field profiles of the mammalian cortical simple cells and exhibit desirable characteristics of spatial locality and orientation selectivity [36], [40], [26]. The Gabor representation of a face image can be obtained by convolving the image by a set of Gabor filters which are commonly defined as follows:\n\u03c8\u00b5,\u03bd = \u2016k\u00b5,\u03bd\u201622\n\u03c32 e\u2212\n\u2016k\u00b5,\u03bd\u2016 2 2 \u2016z\u20162 2\n2\u03c32 [eik\u00b5,\u03bdz \u2212 e\u2212\u03c3 2 2 ], (7)\nwhere z is the coordinate vector; parameters \u00b5 and \u03bd define the orientation and the scale of the Gabor filter; parameters \u03c3 is the standard deviation of Gaussian window; k\u00b5,\u03bd is the wave vector given by k\u00b5,\u03bd = k\u03bde i\u03c6\u00b5 , where k\u03bd = kmax f\u03bd and \u03c6\u00b5 = \u03c0\u00b5\n8 if eight different orientations have\nbeen chosen; kmax is the maximum frequency, and f is the spatial factor between kernels in the frequency domain. In face recognition area, researchers commonly use 40 Gabor filters with five scales \u03bd \u2208 {0, \u00b7 \u00b7 \u00b7 , 4} and eight orientations \u00b5 \u2208 {0, \u00b7 \u00b7 \u00b7 , 7} and with \u03c3 = 2\u03c0, kmax = \u03c02 and f = \u221a 2. However, we set the scale ranges from -1 to 2 rather than from 0 to 4 due to the using of smaller size of face images in our experiments. Thus only 32 Gabor filters are used. Convolving the face image with these 32 Gabor filters and only extracting the magnitudes information can then generate a high dimensional Gabor representation. For example, for an image with 64\u00d7 64 pixels, the total number of Gabor features is 4\u00d7 8\u00d7 64\u00d7 64 = 131, 072.\nA noticeable problem in discrete convolution is the choice of proper size of the convolution mask. It should be large enough to show the nature of Gabor kernels and not be too large for the computation efficiency. As suggested in [10], we truncate the Gabor filters to six times the span of the Gaussian function. As the span of Gaussian function is \u03c3\nk\u03bd , the Gabor mask\nis then truncated to a width w = 6\u03c3 k\u03bd + 1 = 24\u00d72 \u03bd2 +1. Thus in our experiments the size of Gabor filters are 19\u00d719, 25\u00d725, 35\u00d735, 49\u00d749 corresponding to the scale of \u03bd \u2208 {\u22121, \u00b7 \u00b7 \u00b7 , 2}."}, {"heading": "4.2 Feature selection for face recognition", "text": "Now it time to turn our attention to the feature selection of the high dimensional Gabor representation. Similarly to Moghaddam et al. [24], we temporarily cast the face recognition into a classification of the intra-personal (hereafter as positive) and extra-personal (hereafter as negative) variation. For each pair of face images Ii and I \u2032i , we compare the corresponding Gabor feature components. Specifically, for each pair of input images we obtain a feature vector xi whose elements are the absolute difference between the corresponding Gabor representations. Given a training set, we can then get the augmented feature matrix Y following the routine described in Section 2. A by-no-means negligible problem in practical is the overwhelmingly huge size and unbalance of the training samples [16]. For instance, given a training set that includes K images for each of the C individuals, the total number of image pairs is ( CK\n2\n)\nwhereas only a small minority, C ( K\n2\n)\nof these pairs display the intrapersonal variation. Let C = 300 and K = 4, then the size of positive samples and negative samples are 1, 800 and 717, 600 respectively with their ratio be close to 1 : 400. Obviously, such a huge samples size will lead to severe memory and computational problem. In addition, the unbalance training samples may bias the performance of the feature selection. In order to obtain balanced systems of reasonable size, we randomly sample the positive and negative samples with a comparable ratio to build the augmented feature matrix Y. In practical, we can sample negative\nsamples while keeping all positive samples with their ratio varying from 1 : 1 to 1 : 10.\nOnce we build the augmented feature matrix Y, we can find the solution of SMSE criterion (4) with a given or an adaptive margin vector b according to the procedure previously described. Then the Gabor feature components corresponding to non-zero entries of the augmented weight vector a are selected as the most informative ones and used for further face recognition.\nRecalled that the above feature selection frame based on linear discriminant functions also establishes a linear classifier with a bias w0 discriminating the intra-personal and extrapersonal difference, so it can be used for face recognition directly. However, one can also consider its usage as a pure feature selection tool to reduce the numbers of Gabor features and adopt some other common classifiers such as nearest neighbor classifier (NNC), Fisher classifier (FC) [9] or support vector machines (SVM) [7] for the recognition."}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": "In order to evaluate the proposed approach, we carry out some experiments on two large face databases: CAS-PEAL-R1 [11] and LFW [15] face database. The CAS-PEAL-R1 face database contains 30, 863 images of 1, 040 Chinese subjects with different variations of pose, expression, accessories, age, and lighting. The LFW face database contains 13, 233 labeled face images collected from news sites in the Internet. These images belong to 5, 749 different individuals and have high variations in position, pose, lighting, background, camera and quality. Therefore LFW database is more appropriate to evaluating face recognition methods in realistic and unconstrained environments.\nIn all our experiments, each image is rotated and scaled so that the centers of the eyes are placed on specific pixels and then was cropped to 64 \u00d7 64 pixels 2. As described before, we only select 32 Gabor filters to extract the Gabor features.\n2. The eyes locations are given in CAS-PEAL-R1 database. For LFW database, we adopted standard fiducial point detector to extract the eyes locations and annotated them manually whenever the automatic eyes locator failed."}, {"heading": "5.1 Results on CAS-PEAL-R1 database", "text": "We restrictively follow the CAS-PEAL-R1 evaluation protocol which specifies one training set, one gallery set and six probe sets [11]. Therefore the training sets include 1, 200 images of 300 subjects and the ratio of intrapersonal sample size to extra-personal sample size is 1, 800 : 717, 600. We keep all intrapersonal samples while randomly sampling the extra-personal samples with a ratio of 1 : 7. If all Gabor features are considered, the linear problem we are about to build is rather large. In fact, the size of the augmented feature matrixY is come to 131, 073 \u00d7 14, 400. Obviously direct multiplication on such a matrix is infeasible. One possible choice is to reduce the number of Gabor features if possible. With the prior knowledge of that the magnitude of the Gabor filters is not sensitive to the positions, we can reduce the number of positions by a simply down sampling scheme with a factor 16. Thus the number of positions is roughly one sixteenth of the total number of pixels. So after the down sampling, the size of the augmented feature matrix Y is reduced to 8193\u00d7 14, 400."}, {"heading": "5.1.1 Feature selection results", "text": "We conducted experiments on the CAS-PEALR1 training set using SSMES, SFisher and SHK procedure to select 500 most informative Gabor features, respectively. Their characteristics can be observed by their statistics. The location distribution of selected Gabor features are shown\nin Fig. 1. It is interesting to see that most of selected Gabor features resulting from all three methods are located around the prominet facial features such as eyebrows, eyes, nose and mouth, while seldom being located on the\ncheek area. This indicates that the prominent facial features regions carry the most important discriminating information while the cheek region conveying less information. Moreover, a minority of selected features are located on external features such as cheek contour and jaw line. In fact, although the external region does not cover the face much, the external features implicitly uses shape information and thus are useful for distinguishing thin faces from round faces. This result is agreed with Ref. [41].\nWe also compared the frequency of Gabor kernels in the selected Gabor features. Fig. 2 illustrates the frequency of the 32 Gabor kernels in the leading 500 Gabor features selected by SSMES, SFisher and SHK procedure. Obviously, different scales and orientations con-\ntribute different and the distribution of features selected by different methods is somewhat uniform: the 0-scale and 1-scale are more likely important than the other two scales and most of horizontal and vertical Gabor kernels have extracted stronger features than those with other orientation."}, {"heading": "5.1.2 Classification results", "text": "The selected Gabor features are then adopted for face recognition. The classical classifiers, NNC and FC, are chosen to recognize the faces. As mentioned above, the proposed feature selection frame can perform the intra-personal and extra-personal recognition task. Thus we also used it for face recognition by treating the face recognition as a series of pair matching problems. However, in many situations there are more than one subject satisfying the separating condition. In order to make a final decision we simply classify the unknown face as the subject whose samples can maximize the linear discriminant function (1), i.e. the margin. Therefore in some sense it can be seen as a maximum margin classifier (MMC). We also implemented 3 previous Gaborbased approaches for comparison. The first is using Gabor feature without feature selection for face representation and NNC for recognition, which is denoted as \u201dG+NNC\u201d. The second method \u201dG+FC\u201d denotes the GFC method in [20], i.e. the PCA+LDA on downsampled Gabor features. The third method, \u201dG Ada+FC\u201d, is the AGFC method in [26] which using Adaboost to select Gabor features and FC for classification. For clarity, \u201dG SSMES+NNC\u201d, \u201dG SFisher+NNC\u201d and \u201dG SHK+NNC\u201d respectively denote the method using SSMES, SFisher and SHK procedure to select Gabor features and NNC for recognition. Similarly, for the other two classifiers, the corresponding methods are denoted as \u201dG SSMES+MMC\u201d, \u201dG SFisher+MMC\u201d, \u201dG SHK+MMC\u201d and \u201dG SSMES+FC\u201d, \u201dG SFisher+FC\u201d, \u201dG SHK+FC\u201d. We investigated 3 kinds of distance measurements: l1 distance, l2 distance and cosine distance, and found that for NNC, l1 distance achieves the best performance while for FC, the cosine distance performing best. Thus we selected l1 distance for NNC and cosine distance\nfor FC. In our implementation, the number of Gabor features used in \u201dG+FC\u201d is downsampled to the dimension of 8, 192 and in \u201dG Ada+FC\u201d, 2, 000 Gabor features are selected by Adaboost. The optimal dimension for PCA and LDA are determined by testing all possible dimensions. The results on 5 different probes sets are shown and compared in Table 1.\nFrom Table 1, we can obtain several major observations. First, although the proposed feature selection frame also establishes a classifier which can be straightforwardly used for face recognition, its performances are not as satisfactory as expected, especially for the \u201dG SSMES + MMC\u201d method. Our explanation is that though the feature selection frame can select effectively the meaningful features, it may overestimate or underestimate the corresponding weights, leading to the over-fitting problems. Comparing to the \u201dG SSMES + MMC\u201d, the classifiers used in \u201dG SFisher + MMC\u201d and \u201dG SHK + MMC\u201d both consider the bias and the margin and thus achieve better results. The second observation is that the FC based methods (\u201dG+FC\u201d, \u201dC SMESS+FC\u201d, \u201dC SFisher+FC\u201d, \u201dC SHK+FC\u201d and \u201dG Ada+FC\u201d) perform much better than the other two classifiers based methods. In general, the algorithms with regularized-based feature selection procedure only use 500 Gabor features and slightly outperform \u201dG Ada+FC\u201d with 2, 000 Gabor features selected by Adaboost and is comparable to \u201dG+FC\u201d using 8, 192 Gabor features, which shows that the proposed feature\nselection frame is effective for face recognition. Third, SFisher and SHK perform better than SSMES in the sense of both feature selection and classification. This results indicate that the consideration of bias and the margin will not make the learning process overfits the training data but increase the generalizability."}, {"heading": "5.2 Results on LFW database", "text": "We also conducted some experiments on the LFW database for further investigation. Unlike the CAS-PEAL-R1 database, the LFW database have larger degree of variability and the recognition is only to be done by pairs matching, instead of searching for the most similar face in the database. We still followed their protocol which gives two Views: View 1 for model selection and algorithm development while View 2 for performance reporting. View 1 specifies one training set containing 2, 200 pairs and one testing set containing 1, 000 pairs. View 2 consists of ten sets with 600 images in each case. They can be combined into 10 different training/testing set pairs. In our experiments, the training set of View 1 are chosen for training the feature selection model and the performance are reported using 10-fold cross validation on the View 2. The proposed feature selection frame is used as a feature selector to select 500 most informative Gabor features from the original 131, 072 original features. We directly adopted the proposed frame as a classifier to recognize the unknown pairs in company with the SVM classifier. The corresponding methods are referred as \u201dG SMESS\u201d, \u201dG SFisher\u201d, \u201dG SHK\u201d and \u201dG SMESS+SVM\u201d, \u201dG SFisher+SVM\u201d, \u201dG SHK+SVM\u201d respectively. We also investigated the performance of the method \u201dG+FC\u201d which uses all 131, 072 original features as representation and FC as a classifier. The results of the experiments are described in Table 2 below and the ROC comparison curves of different methods are illustrated in Fig. 3. As can be seen, a direct application of proposed feature selection frame as a classifier perform somewhat worse than the performance achieved by using a SVM classifier. Recalled that the \u201dG SFisher\u201d algorithm actually performs a sparse Fisher classification which only\nuses 500 features and achieves a comparable performance of the \u201dG+FC\u201d method using 131, 072 original features both in terms of accuracy and ROC curve (the accuracy is slightly lower, but the ROC performance is better). This phenomena further demonstrates the effectiveness of the proposed feature selection frame. Again, SFisher and especially SHK perform better than SSMES in the sense of both feature selection and classification in this dataset, which can be attributed to the consideration of the bias and adaptive margin in the linear model (2)."}, {"heading": "6 CONCLUSION", "text": "We have presented a novel feature selection algorithm based on well-grounded sparsity-\nenforcing regularization techniques for face recognition. We cast the feature selection problem into a combinatorial sparse approximation problem by enforcing a sparsity penalty term on the MSE criterion, which can be solved by greedy methods or convex relaxation methods. Moreover, we introduced the sparsity constrain into the traditional HK procedure and proposed a sparse HK procedure to obtain simultaneously the optimal sparse solution and the corresponding margin vector of the MSE criterion. The proposed frame was applied to select most informative Gabor features for face recognition and the experimental results on CAS-PEAL-R1 face database and LFW face database are favorable to the previous state-ofthe-art Gabor-based methods. Our future work includes exploring other more effective lowlevel face representation and other sophisticated classification strategy to produce better performance."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research is partially supported by the National Natural Science Funds of China (No. 60803024 and No. 60970098), the Doctoral Program Foundation of Institutions of Higher Education of China (No. 200805331107 and No. 20090162110055), the Major Program of National Natural Science Foundation of China (No. 90715043), and the Open Project Program of the State Key Lab of CAD&CG (No. A0911 and No. A1011 ), Zhejiang University."}], "references": [{"title": "Face recognition using HOG-EBGM,", "author": ["A. Albiol", "D. Monzo", "A. Martin", "J. Sastre"], "venue": "Pattern Recogn. Lett.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Face recognition by independent component analysis,", "author": ["M.S. Bartlett", "J.R. Movellan", "T.J. Sejnowski"], "venue": "IEEE Trans. Neural Netw.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection,", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "On the use of SIFT features for face authentication,", "author": ["M. Bicego", "A. Lagorio", "E. Grosso", "M. Tistarelli"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern RecognitionWorkshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Face Recognition with Learning-based Descriptor,\u201d,Proc", "author": ["Z.M. Cao", "Q. Yin", "X.O. Tang", "J. Sun"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "LIBSVM: a library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "ntu. edu. tw/cjlin/libsvm", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "A regularized framework for feature selection in face detection and authentication", "author": ["A. Destrero", "C.De Mol", "F. Odone", "A. Verri"], "venue": "Int. J. Comput. Vis.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Optimal Gabor filters for texture segmentation,", "author": ["D. Dunn", "W. Higgins"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "The CAS-PEAL large-scale chinese face database and baseline evaluations,", "author": ["W. Gao", "B. Cao", "S.G. Shan", "X.L. Chen", "D.L. Zhou", "X.H. Zhang", "D.B. Zhao"], "venue": "IEEE Trans. Syst. Man Cybern. A.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "An introduction to variable and feature selection,", "author": ["I. Guyon", "E. Elisseeff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Face recognition using laplacianfaces,", "author": ["X. He", "S. Yan", "Y. Hu", "P. Niyogi", "H.J. Zhang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Face recognition: Component-based versus global approaches,", "author": ["B. Heisele", "P. Ho", "J. Wu", "T. Poggio"], "venue": "Comput. Vis. Image Understand.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments,", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Technical Report 07-49,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Face recognition using boosted local features,", "author": ["M. Jones", "P. Viola"], "venue": "Proc. IEEE Int\u2019l Conf. Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Volterrafaces: Discriminant analysis using volterra kernels,", "author": ["R. Kumar", "A. Banerjee", "B.C. Vemuri"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Gabor volume based local binary pattern for face representation and recognition,", "author": ["Z. Lei", "S.C. Liao", "R. He", "M. Pietikainen", "S.T. Li"], "venue": "Proc. IEEE Conf. Automatic Face and Gesture Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Illumination invariant face recognition using near-infrared images,", "author": ["S.Z. Li", "R.F. Chu", "S.C. Liao", "L. Zhang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Matching pursuits with timefrequency dictionaries,", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1993}, {"title": "On the recent use of local binary patterns for face authentication,", "author": ["S. Marcel", "Y. Rodriguez", "G. Heusch"], "venue": "Int. J. Image Video Process, Special Issue on Facial Image Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Using biologically inspired features for face processing,", "author": ["E. Meyers", "L. Wolf"], "venue": "Int. J. Comput. Vis.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Beyond Eigenfaces: Probabilistic matching for face recognition,", "author": ["B. Moghaddam", "Wahid W", "A. Pentland"], "venue": "Proc. IEEE Conf. Automatic Face and Gesture Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Local feature analysis: A general statistical theory for object representation,", "author": ["P. Penev", "J. Atick"], "venue": "Network: Computationin Neural Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "AdaBoost Gabor Fisher classifier for face recognition,", "author": ["S.G. Shan", "P. Yang", "X.L. Chen", "W. Gao"], "venue": "Proc. IEEE Int. Workshop Analysis and Modeling of Faces and Gestures,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Adaboost Gabor feature selection for classification,", "author": ["L. Shen", "L. Bai"], "venue": "Proc. Int\u2019l Conf. Image and Vision Computing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Recognition of faces in unconstrained environments: A comparative study,", "author": ["J. Ruiz del Solar", "R. Verschae", "M. Correa"], "venue": "EURASIP J. ADV. SIG. PR.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso,", "author": ["R. Tibshirani"], "venue": "J Roy. Statist. Soc. B,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1996}, {"title": "Greed is good: Algorithmic results for sparse approximation,", "author": ["J.A. Tropp"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Topics in sparse approximation,", "author": ["J.A. Tropp"], "venue": "Ph.D. dissertation, Univ. of Texas at Austin, Austin,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit,", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Eigenfaces for recognition,", "author": ["M. Turk", "A. Pentland"], "venue": "J. Cogn. Neurosci.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1991}, {"title": "Towards a practical face recognition system: Robust registration and illumination by sparse Representation,", "author": ["A. Wagner", "J. Wright", "A. Ganesh", "Z.H. Zhou", "Y. Ma"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Boosted multi-task learning for face verification with applications to web images and video search,", "author": ["X.G. Wang", "C. Zhang", "Z.Y. Zhang"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Face recognition by elastic bunch graph matching,", "author": ["L. Wiskott", "J.M. Fellous", "N. Kruger", "C.von der Malsburg"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1997}, {"title": "Robust face recognition via sparse representation,", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Implicit elastic matching with random projections for pose-variant face recognition,", "author": ["J. Wright", "G. Hua"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Exploring feature descriptors for face recognition,", "author": ["S.C. Yan", "H. Wang", "X.O. Tang", "T. Huang"], "venue": "Proc. IEEE Int\u2019l Conf. Acoustics, Speech, and Signal Processing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Face recognition using Ada-boosted Gabor features,", "author": ["P. Yang", "S.G. Shan", "W. Gao", "S.Z. Li", "D. Zhang"], "venue": "Proc. IEEE Conf. Automatic Face and Gesture Recognition,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2004}, {"title": "Boosting local binary pattern (LBP)-based face recognition,", "author": ["G.C. Zhang", "X.S. Huang", "S.Z. Li", "Y.S.Wang", "X.H. Wu"], "venue": "Proc. Advances in Biometric Person Authentication,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2004}, {"title": "P.Phillips, and A.Rosenfeld, \u201dFace recognition: A literature survey,", "author": ["W. Zhao", "R. Chellappa"], "venue": "ACM Comp. Survey,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2003}], "referenceMentions": [{"referenceID": 38, "context": "W ITHIN the last several decades, face recognition has received extensive attention due to its wide range of application from identity authentication, access control and surveillance to human-computer interaction and numerous novel face recognition algorithms have been proposed [43], [28].", "startOffset": 279, "endOffset": 283}, {"referenceID": 24, "context": "W ITHIN the last several decades, face recognition has received extensive attention due to its wide range of application from identity authentication, access control and surveillance to human-computer interaction and numerous novel face recognition algorithms have been proposed [43], [28].", "startOffset": 285, "endOffset": 289}, {"referenceID": 29, "context": "After the introduction of the well-known Eigenfaces [33], the holistic representation meth-", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "cn ods were extensively studied [4], [24], [3], [13], [37], [34].", "startOffset": 32, "endOffset": 35}, {"referenceID": 20, "context": "cn ods were extensively studied [4], [24], [3], [13], [37], [34].", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "cn ods were extensively studied [4], [24], [3], [13], [37], [34].", "startOffset": 43, "endOffset": 46}, {"referenceID": 10, "context": "cn ods were extensively studied [4], [24], [3], [13], [37], [34].", "startOffset": 48, "endOffset": 52}, {"referenceID": 33, "context": "cn ods were extensively studied [4], [24], [3], [13], [37], [34].", "startOffset": 54, "endOffset": 58}, {"referenceID": 30, "context": "cn ods were extensively studied [4], [24], [3], [13], [37], [34].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "Local feature analysis (LFA) [25] pioneers the study of local representation for face recognition.", "startOffset": 29, "endOffset": 33}, {"referenceID": 32, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 35, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 19, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 148, "endOffset": 152}, {"referenceID": 14, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 154, "endOffset": 158}, {"referenceID": 34, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 160, "endOffset": 164}, {"referenceID": 4, "context": "Recently, local representation approaches have received more attention and have shown more promising results [36], [14], [16], [5], [1], [39], [2], [23], [17], [38], [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 13, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 90, "endOffset": 93}, {"referenceID": 35, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 129, "endOffset": 133}, {"referenceID": 32, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 156, "endOffset": 160}, {"referenceID": 15, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 201, "endOffset": 205}, {"referenceID": 19, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 229, "endOffset": 233}, {"referenceID": 4, "context": "such as Haar-like features [16], SIFT features [5], histograms of oriented gradient (HOG) [2], edge orientation histograms (EOH) [39], Gabor features [36], [18], local binary patterns (LBP) [1], [18], [22], Bio-inspired features [23], learned descriptor [6] etc.", "startOffset": 254, "endOffset": 257}, {"referenceID": 9, "context": "large cardinality is often limited and a consistent theory is still missing, numerous learned methods are emerging in the empirical practice due to their effectiveness (refer to [12] for an excellent review of feature selection approaches in machine learning).", "startOffset": 178, "endOffset": 182}, {"referenceID": 13, "context": "[16], [42], [40], [27], [26], [19], [35].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[16], [42], [40], [27], [26], [19], [35].", "startOffset": 6, "endOffset": 10}, {"referenceID": 36, "context": "[16], [42], [40], [27], [26], [19], [35].", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "[16], [42], [40], [27], [26], [19], [35].", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "[16], [42], [40], [27], [26], [19], [35].", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "[16], [42], [40], [27], [26], [19], [35].", "startOffset": 30, "endOffset": 34}, {"referenceID": 31, "context": "[16], [42], [40], [27], [26], [19], [35].", "startOffset": 36, "endOffset": 40}, {"referenceID": 25, "context": "An alternative is the regularizedbased method which sparsify with respect to a dictionary of features by the sparsity-enforcing regularization techniques [29], [8].", "startOffset": 154, "endOffset": 158}, {"referenceID": 6, "context": "An alternative is the regularizedbased method which sparsify with respect to a dictionary of features by the sparsity-enforcing regularization techniques [29], [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 6, "context": "The main merits of such a regularized approach are its effectiveness even in the presence of a very small number of data coupled with the fact that it is supported by well-grounded theory [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 17, "context": "penalty term on the MSE criterion and the solution can be obtained by greedy methods such as matching pursuit (MP) [21] or the orthogonal matching pursuit (OMP) [30], [31], [32] and convex relaxation methods [31].", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "penalty term on the MSE criterion and the solution can be obtained by greedy methods such as matching pursuit (MP) [21] or the orthogonal matching pursuit (OMP) [30], [31], [32] and convex relaxation methods [31].", "startOffset": 161, "endOffset": 165}, {"referenceID": 27, "context": "penalty term on the MSE criterion and the solution can be obtained by greedy methods such as matching pursuit (MP) [21] or the orthogonal matching pursuit (OMP) [30], [31], [32] and convex relaxation methods [31].", "startOffset": 167, "endOffset": 171}, {"referenceID": 28, "context": "penalty term on the MSE criterion and the solution can be obtained by greedy methods such as matching pursuit (MP) [21] or the orthogonal matching pursuit (OMP) [30], [31], [32] and convex relaxation methods [31].", "startOffset": 173, "endOffset": 177}, {"referenceID": 27, "context": "penalty term on the MSE criterion and the solution can be obtained by greedy methods such as matching pursuit (MP) [21] or the orthogonal matching pursuit (OMP) [30], [31], [32] and convex relaxation methods [31].", "startOffset": 208, "endOffset": 212}, {"referenceID": 6, "context": "Our method may be mainly inspired by the work [8] which is also applying the sparse regularized term to the linear model to perform the feature selection.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "Nevertheless, the linear model in [8] neglects the bias on the one hand and only enforces the linear dependence between the feature components and the class labels on the other hand.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Our method starts off with the MSE criterion and considers simultaneously the bias and the adaptive margin vector and hence can be seemed as a generalization of the method in [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 6, "context": "Moreover, in [8] the sparse solution is obtained through iterative soft-thresholding method and the convergence relies on the careful normalization of each features component of all training samples at a time, which may de-", "startOffset": 13, "endOffset": 16}, {"referenceID": 20, "context": "personal variation [24], we focus on a binary", "startOffset": 19, "endOffset": 23}, {"referenceID": 27, "context": "solution to SMSE criterion (4) and no others [31].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Moreover, if we set b = 1n and \u03c90 = 0 (we refer to the resulting algorithm as simplified SMES procedure, or SSMES), in this special case the RSMSE criterion (5) degenerates into the linear model described in [8], and thus our method can also be seemed as a generalization of the method in [8].", "startOffset": 208, "endOffset": 211}, {"referenceID": 6, "context": "Moreover, if we set b = 1n and \u03c90 = 0 (we refer to the resulting algorithm as simplified SMES procedure, or SSMES), in this special case the RSMSE criterion (5) degenerates into the linear model described in [8], and thus our method can also be seemed as a generalization of the method in [8].", "startOffset": 289, "endOffset": 292}, {"referenceID": 32, "context": "We start with the widely used Gabor representation because the kernels of Gabor filters are similar to the 2D receptive field profiles of the mammalian cortical simple cells and exhibit desirable characteristics of spatial locality and orientation selectivity [36], [40], [26].", "startOffset": 260, "endOffset": 264}, {"referenceID": 36, "context": "We start with the widely used Gabor representation because the kernels of Gabor filters are similar to the 2D receptive field profiles of the mammalian cortical simple cells and exhibit desirable characteristics of spatial locality and orientation selectivity [36], [40], [26].", "startOffset": 266, "endOffset": 270}, {"referenceID": 22, "context": "We start with the widely used Gabor representation because the kernels of Gabor filters are similar to the 2D receptive field profiles of the mammalian cortical simple cells and exhibit desirable characteristics of spatial locality and orientation selectivity [36], [40], [26].", "startOffset": 272, "endOffset": 276}, {"referenceID": 7, "context": "As suggested in [10], we truncate the Gabor filters to six times the span of the Gaussian function.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "[24], we temporarily cast the face recognition into a classification of the intra-personal (hereafter as positive) and extra-personal (hereafter as negative) variation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "A by-no-means negligible problem in practical is the overwhelmingly huge size and unbalance of the training samples [16].", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "However, one can also consider its usage as a pure feature selection tool to reduce the numbers of Gabor features and adopt some other common classifiers such as nearest neighbor classifier (NNC), Fisher classifier (FC) [9] or support vector machines (SVM) [7] for the recognition.", "startOffset": 257, "endOffset": 260}, {"referenceID": 8, "context": "In order to evaluate the proposed approach, we carry out some experiments on two large face databases: CAS-PEAL-R1 [11] and LFW [15] face database.", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "In order to evaluate the proposed approach, we carry out some experiments on two large face databases: CAS-PEAL-R1 [11] and LFW [15] face database.", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "We restrictively follow the CAS-PEAL-R1 evaluation protocol which specifies one training set, one gallery set and six probe sets [11].", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "The third method, \u201dG Ada+FC\u201d, is the AGFC method in [26] which using Adaboost to select Gabor features and FC for classification.", "startOffset": 52, "endOffset": 56}], "year": 2011, "abstractText": "Inspired by biological vision systems, the over-complete local features with huge cardinality are increasingly used for face recognition during the last decades. Accordingly, feature selection has become more and more important and plays a critical role for face data description and recognition. In this paper, we propose a trainable feature selection algorithm based on the regularized frame for face recognition. By enforcing a sparsity penalty term on the minimum squared error (MSE) criterion, we cast the feature selection problem into a combinatorial sparse approximation problem, which can be solved by greedy methods or convex relaxation methods. Moreover, based on the same frame, we propose a sparse Ho-Kashyap (HK) procedure to obtain simultaneously the optimal sparse solution and the corresponding margin vector of the MSE criterion. The proposed methods are used for selecting the most informative Gabor features of face images for recognition and the experimental results on benchmark face databases demonstrate the effectiveness of the", "creator": "LaTeX with hyperref package"}}}