{"id": "1102.4807", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2011", "title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "abstract": "vawter We elaheh analyze a h\u1ecdc class of phillie estimators based on scott-heron convex hattin relaxation for solving stabilizers high - dimensional matrix decomposition orientable problems. The alike observations are the ods noisy realizations of surucu the institutes sum borodino of asur an (garretson appproximately) elang low rank matrix $ \\ villancicos Theta ^ \\ e-charkhi star $ burbot with cdk1 a belgrad second h\u00e4me matrix $ \\ Gamma ^ \\ star $ channa endowed eso with migros a affected complementary form of dolgorsuren low - dimensional rohan structure. withyham We oxford derive fussier a nighty general oswiu theorem cheerless that gives upper bounds reister on the Frobenius norm xviii error zibri for an estimate of the masochism pair $ (\\ Theta ^ \\ kilbarrack star, \\ lukovic Gamma ^ \\ star) $ bouchardeau obtained by mys solving hegemonist a convex optimization problem afpak that pelecanos combines shige the nuclear norm hanky with limfjord a aharonian general keenest decomposable yansheng regularizer. 3,500-square We headlight then specialize this malmberg result izamal to two cartland cases 585 that marilu have madain been derrell studied in nurkan the incidentals context sammons of robust 15,000 PCA: liveried low vukovich rank plus fascinate an grifols entrywise sparse matrix, hallway and low tikriti rank clijster plus a krekorian columnwise ferruzzi sparse rogal matrix. xuy\u00ean For baran\u00f3w both axelsson models, 1,313 our shochiku theory yields non - rendina asymptotic Frobenius robertson error bounds belaga for both for deterministic thickenings and zenn stochastic curies noise matrices, negotiators and applies shind to matrices $ \\ caernarfonshire Theta ^ \\ 3,588 star $ 87th that can early-to-mid be 1-oscar exactly or goodykprimenet.com approximately low commuter rank, oaxacan and matrices $ \\ ayun Gamma ^ \\ perplexing star $ that inbhir can galliquio be sectionals exactly 97.16 or freyre approximately sparse. dabangg Moreover, for chahal the 15-12 case of 29-day stochastic noise 2262 matrices, sladek we beryllium establish bjarni matching lower chaly bounds marufuji on ixodes the zfa minimax wazed error, 12:47 showing that our 3-5 results anthologized cannot be fosb improved forths beyond maffin constant luo factors. The sharpness of tinniswood our moratoria theoretical 35-year-old predictions is transrapid confirmed by dulness numerical mexx simulations.", "histories": [["v1", "Wed, 23 Feb 2011 18:02:53 GMT  (368kb)", "https://arxiv.org/abs/1102.4807v1", null], ["v2", "Mon, 18 Jul 2011 02:25:47 GMT  (379kb)", "http://arxiv.org/abs/1102.4807v2", null], ["v3", "Tue, 6 Mar 2012 06:59:59 GMT  (384kb)", "http://arxiv.org/abs/1102.4807v3", "41 pages, 2 figures"]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["alekh agarwal", "sahand negahban", "martin j wainwright"], "accepted": true, "id": "1102.4807"}, "pdf": {"name": "1102.4807.pdf", "metadata": {"source": "CRF", "title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "authors": ["Alekh Agarwal", "Sahand Negahban", "Martin J. Wainwright"], "emails": ["alekh@eecs.berkeley.edu", "sahand@eecs.berkeley.edu", "wainwrig@stat.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 2.\n48 07\nv3 [\nst at\n.M L\n] 6\nWe analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are noisy realizations of a linear transformation X of the sum of an (approximately) low rank matrix \u0398\u22c6 with a second matrix \u0393\u22c6 endowed with a complementary form of low-dimensional structure; this set-up includes many statistical models of interest, including forms of factor analysis, multi-task regression with shared structure, and robust covariance estimation. We derive a general theorem that gives upper bounds on the Frobenius norm error for an estimate of the pair (\u0398\u22c6,\u0393\u22c6) obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. Our results are based on imposing a \u201cspikiness\u201d condition that is related to but milder than singular vector incoherence. We specialize our general result to two cases that have been studied in past work: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields non-asymptotic Frobenius error bounds for both deterministic and stochastic noise matrices, and applies to matrices \u0398\u22c6 that can be exactly or approximately low rank, and matrices \u0393\u22c6 that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices and the identity observation operator, we establish matching lower bounds on the minimax error, showing that our results cannot be improved beyond constant factors. The sharpness of our theoretical predictions is confirmed by numerical simulations."}, {"heading": "1 Introduction", "text": "The focus of this paper is a class of high-dimensional matrix decomposition problems of the following variety. Suppose that we observe a matrix Y \u2208 Rd1\u00d7d2 that is (approximately) equal to the sum of two unknown matrices: how to recover good estimates of the pair? Of course, this problem is ill-posed in general, so that it is necessary to impose some kind of low-dimensional structure on the matrix components, one example being rank constraints. The framework of this paper supposes that one matrix component (denoted \u0398\u22c6) is low-rank, either exactly or in an approximate sense, and allows for general forms of low-dimensional structure for the second component \u0393\u22c6. Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].\nProblems of matrix decomposition are motivated by a variety of applications. Many classical methods for dimensionality reduction, among them factor analysis and principal\ncomponents analysis (PCA), are based on estimating a low-rank matrix from data. Different forms of robust PCA can be formulated in terms of matrix decomposition using the matrix \u0393\u22c6 to model the gross errors [9, 7, 29]. Similarly, certain problems of robust covariance estimation can be described using matrix decompositions with a column/row-sparse structure, as we describe in this paper. The problem of low rank plus sparse matrix decomposition also arises in Gaussian covariance selection with hidden variables [8], in which case the inverse covariance of the observed vector can be decomposed as the sum of a sparse matrix with a low rank matrix. Matrix decompositions also arise in multi-task regression [32, 21, 27], which involve solving a collection of regression problems, referred to as tasks, over a common set of features. For some features, one expects their weighting to be preserved across features, which can be modeled by a low-rank constraint, whereas other features are expected to vary across tasks, which can be modeled by a sparse component [5, 2]. See Section 2.1 for further discussion of these motivating applications.\nIn this paper, we study a noisy linear observation that can be used to describe a number of applications in a unified way. Let X be a linear operator that maps matrices in Rd1\u00d7d2 to matrices in Rn1\u00d7n2 . In the simplest of cases, this observation operator is simply the identity mapping, so that we necessarily have n1 = d1 and n2 = d2. However, as we discuss in the sequel, it is useful for certain applications, such as multi-task regression, to consider more general linear operators of this form. Hence, we study the problem matrix decomposition for the general linear observation model\nY = X(\u0398\u22c6 + \u0393\u22c6) +W, (1)\nwhere \u0398\u22c6 and \u0393\u22c6 are unknown d1 \u00d7 d2 matrices, and W \u2208 Rn1\u00d7n2 is some type of observation noise; it is potentially dense, and can either be deterministic or stochastic. The matrix \u0398\u22c6 is assumed to be either exactly low-rank, or well-approximated by a low-rank matrix, whereas the matrix \u0393\u22c6 is assumed to have a complementary type of low-dimensional structure, such as sparsity. As we discuss in Section 2.1, a variety of interesting statistical models can be formulated as instances of the observation model (1). Such models include versions of factor analysis involving non-identity noise matrices, robust forms of covariance estimation, and multi-task regression with some features shared across tasks, and a sparse subset differing across tasks. Given this observation model, our goal is to recover accurate estimates of the decomposition (\u0398\u22c6,\u0393\u22c6) based on the noisy observations Y . In this paper, we analyze simple estimators based on convex relaxations involving the nuclear norm, and a second general norm R.\nMost past work on the model (1) has focused on the noiseless setting (W = 0), and for the identity observation operator (so that X(\u0398\u22c6 + \u0393\u22c6) = \u0398\u22c6 + \u0393\u22c6). Chandrasekaran et al. [9] studied the case when \u0393\u22c6 is assumed to sparse, with a relatively small number s \u226a d1d2 of non-zero entries. In the noiseless setting, they gave sufficient conditions for exact recovery for an adversarial sparsity model, meaning the non-zero positions of \u0393\u22c6 can be arbitrary. Subsequent work by Candes et al. [7] analyzed the same model but under an assumption of random sparsity, meaning that the non-zero positions are chosen uniformly at random. In very recent work, Xu et al. [29] have analyzed a different model, in which the matrix \u0393\u22c6 is assumed to be columnwise sparse, with a relatively small number s \u226a d2 of non-zero columns. Their analysis guaranteed approximate recovery for the low-rank matrix, in particular for the uncorrupted columns. After initial posting of this work, we became aware of recent work by Hsu et al. [14], who derived Frobenius norm error bounds for the case of exact elementwise sparsity. As we discuss in more detail in Section 3.4, in this special case, our bounds are based\non milder conditions, and yield sharper rates for problems where the rank and sparsity scale with the dimension.\nOur main contribution is to provide a general oracle-type result (Theorem 1) on approximate recovery of the unknown decomposition from noisy observations, valid for structural constraints on \u0393\u22c6 imposed via a decomposable regularizer. The class of decomposable regularizers, introduced in past work by Negahban et al. [19], includes the elementwise \u21131-norm and columnwise (2, 1)-norm as special cases, as well as various other regularizers used in practice. Our main result is stated in Theorem 1: it provides finite-sample guarantees for estimates obtained by solving a class of convex programs formed using a composite regularizer. The resulting Frobenius norm error bounds consist of multiple terms, each of which has a natural interpretation in terms of the estimation and approximation errors associated with the subproblems of recovering \u0398\u22c6 and \u0393\u22c6. We then specialize Theorem 1 to the case of elementwise or columnwise sparsity models for \u0393\u22c6, thereby obtaining recovery guarantees for matrices \u0398\u22c6 that may be either exactly or approximately low-rank, as well as matrices \u0393\u22c6 that may be either exactly or approximately sparse. We provide non-asymptotic error bounds for general noise matrices W both for elementwise and columnwise sparse models (see Corollaries 1 through Corollary 6). To the best of our knowledge, these are the first results that apply to this broad class of models, allowing for noisiness (W 6= 0) that is either stochastic or deterministic, matrix components that are only approximately low-rank and/or sparse, and general forms of the observation operator X.\nIn addition, the error bounds obtained by our analysis are sharp, and cannot be improved in general. More precisely, for the case of stochastic noise matrices and the identity observation operator, we prove that the squared Frobenius errors achieved by our estimators are minimaxoptimal (see Theorem 2). An interesting feature of our analysis is that, in contrast to previous work [9, 29, 7], we do not impose incoherence conditions on the singular vectors of \u0398\u22c6; rather, we control the interaction with a milder condition involving the dual norm of the regularizer. In the special case of elementwise sparsity, this dual norm enforces an upper bound on the \u201cspikiness\u201d of the low-rank component, and has proven useful in the related setting of noisy matrix completion [20]. This constraint is not strong enough to guarantee identifiability of the models (and hence exact recovery in the noiseless setting), but it does provide a bound on the degree of non-identifiability. We show that this same term arises in both the upper and lower bounds on the problem of approximate recovery that is of interest in the noisy setting.\nThe remainder of the paper is organized as follows. In Section 2, we set up the problem in a precise way, and describe the estimators. Section 3 is devoted to the statement of our main result on achievability, as well as its various corollaries for special cases of the matrix decomposition problem. We also state a matching lower bound on the minimax error for matrix decomposition with stochastic noise. In Section 4, we provide numerical simulations that illustrate the sharpness of our theoretical predictions. Section 5 is devoted to the proofs of our results, with certain more technical aspects of the argument deferred to the appendices, and we conclude with a discussion in Section 6.\nNotation: For the reader\u2019s convenience, we summarize here some of the standard notation used throughout this paper. For any matrix M \u2208 Rd1\u00d7d2 , we define the Frobenius norm |||M |||F : = \u221a\u2211d1 j=1 \u2211d2 k=1M 2 jk, corresponding to the ordinary Euclidean norm of its entries. We denote its singular values by \u03c31(M) \u2265 \u03c32(M) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3d(M) \u2265 0, where d = min{d1, d2}. Its nuclear norm is given by |||M |||N = \u2211d j=1 \u03c3j(M)."}, {"heading": "2 Convex relaxations and matrix decomposition", "text": "In this paper, we consider a family of regularizers formed by a combination of the nuclear norm |||\u0398|||N : = \u2211min{d1,d2} j=1 \u03c3j(\u0398), which acts as a convex surrogate to a rank constraint for \u0398 \u22c6 (e.g., see Recht et al. [25] and references therein), with a norm-based regularizer R : Rd1\u00d7d2 \u2192 R+ used to constrain the structure of \u0393\u22c6. We provide a general theorem applicable to a class of regularizers R that satisfy a certain decomposability property [19], and then consider in detail a few particular choices of R that have been studied in past work, including the elementwise \u21131-norm, and the columnwise (2, 1)-norm (see Examples 4 and 5 below)."}, {"heading": "2.1 Some motivating applications", "text": "We begin with some motivating applications for the general linear observation model with noise (1).\nExample 1 (Factor analysis with sparse noise). In a factor analysis model, random vectors Zi \u2208 Rd are assumed to be generated in an i.i.d. manner from the model\nZi = LUi + \u03b5i, for i = 1, 2, . . . , n, (2)\nwhere L \u2208 Rd1\u00d7r is a loading matrix, and the vectors Ui \u223c N(0, Ir\u00d7r) and \u03b5i \u223c N(0,\u0393\u22c6) are independent. Given n i.i.d. samples from the model (2), the goal is to estimate the loading matrix L, or the matrix LLT that projects onto column span of L. A simple calculation shows that the covariance matrix of Zi has the form \u03a3 = LL\nT + \u0393\u22c6. Consequently, in the special case when \u0393\u22c6 = \u03c32Id\u00d7d, then the range of L is spanned by the top r eigenvectors of \u03a3, and so we can recover it via standard principal components analysis.\nIn other applications, we might no longer be guaranteed that \u0393\u22c6 is the identity, in which case the top r eigenvectors of \u03a3 need not be close to column span of L. Nonetheless, when \u0393\u22c6 is a sparse matrix, the problem of estimating LLT can be understood as an instance of our general observation model (1) with d1 = d2 = d, and the identity observation operator X (so that n1 = n2 = d). In particular, if the let the observation matrix Y \u2208 Rd\u00d7d be the sample covariance matrix 1n \u2211n i\u22121 ZiZ T i , then some algebra shows that Y = \u0398\n\u22c6 + \u0393\u22c6 + W , where \u0398\u22c6 = LLT is of rank r, and the random matrix W is a re-centered form of Wishart noise [1]\u2014in particular, the zero-mean matrix\nW : = 1\nn\nn\u2211\ni=1\nZiZ T i \u2212\n{ LLT + \u0393\u22c6 } . (3)\nWhen \u0393\u22c6 is assumed to be elementwise sparse (i.e., with relatively few non-zero entries), then this constraint can be enforced via the elementwise \u21131-norm (see Example 4 to follow). \u2663\nExample 2 (Multi-task regression). Suppose that we are given a collection of d2 regression problems in Rd1 , each of the form yj = X\u03b2 \u2217 j + wj for j = 1, 2, . . . , d2. Here each \u03b2\u2217j \u2208 Rd1 is an unknown regression vector, wj \u2208 Rn is observation noise, and X \u2208 Rn\u00d7d1 is the design matrix. This family of models can be written in a convenient matrix form as Y = XB\u2217 +W , where Y = [y1 \u00b7 \u00b7 \u00b7 yd2 ] and W = [w1 \u00b7 \u00b7 \u00b7 wd2 ] are both matrices in Rn\u00d7d2 and B\u2217 : = [\u03b2\u22171 \u00b7 \u00b7 \u00b7 \u03b2\u2217d2 ] \u2208 R\nd1\u00d7d2 is a matrix of regression vectors. Following standard terminology in multi-task learning, we refer to each column of B\u2217 as a task, and each row of B\u2217 as a feature.\nIn many applications, it is natural to assume that the feature weightings\u2014i.e., that is, the vectors \u03b2\u2217j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27]. This type of shared structured can be modeled by imposing a low-rank structure; for instance, in the extreme case of rank one, it would enforce that each \u03b2\u2217j is a multiple of some common underlying vector. However, many multi-task learning problems exhibit more complicated structure, in which some subset of features are shared across tasks, and some other subset of features vary substantially across tasks [2, 4]. For instance, in the Amazon recommendation system, tasks correspond to different classes of products, such as books, electronics and so on, and features include ratings by users. Some ratings (such as numerical scores) should have a meaning that is preserved across tasks, whereas other features (e.g., the label \u201cboring\u201d) are very meaningful in applications to some categories (e.g., books) but less so in others (e.g., electronics).\nThis kind of structure can be captured by assuming that the unknown regression matrix B\u2217 has a low-rank plus sparse decomposition\u2014namely, B\u2217 = \u0398\u22c6+\u0393\u22c6 where \u0398\u22c6 is low-rank and \u0393\u22c6 is sparse, with a relatively small number of non-zero entries, corresponding to feature/task pairs that that differ significantly from the baseline. A variant of this model is based on instead assuming that \u0393\u22c6 is row-sparse, with a small number of non-zero rows. (In Example 5 to follow, we discuss an appropriate regularizer for enforcing such row or column sparsity.) With this model structure, we then define the observation operator X : Rd1\u00d7d2 \u2192 Rn\u00d7d2 via A 7\u2192 XA, so that n1 = n and n2 = d2 in our general notation. In this way, we obtain another instance of the linear observation model (1). \u2663\nExample 3 (Robust covariance estimation). For i = 1, 2, . . . , n, let Ui \u2208 Rd be samples from a zero-mean distribution with unknown covariance matrix \u0398\u22c6. When the vectors Ui are observed without any form of corruption, then it is straightforward to estimate \u0398\u22c6 by performing PCA on the sample covariance matrix. Imagining that j \u2208 {1, 2, . . . , d} indexes different individuals in the population, now suppose that the data associated with some subset S of individuals is arbitrarily corrupted. This adversarial corruption can be modeled by assuming that we observe the vectors Zi = Ui + vi for i = 1, . . . , n, where each vi \u2208 Rd is a vector supported on the subset S. Letting Y = 1n \u2211n i=1 ZiZ T i be the sample covariance matrix of the corrupted samples, some algebra shows that it can be decomposed as Y = \u0398\u22c6+\u2206+W , whereW : = 1n \u2211n i=1 UiU T i \u2212\u0398\u22c6 is again a type of re-centered Wishart noise, and the remaining term can be written as\n\u2206 := 1\nn\nn\u2211\ni=1\nviv T i +\n1\nn\nn\u2211\ni=1\n( Uiv T i + viU T i ). (4)\nNote that \u2206 itself is not a column-sparse or row-sparse matrix; however, since each vector vi \u2208 Rd is supported only on some subset S \u2282 {1, 2, . . . , d}, we can write \u2206 = \u0393\u22c6 + (\u0393\u22c6)T , where \u0393\u22c6 is a column-sparse matrix with entries only in columns indexed by S. This structure can be enforced by the use of the column-sparse regularizer (12), as described in Example 5 to follow.\n\u2663"}, {"heading": "2.2 Convex relaxation for noisy matrix decomposition", "text": "Given the observation model Y = X(\u0398\u22c6 + \u0393\u22c6) + W , it is natural to consider an estimator based on solving the regularized least-squares program\nmin (\u0398,\u0393)\n{ 1\n2 |||Y \u2212 X(\u0398 + \u0393)|||2F + \u03bbd|||\u0398|||N + \u00b5dR(\u0393)\n} .\nHere (\u03bbd, \u00b5d) are non-negative regularizer parameters, to be chosen by the user. Our theory also provides choices of these parameters that guarantee good properties of the associated estimator. Although this estimator is reasonable, it turns out that an additional constraint yields an equally simple estimator that has attractive properties, both in theory and in practice.\nIn order to understand the need for an additional constraint, it should be noted that without further constraints, the model (1) is unidentifiable, even in the noiseless setting (W = 0). Indeed, as has been discussed in past work [9, 7, 29], no method can recover the components (\u0398\u22c6,\u0393\u22c6) unless the low-rank component is \u201cincoherent\u201d with the matrix \u0393\u22c6. For instance, supposing for the moment that \u0393\u22c6 is a sparse matrix, consider a rank one matrix with \u0398\u22c611 6= 0, and zeros in all other positions. In this case, it is clearly impossible to disentangle \u0398\u22c6 from a sparse matrix. Past work on both matrix completion and decomposition [9, 7, 29] has ruled out these types of troublesome cases via conditions on the singular vectors of the low-rank component \u0398\u22c6, and used them to derive sufficient conditions for exact recovery in the noiseless setting (see the discussion following Example 4 for more details).\nIn this paper, we impose a related but milder condition, previously introduced in our past work on matrix completion [20], with the goal of performing approximate recovery. To be clear, this condition does not guarantee identifiability, but rather provides a bound on the radius of non-identifiability. It should be noted that non-identifiability is a feature common to many high-dimensional statistical models.1 Moreover, in the more realistic setting of noisy observations and/or matrices that are not exactly low-rank, such approximate recovery is the best that can be expected. Indeed, one of our main contributions is to establish minimaxoptimality of our rates, meaning that no algorithm can be substantially better over the matrix classes that we consider.\nFor a given regularizer R, we define the quantity \u03bad(R) := supV 6=0 |||V |||F/R(V ), which measures the relation between the regularizer and the Frobenius norm. Moreover, we define the associated dual norm\nR\u2217(U) := sup R(V )\u22641 \u3008\u3008V, U\u3009\u3009, (5)\nwhere \u3008\u3008V, U\u3009\u3009 : = trace(V TU) is the trace inner product on the space Rd1\u00d7d2 . Our estimators are based on constraining the interaction between the low-rank component and \u0393\u22c6 via the quantity\n\u03d5R(\u0398) := \u03bad(R\u2217)R\u2217(\u0398). (6)\nMore specifically, we analyze the family of estimators\nmin (\u0398,\u0393) {1 2 |||Y \u2212 X(\u0398 + \u0393)|||2F + \u03bbd |||\u0398|||N + \u00b5dR(\u0393) } , (7)\n1For instance, see the paper [23] for discussion of non-identifiability in high-dimensional sparse regression.\nsubject to \u03d5R(\u0398) \u2264 \u03b1 for some fixed parameter \u03b1."}, {"heading": "2.3 Some examples", "text": "Let us consider some examples to provide intuition for specific forms of the estimator (7), and the role of the additional constraint.\nExample 4 (Sparsity and elementwise \u21131-norm). Suppose that \u0393 \u22c6 is assumed to be sparse, with s \u226a d1d2 non-zero entries. In this case, the sum \u0398\u22c6 + \u0393\u22c6 corresponds to the sum of a low rank matrix with a sparse matrix. Motivating applications include the problem of factor analysis with a non-identity but sparse noise covariance, as discussed in Example 1, as well as certain formulations of robust PCA [7], and model selection in Gauss-Markov random fields with hidden variables [8]. Given the sparsity of \u0393\u22c6, an appropriate choice of regularizer is the elementwise \u21131-norm\nR(\u0393) = \u2016\u0393\u20161 : = d1\u2211\nj=1\nd2\u2211\nk=1\n|\u0393jk|. (8)\nWith this choice, it is straightforward to verify that\nR\u2217(Z) = \u2016Z\u2016\u221e : = max j=1,...,d1 max k=1,...,d2 |Zjk|, (9)\nand moreover, that \u03bad(R\u2217) = \u221a d1d2. Consequently, in this specific case, the general convex program (7) takes the form\nmin (\u0398,\u0393) {1 2 |||Y \u2212 X(\u0398 + \u0393)|||2F + \u03bbd |||\u0398|||N + \u00b5d \u2016\u0393\u20161 } such that \u2016\u0398\u2016\u221e \u2264 \u03b1\u221ad1 d2 . (10)\nThe constraint involving \u2016\u0398\u2016\u221e serves to control the \u201cspikiness\u201d of the low rank component, with larger settings of \u03b1 allowing for more spiky matrices. Indeed, this type of spikiness control has proven useful in analysis of nuclear norm relaxations for noisy matrix completion [20]. To gain intuition for the parameter \u03b1, if we consider matrices with |||\u0398|||F \u2248 1, as is appropriate to keep a constant signal-to-noise ratio in the noisy model (1), then setting \u03b1 \u2248 1 allows only for matrices for which |\u0398jk| \u2248 1/ \u221a d1d2 in all entries. If we want to permit the maximally spiky matrix with all its mass in a single position, then the parameter \u03b1 must be of the order\u221a d1d2. In practice, we are interested in settings of \u03b1 lying between these two extremes.\n\u2663\nPast work on \u21131-forms of matrix decomposition has imposed singular vector incoherence conditions that are related to but different from our spikiness condition. More concretely, if we write the SVD of the low-rank component as \u0398\u22c6 = UDV T where D is diagonal, and U \u2208 Rd1\u00d7r and V \u2208 Rd2\u00d7r are matrices of the left and right singular vectors. Singular vector incoherence bounds quantities such as\n\u2016UUT \u2212 r d1 Id1\u00d7d1\u2016\u221e, \u2016V V T \u2212 r d2 Id2\u00d7d2\u2016\u221e, and \u2016UV T \u2016\u221e. (11)\nall of which measure the degree of \u201ccoherence\u201d between the singular vectors and the canonical basis. A remarkable feature of such conditions is that they have no dependence on the singular values of \u0398\u22c6. This lack of dependence makes sense in the noiseless setting, where exact recovery is the goal. For noisy models, in contrast, one should only be concerned\nwith recovering components with \u201clarge\u201d singular values. In this context, our bound on the maximum element \u2016\u0398\u22c6\u2016\u221e, or equivalently on the quantity \u2016UDV T \u2016\u221e, is natural. Note that it imposes no constraint on the matrices UUT or V V T , and moreover it uses the diagonal matrix of singular values as a weight in the \u2113\u221e bound. Moreover, we note that there are many matrices for which \u2016\u0398\u22c6\u2016\u221e satisfies a reasonable bound, whereas the incoherence measures are poorly behaved (e.g., see Section 3.4.2 in the paper [20] for one example).\nExample 5 (Column-sparsity and block columnwise regularization). Other applications involve models in which \u0393\u22c6 has a relatively small number s \u226a d2 of non-zero columns (or a relatively small number s \u226a d1 of non-zero rows). Such applications include the multi-task regression problem from Example 2, the robust covariance problem from Example 3, as well as a form of robust PCA considered by Xu et al. [29]. In this case, it is natural to constrain \u0393 via the (2, 1)-norm regularizer\nR(\u0393) = \u2016\u0393\u20162,1 : = d2\u2211\nk=1\n\u2016\u0393k\u20162, (12)\nwhere \u0393k is the k th column of \u0393 (or the (1, 2)-norm regularizer that enforces the analogous constraint on the rows of \u0393). For this choice, it can be verified that\nR\u2217(U) = \u2016U\u20162,\u221e : = max k=1,2,...,d2 \u2016Uk\u20162, (13)\nwhere Uk denotes the k th column of U , and that \u03bad(R\u2217) = \u221a d2. Consequently, in this specific case, the general convex program (7) takes the form\nmin (\u0398,\u0393) {1 2 |||Y \u2212 X(\u0398 + \u0393)|||2F + \u03bbd |||\u0398|||N + \u00b5d \u2016\u0393\u20162,1 } such that \u2016\u0398\u20162,\u221e \u2264 \u03b1\u221ad2 . (14)\nAs before, the constraint \u2016\u0398\u20162,\u221e serves to limit the \u201cspikiness\u201d of the low rank component, where in this case, spikiness is measured in a columnwise manner. Again, it is natural to consider matrices such that |||\u0398\u22c6|||F \u2248 1, so that the signal-to-noise ratio in the observation model (1) stays fixed. Thus, if \u03b1 \u2248 1, then we are restricted to matrices for which \u2016\u0398\u22c6k\u20162 \u2248 1\u221a d2 for all columns k = 1, 2, . . . , d2. At the other extreme, in order to permit a maximally \u201ccolumn-spiky\u201d matrix (i.e., with a single non-zero column of \u21132-norm roughly 1), we need to set \u03b1 \u2248 \u221a d2. As before, of practical interest are settings of \u03b1 lying between these two extremes. \u2663"}, {"heading": "3 Main results and their consequences", "text": "In this section, we state our main results, and discuss some of their consequences. Our first result applies to the family of convex programs (7) whenever R belongs to the class of decomposable regularizers, and the least-squares loss associated with the observation model satisfies a specific form of restricted strong convexity [19]. Accordingly, we begin in Section 3.1 by defining the notion of decomposability, and then illustrating how the elementwise-\u21131 and columnwise-(2, 1)-norms, as discussed in Examples 4 and 5 respectively, are both instances of decomposable regularizers. In Section 3.2, we define the form of restricted strong convexity appropriate to our setting. Section 3.3 contains the statement of our main result about the M -estimator (7), while Sections 3.4 and 3.6 are devoted to its consequences for the cases of\nelementwise sparsity and columnwise sparsity, respectively. In Section 3.5, we complement our analysis of the convex program (7) by showing that, in the special case of the identity operator, a simple two-step method can achieve similar rates (up to constant factors). We also provide an example showing that the two-step method can fail for more general observation operators. In Section 3.7, we state matching lower bounds on the minimax errors in the case of the identity operator and Gaussian noise."}, {"heading": "3.1 Decomposable regularizers", "text": "The notion of decomposability is defined in terms of a pair of subspaces, which (in general) need not be orthogonal complements. Here we consider a special case of decomposability that is sufficient to cover the examples of interest in this paper:\nDefinition 1. Given a subspace M \u2286 Rd1\u00d7d2 and its orthogonal complement M\u22a5, a normbased regularizer R is decomposable with respect (M,M\u22a5) if\nR(U + V ) = R(U) +R(V ) for all U \u2208 M, and V \u2208 M\u22a5. (15)\nTo provide some intuition, the subspace M should be thought of as the nominal model subspace; in our results, it will be chosen such that the matrix \u0393\u22c6 lies within or close to M. The orthogonal complement M\u22a5 represents deviations away from the model subspace, and the equality (15) guarantees that such deviations are penalized as much as possible.\nAs discussed at more length in Negahban et al. [19], a large class of norms are decomposable with respect to interesting2 subspace pairs. Of particular relevance to us is the decomposability of the elementwise \u21131-norm \u2016\u0393\u20161 and the columnwise (2, 1)-norm \u2016\u0393\u20162,1, as previously discussed in Examples 4 and 5 respectively.\nDecomposability of R(\u00b7) = \u2016 \u00b7 \u20161: Beginning with the elementwise \u21131-norm, given an arbitrary subset S \u2286 {1, 2, . . . , d1} \u00d7 {1, 2, . . . , d2} of matrix indices, consider the subspace pair\nM(S) := { U \u2208 Rd1\u00d7d2 | Ujk = 0 for all (j, k) /\u2208 S } , and M\u22a5(S) := (M(S))\u22a5. (16)\nIt is then easy to see that for any pair U \u2208 M(S), U \u2032 \u2208 M\u22a5(S), we have the splitting \u2016U + U \u2032\u20161 = \u2016U\u20161 + \u2016U \u2032\u20161, showing that the elementwise \u21131-norm is decomposable with respect to the pair (M(S),M\u22a5(S)).\nDecomposability of R(\u00b7) = \u2016 \u00b7 \u20162,1: Similarly, the columnwise (2, 1)-norm is also decomposable with respect to appropriately defined subspaces, indexed by subsets C \u2286 {1, 2, . . . , d2} of column indices. Indeed, using Vk to denote the k th column of the matrix V , define\nM(C) := { V \u2208 Rd1\u00d7d2 | Vk = 0 for all k /\u2208 C } , (17)\nand M\u22a5(C) := (M(C))\u22a5. Again, it is easy to verify that for any pair V \u2208 M(C), V \u2032 \u2208 M\u22a5(C), we have \u2016V + V \u2032\u20162,1 = \u2016V \u20162,1 + \u2016V \u2032\u20162,1, thus verifying the decomposability property.\nFor any decomposable regularizer and subspace M 6= {0}, we define the compatibility 2Note that any norm is (trivially) decomposable with respect to the pair (M,M\u22a5) = (Rd1\u00d7d2 , {0}).\nconstant\n\u03a8(M,R) := sup U\u2208M,U 6=0 R(U) |||U |||F . (18)\nThis quantity measures the compatibility between the Frobenius norm and the regularizer over the subspace M. For example, for the \u21131-norm and the set M(S) previously defined (16), an elementary calculation yields \u03a8 ( M(S); \u2016 \u00b7 \u20161 ) = \u221a s."}, {"heading": "3.2 Restricted strong convexity", "text": "Given a loss function, the general notion of strong convexity involves establishing a quadratic lower bound on the error in the first-order Taylor approximation [6]. In our setting, the loss is the quadratic function L(\u2126) = 12 |||Y \u2212 X(\u2126)|||2F (where we use \u2126 = \u0398 + \u0393), so that the first-order Taylor series error at \u2126 in the direction of the matrix \u2206 is given by\nL(\u2126 +\u2206)\u2212 L(\u2126)\u2212 L(\u2126)T \u2206 = 1 2 |||X(\u2206)|||2F. (19)\nConsequently, strong convexity is equivalent to a lower bound of the form 12\u2016X(\u2206)\u201622 \u2265 \u03b3 2 |||\u2206|||2F, where \u03b3 > 0 is the strong convexity constant.\nRestricted strong convexity is a weaker condition that also involves a norm defined by the regularizers. In our case, for any pair (\u00b5d, \u03bbd) of positive numbers, we first define the weighted combination of the two regularizers\u2014namely\nQ(\u0398,\u0393) := |||\u0398|||N + \u00b5d \u03bbd R(\u0393). (20)\nFor a given matrix \u2206, we can use this weighted combination to define an associated norm\n\u03a6(\u2206) := inf \u0398+\u0393=\u2206\nQ(\u0398,\u0393), (21)\ncorresponding to the minimum value of Q(\u0398,\u0393) over all decompositions of \u22063.\nDefinition 2 (RSC). The quadratic loss with linear operator X : Rd1\u00d7d2 \u2192 Rn1\u00d7n2 satisfies restricted strong convexity with respect to the norm \u03a6 and with parameters (\u03b3, \u03c4n) if\n1 2 |||X(\u2206)|||2F \u2265 \u03b3 2 |||\u2206|||2F \u2212 \u03c4n\u03a62(\u2206) for all \u2206 \u2208 Rd1\u00d7d2 . (22)\nNote that if condition (22) holds with \u03c4n = 0 and any \u03b3 > 0, then we recover the usual definition of strong convexity (with respect to the Frobenius norm). In the special case of the identity operator (i.e., X(\u0398) = \u0398), such strong convexity does hold with \u03b3 = 1. More general observation operators require different choices of the parameter \u03b3, and also non-zero choices of the tolerance parameter \u03c4n.\nWhile RSC establishes a form of (approximate) identifiability in general, here the error \u2206 is a combination of the error in estimating \u0398\u22c6 (\u2206\u0398) and \u0393\u22c6 (\u2206\u0393). Consequently, we will need a further lower bound on |||\u2206|||F in terms of |||\u2206\u0398|||F and |||\u2206\u0393|||F in the proof of our main results to demonstrate the (approximate) identifiability of our model under the RSC condition 22.\n3Defined this way, \u03a6(\u2206) is the infimal-convolution of the two norms ||| \u00b7 |||N and R, which is a very well studied object in convex analysis (see e.g. [26])"}, {"heading": "3.3 Results for general regularizers and noise", "text": "We begin by stating a result for a general observation operator X, a general decomposable regularizer R and a general noise matrix W . In later subsections, we specialize this result to particular choices of observation operator, regularizers, and stochastic noise matrices. In all our results, we measure error using the squared Frobenius norm summed across both matrices\ne2(\u0398\u0302, \u0393\u0302) := |||\u0398\u0302\u2212\u0398\u22c6|||2F + |||\u0393\u0302\u2212 \u0393\u22c6|||2F. (23)\nWith this notation, the following result applies to the observation model Y = X(\u0393\u22c6+\u0398\u22c6)+W , where the low-rank matrix satisfies the constraint \u03d5R(\u0398\u22c6) \u2264 \u03b1. Our upper bound on the squared Frobenius error consists of three terms\nK\u0398\u22c6 : = \u03bb2d \u03b32\n{ r + \u03b3\n\u03bbd\nd\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) } (24a)\nK\u0393\u22c6 : = \u00b52d \u03b32\n{ \u03a82(M;R) + \u03b3\n\u00b5d R(\u03a0 M\u22a5 (\u0393\u22c6))\n} (24b)\nK\u03c4n : = \u03c4n \u03b3\n{ d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + \u00b5d \u03bbd\nR(\u0393\u22c6M\u22a5) }2 . (24c)\nAs will be clarified shortly, these three terms correspond to the errors associated with the low-rank term (K\u0398\u22c6), the sparse term (K\u0393\u22c6), and additional error (K\u03c4n) associated with a non-zero tolerance \u03c4n 6= 0 in the RSC condition (22).\nTheorem 1. Suppose that the observation operator X satisfies the RSC condition (22) with curvature \u03b3 > 0, and a tolerance \u03c4n such that there exist integers r = 1, 2, . . . ,min{d1, d2}, for which\n128 \u03c4n r < \u03b3\n4 , and 64 \u03c4n\n( \u03a8(M;R) \u00b5d\n\u03bbd\n)2 < \u03b3\n4 . (25)\nThen if we solve the convex program (7) with regularization parameters (\u03bbd, \u00b5d) satisfying\n\u03bbd \u2265 4|||X\u2217(W )|||op, and \u00b5d \u2265 4R\u2217(X\u2217(W )) + 4 \u03b3 \u03b1\n\u03bad , (26)\nthere are universal constant cj , j = 1, 2, 3 such that for any matrix pair (\u0398 \u22c6,\u0393\u22c6) satisfying \u03d5R(\u0398\u22c6) \u2264 \u03b1 and any R-decomposable pair (M,M\u22a5), any optimal solution (\u0398\u0302, \u0393\u0302) satisfies\ne2(\u0398\u0302, \u0393\u0302) \u2264 c1K\u0398\u22c6 + c2K\u0393\u22c6 + c3K\u03c4n . (27)\nLet us make a few remarks in order to interpret the meaning of this claim.\nDeterministic guarantee: To be clear, Theorem 1 is a deterministic statement that applies to any optimum of the convex program (7). Moreover, it actually provides a whole family of upper bounds, one for each choice of the rank parameter r and each choice of the subspace pair (M,M\u22a5). In practice, these choices are optimized so as to obtain the tightest possible upper bound. As for the condition (25), it will be satisfied for a sufficiently large sample size n as long as \u03b3 > 0, and the tolerance \u03c4n decreases to zero with the sample size. In many cases of interest\u2014including the identity observation operator and multi-task cases\u2014the RSC condition holds with \u03c4n = 0, so that condition (25) holds as long as \u03b3 > 0.\nInterpretation of different terms: Let us focus first on the term K\u0398\u22c6 , which corresponds to the complexity of estimating the low-rank component. It is further sub-divided into two terms, with the term \u03bb2d r corresponding to the estimation error associated with a rank r ma-\ntrix, whereas the term \u03bbd \u2211d j=r+1 \u03c3j(\u0398 \u22c6) corresponds to the approximation error associated with representing \u0398\u22c6 (which might be full rank) by a matrix of rank r. A similar interpretation applies to the two components associated with \u0393\u22c6, the first of which corresponds to a form of estimation error, whereas the second corresponds to a form of approximation error.\nA family of upper bounds: Since the inequality (27) corresponds to a family of upper bounds indexed by r and the subspaceM, these quantities can be chosen adaptively, depending on the structure of the matrices (\u0398\u22c6,\u0393\u22c6), so as to obtain the tightest possible upper bound. In the simplest case, the RSC conditions hold with tolerance \u03c4n = 0, the matrix \u0398\n\u22c6 is exactly low rank (say rank r), and \u0393\u22c6 lies within a R-decomposable subspace M. In this case, the approximation errors vanish, and Theorem 1 guarantees that the squared Frobenius error is at most\ne2(\u0398\u0302; \u0393\u0302) - \u03bb2dr + \u00b5 2 d\u03a8 2(M;R), (28)\nwhere the - notation indicates that we ignore constant factors."}, {"heading": "3.4 Results for \u21131-norm regularization", "text": "Theorem 1 holds for any regularizer that is decomposable with respect to some subspace pair. As previously noted, an important example of a decomposable regularizer is the elementwise \u21131-norm, which is decomposable with respect to subspaces of the form (16).\nCorollary 1. Consider an observation operator X that satisfies the RSC condition (22) with \u03b3 > 0 and \u03c4n = 0. Suppose that we solve the convex program (10) with regularization parameters (\u03bbd, \u00b5d) such that\n\u03bbd \u2265 4|||X\u2217(W )|||op, and \u00b5d \u2265 4 \u2016X\u2217(W )\u2016\u221e + 4\u03b3 \u03b1\u221a d1d2 . (29)\nThen there are universal constants cj such that for any matrix pair (\u0398 \u22c6,\u0393\u22c6) with \u2016\u0398\u22c6\u2016\u221e \u2264 \u03b1\u221ad1d2 and for all integers r = 1, 2, . . . ,min{d1, d2}, and s = 1, 2, . . . , (d1d2), we have\ne2(\u0398\u0302, \u0393\u0302) \u2264 c1 \u03bb2d \u03b32\n{ r + 1\n\u03bbd\nd\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) } + c2\n\u00b52d \u03b32\n{ s+ 1\n\u00b5d\n\u2211\n(j,k)/\u2208S |\u0393\u22c6jk|\n} , (30)\nwhere S is an arbitrary subset of matrix indices of cardinality at most s.\nRemarks: This result follows directly by specializing Theorem 1 to the elementwise \u21131-norm. As noted in Example 4, for this norm, we have \u03bad = \u221a d1d2, so that the choice (29) satisfies the conditions of Theorem 1. The dual norm is given by the elementwise \u2113\u221e-norm R\u2217(\u00b7) = \u2016 \u00b7 \u2016\u221e. As observed in Section 3.1, the \u21131-norm is decomposable with respect to subspace pairs of the form (M(S),M\u22a5(S)), for an arbitrary subset S of matrix indices. Moreover, for any subset S of cardinality s, we have \u03a82(M(S)) = s. It is easy to verify that with this choice, we have\n\u03a0 M\u22a5\n(\u0393\u22c6) = \u2211\n(j,k)/\u2208S |\u0393\u22c6jk|, from which the claim follows.\nIt is worth noting the inequality (27) corresponds to a family of upper bounds indexed by r and the subset S. For any fixed integer s \u2208 {1, 2, . . . , (d1d2)}, it is natural to let S index the largest s values (in absolute value) of \u0393\u22c6. Moreover, the choice of the pair (r, s) can be further adapted to the structure of the matrix. For instance, when \u0398\u22c6 is exactly low rank, and \u0393\u22c6 is exactly sparse, then one natural choice is r = rank(\u0398\u22c6), and s = | supp(\u0393\u22c6)|. With this choice, both the approximation terms vanish, and Corollary 1 guarantees that any solution (\u0398\u0302, \u0393\u0302) of the convex program (10) satisfies\n|||\u0398\u0302\u2212\u0398\u22c6|||2F + |||\u0393\u0302\u2212 \u0393\u22c6|||2F . \u03bb2d r + \u00b52d s. (31)\nFurther specializing to the case of noiseless observations (W = 0), yields a form of approximate recovery\u2014namely\n|||\u0398\u0302\u2212\u0398\u22c6|||2F + |||\u0393\u0302\u2212 \u0393\u22c6|||2F . \u03b12 s\nd1d2 . (32)\nThis guarantee is weaker than the exact recovery results obtained in past work on the noiseless observation model with identity operator [9, 7]; however, these papers imposed incoherence requirements on the singular vectors of the low-rank component \u0398\u22c6 that are more restrictive than the conditions of Theorem 1.\nOur elementwise \u2113\u221e bound is a weaker condition than incoherence, since it allows for singular vectors to be coherent as long as the associated singular value is not too large. Moreover, the bound (32) is unimprovable up to constant factors, due to the non-identifiability of the observation model (1), as shown by the following example for the identity observation operator X = I.\nExample 6. [Unimprovability for elementwise sparse model] Consider a given sparsity index s \u2208 {1, 2, . . . , (d1d2)}, where we may assume without loss of generality that s \u2264 d2. We then form the matrix\n\u0398\u22c6 : = \u03b1\u221a d1d2  \n1 0 ... 0\n  [ 1 1 1 . . . 0 . . . 0 ] \ufe38 \ufe37\ufe37 \ufe38\nfT\n, (33)\nwhere the vector f \u2208 Rd2 has exactly s ones. Note that \u2016\u0398\u22c6\u2016\u221e = \u03b1\u221ad1d2 by construction, and moreover \u0398\u22c6 is rank one, and has s non-zero entries. Since up to s entries of the noise matrix \u0393\u22c6 can be chosen arbitrarily, \u201cnature\u201d can always set \u0393\u22c6 = \u2212\u0398\u22c6, meaning that we would observe Y = \u0398\u22c6 +\u0393\u22c6 = 0. Consequently, based on observing only Y , the pair (\u0398\u22c6,\u0393\u22c6) is indistinguishable from the all-zero matrices (0d1\u00d7d2 , 0d1\u00d7d2). This fact can be used to show that no method can have squared Frobenius error lower than \u2248 \u03b12 sd1d2 ; see Section 3.7 for a precise statement. Therefore, the bound (32) cannot be improved unless one is willing to impose further restrictions on the pair (\u0398\u22c6,\u0393\u22c6). We note that the singular vector incoherence conditions, as imposed in past work [9, 7, 14] and used to guarantee exact recovery, would exclude the matrix (33), since its left singular vector is the unit vector e1 \u2208 Rd1 . \u2663"}, {"heading": "3.4.1 Results for stochastic noise matrices", "text": "Our discussion thus far has applied to general observation operators X, and general noise matrices W . More concrete results can be obtained by assuming particular forms of X, and that the noise matrix W is stochastic. Our first stochastic result applies to the identity operator X = I and a noise matrix W generated with i.i.d. N(0, \u03bd2/(d1d2)) entries. 4\nCorollary 2. Suppose X = I, the matrix \u0398\u22c6 has rank at most r and satisfies \u2016\u0398\u22c6\u2016\u221e \u2264 \u03b1\u221ad1d2 , and \u0393\u22c6 has at most s non-zero entries. If the noise matrix W has i.i.d. N(0, \u03bd2/(d1d2)) entries, and we solve the convex program (10) with regularization parameters\n\u03bbd = 8\u03bd\u221a d1 + 8\u03bd\u221a d2 , and \u00b5d = 16\u03bd\n\u221a log(d1d2)\nd1d2 + 4\u03b1\u221a d1d2 , (34)\nthen with probability greater than 1\u2212 exp ( \u2212 2 log(d1d2) ) , any optimal solution (\u0398\u0302, \u0393\u0302) satisfies\ne2(\u0398\u0302, \u0393\u0302) \u2264 c1\u03bd2 ( r (d1 + d2)\nd1d2\n)\n\ufe38 \ufe37\ufe37 \ufe38 K\u0398\u22c6\n+ c1\u03bd 2\n( s log(d1d2)\nd1d2\n) + c1 \u03b12 s\nd1d2\ufe38 \ufe37\ufe37 \ufe38 K\u0393\u22c6\n(35)\nRemarks: In the statement of this corollary, the settings of \u03bbd and \u00b5d are based on upper bounding \u2016W\u2016\u221e and |||W |||op, using large deviation bounds and some non-asymptotic random matrix theory. With a slightly modified argument, the bound (35) can be sharpened slightly by reducing the logarithmic term to log(d1d2s ). As shown in Theorem 2 to follow in Section 3.7, this sharpened bound is minimax-optimal, meaning that no estimator (regardless of its computational complexity) can achieve much better estimates for the matrix classes and noise model given here.\nIt is also worth observing that both terms in the bound (35) have intuitive interpretations. Considering first the term K\u0398\u22c6 , we note that the numerator term r(d1 + d2) is of the order of the number of free parameters in a rank r matrix of dimensions d1 \u00d7 d2. The multiplicative factor \u03bd 2\nd1d2 corresponds to the noise variance in the problem. On the other hand, the term K\u0393\u22c6\nmeasures the complexity of estimating s non-zero entries in a d1\u00d7 d2 matrix. Note that there are (d1d2 s ) possible subsets of size s, and consequently, the numerator includes a term that\nscales as log (d1d2\ns\n) \u2248 s log(d1d2). As before, the multiplicative pre-factor \u03bd 2\nd1d2 corresponds to\nthe noise variance. Finally, the second term within K\u0393\u22c6\u2014namely the quantity \u03b1 2 s\nd1d2 \u2014arises\nfrom the non-identifiability of the model, and as discussed in Example 6, it cannot be avoided without imposing further restrictions on the pair (\u0393\u22c6,\u0398\u22c6).\nWe now turn to analysis of the sparse factor analysis problem: as previously introduced in Example 1, this involves estimation of a covariance matrix that has a low-rank plus elementwise sparse decomposition. In this case, given n i.i.d. samples from the unknown covariance matrix \u03a3 = \u0398\u22c6 + \u0393\u22c6, the noise matrix W \u2208 Rd\u00d7d is a recentered Wishart noise (see equation (3)). We can use tail bounds for its entries and its operator norm in order to specify appropriate choices of the regularization parameters \u03bbd and \u00b5d. We summarize our conclusions in the following corollary:\n4To be clear, we state our results in terms of the noise scaling \u03bd2/(d1d2) since it corresponds to a model with constant signal-to-noise ratio when the Frobenius norms of \u0398\u22c6 and \u0393\u22c6 remain bounded, independently of the dimension. The same results would hold if the noise were not rescaled, modulo the appropriate rescalings of the various terms.\nCorollary 3. Consider the factor analysis model with n \u2265 d samples, and regularization parameters\n\u03bbd = 16||| \u221a \u03a3|||2\n\u221a d\nn , and \u00b5d = 32\u03c1(\u03a3)\n\u221a log d\nn +\n4\u03b1\nd , where \u03c1(\u03a3) = maxj \u03a3jj. (36)\nThen with probability greater than 1\u2212 c2 exp ( \u2212 c3 log(d) ) , any optimal solution (\u0398\u0302, \u0393\u0302) satisfies\ne2(\u0398\u0302, \u0393\u0302) \u2264 c1 { |||\u03a3|||2 rd\nn + \u03c1(\u03a3)\ns log d\nn\n} + c1 \u03b12s\nd2 .\nWe note that the condition n \u2265 d is necessary to obtain consistent estimates in factor analysis models, even in the case with \u0393\u22c6 = Id\u00d7d where PCA is possible (e.g., see Johnstone [15]). Again, the terms in the bound have a natural interpretation: since a matrix of rank r in d dimensions has roughly rd degrees of freedom, we expect to see a term of the order rdn .\nSimilarly, since there are log ( d2\ns\n) \u2248 s log d subsets of size s in a d\u00d7d matrix, we also expect to\nsee a term of the order s log dn . Moreover, although we have stated our choices of regularization parameter in terms of |||\u03a3|||2 and \u03c1(\u03a3), these can be replaced by the analogous versions using the sample covariance matrix \u03a3\u0302. (By the concentration results that we establish, the population and empirical versions do not differ significantly when n \u2265 d.)"}, {"heading": "3.4.2 Comparison to Hsu et al. [14]", "text": "This recent work focuses on the problem of matrix decomposition with the \u2016 \u00b7 \u20161-norm, and provides results both for the noiseless and noisy setting. All of their work focuses on the case of exactly low rank and exactly sparse matrices, and deals only with the identity observation operator; in contrast, Theorem 1 in this paper provides an upper bound for general matrix pairs and observation operators. Most relevant is comparison of our \u21131-results with exact rank-sparsity constraints to their Theorem 3, which provides various error bounds (in nuclear and Frobenius norm) for such models with additive noise. These bounds are obtained using an estimator similar to our program (10), and in parts of their analysis, they enforce bounds on the \u2113\u221e-norm of the solution. However, this is not done directly with a constraint on \u0398 as in our estimator, but rather by penalizing the difference \u2016Y \u2212 \u0393\u2016\u221e, or by thresholding the solution.\nApart from these minor differences, there are two major differences between our results, and those of Hsu et al. First of all, their analysis involves three quantities (\u03b1, \u03b2, \u03b3) that measure singular vector incoherence, and must satisfy a number of inequalities. In contrast, our analysis is based only on a single condition: the \u201cspikiness\u201d condition on the low-rank component \u0398\u22c6. As we have seen, this constraint is weaker than singular vector incoherence, and consequently, unlike the result of Hsu et al., we do not provide exact recovery guarantees for the noiseless setting. However, it is interesting to see (as shown by our analysis) that a very simple spikiness condition suffices for the approximate recovery guarantees that are of interest for noisy observation models. Given these differing assumptions, the underlying proof techniques are quite distinct, with our methods leveraging the notion of restricted strong convexity introduced by Negahban et al. [19].\nThe second (and perhaps most significant) difference is in the sharpness of the results for the noisy setting, and the permissible scalings of the rank-sparsity pair (r, s). As will be clarified in Section 3.7, the rates that we establish for low-rank plus elementwise sparsity\nfor the noisy Gaussian model (Corollary 2) are minimax-optimal up to constant factors. In contrast, the upper bounds in Theorem 3 of Hsu et al. involve the product rs, and hence are sub-optimal as the rank and sparsity scale. These terms appear only additively both our upper and minimax lower bounds, showing that an upper bound involving the product rs is sub-optimal. Moreover, the bounds of Hsu et al. (see Section IV.D) are limited to matrix decompositions for which the rank-sparsity pair (r, s) are bounded as\nrs - d1d1\nlog(d1) log(d2) (37)\nThis bound precludes many scalings that are of interest. For instance, if the sparse component \u0393\u22c6 has a nearly constant fraction of non-zeros (say s \u224d d1d2log(d1) log(d2) for concreteness), then the bound (37) restricts to \u0398\u22c6 to have constant rank. In contrast, our analysis allows for high-dimensional scaling of both the rank r and sparsity s simultaneously; as can be seen by inspection of Corollary 2, our Frobenius norm error goes to zero under the scalings s \u224d d1d2log(d1) log(d2) and r \u224d d2 log(d2) ."}, {"heading": "3.4.3 Results for multi-task regression", "text": "Let us now extend our results to the setting of multi-task regression, as introduced in Example 2. The observation model is of the form Y = XB\u2217 + W , where X \u2208 Rn\u00d7d1 is a known design matrix, and we observe the matrix Y = Rn\u00d7d2 . Our goal is to estimate the the regression matrix B\u2217 \u2208 Rd1\u00d7d2 , which is assumed to have a decomposition of the form B\u2217 = \u0398\u22c6+\u0393\u22c6, where \u0398\u22c6 models the shared characteristics between each of the tasks, and the matrix \u0393\u22c6 models perturbations away from the shared structure. If we take \u0393\u22c6 to be a sparse matrix, an appropriate choice of regularizer R is the elementwise \u21131-norm, as in Corollary 2. We use \u03c3min and \u03c3max to denote the minimum and maximum singular values (respectively) of the rescaled design matrix X/ \u221a n; we assume that X is invertible so that \u03c3min > 0, and moreover, that its\ncolumns are uniformly bounded in \u21132-norm, meaning that maxj=1,...,d1 \u2016Xj\u20162 \u2264 \u03bamax \u221a n. We note that these assumptions are satisfied for many common examples of random design.\nCorollary 4. Suppose that the matrix \u0398\u22c6 has rank at most r and satisfies \u2016\u0398\u22c6\u2016\u221e \u2264 \u03b1\u221ad1d2 , and the matrix \u0393\u22c6 has at most s non-zero entries. If the entries of W are i.i.d. N(0, \u03bd2), and we solve the convex program (10) with regularization parameters\n\u03bbd = 8\u03bd \u03c3max \u221a n( \u221a d1 + \u221a d2), and \u00b5d = 16\u03bd \u03bamax \u221a n log(d1d2) + 4\u03b1\u03c3min \u221a n\u221a\nd1d2 , (38)\nthen with probability greater than 1\u2212 exp ( \u2212 2 log(d1d2) ) , any optimal solution (\u0398\u0302, \u0393\u0302) satisfies\ne2(\u0398\u0302, \u0393\u0302) \u2264 c1 \u03bd2\u03c32max \u03c34min\n( r (d1 + d2)\nn\n)\n\ufe38 \ufe37\ufe37 \ufe38 K\u0398\u22c6\n+ c2 [\u03bd2 \u03ba2max\n\u03c34min\n( s log(d1d2)\nn\n) + \u03b12 s\nd1d2\n]\n\ufe38 \ufe37\ufe37 \ufe38 K\u0393\u22c6\n. (39)\nRemarks: We see that the results presented above are analogous to those presented in Corollary 2. However, in this setting, we leverage large deviations results in order to find bounds on \u2016X\u2217(W )\u2016\u221e and |||X\u2217(W )|||op that hold with high probability given our observation model."}, {"heading": "3.5 An alternative two-step method", "text": "As suggested by one reviewer, it is possible that a simpler two-step method\u2014namely, based on first thresholding the entries of the observation matrix Y , and then performing a low-rank approximation\u2014might achieve similar rates to the more complex convex relaxation (10). In this section, we provide a detailed analysis of one version of such a procedure in the case of nuclear norm combined with \u21131-regularization. We prove that in the special case of X = I, this procedure can attain the same form of error bounds, with possibly different constants. However, there is also a cautionary message here: we also give an example to show that the two-step method will not necessarily perform well for general observation operators X.\nIn detail, let us consider the following two-step estimator:\n(a) Estimate the sparse component \u0393\u22c6 by solving\n\u0393\u0302 \u2208 argmin \u0393\u2208Rd1\u00d7d2 {1 2 |||Y \u2212 \u0393|||2F + \u00b5d \u2016\u0393\u20161 } . (40)\nAs is well-known, this convex program has an explicit solution based on soft-thresholding the entries of Y .\n(b) Given the estimate \u0393\u0302, estimate the low-rank component \u0398\u22c6 by solving the convex program\n\u0398\u0302 \u2208 argmin \u0398\u2208Rd1\u00d7d2 {1 2 |||Y \u2212\u0398\u2212 \u0393\u0302|||2F + \u03bbd |||\u0398|||N } . (41)\nInterestingly, note that this method can be understood as the first two steps of a blockwise coordinate descent method for solving the convex program (10). In step (a), we fix the low-rank component, and minimize as a function of the sparse component. In step (b), we fix the sparse component, and then minimize as a function of the low-rank component. The following result that these two steps of co-ordinate descent achieve the same rates (up to constant factors) as solving the full convex program (10):\nProposition 1. Given observations Y from the model Y = \u0398\u22c6 + \u0393\u22c6+W with \u2016\u0398\u22c6\u2016\u221e \u2264 \u03b1\u221ad1d2 , consider the two-step procedure (40) and (41) with regularization parameters (\u03bbd, \u00b5d) such that\n\u03bbd \u2265 4|||W |||op, and \u00b5d \u2265 4 \u2016W\u2016\u221e + 4 \u03b1\u221a d1d2 . (42)\nThen the error bound (30) from Corollary 1 holds with \u03b3 = 1.\nConsequently, in the special case that X = I, then there is no need to solve the convex program (10) to optimality; rather, two steps of co-ordinate descent are sufficient.\nOn the other hand, the simple two-stage method will not work for general observation operators X. As shown in the proof of Proposition 1, the two-step method relies critically on having the quantity \u2016X(\u0398\u22c6 + W )\u2016\u221e be upper bounded (up to constant factors) by max{\u2016\u0398\u22c6\u2016\u221e, \u2016W\u2016\u221e}. By triangle inequality, this condition holds trivially when X = I, but can be violated by other choices of the observation operator, as illustrated by the following example.\nExample 7 (Failure of two-step method). Recall the multi-task observation model first introduced in Example 2. In Corollary 4, we showed that the general estimator (10) will recover good estimates under certain assumptions on the observation matrix. In this example, we provide an instance for which the assumptions of Corollary 4 are satisfied, but on the other hand, the two-step method will not return a good estimate.\nMore specifically, let us consider the observation model Y = X(\u0398\u22c6 + \u0393\u22c6) +W , in which Y \u2208 Rd\u00d7d, and the observation matrix X \u2208 Rd\u00d7d takes the form\nX : = Id\u00d7d + 1\u221a d e1~1 T ,\nwhere e1 \u2208 Rd is the standard basis vector with a 1 in the first component, and ~1 denotes the vector of all ones. Suppose that the unknown low-rank matrix is given by \u0398\u22c6 = 1d\n~1~1T . Note that this matrix has rank one, and satisfies \u2016\u0398\u22c6\u2016\u221e = 1d .\nWe now verify that the conditions of Corollary 4 are satisfied. Letting \u03c3min and \u03c3max denote (respectively) the smallest and largest singular values of X, we have \u03c3min = 1 and \u03c3max \u2264 2. Moreover, letting Xj denote the jth column of X, we have maxj=1,...,d \u2016Xj\u20162 \u2264 2. Consequently, if we consider rescaled observations with noise variance \u03bd2/d, the conditions of Corollary 4 are all satisfied with constants (independent of dimension), so that the M - estimator (10) will have good performance.\nOn the other hand, letting E denote expectation over any zero-mean noise matrix W , we have\nE [ \u2016X(\u0398\u22c6 +W )\u2016\u221e ] (i) \u2265 \u2016X(\u0398\u22c6 + E[W ])\u2016\u221e = \u2016X(\u0398\u22c6)\u2016\u221e (ii) \u2265 \u221a d\u2016\u0398\u22c6\u2016\u221e,\nwhere step (i) exploits Jensen\u2019s inequality, and step (ii) uses the fact that\n\u2016X(\u0398\u22c6)\u2016\u221e = 1/d+ 1/ \u221a d = ( 1 + \u221a d ) \u2016\u0398\u22c6\u2016\u221e.\nFor any noise matrix W with reasonable tail behavior, the variable \u2016X(\u0398\u22c6 + W )\u2016\u221e will concentrate around its expectation, showing that \u2016X(\u0398\u22c6 +W )\u2016\u221e will be larger than \u2016\u0398\u22c6\u2016\u221e by an order of magnitude (factor of \u221a d). Consequently, the two-step method will have much larger estimation error in this case. \u2663"}, {"heading": "3.6 Results for \u2016 \u00b7 \u20162,1 regularization", "text": "Let us return again to the general Theorem 1, and illustrate some more of its consequences in application to the columnwise (2, 1)-norm previously defined in Example 5, and methods based on solving the convex program (14). As before, specializing Theorem 1 to this decomposable regularizer yields a number of guarantees. In order to keep our presentation relatively brief, we focus here on the case of the identity observation operator X = I.\nCorollary 5. Suppose that we solve the convex program (14) with regularization parameters (\u03bbd, \u00b5d) such that\n\u03bbd \u2265 4|||W |||op, and \u00b5d \u2265 4 \u2016W\u20162,\u221e + 4\u03b1\u221a d2 . (43)\nThen there is a universal constant c1 such that for any matrix pair (\u0398 \u22c6,\u0393\u22c6) with \u2016\u0398\u22c6\u20162,\u221e \u2264 \u03b1\u221ad2 and for all integers r = 1, 2, . . . , d and s = 1, 2, . . . , d2, we have\n|||\u0398\u0302 \u2212\u0398\u22c6|||2F + |||\u0393\u0302\u2212 \u0393\u22c6|||2F \u2264 c1\u03bb2d { r + 1\n\u03bbd\nd\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) } + c1 \u00b5 2 d { s+ 1\n\u00b5d\n\u2211 k/\u2208C \u2016\u0393\u22c6k\u20162\n} , (44)\nwhere C \u2286 {1, 2, . . . , d2} is an arbitrary subset of column indices of cardinality at most s.\nRemarks: This result follows directly by specializing Theorem 1 to the columnwise (2, 1)- norm and identity observation model, previously discussed in Example 5. Its dual norm is the columnwise (2,\u221e)-norm, and we have \u03bad = \u221a d2. As discussed in Section 3.1, the (2, 1)-norm is decomposable with respect to subspaces of the type M(C), as defined in equation (17), where C \u2286 {1, 2, . . . , d2} is an arbitrary subset of column indices. For any such subset C of cardinality s, it can be calculated that \u03a82(M(C)) = s, and moreover, that \u2016\u03a0M\u22a5(\u0393\u22c6)\u20162,1 =\u2211\nk/\u2208C \u2016\u0393\u22c6k\u20162. Consequently, the bound (44) follows from Theorem 1. As before, if we assume that \u0398\u22c6 has exactly rank r and \u0393\u22c6 has at most s non-zero columns, then both approximation error terms in the bound (44) vanish, and we recover an upper bound of the form |||\u0398\u0302\u2212\u0398\u22c6|||2F + |||\u0393\u0302\u2212 \u0393\u22c6|||2F . \u03bb2dr+ \u00b52ds. If we further specialize to the case of exact observations (W = 0), then Corollary 5 guarantees that\n|||\u0398\u0302\u2212\u0398\u22c6|||2F + |||\u0393\u0302\u2212 \u0393\u22c6|||2F . \u03b12 s\nd2 .\nThe following example shows, that given our conditions, even in the noiseless setting, no method can recover the matrices to precision more accurate than \u03b12s/d2.\nExample 8 (Unimprovability for columnwise sparse model). In order to demonstrate that the term \u03b12s/d2 is unavoidable, it suffices to consider a slight modification of Example 6. In particular, let us define the matrix\n\u0398\u22c6 : = \u03b1\u221a d1d2  \n1 1 ... 1\n  [ 1 1 1 . . . 0 . . . 0 ] \ufe38 \ufe37\ufe37 \ufe38\nfT\n, (45)\nwhere again the vector f \u2208 Rd2 has s non-zeros. Note that the matrix \u0398\u22c6 is rank one, has s non-zero columns, and moreover \u2016\u0398\u22c6\u20162,\u221e = \u03b1\u221ad2 . Consequently, the matrix \u0398 \u22c6 is covered by Corollary 5. Since s columns of the matrix \u0393\u22c6 can be chosen in an arbitrary manner, it is possible that \u0393\u22c6 = \u2212\u0398\u22c6, in which case the observation matrix Y = 0. This fact can be exploited to show that no method can achieve squared Frobenius error much smaller than \u2248 \u03b12sd2 ; see Section 3.7 for the precise statement. Finally, we note that it is difficult to compare directly to the results of Xu et al. [29], since their results do not guarantee exact recovery of the pair (\u0398\u22c6,\u0393\u22c6). \u2663\nAs with the case of elementwise \u21131-norm, more concrete results can be obtained when the noise matrix W is stochastic.\nCorollary 6. Suppose \u0398\u22c6 has rank at most r and satisfies \u2016\u0398\u22c6\u20162,\u221e \u2264 \u03b1\u221ad2 , and \u0393 \u22c6 has at most s non-zero columns. If the noise matrix W has i.i.d. N(0, \u03bd2/(d1d2)) entries, and we solve the convex program (14) with regularization parameters \u03bbd =\n8\u03bd\u221a d1 + 8\u03bd\u221a d2 and\n\u00b5d = 8\u03bd\n\u221a 1\nd2 + \u221a log d2 d1d2 + 4\u03b1\u221a d2 ,\nthen with probability greater than 1\u2212 exp ( \u2212 2 log(d2) ) , any optimal solution (\u0398\u0302, \u0393\u0302) satisfies\ne2(\u0398\u0302, \u0393\u0302) \u2264 c1 \u03bd2 r (d1 + d2)\nd1d2\ufe38 \ufe37\ufe37 \ufe38 K\u0398\u22c6\n+ \u03bd2 {\nsd1 d1d2 + s log d2 d1d2\n} + c2 \u03b12s\nd2\ufe38 \ufe37\ufe37 \ufe38 K\u0393\u22c6\n. (46)\nRemarks: Note that the setting of \u03bbd is the same as in Corollary 2, whereas the parameter \u00b5d is chosen based on upper bounding \u2016W\u20162,\u221e, corresponding to the dual norm of the columnwise (2, 1)-norm. With a slightly modified argument, the bound (46) can be sharpened slightly by reducing the logarithmic term to log(d2s ). As shown in Theorem 2 to follow in Section 3.7, this sharpened bound is minimax-optimal.\nAs with Corollary 2, both terms in the bound (46) are readily interpreted. The term K\u0398\u22c6 has the same interpretation, as a combination of the number of degrees of freedom in a rank r matrix (that is, of the order r(d1+d2)) scaled by the noise variance \u03bd2\nd1d2 . The second term K\u0393\u22c6\nhas a somewhat more subtle interpretation. The problem of estimating s non-zero columns embedded within a d1 \u00d7 d2 matrix can be split into two sub-problems: first, the problem of estimating the sd1 non-zero parameters (in Frobenius norm), and second, the problem of column subset selection\u2014i.e., determining the location of the s non-zero parameters. The estimation sub-problem yields the term \u03bd\n2sd1 d1d2 , whereas the column subset selection sub-problem\nincurs a penalty involving log (d2 s ) \u2248 s log d2, multiplied by the usual noise variance. The final term \u03b12s/d2 arises from the non-identifiability of the model. As discussed in Example 8, it is unavoidable without further restrictions.\nWe now turn to some consequences for the problem of robust covariance estimation formulated in Example 3. As seen from equation (4), the disturbance matrix in this setting can be written as a sum (\u0393\u22c6)T + \u0393\u22c6, where \u0393\u22c6 is a column-wise sparse matrix. Consequently, we can use a variant of the estimator (14), in which the loss function is given by |||Y \u2212 {\u0398\u22c6 + (\u0393\u22c6)T +\u0393\u22c6}|||2F . The following result summarizes the consequences of Theorem 1 in this setting:\nCorollary 7. Consider the problem of robust covariance estimation with n \u2265 d samples, based on a matrix \u0398\u22c6 with rank at most r that satisfies \u2016\u0398\u22c6\u20162,\u221e \u2264 \u03b1\u221ad , and a corrupting matrix \u0393\u22c6 with at most s rows and columns corrupted. If we solve SDP (14) with regularization parameters\n\u03bb2d = 8|||\u0398\u22c6|||2op r\nn , and \u00b52d = 8|||\u0398\u22c6|||2op\nr n +\n16\u03b12\nd , (47)\nthen with probability greater than 1\u2212 c2 exp ( \u2212 c3 log(d) ) , any optimal solution (\u0398\u0302, \u0393\u0302) satisfies\ne2(\u0398\u0302, \u0393\u0302) \u2264 c1|||\u0398\u22c6|||2op {r2 n + sr n } + c2 \u03b12s d .\nSome comments about this result: with the motivation of being concrete, we have given an explicit choice (47) of the regularization parameters, involving the operator norm |||\u0398\u22c6|||op, but any upper bound would suffice. As with the noise variance in Corollary 6, a typical strategy would choose this pre-factor by cross-validation."}, {"heading": "3.7 Lower bounds", "text": "For the case of i.i.d Gaussian noise matrices, Corollaries 2 and 6 provide results of an achievable nature, namely in guaranteeing that our estimators achieve certain Frobenius errors. In this section, we turn to the complementary question: what are the fundamental (algorithmicindependent) limits of accuracy in noisy matrix decomposition? One way in which to address such a question is by analyzing statistical minimax rates.\nMore formally, given some family F of matrices, the associated minimax error is given by\nM(F) := inf (\u0398\u0303,\u0393\u0303) sup (\u0398\u22c6,\u0393\u22c6)\nE [ |||\u0398\u0303\u2212\u0398\u22c6|||2F + |||\u0393\u0303\u2212 \u0393\u22c6|||2F ] , (48)\nwhere the infimum ranges over all estimators (\u0398\u0303, \u0393\u0303) that are (measurable) functions of the data Y , and the supremum ranges over all pairs (\u0398\u22c6,\u0393\u22c6) \u2208 F . Here the expectation is taken over the Gaussian noise matrix W , under the linear observation model (1).\nGiven a matrix \u0393\u22c6, we define its support set supp(\u0393\u22c6) := {(j, k) | \u0393\u22c6jk 6= 0}, as well as its column support colsupp(\u0393\u22c6) := {k | \u0393\u22c6k 6= 0 } , where \u0393\u22c6k denotes the k\nth column. Using this notation, our interest centers on the following two matrix families:\nFsp(r, s, \u03b1) := { (\u0398\u22c6,\u0393\u22c6) | rank(\u0398\u22c6) \u2264 r, | supp(\u0393\u22c6)| \u2264 s, \u2016\u0398\u22c6\u2016\u221e \u2264\n\u03b1\u221a d1d2\n} , and (49a)\nFcol(r, s, \u03b1) := { (\u0398\u22c6,\u0393\u22c6) | rank(\u0398\u22c6) \u2264 r, | colsupp(\u0393\u22c6)| \u2264 s, \u2016\u0398\u22c6\u20162,\u221e \u2264\n\u03b1\u221a d2\n} . (49b)\nBy construction, Corollaries 2 and 6 apply to the families Fsp and Fcol respectively.\nThe following theorem establishes lower bounds on the minimax risks (in squared Frobenius norm) over these two families for the identity observation operator:\nTheorem 2. Consider the linear observation model (1) with identity observation operator: X(\u0398 + \u0393) = \u0398+ \u0393. There is a universal constant c0 > 0 such that for all \u03b1 \u2265 32 \u221a log(d1d2), we have\nM(Fsp(r, s, \u03b1)) \u2265 c0\u03bd2 { r (d1 + d2)\nd1d2 +\ns log(d1d2\u2212ss/2 )\nd1d2\n} + c0 \u03b12 s\nd1d2 , (50)\nand\nM(Fcol(r, s, \u03b1)) \u2265 c0\u03bd2 ( r (d1 + d2)\nd1d2 +\ns\nd2 +\ns log(d2\u2212ss/2 )\nd1d2\n) + c0 \u03b12 s\nd2 . (51)\nNote the agreement with the achievable rates guaranteed in Corollaries 2 and 6 respectively. (As discussed in the remarks following these corollaries, the sharpened forms of the logarithmic factors follow by a more careful analysis.) Theorem 2 shows that in terms of squared Frobenius error, the convex relaxations (10) and (14) are minimax optimal up to constant factors.\nIn addition, it is worth observing that although Theorem 2 is stated in the context of additive Gaussian noise, it also shows that the radius of non-identifiability (involving the parameter \u03b1) is a fundamental limit. In particular, by setting the noise variance to zero, we see that under our milder conditions, even in the noiseless setting, no algorithm can estimate to greater accuracy than c0\n\u03b12 s d1d2 , or the analogous quantity for column-sparse matrices."}, {"heading": "4 Simulation results", "text": "We have implemented the M -estimators based on the convex programs (10) and (14), in particular by adapting first-order optimization methods due to Nesterov [22]. In this section, we report simulation results that demonstrate the excellent agreement between our theoretical predictions and the behavior in practice. In all cases, we used square matrices (d = d1 = d2), and a stochastic noise matrix W with i.i.d. N(0, \u03bd 2\nd2 ) entries, with \u03bd2 = 1. For any given rank\nr, we generated \u0398\u22c6 by randomly choosing the spaces of left and right singular vectors. We formed random sparse (elementwise or columnwise) matrices by choosing the positions of the non-zeros (entries or columns) uniformly at random.\nRecall the estimator (10) from Example 4. It is based on a combination of the nuclear norm with the elementwise \u21131-norm, and is motivated problem of recovering a low-rank matrix \u0398\u22c6 corrupted by an arbitrary sparse matrix \u0393\u22c6. In our first set of experiments, we fixed the matrix dimension d = 100, and then studied a range of ranks r for \u0398\u22c6, as well as a range of sparsity indices s for \u0393\u22c6. More specifically, we studied linear scalings of the form r = \u03b3d for a constant \u03b3 \u2208 (0, 1), and s = \u03b2d2 for a second constant \u03b2 \u2208 (0, 1).\nNote that under this scaling, Corollary 2 predicts that the squared Frobenius error should be upper bounded as c1\u03b3 + c2\u03b2 log(1/\u03b2), for some universal constants c1, c2. Figure 1(a) provides experimental confirmation of the accuracy of these theoretical predictions: varying \u03b3 (with \u03b2 fixed) produces linear growth of the squared error as a function of \u03b3. In Figure 1(b), we study the complementary scaling, with the rank ratio \u03b3 fixed and the sparsity ratio \u03b2 varying in the interval [.01, .1]. Since \u03b2 log(1/\u03b2) \u2248 \u0398(\u03b2) over this interval, we should expect to see roughly linear scaling. Again, the plot shows good agreement with the theoretical predictions.\nNow recall the estimator (14) from Example 5, designed for estimating a low-rank matrix plus a columnwise sparse matrix. We have observed similar linear dependence on the analogs of the parameters \u03b3 and \u03b2, as predicted by our theory. In the interests of exhibiting a different phenomenon, here we report its performance for matrices of varying dimension, in all cases with \u0393\u22c6 having s = 3r non-zero columns. Figure 2(a) shows plots of squared Frobenius error versus the dimension for two choices of the rank (r = 10 and r = 15), and the matrix dimension varying in the range d \u2208 {100 : 25 : 300}. As predicted by our theory, these plots decrease at the rate 1/d. Indeed, this scaling is revealed by replotting the inverse squared error versus d, which produces the roughly linear plots shown in panel (b). Moreover, by comparing the relative slopes of these two curves, we see that the problem with rank r = 15 requires roughly a dimension that is roughly 32 larger than the problem with r = 10 to achieve the same error. Again, this linear scaling in rank is consistent with Corollary 6."}, {"heading": "5 Proofs", "text": "In this section, we provide the proofs of our main results, with the proofs of some more technical lemmas deferred to the appendices."}, {"heading": "5.1 Proof of Theorem 1", "text": "For the reader\u2019s convenience, let us recall here the two assumptions on the regularization parameters:\n\u00b5d \u2265 4R\u2217(X\u2217(W )) + 4 \u03b3 \u03b1 \u03bad > 0, and \u03bbd \u2265 4|||X\u2217(W )|||op. (52)\nFurthermore, so as to simplify notation, let us define the error matrices \u2206\u0302\u0398 : = \u0398\u0302 \u2212 \u0398\u22c6 and \u2206\u0302\u0393 : = \u0393\u0302\u2212 \u0393\u22c6. Let (M,M\u22a5) denote an arbitrary subspace pair for which the regularizer R is decomposable. Throughout these proofs, we adopt the convenient shorthand notation \u2206\u0302\u0393\nM : = \u03a0M(\u2206\u0302 \u0393) and \u2206\u0302\u0393 M\u22a5 = \u03a0M\u22a5(\u2206\u0302 \u0393), with similar definitions for \u0393\u22c6M and \u0393 \u22c6 M\u22a5 .\nWe now turn to a lemma that deals with the behavior of the error matrices (\u2206\u0302\u0398, \u2206\u0302\u0393) when measured together using a weighted sum of the nuclear norm and regularizer R. In order to state the following lemma, let us recall that for any positive (\u00b5d, \u03bbd), the weighted norm is defined as Q(\u0398,\u0393) := |||\u0398|||N + \u00b5d\u03bbdR(\u0393).\nWith this notation, we have the following:\nLemma 1. For any r = 1, 2, . . . , d, there is a decomposition \u2206\u0302\u0398 = \u2206\u0302\u0398A + \u2206\u0302 \u0398 B such that:\n(a) The decomposition satisfies\nrank(\u2206\u0302\u0398A) \u2264 2r, and (\u2206\u0302\u0398A)T \u2206\u0302\u0398B = (\u2206\u0302\u0398B)T \u2206\u0302\u0398A = 0.\n(b) The difference Q(\u0398\u22c6,\u0393\u22c6)\u2212Q(\u0398\u22c6 + \u2206\u0302\u0398,\u0393\u22c6 + \u2206\u0302\u0393) is upper bounded by\nQ(\u2206\u0302\u0398A, \u2206\u0302\u0393M)\u2212Q(\u2206\u0302\u0398B , \u2206\u0302\u0393M\u22a5) + 2 d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + 2\u00b5d \u03bbd R(\u0393\u22c6M\u22a5). (53)\n(c) Under conditions (52) on \u00b5d and \u03bbd, the error matrices \u2206\u0302 \u0398 and \u2206\u0302\u0393 satisfy\nQ ( \u2206\u0302\u0398B, \u2206\u0302 \u0393 M\u22a5 ) \u2264 3Q ( \u2206\u0302\u0398A, \u2206\u0302 \u0393 M ) + 4 { d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + \u00b5d \u03bbd R(\u0393\u22c6 M\u22a5\n) } . (54)\nfor any R-decomposable pair (M,M\u22a5).\nSee Appendix A for the proof of this result.\nOur second lemma guarantees that the cost function L(\u0398,\u0393) = 12 |||Y \u2212 X(\u0398 + \u0393)|||2F is strongly convex in a restricted set of directions. In particular, if we let \u03b4L(\u2206\u0302\u0398, \u2206\u0302\u0393) denote the error in the first-order Taylor series expansion around (\u0398\u22c6,\u0393\u22c6), then some algebra shows that\n\u03b4L(\u2206\u0302\u0398, \u2206\u0302\u0393) = 1 2 |||X(\u2206\u0302\u0398 + \u2206\u0302\u0393)|||2F. (55)\nThe following lemma shows that (up to a slack term) this Taylor error is lower bounded by the squared Frobenius norm.\nLemma 2 (Restricted strong convexity). Under the conditions of Theorem 1, the first-order Taylor series error (55) is lower bounded by\n\u03b3\n4\n( |||\u2206\u0302\u0398|||2F + |||\u2206\u0302\u0393|||2F ) \u2212 \u03bbd\n2 Q(\u2206\u0302\u0398, \u2206\u0302\u0393)\u2212 16 \u03c4n\n{ d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + \u00b5d \u03bbd\nR(\u0393\u22c6M\u22a5) }2 . (56)\nWe prove this result in Appendix B.\nUsing Lemmas 1 and 2, we can now complete the proof of Theorem 1. By the optimality of (\u0398\u0302, \u0393\u0302) and the feasibility of (\u0398\u22c6,\u0393\u22c6), we have\n1 2 |||Y \u2212 X(\u0398\u0302 + \u0393\u0302)|||2F + \u03bbd|||\u0398\u0302|||N + \u00b5dR(\u0393\u0302) \u2264 1 2 |||Y \u2212X(\u0398\u22c6 + \u0393\u22c6)|||2F + \u03bbd|||\u0398\u22c6|||N + \u00b5dR(\u0393\u22c6).\nRecalling that Y = X(\u0398\u22c6 + \u0393\u22c6) +W , and re-arranging in terms of the errors \u2206\u0302\u0398 = \u0398\u0302 \u2212 \u0398\u22c6 and \u2206\u0302\u0393 = \u0393\u0302\u2212 \u0393\u22c6, we obtain\n1 2 |||X(\u2206\u0302\u0398 + \u2206\u0302\u0393)|||2F \u2264 \u3008\u3008\u2206\u0302\u0398 + \u2206\u0302\u0393, X\u2217(W )\u3009\u3009+ \u03bbdQ(\u0398\u22c6,\u0393\u22c6)\u2212 \u03bbdQ(\u0398\u22c6 + \u2206\u0302\u0398,\u0393\u22c6 + \u2206\u0302\u0393),\nwhere the weighted norm Q was previously defined (20). We now substitute inequality (53) from Lemma 1 into the right-hand-side of the above equation to obtain\n1 2 |||X(\u2206\u0302\u0398 + \u2206\u0302\u0393)|||2F \u2264 \u3008\u3008\u2206\u0302\u0398 + \u2206\u0302\u0393, X\u2217(W )\u3009\u3009+ \u03bbdQ(\u2206\u0302\u0398A, \u2206\u0302\u0393M)\u2212 \u03bbdQ(\u2206\u0302\u0398B , \u2206\u0302\u0393M\u22a5)\n+ 2\u03bbd\nd\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + 2\u00b5dR(\u0393\u22c6M\u22a5)\nSome algebra and an application of Ho\u0308lder\u2019s inequality and the triangle inequality allows us to obtain the upper bound\n( |||\u2206\u0302\u0398A|||N + |||\u2206\u0302\u0398B|||N ) |||X\u2217(W )|||op + ( R(\u2206\u0302\u0393 M ) +R(\u2206\u0302\u0393 M\u22a5 ) ) R\u2217(X\u2217(W ))\n\u2212 \u03bbdQ(\u2206\u0302\u0398B , \u2206\u0302\u0393M\u22a5) + \u03bbdQ(\u2206\u0302\u0398A, \u2206\u0302\u0393M) + 2\u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + 2\u00b5dR(\u0393\u22c6M\u22a5).\nRecalling conditions (52) for \u00b5d and \u03bbd, we obtain the inequality\n1 2 |||X(\u2206\u0302\u0398 + \u2206\u0302\u0393)|||2F \u2264 3\u03bbd 2\nQ(\u2206\u0302\u0398A, \u2206\u0302\u0393M) + 2\u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + 2\u00b5dR(\u0393\u22c6M\u22a5).\nUsing inequality (56) from Lemma 2 to lower bound the right-hand side, and then rearranging terms yields\n\u03b3\n4\n( |||\u2206\u0302\u0398|||2F + |||\u2206\u0302\u0393|||2F ) \u2264 3\u03bbd\n2 Q(\u2206\u0302\u0398A, \u2206\u0302\u0393M) + \u03bbd 2 Q(\u2206\u0302\u0398, \u2206\u0302\u0393)\n+ 16 \u03c4n { d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + \u00b5d \u03bbd R(\u0393\u22c6 M\u22a5\n) }2\n+ 2\u03bbd\nd\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + 2\u00b5dR(\u0393\u22c6M\u22a5). (57)\nNow note that by the triangle inequality Q(\u2206\u0302\u0398, \u2206\u0302\u0393) \u2264 Q(\u2206\u0302\u0398A, \u2206\u0302\u0393M) + Q(\u2206\u0302\u0398B , \u2206\u0302\u0393M\u22a5), so that combined with the bound (53) from Lemma 1, we obtain\nQ(\u2206\u0302\u0398, \u2206\u0302\u0393) \u2264 4Q(\u2206\u0302\u0398A, \u2206\u0302\u0393M) + 4{ d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + \u00b5d \u03bbd R(\u0393\u22c6M\u22a5)}.\nSubstituting this upper bound into equation (57) yields\n\u03b3\n4\n( |||\u2206\u0302\u0398|||2F + |||\u2206\u0302\u0393|||2F ) \u2264 4Q(\u2206\u0302\u0398A, \u2206\u0302\u0393M)\n+ 16 \u03c4n { d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + \u00b5d \u03bbd\nR(\u0393\u22c6M\u22a5) }2 + 4 { \u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + \u00b5dR(\u0393\u22c6M\u22a5)\n} . (58)\nNoting that \u2206\u0302\u0398A has rank at most 2r and that \u2206\u0302 \u0393 M lies in the model space M, we find that\n\u03bbdQ(\u2206\u0302\u0398A, \u2206\u0302\u0393M) \u2264 \u221a 2r \u03bbd|||\u2206\u0302\u0398A|||F +\u03a8(M)\u00b5d|||\u2206\u0302\u0393M|||F\n\u2264 \u221a 2r \u03bbd|||\u2206\u0302\u0398|||F +\u03a8(M)\u00b5d|||\u2206\u0302\u0393|||F.\nSubstituting the above inequality into equation (58) and rearranging the terms involving e2(\u2206\u0302\u0398, \u2206\u0302\u0393) yields the claim."}, {"heading": "5.2 Proof of Corollaries 2 and 4", "text": "Note that Corollary 2 can be viewed as a special case of Corollary 4, in which n = d1 and X = Id1\u00d7d1 . Consequently, we may prove the latter result, and then obtain the former result with this specialization. Recall that we let \u03c3min and \u03c3max denote (respectively) the minimum and maximum eigenvalues of X, and that \u03bamax = maxj=1,...,d1 \u2016Xj\u20162 denotes the maximum \u21132-norm over the columns. (In the special case X = Id1\u00d7d2 , we have \u03c3min = \u03c3max = \u03bamax = 1.)\nBoth corollaries are based on the regularizer, R(\u00b7) = \u2016 \u00b7 \u20161, and the associated dual norm R\u2217(\u00b7) = \u2016 \u00b7 \u2016\u221e. We need to verify that the stated choices of (\u03bbd, \u00b5d) satisfy the requirements (29) of Corollary 1. Given our assumptions on the pair (X,W ), a little calculation shows that the matrix Z : = XTW \u2208 Rd1\u00d7d2 has independent columns, with each column Zj \u223c N(0, \u03bd2X TX n ). Since |||XTX|||op \u2264 \u03c32max, known results on the singular values of Gaussian random matrices [10] imply that\nP [ |||XTW |||op \u2265 4\u03bd \u03c3max( \u221a d1 + \u221a d2)\u221a\nn\n] \u2264 2 exp ( \u2212 c(d1 + d2) ) .\nConsequently, setting \u03bbd \u2265 16\u03bd \u03c3max( \u221a d1+ \u221a d2)\u221a\nn ensures that the requirement (26) is satisfied. As\nfor the associated requirement for \u00b5d, it suffices to upper bound the elementwise \u2113\u221e norm of XTW . Since the \u21132 norm of the columns of X are bounded by \u03bamax, the entries of X\nT W are i.i.d. and Gaussian with variance at most (\u03bd\u03bamax)\n2/n. Consequently, the standard Gaussian tail bound combined with union bound yields\nP [ \u2016XT W\u2016\u221e \u2265 4\n\u03bd\u03bamax\u221a n\nlog(d1d2) ] \u2264 exp(\u2212 log d1d2),\nfrom which we conclude that the stated choices of (\u03bbd, \u00b5d) are valid with high probability. Turning now to the RSC condition, we note that in the case of multivariate regression, we have\n1 2 |||X(\u2206)|||2F = 1 2 |||X\u2206|||2F \u2265 \u03c32min 2 |||\u2206|||2F,\nshowing that the RSC condition holds with \u03b3 = \u03c32min.\nIn order to obtain the sharper result for X = Id1\u00d7d1 in Corollary 2\u2014in which log(d1d2) is replaced by the smaller quantity log(d1d2s )\u2014 we need to be more careful in upper bounding the noise term \u3008\u3008W, \u2206\u0302\u0393\u3009\u3009. We refer the reader to Appendix C.1 for details of this argument."}, {"heading": "5.3 Proof of Corollary 3", "text": "For this model, the noise matrix is recentered Wishart noise\u2014namely, W = 1n \u2211n i=1 ZiZ T i \u2212\u03a3, where each Zi \u223c N(0,\u03a3). Letting Ui \u223c N(0, Id\u00d7d) be i.i.d. Gaussian random vectors, we have\n|||W |||op = ||| \u221a \u03a3 ( 1 n n\u2211\ni=1\nUiUi \u2212 Id\u00d7d )\u221a \u03a3|||op \u2264 |||\u03a3|||op ||| 1\nn\nn\u2211\ni=1\nUiU T i \u2212 Id\u00d7d|||op \u2264 4|||\u03a3|||op\n\u221a d\nn ,\nwhere the final bound holds with probability greater than 1 \u2212 2 exp(\u2212c1d), using standard tail bounds on Gaussian random matrices [10]. Thus, we see that the specified choice (36) of \u03bbd is valid for Theorem 1 with high probability.\nWe now turn to the choice of \u00b5d. The entries of W are products of Gaussian variables, and hence have sub-exponential tails (e.g., [3]). Therefore, for any entry (i, j), we have the tail bound P[|Wij | > \u03c1(\u03a3)t] \u2264 2 exp(\u2212nt2/20), valid for all t \u2208 (0, 1]. By union bound over all d2 entries, we conclude that\nP [ \u2016W\u2016\u221e \u2265 8\u03c1(\u03a3)\n\u221a log d\nn\n] \u2264 2 exp(\u2212c2 log d),\nwhich shows that the specified choice of \u00b5d is also valid with high probability."}, {"heading": "5.4 Proof of Proposition 1", "text": "To begin, let us recall condition (52) on the regularization parameters, and that, for this proof, the matrices (\u0398\u0302, \u0393\u0302) denote any optimal solutions to the optimization problems (40) and (41) defining the two-step procedure. We again define the error matrices \u2206\u0302\u0398 = \u0398\u0302\u2212\u0398\u22c6 and \u2206\u0302\u0393 = \u0393\u0302\u2212 \u0393\u22c6, the matrices \u2206\u0302\u0393\nM : = \u03a0M(\u2206\u0302 \u0393) and \u2206\u0302\u0393 M\u22a5 = \u03a0M\u22a5(\u2206\u0302 \u0393), and the matrices \u0393\u22c6M\nand \u0393\u22c6M\u22a5 as previously defined in the proof of Theorem 1.\nOur proof of Proposition 1 is based on two lemmas, of which the first provides control on the error \u2206\u0302\u0393 in estimating the sparse component.\nLemma 3. Under the assumptions of Proposition 1, for any subset S of matrix indices of cardinality at most s, the sparse error \u2206\u0302\u0393 in any solution of the convex program (40) satisfies the bound\n|||\u2206\u0302\u0393|||2F \u2264 c1 \u00b52d { s+ 1\n\u00b5d\n\u2211\n(j,k)/\u2208S |\u0393\u22c6jk|\n} . (59)\nProof. Since \u0393\u0302 and \u0393\u22c6 are optimal and feasible (respectively) for the convex program (40), we have\n1 2 |||\u0393\u0302\u2212 Y |||2F + \u00b5d\u2016\u0393\u0302\u20161 \u2264 1 2 |||\u0398\u22c6 +W |||2F + \u00b5d\u2016\u0393\u22c6\u20161.\nRe-writing this inequality in terms of the error \u2206\u0302\u0393 and re-arranging terms yields\n1 2 |||\u2206\u0302\u0393|||2F \u2264 |\u3008\u3008\u2206\u0302\u0393, W +\u0398\u22c6\u3009\u3009| + \u00b5d\u2016\u0393\u22c6\u20161 \u2212 \u00b5d\u2016\u0393\u22c6 + \u2206\u0302\u0393\u20161.\nBy decomposability of the \u21131-norm, we obtain\n1 2 |||\u2206\u0302\u0393|||2F \u2264 |\u3008\u3008\u2206\u0302\u0393, W +\u0398\u22c6\u3009\u3009| + \u00b5d\n{ \u2016\u0393\u2217S\u20161 + \u2016\u0393\u2217Sc\u20161 \u2212 \u2016\u0393\u2217S + \u2206\u0302\u0393S\u20161 \u2212 \u2016\u0393\u2217Sc + \u2206\u0302\u0393Sc\u20161 }\n\u2264 |\u3008\u3008\u2206\u0302\u0393, W +\u0398\u22c6\u3009\u3009| + \u00b5d { 2\u2016\u0393\u2217Sc\u20161 + \u2016\u2206\u0302\u0393S\u20161 \u2212 \u2016\u2206\u0302\u0393Sc\u20161 } ,\nwhere the second step is based on two applications of the triangle inequality. Now by applying Ho\u0308lder\u2019s inequality and the triangle inequality to the first term on the right-hand side, we obtain\n1 2 |||\u2206\u0302\u0393|||2F \u2264 \u2016\u2206\u0302\u0393\u20161 [\u2016W\u2016\u221e + \u2016\u0398\u22c6\u2016\u221e] + \u00b5d\n{ 2\u2016\u0393\u2217Sc\u20161 + \u2016\u2206\u0302\u0393S\u20161 \u2212 \u2016\u2206\u0302\u0393Sc\u20161 }\n= \u2016\u2206\u0302\u0393S\u20161 { \u2016W\u2016\u221e + \u2016\u0398\u22c6\u2016\u221e + \u00b5d}+ \u2016\u2206\u0302\u0393Sc\u20161 { \u2016W\u2016\u221e + \u2016\u0398\u22c6\u2016\u221e \u2212 \u00b5d}+ 2\u00b5d\u2016\u0393\u2217Sc\u20161 \u2264 2\u00b5d\u2016\u2206\u0302\u0393S\u20161 + 2\u00b5d\u2016\u0393\u2217Sc\u20161,\nwhere the final inequality follows from our stated choice (42) of the regularization parameter \u00b5d. Since \u2016\u2206\u0302\u0393S\u20161 \u2264 \u221a s|||\u2206\u0302S |||F \u2264 \u221a s|||\u2206\u0302\u0393|||F, the claim (59) follows with some algebra.\nOur second lemma provides a bound on the low-rank error \u2206\u0302\u0398 in terms of the sparse matrix error \u2206\u0302\u0393.\nLemma 4. If in addition to the conditions of Proposition 1, the sparse erorr matrix is bounded as |||\u2206\u0302\u0393|||F \u2264 \u03b4, then the low-rank error matrix is bounded as\n|||\u2206\u0302\u0398|||2F \u2264 c1\u03bb2d { r + 1\n\u03bbd\nd\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) } + c2\u03b4 2. (60)\nAs the proof of this lemma is somewhat more involved, we defer it to Appendix D. Finally, combining the low-rank bound (60) with the sparse bound (59) from Lemma 3 yields the claim of Proposition 1."}, {"heading": "5.5 Proof of Corollary 6", "text": "For this corollary, we have R(\u00b7) = \u2016 \u00b7 \u20162,1 and R\u2217(\u00b7) = \u2016 \u00b7 \u20162,\u221e. In order to establish the claim, we need to show that the conditions of Corollary 5 on the regularization pair (\u03bbd, \u00b5d) hold with high probability. The setting of \u03bbd is the same as Corollary 2, and is valid by our earlier argument. Hence, in order to complete the proof, it remains to establish an upper bound on \u2016W\u20162,\u221e.\nLet Wk be the k th column of the matrix. Noting that the function Wk 7\u2192 \u2016Wk\u20162 is\nLipschitz, by concentration of measure for Gaussian Lipschitz functions [16], we have\nP [ \u2016Wk\u20162 \u2265 E\u2016Wk\u20162 + t ] \u2264 exp ( \u2212 t\n2d1d2 2\u03bd2\n) for all t > 0.\nUsing the Gaussianity of Wk, we have E\u2016Wk\u20162 \u2264 \u03bd\u221ad1d2 \u221a d1 = \u03bd\u221a d2 . Applying union bound\nover all d2 columns, we conclude that with probability greater than 1\u2212 exp ( \u2212 t2d1d2\n2\u03bd2 + log d2\n) ,\nwe have maxk \u2016Wk\u20162 \u2264 \u03bd\u221ad2 + t. Setting t = 4\u03bd \u221a log d2 d1d2 yields\nP [ \u2016W\u20162,\u221e \u2265\n\u03bd\u221a d2 + 4\u03bd \u221a log d2 d1d2 ] \u2264 exp ( \u2212 3 log d2 ) ,\nfrom which the claim follows.\nAs before, a sharper bound (with log d2 replaced by log(d2/s)) can be obtained by a refined argument; we refer the reader to Appendix C.2 for the details."}, {"heading": "5.6 Proof of Corollary 7", "text": "For this model, the noise matrix takes the formW : = 1n \u2211n i=1 UiU T i \u2212\u0398\u22c6, where Ui \u223c N(0,\u0398\u22c6). Since \u0398\u22c6 is positive semidefinite with rank at most r, we can write\nW = Q { 1 n ZiZ T i \u2212 Ir\u00d7r } QT ,\nwhere the matrix Q \u2208 Rd\u00d7r satisfies the relationship \u0398\u22c6 = QQT , and Zi \u223c N(0, Ir\u00d7r) is standard Gaussian in dimension r. Consequently, by known results on singular values of Wishart matrices [10], we have |||W |||op \u2264 \u221a 8|||\u0398\u22c6|||op \u221a r n with high probability, showing that the specified choice of \u03bbd is valid. It remains to bound the quantity \u2016W\u20162,\u221e. By known matrix norm bounds [13], we have \u2016W\u20162,\u221e \u2264 |||W |||op, so that the claim follows by the previous argument."}, {"heading": "5.7 Proof of Theorem 2", "text": "Our lower bound proofs are based on a standard reduction [12, 31, 30] from estimation to a multiway hypothesis testing problem over a packing set of matrix pairs. In particular, given a collection {(\u0398j ,\u0393j), j = 1, 2, . . . ,M} of matrix pairs contained in some family F , we say that it forms a \u03b4-packing in Frobenius norm if, for all distinct pairs i, j \u2208 {1, 2, . . . ,M}, we have\n|||\u0398i \u2212\u0398j|||2F + |||\u0393i \u2212 \u0393j|||2F \u2265 \u03b42.\nGiven such a packing set, it is a straightforward consequence of Fano\u2019s inequality that the minimax error over F satisfies the lower bound\nP [ M(F) \u2265 \u03b4 2\n8\n] \u2265 1\u2212 I(Y ;J) + log 2\nlogM , (61)\nwhere I(Y ;J) is the mutual information between the observation matrix Y \u2208 Rd1\u00d7d2 , and J is an index uniformly distributed over {1, 2, . . . ,M}. In order to obtain different components of our bound, we make different choices of the packing set, and use different bounding techniques for the mutual information."}, {"heading": "5.7.1 Lower bounds for elementwise sparsity", "text": "We begin by proving the lower bound (50) for matrix decompositions over the family Fsp(r, s, \u03b1).\nPacking for radius of non-identifiability Let us first establish the lower bound involving the radius of non-identifiability, namely the term scaling as \u03b1\n2s d1d2 in the case of s-sparsity for \u0398\u22c6. Recall from Example 6 the \u201cbad\u201d matrix (33), which we denote here by B\u2217. By construction, we have |||B\u2217|||2F = \u03b1 2s d1d2\n. Using this matrix, we construct a very simple packing set with M = 4 matrix pairs (\u0398,\u0393):\n{ (B\u2217,\u2212B\u2217), (\u2212B\u2217, B\u2217), ( 1\u221a\n2 B\u2217,\u2212 1\u221a 2 B\u2217), (0, 0)\n} (62)\nEach one of these matrix pairs (\u0398,\u0393) belongs to the set Fsp(1, s, \u03b1), so it can be used to establish a lower bound over this set. (Moreover, it also yields a lower bound over the sets Fsp(r, s, \u03b1) for r > 1, since they are supersets.) It can also be verified that for any two distinct pairs of matrices in the set (62), they differ in squared Frobenius norm by at least\n\u03b42 = 12 |||B\u2217|||2F = 12 \u03b1 2s d1d2 . Let J be a random index uniformly distributed over the four possible models in our packing set (62). By construction, for any matrix pair (\u0398,\u0393) in the packing set, we have \u0398 + \u0393 = 0. Consequently, for any one of these models, the observation matrix Y is simply equal to pure noise W , and hence I(Y ;J) = 0. Putting together the pieces, the Fano bound (61) implies that\nP [ M(Fsp(1, s, \u03b1)) \u2265 1\n16\n\u03b12s\nd1d2\n] \u2265 1\u2212 I(Y ;J) + log 2\nlog 4 =\n1 2 .\nPacking for estimation error: We now describe the construction of a packing set for lower bounding the estimation error. In this case, our construction is more subtle, based on the the Cartesian product of two components, one for the low rank matrices, and the other for the sparse matrices. For the low rank component, we re-state a slightly modified form (adapted to the setting of non-square matrices) of Lemma 2 from the paper [20]:\nLemma 5. For d1, d2 \u2265 10, a tolerance \u03b4 > 0, and for each r = 1, 2, . . . , d, there exists a set of d1 \u00d7 d2-dimensional matrices {\u03981, . . . ,\u0398M} with cardinality M \u2265 14 exp ( rd1 256 + rd2 256 ) such that each matrix has rank r, and moreover\n|||\u0398\u2113|||2F = \u03b42 for all \u2113 = 1, 2, . . . ,M , (63a) |||\u0398\u2113 \u2212\u0398k|||2F \u2265 \u03b42 for all \u2113 6= k, (63b)\n\u2016\u0398\u2113\u2016\u221e \u2264 \u03b4 \u221a 32 log(d1d2)\nd1d2 for all \u2113 = 1, 2, . . . ,M . (63c)\nConsequently, as long as \u03b4 \u2264 1, we are guaranteed that the matrices \u0398\u2113 belong to the set Fsp(r, s, \u03b1) for all \u03b1 \u2265 32 \u221a log(d1d2).\nAs for the sparse matrices, the following result is a modification, so as to apply to the matrix setting of interest here, of Lemma 5 from the paper [23]:\nLemma 6 (Sparse matrix packing). For any \u03b4 > 0, and for each integer s < d1d2, there exists a set of matrices {\u03931, . . . ,\u0393N} with cardinality N \u2265 exp ( s 2 log d1d2\u2212s s/2 ) such that\n|||\u0393j \u2212 \u0393k|||2F \u2265 \u03b42, and (64a) |||\u0393j|||2F \u2264 8 \u03b42, (64b)\nand such that each \u0393j has at most s non-zero entries.\nWe now have the necessary ingredients to prove the lower bound (50). By combining Lemmas 5 and 6, we conclude that there exists a set of matrices with cardinality\nM N \u2265 1 4 exp\n{ s\n2 log d1d2 \u2212 s s/2 + rd1 256 + rd2 256\n} (65)\nsuch that\n|||(\u0398\u2113,\u0393k)\u2212 (\u0398\u2113\u2032 ,\u0393k\u2032)|||2F \u2265 \u03b42 for all pairs such that \u2113 6= \u2113\u2032 or k 6= k\u2032, and (66a) |||(\u0398\u2113,\u0393k)|||2F \u2264 9\u03b42 for all (\u2113, k). (66b)\nLet P\u2113,k denote the distribution of the observation matrix Y when \u0398\u2113 and \u0393k are the underlying parameters. We apply the Fano construction over the class of MN such distributions, thereby obtaining that in order to show that the minimax error is lower bounded by c0\u03b4 2 (for some universal constant c0 > 0), it suffices to show that\n1\n(MN 2 ) \u2211 (\u2113,k)6=(\u2113\u2032,k\u2032) D(P\u2113,k \u2016P\u2113\u2032,k\u2032) + log 2\nlog(MN) \u2264 1 2 , (67)\nwhere D(P\u2113,k \u2016P\u2113\u2032,k\u2032) denotes the Kullback-Leibler divergence between the distributions P\u2113,k and P\u2113 \u2032,k\u2032. Given the assumption of Gaussian noise with variance \u03bd2/(d1d2), we have\nD(Pj \u2016Pk) = d1d2 2\u03bd2 |||(\u0398\u2113,\u0393k)\u2212 (\u0398\u2113\u2032 ,\u0393k\u2032)|||2F (i) \u2264 18d1d2\u03b4 2 \u03bd2 , (68)\nwhere the bound (i) follows from the condition (66b). Combined with lower bound (65), we see that it suffices to choose \u03b4 such that\n18d1d2\u03b42\n\u03bd2 + log 2\nlog 14+ { s 2 log d1d2\u2212s s/2 + rd1 256 + rd2 256 } \u2264 1 2 .\nFor d1, d2 larger than a finite constant (to exclude degenerate cases), we see that the choice\n\u03b42 = c0\u03bd 2\n{ r\nd1 +\nr\nd2 + s log d1d2\u2212ss/2 d1d2 } ,\nfor a suitably small constant c0 > 0 is sufficient, thereby establishing the lower bound (50)."}, {"heading": "5.7.2 Lower bounds for columnwise sparsity", "text": "The lower bound (51) for columnwise follows from a similar argument. The only modifications are in the packing sets.\nPacking for radius of non-identifiability In order to establish a lower bound of order \u03b12s d2 , recall the \u201cbad\u201d matrix (45) from Example 8, which we denote by B\u2217. By construction, it has squared Frobenius norm |||B\u2217|||2F = \u03b1 2s d2 . We use it to form the packing set\n{ (B\u2217,\u2212B\u2217), (\u2212B\u2217, B\u2217), ( 1\u221a\n2 B\u2217,\u2212 1\u221a 2 B\u2217), (0, 0)\n} (69)\nEach one of these matrix pairs (\u0398,\u0393) belongs to the set Fcol(1, s, \u03b1), so it can be used to establish a lower bound over this set. (Moreover, it also yields a lower bound over the sets Fcol(r, s, \u03b1) for r > 1, since they are supersets.) It can also be verified that for any two distinct pairs of matrices in the set (69), they differ in squared Frobenius norm by at least \u03b42 = 12 |||B\u2217|||2F = 12 \u03b1 2s d2 . Consequently, the same argument as before shows that\nP [ M(Fcol(1, s, \u03b1)) \u2265 1\n16\n\u03b12s\nd2\n] \u2265 1\u2212 I(Y ;J) + log 2\nlog 4 =\n1 2 .\nPacking for estimation error: We now describe packings for the estimation error terms. For the low-rank packing set, we need to ensure that the (2,\u221e)-norm is controlled. From the bound (63c), we have the guarantee\n\u2016\u0398\u2113\u20162,\u221e \u2264 \u03b4 \u221a 32 log(d1d2)\nd2 for all \u2113 = 1, 2, . . . ,M , (70)\nso that, as long as \u03b4 \u2264 1, the matrices \u0398\u2113 belong to the set Fcol(r, s, \u03b1) for all \u03b1 \u2265 32 \u221a log(d1d2).\nThe following lemma characterizes a suitable packing set for the columnwise sparse component:\nLemma 7 (Columnwise sparse matrix packing). For all d2 \u2265 10 and integers s in the set {1, 2, . . . , d2 \u2212 1}, there exists a family d1 \u00d7 d2 matrices {\u0393k, k = 1, 2, . . . N} with cardinality\nN \u2265 exp (s 8 log d2 \u2212 s s/2 + sd1 8 ) ,\nsatisfying the inequalities\n|||\u0393j \u2212 \u0393k|||2F \u2265 \u03b42, for all j 6= k, and (71a) |||\u0393j |||2F \u2264 64 \u03b42, (71b)\nand such that each \u0393j has at most s non-zero columns.\nThis claim follows by suitably adapting Lemma 5(b) in the paper by Raskutti et al. [24] on minimax rates for kernel classes. In particular, we view column j of a matrix \u0393 as defining a linear function in dimension Rd1 ; for each j = 1, 2, . . . , d1, this defines a Hilbert space Hj of functions. By known results on metric entropy of Euclidean balls [17], this function class has logarithmic metric entropy, so that part (b) of the above lemma applies, and yields the stated result.\nUsing this lemma and the packing set for the low-rank component and following through the Fano construction yields the claimed lower bound (50) on the minimax error for the class Fcol(r, s, \u03b1), which completes the proof of Theorem 2."}, {"heading": "6 Discussion", "text": "In this paper, we analyzed a class of convex relaxations for solving a general class of matrix decomposition problems, in which the goal is recover a pair of matrices, based on observing a noisy contaminated version of their sum. Since the problem is ill-posed in general, it is essential to impose structure, and this paper focuses on the setting in which one matrix is approximately low-rank, and the second has a complementary form of low-dimensional structure enforced by a decomposable regularizer. Particular cases include matrices that are elementwise sparse, or columnwise sparse, and the associated matrix decomposition problems have various applications, including robust PCA, robustness in collaborative filtering, and model selection in Gauss-Markov random fields. We provided a general non-asymptotic bound on the Frobenius error of a convex relaxation based on a regularizing the least-squares loss with a combination of the nuclear norm with a decomposable regularizer. When specialized\nto the case of elementwise and columnwise sparsity, these estimators yield rates that are minimax-optimal up to constant factors.\nVarious extensions of this work are possible. We have not discussed here how our estimator would behave under a partial observation model, in which only a fraction of the entries are observed. This problem is very closely related to matrix completion, a problem for which recent work by Negahban and Wainwright [20] shows that a form of restricted strong convexity holds with high probability. This property could be adapted to the current setting, and would allow for proving Frobenius norm error bounds on the low rank component. Finally, although this paper has focused on the case in which the first matrix component is approximately low rank, much of our theory could be applied to a more general class of matrix decomposition problems, in which the first component is penalized by a decomposable regularizer that is \u201ccomplementary\u201d to the second matrix component. It remains to explore the properties and applications of these different forms of matrix decomposition."}, {"heading": "Acknowledgements", "text": "All three authors were partially supported by grant AFOSR-09NL184. In addition, SN and MJW were partially supported by grant NSF-CDI-0941742, and AA was partially supported a Microsoft Research Graduate Fellowship. All three authors would like to acknowledge the Banff International Research Station (BIRS) in Banff, Canada for hospitality and work facilities that stimulated and supported this collaboration."}, {"heading": "A Proof of Lemma 1", "text": "The decomposition described in part (a) was established by Recht et al. [25], so that it remains to prove part (b). With the appropriate definitions, part (b) can be recovered by exploiting Lemma 1 from Negahban et al. [19]. Their lemma applies to optimization problems of the general form\nmin \u03b8\u2208Rp\n{ L(\u03b8) + \u03b3nr(\u03b8) } ,\nwhere L is a loss function on the parameter space, and r is norm-based regularizer that satisfies a property known as decomposability. The elementwise \u21131-norm as well as the nuclear norm are both instances of decomposable regularizers. Their lemma requires that the regularization parameter \u03b3n be chosen such that \u03b3n \u2265 2 r\u2217 ( \u2207L(\u03b8\u2217) ) , where r\u2217 is the dual norm, and \u2207L(\u03b8\u2217) is the gradient of the loss evaluated at the true parameter.\nWe now discuss how this lemma can be applied in our special case. Here the relevant parameters are of the form \u03b8 = (\u0398,\u0393), and the loss function is given by\nL(\u0398,\u0393) = 1 2 |||Y \u2212 (\u0398 + \u0393)|||2F.\nThe sample size n = d2, since we make one observation for each entry of the matrix. On the other hand, the regularizer is given by the function\nr(\u03b8) = Q(\u0398,\u0393) := |||\u0398|||N + \u00b5d \u03bbd R(\u0393),\ncoupled with the regularization parameter \u03b3n = \u03bbd. By assumption, the regularizer R is decomposable, and as shown in the paper [19], the nuclear norm is also decomposable. Since\nQ is simply a sum of these decomposable regularizers over separate matrices, it is also decomposable.\nIt remains to compute the gradient \u2207L(\u0398\u22c6,\u0393\u22c6), and evaluate the dual norm. A straightforward calculation yields that \u2207L(\u0398\u22c6,\u0393\u22c6) = [ W W ]T . In addition, it can be verified by standard properties of dual norms\nQ\u2217(U, V ) = |||U |||op + \u03bbd \u00b5d R\u2217(V ).\nThus, it suffices to choose the regularization parameter such that\n\u03bbd \u2265 2Q\u2217(W,W ) = 2|||W |||op + 2\u03bbd \u00b5d R\u2217(W ).\nGiven our condition (52), we have\n2|||W |||op + 2\u03bbd \u00b5d R\u2217(W ) \u2264 2|||W |||op + \u03bbd 2 ,\nmeaning that it suffices to have \u03bbd \u2265 4|||W |||op, as stated in the second part of condition (52)."}, {"heading": "B Proof of Lemma 2", "text": "By the RSC condition (22), we have\n1 2 |||X(\u2206\u0302\u0398 + \u2206\u0302\u0393)|||2F \u2212 \u03b3 2 |||\u2206\u0302\u0398 + \u2206\u0302\u0393|||2F \u2265 \u2212\u03c4n\u03a62(\u2206\u0302\u0398 + \u2206\u0302\u0393) \u2265 \u2212\u03c4nQ2(\u2206\u0302\u0398, \u2206\u0302\u0393), (72)\nwhere the second inequality follows by the definitions (20) and (21) of Q and \u03a6 respectively. We now derive a lower bound on |||\u2206\u0302\u0398+\u2206\u0302\u0393|||F , and an upper bound on Q2(\u2206\u0302\u0398, \u2206\u0302\u0393). Beginning with the former term, observe that\n\u03b3\n2\n( |||\u2206\u0302\u0398|||2F + |||\u2206\u0302\u0393|||2F ) \u2212 \u03b3\n2 |||\u2206\u0302\u0398 + \u2206\u0302\u0393|||2F = \u2212\u03b3\u3008\u3008\u2206\u0302\u0398, \u2206\u0302\u0393\u3009\u3009,\nso that it suffices to upper bound \u03b3|\u3008\u3008\u2206\u0302\u0398, \u2206\u0302\u0393\u3009\u3009|. By the duality of the pair (R,R\u2217), we have\n\u03b3 \u2223\u2223\u3008\u3008\u2206\u0302\u0398, \u2206\u0302\u0393\u3009\u3009 \u2223\u2223 \u2264 \u03b3R\u2217(\u2206\u0302\u0398) R(\u2206\u0302\u0393).\nNow since \u0398\u0302 and \u0398\u22c6 are both feasible for the program (7) and recalling that \u2206\u0302\u0398 = \u0398\u0302\u2212 \u0398\u22c6, an application of triangle inequality yields\n\u03b3R\u2217(\u2206\u0302\u0398) \u2264 \u03b3 { R\u2217(\u0398\u0302) +R\u2217(\u0398\u22c6) } \u2264 2\u03b1 \u03b3\n\u03bad\n(i) \u2264 \u00b5d 2 ,\nwhere inequality (i) follows from our choice of \u00b5d. Putting together the pieces, we have shown that\n\u03b3 2 |||\u2206\u0302\u0398 + \u2206\u0302\u0393|||2F \u2265 \u03b3 2\n( |||\u2206\u0302\u0398|||2F + |||\u2206\u0302\u0393|||2F ) \u2212 \u00b5d\n2 R(\u2206\u0302\u0393).\nSince the quantity \u03bbd|||\u2206\u0302\u0398|||N \u2265 0, we can write \u03b3\n2 |||\u2206\u0302\u0398 + \u2206\u0302\u0393|||2F \u2265\n\u03b3\n2\n( |||\u2206\u0302\u0398|||2F + |||\u2206\u0302\u0393|||2F ) \u2212 \u00b5d\n2 R(\u2206\u0302\u0393)\u2212 \u03bbd 2 |||\u2206\u0302\u0398|||N\n= \u03b3\n2\n( |||\u2206\u0302\u0398|||2F + |||\u2206\u0302\u0393|||2F ) \u2212 \u03bbd\n2 Q(\u2206\u0302\u0398, \u2206\u0302\u0393),\nwhere the latter equality follows by the definition (20) of Q. Next we turn to the upper bound on Q(\u2206\u0302\u0398, \u2206\u0302\u0393). By the triangle inequality, we have\nQ(\u2206\u0302\u0398, \u2206\u0302\u0393) \u2264 Q(\u2206\u0302\u0398A, \u2206\u0302\u0393M) +Q(\u2206\u0302\u0398B , \u2206\u0302\u0393M\u22a5).\nFurthermore, substituting in equation (53) into the above equation yields\nQ(\u2206\u0302\u0398, \u2206\u0302\u0393) \u2264 4Q(\u2206\u0302\u0398A, \u2206\u0302\u0393M) + 4{ d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + \u00b5d \u03bbd R(\u0393\u22c6 M\u22a5 )}. (73)\nSince \u2206\u0302\u0398A has rank at most 2r and \u2206\u0302 \u0393 M belongs to the model space M, we have\n\u03bbdQ(\u2206\u0302\u0398A, \u2206\u0302\u0393M) \u2264 \u221a 2r \u03bbd|||\u2206\u0302\u0398A|||F +\u03a8(M)\u00b5d|||\u2206\u0302\u0393M|||F\n\u2264 \u221a 2r \u03bbd|||\u2206\u0302\u0398|||F +\u03a8(M)\u00b5d|||\u2206\u0302\u0393|||F.\nThe claim then follows by substituting the above equation into equation (73), and then substituting the result into the earlier inequality (72)."}, {"heading": "C Refinement of achievability results", "text": "In this appendix, we provide refined arguments that yield sharpened forms of Corollaries 2 and 6. These refinements yield achievable bounds that match the minimax lower bounds in Theorem 2 up to constant factors. We note that these refinements are significantly different only when the sparsity index s scales as \u0398(d1d2) for Corollary 2, or as \u0398(d2) for Corollary 6.\nC.1 Refinement of Corollary 2\nIn the proof of Theorem 1, when specialized to the \u21131-norm, the noise term |\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009| is simply upper bounded by \u2016W\u2016\u221e\u2016\u2206\u0302\u0393\u20161. Here we use a more careful argument to control this noise term. Throughout the proof, we assume that the regularization parameter \u03bbd is set in the usual way, whereas we choose\n\u00b5d = 16\u03bd \u221a log d1d2s d1d2 + 4\u03b1\u221a d1d2 . (74)\nWe split our analysis into two cases.\nCase 1: First, suppose that \u2016\u2206\u0302\u0393\u20161 \u2264 \u221a s|||\u2206\u0302\u0393|||F. In this case, we have the upper bound\n\u2223\u2223\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009 \u2223\u2223 \u2264 sup\n\u2016\u2206\u20161\u2264 \u221a s |||\u2206\u0302\u0393|||F\n|||\u2206|||F\u2264|||\u2206\u0302\u0393|||F\n|\u3008\u3008W, \u2206\u3009\u3009| = |||\u2206\u0302\u0393|||F sup \u2016\u2206\u20161\u2264 \u221a s\n|||\u2206|||F\u22641\n|\u3008\u3008W, \u2206\u3009\u3009|\n\ufe38 \ufe37\ufe37 \ufe38 Z(s)\nIt remains to upper bound the random variable Z(s). Viewed as a function of W , it is a Lipschitz function with parameter \u03bd\u221a\nd1d2 , so that\nP [ Z(s) \u2265 E[Z(s)] + \u03b4] \u2264 exp ( \u2212 d1 d2\u03b4 2\n2\u03bd2\n) .\nSetting \u03b42 = 4s\u03bd 2 d1d2 log(d1d2s ), we have\nZ(s) \u2264 E[Z(s)] + 2s\u03bd d1d2\n[ log ( d1d2 s )]\nwith probability greater than 1\u2212 exp ( \u2212 2s log(d1d2s ) ) .\nIt remains to upper bound the expected value. In order to do so, we apply Theorem 5.1(ii) from Gordon et al. [11] with (q0, q1) = (1, 2), n = d1d2 and t = \u221a s, thereby obtaining\nE[Z(t)] \u2264 c\u2032 \u03bd\u221a d1d2\n\u221a s \u221a 2 + log ( 2d1d2 s ) \u2264 c \u03bd\u221a\nd1d2\n\u221a s log ( d1d2 s ) .\nWith this bound, proceeding through the remainder of the proof yields the claimed rate.\nCase 2: Alternatively, we must have \u2016\u2206\u0302\u0393\u20161 > \u221a s|||\u2206\u0302\u0393|||F. In this case, we need to show that the stated choice (74) of \u00b5d satisfies \u00b5d\u2016\u2206\u0302\u0393\u20161 \u2265 2|\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009| with high probability. As can be seen from examining the proofs, this condition is sufficient to ensure that Lemma 1 and Lemma 2 all hold, as required for our analysis.\nWe have the upper bound\n\u2223\u2223\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009 \u2223\u2223 \u2264 sup\n\u2016\u2206\u20161\u2264\u2016\u2206\u0302\u0393\u20161 |||\u2206|||F\u2264|||\u2206\u0302\u0393|||F\n|\u3008\u3008W, \u2206\u3009\u3009| = |||\u2206\u0302\u0393|||F Z ( \u2016\u2206\u0302\u0393\u20161 |||\u2206\u0302\u0393|||F ) ,\nwhere for any radius t > 0, we define the random variable\nZ(t) := sup \u2016\u2206\u20161\u2264t |||\u2206|||F\u22641\n|\u3008\u3008W, \u2206\u3009\u3009|.\nFor each fixed t, the same argument as before shows that Z(t) is concentrated around its expectation, and Theorem 5.1(ii) from Gordon et al. [11] with (q0, q1) = (1, 2), n = d1d2 yields\nE [ Z(t) ] \u2264 c \u03bd\u221a\nd1d2 t\n\u221a log ( d1d2 t2 ) .\nSetting \u03b42 = 4t 2\u03bd2 d1d2 log(d1d2s ) in the concentration bound, we conclude that\nZ(t) \u2264 c\u2032 t \u03bd\u221a d1d2\n{\u221a log ( d1d2 s ) + \u221a log ( d1d2 t2 )} .\nwith high probability. A standard peeling argument (e.g., [28]) can be used to extend this bound to a uniform one over the choice of radii t, so that it applies to the random one t = \u2016\u2206\u0302 \u0393\u20161\n|||\u2206\u0302\u0393|||F of interest. (The only changes in doing such a peeling are in constant terms.) We\nthus conclude that\nZ ( \u2016\u2206\u0302\u0393\u20161 |||\u2206\u0302\u0393|||F ) \u2264 c\u2032 \u2016\u2206\u0302 \u0393\u20161 |||\u2206\u0302\u0393|||F \u03bd\u221a d1d2 {\u221a log ( d1d2 s ) + \u221a log ( d1d2 \u2016\u2206\u0302\u0393\u201621/|||\u2206\u0302\u0393|||2F )}\nwith high probability. Since \u2016\u2206\u0302\u0393\u20161 > \u221a s|||\u2206\u0302\u0393|||F, we have 1\u2016\u2206\u0302\u0393\u20162\n1 /|||\u2206\u0302\u0393|||2 F\n\u2264 1s , and hence\n\u2223\u2223\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009 \u2223\u2223 \u2264 |||\u2206\u0302\u0393|||F Z ( \u2016\u2206\u0302\u0393\u20161 |||\u2206\u0302\u0393|||F ) \u2264 c\u2032\u2032 \u2016\u2206\u0302\u0393\u20161 \u03bd\u221a d1d2 \u221a log ( d1d2 s )\nwith high probability. With this bound, the remainder of the proof proceeds as before. In particular, the refined choice (74) of \u00b5d is adequate.\nC.2 Refinement of Corollary 6\nAs in the refinement of Corollary 2 from Appendix C.1, we need to be more careful in controlling the noise term \u3008\u3008W, \u2206\u0302\u0393\u3009\u3009. For this corollary, we make the refined choice of regularizer\n\u00b5d = 16\u03bd\n\u221a 1\nd2 + 16\u03bd\n\u221a log(d2/s)\nd1d2 + 4\u03b1\u221a d2\n(75)\nAs in Appendix C.1, we split our analysis into two cases.\nCase 1: First, suppose that \u2016\u2206\u0302\u0393\u20162,1 \u2264 \u221a s|||\u2206\u0302\u0393|||F. In this case, we have\n\u2223\u2223\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009 \u2223\u2223 \u2264 sup\n\u2016\u2206\u20162,1\u2264 \u221a s |||\u2206\u0302\u0393|||F\n|||\u2206|||F\u2264|||\u2206\u0302\u0393|||F\n|\u3008\u3008W, \u2206\u3009\u3009| = |||\u2206\u0302\u0393|||F sup \u2016\u2206\u20162,1\u2264 \u221a s\n|||\u2206|||F\u22641\n|\u3008\u3008W, \u2206\u3009\u3009|\n\ufe38 \ufe37\ufe37 \ufe38 Z\u0303(s)\nThe function W 7\u2192 Z\u0303(s) is a Lipschitz function with parameter \u03bd\u221a d1d2 , so that by concentration of measure for Gaussian Lipschitz functions [16], it satisfies the upper tail bound P [ Z\u0303(s) \u2265 E[Z\u0303(s)] + \u03b4] \u2264 exp ( \u2212 d1d2\u03b422\u03bd2 ) . Setting \u03b42 = 4s\u03bd 2 d1d2 log(d2s ) yields\nZ\u0303(s) \u2264 E[Z\u0303(s)] + 2\u03bd\n\u221a s log(d2s )\nd1d2 (76)\nwith probability greater than 1\u2212 exp ( \u2212 2s log(d2s ) ) .\nIt remains to upper bound the expectation. Applying the Cauchy-Schwarz inequality to each column, we have\nE[Z\u0303(s)] \u2264 E [\nsup \u2016\u2206\u20162,1\u2264 \u221a s\n|||\u2206|||F\u22641\nd2\u2211\nk=1\n\u2016Wk\u20162 \u2016\u2206k\u20162 ]\n= E [ sup\n\u2016\u2206\u20162,1\u2264 \u221a s\n|||\u2206|||F\u22641\nd2\u2211\nk=1\n( \u2016Wk\u20162 \u2212 E[\u2016Wk\u20162] ) \u2016\u2206k\u20162 ] + sup\n\u2016\u2206\u20162,1\u2264 \u221a s\n( d\u2211\nk=1\n\u2016\u2206k\u20162 ) E[\u2016W1\u20162]\n\u2264 E [\nsup \u2016\u2206\u20162,1\u2264 \u221a s\n|||\u2206|||F\u22641\nd2\u2211\nk=1\n( \u2016Wk\u20162 \u2212 E[\u2016Wk\u20162] ) \ufe38 \ufe37\ufe37 \ufe38\nVk\n\u2016\u2206k\u20162 ] + 4\u03bd \u221a s\nd2 ,\nusing the fact that E[\u2016W1\u20162] \u2264 \u03bd \u221a\nd1 d2d2 = \u03bd\u221a d2 .\nNow the variable Vk is zero-mean, and sub-Gaussian with parameter \u03bd\u221a d1d2 , again using concentration of measure for Lipschitz functions of Gaussians [16]. Consequently, by setting \u03b4k = \u2016\u2206k\u20162, we can write\nE[Z\u0303(s)] \u2264 E [\nsup \u2016\u03b4\u20161\u22644 \u221a s\n\u2016\u03b4\u20162\u22641\nd2\u2211\nk=1\nVk\u03b4k\n] + 4\u03bd \u221a s\nd2 ,\nApplying Theorem 5.1(ii) from Gordon et al. [11] with (q0, q1) = (1, 2), n = d2 and t = 4 \u221a s\nthen yields\nE[Z\u0303(s)] \u2264 c \u03bd\u221a d1d2\n\u221a s \u221a 2 + log ( 2d2 16s ) + 4\u03bd \u221a s d2 ,\nwhich combined with the concentration bound (76) yields the refined claim.\nCase 2: Alternatively, we may assume that \u2016\u2206\u0302\u0393\u20162,1 > \u221a s|||\u2206\u0302\u0393|||F. In this case, we need to verify that the choice (75) \u00b5d satisfies \u00b5d\u2016\u2206\u0302\u0393\u20162,1 \u2265 2|\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009| with high probability. We have the upper bound\n\u2223\u2223\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009 \u2223\u2223 \u2264 sup\n\u2016\u2206\u20162,1\u2264\u2016\u2206\u0302\u0393\u20162,1 |||\u2206|||F\u2264|||\u2206\u0302\u0393|||F\n|\u3008\u3008W, \u2206\u3009\u3009| = |||\u2206\u0302\u0393|||F Z\u0303 (\u2016\u2206\u0302\u0393\u20162,1\n|||\u2206\u0302\u0393|||F\n) ,\nwhere for any radius t > 0, we define the random variable\nZ\u0303(t) := sup \u2016\u2206\u20162,1\u2264t |||\u2206|||F\u22641\n|\u3008\u3008W, \u2206\u3009\u3009|.\nFollowing through the same argument as in Case 2 of Appendix C.1 yields that for any fixed t > 0, we have\nZ\u0303(t) \u2264 c \u03bd\u221a d1d2 t\n\u221a 2 + log ( 2d2 t2 ) + 4\u03bd t\u221a d2 + 2\u03bdt \u221a log(d2s ) d1d2\nwith high probability. As before, this can be extended to a uniform bound over t by a peeling argument, and we conclude that\n\u2223\u2223\u3008\u3008W, \u2206\u0302\u0393\u3009\u3009 \u2223\u2223 \u2264 = |||\u2206\u0302\u0393|||F Z\u0303 (\u2016\u2206\u0302\u0393\u20162,1 |||\u2206\u0302\u0393|||F )\n\u2264 c\u2016\u2206\u0302\u0393\u20162,1 { \u03bd\u221a\nd1d2\n\u221a 2 + log ( 2d2\n\u2016\u2206\u0302\u0393\u201622,1/|||\u2206\u0302\u0393|||2F\n) + 4\u03bd\n1\u221a d2 + 2\u03bd\n\u221a log(d2s )\nd1d2\n}\nwith high probability. Since 1\u2016\u2206\u0302\u0393\u20162 2,1/|||\u2206\u0302\u0393|||2F \u2264 1s by assumption, the claim follows."}, {"heading": "D Proof of Lemma 4", "text": "Since \u0398\u0302 and \u0398\u22c6 are optimal and feasible (respectively) for the convex program (41), we have\n1 2 |||Y \u2212 \u0398\u0302\u2212 \u0393\u0302|||2F + \u03bbd|||\u0398\u0302|||N \u2264 1 2 |||Y \u2212\u0398\u22c6 \u2212 \u0393\u0302|||2F + \u03bbd|||\u0398\u22c6|||N.\nRecalling that Y = \u0398\u22c6 + \u0393\u22c6 +W and re-writing in terms of the error matrices \u2206\u0302\u0393 = \u0393\u0302\u2212 \u0393\u22c6 and \u2206\u0302\u0398 = \u0398\u0302\u2212\u0398\u22c6, we find that\n1 2 |||\u2206\u0302\u0398 + \u2206\u0302\u0393 \u2212W |||2F + \u03bbd|||\u0398\u22c6 + \u2206\u0302\u0398|||N \u2264 1 2 |||\u2206\u0302\u0393 \u2212W |||2F + \u03bbd|||\u0398\u22c6|||N.\nExpanding the Frobenius norm and reorganizing terms yields\n1 2 |||\u2206\u0302\u0398|||2F \u2264 |\u3008\u3008\u2206\u0302\u0398, \u2206\u0302\u0393 +W \u3009\u3009|+ \u03bbd\n{ |||\u0398\u22c6|||N \u2212 \u03bbd|||\u0398\u22c6 + \u2206\u0302\u0398|||N } .\nFrom Lemma 1 in the paper [21], there exists a decomposition \u2206\u0302\u0398 = \u2206\u0302\u0398A + \u2206\u0302 \u0398 B such that the rank of \u2206\u0302\u0398A upper-bounded by 2 r and\n|||\u0398\u22c6|||N \u2212 |||\u0398\u22c6 + \u2206\u0302\u0398A + \u2206\u0302\u0398B|||N \u2264 2 d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6) + |||\u2206\u0302\u0398A|||N \u2212 |||\u2206\u0302\u0398B |||N,\nwhich implies that\n1 2 |||\u2206\u0302\u0398|||2F \u2264 |\u3008\u3008\u2206\u0302\u0398, \u2206\u0302\u0393 +W \u3009\u3009|+ \u03bbd\n{ |||\u2206\u0302\u0398A|||N \u2212 |||\u2206\u0302\u0398B |||N } + 2\u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6)\n(i) \u2264 |\u3008\u3008\u2206\u0302\u0398, \u2206\u0302\u0393\u3009\u3009|+ |\u3008\u3008\u2206\u0302\u0398, W \u3009\u3009|+ \u03bbd|||\u2206\u0302\u0398A|||N \u2212 \u03bbd|||\u2206\u0302\u0398B |||N + 2\u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6)\n(ii) \u2264 |||\u2206\u0302\u0398|||F \u03b4 + |||\u2206\u0302\u0398|||N|||W |||op + \u03bbd|||\u2206\u0302\u0398A|||N \u2212 \u03bbd|||\u2206\u0302\u0398B|||N + 2\u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6)\n(iii) \u2264 |||\u2206\u0302\u0398|||F \u03b4 + |||W |||op { |||\u2206\u0302\u0398A|||N + |||\u2206\u0302\u0398A|||N } + \u03bbd|||\u2206\u0302\u0398A|||N \u2212 \u03bbd|||\u2206\u0302\u0398B|||N + 2\u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6)\n= |||\u2206\u0302\u0398|||F \u03b4 + |||\u2206\u0302\u0398A|||N { |||W |||op + \u03bbd}+ |||\u2206\u0302\u0398B |||N { |||W |||op \u2212 \u03bbd } + 2\u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6),\nwhere step (i) follows by triangle inequality; step (ii) by the Cauchy-Schwarz and Ho\u0308lder inequality, and our assumed bound |||\u2206\u0302\u0393|||F \u2264 \u03b4; and step (iii) follows by substituting \u2206\u0302\u0398 = \u2206\u0302\u0398A + \u2206\u0302\u0398B and applying triangle inequality.\nSince we have chosen \u03bbd \u2265 |||W |||op, we conclude that\n1 2 |||\u2206\u0302\u0398|||2F \u2264 |||\u2206\u0302\u0398|||F \u03b4 + 2\u03bbd|||\u2206\u0302\u0398A|||N + 2\u03bbd\nd\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6)\n\u2264 |||\u2206\u0302\u0398|||F \u03b4 + 2\u03bbd \u221a 2r|||\u2206\u0302\u0398|||F + 2\u03bbd d\u2211\nj=r+1\n\u03c3j(\u0398 \u22c6)\nwhere the second inequality follows since |||\u2206\u0302\u0398A|||N \u2264 \u221a 2r|||\u2206\u0302\u0398A|||F \u2264 \u221a 2r|||\u2206\u0302\u0398|||F. We have thus obtained a quadratic inequality in |||\u2206\u0302\u0398|||F, and applying the quadratic formula yields the claim."}], "references": [{"title": "An Introduction to Multivariate Statistical Analysis. Wiley Series in Probability and Mathematical Statistics", "author": ["T.W. Anderson"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Covariance regularization by thresholding", "author": ["P.J. Bickel", "E. Levina"], "venue": "Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Zero-shot domain adaptation: A multiview approach", "author": ["J. Blitzer", "D.P. Foster", "Sham M. Kakade"], "venue": "Technical report, Toyota Technological Institute at Chicago,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. Mcdonald", "F. Pereira"], "venue": "In EMNLP Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Robust Principal Component Analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Technical report, Stanford,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Latent variable graphical model selection via convex optimization", "author": ["V. Chandrasekaran", "P.A. Parillo", "A.S. Willsky"], "venue": "Technical report, Massachusetts Institute of Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "Technical report,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Local operator theory, random matrices, and Banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "In Handbook of Banach Spaces,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Gaussian averages of interpolated bodies and applications to approximate reconstruction", "author": ["Y. Gordon", "A.E. Litvak", "S. Mendelson", "A. Pajor"], "venue": "Journal of Approximation Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Has\u2019minskii. A lower bound on the risks of nonparametric estimates of densities in the uniform metric", "author": ["Z. R"], "venue": "Theory Prob. Appl.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1978}, {"title": "Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "Technical report, Univ. Pennsylvania,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "On the distribution of the largest eigenvalue in principal components analysis", "author": ["I.M. Johnstone"], "venue": "Annals of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "The Concentration of Measure Phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs. American Mathematical Society,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Lectures on discrete geometry", "author": ["J. Matousek"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Two Proposals for Robust PCA using Semidefinite Programming", "author": ["M. McCoy", "J. Tropp"], "venue": "Technical report, California Institute of Technology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A unified framework for highdimensional analysis of M-estimators with decomposable regularizers", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "In NIPS Conference,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Restricted strong convexity and (weighted) matrix completion: Optimal bounds with noise", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "Technical report, UC Berkeley,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "Technical Report 76, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain (UCL),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Minimax rates of estimation for high-dimensional linear regression over lq-balls", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Minimax-optimal rates for sparse additive models over kernel classes via convex programming", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P. Parrilo"], "venue": "SIAM Review,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1970}, {"title": "Estimation of high-dimensional low-rank matrices", "author": ["A. Rohde", "A. Tsybakov"], "venue": "Technical Report arXiv:0912.5338v2, Universite de Paris,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Empirical Processes in M-Estimation", "author": ["S. van de Geer"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Robust PCA via Outlier Pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "Technical report, University of Texas,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Information-theoretic determination of minimax rates of convergence", "author": ["Y. Yang", "A. Barron"], "venue": "Annals of Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "In Festschrift for Lucien Le Cam, pages 423\u2013435", "author": ["B. Yu. Assouad", "Fano", "Le Cam"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1997}, {"title": "Dimension reduction and coefficient estimation in multivariate linear regression", "author": ["M. Yuan", "A. Ekici", "Z. Lu", "R. Monteiro"], "venue": "Journal Of The Royal Statistical Society Series B,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}], "referenceMentions": [{"referenceID": 8, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 105, "endOffset": 114}, {"referenceID": 7, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 105, "endOffset": 114}, {"referenceID": 6, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 105, "endOffset": 114}, {"referenceID": 17, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 140, "endOffset": 148}, {"referenceID": 28, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 140, "endOffset": 148}, {"referenceID": 8, "context": "Different forms of robust PCA can be formulated in terms of matrix decomposition using the matrix \u0393\u22c6 to model the gross errors [9, 7, 29].", "startOffset": 127, "endOffset": 137}, {"referenceID": 6, "context": "Different forms of robust PCA can be formulated in terms of matrix decomposition using the matrix \u0393\u22c6 to model the gross errors [9, 7, 29].", "startOffset": 127, "endOffset": 137}, {"referenceID": 28, "context": "Different forms of robust PCA can be formulated in terms of matrix decomposition using the matrix \u0393\u22c6 to model the gross errors [9, 7, 29].", "startOffset": 127, "endOffset": 137}, {"referenceID": 7, "context": "The problem of low rank plus sparse matrix decomposition also arises in Gaussian covariance selection with hidden variables [8], in which case the inverse covariance of the observed vector can be decomposed as the sum of a sparse matrix with a low rank matrix.", "startOffset": 124, "endOffset": 127}, {"referenceID": 31, "context": "Matrix decompositions also arise in multi-task regression [32, 21, 27], which involve solving a collection of regression problems, referred to as tasks, over a common set of features.", "startOffset": 58, "endOffset": 70}, {"referenceID": 20, "context": "Matrix decompositions also arise in multi-task regression [32, 21, 27], which involve solving a collection of regression problems, referred to as tasks, over a common set of features.", "startOffset": 58, "endOffset": 70}, {"referenceID": 26, "context": "Matrix decompositions also arise in multi-task regression [32, 21, 27], which involve solving a collection of regression problems, referred to as tasks, over a common set of features.", "startOffset": 58, "endOffset": 70}, {"referenceID": 4, "context": "For some features, one expects their weighting to be preserved across features, which can be modeled by a low-rank constraint, whereas other features are expected to vary across tasks, which can be modeled by a sparse component [5, 2].", "startOffset": 228, "endOffset": 234}, {"referenceID": 1, "context": "For some features, one expects their weighting to be preserved across features, which can be modeled by a low-rank constraint, whereas other features are expected to vary across tasks, which can be modeled by a sparse component [5, 2].", "startOffset": 228, "endOffset": 234}, {"referenceID": 8, "context": "[9] studied the case when \u0393\u22c6 is assumed to sparse, with a relatively small number s \u226a d1d2 of non-zero entries.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] analyzed the same model but under an assumption of random sparsity, meaning that the non-zero positions are chosen uniformly at random.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[29] have analyzed a different model, in which the matrix \u0393\u22c6 is assumed to be columnwise sparse, with a relatively small number s \u226a d2 of non-zero columns.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14], who derived Frobenius norm error bounds for the case of exact elementwise sparsity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], includes the elementwise l1-norm and columnwise (2, 1)-norm as special cases, as well as various other regularizers used in practice.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "An interesting feature of our analysis is that, in contrast to previous work [9, 29, 7], we do not impose incoherence conditions on the singular vectors of \u0398\u22c6; rather, we control the interaction with a milder condition involving the dual norm of the regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 28, "context": "An interesting feature of our analysis is that, in contrast to previous work [9, 29, 7], we do not impose incoherence conditions on the singular vectors of \u0398\u22c6; rather, we control the interaction with a milder condition involving the dual norm of the regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 6, "context": "An interesting feature of our analysis is that, in contrast to previous work [9, 29, 7], we do not impose incoherence conditions on the singular vectors of \u0398\u22c6; rather, we control the interaction with a milder condition involving the dual norm of the regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 19, "context": "In the special case of elementwise sparsity, this dual norm enforces an upper bound on the \u201cspikiness\u201d of the low-rank component, and has proven useful in the related setting of noisy matrix completion [20].", "startOffset": 202, "endOffset": 206}, {"referenceID": 24, "context": "[25] and references therein), with a norm-based regularizer R : Rd1\u00d7d2 \u2192 R+ used to constrain the structure of \u0393\u22c6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "We provide a general theorem applicable to a class of regularizers R that satisfy a certain decomposability property [19], and then consider in detail a few particular choices of R that have been studied in past work, including the elementwise l1-norm, and the columnwise (2, 1)-norm (see Examples 4 and 5 below).", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "In particular, if the let the observation matrix Y \u2208 Rd\u00d7d be the sample covariance matrix 1 n \u2211n i\u22121 ZiZ T i , then some algebra shows that Y = \u0398 \u22c6 + \u0393\u22c6 + W , where \u0398\u22c6 = LLT is of rank r, and the random matrix W is a re-centered form of Wishart noise [1]\u2014in particular, the zero-mean matrix", "startOffset": 251, "endOffset": 254}, {"referenceID": 1, "context": ", that is, the vectors \u03b2\u2217 j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27].", "startOffset": 87, "endOffset": 102}, {"referenceID": 31, "context": ", that is, the vectors \u03b2\u2217 j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27].", "startOffset": 87, "endOffset": 102}, {"referenceID": 20, "context": ", that is, the vectors \u03b2\u2217 j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27].", "startOffset": 87, "endOffset": 102}, {"referenceID": 26, "context": ", that is, the vectors \u03b2\u2217 j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27].", "startOffset": 87, "endOffset": 102}, {"referenceID": 1, "context": "However, many multi-task learning problems exhibit more complicated structure, in which some subset of features are shared across tasks, and some other subset of features vary substantially across tasks [2, 4].", "startOffset": 203, "endOffset": 209}, {"referenceID": 3, "context": "However, many multi-task learning problems exhibit more complicated structure, in which some subset of features are shared across tasks, and some other subset of features vary substantially across tasks [2, 4].", "startOffset": 203, "endOffset": 209}, {"referenceID": 8, "context": "Indeed, as has been discussed in past work [9, 7, 29], no method can recover the components (\u0398\u22c6,\u0393\u22c6) unless the low-rank component is \u201cincoherent\u201d with the matrix \u0393\u22c6.", "startOffset": 43, "endOffset": 53}, {"referenceID": 6, "context": "Indeed, as has been discussed in past work [9, 7, 29], no method can recover the components (\u0398\u22c6,\u0393\u22c6) unless the low-rank component is \u201cincoherent\u201d with the matrix \u0393\u22c6.", "startOffset": 43, "endOffset": 53}, {"referenceID": 28, "context": "Indeed, as has been discussed in past work [9, 7, 29], no method can recover the components (\u0398\u22c6,\u0393\u22c6) unless the low-rank component is \u201cincoherent\u201d with the matrix \u0393\u22c6.", "startOffset": 43, "endOffset": 53}, {"referenceID": 8, "context": "Past work on both matrix completion and decomposition [9, 7, 29] has ruled out these types of troublesome cases via conditions on the singular vectors of the low-rank component \u0398\u22c6, and used them to derive sufficient conditions for exact recovery in the noiseless setting (see the discussion following Example 4 for more details).", "startOffset": 54, "endOffset": 64}, {"referenceID": 6, "context": "Past work on both matrix completion and decomposition [9, 7, 29] has ruled out these types of troublesome cases via conditions on the singular vectors of the low-rank component \u0398\u22c6, and used them to derive sufficient conditions for exact recovery in the noiseless setting (see the discussion following Example 4 for more details).", "startOffset": 54, "endOffset": 64}, {"referenceID": 28, "context": "Past work on both matrix completion and decomposition [9, 7, 29] has ruled out these types of troublesome cases via conditions on the singular vectors of the low-rank component \u0398\u22c6, and used them to derive sufficient conditions for exact recovery in the noiseless setting (see the discussion following Example 4 for more details).", "startOffset": 54, "endOffset": 64}, {"referenceID": 19, "context": "In this paper, we impose a related but milder condition, previously introduced in our past work on matrix completion [20], with the goal of performing approximate recovery.", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "For instance, see the paper [23] for discussion of non-identifiability in high-dimensional sparse regression.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "Motivating applications include the problem of factor analysis with a non-identity but sparse noise covariance, as discussed in Example 1, as well as certain formulations of robust PCA [7], and model selection in Gauss-Markov random fields with hidden variables [8].", "startOffset": 185, "endOffset": 188}, {"referenceID": 7, "context": "Motivating applications include the problem of factor analysis with a non-identity but sparse noise covariance, as discussed in Example 1, as well as certain formulations of robust PCA [7], and model selection in Gauss-Markov random fields with hidden variables [8].", "startOffset": 262, "endOffset": 265}, {"referenceID": 19, "context": "Indeed, this type of spikiness control has proven useful in analysis of nuclear norm relaxations for noisy matrix completion [20].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "2 in the paper [20] for one example).", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Our first result applies to the family of convex programs (7) whenever R belongs to the class of decomposable regularizers, and the least-squares loss associated with the observation model satisfies a specific form of restricted strong convexity [19].", "startOffset": 246, "endOffset": 250}, {"referenceID": 18, "context": "[19], a large class of norms are decomposable with respect to interesting2 subspace pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "2 Restricted strong convexity Given a loss function, the general notion of strong convexity involves establishing a quadratic lower bound on the error in the first-order Taylor approximation [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 25, "context": "[26])", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This guarantee is weaker than the exact recovery results obtained in past work on the noiseless observation model with identity operator [9, 7]; however, these papers imposed incoherence requirements on the singular vectors of the low-rank component \u0398\u22c6 that are more restrictive than the conditions of Theorem 1.", "startOffset": 137, "endOffset": 143}, {"referenceID": 6, "context": "This guarantee is weaker than the exact recovery results obtained in past work on the noiseless observation model with identity operator [9, 7]; however, these papers imposed incoherence requirements on the singular vectors of the low-rank component \u0398\u22c6 that are more restrictive than the conditions of Theorem 1.", "startOffset": 137, "endOffset": 143}, {"referenceID": 8, "context": "We note that the singular vector incoherence conditions, as imposed in past work [9, 7, 14] and used to guarantee exact recovery, would exclude the matrix (33), since its left singular vector is the unit vector e1 \u2208 Rd1 .", "startOffset": 81, "endOffset": 91}, {"referenceID": 6, "context": "We note that the singular vector incoherence conditions, as imposed in past work [9, 7, 14] and used to guarantee exact recovery, would exclude the matrix (33), since its left singular vector is the unit vector e1 \u2208 Rd1 .", "startOffset": 81, "endOffset": 91}, {"referenceID": 13, "context": "We note that the singular vector incoherence conditions, as imposed in past work [9, 7, 14] and used to guarantee exact recovery, would exclude the matrix (33), since its left singular vector is the unit vector e1 \u2208 Rd1 .", "startOffset": 81, "endOffset": 91}, {"referenceID": 14, "context": ", see Johnstone [15]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "[14] This recent work focuses on the problem of matrix decomposition with the \u2016 \u00b7 \u20161-norm, and provides results both for the noiseless and noisy setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29], since their results do not guarantee exact recovery of the pair (\u0398\u22c6,\u0393\u22c6).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "We have implemented the M -estimators based on the convex programs (10) and (14), in particular by adapting first-order optimization methods due to Nesterov [22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "Since |||XX|||op \u2264 \u03c32 max, known results on the singular values of Gaussian random matrices [10] imply that", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "where the final bound holds with probability greater than 1 \u2212 2 exp(\u2212c1d), using standard tail bounds on Gaussian random matrices [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 15, "context": "Noting that the function Wk 7\u2192 \u2016Wk\u20162 is Lipschitz, by concentration of measure for Gaussian Lipschitz functions [16], we have", "startOffset": 112, "endOffset": 116}, {"referenceID": 9, "context": "Consequently, by known results on singular values of Wishart matrices [10], we have |||W |||op \u2264 \u221a 8|||\u0398|||op \u221a r n with high probability, showing that the specified choice of \u03bbd is valid.", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "By known matrix norm bounds [13], we have \u2016W\u20162,\u221e \u2264 |||W |||op, so that the claim follows by the previous argument.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "7 Proof of Theorem 2 Our lower bound proofs are based on a standard reduction [12, 31, 30] from estimation to a multiway hypothesis testing problem over a packing set of matrix pairs.", "startOffset": 78, "endOffset": 90}, {"referenceID": 30, "context": "7 Proof of Theorem 2 Our lower bound proofs are based on a standard reduction [12, 31, 30] from estimation to a multiway hypothesis testing problem over a packing set of matrix pairs.", "startOffset": 78, "endOffset": 90}, {"referenceID": 29, "context": "7 Proof of Theorem 2 Our lower bound proofs are based on a standard reduction [12, 31, 30] from estimation to a multiway hypothesis testing problem over a packing set of matrix pairs.", "startOffset": 78, "endOffset": 90}, {"referenceID": 19, "context": "For the low rank component, we re-state a slightly modified form (adapted to the setting of non-square matrices) of Lemma 2 from the paper [20]: Lemma 5.", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "As for the sparse matrices, the following result is a modification, so as to apply to the matrix setting of interest here, of Lemma 5 from the paper [23]:", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "[24] on minimax rates for kernel classes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "By known results on metric entropy of Euclidean balls [17], this function class has logarithmic metric entropy, so that part (b) of the above lemma applies, and yields the stated result.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "This problem is very closely related to matrix completion, a problem for which recent work by Negahban and Wainwright [20] shows that a form of restricted strong convexity holds with high probability.", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "[25], so that it remains to prove part (b).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "By assumption, the regularizer R is decomposable, and as shown in the paper [19], the nuclear norm is also decomposable.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "[11] with (q0, q1) = (1, 2), n = d1d2 and t = \u221a s, thereby obtaining", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] with (q0, q1) = (1, 2), n = d1d2 yields", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": ", [28]) can be used to extend this bound to a uniform one over the choice of radii t, so that it applies to the random one t = \u2016\u2206\u0302 \u20161 |||\u2206\u0302\u0393|||F of interest.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "The function W 7\u2192 Z\u0303(s) is a Lipschitz function with parameter \u03bd \u221a d1d2 , so that by concentration of measure for Gaussian Lipschitz functions [16], it satisfies the upper tail bound", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "Now the variable Vk is zero-mean, and sub-Gaussian with parameter \u03bd \u221a d1d2 , again using concentration of measure for Lipschitz functions of Gaussians [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 10, "context": "[11] with (q0, q1) = (1, 2), n = d2 and t = 4 \u221a s then yields", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "From Lemma 1 in the paper [21], there exists a decomposition \u2206\u0302\u0398 = \u2206\u0302A + \u2206\u0302 \u0398 B such that the rank of \u2206\u0302A upper-bounded by 2 r and |||\u0398|||N \u2212 |||\u0398 + \u2206\u0302A + \u2206\u0302B|||N \u2264 2 d \u2211", "startOffset": 26, "endOffset": 30}], "year": 2012, "abstractText": "We analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are noisy realizations of a linear transformation X of the sum of an (approximately) low rank matrix \u0398\u22c6 with a second matrix \u0393\u22c6 endowed with a complementary form of low-dimensional structure; this set-up includes many statistical models of interest, including forms of factor analysis, multi-task regression with shared structure, and robust covariance estimation. We derive a general theorem that gives upper bounds on the Frobenius norm error for an estimate of the pair (\u0398\u22c6,\u0393\u22c6) obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. Our results are based on imposing a \u201cspikiness\u201d condition that is related to but milder than singular vector incoherence. We specialize our general result to two cases that have been studied in past work: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields non-asymptotic Frobenius error bounds for both deterministic and stochastic noise matrices, and applies to matrices \u0398\u22c6 that can be exactly or approximately low rank, and matrices \u0393\u22c6 that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices and the identity observation operator, we establish matching lower bounds on the minimax error, showing that our results cannot be improved beyond constant factors. The sharpness of our theoretical predictions is confirmed by numerical simulations.", "creator": "LaTeX with hyperref package"}}}