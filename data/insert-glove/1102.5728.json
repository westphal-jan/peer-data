{"id": "1102.5728", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2011", "title": "Named Entity Recognition Using Web Document Corpus", "abstract": "This phthiotis paper introduces 20-part a named exclamation entity louis-nicolas recognition approach in textual 91-11 corpus. This steinback Named h8 Entity (NE) can 1276 be a zippered named: supply-chain location, oksa person, 973 organization, date, time, etc. , characterized by instances. commissary A bme NE faryal is ripston found in rasinski texts ph\u00f2ng accompanied physiatrists by nytt contexts: dobrynin words repainting that 72-66 are polities left or odein right 121.0 of fetishization the NE. farallones The 99.31 work mainly enforcment aims arom at mvnos identifying contexts elfyn inducing the pwa NE ' massouma s ladinos nature. pritchardia As such, 4010 The scheffer occurrence kovals of 732-390-4480 the chelmsley word \" debt-to-gdp President \" in a norwegians text, means thoroughness that antithetical this word 450-million or context may be myre followed ssebaana by creadon the panellinios name 32.77 of bottoms a president decoded as President \" delasalle Obama \". Likewise, a charkhi word fierce preceded by the string \" divin footballer \" 89,500 induces alaams that this kostolac is the sediment name of a emission footballer. NE recognition mosley may be mawasi viewed shafaq as cross-town a 60.28 classification method, koznick where weequahic every six-digit word is shorin-ryu assigned to cui a NE class, regarding the pareidolia context. fiddes The aim of 113.98 this 53-52 study is then gemany to shaweesh identify beechmont and incautious classify sprach the contexts that tewahedo are tbi most relevant shanku to 22.97 recognize lahn-dill-kreis a namgyel NE, santolan those which are day-day frequently found wmc-tv with gibbons the NE. moby A mahlathini learning approach using training corpus: web documents, lihks constructed from learning habibi examples tv-channel is leito then hanina suggested. troicki Frequency doiron representations and modified 4,930 tf - idf givner representations are 71.14 used to calculate 50-5 the mifsud context xueling weights runnion associated 1097 to claribel context frequency, bhattathiri learning jackpots example sieved frequency, frisks and englands document preminger frequency troshev in the corpus.", "histories": [["v1", "Mon, 28 Feb 2011 18:33:09 GMT  (136kb)", "http://arxiv.org/abs/1102.5728v1", "11 pages 4 figures, 2 tables"]], "COMMENTS": "11 pages 4 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["wahiba ben abdessalem karaa"], "accepted": false, "id": "1102.5728"}, "pdf": {"name": "1102.5728.pdf", "metadata": {"source": "CRF", "title": "NAMED ENTITY RECOGNITION USING WEB DOCUMENT CORPUS", "authors": [], "emails": ["wahiba.abdessalem@isg.rnu.tn"], "sections": [{"heading": null, "text": "1\nKeywords: Named entity; Learning; Information extraction; tf-idf; Web document."}, {"heading": "1. Introduction", "text": "Named Entity Recognition is complex in various areas of automatic Natural Language Processing of (NLP), document indexing, document annotation, translation, etc. [2]. It is a fundamental step in various Information Extraction (IE) tasks. It has been an essential task in several research teams such as the Message Understanding Conferences (MUC), the Conferences on Natural Language Learning (CoNLL), etc.\nNamed entities (NE) are phrases containing the names of persons, locations, etc. They are particularly important for the access to document content, since they form the building blocks upon which the analysis of documents is based.\nThis paper discusses the use of learning approach for the problem of NE recognition. The goal is to reveal contextual NE in a document corpus. A context considers words surrounding the NE in the sentence in which it appears, it is a sequence of words, that are left or right of the NE. We use, in this work, the makings of learning technologies, combined with statistical models [17], to extract contexts from Web document corpus, to identify the most pertinent contexts for the recognition of a NE. We investigate the impact of using different feature weighting measures, in the hope that they will yield more context classification.\nThe remaining of the paper is organized as follows: section 2, introduces the state of the art of methods applied in Named Entity Recognition. Section 3 describes the methodology and section 4 gives test results of our approach. Section 5 is devoted the work\u2019s conclusion."}, {"heading": "2. State of the art", "text": "Named entity recognition can be used to perform numerous processing tasks in various areas: Information extraction systems [6], text mining [8], [16], Automatic Speech Recognition (ASR) [5], etc.\nSeveral works are particularly interested in the recognition of named entities. Mikheev et al. [12] have built a system for recognizing named entities, which combines a model based on grammar rules, and statistical models, without resorting to named entity lists.\nCollins et al. [1] suggests an algorithm for named entity classification, based on the meaning word disambiguation, and exploits the redundancy in the contextual characteristics.\nThis system operates a large corpus to produce a generic list of proper nouns. The names are collected by searching for a syntax diagram with specific properties. For example, a proper name is a sequence of consecutive words in a nominal phrase, etc.\nPetasis et al. [14] presented a method that helps to build a rules-based system for recognition and classification of named entities. They have used machine learning, to monitor system performance and avoid manual marking.\nIn his paper [10], Mann explores the idea of fine-grained proper noun ontology and its use in question answering. The ontology is built from unrestricted text using simple textual co-occurrence patterns. This ontology is therefore used on a question answering task to provide primary results on the utility of this information. However, this method has a low coverage.\nThe Nemesis system presented by Fourour [6] is founded on some heuristics, allowing the identification of named entities, and their classification by detecting the boundaries of the entity called \"context\" to the left or right, and by studying syntactic, or morphological of these entities. For example, acronyms are named entities consisting of a single lexical unit comprising several capital letters, etc.\nKrstev et al [9] suggested a basic structure of a relational model of a multilingual dictionary of proper names based on four-level ontology. However, the implementation is not yet completed, it is only expected.\nThe KNOWITALL system planned by Etzioni et al [4] aims at automating the process of extracting named entities from the Web in an unsupervised and scalable manner. This system is not intended for recognizing a named entity, but used to create long lists of named entities. However, it is not designed to resolve the ambiguity in some documents.\nFriburger [7] recommends a method based on rules for finding a large proportion of person names. However, this method has some limitations as errors, and missing responses.\nNadeau et al. [13] have suggested a system for recognizing named entities. Their work is based on those of Collins et al. [1], and Etzioni et al. [4]. The system exploits human-generated HTML markup in Web pages to generate gazetteers, then it uses simple heuristics for the entity disambiguation in the context of a given document.\nMarthineau et al. [11] showed the validity of existing local grammars in the system GRAALWEB to recognize and extract the named entities.\nIn view of works touching the recognition of named entities, we perceive that most of them are based on a set of rules in relation to predefined categories: morphological, grammatical, etc. [7], [11], [14], or on predefined lists of dictionaries [9]. The ontology domain is still in exploration [10].\nWe adopted the idea of Nemesis based on the left and the right context of the named entity. However, our approach does not mark the context derived from syntactic or morphological rules, but identifies the context founded on learning phase. The objective is thus to carry out a system, able to induce the nature of a named entity, following the meeting of certain indicators, and this, in any language, without requiring to dictionaries or lists of named entities.\nFor our approach we use, for the learning phase, documents resulting from the Web. We use context frequency representations, based on tf-idf version, to calculate context weights. These weights are used to determine the most pertinent contexts, to identify the named entity nature."}, {"heading": "3. Methodology", "text": "The system encloses three main phases. The first is the training corpus collection. The second is the context extraction and the contexts classification, according to weighting measures. The third phase is the weighting measures exploitation, in order to build a model for NE recognition. The figure (Fig 1) summarizes the architecture.\n3.1. Training corpus construction\nThe first step is to build an initial corpus containing web documents. This corpus is called learning corpus. Our algorithm proceeds as follows:\n(i) We Furnish a set of learning examples, which are instances of a named entity class. For example for the class entity \"disease\", we provide as learning examples, names of diseases such as \"flu\", \"Eczema\", \"measles\", etc. For the named entity class \"football player\" must be provided as instances, footballer names such as \"platini\", \"Zidane\", \"Maradona\", etc.\n(ii) For all the instances a query is sent to a Web search engine (Yahoo, Google, AltaVista, etc.), by using a specific API related to each engine, searching for context string surrounding the instances: words before, or/and after the instance, the length of the string is an input parameter.\n(iii) The result is a list of links. Afterwards, for each link, a new request is released in order to retrieve and save the document. To increase the number of returned links, the user should provide a maximum of learning examples. The figure (Fig 2) is an extract of the API used with yahoo engine:\n3.2. Learning phase\nA context is a set of words preceding, or following a named entity. We focus, in this study, on the words that precede a named entity instance; we consider that the left context is more relevant then the right one.\nFor instance, we suggest the concept \"President\" with the six instances, considered as learning examples: \"Bush\", \"G W. Bush\", \"Nicolas Sarkozy,\" Sarkozy \",\" Chirac \", and \"Jacques Chirac\". The result of the extraction phase is a set of context candidates. The figure (Fig. 3.) is an extract of a document where the context candidates are pointed:\nThere are eight instances with different contexts. We can notice that the context \"President\" is most common with the learning examples, which makes it the more appropriate to recognize the name of a president. However, the context occurrence number is an insufficient parameter to decide that it is the best context to construct a rule:\nPresident <president-name>.\nThe goal is to find high-quality context. To define the context's quality, we observe some metrics calculated through frequencies, given that instances found on the Web surrounded by the context. Estimating pertinence of contexts is not sufficient by only the Frequency of these contexts found surrounding a NE instance, especially when we have no labelled negative learning examples, but only positive ones. The purpose of this work is to identify the most pertinent contexts for the identification of a given named entity. For this intention we calculate weights to classify contexts taking into account the context frequency regarding learning examples, the inverse context frequency, the learning example frequency in the document corpus, and the frequency of documents containing contexts in the document corpus. To calculate context weights, we use context frequency representations based on tf-idf version. tfidf (Term Frequency-Inverse Document Frequency) is one of the most classics and most common weighting method used to describe documents in the Vector Space Model [17], particularly in Information Retrieval (IR). The tf considers the term frequency in the document: the more a word occurs in a document, the more it is expected to be significant in this document. In addition, idf inverse document frequency measures the term frequency in the corpus: the more a word appears in a corpus, the more it is estimated irrelevant for the document.\nThe Term Trequency (tf ) of a term ti for a document dj is calculated as follows:\n\u2211 =\nk kj\nij ij frequency\nfrequency tf\n(1)\nWhere frequency kj is the occurrence number of the term ti in dj. The denominator is the number of occurrences of all terms in the document dj.\nThe idf, Inverse Document Frequency component is computed as follows:\ni i n\nN ogidf l=\n(2)\nWhere N is the total number of documents in the corpus, and ni is the number of documents in which the term ti emerges. In tf.idf weighting is:\niijij idftfw \u00d7=\n(3)\nMany alternatives have been suggested to the basic tfidf formula, where the tf or idf part is modified using functions related to characteristic selection. It is in our case to measure the importance of a context ci to a NE; in our work we propose a modified formula. We define:\nDefinition 1: context frequency The context frequency (cfi ) of a context ci in a document corpus is calculated as follows:\ni\ni i nc nc cf \u2211 = (4)\nWe consider the variable nci the occurrence number of a context within a document corpus accompanied by a\nNE Learning Example (LE), and \u2211 inc is the occurrence number of all contexts in the document corpus.\nDefinition 2: learning example frequency\nNLE\nnle lef ii=\n(5)\nWe designated nlei the number of LE located with the context ci, in the corpus, and NLE the total number of learning examples used for training.\nDefinition 3: document frequency We used the variable ndi to assign the occurrence number of documents in the corpus containing a context ci, and coming from different sources. Di is the occurrence number of documents in the corpus containing a context ci.\nThe document frequency dfi is calculated as follows:\ni\ni i D\nnd df =\n(6)\nDefinition 4: inverse context frequency We considered the hypothesis that the context is significant for a NE, if it does not often appears with other phrases in the corpus. icf inverse context frequency, measures the context frequency associated to all the phrases escorted by the context: the more a context appears in a corpus accompanied with other phrases, the more it is estimated irrelevant for the NE.\nicfi is calculated using nci, the occurrence number of a context within a document corpus accompanied by learning example, and the variable Ci, which represents the total occurrence number of the context accompanied with other phrases in the corpus.\ni\ni i C\nnc icf =\n(7)\nDefinition 5: context weight The product of the obtained frequencies provides the weight wi adapted to the detection of the context\npertinence:\niiiii icfdflefcfw \u00d7\u00d7\u00d7=\n(8)\n3.3. Recognition model\nNE recognition may be viewed as a classification problem, where every word is assigned to a NE class, regarding the context. Since decision tree algorithms are widely used for data mining [3], classification, etc. in this third phase, we design algorithm recognition, in the form of a decision tree, based on C4.5 algorithm [15].\nIn many cases, the same context can introduce different NE. For example, the context \"Mr.\" precedes both a footballer name and a president name: Mr. Zidane, Mr. Obama. The intention is to decide whether the NE is a president name or a footballer name.\nWe define votei for each NE with each contexti in the document. The value of votei is incremented with the weight of the context, each time a contexti is encountered:\niii w vote vote += (9) Afterward, the algorithm decides the nature of the NE, once the value of votei reaches a threshold, and\ndepending on the value of votej, \u2026, voten reached by the other contexts."}, {"heading": "4. Experimental results", "text": "We conducted several tests with different NE. For example for the NE class \"capital\", we extracted a corpus of documents from 65 URLs, obtained by queries on the search engine Yahoo. For the 13 following instances (or learning examples): \"Paris\", \"Tunis\", \"Cairo\", \"Athens\", \"Abuja\", \"Berlin\", \"Bucharest\", \"Budapest\", \"Brasilia\", \"Freetown\", \"Dublin\", \"Vienna\", and \"Doha\". We obtained 2398 contexts composed of two different words to the left. The learning examples are occurred 4264 times in the corpus with the contexts. We present in the following table an extract of the obtained context classification results:\nContext cf df lef icf W\nHotels in 0,0039869 0,4444445 0,5384616 8,5 0,008110106\nMap of 0,0028143 0,6 0,5384616 6 0,005455413\ntravel to 0,000469 1 0,1538462 2 0,000144308\nGuide to 0,000469 1 0,1538462 2 0,000144308\nPlaces in 0,0011726 0,75 0,3076923 1 0,0002706\nhotels in 0,0021107 0,5714286 0,6153847 3 0,002226673\nmap of 0,0007036 1 0,2307692 1 0,000162369\nHospital in 0,0007036 1 0,0769231 0,4 2,16E-05\nwhen in 0,000469 1 0,1538462 0,4 2,89E-05\nFig. 4. Context weight classification\nIn the table (Table 1), we note that the context \"Hotel in\" is found 17 times, in the corpus with the learning examples. The context frequency is:\n0,0039869 4264\n17 ==inHotelcf (10)\n0\n0,001\n0,002\n0,003\n0,004\n0,005\n0,006\n0,007\n0,008\n0,009\nfrequence\nW\nweight\nHotels in Map of travel to Guide to Places in hotels in map of Hospital in when in\nThe context is found in 9 documents, but among these documents, only 4 documents come from different sources. The document frequency for this context is:\n0,4444445 9\n4 ==inHoteldf (11)\nThe context \"Hotel in\" occurs 17 times in total, but only with 7 learning examples, the learning example frequency is then:\n0,5384616 13\n7 ==inHotellef (12)\nThe context occurs 17 times with learning examples, but also 2 times with other phrases, the inverse context frequency is:\n5,8 2\n17 ==inHotelicf (13)\nThe weight of this context is then:\n008110106.0=\u00d7\u00d7\u00d7= inHotelinHotelinHotelinHotelinHotel icflefdfcfW (14)\nThe context \"Hotel in\" has the highest weight, which makes it more relevant for the recognition of a capital name, compared to the other contexts.\nWe studied in addition, if the context number may change if we increase the size of the analyzed corpus (the document number). In the table (Table 2), we present some statistics associated to the ratio between the contexts number, and the corpus size. These numbers are found for the NE class \"President\", using 89 representations of president names as learning examples:\n\"Jacques Chirac\", \"JACQUES CHIRAC\", \"Chirac\", \"Chirac\", \"CHIRAC\", \"Jacques CHIRAC\"; \"Bush\", \"George Walker Bush\", \"George Bush\", \"GEORGE BUSH\", \"GEORGE W BUSH\", \"GEORGE W. BUSH\", \"George W Bush\", \"BUSH\", \"George W. BUSH\", \"Geroge BUSH\", \"Georges W.\", \"Georges W\", \"Nicolas Sarkozy\", \"NICOLAS SARKOZY\", \"Sarkozy\", \"Sarko\", \"sarkozy\", \"Nicolas sarkozy\", \"Nicolas SARKOZY\", \"SARKOZY\", \"Jalal Talabani\", \"Talabani\", \"TALABANI\", \"Jalal TALABANI\", \"JALAL TALABANI\", \"Zine El Abidine Ben Ali\", \"Zine el Abidine\", etc.\nWe have noticed that the context number becomes stable from a significant size corpus (420 records = 7 MB of text data). Recall and precision are usually admitted approachs of measuring system performance in this field. We suppose that Recall is the number of correct NE found by the system over total number of correct NE in the document corpus. Precision is the number of correct NE found by the system over total number of NE found in the corpus. We obtained the following value: for Precision 80,9 % and recall 69,8 %."}, {"heading": "5. Conclusion", "text": "In this paper, the use of learning approach is suggested for the problem of NE recognition. The goal is to uncover in a document corpus, NE that occurs frequently accompanied by contexts: sequence of words, that are left or right of the NE. Contexts that occur with given learning examples were first extracted from Web documents corpus. Different feature weighting measures were examined to classify the contexts in order to identify the most pertinent contexts for the recognition of a NE. This classification enables to derive a model for the recognition of a NE.\nThe same strategy can be applied to person names, company names, and many other types of named entities in any language. Although, we should mention that we successfully applied this technique to several namedentity types, in French and in English language.\nOne of the future works which we recommend is to discern and to measure similarity between contexts. We can use this measurement to cluster similar contexts.\nSince we have primarily applied our approach to the named entities problem, we can also attempt additional concepts. In effect, the context usually contains enough information to identify the instance as a concept member."}], "references": [{"title": "Unsupervised models for named entity classification", "author": ["M. Collins", "Y. Singer"], "venue": "Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Reconnaissance automatique des noms propres de la langue \u00e9crite: les r\u00e9centes TAL", "author": ["B. Daille", "E. Morin"], "venue": "Traitement automatique des langues", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning from positive and unlabeled examples", "author": ["F. Denis", "R. Gilleron", "F. Letouzey"], "venue": "Elsevier. Theoretical Computer Science", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "D", "author": ["O. Etzioni", "M. Cafarella", "D. Downey", "S. Kok", "A. Popescu", "T. Shaked", "S. Soderland"], "venue": "Weld, and A.Yates, Unsupervised named-entity extraction from the web: An experimental study, Artificial Intelligence", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Noc\u00e9ra, Robust Named Entity Extraction from Spoken Archives", "author": ["B. Favre", "F. B\u00e9chet"], "venue": "Proceedings of HLT-", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Apport du Web dans la reconnaissance des entit\u00e9s nomm\u00e9es", "author": ["N. Fourour", "E.Morin"], "venue": "Revue que\u0301be\u0301coise de linguistique,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Linguistique et reconnaissance automatique des noms propres", "author": ["N. Friburger"], "venue": "Meta : journal des traducteurs", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Fouille du Web pour la collecte d\u2019entit\u00e9s nomm\u00e9es", "author": ["C. Jacquemin", "C. Bush"], "venue": "Proceedings 8\u00e8me Conf\u00e9rence Nationale sur le Traitement Automatique des Langues Naturelles ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Multilingual ontology of proper name", "author": ["C. Krstev", "D. Vitas", "D. Maurel", "M. Tran"], "venue": "Proceedings of the Language and Technology Conference, pp. 116\u2013119, Poznan, Poland", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Fine-grained proper noun anthologies for question answering", "author": ["G.S. Mann"], "venue": "International Conference on Computational Linguistics, COLING-02 on SEMANET: building and using semantic networks", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Les Entit\u00e9s Nomm\u00e9es : usage et degr\u00e9s de pr\u00e9cision et de d\u00e9sambigu\u00efsation", "author": ["C.Martineau", "E. Tolone", "S. Voyatzi"], "venue": "In proceedings of 26th conference on Lexis and Grammar,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Named Entity Recognition without Gazetteers, in Proceedings of Conference of European", "author": ["A. Mikheev", "M. Moens", "C. Grover"], "venue": "Chapter of the Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Unsupervised named-entity recognition: Generating gazetteers and resolving ambiguity", "author": ["D. Nadeau", "P.D. Turney", "S. Matwin"], "venue": "Lecture Notes in Computer Science, Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Using machine learning to maintain rule-based named-entity recognition and classification", "author": ["G Petasis", "F Vichot", "F Wolinski", "G Paliouras", "V. Karkaletsis", "C.D. Spyropoulos"], "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pp. 426 \u2013 43, Toulouse, France", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1993}, {"title": "Feature selection techniques for maximum entropy based biomedical named entity recognition", "author": ["S.K. Saha", "S. Sarkar", "P. Mitra"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "A vector space model for information retrieval", "author": ["G. Salton", "A. Wong", "C.S. Yang"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1975}], "referenceMentions": [{"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "We use, in this work, the makings of learning technologies, combined with statistical models [17], to extract contexts from Web document corpus, to identify the most pertinent contexts for the recognition of a NE.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "State of the art Named entity recognition can be used to perform numerous processing tasks in various areas: Information extraction systems [6], text mining [8], [16], Automatic Speech Recognition (ASR) [5], etc.", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "State of the art Named entity recognition can be used to perform numerous processing tasks in various areas: Information extraction systems [6], text mining [8], [16], Automatic Speech Recognition (ASR) [5], etc.", "startOffset": 157, "endOffset": 160}, {"referenceID": 15, "context": "State of the art Named entity recognition can be used to perform numerous processing tasks in various areas: Information extraction systems [6], text mining [8], [16], Automatic Speech Recognition (ASR) [5], etc.", "startOffset": 162, "endOffset": 166}, {"referenceID": 4, "context": "State of the art Named entity recognition can be used to perform numerous processing tasks in various areas: Information extraction systems [6], text mining [8], [16], Automatic Speech Recognition (ASR) [5], etc.", "startOffset": 203, "endOffset": 206}, {"referenceID": 11, "context": "[12] have built a system for recognizing named entities, which combines a model based on grammar rules, and statistical models, without resorting to named entity lists.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] suggests an algorithm for named entity classification, based on the meaning word disambiguation, and exploits the redundancy in the contextual characteristics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] presented a method that helps to build a rules-based system for recognition and classification of named entities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "In his paper [10], Mann explores the idea of fine-grained proper noun ontology and its use in question answering.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The Nemesis system presented by Fourour [6] is founded on some heuristics, allowing the identification of named entities, and their classification by detecting the boundaries of the entity called \"context\" to the left or right, and by studying syntactic, or morphological of these entities.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "Krstev et al [9] suggested a basic structure of a relational model of a multilingual dictionary of proper names based on four-level ontology.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "The KNOWITALL system planned by Etzioni et al [4] aims at automating the process of extracting named entities from the Web in an unsupervised and scalable manner.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "Friburger [7] recommends a method based on rules for finding a large proportion of person names.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "[13] have suggested a system for recognizing named entities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1], and Etzioni et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] showed the validity of existing local grammars in the system GRAALWEB to recognize and extract the named entities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7], [11], [14], or on predefined lists of dictionaries [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[7], [11], [14], or on predefined lists of dictionaries [9].", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "[7], [11], [14], or on predefined lists of dictionaries [9].", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "[7], [11], [14], or on predefined lists of dictionaries [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "The ontology domain is still in exploration [10].", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "tfidf (Term Frequency-Inverse Document Frequency) is one of the most classics and most common weighting method used to describe documents in the Vector Space Model [17], particularly in Information Retrieval (IR).", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": "Since decision tree algorithms are widely used for data mining [3], classification, etc.", "startOffset": 63, "endOffset": 66}, {"referenceID": 14, "context": "5 algorithm [15].", "startOffset": 12, "endOffset": 16}], "year": 2011, "abstractText": "This paper introduces a named entity recognition approach in textual corpus. This Named Entity (NE) can be a named: location, person, organization, date, time, etc., characterized by instances. A NE is found in texts accompanied by contexts: words that are left or right of the NE. The work mainly aims at identifying contexts inducing the NE\u2019s nature. As such, The occurrence of the word \"President\" in a text, means that this word or context may be followed by the name of a president as President \"Obama\". Likewise, a word preceded by the string \"footballer\" induces that this is the name of a footballer. NE recognition may be viewed as a classification method, where every word is assigned to a NE class, regarding the context. The aim of this study is then to identify and classify the contexts that are most relevant to recognize a NE, those which are frequently found with the NE. A learning approach using training corpus: web documents, constructed from learning examples is then suggested. Frequency representations and modified tf-idf representations are used to calculate the context weights associated to context frequency, learning example frequency, and document frequency in the corpus.", "creator": "PDFCreator Version 0.9.5"}}}