{"id": "1104.3250", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2011", "title": "Adding noise to the input of a model trained with a regularized objective", "abstract": "Regularization ruiter is saucepans a well 59kg studied fireplace problem grimandi in action-packed the context 1274 of neural networks. kirsi It komisarjevsky is marojejy usually briefers used to translocator improve egba the generalization irish-language performance when the 218.7 number wwp of input nitobe samples 1920s is taie relatively selters small subedar or keegan heavily zamyatin contaminated with noise. The regularization single-crystal of a parametric model padjadjaran can chenglingji be emediamillworks achieved in different manners some of israelson which ork are early stopping (Morgan hhgregg and damilola Bourlard, 1990 ), vergnes weight alabi decay, strigl output budlong smoothing bacrot that are thaton used to avoid recouderc overfitting domitien during chengyi the training of yachi the ikaruga considered model. From ryman\u00f3w a Bayesian shirred point sjahril of unharvested view, many akrobata regularization techniques maclehose correspond to poortvliet imposing danann certain kilian prior distributions pavlodar on model parameters (Krogh fraternize and 383.75 Hertz, gondwana 1991 ). Using Bishop ' s mergenthaler approximation (hatchlings Bishop, s\u00e9es 1995) translocated of linesmen the geballe objective variante function when leonids a gewiss restricted type retro of noise is persiwa added to swahili the input hailu of a junnosuke parametric witkiewicz function, we bahria derive chippewas the higher order 1-of-5 terms stuart of the campin Taylor expansion and analyze suryono the cadenzas coefficients of cambodiana the regularization lp04 terms induced innately by the gabapentin noisy input. spetsnaz In particular we takakuwa study sabots the effect of penalizing the fallsview Hessian of teachings the tuxtlas mapping salunkhe function lewins with eloquence respect to the input in terms of generalization dial-in performance. single-member We also blakeney show 1,409 how 106.2 we donggala can n\u00famenor control trapster independently this junii coefficient macfarland by isocrates explicitly penalizing elvina the Jacobian of lessens the mapping function on bartik corrupted semi-nude inputs.", "histories": [["v1", "Sat, 16 Apr 2011 18:09:13 GMT  (39kb,D)", "http://arxiv.org/abs/1104.3250v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["salah rifai", "xavier glorot", "yoshua bengio", "pascal vincent"], "accepted": false, "id": "1104.3250"}, "pdf": {"name": "1104.3250.pdf", "metadata": {"source": "CRF", "title": "Adding noise to the input of a model trained with a regularized objective", "authors": ["Salah Rifai", "Xavier Glorot", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Regularization is a well studied problem in the context of neural networks. It is usually used to improve the generalization performance when the number of input samples is relatively small or heavily contaminated with noise. The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay or output smoothing, and are used to avoid overfitting during the training of the considered model. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991). In this paper we propose a novel approach to achieve regularization that combines noise in the input and explicit output smoothing by regularizing the L2-norm of the Jacobian\u2019s mapping function with respect to the input. Bishop (1995) has proved that the\nar X\niv :1\n10 4.\n32 50\nv1 [\ncs .A\nI] 1\n6 A\npr 2\ntwo approaches are essentially equivalent under some assumptions using a Taylor approximation up to the second order of the noisy objective function. Using his theoretical analysis, we derive the approximation of our cost function in the weak noise limit and show the advantage of our technique from the theoretical and empirical point of view. In particular, we show that we achieve a better smoothing of the output of the considered model with a little computational overhead."}, {"heading": "2 Definitions", "text": "For the ease of readability, most of our analysis involves only vectors and matrices except for section 6 for which it was not possible to avoid using tensors objects. Also our analysis assumes that the model\u2019s output is scalar which will prevent the use of tensors for the low order terms of the Taylor expansion. We will use the following notations:\n\u2022 \u3008., .\u3009 : inner product,\n\u2022 \u2297 : tensor product,\n\u2022 Jf (x), Hf (x), T (n) f (x) : respectively the Jacobian, Hessian and n-th order deriva-\ntive of f with respect to vector x.\nWe consider the following set of points: Dn = { zi = (xi, yi) \u2208 (Rd,R)|\u2200i \u2208 [[1;n]] } where the (xi, yi) are the (input,target) of an arbitrary dataset. In the paper we will consider a particular family of parametric models\nF = { F\u03b8 \u2208 C\u221e ( Rd ) | \u03b8 \u2208 Rp | p \u2208 N } and F\u03b8(Rd) \u2286 R. We define the expected cost of the true distribution p(z) of our data points as being:\nC(\u03b8) = \u222b L(z, \u03b8)p(z)dz (1)\nThe expected empirical cost when using the data without noise can be expressed as: Cclean(\u03b8) = \u222b L(z, \u03b8)\u03b4(zi \u2212 z)dz = 1\nn n\u2211 i L(zi, \u03b8) (2)\nwhere \u03b4 is the Dirac function. When adding noise to the input, we will consider p(z) as being a parzen density estimator:\np(z) = 1\nn n\u2211 i \u03c8(zi \u2212 z) (3)\nwith the kernels \u03c8 centered on the points of our dataset. In the rest of this paper we will consider kernels for which the following assumptions hold:\n(a) every kernel has zero mean,\n(b) different components of a kernel are independent.\nNote that the normal and uniform distribution have these properties. Using (a), we can write (An, 1996): \u2200i, \u222b \u03b5i\u03c8(\u03b5)d\u03b5 = 0 (4)\nand using (b) :\n\u2200(i, j), \u222b \u03b5i\u03b5j\u03c8(\u03b5)d\u03b5 = \u03c3 2\u03b4ij (5)\nwhere \u03c3 is the variance of the distribution \u03c6, and \u03b5 = (\u03b51, . . . , \u03b5d). In our analysis we will restrict ourselves to gaussian kernels:\n\u03c8(zi \u2212 z) = Nzi,\u03c32(z) (6)\nSubstituting (6) in (3) we can write the objective function with noisy input as being:\nCnoisy(\u03b8) = 1 n n\u2211 i \u222b L(z, \u03b8)Nzi,\u03c32(z)dz (7)"}, {"heading": "3 Penalty term induced by noisy input", "text": "Bishop (1995) already showed that tuning the parameters of a model with corrupted inputs is asymptotically equivalent to minimizing the true error function when simultaneously decreasing the level of corruption to zero as the number of corrupted inputs tends to infinity. He also showed that adding noise in the input is equivalent to minimize a different objective function that includes one or more penalty terms, and he uses a Taylor expansion to derive an analytic approximation of the noise induced penalty. Using the above assumption we can write:\nCnoisy(\u03b8) = Cclean(\u03b8) + \u03c6(\u03b8) (8)\nwhere \u03c6 is the penalty term. Substituting (7) and (2) in (8) we express the penalty term as being:\n\u03c6(\u03b8) = 1\nn n\u2211 i [\u222b L(z, \u03b8)Nzi,\u03c32(z)dz \u2212 L(zi, \u03b8) ] (9)\nWe define the noise vector as being \u03b5 = z\u2212 zi and omitting \u03b8 for simplicity we can write \u2200i, the term inside the sum of (9):\nD = \u222b L(zi + \u03b5)N0,\u03c32(\u03b5)d\u03b5\u2212 L(zi) (10)\nNow that we have identified the term to approximate, let\u2019s write the Taylor approximation of our loss function when our sample is shifted by a noise vector \u03b5:\nL(z + \u03b5) = L(z) + \u3008JL(z), \u03b5\u3009+ 1\n2 \u03b5T .HL(z).\u03b5+ o(\u03b5 2) (11)\nTo match equation (10) we multiply with (6) and integrate with respect to \u03b5 both sides of (11): \u222b\nL(z + \u03b5)N0,\u03c32(\u03b5)d\u03b5 =\n\u222b [ L(z) + \u3008JL(z), \u03b5\u3009+ 1\n2 \u03b5T .HL(z).\u03b5+ o(\u03b5)\n] N0,\u03c32(\u03b5)d\u03b5\n(12)\nEquation (4) implies that all odd-moments of the approximation are null and in conjunction with (5) we can now simplify (12) into:\u222b\nL(z + \u03b5)N0,\u03c32(\u03b5)d\u03b5\n= L(z) \u222b N0,\u03c32(\u03b5)d\u03b5\ufe38 \ufe37\ufe37 \ufe38\n1\n+ \u222b \u3008JL(z), \u03b5\u3009N0\u03c32(\u03b5)d\u03b5\ufe38 \ufe37\ufe37 \ufe38\n0\n+ 1\n2\n\u222b \u03b5T .HL(z).\u03b5N0,\u03c32(\u03b5)d\u03b5+R\n(13)\nand with some algebra we can finally write:\u222b L(z + \u03b5)N0,\u03c32(\u03b5)d\u03b5\u2212 L(z) \u2248 \u03c32\n2 Tr(HL(z)) (14)\nby substituting z = zi and summing over all the elements of Dn:\n\u03c6(\u03b8) \u2248 \u03c3 2\n2n n\u2211 i Tr(HL(zi)) (15)\nHence training with corrupted inputs is approximately equivalent to minimize the following objective:\nCnoise(\u03b8) \u2248 Cclean(\u03b8) + \u03bbTr(HC\u0304(\u03b8)) (16)\nThis relation holds for any objective C with the above two assumptions.\nAll the above results are already well established in the literature of noise injection (Bishop, 1995; An, 1996). Our contribution is to show that, for the second order Taylor expansion, adding noise to the input on a well chosen regularized objective is equivalent to add\na L2-norm on the Hessian of the output function of the considered model respectively to its input, which is not the case when adding noise to a non-regularized objective. In the following sections we will consider the MSE as the objective function to tune the parameters of our model but this choice does not affect the generality of our analysis."}, {"heading": "4 Non regularized objective", "text": "We define the following error function:\nLclean(zi, \u03b8) =\u2016 F (xi, \u03b8)\u2212 yi \u20162\nand the Hessian of the cost function being the average over the Hessian of all individual errors:\nHCclean(\u03b8) = 1\nn n\u2211 i HLclean(zi, \u03b8)\nAssuming without loss of generality that F is a scalar function1, and that the noise is added to the input x, we will only consider the Hessian of the loss function with respect to x :\nHLclean(x) = \u2202\n\u2202x\n[ \u2202\n\u2202x\n( \u2016 F (x, \u03b8)\u2212 y \u20162 )] = 2 \u2202\n\u2202x\n[ \u2202F (x, \u03b8)\n\u2202x (F (x, \u03b8)\u2212 y)\n]\n= 2\n[ \u22022F (x, \u03b8)\n(\u2202x)2 (F (x, \u03b8)\u2212 y) +\n\u2329 \u2202F (x, \u03b8)\n\u2202x \u232a2] HLclean(x) = 2HF (x) (F (x, \u03b8)\u2212 y) + 2 \u2329 JF (x) T , JF (x) \u232a\nA standard result of linear algebra is the relation between the Frobenius norm and the trace operator: Tr (\u2329 AT , A \u232a) = ||A||2F\nBy taking the trace of the above results, we get:\nTr (HLclean(x)) = 2 (F (x, \u03b8)\u2212 y) Tr (HF (x)) + 2 ||JF (x)|| 2 F\nand plugging this in (16) gives us the following second order approximation of the noisy objective:\nLnoise(zi, \u03b8) \u2248\u2016 F (xi, \u03b8)\u2212 yi \u20162 +2\u03bb ( (F (x, \u03b8)\u2212 y) Tr (HF (x)) + ||JF (x)||2F )\n(17)\n1in the case of multiple regression such as a reconstruction function, only the orders of the tensors involved in the approximation of L will change.\nWe obtain a L2-norm on the gradient of the mapping function F added to our error function whereas the Hessian term is not constrained to be positive and it is not sure that its terms are going to cancel-out. E.g. if prediction overshoots or undershoots on average, then penalty may encourage very large Hessian trace inducing high curvature which would potentially harm a stochastic gradient descent and converge to a poor local minimum."}, {"heading": "5 Our regularized objective", "text": "As the results of the previous section suggest, adding noise to the input of the objective function yields an undesirable term that might interfere with the goal of smoothing the output of our function. We propose here to overcome this difficulty by adding noise only to the input of the objective function\u2019s Jacobian JF , doing so will avoid the unpredictable effect of additional unwanted terms. We define the error function we previously used to which we add a regularization term that is the L2-norm of the Jacobian of F with respect to x : Lreg+noise(zi, \u03b8) =\u2016 F (xi, \u03b8)\u2212 yi \u20162 +\u03bb ||JF (x\u0303i)||2F We now calculate the Hessian of L\u0303 by omitting the first term since the noise is added only to the input of the regularization term, and using the approximation derived in (16): \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2202F (x\u0303i, \u03b8)\u2202x \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F \u2248 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2202F (xi, \u03b8)\u2202x \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F + \u03b52 Tr ( H|| \u2202F\u2202x || 2(x)\n) We can now calculate the Hessian of the regularization term as being:\nH|| \u2202F\u2202x || 2(x) = 2\n\u2202\n\u2202x\n[ \u22022F (x, \u03b8)\n(\u2202x)2 \u2202F (x, \u03b8) \u2202x\n]\n= 2\n[ \u22023F (x, \u03b8)\n(\u2202x)3 \u2202F (x, \u03b8) \u2202x +\n\u2329 \u22022F (x, \u03b8)\n(\u2202x)2\n\u232a2]\nH|| \u2202F\u2202x || 2(x) = 2\n\u22023F (x, \u03b8)\n(\u2202x)3 JF (x) + 2\n\u2329 HF (x) T , HF (x) \u232a\nTr ( H|| \u2202F\u2202x || 2(x) ) = 2 Tr ( \u22023F (x, \u03b8) (\u2202x)3 JF (x) ) + 2 ||HF (x)||2F (18)\nwhich gives us the approximation of our regularized objective:\nLreg+noise(zi, \u03b8) =\u2016 F (xi, \u03b8)\u2212 yi \u20162 +\u03bb ||JF (x)||2F + 2\u03bb\u03c3 2 ||HF (x)||2F +R (19)\nWe have shown that adding noise to a well chosen regularized objective clearly penalize the L2-norm on the Hessian of the considered model F (without ever calculating it) using\na second order Taylor approximation of the noisy objective under two necessary assumptions on the noise distribution. In statistics regularizing the norm of the derivatives of the model to be tuned is often referred as roughness penalty (Green, 1993) and is used in the context of cubic splines (De Boor, 1998)."}, {"heading": "6 Higher order terms of the Taylor expansion", "text": "In this section we are interested in the higher order terms of the cost approximation, we find it convenient to use the following formalism: if TnL (z, \u03b5) denotes the n-th order derivative of L with respect to z, then:\nTnL (z, \u03b5) = 1\nn! \u2211 i1,...,in \u03b5i1 , . . . , \u03b5inT n i1,...,in(z)\nwhere Tn is a tensor of order n and\nTni1,...,in(z) = \u2202nL(z)\n\u2202zi1 , ..., \u2202zin\nusing this formalism we can write the fourth order derivative as being:\nT 4L(z, \u03b5) = 1\n24 \u2211 i,j,k,l \u03b5i\u03b5j\u03b5k\u03b5lT 4 ijkl(z)\nUsing the two assumptions made on the noise distribution, we know that the third order derivative of the approximation is zero. As for the fourth order derivative, using the second assumption of the noise distribution we know that only the terms that are on the diagonal of the T 4 will be non-zero, we can then write:\n\u222b T 4L(z, \u03b5)N0,\u03c32(\u03b5)d\u03b5 = \u03c34\n4! \u2211 i T 4iiii(z)\nUsing the above result we can approximate our cost function in the noisy input setting more finely, for this purpose we will use the results obtained above for the Hessian and differentiate them again twice with respect to x.\nT 4L(x) = \u22022 (HL (x))\n(\u2202x)2\n= \u22022 (\u2202x)2 [ 2HF (x) (F (x, \u03b8)\u2212 y) + 2 \u2329 JF (x) T , JF (x) \u232a]\n= 2 \u2202\n\u2202x\n[ T 3F (x) (F (x, \u03b8)\u2212 y) + 3HF (x)JF (x) ] = 2 [ T 4F (x) (F (x, \u03b8)\u2212 y) + 4T 3F (x)JF (x) + 3 \u2329 HF (x) T , HF (x) \u232a]\nhence,\nTr ( T 4L (z) ) = 6 ||HF (x)||2F + Tr(R)\nwhere R = 2T 4F (x) (F (x, \u03b8)\u2212 y) + 8T 3F (x)JF (x)."}, {"heading": "7 Comparison", "text": ""}, {"heading": "7.1 Noise added to the input of the objective function", "text": "Now that we have a higher order approximation in the case where noise is added to the input of the function, we can compare the magnitude of the coefficients that penalize the Hessian HF , note that in this case the Hessian term appears in the fourth order of the Taylor expansion of the cost function, whereas we need only a second order approximation in the case where we add a regularization term evaluated on a corrupted input. We can write the approximation of the cost function without regularization at the fourth order as being:\nLnoise(z) \u2248 Lclean(z) + \u03c3 2\n2! Tr(HLclean(z)) +\n\u03c34 4! Tr ( T 4Lclean (z) ) Lnoise(z) =\u2016 F (xi, \u03b8)\u2212 yi \u20162 +\u03c32 ||JF (x)||2F + \u03c34\n4 ||HF (x)||2F +R (20)\nwhere the number i of overline denotes the terms induced by the noise obtained at the i-th order of the Taylor expansion."}, {"heading": "7.2 Noise added to the input of the Jacobian of the objective function", "text": "In this case we just need to approximate the cost function up to the second order of the Taylor expansion:\nLreg+noise(zi, \u03b8) =\u2016 F (xi, \u03b8)\u2212 yi \u20162 +\u03bb ||JF (x)||2F + 2\u03bb\u03c32 ||HF (x)|| 2 F +R (21)"}, {"heading": "8 Experimental results", "text": "We have tried several experiments in order to benchmark the effect of regularization and noise combined, for this task we used the well known MNIST (LeCun et al., 1998), MNIST binarised and the USPS database. Surprisingly, we were able to achieve results close to those obtained with unsupervised pretraining (Erhan et al., 2010). MNIST is composed of 70k handwritten digits represented by a vector of pixels. It is divided in 50k for the training set, 10k for each of the validation and test set, the range of the features were rescaled to be within [0, 1]. MNIST-Binary is divided exactly the same way as\nMNIST, the only difference is that the intensity of the pixels superior to 2552 where set to 1 and the others to 0. USPS dataset consists of a training set with 7291 images and a test set with 2007 images, the validation set was formed using the last 2000 images of the training set. The model F (x), with parameters \u03b8 = {W (1), . . . ,W (n), b(1), . . . , b(n)}, we considered to solve the classification task was a standard neural network with one or more hidden layers, and a hyperbolic tangent non-linearity in between the layers. For example, with one hidden layer we have:\nF (x) = \u03c3(W (2) tanh(W (1).x+ b(1)) + b(2))\nwhere \u03c3(.) is the logistic sigmoid function. In this setting we can write the Frobenius norm of the Jacobian of F with respect to x as being:\n||JF (x)||2 = \u2211 i,j J2ij\nand with some calculus we get:\nJij = Fi(x)(1\u2212 Fi(x)) \u2211 l W (2) il W (1) lj\n( 1\u2212 tanh2 (\u2211 m W (1) lm xm + b (1) l ))\nwhere Fi(x) is i-th output of the network. For the results in table (1), we used a number of hidden units ranging from 400 to 1000 per layer, the best results were obtained with two hidden layers on MNIST, and one hidden layer on MNIST-BINARY and USPS. The parameters of the model were learned through stochastic gradient descent with a learning rate ranging from 0.1 to 0.001. We also investigated the use of Rectifying units (i.e. max(0, x)) (Nair and Hinton, 2010; Glorot et al., 2010) as non-linearity in the hidden layer, surprisingly they seemed to benefit less from the added noise to the input than from the regularization term alone, they achieved a test classification performance of 4.8% on the USPS dataset equaling the best performance of the hyperbolic tangent activation with both regularization and added noise to the input. The best results where obtained with a gaussian isotropic noise with a standard deviation of 0.1 around training samples. Figure 1 shows the histograms of activation values on the MNIST test set of our best MLPs with and without Jacobian regularization. The activations of the regularized model are more densely distributed at the saturation and linear regime."}, {"heading": "9 Discussion", "text": ""}, {"heading": "9.1 Constraining the solution space", "text": "When optimizing a non-convex function with an infinite amount of local minimum, it is not clear which of them yields a reasonable generalization performance, the concept of overfitting clearly illustrate this point. The proposed regularizer tries to avoid this scheme by flattening the mapping function over the training examples inducing a local\ninvariance in the mapping space to infinitesimal variance in the input space. Figure 2 shows that when the input is corrupted, the models learned with the regularization term are more robust to noise on the input. Geometrically, the added regularization imposes to the model to be a Lipschitz function or a contracting map around the training examples imposing the constraint F (x+ \u03b5) \u2248 F (x)."}, {"heading": "9.2 Smoothing away from the training points", "text": "Penalizing only the Jacobian of the model F with respect to the input x gives only a guarantee of flatness for an infinitesimal move away of the training example. To illustrate this point one can imagine that on the tip of a dirac function the norm of the jacobian is null and infinite just around it. Although this situation is not possible in the context of neural networks because of their smooth activation function. Given enough capacity we could converge to this solution if we do not add additional constraints. One of them would be to adjust the locality of the flatness as a hyper-parameter of the model. It requires to compute the higher order terms of the mapping function in order to regularize the magnitude of their norms. As it was discussed before, it is computationally expansive\nto explicitly calculate the norm of the high order derivatives because their number of components increases exponentially, instead our approach proposes an approximation of the Hessian term that allows you to simultaneously control the magnitude of both the Jacobian and hessian norms independently."}, {"heading": "9.3 The other terms induced by the noise", "text": "As we have seen in the equation 17, the added noise does not yield only in a penalty on the norm of the successive derivatives of the mapping function and it is somehow unclear how these terms behave during gradient descent since they are not constrained to be positive. In a supervised setting, it is empirically feasible to drive those terms to zero because of the small dimensionality of the target points, whereas in a multidimensional regression task such as the reconstruction objective of an auto-encoder it is often impossible to achieve a \u201cnear\u201d zero minimization of the cost with a first order optimization such as a stochastic gradient descent. The reader should note that the approximation of the noisy cost is valid when the number of corrupted inputs tends to infinity, though in practise this is never the case. It would be interesting to do an estimate of the difference between the terms induced by the noise and the real values of the term in function of the number of corrupted samples."}, {"heading": "10 Conclusion", "text": "We have shown how to obtain a better generalization performance using a regularization term that adds a marginal computational overhead compared to the traditional approach. Using a Taylor expansion of the cost function, we also showed that by adding noise to the input of the regularization term we are able to penalize with a greater magnitude the norm of the higher order derivatives of the model avoiding the need to explicitly calculate them, which would be obviously computationally prohibitive. Initial results suggests that different parametric models clearly benefit from this approach in terms of predicting outof-sample points. It would be interesting to investigate how this regularization approach would behave when used with non-parametric models such as gaussian-mixtures."}], "references": [{"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["G. An"], "venue": "Neural Comput.,", "citeRegEx": "An,? \\Q1996\\E", "shortCiteRegEx": "An", "year": 1996}, {"title": "Training with noise is equivalent to Tikhonov regularization", "author": ["C.M. Bishop"], "venue": "Neural Computation,", "citeRegEx": "Bishop,? \\Q1995\\E", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Calculation of the smoothing spline with weighted roughness measure", "author": ["C. De Boor"], "venue": "In Mathematical Models and Methods in Applied Sciences,", "citeRegEx": "Boor,? \\Q1998\\E", "shortCiteRegEx": "Boor", "year": 1998}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "Manzagol", "P.-A", "P. Vincent", "S. Bengio"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Deep sparse rectifier neural networks. Deep Learning and Unsupervised Feature Learning Workshop \u2014 NIPS \u201910", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Nonparametric Regression and Generalized Linear Models", "author": ["Green", "B.W.S. Bernard W Silverman"], "venue": null, "citeRegEx": "Green and Silverman,? \\Q1993\\E", "shortCiteRegEx": "Green and Silverman", "year": 1993}, {"title": "A simple weight decay can improve generalization", "author": ["A. Krogh", "J.A. Hertz"], "venue": null, "citeRegEx": "Krogh and Hertz,? \\Q1991\\E", "shortCiteRegEx": "Krogh and Hertz", "year": 1991}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE ,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generalization and parameter estimation in feedforward", "author": ["N. Morgan", "H. Bourlard"], "venue": null, "citeRegEx": "Morgan and Bourlard,? \\Q1990\\E", "shortCiteRegEx": "Morgan and Bourlard", "year": 1990}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Denver", "V. CO. Morgan Kaufmann. Nair", "G.E. Hinton"], "venue": null, "citeRegEx": "Denver et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Denver et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model.", "startOffset": 111, "endOffset": 138}, {"referenceID": 6, "context": "From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991).", "startOffset": 133, "endOffset": 156}, {"referenceID": 1, "context": "Using Bishop\u2019s approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input.", "startOffset": 29, "endOffset": 43}, {"referenceID": 8, "context": "The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay or output smoothing, and are used to avoid overfitting during the training of the considered model.", "startOffset": 111, "endOffset": 138}, {"referenceID": 6, "context": "From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991).", "startOffset": 133, "endOffset": 156}, {"referenceID": 1, "context": "Bishop (1995) has proved that the", "startOffset": 0, "endOffset": 14}, {"referenceID": 0, "context": "Using (a), we can write (An, 1996): \u2200i, \u222b \u03b5i\u03c8(\u03b5)d\u03b5 = 0 (4)", "startOffset": 24, "endOffset": 34}, {"referenceID": 1, "context": "3 Penalty term induced by noisy input Bishop (1995) already showed that tuning the parameters of a model with corrupted inputs is asymptotically equivalent to minimizing the true error function when simultaneously decreasing the level of corruption to zero as the number of corrupted inputs tends to infinity.", "startOffset": 38, "endOffset": 52}, {"referenceID": 1, "context": "All the above results are already well established in the literature of noise injection (Bishop, 1995; An, 1996).", "startOffset": 88, "endOffset": 112}, {"referenceID": 0, "context": "All the above results are already well established in the literature of noise injection (Bishop, 1995; An, 1996).", "startOffset": 88, "endOffset": 112}, {"referenceID": 7, "context": "8 Experimental results We have tried several experiments in order to benchmark the effect of regularization and noise combined, for this task we used the well known MNIST (LeCun et al., 1998), MNIST binarised and the USPS database.", "startOffset": 171, "endOffset": 191}, {"referenceID": 3, "context": "Surprisingly, we were able to achieve results close to those obtained with unsupervised pretraining (Erhan et al., 2010).", "startOffset": 100, "endOffset": 120}, {"referenceID": 4, "context": "max(0, x)) (Nair and Hinton, 2010; Glorot et al., 2010) as non-linearity in the hidden layer, surprisingly they seemed to benefit less from the added noise to the input than from the regularization term alone, they achieved a test classification performance of 4.", "startOffset": 11, "endOffset": 55}], "year": 2011, "abstractText": "Regularization is a well studied problem in the context of neural networks. It is usually used to improve the generalization performance when the number of input samples is relatively small or heavily contaminated with noise. The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991). Using Bishop\u2019s approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input. In particular we study the effect of penalizing the Hessian of the mapping function with respect to the input in terms of generalization performance. We also show how we can control independently this coefficient by explicitly penalizing the Jacobian of the mapping function on corrupted inputs.", "creator": "LaTeX with hyperref package"}}}