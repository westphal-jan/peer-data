{"id": "1106.0357", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2011", "title": "Learning Hierarchical Sparse Representations using Iterative Dictionary Learning and Dimension Reduction", "abstract": "chlorite This paper thalassa introduces an yde elemental 23.01 building hajkova block which sauteing combines Dictionary rachida Learning scholarshare and Dimension demersal Reduction (carbines DRDL ). We 1.3927 show 88-82 how presenta this raybon foundational element melgren can be tenaciousness used to indecisively iteratively construct a Hierarchical aaj Sparse wabag Representation (franchet HSR) 1,562 of homophones a user-centered sensory stream. We bengt compare pombaline our woodsy approach to nocona existing models showing lingonberry the frumos generality of 9,000-seat our simple prescription. delal We then perform preliminary embedding experiments using aphids this disenchanted framework, stalinist illustrating with the example of an newspapers object iolaus recognition ayisha task 93,500 using three-hour standard datasets. communicates This argenbright work introduces sumar the very first seguenzioidea steps towards an minneapolis-saint integrated framework ramasamy for war-ravaged designing and analyzing world-systems various trbovich computational tasks unfcc from great-grandparents learning to attention to approximation action. naknek The gandolfi ultimate goal teleshopping is building morcheeba a mathematically hillsboro rigorous, chanoch integrated theory transversus of intelligence.", "histories": [["v1", "Thu, 2 Jun 2011 02:31:04 GMT  (438kb,D)", "http://arxiv.org/abs/1106.0357v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["mohamad tarifi", "meera sitharam", "jeffery ho"], "accepted": false, "id": "1106.0357"}, "pdf": {"name": "1106.0357.pdf", "metadata": {"source": "CRF", "title": "Learning Hierarchical Sparse Representations using Iterative Dictionary Learning and Dimension Reduction", "authors": ["Mohamad Tarifi", "Meera Sitharam", "Jeffery Ho"], "emails": [], "sections": [{"heading": null, "text": "Introduction\nWorking towards a Computational Theory of Intelligence, we develop a computational framework inspired by ideas from Neuroscience. Specifically, we integrate notions of columnar organization, hierarchical structure, sparse distributed representations, and sparse coding.\nAn integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12]. In this framework, Intelligence is viewed as a surrogate minimization of the entropy of this sensorium. This work is intuitively inspired by this view, aiming to provide a computational foundation for a theory of intelligence from the perspective of theoretical computer science, thereby connecting to ideas in mathematics. By building foundations for a principled approach, the computational essence of problems can be isolated, formalized, and their relationship to fundamental problems in mathematics and theoretical computer science can be illuminated and the full power of available mathematical techniques can be brought to bear.\nA computational approach is focused on developing tractable algorithms. exploring the complexity limits of Intelligence. Thereby improving the quality of available guarantees for evaluating performance of models, improving comparisons among models, and moving towards provable guarantees such as sample size, time complexity, generalization error, assumptions about prior. This furnishes a solid theoretical foundation which may be used, among other things, as a basis for building Artificial Intelligence.\nar X\niv :1\n10 6.\n03 57\nv1 [\ncs .L\nG ]\n2 J\nun 2\n01 1\n0.1 Background Literature In A Glance\nSpeculation on a cortical micro-circuit element dates back to Mountcastle\u2019s observation that a cortical column may serve as an algorithmic building block of the neocortex [18]. Later work by Lee and Mumford [16], Hawkins and George [14] attempted further investigation of this process.\nThe bottom-up organization of cortex is generally assumed to be a hetrarchical topology of columns. This can be modeled as a directed acyclic graph, but is usually simplified to a hierarchical tree. Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology. Smale et al attempts to develop a theory accounting for the importance of the hierarchical structure [24, 3].\nWork on modeling early stages of sensory processing by Olshausen [19, 1] using sparse coding produced results that account for the observed receptive fields in early visual processing. This is usually done by learning an overcomplete dictionary. However it remained unclear how to extend this to higher layers. Our work can be partially viewed as a progress in this direction.\nComputational Learning Theory is the formal study of learning algorithms. PAC defines a natural setting for analyzing such algorithms. However, with few notable exceptions (Boosting, inspiration for SVM, etc) the produced guarantees are divorced from practice. Without tight guarantees Machine Learning is studied using experimental results on standard benchmarks, which is problematic. We aim at closing the gap between theory and practice by providing stronger assumptions on the structures and forms considered by the theory, through constraints inspired by biology and complex systems.\n0.2 A Variety of Hierarchical Models\nSeveral hierarchical models have been introduced in the literature. H-Max is based on Simple-Complex cell hierarchy of Hubel and Wiesel. It is basically a hierarchical succession of template matching and a max-operations, corresponding to simple and complex cells respectively [20].\nHierarchical Temporal Memory (HTM) is a learning model composed of a hierarchy of spatial coincidence detection and temporal pooling. Coincidence detection involves finding a spatial clustering of the input, while temporal pooling is about finding variable order Markov chains describing temporal sequences in the data.\nH-Max can be mapped into HTM in a straightforward manner. In HTM, the transformations in which the data remains invariant is learned in the temporal pooling step. H-Max explicitly hard codes translational transformations through the max operation. This gives H-Max better sample complexity for the specific problems where translational invariance is present.\nBouvrie et al [3, 2] introduced a generalization of hierarchical architectures centered around a foundational element involving two steps, Filtering and Pooling. Filtering is described through a reproducing Kernel K(x, y), such as the standard inner product K(x, y) = \u3008x, y\u3009, or a Gaussian kernel K(x, y) = e\u2212\u03b3\u2016x\u2212y\u2016 2\n. Pooling then remaps the result to a single value. Examples of pooling functions include max, mean, and lp norm (such as l1 or l\u221e). H-max, Convolutional Neural Nets, and Deep Feedforward Neural Networks all belong to this category of hierarchical architectures corresponding to\ndifferent choices of the Kernel and Pooling functions. Our model does not fall within Bouvrie\u2019s present framework, and can be viewed as a generalization of hierarchical models in which both HTM and Bouvrie\u2019s framework are a special case.\nFriston proposed Hierarchical Dynamic Models (HDM) which are similar to the above mentioned architectures but framed in a control theoretic framework operating in continuous time [8]. A computational formalism of his approach is thus prohibitively difficult.\n0.3 Scope\nOur approach to the circuit element is an attempt to abstract the computationally fundamental processes. We conjecture a class of possible circuit elements for bottom-up processing of the sensory stream.\nFeedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9]. We choose to leave feedback for future work, allowing us to focus here on basic aspects of this model.\n0.4 Contribution and Organization\nThe following section introduces a novel formulation of an elemental building block that could serve as a the bottom-up piece in the common cortical algorithm. This circuit element combines Dictionary Learning and Dimension Reduction (DLDR). After formally introducing DRDL, we show how it can be used to iteratively construct a Hierarchical Sparse Representations (HSR) of a sensory stream. Comparisons to relevant known models are presented. To gain further insight, the model is applied to standard vision datasets. This immediately leads to a classification algorithm that uses HSR for feature extraction. In the appendix, we discuss how further assumptions about the prior can naturally be expressed in our framework."}, {"heading": "1 DRDL Circuit Element", "text": "Our circuit element is a simple concatenation of a Dictionary Learning (DL) step followed by a Dimension Reduction (DR) step. Using an overcomplete dictionary increase the dimension of the data, but since the data is now sparse, we can use Compressed Sensing to obtain a dimension reduction."}, {"heading": "1.1 Dictionary Learning", "text": "Dictionary Learning obtains a sparse representation by learning features on which the data xi can be written in sparse linear combinations.\nDefinition 1. Given an input set X = [x0 . . . xm], xi \u2208 Rd, Dictionary Learning finds D = [v0 . . . vn] and \u03b8 = [\u03b8i . . . \u03b8m], such that xi = D\u03b8i and \u2016\u03b8i\u20160 \u2264 s.\nWhere the \u2016.\u20160 is the L0-norm or sparsity. If all entries of \u03b8i are restricted to be non-negative, we obtain Sparse-Non-negative Matrix Factorization (SNMF).\nAn optimization version of Dictionary Learning can be written as:\nmin D\u2208Rd,n max xi\nmin \u2016\u03b8i\u20160 : xi = D\u03b8i\nIn practice, the Dictionary Learning problem is often relaxed to the Lagrangian:\nmin \u2016X \u2212D\u03b8\u20162 + \u03bb\u2016\u03b8\u20161 Where X = [x0...xm] and \u03b8 = [\u03b80...\u03b8m]. Several dictionary learning algo-\nrithms work by iterating the following two steps.\n1. Solve the vector selection problem for all vectors X. This can be done using your favourite vector selection algorithm, such as basis pursuit.\n2. Given X, the optimization problems is now convex in D. Use your favorite method to find D.\nUsing a maximum likelihood formalism, the Method of Optimal Dictionary (MOD) uses the psuedoinverse to compute D:\nD(i+1) = X\u03b8(i) T (\u03b8n\u03b8i T )\u22121.\nThe MOD can be extended to Maximum A-Posteriori probability setting with different priors to take into account preferences in the recovered dictionary. Similarly, k-SVD uses an two step iterative process, with a Truncated Singular Value Decomposition to update D. This is done by taking every atom in D and applying SVD to X and \u03b8 restricted to only the columns that that have contribution from that atom.\nWhen D is restricted to be of the form D = [B1, B2 . . . BL] where Bi\u2019s are orthonormal matrices, then a more efficient pursuit algorithm is obtained for the sparse coding stage using a block coordinate relaxation."}, {"heading": "1.2 Dimension Reduction", "text": "The DL step learns a representation \u03b8i of the input that lives in a high dimension, but we can obtain a lower dimensional representation, since \u03b8i is now readily seen as sparse in the standard orthonormal basis of dimension n. We can obtain a dimension reduction by using applying a linear operator satisfying the Restricted Isometry Property from Compressed Sensing theory.\nDefinition 2. A linear operator A has the Restricted Isometry Property (RIP) for s, iff \u2203\u03b4s such that:\n(1\u2212 \u03b4s) \u2264 \u2016Ax\u201622 \u2016x\u201622 \u2264 (1 + \u03b4s) (1)\nAn RIP matrix can compress sparse data while maintaining their approximate relative distances. This can be seen by considering two s-sparse vectors x1 and x2, then:\n(1\u2212 \u03b42s) \u2264 \u2016Ax1 \u2212Ax2\u201622 \u2016x1 \u2212 x2\u201622 \u2264 (1 + \u03b42s) (2)\nGiven an s-sparse vector of dimension n, RIP reduces the dimension to O(s log(n)). Since we are using an RIP matrix, efficient decompression is guaranteed using L1 approximation. The data can be recovered exactly using L1 minimization algorithms such as Basis Pursuit.\nRIP matrices can be obtained probabilistically from matrices with random Gaussian entries. Alternatively RIP matrices can be obtained using sparse random matrices [15]. In this paper we follow the latter approach. The question of deterministically constructing RIP matrices with similar bounds is still open."}, {"heading": "1.3 Illustrating the model with simple examples", "text": "Let us consider in more detail the working of a single DRDL node. Figure 2 illustrates a particular distribution that is s = 1-sparse in the 3 drawn vectors. The Dictionary D1 is (\n1 0 1 0 1 1 ) The columns of the dictionary learned correspond to the drawn vectors, and the data is expressed simply as a vector with one coefficient corresponding to inner product with the closest vector and zero otherwise. This produces an\ns = 1 sparse vector in dimension 3. We can then apply a dimension reduction that preserves distances between those representations. The representations correspond to the standard basis in d = 3. The best dimension reduction to d = 2 is then simply the projection of the representations onto the plane perpendicular to (1, 1, 1). Whereby points on the unit basis project to the vertices of a triangle as illustrated in Figure 1. This forms the output of the current node.\nA slightly more complicated example is s = 2 in the Dictionary D2 shown bellow. Figure 3 illustrates this distribution for non-negative coefficients. The data was normalized for convenience. 1 0 0 10 1 0 1\n0 0 1 1 "}, {"heading": "1.4 Relation between DR and DL", "text": "There is a symmetry in DRDL due to the fact that DR and DL steps are intimately related. To show their relationship clearly, we rewrite the two problems with the same variable names. These variables are only relevant for this section. The two problems can be stated as:\n1. DL asks for D and {x1 . . . xm}, given {y1 . . . ym}, for Dxi = yi, such that the sparsity \u2016xi\u20160 is minimized for a fixed dimension of yi.\n2. DR asks for D and {y1 . . . ym}, given {x1 . . . xm}, for Dxi = yi, such that the dimension of yi\u2019s is minimized for a fixed sparsity \u2016xi\u20160.\nIn practice, both problems use L1 approximation as a proxy for L0 optimization. This leads to the following observation\nObservation. The inverse of a DRDL is a DRDL.\nThis means that the space of mappings/functions of our model is the same as it\u2019s inverse. This property will be useful for incorporating feedback."}, {"heading": "1.5 Discussion of Trade-offs in DRDL", "text": "DRDL can be thought of a memory system (\u2019memory pocket\u2019) or a dimension reduction technique for data that can be expressed sparsely in a dictionary. One parameter trade-off is between n, the number of columns in D, and s, the sparsity of the representation.\nOn one hand, we note that the DR step puts the data in O(slog(n)) dimension. Therefore, if we desire to maximize the reduction in dimension, increasing n by raising it to a constant power k is comparable to multiplying s by k. This means that we much rather increase the number of columns in the dictionary than the sparsity. On other hand, increasing the number of columns in D forces the columns to be highly correlated. Which will become problematic for Basis Pursuit vector selection.\nThis trade-off highlights the importance of investigating approaches to dictionary learning and vector selection that can go beyond current results into highly coherent dictionaries. We will further discuss this topic in future papers."}, {"heading": "2 A Hierarchical Sparse Representation", "text": "If we assume a hierarchical architecture modeling the topographic organization of the visual cortex, a singular DRDL element can be factorized and expressed as a tree of simpler DRDL elements. With this architecture we can learn a Hierarchical Sparse Representation by iterating DRDL elements."}, {"heading": "2.1 Assumptions of Generative Model", "text": "Our models assumes that data is generated by a hierarchy of spatiotemporal invariants. At any given level i, each node in the generative model is assumed to be composed of a small number of features si . Generation proceeds by recursively decompressing the pattern from parent nodes then producing patterns to child nodes. This input is fed to the learning algorithm bellow. In this paper we assume that both the topology of generative model and the spatial and temporal extent of each node is known. Discussion of algorithms for learning the topology and internal dimensions is left for future work.\nConsider a simple data stream consisting of a spatiotemporal sequences from a generative model defined above. Figure 1 shows a potential learning hierarchy. For simple vision problems, we can consider all dictionaries within a layer as the same. In this paper, processing proceeds bottom-up the hierarchy only."}, {"heading": "2.2 Learning Algorithm", "text": "Recursively divide the spatiotemporal signal xi to obtain a tree representing the known topographic hierarchy of spatiotemporal blocks. Let x0i.j be the jth block at level 0. Then, starting a the bottom of the tree, do:\n1. Learn a Dictionary Dj,k in which the spatiotemporal data x k i,j can be\nrepresented sparsely. This produces a vector of weights \u03b8i,j,k.\n2. Apply dimensionality reduction to the sparse representation to obtain ui,j,k = A\u03b8i,j,k.\n3. Generate xk+1i,j by concatenating vectors ui,l,k for all l that is child of j in at level k in the tree. Replace k = k+ 1. And now j ranges over elements of level k. If k is still less than the depth of the tree, go to Step 1.\nNote that in domains such as computer vision, it is reasonable to assume that all Dictionaries at level i are the same Dj,k = Dk . This algorithm attempts to mirror the generative model. It outputs an inference algorithm that induces a hierarchy of sparse representations for a given data point. This can be used to abstract invariant features in the new data. One can then use a supervised learning algorithm on top of the invariant features to solve classification problems."}, {"heading": "2.3 Representation Inference", "text": "For new data points, the representation is obtained, in analogy to the learning algorithm, recursively dividing the spatiotemporal signal to obtain a tree representing the known topographic hierarchy of spatiotemporal blocks. The representation is inferred naturally by iteratively applying Vector Selection and Compressed Sensing. For Vector Selection, we employ a common variational technique called Basis Pursuit De-Noising (BPDN), which minimizes \u2016D\u03b8i \u2212 xi\u201622 + \u03bb\u2016\u03b8i\u20161. This technique produces optimal results when the sparsity\n\u2016\u03b8\u20160 < 1\n2 +\n1\n2C . (3)\nWhere C is the coherence of the dictionary D.\nC = max k,l\n(DTD)k,l (4)\nThis is a limitation in practice since it is desirable to have highly coherent dictionaries. Representation inference proceeded by iteratively applying Basis Pursuit and compressing the resulting sparse representation using the RIP matrix."}, {"heading": "2.4 Mapping between HSR and current models", "text": "This simple, yet powerful, toy model is used as a basis investigation tool for our future work. We abstracted this model out of a need for conceptual simplicity and generality. Several models inspired by the neocortex have been proposed, many sharing similar characteristics. We present a few here and compare to our model.\nH-Max is mapped to HSR by replacing Basis Pursuit for Sparse Approximation with Template Matching. H-Max uses random templates from the data, whereas HSR uses Dictionary Learning. The \u2019max\u2019 operation in H-Max can be understood in terms of the operation of sparse coding, which produces local competition among features representing slight variations in the data. Alternatively, the \u2019max\u2019 operation can be viewed as a limited form of dimension reduction.\nHTM can be mapped to HSR by considering time as another dimension of space. This way, bounded variable order markov chains can be written as Sparse Approximation of spatiotemporal vectors representing sequences of spacial vectors in time. HTM constructs a set of spatial coincidences that is automatically shared across time. HSR may do the same when fed a moving window of spatial sequences. Alternatively, HSR will simulate an HTM by alternating between time-only and space-only spatiotemporally extended blocks. In this view a single HTM node is mapped to two layer HSR nodes, one representing time, and the other representing space. Unlike HTM, treating time and space on equal footing has the added advantage that the same algorithms used be used for both. HTM with winner-takes-all policy can then be mapped to our model by assuming sparsity s = 1. HTM with distribution on belief states can be mapped to our model with Template Matching instead of Sparse Approximation in the inference step. Finally, HTM does not leverage RIP dimension reduction step. HTM uses feedback connections for prediction, which is restricted to predictions forward in time. Extending HSR with feedback connections, which accounts for dependency between nodes that are not connected directly, enables feedback to affect all space-time."}, {"heading": "2.5 What does HSR offer beyond current models?", "text": "One advantage of our approach is in providing invertibility without a dimensionality blow-up for hierarchically sparse data. Models falling under Bouvrie et al\u2019s framework loose information as you proceed up the hierarchy due to the Pooling operation. This becomes problematic when extending the models to incorporate feedback. Moreover, this information loss forces an algorithm designer to hardcode what invariances a particular model must select for (such as translational invariance in H-max). On the other hand, invertible models such as HTM suffer from dimensionality blow up, when the number of features learned at a given level is greater than the input dimension to that level - as is usually the case. Dimensionality reduction achieves both a savings in computational resources as well as better noise resilience by avoiding overfitting.\nDictionary learning represents the data by sparse combinations of dictionary columns. This can be viewed as a L0 or L1 regularization, which provides noise tolerance and better generalization. This type of regularization is intuitive and is well motivated by neuroscience [19, 1] and the organization of complex systems.\nOur approach departs from current models that use simple template matching and leverages the expressive power of Sparse Approximation. This provides a disciplined prescription for learning features at every level. Finally, HSR is a conceptually simple model. This elegance lends itself well to analysis."}, {"heading": "2.6 Discussion of Trade-offs in HSR", "text": "There are several design decisions when constructing an HSR. Informally, the hierarchy is useful for reducing the sample complexity and dimensionality of the problem. For instance consider the simplified case of binary {0, 1} coefficients and translation invariance (such as vision). An HSR generative model of two layers will produce ( m2 s2 ) patterns. Learning this with a single layer HSR would involve learning a dictionary of m2 columns and s2 sparsity using |X| samples in dimension d. Using two layers, we have the first layer learning a dictionary of size m1 and sparsity s1 using k \u2217 |X| samples in dimension dk , the second layer learns a dictionary of size m2 columns and s2 with |X| samples in dimension k \u2217O(s1 \u2217 logm1) < d. In effect, by adding a layer, we have divided the problem into two simpler problems in terms of dimensionality and sample complexity. A more formal and complete discussion will be presented in future work."}, {"heading": "3 Experiments", "text": "In this section we elaborate on preliminary numerical experiments performed with DRDL and HSR on basic standard Machine Learning datasets. We applied our model to the MNIST and COIL datasets and subsequently used the representation as a feature extraction step for a classification algorithm such as Support Vector Machines (SVM) or k-Nearest Neighbors (kNN). In practice, additional prior assumptions can be included in our model as discussed in the appendix."}, {"heading": "3.1 MNIST Results", "text": "We applied our model to all pairs of the MNIST data set. For the RIP step, we tried random matrices and sparse-random matrices, and tested the efficacy of the approach by reconstructing the training data with basis pursuit. We used\nonly two layers. After the features are learned we applied a k-NN with k = 3. We refrained from tweaking the initial parameters since we expect our model to work off-shelf. For one layer of DRDL we obtained an error rate of 1.24% with a standard deviation of 0.011. Using two layers we obtained an error of 2.01% and standard deviation of 0.016."}, {"heading": "3.2 COIL Results", "text": "We applied our model to all pairs of the COIL-30 (which is a subset of 30 objects out of the entire 100 objects in the COIL datasets). The data set consists of 72 images for every class. We used only 4 labeled images per class for training. These are taken at equally spaced angles (0, 90, 180, 270). We used the same procedure as before for obtaining and checking the RIP matrix. We applied a single layer DRDRL, then trained a k-NN with k = 3. We also refrained from tweaking the initial parameters. We obtained a mean error of 12.2%."}, {"heading": "4 Discussion", "text": "We introduced a novel formulation of an elemental building block that could serve as a the bottom-up piece in the common cortical algorithm. This model leads to several interesting theoretical questions. In the appendix, we illustrate how additional prior assumptions on the generative model can be expressed within our integrated framework. Furthermore, this framework can also be extended to address feedback, attention, action, complimentary learning and the role of time. In future work, we will closely probe these issues, propose\nalternative dictionary learning algorithms, dimension reduction techniques, and incorporate feedback."}, {"heading": "Acknowledgements", "text": "We benefited from discussions with several neuroscientists, specifically we would like to thank Dr Linda Hermer."}, {"heading": "Appendix: Expressing Prior Assumptions", "text": "Additional assumptions reflecting our knowledge of the prior can be explicitly considered within the DRDL and HSR framework. Additional prior information can be separately encoded at different levels in the hierarchical topology and in the Dictionary Learning and Dimension Reduction steps. One type of knowledge is through invariances specified on the data. Invariances can be thought of as transformations that map our data to itself. Specifically a set of points X is invariant under a transformation I iff I(X) = X. These assumptions can be encoded to:"}], "references": [{"title": "Emergence of simple cell receptive field properties by learning a sparse code for natural", "author": ["Bruno a Olshausen", "David J Field"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Computer Science and Artificial Intelligence Laboratory Technical Report Generalization and Properties of the Neural Response Generalization and Properties of the Neural Response", "author": ["Jake Bouvrie", "Tomaso Poggio", "Lorenzo Rosasco", "Steve Smale", "Andre Wibisono"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "What and where: A Bayesian inference theory of attention", "author": ["Sharat S. Chikkerur", "Thomas Serre", "Cheston Tan", "Tomaso Poggio"], "venue": "Vision Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "An integrated model of visual attention using shape-based features", "author": ["Sharat S. Chikkerur", "Cheston Tan", "Thomas Serre", "Tomaso Poggio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "On the Prospects for Building a Working Model of the Visual Cortex", "author": ["Thomas Dean", "Glenn Carroll", "Richard Washington"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Hierarchical models in the brain", "author": ["Karl J Friston"], "venue": "PLoS computational biology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Reinforcement learning or active inference", "author": ["Karl J Friston", "Jean Daunizeau", "Stefan J Kiebel"], "venue": "PloS one,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Action and behavior: a free-energy formulation", "author": ["Karl J Friston", "Jean Daunizeau", "James Kilner", "Stefan J Kiebel"], "venue": "Biological cybernetics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Action understanding and active inference", "author": ["Karl J Friston", "J\u00e9r\u00e9mie Mattout", "James Kilner"], "venue": "Biological cybernetics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Towards a mathematical theory of cortical micro-circuits", "author": ["Dileep George", "Jeff Hawkins"], "venue": "PLoS computational biology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sparse Recovery Using Sparse Matrices", "author": ["Anna Gilbert", "Piotr Indyk"], "venue": "Proceedings of the IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Hierarchical Bayesian inference in the visual cortex", "author": ["Tai Sing Lee", "David Mumford"], "venue": "Journal of the Optical Society of America. A, Optics, image science, and vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Perceptual Neuroscience: The Cerebral Cortex", "author": ["Vernon B. Mountcastle"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["Bruno A Olshausen", "D J Field"], "venue": "Vision research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M Riesenhuber", "Tomaso Poggio"], "venue": "Nature neuroscience,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "A quantitative theory of immediate visual recognition", "author": ["Thomas Serre", "Gabriel Kreiman", "Minjoon Kouh", "Charles Cadieu", "Ulf Knoblich", "Tomaso Poggio"], "venue": "Brain, 165:33\u201356,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "A feedforward architecture accounts for rapid categorization", "author": ["Thomas Serre", "Aude Oliva", "Tomaso Poggio"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["Thomas Serre", "Lior Wolf", "Stanley Bileschi", "Maximilian Riesenhuber", "Tomaso Poggio"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "An integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12].", "startOffset": 91, "endOffset": 113}, {"referenceID": 6, "context": "An integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12].", "startOffset": 91, "endOffset": 113}, {"referenceID": 7, "context": "An integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12].", "startOffset": 91, "endOffset": 113}, {"referenceID": 8, "context": "An integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12].", "startOffset": 91, "endOffset": 113}, {"referenceID": 12, "context": "1 Background Literature In A Glance Speculation on a cortical micro-circuit element dates back to Mountcastle\u2019s observation that a cortical column may serve as an algorithmic building block of the neocortex [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 11, "context": "Later work by Lee and Mumford [16], Hawkins and George [14] attempted further investigation of this process.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Later work by Lee and Mumford [16], Hawkins and George [14] attempted further investigation of this process.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 29, "endOffset": 45}, {"referenceID": 15, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 29, "endOffset": 45}, {"referenceID": 16, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 29, "endOffset": 45}, {"referenceID": 17, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 29, "endOffset": 45}, {"referenceID": 4, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 52, "endOffset": 58}, {"referenceID": 13, "context": "Work on modeling early stages of sensory processing by Olshausen [19, 1] using sparse coding produced results that account for the observed receptive fields in early visual processing.", "startOffset": 65, "endOffset": 72}, {"referenceID": 0, "context": "Work on modeling early stages of sensory processing by Olshausen [19, 1] using sparse coding produced results that account for the observed receptive fields in early visual processing.", "startOffset": 65, "endOffset": 72}, {"referenceID": 14, "context": "It is basically a hierarchical succession of template matching and a max-operations, corresponding to simple and complex cells respectively [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "Bouvrie et al [3, 2] introduced a generalization of hierarchical architectures centered around a foundational element involving two steps, Filtering and Pooling.", "startOffset": 14, "endOffset": 20}, {"referenceID": 5, "context": "Friston proposed Hierarchical Dynamic Models (HDM) which are similar to the above mentioned architectures but framed in a control theoretic framework operating in continuous time [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "Feedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9].", "startOffset": 124, "endOffset": 130}, {"referenceID": 2, "context": "Feedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9].", "startOffset": 124, "endOffset": 130}, {"referenceID": 7, "context": "Feedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9].", "startOffset": 176, "endOffset": 183}, {"referenceID": 6, "context": "Feedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9].", "startOffset": 176, "endOffset": 183}, {"referenceID": 10, "context": "Alternatively RIP matrices can be obtained using sparse random matrices [15].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "This type of regularization is intuitive and is well motivated by neuroscience [19, 1] and the organization of complex systems.", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "This type of regularization is intuitive and is well motivated by neuroscience [19, 1] and the organization of complex systems.", "startOffset": 79, "endOffset": 86}], "year": 2011, "abstractText": "This paper introduces an elemental building block which combines Dictionary Learning and Dimension Reduction (DRDL). We show how this foundational element can be used to iteratively construct a Hierarchical Sparse Representation (HSR) of a sensory stream. We compare our approach to existing models showing the generality of our simple prescription. We then perform preliminary experiments using this framework, illustrating with the example of an object recognition task using standard datasets. This work introduces the very first steps towards an integrated framework for designing and analyzing various computational tasks from learning to attention to action. The ultimate goal is building a mathematically rigorous, integrated theory of intelligence.", "creator": "LaTeX with hyperref package"}}}