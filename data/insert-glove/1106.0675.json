{"id": "1106.0675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2011", "title": "The FF Planning System: Fast Plan Generation Through Heuristic Search", "abstract": "We gilan-e describe and gangways evaluate the mink algorithmic techniques geoff that are used fincastle in 35-17 the thekkady FF planning system. timpanogos Like newenham the lattner HSP vishishtadvaita system, FF goalies relies on solvberg forward lidell state galego space search, o'neals using a lanier heuristic insect that estimates goal mat distances by ignoring delete \u00a8 lists. chicago Unlike cooperage HSP ' s heuristic, whisenant our wikileaks.org method communicators does gudenus not assume portocarrero facts to be acos independent. We introduce higuita a novel search cias strategy that chetan combines self-pollination hill - nuaman climbing side-projects with togawa systematic 27-30 search, and we dipeptidase show federalism how duplexing other powerful 44-141 heuristic gudrun information matory can be extracted checchi and used to rebelde prune muramatsu the panufnik search ledlie space. FF niinist\u00f6 was 109.87 the most evo successful jahangir automatic planner at birinus the officialdom recent jigsaw AIPS - rarefaction 2000 planning jongi competition. larochelle We review the results casoria of borle the wooed competition, give data for catalan other jabeen benchmark plimsouls domains, and yuuichi investigate the choto reasons 900-acre for the runtime performance mont\u00e9limar of FF taiwan compared to HSP.", "histories": [["v1", "Fri, 3 Jun 2011 14:55:02 GMT  (181kb)", "http://arxiv.org/abs/1106.0675v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["j hoffmann", "b nebel"], "accepted": false, "id": "1106.0675"}, "pdf": {"name": "1106.0675.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bernhard Nebel"], "emails": ["hoffmann@informatik.uni-freiburg.de", "nebel@informatik.uni-freiburg.de"], "sections": [{"heading": null, "text": "Journal of Arti cial Intelligence Research 14 (2001) 253-302 Submitted 2/01; published 5/01 The FF Planning System: Fast Plan Generation ThroughHeuristic SearchJ org Ho mann hoffmann@informatik.uni-freiburg.deBernhard Nebel nebel@informatik.uni-freiburg.deGeorges-K ohler-Allee, Geb. 52,79110 Freiburg, Germany AbstractWe describe and evaluate the algorithmic techniques that are used in the FF planningsystem. Like the HSP system, FF relies on forward state space search, using a heuristic thatestimates goal distances by ignoring delete lists. Unlike HSP's heuristic, our method doesnot assume facts to be independent. We introduce a novel search strategy that combineshill-climbing with systematic search, and we show how other powerful heuristic informa-tion can be extracted and used to prune the search space. FF was the most successfulautomatic planner at the recent AIPS-2000 planning competition. We review the resultsof the competition, give data for other benchmark domains, and investigate the reasons forthe runtime performance of FF compared to HSP.1. IntroductionOver the last few years we have seen a signi cant increase of the e ciency of planningsystems. This increase is mainly due to three new approaches in plan generation.The rst approach was developed by Blum and Furst (1995, 1997). In their seminal paperon the GRAPHPLAN system (Blum & Furst, 1995), they described a new plan generationtechnique based on planning graphs, which was much faster than any other technique knownat this time. Their paper started a whole series of research e orts that re ned this approachby making it even more e cient (Fox & Long, 1998; Kambhampati, Parker, & Lambrecht,1997) and by extending it to cope with more expressive planning languages (Koehler, Nebel,Ho mann, & Dimopoulos, 1997; Gazen & Knoblock, 1997; Anderson, Smith, & Weld, 1998;Nebel, 2000).The second approach is the planning as satis ability method, which translates planningto propositional satis ability (Kautz & Selman, 1996). In particular there is the hope thatadvances in the state of the art of propositional reasoning systems carry directly over toplanning systems relying on this technology. In fact, Kautz and Selman (1999) predictedthat research on planning methods will become super uous because the state of the art inpropositional reasoning systems will advance much faster than in planning systems.A third new approach is heuristic-search planning as proposed by Bonet and Ge ner(1998, 1999). In this approach a heuristic function is derived from the speci cation of theplanning instance and used for guiding the search through the state space. As demonstratedby the system FF (short for Fast-Forward) at the planning competition at AIPS-2000, thisapproach proved to be competitive. In fact, FF outperformed all the other fully automaticsystems and was nominated Group A Distinguished Performance Planning System at thecompetition.c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nHoffmann & NebelIn HSP (Bonet & Ge ner, 1998), goal distances are estimated by approximating solutionlength to a relaxation of the planning task (Bonet, Loerincs, & Ge ner, 1997). While FFuses the same relaxation for deriving its heuristics, it di ers from HSP in a number of im-portant details. Its base heuristic technique can be seen as an application of GRAPHPLANto the relaxation. This yields goal distance estimates that, in di erence to HSP's estimates,do not rely on an independence assumption. FF uses a di erent search technique than HSP,namely an enforced form of hill-climbing, combining local and systematic search. Finally,it employs a powerful pruning technique that selects a set of promising successors to eachsearch node, and another pruning technique that cuts out branches where it appears thatsome goal has been achieved too early. Both techniques are obtained as a side e ect of thebase heuristic method.Concerning the research strategy that FF is based on, we remark the following. A lot ofclassical planning approaches, like partial-order planning (McAllester & Rosenblitt, 1991)or planning graph analysis (Blum & Furst, 1997), are generic problem solving methods,developed following some theoretical concept, and tested on examples from the literatureafterwards. In our approach, exploring the idea of heuristic search, there is no such cleardistinction between development and testing. The search strategy, as well as the pruningtechniques, are generic methods that have been motivated by observing examples. Also,design decisions were made on the basis of careful experimentation. This introduces into thesystem a bias towards the examples used for testing during development. We were testingour algorithms on a range of domains often used in the planning literature. Throughoutthe paper, we will refer to domains that are frequently used in the literature, and to tasksfrom such domains, as benchmarks. In the development phase, we used benchmark examplesfrom the Assembly, Blocksworld, Grid, Gripper, Logistics,Mystery, Mprime, and Tireworlddomains. When describing our algorithms in the paper, we indicate the points where thosetesting examples played a role for design decision making.Planning is known to be PSPACE-complete even in its simplest form (Bylander, 1994).Thus, in the general case, there is no e cient algorithmic method. It is therefore worthwhileto look for algorithms that are e cient at least on restricted subclasses. To some extent,this idea has been pursued by posing severe syntactical restrictions to the planning taskspeci cations (Bylander, 1994). Our approach is complementary to this. Examining theexisting benchmarks, one nds that they, indeed, do not exploit the full expressivity of theunderlying planning formalism. Though they do not ful ll any obvious rigid syntactical re-strictions, almost none of them is particularly hard. In almost all of the existing benchmarkdomains, a non-optimal plan can, in principle, be generated in polynomial time. Usingthe benchmarks for inspiration during development, we have been able to come up with aheuristic method that is not provably e cient, but does work well empirically on a largeclass of planning tasks. This class includes almost all of the current planning benchmarks.Intuitively, the algorithms exploit the simple structure underlying these tasks. Our ongoingwork is concerned with nding a formal characterization of that \\simple\" structure, andthereby formalizing the class of planning tasks that FF works well on.Section 2 gives a schematic view on FF's system architecture, and Section 3 introducesour notational conventions for STRIPS domains. Sections 4 to 6 describe the base heuristictechnique, search algorithm, and pruning methods, respectively. Section 7 shows how thealgorithms are extended to deal with ADL domains. System performance is evaluated in254\nFast Plan Generation Through Heuristic SearchSection 8, demonstrating that FF generates solutions extremely fast in a large range ofplanning benchmark domains. In order to illustrate our intuitions on the kind of structurethat FF can exploit successfully, the section also gives examples of domains where themethod is less appropriate. Finally, to clarify the performance di erences between FF andHSP, the section describes a number of experiments we made in order to estimate which ofthe new algorithmic techniques is most useful. We show connections to related work at thepoints in the text where they apply, and overview other connections in Section 9. Section 10outlines our current avenue of research.2. System ArchitectureTo give the reader an overview of FF's system architecture, Figure 1 shows how FF's mostfundamental techniques are arranged. Enforced Hill-climbing Relaxed GRAPHPLAN Task Specification Solution / \"Fail\" State Goal Distance\nHelpful ActionsFigure 1: FF's base system architecture.The fundamental heuristic technique in FF is relaxed GRAPHPLAN, which we will de-scribe in Section 4. The technique gets called on every search state by enforced hill-climbing,our search algorithm. This is a forward searching engine, to be described in Section 5. Givena state, relaxed GRAPHPLAN informs the search with a goal distance estimate, and addi-tionally with a set of promising successors for the state, the helpful actions, to be describedin Section 6. Upon termination, enforced hill-climbing either outputs a solution plan, orreports that it has failed.On top of the base architecture shown in Figure 1, we have integrated a few optimizationsto cope with special cases that arose during testing: If a planning task contains states from which the goal is unreachable (dead ends,de ned in Section 5.2), then enforced hill-climbing can fail to nd a solution. In thatcase, a complete heuristic search engine is invoked to solve the task from scratch. In the presence of goal orderings, enforced hill-climbing sometimes wastes a lot of timeachieving goals that need to be cared for later on. Two techniques trying to avoidthis are integrated:{ Added goal deletion, introduced in Section 6.2, cuts out branches where somegoal has apparently been achieved too early.{ The goal agenda technique, adapted from work by Jana Koehler (1998), feedsthe goals to the planner in an order determined as a pre-process (Section 6.2.2).255\nHoffmann & Nebel3. Notational ConventionsFor introducing FF's basic techniques, we consider simple STRIPS planning tasks, as wereintroduced by Fikes and Nilsson (1971). Our notations are as follows.De nition 1 (State) A state S is a nite set of logical atoms.We assume that all operator schemata are grounded, i.e., we only talk about actions.De nition 2 (Strips Action) A STRIPS action o is a tripleo = (pre(o); add(o); del(o))where pre(o) are the preconditions of o, add(o) is the add list of o and del(o) is the deletelist of the action, each being a set of atoms. For an atom f 2 add(o), we say that o achievesf . The result of applying a single STRIPS action to a state is de ned as follows:Result(S; hoi) = ( (S [ add(o)) n del(o) pre(o) Sunde ned otherwiseIn the rst case, where pre(o) S, the action is said to be applicable in S. The result ofapplying a sequence of more than one action to a state is recursively de ned asResult(S; ho1; : : : ; oni) = Result(Result(S; ho1; : : : ; on 1i); honi):De nition 3 (Planning Task) A planning task P = (O;I;G) is a triple where O is theset of actions, and I (the initial state) and G (the goals) are sets of atoms.Our heuristic method is based on relaxed planning tasks, which are de ned as follows.De nition 4 (Relaxed Planning Task) Given a planning task P = (O;I;G). The re-laxation P 0 of P is de ned as P 0 = (O0;I;G), withO0 = f(pre(o); add(o); ;) j (pre(o); add(o); del(o)) 2 OgIn words, one obtains the relaxed planning task by ignoring the delete lists of all actions.Plans are simple sequences of actions in our framework.De nition 5 (Plan) Given a planning task P = (O;I;G). A plan is a sequence P =ho1; : : : ; oni of actions in O that solves the task, i.e., for which G Result(I; P ) holds. Anaction sequence is called a relaxed plan to P, i it solves the relaxation P 0 of P.256\nFast Plan Generation Through Heuristic Search4. GRAPHPLAN as a Heuristic EstimatorIn this section, we introduce the base heuristic method used in FF. It is derived by applyingGRAPHPLAN to relaxed planning tasks. The resulting goal distance estimates do not, likeHSP's estimates, rely on an independence assumption. We prove that the heuristic com-putation is polynomial, give some notions on how distance estimates can be kept cautious,and describe how the method can be implemented e ciently.Consider the heuristic method that is used in HSP (Bonet & Ge ner, 1998). Given aplanning task P = (O;I;G), HSP estimates for each state S that is reached in a forwardsearch the solution length of the task P 0S = (O0; S;G), i.e., the length of a relaxed plan thatachieves the goals starting out from S. As computing the optimal solution length to P 0S|which would make an admissible heuristic|is NP-hard (Bylander, 1994), the HSP estimateis a rough approximation based on computing the following weight values.weightS(f) := 8><>: 0 if f 2 Si if [mino2O;f2add(o)Pp2pre(o) weightS(p)] = i 11 otherwise (1)HSP assumes facts to be achieved independently in the sense that the weight of a setof facts|an action's preconditions|is estimated as the sum of the individual weights. Thestate's heuristic estimate ish(S) := weightS(G) =Xg2GweightS(g) (2)Assuming facts to be achieved independently, this heuristic ignores positive interactionsthat can occur. Consider the following short example planning task, where the initial stateis empty, the goals are fG1; G2g, and there are the following three actions:name (pre; add; del)opG1 = (fPg; fG1g; ;)opG2 = (fPg; fG2g; ;)opP = (;; fPg; ;)HSP's weight value computation results in P having weight one, and each goal havingweight two. Assuming facts to be achieved independently, the distance of the initial stateto a goal state is therefore estimated to four. Obviously, however, the task is solvable inonly three steps, as opG1 and opG2 share the precondition P . In order to take account ofsuch positive interactions, our idea is to start GRAPHPLAN on the tasks (O0; S;G), andextract an explicit solution, i.e., a relaxed plan. One can then use this plan for heuristicevaluation. We will see in the next section that this approach is feasible: GRAPHPLANcan be proven to solve relaxed tasks in polynomial time.4.1 Planning Graphs for Relaxed TasksLet us examine how GRAPHPLAN behaves when it is started on a planning task that doesnot contain any delete lists. We brie y review the basic notations of the GRAPHPLANalgorithm (Blum & Furst, 1997). 257\nHoffmann & NebelA planning graph is a directed, layered graph that contains two kinds of nodes: factnodes and action nodes. The layers alternate between fact and action layers, where one factand action layer together make up a time step. In the rst time step, number 0, we havethe fact layer corresponding to the initial state and the action layer corresponding to allactions that are applicable in the initial state. In each subsequent time step i, we have thelayer of all facts that can possibly be made true in i time steps, and the layer of all actionsthat are possibly applicable given those facts.One crucial thing that GRAPHPLAN does when building the planning graph is theinference of mutual exclusion relations. A pair of actions (o; o0) at time step 0 is markedmutually exclusive, if o and o0 interfere, i.e., if one action deletes a precondition or an adde ect of the other. A pair of facts (f; f 0) at a time step i > 0 is marked mutually exclusive,if each action at level i 1 that achieves f is exclusive of each action at level i 1 thatachieves f 0. A pair of actions (o; o0) at a time step i > 0 is marked mutually exclusive,if the actions interfere, or if they have competing needs, i.e., if some precondition of o isexclusive of some precondition of o0. The planning graph of a relaxed task does not containany exclusion relations at all.Proposition 1 Let P 0 = (O0;I;G) be a relaxed STRIPS task. Started on P 0, GRAPH-PLAN will not mark any pair of facts or actions as mutually exclusive.Proof: The Proposition is easily proven by induction over the depth of the planning graph.Base case: time step 0. Only interfering actions are marked mutual exclusive at timestep 0. As there are no delete e ects, no pair of actions interferes.Inductive case: time step i ! time step i + 1. Per induction hypothesis, the facts arenot exclusive as their achievers one time step ahead are not. From this it follows that nopair of actions has competing needs. They do not interfere either. 2When started on a planning task, GRAPHPLAN extends the planning graph layer bylayer until a fact layer is reached that contains all goal facts, and in which no two goal factsare marked exclusive.1 Starting from that layer, a recursive backward search algorithm isinvoked. To nd a plan for a set of facts at layer i > 0, initialize the set of selected actionsat layer i 1 to the empty set. Then, for each fact, consider all achieving actions at layeri 1 one after the other and select the rst one that is not exclusive of any action that hasalready been selected. If there exists such an action, proceed with the next fact. If not,backtrack to the last fact and try to achieve it with a di erent action. If an achieving actionhas been selected for each fact, then collect the preconditions of all these actions to makeup a new set of facts one time step earlier. Succeed when fact layer 0|the initial state|isreached, where no achieving actions need to be selected. On relaxed tasks, no backtrackingoccurs in GRAPHPLAN's search algorithm.Proposition 2 Let P 0 = (O0;I;G) be a relaxed STRIPS task. Started on P 0, GRAPH-PLAN will never backtrack.Proof: Backtracking only occurs if all achievers for a fact f are exclusive of some alreadyselected action. With Proposition 1, we know that no exclusions exist, and thus, that this1. If no such fact layer can be reached, then the task is proven to be unsolvable (Blum & Furst, 1997).258\nFast Plan Generation Through Heuristic Searchdoes not happen. Also, if f is in graph layer i, then there is at least one achiever in layeri 1 supporting it. 2While the above argumentation is su cient for showing Proposition 2, it does not tellus much about what is actually going on when one starts GRAPHPLAN on a task withoutdelete lists. What happens is this. Given the task is solvable, the planning graph getsextended until some fact layer is reached that contains all the goals. Then the recursivesearch starts by selecting achievers for the goals at this level. The rst attempt succeeds, andnew goals are set up one time step earlier. Again, the rst selection of achievers succeeds,and so forth, until the initial state is reached. Thus, search performs only a single sweepover the graph, starting from the top layer going down to the initial layer, and collects arelaxed plan on its way. In particular, the procedure takes only polynomial time in the sizeof the task.Theorem 1 Let P 0 = (O0;I;G) be a solvable relaxed STRIPS task, where the length of thelongest add list of any action is l. Then GRAPHPLAN will nd a solution to P 0 in timepolynomial in l, jO0j and jIj.Proof: Building the planning graph is polynomial in l, jO0j, jIj and t, where t is the numberof time steps built (Blum & Furst, 1997). Now, in our case the total number jO0j of actionsis an upper limit to the number of time steps. This is just because after this number oftime steps has been built, all actions appear at some layer in the graph. Otherwise, there isa layer i where no new action comes in, i.e., action layer i 1 is identical to action layer i.As the task is solvable, this implies that all goals are contained in fact layer i, which wouldhave made the process stop right away. Similarly, action layer jO0j would be identical toaction layer jO0j 1, implying termination. The graph building phase is thus polynomialin l, jO0j and jIj.Concerning the plan extraction phase: With Proposition 2, search traverses the graphfrom top to bottom, collecting a set of achieving actions at each layer. Selecting achieversfor a set of facts is O(l jO0j+ jIj): A set of facts has at most size l jO0j+ jIj, the maximalnumber of distinct facts in the graph. An achieving action can be found to each fact inconstant time using the planning graph. As the number of layers to be looked at is O(jO0j),search is polynomial in the desired parameters. 2Starting GRAPHPLAN on a solvable search state task (O0; S;G) yields|in polynomialtime, with Theorem 1|a relaxed solution hO0; : : : ; Om 1i, where each Oi is the set of actionsselected in parallel at time step i, and m is the number of the rst fact layer containingall goals. As we are interested in an estimation of sequential solution length, we de ne ourheuristic as follows. h(S) := Xi=0;:::;m 1 jOij (3)The estimation values obtained this way are, on our testing examples, usually lowerthan HSP's estimates (Equations 1 and 2), as extracting a plan takes account of positiveinteractions between facts. Consider again the short example from the beginning of thissection, empty initial state, two goals fG1; G2g, and three actions:259\nHoffmann & Nebelname (pre; add; del)opG1 = (fPg; fG1g; ;)opG2 = (fPg; fG2g; ;)opP = (;; fPg; ;)Starting GRAPHPLAN on the initial state, the goals are contained in fact layer two,causing selection of opG1 and opG2 in action layer one. This yields the new goal P at factlayer one, which is achieved with opP . The resulting plan is hf opP g; f opG1, opG2 gi,giving us the correct goal distance estimate three, as distinct from HSP's estimate four.4.2 Solution Length OptimizationWe use GRAPHPLAN's heuristic estimates, Equation 3, in a greedy strategy, to be intro-duced in Section 5.1, that does not take its decisions back once it has made them. From ourexperience with running this strategy on our testing examples, this works best when distanceestimates are cautious, i.e., as low as possible. As already said, an optimal sequential solu-tion can not be synthesized e ciently. What one can do is apply some techniques to makeGRAPHPLAN return as short solutions as possible. Below, we describe some ways of doingthat. The rst technique is a built-in feature of GRAPHPLAN and ensures a minimalitycriterion for the relaxed plan. The two other techniques are heuristic optimizations.4.2.1 NOOPs-firstThe original GRAPHPLAN algorithm makes extensive use of so-called NOOPs. These aredummy actions that simply propagate facts from one fact layer to the next. For each factf that gets inserted into some fact layer, a NOOP corresponding to that fact is insertedinto the action layer at the same time step. This NOOP has no other e ect than addingf , and no other precondition than f . When performing backward search, the NOOPs areconsidered just like any other achiever, i.e., one way of making a fact true at time i > 0 isto simply keep it true from time i 1.In GRAPHPLAN, the implementation uses as a default the NOOPs- rst heuristic, i.e., ifthere is a NOOP present for achieving a fact f , then this NOOP is considered rst, beforethe planner tries selecting other \\real\" actions that achieve f . On relaxed tasks, the NOOPs- rst heuristic ensures a minimality criterion for the returned plan as follows.Proposition 3 Let (O0;I;G) be a relaxed STRIPS task, which is solvable. Using theNOOPs- rst strategy, the plan that GRAPHPLAN returns will contain each action at mostonce.Proof: Let us assume the opposite, i.e., one action o occurs twice in the plan hO0; : : : ; Om 1ithat GRAPHPLAN nds. We have o 2 Oi and o 2 Oj for some layers i; j with i < j.Now, the action o has been selected at layer j to achieve some fact f at layer j + 1.As the algorithm is using the NOOPs- rst strategy, this implies that there is no NOOP forfact f contained in action layer j: otherwise, the NOOP|not action o|would have beenselected for achieving f .In contradiction to this, action layer j does indeed contain a NOOP for fact f . This isbecause action o already appears in action layer i < j. As f gets added by o, it appears in260\nFast Plan Generation Through Heuristic Searchfact layer i+ 1 j. Therefore, a NOOP for f is inserted in action layer i+ 1 j, and, inturn, will be inserted into each action layer i0 i+ 1. 24.2.2 Difficulty HeuristicWith the above argumentation, if we can achieve a fact by using a NOOP, we should dothat. The question is, which achiever should we choose when no NOOP is available? It iscertainly a good idea to select an achiever whose preconditions seem to be \\easy\". Fromthe graph building phase, we can obtain a simple measure for the di culty of an action'spreconditions as follows.di culty(o) := Xp2pre(o)minfi j p is member of the fact layer at time step ig (4)The di culty of each action can be set when it is rst inserted into the graph. Duringplan extraction, facing a fact for which no NOOP is available, we then simply select anachieving action with minimal di culty. This heuristic works well in situations where thereare several ways to achieve one fact, but some ways need less e ort than others.4.2.3 Action Set LinearizationAssume GRAPHPLAN has settled for a parallel set Oi of achievers at a time step i,i.e., achieving actions have been selected for all goals at time step i + 1. As we are onlyinterested in sequential solution length, we still have a choice on how to linearize the ac-tions. Some linearizations can lead to shorter plans than others. If an action o 2 Oi addsa precondition p of another action o0 2 Oi, then we do not need to include p in the new setof facts to be achieved one time step earlier, given that we restrict ourselves to execute obefore o0. The question now is, how do we nd a linearization of the actions that minimizesour new fact set? The corresponding decision problem is NP-complete.De nition 6 Let OPTIMAL ACTION LINEARIZATION denote the following problem.Given a set O of relaxed STRIPS actions and a positive integer K. Is there a one-to-onefunction f : O 7! f1; 2; : : : ; jOjg such that the number of unsatis ed preconditions whenexecuting the sequence hf 1(1); : : : ; f 1(jOj)i is at most K ?Theorem 2 Deciding OPTIMAL ACTION LINEARIZATION is NP-complete.Proof: Membership is obvious. Hardness is proven by transformation from DIRECTEDOPTIMAL LINEAR ARRANGEMENT (Even & Shiloach, 1975). Given a directed graphG = (V;A) and a positive integer K, the question is, does there exists a one-to-one func-tion f : V 7! f1; 2; : : : ; jV jg such that f(u) < f(v) whenever (u; v) 2 A and such thatP(u;v)2A(f(v) f(u)) K ?To a given directed graph, we de ne a set of actions as follows. For each node w inthe graph, we de ne an action in our set O. For simplicity of presentation, we identify theactions with their corresponding nodes. To begin with, we set pre(w) = add(w) = ; for allw 2 V . Then, for each edge (u; v) 2 A, we create new logical facts P (u;v)w and R(u;v)w forw 2 V . 261\nHoffmann & NebelUsing these new logical facts, we now adjust all precondition and add lists to expressthe constraint that is given by the edge (u; v). Say action u is ordered before action v ina linearization. We need to simulate the di erence between the positions of v and u. Todo this, we de ne our actions in a way such that the bigger this di erence is, the moreunsatis ed preconditions there are when executing the linearization. First, we \\punish\" allactions that are ordered before v, by giving them an unsatis ed precondition.pre(w) := pre(w) [ P (u;v)w for w 2 V; add(v) := add(v) [ fP (u;v)w j w 2 V gWith this de nition, the actions w ordered before v|and v itself|will have the unsatis edprecondition P (u;v)w , while those ordered after will get this precondition added by v. Thus,the number of unsatis ed preconditions we get here is exactly f(v).Secondly, we \\give a reward\" to each action that is ordered before u. We simply do thisby letting those actions add a precondition of u, which would otherwise go unsatis ed.add(w) := add(w) [R(u;v)w for w 2 V; pre(u) := pre(u) [ fR(u;v)w j w 2 V gThat way, we will have exactly jV j (f(u) 1) unsatis ed preconditions, namely the R(u;v)wfacts for all actions except those that are ordered before u.Summing up the number of unsatis ed preconditions we get for a linearization f , wearrive at X(u;v)2A(f(v) + jV j (f(u) 1)) = X(u;v)2A(f(v) f(u)) + jAj (jV j+ 1)We thus de ne our new positive integer K 0 := K + jAj (jV j+ 1).Finally, we make sure that actions u get ordered before actions v for (u; v) 2 A. We dothis by inserting new logical \\safety\" facts S(u;v)1 ; : : : ; S(u;v)K0+1 into v's precondition- and u'sadd list.pre(v) := pre(v) [ fS(u;v)1 ; : : : ; S(u;v)K0+1g; add(u) := add(u) [ fS(u;v)1 ; : : : ; S(u;v)K0+1gAltogether, a linearization f of our actions leads to at most K 0 unsatis ed preconditions ifand only if f satis es the requirements for a directed optimal linear arrangement. Obviously,the action set and K 0 can be computed in polynomial time. 2Our sole purpose with linearizing an action set in a certain order is to achieve asmaller number of unsatis ed preconditions, which, in turn, might lead to a shorter re-laxed solution.2 Thus, we are certainly not willing to pay the price that nding an optimallinearization of the actions is likely to cost, according to Theorem 2. There are a few meth-ods how one can approximate such a linearization, like introducing an ordering constrainto < o0 for each action o that adds a precondition of another action o0, and trying to linearizethe actions such that many of these constraints are met. During our experimentations, wefound that parallel actions adding each other's preconditions occur so rarely in our testingtasks that even approximating is not worth the e ort. We thus simply linearize all actionsin the order they get selected, causing almost no computational overhead at all.2. It should be noted here that using optimal action linearizations at each time step does not guarantee theresulting relaxed solution to be optimal, which would give us an admissible heuristic.262\nFast Plan Generation Through Heuristic Search4.3 E cient ImplementationWe have implemented our own version of GRAPHPLAN, highly optimized for solving re-laxed planning tasks. It exploits the fact that the planning graph of a relaxed task doesnot contain any exclusion relations (Proposition 1). Our implementation is also highly op-timized for repeatedly solving planning tasks which all share the same set of actions|thetasks P 0S = (O0; S;G) as described at the beginning of this section.Planning task speci cations usually contain some operator schemata, and a set of con-stants. Instantiating the schemata with the constants yields the actions to the task. Oursystem instantiates all operator schemata in a way such that all, and only, reachable actionsare built. Reachability of an action here means that, when successively applying operatorsto the initial state, all of the action's preconditions appear eventually. We then build whatwe call the connectivity graph. This graph consists of two layers, one containing all (reach-able) actions, and the other all (reachable) facts. From each action, there are pointers toall preconditions, add e ects and delete e ects. All of FF's computations are e cientlyimplemented using this graph structure. For the subsequently described implementation ofrelaxed GRAPHPLAN, we only need the information about preconditions and add e ects.As a relaxed planning graph does not contain any exclusion relations, the only informa-tion one needs to represent it are what we call the layer memberships, i.e., for each fact oraction, the number of the rst layer at which it appears in the graph. Called on an interme-diate task P 0S = (O0; S;G), our version of GRAPHPLAN computes these layer membershipsby using the following xpoint computation. The layer memberships of all facts and actionsare initialized to1. For each action, there is also a counter, which is initialized to 0. Then,fact layer 0 is built implicitly by setting the layer membership of all facts f 2 S to 0. Eachtime when a fact f gets its layer membership set, all actions of which f is a preconditionget their counter incremented. As soon as the counter for an action o reaches the totalnumber of o's preconditions, o is put to a list of scheduled actions for the current layer.After a fact layer i is nished, all actions scheduled for step i have their layer membershipset to i, and their adds, if not already present, are put to the list of scheduled facts for thenext fact layer at time step i+1. Having nished with action layer i, all scheduled facts atstep i+ 1 have their membership set, and so on. The process continues until all goals havea layer membership lower than 1. It should be noticed here that this view of planninggraph building corresponds closely to the computation of the weight values in HSP. Thosecan be computed by applying the actions in layers as above, updating weight values andpropagating the changes each time an action comes in, and stopping when no changes occurin a layer. Having nished the relaxed version of planning graph building, a similarly trivialversion of GRAPHPLAN's solution extraction mechanism is invoked. See Figure 2.Instead of putting all goals into the top layer in GRAPHPLAN style, and then propa-gating them down by using NOOPs- rst, each goal g is simply put into a goal set Gi locatedat g's rst layer i. Then, there is a for-next loop down from the top to the initial layer. Ateach layer i, an achieving action with layer membership i 1 gets selected for each fact inthe corresponding goal set. If there is more than one such achiever, a best one is pickedaccording to the di culty heuristic. The preconditions are put into their correspondinggoal sets. Each time an action is selected, all of its adds are marked true at times i andi 1. The marker at time i prevents achievers to be selected for facts that are already true263\nHoffmann & Nebel for i := 1; : : : ;m doGi := fg 2 G j layer-membership(g) = igendforfor i := m; : : : ; 1 dofor all g 2 Gi; g not marked true at time i doselect an action o with g 2 add(o) and layer membership i 1, o's di culty being minimalfor all f 2 pre(o); layer-membership(f) 6= 0; f not marked true at time i 1 doGlayer-membership(f) := Glayer-membership(f) [ ffgendforfor all f 2 add(o) domark f as true at times i 1 and iendforendforendfor Figure 2: Relaxed plan extractionanyway. Marking at time i 1 assumes that actions are linearized in the order they getselected: A precondition that was achieved by an action ahead is not considered as a newgoal.5. A Novel Variation of Hill-climbingIn this section, we introduce FF's base search algorithm. We discuss the algorithm's theo-retical properties regarding completeness, and derive FF's overall search strategy.In the rst HSP version (Bonet & Ge ner, 1998), HSP1 as was used in the AIPS-1998 competition, the search strategy is a variation of hill-climbing, always selecting onebest successor to the state it is currently facing. Because state evaluations are costly, wealso chose to use local search, in the hope to reach goal states with as few evaluations aspossible. We settled for a di erent search algorithm, an \\enforced\" form of hill-climbing,which combines local and systematic search. The strategy is motivated by the simplestructure that the search spaces of our testing benchmarks tend to have.5.1 Enforced Hill-climbingDoing planning by heuristic forward search, the search space is the space of all reachablestates, together with their heuristic evaluation. Now, evaluating states in our testing bench-marks with the heuristic de ned by Equation 3, one often nds that the resulting searchspaces are simple in structure, speci cally, that local minima and plateaus tend to be small.For any search state, the next state with strictly better heuristic evaluation is usually onlya few steps away (an example for this is the Logistics domain described in Section 8.1.1).Our idea is to perform exhaustive search for the better states. The algorithm is shown inFigure 3.Like hill-climbing, the algorithm depicted in Figure 3 starts out in the initial state.Then, facing an intermediate search state S, a complete breadth rst search starting out264\nFast Plan Generation Through Heuristic Search initialize the current plan to the empty plan <>S := Iwhile h(S) 6= 0 doperform breadth rst search for a state S0 with h(S0) < h(S)if no such state can be found thenoutput \"Fail\", stopendifadd the actions on the path to S0 at the end of the current planS := S0endwhile Figure 3: The enforced hill-climbing algorithm.from S is invoked. This nds the closest better successor, i.e., the nearest state S0 withstrictly better evaluation, or fails. In the latter case, the whole algorithm fails, in the formercase, the path from S to S0 is added to the current plan, and search is iterated. When agoal state|a state with evaluation 0|is reached, search stops.Our implementation of breadth rst search starting out from S is standard, where statesare kept in a queue. One search iteration removes the rst state S0 from the queue, andevaluates it by running GRAPHPLAN. If the evaluation is better than that of S, searchsucceeds. Otherwise, the successors of S0 are put to the end of the queue. Repeated statesare avoided by keeping a hash table of visited states in memory. If no new states can bereached anymore, breadth rst search fails.5.2 CompletenessIf in one iteration breadth rst search for a better state fails, then enforced hill-climbingstops without nding a solution. This can happen because once enforced hill-climbing haschosen to include an action in the plan, it never takes this decision back. The method istherefore only complete on tasks where no fatally wrong decisions can be made. These arethe tasks that do not contain \\dead ends.\"De nition 7 (Dead End) Let (O;I;G) be a planning task. A state S is called a deadend i it is reachable and no sequence of actions achieves the goal from it, i.e., i 9 P :S = Result(I; P ) and :9 P 0 : G Result(S; P 0).Naturally, a task is called dead-end free if it does not contain any dead end states. Weremark that being dead-end free implies solvability, as otherwise the initial state itself wouldalready be a dead end.Proposition 4 Let P = (O;I;G) be a planning task. If P is dead-end free, then enforcedhill-climbing will nd a solution.Proof: Assume enforced hill-climbing does not reach the goal. Then we have some inter-mediate state S = Result(I; P ), P being the current plan, where breadth rst search cannot improve on the situation. Now, h(S) > 0 as search has not stopped yet. If there was a265\nHoffmann & Nebelpath from S to some goal state S0, then complete breadth rst search would nd that path,obtain h(S0) = 0 < h(S), and terminate positively. Such a path can therefore not exist,showing that S is a dead end state in contradiction to the assumption. 2We remark that Proposition 4 holds only when h is a function from states to naturalnumbers including 0, where h(S) = 0 i G S. The proposition identi es a class of planningtasks where we can safely apply enforced hill-climbing. Unfortunately, it is PSPACE-hardto decide whether a given planning task belongs to that class.De nition 8 Let DEADEND-FREE denote the following problem:Given a planning task P = (O;I;G), is P dead-end free?Theorem 3 Deciding DEADEND-FREE is PSPACE-complete.Proof: Hardness is proven by polynomially reducing PLANSAT (Bylander, 1994)|thedecision problem of whether P is solvable|to the problem of deciding DEADEND-FREE.We simply add an operator to O that is executable in all states, and re-establishes theinitial state. O1 := O [ foI := h;;I; [o2O add(o) n IigApplying oI to any state reachable in P leads back to the initial state: all facts that canever become true are removed, and those in the initial state are added. Now, the modi edproblem P1 = (O1;I;G) is dead-end free i P is solvable. From left to right, if P1 is dead-end free, then it is solvable, which implies that P is solvable, as we have not added any newpossibility of reaching the goal. From right to left, if P is solvable, then also is P1, by thesame solution plan P . One can then, from all states in P1, achieve the goal by going backto the initial state with the new operator, and executing P thereafter.Membership in PSPACE follows from the fact that PLANSAT and its complement areboth in PSPACE. A non-deterministic algorithm that decides the complement of DEADEND-FREE and that needs only polynomial space can be speci ed as follows. Guess a state S.Verify in polynomial space that S is reachable from the initial state. Further, verify thatthe goal cannot be reached from S. If this algorithm succeeds, it follows that the instanceis not dead-end free|since S constitutes a dead end. This implies that DEADEND-FREEis in NPSPACE, and hence in PSPACE. 2Though we can not e ciently decide whether a given task is dead-end free, there areeasily testable su cient criteria in the literature. Johnsson et al. (2000) de ne a notionof symmetric planning tasks, which is su cient for dead-end freeness, but co-NP-complete.They also give a polynomial su cient criterion for symmetry. This is, however, very trivial.Hardly any of the current benchmarks ful lls it. Koehler and Ho mann (2000a) have de nednotions of invertible planning tasks|su cient for dead-end freeness, and inverse actions|su cient for invertibility, under certain restrictions. The existence of inverse actions, andsu cient criteria for the additional restrictions, can be decided in polynomial time. Manybenchmark tasks do, in fact, ful ll those criteria and can thus e ciently be proven dead-endfree. 266\nFast Plan Generation Through Heuristic SearchOne could adopt Koehler and Ho mann's methodology, and use the existence of inverseactions to recognize dead-end free tasks. If the test fails, one could then employ a di erentsearch strategy than enforced hill-climbing. We have two reasons for not going this way: Even amongst our benchmarks, there are tasks that do not contain inverse actions, butare nevertheless dead-end free. An example is the Tireworld domain, where enforcedhill-climbing leads to excellent results. Enforced hill-climbing can often quite successfully solve tasks that do contain deadends, as it does not necessarily get caught in one. Examples for that are contained inthe Mystery and Mprime domains, which we will look at in Section 8.2.1.The observation that forms the basis for our way of dealing with completeness is the follow-ing. If enforced hill-climbing can not solve a planning task, it usually fails very quickly. Onecan then simply switch to a di erent search algorithm. We have experimented with ran-domizing enforced hill-climbing, and doing a restart when one attempt failed. This didn'tlead to convincing results. Though we tried a large variety of randomization strategies, wedid not nd a planning task in our testing domains where one randomized restart did sig-ni cantly better than the previous one, i.e., all attempts su ered from the same problems.The tasks that enforced hill-climbing does not solve right away are apparently so full ofdead ends that one can not avoid those dead ends at random. We have therefore arrangedour overall search strategy in FF as follows:1. Do enforced hill-climbing until the goal is reached or the algorithm fails.2. If enforced hill-climbing failed, skip everything done so far and try to solve the taskby a complete heuristic search algorithm. In the current implementation, this is whatRussel and Norvig (1995) term greedy best- rst search. This strategy simply expandsall search nodes by increasing order of goal distance estimation.To summarize, FF uses enforced hill-climbing as the base search method, and a completebest- rst algorithm to deal with those special cases where enforced hill-climbing has runinto a dead end and failed.6. Pruning TechniquesIn this section, we introduce two heuristic techniques that can, in principle, be used toprune the search space in any forward state space search algorithm:1. Helpful actions selects a set of promising successors to a search state. As we willdemonstrate in Section 8.3, the heuristic is crucial for FF's performance on manydomains.2. Added goal deletion cuts out branches where some goal has apparently been achievedtoo early. Testing the heuristic, we found that it can yield savings on tasks thatcontain goal orderings, and has no e ect on tasks that don't.267\nHoffmann & NebelBoth techniques are obtained as a side e ect of using GRAPHPLAN as a heuristic esti-mator in the manner described in Section 4. Also, both of them do not preserve completenessof any hypothetical forward search. In the context of our search algorithm, we integratethem such that they prune the search space in the single enforced hill-climbing try|whichis not complete in general anyway|and completely turn them o during best- rst search,if enforced hill-climbing failed.6.1 Helpful ActionsTo a state S, we de ne a set H(S) of actions that seem to be most promising among theactions applicable in S. The technique is derived by having a closer look at the relaxed plansthat GRAPHPLAN extracts on search states in our testing tasks. Consider the Gripperdomain, as it was used in the 1998 AIPS planning competition. There are two rooms, Aand B, and a certain number of balls, which are all in room A initially and shall be movedinto room B. The planner controls a robot, which changes rooms via the move operator,and which has two grippers to pick or drop balls. Each gripper can hold only one ball ata time. We look at a small task where 2 balls must be moved into room B. Say the robothas already picked up both balls, i.e., in the current search state, the robot is in room A,and each gripper holds one ball. There are three applicable actions in this state: move toroom B, or drop one of the balls back into room A. The relaxed solution that our heuristicextracts is the following.< f move A B g,f drop ball1 B left,drop ball2 B right g >This is a parallel relaxed plan consisting of two time steps. The action set selected at the rst time step contains the only action that makes sense in the state at hand, move toroom B. We therefore pursue the idea of restricting the action choice in any planning stateto only those actions that are selected in the rst time step of the relaxed plan. We callthese the actions that seem to be helpful. In the above example state, this strategy cutsdown the branching factor from three to one.Sometimes, restricting oneself to only the actions that are selected by the relaxed plannercan be too much. Consider the following Blocksworld example. Say we use the well knownrepresentation with four operators, stack, unstack, pickup and putdown. The plannercontrols a single robot arm, and the operators can be used to stack one block on top ofanother one, unstack a block from another one, pickup a block from the table, or puta block that the arm is holding down onto the table. Initially, the arm is holding blockC, and blocks A and B are on the table. The goal is to stack A onto B. Started on thisstate, relaxed GRAPHPLAN will return one out of the following three time step optimalsolutions.< f putdown C g,f pickup A g,f stack A B g >or 268\nFast Plan Generation Through Heuristic Search< f stack C A g,f pickup A g,f stack A B g >or< f stack C B g,f pickup A g,f stack A B g >All of these are valid relaxed solutions, as in the relaxation it does not matter that stackingC onto A or B deletes facts that we still need. If C is on A, we can not pickup A anymore,and if C is on B, we can not stack A onto B anymore.The rst action in each relaxed plan is only inserted to get rid of C, i.e., free the robotarm, and from the point of view of the relaxed planner, all of the three starting actions dothe job. Thus the relaxed solution extracted might be any of the three above. If it happensto be the second or third one, then we lose the path to an optimal solution by restrictingourselves to the corresponding actions, stack C A or stack C B. Therefore, we de ne theset H(S) of helpful actions to a state S as follows.H(S) := fo j pre(o) S; add(o) \\G1(S) 6= ;g (5)Here, G1(S) denotes the set of goals that is constructed by relaxed GRAPHPLAN attime step 1|one level ahead of the initial layer|when started on the task (O0; S;G). Inwords, we consider as helpful actions all those applicable ones, which add at least one goalat the rst time step. In the above Blocksworld example, freeing the robot arm is amongthese goals, which causes all the three starting actions to be helpful in the initial state,i.e., to be elements of H(I). In the above Gripper example, the modi cation does notchange anything.The notion of helpful actions shares some similarities with what Drew McDermott callsthe favored actions (McDermott, 1996, 1999), in the context of computing greedy regressiongraphs for heuristic estimation. In a nutshell, greedy regression graphs backchain fromthe goals until facts are reached that are contained in the current state. Amongst otherthings, the graphs provide an estimation of which actions might be useful in getting closerto the goal: Those applicable ones which are members of the e ective subgraph, which isthe minimal cost subgraph achieving the goals.There is also a similarity between the helpful actions heuristic and what is known asrelevance from the literature (Nebel, Dimopoulos, & Koehler, 1997). Consider a Blocksworldtask where hundreds of blocks are on the table initially, but the goal is only to stack oneblock A on top of another block B. The set H(I) will in this case contain only the singleaction pickup A, throwing away all those applicable actions moving around blocks thatare not mentioned in the goal, i.e., throwing away all those actions that are irrelevant.The main di erence between the helpful actions heuristic and the concept of relevance isthat relevance in the usual sense refers to what is useful for solving the whole task. Beinghelpful, on the other hand, refers to something that is useful in the next step. This has thedisadvantage that the helpful things need to be recomputed for each search state, but the269\nHoffmann & Nebeladvantage that possibly far less things are helpful than are relevant. In our speci c setting,we get the helpful actions for free anyway, as a side e ect of running relaxed GRAPHPLAN.We conclude this subsection with an example showing that helpful actions pruning doesnot preserve completeness, and a few remarks on the current integration of the techniqueinto our search algorithm.6.1.1 CompletenessIn the following short example, the helpful actions heuristic prunes out all solutions fromthe state space. Say the initial state is fBg, the goals are fA;Bg, and there are the followingactions: name (pre; add; del)opA1 = (;; fAg; fBg)opA2 = (fPAg; fAg; ;)opPA = (;; fPAg; ;)opB1 = (;; fBg; fAg)opB2 = (fPBg; fBg; ;)opPB = (;; fPBg; ;)In this planning task, there are two ways of achieving the missing goal A. One of these,opA1, deletes the other goal B. The other one, opA2, needs the precondition PA to beachieved rst by opPA, and thus involves using two planning actions instead of one in the rst case. Relaxed GRAPHPLAN recognizes only the rst alternative, as it's the only timestep optimal one. The set of goals at the single time step created by graph construction isG1(I) = f A;B gThis gives us two helpful actions, namelyH(I) = f opA1;opB1 gOne of these, opB1, does not cause any state transition in the initial state. The other one,opA1, leads to the state where only A is true. To this state, we obtain the same set ofhelpful actions, containing, again, opA1 and opB1. This time, the rst action causes nostate transition, while the second one leads us back to the initial state. Helpful actionsthus cuts out the solutions from the state space of this example task. We remark that thetask is dead-end free|one can always reach A and B by applying opPA, opA2, opPB , andopB2|and that one can easily make the task invertible without changing the behavior.In STRIPS domains, one could theoretically overcome the incompleteness of helpfulactions pruning by considering not only the rst relaxed plan that GRAPHPLAN nds,but computing a kind of union over all relaxed plans that GRAPHPLAN could possibly nd, when allowing non time step optimal plans. More precisely, in a search state S,consider the relaxed task (O0; S;G). Extend the relaxed planning graph until fact level jO0jis reached. Set a goal set GjO0j at the top fact level to GjO0j := G. Then, proceed fromfact level jO0j 1 down to fact level 1, where, at each level i, a set Gi of goals is generated270\nFast Plan Generation Through Heuristic Searchas the union of Gi+1 with the preconditions of all actions in level i that add at least onefact in Gi+1. Upon termination, de ne as helpful all actions that add at least one fact inG1. It can be proven that, this way, the starting actions of all optimal solutions from Sare considered helpful. However, in all our STRIPS testing domains, the complete methodalways selects all applicable actions as helpful.6.1.2 Integration into SearchAs has already been noted at the very beginning of this section, we integrate helpful actionspruning into our search algorithm by only applying it during the single enforced hill-climbingtry, leaving the complete best- rst search algorithm unchanged (see Section 5). Facing astate S during breadth rst search for a better state in enforced hill-climbing, we look onlyat those successors generated by H(S). This renders our implementation of enforced hill-climbing incomplete even on invertible planning tasks. However, in all our testing domains,the tasks that cannot be solved by enforced hill-climbing using helpful actions pruning areexactly those that cannot be solved by enforced hill-climbing anyway.6.2 Added Goal DeletionThe second pruning technique that we introduce in this section is motivated by the ob-servation that in some planning domains there are goal ordering constraints, as has beenrecognized by quite a number of researchers in the past (Irani & Cheng, 1987; Drummond& Currie, 1989; Joslin & Roach, 1990). In our experiments on tasks with goal ordering con-straints, FF's base architecture sometimes wasted a lot of time achieving goals that neededto be cared for later on. We therefore developed a heuristic to inform search about goalorderings.The classical example for a planning domain with goal ordering constraints is the wellknown Blocksworld. Say we have three blocks A, B and C on the table initially, and wantto stack them such that we have B on top of C, and A on top of B. Obviously, thereis not much point in stacking A on B rst. Now, imagine a forward searching plannerconfronted with a search state S, where some goal G has just been achieved, i.e., S resultedfrom some other state by applying an action o with G 2 add(o). What one can ask in asituation like this is, was it a good idea to achieve G right now? Or should some othergoal be achieved rst? Our answer is inspired by recent work of Koehler and Ho mann(2000a), which argues that achieving G should be postponed if the remaining goals cannot be achieved without destroying G again. Of course, nding out about this involvessolving the remaining planning task. However, we can arrive at a very simple but|inour testing domains|surprisingly accurate approximation by using the relaxed plan thatGRAPHPLAN generates for the state S. The method we are using is as simple as this:If the relaxed solution plan, P , that GRAPHPLAN generates for S, contains an action o,o 2 P , that deletes G (G 2 del(o) in o's non-relaxed version), then we remove S from thesearch space, i.e., do not generate any successors to S. We call this method the added goaldeletion heuristic.Let us exemplify the heuristic with the above Blocksworld example. Say the plannerhas just achieved on(A,B), but with on(B,C) still being false, i.e., we are in the situation271\nHoffmann & Nebelwhere A is on top of B, and B and C are standing on the table. The relaxed solution thatGRAPHPLAN nds to this situation is the following.< f unstack A B g,f pickup B g,f stack B C g >The goal on(A,B), which has just been achieved, gets deleted by the rst action unstack AB. Consequently, we realize that stacking A onto B right now was probably a bad idea, andprune this possibility from the search space, which results in a solution plan that stacks Bonto C rst.Like in the preceding subsection, we conclude with an example showing that pruningsearch states in the manner described above does not preserve completeness, and with afew remarks on our current search algorithm implementation.6.2.1 CompletenessIn the following small example, one of the goals must be destroyed temporarily in order toachieve the other goal. This renders the planning task unsolvable when one is using theadded goal deletion heuristic. Say the initial state is empty, the goals are fA;Bg, and thereare the following actions: name (pre; add; del)opA = (;; fAg; ;)opB = (fAg; fBg; fAg)All solutions to this task need to apply opA, use opB thereafter, and re-establish A. Thecrucial point here is that Amust be temporarily destroyed. The added goal deletion heuristicis not adequate for such planning tasks. The example is dead-end free, and one can easilymake the scenario invertible without changing the behavior of the heuristic.Unlike for helpful actions, completeness can not be regained by somehow enumeratingall relaxed plans to a situation. In the above example, when A has been achieved but B isstill false, then all relaxed plans contain opB, deleting A.6.2.2 Integration into SearchWe use the added goal deletion heuristic in a way similar to the integration of the helpfulactions heuristic. As indicated at the very beginning of the section, it is integrated into thesingle enforced hill-climbing try that search does, and completely turned o during best- rstsearch, in case enforced hill-climbing didn't make it to the goal.We also use another goal ordering technique, taken from the literature. One of themost common approaches to dealing with goal orderings is trying to recognize them ina preprocessing phase, and then use them to prune fractions of the search space duringplanning (Irani & Cheng, 1987; Cheng & Irani, 1989; Joslin & Roach, 1990). This is alsothe basic principle underlying the so-called \\goal agenda\" approach (Koehler, 1998). Forour system, we have implemented a slightly simpli ed version of the goal agenda algorithm,and use it to further enhance performance. A very short summary of what happens is this.272\nFast Plan Generation Through Heuristic SearchIn a preprocessing phase, the planner looks at all pairs of goals and decides heuristicallywhether there is an ordering constraint between them. Afterwards, the goal set is split intoa totally ordered series of subsets respecting these orderings. These are then fed to enforcedhill-climbing in an incremental manner. Precisely, if G1; : : : ; Gn is the ordered series ofsubsets, enforced hill-climbing gets rst started on the original initial state and G1. If thatworks out, search ends in some state S satisfying the goals in G1. Enforced hill-climbing isthen called again on the new starting state S and the larger goal set G1 [G2. From a statesatisfying this, search gets started for the goals G1[G2[G3, and so on. The incremental, oragenda-driven, planning process can be applied to any planner, in principle, and preservescompleteness only on dead-end free tasks (Koehler & Ho mann, 2000a), i.e., again, we havean enhancement that loses completeness in general. Thus, we use the goal agenda only inenforced hill-climbing, leaving the complete best- rst search phase unchanged.The goal agenda technique yields runtime savings in domains where there are orderingconstraints between the goals. In our testing suite, these are the Blocksworld and theTireworld. In planning tasks without ordering constraints, the series of subsets collapsesinto a single entry, such that the agenda mechanism does not change anything there. Theruntime taken for the pre-process itself was neglectible in all our experiments.7. Extension to ADLSo far, we have restricted ourselves to planning tasks speci ed in the simple STRIPS lan-guage. We will now show how our approach can be extended to deal with ADL (Pednault,1989) tasks, more precisely, with the ADL subset of PDDL (McDermott et al., 1998) thatwas used in the 2nd international planning systems competition (Bacchus, 2000). Thisinvolves dealing with arbitrary function-symbol free rst order logic formulae, and withconditional e ects. Our extension work is divided into the following four subareas:1. Apply a preprocessing approach to the ADL domain and task description, compilingthe speci ed task down into a propositional normal form.2. Extend the heuristic evaluation of planning states to deal with these normal formconstructs.3. Adjust the pruning techniques.4. Adjust the search mechanisms.7.1 Preprocessing an ADL Planning TaskFF's preprocessing phase is almost identical to the methodology that has been developedfor the IPP planning system. For details, we refer the reader to the work that's been donethere (Koehler & Ho mann, 2000b), and give only the basic principles here.The planner starts with a planning task speci cation given in the subset of PDDLde ned for the AIPS-2000 planning competition (Bacchus, 2000). The input is a set ofoperator schemata, the initial state, and a goal formula. The initial state is simply a setof ground atoms, and the goal formula is an arbitrary rst order logical formula using therelational symbols de ned for the planning task. Any operator schema o is de ned by a273\nHoffmann & Nebellist of parameters, a precondition, and a list of e ects. Instantiating the parameters yields,just like STRIPS tasks are usually speci ed, the actions to the schema. The preconditionis an arbitrary ( rst order) formula. For an action to be applicable in a given state S, itsinstantiation of this formula must be satis ed in S. Each e ect i in the list has the form8yi0; : : : ; yini : ( i(o); addi(o);deli(o))Here, yi0; : : : ; yini are the e ect parameters, i(o) is the e ect condition|again, an arbitraryformula|and addi(o) and deli(o) are the atomic add and delete e ects, respectively. Theatomic e ects are sets of uninstantiated atoms, i.e., relational symbols containing variables.The semantics are that, if an instantiated action is executed, then, for each single e ect iin the list, and for each instantiation of its parameters, the condition i(o) is evaluated.If i(o) holds in the current state, then the corresponding instantiations of the atoms inaddi(o) are added to the state, and the instantiations of atoms in deli(o) are removed fromthe state.In FF's heuristic method, each single state evaluation can involve thousands of operatorapplications|building the relaxed planning graph, one needs to determine all applicableactions at each single fact layer. We therefore invest the e ort to compile the operatordescriptions down into a much simpler propositional normal form, such that heuristic eval-uation can be implemented e ciently. Our nal normal form actions o have the followingformat.Precondition: pre(o)E ects: (pre0(o); add0(o);del0(o)) ^(pre1(o); add1(o);del1(o)) ^...(prem(o); addm(o);delm(o))The precondition is a set of ground atoms. Likewise, the e ect conditions prei(o) of thesingle e ects are restricted to be ground atoms. We also represent the goal state as a set ofatoms. Thus, we compile away everything except the conditional e ects. Compiling awaythe logical formulae involves transforming them into DNF, which causes an exponentialblow up in general. In our testing domains, however, we found that this transformation canbe done in reasonable time. Concerning the conditional e ects, those can not be compiledaway without another exponential blow up, given that we want to preserve solution length.This was proven by Nebel (2000). As we will see, conditional e ects can e ciently beintegrated into our algorithmic framework, so there is no need for compiling them away.The compilation process proceeds as follows:1. Determine predicates that are static, in the sense that no operator has an e ect onthem. Such predicates are a common phenomenon in benchmark tasks. An exampleare the (in-city ?l ?c) facts in Logistics tasks: Any location ?l stays, of course, locatedwithin the same city ?c throughout the whole planning process. We recognize staticpredicates by a simple sweep over all operator schemata.274\nFast Plan Generation Through Heuristic Search2. Transform all formulae into quanti er-free DNF. This is subdivided into three steps:(a) Pre-normalize all logical formulae. Following Gazen and Knoblock (1997), thisprocess expands all quanti ers, and translates negations. We end up with for-mulae that are made up out of conjunctions, disjunctions, and atoms containingvariables.(b) Instantiate all parameters. This is simply done by instantiating all operator ande ect parameters with all type consistent constants one after the other. Theprocess makes use of knowledge about static predicates, in the sense that theinstantiated formulae can often be simpli ed (Koehler & Ho mann, 2000b). Forexample, if an instantiated static predicate (p ~a) occurs in a formula, and thatinstantiation is not contained in the initial state, then (p ~a) can be replaced withfalse.(c) Transform formulae into DNF. This is postponed until after instantiation, be-cause it can be costly, so it should be applied to as small formulae as possible.In a fully instantiated formula, it is likely that many static or one-way predi-cate occurrences can be replaced by true or false, resulting in a much simplerformula structure.3. Finally, if the DNF of any formula contains more than one disjunct, then the corre-sponding e ect, operator, or goal condition gets split up in the manner proposed byGazen and Knoblock (1997).7.2 Relaxed GRAPHPLAN with Conditional E ectsWe now show how our specialized GRAPHPLAN implementation, as was described inSection 4.3, is changed to deal with ADL constructs. Building on our normalized taskrepresentation, it su ces to take care of conditional e ects.7.2.1 Relaxed Planning Graphs with Conditional EffectsOur encoding of planning graph building for relaxed tasks almost immediately carries overto ADL actions in the above propositional normal form. One simply needs to keep anadditional layer membership value for all e ects of an action. The layer membership of ane ect indicates the rst layer where all its e ect conditions plus the corresponding action'spreconditions are present. To compute these membership integers in an e cient manner, wekeep a counter for each e ect i of an action o, which gets incremented each time a conditionc 2 prei(o) becomes present, and each time a precondition p 2 pre(o) of the correspondingaction becomes present. The e ect gets its layer membership set as soon as its counterreaches jprei(o)j+ jpre(o)j. The e ect's add e ects addi(o) are then scheduled for the nextlayer. The process is iterated until all goals are reached the rst time.7.2.2 Relaxed Plan Extraction with Conditional EffectsThe relaxed plan extraction mechanism for ADL di ers from its STRIPS counterpart inmerely two little details. Instead of selecting achieving actions, the extraction mechanismselects achieving e ects. Once an e ect i of action o is selected, all of its e ect conditions275\nHoffmann & Nebelplus o's preconditions need to be put into their corresponding goal sets. Afterwards, notonly the e ect's own add e ects addi(o) are marked true at the time being, but also theadded facts of all e ects that are implied, i.e., those e ects j of o with prej(o) prei(o) (inparticular, this will be the unconditional e ects of o, which have an empty e ect condition).7.3 ADL Pruning TechniquesBoth pruning techniques from Section 6 easily carry over to actions with conditional e ects.7.3.1 Helpful ActionsFor STRIPS, we de ned as helpful all applicable actions achieving at least one goal at timestep 1, cf. Section 6.1. For our ADL normal form, we simply change this to all applicableactions having an appearing e ect that achieves a goal at time step 1, where an e ect appearsi its e ect condition is satis ed in the current state.H(S) := fo j pre(o) S;9i : prei(o) S ^ addi(o) \\G1(S) 6= ;g (6)7.3.2 Added Goal DeletionOriginally, we cut o a state S if one of the actions selected for the relaxed plan to S deleteda goal A that had just been achieved, cf. Section 6.2. We now simply take as criterion thee ects that are selected for the relaxed plan, i.e., a state is cut o if one of the e ectsselected for its relaxed solution deletes a goal A that has just been achieved.7.4 ADL State TransitionsFinally, for enabling the search algorithms to handle our propositional ADL normal form,it is su cient to rede ne the state transition function. Forward search, no matter if it doeshill-climbing, best- rst search, or whatsoever, always faces a completely speci ed searchstate.3 It can therefore compute exactly the e ects of executing a context dependent action.Following Koehler et al.(1997), we de ne our ADL state transition function Res, mappingstates and ADL normal form actions to states, as follows.Res(S; o) = ( (S [A(S; o)) nD(S; o) if pre(o) Sunde ned otherwisewithA(S; o) = [prei(o) S addi(o) and D(S; o) = [prei(o) S deli(o) :3. This holds if the initial state is completely speci ed and all actions are deterministic, which we bothassume. 276\nFast Plan Generation Through Heuristic Search8. Performance EvaluationWe have implemented the methodology presented in the preceding sections in C.4 In thissection, we evaluate the performance of the resulting planning system. Empirical data isdivided into three subareas:1. The FF system took part in the fully automated track of the 2nd international plan-ning systems competition, carried out alongside with AIPS-2000 in Breckenridge, Col-orado. We review the results, demonstrating FF's good runtime and solution lengthbehavior in the competition. We also give some intuitions on why FF behaves theway it does.2. From our own experiments, we present some of the results that we have obtained indomains that were not used in the AIPS-2000 competition. First, we brie y summarizeour ndings in some more domains where FF works well. Then, to illustrate ourintuitions on the reasons for FF's performance, we give a few examples of domainswhere the approach is less appropriate.3. We nally present a detailed comparison of FF's performance to that of HSP, inthe sense that we investigate which di erences between FF and HSP lead to whichperformance results.8.1 The AIPS-2000 Planning Systems CompetitionFrom March to April 2000, the 2nd international planning systems competition, organizedby Fahiem Bacchus, was carried out in the general setting of the AIPS-2000 conference inBreckenridge, Colorado. There were two main tracks, one for fully-automated planners,and one for hand-tailored planners. Both tracks were divided into ve parts, each oneconcerned with a di erent planning domain. Our FF system took part in the fully automatedtrack. In the competition, FF demonstrated runtime behavior superior to that of the otherfully automatic planners and was therefore granted \\Group A distinguished performancePlanning System\" (Bacchus & Nau, 2001). It also won the Schindler Award for the rstplace in the Miconic 10 Elevator domain, ADL track. In this section, we brie y presentthe data collected in the fully automated track, and give, for each domain, some intuitionson the reasons for FF's behavior. The reader should be aware that the competition madeno distinction between optimal and suboptimal planners, putting together the runtimecurves for both groups. In the text to each domain, we state which planners found optimalsolutions, and which didn't. Per planning task, all planners were given half an hour runningtime on a 500 MHz Pentium III with 1GB main memory. If no solution was found withinthese resource bounds, the planner was declared to have failed on the respective task.8.1.1 The Logistics DomainThe rst two domains that were used in the competition were the Logistics and Blocksworlddomains. We rst look at the former. This is a classical domain, involving the transportation4. The source code is available in an online appendix, and can be downloaded from the FF Homepage athttp://www.informatik.uni-freiburg.de/~ ho mann/ .html.277\nHoffmann & Nebelof packets via trucks and airplanes. Figure 4 shows the runtime curves of those plannersthat were able to scale to bigger instances in the competition.\n0.1\n1\n10\n100\n1000\n10000\n16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\nse c.\nproblem size\nFF HSP2 System-R GRT Mips\nSTAN\nFigure 4: Runtime curves on large Logistics instances for those six planners that could scaleup to them. Time is shown on a logarithmic scale.The Logistics tasks were subdivided into two sets of instances, the easy and the harderones. Those planners that did well on all of the easy instances were also run on the harderset. These planners were FF, HSP2 (Bonet & Ge ner, 1998, 1999), System-R, GRT (Re-fanidis & Vlahavas, 1999), Mips (Edelkamp, 2000), and STAN (Long & Fox, 1999; Fox &Long, 2001). Two observations can be made:1. System-R does signi cantly worse than the other planners.2. The better planners all behave quite similar, with FF and Mips tending to be thefastest.Note also that times are shown on a logarithmic scale, so we are not looking at lineartime Logistics planners. Concerning solution plan length, we do not show a gure here.None of the shown planners guarantees the returned plans to be optimal. It turns out thatSTAN nds the shortest plans on most instances. System-R nds signi cantly longer plansthan the others, ranging from 178% to 261% of STAN's plan lengths, with an average of224%. The lengths of FF's plans are within 97% to 115% of STAN's plan lengths, with anaverage length of 105%. Concerning FF's good runtime behavior, we think that there aremainly two reasons for that: 278\nFast Plan Generation Through Heuristic Search1. In all iterations of enforced hill-climbing, breadth rst search nds a state with betterevaluation at very small depths (motivating our search algorithm, cf. Section 5.1). Inmost cases, the next better successor is at depth 1, i.e., a direct one. There are somecases where the shallowest better successor is at depth 2, and only very rarely breadth rst needs to go down to depth 3. These observations are independent of task size.2. The helpful actions heuristic prunes large fractions of the search space. Looking atthe states that FF encounters during search, only between 40 and 5 percent of all of astate's successors were considered helpful in our experiments, with the tendency thatthe larger the task, the less helpful successors there are.There is a theoretical note to be made on the rst observation. With the commonrepresentation of Logistics tasks, the following can be proven. Let d be the maximal distancebetween two locations, i.e., the number of move actions a mobile needs to take to get fromone location to another. Using a heuristic function that assigns to each state the length of anoptimal relaxed solution as the heuristic value, the distance of each state to the next betterevaluated state is maximal d+1. Thus, an algorithm that used enforced hill-climbingwith anoracle function returning the length of an optimal relaxed solution would be polynomial onstandard Logistics representations, given an upper limit to d. In the benchmarks available,mobiles can reach any location accessible to them in just one step, i.e., the maximal distancein those tasks is constantly d = 1. Also, FF's heuristic usually does nd optimal, or closeto optimal, relaxed solutions there, such that enforced hill-climbing almost never needs tolook more than d+ 1 = 2 steps ahead.8.1.2 The Blocksworld DomainThe Blocksworld is one of the best known benchmark planning domains, where the plannerneeds to rearrange a bunch of blocks into a speci ed goal position, using a robot arm. Justlike the Logistics tasks, the competition instances were divided into a set of easier, and ofharder ones. Figure 5 shows the runtime curves of the planners that scaled to the harderones.System-R scales most steadily to the Blocksworld tasks used in the competition. Inparticular, it is the only planner that can solve all of those tasks. HSP2 solves some of thesmaller instances, and FF solves about two thirds of the set. If FF succeeds on an instance,then it does so quite fast. For example, FF solves one of the size-50 tasks in 1:27 seconds,where System-R needs 892:31 seconds. None of the three planners nds optimal plans. Onthe tasks that HSP2 manages to solve, its plans are within 97% to 177% of System-R's planlengths, with an average of 153%. On the tasks that FF manages to solve, its plans arewithin 83% to 108% of System-R's plan lengths, average 96%.By experimenting with di erent con gurations of FF, we found that the behavior ofFF on these tasks is largely due to the goal ordering heuristics from Section 6.2. Goaldistance estimates are not so good|the planner grabs a whole bunch of blocks with itssingle arm|and neither is the helpful actions heuristic|when the arm holds a block, allpositions where the arm could possibly put the block are usually considered helpful. Thegoal agenda (Section 6.2.2), on the other hand, divides the tasks into small subtasks, andadded goal deletion (Section 6.2) prevents the planner from putting blocks onto stacks where279\nHoffmann & Nebel\n0.01\n0.1\n1\n10\n100\n1000\n10000\n18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50\nse c.\nproblem size\nFF HSP2 System-R\nFigure 5: Runtime curves on large Blocksworld instances for those three planners that couldscale up to them: FF, HSP2, and System-R. Time is shown an a logarithmic scale.some block beneath still needs to be moved. However, in some cases achieving the goalsfrom earlier entries in the goal agenda cuts o goals that are still ahead. Not aware of theblocks that it will need to stack for achieving goals ahead, the planner might put the currentblocks onto stacks that need to be disassembled later on. If that happens with too manyblocks|which depends more or less randomly on the speci c task and the actions that theplanner chooses|then the planner can not nd its way out of the situation again. Theseare probably the instances that FF couldn't solve in the competition.8.1.3 The Schedule DomainIn the Schedule domain, the planner is facing a bunch of objects to be worked on with a setof machines, i.e., the planner is required to create a job schedule in which the objects shallbe assigned to the machines. The competition representation makes use of a simple form ofquanti ed conditional e ects. For example, if an object gets painted red, then that is its newcolor, and for all colors that it is currently painted in, it is not of that color anymore. Onlya subset of the planners in the competition could handle this kind of conditional e ects.Their runtime curves are shown in Figure 6.Apart from those planners already seen, we have runtime curves in Figure 6 for IPP(Koehler et al., 1997), PropPlan, and BDDPlan (H olldobler & St orr, 2000). FF outper-forms the other planners by many orders of magnitude|remember that time is shown on alogarithmic scale. Concerning solution length, FF's plans tend to be slightly longer than the280\nFast Plan Generation Through Heuristic Search\n0.01\n0.1\n1\n10\n100\n1000\n10000\n2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50\nse c.\nproblem size\nFF HSP2 Mips IPP PropPlan BDDPlan\nFigure 6: Runtime curves on Schedule instances for those planners that could handle con-ditional e ects. Time is shown on a logarithmic scale.plans returned by the other planners on the smaller instances. Optimal plans are found byMips, PropPlan, and BDDPlan. FF's plan lengths are within 175% of the optimal lengths,with an average of 116%. Only HSP sometimes nds longer plans than FF, being in a rangefrom 62% to 117% of FF's plan lengths, 94% on average.Responsible for the outstanding runtime behavior of FF on the Schedule domain is,apparently, the helpful actions heuristic. Measuring, for some example states, the percent-age of successors that were considered helpful, we usually found it was close to 2 percent,i.e., only two out of a hundred applicable actions were considered by the planner. For ex-ample, all of the 637 states that FF looks at for solving one of the size-50 tasks have 523030successors altogether, where the sum of all helpful successors is only 7663. Also, the bettersuccessors, similar to the Logistics domain, lie at shallow depths. Breadth rst search nevergoes deeper than three steps on the Schedule tasks in the competition suite. Finally, ina few experiments we ran for testing that, the goal agenda helped by about a factor 2 interms of running time.8.1.4 The Freecell DomainThe Freecell domain formalizes a solitaire card game that comes with Microsoft Windows.The largest tasks entered in the competition (size 13 in Figure 7) correspond directly tosome real-world sized tasks, while in the smaller tasks, there are less cards to be considered.Figure 7 shows the runtime curves of the four best performing planners.281\nHoffmann & Nebel\nFast Plan Generation Through Heuristic Search8.1.5 The Miconic DomainThe nal domain used in the competition comes from a real-world application, where movingsequences of elevators need to be planned. The sequences are due to all kinds of restrictions,like that the VIPs need to be served rst. To formulate all of these restrictions, complex rstorder preconditions are used in the representation (Koehler & Schuster, 2000). As only afew planners could handle the full ADL representation, the domain was subdivided into theeasier STRIPS and SIMPLE (conditional e ects) classes, the full ADL class, and an evenmore expressive class where numerical constraints (the number of passengers in the elevatorat a time) needed to be considered. We show the runtime curves for the participants inthe full ADL class in Figure 8. In di erence to the previous domains, the Miconic domainwas run on site at AIPS-2000, using 450 MHz Pentium III machines with 256 MByte mainmemory.\n0.01\n0.1\n1\n10\n100\n1000\n10000\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29\nse c.\nproblem size\nFF IPP PropPlan\nFigure 8: Runtime curves on Elevator tasks for those planners which handled the full ADLMiconic 10 domain representation. Time is shown on a logarithmic scale.FF outperforms the two other full ADL planners in terms of solution time. It mustbe noticed, however, that IPP and PropPlan generate provably optimal plans here, suchthat one needs to be careful when directly comparing those running times. On the otherhand, FF's plans are quite close to optimal on these instances, being within in a range ofmaximally 133% of the optimal solution lengths to the instances solved by PropPlan, 111%on average.The large variation of FF's running times is apparently due to the same phenomenonas the variation in Freecell is: sometimes, as we observed, enforced hill-climbing runs into283\nHoffmann & Nebela dead end, which causes a switch to best- rst search, solving the task in more time, butreliably. The helpful actions percentage takes very low values on average, around 15%, andbreadth rst search rarely goes deeper than four or ve steps, where the large majority ofthe better successors lie at depth 1.8.2 Some more ExamplesIn this section, we present some of the results that we have obtained in domains that werenot used in the AIPS-2000 competition. We give some more examples of domains whereFF works well, and, to illustrate our intuitions on the reasons for FF's behavior, also someexamples of domains where FF is less appropriate.For evaluation, we ran FF on a collection of 20 benchmark planning domains, includingall domains from the AIPS-1998 and AIPS-2000 competitions, and seven more domains fromthe literature. Precisely, the domains in our suite were Assembly, two Blocksworlds (three-and four-operator representation), Briefcaseworld, Bulldozer, Freecell, Fridge, Grid, Grip-per, Hanoi, Logistics, Miconic-ADL, Miconic-SIMPLE, Miconic-STRIPS, Movie, Mprime,Mystery, Schedule, Tireworld, and Tsp. Instances were either taken from published dis-tributions, from the literature, or modi ed to show scaling behavior.5 Times for FF weremeasured on a Sparc Ultra 10 running at 350 MHz, with a main memory of 256 MBytes.Running times that we show for other planners were taken on the same machine, if not oth-erwise indicated in the text. We found that FF shows extremely competitive performanceon 16 of the 20 domains listed above. On the two Blocksworlds, Mprime, and Mystery, itstill shows satisfying behavior. Some examples that have not been used in the AIPS-2000competition are: The Assembly Domain. FF solves 25 of the 30 tasks in the AIPS-1998 test suite inless than ve seconds, where the ve others are either unsolvable, or have speci cationerrors. The only other planner we know of that can solve any of the Assembly tasks isIPP. The latest version IPP4.0 solves only four of the very small instances, taking upto 12 hours running time. FF's plan lengths are, in terms of the number of actions,shorter than IPP's time step optimal ones, ranging from 90% to 96%. The Briefcaseworld Domain. This is a classical domain, where n objects need to betransported using a briefcase. Whenever the briefcase is moved, a conditional e ectforces all objects inside the briefcase to move with it. From our suite, IPP4.0 easilysolves the tasks with n 5 objects, but fails to solve any task where n 7. FF, onthe other hand, solves even the 11-objects tasks in less than a second. On the tasksthat IPP solves, plan lengths of FF are within 84% to 111% of IPP's lengths, 99% onaverage. The Grid Domain. The 1998 competition featured ve instances. For these tasks,the fastest planning mechanism we know of from the literature is a version of GRTthat is enhanced with a simple kind of domain dependent knowledge, supplied by the5. All PDDL les, and random instance generators for all domains, are available in an online ap-pendix. The generators, together with descriptions of our randomization strategies, are also available athttp://www.informatik.uni-freiburg.de/~ ho mann/ -domains.html.284\nFast Plan Generation Through Heuristic Searchuser. It solves the tasks in 1:04, 6:63, 21:35, 19:92 and 118:65 seconds on a 300 MHzPentium Celeron machine with 64 MByte main memory (Refanidis & Vlahavas, 2000).FF solves the same tasks within 0:15, 0:47, 2:11, 1:93 and 19:54 seconds, respectively.Plan lengths of FF are within 89% to 139% of GRT's lengths, 112% on average. The Gripper Domain, used in the 1998 competition. The number of states that FFevaluates before returning an optimal sequential solution is linear in the size of thetask there. The biggest AIPS-1998 example gets solved in 0:16 seconds. The Tireworld Domain. The original task formulated by Stuart Russel asks the plan-ner to nd out how to replace a at tire. Koehler and Ho mann (2000a) modi ed thetask such that an arbitrary number of n tires need to be replaced. IPP3.2, using thegoal agenda technique, solves the 1, 2, and 3-tire tasks in 0:08, 0:21, and 1:33 seconds,respectively, but exhausts memory resources as soon as n 4. FF scales to muchlarger tasks, taking less than a tenth of a second when n 6, still solving the 10-tiretask in 0:33 seconds. FF's plan lengths are, on the tasks that IPP manages to solve,equally long in terms of the number of actions.As was already said, our intuition is that the majority of the currently available benchmarkplanning domains|at least those represented by our domain collection|are \\simple\" instructure, and that it is this simplicity which makes them solvable so easily by a greedyalgorithm such as FF. To illustrate our intuitions, we now give data for a few domains thathave a less simple structure. They are therefore challenging for FF.8.2.1 The Mystery and Mprime DomainsThe Mystery and Mprime domains were used in the AIPS-1998 competition. Both arevariations of the Logistics domain, where there are additional constraints on the capacityof each vehicle, and, in particular, on the amount of fuel that is available. Both domainsare closely related, the only di erence being that in Mprime, fuel items can be transportedbetween two locations, if one of those has more than one such item. In Figure 9, we compareFF's results on both domains to that reported by Drew McDermott for the Unpop system(McDermott, 1999).Instances are the same for both domains in Figure 9. Results for Unpop have been takenby McDermott on a 300 MHz Pentium-II workstation (McDermott, 1999). A dash indicatesthat the task couldn't be solved by the corresponding planner.One needs to be careful when comparing the running times in Figure 9: unlike FF, codedin C, Unpop is written in Lisp. Thus, the apparent runtime superiority of FF in Figure 9 isnot signi cant. On the contrary, Unpop seems to solve these task collections more reliablythan FF: it nds solutions to four Mystery and three Mprime instances which FF does notmanage to solve. None of the planners is superior in terms of solution lengths: On Mystery,FF ranges within 55% to 185% of Unpop's lengths, 103% on average, on Mprime, FF rangeswithin 45% to 150%, 93% on average.We think that FF's behavior on these two domains is due to the large amount of deadends in the corresponding state spaces|we tried to randomize FF's search strategy, runningit on the Mystery and Mprime suits. Regardless of the randomization strategy we tried, onthe tasks that original FF couldn't solve search ended up being stuck in a dead end. Dead285\nHoffmann & Nebel Mystery MprimeUnpop FF Unpop FFtask time steps time steps time steps time stepsprob-01 0.3 5 0.04 5 0.4 5 0.04 5prob-02 3.3 8 0.25 10 13.5 8 0.27 10prob-03 2.1 4 0.08 4 5.9 4 0.09 4prob-04 - - - - 3.9 9 0.04 10prob-05 - - - - 19.2 17 - -prob-06 - - - - - - - -prob-07 - - - - - - - -prob-08 - - - - 52.5 10 0.40 10prob-09 3.3 8 - - 13.5 8 0.16 10prob-10 - - - - 79.0 19 - -prob-11 1.4 11 0.05 9 2.9 11 0.06 9prob-12 - - - - 8.0 12 0.20 10prob-13 370.1 16 - - 89.3 15 0.16 10prob-14 162.1 18 - - - - - -prob-15 17.3 6 0.98 8 14.6 6 3.39 8prob-16 - - - - 25.2 13 0.28 7prob-17 13.1 5 0.70 4 4.0 5 0.92 4prob-18 - - - - - - - -prob-19 11.8 6 - - 24.7 6 0.99 9prob-20 22.5 7 0.41 13 62.8 17 3.11 13prob-21 - - - - 22.1 11 - -prob-22 - - - - 135.7 16 643.19 23prob-23 - - - - 55.0 18 3.09 14prob-24 - - - - 24.8 15 2.7 9prob-25 0.4 4 0.02 4 0.5 4 0.02 4prob-26 6.0 6 0.85 7 16.4 14 0.16 10prob-27 3.8 9 0.05 5 2.8 7 0.78 5prob-28 1.4 9 0.01 7 1.6 11 0.08 5prob-29 0.9 4 0.06 4 1.5 4 0.30 4prob-30 20.8 14 0.23 11 17.7 12 1.86 11Figure 9: Running times and solution length results on the AIPS-1998Mystery andMprimesuites.ends are a frequent phenomenon in the Mystery and Mprime domains, where, for example,an important vehicle can run out of fuel. In that sense, the tasks in these domains have amore complex structure than those in a lot of other benchmark domains, where the tasksare dead-end free. Depending more or less randomly on task structure and selected actions,FF can either solve Mystery and Mprime tasks quite fast, or fails, i.e., encounters a deadend state with enforced hill-climbing. Trying to solve the tasks with complete best- rstsearch exhausts memory resources for larger instances.8.2.2 Random SAT InstancesOur last example domain is not a classical planning benchmark. To give an example ofa planning task collection where FF really encounters di culties, we created a planningdomain containing hard random SAT instances. Figure 10 shows runtime curves for FF,IPP4.0, and BLACKBOX3.6.The tasks in Figure 10 are solvable SAT instances that were randomly generated accord-ing to the xed clause-length model with 4:3 times as many clauses as variables (Mitchell,Selman, & Levesque, 1992). Random instance generation and translation software to PDDLhave both been provided by Jussi Rintanen. Our gure shows running times for SAT in-stances with 5, 10, 15, 20, 25, and 30 variables, ve tasks of each size. Values for tasks ofthe same size are displayed in turn, i.e., all data points below 10 on the x-axis show runningtimes for 5 variable tasks, and so on. Though the data set is small, the observation to be286\nFast Plan Generation Through Heuristic Search\n0.01\n0.1\n1\n10\n100\n1000\n5 10 15 20 25 30\nse c.\n#variables\nFF IPP\nBLACKBOX\nFigure 10: Runtime curves for FF, IPP, and BLACKBOX, when run on hard random SATinstances with an increasing number of variables.made is clear: FF can only solve the small instances, and two of the bigger ones. IPP andBLACKBOX scale much better, with the tendency that BLACKBOX is fastest.The encoding of the SAT instances is the following. An operator corresponds to assigninga truth value to a variable, which makes all clauses true that contain the respective literal.Once a variable has been assigned, its value is xed. The goal is having all clauses true. Itis not surprising that BLACKBOX does best. After all, this planner uses SAT technologyfor solving the tasks.6 For IPP and FF, the search space is the space of all partial truthassignments. Due to exclusion relations, IPP can rule out quite many such assignmentsearly, when it nds they can't be completed. FF, on the other hand, does no such reasoning,and gets lost in the exponential search space, using a heuristic that merely tells it how manyvariables it will still need to assign truth values to, unaware of the interactions that might,and most likely will, occur.In contrast to most of the current benchmark planning domains, nding a non-optimalsolution to the planning tasks used here is NP-hard. FF's behavior on these tasks sup-ports our intuition that FF's e ciency is due to the inherent simplicity of the planningbenchmarks.6. In these experiments, we ran BLACKBOX with the default parameters. Most likely, one can boost theperformance by parameter tuning. 287\nHoffmann & Nebel8.3 What Makes the Di erence to HSP?One of the questions that the authors have been asked most frequently at the AIPS-2000planning competition is this: If FF is so closely related to HSP, then why does it perform somuch better? FF uses the same basic ideas as classical HSP, forward search in state space,and heuristic evaluation by ignoring delete lists (Bonet & Ge ner, 1998). The di erences liein the way FF estimates goal distances, the search strategy, and FF's pruning techniques.To obtain a picture of which new technique yields which performance results, we conducteda number of experiments where those techniques could be turned on and o independentlyof each other. Using all combinations of techniques, we measured runtime and solutionlength performance on a large set of planning benchmark tasks. In this section, we describethe experimental setup, and summarize our ndings. The raw data and detailed graphicalrepresentations of the results are available in an online appendix.8.3.1 Experimental SetupWe focused our investigation on FF's key features, i.e., we restricted our experiments to theFF base architecture, rather than taking into account all of FF's new techniques. Rememberthat FF's base architecture (cf. Section 2) is the enforced hill-climbing algorithm, using FF'sgoal distances estimates, and pruning the search space with the helpful actions heuristic.The additional techniques integrated deal with special cases, i.e., the added goal deletionheuristic and the goal agenda are concerned with goal orderings, and the complete best- rstsearch serves as a kind of safety net when local search has run into a dead end. Consideringall techniques independently would give us 26 = 64 di erent planner con gurations. As eachof the special case techniques yields savings only in a small subset (between 4 and 6) of our20 domains, large groups of those 64 con gurations would behave exactly the same on themajority of our domains. We decided to concentrate on FF's more fundamental techniques.The di erences between classical HSP and FF's base architecture are the following:1. Goal distance estimates: while HSP approximates relaxed solution lengths by com-puting certain weight values, FF extracts explicit relaxed solutions, cf. Section 4.2. Search strategy: while classical HSP employs a variation of standard hill-climbing,FF uses enforced hill-climbing as was introduced in Section 5.3. Pruning technique: while HSP expands all children of any search node, FF expandsonly those children that are considered helpful, cf. Section 6.1.We have implemented experimental code where each of these algorithmic di erences isattached to a switch, turning the new technique on or o . The eight di erent con gurationsof the switches yield eight di erent heuristic planners. When all switches are on, the result-ing planner is exactly FF's base architecture. With all switches o , our intention was toimitate classical HSP, i.e., HSP1 as it was used in the AIPS-1998 competition. Concerningthe goal distance estimates switch and the pruning techniques switch, we implemented theoriginal methods. Concerning the search strategy, we used the following simple hill-climbingdesign: Always select one best evaluated successor randomly.288\nFast Plan Generation Through Heuristic Search Keep a memory of past states to avoid cycles in the hill-climbing path. Count the number of consecutive times in which the child of a node does not improvethe heuristic estimate. If that counter exceeds a threshold, then restart, where thethreshold is 2 times the initial state's goal distance estimate. Keep visited nodes in memory across restart trials in order to avoid multiple compu-tation of the heuristic for the same state.In HSP1, some more variations of restart techniques are implemented. In personal com-munication with Blai Bonet and Hector Ge ner, we decided not to imitate those variations|which a ect behavior only in a few special cases|and use the simplest possible design in-stead. We compared the performance of our implementation with all switches turned o to the performance of HSP1, running the planners on 12 untyped STRIPS domains (theinput required for HSP1). Except in four domains, the tasks solved were the same for bothplanners. In Freecell and Logistics, our planner solved more tasks, apparently due to imple-mentation details: though HSP1 did not visit more states than our planner on the smallertasks, it ran out of memory on the larger tasks. In Tireworld and Hanoi, the restartingtechniques seem to make a di erence: In Tireworld, HSP1 cannot solve tasks with morethan one tire because it always restarts before getting close to the goal (our planner solvestasks with up to 3 tires), whereas in Hanoi our implementation can not cope with morethan 5 discs for the same reason (HSP1 solves tasks with up to 7 discs). Altogether, in mostcases there is a close correspondence between the behavior of HSP1 and our con gurationwith all switches turned o . In any case, our experiments provide useful insights into theperformance of enforced hill-climbing compared to a simple straightforward hill-climbingstrategy.To obtain data, we set up a large example suite, containing a total of 939 planning tasksfrom our 20 benchmark domains. As said at the beginning of Section 8.2, our domains wereAssembly, two Blocksworlds (three- and four-operator representation), Briefcaseworld, Bull-dozer, Freecell, Fridge, Grid, Gripper, Hanoi, Logistics, Miconic-ADL, Miconic-SIMPLE,Miconic-STRIPS, Movie, Mprime, Mystery, Schedule, Tireworld, and Tsp. In Hanoi, therewere 8 tasks|3 to 10 discs to be moved|in the other domains, we used from 30 to 69di erent instances. As very small instances are likely to produce noisy data, we tried toavoid those by rejecting tasks that were solved by FF in less than 0:2 seconds. This waspossible in all domains but Movie, where all tasks in the AIPS-1998 suite get solved in atmost 0:03 seconds. In the two Blocksworld representations, we randomly generated taskswith 7 to 17 blocks, using the state generator provided by John Slaney and Sylvie Thiebaux(2001). In Assembly and Grid, we used the AIPS-1998 instances, plus a number of ran-domly generated ones similar in size to the biggest examples in the competition suites. InGripper, our tasks contained from 10 to 59 balls to be transported. In the remaining 9competition domains, we used the larger instances of the respective competition suites. InBriefcaseworld and Bulldozer, we randomly generated around 50 large tasks, with 10 to 20objects, and 14 to 24 locations, respectively. In Fridge, from 1 to 14 compressors had to289\nHoffmann & Nebelbe exchanged, in Tireworld, 1 to 30 wheels needed to be replaced, and in Tsp, 10 to 59locations needed to be visited.7For each of the eight con gurations of switches, we ran the respective planner on eachof the tasks in our example suite. Those con gurations using randomized hill-climbingwere run ve times on each task, and the results averaged afterwards. Though ve trialsmight sound like a small number here|way too small if we were to compare di erent hill-climbing strategies for SAT problems, for example|the number seemed to be reasonableto us: remember that, in the planning framework, all hill-climbing trials start from thesame state. The variance that we found between di erent trials was usually low in ourtesting runs. To complete the experiments in a reasonable time, we restricted memoryconsumption to 128 MByte, and time consumption to 150 seconds|usually, if FF needsmore time or memory on a planning task of reasonable size, then it doesn't manage tosolve it at all. As said at the beginning of the section, the raw data is available in anonline appendix, accompanied by detailed graphical representations. Here, we summarizethe results, and discuss the most interesting observations. We examined the data separatelyfor each domain, as our algorithmic techniques typically show similar behavior for all taskswithin a domain. In contrast, there can be essential di erences in the behavior of the sametechnique when it is applied to tasks from di erent domains.8.3.2 Running TimeFor our running time investigation, if a con guration did not nd a solution plan to a giventask, we set the respective running time value to the time limit of 150 seconds (sometimes,a con guration can terminate faster without nding a plan, for example an enforced hill-climbing planner running into a dead end). In the following, we designate each switchcon guration by 3 letters: \\H\" stands for helpful actions on, \\E\" stands for enforced hill-climbing on, \\F\" stands for FF estimates on. If a switch is turned o , the respective letteris replaced by a \\ \": FF's base architecture is con guration \\HEF\", our HSP1 imitationis \\ \", and \\H \", for example, is hill-climbing with HSP goal distances and helpfulactions pruning. For a rst impression of our running time results, see the averaged valuesper domain in Figure 11.Figure 11 shows, for each domain and each con guration, the averaged running time overall instances in that domain. As the instances in each domain are not all the same size, buttypically scale from smaller to very large tasks, averaging over all running times is, of course,a very crude approximation of runtime behavior. The data in Figure 11 provides a generalimpression of our runtime results per domain, and gives a few hints on the phenomenathat might be present in the data. Compare, for example, the values on the right handside|those planners using helpful actions|to those on the left hand side|those plannersexpanding all sons of search nodes. In Briefcaseworld and Bulldozer, the right hand sidevalues are higher, but in almost all other domains, they are considerably lower. This isespecially true for the two rightmost columns, showing values for planners using helpfulactions and enforced hill-climbing. This indicates that the main sources of performance lie7. All PDDL les, and the source code of all instance generators we used, are available in an onlineappendix. The generators, together with descriptions of the randomization strategies, are also availableat http://www.informatik.uni-freiburg.de/~ ho mann/ -domains.html.290\nFast Plan Generation Through Heuristic Search F E EF H H F HE HEFAssembly 117.39 31.75 92.95 61.10 47.81 20.25 20.34 16.94Blocksworld-3ops 4.06 2.53 8.37 30.11 1.41 0.83 0.27 6.11Blocksworld-4ops 0.60 8.81 80.02 56.20 1.21 10.13 25.19 40.65Briefcaseworld 16.35 5.84 66.51 116.24 150.00 150.00 150.00 150.00Bulldozer 4.47 3.24 31.02 15.74 81.90 126.50 128.40 141.04Freecell 65.73 46.05 54.15 51.27 57.35 42.68 43.99 41.44Fridge 28.52 53.58 31.89 52.60 0.85 0.69 1.88 2.77Grid 138.06 119.53 115.05 99.18 115.00 95.10 18.73 11.73Gripper 2.75 1.21 15.16 1.00 1.17 0.48 0.17 0.11Hanoi 93.76 75.05 6.29 3.91 150.00 78.82 4.47 2.70Logistics 79.27 102.09 79.77 111.47 36.88 39.69 10.18 11.94Miconic-ADL 150.00 150.00 102.54 54.23 142.51 128.28 95.45 59.00Miconic-SIMPLE 2.61 2.01 2.47 1.93 1.35 0.86 0.55 0.56Miconic-STRIPS 2.71 2.32 4.84 1.53 1.44 1.01 0.64 0.36Movie 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02Mprime 73.09 69.27 82.89 81.43 47.09 58.45 18.56 26.62Mystery 78.54 90.55 71.60 86.01 75.73 95.24 85.13 86.21Schedule 135.50 131.12 143.59 141.42 77.58 38.23 12.23 13.77Tireworld 135.30 110.38 119.22 121.34 121.13 105.67 97.41 85.64Tsp 4.11 0.82 2.45 0.75 2.48 0.57 0.15 0.07Figure 11: Averaged running time per domain for all eight con gurations of switches.in the pruning technique and the search strategy|looking at the rightmost \\HE \" and\\HEF\" columns, which only di er in the goal distance estimate, those two con gurationvalues are usually close to each other, compared to the other con gurations in the samedomain.To put our observations on a solid basis, we looked, for each domain, at each pairof con gurations in turn, amounting to 20 8 72 = 560 pairs of planner performances.For each such pair, we decided whether one con guration performed signi cantly betterthan the other one. To decide signi cance, we counted the number of tasks that onecon guration solved faster. We found this to be a more reliable criterion than things likethe di erence between running times for each task. As tasks grow in size, rather thanbeing taken from a population with nite mean size, parametric statistical procedures,like computing con dence intervals for runtime di erences, make questionable assumptionsabout the distribution of data. We thus used the following non-parametric statistical test,known as the two-tailed sign test (Siegel & N. J. Castellan, 1988). Assume that bothplanners, A and B, perform equally on a given domain. Then, given a random instancefrom the domain, the probability that B is faster than A should be equal to the probabilitythat A is faster than B. Take this as the null hypothesis. Under that hypothesis, if A andB behave di erently on an instance, then B is faster than A with probability 12 . Thus, thetasks where B is faster are distributed over the tasks with di erent behavior according to aBinomial distribution with p = 12 . Compute the probability of the observed outcome underthe null hypothesis, i.e., if there are n tasks where A and B behave di erently, and k tasks291\nHoffmann & Nebelwhere B is faster, then compute the probability that, according to a binomial distributionwith p = 12 , at least k positive outcomes are obtained in n trials. If that probability isless or equal than :01, then reject the null hypothesis and say that B performs signi cantlybetter than A. Symmetrically, decide whether A performs signi cantly better than B. Weremark that in all domains exceptMovie the tasks where two con gurations behaved equallywere exactly those that could not be solved by either of the con gurations. In 60% of thecases where we found that one con guration B performed signi cantly better than anothercon guration, B was faster on all instances with di erent behavior. In 71%, B was fasteron all but one such instance.We are particularly interested in pairs A and B of con gurations where B results fromA by turning one of the switches on, leaving the two others unchanged. Deciding aboutsigni cant improvement in such cases tells us about the e ect that the respective techniquehas on performance in a domain. There are 12 pairs of con gurations where one switch isturned on. Figure 12 shows our ndings in these cases.F E Hdomain E H HE F H HF F E EFAssembly + + + + + + + + + +Blocksworld-3ops + + + + + + + + +Blocksworld-4ops + + +Briefcaseworld + Bulldozer + + Freecell + + + + + + + + + + + +Fridge + + + +Grid + + + + + + + + + +Gripper + + + + + + + + + + +Hanoi + + + + +Logistics + + + + + + +Miconic-ADL + + + + + + + + +Miconic-SIMPLE + + + + + + + + + + +Miconic-STRIPS + + + + + + + + + + +Movie + + + +Mprime + + + + + + + + +Mystery + + + +Schedule + + + + + + + +Tireworld + + + + + + + +Tsp + + + + + + + + + + + +Figure 12: The e ect of turning on a single switch, keeping the others unchanged. Summa-rized in terms of signi cantly improved or degraded running time performanceper domain, and per switch con guration.Figure 12 is to be understood as follows. It shows our results for the \\F\", \\E\", and \\H\"switches, which become active in turn from left to right. For each of these switches, thereare four con gurations of the two other, background, switches, displayed by four columns292\nFast Plan Generation Through Heuristic Searchin the table. In each column, the behavior of the respective background con guration withthe active switch turned o is compared to the behavior with the active switch turnedon. If performance is improved signi cantly, the table shows a \\+\", if it is signi cantlydegraded, the table shows a \\ \", and otherwise the respective table entry is empty. Forexample, consider the top left corner, where the \\F\" switch is active, and the backgroundcon guration is \\ \", i.e., hill-climbing without helpful actions. Planner A is \\ \",using HSP distances, and planner B is \\ F\", using FF distances. B's performance issigni cantly better than A's, indicated by a \\+\".The leftmost four columns in Figure 12 show our results for HSP distance estimates ver-sus FF distance estimates. Clearly, the latter estimates are superior in our domains, in thesense that, for each background con guration, the behavior gets signi cantly improved in 8to 10 domains. In contrast, there are only 5 cases altogether where performance gets worse.The signi cances are quite scattered over the domains and background con gurations, in-dicating that a lot of the signi cances result from interactions between the techniques thatoccur only in the context of certain domains. For example, performance is improved inBulldozer when the background con guration does not use helpful actions, but degradedwhen the background con guration uses hill-climbing with helpful actions. This kind of be-havior can not be observed in any other domain. There are 4 domains where performanceis improved in all but one background con guration. Apparently in these cases some inter-action between the techniques occurs only in one speci c con guration. We remark thatoften running times with FF's estimates are only a little better than with HSP's estimates,i.e., behavior gets improved reliably over all instances, but only by a small factor (to get anidea of that, compare the di erences between average running times in Figure 11, for con g-urations where only the distance estimate changes). In 5 domains, FF's estimates improveperformance consistently over all background con gurations, indicating a real advantageof the di erent distance estimates. In Gripper (described in Section 6.1), for example, wefound the following. If the robot is in room A, and holds only one ball, FF's heuristicprefers picking up another ball over moving to room B, i.e., the picking action leads to astate with better evaluation. Now, if there are n balls left in room A, then HSP's heuristicestimate of picking up another ball is 4n 2, while the estimate of moving to room B is3n + 1. Thus, if there are at least 4 balls left in room A, moving to room B gets a betterevaluation. Summing up weights, HSP overestimates the usefulness of the moving action.Comparing hill-climbing versus enforced hill-climbing, i.e., looking at the four columnsin the middle of Figure 12, the observation is this. The di erent search technique is a bitquestionable when the background con guration does not use helpful actions, but otherwise,enforced hill-climbing yields excellent results. Without helpful actions, performance getsdegraded almost as many times as it gets improved, whereas, with helpful actions, enforcedhill-climbing improves performance signi cantly in 16 of our 20 domains, being degradedonly in Fridge. We draw two conclusions. First, whether one or the other search strategy isadequate depends very much on the domain. A simple example for that is the Hanoi domain,where hill-climbing always restarts before it can reach the goal|on all paths to the goal,there are exponentially many state transitions where the son has no better evaluation thanthe father. Second, there is an interaction between enforced hill-climbing and helpful actionspruning that occurs consistently across almost all of our planning domains. This can beexplained by the e ect that the pruning technique has on the di erent search strategies.293\nHoffmann & NebelIn hill-climbing, helpful actions pruning prevents the planner from looking at too manysuper uous successors on each single state that a path goes through. This saves timeproportional to the length of the path. The e ects on enforced hill-climbing are much moredrastic. There, helpful actions prunes out unnecessary successors of each state during abreadth rst search, i.e., it cuts down the branching factor, yielding performance speedupsexponential in the depths that are encountered.We nally compare consideration of all actions versus consideration of only the helpfulones. Look at the rightmost four columns of Figure 12. The observation is simply thathelpful actions are really helpful|they improve performance signi cantly in almost all ofour planning domains. This is especially true for those background con gurations usingenforced hill-climbing, due to the same interaction that we have outlined above. In somedomains, helpful actions pruning imposes a very rigid restriction on the search space: inSchedule, as said in Section 8.1.3, we found that states can have hundreds of successors,where only about 2% of those are considered helpful. In other domains, only a few actionsare pruned, like in Hanoi, where at most three actions are applicable in each state, which areall considered helpful in most of the cases. Even a small degree of restriction does usuallylead to a signi cant improvement in performance. In two domains, Briefcaseworld andBulldozer, helpful actions can prune out too many possibilities, i.e., they cut away solutionpaths. This happens because there, the relaxed plan can ignore things that are crucial forsolving the real task. Consider the Briefcaseworld, brie y described in Section 8.2, whereobjects need to be moved using a briefcase. Whenever the briefcase is moved, all objectsinside it are moved with it by a conditional e ect. Now, the relaxed planner never needs totake any object out of the briefcase|the delete e ects say that moving an object means theobject is no longer at the start location. Ignoring this, keeping objects inside the briefcasenever hurts.8.3.3 Solution LengthWe also investigated the e ects that FF's new techniques have on solution length. Com-paring two con gurations A and B, we took as the data set the respective solution lengthfor those tasks that both A and B managed to solve|obviously, there is not much pointin comparing solution length when one planner can not nd a solution at all. We thencounted the number n of tasks where A and B behaved di erently, and the number k whereB's solution was shorter, and decided about signi cance like described in the last section.Figure 13 shows our results in those cases where a single switch is turned.The data in Figure 13 are organized in the obvious manner analogous to Figure 12. A rst glance at the table tells us that FF's new techniques are also useful for shorteningsolution length in comparison to HSP1, but not as useful as they are for improving runtimebehavior. Let us focus on the leftmost four columns, HSP distance estimates versus FF dis-tance estimates. The observations are that, with enforced hill-climbing in the background,FF estimates often result in shorter plans, and that there are two domains where solutionlengths are improved across all background con gurations. Concerning the second obser-vation, this is due to properties of the domain that FF's heuristic recognizes, but HSP'sdoesn't. Recall what we observed about the Gripper domain in the preceding section. Withthe robot standing in room A, holding only one ball, the FF heuristic gives picking up294\nFast Plan Generation Through Heuristic Search F E Hdomain E H HE F H HF F E EFAssembly + + + + + + +Blocksworld-3ops + + + + + +Blocksworld-4ops + + + + + + + +Briefcaseworld + + +Bulldozer + +Freecell + + + +Fridge + + + + +Grid + + + + + + +Gripper + + + + + HanoiLogistics + + + + + + + +Miconic-ADL + + + +Miconic-SIMPLE + + + + + + + + +Miconic-STRIPS + + + + + + + + + +Movie MprimeMysterySchedule + + +Tireworld + +TspFigure 13: The e ect of turning on a single switch, keeping the others unchanged. Summa-rized in terms of signi cantly improved or degraded solution length performanceper domain, and per switch con guration.the ball a better evaluation than moving to room B. The HSP heuristic doesn't do this.Therefore, using the HSP heuristic results in longer plans. Concerning the rst observation,improved solution lengths when enforced hill-climbing is in the background, we do not havea good explanation for this. It seems that the greedy way in which enforced hill-climbingbuilds its plans is just better suited when distance estimates are cautious, i.e., low.Consider the four columns in the middle of Figure 13, hill-climbing versus enforcedhill-climbing. There are many cases where the di erent search strategy results in shorterplans. We gure that this is due to the di erent plateau behavior that the search methodsexhibit, i.e., their behavior in at regions of the search space. Enforced hill-climbing entersa plateau somewhere, performs complete search for a state with better evaluation, andadds the shortest path to that state to its current plan pre x. When hill-climbing enters aplateau, it strolls around more or less randomly, until it hits a state with better evaluation,or has enough of it and restarts. All the actions on its journey to the better state arekept in the nal plan. In Movie, the phenomenon is this. If a planner chooses to resetthe counter on the VCR before it chooses to rewind the movie (initially, neither heuristicmakes a distinction between these two actions), then it has to reset the counter again. Theenforced hill-climbing planners always reset the counter rst. The hill-climbing planners,295\nHoffmann & Nebelon the other hand, randomly choose either ordering with equal probability. As said inSection 8.3.1, hill-climbing was given ve tries on each task, and results averaged. In vetries, around half of the solutions use the correct ordering, such that, for all tasks, theaverage value is lower than the corresponding value for the enforced hill-climbing planners.Finally, we compare consideration of all actions versus consideration of only the helpfulones, results depicted in the rightmost four columns of Figure 12. Coming a bit unexpected,there is only one single case where solution length performance is degraded by turning onhelpful actions. This indicates that the actions on the shortest path to the goal are, infact, usually considered helpful|unless all solution paths are thrown away, as is sometimesthe case only in the Briefcaseworld and Bulldozer domains. Quite the other way aroundthan one should think, pruning the search space with helpful actions sometimes leads tosigni cantly shorter solution plans, especially when the underlying search method is hill-climbing. Though this may sound paradoxical, there is a simple explanation to it. Considerwhat we said above about the plateau behavior of hill-climbing, randomly adding actionsto the current plan in the search for a better state. If such a search engine is armed withthe helpful actions successors choice, focusing it into the direction of the goals, it might welltake less steps to nd the way o a plateau.9. Related WorkThe most important connections of the FF approach to methodologies reported in theliterature are the following: HSP's basic idea of forward state space search and heuristic evaluation by ignoringdelete lists (Bonet & Ge ner, 1998). The view of our heuristic as a special case of GRAPHPLAN (Blum & Furst, 1995),and its connection to HSP's heuristic method. The similarity of the helpful actions heuristic to McDermott's favored actions (1996),and to irrelevance detection mechanisms (Nebel et al., 1997). The inspiration of the added goal deletion heuristic by work done by Koehler andHo mann (2000a), and the adaption of the goal agenda approach (Koehler, 1998). The adaption of IPP's ADL preprocessing phase (Koehler & Ho mann, 2000b), in-spired by ideas from Gazen and Knoblock (1997).We have discussed all of these connections in the respective sections already. So let usfocus on a connection that has not yet been mentioned. It has been recognized after the rst planning competition at AIPS-1998 that the main bottleneck in HSP1 is the recom-putation of the heuristic on each single search state. Two recent approaches are based onthe observation that the repeated recomputation is necessary because HSP1 does forwardsearch with a forward heuristic, i.e., the directions of search and heuristic are the same.The authors of HSP themselves stick to their heuristic, but change the search direction,going backwards from the goal in HSP-r (Bonet & Ge ner, 1999). This way, they need to296\nFast Plan Generation Through Heuristic Searchcompute weight values only once, estimating each fact's distance to the initial state, andonly sum the weights up for a state later during search.8Refanidis and Vlahavas (1999) invert the direction of the HSP heuristic instead. WhileHSP computes distances going from the current state towards the goal, GRT goes from thegoal to each fact. The function that then extracts, for each state during forward search,the states heuristic estimate, uses the pre computed distances as well as some informationon which facts will probably be achieved simultaneously.Interestingly, FF recomputes, like HSP, the heuristic from scratch on each search state,but nevertheless outperforms the other approaches. As we have seen in Section 8.3, this isfor the most part due to FF's search strategy and the helpful actions pruning technique.10. Conclusion and OutlookWe have presented an approach to domain independent planning that, at the time being,outperforms all existing technology on the majority of the currently available benchmarkdomains. Just like the well known HSP1 system, it relies completely on forward state spacesearch and heuristic evaluation of states by ignoring delete lists. Unlike HSP, the methoduses a GRAPHPLAN-style algorithm to nd an explicit relaxed solution to each searchstate. Those solutions give a more careful estimation of a state's di culty. As a secondmajor di erence to HSP, our system employs a novel local search strategy, combining hill-climbing with complete search. Finally, the method makes use of powerful heuristic pruningtechniques, which are based on examining relaxed solutions.As we have mentioned earlier, our intuition is that the reasons for FF's e ciency liein structural properties that the current planning benchmarks tend to have. As a matterof fact, the simplicity of the benchmarks quite immediately meets the eye, once one triesto look for it. It should be clear that the Gripper tasks, where some balls need to betransported from one room to another, exhibit a totally di erent search space structurethan, for example, hard random SAT instances. Therefore, it's intuitively unsurprising thatdi erent search methods are appropriate for the former tasks than are traditionally usedfor the latter. The e ciency of FF on many of the benchmarks can be seen as putting thatobservation to the surface.To make explicit the hypotheses stated above, we have investigated the state spaces ofthe planning benchmarks. Following Frank et al. (1997), we have collected empirical data,identifying characteristic parameters for di erent kinds of planning tasks, like the densityand size of local minima and plateaus in the search space. This has lead us to a taxonomyfor planning domains, dividing them by the degree of complexity that the respective task'sstate spaces exhibit with respect to relaxed goal distances. Most of the current benchmarkdomains apparently belong to the \\simpler\" parts of that taxonomy (Ho mann, 2001). Wealso approach our hypotheses from a theoretical point of view, where we measure the degreeof interaction that facts in a planning task exhibit, and draw conclusions on the search spacestructure from that. Our goal in that research is to devise a method that automaticallydecides which part of the taxonomy a given planning task belongs to.In that context, there are some remarks to be made on what AI planning research isheading for. Our point of view is that the goal in the eld should not be to develop a8. HSP-r is integrated into HSP2 as an option of con guring the search process (Bonet & Ge ner, 2001).297\nHoffmann & Nebeltechnology that works well on all kinds of tasks one can express with planning languages.This will hardly be possible, as even simple languages as STRIPS can express NP-hardproblems like SAT. What might be possible, however, is to devise a technology that workswell on those tasks that can be solved e ciently. In particular, if a planning task does notconstitute much of a problem to an uninformed human solver, then it neither should do soto our planning algorithms. With the FF system, we already seem to have a method thataccomplishes this quite well, at least for sequential planning in STRIPS and ADL. While FFis not particularly well suited for solving random SAT instances, it easily solves intuitivelysimple tasks like the Gripper and Logistics ones, and is well suited for a number of otherdomains where nding a non-optimal solution is not NP-hard. This sheds a critical lighton the predictions of Kautz and Selman (1999), who suspected that planning technologywill become super uous because of the fast advance of the state of the art in propositionalreasoning systems. The methods developed there are surely useful for solving SAT. Theymight, however, not be appropriate for the typical structures of tasks that AI planningshould be interested in.AcknowledgmentsThis article is an extended and revised version of a paper (Ho mann, 2000) that has beenpublished at ISMIS-00. The authors wish to thank Blai Bonet and Hector Ge ner for theirhelp in setting up the experiments on the comparison of FF with HSP. We thank JussiRintanen for providing us with software to create random SAT instances in the PDDLlanguage, and acknowledge the anonymous reviewer's comments, which helped improve thepaper.ReferencesAnderson, C. R., Smith, D. E., & Weld, D. S. (1998). Conditional e ects in Graphplan.In Simmons, R., Veloso, M., & Smith, S. (Eds.), Proceedings of the 4th InternationalConference on Arti cial Intelligence Planning Systems (AIPS-98), pp. 44{53. AAAIPress, Menlo Park.Bacchus, F. (2000). Subset of PDDL for the AIPS2000 Planning Competition. The AIPS-00Planning Competition Comitee.Bacchus, F., & Nau, D. (2001). The 2000 AI planning systems competition. The AIMagazine. Forthcoming.Blum, A. L., & Furst, M. L. (1995). Fast planning through planning graph analysis. In Pro-ceedings of the 14th International Joint Conference on Arti cial Intelligence (IJCAI-95), pp. 1636{1642 Montreal, Canada. Morgan Kaufmann.Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Arti cialIntelligence, 90 (1-2), 279{298.Bonet, B., & Ge ner, H. (1998). HSP: Heuristic search planner. In AIPS-98 PlanningCompetition Pittsburgh, PA. 298\nFast Plan Generation Through Heuristic SearchBonet, B., & Ge ner, H. (1999). Planning as heuristic search: New results. In Biundo,S., & Fox, M. (Eds.), Recent Advances in AI Planning. 5th European Conference onPlanning (ECP'99) Durham, UK. Springer-Verlag.Bonet, B., & Ge ner, H. (2001). Planning as heuristic search. Arti cial Intelligence. Forth-coming.Bonet, B., Loerincs, G., & Ge ner, H. (1997). A robust and fast action selection mecha-nism for planning. In Proceedings of the 14th National Conference of the AmericanAssociation for Arti cial Intelligence (AAAI-97), pp. 714{719. MIT Press.Bylander, T. (1994). The computational complexity of propositional STRIPS planning.Arti cial Intelligence, 69 (1{2), 165{204.Cheng, J., & Irani, K. B. (1989). Ordering problem subgoals. In Sridharan, N. S. (Ed.), Pro-ceedings of the 11th International Joint Conference on Arti cial Intelligence (IJCAI-89), pp. 931{936 Detroit, MI. Morgan Kaufmann.Drummond, M., & Currie, K. (1989). Goal ordering in partially ordered plans. In Sridha-ran, N. S. (Ed.), Proceedings of the 11th International Joint Conference on Arti cialIntelligence (IJCAI-89), pp. 960{965 Detroit, MI. Morgan Kaufmann.Edelkamp, S. (2000). Heuristic search planning with BDDs. In ECAI-Workshop: PuK.Even, S., & Shiloach, Y. (1975). NP-completeness of several arrangement problems. Tech.rep. 43, Department of Computer Science, Haifa, Israel.Fikes, R. E., & Nilsson, N. (1971). STRIPS: A new approach to the application of theoremproving to problem solving. Arti cial Intelligence, 2, 189{208.Fox, M., & Long, D. (1998). The automatic inference of state invariants in tim. Journal ofArti cial Intelligence Research, 9, 367{421.Fox, M., & Long, D. (2001). Hybrid STAN: Identifying and managing combinatorial op-timisation sub-problems in planning. In Proceedings of the 17th International JointConference on Arti cial Intelligence (IJCAI-01) Seattle, Washington, USA. MorganKaufmann. Accepted for publication.Frank, J., Cheeseman, P., & Stutz, J. (1997). When gravity fails: Local search topology.Journal of Arti cial Intelligence Research, 7, 249{281.Gazen, B. C., & Knoblock, C. (1997). Combining the expressiveness of UCPOP withthe e ciency of Graphplan. In Steel, S., & Alami, R. (Eds.), Recent Advances inAI Planning. 4th European Conference on Planning (ECP'97), Vol. 1348 of LectureNotes in Arti cial Intelligence, pp. 221{233 Toulouse, France. Springer-Verlag.Ho mann, J. (2000). A heuristic for domain independent planning and its use in an en-forced hill-climbing algorithm. In Proceedings of the 12th International Symposiumon Methodologies for Intelligent Systems (ISMIS-00), pp. 216{227. Springer-Verlag.299\nHoffmann & NebelHo mann, J. (2001). Local search topology in planning benchmarks: An empirical analysis.In Proceedings of the 17th International Joint Conference on Arti cial Intelligence(IJCAI-01) Seattle, Washington, USA. Morgan Kaufmann. Accepted for publication.H olldobler, S., & St orr, H.-P. (2000). Solving the entailment problem in the uent calculususing binary decision diagrams. In Proceedings of the First International Conferenceon Computational Logic (CL). To appear.Irani, K. B., & Cheng, J. (1987). Subgoal ordering and goal augmentation for heuristicproblem solving. In McDermott, J. (Ed.), Proceedings of the 10th International JointConference on Arti cial Intelligence (IJCAI-87), pp. 1018{1024 Milan, Italy. MorganKaufmann.Jonsson, P., Haslum, P., & B ackstr om, C. (2000). Planning - a randomized approach.Arti cial Intelligence, 117 (1), 1{29.Joslin, D., & Roach, J. W. (1990). A theoretical analysis of conjunctive-goal problems.Arti cial Intelligence, 41, 97{106.Kambhampati, S., Parker, E., & Lambrecht, E. (1997). Understanding and extendingGraphplan. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI Planning. 4thEuropean Conference on Planning (ECP'97), Vol. 1348 of Lecture Notes in Arti cialIntelligence, pp. 260{272 Toulouse, France. Springer-Verlag.Kautz, H., & Selman, B. (1999). Unifying SAT-based and graph-based planning. In Proceed-ings of the 16th International Joint Conference on Arti cial Intelligence (IJCAI-99),pp. 318{325 Stockholm, Sweden. Morgan Kaufmann.Kautz, H. A., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic,and stochastic search. In Proceedings of the 13th National Conference of the AmericanAssociation for Arti cial Intelligence (AAAI-96), pp. 1194{1201. MIT Press.Koehler, J. (1998). Solving complex planning tasks through extraction of subproblems. InSimmons, R., Veloso, M., & Smith, S. (Eds.), Proceedings of the 4th InternationalConference on Arti cial Intelligence Planning Systems (AIPS-98), pp. 62{69. AAAIPress, Menlo Park.Koehler, J., & Ho mann, J. (2000a). On reasonable and forced goal orderings and their usein an agenda-driven planning algorithm. Journal of Arti cial Intelligence Research,12, 338{386.Koehler, J., & Ho mann, J. (2000b). On the instantiation of ADL operators involvingarbitrary rst-order formulas. In Proceedings ECAI-00 Workshop on New Results inPlanning, Scheduling and Design.Koehler, J., Nebel, B., Ho mann, J., & Dimopoulos, Y. (1997). Extending planning graphsto an ADL subset. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI Plan-ning. 4th European Conference on Planning (ECP'97), Vol. 1348 of Lecture Notes inArti cial Intelligence, pp. 273{285 Toulouse, France. Springer-Verlag.300\nFast Plan Generation Through Heuristic SearchKoehler, J., & Schuster, K. (2000). Elevator control as a planning problem. In Chien,S., Kambhampati, R., & Knoblock, C. (Eds.), Proceedings of the 5th InternationalConference on Arti cial Intelligence Planning Systems (AIPS-00). AAAI Press, MenloPark.Long, D., & Fox, M. (1999). E cient implementation of the plan graph in stan. Journal ofArti cial Intelligence Research, 10, 87{115.McAllester, D. A., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedingsof the 9th National Conference of the American Association for Arti cial Intelligence(AAAI-91), pp. 634{639 Anaheim, CA. MIT Press.McDermott, D. (1996). A heuristic estimator for means-ends analysis in planning. InProceedings of the 3rd International Conference on Arti cial Intelligence PlanningSystems (AIPS-96), pp. 142{149. AAAI Press, Menlo Park.McDermott, D., et al. (1998). The PDDL Planning Domain De nition Language. TheAIPS-98 Planning Competition Comitee.McDermott, D. V. (1999). Using regression-match graphs to control search in planning.Arti cial Intelligence, 109 (1-2), 111{159.Mitchell, D., Selman, B., & Levesque, H. J. (1992). Hard and easy distributions of SATproblems. In Proceedings of the 10th National Conference of the American Associationfor Arti cial Intelligence (AAAI-92), pp. 459{465 San Jose, CA. MIT Press.Nebel, B. (2000). On the compilability and expressive power of propositional planningformalisms. Journal of Arti cial Intelligence Research, 12, 271{315.Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts and operators inplan generation. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI Planning. 4thEuropean Conference on Planning (ECP'97), Vol. 1348 of Lecture Notes in Arti cialIntelligence, pp. 338{350 Toulouse, France. Springer-Verlag.Pednault, E. P. (1989). ADL: Exploring the middle ground between STRIPS and thesituation calculus. In Brachman, R., Levesque, H. J., & Reiter, R. (Eds.), Principlesof Knowledge Representation and Reasoning: Proceedings of the 1st InternationalConference (KR-89), pp. 324{331 Toronto, ON. Morgan Kaufmann.Refanidis, I., & Vlahavas, I. (1999). GRT: a domain independent heuristic for STRIPSworlds based on greedy regression tables. In Biundo, S., & Fox, M. (Eds.), RecentAdvances in AI Planning. 5th European Conference on Planning (ECP'99) Durham,UK. Springer-Verlag.Refanidis, I., & Vlahavas, I. (2000). Exploiting state constraints in heuristic state-spaceplanning. In Chien, S., Kambhampati, R., & Knoblock, C. (Eds.), Proceedings of the5th International Conference on Arti cial Intelligence Planning Systems (AIPS-00),pp. 363{370. AAAI Press, Menlo Park.301\nHoffmann & NebelRussell, S., & Norvig, P. (1995). Arti cial Intelligence: A Modern Approach. Prentice-Hall,Englewood Cli s, NJ.Siegel, S., & N. J. Castellan, J. (1988). Nonparametric Statistics for the Behavioral Sciences(2nd edition). McGraw-Hill.Slaney, J., & Thiebaux, S. (2001). Blocks world revisited. Arti cial Intelligence, 125,119{153.\n302"}], "references": [{"title": "Conditional e ects in Graphplan", "author": ["C.R. Anderson", "D.E. Smith", "D.S. Weld"], "venue": null, "citeRegEx": "Anderson et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 1998}, {"title": "Subset of PDDL for the AIPS2000 Planning Competition", "author": ["F. Bacchus"], "venue": null, "citeRegEx": "Bacchus,? \\Q2000\\E", "shortCiteRegEx": "Bacchus", "year": 2000}, {"title": "Fast planning through planning graph analysis", "author": ["A.L. Blum", "M.L. Furst"], "venue": null, "citeRegEx": "Blum and Furst,? \\Q1995\\E", "shortCiteRegEx": "Blum and Furst", "year": 1995}, {"title": "Fast planning through planning graph analysis", "author": ["A.L. Blum", "M.L. Furst"], "venue": null, "citeRegEx": "Blum and Furst,? \\Q1997\\E", "shortCiteRegEx": "Blum and Furst", "year": 1997}, {"title": "HSP: Heuristic search planner", "author": ["B. Bonet", "H. Ge ner"], "venue": null, "citeRegEx": "Bonet and ner,? \\Q1998\\E", "shortCiteRegEx": "Bonet and ner", "year": 1998}, {"title": "Planning as heuristic search: New results", "author": ["B. Bonet", "H. Ge ner"], "venue": null, "citeRegEx": "Bonet and ner,? \\Q1999\\E", "shortCiteRegEx": "Bonet and ner", "year": 1999}, {"title": "Planning as heuristic search", "author": ["B. Bonet", "H. Ge ner"], "venue": null, "citeRegEx": "Bonet and ner,? \\Q2001\\E", "shortCiteRegEx": "Bonet and ner", "year": 2001}, {"title": "A robust and fast action selection", "author": ["B. Bonet", "G. Loerincs", "H. Ge ner"], "venue": null, "citeRegEx": "Bonet et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bonet et al\\.", "year": 1997}, {"title": "The computational complexity of propositional STRIPS planning", "author": ["T. Bylander"], "venue": null, "citeRegEx": "Bylander,? \\Q1994\\E", "shortCiteRegEx": "Bylander", "year": 1994}, {"title": "Ordering problem subgoals", "author": ["J. Cheng", "K.B. Irani"], "venue": null, "citeRegEx": "Cheng and Irani,? \\Q1989\\E", "shortCiteRegEx": "Cheng and Irani", "year": 1989}, {"title": "Goal ordering in partially ordered plans", "author": ["M. Drummond", "K. Currie"], "venue": "In Sridha-", "citeRegEx": "Drummond and Currie,? \\Q1989\\E", "shortCiteRegEx": "Drummond and Currie", "year": 1989}, {"title": "Heuristic search planning with BDDs. In ECAI-Workshop: PuK", "author": ["S. Edelkamp"], "venue": null, "citeRegEx": "Edelkamp,? \\Q2000\\E", "shortCiteRegEx": "Edelkamp", "year": 2000}, {"title": "NP-completeness of several arrangement problems", "author": ["S. Even", "Y. Shiloach"], "venue": null, "citeRegEx": "Even and Shiloach,? \\Q1975\\E", "shortCiteRegEx": "Even and Shiloach", "year": 1975}, {"title": "STRIPS: A new approach to the application of theorem", "author": ["R.E. Fikes", "N. Nilsson"], "venue": null, "citeRegEx": "Fikes and Nilsson,? \\Q1971\\E", "shortCiteRegEx": "Fikes and Nilsson", "year": 1971}, {"title": "The automatic inference of state invariants in tim", "author": ["M. Fox", "D. Long"], "venue": null, "citeRegEx": "Fox and Long,? \\Q1998\\E", "shortCiteRegEx": "Fox and Long", "year": 1998}, {"title": "Hybrid STAN: Identifying and managing combinatorial op", "author": ["M. Fox", "D. Long"], "venue": null, "citeRegEx": "Fox and Long,? \\Q2001\\E", "shortCiteRegEx": "Fox and Long", "year": 2001}, {"title": "When gravity fails: Local search topology", "author": ["J. Frank", "P. Cheeseman", "J. Stutz"], "venue": null, "citeRegEx": "Frank et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Frank et al\\.", "year": 1997}, {"title": "Combining the expressiveness of UCPOP with", "author": ["B.C. Gazen", "C. Knoblock"], "venue": null, "citeRegEx": "Gazen and Knoblock,? \\Q1997\\E", "shortCiteRegEx": "Gazen and Knoblock", "year": 1997}, {"title": "A heuristic for domain independent planning and its use", "author": ["J. Ho mann"], "venue": null, "citeRegEx": "mann,? \\Q2000\\E", "shortCiteRegEx": "mann", "year": 2000}, {"title": "Local search topology in planning benchmarks: An empirical analysis", "author": ["J. Ho mann"], "venue": null, "citeRegEx": "mann,? \\Q2001\\E", "shortCiteRegEx": "mann", "year": 2001}, {"title": "Solving the entailment problem in the uent calculus", "author": ["orr", "H.-P"], "venue": null, "citeRegEx": "orr and H..P.,? \\Q2000\\E", "shortCiteRegEx": "orr and H..P.", "year": 2000}, {"title": "Subgoal ordering and goal augmentation for heuristic", "author": ["K.B. Irani", "J. Cheng"], "venue": null, "citeRegEx": "Irani and Cheng,? \\Q1987\\E", "shortCiteRegEx": "Irani and Cheng", "year": 1987}, {"title": "Planning - a randomized approach", "author": ["P. Jonsson", "P. Haslum", "C. B\u007fackstr\u007fom"], "venue": null, "citeRegEx": "Jonsson et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jonsson et al\\.", "year": 2000}, {"title": "A theoretical analysis of conjunctive-goal problems", "author": ["D. Joslin", "J.W. Roach"], "venue": null, "citeRegEx": "Joslin and Roach,? \\Q1990\\E", "shortCiteRegEx": "Joslin and Roach", "year": 1990}, {"title": "Understanding and extending", "author": ["S. Kambhampati", "E. Parker", "E. Lambrecht"], "venue": null, "citeRegEx": "Kambhampati et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kambhampati et al\\.", "year": 1997}, {"title": "Unifying SAT-based and graph-based planning", "author": ["H. Kautz", "B. Selman"], "venue": null, "citeRegEx": "Kautz and Selman,? \\Q1999\\E", "shortCiteRegEx": "Kautz and Selman", "year": 1999}, {"title": "Pushing the envelope: Planning, propositional logic", "author": ["H.A. Kautz", "B. Selman"], "venue": null, "citeRegEx": "Kautz and Selman,? \\Q1996\\E", "shortCiteRegEx": "Kautz and Selman", "year": 1996}, {"title": "Solving complex planning tasks through extraction of subproblems", "author": ["J. Koehler"], "venue": null, "citeRegEx": "Koehler,? \\Q1998\\E", "shortCiteRegEx": "Koehler", "year": 1998}, {"title": "On reasonable and forced goal orderings and their use", "author": ["J. Koehler", "J. Ho mann"], "venue": null, "citeRegEx": "Koehler and mann,? \\Q2000\\E", "shortCiteRegEx": "Koehler and mann", "year": 2000}, {"title": "On the instantiation of ADL operators involving", "author": ["J. Koehler", "J. Ho mann"], "venue": null, "citeRegEx": "Koehler and mann,? \\Q2000\\E", "shortCiteRegEx": "Koehler and mann", "year": 2000}, {"title": "Extending planning graphs", "author": ["J. Koehler", "B. Nebel", "J. Ho mann", "Y. Dimopoulos"], "venue": null, "citeRegEx": "Koehler et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Koehler et al\\.", "year": 1997}, {"title": "Elevator control as a planning problem", "author": ["J. Koehler", "K. Schuster"], "venue": null, "citeRegEx": "Koehler and Schuster,? \\Q2000\\E", "shortCiteRegEx": "Koehler and Schuster", "year": 2000}, {"title": "E cient implementation of the plan graph in stan", "author": ["D. Long", "M. Fox"], "venue": null, "citeRegEx": "Long and Fox,? \\Q1999\\E", "shortCiteRegEx": "Long and Fox", "year": 1999}, {"title": "Systematic nonlinear planning", "author": ["D.A. McAllester", "D. Rosenblitt"], "venue": null, "citeRegEx": "McAllester and Rosenblitt,? \\Q1991\\E", "shortCiteRegEx": "McAllester and Rosenblitt", "year": 1991}, {"title": "A heuristic estimator for means-ends analysis in planning", "author": ["D. McDermott"], "venue": null, "citeRegEx": "McDermott,? \\Q1996\\E", "shortCiteRegEx": "McDermott", "year": 1996}, {"title": "The PDDL Planning Domain De nition Language", "author": ["D McDermott"], "venue": null, "citeRegEx": "McDermott,? \\Q1998\\E", "shortCiteRegEx": "McDermott", "year": 1998}, {"title": "Using regression-match graphs to control search in planning", "author": ["D.V. McDermott"], "venue": null, "citeRegEx": "McDermott,? \\Q1999\\E", "shortCiteRegEx": "McDermott", "year": 1999}, {"title": "Hard and easy distributions of SAT", "author": ["D. Mitchell", "B. Selman", "H.J. Levesque"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 1992}, {"title": "On the compilability and expressive power of propositional planning", "author": ["B. Nebel"], "venue": null, "citeRegEx": "Nebel,? \\Q2000\\E", "shortCiteRegEx": "Nebel", "year": 2000}, {"title": "Ignoring irrelevant facts and operators", "author": ["B. Nebel", "Y. Dimopoulos", "J. Koehler"], "venue": null, "citeRegEx": "Nebel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Nebel et al\\.", "year": 1997}, {"title": "ADL: Exploring the middle ground between STRIPS and the", "author": ["E.P. Pednault"], "venue": null, "citeRegEx": "Pednault,? \\Q1989\\E", "shortCiteRegEx": "Pednault", "year": 1989}, {"title": "GRT: a domain independent heuristic for STRIPS", "author": ["I. Refanidis", "I. Vlahavas"], "venue": null, "citeRegEx": "Refanidis and Vlahavas,? \\Q1999\\E", "shortCiteRegEx": "Refanidis and Vlahavas", "year": 1999}, {"title": "Exploiting state constraints in heuristic state-space", "author": ["I. Refanidis", "I. Vlahavas"], "venue": null, "citeRegEx": "Refanidis and Vlahavas,? \\Q2000\\E", "shortCiteRegEx": "Refanidis and Vlahavas", "year": 2000}, {"title": "Nonparametric Statistics for the Behavioral Sciences", "author": ["S. Siegel", "J.N.J. Castellan"], "venue": null, "citeRegEx": "Siegel and Castellan,? \\Q1988\\E", "shortCiteRegEx": "Siegel and Castellan", "year": 1988}, {"title": "Blocks world revisited", "author": ["J. Slaney", "S. Thiebaux"], "venue": "Arti cial Intelligence,", "citeRegEx": "Slaney and Thiebaux,? \\Q2001\\E", "shortCiteRegEx": "Slaney and Thiebaux", "year": 2001}], "referenceMentions": [{"referenceID": 38, "context": "Their paper started a whole series of research e orts that re ned this approach by making it even more e cient (Fox & Long, 1998; Kambhampati, Parker, & Lambrecht, 1997) and by extending it to cope with more expressive planning languages (Koehler, Nebel, Ho mann, & Dimopoulos, 1997; Gazen & Knoblock, 1997; Anderson, Smith, & Weld, 1998; Nebel, 2000).", "startOffset": 238, "endOffset": 351}, {"referenceID": 2, "context": "The rst approach was developed by Blum and Furst (1995, 1997). In their seminal paper on the GRAPHPLAN system (Blum & Furst, 1995), they described a new plan generation technique based on planning graphs, which was much faster than any other technique known at this time. Their paper started a whole series of research e orts that re ned this approach by making it even more e cient (Fox & Long, 1998; Kambhampati, Parker, & Lambrecht, 1997) and by extending it to cope with more expressive planning languages (Koehler, Nebel, Ho mann, & Dimopoulos, 1997; Gazen & Knoblock, 1997; Anderson, Smith, & Weld, 1998; Nebel, 2000). The second approach is the planning as satis ability method, which translates planning to propositional satis ability (Kautz & Selman, 1996). In particular there is the hope that advances in the state of the art of propositional reasoning systems carry directly over to planning systems relying on this technology. In fact, Kautz and Selman (1999) predicted that research on planning methods will become super uous because the state of the art in propositional reasoning systems will advance much faster than in planning systems.", "startOffset": 34, "endOffset": 973}, {"referenceID": 8, "context": "Planning is known to be PSPACE-complete even in its simplest form (Bylander, 1994).", "startOffset": 66, "endOffset": 82}, {"referenceID": 8, "context": "To some extent, this idea has been pursued by posing severe syntactical restrictions to the planning task speci cations (Bylander, 1994).", "startOffset": 120, "endOffset": 136}, {"referenceID": 27, "context": "{ The goal agenda technique, adapted from work by Jana Koehler (1998), feeds the goals to the planner in an order determined as a pre-process (Section 6.", "startOffset": 55, "endOffset": 70}, {"referenceID": 13, "context": "Notational Conventions For introducing FF's basic techniques, we consider simple STRIPS planning tasks, as were introduced by Fikes and Nilsson (1971). Our notations are as follows.", "startOffset": 126, "endOffset": 151}, {"referenceID": 8, "context": "As computing the optimal solution length to P 0 S| which would make an admissible heuristic|is NP-hard (Bylander, 1994), the HSP estimate is a rough approximation based on computing the following weight values.", "startOffset": 103, "endOffset": 119}, {"referenceID": 8, "context": "Proof: Hardness is proven by polynomially reducing PLANSAT (Bylander, 1994)|the decision problem of whether P is solvable|to the problem of deciding DEADEND-FREE.", "startOffset": 59, "endOffset": 75}, {"referenceID": 8, "context": "Proof: Hardness is proven by polynomially reducing PLANSAT (Bylander, 1994)|the decision problem of whether P is solvable|to the problem of deciding DEADEND-FREE. We simply add an operator to O that is executable in all states, and re-establishes the initial state. O1 := O [ foI := h;;I; [ o2O add(o) n Iig Applying oI to any state reachable in P leads back to the initial state: all facts that can ever become true are removed, and those in the initial state are added. Now, the modi ed problem P1 = (O1;I;G) is dead-end free i P is solvable. From left to right, if P1 is deadend free, then it is solvable, which implies that P is solvable, as we have not added any new possibility of reaching the goal. From right to left, if P is solvable, then also is P1, by the same solution plan P . One can then, from all states in P1, achieve the goal by going back to the initial state with the new operator, and executing P thereafter. Membership in PSPACE follows from the fact that PLANSAT and its complement are both in PSPACE. A non-deterministic algorithm that decides the complement of DEADENDFREE and that needs only polynomial space can be speci ed as follows. Guess a state S. Verify in polynomial space that S is reachable from the initial state. Further, verify that the goal cannot be reached from S. If this algorithm succeeds, it follows that the instance is not dead-end free|since S constitutes a dead end. This implies that DEADEND-FREE is in NPSPACE, and hence in PSPACE. 2 Though we can not e ciently decide whether a given task is dead-end free, there are easily testable su cient criteria in the literature. Johnsson et al. (2000) de ne a notion of symmetric planning tasks, which is su cient for dead-end freeness, but co-NP-complete.", "startOffset": 60, "endOffset": 1647}, {"referenceID": 8, "context": "Proof: Hardness is proven by polynomially reducing PLANSAT (Bylander, 1994)|the decision problem of whether P is solvable|to the problem of deciding DEADEND-FREE. We simply add an operator to O that is executable in all states, and re-establishes the initial state. O1 := O [ foI := h;;I; [ o2O add(o) n Iig Applying oI to any state reachable in P leads back to the initial state: all facts that can ever become true are removed, and those in the initial state are added. Now, the modi ed problem P1 = (O1;I;G) is dead-end free i P is solvable. From left to right, if P1 is deadend free, then it is solvable, which implies that P is solvable, as we have not added any new possibility of reaching the goal. From right to left, if P is solvable, then also is P1, by the same solution plan P . One can then, from all states in P1, achieve the goal by going back to the initial state with the new operator, and executing P thereafter. Membership in PSPACE follows from the fact that PLANSAT and its complement are both in PSPACE. A non-deterministic algorithm that decides the complement of DEADENDFREE and that needs only polynomial space can be speci ed as follows. Guess a state S. Verify in polynomial space that S is reachable from the initial state. Further, verify that the goal cannot be reached from S. If this algorithm succeeds, it follows that the instance is not dead-end free|since S constitutes a dead end. This implies that DEADEND-FREE is in NPSPACE, and hence in PSPACE. 2 Though we can not e ciently decide whether a given task is dead-end free, there are easily testable su cient criteria in the literature. Johnsson et al. (2000) de ne a notion of symmetric planning tasks, which is su cient for dead-end freeness, but co-NP-complete. They also give a polynomial su cient criterion for symmetry. This is, however, very trivial. Hardly any of the current benchmarks ful lls it. Koehler and Ho mann (2000a) have de ned notions of invertible planning tasks|su cient for dead-end freeness, and inverse actions| su cient for invertibility, under certain restrictions.", "startOffset": 60, "endOffset": 1922}, {"referenceID": 18, "context": "Fast Plan Generation Through Heuristic Search One could adopt Koehler and Ho mann's methodology, and use the existence of inverse actions to recognize dead-end free tasks. If the test fails, one could then employ a di erent search strategy than enforced hill-climbing. We have two reasons for not going this way: Even amongst our benchmarks, there are tasks that do not contain inverse actions, but are nevertheless dead-end free. An example is the Tireworld domain, where enforced hill-climbing leads to excellent results. Enforced hill-climbing can often quite successfully solve tasks that do contain dead ends, as it does not necessarily get caught in one. Examples for that are contained in the Mystery and Mprime domains, which we will look at in Section 8.2.1. The observation that forms the basis for our way of dealing with completeness is the following. If enforced hill-climbing can not solve a planning task, it usually fails very quickly. One can then simply switch to a di erent search algorithm. We have experimented with randomizing enforced hill-climbing, and doing a restart when one attempt failed. This didn't lead to convincing results. Though we tried a large variety of randomization strategies, we did not nd a planning task in our testing domains where one randomized restart did signi cantly better than the previous one, i.e., all attempts su ered from the same problems. The tasks that enforced hill-climbing does not solve right away are apparently so full of dead ends that one can not avoid those dead ends at random. We have therefore arranged our overall search strategy in FF as follows: 1. Do enforced hill-climbing until the goal is reached or the algorithm fails. 2. If enforced hill-climbing failed, skip everything done so far and try to solve the task by a complete heuristic search algorithm. In the current implementation, this is what Russel and Norvig (1995) term greedy best- rst search.", "startOffset": 77, "endOffset": 1903}, {"referenceID": 18, "context": "What one can ask in a situation like this is, was it a good idea to achieve G right now? Or should some other goal be achieved rst? Our answer is inspired by recent work of Koehler and Ho mann (2000a), which argues that achieving G should be postponed if the remaining goals can not be achieved without destroying G again.", "startOffset": 188, "endOffset": 201}, {"referenceID": 27, "context": "This is also the basic principle underlying the so-called \\goal agenda\" approach (Koehler, 1998).", "startOffset": 81, "endOffset": 96}, {"referenceID": 40, "context": "We will now show how our approach can be extended to deal with ADL (Pednault, 1989) tasks, more precisely, with the ADL subset of PDDL (McDermott et al.", "startOffset": 67, "endOffset": 83}, {"referenceID": 1, "context": ", 1998) that was used in the 2nd international planning systems competition (Bacchus, 2000).", "startOffset": 76, "endOffset": 91}, {"referenceID": 1, "context": "The planner starts with a planning task speci cation given in the subset of PDDL de ned for the AIPS-2000 planning competition (Bacchus, 2000).", "startOffset": 127, "endOffset": 142}, {"referenceID": 18, "context": "Hoffmann & Nebel list of parameters, a precondition, and a list of e ects. Instantiating the parameters yields, just like STRIPS tasks are usually speci ed, the actions to the schema. The precondition is an arbitrary ( rst order) formula. For an action to be applicable in a given state S, its instantiation of this formula must be satis ed in S. Each e ect i in the list has the form 8yi 0; : : : ; yi ni : ( i(o); addi(o);deli(o)) Here, yi 0; : : : ; yi ni are the e ect parameters, i(o) is the e ect condition|again, an arbitrary formula|and addi(o) and deli(o) are the atomic add and delete e ects, respectively. The atomic e ects are sets of uninstantiated atoms, i.e., relational symbols containing variables. The semantics are that, if an instantiated action is executed, then, for each single e ect i in the list, and for each instantiation of its parameters, the condition i(o) is evaluated. If i(o) holds in the current state, then the corresponding instantiations of the atoms in addi(o) are added to the state, and the instantiations of atoms in deli(o) are removed from the state. In FF's heuristic method, each single state evaluation can involve thousands of operator applications|building the relaxed planning graph, one needs to determine all applicable actions at each single fact layer. We therefore invest the e ort to compile the operator descriptions down into a much simpler propositional normal form, such that heuristic evaluation can be implemented e ciently. Our nal normal form actions o have the following format. Precondition: pre(o) E ects: (pre0(o); add0(o);del0(o)) ^ (pre1(o); add1(o);del1(o)) ^ ...(prem(o); addm(o);delm(o)) The precondition is a set of ground atoms. Likewise, the e ect conditions prei(o) of the single e ects are restricted to be ground atoms. We also represent the goal state as a set of atoms. Thus, we compile away everything except the conditional e ects. Compiling away the logical formulae involves transforming them into DNF, which causes an exponential blow up in general. In our testing domains, however, we found that this transformation can be done in reasonable time. Concerning the conditional e ects, those can not be compiled away without another exponential blow up, given that we want to preserve solution length. This was proven by Nebel (2000). As we will see, conditional e ects can e ciently be integrated into our algorithmic framework, so there is no need for compiling them away.", "startOffset": 4, "endOffset": 2317}, {"referenceID": 17, "context": "Following Gazen and Knoblock (1997), this process expands all quanti ers, and translates negations.", "startOffset": 10, "endOffset": 36}, {"referenceID": 17, "context": "Following Gazen and Knoblock (1997), this process expands all quanti ers, and translates negations. We end up with formulae that are made up out of conjunctions, disjunctions, and atoms containing variables. (b) Instantiate all parameters. This is simply done by instantiating all operator and e ect parameters with all type consistent constants one after the other. The process makes use of knowledge about static predicates, in the sense that the instantiated formulae can often be simpli ed (Koehler & Ho mann, 2000b). For example, if an instantiated static predicate (p ~a) occurs in a formula, and that instantiation is not contained in the initial state, then (p ~a) can be replaced with false. (c) Transform formulae into DNF. This is postponed until after instantiation, because it can be costly, so it should be applied to as small formulae as possible. In a fully instantiated formula, it is likely that many static or one-way predicate occurrences can be replaced by true or false, resulting in a much simpler formula structure. 3. Finally, if the DNF of any formula contains more than one disjunct, then the corresponding e ect, operator, or goal condition gets split up in the manner proposed by Gazen and Knoblock (1997). 7.", "startOffset": 10, "endOffset": 1235}, {"referenceID": 18, "context": "Hoffmann & Nebel plus o's preconditions need to be put into their corresponding goal sets. Afterwards, not only the e ect's own add e ects addi(o) are marked true at the time being, but also the added facts of all e ects that are implied, i.e., those e ects j of o with prej(o) prei(o) (in particular, this will be the unconditional e ects of o, which have an empty e ect condition). 7.3 ADL Pruning Techniques Both pruning techniques from Section 6 easily carry over to actions with conditional e ects. 7.3.1 Helpful Actions For STRIPS, we de ned as helpful all applicable actions achieving at least one goal at time step 1, cf. Section 6.1. For our ADL normal form, we simply change this to all applicable actions having an appearing e ect that achieves a goal at time step 1, where an e ect appears i its e ect condition is satis ed in the current state. H(S) := fo j pre(o) S;9i : prei(o) S ^ addi(o) \\G1(S) 6= ;g (6) 7.3.2 Added Goal Deletion Originally, we cut o a state S if one of the actions selected for the relaxed plan to S deleted a goal A that had just been achieved, cf. Section 6.2. We now simply take as criterion the e ects that are selected for the relaxed plan, i.e., a state is cut o if one of the e ects selected for its relaxed solution deletes a goal A that has just been achieved. 7.4 ADL State Transitions Finally, for enabling the search algorithms to handle our propositional ADL normal form, it is su cient to rede ne the state transition function. Forward search, no matter if it does hill-climbing, best- rst search, or whatsoever, always faces a completely speci ed search state.3 It can therefore compute exactly the e ects of executing a context dependent action. Following Koehler et al.(1997), we de ne our ADL state transition function Res, mapping states and ADL normal form actions to states, as follows.", "startOffset": 4, "endOffset": 1729}, {"referenceID": 11, "context": "These planners were FF, HSP2 (Bonet & Ge ner, 1998, 1999), System-R, GRT (Refanidis & Vlahavas, 1999), Mips (Edelkamp, 2000), and STAN (Long & Fox, 1999; Fox & Long, 2001).", "startOffset": 108, "endOffset": 124}, {"referenceID": 30, "context": "Apart from those planners already seen, we have runtime curves in Figure 6 for IPP (Koehler et al., 1997), PropPlan, and BDDPlan (H\u007folldobler & St\u007f orr, 2000).", "startOffset": 83, "endOffset": 105}, {"referenceID": 36, "context": "In Figure 9, we compare FF's results on both domains to that reported by Drew McDermott for the Unpop system (McDermott, 1999).", "startOffset": 109, "endOffset": 126}, {"referenceID": 36, "context": "Results for Unpop have been taken by McDermott on a 300 MHz Pentium-II workstation (McDermott, 1999).", "startOffset": 83, "endOffset": 100}, {"referenceID": 18, "context": "Koehler and Ho mann (2000a) modi ed the task such that an arbitrary number of n tires need to be replaced.", "startOffset": 15, "endOffset": 28}, {"referenceID": 39, "context": "The similarity of the helpful actions heuristic to McDermott's favored actions (1996), and to irrelevance detection mechanisms (Nebel et al., 1997).", "startOffset": 127, "endOffset": 147}, {"referenceID": 27, "context": "The inspiration of the added goal deletion heuristic by work done by Koehler and Ho mann (2000a), and the adaption of the goal agenda approach (Koehler, 1998).", "startOffset": 143, "endOffset": 158}, {"referenceID": 17, "context": "Hoffmann & Nebel on the other hand, randomly choose either ordering with equal probability. As said in Section 8.3.1, hill-climbing was given ve tries on each task, and results averaged. In ve tries, around half of the solutions use the correct ordering, such that, for all tasks, the average value is lower than the corresponding value for the enforced hill-climbing planners. Finally, we compare consideration of all actions versus consideration of only the helpful ones, results depicted in the rightmost four columns of Figure 12. Coming a bit unexpected, there is only one single case where solution length performance is degraded by turning on helpful actions. This indicates that the actions on the shortest path to the goal are, in fact, usually considered helpful|unless all solution paths are thrown away, as is sometimes the case only in the Briefcaseworld and Bulldozer domains. Quite the other way around than one should think, pruning the search space with helpful actions sometimes leads to signi cantly shorter solution plans, especially when the underlying search method is hillclimbing. Though this may sound paradoxical, there is a simple explanation to it. Consider what we said above about the plateau behavior of hill-climbing, randomly adding actions to the current plan in the search for a better state. If such a search engine is armed with the helpful actions successors choice, focusing it into the direction of the goals, it might well take less steps to nd the way o a plateau. 9. Related Work The most important connections of the FF approach to methodologies reported in the literature are the following: HSP's basic idea of forward state space search and heuristic evaluation by ignoring delete lists (Bonet & Ge ner, 1998). The view of our heuristic as a special case of GRAPHPLAN (Blum & Furst, 1995), and its connection to HSP's heuristic method. The similarity of the helpful actions heuristic to McDermott's favored actions (1996), and to irrelevance detection mechanisms (Nebel et al.", "startOffset": 4, "endOffset": 1968}, {"referenceID": 17, "context": "Hoffmann & Nebel on the other hand, randomly choose either ordering with equal probability. As said in Section 8.3.1, hill-climbing was given ve tries on each task, and results averaged. In ve tries, around half of the solutions use the correct ordering, such that, for all tasks, the average value is lower than the corresponding value for the enforced hill-climbing planners. Finally, we compare consideration of all actions versus consideration of only the helpful ones, results depicted in the rightmost four columns of Figure 12. Coming a bit unexpected, there is only one single case where solution length performance is degraded by turning on helpful actions. This indicates that the actions on the shortest path to the goal are, in fact, usually considered helpful|unless all solution paths are thrown away, as is sometimes the case only in the Briefcaseworld and Bulldozer domains. Quite the other way around than one should think, pruning the search space with helpful actions sometimes leads to signi cantly shorter solution plans, especially when the underlying search method is hillclimbing. Though this may sound paradoxical, there is a simple explanation to it. Consider what we said above about the plateau behavior of hill-climbing, randomly adding actions to the current plan in the search for a better state. If such a search engine is armed with the helpful actions successors choice, focusing it into the direction of the goals, it might well take less steps to nd the way o a plateau. 9. Related Work The most important connections of the FF approach to methodologies reported in the literature are the following: HSP's basic idea of forward state space search and heuristic evaluation by ignoring delete lists (Bonet & Ge ner, 1998). The view of our heuristic as a special case of GRAPHPLAN (Blum & Furst, 1995), and its connection to HSP's heuristic method. The similarity of the helpful actions heuristic to McDermott's favored actions (1996), and to irrelevance detection mechanisms (Nebel et al., 1997). The inspiration of the added goal deletion heuristic by work done by Koehler and Ho mann (2000a), and the adaption of the goal agenda approach (Koehler, 1998).", "startOffset": 4, "endOffset": 2128}, {"referenceID": 17, "context": "The adaption of IPP's ADL preprocessing phase (Koehler & Ho mann, 2000b), inspired by ideas from Gazen and Knoblock (1997). We have discussed all of these connections in the respective sections already.", "startOffset": 97, "endOffset": 123}], "year": 2011, "abstractText": null, "creator": "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"}}}