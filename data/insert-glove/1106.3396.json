{"id": "1106.3396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2011", "title": "Large margin filtering for signal sequence labeling", "abstract": "Signal Sequence Labeling consists in predicting ditto a nrao sequence of souissi labels upin given an costings observed marsaglia sequence of bulluck samples. A naive ludgate way is to ingestre filter 172-seat the signal c\u0103linescu in tusken order cctv-1 to to-1 reduce the noise and to apply 116,500 a 7:37 classification delme algorithm on kick the valliant filtered megawatts samples. hogfish We propose 148,400 in bazars this paper to careerism jointly sheraz learn the kabelo filter etd with the classifier klaveren leading com to a large sotavento margin filtering for classification. buntline This method allows to uncaught learn nacif the m.k. optimal valorem cutoff 21.15 frequency and 371st phase excused of mahaz the filter that may beccari be 214.7 different 1.146 from gulnara zero. dalia Two gbf methods are proposed and tested camisa on a presso toy under-secretary dataset and on recoding a calendarists real 103-89 life yock BCI dataset from BCI interbay Competition III.", "histories": [["v1", "Fri, 17 Jun 2011 06:54:35 GMT  (1560kb)", "http://arxiv.org/abs/1106.3396v1", "IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2010, Dallas : United States (2010)"]], "COMMENTS": "IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2010, Dallas : United States (2010)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r\\'emi flamary", "benjamin labb\\'e", "alain rakotomamonjy"], "accepted": false, "id": "1106.3396"}, "pdf": {"name": "1106.3396.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 6.\n33 96\nv1 [\ncs .L\nG ]\n1 7\nJu n\n20 11\nIndex Terms\u2014 Filtering, SVM ,BCI , Sequence Labeling"}, {"heading": "1. INTRODUCTION", "text": "The aim of signal sequence labeling is to assign a label to each sample of a multichannel signal while taking into account the sequentiality of the samples. This problem typically arises in speech signal segmentation or in Brain Computer Interfaces (BCI). Indeed, in real-time BCI applications, each sample of an electro-encephalography signal has to be interpreted as a specific command for a virtual keyboard or a robot hence the need for sample labeling [1, 2].\nMany methods and algorithms have already been proposed for signal sequence labeling. For instance, Hidden Markov Models (HMM) [3] are statistical models that are able to learn a joint probability distribution of samples in a sequence and their labels. In some cases, Conditional Random Fields (CRF) [4] have been shown to outperform the HMM approach as they do not suppose the observation are independent. Structural Support Vector Machines (StructSVM), which are SVMs that learn a mapping from structured input to structured output, have also been considered for signal segmentation [5]. Signal sequence labeling can also be viewed from a very different perspective by considering a change detection method coupled with a supervised classifier. For instance, a Kernel Change Detection algorithm [6] can be used for detecting abrupt changes in a signal and afterwards a classifier applied for labeling the segmented regions.\nThis work is funded in part by the FP7-ICT Programme of the European Community, under the PASCAL2 Network of Excellence, ICT- 216886 and by the French ANR Project ANR-09-EMER-001.\nIn order to preprocess the signal, a filtering is often applied and the resulting filtered samples are used as training examples for learning. Such an approach poses the issue of the filter choice, which is oftenly based on prior knowledge on the information brought by the signals. Moreover, measured signals and extracted features may not be in phase with the labels and a time-lag due to the acquisition process appears in the signals. For example, in the problem of decoding arm movements from brain signals, there exists a natural time shift between these two entries, hence in their works, Pistohl et al. [7] had to select by a validation method a delay in their signal processing method.\nIn this work, we address the problem of automated tuning of the filtering stage including its time-lag. Indeed, our objective is to adapt the preprocessing filter and all its properties by including its setting into the learning process. Our hypothesis is that by fitting properly the filter to the classification problem at hand, without relying on ad-hoc prior-knowledge, we should be able to considerably improve the sequence labeling performance. So we propose to take into account the temporal neighborhood of the current sample directly into the decision function and the learning process, leading to an automatic setting of the signal filtering.\nFor this purpose, we first propose a naive approach based on SVMs which consists in considering, instead of a given time sample, a time-window around the sample. This method named as Window-SVM, allows us to learn a spatio-temporal classifier that will adapt itself to the signal time-lag. Then, we introduce another approach denoted as Filter-SVM which dissociates the filter and the classifier. This novel method jointly learns a SVM classifier and FIR filters coefficients. By doing so, we can interpret our filter as a large-margin filter for the problem at hand. These two methods are tested on a toy dataset and on a real life BCI signal sequence labeling problem from BCI Competition III [1]."}, {"heading": "2. LARGE MARGIN FILTER", "text": ""}, {"heading": "2.1. Problem definition", "text": "Our concern is a signal sequence labeling problem : we want to obtain a sequence of labels from a multichannel timesample of a signal or from multi-channel features extracted\nfrom that signal. We suppose that the training samples are gathered in a matrix X \u2208 RN\u00d7d containing d channels and N samples. Xi,j is the value of channel j for the ith sample. The vector y \u2208 {\u22121, 1}N contains the class of each sample.\nIn order to reduce noise in the samples or variability in the features, an usual approach is to filter X before the classifier learning stage. In literature, all channels are usually filtered with the same filter (Savisky-Golay for instance in [7]) although there is no reason for a single filter to be optimal for all channels. Let us define the filters applied to X by the matrix F \u2208 Rf\u00d7d. Each column of F is a filter for the corresponding channel in X and f is the size of the FIR filters.\nWe define the filtered data matrix X\u0303 by:\nX\u0303i,j =\nf\u2211\nm=1\nFm,j Xi+1\u2212m+n0,j (1)\nwhere the sum is a unidimensional convolution of each channel by the filter in the appropriate column of F . n0 is the delay of the filter, for instance n0 = 0 corresponds to a causal filter and n0 = f/2 corresponds to a filter centered on the current sample."}, {"heading": "2.2. Windowed-SVM (W-SVM)", "text": "As highlighted by Equation (1), a filtering stage essentially consists in taking into account for a given time i, instead of the sample Xi,\u00b7, a linear combination of its temporal neighborhood. However, instead of introducing a filter F , it is possible to consider for classification a temporal window around the current sample. Such an approach would lead to this decision function for the ith sample of X :\nfW (i,X) =\nf\u2211\nm=1\nd\u2211\nj=1\nWm,jXi+1\u2212m+n0,j + w0 (2)\nwhere W \u2208 Rf\u00d7d and w0 \u2208 R are the classification parameters and f is the size of the time-window. Note that W plays the role of the filter and the weights of a linear classifier. In a large-margin framework, W and w0 may be learned by minimizing this functional:\nJWSV M (W ) = 1\n2 ||W ||2F +\nC\n2\nN\u2211\ni=1\nH(y, X, fW , i) 2 (3)\nwhere ||W ||2F = \u2211 i,j W 2 i,j is the squared Frobenius norm of W , C is a regularization term to be tuned andH(y, X, f, i) = max(0, 1\u2212yif(i,X)) is the SVM hinge loss. By vectorizing appropriatelyX and W , problem (3) may be transformed into a linear SVM. Hence, we can take advantage of many linear SVM solvers existing in the literature such as the one proposed by Chapelle [8]. By using that solver, Window-SVM complexity is about O(N.(f.d)2) which scales quadratically with the filter dimension.\nThe matrix W weights the importance of each sample value Xi,j into the decision function. Hence, channels may have different weights and time-lag. Indeed, W will automatically adapt to a phase difference between the sample labels and the channel signals. However, in this method since space and time are treated independently, W does not take into account the multi-channel structure and the sequentiality of the samples. Since the samples of a given channel are known to be time-dependent due to the underlying physical process, it seems preferable to process them with a filter and to classify the filtered samples. So we propose in the sequel another method that jointly learns the time-filtering and a linear classifier on the filtered sample defined by Eq. (1)."}, {"heading": "2.3. Large margin filtering (Filter-SVM)", "text": "We propose to find the filter F that maximizes the margin of the linear classifier for the filtered samples. In this case, the decision function is:\nfF (i,X) =\nf\u2211\nm=1\nd\u2211\nj=1\nwjFm,jXi+1\u2212m+n0,j + w0 (4)\nwhere w and w0 are the parameters of the linear SVM classifier corresponding to a weighting of the channels. By dissociating the filter and the decision function weights, we expect that some useless channels (non-informative or too noisy) for the decision function get small weights. Indeed, due to the double weighting wj and F.,j , and the specific channel weighting role played by wj , this approach, as shown in the experimental section is able to perform channel selection.\nThe decision function given in Equation (4) can be obtained by minimizing:\nJFSV M = 1\n2 ||w||2+\nC\n2\nn\u2211\ni=1\nH(y, X, fF , i) 2+\n\u03bb 2 ||F || 2 F (5)\nw.r.t. (F,w, w0) where ||F ||F is the Frobenius norm, and \u03bb is a regularization term to be tuned. Note that without the regularization term ||F ||2F , the problem is ill-posed. Indeed, in such a case, one can always decrease ||w||2 while keeping the empirical hinge loss constant by multiplying w by \u03b1 < 1 and F by 1\n\u03b1 .\nThe cost defined in Equation (5) is differentiable and provably non-convex when jointly optimized with respect to all parameters. However, JFSV M is differentiable and convex with respect to w and w0 when F is fixed as it corresponds to a linear SVM with squared hinge loss. Hence, for a given value of F , we can define\nJ(F ) = min w,w0\n1 2 ||w||2 + C 2\nn\u2211\ni=1\nH(y, X, fF , i) 2\nwhich according to Bonnans et al. [9] is differentiable. Then if w\u2217 and w\u22170 are the optimal values for a given F\n\u2217, the gradient of the second term of J(\u00b7) with respect to F at the point\nF \u2217 is:\n\u2207Fm,jJ(F \u2217) = \u2212\nN\u2211\ni=1\nyi(w \u2217 jXi\u2212m+1+n0,j)\u00d7H(y, X, fF\u2217 , i)\nNow, since J(F ) is differentiable and since its value can be easily computed by a linear SVM, we choose for learning the decision function to minimize J(F ) + \u03bb\n2 \u2016F\u20162F with respects\nto F instead of minimizing problem (5). Note that due to the objective function non-convexity in problem (5), these two minimization problems are not strictly equivalent, but our approach has the advantage of taking into account the intrinsic large-margin structure of the problem.\nAlgorithm 1 Filter-SVM solver Set Fl,k = 1/f for k = 1 \u00b7 \u00b7 \u00b7 d and l = 1 \u00b7 \u00b7 \u00b7 f repeat DF \u2190 gradient of JFSV M with respect to F (F,w\u2217, w\u22170) \u2190 Line-Search along DF\nuntil Stopping criterion is reached\nFor solving the optimization problem, we propose a gradient descent algorithm along F with a line search method for finding the optimal step. The method is detailed in algorithm 1. Note that at each computation of J(F ) in the line search, the optimal w\u2217 and w\u22170 are found by solving a linear SVM. The iterations in the algorithm may be stopped by two stopping criteria: a threshold on the relative variation of J(F ) or a threshold on variations of F norm.\nDue to the non-convexity of the objective function, it is difficult to provide an exact evaluation of the solution complexity. However, we know that the gradient computation has order of O(N.f.d) and that when J(F ) is computed at each step of the line search, a O(N.d2) linear SVM is solved and a O(N.f.d) filtering is applied."}, {"heading": "3. RESULTS", "text": ""}, {"heading": "3.1. Toy Example", "text": "We use a toy example that consists of nbtot channels, only nbrel of them being discriminative. Discriminative channels have a switching mean {\u22121, 1} controlled by the label and corrupted by a gaussian noise of deviation \u03c3. The length of the regions with constant label follows a uniform distribution law between [30, 40] samples and different time-lags are applied to the channels. We selected f = 21 and n0 = 11 corresponding to a good average filtering centered on the current sample. Figure 1 shows how the samples are transformed thanks to the filter F for a unidimensional signal. In this case, the mean test error due to the noise is 16% for the unfiltered signal, while only 2% for the optimally filtered signal.\nWindow-SVM and Filter-SVM are compared to SVM without filtering, SVM with an average filter of size f (AvgSVM) and HMM with a Viterbi decoding. The regularization\nparameters are selected by a validation method. The size of the signals is of 1000 samples for the learning and the validation sets and of 5000 samples for the test set. All the processes are run ten times, the test error is the the average over the runs.\nThe methods are compared for different \u03c3 values with (nbtot = 30, nbrel = 3). The test error is plotted on the left of Figure 2. We can see that only Avg-SVM, WindowSVM and Filter-SVM adapt to time-lags between the channels and the labels. Both Window-SVM and Filter-SVM outperform the other methods, even if for a heavy noise, the last one seems to be slightly better. Then we test our methods for a varying number of channels in order to see how dimension is handled (nbrel = 3, \u03c3 = 3). Figure 2 (right) shows the interest of Filter-SVM over Window-SVM in hight dimension as we can see that the last one tends to lose his efficiency, and\neven to be similar to Avg-SVM. This comes from the fact that Filter-SVM can more efficiently perform a channel selection thanks to the weighting of w. Figure 3 shows the filters returned by both methods. We observe that only the coefficients of the relevant signals are important and that the other signals tend to be eliminated by small weights for Filter-SVM, explaining the better results in high dimension."}, {"heading": "3.2. BCI Dataset", "text": "We test our method on the BCI Dataset from BCI Competition III [1]. The problem is to obtain a sequence of labels out of brain activity signals for 3 human subjects. The data consists in 96 channels containing PSD features (3 training sessions, 1 test session, N \u2248 3000 per session) and the problem has 3 labels (left arm, right arm or feet).\nWe use Filter-SVM that showed better result in hight dimension for the toy example. The multi-class aspect of the problem is handled by using a One-Against-All strategy. The regularization parameters are tuned using a grid search validation method on the third training set. We compare our method to the best BCI competition results (using only 8 samples) and to the SVM without filtering. Test error for different filter size f and delay n0 may be seen on Table 1. Results show that one can improve drastically the result by using longer filtering with causal filters (n0 = 0). Note that Filter-SVM outperform Avg-SVM with a centered filter.\nAnother advantage of this method is that one can visualize a discriminative space-time map (channel selection, shape of\nthe filter and delays). We show for instance in Figure 4 the discriminative filters F obtained for subject 1, and we can see that the filtering is extremely different depending on the task.\nThe Matlab code corresponding to these results will be provided on our website for reproducibility."}, {"heading": "4. CONCLUSIONS", "text": "We have proposed two methods for automatically learning a spatio-temporal filter used for multi-channel signal classification. Both methods have been tested on a toy example and on a real life dataset from BCI Competition III.\nEmpirical results clearly show the benefits of adapting the signal filter to the large-margin classification problem despite the non-convexity of the criterion.\nIn future work, we plan to extend our approach to nonlinear case, we believe that a differentiable kernel can be used instead of inner products at the cost of solving the SVM in the dual space. Another perspective would be to adapt our methods to the multi-task situation, where one wants to jointly learn one matrix F and several classifiers (one per task)."}, {"heading": "5. REFERENCES", "text": "[1] B. Blankertz et al., \u201cThe BCI competition 2003: progress and perspectives in detection and discrimination of EEG single trials,\u201d IEEE Transactions on Biomedical Engineering, vol. 51, no. 6, pp. 1044\u20131051, 2004.\n[2] J. del R Milla\u0301n, \u201cOn the need for on-line learning in braincomputer interfaces,\u201d in Proc. Int. Joint Conf. on Neural Networks, 2004.\n[3] O. Cappe\u0301, E. Moulines, and T. Ryde\u0300n, Inference in Hidden Markov Models, Springer, 2005.\n[4] J. Lafferty, A.McCallum, and F. Pereira, \u201cConditional random fields: Probabilistic models for segmenting and labeling sequence data,\u201d in Proc. 18th International Conf. on Machine Learning, 2001, pp. 282\u2013289.\n[5] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, \u201cLarge margin methods for structured and interdependent output variables,\u201d in Journal Of Machine Learning Research. 2005, vol. 6, pp. 1453\u20131484, MIT Press.\n[6] F. Desobry, M. Davy, and C. Doncarli, \u201cAn online kernel change detection algorithm,\u201d IEEE Transactions on Signal Processing, vol. 53, pp. 2961\u20132974, 2005.\n[7] T. Pistohl, T. Ball, A. Schulze-Bonhage, A. Aertsen, and C. Mehring, \u201cPrediction of arm movement trajectories from ecog-recordings in humans,\u201d Journal of Neuroscience Methods, vol. 167, no. 1, pp. 105\u2013114, Jan. 2008.\n[8] O. Chapelle, \u201cTraining a support vector machine in the primal,\u201d Neural Comput., vol. 19, no. 5, pp. 1155\u20131178, 2007.\n[9] J.F. Bonnans and A. Shapiro, \u201cOptimization problems with pertubation : A guided tour,\u201d SIAM Review, vol. 40, no. 2, pp. 202\u2013227, 1998."}], "references": [{"title": "The BCI competition 2003: progress and perspectives in detection and discrimination of EEG single trials,", "author": ["B. Blankertz"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "On the need for on-line learning in braincomputer interfaces,", "author": ["J. del R Mill\u00e1n"], "venue": "in Proc. Int. Joint Conf. on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Inference in Hidden Markov Models", "author": ["O. Capp\u00e9", "E. Moulines", "T. Ryd\u00e8n"], "venue": "Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data,", "author": ["J. Lafferty", "A.McCallum", "F. Pereira"], "venue": "in Proc. 18th International Conf. on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Large margin methods for structured and interdependent output variables,", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal Of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "and C", "author": ["F. Desobry", "M. Davy"], "venue": "Doncarli, \u201cAn online kernel change detection algorithm,\u201d IEEE Transactions on Signal Processing, vol. 53, pp. 2961\u20132974", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Prediction of arm movement trajectories from ecog-recordings in humans,", "author": ["T. Pistohl", "T. Ball", "A. Schulze-Bonhage", "A. Aertsen", "C. Mehring"], "venue": "Journal of Neuroscience Methods,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Training a support vector machine in the primal,", "author": ["O. Chapelle"], "venue": "Neural Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Optimization problems with pertubation : A guided tour,", "author": ["J.F. Bonnans", "A. Shapiro"], "venue": "SIAM Review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Indeed, in real-time BCI applications, each sample of an electro-encephalography signal has to be interpreted as a specific command for a virtual keyboard or a robot hence the need for sample labeling [1, 2].", "startOffset": 201, "endOffset": 207}, {"referenceID": 1, "context": "Indeed, in real-time BCI applications, each sample of an electro-encephalography signal has to be interpreted as a specific command for a virtual keyboard or a robot hence the need for sample labeling [1, 2].", "startOffset": 201, "endOffset": 207}, {"referenceID": 2, "context": "For instance, Hidden Markov Models (HMM) [3] are statistical models that are able to learn a joint probability distribution of samples in a sequence and their labels.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "In some cases, Conditional Random Fields (CRF) [4] have been shown to outperform the HMM approach as they do not suppose the observation are independent.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Structural Support Vector Machines (StructSVM), which are SVMs that learn a mapping from structured input to structured output, have also been considered for signal segmentation [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 5, "context": "For instance, a Kernel Change Detection algorithm [6] can be used for detecting abrupt changes in a signal and afterwards a classifier applied for labeling the segmented regions.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "[7] had to select by a validation method a delay in their signal processing method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "These two methods are tested on a toy dataset and on a real life BCI signal sequence labeling problem from BCI Competition III [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "In literature, all channels are usually filtered with the same filter (Savisky-Golay for instance in [7]) although there is no reason for a single filter to be optimal for all channels.", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "Hence, we can take advantage of many linear SVM solvers existing in the literature such as the one proposed by Chapelle [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "[9] is differentiable.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We test our method on the BCI Dataset from BCI Competition III [1].", "startOffset": 63, "endOffset": 66}], "year": 2014, "abstractText": "Signal Sequence Labeling consists in predicting a sequence of labels given an observed sequence of samples. A naive way is to filter the signal in order to reduce the noise and to apply a classification algorithm on the filtered samples. We propose in this paper to jointly learn the filter with the classifier leading to a large margin filtering for classification. This method allows to learn the optimal cutoff frequency and phase of the filter that may be different from zero. Two methods are proposed and tested on a toy dataset and on a real life BCI dataset from BCI Competition III.", "creator": "LaTeX with hyperref package"}}}