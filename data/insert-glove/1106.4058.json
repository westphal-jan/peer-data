{"id": "1106.4058", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2011", "title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "abstract": "luthi Modelling compositional motivational meaning for mythologizing sentences using unenumerated empirical networks distributional deleg methods has jointly been sandwicense a challenge for computational linguists. uefa.com We implement greenroom the abstract arhab categorical accommodate model of asimco Coecke et anoka al. (", "histories": [["v1", "Mon, 20 Jun 2011 23:23:11 GMT  (97kb,AD)", "http://arxiv.org/abs/1106.4058v1", "11 pages, to be presented at EMNLP 2011, to be published in Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing"]], "COMMENTS": "11 pages, to be presented at EMNLP 2011, to be published in Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing", "reviews": [], "SUBJECTS": "cs.CL math.CT", "authors": ["edward grefenstette", "mehrnoosh sadrzadeh"], "accepted": true, "id": "1106.4058"}, "pdf": {"name": "1106.4058.pdf", "metadata": {"source": "CRF", "title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "authors": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "emails": ["edward.grefenstette@cs.ox.ac.uk", "mehrs@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "As competent language speakers, we humans can almost trivially make sense of sentences we\u2019ve never seen or heard before. We are naturally good at understanding ambiguous words given a context, and forming the meaning of a sentence from the meaning of its parts. But while human beings seem comfortable doing this, machines fail to deliver. Search engines such as Google either fall back on bag of words models\u2014ignoring syntax and lexical relations\u2014or exploit superficial models of lexical semantics to retrieve pages with terms related to those in the query (Manning et al., 2008).\nHowever, such models fail to shine when it comes to processing the semantics of phrases and sen-\ntences. Discovering the process of meaning assignment in natural language is among the most challenging and foundational questions of linguistics and computer science. The findings thereof will increase our understanding of cognition and intelligence and shall assist in applications to automating language-related tasks such as document search.\nCompositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question. Compositional formal semantic models stem from classical ideas from mathematical logic, mainly Frege\u2019s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892). Distributional models are more recent and can be related to Wittgenstein\u2019s later philosophy of \u2018meaning is use\u2019, whereby meanings of words can be determined from their context (Wittgenstein, 1953). The logical models relate to well known and robust logical formalisms, hence offering a scalable theory of meaning which can be used to reason inferentially. The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al., 2008). This two-sortedness of defining properties of meaning: \u2018logical form\u2019 versus \u2018contextual use\u2019, has left the quest for \u2018what is the foundational structure of meaning?\u2019 even more of a challenge.\nRecently, Coecke et al. (2010) used high level cross-disciplinary techniques from logic, category\nar X\niv :1\n10 6.\n40 58\nv1 [\ncs .C\nL ]\n2 0\nJu n\n20 11\ntheory, and physics to bring the above two approaches together. They developed a unified mathematical framework whereby a sentence vector is by definition a function of the Kronecker product of its word vectors. A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by Grefenstette et al. (2011). In this paper we implement it by training the model over the entire BNC. The highlight of our implementation is that words with relational types, such as verbs, adjectives, and adverbs are matrices that act on their arguments. We provide a general algorithm for building (or indeed learning) these matrices from the corpus.\nThe implementation is evaluated against the task provided by Mitchell and Lapata (2008) for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs. Our model improves on the best method evaluated in Mitchell and Lapata (2008) and offers promising results for the transitive case, demonstrating its scalability in comparison to that of other models. But we still feel there is need for a different class of experiments to showcase merits of compositionality in a statistically significant manner. Our work shows that the categorical compositional distributional model of meaning permits a practical implementation and that this opens the way to the production of large scale compositional models."}, {"heading": "2 Two Orthogonal Semantic Models", "text": "Formal Semantics To compute the meaning of a sentence consisting of n words, meanings of these words must interact with one another. In formal semantics, this further interaction is represented as a function derived from the grammatical structure of the sentence, but meanings of words are amorphous objects of the domain: no distinction is made between words that have the same type. Such models consist of a pairing of syntactic interpretation rules (in the form of a grammar) with semantic interpretation rules, as exemplified by the simple model presented in Figure 1.\nThe parse of a sentence such as \u201ccats like milk\u201d typically produces its semantic interpretation by substituting semantic representation for their grammatical constituents and applying \u03b2-reduction where needed. Such a derivation is shown in Figure 2.\nThis methodology is used to translate sentences of natural language into logical formulae, then use computer-aided automation tools to reason about them (Alshawi, 1992). One major drawback is that the result of such analysis can only deal with truth or falsity as the meaning of a sentence, and says nothing about the closeness in meaning or topic of expressions beyond their truth-conditions and what models satisfy them, hence do not perform well on language tasks such as search. Furthermore, an underlying domain of objects and a valuation function must be provided, as with any logic, leaving open the question of how we might learn the meaning of language using such a model, rather than just use it.\nDistributional Models Distributional models of semantics, on the other hand, dismiss the interaction between syntactically linked words and are solely concerned with lexical semantics. Word meaning is obtained empirically by examining the contexts1 in which a word appears, and equating the meaning of a word with the distribution of contexts it shares. The intuition is that context of use is what we appeal to in learning the meaning of a word, and that words that frequently have the same sort of context in common are likely to be semantically related.\nFor instance, beer and sherry are both drinks, alcoholic, and often cause a hangover. We expect these facts to be reflected in a sufficiently large corpus: the words \u2018beer\u2019 and \u2018sherry\u2019 occur within the\n1E.g. words which appear in the same sentence or n-word window, or words which hold particular grammatical or dependency relations to the word being learned.\ncontext of identifying words such as \u2018drink\u2019, \u2018alcoholic\u2019 and \u2018hangover\u2019 more frequently than they occur with other content words.\nSuch context distributions can be encoded as vectors in a high dimensional space with contexts as basis vectors. For any word vector \u2212\u2212\u2192 word, the scalar weight cwordi associated with each context basis vector \u2212\u2192ni is a function of the number of times the word has appeared in that context. Semantic vectors (cword1 , c word 2 , \u00b7 \u00b7 \u00b7 , cwordn ) are also denoted by sums of such weight/basis vector pairs:\n\u2212\u2212\u2192 word = \u2211 i cwordi \u2212\u2192ni\nLearning a semantic vector is just learning its basis weights from the corpus. This setting offers geometric means to reason about semantic similarity (e.g. via cosine measure or k-means clustering), as discussed in Widdows (2005).\nThe principal drawback of such models is their non-compositional nature: they ignore grammatical structure and logical words, and hence cannot compute the meanings of phrases and sentences in the same efficient way that they do for words. Common operations discussed in (Mitchell and Lapata, 2008) such as vector addition (+) and componentwise multiplication ( , cf. \u00a74 for details) are commutative, hence if \u2212\u2192vw = \u2212\u2192v + \u2212\u2192w or \u2212\u2192v \u2212\u2192w , then \u2212\u2192vw = \u2212\u2192wv, leading to unwelcome equalities such as\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 the dog bit the man = \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 the man bit the dog\nNon-commutative operations, such as the Kronecker product (cf. \u00a74 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007). However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing exponentially with sentence length hence quickly becoming computationally intractable."}, {"heading": "3 A Hybrid Logico-Distributional Model", "text": "Whereas semantic compositional mechanisms for set-theoretic constructions are well understood, there are no obvious corresponding methods for vector spaces. To solve this problem, Coecke et al.\n(2010) use the abstract setting of category theory to turn the grammatical structure of a sentence into a morphism compatible with the higher level logical structure of vector spaces.\nOne pragmatic consequence of this abstract idea is as follows. In distributional models, there is a meaning vector for each word, e.g. \u2212\u2192cats, \u2212\u2192like, and\u2212\u2212\u2192 milk. The logical recipe tells us to apply the meaning of the verb to the meanings of subject and object. But how can a vector apply to other vectors? The solution proposed above implies that one needs to have different levels of meaning for words with different types. This is similar to logical models where verbs are relations and nouns are atomic sets. So verb vectors should be built differently from noun vectors, for instance as matrices.\nThe general information as to which words should be matrices and which words atomic vectors is in fact encoded in the type-logical representation of the grammatical structure of the sentence. This is the linear map with word vectors as input and sentence vectors as output. Hence, at least theoretically, one should be able to build sentence vectors and compare their synonymity in exactly the same way as one measures word synonymity.\nPregroup Grammars The aforementioned linear maps turn out to be the grammatical reductions of a type-logic called a Lambek pregroup grammar (Lambek, 2008)2. Pregroups and vector spaces share the same high level mathematical structure, referred to as a compact closed category, for a proof and details of this claim see Coecke et al. (2010); for a friendly introduction to category theory, see Coecke and Paquette (2011). One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors.\nIn a nutshell, pregroup types are either atomic or compound. Atomic types can be simple (e.g. n for noun phrases, s for statements) or left/right superscripted\u2014referred to as adjoint types (e.g. nr and nl). An example of a compound type is that of a verb nrsnl. The superscripted types express that the verb is a relation with two arguments of type n,\n2The usage of pregroup types is not essential, the types of any other logic, for instance CCG can be used, but should be translated into the language of pregroups.\nwhich have to occur to the right and to the left of it, and that it outputs an argument of the type s. A transitive sentence has types as shown in Figure 3.\nEach type n cancels out with its right adjoint nr from the right and its left adjoint nl from the left; mathematically speaking these mean3\nnln \u2264 1 and nnr \u2264 1\nHere 1 is the unit of concatenation: 1n = n1 = n. The corresponding grammatical reduction of a transitive sentence is nnrsnl \u2264 1s1 = s. Each such reduction can be depicted as a wire diagram. The diagram of a transitive sentence is shown in Figure 3.\nSyntax-guided Semantic Composition According to Coecke et al. (2010) and based on a general completeness theorem between compact categories, wire diagrams, and vector spaces, the meaning of sentences can be canonically reduced to linear algebraic formulae. The following is the meaning vector of our transitive sentence:\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 cats like milk = (f) (\u2212\u2192cats\u2297\u2212\u2192like\u2297\u2212\u2212\u2192milk) (I) Here f is the linear map that encodes the grammatical structure. The categorical morphism corresponding to it is denoted by the tensor product of 3 components: V \u22971S\u2297 W , where V andW are subject and object spaces, S is the sentence space, the \u2019s are the cups, and 1S is the straight line in the diagram. The cups stand for taking inner products, which when done with the basis vectors imitate substitution. The straight line stands for the identity map that does nothing. By the rules of the category, equation (I) reduces to the following linear algebraic formula with\n3The relation\u2264 is the partial order of the pregroup. It corresponds to implication =\u21d2 in a logical reading thereof. If these inequalities are replaced by equalities, i.e. if nln = 1 = nnr , then the pregroup collapses into a group where nl = nr .\nlower dimensions, hence the dimensional explosion problem for Kronecker products is avoided:\n\u2211 itj citj\u3008 \u2212\u2192cats|\u2212\u2192vi \u3009\u2212\u2192st \u3008\u2212\u2192wj| \u2212\u2212\u2192 milk\u3009 \u2208 S (II)\n\u2212\u2192vi ,\u2212\u2192wj are basis vectors of V and W . The inner product \u3008\u2212\u2192cats | \u2212\u2192vi \u3009 substitutes the weights of\n\u2212\u2192cats into the first argument place of the verb (similarly for object and second argument place). \u2212\u2192st is a basis vector of the sentence space S in which meanings of sentences live, regardless of their grammatical structure.\nThe degree of synonymity of sentences is obtained by taking the cosine measure of their vectors. S is an abstract space: it needs to be instantiated to provide concrete meanings and synonymity measures. For instance, a truth-theoretic model is obtained by taking the sentence space S to be the 2- dimensional space with basis vectors |1\u3009 (True) and |0\u3009 (False)."}, {"heading": "4 Building Matrices for Relational Words", "text": "In this section we present a general scheme to build matrices for relational words. Recall that given a vector space A with basis {\u2212\u2192ni}i, the Kronecker product of two vectors \u2212\u2192v = \u2211 i c a i \u2212\u2192ni and \u2212\u2192w =\u2211\ni c b i \u2212\u2192ni is defined as follows:\n\u2212\u2192v \u2297\u2212\u2192w = \u2211 ij cai c b j ( \u2212\u2192ni \u2297\u2212\u2192nj)\nwhere (\u2212\u2192ni \u2297\u2212\u2192nj) is just the pairing of the basis of A, i.e. (\u2212\u2192ni ,\u2212\u2192nj). The Kronecker product vectors belong in the tensor product of A with itself: A\u2297A, hence ifA has dimension r, these will be of dimensionality r\u00d7r. The point-wise multiplication of these vectors is defined as follows\n\u2212\u2192v \u2212\u2192w = \u2211 i cai c b i \u2212\u2192ni\nThe intuition behind having a matrix for a relational word is that any relation R on sets X and Y , i.e. R \u2286 X \u00d7 Y can be represented as a matrix, namely one that has as row-bases x \u2208 X and as column-bases y \u2208 Y , with weight cxy = 1 where (x, y) \u2208 R and 0 otherwise. In a distributional setting, the weights, which are natural or real numbers,\nwill represent more: \u2018the extent according to which x and y are related\u2019. This can be determined in different ways.\nSuppose X is the set of animals, and \u2018chase\u2019 is a relation on it: chase \u2286 X \u00d7 X . Take x = \u2018dog\u2019 and y = \u2018cat\u2019: with our type-logical glasses on, the obvious choice would be to take cxy to be the number of times \u2018dog\u2019 has chased \u2018cat\u2019, i.e. the number of times the sentence \u2018the dog chases the cat\u2019 has appeared in the corpus. But in the distributional setting, this method will be too syntactic and dismissive of the actual meaning of \u2018cat\u2019 and \u2018dog\u2019. If instead the corpus contains the sentence \u2018the hound hunted the wild cat\u2019, cxy will be 0, restricting us to only assign meaning to sentences that have directly appeared in the corpus. We propose to, instead, use a level of abstraction by taking words such as verbs to be distributions over the semantic information in the vectors of their context words, rather than over the context words themselves.\nStart with an r-dimensional vector space N with basis {\u2212\u2192n i}i, in which meaning vectors of atomic words, such as nouns, live. The basis vectors of N are in principle all the words from the corpus, however in practice and following Mitchell and Lapata (2008) we had to restrict these to a subset of the most occurring words. These basis vectors are not restricted to nouns: they can as well be verbs, adjectives, and adverbs, so that we can define the meaning of a noun in all possible contexts\u2014as is usual in context-based models\u2014and not only in the context of other nouns. Note that basis words with relational types are treated as pure lexical items rather than as semantic objects represented as matrices. In short, we count how many times a noun has occurred close to words of other syntactic types such as \u2018elect\u2019 and \u2018scientific\u2019, rather than count how many times it has occurred close to their corresponding matrices: it is the lexical tokens that form the context, not their meaning.\nEach relational word P with grammatical type \u03c0 and m adjoint types \u03b11, \u03b12, \u00b7 \u00b7 \u00b7 , \u03b1m is encoded as an (r \u00d7 . . .\u00d7 r) matrix with m dimensions. Since our vector space N has a fixed basis, each such ma-\ntrix is represented in vector form as follows:\n\u2212\u2192 P = \u2211 ij \u00b7 \u00b7 \u00b7 \u03b6\ufe38 \ufe37\ufe37 \ufe38\nm\ncij\u00b7\u00b7\u00b7\u03b6 ( \u2212\u2192n i \u2297\u2212\u2192n j \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u2212\u2192n \u03b6)\ufe38 \ufe37\ufe37 \ufe38\nm\nThis vector lives in the tensor space N \u2297N \u2297 \u00b7 \u00b7 \u00b7 \u2297N\ufe38 \ufe37\ufe37 \ufe38\nm\n. Each cij\u00b7\u00b7\u00b7\u03b6 is computed\naccording to the procedure described in Figure 4.\nLinear algebraically, this procedure corresponds to computing the following\n\u2212\u2192 P = \u2211 k (\u2212\u2192w 1 \u2297\u2212\u2192w 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u2212\u2192wm)k Type-logical examples of relational words are verbs, adjectives, and adverbs. A transitive verb is represented as a 2 dimensional matrix since its type is nrsnl with two adjoint types nr and nl. The corresponding vector of this matrix is\n\u2212\u2212\u2192 verb = \u2211 ij cij ( \u2212\u2192n i \u2297\u2212\u2192n j)\nThe weight cij corresponding to basis vector\u2212\u2192n i\u2297\u2212\u2192n j , is the extent according to which words that have co-occurred with \u2212\u2192n i have been the subject of the \u2018verb\u2019 and words that have co-occurred with \u2212\u2192n j have been the object of the \u2018verb\u2019. This example computation is demonstrated in Figure 5.\nLinear algebraically, we are computing\n\u2212\u2212\u2192 verb = \u2211 k (\u2212\u2192w 1 \u2297\u2212\u2192w 2)k As an example, consider the verb \u2018show\u2019 and suppose there are two \u2018show\u2019-relations in the corpus:\ns1 = table show result s2 = map show location\nThe vector of \u2018show\u2019 is\n\u2212\u2212\u2192 show = \u2212\u2212\u2192 table\u2297\u2212\u2212\u2212\u2192result + \u2212\u2212\u2192map\u2297\u2212\u2212\u2212\u2212\u2192location\nConsider an N space with four basis vectors \u2018far\u2019, \u2018room\u2019, \u2018scientific\u2019, and \u2018elect\u2019. The TF/IDFweighted values for vectors of the above four nouns (built from the BNC) are as shown in Table 1.\nPart of the matrix of \u2018show\u2019 is presented in Table 2.\nAs a sample computation, the weight c11 for vector (1, 1), i.e. ( \u2212\u2192 far, \u2212\u2192 far) is computed by multiplying weights of \u2018table\u2019 and \u2018result\u2019 on \u2212\u2192 far, i.e. 6.6\u00d77,\nmultiplying weights of \u2018map\u2019 and \u2018location\u2019 on \u2212\u2192 far, i.e. 5.6 \u00d7 5.9 then adding these 46.2 + 33.04 and obtaining the total weight 79.24.\nThe same method is applied to build matrices for ditransitive verbs, which will have 3 dimensions, and adjectives and adverbs, which will be of 1 dimension each."}, {"heading": "5 Computing Sentence Vectors", "text": "Meaning of sentences are vectors computed by taking the variables of the categorical prescription of meaning (the linear map f obtained from the grammatical reduction of the sentence) to be determined by the matrices of the relational words. For instance the meaning of the transitive sentence \u2018sub verb obj\u2019 is:\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 sub verb obj = \u2211 itj \u3008\u2212\u2192sub | \u2212\u2192v i\u3009\u3008\u2212\u2192w j | \u2212\u2192 obj\u3009 citj\u2212\u2192s t\nWe take V := W := N and S = N \u2297 N , then\u2211 itj citj\n\u2212\u2192s t is determined by the matrix of the verb, i.e. substitute it by \u2211 ij cij(\n\u2212\u2192n i \u2297 \u2212\u2192n j)4. Hence \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 sub verb obj becomes:\u2211\nij \u3008\u2212\u2192sub | \u2212\u2192n i\u3009\u3008\u2212\u2192n j | \u2212\u2192 obj\u3009cij(\u2212\u2192n i \u2297\u2212\u2192n j) =\u2211\nij\ncsubi c obj j cij( \u2212\u2192n i \u2297\u2212\u2192n j)\nThis can be decomposed to point-wise multiplication of two vectors as follows:(\u2211\nij\ncsubi c obj j ( \u2212\u2192n i\u2297\u2212\u2192n j)\n) (\u2211\nij\ncij( \u2212\u2192n i\u2297\u2212\u2192n j) ) 4Note that by doing so we are also reducing the verb space from N \u2297 (N \u2297N)\u2297N to N \u2297N , since for our construction we only need tuples of the form \u2212\u2192n i \u2297\u2212\u2192n i \u2297\u2212\u2192n j \u2297\u2212\u2192n j which are isomorphic to pairs (\u2212\u2192n i \u2297\u2212\u2192n j).\nThe left argument is the Kronecker product of subject and object vectors and the right argument is the vector of the verb, so we obtain(\u2212\u2192\nsub\u2297\u2212\u2192obj ) \u2212\u2212\u2192verb\nSince is commutative, this provides us with a distributional version of the type-logical meaning of the sentence: point-wise multiplication of the meaning of the verb to the Kronecker product of its subject and object:\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 sub verb obj = \u2212\u2212\u2192 verb (\u2212\u2192 sub\u2297\u2212\u2192obj ) This mathematical operation can be informally described as a structured \u2018mixing\u2019 of the information of the subject and object, followed by it being \u2018filtered\u2019 through the information of the verb applied to them, in order to produce the information of the sentence. In the transitive case, S = N \u2297 N , hence \u2212\u2192s t =\u2212\u2192n i \u2297 \u2212\u2192n j . More generally, the vector space corresponding to the abstract sentence space S is the concrete tensor space (N \u2297 . . .\u2297N) for m the dimension of the matrix of the \u2018verb\u2019. As we have seen above, in practice we do not need to build this tensor space, as the computations thereof reduce to point-wise multiplications and summations.\nSimilar computations yield meanings of sentences with adjectives and adverbs. For instance the meaning of a transitive sentence with a modified subject and a modified verb we have \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 adj sub verb obj adv =(\u2212\u2192\nadv \u2212\u2212\u2192verb ) ((\u2212\u2192 adj \u2212\u2192sub ) \u2297\u2212\u2192obj ) After building vectors for sentences, we can compare their meaning and measure their degree of synonymy by taking their cosine measure."}, {"heading": "6 Evaluation", "text": "Evaluating such a framework is no easy task. What to evaluate depends heavily on what sort of application a practical instantiation of the model is geared towards. In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally\nbuilt sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset. Following this, we present a new evaluation task extending the experimental methodology of Mitchell and Lapata (2008) to transitive verb-centric sentences, and compare our model to those discussed by Mitchell and Lapata (2008) within this new experiment.\nFirst Dataset Description The first experiment, described in detail by Mitchell and Lapata (2008), evaluates how well compositional models disambiguate ambiguous words given the context of a potentially disambiguating noun. Each entry of the dataset provides a noun, a target verb and landmark verb (both intransitive). The noun must be composed with both verbs to produce short phrase vectors the similarity of which is measured by the candidate. Also provided with each entry is a classification (\u201cHigh\u201d or \u201cLow\u201d) indicating whether or not the verbs are indeed semantically close within the context of the noun, as well as an evaluator-set similarity score between 1 and 7 (along with an evaluator identifier), where 1 is low similarity and 7 is high.\nEvaluation Methodology Candidate models provide a similarity score for each entry. The scores of high similarity entries and low similarity entries are averaged to produce a mean High score and mean Low score for the model. The correlation of the model\u2019s similarity judgements with the human judgements is also calculated using Spearman\u2019s \u03c1, a metric which is deemed to be more scrupulous, and ultimately that by which models should be ranked, by Mitchell and Lapata (2008). The mean for each model is on a [0, 1] scale, except for UpperBound which is on the same [1, 7] scale the annotators used. The \u03c1 scores are on a [\u22121, 1] scale. It is assumed that inter-annotator agreement provides the theoretical maximum \u03c1 for any model for this experiment. The cosine measure of the verb vectors, ignoring the noun, is taken to be the baseline (no composition).\nOther Models The other models we compare ours to are those evaluated by Mitchell and Lapata (2008). We provide a selection of the results\nfrom that paper for the worst (Add) and best5 (Multiply) performing models, as well as the previous second-best performing model (Kintsch). The additive and multiplicative models are simply applications of vector addition and component-wise multiplication. We invite the reader to consult (Mitchell and Lapata, 2008) for the description of Kintsch\u2019s additive model and parametric choices.\nModel Parameters To provide the most accurate comparison with the existing multiplicative model, and exploiting the aforementioned feature that the categorical model can be built \u201con top of\u201d existing lexical distributional models, we used the parameters described by Mitchell and Lapata (2008) to reproduce the vectors evaluated in the original experiment as our noun vectors. All vectors were built from a lemmatised version of the BNC. The noun basis was the 2000 most common context words, basis weights were the probability of context words given the target word divided by the overall probability of the context word. Intransitive verb functionvectors were trained using the procedure presented in \u00a74. Since the dataset only contains intransitive verbs and nouns, we used S = N . The cosine measure of vectors was used as a similarity metric.\nFirst Experiment Results In Table 3 we present the comparison of the selected models. Our categorical model performs significantly better than the existing second-place (Kintsch) and obtains a \u03c1 quasiidentical to the multiplicative model, indicating significant correlation with the annotator scores.\nThere is not a large difference between the mean High score and mean Low score, but the distribution in Figure 6 shows that our model makes a non-negligible distinction between high similarity phrases and low similarity phrases, despite the absolute scores not being different by more than a few percentiles.\n5The multiplicative model presented here is what is qualified as best in (Mitchell and Lapata, 2008). However, they also present a slightly better performing (\u03c1 = 0.19) model which is a combination of their multiplicative model and a weighted additive model. The difference in \u03c1 is qualified as \u201cnot statistically significant\u201d in the original paper, and furthermore the mixed model requires parametric optimisation hence was not evaluated against the entire test set. For these reasons, we chose not to include it in the comparison.\nSecond Dataset Description The second dataset6, developed by the authors, follows the format of the (Mitchell and Lapata, 2008) dataset used for the first experiment, with the exception that the target and landmark verbs are transitive, and an object noun is provided in addition to the subject noun, hence forming a small transitive sentence. The dataset comprises 200 entries consisting of sentence pairs (hence a total of 400 sentences) constructed by following the procedure outlined in \u00a74 of (Mitchell and Lapata, 2008), using transitive verbs from CELEX7. For examples of these sentences, see Table 4. The dataset was split into four sections of 100 entries each, with guaranteed 50% exclusive overlap with\n6http://www.cs.ox.ac.uk/activities/CompD istMeaning/GS2011data.txt\n7http://celex.mpi.nl/\nexactly two other datasets. Each section was given to a group of evaluators, with a total of 25, who were asked to form simple transitive sentence pairs from the verbs, subject and object provided in each entry; for instance \u2018the table showed the result\u2019 from \u2018table show result\u2019. The evaluators were then asked to rate the semantic similarity of each verb pair within the context of those sentences, and offer a score between 1 and 7 for each entry. Each entry was given an arbitrary classification of HIGH or LOW by the authors, for the purpose of calculating mean high/low scores for each model. For example, the first two pairs in table 4 were classified as HIGH, whereas the second two pairs as LOW.\nEvaluation Methodology The evaluation methodology for the second experiment was identical to that of the first, as are the scales for means and scores. Here also, Spearman\u2019s \u03c1 is deemed a more rigorous way of determining how well a model tracks difference in meaning. This is both because of the imprecise nature of the classification of verb pairs as HIGH or LOW; and since the objective similarity scores produced by a model that distinguishes sentences of different meaning from those of similar meaning can be renormalised in practice. Therefore the delta between HIGH means and LOW mean cannot serve as a definite indication of the practical applicability (or lack thereof) of semantic models; the means are provided just to aid comparison with the results of the first experiment.\nModel Parameters As in the first experiment, the lexical vectors from (Mitchell and Lapata, 2008) were used for the other models evaluated (additive, multiplicative and baseline)8 and for the noun vec-\n8Kintsch was not evaluated as it required optimising model parameters against a held-out segment of the test set, and we could not replicate the methodology of Mitchell and Lapata\ntors of our categorical model. Transitive verb vectors were trained as described in \u00a74 with S = N\u2297N .\nSecond Experiment Results The results for the models evaluated against the second dataset are presented in Table 5.\nWe observe a significant (according to p < 0.0.5) improvement in the alignment of our categorical model with the human judgements, from 0.17 to 0.21. The additive model continues to make little distinction between senses of the verb during composition, and the multiplicative model\u2019s alignment does not change, but becomes statistically indistinguishable from the non-compositional baseline model.\nOnce again we note that the high-low means are not very indicative of model performance, as the difference between high mean and the low mean of the categorical model is much smaller than that of the both the baseline model and multiplicative model, despite better alignment with annotator judgements."}, {"heading": "7 Discussion", "text": "In this paper, we described an implementation of the categorical model of meaning (Coecke et al., 2010), which combines the formal logical and the empirical distributional frameworks into a unified semantic model. The implementation is based on building matrices for words with relational types (adjectives, verbs), and vectors for words with atomic types (nouns), based on data from the BNC. We then show how to apply verbs to their subject/object, in order to compute the meaning of intransitive and transitive sentences.\n(2008) with full confidence.\nOther work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases. Our approach easily applies to such compositions, as well as to sentences containing combinations of adjectives, nouns, verbs, and adverbs. The other key difference is that they learn their matrices in a top-down fashion, i.e. by regression from the composite adjective-noun context vectors, whereas our model is bottom-up: it learns sentence/phrase meaning compositionally from the vectors of the compartments of the composites. Finally, very similar functions, for example a verb with argument alternations such as \u2018break\u2019 in \u2018Y breaks\u2019 and \u2018X breaks Y\u2019, are not treated as unrelated. The matrix of the intransitive \u2018break\u2019 uses the corpusobserved information about the subject of break, including that of \u2018Y\u2019, similarly the matrix of the transitive \u2018break\u2019 uses information about its subject and object, including that of \u2018X\u2019 and \u2018Y\u2019. We leave a thorough study of these phenomena, which fall under providing a modular representation of passiveactive similarities, to future work.\nWe evaluated our model in two ways: first against the word disambiguation task of Mitchell and Lapata (2008) for intransitive verbs, and then against a similar new experiment for transitive verbs, which we developed.\nOur findings in the first experiment show that the categorical method performs on par with the leading existing approaches. This should not surprise us given that the context is so small and our method becomes similar to the multiplicative model of Mitchell and Lapata (2008). However, our approach is sensitive to grammatical structure, leading us to develop a second experiment taking this into account and differentiating it from models with commutative composition operations.\nThe second experiment\u2019s results deliver the expected qualitative difference between models, with our categorical model outperforming the others and showing an increase in alignment with human judgements in correlation with the increase in sentence complexity. We use this second evaluation principally to show that there is a strong case for the development of more complex experiments measuring not only the disambiguating qualities of compositional models, but also their syntactic sensitivity, which is not directly measured in the existing experiments.\nThese results show that the high level categorical distributional model, uniting empirical data with logical form, can be implemented just like any other concrete model. Furthermore it shows better results in experiments involving higher syntactic complexity. This is just the tip of the iceberg: the mathematics underlying the implementation ensures that it uniformly scales to larger, more complicated sentences and enables it to compare synonymity of sentences that are of different grammatical structure."}, {"heading": "8 Future Work", "text": "Treatment of function words such as \u2018that\u2019, \u2018who\u2019, as well as logical words such as quantifiers and conjunctives are left to future work. This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005). It is not yet entirely clear how existing set-theoretic approaches, for example that of discourse representation and generalised quantifiers, apply to our setting. Preliminary work on integration of the two has been presented by Preller (2007) and more recently also by Preller and Sadrzadeh ( 2009).\nAs mentioned by one of the reviewers, our pregroup approach to grammar flattens the sentence representation, in that the verb is applied to its subject and object at the same time; whereas in other approaches such as CCG, it is first applied to the object to produce a verb phrase, then applied to the subject to produce the sentence. The advantages and disadvantages of this method and comparisons with other systems, in particular CCG, constitutes ongoing work."}, {"heading": "9 Acknowledgement", "text": "We wish to thank P. Blunsom, S. Clark, B. Coecke, S. Pulman, and the anonymous EMNLP reviewers for discussions and comments. Support from EPSRC grant EP/F042728/1 is gratefully acknowledged by M. Sadrzadeh."}], "references": [{"title": "Combining Symbolic and Distributional Models of Meaning", "author": ["S. Clark", "S. Pulman."], "venue": "Proceedings of AAAI Spring Symposium on Quantum Interaction. AAAI Press.", "citeRegEx": "Clark and Pulman.,? 2007", "shortCiteRegEx": "Clark and Pulman.", "year": 2007}, {"title": "Categories for the Practicing Physicist", "author": ["B. Coecke", "E. Paquette."], "venue": "New Structures for Physics, 167271. B. Coecke (ed.). Lecture Notes in Physics 813. Springer.", "citeRegEx": "Coecke and Paquette.,? 2011", "shortCiteRegEx": "Coecke and Paquette.", "year": 2011}, {"title": "Mathematical Foundations for Distributed Compositional Model of Meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark."], "venue": "Lambek Festschrift. Linguistic Analysis 36, 345\u2013384. J. van Benthem, M. Moortgat and W. Buszkowski (eds.).", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "From Distributional to Semantic Similarity", "author": ["J. Curran."], "venue": "PhD Thesis, University of Edinburgh.", "citeRegEx": "Curran.,? 2004", "shortCiteRegEx": "Curran.", "year": 2004}, {"title": "A Structured Vector Space Model for Word Meaning in Context", "author": ["K. Erk", "S. Pad\u00f3."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 897\u2013906.", "citeRegEx": "Erk and Pad\u00f3.,? 2004", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2004}, {"title": "\u00dcber Sinn und Bedeutung", "author": ["G. Frege"], "venue": "Zeitschrift f\u00fcr Philosophie und philosophische Kritik 100.", "citeRegEx": "Frege,? 1892", "shortCiteRegEx": "Frege", "year": 1892}, {"title": "A synopsis of linguistic theory 19301955", "author": ["J.R. Firth."], "venue": "Studies in Linguistic Analysis.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Concrete Compositional Sentence Spaces for a Compositional Distributional Model of Meaning", "author": ["E. Grefenstette", "M. Sadrzadeh", "S. Clark", "B. Coecke", "S. Pulman."], "venue": "International Conference on Computational Semantics (IWCS\u201911). Oxford.", "citeRegEx": "Grefenstette et al\\.,? 2011", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["G. Grefenstette."], "venue": "Kluwer.", "citeRegEx": "Grefenstette.,? 1994", "shortCiteRegEx": "Grefenstette.", "year": 1994}, {"title": "A Regression Model of AdjectiveNoun Compositionality in Distributional Semantics", "author": ["E. Guevara."], "venue": "Proceedings of the ACL GEMS Workshop.", "citeRegEx": "Guevara.,? 2010", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "A Cycling Cancellation-Automaton for Sentence Well-Formedness", "author": ["Z.S. Harris."], "venue": "International Computation Centre Bulletin 5, 69\u201394.", "citeRegEx": "Harris.,? 1966", "shortCiteRegEx": "Harris.", "year": 1966}, {"title": "Word Grammar", "author": ["R. Hudson."], "venue": "Blackwell.", "citeRegEx": "Hudson.,? 1984", "shortCiteRegEx": "Hudson.", "year": 1984}, {"title": "From Word to Sentence", "author": ["J. Lambek."], "venue": "Polimetrica, Milan.", "citeRegEx": "Lambek.,? 2008", "shortCiteRegEx": "Lambek.", "year": 2008}, {"title": "A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T. Landauer", "S. Dumais."], "venue": "Psychological review.", "citeRegEx": "Landauer and Dumais.,? 2008", "shortCiteRegEx": "Landauer and Dumais.", "year": 2008}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze."], "venue": "Cambridge University Press.", "citeRegEx": "Manning et al\\.,? 2008", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, 236\u2013244.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "English as a formal language", "author": ["R. Montague."], "venue": "Formal Philosophy, 189\u2013223.", "citeRegEx": "Montague.,? 1974", "shortCiteRegEx": "Montague.", "year": 1974}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["J. Nivre."], "venue": "Proceedings of the 8th International Workshop on Parsing Technologies (IWPT).", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Word Segmentation: The role of distributional cues", "author": ["J. Saffron", "E. Newport", "R. Asling."], "venue": "Journal of Memory and Language 35, 606\u2013621.", "citeRegEx": "Saffron et al\\.,? 1999", "shortCiteRegEx": "Saffron et al\\.", "year": 1999}, {"title": "Automatic Word Sense Discrimination", "author": ["H. Schuetze."], "venue": "Computational Linguistics 24, 97\u2013123.", "citeRegEx": "Schuetze.,? 1998", "shortCiteRegEx": "Schuetze.", "year": 1998}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems", "author": ["P. Smolensky."], "venue": "Computational Linguistics 46, 1\u2013 2, 159\u2013216.", "citeRegEx": "Smolensky.,? 1990", "shortCiteRegEx": "Smolensky.", "year": 1990}, {"title": "The Syntactic Process", "author": ["M. Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Geometry and Meaning", "author": ["D. Widdows."], "venue": "University of Chicago Press.", "citeRegEx": "Widdows.,? 2005", "shortCiteRegEx": "Widdows.", "year": 2005}, {"title": "Philosophical Investigations", "author": ["L. Wittgenstein."], "venue": "Blackwell.", "citeRegEx": "Wittgenstein.,? 1953", "shortCiteRegEx": "Wittgenstein.", "year": 1953}], "referenceMentions": [{"referenceID": 2, "context": "We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it.", "startOffset": 47, "endOffset": 68}, {"referenceID": 2, "context": "We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.", "startOffset": 47, "endOffset": 344}, {"referenceID": 14, "context": "Search engines such as Google either fall back on bag of words models\u2014ignoring syntax and lexical relations\u2014or exploit superficial models of lexical semantics to retrieve pages with terms related to those in the query (Manning et al., 2008).", "startOffset": 218, "endOffset": 240}, {"referenceID": 16, "context": "Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question.", "startOffset": 38, "endOffset": 68}, {"referenceID": 12, "context": "Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question.", "startOffset": 38, "endOffset": 68}, {"referenceID": 6, "context": "Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question.", "startOffset": 116, "endOffset": 144}, {"referenceID": 5, "context": "Compositional formal semantic models stem from classical ideas from mathematical logic, mainly Frege\u2019s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892).", "startOffset": 186, "endOffset": 199}, {"referenceID": 23, "context": "Distributional models are more recent and can be related to Wittgenstein\u2019s later philosophy of \u2018meaning is use\u2019, whereby meanings of words can be determined from their context (Wittgenstein, 1953).", "startOffset": 176, "endOffset": 196}, {"referenceID": 8, "context": "The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al.", "startOffset": 105, "endOffset": 139}, {"referenceID": 3, "context": "The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al.", "startOffset": 105, "endOffset": 139}, {"referenceID": 14, "context": "The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al., 2008).", "startOffset": 254, "endOffset": 276}, {"referenceID": 2, "context": "Recently, Coecke et al. (2010) used high level cross-disciplinary techniques from logic, category ar X iv :1 10 6.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": "A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by Grefenstette et al. (2011). In this paper we implement it by training the model over the entire BNC.", "startOffset": 88, "endOffset": 115}, {"referenceID": 15, "context": "The implementation is evaluated against the task provided by Mitchell and Lapata (2008) for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs.", "startOffset": 61, "endOffset": 88}, {"referenceID": 15, "context": "The implementation is evaluated against the task provided by Mitchell and Lapata (2008) for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs. Our model improves on the best method evaluated in Mitchell and Lapata (2008) and offers promising results for the transitive case, demonstrating its scalability in comparison to that of other models.", "startOffset": 61, "endOffset": 263}, {"referenceID": 15, "context": "Common operations discussed in (Mitchell and Lapata, 2008) such as vector addition (+) and componentwise multiplication ( , cf.", "startOffset": 31, "endOffset": 58}, {"referenceID": 21, "context": "via cosine measure or k-means clustering), as discussed in Widdows (2005). The principal drawback of such models is their non-compositional nature: they ignore grammatical structure and logical words, and hence cannot compute the meanings of phrases and sentences in the same efficient way that they do for words.", "startOffset": 59, "endOffset": 74}, {"referenceID": 20, "context": "\u00a74 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007).", "startOffset": 52, "endOffset": 69}, {"referenceID": 0, "context": "\u00a74 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007). However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing exponentially with sentence length hence quickly becoming computationally intractable.", "startOffset": 133, "endOffset": 157}, {"referenceID": 2, "context": "To solve this problem, Coecke et al. (2010) use the abstract setting of category theory to turn the grammatical structure of a sentence into a morphism compatible with the higher level logical structure of vector spaces.", "startOffset": 23, "endOffset": 44}, {"referenceID": 12, "context": "Pregroup Grammars The aforementioned linear maps turn out to be the grammatical reductions of a type-logic called a Lambek pregroup grammar (Lambek, 2008)2.", "startOffset": 140, "endOffset": 154}, {"referenceID": 1, "context": "Pregroups and vector spaces share the same high level mathematical structure, referred to as a compact closed category, for a proof and details of this claim see Coecke et al. (2010); for a friendly introduction to category theory, see Coecke and Paquette (2011).", "startOffset": 162, "endOffset": 183}, {"referenceID": 1, "context": "(2010); for a friendly introduction to category theory, see Coecke and Paquette (2011). One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors.", "startOffset": 60, "endOffset": 87}, {"referenceID": 2, "context": "Syntax-guided Semantic Composition According to Coecke et al. (2010) and based on a general completeness theorem between compact categories, wire diagrams, and vector spaces, the meaning of sentences can be canonically reduced to linear algebraic formulae.", "startOffset": 48, "endOffset": 69}, {"referenceID": 15, "context": "The basis vectors of N are in principle all the words from the corpus, however in practice and following Mitchell and Lapata (2008) we had to restrict these to a subset of the most occurring words.", "startOffset": 105, "endOffset": 132}, {"referenceID": 7, "context": "In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008).", "startOffset": 3, "endOffset": 30}, {"referenceID": 7, "context": "In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset.", "startOffset": 4, "endOffset": 297}, {"referenceID": 7, "context": "In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset. Following this, we present a new evaluation task extending the experimental methodology of Mitchell and Lapata (2008) to transitive verb-centric sentences, and compare our model to those discussed by Mitchell and Lapata (2008) within this new experiment.", "startOffset": 4, "endOffset": 503}, {"referenceID": 7, "context": "In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset. Following this, we present a new evaluation task extending the experimental methodology of Mitchell and Lapata (2008) to transitive verb-centric sentences, and compare our model to those discussed by Mitchell and Lapata (2008) within this new experiment.", "startOffset": 4, "endOffset": 612}, {"referenceID": 15, "context": "First Dataset Description The first experiment, described in detail by Mitchell and Lapata (2008), evaluates how well compositional models disambiguate ambiguous words given the context of a potentially disambiguating noun.", "startOffset": 71, "endOffset": 98}, {"referenceID": 15, "context": "The correlation of the model\u2019s similarity judgements with the human judgements is also calculated using Spearman\u2019s \u03c1, a metric which is deemed to be more scrupulous, and ultimately that by which models should be ranked, by Mitchell and Lapata (2008). The mean for each model is on a [0, 1] scale, except for UpperBound which is on the same [1, 7] scale the annotators used.", "startOffset": 223, "endOffset": 250}, {"referenceID": 15, "context": "Other Models The other models we compare ours to are those evaluated by Mitchell and Lapata (2008). We provide a selection of the results", "startOffset": 72, "endOffset": 99}, {"referenceID": 15, "context": "We invite the reader to consult (Mitchell and Lapata, 2008) for the description of Kintsch\u2019s additive model and parametric choices.", "startOffset": 32, "endOffset": 59}, {"referenceID": 15, "context": "Model Parameters To provide the most accurate comparison with the existing multiplicative model, and exploiting the aforementioned feature that the categorical model can be built \u201con top of\u201d existing lexical distributional models, we used the parameters described by Mitchell and Lapata (2008) to reproduce the vectors evaluated in the original experiment as our noun vectors.", "startOffset": 267, "endOffset": 294}, {"referenceID": 15, "context": "The multiplicative model presented here is what is qualified as best in (Mitchell and Lapata, 2008).", "startOffset": 72, "endOffset": 99}, {"referenceID": 15, "context": "Table 3: Selected model means for High and Low similarity items and correlation coefficients with human judgements, first experiment (Mitchell and Lapata, 2008).", "startOffset": 133, "endOffset": 160}, {"referenceID": 15, "context": "Second Dataset Description The second dataset6, developed by the authors, follows the format of the (Mitchell and Lapata, 2008) dataset used for the first experiment, with the exception that the target and landmark verbs are transitive, and an object noun is provided in addition to the subject noun, hence forming a small transitive sentence.", "startOffset": 100, "endOffset": 127}, {"referenceID": 15, "context": "The dataset comprises 200 entries consisting of sentence pairs (hence a total of 400 sentences) constructed by following the procedure outlined in \u00a74 of (Mitchell and Lapata, 2008), using transitive verbs from CELEX7.", "startOffset": 153, "endOffset": 180}, {"referenceID": 15, "context": "Model Parameters As in the first experiment, the lexical vectors from (Mitchell and Lapata, 2008) were used for the other models evaluated (additive, multiplicative and baseline)8 and for the noun vec-", "startOffset": 70, "endOffset": 97}, {"referenceID": 2, "context": "In this paper, we described an implementation of the categorical model of meaning (Coecke et al., 2010), which combines the formal logical and the empirical distributional frameworks into a unified semantic model.", "startOffset": 82, "endOffset": 103}, {"referenceID": 9, "context": "Other work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases.", "startOffset": 42, "endOffset": 86}, {"referenceID": 15, "context": "We evaluated our model in two ways: first against the word disambiguation task of Mitchell and Lapata (2008) for intransitive verbs, and then against a similar new experiment for transitive verbs, which we developed.", "startOffset": 82, "endOffset": 109}, {"referenceID": 15, "context": "This should not surprise us given that the context is so small and our method becomes similar to the multiplicative model of Mitchell and Lapata (2008). However, our approach is sensitive to grammatical structure, leading us to develop a second experiment taking this into account and differentiating it from models with commutative composition operations.", "startOffset": 125, "endOffset": 152}, {"referenceID": 2, "context": "This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005).", "startOffset": 52, "endOffset": 73}, {"referenceID": 2, "context": "This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005). It is not yet entirely clear how existing set-theoretic approaches, for example that of discourse representation and generalised quantifiers, apply to our setting.", "startOffset": 52, "endOffset": 127}, {"referenceID": 2, "context": "This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005). It is not yet entirely clear how existing set-theoretic approaches, for example that of discourse representation and generalised quantifiers, apply to our setting. Preliminary work on integration of the two has been presented by Preller (2007) and more recently also by Preller and Sadrzadeh ( 2009).", "startOffset": 52, "endOffset": 372}], "year": 2015, "abstractText": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.", "creator": "TeX"}}}