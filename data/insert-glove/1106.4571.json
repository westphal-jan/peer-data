{"id": "1106.4571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2011", "title": "Acquiring Word-Meaning Mappings for Natural Language Interfaces", "abstract": "davaa This nenana paper proll focuses 37.56 on pertuis a 3.145 system, WOLFIE (WOrd Learning partitions From neustar Interpreted frederiksberg Examples ), that fishponds acquires lina a three-metre semantic lexicon champigny from mock-heroic a corpus tufton of sentences \u00e7iller paired ultra-short with srun semantic falkowski representations. The margaritidae lexicon gomoh learned consists keong of urbanism phrases paired with mahals meaning 6/10 representations. kandiah WOLFIE is su-22 part of mcpeek an integrated rudai system beat that 37.84 learns 29a to transform sentences into representations such as logical database kingshighway queries. wesen Experimental service results are dragostea presented demonstrating gustatory WOLFIE ' eleftherotypia s ability to elektronik learn maure useful lexicons for a database interface metra in eyeglasses four tanovic different ra'ad natural languages. The usefulness gartland of the lexicons learned by WOLFIE are 90.42 compared to wujek those 17.93 acquired all-metro by 11.63 a malpighiales similar 1/7 system, fabius with coeurs results kreindler favorable boerhaave to WOLFIE. A second rataje set nutrient-rich of experiments demonstrates nemec WOLFIE ' -------------------------------------------------- s firor ability arborists to conform scale arroyos to peeves larger and faliva more difficult, prionopidae albeit p\u00f3voa artificially hcf generated, corpora. ratel In nakasend\u014d natural frac34 language shiwan acquisition, lent it antithetical is difficult vosganian to matua gather the crovan annotated data needed for supervised nicodemus learning; vintil\u0103 however, dongli unannotated data unimas is fairly plentiful. Active waiapu learning methods supplication attempt to 3,337 select for annotation edith and training bbundy only cadent the forever most informative intensify examples, bantoid and yukhary therefore ninety-eight are potentially seric very useful in natural kanno language diffusa applications. bieger However, most shifnal results bolthouse to alcmaeon date jaycie for active learning have only considered morocco standard classification tasks. To five-card reduce nuttall annotation effort while maintaining accuracy, we guayabal apply active electrospinning learning to semantic 1306 lexicons. screwballs We unwelcoming show radebaugh that lauzun active learning falae can significantly endorsing reduce ratnagiri the heiligenstadt number of annotated examples sider required to nunez achieve a nunno given level corinna of upsetter performance.", "histories": [["v1", "Wed, 22 Jun 2011 20:57:18 GMT  (226kb)", "http://arxiv.org/abs/1106.4571v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["c thompson"], "accepted": false, "id": "1106.4571"}, "pdf": {"name": "1106.4571.pdf", "metadata": {"source": "CRF", "title": "Acquiring Word-Meaning Mappings for Natural Language Interfaces", "authors": ["Cynthia A. Thompson", "Raymond J. Mooney"], "emails": ["cindi@cs.utah.edu", "mooney@cs.utexas.edu"], "sections": [{"heading": null, "text": "Experimental results are presented demonstrating Wolfie\u2019s ability to learn useful lexicons for a database interface in four different natural languages. The usefulness of the lexicons learned by Wolfie are compared to those acquired by a similar system, with results favorable to Wolfie. A second set of experiments demonstrates Wolfie\u2019s ability to scale to larger and more difficult, albeit artificially generated, corpora.\nIn natural language acquisition, it is difficult to gather the annotated data needed for supervised learning; however, unannotated data is fairly plentiful. Active learning methods attempt to select for annotation and training only the most informative examples, and therefore are potentially very useful in natural language applications. However, most results to date for active learning have only considered standard classification tasks. To reduce annotation effort while maintaining accuracy, we apply active learning to semantic lexicons. We show that active learning can significantly reduce the number of annotated examples required to achieve a given level of performance."}, {"heading": "1. Introduction and Overview", "text": "A long-standing goal for the field of artificial intelligence is to enable computer understanding of human languages. Much progress has been made in reaching this goal, but much also remains to be done. Before artificial intelligence systems can meet this goal, they first need the ability to parse sentences, or transform them into a representation that is more easily manipulated by computers. Several knowledge sources are required for parsing, such as a grammar, lexicon, and parsing mechanism.\nNatural language processing (NLP) researchers have traditionally attempted to build these knowledge sources by hand, often resulting in brittle, inefficient systems that take a significant effort to build. Our goal here is to overcome this \u201cknowledge acquisition bottleneck\u201d by applying methods from machine learning. We develop and apply methods from empirical or corpus-based NLP to learn semantic lexicons, and from active learning to reduce the annotation effort required to learn them.\nc\u00a92003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nThe semantic lexicon is one NLP component that is typically challenging and time consuming to construct and update by hand. Our notion of semantic lexicon, formally defined in Section 3, is that of a list of phrase-meaning pairs, where the meaning representation is determined by the language understanding task at hand, and where we are taking a compositional view of sentence meaning (Partee, Meulen, & Wall, 1990). This paper describes a system, Wolfie (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon of phrase-meaning pairs from a corpus of sentences paired with semantic representations. The goal is to automate lexicon construction for an integrated NLP system that acquires both semantic lexicons and parsers for natural language interfaces from a single training set of annotated sentences.\nAlthough many others (Se\u0301billot, Bouillon, & Fabre, 2000; Riloff & Jones, 1999; Siskind, 1996; Hastings, 1996; Grefenstette, 1994; Brent, 1991) have presented systems for learning information about lexical semantics, we present here a system for learning lexicons of phrasemeaning pairs. Further, our work is unique in its combination of several features, though prior work has included some of these aspects. First, its output can be used by a system, Chill (Zelle & Mooney, 1996; Zelle, 1995), that learns to parse sentences into semantic representations. Second, it uses a fairly straightforward batch, greedy, heuristic learning algorithm that requires only a small number of examples to generalize well. Third, it is easily extendible to new representation formalisms. Fourth, it requires no prior knowledge although it can exploit an initial lexicon if provided. Finally, it simplifies the learning problem by making several assumptions about the training data, as described further in Section 3.2.\nWe test Wolfie\u2019s ability to acquire a semantic lexicon for a natural language interface to a geographical database using a corpus of queries collected from human subjects and annotated with their logical form. In this test, Wolfie is integrated with Chill, which learns parsers but requires a semantic lexicon (previously built manually). The results demonstrate that the final acquired parser performs nearly as accurately at answering novel questions when using a learned lexicon as when using a hand-built lexicon. Wolfie is also compared to an alternative lexicon acquisition system developed by Siskind (1996), demonstrating superior performance on this task. Finally, the corpus is translated into Spanish, Japanese, and Turkish, and experiments are conducted demonstrating an ability to learn successful lexicons and parsers for a variety of languages.\nA second set of experiments demonstrates Wolfie\u2019s ability to scale to larger and more difficult, albeit artificially generated, corpora. Overall, the results demonstrate a robust ability to acquire accurate lexicons directly usable for semantic parsing. With such an integrated system, the task of building a semantic parser for a new domain is simplified. A single representative corpus of sentence-representation pairs allows the acquisition of both a semantic lexicon and parser that generalizes well to novel sentences.\nWhile building an annotated corpus is arguably less work than building an entire NLP system, it is still not a simple task. Redundancies and errors may occur in the training data. A goal should be to also minimize the annotation effort, yet still achieve a reasonable level of generalization performance. In the case of natural language, there is frequently a large amount of unannotated text available. We would like to automatically, but intelligently, choose which of the available sentences to annotate.\nWe do this here using a technique called active learning. Active learning is a research area in machine learning that features systems that automatically select the most informative examples for annotation and training (Cohn, Atlas, & Ladner, 1994). The primary goal of active learning is to reduce the number of examples that the system is trained on, thereby reducing the example annotation cost, while maintaining the accuracy of the acquired information. To demonstrate the usefulness of our active learning techniques, we compared the accuracy of parsers and lexicons learned using examples chosen by active learning for lexicon acquisition, to those learned using randomly chosen examples, finding that active learning saved significant annotation cost over training on randomly chosen examples. This savings is demonstrated in the geography query domain.\nIn summary, this paper provides a new statement of the lexicon acquisition problem and demonstrates a machine learning technique for solving this problem. Next, by combining this with previous research, we show that an entire natural language interface can be acquired from one training corpus. Further, we demonstrate the application of active learning techniques to minimize the number of sentences to annotate as training input for the integrated learning system.\nThe remainder of the paper is organized as follows. Section 2 gives more background information on Chill and introduces Siskind\u2019s lexicon acquisition system, which we will compare to Wolfie in Section 5. Sections 3 and 4 formally define the learning problem and describe the Wolfie algorithm in detail. In Section 5 we present and discuss experiments evaluating Wolfie\u2019s performance in learning lexicons in a database query domain and for an artificial corpus. Next, Section 6 describes and evaluates our use of active learning techniques for Wolfie. Sections 7 and 8 discuss related research and future directions, respectively. Finally, Section 9 summarizes our research and results."}, {"heading": "2. Background", "text": "In this section we give an overview of Chill, the system that our research adds to. We also describe Jeff Siskind\u2019s lexicon acquisition system.\n2.1 Chill\nThe output produced by Wolfie can be used to assist a larger language acquisition system; in particular, it is currently used as part of the input to a parser acquisition system called Chill (Constructive Heuristics Induction for Language Learning). Chill uses inductive logic programming (Muggleton, 1992; Lavrac\u0306 & Dz\u0306eroski, 1994) to learn a deterministic shift-reduce parser (Tomita, 1986) written in Prolog. The input to Chill is a corpus of sentences paired with semantic representations, the same input required by Wolfie. The parser learned is capable of mapping the sentences into their correct representations, as well as generalizing well to novel sentences. In this paper, we limit our discussion to Chill\u2019s ability to acquire parsers that map natural language questions directly into Prolog queries that can be executed to produce an answer (Zelle & Mooney, 1996). Following are two sample queries for a database on U.S. geography, paired with their corresponding Prolog query:\nWhat is the capital of the state with the biggest population? answer(C, (capital(S,C), largest(P, (state(S), population(S,P))))).\nWhat state is Texarkana located in? answer(S, (state(S), eq(C,cityid(texarkana, )), loc(C,S))).\nChill treats parser induction as the problem of learning rules to control the actions of a shift-reduce parser. During parsing, the current context is maintained in a stack and a buffer containing the remaining input. When parsing is complete, the stack contains the representation of the input sentence. There are three types of operators that the parser uses to construct logical queries. One is the introduction onto the stack of a predicate needed in the sentence representation due to a phrase\u2019s appearance at the front of the input buffer. These operators require a semantic lexicon as background knowledge. For details on this and the other two parsing operators, see Zelle and Mooney (1996). By using Wolfie, the lexicon is provided automatically. Figure 1 illustrates the complete system."}, {"heading": "2.2 Jeff Siskind\u2019s Lexicon Learning Research", "text": "The most closely related previous research into automated lexicon acquisition is that of Siskind (1996), itself inspired by work by Rayner, Hugosson, and Hagert (1988). As we will be comparing our system to his in Section 5, we describe the main features of his research in this section. His goal is one of cognitive modeling of children\u2019s acquisition of the lexicon, where that lexicon can be used for both comprehension and generation. Our goal is a machine learning and engineering one, and focuses on a lexicon for comprehension and use in parsing, using a learning process that does not claim any cognitive plausibility, and with the goal of learning a lexicon that generalizes well from a small number of training examples.\nHis system takes an incremental approach to acquiring a lexicon. Learning proceeds in two stages. The first stage learns which symbols in the representation are to be used in the\nfinal \u201cconceptual expression\u201d that represents the meaning of a word, by using a versionspace approach. The second stage learns how these symbols are put together to form the final representation. For example, when learning the meaning of the word \u201craise\u201d, the algorithm may learn the set {CAUSE, GO, UP} during the first stage and put them together to form the expression CAUSE(x, GO(y, UP)) during the second stage.\nSiskind (1996) shows the effectiveness of his approach on a series of artificial corpora. The system handles noise, lexical ambiguity, referential uncertainty, and very large corpora, but the usefulness of lexicons learned is only compared to the \u201ccorrect,\u201d artificial lexicon. The goal of the experiments presented there was to evaluate the correctness and completeness of learned lexicons. Earlier work (Siskind, 1992) also evaluated versions of his technique on a quite small corpus of real English and Japanese sentences. We extend that evaluation to a demonstration of the system\u2019s usefulness in performing real world natural language processing tasks, using a larger corpus of real sentences."}, {"heading": "3. The Lexicon Acquisition Problem", "text": "Although in the end our goal is to acquire an entire natural language interface, we currently divide the task into two parts, the lexicon acquisition component and the parser acquisition component. In this section, we discuss the problem of acquiring semantic lexicons that assist parsing and the acquisition of parsers. The training input consists of natural language sentences paired with their meaning representations. From these pairs we extract a lexicon consisting of phrases paired with their meaning representations. Some training pairs were given in the previous section, and a sample lexicon is shown in Figure 2."}, {"heading": "3.1 Formal Definition", "text": "To present the learning problem more formally, some definitions are needed. While in the following we use the terms \u201cstring\u201d and \u201csubstring,\u201d these extend straight-forwardly to natural language sentences and phrases, respectively. We also refer to labeled trees, making the assumption that the semantic meanings of interest can be represented as such. Most common representations can be recast as labeled trees or forests, and our formalism extends easily to the latter.\nDefinition: Let \u03a3V , \u03a3E be finite alphabets of vertex labels and edge labels, respectively. Let V be a finite nonempty set of vertices, l a total function l : V \u2192 \u03a3V , E a set of unordered pairs of distinct vertices called edges, and a a total function a : E \u2192 \u03a3E. G = (V, l, E, a) is a labeled graph.\n1 s\nString : \u2018\u2018The girl ate the pasta with the cheese.\u2019\u2019\nDefinition: A labeled tree is a connected, acyclic labeled graph.\nFigure 3 shows the labeled tree t1 (with vertices 1-8) on the left, with associated vertex and edge labels on the right. The function l is:1\n{ (1, ingest), (2, person), (3, food), (4, female), (5, child), (6, pasta), (7, food), (8, cheese) }.\nThe tree t1 is a semantic representation of the sentence s1: \u201cThe girl ate the pasta with the cheese.\u201d Using a conceptual dependency (Schank, 1975) representation in Prolog list form, the meaning is:\n[ingest, agent:[person, sex:female, age:child],\npatient:[food, type:pasta, accomp:[food, type:cheese]]].\nDefinition: A u-v path in a graph G is a finite alternating sequence of vertices and edges of G, in which no vertex is repeated, that begins with vertex u and ends with vertex v, and in which each edge in the sequence connects the vertex that precedes it in the sequence to the vertex that follows it in the sequence.\nDefinition: A directed, labeled tree T = (V, l, E, a) is a labeled tree whose edges consist of ordered pairs of vertices, with a distinguished vertex r, called the root, with the property that for every v \u2208 V , there is a directed r-v path in T , and such that the underlying undirected unlabeled graph induced by (V,E) is a connected, acyclic graph.\nDefinition: An interpretation f from a finite string s to a directed, labeled tree t is a one-to-one function mapping a subset s\u2032 of the substrings of s, such that no two strings in s\u2032 overlap, into the vertices of t such that the root of t is in the range of f .\n1. We omit enumeration of the function e but it could be given in a similar manner, for example ((1,2), agent) is an element of e.\nThe interpretation provides information about what parts of the meaning of a sentence originate from which of its phrases. In Figure 3, we show an interpretation, f1, of s1 to t1. Note that \u201cwith\u201d is not in the domain of f1, since s\n\u2032 is a subset of the substrings of s, thus allowing some words in s to have no meaning. Because we disallow overlapping substrings in the domain, both \u201ccheese\u201d and \u201cthe cheese\u201d could not map to vertices in t1.\nDefinition: Given an interpretation f of string s to tree t, and an element p of the domain of f , the meaning of p relative to s, t, f is the connected subgraph of t whose vertices include f(p) and all its descendents except any other vertices in the range of f and their descendents.\nMeanings in this sense concern the \u201clowest level\u201d of phrasal meanings, occurring at the terminal nodes of a semantic grammar, namely the entries in the semantic lexicon. The grammar can then be used to construct the meanings of longer phrases and entire sentences. This is our motivation for the previously stated constraint that the root must be included in the range of f : we want all vertices in the sentence representation to be included in the meaning of some phrase. Note that the meaning of p is also a directed tree with f(p) as its root. Figure 4 shows the meanings of each phrase in the domain of interpretation function f1 shown in Figure 3. We show only the labels on the vertices and edges for readability.\nDefinition: Given a finite set STF of triples < s1, t1, f1 >, . . . , < sn, tn, fn >, where each si is a finite string, each ti is a directed, labeled tree, and each fi is an interpretation function from si to ti, let the language LSTF = {p1, . . . , pk} of STF be the union of all substrings 2 that occur in the domain of some fi. For each pj \u2208 LSTF , the meaning set of pj, denoted MSTF (pj),\n3 is the set of all meanings of pj relative to si, ti, fi for some < si, ti, fi >\u2208 STF . We consider two meanings to be the same if they are isomorphic trees taking labels into account.\nFor example, given sentence s2: \u201cThe man ate the cheese,\u201d the labeled tree t2 pictured in Figure 5, and f2 defined as: f2(\u201cate\u201d) = 1, f2(\u201cman\u201d) = 2, f2(\u201cthe cheese\u201d) = 3; the\n2. We consider two substrings to be the same string if they contain the same characters in the same order, irrespective of their positions within the larger string in which they occur. 3. We omit the subscript on M when the set STF is obvious from context.\nString : \u2018\u2018The man ate the cheese.\"\ns\n2\nmeaning set of \u201cthe cheese\u201d with respect to STF = {< s1, t1, f1 >,< s2, t2, f2 >} is {[food, type:cheese]}, just one meaning though f1 and f2 map \u201cthe cheese\u201d to different vertices in the two trees, because the subgraphs denoting the meaning of \u201cthe cheese\u201d for the two functions are isomorphic.\nDefinition: Given a finite set STF of triples < s1, t1, f1 >, . . . , < sn, tn, fn >, where each si is a finite string, each ti is a directed, labeled tree, and each fi is an interpretation function from si to ti, the covering lexicon expressed by STF is\n{(p,m) : p \u2208 LSTF ,m \u2208 M(p)}.\nThe covering lexicon L expressed by STF = {< s1, t1, f1 >,< s2, t2, f2 >} is:\n{ (\u201cgirl\u201d, [person, sex:female, age:child]), (\u201cman\u201d, [person, sex:male, age:adult]), (\u201cate\u201d, [ingest]), (\u201cpasta\u201d, [food, type:pasta]), (\u201cthe cheese\u201d, [food, type:cheese]) }.\nThe idea of a covering lexicon is that it provides, for each string (sentence) si, a meaning for some of the phrases in that sentence. Further, these meanings are trees whose labeled vertices together include each of the labeled vertices in the tree ti representing the meaning of si, with no vertices duplicated, and containing no vertices not in ti. Edge labels may or may not be included, since the idea is that some of them are due to syntax, which the parser will provide; those edges capturing lexical semantics are in the lexicon. Note that because we only include in the covering lexicon phrases (substrings) that are in the domains of the fi\u2019s, words with the empty tree as meaning are not included in the covering lexicon. Note also that we will in general use \u201cphrase\u201d to mean substrings of sentences, whether they consist of one word, or more than one. Finally the strings in the covering lexicon may contain overlapping words even though those in the domain of an individual interpretation function must not, since those overlapping words could have occurred in different sentences.\nFinally, we are ready to define the learning problem at hand.\nThe Lexicon Acquisition Problem: Given: a multiset of strings S = {s1, . . . , sn} and a multiset of labeled trees T = {t1, . . . , tn}, Find: a multiset of interpretation functions, F = {f1, . . . , fn}, such that the cardinality of the covering lexicon expressed by STF = {< s1, t1, f1 >, . . . , < sn, tn, fn >} is minimized. If such a set is found, we say we have found a minimal set of interpretations (or a minimal covering lexicon). \u2737\nLess formally, a learner is presented with a multiset of sentences (S) paired with their meanings (T ); the goal of learning is to find the smallest lexicon consistent with this data. This lexicon is the paired listing of all phrases occurring in the domain of some fi \u2208 F (where F is the multiset of interpretation functions found) with each of the elements in their meaning sets. The motivation for finding a lexicon of minimal size is the usual bias towards simplicity of representation and generalization beyond the training data. While this definition allows for phrases of any length, we will usually want to limit the length of phrases to be considered for inclusion in the domain of the interpretation functions, for efficiency purposes.\nOnce we determine a set of interpretation functions for a set of strings and trees, there is only one unique covering lexicon expressed by STF . However, this might not be the only set of interpretation functions possible, and may not result in the lexicon with smallest cardinality. For example, the covering lexicon given with the previous example is not a minimal covering lexicon. For the two sentences given, we could find minimal, though rather degenerate, lexicons such as:\n{ (\u201cgirl\u201d, [ingest, agent:[person, sex:female, age:child], patient:[food, type:pasta, accomp:[food, type:cheese]]]),\n(\u201cman\u201d, [ingest, agent:[person, sex:male, age:adult], patient:[food, type:cheese]]) }\nThis type of lexicon becomes less likely as the size of the corpus grows."}, {"heading": "3.2 Implications of the Definition", "text": "This definition of the lexicon acquisition problem differs from that given by other authors, including Riloff and Jones (1999), Siskind (1996), Manning (1993), Brent (1991) and others, as further discussed in Section 7. Our definition of the problem makes some assumptions about the training input. First, by making f a function instead of a relation, the definition assumes that the meaning for each phrase in a sentence appears once in the representation of that sentence, the single-use assumption. Second, by making f one-to-one, it assumes exclusivity, that each vertex in a sentence\u2019s representation is due to only one phrase in the sentence. Third, it assumes that a phrase\u2019s meaning is a connected subgraph of a sentence\u2019s representation, not a more distributed representation, the connectedness assumption. While the first assumption may not hold for some representation languages, it does not present a problem in the domains we have considered. The second and third assumptions are perhaps less problematic with respect to general language use.\nOur definition also assumes compositionality: that the meaning of a sentence is derived from the meanings of the phrases it contains, in addition, perhaps to some \u201cconnecting\u201d information specific to the representation at hand, but is not derived from external sources\nsuch as noise. In other words, all the vertices of a sentence\u2019s representation are included within the meaning of some word or phrase in that sentence. This assumption is similar to the linking rules of Jackendoff (1990), and has been used in previous work on grammar and language acquisition (e.g., Haas and Jayaraman, 1997; Siskind, 19964) While there is some debate in the linguistics community about the ability of compositional techniques to handle all phenomena (Fillmore, 1988; Goldberg, 1995), making this assumption simplifies the learning process and works reasonably for the domains of interest here. Also, since we allow multi-word phrases in the lexicon (e.g., (\u201ckick the bucket\u201d, die( ))), one objection to compositionality can be addressed.\nThis definition also allows training input in which:\n1. Words and phrases have multiple meanings. That is, homonymy might occur in the lexicon.\n2. Several phrases map to the same meaning. That is, synonymy might occur in the lexicon.\n3. Some words in a sentence do not map to any meanings, leaving them unused in the assignment of words to meanings.5\n4. Phrases of contiguous words map to parts of a sentence\u2019s meaning representation.\nOf particular note is lexical ambiguity (1 above). Note that we could have also derived an ambiguous lexicon such as:\n{ (\u201cgirl\u201d, [person, sex:female, age:child]), (\u201cate\u201d, [ingest]), (\u201cate\u201d, [ingest, agent:[person, sex:male, age:adult]]), (\u201cpasta\u201d, [food, type:pasta]), (\u201cthe cheese\u201d, [food, type:cheese]) }.\nfrom our sample corpus. In this lexicon, \u201cate\u201d is an ambiguous word. The earlier example minimizes ambiguity resulting in an alternative, more intuitively pleasing lexicon. While our problem definition first minimizes the number of entries in the lexicon, our learning algorithm will also exploit a preference for minimizing ambiguity.\nAlso note that our definition allows training input in which sentences themselves are ambiguous (paired with more than one meaning), since a given sentence in S (a multiset) might appear multiple times appear with more than one meaning. In fact, the training data that we consider in Section 5 does have some ambiguous sentences.\nOur definition of the lexicon acquisition problem does not fit cleanly into the traditional definition of learning for classification. Each training example contains a sentence and its semantic parse, and we are trying to extract semantic information about some of the phrases in that sentence. So each example potentially contains information about multiple target concepts (phrases), and we are trying to pick out the relevant \u201cfeatures,\u201d or vertices of the\n4. In fact, all of these assumptions except for single-use were made by Siskind (1996); see Section 7 for details. 5. These words may, however, serve as cues to a parser on how to assemble sentence meanings from word meanings.\nrepresentation, corresponding to the correct meaning of each phrase. Of course, our assumptions of single-use, exclusivity, connectedness, and compositionality impose additional constraints. In addition to this \u201cmultiple examples in one\u201d learning scenario, we do not have access to negative examples, nor can we derive any implicit negatives, because of the possibility of ambiguous and synonymous phrases.\nIn some ways the problem is related to clustering, which is also capable of learning multiple, potentially non-disjoint categories. However, it is not clear how a clustering system could be made to learn the phrase-meaning mappings needed for parsing. Finally, current systems that learn multiple concepts commonly use examples for other concepts as negative examples of the concept currently being learned. The implicit assumption made by doing this is that concepts are disjoint, an unwarranted assumption in the presence of synonymy.\n4. The Wolfie Algorithm and an Example\nIn this section, we first discuss some issues we considered in the design of our algorithm, then describe it fully in Section 4.2."}, {"heading": "4.1 Solving the Lexicon Acquisition Problem", "text": "A first attempt to solve the Lexicon Acquisition Problem might be to examine all interpretation functions across the corpus, then choose the one(s) with minimal lexicon size. The number of possible interpretation functions for a given input pair is dependent on both the size of the sentence and its representation. In a sentence with w words, there are \u0398(w2) possible phrases, not a particular challenge.\nHowever, the number of possible interpretation functions grows extremely quickly with the size of the input. For a sentence with p phrases and an associated tree with n vertices, the number of possible interpretation functions is:\nc!(n \u2212 1)! c \u2211\ni=1\n1\n(i \u2212 1)!(n \u2212 i)!(c \u2212 i)! . (1)\nwhere c is min(p, n). The derivation of the above formula is as follows. We must choose which phrases to use in the domain of f , and we can choose one phrase, or two, or any number up to min(p, n) (if n < p we can only assign n phrases since f is one-to-one), or\n(\np i\n)\n= p!\ni!(p \u2212 i)!\nwhere i is the number of phrases chosen. But we can also permute these phrases, so that the \u201corder\u201d in which they are assigned to the vertices is different. There are i! such permutations. We must also choose which vertices to include in the range of the interpretation function. We have to choose the root each time, so if we are choosing i vertices, we have n \u2212 1 choose i \u2212 1 vertices left after choosing the root, or\n(\nn \u2212 1 i \u2212 1\n)\n= (n \u2212 1)!\n(i \u2212 1)!(n \u2212 i)! .\nThe full number of possible interpretation functions is then:\nmin(p,n) \u2211\ni=1\np!\ni!(p \u2212 i)! \u00d7 i!\u00d7\n(n \u2212 1)!\n(i \u2212 1)!(n \u2212 i)! ,\nwhich simplifies to Equation 1. When n = p, the largest term of this equation is c! = p!, which grows at least exponentially with p, so in general the number of interpretation functions is too large to allow enumeration. Therefore, finding a lexicon by examining all interpretations across the corpus, then choosing the lexicon(s) of minimum size, is clearly not tractable.\nInstead of finding all interpretations, one could find a set of candidate meanings for each phrase, from which the final meaning(s) for that phrase could be chosen in a way that minimizes lexicon size. One way to find candidate meanings is to fracture the meanings of sentences in which a phrase appears. Siskind (1993) defined fracturing (he also calls it the Unlink* operation) over terms such that the result includes all subterms of an expression plus \u22a5. In our representation formalism, this corresponds to finding all possible connected subgraphs of a meaning, and adding the empty graph. Like the interpretation function technique just discussed, fracturing would also lead to an exponential blowup in the number of candidate meanings for a phrase: A lower bound on the number of connected subgraphs for a full binary tree with n vertices is obtained by noting that any subset of the (n + 1)/2 leaves may be deleted and still maintain connectivity of the remaining tree. Thus, counting all of the ways that leaves can be deleted gives us a lower bound of 2(n+1)/2 fractures.6 This does not completely rule out fracturing as part of a technique for lexicon learning since trees do not tend to get very large, and indeed Siskind uses it in many of his systems, with other constraints to help control the search. However, we wish to avoid any chance of exponential blowup to preserve the generality of our approach for other tasks.\nAnother option is to force Chill to essentially induce a lexicon on its own. In this model, we would provide to Chill an ambiguous lexicon in which each phrase is paired with every fracture of every sentence in which it appears. Chill would then have to decide which set of fractures leads to the correct parse for each training sentence, and would only include those in a final learned parser-lexicon combination. Thus the search would again become exponential. Furthermore, even with small representations, it would likely lead to a system with poor generalization ability. While some of Siskind\u2019s work (e.g., Siskind, 1992) took syntactic constraints into account and did not encounter such difficulties, those versions did not handle lexical ambiguity.\nIf we could efficiently find some good candidates, a standard induction algorithm could then attempt to use them as a source of training examples for each phrase. However, any attempt to use the list of candidate meanings of one phrase as negative examples for another phrase would be flawed. The learner could not know in advance which phrases are possibly synonymous, and thus which phrase lists to use as negative examples of other phrase meanings. Also, many representation components would be present in the lists of more than one phrase. This is a source of conflicting evidence for a learner, even without the presence of synonymy. Since only positive examples are available, one might think of using most specific conjunctive learning, or finding the intersection of all the representations\n6. Thanks to net-citizen Dan Hirshberg for help with this analysis.\nfor each phrase, as proposed by Anderson (1977). However, the meanings of an ambiguous phrase are disjunctive, and this intersection would be empty. A similar difficulty would be expected with the positive-only compression of Muggleton (1995).\n4.2 Our Solution: Wolfie\nThe above analysis leads us to believe that the Lexicon Acquisition Problem is computationally intractable. Therefore, we can not perform an efficient search for the best lexicon. Nor can we use a standard induction algorithm. Therefore, we have implemented Wolfie7, outlined in Figure 6, which finds an approximate solution to the Lexicon Acquisition Problem. Our approach is to generate a set of candidate lexicon entries, from which the final learned lexicon is derived by greedily choosing the \u201cbest\u201d lexicon item at each point, in the hopes of finding a final (minimal) covering lexicon. We do not actually learn interpretation functions, so do not guarantee that we will find a covering lexicon.8 Even if we were to search for interpretation functions, using a greedy search would also not guarantee covering the input, and of course it also does not guarantee that a minimal lexicon is found. However, we will later present experimental results demonstrating that our greedy approach performs well.\nWolfie first derives an initial set of candidate meanings for each phrase. The algorithm for generating candidates, LICS, attempts to find a \u201cmaximally common\u201d meaning for each phrase, which biases toward both finding a small lexicon by covering many vertices of a tree at once, and finding a lexicon that actually does cover the input. Second, Wolfie chooses final lexicon entries from this candidate set, one at a time, updating the candidate set as it goes, taking into account our assumptions of single-use, connectedness, and exclusivity. The basic scheme for choosing entries from the candidate set is to maximize the prediction of meanings given phrases, but also to find general meanings. This adds a tension between LICS, which cover many vertices, and generality, which biases towards fewer vertices. However, generality, like LICS, helps lead to a small lexicon since a general meaning will more likely apply widely across a corpus.\n7. The code is available upon request from the first author. 8. Though, of course, interpretation functions are not the only way to guarantee a covering lexicon \u2013 see\nSiskind (1993) for an alternative.\nLet us explain the algorithm in further detail by way of an example, using Spanish instead of English to illustrate the difficulty somewhat more clearly. Consider the following corpus:\n1. \u00bf Cua\u0301l es el capital del estado con la poblacio\u0301n ma\u0301s grande? answer(C, (capital(S,C), largest(P, (state(S), population(S,P))))).\n2. \u00bf Cua\u0301l es la punta ma\u0301s alta del estado con la area ma\u0301s grande? answer(P, (high point(S,P), largest(A, (state(S), area(S,A))))).\n3. \u00bf En que estado se encuentra Texarkana? answer(S, (state(S), eq(C,cityid(texarkana, )), loc(C,S))).\n4. \u00bf Que\u0301 capital es la ma\u0301s grande? answer(A, largest(A, capital(A))).\n5. \u00bf Que\u0301 es la area de los estados unitos? answer(A, (area(C,A), eq(C,countryid(usa)))).\n6. \u00bf Cua\u0301l es la poblacio\u0301n de un estado que bordean a Utah? answer(P, (population(S,P), state(S), next to(S,M), eq(M,stateid(utah)))).\n7. \u00bf Que\u0301 es la punta ma\u0301s alta del estado con la capital Madison? answer(C, (high point(B,C), loc(C,B), state(B),\ncapital(B,A), eq(A,cityid(madison, )))).\nThe sentence representations here are slightly different than the tree representations given in the problem definition, with the main difference being the addition of existentially quantified variables shared between some leaves of a representation tree. As mentioned in Section 2.1, the representations are Prolog queries to a database. Given such a query, we can create a tree that conforms to our formalism, but with this addition of quantified variables. An example is shown in Figure 7 for the representation of the third sentence. Each vertex is a predicate name and its arity, in the Prolog style, e.g., state/1, with quantified variables at some of the leaves. For each outgoing edge (n,m) of a vertex n, the edge is labeled with the argument position filled by the subtree rooted by m. If there is not an edge labeled with a given argument position, the argument is a free variable. Each vertex labeled with a\nvariable (which can occur only at leaves) is an existentially quantified variable whose scope is the entire tree (or query). The learned lexicon, however, does not need to maintain the identity between variables across distinct lexical entries.\nAnother representation difference is that we will strip the answer predicate from the input to our learner,9 thus allowing a forest of directed trees as input rather than a single tree. The definition of the problem easily extends such that the root of each tree in the forest must be in the domain of some interpretation function.\nEvaluation of our system using this representation is given in Section 5.1; evaluation using a representation without variables or forests is presented in Section 5.2. We previously (Thompson, 1995) presented results demonstrating learning representations of a different form, that of a case-role representation (Fillmore, 1968) augmented with Conceptual Dependency (Schank, 1975) information. This last representation conforms directly to our problem definition.\nNow, continuing with the example of solving the Lexicon Acquisition Problem for this corpus, let us also assume for simplification, although not required, that sentences are stripped of phrases that we know have empty meanings (e.g., \u201cque\u0301\u201d, \u201ces\u201d, \u201ccon\u201d, and \u201cla\u201d). We will similarly assume that it is known that some phrases refer directly to given database constants (e.g., location names), and remove those phrases and their meaning from the training input."}, {"heading": "4.2.1 Candidate Generation Phase", "text": "Initial candidate meanings for a phrase are produced by computing the maximally common substructure(s) between sampled pairs of representations of sentences that contain it. We derive common substructure by computing the Largest Isomorphic Connected Subgraphs (LICS) of two labeled trees, taking labels into account in the isomorphism. The analogous Largest Common Subgraph problem (Garey & Johnson, 1979) is solvable in polynomial time if, as we assume, both inputs are trees and if K, the number of edges to include, is given. Thus, we start with K set equal to the largest number of edges in the two trees being compared, test for common subgraph(s), and iterate down to K = 1, stopping when one or more subgraphs are found for a given K.\nFor the Prolog query representation, the algorithm is complicated a bit by variables. Therefore, we use LICS with an addition similar to computing the Least General Generalization of first-order clauses (Plotkin, 1970). The LGG of two sets of literals is the least general set of literals that subsumes both sets of literals. We add to this by allowing that when a term in the argument of a literal is a conjunction, the algorithm tries all orderings in its matching of the terms in the conjunction. Overall, our algorithm for finding the LICS between two trees in the Prolog representation first finds the common labeled edges and vertices as usual in LICS, but treats all variables as equivalent. Then, it computes the Least General Generalization, with conjunction taken into account, of the resulting trees as converted back into literals. For example, given the two trees:\n9. The predicate is omitted because Chill initializes the parse stack with the answer predicate, and thus no word has to be mapped to it.\nanswer(C, (largest(P, (state(S), population(S,P))), capital(S,C))).\nanswer(P, (high point(S,P), largest(A, (state(S), area(S,A))))).,\nthe common meaning is answer( ,largest( ,state( )). Note that the LICS of two trees may not be unique: there may be multiple common subtrees that both contain the same number of edges; in this case LICS returns multiple answers.\nThe sets of initial candidate meanings for some of the phrases in the sample corpus are shown in Table 1. While in this example we show the LICS for all pairs that a phrase appears in, in the actual algorithm we randomly sample a subset for efficiency reasons, as in Golem (Muggleton & Feng, 1990). For phrases appearing in only one sentence (e.g., \u201cencuentra\u201d), the entire sentence representation (excluding the database constant given as background knowledge) is used as an initial candidate meaning. Such candidates are typically generalized in step 2.2 of the algorithm to only the correct portion of the representation before they are added to the lexicon; we will see an example of this below."}, {"heading": "4.2.2 Adding to the Final Lexicon", "text": "After deriving initial candidates, the greedy search begins. The heuristic used to evaluate candidates attempts to help assure that a small but covering lexicon is learned. The heuristic first looks at the weighted sum of two components, where p is the phrase and m its candidate meaning:\n1. P (m | p)\u00d7 P (p | m)\u00d7 P (m) = P (p)\u00d7 P (m | p)2\n2. The generality of m\nThen, ties in this value are broken by preferring less ambiguous (those with fewer current meanings) and shorter phrases. The first component is analogous the cluster evaluation\nheuristic used by Cobweb (Fisher, 1987), which measures the utility of clusters based on attribute-value pairs and categories, instead of meanings and phrases. The probabilities are estimated from the training data and then updated as learning progresses to account for phrases and meanings already covered. We will see how this updating works as we continue through our example of the algorithm. The goal of this part of the heuristic is to maximize the probability of predicting the correct meaning for a randomly sampled phrase. The equality holds by Bayes Theorem. Looking at the right side, P (m | p)2 is the expected probability that meaning m is correctly guessed for a given phrase, p. This assumes a strategy of probability matching, in which a meaning m is chosen for p with probability P (m | p) and correct with the same probability. The other term, P (p), biases the component by how common the phrase is. Interpreting the left side of the equation, the first term biases towards lexicons with low ambiguity, the second towards low synonymy, and the third towards frequent meanings.\nThe second component of the heuristic, generality, is computed as the negation of the number of vertices in the meaning\u2019s tree structure, and helps prefer smaller, more general meanings. For example, in the candidate set above, if all else were equal, the generality portion of the heuristic would prefer state( ), with generality value -1, over largest( ,state( )) and (state(S),loc( ,S)), each with generality value -2, as the meaning of \u201cestado\u201d. Learning a meaning with fewer terms helps evenly distribute the vertices in a sentence\u2019s representation among the meanings of the phrases in that sentence, and thus leads to a lexicon that is more likely to be correct. To see this, we note that some pairs of words tend to frequently co-occur (\u201cgrande\u201d and \u201cestado\u201d in our example), and so their joint representation (meaning) is likely to be in the set of candidate meanings for both words. By preferring a more general meaning, we easily ignore these incorrect joint meanings.\nIn this example and all experiments, we use a weight of 10 for the first component of the heuristic, and a weight of 1 for the second. The first component has smaller absolute values and is therefore given a higher weight. Modulo this consideration, results are not overly-sensitive to the weights and automatically setting them using cross-validation on the training set (Kohavi & John, 1995) had little effect on overall performance. In Table 2 we illustrate the calculation of the heuristic measure for some of the above fourteen pairs, and its value for all. The calculation shows the sum of multiplying 10 by the first component of the heuristic and multiplying 1 by the second component. The first component is simplified as follows:\nP (p)\u00d7 P (m | p)2 = | p |\nt \u00d7\n| m \u2229 p |2\n| p |2 \u2248\n| m \u2229 p |2\n| p | ,\nwhere | p | is the number of times phrase p appears in the corpus, t is the initial number of candidate phrases, and | m \u2229 p | is the number of times that meaning m is paired with phrase p. We can ignore t since the number of phrases in the corpus is the same for each pair, and has no effect on the ranking. The highest scoring pair is (\u201cestado\u201d, state( )), so it is added to the lexicon.\nNext is the candidate generalization step (2.2), described algorithmically in Figure 8. One of the key ideas of the algorithm is that each phrase-meaning choice can constrain the candidate meanings of phrases yet to be learned. Given the assumption that each portion of the representation is due to at most one phrase in the sentence (exclusivity), once part of a\nGiven: A learned phrase-meaning pair (l, g)\nFor all sentence-representation pairs containing l and g, mark them as covered. For each candidate phrase-meaning pair (p,m):\nIf p occurs in some training pairs with (l, g) then If the vertices of m intersect the vertices of g then\nIf all occurrences of m are now covered then Remove (p,m) from the set of candidate pairs.\nElse\nAdjust the heuristic value of (p,m) as needed to account for newly covered nodes of the training representations.\nGeneralize m to remove covered nodes, obtaining m\u2032, and Calculate the heuristic value of the new candidate pair (p,m\u2032).\nIf no candidate meanings remain for an uncovered phrase then Derive new LICS from uncovered representations and\nrepresentation is covered, no other phrase in the sentence can be paired with that meaning (at least for that sentence). Therefore, in step 2.2 the candidate meanings for words in the same sentences as the word just learned are generalized to exclude the representation just learned. We use an operation analogous to set difference when finding the remaining uncovered vertices of the representation when generalizing meanings to eliminate covered vertices from candidate pairs. For example, if the meaning largest( , ) were learned for a phrase in sentence 2, the meaning left behind would be a forest consisting of the trees high point(S, ) and (state(S), area(S, )). Also, if the generalization results in an empty tree, new LICS are calculated. In our example, since state( ) is covered in sentences 1, 2, 3, 6, and 7, the candidates for several other words in those sentences are generalized. For example, the meaning (state(S), loc( ,S)) for \u201cencuentra\u201d, is generalized to loc( , ), with a new heuristic value of 10(12/1) + 1(\u22121) = 9. Also, our single-use assumption allows us to remove all candidate pairs containing \u201cestado\u201d from the set of candidate meanings, since the learned pair covers all occurrences of \u201cestado\u201d in that set.\nNote that the pairwise matchings to generate candidate items, together with this updating of the candidate set, enable multiple meanings to be learned for ambiguous phrases, and makes the algorithm less sensitive to the initial rate of sampling for LICS. For example, note that \u201ccapital\u201d is ambiguous in this data set, though its ambiguity is an artifact of the way that the query language was designed, and one does not ordinarily think of it as an ambiguous word. However, both meanings will be learned: The second pair added to the final lexicon is (\u201cgrande\u201d, largest( , )), which causes a generalization to the empty meaning for the first candidate entry in Table 2, and since no new LICS from sentence 4 can be generated, its entire remaining meaning is added to the candidate meaning set for both \u201ccapital\u201d and \u201cma\u0301s.\u201d\nSubsequently, the greedy search continues until the resulting lexicon covers the training corpus, or until no candidate phrase meanings remain. In rare cases, learning errors occur that leave some portions of representations uncovered. In our example, the following lexicon is learned:\n(\u201cestado\u201d, state( )),\n(\u201cgrande\u201d, largest( )),\n(\u201carea\u201d, area( )),\n(\u201cpunta\u201d, high point( , )),\n(\u201cpoblacio\u0301n\u201d, population( , )),\n(\u201ccapital\u201d, capital( , )),\n(\u201cencuentra\u201d, loc( , )),\n(\u201calta\u201d, loc( , )),\n(\u201cbordean\u201d, next to( )),\n(\u201ccapital\u201d, capital( )).\nIn the next section, we discuss the ability of Wolfie to learn lexicons that are useful for parsers and parser acquisition.\n5. Evaluation of Wolfie\nThe following two sections discuss experiments testing Wolfie\u2019s success in learning lexicons for both real and artificial corpora, comparing it in several cases to a previously developed lexicon learning system."}, {"heading": "5.1 A Database Query Application", "text": "This section describes our experimental results on a database query application. The first corpus discussed contains 250 questions about U.S. geography, paired with their Prolog query to extract the answer to the question from a database. This domain was originally chosen due to the availability of a hand-built natural language interface, Geobase, to a database containing about 800 facts. Geobase was supplied with Turbo Prolog 2.0 (Borland International, 1988), and designed specifically for this domain. The questions in the corpus were collected by asking undergraduate students to generate English questions for this database, though they were given only cursory knowledge of the database without being given a chance to use it. To broaden the test, we had the same 250 sentences translated into Spanish, Turkish, and Japanese. The Japanese translations are in word-segmented Roman orthography. Translated questions were paired with the appropriate logical queries from the English corpus.\nTo evaluate the learned lexicons, we measured their utility as background knowledge for Chill. This is performed by choosing a random set of 25 test examples and then learning lexicons and parsers from increasingly larger subsets of the remaining 225 examples (increasing by 50 examples each time). After training, the test examples are parsed using the learned parser. We then submit the resulting queries to the database, compare the answers to those generated by submitting the correct representation to the database, and record the percentage of correct (matching) answers. By using the difficult \u201cgold standard\u201d of retrieving a correct answer, we avoid measures of partial accuracy that we believe do not adequately measure final utility. We repeated this process for ten different random training and test sets and evaluated performance differences using a two-tailed, paired t-test with a significance level of p \u2264 0.05.\nWe compared our system to an incremental (on-line) lexicon learner developed by Siskind (1996). To make a more equitable comparison to our batch algorithm, we ran his in a \u201csimulated\u201d batch mode, by repeatedly presenting the corpus 500 times, analogous to running 500 epochs to train a neural network. While this does not actually add new kinds of data over which to learn, it allows his algorithm to perform inter-sentential inference in both directions over the corpus instead of just one. Our point here is to compare accuracy over the same size training corpus, a metric not optimized for by Siskind. We are not worried about the difference in execution time here,10 and the lexicons learned when running Siskind\u2019s system in incremental mode (presenting the corpus a single time) resulted in substantially lower performance in preliminary experiments with this data. We also removed Wolfie\u2019s ability to learn phrases of more than one word, since the current version of Siskind\u2019s system\n10. The CPU times of the two system are not directly comparable since one is written in Prolog and the other in Lisp. However, the learning time of the two systems is approximately the same if Siskind\u2019s system is run in incremental mode, just a few seconds with 225 training examples.\ndoes not have this ability. Finally, we made comparisons to the parsers learned by Chill when using a hand-coded lexicon as background knowledge.\nIn this and similar applications, there are many terms, such as state and city names, whose meanings can be automatically extracted from the database. Therefore, all tests below were run with such names given to the learner as an initial lexicon; this is helpful but not required. Section 5.2 gives results for a different task with no such initial lexicon. However, unless otherwise noted, for all tests within this Section (5.1) we did not strip sentences of phrases known to have empty meanings, unlike in the example of Section 4."}, {"heading": "5.1.1 Comparisons using English", "text": "The first experiment was a comparison on the original English corpus. Figure 9 shows learning curves for Chill when using the lexicons learned by Wolfie (CHILL+Wolfie) and by Siskind\u2019s system (CHILL+Siskind). The uppermost curve (CHILL+handbuilt) shows Chill\u2019s performance when given the hand-built lexicon. CHILL-testlex shows the performance when words that never appear in the training data (e.g., are only in the test sentences) are deleted from the hand-built lexicon (since a learning algorithm has no chance of learning these). Finally, the horizontal line shows the performance of the Geobase benchmark.\nThe results show that a lexicon learned by Wolfie led to parsers that were almost as accurate as those generated using a hand-built lexicon. The best accuracy is achieved by parsers using the hand-built lexicon, followed by the hand-built lexicon with words only in the test set removed, followed by Wolfie, followed by Siskind\u2019s system. All the systems do as well or better than Geobase by the time they reach 125 training examples. The differences between Wolfie and Siskind\u2019s system are statistically significant at all training\nexample sizes. These results show that Wolfie can learn lexicons that support the learning of successful parsers, and that are better from this perspective than those learned by a competing system. Also, comparing to the CHILL-testlex curve, we see that most of the drop in accuracy from a hand-built lexicon is due to words in the test set that the system has not seen during training. In fact, none of the differences between CHILL+Wolfie and CHILL-testlex are statistically significant.\nOne of the implicit hypotheses of our problem definition is that coverage of the training data implies a good lexicon. The results show a coverage of 100% of the 225 training examples for Wolfie versus 94.4% for Siskind. In addition, the lexicons learned by Siskind\u2019s system were more ambiguous and larger than those learned by Wolfie. Wolfie\u2019s lexicons had an average of 1.1 meanings per word, and an average size of 56.5 entries (after 225 training examples) versus 1.7 meanings per word and 154.8 entries in Siskind\u2019s lexicons. For comparison, the hand-built lexicon had 1.2 meanings per word and 88 entries. These differences, summarized in Table 3, undoubtedly contribute to the final performance differences."}, {"heading": "5.1.2 Performance for Other Natural Languages", "text": "Next, we examined the performance of the two systems on the Spanish version of the corpus. Figure 10 shows the results. The differences between using Wolfie and Siskind\u2019s learned lexicons for Chill are again statistically significant at all training set sizes. We also again show the performance with hand-built lexicons, both with and without phrases present only in the testing set. The performance compared to the hand-built lexicon with test-set phrases removed is still competitive, with the difference being significant only at 225 examples.\nFigure 11 shows the accuracy of learned parsers with Wolfie\u2019s learned lexicons for all four languages. The performance differences among the four languages are quite small, demonstrating that our methods are not language dependent."}, {"heading": "5.1.3 A Larger Corpus", "text": "Next, we present results on a larger, more diverse corpus from the geography domain, where the additional sentences were collected from computer science undergraduates in an introductory AI course. The set of questions in the smaller corpus was collected from students in a German class, with no special instructions on the complexity of queries desired. The AI students tended to ask more complex and diverse queries: their task was to give five interesting questions and the associated logical form for a homework assignment, though again they did not have direct access to the database. They were requested to give at least one sentence whose representation included a predicate containing embedded predicates, for\nexample largest(S, state(S)), and we asked for variety in their sentences. There were 221 new sentences, for a total of 471 (including the original 250 sentences).\nFor these experiments, we split the data into 425 training sentences and 46 test sentences, for 10 random splits, then trained Wolfie and then Chill as before. Our goal was to see whether Wolfie was still effective for this more difficult corpus, since there were approximately 40 novel words in the new sentences. Therefore, we tested against the performance of Chill with an extended hand-built lexicon. For this test, we stripped sentences of phrases known to have empty meanings, as in the example of Section 4.2. Again, we did not use phrases of more than one word, since these do not seem to make a significant difference in this domain. For these results, we compare Wolfie\u2019s lexicons for Chill using hand-built lexicons without phrases that only appear in the test set.\nFigure 12 shows the resulting learning curves. The differences between Chill using the hand-built and learned lexicons are statistically significant at 175, 225, 325, and 425 examples (four out of the nine data points). The more mixed results here indicate both the difficulty of the domain and the more variable vocabulary. However, the improvement of machine learning methods over the Geobase hand-built interface is much more dramatic for this corpus."}, {"heading": "5.1.4 LICS versus Fracturing", "text": "One component of the algorithm not yet evaluated explicitly is the candidate generation method. As mentioned in Section 4.1, we could use fractures of representations of sentences in which a phrase appears to generate the candidate meanings for that phrase, instead of LICS. We used this approach and compared it to the previously described method of using the largest isomorphic connected subgraphs of sampled pairs of representations as\ncandidate meanings. To attempt a more fair comparison, we also sampled representations for fracturing, using the same number of source representations as the number of pairs sampled for LICS.\nThe accuracy of Chill when using the resulting learned lexicons as background knowledge are shown in Figure 13. Using fracturing (fractWOLFIE) shows little or no advantage; none of the differences between the two systems are statistically significant.\nIn addition, the number of initial candidate lexicon entries from which to choose is much larger for fracturing than our LICS method, as shown in Figure 14. This is true even though we sampled the same number of representations as pairs for LICS, because there are a larger number of fractures for an arbitrary representation than the number of LICS for an arbitrary pair. Finally, Wolfie\u2019s learning time when using fracturing is greater than that when using LICS, as shown in Figure 15, where the CPU time is shown in seconds.\nIn summary, these differences show the utility of LICS as a method for generating candidates: a more thorough method does not result in better performance, and also results in longer learning times. One could claim that we are handicapping fracturing since we are only sampling representations for fracturing. This may indeed help the accuracy, but the learning time and the number of candidates would likely suffer even further. In a domain with larger representations, the differences in learning time would be even more dramatic."}, {"heading": "5.2 Artificial Data", "text": "The previous section showed that Wolfie successfully learns lexicons for a natural corpus and a realistic task. However, this demonstrates success on only a relatively small corpus and with one representation formalism. We now show that our algorithm scales up well with more lexicon items to learn, more ambiguity, and more synonymy. These factors are\ndifficult to control when using real data as input. Also, there are no large corpora available that are annotated with semantic parses. We therefore present experimental results on an artificial corpus. In this corpus, both the sentences and their representations are completely artificial, and the sentence representation is a variable-free representation, as suggested by the work of Jackendoff (1990) and others.\nFor each corpus discussed below, a random lexicon mapping words to simulated meanings was first constructed.11 This original lexicon was then used to generate a corpus of random utterances each paired with a meaning representation. After using this corpus as input to Wolfie12, the learned lexicon was compared to the original lexicon, and weighted precision and weighted recall of the learned lexicon were measured. Precision measures the percentage of the lexicon entries (i.e., word-meaning pairs) that the system learns that are correct. Recall measures the percentage of the lexicon entries in the hand-built lexicon that are correctly learned by the system:\nprecision = # correct pairs\n# pairs learned\nrecall = # correct pairs\n# pairs in hand-built lexicon .\nTo get weighted precision and recall measures, we then weight the results for each pair by the word\u2019s frequency in the entire corpus (not just the training corpus). This models how likely we are to have learned the correct meaning for an arbitrarily chosen word in the corpus.\nWe generated several lexicons and associated corpora, varying the ambiguity rate (number of meanings per word) and synonymy rate (number of words per meaning), as in Siskind (1996). Meaning representations were generated using a set of \u201cconceptual symbols\u201d that combined to form the meaning for each word. The number of conceptual symbols used in each lexicon will be noted when we describe each corpus below. In each lexicon, 47.5% of the senses were variable-free to simulate noun-like meanings, and 47.5% contained from one to three variables to denote open argument positions to simulate verb-like meanings. The remainder of the words (the remaining 5%) had the empty meaning to simulate function words. In addition, the functors in each meaning could have a depth of up to two and an arity of up to two. An example noun-like meaning is f23(f2(f14)), and a verbmeaning f10(A,f15(B)); the conceptual symbols in this example are f23, f2, f14, f10, and f15. By using these multi-level meaning representations we demonstrate the learning of more complex representations than those in the geography database domain: none of the hand-built meanings for phrases in that lexicon had functors embedded in arguments. We used a grammar to generate utterances and their meanings from each original lexicon, with terminal categories selected using a distribution based on Zipf\u2019s Law (Zipf, 1949). Under Zipf\u2019s Law, the occurrence frequency of a word is inversely proportional to its ranking by occurrence.\nWe started with a baseline corpus generated from a lexicon of 100 words using 25 conceptual symbols and no ambiguity or synonymy; 1949 sentence-meaning pairs were generated.\n11. Thanks to Jeff Siskind for the initial corpus generation software, which we enhanced for these tests. 12. In these tests, we allowed Wolfie to learn phrases of up to length two.\nWe split this into five training sets of 1700 sentences each. Figure 16 shows the weighted precision and recall curves for this initial test. This demonstrates good scalability to a slightly larger corpus and lexicon than that of the U.S. geography query domain.\nA second corpus was generated from a second lexicon, also of 100 words using 25 conceptual symbols, but increasing the ambiguity to 1.25 meanings per word. This time, 1937 pairs were generated and the corpus split into five sets of 1700 training examples each. Weighted precision at 1650 examples drops to 65.4% from the previous level of 99.3%, and weighted recall to 58.9% from 99.3%. The full learning curve is shown in Figure 17. A quick comparison to Siskind\u2019s performance on this corpus confirmed that his system achieved comparable performance, showing that with current methods, this is close to the best performance that we are able to obtain on this more difficult corpus. One possible explanation for the smaller performance difference between the two systems on this corpus versus the geography domain is that in this domain, the correct meaning for a word is not necessarily the most \u201cgeneral,\u201d in terms of number of vertices, of all its candidate meanings. Therefore, the generality portion of the heuristic may negatively influence the performance of Wolfie in this domain.\nFinally, we show the change in performance with increasing ambiguity and increasing synonymy, holding the number of words and conceptual symbols constant. Figure 18 shows the weighted precision and recall with 1050 training examples for increasing levels of ambiguity, holding the synonymy level constant. Figure 19 shows the results at increasing levels of synonymy, holding ambiguity constant. Increasing the level of synonymy does not effect the results as much as increasing the level of ambiguity, which is as we expected. Holding the corpus size constant but increasing the number of competing meanings for a word increases the number of candidate meanings created by Wolfie while decreasing the amount of evidence available for each meaning (e.g., the first component of the heuristic\nmeasure) and makes the learning task more difficult. On the other hand, increasing the level of synonymy does not have the potential to mislead the learner.\nThe number of training examples required to reach a certain level of accuracy is also informative. In Table 4, we show the point at which a standard precision of 75% was first\nreached for each level of ambiguity. Note, however, that we only measured accuracy after each set of 100 training examples, so the numbers in the table are approximate.\nWe performed a second test of scalability on two corpora generated from lexicons an order of magnitude larger than those in the above tests. In these tests, we use a lexicon containing 1000 words and using 250 conceptual symbols. We generated both a corpus with no ambiguity, and one from a lexicon with ambiguity and synonymy similar to that found in the WordNet database (Beckwith, Fellbaum, Gross, & Miller, 1991); the ambiguity there is approximately 1.68 meanings per word and the synonymy 1.3 words per meaning. These corpora contained 9904 (no ambiguity) and 9948 examples, respectively, and we split the data into five sets of 9000 training examples each. For the easier large corpus, the maximum average of weighted precision and recall was 85.6%, at 8100 training examples, while for the harder corpus, the maximum average was 63.1% at 8600 training examples."}, {"heading": "6. Active Learning", "text": "As indicated in the previous sections, we have built an integrated system for language acquisition that is flexible and useful. However, a major difficulty remains: the construction of training corpora. Though annotating sentences is still arguably less work than building\nApply the learner to n bootstrap examples, creating a classifier. Until no examples remain or the annotator is unwilling to label more examples, do:\nan entire system by hand, the annotation task is also time-consuming and error-prone. Further, the training pairs often contain redundant information. We would like to minimize the amount of annotation required while still maintaining good generalization accuracy.\nTo do this, we turned to methods in active learning. Active learning is a research area in machine learning that features systems that automatically select the most informative examples for annotation and training (Angluin, 1988; Seung, Opper, & Sompolinsky, 1992), rather than relying on a benevolent teacher or random sampling. The primary goal of active learning is to reduce the number of examples that the system is trained on, while maintaining the accuracy of the acquired information. Active learning systems may construct their own examples, request certain types of examples, or determine which of a set of unsupervised examples are most usefully labeled. The last approach, selective sampling (Cohn et al., 1994), is particularly attractive in natural language learning, since there is an abundance of text, and we would like to annotate only the most informative sentences. For many language learning tasks, annotation is particularly time-consuming since it requires specifying a complex output rather than just a category label, so reducing the number of training examples required can greatly increase the utility of learning.\nIn this section, we explore the use of active learning, specifically selective sampling, for lexicon acquisition, and demonstrate that with active learning, fewer examples are required to achieve the same accuracy obtained by training on randomly chosen examples.\nThe basic algorithm for selective sampling is relatively simple. Learning begins with a small pool of annotated examples and a large pool of unannotated examples, and the learner attempts to choose the most informative additional examples for annotation. Existing work in the area has emphasized two approaches, certainty-based methods (Lewis & Catlett, 1994), and committee-based methods (McCallum & Nigam, 1998; Freund, Seung, Shamir, & Tishby, 1997; Liere & Tadepalli, 1997; Dagan & Engelson, 1995; Cohn et al., 1994); we focus here on the former.\nIn the certainty-based paradigm, a system is trained on a small number of annotated examples to learn an initial classifier. Next, the system examines unannotated examples, and attaches certainties to the predicted annotation of those examples. The k examples with the lowest certainties are then presented to the user for annotation and retraining. Many methods for attaching certainties have been used, but they typically attempt to estimate the probability that a classifier consistent with the prior training data will classify a new example correctly.\nLearn a lexicon with the examples annotated so far 1) For each phrase in an unannotated sentence:\nIf it has entries in the learned lexicon then its certainty is the average of the heuristic values of those entries Else, if it is a one-word phrase then its certainty is zero\n2) To rank sentences use: Total certainty of phrases from step 1\nFigure 20 presents abstract pseudocode for certainty-based selective sampling. In an ideal situation, the batch size, k, would be set to one to make the most intelligent decisions in future choices, but for efficiency reasons in retraining batch learning algorithms, it is frequently set higher. Results on a number of classification tasks have demonstrated that this general approach is effective in reducing the need for labeled examples (see citations above).\nApplying certainty-based sample selection to Wolfie requires determining the certainty of a complete annotation of a potential new training example, despite the fact that individual learned lexical entries and parsing operators perform only part of the overall annotation task. Therefore, our general approach is to compute certainties for pieces of an example, in our case, phrases, and combine these to obtain an overall certainty for an example. Since lexicon entries contain no explicit uncertainty parameters, we used Wolfie\u2019s heuristic measure to estimate uncertainty.\nTo choose the sentences to be annotated in each round, we first bootstrapped an initial lexicon from a small corpus, keeping track of the heuristic values of the learned items. Then, for each unannotated sentence, we took an average of the heuristic values of the lexicon entries learned for phrases in that sentence, giving a value of zero to unknown words but eliminating from consideration any words that we assume are known in advance, such as database constants. Thus, longer sentences with only a few known phrases would have a lower certainty than shorter sentences with the same number of known phrases; this is desirable since longer sentences will be more informative from a lexicon learning point of view. The sentences with the lowest values were chosen for annotation, added to the bootstrap corpus, and a new lexicon learned. Our technique is summarized in Figure 21.\nTo evaluate our technique, we compared active learning to learning from randomly selected examples, again measuring the effectiveness of learned lexicons as background knowledge for Chill. We again used the (smaller) U.S. Geography corpus, as in the original Wolfie tests, using the lexicons as background knowledge during parser acquisition (and using the same examples for parser acquisition).\nFor each trial in the following experiments, we first randomly divide the data into a training and test set. Then, n = 25 bootstrap examples are randomly selected from the\ntraining examples and in each step of active learning, the least certain k = 10 examples of the remaining training examples are selected and added to the training set. The result of learning on this set is evaluated after each step. The accuracy of the resulting learned parsers was compared to the accuracy of those learned using randomly chosen examples to learn lexicons and parsers, as in Section 5; in other words, we can think of the k examples in each round as being chosen randomly.\nFigure 22 shows the accuracy on unseen data of parsers learned using the lexicons learned by Wolfie when examples are chosen randomly and actively. There is an annotation savings of around 50 examples by using active learning: the maximum accuracy is reached after 175 examples, versus 225 with random examples. The advantage of using active learning is clear from the beginning, though the differences between the two curves are only statistically significant at 175 training examples. Since we are learning both lexicons and parsers, but only choosing examples based on Wolfie\u2019s certainty measures, the boost could be improved even further if Chill had a say in the examples chosen. See Thompson, Califf, and Mooney (1999) for a description of active learning for Chill."}, {"heading": "7. Related Work", "text": "In this section, we divide the previous research on related topics into the areas of lexicon acquisition and active learning."}, {"heading": "7.1 Lexicon Acquisition", "text": "Work on automated lexicon and language acquisition dates back to Siklossy (1972), who demonstrated a system that learned transformation patterns from logic back to natural\nlanguage. As already noted, the most closely related work is that of Jeff Siskind, which we described briefly in Section 2 and whose system we ran comparisons to in Section 5. Our definition of the learning problem can be compared to his \u201cmapping problem\u201d (Siskind, 1993). That formulation differs from ours in several respects. First, his sentence representations are terms instead of trees. However, as shown in Figure 7, terms can also be represented as trees that conform to our formalism with some minor additions. Next, his notion of interpretation does involve a type of tree, but carries the entire representation of a sentence up to the root. Also, it is not clear how he would handle quantified variables in the representation of sentences. Skolemization is possible, but then generalization across sentences would require special handling. We make the single-use assumption and he does not. Another difference is our bias towards a minimal number of lexicon entries, while he attempts to find a monosemous lexicon. His later work (Siskind, 2000) relaxes this to allow ambiguity and noise, but still biases towards minimizing ambiguity. However, his formal definition does not explicitly allow lexical ambiguity, but handles it in a heuristic manner. This, though, may lead to more robustness than our method in the face of noise. Finally, our definition allows phrasal lexicon entries.\nSiskind\u2019s work on this topic has explored many different variations along a continuum of using many constraints but requiring more time to incorporate each new example (Siskind, 1993), versus few constraints but requiring more training data (Siskind, 1996). Thus, perhaps his earlier systems would have been able to learn the lexicons of Section 5 more quickly; but crucially those systems did not allow lexical ambiguity, and thus also may not have learned as accurate a lexicon. More detailed comparisons to such versions of the system are outside the scope of this paper. Our goal with Wolfie is to learn a possibly ambiguous lexicon from as few examples as possible, and we thus made comparisons along this dimension alone.\nSiskind\u2019s approach, like ours, takes into account constraints between word meanings that are justified by the exclusivity and compositionality assumptions. His approach is somewhat more general in that it handles noise and referential uncertainty (uncertainty about the meaning of a sentence and thus multiple possible candidates), while ours is specialized for applications where the meaning (or meanings) is known. The experimental results in Section 5 demonstrate the advantage of our method for such an application. He has demonstrated his system to be capable of learning reasonably accurate lexicons from large, ambiguous, and noisy artificial corpora, but this accuracy is only assured if the learning algorithm converges, which did not occur for our smaller corpus in the experiments we ran. Also, as already noted, his system operates in an incremental or on-line fashion, discarding each sentence as it processes it, while ours is batch. In addition, his search for word meanings proceeds in two stages, as discussed in Section 2.2. By using common substructures, we combine these two stages in Wolfie. Both systems do have greedy aspects, ours in the choice of the next best lexical entry, his in the choice to discard utterances as noise or create a homonymous lexical entry. Finally, his system does not compute statistical correlations between words and their possible meanings, while ours does.\nBesides Siskind\u2019s work, there are others who approach the problem from a cognitive perspective. For example, De Marcken (1994) also uses child language learning as a motivation, but approaches the segmentation problem instead of the learning of semantics. For training input, he uses a flat list of tokens for semantic representations, but does not\nsegment sentences into words. He uses a variant of expectation-maximization (Dempster, Laird, & Rubin, 1977), together with a form of parsing and dictionary matching techniques, to segment the sentences and associate the segments with their most likely meaning. On the Childes corpus, the algorithm achieves very high precision, but recall is not provided.\nOthers taking the cognitive approach demonstrate language understanding by the ability to carry out some task such as parsing. For example, Nenov and Dyer (1994) describe a neural network model to map between visual and verbal-motor commands, and Colunga and Gasser (1998) use neural network modeling techniques for learning spatial concepts. Feldman and his colleagues at Berkeley (Feldman, Lakoff, & Shastri, 1995) are actively pursuing cognitive models of the acquisition of semantic concepts. Another Berkeley effort, the system by Regier (1996) is given examples of pictures paired with natural language descriptions that apply to the picture, and learns to judge whether a new sentence is true of a given picture.\nSimilar work by Suppes, Liang, and Bo\u0308ttner (1991) uses robots to demonstrate lexicon learning. A robot is trained on cognitive and perceptual concepts and their associated actions, and learns to execute simple commands. Along similar lines, Tishby and Gorin (1994) have a system that learns associations between words and actions, but they use a statistical framework to learn these associations, and do not handle structured representations. Similarly, Oates, Eyler-Walker, and Cohen (1999) discuss the acquisition of lexical hierarchies and their associated meaning as defined by the sensory environment of a robot.\nThe problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own. While most of these methods also compute association scores between pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. Also, Yamazaki, Pazzani, and Merz (1995) learn both translation rules and semantic hierarchies from parsed parallel sentences in Japanese and English. Of course, the main difference between this body of work and this paper is that we map words to semantic structures, not to other words.\nAs mentioned in the introduction, there is also a large body of work on learning lexical semantics but using different problem formulations than our own. For example, Collins and Singer (1999), Riloff and Jones (1999), Roark and Charniak (1998), and Schneider (1998) define semantic lexicons as a grouping of words into semantic categories, and in the latter case, add relational information. The result is typically applied as a semantic lexicon for information extraction or entity tagging. Pedersen and Chen (1995) describe a method for acquiring syntactic and semantic features of an unknown word, assuming access to an initial concept hierarchy, but they give no experimental results. Many systems (Fukumoto & Tsujii, 1995; Haruno, 1995; Johnston, Boguraev, & Pustejovsky, 1995; Webster & Marcus, 1995) focus only on acquisition of verbs or nouns, rather than all types of words. Also, the authors just named either do not experimentally evaluate their systems, or do not show the usefulness of the learned lexicons for a specific application.\nSeveral authors (Rooth, Riezler, Prescher, Carroll, & Beil, 1999; Collins, 1997; Ribas, 1994; Manning, 1993; Resnik, 1993; Brent, 1991) discuss the acquisition of subcategoriza-\ntion information for verbs, and others describe work on learning selectional restrictions (Manning, 1993; Brent, 1991). Both of these are different from the information required for mapping to semantic representation, but could be useful as a source of information to further constrain the search. Li (1998) further expands on the subcategorization work by inducing clustering information. Finally, several systems (Knight, 1996; Hastings, 1996; Russell, 1993) learn new words from context, assuming that a large initial lexicon and parsing system are already available.\nAnother related body of work is grammar acquisition, especially those areas that tightly integrate the grammar with a lexicon, such as with Categorial Grammars (Retore & Bonato, 2001; Dudau-Sofronie, Tellier, & Tommasi, 2001; Watkinson & Manandhar, 1999). The theory of Categorial Grammar also has ties with lexical semantics, but these semantics have not often been used for inference in support of high-level tasks such as database retrieval. While learning syntax and semantics together is arguably a more difficult task, the aforementioned work has not been evaluated on large corpora, presumably primarily due to the difficulty of annotation."}, {"heading": "7.2 Active Learning", "text": "With respect to additional active learning techniques, Cohn et al. (1994) were among the first to discuss certainty-based active learning methods in detail. They focus on a neural network approach to active learning in a version-space of concepts. Only a few of the researchers applying machine learning to natural language processing have utilized active learning (Hwa, 2001; Schohn & Cohn, 2000; Tong & Koller, 2000; Thompson et al., 1999; Argamon-Engelson & Dagan, 1999; Liere & Tadepalli, 1997; Lewis & Catlett, 1994), and the majority of these have addressed classification tasks such as part of speech tagging and text categorization. For example, Liere and Tadepalli (1997) apply active learning with committees to the problem of text categorization. They show improvements with active learning similar to those that we obtain, but use a committee of Winnow-based learners on a traditional classification task. Argamon-Engelson and Dagan (1999) also apply committee-based learning to part-of-speech tagging. In their work, a committee of hidden Markov models is used to select examples for annotation. Lewis and Catlett (1994) use heterogeneous certainty-based methods, in which a simple classifier is used to select examples that are then annotated and presented to a more powerful classifier.\nHowever, many language learning tasks require annotating natural language text with a complex output, such as a parse tree, semantic representation, or filled template. The application of active learning to tasks requiring such complex outputs has not been well studied, the exceptions being Hwa (2001), Soderland (1999), Thompson et al. (1999). The latter two include work on active learning applied to information extraction, and Thompson et al. (1999) includes work on active learning for semantic parsing. Hwa (2001) describes an interesting method for evaluating a statistical parser\u2019s uncertainty, when applied for syntactic parsing."}, {"heading": "8. Future Work", "text": "Although Wolfie\u2019s current greedy search method has performed quite well, a better search heuristic or alternative search strategy could result in improvements. We should also more\nthoroughly evaluate Wolfie\u2019s ability to learn long phrases, as we restricted this ability in the evaluations here. Another issue is robustness in the face of noise. The current algorithm is not guaranteed to learn a correct lexicon in even a noise-free corpus. The addition of noise complicates an analysis of circumstances in which mistakes are likely to happen. Further theoretical and empirical analysis of these issues is warranted.\nReferential uncertainty could be handled, with an increase in complexity, by forming LICS from more pairs of representations with which a phrase appears, but not between alternative representations of the same sentence. Then, once a pair is added to the lexicon, for each sentence containing that word, representations can be eliminated if they do not contain the learned meaning, provided another representation does contain it (thus allowing for lexical ambiguity). We plan to flesh this out and evaluate the results.\nA different avenue of exploration is to apply Wolfie to a corpus of sentences paired with the more common query language, SQL. Such corpora should be easily constructible by recording queries submitted to existing SQL applications along with their English forms, or translating existing lists of SQL queries into English (presumably an easier direction to translate). The fact that the same training data can be used to learn both a semantic lexicon and a parser also helps limit the overall burden of constructing a complete natural language interface.\nWith respect to active learning, experiments on additional corpora are needed to test the ability of our approach to reduce annotation costs in a variety of domains. It would also be interesting to explore active learning for other natural language processing problems such as syntactic parsing, word-sense disambiguation, and machine translation.\nOur current results have involved a certainty-based approach; however, proponents of committee-based approaches have convincing arguments for their theoretical advantages. Our initial attempts at adapting committee-based approaches to our systems were not very successful; however, additional research on this topic is indicated. One critical problem is obtaining diverse committees that properly sample the version space (Cohn et al., 1994)."}, {"heading": "9. Conclusions", "text": "Acquiring a semantic lexicon from a corpus of sentences labeled with representations of their meaning is an important problem that has not been widely studied. We present both a formalism of the learning problem and a greedy algorithm to find an approximate solution to it. Wolfie demonstrates that a fairly simple, greedy, symbolic learning algorithm performs well on this task and obtains performance superior to a previous lexicon acquisition system on a corpus of geography queries. Our results also demonstrate that our methods extend to a variety of natural languages besides English, and that they scale fairly well to larger, more difficult corpora.\nActive learning is a new area of machine learning that has been almost exclusively applied to classification tasks. We have demonstrated its successful application to more complex natural language mappings from phrases to semantic meanings, supporting the acquisition of lexicons and parsers. The wealth of unannotated natural language data, along with the difficulty of annotating such data, make selective sampling a potentially invaluable technique for natural language learning. Our results on realistic corpora indicate that example annotations savings as high as 22% can be achieved by employing active\nsample selection using only simple certainty measures for predictions on unannotated data. Improved sample selection methods and applications to other important language problems hold the promise of continued progress in using machine learning to construct effective natural language processing systems.\nMost experiments in corpus-based natural language have presented results on some subtask of natural language, and there are few results on whether the learned subsystems can be successfully integrated to build a complete NLP system. The experiments presented in this paper demonstrated how two learning systems, Wolfie and Chill, were successfully integrated to learn a complete NLP system for parsing database queries into executable logical form given only a single corpus of annotated queries, and further demonstrated the potential of active learning to reduce the annotation effort for learning for NLP."}, {"heading": "Acknowledgments", "text": "We would like to thank Jeff Siskind for providing us with his software, and for all his help in adapting it for use with our corpus. Thanks also to Agapito Sustaita, Esra Erdem, and Marshall Mayberry for their translation efforts, and to the three anonymous reviewers for their comments which helped improve the paper. This research was supported by the National Science Foundation under grants IRI-9310819 and IRI-9704943."}], "references": [{"title": "Induction of augmented transition networks", "author": ["J.R. Anderson"], "venue": "Cognitive Science,", "citeRegEx": "Anderson,? \\Q1977\\E", "shortCiteRegEx": "Anderson", "year": 1977}, {"title": "Queries and concept learning", "author": ["D. Angluin"], "venue": "Machine Learning,", "citeRegEx": "Angluin,? \\Q1988\\E", "shortCiteRegEx": "Angluin", "year": 1988}, {"title": "Committee-based sample selection for probabilistic classifiers", "author": ["S. Argamon-Engelson", "I. Dagan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Argamon.Engelson and Dagan,? \\Q1999\\E", "shortCiteRegEx": "Argamon.Engelson and Dagan", "year": 1999}, {"title": "WordNet: A lexical database organized on psycholinguistic principles", "author": ["R. Beckwith", "C. Fellbaum", "D. Gross", "G. Miller"], "venue": null, "citeRegEx": "Beckwith et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Beckwith et al\\.", "year": 1991}, {"title": "Automatic acquisition of subcategorization frames from untagged text", "author": ["M. Brent"], "venue": "In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Brent,? \\Q1991\\E", "shortCiteRegEx": "Brent", "year": 1991}, {"title": "A statistical approach to machine translation", "author": ["P Brown"], "venue": "Computational Linguistics,", "citeRegEx": "Brown,? \\Q1990\\E", "shortCiteRegEx": "Brown", "year": 1990}, {"title": "Deriving translation data from bilingual texts", "author": ["R. Catizone", "G. Russell", "S. Warwick"], "venue": "In Proceedings of the First International Lexical Acquisition Workshop", "citeRegEx": "Catizone et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Catizone et al\\.", "year": 1993}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Unsupervised models for named entity classification", "author": ["M. Collins", "Y. Singer"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99) University of Maryland", "citeRegEx": "Collins and Singer,? \\Q1999\\E", "shortCiteRegEx": "Collins and Singer", "year": 1999}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["M.J. Collins"], "venue": "In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Collins,? \\Q1997\\E", "shortCiteRegEx": "Collins", "year": 1997}, {"title": "Linguistic relativity and word acquisition: a computational approach", "author": ["E. Colunga", "M. Gasser"], "venue": "In Proceedings of the Twenty First Annual Conference of the Cognitive Science Society,", "citeRegEx": "Colunga and Gasser,? \\Q1998\\E", "shortCiteRegEx": "Colunga and Gasser", "year": 1998}, {"title": "Committee-based sampling for training probabilistic classifiers", "author": ["I. Dagan", "S.P. Engelson"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning", "citeRegEx": "Dagan and Engelson,? \\Q1995\\E", "shortCiteRegEx": "Dagan and Engelson", "year": 1995}, {"title": "The acquisition of a lexicon from paired phoneme sequences and semantic representations", "author": ["C. De Marcken"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Marcken,? \\Q1994\\E", "shortCiteRegEx": "Marcken", "year": 1994}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Learning categorial grammars from semantic types", "author": ["Dudau-Sofronie", "Tellier", "Tommasi"], "venue": "In Proceedings of the 13th Amsterdam Colloquium,", "citeRegEx": "Dudau.Sofronie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dudau.Sofronie et al\\.", "year": 2001}, {"title": "The neural theory of language project http://www.icsi.berkeley.edu/ntl", "author": ["J. Feldman", "G. Lakoff", "L. Shastri"], "venue": "International Computer Science Institute,", "citeRegEx": "Feldman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 1995}, {"title": "The case for case", "author": ["C. Fillmore"], "venue": "Universals in Linguistic Theory", "citeRegEx": "Fillmore,? \\Q1968\\E", "shortCiteRegEx": "Fillmore", "year": 1968}, {"title": "The mechanisms of \u201cConstruction Grammar", "author": ["C. Fillmore"], "venue": "Proceedings of the Fourteenth Annual Meeting of the Berkeley Linguistics Society,", "citeRegEx": "Fillmore,? \\Q1988\\E", "shortCiteRegEx": "Fillmore", "year": 1988}, {"title": "Knowledge acquisition via incremental conceptual clustering", "author": ["D.H. Fisher"], "venue": "Machine Learning,", "citeRegEx": "Fisher,? \\Q1987\\E", "shortCiteRegEx": "Fisher", "year": 1987}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Representation and acquisition of verbal polysemy", "author": ["F. Fukumoto", "J. Tsujii"], "venue": null, "citeRegEx": "Fukumoto and Tsujii,? \\Q1995\\E", "shortCiteRegEx": "Fukumoto and Tsujii", "year": 1995}, {"title": "Identifying word correspondences in parallel texts", "author": ["W. Gale", "K. Church"], "venue": "In Proceedings of the Fourth DARPA Speech and Natural Language Workshop", "citeRegEx": "Gale and Church,? \\Q1991\\E", "shortCiteRegEx": "Gale and Church", "year": 1991}, {"title": "Computers and Intractability: A Guide to the Theory of NP-Completeness", "author": ["M. Garey", "D. Johnson"], "venue": null, "citeRegEx": "Garey and Johnson,? \\Q1979\\E", "shortCiteRegEx": "Garey and Johnson", "year": 1979}, {"title": "Constructions: A Construction Grammar Approach to Argument Structure", "author": ["A. Goldberg"], "venue": null, "citeRegEx": "Goldberg,? \\Q1995\\E", "shortCiteRegEx": "Goldberg", "year": 1995}, {"title": "Sextant: Extracting semantics from raw text, implementation details", "author": ["G. Grefenstette"], "venue": "Integrated Computer-Aided Engineering,", "citeRegEx": "Grefenstette,? \\Q1994\\E", "shortCiteRegEx": "Grefenstette", "year": 1994}, {"title": "From context-free to definite-clause grammars: a typetheoretic approach", "author": ["J. Haas", "B. Jayaraman"], "venue": "Journal of Logic Programming,", "citeRegEx": "Haas and Jayaraman,? \\Q1997\\E", "shortCiteRegEx": "Haas and Jayaraman", "year": 1997}, {"title": "A case frame learning method for Japanese polysemous verbs. In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity", "author": ["M. Haruno"], "venue": null, "citeRegEx": "Haruno,? \\Q1995\\E", "shortCiteRegEx": "Haruno", "year": 1995}, {"title": "Implications of an automatic lexical acquisition mechanism", "author": ["P. Hastings"], "venue": null, "citeRegEx": "Hastings,? \\Q1996\\E", "shortCiteRegEx": "Hastings", "year": 1996}, {"title": "On minimizing training corpus for parser acquisition", "author": ["R. Hwa"], "venue": "In Proceedings of the Fifth Computational Natural Language Learning Workshop", "citeRegEx": "Hwa,? \\Q2001\\E", "shortCiteRegEx": "Hwa", "year": 2001}, {"title": "Semantic Structures", "author": ["R. Jackendoff"], "venue": null, "citeRegEx": "Jackendoff,? \\Q1990\\E", "shortCiteRegEx": "Jackendoff", "year": 1990}, {"title": "The acquisition and interpretation of complex nominals. In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity", "author": ["M. Johnston", "B. Boguraev", "J. Pustejovsky"], "venue": null, "citeRegEx": "Johnston et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Johnston et al\\.", "year": 1995}, {"title": "Learning word meanings by instruction", "author": ["K. Knight"], "venue": "In Proceedings of the Thirteenth National Conference on Artificial Intelligence", "citeRegEx": "Knight,? \\Q1996\\E", "shortCiteRegEx": "Knight", "year": 1996}, {"title": "Automatic parameter selection by minimizing estimated error", "author": ["R. Kohavi", "G. John"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning", "citeRegEx": "Kohavi and John,? \\Q1995\\E", "shortCiteRegEx": "Kohavi and John", "year": 1995}, {"title": "Building an MT dictionary from parallel texts based on linguistic and statistical information", "author": ["A. Kumano", "H. Hirakawa"], "venue": "In Proceedings of the Fifteenth International Conference on Computational Linguistics,", "citeRegEx": "Kumano and Hirakawa,? \\Q1994\\E", "shortCiteRegEx": "Kumano and Hirakawa", "year": 1994}, {"title": "Inductive Logic Programming: Techniques and Applications", "author": ["N. Lavrac", "S. Dz\u0306eroski"], "venue": null, "citeRegEx": "Lavrac\u0306 and Dz\u0306eroski,? \\Q1994\\E", "shortCiteRegEx": "Lavrac\u0306 and Dz\u0306eroski", "year": 1994}, {"title": "Heterogeneous uncertainty sampling for supervised learning", "author": ["D.D. Lewis", "J. Catlett"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning", "citeRegEx": "Lewis and Catlett,? \\Q1994\\E", "shortCiteRegEx": "Lewis and Catlett", "year": 1994}, {"title": "A probabilistic approach to lexical semantic knowledge acquisition and structural disambiguation", "author": ["H. Li"], "venue": "Ph.D. thesis,", "citeRegEx": "Li,? \\Q1998\\E", "shortCiteRegEx": "Li", "year": 1998}, {"title": "Active learning with committees for text categorization", "author": ["R. Liere", "P. Tadepalli"], "venue": "In Proceedings of the Fourteenth National Conference on Artificial Intelligence", "citeRegEx": "Liere and Tadepalli,? \\Q1997\\E", "shortCiteRegEx": "Liere and Tadepalli", "year": 1997}, {"title": "Automatic acquisition of a large subcategorization dictionary from corpora", "author": ["C.D. Manning"], "venue": "In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Manning,? \\Q1993\\E", "shortCiteRegEx": "Manning", "year": 1993}, {"title": "Employing EM and pool-based active learning for text classification", "author": ["A.K. McCallum", "K. Nigam"], "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning", "citeRegEx": "McCallum and Nigam,? \\Q1998\\E", "shortCiteRegEx": "McCallum and Nigam", "year": 1998}, {"title": "Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons", "author": ["I.D. Melamed"], "venue": "In Proceedings of the Third Workshop on Very Large Corpora", "citeRegEx": "Melamed,? \\Q1995\\E", "shortCiteRegEx": "Melamed", "year": 1995}, {"title": "Models of translational equivalence among words", "author": ["I.D. Melamed"], "venue": "Computational Linguistics,", "citeRegEx": "Melamed,? \\Q2000\\E", "shortCiteRegEx": "Melamed", "year": 2000}, {"title": "Inductive Logic Programming", "author": ["S. Muggleton"], "venue": null, "citeRegEx": "Muggleton,? \\Q1992\\E", "shortCiteRegEx": "Muggleton", "year": 1992}, {"title": "Inverse entailment and Progol", "author": ["S. Muggleton"], "venue": "New Generation Computing Journal,", "citeRegEx": "Muggleton,? \\Q1995\\E", "shortCiteRegEx": "Muggleton", "year": 1995}, {"title": "Efficient induction of logic programs", "author": ["S. Muggleton", "C. Feng"], "venue": "In Proceedings of the First Conference on Algorithmic Learning Theory Tokyo, Japan. Ohmsha", "citeRegEx": "Muggleton and Feng,? \\Q1990\\E", "shortCiteRegEx": "Muggleton and Feng", "year": 1990}, {"title": "Perceptually grounded language learning: Part 2\u2013 DETE: A neural/procedural model", "author": ["V.I. Nenov", "M.G. Dyer"], "venue": "Connection Science,", "citeRegEx": "Nenov and Dyer,? \\Q1994\\E", "shortCiteRegEx": "Nenov and Dyer", "year": 1994}, {"title": "Using syntax to learn semantics: an experiment in language acquisition with a mobile robot", "author": ["T. Oates", "Z. Eyler-Walker", "P. Cohen"], "venue": "Tech. rep. 99-35,", "citeRegEx": "Oates et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Oates et al\\.", "year": 1999}, {"title": "Mathematical Methods in Linguistics", "author": ["B. Partee", "A. Meulen", "R. Wall"], "venue": null, "citeRegEx": "Partee et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Partee et al\\.", "year": 1990}, {"title": "Lexical acquisition via constraint solving", "author": ["T. Pedersen", "W. Chen"], "venue": null, "citeRegEx": "Pedersen and Chen,? \\Q1995\\E", "shortCiteRegEx": "Pedersen and Chen", "year": 1995}, {"title": "A note on inductive generalization", "author": ["G.D. Plotkin"], "venue": "Machine Intelligence (Vol", "citeRegEx": "Plotkin,? \\Q1970\\E", "shortCiteRegEx": "Plotkin", "year": 1970}, {"title": "Using a logic grammar to learn a lexicon", "author": ["M. Rayner", "A. Hugosson", "G. Hagert"], "venue": "Tech. rep. R88001, Swedish Institute of Computer Science", "citeRegEx": "Rayner et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rayner et al\\.", "year": 1988}, {"title": "The human semantic potential: spatial language and constrained connectionism", "author": ["T. Regier"], "venue": null, "citeRegEx": "Regier,? \\Q1996\\E", "shortCiteRegEx": "Regier", "year": 1996}, {"title": "Selection and information: a class-based approach to lexical relationships", "author": ["P. Resnik"], "venue": "Ph.D. thesis,", "citeRegEx": "Resnik,? \\Q1993\\E", "shortCiteRegEx": "Resnik", "year": 1993}, {"title": "Learning rigid lambek grammars and minimalist grammars from structured sentences", "author": ["C. Retore", "R. Bonato"], "venue": "In Proceedings of the Third Learning Language in Logic Workshop Strasbourg, France", "citeRegEx": "Retore and Bonato,? \\Q2001\\E", "shortCiteRegEx": "Retore and Bonato", "year": 2001}, {"title": "An experiment on learning appropriate selectional restrictions from a parsed corpus", "author": ["F. Ribas"], "venue": "In Proceedings of the Fifteenth International Conference on Computational Linguistics,", "citeRegEx": "Ribas,? \\Q1994\\E", "shortCiteRegEx": "Ribas", "year": 1994}, {"title": "Learning dictionaries for information extraction by multilevel bootstrapping", "author": ["E. Riloff", "R. Jones"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence", "citeRegEx": "Riloff and Jones,? \\Q1999\\E", "shortCiteRegEx": "Riloff and Jones", "year": 1999}, {"title": "Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction", "author": ["B. Roark", "E. Charniak"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and COLING-98", "citeRegEx": "Roark and Charniak,? \\Q1998\\E", "shortCiteRegEx": "Roark and Charniak", "year": 1998}, {"title": "Inducing a semantically annotated lexicon via EM-based clustering", "author": ["M. Rooth", "S. Riezler", "D. Prescher", "G. Carroll", "F. Beil"], "venue": "In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Rooth et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Rooth et al\\.", "year": 1999}, {"title": "Language Acquisition in a Unification-Based Grammar Processing System Using a Real World Knowledge Base", "author": ["D. Russell"], "venue": "Ph.D. thesis,", "citeRegEx": "Russell,? \\Q1993\\E", "shortCiteRegEx": "Russell", "year": 1993}, {"title": "Conceptual Information Processing", "author": ["R.C. Schank"], "venue": null, "citeRegEx": "Schank,? \\Q1975\\E", "shortCiteRegEx": "Schank", "year": 1975}, {"title": "A lexically-intensive algorithm for domain-specific knowledge acquisition", "author": ["R. Schneider"], "venue": "In Proceedings of the Joint Conference on New Methods in Language Processing and Computational Natural Language Learning,", "citeRegEx": "Schneider,? \\Q1998\\E", "shortCiteRegEx": "Schneider", "year": 1998}, {"title": "Less is more: Active learning with support vector machines", "author": ["G. Schohn", "D. Cohn"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning", "citeRegEx": "Schohn and Cohn,? \\Q2000\\E", "shortCiteRegEx": "Schohn and Cohn", "year": 2000}, {"title": "Inductive logic programming for corpus-based acquisition of semantic lexicons", "author": ["P. S\u00e9billot", "P. Bouillon", "C. Fabre"], "venue": "In Proceedings of 2nd Learning Language in Logic (LLL) Workshop Lisbon,", "citeRegEx": "S\u00e9billot et al\\.,? \\Q2000\\E", "shortCiteRegEx": "S\u00e9billot et al\\.", "year": 2000}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "In Proceedings of the ACM Workshop on Computational Learning Theory Pittsburgh,", "citeRegEx": "Seung et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Seung et al\\.", "year": 1992}, {"title": "Natural language learning by computer", "author": ["L. Siklossy"], "venue": null, "citeRegEx": "Siklossy,? \\Q1972\\E", "shortCiteRegEx": "Siklossy", "year": 1972}, {"title": "Learning word-to-meaning mappings", "author": ["J.M. Siskind"], "venue": null, "citeRegEx": "Siskind,? \\Q2000\\E", "shortCiteRegEx": "Siskind", "year": 2000}, {"title": "Naive Physics, Event Perception, Lexical Semantics and Language Acquisition", "author": ["J.M. Siskind"], "venue": "Ph.D. thesis,", "citeRegEx": "Siskind,? \\Q1992\\E", "shortCiteRegEx": "Siskind", "year": 1992}, {"title": "A computational study of cross-situational techniques for learning word-to-meaning", "author": ["J.M. Siskind"], "venue": "mappings. Cognition,", "citeRegEx": "Siskind,? \\Q1996\\E", "shortCiteRegEx": "Siskind", "year": 1996}, {"title": "Lexical acquisition as constraint satisfaction", "author": ["J.M. Siskind"], "venue": "Tech. rep. IRCS-93-41,", "citeRegEx": "Siskind,? \\Q1993\\E", "shortCiteRegEx": "Siskind", "year": 1993}, {"title": "Translating collocations for bilingual lexicons: A statistical approach", "author": ["F. Smadja", "K.R. McKeown", "V. Hatzivassiloglou"], "venue": "Computational Linguistics,", "citeRegEx": "Smadja et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Smadja et al\\.", "year": 1996}, {"title": "Learning information extraction rules for semi-structured and free text", "author": ["S. Soderland"], "venue": "Machine Learning,", "citeRegEx": "Soderland,? \\Q1999\\E", "shortCiteRegEx": "Soderland", "year": 1999}, {"title": "Complexity issues in robotic machine learning of natural language", "author": ["P. Suppes", "L. Liang", "M. B\u00f6ttner"], "venue": "Modeling Complex Phenomena, Proceedings of the 3rd Woodward Conference,", "citeRegEx": "Suppes et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Suppes et al\\.", "year": 1991}, {"title": "Active learning for natural language parsing and information extraction", "author": ["C.A. Thompson", "M.E. Califf", "R.J. Mooney"], "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning", "citeRegEx": "Thompson et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Thompson et al\\.", "year": 1999}, {"title": "Acquisition of a lexicon from semantic representations of sentences", "author": ["C.A. Thompson"], "venue": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Thompson,? \\Q1995\\E", "shortCiteRegEx": "Thompson", "year": 1995}, {"title": "Algebraic learning of statistical associations for language acquisition", "author": ["N. Tishby", "A. Gorin"], "venue": "Computer Speech and Language,", "citeRegEx": "Tishby and Gorin,? \\Q1994\\E", "shortCiteRegEx": "Tishby and Gorin", "year": 1994}, {"title": "Efficient Parsing for Natural Language", "author": ["M. Tomita"], "venue": null, "citeRegEx": "Tomita,? \\Q1986\\E", "shortCiteRegEx": "Tomita", "year": 1986}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning", "citeRegEx": "Tong and Koller,? \\Q2000\\E", "shortCiteRegEx": "Tong and Koller", "year": 2000}, {"title": "Unsupervised lexical learning with categorial grammars using the lll corpus. In Learning Language In Logic (LLL) Workshop Bled, Slovenia", "author": ["S. Watkinson", "S. Manandhar"], "venue": null, "citeRegEx": "Watkinson and Manandhar,? \\Q1999\\E", "shortCiteRegEx": "Watkinson and Manandhar", "year": 1999}, {"title": "Automatic acquisition of the lexical semantics of verbs from sentence frames", "author": ["M. Webster", "M. Marcus"], "venue": "In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL-89),", "citeRegEx": "Webster and Marcus,? \\Q1995\\E", "shortCiteRegEx": "Webster and Marcus", "year": 1995}, {"title": "Large-scale automatic extraction of an English-Chinese translation lexicon", "author": ["D. Wu", "X. Xia"], "venue": "Machine Translation,", "citeRegEx": "Wu and Xia,? \\Q1995\\E", "shortCiteRegEx": "Wu and Xia", "year": 1995}, {"title": "Learning hierarchies from ambiguous natural language data", "author": ["T. Yamazaki", "M. Pazzani", "C. Merz"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning", "citeRegEx": "Yamazaki et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Yamazaki et al\\.", "year": 1995}, {"title": "Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers", "author": ["J.M. Zelle"], "venue": "Ph.D. thesis, Department of Computer Sciences,", "citeRegEx": "Zelle,? \\Q1995\\E", "shortCiteRegEx": "Zelle", "year": 1995}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["J.M. Zelle", "R.J. Mooney"], "venue": "In Proceedings of the Thirteenth National Conference on Artificial Intelligence", "citeRegEx": "Zelle and Mooney,? \\Q1996\\E", "shortCiteRegEx": "Zelle and Mooney", "year": 1996}, {"title": "Human behavior and the principle of least effort", "author": ["G. Zipf"], "venue": null, "citeRegEx": "Zipf,? \\Q1949\\E", "shortCiteRegEx": "Zipf", "year": 1949}], "referenceMentions": [{"referenceID": 67, "context": "Although many others (S\u00e9billot, Bouillon, & Fabre, 2000; Riloff & Jones, 1999; Siskind, 1996; Hastings, 1996; Grefenstette, 1994; Brent, 1991) have presented systems for learning information about lexical semantics, we present here a system for learning lexicons of phrasemeaning pairs.", "startOffset": 21, "endOffset": 142}, {"referenceID": 27, "context": "Although many others (S\u00e9billot, Bouillon, & Fabre, 2000; Riloff & Jones, 1999; Siskind, 1996; Hastings, 1996; Grefenstette, 1994; Brent, 1991) have presented systems for learning information about lexical semantics, we present here a system for learning lexicons of phrasemeaning pairs.", "startOffset": 21, "endOffset": 142}, {"referenceID": 24, "context": "Although many others (S\u00e9billot, Bouillon, & Fabre, 2000; Riloff & Jones, 1999; Siskind, 1996; Hastings, 1996; Grefenstette, 1994; Brent, 1991) have presented systems for learning information about lexical semantics, we present here a system for learning lexicons of phrasemeaning pairs.", "startOffset": 21, "endOffset": 142}, {"referenceID": 4, "context": "Although many others (S\u00e9billot, Bouillon, & Fabre, 2000; Riloff & Jones, 1999; Siskind, 1996; Hastings, 1996; Grefenstette, 1994; Brent, 1991) have presented systems for learning information about lexical semantics, we present here a system for learning lexicons of phrasemeaning pairs.", "startOffset": 21, "endOffset": 142}, {"referenceID": 81, "context": "First, its output can be used by a system, Chill (Zelle & Mooney, 1996; Zelle, 1995), that learns to parse sentences into semantic representations.", "startOffset": 49, "endOffset": 84}, {"referenceID": 4, "context": "Although many others (S\u00e9billot, Bouillon, & Fabre, 2000; Riloff & Jones, 1999; Siskind, 1996; Hastings, 1996; Grefenstette, 1994; Brent, 1991) have presented systems for learning information about lexical semantics, we present here a system for learning lexicons of phrasemeaning pairs. Further, our work is unique in its combination of several features, though prior work has included some of these aspects. First, its output can be used by a system, Chill (Zelle & Mooney, 1996; Zelle, 1995), that learns to parse sentences into semantic representations. Second, it uses a fairly straightforward batch, greedy, heuristic learning algorithm that requires only a small number of examples to generalize well. Third, it is easily extendible to new representation formalisms. Fourth, it requires no prior knowledge although it can exploit an initial lexicon if provided. Finally, it simplifies the learning problem by making several assumptions about the training data, as described further in Section 3.2. We test Wolfie\u2019s ability to acquire a semantic lexicon for a natural language interface to a geographical database using a corpus of queries collected from human subjects and annotated with their logical form. In this test, Wolfie is integrated with Chill, which learns parsers but requires a semantic lexicon (previously built manually). The results demonstrate that the final acquired parser performs nearly as accurately at answering novel questions when using a learned lexicon as when using a hand-built lexicon. Wolfie is also compared to an alternative lexicon acquisition system developed by Siskind (1996), demonstrating superior performance on this task.", "startOffset": 130, "endOffset": 1619}, {"referenceID": 42, "context": "Chill uses inductive logic programming (Muggleton, 1992; Lavrac\u0306 & Dz\u0306eroski, 1994) to learn a deterministic shift-reduce parser (Tomita, 1986) written in Prolog.", "startOffset": 39, "endOffset": 83}, {"referenceID": 75, "context": "Chill uses inductive logic programming (Muggleton, 1992; Lavrac\u0306 & Dz\u0306eroski, 1994) to learn a deterministic shift-reduce parser (Tomita, 1986) written in Prolog.", "startOffset": 129, "endOffset": 143}, {"referenceID": 81, "context": "For details on this and the other two parsing operators, see Zelle and Mooney (1996). By using Wolfie, the lexicon is provided automatically.", "startOffset": 61, "endOffset": 85}, {"referenceID": 65, "context": "2 Jeff Siskind\u2019s Lexicon Learning Research The most closely related previous research into automated lexicon acquisition is that of Siskind (1996), itself inspired by work by Rayner, Hugosson, and Hagert (1988).", "startOffset": 7, "endOffset": 147}, {"referenceID": 65, "context": "2 Jeff Siskind\u2019s Lexicon Learning Research The most closely related previous research into automated lexicon acquisition is that of Siskind (1996), itself inspired by work by Rayner, Hugosson, and Hagert (1988). As we will be comparing our system to his in Section 5, we describe the main features of his research in this section.", "startOffset": 7, "endOffset": 211}, {"referenceID": 66, "context": "Earlier work (Siskind, 1992) also evaluated versions of his technique on a quite small corpus of real English and Japanese sentences.", "startOffset": 13, "endOffset": 28}, {"referenceID": 65, "context": "Siskind (1996) shows the effectiveness of his approach on a series of artificial corpora.", "startOffset": 0, "endOffset": 15}, {"referenceID": 59, "context": "\u201d Using a conceptual dependency (Schank, 1975) representation in Prolog list form, the meaning is: [ingest, agent:[person, sex:female, age:child], patient:[food, type:pasta, accomp:[food, type:cheese]]].", "startOffset": 32, "endOffset": 46}, {"referenceID": 53, "context": "2 Implications of the Definition This definition of the lexicon acquisition problem differs from that given by other authors, including Riloff and Jones (1999), Siskind (1996), Manning (1993), Brent (1991) and others, as further discussed in Section 7.", "startOffset": 136, "endOffset": 160}, {"referenceID": 53, "context": "2 Implications of the Definition This definition of the lexicon acquisition problem differs from that given by other authors, including Riloff and Jones (1999), Siskind (1996), Manning (1993), Brent (1991) and others, as further discussed in Section 7.", "startOffset": 136, "endOffset": 176}, {"referenceID": 37, "context": "2 Implications of the Definition This definition of the lexicon acquisition problem differs from that given by other authors, including Riloff and Jones (1999), Siskind (1996), Manning (1993), Brent (1991) and others, as further discussed in Section 7.", "startOffset": 177, "endOffset": 192}, {"referenceID": 4, "context": "2 Implications of the Definition This definition of the lexicon acquisition problem differs from that given by other authors, including Riloff and Jones (1999), Siskind (1996), Manning (1993), Brent (1991) and others, as further discussed in Section 7.", "startOffset": 193, "endOffset": 206}, {"referenceID": 17, "context": ", Haas and Jayaraman, 1997; Siskind, 19964) While there is some debate in the linguistics community about the ability of compositional techniques to handle all phenomena (Fillmore, 1988; Goldberg, 1995), making this assumption simplifies the learning process and works reasonably for the domains of interest here.", "startOffset": 170, "endOffset": 202}, {"referenceID": 23, "context": ", Haas and Jayaraman, 1997; Siskind, 19964) While there is some debate in the linguistics community about the ability of compositional techniques to handle all phenomena (Fillmore, 1988; Goldberg, 1995), making this assumption simplifies the learning process and works reasonably for the domains of interest here.", "startOffset": 170, "endOffset": 202}, {"referenceID": 25, "context": "This assumption is similar to the linking rules of Jackendoff (1990), and has been used in previous work on grammar and language acquisition (e.", "startOffset": 51, "endOffset": 69}, {"referenceID": 65, "context": "In fact, all of these assumptions except for single-use were made by Siskind (1996); see Section 7 for details.", "startOffset": 69, "endOffset": 84}, {"referenceID": 64, "context": "Siskind (1993) defined fracturing (he also calls it the Unlink* operation) over terms such that the result includes all subterms of an expression plus \u22a5.", "startOffset": 0, "endOffset": 15}, {"referenceID": 0, "context": "for each phrase, as proposed by Anderson (1977). However, the meanings of an ambiguous phrase are disjunctive, and this intersection would be empty.", "startOffset": 32, "endOffset": 48}, {"referenceID": 0, "context": "for each phrase, as proposed by Anderson (1977). However, the meanings of an ambiguous phrase are disjunctive, and this intersection would be empty. A similar difficulty would be expected with the positive-only compression of Muggleton (1995).", "startOffset": 32, "endOffset": 243}, {"referenceID": 65, "context": "Though, of course, interpretation functions are not the only way to guarantee a covering lexicon \u2013 see Siskind (1993) for an alternative.", "startOffset": 103, "endOffset": 118}, {"referenceID": 73, "context": "We previously (Thompson, 1995) presented results demonstrating learning representations of a different form, that of a case-role representation (Fillmore, 1968) augmented with Conceptual Dependency (Schank, 1975) information.", "startOffset": 14, "endOffset": 30}, {"referenceID": 16, "context": "We previously (Thompson, 1995) presented results demonstrating learning representations of a different form, that of a case-role representation (Fillmore, 1968) augmented with Conceptual Dependency (Schank, 1975) information.", "startOffset": 144, "endOffset": 160}, {"referenceID": 59, "context": "We previously (Thompson, 1995) presented results demonstrating learning representations of a different form, that of a case-role representation (Fillmore, 1968) augmented with Conceptual Dependency (Schank, 1975) information.", "startOffset": 198, "endOffset": 212}, {"referenceID": 49, "context": "Therefore, we use LICS with an addition similar to computing the Least General Generalization of first-order clauses (Plotkin, 1970).", "startOffset": 117, "endOffset": 132}, {"referenceID": 18, "context": "heuristic used by Cobweb (Fisher, 1987), which measures the utility of clusters based on attribute-value pairs and categories, instead of meanings and phrases.", "startOffset": 25, "endOffset": 39}, {"referenceID": 65, "context": "We compared our system to an incremental (on-line) lexicon learner developed by Siskind (1996). To make a more equitable comparison to our batch algorithm, we ran his in a \u201csimulated\u201d batch mode, by repeatedly presenting the corpus 500 times, analogous to running 500 epochs to train a neural network.", "startOffset": 80, "endOffset": 95}, {"referenceID": 29, "context": "In this corpus, both the sentences and their representations are completely artificial, and the sentence representation is a variable-free representation, as suggested by the work of Jackendoff (1990) and others.", "startOffset": 183, "endOffset": 201}, {"referenceID": 83, "context": "We used a grammar to generate utterances and their meanings from each original lexicon, with terminal categories selected using a distribution based on Zipf\u2019s Law (Zipf, 1949).", "startOffset": 163, "endOffset": 175}, {"referenceID": 65, "context": "We generated several lexicons and associated corpora, varying the ambiguity rate (number of meanings per word) and synonymy rate (number of words per meaning), as in Siskind (1996). Meaning representations were generated using a set of \u201cconceptual symbols\u201d that combined to form the meaning for each word.", "startOffset": 166, "endOffset": 181}, {"referenceID": 1, "context": "Active learning is a research area in machine learning that features systems that automatically select the most informative examples for annotation and training (Angluin, 1988; Seung, Opper, & Sompolinsky, 1992), rather than relying on a benevolent teacher or random sampling.", "startOffset": 161, "endOffset": 211}, {"referenceID": 7, "context": "The last approach, selective sampling (Cohn et al., 1994), is particularly attractive in natural language learning, since there is an abundance of text, and we would like to annotate only the most informative sentences.", "startOffset": 38, "endOffset": 57}, {"referenceID": 7, "context": "Existing work in the area has emphasized two approaches, certainty-based methods (Lewis & Catlett, 1994), and committee-based methods (McCallum & Nigam, 1998; Freund, Seung, Shamir, & Tishby, 1997; Liere & Tadepalli, 1997; Dagan & Engelson, 1995; Cohn et al., 1994); we focus here on the former.", "startOffset": 134, "endOffset": 265}, {"referenceID": 73, "context": "See Thompson, Califf, and Mooney (1999) for a description of active learning for Chill.", "startOffset": 4, "endOffset": 40}, {"referenceID": 64, "context": "Work on automated lexicon and language acquisition dates back to Siklossy (1972), who demonstrated a system that learned transformation patterns from logic back to natural", "startOffset": 65, "endOffset": 81}, {"referenceID": 68, "context": "Our definition of the learning problem can be compared to his \u201cmapping problem\u201d (Siskind, 1993).", "startOffset": 80, "endOffset": 95}, {"referenceID": 65, "context": "His later work (Siskind, 2000) relaxes this to allow ambiguity and noise, but still biases towards minimizing ambiguity.", "startOffset": 15, "endOffset": 30}, {"referenceID": 68, "context": "Siskind\u2019s work on this topic has explored many different variations along a continuum of using many constraints but requiring more time to incorporate each new example (Siskind, 1993), versus few constraints but requiring more training data (Siskind, 1996).", "startOffset": 168, "endOffset": 183}, {"referenceID": 67, "context": "Siskind\u2019s work on this topic has explored many different variations along a continuum of using many constraints but requiring more time to incorporate each new example (Siskind, 1993), versus few constraints but requiring more training data (Siskind, 1996).", "startOffset": 241, "endOffset": 256}, {"referenceID": 12, "context": "For example, De Marcken (1994) also uses child language learning as a motivation, but approaches the segmentation problem instead of the learning of semantics.", "startOffset": 16, "endOffset": 31}, {"referenceID": 40, "context": "The problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own.", "startOffset": 62, "endOffset": 240}, {"referenceID": 26, "context": "Many systems (Fukumoto & Tsujii, 1995; Haruno, 1995; Johnston, Boguraev, & Pustejovsky, 1995; Webster & Marcus, 1995) focus only on acquisition of verbs or nouns, rather than all types of words.", "startOffset": 13, "endOffset": 117}, {"referenceID": 9, "context": "Several authors (Rooth, Riezler, Prescher, Carroll, & Beil, 1999; Collins, 1997; Ribas, 1994; Manning, 1993; Resnik, 1993; Brent, 1991) discuss the acquisition of subcategoriza-", "startOffset": 16, "endOffset": 135}, {"referenceID": 54, "context": "Several authors (Rooth, Riezler, Prescher, Carroll, & Beil, 1999; Collins, 1997; Ribas, 1994; Manning, 1993; Resnik, 1993; Brent, 1991) discuss the acquisition of subcategoriza-", "startOffset": 16, "endOffset": 135}, {"referenceID": 38, "context": "Several authors (Rooth, Riezler, Prescher, Carroll, & Beil, 1999; Collins, 1997; Ribas, 1994; Manning, 1993; Resnik, 1993; Brent, 1991) discuss the acquisition of subcategoriza-", "startOffset": 16, "endOffset": 135}, {"referenceID": 52, "context": "Several authors (Rooth, Riezler, Prescher, Carroll, & Beil, 1999; Collins, 1997; Ribas, 1994; Manning, 1993; Resnik, 1993; Brent, 1991) discuss the acquisition of subcategoriza-", "startOffset": 16, "endOffset": 135}, {"referenceID": 4, "context": "Several authors (Rooth, Riezler, Prescher, Carroll, & Beil, 1999; Collins, 1997; Ribas, 1994; Manning, 1993; Resnik, 1993; Brent, 1991) discuss the acquisition of subcategoriza-", "startOffset": 16, "endOffset": 135}, {"referenceID": 35, "context": "For example, Nenov and Dyer (1994) describe a neural network model to map between visual and verbal-motor commands, and Colunga and Gasser (1998) use neural network modeling techniques for learning spatial concepts.", "startOffset": 13, "endOffset": 35}, {"referenceID": 6, "context": "For example, Nenov and Dyer (1994) describe a neural network model to map between visual and verbal-motor commands, and Colunga and Gasser (1998) use neural network modeling techniques for learning spatial concepts.", "startOffset": 120, "endOffset": 146}, {"referenceID": 6, "context": "For example, Nenov and Dyer (1994) describe a neural network model to map between visual and verbal-motor commands, and Colunga and Gasser (1998) use neural network modeling techniques for learning spatial concepts. Feldman and his colleagues at Berkeley (Feldman, Lakoff, & Shastri, 1995) are actively pursuing cognitive models of the acquisition of semantic concepts. Another Berkeley effort, the system by Regier (1996) is given examples of pictures paired with natural language descriptions that apply to the picture, and learns to judge whether a new sentence is true of a given picture.", "startOffset": 120, "endOffset": 423}, {"referenceID": 6, "context": "For example, Nenov and Dyer (1994) describe a neural network model to map between visual and verbal-motor commands, and Colunga and Gasser (1998) use neural network modeling techniques for learning spatial concepts. Feldman and his colleagues at Berkeley (Feldman, Lakoff, & Shastri, 1995) are actively pursuing cognitive models of the acquisition of semantic concepts. Another Berkeley effort, the system by Regier (1996) is given examples of pictures paired with natural language descriptions that apply to the picture, and learns to judge whether a new sentence is true of a given picture. Similar work by Suppes, Liang, and B\u00f6ttner (1991) uses robots to demonstrate lexicon learning.", "startOffset": 120, "endOffset": 643}, {"referenceID": 6, "context": "For example, Nenov and Dyer (1994) describe a neural network model to map between visual and verbal-motor commands, and Colunga and Gasser (1998) use neural network modeling techniques for learning spatial concepts. Feldman and his colleagues at Berkeley (Feldman, Lakoff, & Shastri, 1995) are actively pursuing cognitive models of the acquisition of semantic concepts. Another Berkeley effort, the system by Regier (1996) is given examples of pictures paired with natural language descriptions that apply to the picture, and learns to judge whether a new sentence is true of a given picture. Similar work by Suppes, Liang, and B\u00f6ttner (1991) uses robots to demonstrate lexicon learning. A robot is trained on cognitive and perceptual concepts and their associated actions, and learns to execute simple commands. Along similar lines, Tishby and Gorin (1994) have a system that learns associations between words and actions, but they use a statistical framework to learn these associations, and do not handle structured representations.", "startOffset": 120, "endOffset": 858}, {"referenceID": 6, "context": "For example, Nenov and Dyer (1994) describe a neural network model to map between visual and verbal-motor commands, and Colunga and Gasser (1998) use neural network modeling techniques for learning spatial concepts. Feldman and his colleagues at Berkeley (Feldman, Lakoff, & Shastri, 1995) are actively pursuing cognitive models of the acquisition of semantic concepts. Another Berkeley effort, the system by Regier (1996) is given examples of pictures paired with natural language descriptions that apply to the picture, and learns to judge whether a new sentence is true of a given picture. Similar work by Suppes, Liang, and B\u00f6ttner (1991) uses robots to demonstrate lexicon learning. A robot is trained on cognitive and perceptual concepts and their associated actions, and learns to execute simple commands. Along similar lines, Tishby and Gorin (1994) have a system that learns associations between words and actions, but they use a statistical framework to learn these associations, and do not handle structured representations. Similarly, Oates, Eyler-Walker, and Cohen (1999) discuss the acquisition of lexical hierarchies and their associated meaning as defined by the sensory environment of a robot.", "startOffset": 120, "endOffset": 1085}, {"referenceID": 4, "context": "The problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own. While most of these methods also compute association scores between pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does.", "startOffset": 219, "endOffset": 554}, {"referenceID": 4, "context": "The problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own. While most of these methods also compute association scores between pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. Also, Yamazaki, Pazzani, and Merz (1995) learn both translation rules and semantic hierarchies from parsed parallel sentences in Japanese and English.", "startOffset": 219, "endOffset": 718}, {"referenceID": 4, "context": "The problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own. While most of these methods also compute association scores between pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. Also, Yamazaki, Pazzani, and Merz (1995) learn both translation rules and semantic hierarchies from parsed parallel sentences in Japanese and English. Of course, the main difference between this body of work and this paper is that we map words to semantic structures, not to other words. As mentioned in the introduction, there is also a large body of work on learning lexical semantics but using different problem formulations than our own. For example, Collins and Singer (1999), Riloff and Jones (1999), Roark and Charniak (1998), and Schneider (1998) define semantic lexicons as a grouping of words into semantic categories, and in the latter case, add relational information.", "startOffset": 219, "endOffset": 1158}, {"referenceID": 4, "context": "The problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own. While most of these methods also compute association scores between pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. Also, Yamazaki, Pazzani, and Merz (1995) learn both translation rules and semantic hierarchies from parsed parallel sentences in Japanese and English. Of course, the main difference between this body of work and this paper is that we map words to semantic structures, not to other words. As mentioned in the introduction, there is also a large body of work on learning lexical semantics but using different problem formulations than our own. For example, Collins and Singer (1999), Riloff and Jones (1999), Roark and Charniak (1998), and Schneider (1998) define semantic lexicons as a grouping of words into semantic categories, and in the latter case, add relational information.", "startOffset": 219, "endOffset": 1183}, {"referenceID": 4, "context": "The problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own. While most of these methods also compute association scores between pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. Also, Yamazaki, Pazzani, and Merz (1995) learn both translation rules and semantic hierarchies from parsed parallel sentences in Japanese and English. Of course, the main difference between this body of work and this paper is that we map words to semantic structures, not to other words. As mentioned in the introduction, there is also a large body of work on learning lexical semantics but using different problem formulations than our own. For example, Collins and Singer (1999), Riloff and Jones (1999), Roark and Charniak (1998), and Schneider (1998) define semantic lexicons as a grouping of words into semantic categories, and in the latter case, add relational information.", "startOffset": 219, "endOffset": 1210}, {"referenceID": 4, "context": "The problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own. While most of these methods also compute association scores between pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. Also, Yamazaki, Pazzani, and Merz (1995) learn both translation rules and semantic hierarchies from parsed parallel sentences in Japanese and English. Of course, the main difference between this body of work and this paper is that we map words to semantic structures, not to other words. As mentioned in the introduction, there is also a large body of work on learning lexical semantics but using different problem formulations than our own. For example, Collins and Singer (1999), Riloff and Jones (1999), Roark and Charniak (1998), and Schneider (1998) define semantic lexicons as a grouping of words into semantic categories, and in the latter case, add relational information.", "startOffset": 219, "endOffset": 1232}, {"referenceID": 4, "context": "The problem of automatic construction of translation lexicons (Smadja, McKeown, & Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition similar to our own. While most of these methods also compute association scores between pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. Also, Yamazaki, Pazzani, and Merz (1995) learn both translation rules and semantic hierarchies from parsed parallel sentences in Japanese and English. Of course, the main difference between this body of work and this paper is that we map words to semantic structures, not to other words. As mentioned in the introduction, there is also a large body of work on learning lexical semantics but using different problem formulations than our own. For example, Collins and Singer (1999), Riloff and Jones (1999), Roark and Charniak (1998), and Schneider (1998) define semantic lexicons as a grouping of words into semantic categories, and in the latter case, add relational information. The result is typically applied as a semantic lexicon for information extraction or entity tagging. Pedersen and Chen (1995) describe a method for acquiring syntactic and semantic features of an unknown word, assuming access to an initial concept hierarchy, but they give no experimental results.", "startOffset": 219, "endOffset": 1483}, {"referenceID": 38, "context": "tion information for verbs, and others describe work on learning selectional restrictions (Manning, 1993; Brent, 1991).", "startOffset": 90, "endOffset": 118}, {"referenceID": 4, "context": "tion information for verbs, and others describe work on learning selectional restrictions (Manning, 1993; Brent, 1991).", "startOffset": 90, "endOffset": 118}, {"referenceID": 31, "context": "Finally, several systems (Knight, 1996; Hastings, 1996; Russell, 1993) learn new words from context, assuming that a large initial lexicon and parsing system are already available.", "startOffset": 25, "endOffset": 70}, {"referenceID": 27, "context": "Finally, several systems (Knight, 1996; Hastings, 1996; Russell, 1993) learn new words from context, assuming that a large initial lexicon and parsing system are already available.", "startOffset": 25, "endOffset": 70}, {"referenceID": 58, "context": "Finally, several systems (Knight, 1996; Hastings, 1996; Russell, 1993) learn new words from context, assuming that a large initial lexicon and parsing system are already available.", "startOffset": 25, "endOffset": 70}, {"referenceID": 4, "context": "tion information for verbs, and others describe work on learning selectional restrictions (Manning, 1993; Brent, 1991). Both of these are different from the information required for mapping to semantic representation, but could be useful as a source of information to further constrain the search. Li (1998) further expands on the subcategorization work by inducing clustering information.", "startOffset": 106, "endOffset": 308}, {"referenceID": 28, "context": "Only a few of the researchers applying machine learning to natural language processing have utilized active learning (Hwa, 2001; Schohn & Cohn, 2000; Tong & Koller, 2000; Thompson et al., 1999; Argamon-Engelson & Dagan, 1999; Liere & Tadepalli, 1997; Lewis & Catlett, 1994), and the majority of these have addressed classification tasks such as part of speech tagging and text categorization.", "startOffset": 117, "endOffset": 273}, {"referenceID": 72, "context": "Only a few of the researchers applying machine learning to natural language processing have utilized active learning (Hwa, 2001; Schohn & Cohn, 2000; Tong & Koller, 2000; Thompson et al., 1999; Argamon-Engelson & Dagan, 1999; Liere & Tadepalli, 1997; Lewis & Catlett, 1994), and the majority of these have addressed classification tasks such as part of speech tagging and text categorization.", "startOffset": 117, "endOffset": 273}, {"referenceID": 6, "context": "2 Active Learning With respect to additional active learning techniques, Cohn et al. (1994) were among the first to discuss certainty-based active learning methods in detail.", "startOffset": 73, "endOffset": 92}, {"referenceID": 6, "context": "2 Active Learning With respect to additional active learning techniques, Cohn et al. (1994) were among the first to discuss certainty-based active learning methods in detail. They focus on a neural network approach to active learning in a version-space of concepts. Only a few of the researchers applying machine learning to natural language processing have utilized active learning (Hwa, 2001; Schohn & Cohn, 2000; Tong & Koller, 2000; Thompson et al., 1999; Argamon-Engelson & Dagan, 1999; Liere & Tadepalli, 1997; Lewis & Catlett, 1994), and the majority of these have addressed classification tasks such as part of speech tagging and text categorization. For example, Liere and Tadepalli (1997) apply active learning with committees to the problem of text categorization.", "startOffset": 73, "endOffset": 699}, {"referenceID": 2, "context": "Argamon-Engelson and Dagan (1999) also apply committee-based learning to part-of-speech tagging.", "startOffset": 0, "endOffset": 34}, {"referenceID": 2, "context": "Argamon-Engelson and Dagan (1999) also apply committee-based learning to part-of-speech tagging. In their work, a committee of hidden Markov models is used to select examples for annotation. Lewis and Catlett (1994) use heterogeneous certainty-based methods, in which a simple classifier is used to select examples that are then annotated and presented to a more powerful classifier.", "startOffset": 0, "endOffset": 216}, {"referenceID": 2, "context": "Argamon-Engelson and Dagan (1999) also apply committee-based learning to part-of-speech tagging. In their work, a committee of hidden Markov models is used to select examples for annotation. Lewis and Catlett (1994) use heterogeneous certainty-based methods, in which a simple classifier is used to select examples that are then annotated and presented to a more powerful classifier. However, many language learning tasks require annotating natural language text with a complex output, such as a parse tree, semantic representation, or filled template. The application of active learning to tasks requiring such complex outputs has not been well studied, the exceptions being Hwa (2001), Soderland (1999), Thompson et al.", "startOffset": 0, "endOffset": 687}, {"referenceID": 2, "context": "Argamon-Engelson and Dagan (1999) also apply committee-based learning to part-of-speech tagging. In their work, a committee of hidden Markov models is used to select examples for annotation. Lewis and Catlett (1994) use heterogeneous certainty-based methods, in which a simple classifier is used to select examples that are then annotated and presented to a more powerful classifier. However, many language learning tasks require annotating natural language text with a complex output, such as a parse tree, semantic representation, or filled template. The application of active learning to tasks requiring such complex outputs has not been well studied, the exceptions being Hwa (2001), Soderland (1999), Thompson et al.", "startOffset": 0, "endOffset": 705}, {"referenceID": 2, "context": "Argamon-Engelson and Dagan (1999) also apply committee-based learning to part-of-speech tagging. In their work, a committee of hidden Markov models is used to select examples for annotation. Lewis and Catlett (1994) use heterogeneous certainty-based methods, in which a simple classifier is used to select examples that are then annotated and presented to a more powerful classifier. However, many language learning tasks require annotating natural language text with a complex output, such as a parse tree, semantic representation, or filled template. The application of active learning to tasks requiring such complex outputs has not been well studied, the exceptions being Hwa (2001), Soderland (1999), Thompson et al. (1999). The latter two include work on active learning applied to information extraction, and Thompson et al.", "startOffset": 0, "endOffset": 729}, {"referenceID": 2, "context": "Argamon-Engelson and Dagan (1999) also apply committee-based learning to part-of-speech tagging. In their work, a committee of hidden Markov models is used to select examples for annotation. Lewis and Catlett (1994) use heterogeneous certainty-based methods, in which a simple classifier is used to select examples that are then annotated and presented to a more powerful classifier. However, many language learning tasks require annotating natural language text with a complex output, such as a parse tree, semantic representation, or filled template. The application of active learning to tasks requiring such complex outputs has not been well studied, the exceptions being Hwa (2001), Soderland (1999), Thompson et al. (1999). The latter two include work on active learning applied to information extraction, and Thompson et al. (1999) includes work on active learning for semantic parsing.", "startOffset": 0, "endOffset": 839}, {"referenceID": 2, "context": "Argamon-Engelson and Dagan (1999) also apply committee-based learning to part-of-speech tagging. In their work, a committee of hidden Markov models is used to select examples for annotation. Lewis and Catlett (1994) use heterogeneous certainty-based methods, in which a simple classifier is used to select examples that are then annotated and presented to a more powerful classifier. However, many language learning tasks require annotating natural language text with a complex output, such as a parse tree, semantic representation, or filled template. The application of active learning to tasks requiring such complex outputs has not been well studied, the exceptions being Hwa (2001), Soderland (1999), Thompson et al. (1999). The latter two include work on active learning applied to information extraction, and Thompson et al. (1999) includes work on active learning for semantic parsing. Hwa (2001) describes an interesting method for evaluating a statistical parser\u2019s uncertainty, when applied for syntactic parsing.", "startOffset": 0, "endOffset": 905}, {"referenceID": 7, "context": "One critical problem is obtaining diverse committees that properly sample the version space (Cohn et al., 1994).", "startOffset": 92, "endOffset": 111}], "year": 2017, "abstractText": "This paper focuses on a system, Wolfie (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with semantic representations. The lexicon learned consists of phrases paired with meaning representations. Wolfie is part of an integrated system that learns to transform sentences into representations such as logical database queries. Experimental results are presented demonstrating Wolfie\u2019s ability to learn useful lexicons for a database interface in four different natural languages. The usefulness of the lexicons learned by Wolfie are compared to those acquired by a similar system, with results favorable to Wolfie. A second set of experiments demonstrates Wolfie\u2019s ability to scale to larger and more difficult, albeit artificially generated, corpora. In natural language acquisition, it is difficult to gather the annotated data needed for supervised learning; however, unannotated data is fairly plentiful. Active learning methods attempt to select for annotation and training only the most informative examples, and therefore are potentially very useful in natural language applications. However, most results to date for active learning have only considered standard classification tasks. To reduce annotation effort while maintaining accuracy, we apply active learning to semantic lexicons. We show that active learning can significantly reduce the number of annotated examples required to achieve a given level of performance.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}