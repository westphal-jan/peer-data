{"id": "1112.4133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2011", "title": "Evaluation of Performance Measures for Classifiers Comparison", "abstract": "The selection nightrise of auditions the osanna best classification algorithm alamsyah for hawing a wnbf given soleiman dataset is a very widespread problem, occuring updating each stockyard time one phoenicopteriformes has to choose a raivis classifier to intracity solve a real - hochschule world unclimbed problem. 3,103 It 7.08 is also fluorite a complex stonecutters task subproject with many engelbrektsson important skitch methodological gwilt decisions to make. aetosaurs Among nuseibeh those, orenco one of c-23 the hochstetter most podia crucial o'rahilly is the choice of tepito an appropriate measure aspartic in anglo-americans order estreno to properly overtired assess pullover the classification performance kitrey and 1697 rank the algorithms. In euro392 this 77-85 article, manasieva we focus feickert on arioso this franchised specific attwood task. We present the most breon popular measures leyritz and preorders compare rosenwasser their bzura behavior through discrimination plots. We then discuss serkadji their dicer properties from a q\u016b more theoretical thoth-amon perspective. It turns out 252.2 several of participated them 54-million are ibori equivalent for jurcina classifiers 56.74 comparison purposes. recopilar Futhermore. barbel they syah can gametocytes also lead vivants to interpretation kitna problems. thornbury Among rorke the taras numerous measures proposed over the years, it lenca appears newtyle that itap the 4-gigabyte classical fuamatu overall success nbl rate 9.60 and marginal tallo rates wiredu are ikegami the more suitable temperley for congregate classifier shohat comparison x50 task.", "histories": [["v1", "Sun, 18 Dec 2011 08:02:49 GMT  (445kb)", "http://arxiv.org/abs/1112.4133v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vincent labatut", "hocine cherifi"], "accepted": false, "id": "1112.4133"}, "pdf": {"name": "1112.4133.pdf", "metadata": {"source": "CRF", "title": "EVALUATION OF PERFORMANCE MEASURES FOR CLASSIFIERS COMPARISON", "authors": ["Vincent Labatut", "Hocine Cherifi"], "emails": ["vlabatut@gsu.edu.tr", "hocine.cherifi@u-bourgogne.fr"], "sections": [{"heading": null, "text": "The selection of the best classification algorithm for a given dataset is a very widespread problem, occuring each time one has to choose a classifier to solve a real-world problem. It is also a complex task with many important methodological decisions to make. Among those, one of the most crucial is the choice of an appropriate measure in order to properly assess the classification performance and rank the algorithms. In this article, we focus on this specific task. We present the most popular measures and compare their behavior through discrimination plots. We then discuss their properties from a more theoretical perspective. It turns out several of them are equivalent for classifiers comparison purposes. Futhermore. they can also lead to interpretation problems. Among the numerous measures proposed over the years, it appears that the classical overall success rate and marginal rates are the more suitable for classifier comparison task.\nKeywords: Classification, Accuracy Measure, Classifier Comparison, Discrimination Plot."}, {"heading": "1 INTRODUCTION", "text": "The comparison of classification algorithms is a complex and open problem. First, the notion of performance can be defined in many ways: accuracy, speed, cost, readability, etc. Second, an appropriate tool is necessary to quantify this performance. Third, a consistent method must be selected to compare the measured values.\nAs performance is most of the time expressed in terms of accuracy, we focus on this point in this work. The number of accuracy measures appearing in the classification literature is extremely large. Some were specifically designed to compare classifiers , but most were initially defined for other purposes, such as measuring the association between two random variables [2], the agreement between two raters [3] or the similarity between two sets [4]. Furthermore, the same measure may have been independently developed by different authors, at different times, in different domains, for different purposes, leading to very confusing typology and terminology. Besides its purpose or name, what characterizes a measure is the definition of the concept of accuracy it relies on. Most measures are designed to focus on a specific aspect of the overall classification results [5]. This leads to measures with different interpretations, and some\ndo not even have any clear interpretation. Finally, the measures may also differ in the nature of the situations they can handle [6]. They can be designed for binary (only two classes) or multiclass (more than two classes) problems. They can be dedicated to mutually exclusive (one instance belongs to exactly one class) or overlapping classes (one instance can belong to several classes) situations. Some expect the classifier to output a discrete score (Boolean classifiers), whereas other can take advantage of the additional information conveyed by a real-valued score (probabilistic or fuzzy classifiers). One can also oppose flat (all classes on the same level) and hierarchical classification (a set of classes at a lower level constitutes a class at a higher level). Finally, some measures are sensitive to the sampling design used to retrieve the test data [7].\nMany different measures exist, but yet, there is no such thing as a perfect measure, which would be the best in every situation [8]: an appropriate measure must be chosen according to the classification context and objectives. Because of the overwhelming number of measures and of their heterogeneity, choosing the most adapted one is a difficult problem. Moreover, it is not always clear what the measures properties are, either because they were never rigorously studied, or because\nspecialists do not agree on the question (e.g. the question of chance-correction [9]). Maybe for these reasons, authors very often select an accuracy measure by relying on the tradition or consensus observed in their field. The point is then more to use the same measure than their peers rather than the most appropriate one.\nIn this work, we reduce the complexity of choosing an accuracy measure by restraining our analysis to a very specific but widespread, situation. We discuss the case where one wants to select the best classification algorithm to process a given data set [10]. An appropriate way to perform this task would be to study the data properties first, then to select a suitable classification algorithm and determine the most appropriate parameter values, and finally to use it to build the classifier. But not everyone has the statistical expertise required to perform this analytic work. Therefore, in practice, the most popular method consists in sampling a training set from the considered population, building various classifiers with different classification algorithms and/or parameters, and then comparing their performances empirically on some test sets sampled from the same population. Finally, the classifier with highest performance is selected and used on the rest of the population.\nWe will not address the question of the method used to compare performances. Instead, we will discuss the existing accuracy measures and their relevance to our specific context. We will be focusing on comparing basic classifiers, outputting discrete scores for flat mutually-exclusive classes. Throughout this paper, we will make the following assumptions linked to our context. First, as we want to discriminate some classifiers, if two measures rank them similarly, we consider these measures as equivalent, even if they do not return the exact same accuracy values. Second, since we compare some classifiers on the same dataset, the class proportions in the processed data are fixed.\nIn the next section, we review the works dealing with similar problems. We then introduce the notations used in the rest of the paper in section 3. In section 4, we review the main measures used as accuracy measures in the classification literature. In section 5, we compare them empirically, by considering some typical cases. In section 6, we introduce the notion of discrimination plot to compare and analyze the behavior of the measures. Finally, in section 7, we compare their functional properties and discuss their relevance relatively to our specific case."}, {"heading": "2 RELATED WORKS", "text": "Several previous works already compared various measures, but with different purposes or methods. In [11], Congalton described the various aspects of accuracy assessment and compared a few\nmeasures in terms of functional traits. However, the focus is rather on estimating the quality of a single classifier than on comparing several of them. In other words, the discussion concerns whether or not the value of a given measure is close to the studied classifier actual accuracy, and not on the ability of this measure to discriminate between classifiers.\nLing et al. defined the notions of consistency and discriminancy to compare measures [12]. They stated two measures are consistent if they always discriminate algorithms similarly. A measure is more discriminant than the other if it is the only one (of the two) sensitive to differences in the processed data. The authors use these concepts to compare 2 widespread measures. The notion of consistency fits the previous definition of measure equivalence we adopted in our specific context. However, Ling et al.\u2019s focus is on real-valued output scores and binary classification problems.\nIn [13], Flach compared 7 measures through the use of ROC plots. He studied how these measures behave when varying the classes relative proportions in the dataset. For this purpose, he considered the isometrics of a given measure (i.e. the zones of the ROC space for which the measure returns the same value), and investigated how changes in the class proportions affect them. He defined the equivalence of two measures in the context of classifiers comparison in a way relatively similar to Ling et al.\u2019s consistency [12]. His work also focused on binary problems.\nSokolova & Lapalme considered 24 measures, on both binary and multiclass problems (and others) [6]. They studied the sensitivity of these measures to specific changes in the classified dataset properties. Using the same general idea than Flach [13] (isometrics), they developed the notion of invariance, by identifying the changes in the confusion matrix which did not affect the measure value. Note they focused on class-specific changes. The measures were compared in terms of invariance: two measures are said to be similar if they are invariant to the same modifications. This is stricter than what we need in our context, since some modification might change the accuracy but not the algorithms relative ranking.\nIn [14], Albatineh et. al performed an analytical study of 28 accuracy measures. They considered these measures in the context of cluster analysis accuracy assessment, but the 22 confusion matrices they analyzed are similar to those obtained for binary classification problems. They showed many of the considered measures are equivalent (i.e. return the same values) when a correction for chance (cf. section 4.6) is applied. Besides the fact the authors focus on binary problems, this work also differs from ours because of the much stricter notion of equivalence: two measures can provide different values but rank classifiers similarly. Moreover, the relevance of chance correction has\nyet to be discussed in our context.\nBy opposition to the previous analytical works, a number of authors adopted an empirical approach. The general idea is to apply several classifiers to a selection of real-world data sets, and to process their accuracy through various measures. These are then compared in terms of correlation. Caruana & Niculescu-Mizil adopted this method to compare 9 accuracy measures [15], but their focus was on binary classification problems, and classifiers able to output real-valued scores (by opposition to the discrete scores we treat here). Liu et al. [16] and and Ferri et al. [17] considered 34 and 18 measures, respectively, for both binary and multiclass problems (amongst others). The main limitation with these studies is they either use data coming from a single applicative domain (such as remote sensing in [16]), or rely on a small number of datasets (7 in [15] and 30 in [17]). In both cases, this prevents a proper generalization of the obtained observations. Ferri et al. completed their empirical analysis by studying the effect of various types of noise on the measures, through randomly generated data. However their goal was more to characterize the measures sensitivity than to compare them directly."}, {"heading": "3 NOTATIONS AND TERMINOLOGY", "text": "Consider the problem of estimating k classes for a test set containing n instances. The true\nclasses are noted iC , whereas the estimated classes,\nas defined by the considered classifier, are noted\niC\u0302 ( ki1 ). The proportion of instances\nbelonging to class iC in the dataset is noted i .\nMost measures are not processed directly from the raw classifier outputs, but from the confusion matrix built from these results. This matrix represents how the instances are distributed over estimated (rows) and true (columns) classes.\ncorrespond to the proportion of instances estimated\nto be in class number i by the classifier (i.e. iC\u0302 ),\nwhen they actually belong to class number j (i.e.\njC ). Consequently, diagonal terms ( ji )\ncorrespond to correctly classified instances, whereas off-diagonal terms ( ji ) represent\nincorrectly classified ones. Note some authors invert estimated and true classes, resulting in a transposed matrix [13, 18].\nThe sums of the confusion matrix elements\nover row i and column j are noted ip and jp ,\nrespectively, so we have jjp .\nWhen considering one class i in particular, one\nmay distinguish four types of instances: true positives (TP) and false positives (FP) are instances\ncorrectly and incorrectly classified as iC\u0302 , whereas true negatives (TN) and false negatives (FN) are instances correctly and incorrectly not classified as\niC\u0302 , respectively. The corresponding proportions\nare defined as iiTP pp , iiiFN ppp ,\niiiFP ppp and FNFPTPTN pppp 1 ,\nrespectively.\nNote some authors prefer to define the confusion matrix in terms of counts rather than\nproportions, using values of the form ijij npn .\nSince using proportions is generally more convenient when expressing the accuracy measures, we prefer to use this notation in this article."}, {"heading": "4 SELECTED MEASURES", "text": "In this section, we describe formally the most widespread measures used to compare classifiers in the literature. These include association measures, various measures based on marginal rates of the confusion matrix, and chance-corrected agreement coefficients. Since these can be described according to many traits, there are as many typologies as authors. In this article, we will mainly oppose classspecific measures, i.e. those designed to assess the accuracy of a single class, and multiclass measures, able to assess the overall classifier accuracy. Note the class-specific ones generally correspond to measures defined for binary problems and applied on multiclass ones."}, {"heading": "4.1 Nominal Association Measures", "text": "A measure of association is a numerical index, a single number, which describes the strength or magnitude of a relationship. Many association measures were used to assess classification accuracy, such as: chi-square-based measures ( coefficient, Pearson\u2019s C , Cramer\u2019s V , etc. [2]), Yule\u2019s coefficients, Matthew\u2019s correlation coefficient, Proportional reduction in error measures (Goodman & Kruskal\u2019s and , Theil\u2019s\nuncertainty coefficient, etc.), mutual informationbased measures [19] and others. Association\nmeasures quantify how predictable a variable is when knowing the other one. They have been applied to classification accuracy assessment by considering these variables are defined by the distributions of instances over the true and estimated classes, respectively.\nIn our context, we consider the distribution of instances over estimated classes, and want to measure how much similar it is to their distribution over the true classes. The relationship assessed by an association measure is more general [2], since a high level of association only means it is possible to predict estimated classes when knowing the true ones (and vice-versa). In other terms, a high association does not necessary correspond to a match between estimated and true classes. For instance, if one considers a binary classification problem, both perfect classification and perfect misclassification give the same maximal association value.\nConsequently, a confusion matrix can convey both a low accuracy and a high association at the same time (as shown in Table 2), which makes association measures unsuitable for accuracy assessment."}, {"heading": "4.2 Overall Success Rate", "text": "Certainly the most popular measure for classification accuracy [20], the overall success rate is defined as the trace of the confusion matrix:\nk\ni iipOSR 1\n(1)\nThis measure is multiclass, symmetrical, and ranges from 0 (perfect misclassification) to 1 (perfect classification). Its popularity is certainly due to its simplicity, not only in terms of processing but also of interpretation, since it corresponds to the observed proportion of correctly classified instances."}, {"heading": "4.3 Marginal Rates", "text": "We gather under the term marginal rates a number of widely spread asymmetric class-specific\nmeasures. The TP Rate and TN Rate are both reference-oriented, i.e. they consider the confusion matrix columns (true classes). The former is also called sensitivity [20], producer\u2019s accuracy [11] and Dice\u2019s asymmetric index [21]. The latter is alternatively called specificity [20].\nFNTPTPi pppTPR (2)\nFPTNTNi pppTNR (3)\nThe estimation-oriented measures, which focus on the confusion matrix rows (estimated classes), are the Positive Predictive Value (PPV) and Negative Predictive Value (NPV) [20]. The former is also called precision [20], user\u2019s accuracy [11] and Dice\u2019s association index [21].\nFPTPTPi pppPPV (4)\nFNTNTNi pppNPV (5)\nTNR and PPV are related to type I error (FP) whereas TPR and NPV are related to type II error (FN). All four measures range from 0 to 1, and their interpretation is straightforward. TPR (resp. TNR) corresponds to the proportion of instances belonging (resp. not belonging) to the considered class and actually classified as such. PPV (resp. NPV) corresponds to the proportion of instances predicted to belong (resp. not to belong) to the considered class, and which indeed do (resp. do not).\nFinally, note some authors use the complements of these measures. For instance, the\nFalse Positive Rate ii TNRFPR 1 is also called\nfallout [22] or false alarm rate [23], and is notably used to build ROC curves [20]."}, {"heading": "4.4 F-measure and Jaccard Coefficient", "text": "The F-measure corresponds to the harmonic mean of PPV and TPR [20], therefore it is classspecific and symmetric. It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:\nFPFNTP\nTP\nii\nii i\nppp\np\nTPRPPV\nTPRPPV F\n2\n2 2 (6)\nIt can be interpreted as a measure of overlapping between the true and estimated classes (other instances, i.e. TN, are ignored), ranging from 0 (no overlap at all to 1 (complete overlap).\nThe measure known as Jaccard\u2019s coefficient of\ncommunity was initially defined to compare sets [4], too. It is a class-specific symmetric measure defined as:\nFNFPTPTPi ppppJCC (7)\nIt is alternatively called Short\u2019s measure [26]. For a given class, it can be interpreted as the ratio of the estimated and true classes intersection to their union (in terms of set cardinality). It ranges from 0 (no overlap) to 1 (complete overlap). It is\nrelated to the F-measure [27]: iii FFJCC 2 ,\nwhich is why we describe it in the same section."}, {"heading": "4.5 Classification Success Index", "text": "The Individual Classification Success Index (ICSI), is a class-specific symmetric measure defined for classification assessment purpose [1]:\n1\n111\nii\niii\nTPRPPV\nTPRPPVICSI (8)\nThe terms iPPV1 and iTPR1 correspond\nto the proportions of type I and II errors for the considered class, respectively. ICSI is hence one minus the sum of these errors. It ranges from \u20131 (both errors are maximal, i.e. 1) to 1 (both errors are minimal, i.e. 0), but the value 0 does not have any clear meaning. The measure is symmetric, and linearly related to the arithmetic mean of TPR and PPV, which is itself called Kulczynski\u2019s measure [28].\nThe Classification Success Index (CSI) is an overall measure defined simply by averaging ICSI over all classes [1]."}, {"heading": "4.6 Agreement Coefficients", "text": "A family of chance-corrected inter-rater agreement coefficients has been widely used in the context of classification accuracy assessment. It relies on the following general formula:\neeo PPPA 1 (9)\nWhere oP and eP are the observed and\nexpected agreements, respectively. The idea is to consider the observed agreement as the result of an intended agreement and a chance agreement. In order to get the intended agreement, one must estimate the chance agreement and remove it from the observed one.\nMost authors use OSRPo , but disagree on\nhow the chance agreement should be formally\ndefined, leading to different estimations of eP . For\nhis popular kappa coefficient (CKC), Cohen used the product of the confusion matrix marginal\nproportions [3]:\ni\niie ppP . Scott\u2019s pi\ncoefficient (SPC) relies instead on the class proportions measured on the whole data set (or its\nestimation), noted ip [29]:\n2\ni\nie pP . Various\nauthors, including Maxwell for his Random Error (MRE) [30], made the assumption classes are\nevenly distributed: kPe 1 .\nThe problems of assessing inter-rater agreement and classifier accuracy are slightly different though. Indeed, in the former, the true class distribution is unknown, whereas in the latter it is completely known. Both raters are considered as equivalent and interchangeable, in the sense they are both trying to estimate the true classes. On the contrary, in our case, the classes estimated by the classifier are evaluated relatively to the true classes. The correction for chance strategies presented above are defined in function of this specific trait of the inter-rater agreement problem. They might not be relevant in our situation."}, {"heading": "4.7 Ground Truth Index", "text": "T\u00fcrk\u2019s Ground Truth Index (GTI) is another chance-corrected measure, but this one was defined specially for classification accuracy assessment [18]. T\u00fcrk supposes the classifier has two components: one is always correct, and the other\nclassify randomly. For a given class jC , a\nproportion j of the instances are supposed to be\nclassified by the infallible classifier, and therefore\nput in jC\u0302 . The remaining instances (i.e. a\nproportion jjb 1 ) are distributed by the\nrandom classifier over all estimated classes\n(including jC\u0302 ) with a probability ia for iC\u0302 . In\nother words, according to this model, each offdiagonal term of the confusion matrix can be\nwritten as a product of the form jiij bap ( ji ).\nThis corresponds to the hypothesis of quasiindependence of non-diagonals of Goodman, whose iterative proportional fitting method allows\nestimating ia and jb .\nT\u00fcrk based his GTI on the general formula\nof Eq. (9), but unlike the previously presented\nagreement coefficients, he uses io TPRP\nand ie aP . He therefore designs a class-specific\nmeasure, corresponding to a chance-corrected version of the TPR. It is interpreted as the proportion of instances the classifier will always classify correctly, even when processing other data. The way this measure handles chance correction is more adapted to classification than the agreement coefficients [27]. However, it has several limitations regarding the processed data: it cannot be used with less than three classes, or on perfectly classified data, and most of all it relies on the quasiindependence hypothesis. This condition is\nextremely rarely met in practice (e.g. less than 10% of the real-world cases considered in [16]). For this reason we will not retain the GT index in our study.\nFinally, Table 3 displays the measures selected to be studied more thoroughly in the rest of this article, with their main properties"}, {"heading": "5 CASE STUDIES", "text": "In this section, we discuss the results obtained on a few confusion matrices in order to analyze the properties and behavior of the measures reviewed in the previous section. We first consider extreme cases, i.e. perfect classification and misclassification. Then study a more realistic confusion matrix including a few classification errors."}, {"heading": "5.1 Extreme Cases", "text": "All measures agree to consider diagonal confusion matrices such as the one presented in Table 4 as the result of a perfect classification. In this case, the classifier assigns each instance to its true class and the measure reaches its maximal value.\nA perfect misclassification corresponds to a matrix whose trace is zero, as shown in Tables 2 and 5. In this case, the measures diverge. Their behavior depends on the way they consider the distribution of errors over the off-diagonal cells. OSR is not sensitive to this distribution since only the trace of the confusion matrix is considered. TPR, PPV, JCC and F-measure are not concerned neither,\nsince having no TP automatically causes these measure to have a zero value. CSI consequently always reach its minimal value too, since it depends directly on TPR and PPV, and so does ICSI.\nThe chance-corrected measures are affected according to the model of random agreement they are based upon. For the misclassification case depicted by Table 2, all of them have the same value 0.5. But for the misclassification case observed in Table 5, we obtain the following values: 43.0CKC , 61.0SPC and\n50.0MRE . This is to compare with the previously cited measures, which do not discriminate these two cases of perfect misclassification.\nTNR and NPV react differently to the perfect misclassification case. Indeed, they are both related to the number of TN, which might not be zero even in case of perfect misclassification. In other words, provided each class is represented in the considered dataset, these measures cannot reach their minimal value for all classes in case of perfect misclassification. For instance, in Table 5\n6.01TNR because 60% of the non- 1C instances\nare not classified as 1C\u0302 ."}, {"heading": "5.2 Intermediate Cases", "text": "Let us consider the confusion matrix displayed in Table 6, whose associated accuracy values are given in Table 7. We focus on the marginal rates first, beginning with the first class. The very high\nTPR indicates the classifier is good at classifying\ninstances belonging to 1C (91% of them are placed\nin 1C\u0302 ), but not as much for instances belonging to\nother classes (lower TNR: only 79% of the non1C\ninstances are not put in 1C\u0302 ). The predictive rates address the quality of the estimated classes, showing the classifier estimation is reliable for\nclasses other than 1C\u0302 (high NPV, 95% of the non-\n1C\u0302 instances actually do not belong to 1C ), but not\nas much for 1C\u0302 (lower PPV, only 68% of the\ninstances in 1C\u0302 actually belong to 1C ).\nThe third class is interesting, because the other values in its column are the same as for the first class, whereas those on its row are better (i.e. smaller). The first observation explains why its TPR and NPV are similar to those of the first class, and the second why its TNR and PPV are higher. In other words, the classifier is better as classifying\ninstances not belonging to 3C (higher TNR, 94% of\nthe non3C instances are not put in 3C\u0302 ) and the\nestimated class 3C\u0302 is much more reliable (higher\nPPV, 88% of the instances in 3C\u0302 actually belong to\n3C ).\nFinally, let us consider the second class, which is clearly the weakest for this classifier. The low TPR indicates the classifier has trouble recognizing\nall instances from 2C (only 56% are correctly classified). However, the other measures are\nrelatively high: it manages not putting in 2C\u0302 95%\nof the non2C instances (TNR), and its estimations\nfor 2C\u0302 and the other classes are reliable: 86% of the\ninstances put in 2C\u0302 actually belong to 2C (PPV)\nand 81% of the instances not put in 2C\u0302 actually do\nnot belong to 2C (NPV).\nThe other class-specific measure (F-measure, JCC and CSI) corroborates the conclusions drawn for the marginal rates. They indicate the classifier is better on the third, then first, and second classes. Intuitively, one can deduce from these comments the classifier confuses some of the second class instances and incorrectly put them in the first one.\nNow suppose we want to consider the performance of another classifier on the same data: then only the repartitions of instances in each column can change (the repartitions cannot change along the rows, since these depend on the dataset).\nLet us assume the new classifier is perfect on 1C\nand 3C , but distributes 2C instances uniformly in\n1C\u0302 , 2C\u0302 and 3C\u0302 , as shown in Table 8. We obtain the following values for the multiclass measures: 78.0OSR , 62.0ICSI , CKP , SPC ,\n67.0MRE . Interestingly, all multiclass measures consider the first classifier as better, except ICSI. This is due to the fact the average decrease in TPR observed for the second classifier relatively to the first is compensated by the PPV increase. This illustrates the fact all measures do not necessarily rank the classifiers similarly. Note that from the results reported in Table 7 one could have thought the contrary."}, {"heading": "6 SENSITIVITY TO MATRIX CHANGES", "text": "Since measures possibly discriminate classifiers differently, we now focus on the nature of this disagreement. We study three points likely\nto affect the accuracy measures: the classifier distribution of error, the dataset class proportions and the number of classes."}, {"heading": "6.1 Methods", "text": "We first give an overview of our methods, and then focus on its different steps. We generate two series of matrices with various error distribution and fixed class proportions. We compute the accuracy according to every measure under study. For a given measure we then consider all possible pairs of matrices obtained by associating one matrix from the first series to one of the second series. For each pair we compute the difference between the two values of the measure. The line corresponding to a zero difference separates the plane between pairs for which the first matrix is preferred by the measure, and pairs for which the second one is. We call this line the discrimination line.\nOur approach is related to the isometrics concept described in [13]. The main difference is our discrimination lines are function of the error distribution, while ROC curves uses TPR and FPR. Moreover, we focus on a single isometrics: the one associated with a zero difference. This allows us to represent several discrimination lines on the same plots. By repeating the same process for different class proportions, or number of classes, we can therefore study if and how the discrimination lines are affected by these parameters.\nWe now focus on the modeling of classification error distribution. Let us consider a confusion matrix corresponding to a perfect classification, as presented in Table 4. Applying a classifier with lower performance on the same dataset will lead to a matrix diverging only in the distribution of instances in columns taken independently. Indeed, since the dataset is fixed, the class proportions, and hence the distributions inside rows, cannot change\n(i.e. the i are constant).\nFor simplicity purposes, we suppose the\nmisclassified instances for some class iC are\nuniformly distributed by the classifier on the other\nestimated classes ijC\u0302 . In other words, the perfect\nclassifier correctly puts a proportion i of the\ndataset instances in iC\u0302 and none in ijC\u0302 , whereas\nour imperfect classifier correctly process only a\nproportion iic ( 10 ic ) and incorrectly puts a\nproportion ij\ni\nk\nc\n1\n1 in each other class\nijC\u0302 ,\nwhere ic1 is the accuracy drop for this class.\nThis allows us to control the error level in the confusion matrix, a perfect classification\ncorresponding to 1ic for all classes. Table 9\nrepresents the confusion matrix obtained for a k - class problem in the case of a classifier undergoing an accuracy drop in all classes.\nBy using a range of values in 1;0 for c , we\ncan generate a series of matrices with decreasing error level. However, comparing pairs of matrices from the same series is fruitless, since it will lead by construction to the same discrimination lines for all measures, when we want to study their differences. We therefore considered two different series: in the first (represented on the x axis), the same accuracy drop c is applied to all classes,\nwhereas in the second ( y axis), it is applied only to\nthe first class. In Table 9, the first series\ncorresponds to cci ( i ), and the second to\ncc1 and 12ic . We thus expect the accuracy measures to favor the second series, since only its first class is subject to classification errors..\nTo investigate the sensitivity of the measures to\ndifferent class proportions values (i.e. i ), we\ngenerated several pairs of series with controlled class imbalance. In the balanced case, each class\nrepresents a proportion ki 1 of the instances.\nWe define the completely imbalanced case by defining the 1st class as having twice the number of instances in the 2nd one, which has itself twice the size of the 3rd one, and so on. In other words,\n122 kiki , where the denominator\ncorresponds to the quantity 1\n0\n2 k m m and allows the\ni summing to unity. To control the amount of\nvariation in the class proportion between the balanced and imbalanced cases, we use a multiplicative coefficient p ( 10 p ). Finally,\nthe class proportions are defined as:\n1221 kiki pkpp (10)\nThe classes are perfectly balanced for 0p\nand they become more and more imbalanced as p\nincreases. For instance, a fully imbalanced 5-class datasets will have the following proportions: 0.52, 0.26, 0.13, 0.06 and 0.03, from the 1st to 5th classes, respectively. For each measure, we are now able to plot a discrimination line for each considered value of p . This allows us not only to compare several\nmeasures for a given p value but also the different\ndiscrimination lines of a single measure as a function of p ."}, {"heading": "6.2 Error Distribution Sensitivity", "text": "We generated matrices for 3 balanced classes ( 0p ) using the methodology described above.\nFig. 1 and 2 show the discrimination lines for classspecific and multiclass measures, respectively. Except for TPR, all discrimination lines are located under the xy line. So, as expected, the measures\nfavor the case where the errors are uniformly distributed in one class ( y series) against the case\nwhere the errors affect all the classes ( x series).\nClass 2\n0 .0\n0 .2\n0 .4\n0 .6\n0 .8\n1 .0\nClass 1\n0 .0\n0 .2\n0 .4\n0 .6\n0 .8\n1 .0\n0.0 0.2 0.4 0.6 0.8 1.0\nFigure 1: Discrimination lines of all class-specific measures for classes 1 (top) and 2 (bottom), for 3\nbalanced class ( 0p , 3k ).\nFor class-specific measures, we considered first class 1, which is affected by error distribution changes in both series. The discrimination lines are clearly different for all measures. TPR is affected by changes in the distribution of instances only inside the column associated to the considered class. In the case of the first class, these columns are similar on both axes: this explains the xy\ndiscrimination line. The F-measure additionally integrates the PPV value. This explains why it favors the y series matrices. Indeed, the PPV is\nalways greater (or equal) for this series due to the fact errors are present in the first class only. The discrimination line of JCC is exactly similar. NPV does not consider TP, so it is constant for the y\nseries, whereas it decreases for the x series. This is\ndue to the fact more and more errors are added to classes 2 and 3 in these matrices when p increases.\nThis explains why matrices of the y series are\nlargely favored by this measure. PPV and TNR are represented as a vertical line on the extreme right of the plot. According to these measures, the y series\nmatrices are always more accurate. This is due to the fact both measures decrease when the error\nlevel increases for the x series ( TNp decreases,\nFPp increases) whereas TNR is constant and PPV\ndecreases less for the y series. Finally, ICSI, which\nis a linear combination of PPV and TPR, lies in between those measures.\nmeasures, with 0p and 3k .\nThe two other classes undergo similar changes,\nso we only report the results for class 2. Unlike class 1, both classes 2 and 3 are affected by errors only in the x series matrices. Consequently, all\nmeasures clearly favor the y series matrices, even\nmore than for class 1. The discrimination lines for NPV and TPR take the form of a vertical line on the right of the plot. This is due to the fact both measures decrease only for x series matrices\n(because of the increase in FNp and TPp ). The Fmeasure and JCC are still not discernable. TNR and PPV favor the y series less than the other measures.\nThis is due to the fact that on the one hand TPp decreases only for the x series matrices, but on the\nother hand FPp and TNp decrease for both series.\nFinally, ICSI still lies in between PPV and TPR.\nExcept for CSI the discrimination lines of multiclass measures are identical. We can conclude that for balanced classes ( 0p ) these measures\nare equivalent. CSI is more sensitive to the type of error we introduced. Indeed it clearly favors the y\nseries more than the other measures.\nclasses 1 (top) and 3 (bottom), with 3k ."}, {"heading": "6.3 Class Proportions Distribution Sensitivity", "text": "We now study the sensitivity of the on measures on variation in the class proportions. Roughly speaking we observe two different type of behaviors: measures are either sensitive or insensitive to variations in the class proportions distribution. In the first case, the discrimination lines for the different values of p are identical. In\nthe second case, increasing the imbalance leads to lines located on the left of the 0p line. The\nstronger the imbalance and the more the line is located on the left. This can be explained by the fact the more imbalanced the classes and the more similar the two series of matrices become, dragging the discrimination line closer to the xy line. Fig.\n3 is a typical example of this behavior. It represents the results obtained for the F-measure applied to classes 1 and 3. Note that, like before, JCC and the F-measure have similar discrimination lines.\nCKC (bottom), with 3k .\nOther than the F-measure, measures combining two marginal rates (ICSI, JCC) are sensitive to class proportions changes for all classes. This is not\nthe case for simple marginal rates. TPR and NPV are not sensitive at all for any classes. TNR and PPV present the behavior of measures sensitive to this parameter, but only for classes 2 and 3. As mentioned before, by construction of the considered matrices (the y series has errors only in class 1)\nthey are always higher for the y than the x series,\nindependently of the class proportions. Fig. 4 represents results obtained for the multiclass measures. As previously observed in the balanced case ( 0p ), OSR, SPC and MRE share the same\ndiscrimination lines, and this independently of p .\nCKC was matching them for 0p , but this is no\nmore the case for imbalanced classes. The plot for CSI (not represented here) is similar but with tighter discrimination lines, indicating it is less sensitive to proportion changes.\n(top) and 1p (bottom), with 10;3k ."}, {"heading": "6.4 Class Number Sensitivity", "text": "We finally focus on the effect of the number of classes on the multiclass measures. Fig. 5 shows the results for OSR applied on matrices with size\nranging from 3 to 10, for balanced ( 0p ) and\nimbalanced ( 1p ) cases. All the measures follow\nthe same behavior. Increasing the number of classes strengthens the preference towards the y series\nmatrices. In other words, having more classes gives more importance to the additional errors contained in the x series matrices. The effect is stronger on\nthe imbalanced matrices. In this case, most of the instances are in the first class, which is the only one similar between the two models, so its dilution has a stronger impact on the measured accuracy."}, {"heading": "7 DISCUSSION", "text": "As shown in the previous sections, measures differ in the way they discriminate different classifiers. However, besides this important aspect, they must also be compared according to several more theoretical traits."}, {"heading": "7.1 Class Focus", "text": "As illustrated in the previous sections, a measure can assess the accuracy for a specific class or over all classes. The former is adapted to situations where one is interested in a given class, or wants to conduct a class-by-class analysis of the classification results.\nIt is possible to define an overall measure by combining class-specific values measured for all classes, for example by averaging them, like in CSI. However, even if the considered class-specific measure has a clear meaning, it is difficult to give a straightforward interpretation to the resulting overall measure, other than in terms of combination of the class-specific values. Inversely, it is possible to use an overall measure to assess a given class accuracy, by merging all classes except the considered one [2]. In this case, the interpretation is straightforward though, and depends directly on the overall measure.\nOne generally uses a class-specific measure in order to distinguish classes in terms of importance. This is not possible with most basic overall measures, because they consider all classes to be equally important. Certain more sophisticated measures allow associating a weight to each class, though [7]. However, a more flexible method makes this built-in feature redundant. It consists in associating a weight to each cell in the confusion matrix, and then using a regular (unweighted) overall measure [27]. This method allows distinguishing, in terms of importance, not only classes, but also any possible case of classification error."}, {"heading": "7.2 Functional Relationships", "text": "It is interesting to notice that various combinations of two quantities can be sorted by increasing order, independently from the considered\nquantities: minimum, harmonic mean, geometric mean, arithmetic mean, quadratic mean, maximum\n[32]. If the quantities belong to 1;0 , we can even\nput their product at the beginning of the previous list, as the smallest combination. If we consider the presented measures, this means combinations of the same marginal rates have a predefined order for a given classifier. For instance, the sensitivityprecision product will always be smaller than the Fmeasure (harmonic mean), which in turn will always be smaller than Kulczynski\u2019s measure (arithmetic mean). Besides these combinations of TPR and PPV, this also holds for various measures corresponding to combinations of TPR and TNR, not presented here because they are not very popular [20, 33].\nMore importantly, some of the measures we presented are monotonically related, and this property takes a particular importance in our situation. Indeed, our goal is to sort classifiers depending on their performance on a given data set. If two measures are monotonically related, then the order will be the same for both measures. This makes the F-measure and Jaccard\u2019s coefficient similar for classifier comparison, and so are the ICSI and Kulczynski\u2019s measure, and of course all measures defined as complements of other measures, such as the FNR. This confirms some of our observations from the previous section: it explains the systematic matching between JCC and the F-measure discrimination lines."}, {"heading": "7.3 Range", "text": "In the classification context, one can consider two extreme situations: perfect classification (i.e. diagonal confusion matrix) and perfect misclassification (i.e. all diagonal elements are zeros). The former should be associated to the upper bound of the accuracy measure, and the latter to its lower bound.\nMeasure bounds can either be fixed or depend on the processed data. The former is generally considered as a favorable trait , because it allows comparing values measured on different data sets without having to normalize them for scale matters. Moreover, having fixed bounds makes it easier to give an absolute interpretation of the measured features.\nIn our case, we want to compare classifiers evaluated on the same data. Furthermore, we are interested in their relative accuracies, i.e. we focus only on their relative differences. Consequently, this trait is not necessary. But it turns out most authors normalized their measures in order to give\nthem fixed bounds (usually 1;1 or 1;0 ). Note\ntheir exact values are of little importance, since any measure defined on a given interval can easily be rescaled to fit another one. Thus, several supposedly different measures are actually the same,\nbut transposed to different scales [34]."}, {"heading": "7.4 Interpretation", "text": "Our goal is to compare classifiers on a given dataset, for which all we need is the measured accuracies. In other words, numerical values are enough to assess which classifier is the best on the considered data. But identifying the best classifier is useless if we do not know the criteria underlying this discrimination, i.e. if we are not able to interpret the measure. For instance, being the best in terms of PPV or TPR has a totally different meaning, since these measures focus on type I and II errors, respectively.\nAmong the measures used in the literature to assess classifiers accuracy, some have been designed analytically, in order to have a clear interpretation (e.g. Jaccard\u2019s coefficient [4]). Sometimes, this interpretation is questioned, or different alternatives exist, leading to several related measures (e.g. agreement coefficients). In some other cases, the measure is an ad hoc construct, which can be justified by practical constraints or observation, but may lack an actual interpretation (e.g. CSI). Finally, some measures are heterogeneous mixes of other measures, and have no direct meaning (e.g. the combination of OSR and marginal rates described in [35]). They can only be interpreted in terms of the measures forming them, and this is generally considered to be a difficult task."}, {"heading": "7.5 Correction for Chance", "text": "Correcting measures for chance is still an open debate. First, authors disagree on the necessity of this correction, depending on the application context [7, 27]. In our case, we want to generalize the accuracy measured on a sample to the whole population. In other terms, we want to distinguish the proportion of success the algorithm will be able to reproduce on different data from the lucky guesses made on the testing sample, so this correction seems necessary.\nSecond, authors disagree on the nature of the correction term, as illustrated in our description of agreement coefficients. We can distinguish two kinds of corrections: those depending only on the true class distribution (e.g. Scott\u2019s and Maxwell\u2019s) and those depending also on the estimated class distribution (e.g. Cohen\u2019s and T\u00fcrk\u2019s). The former is of little practical interest for us, because such a measure is linearly related to the OSR (the correction value being the same for every tested algorithm), and would therefore lead to the same ordering of algorithms. This explains the systematic matching observed between the discrimination lines of these measures in the previous section. The latter correction is more relevant, but there is still concern regarding how chance should be modeled. Indeed, lucky guesses depend completely on the algorithm\nbehind the considered classifier. In other words, a very specific model would have to be designed for each algorithm in order to efficiently account for chance, which seems difficult or even impossible."}, {"heading": "8 CONCLUSION", "text": "In this work, we reviewed the main measures used for accuracy assessment, from a specific classification perspective. We consider the case where one wants to compare different classification algorithms by testing them on a given data sample, in order to determine which one will be the best on the sampled population.\nWe first reviewed and described the most widespread measures, and introduced the notion of discrimination plot to compare their behavior in the context of our specific situation. We considered three factors: changes in the error level, in the class proportions, and in the number of classes. As expected, most measures have a proper way to handle the error factor, although some similarities exist between some of them. The effect of the other factors is more homogeneous: decreasing the number of classes and/or increasing their imbalance tend to lower the importance of the error level for all measures.\nWe then compared the measure from a more theoretical point of view. In the situation studied here, it turns out several traits of the measures are not relevant to discriminate them. First, all monotonically related measures are similar to us, because they all lead to the same ordering of algorithms. This notably discards a type of chance correction. Second, their range is of little importance, because we are considering relative values. Moreover, a whole subset of measures associating weights to classes can be discarded, because a simpler method allows distinguishing classes in terms of importance while using an unweighted multiclass measure. Concerning chance-correction, it appears it is needed for our purpose; however no existing estimation for chance seems relevant. Finally, complex measures based on the combination of other measures are difficult or impossible to interpret correctly.\nUnder these conditions, we advise the user to choose the simplest measures, whose interpretation is straightforward. For overall accuracy assessment, the OSR seems to be the most adapted. If the focus has to be made on a specific class, we recommend using both the TPR and PPV, or a meaningful combination such as the F-measure. A weight matrix can be used to specify differences between classes or errors.\nWe plan to complete this work by focusing on the slightly different case of classifiers with realvalued output. This property allows using additional measures such as the area under the ROC curve and various error measures [20]."}, {"heading": "9 REFERENCES", "text": "[1] S. Koukoulas and G. A. Blackburn: Introducing\nnew indices for accuracy evaluation of classified images representing semi-natural woodland environments, Photogramm Eng Rem S, vol. 67, pp. 499-510 (2001). [2] L. A. Goodman and W. H. Kruskal: Measures\nof Association for Cross Classification, J Am Stat Assoc, vol. 49, pp. 732-64 (1954). [3] J. Cohen: A Coefficient of Agreement for\nNominal Scales, Educ Psychol Meas, vol. 20, pp. 37-46 (1960). [4] P. Jaccard: The distribution of the flora in the\nalpine zone, New Phytol, vol. 11, pp. 37-50 (1912). [5] G. M. Foody: Status of land cover classification\naccuracy assessment, Remote Sens Environ, vol. 80, pp. 185-201 (2002). [6] M. Sokolova and G. Lapalme: A systematic\nanalysis of performance measures for classification tasks, Information Processing &amp; Management, vol. 45, pp. 427-437 (2009). [7] S. V. Stehman: Comparing thematic maps\nbased on map value, Int J Remote Sens, vol. 20, pp. 2347-2366 (1999). [8] S. V. Stehman: Selecting and interpreting\nmeasures of thematic classification accuracy, Remote Sens Environ, vol. 62, pp. 77-89 (1997). [9] I. Guggenmoos-Holzmann: How Reliable Are\nChance-Corrected Measures of Agreement, Stat Med, vol. 12, pp. 2191-2205 (1993). [10] V. Labatut and H. Cherifi: Accuracy Measures\nfor the Comparison of Classifiers, in International Conference on Information Technology Amman, JO (2011). [11] R. G. Congalton: A Review of Assessing the\nAccuracy of Classifications of Remotely Sensed Data, Remote Sens Environ, vol. 37, pp. 35-46 (1991).\n[12] C. X. Ling, J. Huang, and H. Zhang: AUC: a\nstatistically consistent and more discriminating measure than accuracy, in 18th International Conference on Artificial Intelligence (2003). [13] P. A. Flach: The geometry of ROC space:\nunderstanding machine learning metrics through ROC isometrics, in Twentieth International Conference on Machine Learning (ICML) Washington DC (2003). [14] A. N. Albatineh, M. Niewiadomska-Bugaj, and\nD. Mihalko: On Similarity Indices and Correction for Chance Agreement J Classif, vol. 23, pp. 301-313 (2006).\n[15] R. Caruana and A. Niculescu-Mizil: Data\nMining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria, in International Conference on\nKnowledge Discovery and Data Mining Seattle, US-WA (2004).\n[16] C. R. Liu, P. Frazier, and L. Kumar:\nComparative assessment of the measures of thematic classification accuracy, Remote Sens Environ, vol. 107, pp. 606-616 (2007). [17] C. Ferri, J. Hern\u00e1ndez-Orallo, and R. Modroiu:\nAn experimental comparison of performance measures for classification, Pattern Recognition Letters, vol. 30, pp. 27-38 (2009). [18] G. T\u00fcrk: GT index: A measure of the success\nof predictions, Remote Sens Environ, vol. 8, p. 65\u221275 (1979). [19] J. T. Finn: Use of the average mutual\ninformation index in evaluating classification error and consistency, Int J Geogr Inf Syst, vol. 7, p. 349\u2212366 (1993). [20] I. H. Witten and E. Frank: Data Mining:\nPractical Machine Learning Tools and Techniques, 2nd ed.: Morgan Kaufmann (2005).\n[21] L. R. Dice: Measures of the amount of ecologic\nassociation between species, Ecology, vol. 26, pp. 297-302 (1945). [22] P. Villegas, E. Bru, B. Mayayo, L. Carpio, E.\nAlonso, and V. J. Ruiz: Visual scene classification for image and video home content, in International Workshop on ContentBased Multimedia Indexing (CBMI), pp. 77-84 (2008). [23] J.-P. Linnartz, T. Kalker, and G. Depovere:\nModelling the False Alarm and Missed Detection Rate for Electronic Watermarks, Lecture Notes in Computer Science, vol. 1525/1998, pp. 329-343 (1998). [24] T. S\u00f8rensen: A method of establishing groups\nof equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons, Biologiske Skrifter / Kongelige Danske Videnskabernes Selskab, vol. 5, pp. 1-34 (1948). [25] U. Hellden: A test of landsat-2 imagery and\ndigital data for thematic mapping illustrated by\nan environmental study in northern Kenya, Lund Univ. Nat. Geog. Inst, Lund, Sweden 47 (1980). [26] N. M. Short: The landsat tutorical workbook\u2014\nBasics of satellite remote sensing, Goddard Space Flight Center, Greenbelt, MD NASA ref. pub. 1078 (1982). [27] G. T\u00fcrk: Map Evaluation and \u2015Chance\nCorrection\u2016, Photogramm Eng Rem S, vol. 68, pp. 123-129 (2002). [28] S. Kulczynski: Die Pflanzenassociationen der\nPienenen, Bulletin International de L\u2019Acad\u00e9mie Polonaise des Sciences et des lettres, Classe des sciences math\u00e9matiques et naturelles, S\u00e9rie B, Suppl\u00e9ment II, vol. 2, pp. 57-203 (1927). [29] W. A. Scott: Reliability of Content Analysis:\nThe Case of Nominal Scale Coding, Public Opin Quart, vol. 19, pp. 321-325 (1955). [30] A. E. Maxwell: Coefficients of agreement\nbetween observers and their interpretation, British Journal of Psychiatry, vol. 130, pp. 79- 83 (1977). [31] L. A. Goodman: The analysis of cross-\nclassified data: independence, quasiindependence, and interaction in contingency tables with and without missing entries, J Am Stat Assoc, vol. 63, pp. 1091-1131 (1968). [32] P. S. Bullen: Handbook of Means and Their\nInequalities, 2nd ed. Dordrecht, NL: Kluwer (2003). [33] D. V. Cicchetti and A. R. Feinstein: High\nagreement but low kappa: II. Resolving the paradoxes, J Clin Epidemiol, vol. 43, pp. 551-8 (1990). [34] F. E. Zegers and J. M. F. ten Berge: A Family\nof Association Coefficients for Metric Scales, Psychometrika, vol. 50, pp. 17-24 (1985). [35] T. Fung and E. LeDrew: The determination of\noptimal threshold levels for change detection using various accuracy indices, Photogramm Eng Rem S, vol. 54, p. 1449\u22121454 (1988)."}], "references": [{"title": "Introducing new indices for accuracy evaluation of classified images representing semi-natural woodland environments", "author": ["S. Koukoulas", "G.A. Blackburn"], "venue": "Photogramm Eng Rem S,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Measures of Association for Cross Classification", "author": ["L.A. Goodman", "W.H. Kruskal"], "venue": "J Am Stat Assoc,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1954}, {"title": "A Coefficient of Agreement for Nominal Scales", "author": ["J. Cohen"], "venue": "Educ Psychol Meas,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1960}, {"title": "The distribution of the flora in the alpine zone", "author": ["P. Jaccard"], "venue": "New Phytol,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1912}, {"title": "Status of land cover classification accuracy assessment", "author": ["G.M. Foody"], "venue": "Remote Sens Environ,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "A systematic analysis of performance measures for classification tasks, Information", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Processing  Management,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Comparing thematic maps based on map value", "author": ["S.V. Stehman"], "venue": "Int J Remote Sens,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Selecting and interpreting measures of thematic classification accuracy", "author": ["S.V. Stehman"], "venue": "Remote Sens Environ,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "How Reliable Are Chance-Corrected Measures of Agreement", "author": ["I. Guggenmoos-Holzmann"], "venue": "Stat Med,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Accuracy Measures for the Comparison of Classifiers, in International Conference on Information Technology", "author": ["V. Labatut", "H. Cherifi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Congalton: A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data", "author": ["G. R"], "venue": "Remote Sens Environ,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "AUC: a statistically consistent and more discriminating measure than accuracy", "author": ["C.X. Ling", "J. Huang", "H. Zhang"], "venue": "in 18th International Conference on Artificial Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "The geometry of ROC space: understanding machine learning metrics through ROC isometrics", "author": ["P.A. Flach"], "venue": "Twentieth International Conference on Machine Learning (ICML) Washington DC", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "On Similarity Indices and Correction for Chance Agreement", "author": ["A.N. Albatineh", "M. Niewiadomska-Bugaj", "D. Mihalko"], "venue": "J Classif,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Data Mining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria, in International Conference on  Knowledge Discovery and Data Mining Seattle, US-WA", "author": ["R. Caruana", "A. Niculescu-Mizil"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Comparative assessment of the measures of thematic classification accuracy", "author": ["C.R. Liu", "P. Frazier", "L. Kumar"], "venue": "Remote Sens Environ,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Modroiu: An experimental comparison of performance measures for classification", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo"], "venue": "Pattern Recognition Letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "index: A measure of the success of predictions", "author": ["GT G. T\u00fcrk"], "venue": "Remote Sens Environ,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1979}, {"title": "Use of the average mutual information index in evaluating classification error and consistency", "author": ["J.T. Finn"], "venue": "Int J Geogr Inf Syst,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Data Mining: Practical Machine Learning", "author": ["I.H. Witten", "E. Frank"], "venue": "Tools and Techniques,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Dice: Measures of the amount of ecologic association between species, Ecology", "author": ["R. L"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1945}, {"title": "Visual scene classification for image and video home content, in International Workshop on Content- Based Multimedia Indexing (CBMI), pp", "author": ["P. Villegas", "E. Bru", "B. Mayayo", "L. Carpio", "E. Alonso", "V.J. Ruiz"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Modelling the False Alarm and Missed Detection Rate for Electronic Watermarks", "author": ["J.-P. Linnartz", "T. Kalker", "G. Depovere"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons", "author": ["T. S\u00f8rensen"], "venue": "Biologiske Skrifter / Kongelige Danske Videnskabernes Selskab,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1948}, {"title": "A test of landsat-2 imagery and digital data for thematic mapping illustrated by  an environmental study in northern Kenya", "author": ["U. Hellden"], "venue": "Lund Univ. Nat. Geog. Inst, Lund, Sweden", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1980}, {"title": "Short: The landsat tutorical workbook\u2014 Basics of satellite remote sensing, Goddard Space Flight Center, Greenbelt, MD", "author": ["M. N"], "venue": "NASA ref. pub", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1982}, {"title": "Map Evaluation and \u2015Chance Correction", "author": ["G. T\u00fcrk"], "venue": "Photogramm Eng Rem S,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Reliability of Content Analysis: The Case of Nominal Scale Coding", "author": ["W.A. Scott"], "venue": "Public Opin Quart,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1955}, {"title": "Maxwell: Coefficients of agreement between observers and their interpretation", "author": ["E. A"], "venue": "British Journal of Psychiatry,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1977}, {"title": "The analysis of crossclassified data: independence, quasiindependence, and interaction in contingency tables with and without missing entries", "author": ["L.A. Goodman"], "venue": "J Am Stat Assoc,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1968}, {"title": "Handbook of Means and Their Inequalities, 2nd ed", "author": ["P.S. Bullen"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "High agreement but low kappa: II. Resolving the paradoxes", "author": ["D.V. Cicchetti", "A.R. Feinstein"], "venue": "J Clin Epidemiol,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1990}, {"title": "A Family of Association Coefficients for Metric", "author": ["F.E. Zegers", "J.M.F. ten Berge"], "venue": "Scales, Psychometrika,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1985}, {"title": "The determination of optimal threshold levels for change detection using various accuracy indices", "author": ["T. Fung", "E. LeDrew"], "venue": "Photogramm Eng Rem S,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1988}], "referenceMentions": [{"referenceID": 1, "context": "Some were specifically designed to compare classifiers , but most were initially defined for other purposes, such as measuring the association between two random variables [2], the agreement between two raters [3] or the similarity between two sets [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 2, "context": "Some were specifically designed to compare classifiers , but most were initially defined for other purposes, such as measuring the association between two random variables [2], the agreement between two raters [3] or the similarity between two sets [4].", "startOffset": 210, "endOffset": 213}, {"referenceID": 3, "context": "Some were specifically designed to compare classifiers , but most were initially defined for other purposes, such as measuring the association between two random variables [2], the agreement between two raters [3] or the similarity between two sets [4].", "startOffset": 249, "endOffset": 252}, {"referenceID": 4, "context": "Most measures are designed to focus on a specific aspect of the overall classification results [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "Finally, the measures may also differ in the nature of the situations they can handle [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "Finally, some measures are sensitive to the sampling design used to retrieve the test data [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "Many different measures exist, but yet, there is no such thing as a perfect measure, which would be the best in every situation [8]: an appropriate measure must be chosen according to the classification context and objectives.", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "the question of chance-correction [9]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "We discuss the case where one wants to select the best classification algorithm to process a given data set [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "In [11], Congalton described the various aspects of accuracy assessment and compared a few measures in terms of functional traits.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "defined the notions of consistency and discriminancy to compare measures [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "In [13], Flach compared 7 measures through the use of ROC plots.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "\u2019s consistency [12].", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Sokolova & Lapalme considered 24 measures, on both binary and multiclass problems (and others) [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "Using the same general idea than Flach [13] (isometrics), they developed the notion of invariance, by identifying the changes in the confusion matrix which did not affect the measure value.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "In [14], Albatineh et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "Caruana & Niculescu-Mizil adopted this method to compare 9 accuracy measures [15], but their focus was on binary classification problems, and classifiers able to output real-valued scores (by opposition to the discrete scores we treat here).", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "[16] and and Ferri et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] considered 34 and 18 measures, respectively, for both binary and multiclass problems (amongst others).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The main limitation with these studies is they either use data coming from a single applicative domain (such as remote sensing in [16]), or rely on a small number of datasets (7 in [15] and 30 in [17]).", "startOffset": 130, "endOffset": 134}, {"referenceID": 14, "context": "The main limitation with these studies is they either use data coming from a single applicative domain (such as remote sensing in [16]), or rely on a small number of datasets (7 in [15] and 30 in [17]).", "startOffset": 181, "endOffset": 185}, {"referenceID": 16, "context": "The main limitation with these studies is they either use data coming from a single applicative domain (such as remote sensing in [16]), or rely on a small number of datasets (7 in [15] and 30 in [17]).", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "Note some authors invert estimated and true classes, resulting in a transposed matrix [13, 18].", "startOffset": 86, "endOffset": 94}, {"referenceID": 17, "context": "Note some authors invert estimated and true classes, resulting in a transposed matrix [13, 18].", "startOffset": 86, "endOffset": 94}, {"referenceID": 1, "context": "[2]), Yule\u2019s coefficients, Matthew\u2019s correlation coefficient, Proportional reduction in error measures (Goodman & Kruskal\u2019s and , Theil\u2019s uncertainty coefficient, etc.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "), mutual informationbased measures [19] and others.", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "The relationship assessed by an association measure is more general [2], since a high level of association only means it is possible to predict estimated classes when knowing the true ones (and vice-versa).", "startOffset": 68, "endOffset": 71}, {"referenceID": 19, "context": "2 Overall Success Rate Certainly the most popular measure for classification accuracy [20], the overall success rate is defined as the trace of the confusion matrix:", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "The former is also called sensitivity [20], producer\u2019s accuracy [11] and Dice\u2019s asymmetric index [21].", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "The former is also called sensitivity [20], producer\u2019s accuracy [11] and Dice\u2019s asymmetric index [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "The former is also called sensitivity [20], producer\u2019s accuracy [11] and Dice\u2019s asymmetric index [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "The latter is alternatively called specificity [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "The estimation-oriented measures, which focus on the confusion matrix rows (estimated classes), are the Positive Predictive Value (PPV) and Negative Predictive Value (NPV) [20].", "startOffset": 172, "endOffset": 176}, {"referenceID": 19, "context": "The former is also called precision [20], user\u2019s accuracy [11] and Dice\u2019s association index [21].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "The former is also called precision [20], user\u2019s accuracy [11] and Dice\u2019s association index [21].", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "The former is also called precision [20], user\u2019s accuracy [11] and Dice\u2019s association index [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "fallout [22] or false alarm rate [23], and is notably used to build ROC curves [20].", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "fallout [22] or false alarm rate [23], and is notably used to build ROC curves [20].", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "fallout [22] or false alarm rate [23], and is notably used to build ROC curves [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "4 F-measure and Jaccard Coefficient The F-measure corresponds to the harmonic mean of PPV and TPR [20], therefore it is classspecific and symmetric.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "The measure known as Jaccard\u2019s coefficient of community was initially defined to compare sets [4], too.", "startOffset": 94, "endOffset": 97}, {"referenceID": 25, "context": "It is alternatively called Short\u2019s measure [26].", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "It is related to the F-measure [27]: i i i F F JCC 2 ,", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "5 Classification Success Index The Individual Classification Success Index (ICSI), is a class-specific symmetric measure defined for classification assessment purpose [1]:", "startOffset": 167, "endOffset": 170}, {"referenceID": 0, "context": "The Classification Success Index (CSI) is an overall measure defined simply by averaging ICSI over all classes [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "proportions [3]:", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "coefficient (SPC) relies instead on the class proportions measured on the whole data set (or its estimation), noted i p [29]: 2", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "authors, including Maxwell for his Random Error (MRE) [30], made the assumption classes are", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "7 Ground Truth Index T\u00fcrk\u2019s Ground Truth Index (GTI) is another chance-corrected measure, but this one was defined specially for classification accuracy assessment [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 26, "context": "The way this measure handles chance correction is more adapted to classification than the agreement coefficients [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "less than 10% of the real-world cases considered in [16]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "Our approach is related to the isometrics concept described in [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "Inversely, it is possible to use an overall measure to assess a given class accuracy, by merging all classes except the considered one [2].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "Certain more sophisticated measures allow associating a weight to each class, though [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 26, "context": "It consists in associating a weight to each cell in the confusion matrix, and then using a regular (unweighted) overall measure [27].", "startOffset": 128, "endOffset": 132}, {"referenceID": 30, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Besides these combinations of TPR and PPV, this also holds for various measures corresponding to combinations of TPR and TNR, not presented here because they are not very popular [20, 33].", "startOffset": 179, "endOffset": 187}, {"referenceID": 31, "context": "Besides these combinations of TPR and PPV, this also holds for various measures corresponding to combinations of TPR and TNR, not presented here because they are not very popular [20, 33].", "startOffset": 179, "endOffset": 187}, {"referenceID": 32, "context": "Thus, several supposedly different measures are actually the same, but transposed to different scales [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "Jaccard\u2019s coefficient [4]).", "startOffset": 22, "endOffset": 25}, {"referenceID": 33, "context": "the combination of OSR and marginal rates described in [35]).", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "First, authors disagree on the necessity of this correction, depending on the application context [7, 27].", "startOffset": 98, "endOffset": 105}, {"referenceID": 26, "context": "First, authors disagree on the necessity of this correction, depending on the application context [7, 27].", "startOffset": 98, "endOffset": 105}], "year": 2011, "abstractText": "The selection of the best classification algorithm for a given dataset is a very widespread problem, occuring each time one has to choose a classifier to solve a real-world problem. It is also a complex task with many important methodological decisions to make. Among those, one of the most crucial is the choice of an appropriate measure in order to properly assess the classification performance and rank the algorithms. In this article, we focus on this specific task. We present the most popular measures and compare their behavior through discrimination plots. We then discuss their properties from a more theoretical perspective. It turns out several of them are equivalent for classifiers comparison purposes. Futhermore. they can also lead to interpretation problems. Among the numerous measures proposed over the years, it appears that the classical overall success rate and marginal rates are the more suitable for classifier comparison task.", "creator": "Microsoft\u00ae Word 2010"}}}