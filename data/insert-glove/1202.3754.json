{"id": "1202.3754", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "A Geometric Traversal Algorithm for Reward-Uncertain MDPs", "abstract": "rassmussen Markov decision processes (MDPs) are widely hideaway used in modeling ickler decision making otwell problems delpierre in mcguff stochastic environments. gosfield However, denmark-norway precise specification of rosettabooks the 22.22 reward detours functions in MDPs argerich is 116.30 often plectrum very tuc difficult. Recent narz approaches have croton focused on computing jerauld an optimal policy k10 based 1.455 on the minimax steel regret criterion 8:51 for sadu obtaining moayed a robust maf policy under uncertainty in the crs reward function. griswold One of the military-industrial core tasks suzak in sinkinson computing the cumberland minimax hits regret policy alit is jcw to obtain meli the set 766 of hopeful all imperious policies earling that elston can be optimal referring for polish-american some geriatric candidate reward function. hadi In paprocki this paper, we artur propose an 0.68 efficient 1.5555 algorithm slammers that exploits the geometric properties of 130.2 the gadabout reward chis function sarpolus associated passow with zheng the policies. pivar We fespic also chungu present dailycandy an furnariidae approximate version of the method for baha'is further baldwinsville speed ncs up. We title-winning experimentally lichnowsky demonstrate that our carner algorithm jwg improves tpms the making performance by plan orders of magnitude.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (142kb)", "http://arxiv.org/abs/1202.3754v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eunsoo oh", "kee-eung kim"], "accepted": false, "id": "1202.3754"}, "pdf": {"name": "1202.3754.pdf", "metadata": {"source": "CRF", "title": "A Geometric Traversal Algorithm for Reward-Uncertain MDPs", "authors": ["Eunsoo Oh"], "emails": ["esoh@ai.kaist.ac.kr", "kekim@cs.kaist.ac.kr"], "sections": [{"heading": null, "text": "Markov decision processes (MDPs) are widely used in modeling decision making problems in stochastic environments. However, precise specification of the reward functions in MDPs is often very difficult. Recent approaches have focused on computing an optimal policy based on the minimax regret criterion for obtaining a robust policy under uncertainty in the reward function. One of the core tasks in computing the minimax regret policy is to obtain the set of all policies that can be optimal for some candidate reward function. In this paper, we propose an efficient algorithm that exploits the geometric properties of the reward function associated with the policies. We also present an approximate version of the method for further speed up. We experimentally demonstrate that our algorithm improves the performance by orders of magnitude."}, {"heading": "1 Introduction", "text": "Markov Decision Processes (MDPs) have been a popular model for stochastic sequential decision making problems (Puterman 1994). Once we obtain the MDP model of the problem, we can solve it efficiently using various methods such as value iteration, policy iteration, or linear programming (LP) solver. However, the specification of model parameters, the transition probabilities and the rewards, can be a difficult task. For example, since the transition probabilities are estimated from the data or specified by the domain expert, there inevitably exists uncertainty regarding the accuracy of the estimation, which affects the real performance of the optimal policy obtained from the model. The specification of rewards poses a greater challenge in the sense that the current practice involves manual\nspecification by the domain expert, and estimating it from the data still remains as an emerging research area, e.g., inverse reinforcement learning (Ng and Russell 2000).\nOver the recent years, a significant body of research has dealt with finding the robust solution to MDPs with imprecisely specified parameters. While most of the work in the past uses the maximin criterion to address the uncertainty in the transition probabilities only (White and Eldeib 1994; Nilim and Ghaoui 2005) or both (White and Eldeib 1986; Givan et al. 1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010). This is not only because the rewards are more difficult to specify than the transition probabilities, but also the uncertainty in the rewards is an important subject for preference elicitation algorithms (Regan and Boutilier 2009). The minimax regret criterion was also proposed for computing the robust policy for MDPs with imprecise rewards (Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010). The challenge is in finding an effective algorithm to address the intractability result for computing the minimax regret policy.\nIn this paper, we propose an efficient algorithm for computing the minimax regret policy. Specifically, we build on the \u03c0Witness algorithm by Regan and Boutilier (2010) for computing the set of nondominated policies that are optimal for some reward function. Instead of using linear programming (LP) in finding nondominated policies, we leverage the condition on the reward function that makes a nondominated policy optimal. This technique allows the running time of our method to become linear in the number of nondominated policies. Since the number of nondominated policies is a dominant factor for the running time of the \u03c0Witness algorithm, our method is orders of magnitude faster. We also propose an anytime algorithm based on the same technique and empirically\ncompare it with the anytime version of the \u03c0Witness algorithm."}, {"heading": "2 Background", "text": "In this section, we cover the fundamentals of MDPs, reward-uncertain MDPs, and the definition of the minimax regret criterion for reward-uncertain MDPs. We also overview the previous work on computing the minimax regret policy of reward-uncertain MDPs."}, {"heading": "2.1 Markov Decision Processes (MDPs)", "text": "An MDP with an infinite-horizon, finite state and action spaces is defined as \u3008S,A, T, r, \u03b1, \u03b3\u3009, with the set S of n states, the set A of m actions, the state transition function T which is defined as T (s, a, s\u2032) = Pr(s\u2032|s, a), the reward function r(s, a), the initial state distribution \u03b1, and the discount factor \u03b3 \u2208 [0, 1).\nA (deterministic) policy in MDPs is a mapping \u03c0 : S \u2192 A with the associated value function V \u03c0 defined as\nV \u03c0(s) = r (s, \u03c0(s)) + \u03b3 \u2211\ns\u2032\u2208S\nT (s, a, s\u2032)V \u03c0(s\u2032) (1)\nwhich is the expected discounted sum of rewards for each state when executing the policy \u03c0. Using vector notations, the reward function is denoted as an nm-dimensional vector r, and the transition function is denoted as an nm \u00d7 n matrix T. We also use ndimensional vector r\u03c0 and n\u00d7 n matrix T\u03c0 to denote the reward and transition functions for policy \u03c0. We can then rewrite Equation (1) as\nV\u03c0 = r\u03c0 + \u03b3T\u03c0V \u03c0. (2)\nThe Q-function Q : S\u00d7A \u2192 R represents the expected value of taking action a followed by the execution of policy \u03c0. Using vector notation, it is defined as\nQ\u03c0a = ra + \u03b3TaV \u03c0 (3)\nEach policy \u03c0 has an associated occupancy frequency f\u03c0 defined as\nf\u03c0(s, a) = \u2211\ns\u2032\n\u03b1(s\u2032)\n\u221e \u2211\nt=0\n\u03b3tPr\u03c0(St = s,At = a|S0 = s \u2032)\nwhich is the total discounted probability of being in state s and executing action a. f\u03c0 can be obtained from the equation\n\u03b3E\u22a4\u03c0 f \u2032\u03c0 + \u03b1 = 0,\nwhere E\u03c0 is an n\u00d7 n matrix with\nE\u03c0(s, s \u2032) =\n{\nT (s, \u03c0(s), s\u2032) if s 6= s\u2032 T (s, \u03c0(s), s\u2032)\u2212 1 \u03b3 if s = s\u2032\nand f \u2032 \u03c0 is an n-dimensional vector so that\nf\u03c0(s, a) =\n{\nf \u2032\u03c0(s) if a = \u03c0(s)\n0 otherwise\nOn the other hand, we can obtain \u03c0 from f\u03c0 using\n\u03c0(s, a) = f\u03c0(s, a) \u2211\na\u2032 f \u03c0(s, a\u2032)\nfor a randomized policy, and \u03c0(s) = argmaxa f \u03c0(s, a) for a deterministic policy. In addition, the expected discounted sum of rewards by following policy \u03c0 can be obtained using\n\u03b1\u22a4V\u03c0 = r\u22a4f\u03c0. (4)\nThus, we can interchangeably use the terms policy and occupancy frequency.\nIf we generalize E\u03c0 to cover the set of all policies,\nE(sa, s\u2032) =\n{\nT (s, a, s\u2032) if s 6= s\u2032 T (s, a, s\u2032)\u2212 1 \u03b3 if s = s\u2032\nthen any f satisfying the equation\n\u03b3E\u22a4f + \u03b1 = 0\nis called a valid occupancy frequency since it is associated to some policy. Hence, if we let F be the set of all valid occupancy frequencies, it corresponds to the set of all possible policies.\nThe relationship between \u03c0 and f\u03c0 becomes more evident when we use linear programming (LP) to solve MDPs. The primal LP is formulated\nminV \u03b1 \u22a4V\ns.t. V (s) \u2265 r(s, a) + \u03b3 \u2211\ns\u2032\nT (s, a, s\u2032)V (s\u2032) \u2200s, a\nof which the dual LP is formulated as\nmaxf r \u22a4f\ns.t. \u03b3E\u22a4f + \u03b1 = 0 (5)\nf \u2265 0.\nNote that the solution of the dual LP is the occupancy frequency of the optimal policy."}, {"heading": "2.2 Reward-Uncertain MDPs and Minimax Regret Policies", "text": "The exact specification of the reward function can be difficult in practice. The reward-uncertain MDP (RUMDP) (Regan and Boutilier 2010) extends the standard MDP by allowing a set of feasible reward\nfunctions instead of requiring a single exact reward function. The RUMDP is defined by \u3008S,A, T,R, \u03b1, \u03b3\u3009 where R is the space of feasible reward functions, replacing the reward function r in the standard MDP. Hence, the uncertainty in the reward function is confined to the space R. In addition, following the original work on RUMDPs, we assume that R is a bounded and convex polytope defined by the set of linear constraints {r|Ar \u2264 b}. We use |R| to denote the number of constraints (the number of rows in A) and dim(R) to denote the dimension of R.1\nGiven an RUMDP, we want to find a policy that is robust to the uncertainty in the reward function. We thus require a criterion to compare the robustness of policies. We adopt the minimax regret criterion (Boutilier et. al. 2006, Xu and Mannor 2009, Regan and Boutilier 2009, Regan and Boutilier 2010), which is defined as\nMMR(R) = min f\u2208F max g\u2208F max r\u2208R\nr\u22a4g \u2212 r\u22a4f . (6)\nThe formulation can be viewed as if there is an adversary choosing the reward function r to maximize the loss with respect to the optimal policy (i.e. occupancy frequency) g, while our agent is selecting policy f to minimize the loss. The minimax regret corresponds to the worst-case bound on the loss. Formally, let fMMR(R) = argminf maxg maxr r\n\u22a4g \u2212 r\u22a4f be the minimax regret policy and MMR(R) be the minimax regret achieved. Then, given any realization of r, no policy can outperform f by more than MMR(R) in terms of the expected value."}, {"heading": "2.3 Computing Minimax Regret Policies", "text": "Although the minimax regret is a natural criterion for robustness, computing the minimax policy for a RUMDP is known to be NP-hard (Xu and Mannor 2009). A number of approaches exist, but we only review those involving nondominated policies.\nA policy g is defined to be nondominated with respect to the feasible reward space R if and only if\n\u2203r \u2208 R s.t. r\u22a4g \u2265 r\u22a4f , \u2200f \u2208 F,\nwhich states that a nondominated policy should be optimal for some feasible reward function. Let \u0393 denote the set of nondominated policies. The set \u0393 is useful for computing the minimax policy in equation (6) because, for any f \u2208 F , (argmaxg\u2208F maxr\u2208R r \u22a4g \u2212\n1Generally speaking, dim(R) is equal to the dimension of r, i.e., |S| \u00d7 |A|. In some cases, however, the reward function can be compactly represented in a factored form, so that dim(R) is proportional to the number of basis functions and significantly less than |S| \u00d7 |A|.\nr\u22a4f) \u2208 \u0393. We can verify this property from the fact that the policy g is chosen to maximize the expected value for some reward r \u2208 R. In addition, if we view the minimax regret policy as a randomization over a set of deterministic policies, the probability of choosing a dominated policy can be made be zero since there should be a nondominated policy that performs better.\nXu and Mannor (2009) propose the following LP formulation for computing the minimax regret policy of RUMDP with R = {r|Ar \u2264 b} using the set \u0393 of nondominated policies:\nminz,c,\u03b4 \u03b4 subject to \u2211|\u0393|\ni=1 ci = 1 c \u2265 0 \u03b4 \u2265 b\u22a4zi A\u22a4zi + \u0393\u0302c = gi\nzi \u2265 0\n\n \n \ni = 1, . . . , |\u0393|\nwhere \u0393\u0302 is the matrix formed by taking the elements in \u0393 as columns. The |\u0393|-dimensional vector c represents the convex combination of nondominated policies in the minimax regret policy. The last three constraints are from the dual of\nmaxr r \u22a4(gi \u2212 c \u22a4\u0393\u0302)\nsubject to Ar \u2264 b\nfor each adversarial policy gi \u2208 \u0393. The overall LP has O(|R||\u0393|) variables and O(|\u0393|) constraints.\nSince |\u0393| can be very large, Regan and Boutilier (2010) propose ICG-ND, a technique based on constraint generation for LPs. Given the subset GEN of possible adversarial policies and rewards, the minimax regret policy can be found by LP\nminf ,\u03b4 \u03b4\nsubject to r\u22a4i gi \u2212 r \u22a4 i f \u2264 \u03b4 \u2200\u3008gi, ri\u3009 \u2208 GEN\n\u03b3E\u22a4f + \u03b1 = 0\nIn order to iteratively generate the set GEN, an LP is solved for each g \u2208 \u0393 to determine the reward that maximizes the regret for the current solution f :\nminr r \u22a4g \u2212 r\u22a4f subject to Ar \u2264 b\nThe g with the largest objective value is regarded as the maximally violated constraint, and included into GEN along with the associated r.\nWhile it is evident from ICG-ND that the set \u0393 of nondominated policies is important in computing minimax regret policies, enumerating the set \u0393 still remains as a challenge. Hence, Regan and Boutilier (2010) propose the \u03c0Witness algorithm for the exact computation of \u0393. The algorithm incrementally constructs the\nAlgorithm 1: The \u03c0Witness Algorithm\nbegin r \u2190 some arbitrary r \u2208 R f \u2190 findOptPolicy(r) \u0393 \u2190 {f} agenda \u2190 {\u3008r, f\u3009} while agenda is not empty do\nf \u2190 next item in agenda for s, a do\nr\u2032 \u2190 findWitnessRewardFn(f , s, a,\u0393) while witness found do\nf \u2032 \u2190 findOptPolicy(r\u2032) add f \u2032 to \u0393 add \u3008r\u2032, f \u2032\u3009 to agenda r\u2032\u2190findWitnessRewardFn(f , s, a,\u0393)\nset of nondominated policies by finding a reward function that makes some policy optimal but not yet in \u0393. This method is analogous to the witness algorithm in the POMDP literature. Algorithm 1 presents the pseudo-code of the algorithm.\nfindOptPolicy(r) computes an optimal policy for the MDP with reward r. We can use any MDP algorithm for this procedure, e.g., value iteration, policy iteration, or LP.\nfindWitnessRewardFn(f , s, a,\u0393) attempts to find r for which a local adjustment of f at (s, a) has higher value than any f \u2032 \u2208 \u0393 by solving an LP. This LP has O(|\u0393|) constraints and O(|S||A|) variables.\nIts running time is polynomial in |\u0393|, |S|, and |A|, but not linear in |\u0393|. Since |\u0393| \u226b |S||A| in general, the size of \u0393 is a dominant factor in running time of the \u03c0Witness algorithm. In the later section, we empirically show that computing \u0393 takes much more time than ICG-ND alone. Hence, improvement in computing \u0393 is critical to the efficient computation of the minimax regret policies in RUMDPs."}, {"heading": "3 Geometric Traversal for Nondominated Policies", "text": "In this section, we present our algorithm for computing the set of nondominated policies in RUMDPs, leveraging the optimality condition of reward function in MDPs."}, {"heading": "3.1 Optimality Condition for Rewards", "text": "Consider an MDP M = \u3008S,A, T, r\u2217, \u03b1, \u03b3\u3009 with an optimal policy \u03c0. If the change \u2206r in the reward function r\u2217 is very small, \u03c0 may still remain as an optimal pol-\nicy. Treating the reward function r = r\u2217 + \u2206r as a variable vector, we can obtain a necessary and sufficient condition for r that guarantees the optimality of \u03c0 using the result in (Ng and Russell 2000):\nV\u03c0 \u2265 Q\u03c0a \u2200a.\nUsing equations (2) and (3), the above inequality becomes\nr\u03c0 + \u03b3T\u03c0V \u03c0 \u2265 ra + \u03b3TaV \u03c0 \u2200a. (7)\nNote also that, from equation (2),\nV\u03c0 = (I\u2212 \u03b3T\u03c0) \u22121r\u03c0 = (\u2212\u03b3E\u03c0) \u22121r\u03c0.\nHence equation (7) becomes \u2200a,\nr\u03c0 \u2212 \u03b3T\u03c0(\u03b3E\u03c0) \u22121r\u03c0 \u2265 ra \u2212 \u03b3Ta(\u03b3E\u03c0) \u22121r\u03c0\n\u21d4 r\u03c0 + (I\u2212 \u03b3T\u03c0)(\u03b3E\u03c0) \u22121r\u03c0\n\u2265 ra + (I\u2212 \u03b3Ta)(\u03b3E\u03c0) \u22121r\u03c0\n\u21d4 r\u03c0 \u2212 \u03b3E\u03c0(\u03b3E\u03c0) \u22121r\u03c0 \u2265 ra \u2212 \u03b3Ea(\u03b3E\u03c0) \u22121r\u03c0.\nSince the left-hand side r\u03c0 \u2212 \u03b3E\u03c0(\u03b3E\u03c0) \u22121r\u03c0 = 0, we obtain r\u2212E(\u03b3E\u03c0) \u22121r\u03c0 \u2264 0.\nUsing the definitions r = r\u2217 +\u2206r and r\u03c0 = r\u03c0 +\u2206r\u03c0, we have\nr\u2217 +\u2206r\u2212E(\u03b3E\u03c0) \u22121(r\u2217\u03c0 +\u2206r\u03c0) \u2264 0. (8)\nWe refer to equation (8) as the reward optimality condition with respect to policy \u03c0. It yields the set of linear inequalities for the changes in the reward function \u2206r(s, a) that defines the boundary around r\u2217 where \u03c0 remains as optimal.\nThe reward optimality condition is essentially equivalent to performing sensitivity analysis on LP (Bertsimas and Tsitsiklis 1997) for solving the MDP in equation (5). However, we cannot leverage LP solvers for computing the exact boundary since their sensitivity analysis results project the system of inequalities into each state and action, i.e., the change in the reward function is analyzed independently for each state and action."}, {"heading": "3.2 Geometric Traversal Algorithm", "text": "The reward optimality condition in equation (8) is essentially a set of inequalities, each of which describes a hyperplanar boundary. Note that the region defined by the reward optimality condition is a convex bounded polytope. The space of feasible reward functions is visualized in Figure 1 (a). The space is partitioned into the regions defined by the reward optimality condition, each region corresponding to a nondominated policy. Hence, we can view the partitioned\nspace as a connected undirected graph with nodes corresponding to nondominated policies and edges corresponding to the adjacency of their optimal reward regions, as shown in Figure 1 (b).\nOur geometric traversal algorithm for finding nondominated policies essentially constructs this graph using the reward optimality condition. Since all the nodes are connected, any exhaustive traversal algorithm can be used, e.g., breadth-first or depth-first. Algorithm 2 presents the pseudo-code of the algorithm.\nfindRewardOptRgn(r, f) constructs the set of hyperplanes defined by the reward optimality condition in equation (8). Specifically, it computes the set H of hyperplanes, where each hyperplane h \u2208 H is represented as c\u22a4h r \u2264 dh. Hence each hyperplane can be represented as a pair \u3008ch, dh\u3009. Each hyperplane corresponds to one of the edges in the graph of nondominated policies.\nfindAdjRewardFn(h,H) yields a reward function which is located in the adjacent reward region across the hyperplane h. It is obtained by solving the following LP with a small positive constant \u03b4 for excluding\nAlgorithm 2: Geometric Traversal Algorithm\nbegin r \u2190 some arbitrary r \u2208 R f \u2190 findOptPolicy(r) \u0393 \u2190 {f} agenda \u2190 {\u3008r, f\u3009} while agenda is not empty do\n\u3008r, f\u3009 \u2190 next item in agenda H \u2190 findRewardOptRgn(r, f) for h \u2208 H do\nr\u2032 \u2190 findAdjRewardFn(h,H) if r\u2032 is found then\nf \u2032 \u2190 findOptPolicy(r\u2032) if f \u2032 /\u2208 \u0393 then\nadd f \u2032 to \u0393 add \u3008r\u2032, f \u2032\u3009 to agenda\npoints exactly on h:\nmaxr\u2032 0 s.t. Ar\u2032 \u2264 b\nc\u22a4h\u2032r \u2032 \u2264 dh\u2032 if h \u2032 6= h c\u22a4h\u2032r \u2032 \u2265 dh\u2032 + \u03b4 if h \u2032 = h\n}\n\u2200h\u2032 \u2208 H (9)\nNote that the reward region of interest is adjacent to the current reward region by h, since we reverse the direction of the inequality for h while keeping every other h\u2032 \u2208 H unchanged. If the above LP yields a feasible solution, it implies that there exists an adjacent reward region with potentially a new nondominated policy.\nUpon the termination of the geometric traversal algorithm, \u0393 will be the complete set of nondominated policies. If we implement \u0393 using a hash set, all the set operations used in the algorithm take O(1) time.\nThe running time of our geometric traversal algorithm is polynomial in |S| and |A|, and linear in |\u0393| since:\n\u2022 Each call to findRewardOptRgn(r, f) takes O(|S|2|A|dim(R)) and is called |\u0393| times. Note that the running time of the procedure is independent of |\u0393|.\n\u2022 Since the size of each H is at most |S||A|, findAdjRewardFn(h,H) is called at most |S||A| times for each f \u2208 \u0393, hence it is called a total of |\u0393||S||A| times. Each call to the procedure requires solving an LP with dim(R) variables and |H| \u2264 |S||A| constraints, of which the running time is independent of |\u0393|.\n\u2022 findOptPolicy(r) is called only when an adjacent reward function is found, so it is called\nAlgorithm 3: Approximate Geometric Traversal Algorithm\nbegin \u0393 \u2190 {} agenda \u2190 {} while \u0393 is not sufficiently gathered do\nr \u2190 some arbitrary r \u2208 R l \u2190 arbitrary straight line passing through r f \u2190 findOptPolicy(r) add f to \u0393 add \u3008r, f\u3009 to agenda while agenda is not empty do\n\u3008r, f\u3009 \u2190 next item in agenda H \u2190 findRewardOptRgn(r, f) {r1, r2} \u2190 find two intersections from H for r\u2032 \u2208 {r1, r2} do\nf \u2032 \u2190 findOptPolicy(r\u2032) add \u3008r\u2032, f \u2032\u3009 to agenda if f \u2032 /\u2208 \u0393 then\nadd f \u2032 to \u0393\nO(|\u0393||S||A|) times. The running time of the procedure is again independent of |\u0393|.\nIn short, each iteration in the while loop takes the running time polynomial in |S| and |A|, but independent of |\u0393| , so the overall time complexity of the geometric traversal algorithm is linear in |\u0393|."}, {"heading": "3.3 Approximate Method For Computing Nondominated Policies", "text": "Although the geometric traversal algorithm significantly improves the running time, it still can take a large amount of time since the algorithm collects every nondominated policy, potentially as many as |A||S|. Regan and Boutilier (2010) propose a method for computing a subset of nondominated policies, using the \u03c0Witness algorithm in an anytime manner. Using a subset of nondominated policies, they use ICG-ND to compute an approximate minimax regret policy.\nSince our algorithm also incrementally constructs \u0393, it can be also used in an anytime fashion to compute a subset of nondominated policies. The idea is to traverse in each iteration a subset of adjacent reward regions that are encountered while moving along a straight line. Specifically, our approximate algorithm starts with an arbitrary reward function r in R and a random straight line l that passes through r. All the points r\u2032 on the line l with direction vector w are represented by the equation r\u2032 = r+w \u00b7 t.\nThen, once we compute the setH of hyperplanes defining the boundary of the current optimal reward region using findRewardOptRgn(r, f), we can obtain the intersection of line l and hyperplane h \u2208 H by solving the system of linear equations:\nr\u2032 = r+w \u00b7 t\nc\u22a4h r \u2032 = dh\nTwo intersections with line l and the boundary defined by H is obtained by taking r\u2032 with the minimum among the positive solutions and the maximum among the negative solutions of t. By adding and subtracting a small positive constant \u03b4 to the solutions, we obtain the rewards in the two adjacent reward regions. Once we gather all the adjacent reward regions along the current line l, we restart with an arbitrary reward function r and a new random straight line l. Algorithm 3 presents the pseudo-code of our approximate method based on the geometric traversal algorithm."}, {"heading": "4 Experiments", "text": "We tested the performance of our algorithm on randomly generated instances of RUMDPs with different state sizes and reward function dimensions. For each setting of the state size and the reward function dimension, we randomly generated 100 instances of RUMDPs, following the same experimental evaluation setup in (Regan and Boutilier 2010). We ran geometric traversal (GT) algorithm, \u03c0Witness, and ICGND. Note that GT and \u03c0Witness are used to precom-\npute the set \u0393 of nondominated policies, while ICG-ND computes the minimax regret policy using \u0393.\nFigure 2 compares the running times of the algorithms with respect to different sizes of \u0393. The figure shows the running time plots of \u03c0Witness only, GT only, \u03c0Witness with ICG-ND, and GT with ICG-ND. First, notice from the graph that whether we use \u03c0Witness or GT, computing \u0393 is the dominant factor in the overall running time for computing the minimax regret policy. Second, GT is significantly faster than \u03c0Witness due to its time complexity linear in |\u0393|.\nWhen the running times are compared on larger RUMDPs, the performance gain achieved by GT is much clearer. Figure 3 shows the comparisons of running times on 5 different settings in the sizes of RUMDPs. It is evident from the graph that we typically obtain several magnitudes of order improvement in the running time using GT.\nWe also experimented with the approximate version of GT. Table 1 summarizes the results on RUMDPs with different sizes. For each of the 4 size settings, we generated 5 to 10 random instances of RUMDPs, and ran approximate GT and anytime \u03c0Witness algorithms until the relative error in the minimax regret falls below 10%, 5% and 1%, or time out at 20 minutes. We measured the execution time and the number of produced nondominated policies upon termination. As expected, our approximate GT signifi-\ncantly outperformed anytime \u03c0Witness in terms of execution time, while generating significantly more nondominated policies due to the lack of a prioritization heuristic. Developing a good heuristic for approximate GT remains as a future work.\nAll the algorithms were implemented in Java, and CPLEX 12.1 was used as the LP solver."}, {"heading": "5 Conclusion", "text": "We have presented methods to significantly improve the speed of computing the minimax regret policies in RUMDPs. Specifically, we identified that the bottleneck of the state-of-the-art RUMDP algorithm, \u03c0Witness, is in computing the set of nondominated policies, and proposed an efficient algorithm that exploits the geometric properties of reward functions associated with nondominated policies. The end result is a linear time algorithm with respect to the number of nondominated policies in the model, achieving orders of magnitude performance improvement. We also presented an approximate version of the method which does not depend on solving any LP. Experimental results show that the approximate method outperforms anytime version of the \u03c0Witness algorithm in terms of execution time.\nThere are a number of future research directions worth pursuing. First, it would be useful to extend the al-\ngorithm to factored domains. Our approach currently requires construction of |S||A| hyperplanes that define the reward optimality condition, but we conjecture that we can obtain a compact representation. Second, although we achieved significant running time improvement in anytime performance, but we currently lack a heuristic to prioritize nondominated policies, producing a significantly larger set of nondominated policies than \u03c0Witness to achieve the same level of minimax regret error. It would be interesting to investigate whether we can adapt the idea behind the Regan and Boutilier (2009) heuristic into our algorithm."}, {"heading": "Acknowledgements", "text": "This work was supported by the National Research Foundation of Korea (Grant# 2009-0069702) and by the Defense Acquisition Program Administration and the Agency for Defense Development of Korea (Contract# UD080042AD)."}], "references": [{"title": "Introduction to Linear Optimization", "author": ["D. Bertsimas", "J. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsimas and Tsitsiklis,? \\Q1997\\E", "shortCiteRegEx": "Bertsimas and Tsitsiklis", "year": 1997}, {"title": "Percentile Optimization for Markov Decision Processes with Parameter Uncertainty", "author": ["E. Delage", "S. Mannor"], "venue": "Operations Research", "citeRegEx": "Delage and Mannor,? \\Q2009\\E", "shortCiteRegEx": "Delage and Mannor", "year": 2009}, {"title": "Bounded Parameter Markov Decision Processes", "author": ["R. Givan", "S. Leach", "T. Dean"], "venue": "In Proceedings of the 4 European Conference on Planning,", "citeRegEx": "Givan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Givan et al\\.", "year": 1997}, {"title": "Algorithms for Inverse Reinforcement Learning", "author": ["A. Ng", "S. Russell"], "venue": "In Proceedings of the International Conference on Machine Learning", "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Robust Control of Markov Decision Processes with Uncertain Transition Matrices. Operations Research 53(5):780-798", "author": ["A. Nilim", "L.E. Ghaoui"], "venue": null, "citeRegEx": "Nilim and Ghaoui,? \\Q2005\\E", "shortCiteRegEx": "Nilim and Ghaoui", "year": 2005}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming, Wiley, Newyork", "author": ["M. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Regret-based Reward Elicitation for Markov Decision Processes", "author": ["K. Regan", "C. Boutilier"], "venue": "In Proceedings of the 25 Conference on Uncertainty in Artificial Intelligence (UAI-09)", "citeRegEx": "Regan and Boutilier,? \\Q2009\\E", "shortCiteRegEx": "Regan and Boutilier", "year": 2009}, {"title": "Robust Policy Computation in Reward-uncertain MDPs using Nondominated Policies", "author": ["K. Regan", "C. Boutilier"], "venue": "In Proceedings of the 25 National Conference on Artificial Intelligence (AAAI-10),", "citeRegEx": "Regan and Boutilier,? \\Q2010\\E", "shortCiteRegEx": "Regan and Boutilier", "year": 2010}, {"title": "Parameter Imprecision in Finite State, Finite Action Dynamic Programs", "author": ["C.C. White", "H.K. Eldeib"], "venue": "Operations Research", "citeRegEx": "White and Eldeib,? \\Q1986\\E", "shortCiteRegEx": "White and Eldeib", "year": 1986}, {"title": "Markov Decision Processes with Imprecise Transition Probabilities", "author": ["H.K.C.C. White"], "venue": "Eldeib", "citeRegEx": "White,? \\Q1994\\E", "shortCiteRegEx": "White", "year": 1994}, {"title": "Parametric Regret in Uncertain Markov Decision Processes", "author": ["H. Xu", "S. Mannor"], "venue": "In Proceedings of the 48 IEEE Conference on Decision and Control", "citeRegEx": "Xu and Mannor,? \\Q2009\\E", "shortCiteRegEx": "Xu and Mannor", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "Markov Decision Processes (MDPs) have been a popular model for stochastic sequential decision making problems (Puterman 1994).", "startOffset": 110, "endOffset": 125}, {"referenceID": 3, "context": ", inverse reinforcement learning (Ng and Russell 2000).", "startOffset": 33, "endOffset": 54}, {"referenceID": 4, "context": "While most of the work in the past uses the maximin criterion to address the uncertainty in the transition probabilities only (White and Eldeib 1994; Nilim and Ghaoui 2005) or both (White and Eldeib 1986; Givan et al.", "startOffset": 126, "endOffset": 172}, {"referenceID": 8, "context": "While most of the work in the past uses the maximin criterion to address the uncertainty in the transition probabilities only (White and Eldeib 1994; Nilim and Ghaoui 2005) or both (White and Eldeib 1986; Givan et al. 1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 181, "endOffset": 223}, {"referenceID": 2, "context": "While most of the work in the past uses the maximin criterion to address the uncertainty in the transition probabilities only (White and Eldeib 1994; Nilim and Ghaoui 2005) or both (White and Eldeib 1986; Givan et al. 1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 181, "endOffset": 223}, {"referenceID": 1, "context": "1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 85, "endOffset": 181}, {"referenceID": 10, "context": "1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 85, "endOffset": 181}, {"referenceID": 6, "context": "1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 85, "endOffset": 181}, {"referenceID": 7, "context": "1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 85, "endOffset": 181}, {"referenceID": 6, "context": "This is not only because the rewards are more difficult to specify than the transition probabilities, but also the uncertainty in the rewards is an important subject for preference elicitation algorithms (Regan and Boutilier 2009).", "startOffset": 204, "endOffset": 230}, {"referenceID": 10, "context": "The minimax regret criterion was also proposed for computing the robust policy for MDPs with imprecise rewards (Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 111, "endOffset": 183}, {"referenceID": 6, "context": "The minimax regret criterion was also proposed for computing the robust policy for MDPs with imprecise rewards (Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 111, "endOffset": 183}, {"referenceID": 7, "context": "The minimax regret criterion was also proposed for computing the robust policy for MDPs with imprecise rewards (Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 111, "endOffset": 183}, {"referenceID": 6, "context": "Specifically, we build on the \u03c0Witness algorithm by Regan and Boutilier (2010) for computing the set of nondominated policies that are optimal for some reward function.", "startOffset": 52, "endOffset": 79}, {"referenceID": 7, "context": "The reward-uncertain MDP (RUMDP) (Regan and Boutilier 2010) extends the standard MDP by allowing a set of feasible reward", "startOffset": 33, "endOffset": 59}, {"referenceID": 10, "context": "Although the minimax regret is a natural criterion for robustness, computing the minimax policy for a RUMDP is known to be NP-hard (Xu and Mannor 2009).", "startOffset": 131, "endOffset": 151}, {"referenceID": 6, "context": "Since |\u0393| can be very large, Regan and Boutilier (2010) propose ICG-ND, a technique based on constraint generation for LPs.", "startOffset": 29, "endOffset": 56}, {"referenceID": 6, "context": "Hence, Regan and Boutilier (2010) propose the \u03c0Witness algorithm for the exact computation of \u0393.", "startOffset": 7, "endOffset": 34}, {"referenceID": 3, "context": "Treating the reward function r = r + \u2206r as a variable vector, we can obtain a necessary and sufficient condition for r that guarantees the optimality of \u03c0 using the result in (Ng and Russell 2000): V \u2265 Q\u03c0a \u2200a.", "startOffset": 175, "endOffset": 196}, {"referenceID": 0, "context": "The reward optimality condition is essentially equivalent to performing sensitivity analysis on LP (Bertsimas and Tsitsiklis 1997) for solving the MDP in equation (5).", "startOffset": 99, "endOffset": 130}, {"referenceID": 6, "context": "Regan and Boutilier (2010) propose a method for computing a subset of nondominated policies, using the \u03c0Witness algorithm in an anytime manner.", "startOffset": 0, "endOffset": 27}, {"referenceID": 7, "context": "For each setting of the state size and the reward function dimension, we randomly generated 100 instances of RUMDPs, following the same experimental evaluation setup in (Regan and Boutilier 2010).", "startOffset": 169, "endOffset": 195}, {"referenceID": 6, "context": "It would be interesting to investigate whether we can adapt the idea behind the Regan and Boutilier (2009) heuristic into our algorithm.", "startOffset": 80, "endOffset": 107}], "year": 2011, "abstractText": "Markov decision processes (MDPs) are widely used in modeling decision making problems in stochastic environments. However, precise specification of the reward functions in MDPs is often very difficult. Recent approaches have focused on computing an optimal policy based on the minimax regret criterion for obtaining a robust policy under uncertainty in the reward function. One of the core tasks in computing the minimax regret policy is to obtain the set of all policies that can be optimal for some candidate reward function. In this paper, we propose an efficient algorithm that exploits the geometric properties of the reward function associated with the policies. We also present an approximate version of the method for further speed up. We experimentally demonstrate that our algorithm improves the performance by orders of magnitude.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}