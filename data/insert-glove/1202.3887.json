{"id": "1202.3887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2012", "title": "Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method and Modified Cuckoo Search", "abstract": "unconstructed This vorgan paper harnage investigates a xk8 new olandis method intractably for improving worcester the 197.5 learning algorithm redesigned of tretick Mixture 26.27 of weight-saving Experts (bhava ME) scumbag model endresen using gitex a hybrid of business-related Modified facebook Cuckoo Search (granillo MCS) gryposaurus and drowne Conjugate bowbazar Gradient (CG) as decha a second e-taliban order liep\u0101jas optimization technique. The CG technique is capit\u00e3o combined with bgan Back - Propagation (BP) khandesh algorithm to yield a much posession more fitzjohn efficient pudi learning algorithm for izzi ME 35-17 structure. mythical In addition, sprengel the experts xanthe and carlone gating steege networks in enhanced ibibio model are bartosz replaced by castigating CG unranked based Multi - d'ornano Layer Perceptrons (z\u0142oty MLPs) r\u00e9publicain to valedictory provide faster pruebas and aprotic more episcopate accurate learning. vu\u010ditrn The khorafi CG nbty is impolitic considerably archipelagos depends 38.45 on tamera initial weights of cells connections 519 of exactas Artificial 170-kilometer Neural Network (5-123 ANN ), so, a metaheuristic groveland algorithm, 12.27 the tuy\u1ebft so - called ocl Modified epicenter Cuckoo auto-tuned Search merrett is downscaled applied in order podhurst to select afrosoricida the kersting optimal recherche weights. The performance 358,000 of proposed coachbuilt method is emancipating compared with huraira Gradient hushen Decent Based desktop ME (concertation GDME) accessible and 45.66 Conjugate 10,390 Gradient rightwinger Based ME (CGME) in states classification 45.88 and regression necrolysis problems. The txeroki experimental collinelli results show that hybrid MSC and CG nonplanar based balotesti ME (MCS - concede CGME) lovatt has pyridoxamine faster convergence and better afrc performance 600-pound in utilized madhavi benchmark conformably data sets.", "histories": [["v1", "Fri, 17 Feb 2012 11:49:56 GMT  (205kb)", "http://arxiv.org/abs/1202.3887v1", "13 pages, 2 figures"]], "COMMENTS": "13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hamid salimi", "davar giveki", "mohammad ali soltanshahi", "javad hatami"], "accepted": false, "id": "1202.3887"}, "pdf": {"name": "1202.3887.pdf", "metadata": {"source": "CRF", "title": "Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method and Modified Cuckoo Search", "authors": ["Hamid Salimi", "Davar Giveki", "Mohammad Ali Soltanshahi", "Javad Hatami"], "emails": ["salimi.hamid86@gmail.com,", "ali.soltanshahi@gmail.com,", "jvdhtm@gmail.com", "s9dagive@stud.uni-saarland.de"], "sections": [{"heading": null, "text": "DOI : 10.5121/ijaia.2012.3101 1\nThis paper investigates a new method for improving the learning algorithm of Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS) and Conjugate Gradient (CG) as a second order optimization technique. The CG technique is combined with Back-Propagation (BP) algorithm to yield a much more efficient learning algorithm for ME structure. In addition, the experts and gating networks in enhanced model are replaced by CG based Multi-Layer Perceptrons (MLPs) to provide faster and more accurate learning. The CG is considerably depends on initial weights of connections of Artificial Neural Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo Search is applied in order to select the optimal weights. The performance of proposed method is compared with Gradient Decent Based"}, {"heading": "ME (GDME) and Conjugate Gradient Based ME (CGME) in classification and regression problems. The experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster convergence and better performance in utilized benchmark data sets.", "text": "KEYWORDS\nBack Propagation (BP) algorithm, Gradient Decent (GD), Conjugate Gradient (CG), Modified Cuckoo Search (MCS), Mixture of Experts (MEs)"}, {"heading": "1. INTRODUCTION", "text": "Combining classifier is one of the most popular approaches in pattern recognition, which leads to have a better classification. It increases the recognition rate and improves the reliability of the system. It is usually a good approach in complicated problems due to the small sample size, class overlapping, dimensionality, and substantial noise in the input samples.\nPrevious experimental and theoretic results show that the combining classifiers with each other lead to higher performance when the base classifiers have small error rates, and their errors are different [1]; in other words, the base classifiers make uncorrelated decision in this case. Generally, classifier selection and classifier fusion are two types of combining classifiers [2]. One of the most popular methods of classifier selection is ME, originally proposed by Jacobs et al. [3]. The ME models the conditional probability density of the target output by mixing the outputs from a set of local experts, each of which separately derives a conditional probability density of the target output. The outputs of expert networks are combined by a gating network which is trained to select the expert(s) that is performing the best at solving the problem [4, 5, and 6]. In the basic form of ME [3], the expert and gating networks are linear classifiers, however, for more\ncomplex classification tasks, the expert and gating networks could be of more complicated types. For instance, Ebrahimpour et al. [7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.\nThe Back Propagation (BP) algorithm is the most commonly used technique for training neural networks. It is an approximation of the Least Mean Square (LMS) algorithm, which is based on the steepest descent method. While the BP technique follows a straightforward and wellestablished algorithm, there are some disadvantages associated with it. For instance, the convergence behavior of the BP algorithm highly depends on the choice of initial values of connection weights and other parameters used in the algorithm such as the learning rate and the momentum term [11, 8].\nTo tackle the mentioned problems, we decided to employ much more powerful methods such as Conjugate Gradient (CG) and Modified Cuckoo Search (MCS) algorithm as a hybrid model. The CG method as a second-order optimization techniques is applied to the learning of MLP in ME structure. The MCS algorithm proposed by Walton et al [12] is an improved version of another metaheuristic algorithm the so called Cuckoo Search (CS) [13] and it can be used for initialing optimal values of connection weights. This is an evolutionary optimization algorithm inspired by the nature. This algorithm is inspired by the reproduction strategy of cuckoos. At the most basic level, cuckoos lay their eggs in the nests of other host birds, which may be of different species. The host bird may discover that the eggs which are not it\u2019s own and either destroy the egg or abandon the nest all together. This has resulted in the evolution of cuckoo eggs which mimic the eggs of local host birds. Experimental results show that this modification causes faster convergence and better performance in classification and regression benchmark problems.\nThe rest of paper is organized as follow: In Section 2, Conjugate Gradient Multi-Layer Perceptronsis is explained. Mixture of Expert method is introduced in Section 3. Section 4 discusses Cuckoo Search and Modified Cuckoo Search. Experimental results are discussed in Section 5; finally, the conclusion is made in Section 6."}, {"heading": "2. CONJUGATE GRADIENT MULTI-LAYER PERCEPTRONS (CGMLP)", "text": "The BP learning algorithm is a supervised learning method for multi-layer feed-forward neural networks. It is a gradient descent local optimization technique which involves backward error correction of network weights. Despite the general success of BP method in the learning process, several major deficiencies are still needed to be solved. The convergence rate of BP is slow and hence it becomes unsuitable for large problems. Furthermore, the convergence behavior of the BP algorithm depends on the choice of initial values of connection weights and other parameters used in the algorithm such as the learning rate and the momentum term.\nOne way for speeding up the learning phase is using higher-order optimization. In the BP case the function is approximated by only the linear terms that include the first-order derivatives in a Taylor series expansion. In this case the second-order nonlinear terms that include the secondorder derivatives are also used in the Taylor series expansion, resulting more precise approximation [14].\nGiven a vector 0w\u2206 in the weight space, a second-order Taylor series approximation of the error function around this vector is expressed as\n0\n1 ( ) ( ) ( )\n2\nT T E w E w d w w H w= + \u2206 + \u2206 \u2206\n(1)\nwhere ,d H are the gradient vector and the Hessian matrix, respectively. The minima of the\nfunction E are located where the gradient of E was equal to zero, therefore, the optimal value of w is given by 1\n0 .w w H d \u2212 = \u2212 (2)\nThe form of the basic updating equation of the CG algorithm is the same as the general gradient algorithm, and is given by\n( 1) ( ) ( ) ( )w k w k k w k\u03b7+ = + \u2206 (3)\nwhere ( )k\u03b7 is a time-varying learning parameter that may be updated using the following line\nsearch method:\n( ) min{ ( ( ) ( )) : 0}k E w k w k \u03b7\n\u03b7 \u03b7 \u03b7= + \u2206 \u2265 (4)\nThe conjugate condition for the incremental weight vector w\u2206 is designed as ( ( )) ( ) ( 1) 0 T w k H k w k\u2206 \u2206 + = (5)\nwhere the Hessian matrix ( )H k is calculated at the point ( )w k . The updating for ( )w k , in this case,\nis chosen as:\n( ) ( ) ( 1) ( 1)w k d k k w k\u03b1\u2206 = \u2212 + \u2212 \u2206 \u2212 (6)\nIn order to satisfy the conjugate condition between the vectors ( )w k\u2206 and ( 1)w k\u2206 + , the update\nprocedure for ( )k\u03b1 is expressed using the Fletcher-Reeves formulation, the Hestenes-Stiefel\nformulation and the Hestenes-Stiefel formulation given by equation 7, 8 and 9 respectively:\n( ( 1)) ( 1) ( )\n( ( )) ( )\nT\nT\nd k d k k\nd k d k \u03b1\n+ + =\n(7)\n( ( 1)) ( ( 1) ( )) ( )\n( ( )) ( )\nT\nT\nd k d k d k k\nd k d k \u03b1\n+ + \u2212 =\n(8)\n[ ( ) ( 1)] ( ) ( )\n( ( 1)) [ ( ) ( 1)]\nT\nT\nd k d k d k k\nw k d k d k \u03b1\n\u2212 \u2212 =\n\u2206 \u2212 \u2212 \u2212\n(9)\nIn fact, this algorithm exploits information about the direction of search for w\u2206 from the previous iteration in order to accelerate the convergence."}, {"heading": "3. MIXTURE OF EXPERTS (ME)", "text": "MLPs have been used successfully for solving different regression and classification problems. However, for large problems the parameter space of MLPs becomes huge and hence in training phase it becomes computationally intractable. To tackle this problem, one can take advantage of the principle of \u201cdivide and conquer\u201d. According to the divide and conquer approach, one can solve a complex task by dividing it into simple tasks and then combining the solutions of them appropriately. A well-known method that works based on this principle is ME. This method is in the category of dynamic classifiers combining where the input signal is directly involved in the mechanism that integrates the output of the experts into an overall result [8]. For the first time ME was proposed in [3]. Their proposed model contains a population of simple linear classifiers (the experts). Outputs of the experts are mixed using a gating network. Technically the experts perform supervised learning, since for modeling the desired response; outputs of individual experts are combined. The experts are also self-organize to find a good partitioning of the input space and each expert model has its own subspace, and combination of all experts model the input\nspace well. In [13, 14] this method was extended to the so-called \u201cHierarchical Mixture of Experts\u201d (HME). In HME instead of single experts, mixtures of experts\u2019 models are used for each component. The ME has been studied for a wide range of research [15-18]."}, {"heading": "3.1. Mixture of Experts based on Gradient Decent Based (GDME)", "text": "In this version of Mixture of Experts, in order to improve the performance of the expert networks, MLPs are used instead of linear networks or experts. The learning algorithm is modified using an estimation of the posterior probability of the desired output by each expert. So, the gating and expert networks match and this improves the proposed model to select the best expert(s). The weights of MLPs expert networks are updated based on those estimations and the procedure is repeated.\nAs mentioned before, the convergence rate of GD learning algorithms is slow and hence it becomes unsuitable for large problems. To solve this problem we use Conjugate Gradient method in learning process of ME."}, {"heading": "3.2. Mixture of Expert based on Conjugate Gradient (CGME)", "text": "In this section, we use Conjugate Gradient (CG) Methods for speeding up the learning phase of neural networks classifiers and gating network of ME. Each expert network is an MLP network\nwith one hidden layer that computes an output iO as a function of the input vector, x and weights of hidden and output layers and a sigmoid activation function. We assume that each expert\nspecializes in a specific area of the input space. The gating network assigns a weight ig to each of\nthe expert\u2019s output, iO . The gating network determines ig as a function of the input vector x and a set of parameters such as weights of its hidden and output layers. The activation function of the gating hidden layer is sigmoid, and in the output layer we use linear activation function. The softmax function was applied on the gating network outputs in order to make more diversity.\nThe ig can be interpreted as estimates of the prior probability that expert i can generate the desired output y .\nFigure1 The block diagram of CGME\nThe experts compete to learn the training patterns and the gating network mediates the\ncompetition. Thus the gating network computes g O , which is the output of the MLP layer of the\ngating network, then applies the softmax function to get:\n1\nexp( ) 1,....,\nexp( )\ng i\ni N\ng jj\nO g i N\nO =\n= =\n\u2211\n(10)\nwhere N is the number of expert networks, so ig s are nonnegative and sum up to 1. The final mixed output of the entire network is:\n1,...., T i i\ni\nO O g i N= =\u2211 (11)\nWe train MLPs using the Conjugate Gradient algorithm. For each expert i and the gating network, the weights are updated according to the following rules:\n( ) [ ( ) (1 ) ( 1) ( 1)]y e i i i i y yw k h y O O O k w k\u03b7 \u03b1\u2206 = \u2212 \u2212 \u2212 + \u2212 \u2206 \u2212 (12)\n( )( (1 )) (1 ) ( 1)( ) [ ( 1)]\u03b7 \u03b1\u2206 = \u2212 \u2212 \u2212 + \u2212 \u2206\u2212 \u2212 i i\nT h e i y i i i h h h h w h w y O O O O O k wk k (13)\n( ) ( ) (1 ) ( 1) ( 1) yg g g g yg yg\nw k h g O O k w k\u03b7 \u03b1\u2206 = \u2212 \u2212 \u2212 + \u2212 \u2206 \u2212 (14)\n( ) (1 ) ( 1)( ) (1 ) ( 1)\u03b7 \u03b1\u2206 = \u2212 \u2212 + \u2212 \u2206\u2212 \u2212 \u2212T hg g yg hg hgg g hg hg w w h g O O O O k wk k (15)\nwhere e \u03b7 and g \u03b7 are learning rates for the expert and the gating network, respectively. h w and\ny\nw are the weights of input to hidden and hidden to output layer, respectively, for experts\nhg w and yg w are the weights of input to hidden and hidden to output layer, respectively, for the\ngating network. T hi O and T hg O are the transpose of hi O and hg O which are the outputs of the\nhidden layer of expert and gating networks, respectively. ih is an estimation of the posterior probability that expert i can generate for the desired output y :\n1 exp( ( ) ( ))\n2 1\nexp( ( ) ( )) 2\nT\ni i i\ni\nT\nj j jj\ng y O y O\nh\ng y O y O\n\u2212 \u2212 \u2212\n=\n\u2212 \u2212 \u2212\u2211\n(16)\nAs pointed out in [19], in the network\u2019s learning process, the experts \u201ccompete\u201d for explaining the input while the gate network rewards the winner of each competition using larger feedbacks. Thus, the gate divides the input space according to the performance of experts."}, {"heading": "4. METAHEURISTIC ALGORITHM", "text": "Although CG converge faster than BP, but it remarkably depends on initial weights among neurons. If optimal weights are assigned to connections of neurons, it can be expected that the final performance increases. In order to tackle this problem, we applied a metaheuristic algorithm so called Modified Cuckoo Search.\nDuring the 1950s and 1960s, computer scientists investigated the possibility of applying the concepts of evolution as an optimization tool for engineers and this gave birth to a subclass of gradient free methods called genetic algorithms (GA) [20]. Since then many other algorithms have been developed that have been inspired by nature, for example particle swarm optimization (PSO) [21], differential evolution (DE) [22] and, more recently, the cuckoo search (CS) [23]. These are heuristic techniques which make use of a large population of possible is design at each iteration. For each member of the population, the objective function is evaluated and a fitness is assigned. A set of rules is then used to move the population towards the optimum solution. Here, we used a new version of CS, the so called Modified Cuckoo Search (MCS) which was introduced by Walton et al [24]. They showed that MCS is more reliable and faster than CS, GA, DE and PSO [24]."}, {"heading": "4.1. Cuckoo Search (CS)", "text": "CS is a metaheuristic search algorithm which has been recently proposed by Yang and Deb [26]. The algorithm is inspired by the reproduction strategy of cuckoos. At the most basic level, cuckoos lay their eggs in the nests of other host birds, which may be of different species. The host bird may discover that the eggs are not it\u2019s own and either destroy the egg or abandon the nest all together. This has resulted in the evolution of cuckoo eggs which mimic the eggs of local host birds [25]. To apply this as an optimization tool, Yang and Deb [26] used three idealized rules:\n\u2022 Each cuckoo lays one egg, which represents a set of solution co-ordinates, at a time and\ndumps it in a random nest;\n\u2022 A fraction of the nests containing the best eggs, or solutions, will carry over to the next\ngeneration;\n\u2022 The number of nests is fixed and there is a probability that a host can discover an alien\negg, say [0,1]ap \u2208 . If this happens, the host can either discard the egg or the nest and\nthese results in building a new nest in a new location.\nAlgorithm 1. Cuckoo Search (CS)\nInitialize a population of n host nests , 1, 2, ..., .x i ni = for all xi do\n1.1 Calculate fitness Fi = f(xi)\nend for while Number Objective Evaluations <Max Number Evaluations do\nGenerate a cuckoo egg ( ) j x by taking a L\u00e9vy flight from random nest\n( )jj F f x= .\nChoose a random nest I\nif ( j i F F> ) then\ni j x x\u2190\ni j\nF F\u2190\nend if\nAbandon a fraction ap of the worst nests Build new nests at new locations via L\u00e9vy flights to replace nests lost Evaluate fitness of new nests and rank all solutions\nend while\nThe steps involved in the CS are then derived from these rules and are shown in Algorithm 1 [23]. An important component of a CS is the use of L\u00e9vy flights for both local and global searching. The L\u00e9vy flight process, which has previously been used in search algorithms [27], is a random walk that is characterized by a series of instantaneous jumps chosen from a probability density function which has a power law tail Eq.7. -\n1 3L\u00e9vy u t \u03bb \u03bb\u223c = \u2264 \u2264 (17)\nThis process represents the optimum random search pattern and is frequently found in nature [28]. When generating a new egg in Algorithm 1, a L\u00e9vy flight is performed starting at the position of a randomly selected egg, if the objective function value at these new coordinates is better than another randomly selected egg then that egg is moved to this new position.\n( 1) ( ) ( ) t t\ni i x x L\u00e9vy\u03b1 \u03bb\n+ = + \u2295\n(18)\nThe scale of this random search is controlled by multiplying the generated L\u00e9vy flight by a step size\u03b1 . For example setting \u03b1 = 0.1 could be beneficial in problems of small domains, in the examples presented here \u03b1 = 1 is used in line with the work by Yang and Deb [26]. Yang and Deb [26] do not discuss boundary handling in their formulation, but an approach similar to PSO boundary handling [29] is adopted here. When a L\u00e9vy flight results in an egg location outside the bounds of the objective function, the fitness and position of the original egg are not changed. One\nof the advantages of CS over PSO is that only one parameter, the fraction of nests to abandon ap , needs to be adjusted. Yang and Deb [26] found that the convergence rate was not strongly\naffected by the value and they suggested setting ap = 0.25. The use of L\u00e9vy flights as the search method means that the CS can simultaneously find all optima in a design space and the method has been shown to perform well in comparison with PSO and GA [24]."}, {"heading": "4.2. Modified Cuckoo Search (MCS)", "text": "Given enough computation, the CS will always find the optimum [24] but, as the search relies entirely on random walks, a fast convergence cannot be guaranteed. In [25] two modifications to the method were made with the aim of increasing the convergence rate which leads to making the method more practical for a wider range of applications but without losing the attractive features of the original method. The first modification was made to the size of the L\u00e9vy flight step size \u03b1 . In CS, \u03b1 is constant and the value 1\u03b1 = is employed [26]. In the MCS, the value of \u03b1 decreases as the number of generations increases. This is done for the same reasons that the inertia constant is reduced in the PSO [22], i.e. to encourage more localized searching as the individuals, or the eggs, get closer to the solution. An initial value of the L\u00e9vy flight step size 1A = is chosen and, at each generation, a new L\u00e9vy flight step is calculated by using AG\u03b1 = , where G is the generation number. This exploratory search is only performed on the fraction of nests to be abandoned. The second modification is to add information exchange between the eggs in an attempt to speed up convergence to a minimum. In the CS, there is no information exchange between individuals and, essentially, the searches are performed independently. In the MCS, a fraction of the eggs with the best fitness are put into a group of top eggs. For each of the top eggs, a second egg in this group is picked at random and a new egg is then generated on the line connecting these two top eggs. The distance along this line at which the new egg is located is\ncalculated, using the inverse of the golden ratio (1 5) 2\u03d5 = + , such that it is closer to the egg\nwith the best fitness. In the case that both eggs have the same fitness, the new egg is generated at the midpoint. Whilst developing the method a random fraction was used in place of the golden ratio, it was found that the golden ratio showed significantly greater performance than a random fraction. There is a possibility that, in this step, the same egg is picked twice. In this case, a local L\u00e9vy flight search is performed from the randomly picked nest with step size 2A G\u03b1 = . The\nsteps involved in the modified cuckoo search are shown in detail in Algorithm 2. There are two parameters, the fraction of nests to be abandoned and the fraction of nests to make up the top nests, which need to be adjusted in the MCS. Through testing on benchmark problems, it was found that setting the fraction of nests to be abandoned to 0.75 and the fraction of nests placed in the top nests group to 0.25 yielded the best results over a variety of functions.\nAlgorithm 2. Modified Cuckoo Search (MCS)\nA MaxL\u00e9vyStepSize\u2190\nGoldenRatio\u03d5 \u2190\nInitialize a population of n host nests ( 1, 2, ..., )x i n i = for all ix do Calculate fitness ( )i iF f x= end for Generation Number 1G \u2190 While Number Objective Evaluations < Max Number Evaluations do\n1G G\u2190 + Sort nests by order of fitness for all nests to be abandoned do\nCurrent position ix\nCalculate L\u00e9vy flight step size A G\u03b1 \u2190\nPerform L\u00e9vy flight from ix to generate new egg kx\n( )\ni k\ni i\nx x\nF f x\n\u2190\n\u2190\nend for for all of the top nests do\nCurrent position ix Pick another nest from the top nests at random jx\nif i j x x= then\nCalculate L\u00e9vy flight step size 2A G\u03b1 \u2190\nPerform L\u00e9vy flight from ix to generate new egg kx ( )k kF f x= Choose a random nest l from all nests if ( k lF F> ) do\nl k\nl k\nx x\nF F\n\u2190\n\u2190\nend if else\ni j dx x x \u03d5= \u2212\nMove distance dx from the worst nest to the best nest to find kx ( )k kF f x= Choose a random nest l from all nests\nif ( k l F F> ) then\nl k\nl k\nx x\nF F\n\u2190\n\u2190\nend if\nend if\nend for\nend while"}, {"heading": "5. EXPERIMENTAL RESULTS", "text": "In order to evaluate the performance of the proposed method, we performed three set of experiments. In the first experiment, we employed hybrid MCS and CGME method in function approximation problem. In second experiment, we tested this method on an artificial data set, and in third experiment we evaluated this method on seven UCI data sets [30]."}, {"heading": "5.1. Static function approximation", "text": "For the sake of comparison, the underlying function to be approximated is a three-input nonlinear function which is widely used to verify the efficiency of proposed algorithms [31]: 0.5 1 1.5 2\n(1 )f x y z \u2212 \u2212 = + + + (19)\nThe training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6]\u00d7 \u00d7 and the corresponding target data. Other 250 testing samples are uniformly\nsampled from[2, 5] [2, 5] [2, 5]\u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and\nCGME in the function approximation task."}, {"heading": "5.2. Artificial Dataset", "text": "We consider a complex synthetic data set; this data set is generated using identical class of parametric distribution. In this data set, each class consists of three two-dimensional Gaussians:\n2\n2\n3 ( , )\n( , ) 1\nij ij\nij ij\nN\ni N j\nC \u00b5 \u03b4\n\u00b5 \u03b4\u2032 \u2032 =\n=U where 1, 2,3=i\n(20)\nThe classes are parameterized as follows: (6,1) (14,1) (18,1)\n1 (2,1) (3,1) (2,1){ , , }= N N N N N NC (21)\n(5,1) (10.5,1) (20,1) 2 ( 1,1) (3.5,1) (0,1){ , , }\u2212= N N N N N NC (22)\n(3,1) (12,1) (18,1) 3 (2,1) (6,1) ( 2,1){ , , }. N N N N N N C \u2212=\n(23)\nFigure 2 shows our synthetic data set and class boundaries. Further, we summarize our experimental results on each method in Table 2."}, {"heading": "5.3. UCI data sets", "text": "Seven UCI data sets [30] are used in the experiments. Information of these data sets is shown in Table 3.\nWe compared our proposed model with GDME and CGME. Each of these ensemble models comprises five identical MLP networks initialized with (different) random weights, four of which are used as experts and the fifth as the gating network. The MLPs and the gating network consist of 5 and 15 sigmoid neurons in the hidden layer respectively. The classification performance is measured using 10-fold cross validation. Three ensemble models are trained using 100 epochs\nand the learning rate values of experts and gating networks were set to 0.1 and 0.15 respectively. The results are summarized in Table 4. To examine the superiority of our proposed combining method to stand-alone MLP, a single MLP network is used as baseline (c.f. Table 4). This MLP consists of one hidden layer with 25 neurons so the complexity of this MLP is similar to that of CGME and GDME.\n(*In our experiment 10% of this data set is used)"}, {"heading": "6. CONCLUSION", "text": "In this paper, an extended version of ME, MCS-CGME is introduced. In our proposed method, CG optimization technique is employed in BP learning algorithm of experts and gating network in ME model in order to tackle the problems associated with GD method. Moreover, MCS is applied in order to initial optimal weights for NN and as a result, convergence rate and performance of model are increased. Experimental results on regression and classification benchmark datasets demonstrate the supremacy of our proposed method in comparison with GDME and CGME."}], "references": [{"title": "and R", "author": ["L.I. Kuncheva", "M. Skurichina"], "venue": "P. W. Duin, \u201cAn experimental study on diversity for bagging and boosting with linear classifiers,\u201d information Fusion, val. 3, pp. 245-258", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Combination of multiple classifiers using local accuracy estimates", "author": ["K. Woods", "W.P. Kegelmeyer", "K. Bowyer"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Adaptive mixture of experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.E. Nowlan", "G.E. Hinton"], "venue": "Neural Comput.Vol. 3 pp.79\u201387", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "A modified mixture of experts network structure for ECG beats classification with diverse features", "author": ["I. Guler", "E.D. Ubeyli"], "venue": "Eng. Appl. Artif. Intell. 18 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "A mixture of experts network structure construction algorithm for modelling and control", "author": ["X. Hong", "C.J. Harris"], "venue": "Appl. Intell. 16 (1) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Improved learning algorithms for mixture of experts in multiclass classification", "author": ["K. Chen", "L. Xu", "H. Chi"], "venue": "Neural Networks 12 (9) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1252}, {"title": "Face detection using mixture of MLP", "author": ["R. Ebrahimpour", "E. Kabir", "M.R. Yousefi"], "venue": "experts, Neural Process. Lett. 26 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural Networks- A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "Prentice-Hall, 2nd Edition", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning internal representations by error backpropagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "in: D.E. Rumelhart, J.L. McClelland (Eds.), Parallel Distributed Processing, vol. 1, The MIT Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1986}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": "Clarendon Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Understanding Neural Networks and Fuzzy Logic", "author": ["S.V. Kartalopoulos"], "venue": "IEEE Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Modified cuckoo search: A new gradient free optimisation algorithm. Chaos, Solitons& Fractals.Volume", "author": ["S. Walton", "O. Hassan", "K. Morgan", "R. Brown M"], "venue": "Issue", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Cuckoo search via L vy flights", "author": ["Yang X-S", "Deb S"], "venue": "Proceedings of World Congress on Nature & Biologically Inspired Computing (NaBIC", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Hierarchies of adaptive experts\u201d, In Proceedings of the advances in neural information processing systems", "author": ["R. Jordan R", "M. Jacobs"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Hierarchical mixtures of experts and the EM algorithm", "author": ["M. Jordan", "R. Jacobs"], "venue": "Neural Computing, vol. 2, pp. 181\u2013214", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Convergence results for the EM approach to mixtures of experts architectures", "author": ["M.I. Jordan", "L. Xu"], "venue": "Neural Networks, vol. 8, pp. 1409\u20131431", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical mechanics of the mixture of experts", "author": ["K. Kang", "J-H. Oh"], "venue": "Proceedings of Advances in Neural Information Processing Systems, vol. 9, pp. 183\u2013189", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Bayesian methods for mixture of experts", "author": ["S. Waterhouse", "D. MacKay", "T. Robinson"], "venue": "\u201d, In Proceedings of Advances in Neural Information Processing Systems, vol. 8., pp.351\u2013357", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Mixtures of experts estimate a posteriori probabilities.", "author": ["P. Moerland"], "venue": "Proceedings ofthe international conference on artificial neural networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Organization of face and object recognition in modular neural networks", "author": ["M.N. Dailey", "G.W. Cottrell"], "venue": "Neural Networks, vol.12, pp. 1053\u20131073", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "An introduction to genetic algorithms.sixth ed", "author": ["M. Mitchell"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Defining a standard for particle swarm optimization", "author": ["D Bratton", "J. Kennedy"], "venue": "Proceedings of the 2007 IEEE Swarm Intelligence Symposium,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Differential evolution \u2013 a simple and efficient heuristic for global optimization over continuous spaces", "author": ["R Storn", "K. Price"], "venue": "Journal of Global Optimisation", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Engineering optimisation by cuckoo search. International Journal of Mathematical Modelling and Numerical Optimisation 2010;1:330\u201343", "author": ["Yang X-S", "Deb S"], "venue": "Journal of Artificial Intelligence & Applications (IJAIA),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Cuckoo search via L \u0301evy flights", "author": ["Yang X-S", "Deb S"], "venue": "Proceedings of World Congress on Nature & Biologically Inspired Computing (NaBIC", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "\u0301evy flights and superdiffusion in the context of biological encounters and random searches", "author": ["L Viswanathan GM"], "venue": "Physics of Life Reviews", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "A particle swarm optimization approach for hexahedral mesh smoothing. International Journal for Numerical Methods in Fluids 2009;60:5578", "author": ["A EgemenYilmaz", "M. Kuzuoglu"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Newman,\u201dUCI Machine Learning Repository\u201d,[http://www.ics.uci.edu/~mlearn/MLRepository.html", "author": ["D.J.A. Asuncion"], "venue": "University of California, School of Information and Computer Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Handbook of pattern recognition and computer vision.3rd ed", "author": ["Chen CH", "Wang PSP"], "venue": "World Scientific;", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Previous experimental and theoretic results show that the combining classifiers with each other lead to higher performance when the base classifiers have small error rates, and their errors are different [1]; in other words, the base classifiers make uncorrelated decision in this case.", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "Generally, classifier selection and classifier fusion are two types of combining classifiers [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In the basic form of ME [3], the expert and gating networks are linear classifiers, however, for more", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "[7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.", "startOffset": 86, "endOffset": 96}, {"referenceID": 8, "context": "[7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.", "startOffset": 86, "endOffset": 96}, {"referenceID": 9, "context": "[7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.", "startOffset": 86, "endOffset": 96}, {"referenceID": 10, "context": "For instance, the convergence behavior of the BP algorithm highly depends on the choice of initial values of connection weights and other parameters used in the algorithm such as the learning rate and the momentum term [11, 8].", "startOffset": 219, "endOffset": 226}, {"referenceID": 7, "context": "For instance, the convergence behavior of the BP algorithm highly depends on the choice of initial values of connection weights and other parameters used in the algorithm such as the learning rate and the momentum term [11, 8].", "startOffset": 219, "endOffset": 226}, {"referenceID": 11, "context": "The MCS algorithm proposed by Walton et al [12] is an improved version of another metaheuristic algorithm the so called Cuckoo Search (CS) [13] and it can be used for initialing optimal values of connection weights.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "The MCS algorithm proposed by Walton et al [12] is an improved version of another metaheuristic algorithm the so called Cuckoo Search (CS) [13] and it can be used for initialing optimal values of connection weights.", "startOffset": 139, "endOffset": 143}, {"referenceID": 7, "context": "This method is in the category of dynamic classifiers combining where the input signal is directly involved in the mechanism that integrates the output of the experts into an overall result [8].", "startOffset": 190, "endOffset": 193}, {"referenceID": 2, "context": "For the first time ME was proposed in [3].", "startOffset": 38, "endOffset": 41}, {"referenceID": 12, "context": "In [13, 14] this method was extended to the so-called \u201cHierarchical Mixture of Experts\u201d (HME).", "startOffset": 3, "endOffset": 11}, {"referenceID": 13, "context": "The ME has been studied for a wide range of research [15-18].", "startOffset": 53, "endOffset": 60}, {"referenceID": 14, "context": "The ME has been studied for a wide range of research [15-18].", "startOffset": 53, "endOffset": 60}, {"referenceID": 15, "context": "The ME has been studied for a wide range of research [15-18].", "startOffset": 53, "endOffset": 60}, {"referenceID": 16, "context": "The ME has been studied for a wide range of research [15-18].", "startOffset": 53, "endOffset": 60}, {"referenceID": 17, "context": "As pointed out in [19], in the network\u2019s learning process, the experts \u201ccompete\u201d for explaining the input while the gate network rewards the winner of each competition using larger feedbacks.", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "During the 1950s and 1960s, computer scientists investigated the possibility of applying the concepts of evolution as an optimization tool for engineers and this gave birth to a subclass of gradient free methods called genetic algorithms (GA) [20].", "startOffset": 243, "endOffset": 247}, {"referenceID": 19, "context": "Since then many other algorithms have been developed that have been inspired by nature, for example particle swarm optimization (PSO) [21], differential evolution (DE) [22] and, more recently, the cuckoo search (CS) [23].", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "Since then many other algorithms have been developed that have been inspired by nature, for example particle swarm optimization (PSO) [21], differential evolution (DE) [22] and, more recently, the cuckoo search (CS) [23].", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "Since then many other algorithms have been developed that have been inspired by nature, for example particle swarm optimization (PSO) [21], differential evolution (DE) [22] and, more recently, the cuckoo search (CS) [23].", "startOffset": 216, "endOffset": 220}, {"referenceID": 22, "context": "Here, we used a new version of CS, the so called Modified Cuckoo Search (MCS) which was introduced by Walton et al [24].", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "They showed that MCS is more reliable and faster than CS, GA, DE and PSO [24].", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "CS is a metaheuristic search algorithm which has been recently proposed by Yang and Deb [26].", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "This has resulted in the evolution of cuckoo eggs which mimic the eggs of local host birds [25].", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "To apply this as an optimization tool, Yang and Deb [26] used three idealized rules:", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "\u2022 Each cuckoo lays one egg, which represents a set of solution co-ordinates, at a time and dumps it in a random nest; \u2022 A fraction of the nests containing the best eggs, or solutions, will carry over to the next generation; \u2022 The number of nests is fixed and there is a probability that a host can discover an alien egg, say [0,1] a p \u2208 .", "startOffset": 325, "endOffset": 330}, {"referenceID": 21, "context": "The steps involved in the CS are then derived from these rules and are shown in Algorithm 1 [23].", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "1 3 L\u00e9vy u t \u03bb \u03bb \u223c = \u2264 \u2264 (17) This process represents the optimum random search pattern and is frequently found in nature [28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "1 could be beneficial in problems of small domains, in the examples presented here \u03b1 = 1 is used in line with the work by Yang and Deb [26].", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Yang and Deb [26] do not discuss boundary handling in their formulation, but an approach similar to PSO boundary handling [29] is adopted here.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Yang and Deb [26] do not discuss boundary handling in their formulation, but an approach similar to PSO boundary handling [29] is adopted here.", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "Yang and Deb [26] found that the convergence rate was not strongly affected by the value and they suggested setting a p = 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "The use of L\u00e9vy flights as the search method means that the CS can simultaneously find all optima in a design space and the method has been shown to perform well in comparison with PSO and GA [24].", "startOffset": 192, "endOffset": 196}, {"referenceID": 22, "context": "Modified Cuckoo Search (MCS) Given enough computation, the CS will always find the optimum [24] but, as the search relies entirely on random walks, a fast convergence cannot be guaranteed.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "In [25] two modifications to the method were made with the aim of increasing the convergence rate which leads to making the method more practical for a wider range of applications but without losing the attractive features of the original method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In CS, \u03b1 is constant and the value 1 \u03b1 = is employed [26].", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "This is done for the same reasons that the inertia constant is reduced in the PSO [22], i.", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "In second experiment, we tested this method on an artificial data set, and in third experiment we evaluated this method on seven UCI data sets [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 28, "context": "For the sake of comparison, the underlying function to be approximated is a three-input nonlinear function which is widely used to verify the efficiency of proposed algorithms [31]: 0.", "startOffset": 176, "endOffset": 180}, {"referenceID": 0, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 127, "endOffset": 133}, {"referenceID": 5, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 127, "endOffset": 133}, {"referenceID": 0, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 134, "endOffset": 140}, {"referenceID": 5, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 141, "endOffset": 147}, {"referenceID": 5, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 141, "endOffset": 147}, {"referenceID": 1, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 52, "endOffset": 58}, {"referenceID": 4, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 52, "endOffset": 58}, {"referenceID": 1, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 59, "endOffset": 65}, {"referenceID": 4, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 59, "endOffset": 65}, {"referenceID": 1, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 66, "endOffset": 72}, {"referenceID": 4, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 66, "endOffset": 72}, {"referenceID": 27, "context": "Seven UCI data sets [30] are used in the experiments.", "startOffset": 20, "endOffset": 24}], "year": 2012, "abstractText": "This paper investigates a new method for improving the learning algorithm of Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS) and Conjugate Gradient (CG) as a second order optimization technique. The CG technique is combined with Back-Propagation (BP) algorithm to yield a much more efficient learning algorithm for ME structure. In addition, the experts and gating networks in enhanced model are replaced by CG based Multi-Layer Perceptrons (MLPs) to provide faster and more accurate learning. The CG is considerably depends on initial weights of connections of Artificial Neural Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo Search is applied in order to select the optimal weights. The performance of proposed method is compared with Gradient Decent Based ME (GDME) and Conjugate Gradient Based ME (CGME) in classification and regression problems. The experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster convergence and better performance in utilized benchmark data sets.", "creator": "PScript5.dll Version 5.2.2"}}}