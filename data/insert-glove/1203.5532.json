{"id": "1203.5532", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2012", "title": "On the Use of Non-Stationary Policies for Infinite-Horizon Discounted Markov Decision Processes", "abstract": "gumla We comets consider infinite - hederman horizon autechre discounted Markov nezir Decision Processes, 19.88 for babolsar which it sh\u012b is known awepa that dim there exists fortune-telling a stationary marlou optimal baturina policy. jean-marc We dinsamo consider strangeland the cosmogonic algorithm Value Iteration limning and chelios the egnatius sequence novar of nuuk policies $ \\ micromasters pi_1, .. ., \\ turjeman pi_k $ roha it 2048 gen tomatometer erates birtwistle until gelernt some touchline iteration $ rightwards k $. We giugliano provide performance jadriya bounds efficacious for non - stationary 1992 policies fewer involving brokerages the oliseh last $ vincour m $ \u0113l generated 460th policies atrs that reduce ravenhead the monklands state - jarno of - the - art poitras bound for tryscoring the muyembe last tenochtitl\u00e1n stationary uygur policy $ \\ lenn\u00e9 pi_k $ khadafi by lemony a shalah factor $ \\ correctable frac {500-mile 1 - \\ goest gamma} {1 - \\ wilsonian gamma ^ sheresky m} $. In other words, and toady contrary disincentive to readmission a common hitherto intuition, we quatorze show sudhakar that it may be much mouthfeel easier mondego to epiblast find a redlist non - stationary 8,000-ton approximately - healthcorp optimal ippar policy canseco than a virginijus stationary one.", "histories": [["v1", "Sun, 25 Mar 2012 19:44:41 GMT  (4kb)", "https://arxiv.org/abs/1203.5532v1", "(2012)"], ["v2", "Fri, 30 Mar 2012 18:18:05 GMT  (19kb)", "http://arxiv.org/abs/1203.5532v2", null]], "COMMENTS": "(2012)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bruno scherrer"], "accepted": false, "id": "1203.5532"}, "pdf": {"name": "1203.5532.pdf", "metadata": {"source": "CRF", "title": "On the Use of Non-Stationary Policies for Infinite-Horizon Discounted Markov Decision Processes", "authors": ["Bruno Scherrer"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n20 3.\n55 32\nv2 [\ncs .A\nI] 3\n0 M\nar 2\n01 2\nWe consider infinite-horizon \u03b3-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. We consider the algorithm Value Iteration and the sequence of policies \u03c01, . . . , \u03c0k it implicitely generates until some iteration k. We provide performance bounds for non-stationary policies involving the last m generated policies that reduce the state-of-the-art bound for the last stationary policy \u03c0k by a factor 1\u2212\u03b3\n1\u2212\u03b3m . In particular, the use of non-stationary policies allows\nto reduce the usual asymptotic performance bounds of Value Iteration with errors bounded by \u01eb at each iteration from \u03b3 (1\u2212\u03b3)2 \u01eb to \u03b3 1\u2212\u03b3 \u01eb, which is significant in the usual situation when \u03b3 is close to 1. Given Bellman operators that can only be computed with some error \u01eb, a surprising consequence of this result is that the problem of \u201ccomputing an approximately optimal non-stationary policy\u201d is much simpler than that of \u201ccomputing an approximately optimal stationary policy\u201d, and even slightly simpler than that of \u201capproximately computing the value of some fixed policy\u201d, since this last problem only has a guarantee of 1 1\u2212\u03b3 \u01eb.\nGiven a Markov Decision Process, suppose on runs an approximate version of Value Iteration, that is one builds a sequence of value-policy pairs as follows:\nPick any \u03c0k+1 in Gvk\nvk+1 = T\u03c0k+1vk + \u01ebk+1\nwhere v0 is arbitrary, Gvk is the set of policies that are greedy 1 with respect to vk, and T\u03c0k is the linear Bellman operator associated to policy \u03c0k. Though it does not appear exactly in this form in the literature, the following performance bound is somewhat standard.\nTheorem 1. Let \u01eb = max1\u2264j<k \u2016\u01ebj\u2016sp be a uniform upper bound on the span seminorm 2 of the errors before iteration k. The loss of policy \u03c0k is bounded as follows:\n\u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 1\n1\u2212 \u03b3\n(\n\u03b3 \u2212 \u03b3k\n1\u2212 \u03b3 \u01eb+ \u03b3k\u2016v\u2217 \u2212 v0\u2016sp\n)\n. (1)\nIn Theorem 2, we will prove a generalization of this result, so we do not provide a proof here. Since for any f , \u2016f\u2016sp \u2264 2\u2016f\u2016\u221e, Theorem 1 constitutes a slight improvement and a (finite-iteration) generalization of the following well-known performance bound (see [1]):\nlim sup k\u2192\u221e\n\u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 2\u03b3\n(1\u2212 \u03b3)2 max k \u2016\u01ebk\u2016\u221e.\n1There may be several greedy policies with respect to some value v, and what we write here holds whichever one is picked. 2For any function f defined on the state space, the span seminorm of f is \u2016f\u2016sp = maxs f(s) \u2212 mins f(s). The motivation for using the span seminorm instead of a more usual L\u221e-norm is twofold: 1) it slightly improves on the state-of-the-art bounds and 2) it simplifies the construction of an example in the proof of the forthcoming Proposition 1.\nAsymptotically, the above bounds involve a \u03b3(1\u2212\u03b3)2 constant that may be really big when \u03b3 is close to 1. Compared to a value-iteration algorithm for approximately computing the value of some fixed policy, and for which one can prove a dependency of the form 11\u2212\u03b3 \u01eb, there is an extra term \u03b3 1\u2212\u03b3 that suggests that the problem of \u201ccomputing an approximately optimal policy\u201d is significantly harder than that of \u201capproximately computing the value of some fixed policy\u201d. To our knowledge, there does not exist any example in the literature that supports the tightness of the above mentionned bounds. The following proposition shows that the bound of Theorem 1 is in fact tight.\nProposition 1. For all \u01eb \u2265 0, \u2206 \u2265 0, and k > 0, there exists a k+1-state MDP, an initial value v0 such that \u2016v\u2217 \u2212 v0\u2016sp = \u2206, a sequence of noise terms (\u01ebj) with \u2016\u01ebj\u2016sp \u2264 \u01eb, such that running Value Iteration during iterations with errors (\u01ebj) outputs a value function vk\u22121 of which a greedy policy \u03c0k satisfies Equation (1) with equality.\nProof. Consider the deterministic MDP of Figure 1. The only decision is in state sk, where one can stay with reward r = \u2212 \u03b3\u2212\u03b3 k\n1\u2212\u03b3 \u01eb \u2212 \u03b3 k\u2206 or move to sk\u22121 with 0 reward. All other transitions give 0 reward. Thus,\nthere are only two policies, the optimal policy \u03c0\u2217 with value equal to 0, and a policy \u03c0\u0304 for which the value in sk is r 1\u2212\u03b3 . Take\nv0(sl) =\n{\n\u2212\u2206 if l = 0 0 else\nand for all j < k, \u01ebj(sl) =\n{\n\u2212\u01eb if j = l 0 else.\nBy induction, it can be seen that for all j \u2208 {1, 2, . . . , k \u2212 1},\nvj(sl) =\n{\n\u2212\u01eb\u2212 \u03b3\u01eb\u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03b3j\u22121\u01eb\u2212 \u03b3j\u2206 = \u2212 1\u2212\u03b3 j\n1\u2212\u03b3 \u01eb\u2212 \u03b3 j\u2206 if j = l\n0 if j < l \u2264 k\nSince \u03b3vk\u22121(sk\u22121) = r and vk\u22121(sk) = 0, both policies are greedy with respect to vk\u22121, and the bound of Equation (1) holds with equality for \u03c0\u0304.\nInstead of running the last stationary policy \u03c0k, one may consider running a periodic non-stationary policy, which is made of the last m policies. The following theorem shows that it is indeed a good idea.\nTheorem 2. Let \u03c0k,m be the following policy\n\u03c0k,m = \u03c0k \u03c0k\u22121 \u00b7 \u00b7 \u00b7 \u03c0k\u2212m+1 \u03c0k \u03c0k\u22121 \u00b7 \u00b7 \u00b7 .\nThen its performance loss is bounded as follows:\n\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e \u2264 1\n1\u2212 \u03b3m\n(\n\u03b3 \u2212 \u03b3k\n1\u2212 \u03b3 \u01eb+ \u03b3k\u2016v\u2217 \u2212 v0\u2016sp\n)\n.\nWhen m = 1, one exactly recovers the result of Theorem 1. For general m, this new bound is a factor 1\u2212\u03b3 1\u2212\u03b3m better than the usual bound. Taking m = k, that is considering all the policies generated from the very start, one obtains the following bound:\n\u2016v\u2217 \u2212 v\u03c0k,k\u2016\u221e \u2264\n(\n\u03b3\n1\u2212 \u03b3 \u2212\n\u03b3k\n1\u2212 \u03b3k\n)\n\u01eb+ \u03b3k\n1\u2212 \u03b3k \u2016v\u2217 \u2212 v0\u2016sp.\nthat tends to \u03b31\u2212\u03b3 \u01eb when k tends to \u221e. In other words, we can see here that the problem of \u201ccomputing a (non stationary) approximately-optimal policy\u201d is not harder than that of \u201ccomputing approximately the value of some fixed policy\u201d. Since the respective asymptotic errors are \u03b31\u2212\u03b3 \u01eb and 1 1\u2212\u03b3 \u01eb, it seems even simpler !\nProof of Theorem 2. The value of \u03c0k,m satisfies:\nv\u03c0k,m = T\u03c0kT\u03c0k\u22121 \u00b7 \u00b7 \u00b7T\u03c0k\u2212m+1v\u03c0k,m . (2)\nBy induction, it can be shown that the sequence of values generated by the algorithm satisfies:\nT\u03c0kvk\u22121 = T\u03c0kT\u03c0k\u22121 \u00b7 \u00b7 \u00b7T\u03c0k\u2212m+1vk\u2212m + m\u22121 \u2211\ni=1\n\u0393k,i\u01ebk\u2212i (3)\nwhere \u0393k,i = P\u03c0kP\u03c0k\u22121 \u00b7 \u00b7 \u00b7P\u03c0k\u2212i+1\nin which, for all \u03c0, P\u03c0 denotes the stochastic matrix associated to policy \u03c0. By substracting Equations (3) and (2), one obtains:\nT\u03c0kvk\u22121 \u2212 v\u03c0k,m = \u0393k,m(vk\u2212m \u2212 v\u03c0k,m) +\nm\u22121 \u2211\ni=1\n\u0393k,i\u01ebk\u2212i\nand by taking the norm\n\u2016T\u03c0kvk\u22121 \u2212 v\u03c0k,m\u2016\u221e = \u03b3 m\u2016vk\u2212m \u2212 v\u03c0k,m\u2016\u221e +\n\u03b3 \u2212 \u03b3m\n1\u2212 \u03b3 \u01eb\u221e (4)\nwhere \u01eb\u221e = max1\u2264j\u2264k \u2016\u01ebj\u2016\u221e. Essentially, Equation (4) shows that for sufficiently big m, T\u03c0kvk\u22121 is an \u03b3 1\u2212\u03b3 \u01eb approximation of the value of the non-stationary policy \u03c0k,m (whereas in general, it may be a much poorer approximation of the value of the stationary policy \u03c0k.\nBy induction, it can also be proved that\n\u2016v\u2217 \u2212 vk\u2016\u221e \u2264 \u03b3 k\u2016v\u2217 \u2212 v0\u2016\u221e +\n1\u2212 \u03b3k\n1\u2212 \u03b3 \u01eb\u221e. (5)\nUsing the fact that \u2016T\u03c0\u2217v\u2217 \u2212 T\u03c0kvk\u22121\u2016\u221e \u2264 \u03b3\u2016v\u2217 \u2212 vk\u22121\u2016\u221e since \u03c0\u2217 (resp. \u03c0k) is greedy with respect to v\u2217 (resp. vk\u22121), as well as Equations (4) and (5), we can conclude by observing that\n\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e \u2264 \u2016T\u03c0\u2217v\u2217 \u2212 T\u03c0kvk\u22121\u2016\u221e + \u2016T\u03c0kvk\u22121 \u2212 v\u03c0k,m\u2016\u221e\n\u2264 \u03b3\u2016v\u2217 \u2212 vk\u22121\u2016\u221e + \u03b3 m\u2016vk\u2212m \u2212 v\u03c0k,m\u2016\u221e +\n\u03b3 \u2212 \u03b3m\n1\u2212 \u03b3 \u01eb\u221e\n\u2264 \u03b3\n(\n\u03b3k\u22121\u2016v\u2217 \u2212 v0\u2016\u221e + 1\u2212 \u03b3k\u22121\n1\u2212 \u03b3 \u01eb\u221e\n)\n+ \u03b3m ( \u2016vk\u2212m \u2212 v\u2217\u2016\u221e + \u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e )\n+ \u03b3 \u2212 \u03b3m\n1\u2212 \u03b3 \u01eb\u221e\n\u2264 \u03b3k\u2016v\u2217 \u2212 v0\u2016\u221e + \u03b3 \u2212 \u03b3k\n1\u2212 \u03b3 \u01eb\u221e\n+ \u03b3m ( \u03b3k\u2212m\u2016v\u2217 \u2212 v0\u2016\u221e + 1\u2212 \u03b3k\u2212m\n1\u2212 \u03b3 \u01eb\u221e + \u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e\n)\n+ \u03b3 \u2212 \u03b3m\n1\u2212 \u03b3 \u01eb\u221e\n= \u03b3m\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e + 2\u03b3 k\u2016v\u2217 \u2212 v0\u2016\u221e +\n2(\u03b3 \u2212 \u03b3k)\n1\u2212 \u03b3 \u01eb\u221e.\nAdding a constant to the value vj at any step j of the algorithm does not affect the greedy policy set Gvj and only adds a constant to the next value vj+1. As a consequence, we can assume witout loss of generality that \u2016v\u2217 \u2212 v0\u2016sp = 2\u2016v\u2217 \u2212 v0\u2016\u221e, \u2016\u01ebj\u2016sp = 2\u2016\u01ebj\u2016\u221e and the result follows.\nFrom a bibliographical point of view, the idea of using non-stationary policies to improve error bounds already appears in [2]. However, in these works, the author considers finite-horizon problems where the policy to be computed is naturally non-stationary. The fact that non-stationary policies (that loop over the last m computed policies) can also be useful in an infinite horizon context is to our knowledge new.\nAcknowledgements I thank Boris Lesner for pointing out a flaw in a previous temptative proof of Proposition 1."}], "references": [{"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["S.M. Kakade"], "venue": "PhD thesis, University College London", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Since for any f , \u2016f\u2016sp \u2264 2\u2016f\u2016\u221e, Theorem 1 constitutes a slight improvement and a (finite-iteration) generalization of the following well-known performance bound (see [1]): lim sup k\u2192\u221e \u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 2\u03b3 (1\u2212 \u03b3)2 max k \u2016\u01ebk\u2016\u221e.", "startOffset": 167, "endOffset": 170}], "year": 2012, "abstractText": "We consider infinite-horizon \u03b3-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. We consider the algorithm Value Iteration and the sequence of policies \u03c01, . . . , \u03c0k it implicitely generates until some iteration k. We provide performance bounds for non-stationary policies involving the last m generated policies that reduce the state-of-the-art bound for the last stationary policy \u03c0k by a factor 1\u2212\u03b3 1\u2212\u03b3m . In particular, the use of non-stationary policies allows to reduce the usual asymptotic performance bounds of Value Iteration with errors bounded by \u01eb at each iteration from \u03b3 (1\u2212\u03b3)2 \u01eb to \u03b3 1\u2212\u03b3 \u01eb, which is significant in the usual situation when \u03b3 is close to 1. Given Bellman operators that can only be computed with some error \u01eb, a surprising consequence of this result is that the problem of \u201ccomputing an approximately optimal non-stationary policy\u201d is much simpler than that of \u201ccomputing an approximately optimal stationary policy\u201d, and even slightly simpler than that of \u201capproximately computing the value of some fixed policy\u201d, since this last problem only has a guarantee of 1 1\u2212\u03b3 \u01eb. Given a Markov Decision Process, suppose on runs an approximate version of Value Iteration, that is one builds a sequence of value-policy pairs as follows: Pick any \u03c0k+1 in Gvk vk+1 = T\u03c0k+1vk + \u01ebk+1 where v0 is arbitrary, Gvk is the set of policies that are greedy 1 with respect to vk, and T\u03c0k is the linear Bellman operator associated to policy \u03c0k. Though it does not appear exactly in this form in the literature, the following performance bound is somewhat standard. Theorem 1. Let \u01eb = max1\u2264j<k \u2016\u01ebj\u2016sp be a uniform upper bound on the span seminorm 2 of the errors before iteration k. The loss of policy \u03c0k is bounded as follows: \u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 1 1\u2212 \u03b3 ( \u03b3 \u2212 \u03b3 1\u2212 \u03b3 \u01eb+ \u03b3\u2016v\u2217 \u2212 v0\u2016sp ) . (1) In Theorem 2, we will prove a generalization of this result, so we do not provide a proof here. Since for any f , \u2016f\u2016sp \u2264 2\u2016f\u2016\u221e, Theorem 1 constitutes a slight improvement and a (finite-iteration) generalization of the following well-known performance bound (see [1]): lim sup k\u2192\u221e \u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 2\u03b3 (1\u2212 \u03b3)2 max k \u2016\u01ebk\u2016\u221e. 1There may be several greedy policies with respect to some value v, and what we write here holds whichever one is picked. 2For any function f defined on the state space, the span seminorm of f is \u2016f\u2016sp = maxs f(s) \u2212 mins f(s). The motivation for using the span seminorm instead of a more usual L\u221e-norm is twofold: 1) it slightly improves on the state-of-the-art bounds and 2) it simplifies the construction of an example in the proof of the forthcoming Proposition 1.", "creator": "LaTeX with hyperref package"}}}