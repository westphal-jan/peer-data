{"id": "1204.2741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2012", "title": "Simultaneous Object Detection, Tracking, and Event Recognition", "abstract": "The common camelo internal structure ryamizard and menzi algorithmic b-25s organization churchfield of necesary object aliens detection, 32-year-old detection - dendur based corps. tracking, homologies and 1,350,000 event recognition facilitates burkinabe a navara general maglia approach sportszone to integrating lipopolysaccharide these romanced three rtoday components. dansk This commanders supports multidirectional information flow 36.78 between berrard these components materi allowing object detection to 1961-63 influence failures tracking and event recognition kazu and 12-team event 92nd recognition to influence 6.57 tracking and object phedre detection. The performance womenpriests of diffused the combination zaki can camorra exceed cosmonaut the performance compuserve of antyukh the components in isolation. This can be done inexpedient with linear asymptotic movsesian complexity.", "histories": [["v1", "Thu, 12 Apr 2012 14:47:41 GMT  (2906kb,D)", "http://arxiv.org/abs/1204.2741v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["andrei barbu", "aaron michaux", "siddharth narayanaswamy", "jeffrey mark siskind"], "accepted": false, "id": "1204.2741"}, "pdf": {"name": "1204.2741.pdf", "metadata": {"source": "CRF", "title": "Simultaneous Object Detection, Tracking, and Event Recognition", "authors": ["Andrei Barbu", "Aaron Michaux", "Siddharth Narayanaswamy", "Jeffrey Mark Siskind"], "emails": [], "sections": [{"heading": null, "text": "The common internal structure and algorithmic organization of object detection, detection-based tracking, and event recognition facilitates a general approach to integrating these three components. This supports multidirectional information flow between these components allowing object detection to influence tracking and event recognition and event recognition to influence tracking and object detection. The performance of the combination can exceed the performance of the components in isolation. This can be done with linear asymptotic complexity."}, {"heading": "1 Introduction", "text": "Many common approaches to event recognition (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005) classify events based on their motion profile. This requires detecting and tracking the event participants. Adaptive approaches to tracking (Yilmaz et al., 2006), e.g. Kalman filtering (Comaniciu et al., 2003), suffer from three difficulties that impact their utility for event recognition. First, they must be initialized. One cannot initialize on the basis of motion since many event participants move only for a portion of the event, and sometimes not at all. Second, they exhibit drift and often must be\nAdditional images and videos as well as all code and datasets are available at http://engineering.purdue. edu/\u02dcqobi/arxiv2012a.\nperiodically reinitialized to compensate. Third, they have difficulty tracking small, deformable, or partially occluded objects as well as ones whose appearance changes dramatically. This is particularly of concern since many events, e.g. picking things up, involve humans interacting with objects that are sufficiently small for humans to grasp and where such interaction causes appearance change by out-of-plane rotation, occlusion, or deformation.\nDetection-based tracking is an alternate approach that attempts to address these issues. In detection-based tracking an object detector is applied to each frame of a video to yield a set of candidate detections which are composed into tracks by selecting a single candidate detection from each frame that maximizes temporal coherency of the track. However, current object detectors are far from perfect. On the PASCAL VOC Challenge, they typically achieve average precision scores of 40% to 50% (Everingham et al., 2010). Directly applying such detectors on a per-frame basis would be ill-suited to event recognition. Since the failure modes include both false positives and false negatives, interpolation does not suffice to address this shortcoming. A better approach is to combine object detection and tracking with a single objective function that maximizes temporal coherency to allow object detection to inform the tracker and vice versa.\nOne can carry this approach even further and integrate event recognition with both object detection and tracking. One way to do this is to incorporate coherence with a target event model into the temporal coherency measure. For example, a top-down expectation of observing a pick up event can bias the object detector and tracker to search for event\nar X\niv :1\n20 4.\n27 41\nv1 [\ncs .C\nparticipants that exhibit the particular joint motion profile of that event: an object in close proximity to the agent, the object starting out at rest while the agent approaches the object, then the agent touching the object, followed by the object moving with the agent. Such information can also flow bidirectionally. Mutual detection of a baseball bat and a hitting event can be easier than detecting each in isolation or having a fixed direction of information flow.\nThe common internal structure and algorithmic organization of current object detectors (Felzenszwalb et al., 2010a,b), detection-based trackers (Wolf et al., 1989), and HMM-based approaches to event recognition (Baum and Petrie, 1966) facilitates a general approach to integrating these three components. We demonstrate an approach to integrating object detection, tracking, and event recognition and show how it improves each of the these three components in isolation. Further, while prior detection-based trackers exhibit quadratic complexity, we show how such integration can be fast, with linear asymptotic complexity."}, {"heading": "2 Detection-based tracking", "text": "The methods described in sections 4, 5, and 6 extend a popular dynamic-programming approach to detection-based tracking. We review that approach here to set forth the concepts, terminology, and notation that will be needed to describe the extensions.\nDetection-based tracking is a general framework where an object detector is applied to each frame of a video to yield a set of candidate detections which are composed into tracks by selecting a single candidate detection from each frame that maximizes temporal coherency of the track. This general framework can be instantiated with answers to the following questions:\n1. What is the representation of a detection? 2. What is the detection source? 3. What is the measure of temporal coherency? 4. What is the procedure for finding the track with max-\nimal temporal coherency?\nWe answer questions 1 and 2 by taking a detection to be a scored axis-aligned rectangle (box), such as produced by the Felzenszwalb et al. (2010a,b) object detectors, though our approach is compatible with any method for producing scored axis-aligned rectangular detections. If btj denotes the jth detection in frame t, f(btj) denotes the score of that detection, T denotes the number of frames, and j = \u3008j1, . . . , jT \u3009 denotes a track comprising the jtth detection in frame t, we answer question 3 by formulating temporal coherency of a track j = \u3008j1, . . . , jT \u3009 as:\nmax j1,...,jT T\u2211 t=1 f(btjt) + T\u2211 t=2 g(bt\u22121jt\u22121 , b t jt) (1)\nwhere g scores the local temporal coherency between de-\ntections in adjacent frames. We take g to be the negative Euclidean distance between the center of btjt and the center of bt\u22121jt\u22121 projected forward one frame, though, as discussed below, our approach is compatible with a variety of functions discussed by Felzenszwalb and Huttenlocher (2004). The forward projection internal to g can be done in a variety of ways including optical flow and the Kanade-LucasTomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker. We answer question 4 by observing that Eq. 1 can be optimized in polynomial time with the Viterbi algorithm (Viterbi, 1971):\nfor j = 1 to J1 do \u03b41j := f(b1j ) for t = 2 to T do for j = 1 to Jt\ndo \u03b4tj := f(btj) + Jt\u22121 max j\u2032=1 g(bt\u22121j\u2032 , b t j) + \u03b4 t\u22121 j\u2032\n(2)\nwhere Jt is the number of detections in frame t. This leads to a lattice as shown in Fig. 1.\nDetection-based trackers exhibit less drift than adaptive approaches to tracking due to fixed target models. They also tend to perform better than simply picking the best detection in each frame. The reason is that one can allow the detection source to produce multiple candidates and use the combination of the detection score f and the adjacentframe temporal-coherency score g to select the track. The essential attribute of detection-based tracking is that g can overpower f to assemble a more coherent track out of weaker detections. The nonlocal nature of Eq. 1 can allow more-reliable tracking with less-reliable detection sources.\nA crucial practical issue arises: How many candidate detections should be produced in each frame? Producing too few may risk failing to produce the desired detection that is necessary to yield a coherent track. In the limit, it is impossible to construct any track if even a single frame lacks any detections. The current state-of-the-art in object detection is unable to simultaneously achieve high precision\nOne can ameliorate this somewhat by constructing a lattice that skips frames (Sala et al., 2010). This increases the asymp-\nand recall and thus it is necessary to explore the trade-off between the two (Everingham et al., 2010). A detectionbased tracker can bias the detection source to yield higher recall at the expense of lower precision and rely on temporal coherency to compensate for the resulting lower precision. This can be done in at least three ways. First, one can depress the detection-source acceptance thresholds. One way this can be done with the Felzenszwalb et al. detectors is to lower the trained model thresholds. Second, one can pool the detections output by multiple detection sources with complementary failure modes. One way this can be done is by training multiple models for people in different poses. Third, one can use adaptive-tracking methods to project detections forward to augment the raw detector output and compensate for detection failure in subsequent frames. This can be done in a variety of ways including optical flow and KLT. The essence of our paper is a more principled collection of approaches for compensating for low recall in the object detector.\nA practical issue arises when pooling the detections output by multiple detection sources. It is necessary to normalize the detection scores for such pooled detections by a per-model offset. One can derive an offset by computing a histogram of scores of the top detection in each frame of a video and taking the offset to be the minimum of the value that maximizes the between-class variance (Otsu, 1979) when bipartitioning this histogram and the trained acceptance threshold offset by a small but fixed amount.\nThe operation of a detection-based tracker is illustrated in Fig. 2. This example demonstrates several things of note. First, reliable tracks are produced despite an unreliable detection source. Second, the optimal track contains detections with suboptimal score. Row (b) demonstrates that selecting the top-scoring detection does not yield a temporally-coherent track. Third, forward-projection of detections from the second to third column in row (c) compensates for the lack of raw detections in the third column of row (a).\nDetection-based tracking runs in time O(TJ2) on videos of length T with J detections per frame. In practice, the run time is dominated by the detection process and the dynamic-programming step. Limiting J to a small number speeds up the tracker considerably while minimally impacting track quality. We further improve the speed of the detectors when running many object classes by factoring the computation of the HOG pyramid."}, {"heading": "3 Evaluation of detection-based tracking", "text": "We evaluated detection-base tracking using the year-one (Y1) corpus produced by DARPA for the Mind\u2019s Eye pro-\ntotic complexity to be exponential in the number of frame skips allowed.\ngram. These videos are provided at 720p@30fps and range from 42 to 1727 frames in length, with an average of 438.84 frames, and depict people interacting with a variety of objects to enact common English verbs.\nFour Mind\u2019s Eye teams (University at Buffalo, Corso 2011, Stanford Research Institute, Bui 2011, University of California at Berkeley, Saenko 2011, and University of Southern California, Navatia 2011) independently produced human-annotated tracks for different portions of Y1. We used these sources of human-annotated tracks to evaluate the performance of detection-based tracking by computing human-human intercoder agreement between all pairs of the four sources of human-annotated tracks and human-machine intercoder agreement between a detectionbased tracker and all four of these sources. Since each team annotated different portions of Y1, each such intercoder agreement measure was computed only over the N videos shared by each pair, as reported in Table 1(a). One team (University at Buffalo, Corso 2011) annotated detections as clusters of quadrilaterals around object parts. These were converted to a single bounding box.\nDifferent teams labeled the tracks with different class labels. It was possible to determine from these labels whether the track was for a person or nonperson by assuming that the labels \u2018person\u2019 and \u2018human\u2019, and only those labels, denoted person tracks, but it was not possible to automatically make finer-grained class comparisons. Thus we independently compared person tracks with person tracks and nonperson tracks with nonperson tracks. When comparing an annotation u of a video n containing lun person tracks with an annotation v of that same video containing lvn person tracks, we compared blun, lvnc person tracks. We selected the best over all blun, lvnc! permutation mappings \u03c1n between person tracks in u and person tracks in v. A permutation mapping was preferred when it had higher average overlap score among corresponding boxes across the tracks and the frames in a video, where the overlap score was that used by the PASCAL VOC Challenge (Everingham et al., 2010), namely the ratio of the area of their intersection to the area of their union. Different tracks could annotate different frames of a video. When comparing such, we considered only the shared frames.\nFor every pair of teams, we computed the mean and standard deviation of the overlap score across all shared frames in all tracks in the best permutation mappings for all shared videos. The averaging process used both to determine the best permutation mapping for each video pair and to determine overall mean and standard deviation measures weighted each overlap score equally. More precisely, if 1 \u2264 n \u2264 N , 1 \u2264 l \u2264 blun, lvnc denotes a shared track for video n, T ln denotes the set of shared frames for that shared track l in video n, Unt and V n t denote the vector of boxes for frame t in video n for annotations u and v respectively, andO denotes the overlap measure, we score a permutation\nmapping \u03c1n for video n as:\n1 blun,l v nc\u2211\nl=1\n|T ln|\nblun,l v nc\u2211\nl=1 \u2211 t\u2208T ln O(\u03c1n(U n t )[l], V n t [l])\nand computed the mean overlap for a pair of teams as:\n1\nN\u2211 n=1 blun,l v nc\u2211 l=1 |T ln|\nN\u2211 n=1 blun,l v nc\u2211 l=1 \u2211 t\u2208T ln O(\u03c1n(U n t )[l], V n t [l])\nwith an analogous computation for standard deviation and nonperson tracks.\nThe overall mean and standard deviation measures, reported in Table 1(b,c), indicate that the mean humanhuman overlap is only marginally greater than the mean human-machine overlap by about one standard deviation. This suggests that improvement in tracker performance is\nunlikely to lead to significant improvement in action recognition performance and sentential description quality."}, {"heading": "4 Combining object detection and tracking", "text": "While detection-based tracking is resilient to low precision, it requires perfect recall; it cannot generate a track through a frame that has no detections and it cannot generate a track through a portion of the field of view which has no detections regardless of how good the temporal-coherence of the resulting track would be. This brittleness means that any detection source employed will have to significantly overgenerate detections to achieve near-perfect recall. This has a downside. While the Viterbi algorithm has linear complexity in the number of frames, it is quadratic in the number of detections per frame. This drastically limits the number of detections that can reasonably be processed leading to the necessity of tuning the thresholds on the detection sources. We have developed a novel mechanism to eliminate the need for a threshold and track every possible detection, at every position and scale in the image, in\ntime linear in the number of detections and frames. At the same time our approach eliminates the need for forward projection since every detection is already present. Our approach involves simultaneously performing object detection and tracking, optimizing the joint object-detection and temporal-coherency score.\nOur general approach is to compute the distance between pairs of detection pyramids for adjacent frames, rather than using g to compute the distance between pairs of individual detections. These pyramids represent the set of all possible detections at all locations and scales in the associated frame. Employing a distance transform makes this process linear in the number of location and scale positions in the pyramid. Many detectors, e.g. those of Felzenszwalb et al., use such a scale-space representation of frames to represent detections internally even though they might not output such. Our approach requires instrumenting such a detector to provide access to this internal representation.\nAt a high-level, the Felzenszwalb et al. detectors learn a forest of HOG (Freeman and Roth, 1995) filters for each object class along with their characteristic displacements. Detection proceeds by applying each HOG filter at every position in an image pyramid followed by computing the optimal displacements at every position in that image pyramid, thereby creating a new pyramid, the detection pyramid. Finally, the detector searches the detection pyramid for high-scoring detections and extracts those above a threshold. The detector employs a dynamicprogramming algorithm to efficiently compute the optimal part displacements for the entire image pyramid. This algorithm (Felzenszwalb et al., 2010a) is very similar to the Viterbi algorithm. It is made tractable by the use of a generalized distance transform (Felzenszwalb and Huttenlocher, 2004) that allows it to scale linearly with the number of image pyramid positions. Given a set G of points (which in our case denotes an image pyramid), a distance metric d between pairs of points p and q, and an arbitrary function \u03c6 : G \u2192 <, the generalized distance transformD\u03c6(q) computes:\nD\u03c6(q) = min p\u2208G (d(p, q) + \u03c6(q))\nin linear time for certain distance metrics including squared Euclidean distance.\nInstead of extracting and tracking just the thresholded detections, one can directly track all detections in the entire pyramid simultaneously by defining a distance measure between detection pyramids for adjacent frames and performing the Viterbi tracking algorithm on these pyramids instead of sets of detections in each frame. To allow comparison between detections at different scales in the detection pyramid, we convert the detection pyramid to a rectangular prism by scaling the coordinates of the detections at scale s by \u03c0(s), chosen to map the detection coordinates back to the coordinate system of the input frame. We define the distance between two detections, b and b\u2032, in two detection pyramids as a scaled squared Euclidean distance:\nd(bxys, b \u2032 x\u2032y\u2032s\u2032) = (\u03c0(s)x\u2212 \u03c0(s\u2032)x\u2032)2\n+ (\u03c0(s)y \u2212 \u03c0(s\u2032)y\u2032)2 + \u03b1(s\u2212 s\u2032)2\n(3)\nwhere x and y denote the original image coordinates of a detection center at scale s. Nominally, detections are boxes. Comparing two such boxes involves a four-dimensional distance metric. However, with a detection pyramid, the aspect ratio of detections is fixed, reducing this to a threedimensional distance metric. The coefficient \u03b1 in the distance metric weights a difference in detection area differently than detection position.\nThe above amounts to replacing detections btj with b t xys, lattice values \u03b4tj with \u03b4 t xys, and Eq. 2 with:\nfor x = 1 to X do for y = 1 to Y\ndo for s = 1 to S do \u03b41xys := f(b1xys) for t = 2 to T do for x = 1 to X\ndo for y = 1 to Y do for s = 1 to S\ndo \u03b4txys := f(btxys) + max x\u2032,y\u2032,s\u2032 g(bt\u22121x\u2032y\u2032s\u2032 , b t xys) + \u03b4 t\u22121 x\u2032y\u2032s\u2032\n(4)\nThe above formulation allows us to employ the generalized distance transform as an analog to g in Eq. 1, although it restricts consideration of g to be squared Euclidean distance rather than Euclidean distance. We avail ourselves of the\nfact that the generalized distance transform operates independently on each of the three dimensions x, y, and s in order to incorporate \u03b1 into Eq. 3. While linear-time use of the distance transform restricts the form of g, it places no restrictions on the form of f .\nOne way to view the above is that the vector of \u03b4tj for all 1 \u2264 j \u2264 Jt from Eq. 2 is being represented as a pyramid and the loop:\nfor j = 1 to Jt do \u03b4tj := f(btj) +\nJt\u22121 max j\u2032=1 g(bt\u22121j\u2032 , b t j) + \u03b4 t\u22121 j\u2032\n(5)\nis being performed as a linear-time construction of a generalized distance transform rather than a quadratic-time nested pair of loops. Another way to view the above is that we generalize the notion of a detection pyramid from representing per-frame detections bxys at threedimensional pyramid positions \u3008x, y, s\u3009 to representing per-video detections btxys at four-dimensional pyramid positions \u3008x, y, s, t\u3009 and finding a sequence of per-video detections for 1 \u2264 t \u2264 T that optimizes the following variant of Eq. 1:\nmax x1,...,xT y1,...,yT s1,...,sT\nT\u2211 t=1 f(btxtytst)+ T\u2211 t=2 g(bt\u22121xt\u22121yt\u22121st\u22121 , b t xtytst) (6)\nThis combination of the detector and the tracker is performing simultaneous detection and tracking integrating the information between the two. Before, the tracker was affected by the detector but the detector was unaffected by the tracker: potential low-scoring but temporally-coherent detections would not even be generated by the detector despite the fact that they would yield good tracks. Because now, the detector no longer chooses which detections to produce but instead scores all detections at every position and scale, the tracker is able to choose among any possible detection. Such tight integration of higher- and lower-level information will be revisited when integrating event models into this framework."}, {"heading": "5 Combining tracking and event detection", "text": "It is popular to use Hidden Markov Models (HMMs) to perform event recognition (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005). When doing so, the log likelihood of a video conditioned on an event model is:\nlog \u2211\nk1,...,kT\nexp T\u2211 t=1 h(kt, b t j\u2217t ) + T\u2211 t=2 a(kt, kt\u22121)\nwhere kt denotes the state of the HMM for frame t, h(k, b) denotes the log probability of generating a detection b conditioned on being in state k, a(k, k\u2032) denotes the log probability of transitioning from state k to k\u2032, and j\u2217t denotes\nindex of the detection produced by the tracker in frame t. This log likelihood can be computed with the forward algorithm (Baum and Petrie, 1966) which is analogous to the Viterbi algorithm. Maximum likelihood (ML), the standard approach to using HMMs for classification, selects the event model that maximizes the likelihood of an observed event. One can instead select the model with the maximum a posteriori (log) probability (MAP).\nmax k1,...,kT T\u2211 t=1 h(kt, b t j\u2217t ) + T\u2211 t=2 a(kt, kt\u22121) (7)\nThis can be computed with the Viterbi algorithm. The advantage of doing so is that one can combine the Viterbi algorithm used for detection-based tracking with the Viterbi algorithm used for event classification.\nOne can combine Eq. 1 with Eq. 7 to yield a unified cost function:\nmax j1,...,jT max k1,...,kT T\u2211 t=1 f(btjt) + T\u2211 t=2 g(bt\u22121jt\u22121 , b t jt)\n+ T\u2211 t=1 h(kt, b t jt) + T\u2211 t=2 a(kt, kt\u22121)\n(8)\nthat computes the joint MAP of the best possible track and the best possible state sequence by replacing j\u2217t with jt inside nested quantification. This too can be computed with the Viterbi algorithm, taking the lattice values \u03b4tjk to be indexed by the detection index j and the state k, forming the cross product of the tracker lattice nodes and the event lattice nodes:\nfor j = 1 to J1 do for k = 1 to K do \u03b41jk := f(b1j ) + h(k, b1j ) for t = 2 to T do for j = 1 to Jt\ndo for k = 1 to K do \u03b4tjk := f(btj) + h(k, btj)\n+ Jt\u22121 max j\u2032=1 K max k\u2032=1 g(bt\u22121j\u2032 , b t j) + a(k, k \u2032)\n+ \u03b4t\u22121j\u2032k\u2032\n(9)\nThis finds the optimal path through a graph where the nodes at every frame represent the cross product of the detections and the HMM states.\nDoing so performs simultaneous tracking and event classification. Before, the event classifier was affected by the tracker but the tracker was unaffected by the event classifier: potential low-scoring tracks would not even be generated by the tracker despite the fact that they would yield a high MAP estimate for some event class. Because now, the tracker no longer chooses which tracks to produce but instead scores all tracks, the event classifier is able to choose among any possible track. This amounts to a different kind of track-coherence measure that is tuned to specific events.\nSuch a measure might otherwise be difficult to achieve without top-down information from the event classifier. For example applying this method to a video of a running person along with an event model for running, will be more likely to compose a track out of person detections that has high velocity and low change in direction.\nProcessing each frame t with the algorithm in Eq. 9 is quadratic in JtK. This can be problematic since JtK can be large. As before, we can make this linear in Jt using a generalized distance transform. One can make this linear inK for suitable state-transition functions a (Felzenszwalb et al., 2003).\nTwo practical issues arise when applying the above method. First, one can factor Eq. 10 as Eq. 11:\nJt\u22121 max j\u2032=1 K max k\u2032=1\n( g(bt\u22121j\u2032 , b t j) + a(k, k \u2032) + \u03b4t\u22121j\u2032k\u2032 )\n(10)\nJt\u22121 max j\u2032=1\n( g(bt\u22121j\u2032 , b t j) +\nK max k\u2032=1\n( a(k, k\u2032) + \u03b4t\u22121j\u2032k\u2032 )) (11)\nThis is important because the computation of g(bt\u22121j\u2032 , b t j) might be expensive as it involves a projection of bt\u22121j\u2032 forward one frame (e.g. using optical flow or KLT). Second, when applying this method to multiple event models, the same factorization can be extended to cache the computation of g(bt\u22121j\u2032 , b t j) across different event models as this term does not depend on the event model."}, {"heading": "6 Combining object detection, tracking and event detection", "text": "One can combine the methods of Sections 4 and 5 to optimize a cost function:\nmax x1,...,xT y1,...,yT s1,...,sT k1,...,kT\nT\u2211 t=1 f(btxtytst) + h(kt, b t xtytst)\n+ T\u2211 t=2 g(bt\u22121xt\u22121yt\u22121st\u22121 , b t xtytst) + a(kt, kt\u22121)\n(12)\nthat combines Eq. 6 with Eq. 8 by forming a large Viterbi lattice with values \u03b4txysk.\nOne practical issue arises when applying the above method. In Eq. 12, h is a function of btxtytst , the detection in the current frame. This allows the HMM event model to depend on static object characteristics such as position, shape, and pose. However, many approaches to event recognition using HMMs use temporal derivatives of such characteristics to provide object velocity and acceleration information (Siskind and Morris, 1996; Starner et al., 1998). Having h also be a function of bt\u22121xt\u22121yt\u22121st\u22121 , the detection in the previous frame, requires incorporation h into the generalized distance transform and thus restricts its form.\nThe above combination performs simultaneous object detection, tracking, and event classification, integrating information across all three. Without such information integration, the object detector is unaffected by the tracker which is in turn unaffected by the event model. With such integration, the event model can influence the tracker and both can influence the object detector.\nThis is important because current object detectors cannot reliably detect small, deformable, or partially occluded objects. Moreover, current trackers also fail to track such objects. Information from the event model can focus the object detector and tracker on those particular objects that participate in a specified event. An event model for recognizing an agent picking an object up can bias the object detector and tracker to search for an object that exhibits a particular profile of motion relative to the agent, namely where the object is in close proximity to the agent, the object starts out being at rest while the agent approaches the object, then the agent touches the object, followed by the object moving with the agent.\nA traditional view of the relationship between object and event detection suggests that one recognizes a hammering event, in part, because one detects a hammer. Our unified approach inverts the traditional view, suggesting that one can recognize a hammer, in part, by detecting a hammering event. Furthermore, a strength of our approach is that such relationships are not encoded explicitly, do not have to be annotated in the training data for the event models, and are learned automatically as part of learning the parameters of the different event models. This is to say that the relationship between a person and the objects they manipulate can be learned from the co-occurrence of tracks in the training data, rather than from manually annotated symbolic relationships."}, {"heading": "7 Experimental results", "text": "Figure 3 demonstrates improved performance of simultaneous object detection and tracking (c) over object detection (a) and tracking (b) in isolation. This happens for different reasons: motion blur, even for large objects, can lead to poor detection results and hence poor tracks, small objects are difficult to detect and track, and integration can improve detection and tracking of deformable objects, such as a person transitioning from an upright pose to sitting down.\nFigure 4 demonstrates improved performance of simultaneous tracking and event recognition (c) over tracking (b) in isolation. These results were obtained with object and event models that were trained independently. The object\nIt would appear possible to co-train object and event models by combining Baum-Welch (Baum, 1972; Baum et al., 1970) with the training procedure for object models (Felzenszwalb et al., 2010a).\nmodels were trained on isolated frames using the standard Felzenszwalb training software. The event models were trained using tracks produced by the detection-based tracking method described in Section 2. It is difficult to track the person running with detection-based tracking alone due to articulated appearance change and motion blur. Imposing the prior of detecting running biases the tracker to find the desired track.\nFigure 5 demonstrates improved performance of simultaneous object detection, tracking, and event recognition (c) over object detection (a) and tracking (b) in isolation. As before, these results were obtained with object and event models that were trained independently."}, {"heading": "8 Conclusion", "text": "Detection-base tracking using dynamic programming has a long history (Castanon, 1990; Wolf et al., 1989), as do motion-profile-based approaches to event recognition using HMMs (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005). Moreover, there have been attempts to integrate object detection and tracking (Li and Nevatia, 2008; Pirsiavash et al., 2011), tracking and event recognition (Li and Chellappa, 2002), and object detection and event recognition (Gupta and Davis, 2007; Moore et al., 1999; Peursum et al., 2005). However, we are unaware of prior work that integrates all three and does so in a fashion that efficiently finds a global optimum to a simple unified cost function.\nWe have demonstrated a general framework for simultaneous object detection, tracking, and event recognition Many object detectors can naturally be transformed into trackers\nby introducing time into their cost functions, thus tracking every possible detection in each frame. Furthermore, the distance transform can be used to reduce the complexity of doing so from quadratic to linear. The common internal structure and algorithmic organization of object detection, detection-based tracking, and event recognition further allows an HMM-based approach to event recognition to be incorporated into the general dynamic-programming approach. This facilitates multidirectional information flow where not only can object detection influence tracking and, in turn, event recognition, event recognition can influence tracking and, in turn object detection."}, {"heading": "Acknowledgments", "text": "This work was supported, in part, by NSF grant CCF0438806, by the Naval Research Laboratory under Contract Number N00173-10-1-G023, by the Army Research Laboratory accomplished under Cooperative Agreement Number W911NF-10-2-0060, and by computational resources provided by Information Technology at Purdue through its Rosen Center for Advanced Computing. Any views, opinions, findings, conclusions, or recommendations contained or expressed in this document or material are those of the author(s) and do not necessarily reflect or represent the views or official policies, either expressed or implied, of NSF, the Naval Research Laboratory, the Office of Naval Research, the Army Research Laboratory, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein."}], "references": [{"title": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process", "author": ["L.E. Baum"], "venue": null, "citeRegEx": "Baum.,? \\Q1972\\E", "shortCiteRegEx": "Baum.", "year": 1972}, {"title": "Statistical inference for probabilistic functions of finite state Markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Ann. Math. Stat,", "citeRegEx": "Baum and Petrie.,? \\Q1966\\E", "shortCiteRegEx": "Baum and Petrie.", "year": 1966}, {"title": "A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains", "author": ["L.E. Baum", "T. Petrie", "G. Soules", "N. Weiss"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Baum et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Baum et al\\.", "year": 1970}, {"title": "Efficient algorithms for finding the K best paths through a trellis", "author": ["D.A. Castanon"], "venue": "IEEE Transactions on Aerospace and Electronic Systems,", "citeRegEx": "Castanon.,? \\Q1990\\E", "shortCiteRegEx": "Castanon.", "year": 1990}, {"title": "Kernel-based object tracking", "author": ["D. Comaniciu", "V. Ramesh", "P. Meer"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Comaniciu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Comaniciu et al\\.", "year": 2003}, {"title": "URL http://www.cse.buffalo. edu/ \u0303jcorso/bigshare/mindseye_human_ annotation_may11_buffalo.tar.bz", "author": ["J. Corso"], "venue": null, "citeRegEx": "Corso,? \\Q2011\\E", "shortCiteRegEx": "Corso", "year": 2011}, {"title": "The PASCAL Visual Object Classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Distance transforms of sampled functions", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "Technical Report TR2004-1963, Cornell Computing and Information Science,", "citeRegEx": "Felzenszwalb and Huttenlocher.,? \\Q2004\\E", "shortCiteRegEx": "Felzenszwalb and Huttenlocher.", "year": 2004}, {"title": "Cascade object detection with deformable part models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Fast algorithms for large-state-space HMMs with applications to web usage analysis", "author": ["Pedro F. Felzenszwalb", "Daniel P. Huttenlocher", "Jon M. Kleinberg"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2003}, {"title": "Orientation histograms for hand gesture recognition", "author": ["W.T. Freeman", "M. Roth"], "venue": "In International Workshop on Automatic Face and Gesture Recognition,", "citeRegEx": "Freeman and Roth.,? \\Q1995\\E", "shortCiteRegEx": "Freeman and Roth.", "year": 1995}, {"title": "Objects in action: an approach for combining action understanding and object perception", "author": ["A. Gupta", "L.S. Davis"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gupta and Davis.,? \\Q2007\\E", "shortCiteRegEx": "Gupta and Davis.", "year": 2007}, {"title": "A generic approach to simultaneous tracking and verification in video", "author": ["Baoxin Li", "Rama Chellappa"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Li and Chellappa.,? \\Q2002\\E", "shortCiteRegEx": "Li and Chellappa.", "year": 2002}, {"title": "Key object driven multi-category object recognition, localization, and tracking using spatiotemporal context", "author": ["Yuan Li", "Ramakant Nevatia"], "venue": "In Proceedings of the European Conference on Computer Vision,", "citeRegEx": "Li and Nevatia.,? \\Q2008\\E", "shortCiteRegEx": "Li and Nevatia.", "year": 2008}, {"title": "Exploting human actions and object context for recognition tasks", "author": ["D.J. Moore", "I.A. Essa", "M.H. Heyes"], "venue": "In Proceedings of the 7th International Conference on Computer Vision,", "citeRegEx": "Moore et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Moore et al\\.", "year": 1999}, {"title": "A threshold selection method from gray-level histograms", "author": ["N. Otsu"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Otsu.,? \\Q1979\\E", "shortCiteRegEx": "Otsu.", "year": 1979}, {"title": "Combining image regions and human activity for indirect object recognition in indoor wide-angle views", "author": ["P. Peursum", "G. West", "S. Venkatesh"], "venue": "In Proceedings of the 10th International Conference on Computer Vision,", "citeRegEx": "Peursum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peursum et al\\.", "year": 2005}, {"title": "Globally-optimal greedy algorithms for tracking a variable number of objects", "author": ["H. Pirsiavash", "D. Ramanan", "C.C. Fowlkes"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Pirsiavash et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pirsiavash et al\\.", "year": 2011}, {"title": "URL https://s3.amazonaws.com/ Annotations/vaticlabels_C-D1_0819.tar.gz", "author": ["K. Saenko"], "venue": null, "citeRegEx": "Saenko,? \\Q2011\\E", "shortCiteRegEx": "Saenko", "year": 2011}, {"title": "Spatiotemporal contour grouping using abstract part models", "author": ["Pablo Sala", "Diego Macrini", "Sven J. Dickinson"], "venue": "In Proceedings of the 10th Asian Conference on Computer Vision,", "citeRegEx": "Sala et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sala et al\\.", "year": 2010}, {"title": "Good features to track", "author": ["J. Shi", "C. Tomasi"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Shi and Tomasi.,? \\Q1994\\E", "shortCiteRegEx": "Shi and Tomasi.", "year": 1994}, {"title": "A maximum-likelihood approach to visual event classification", "author": ["J.M. Siskind", "Q. Morris"], "venue": "In Proceedings of the Fourth European Conference on Computer Vision,", "citeRegEx": "Siskind and Morris.,? \\Q1996\\E", "shortCiteRegEx": "Siskind and Morris.", "year": 1996}, {"title": "Real-time American sign language recognition using desk and wearable computer based video", "author": ["Thad Starner", "Joshua Weaver", "Alex Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Starner et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Starner et al\\.", "year": 1998}, {"title": "Detection and tracking of point features", "author": ["C. Tomasi", "T. Kanade"], "venue": "Technical Report CMU-CS-91-132,", "citeRegEx": "Tomasi and Kanade.,? \\Q1991\\E", "shortCiteRegEx": "Tomasi and Kanade.", "year": 1991}, {"title": "Convolutional codes and their performance in communication systems", "author": ["A.J. Viterbi"], "venue": "IEEE Transactions on Communication,", "citeRegEx": "Viterbi.,? \\Q1971\\E", "shortCiteRegEx": "Viterbi.", "year": 1971}, {"title": "Event recognition with time varying hidden Markov model", "author": ["Zhaowen Wang", "Ercan E. Kuruoglu", "Xiaokang Yang", "Yi Xu", "Songyu Yu"], "venue": "In Proceedings of the International Conference on Accoustic and Speech Signal Processing,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Finding the best set of K paths through a trellis with application to multitarget tracking", "author": ["J.K. Wolf", "A.M. Viterbi", "G.S. Dixon"], "venue": "IEEE Transactions on Aerospace and Electronic Systems,", "citeRegEx": "Wolf et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Wolf et al\\.", "year": 1989}, {"title": "Motion based event recognition using HMM", "author": ["Gu Xu", "Yu-Fei Ma", "HongJiang Zhang", "Shiqiang Yang"], "venue": "In Proceedings of the International Conference on Pattern Recognition,", "citeRegEx": "Xu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2002}, {"title": "An HMM-based framework for video semantic analysis", "author": ["Gu Xu", "Yu-Fei Ma", "HongJiang Zhang", "Shi-Qiang Yang"], "venue": "IEEE Trans. Circuits Syst. Video Techn.,", "citeRegEx": "Xu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2005}, {"title": "Object tracking: A survey", "author": ["A. Yilmaz", "O. Javed", "M. Shah"], "venue": "ACM Computing Surveys,", "citeRegEx": "Yilmaz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yilmaz et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 22, "context": "Many common approaches to event recognition (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005) classify events based on their motion profile.", "startOffset": 44, "endOffset": 134}, {"referenceID": 23, "context": "Many common approaches to event recognition (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005) classify events based on their motion profile.", "startOffset": 44, "endOffset": 134}, {"referenceID": 26, "context": "Many common approaches to event recognition (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005) classify events based on their motion profile.", "startOffset": 44, "endOffset": 134}, {"referenceID": 30, "context": "Adaptive approaches to tracking (Yilmaz et al., 2006), e.", "startOffset": 32, "endOffset": 53}, {"referenceID": 4, "context": "Kalman filtering (Comaniciu et al., 2003), suffer from three difficulties that impact their utility for event recognition.", "startOffset": 17, "endOffset": 41}, {"referenceID": 6, "context": "On the PASCAL VOC Challenge, they typically achieve average precision scores of 40% to 50% (Everingham et al., 2010).", "startOffset": 91, "endOffset": 116}, {"referenceID": 27, "context": ", 2010a,b), detection-based trackers (Wolf et al., 1989), and HMM-based approaches to event recognition (Baum and Petrie, 1966) facilitates a general approach to integrating these three components.", "startOffset": 37, "endOffset": 56}, {"referenceID": 1, "context": ", 1989), and HMM-based approaches to event recognition (Baum and Petrie, 1966) facilitates a general approach to integrating these three components.", "startOffset": 55, "endOffset": 78}, {"referenceID": 21, "context": "The forward projection internal to g can be done in a variety of ways including optical flow and the Kanade-LucasTomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker.", "startOffset": 126, "endOffset": 173}, {"referenceID": 24, "context": "The forward projection internal to g can be done in a variety of ways including optical flow and the Kanade-LucasTomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker.", "startOffset": 126, "endOffset": 173}, {"referenceID": 25, "context": "1 can be optimized in polynomial time with the Viterbi algorithm (Viterbi, 1971):", "startOffset": 65, "endOffset": 80}, {"referenceID": 7, "context": "We take g to be the negative Euclidean distance between the center of btjt and the center of bt\u22121 jt\u22121 projected forward one frame, though, as discussed below, our approach is compatible with a variety of functions discussed by Felzenszwalb and Huttenlocher (2004). The forward projection internal to g can be done in a variety of ways including optical flow and the Kanade-LucasTomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker.", "startOffset": 228, "endOffset": 265}, {"referenceID": 20, "context": "One can ameliorate this somewhat by constructing a lattice that skips frames (Sala et al., 2010).", "startOffset": 77, "endOffset": 96}, {"referenceID": 6, "context": "and recall and thus it is necessary to explore the trade-off between the two (Everingham et al., 2010).", "startOffset": 77, "endOffset": 102}, {"referenceID": 16, "context": "One can derive an offset by computing a histogram of scores of the top detection in each frame of a video and taking the offset to be the minimum of the value that maximizes the between-class variance (Otsu, 1979) when bipartitioning this histogram and the trained acceptance threshold offset by a small but fixed amount.", "startOffset": 201, "endOffset": 213}, {"referenceID": 6, "context": "A permutation mapping was preferred when it had higher average overlap score among corresponding boxes across the tracks and the frames in a video, where the overlap score was that used by the PASCAL VOC Challenge (Everingham et al., 2010), namely the ratio of the area of their intersection to the area of their union.", "startOffset": 214, "endOffset": 239}, {"referenceID": 11, "context": "detectors learn a forest of HOG (Freeman and Roth, 1995) filters for each object class along with their characteristic displacements.", "startOffset": 32, "endOffset": 56}, {"referenceID": 7, "context": "It is made tractable by the use of a generalized distance transform (Felzenszwalb and Huttenlocher, 2004) that allows it to scale linearly with the number of image pyramid positions.", "startOffset": 68, "endOffset": 105}, {"referenceID": 22, "context": "It is popular to use Hidden Markov Models (HMMs) to perform event recognition (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005).", "startOffset": 78, "endOffset": 168}, {"referenceID": 23, "context": "It is popular to use Hidden Markov Models (HMMs) to perform event recognition (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005).", "startOffset": 78, "endOffset": 168}, {"referenceID": 26, "context": "It is popular to use Hidden Markov Models (HMMs) to perform event recognition (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005).", "startOffset": 78, "endOffset": 168}, {"referenceID": 1, "context": "This log likelihood can be computed with the forward algorithm (Baum and Petrie, 1966) which is analogous to the Viterbi algorithm.", "startOffset": 63, "endOffset": 86}, {"referenceID": 10, "context": "One can make this linear inK for suitable state-transition functions a (Felzenszwalb et al., 2003).", "startOffset": 71, "endOffset": 98}, {"referenceID": 22, "context": "However, many approaches to event recognition using HMMs use temporal derivatives of such characteristics to provide object velocity and acceleration information (Siskind and Morris, 1996; Starner et al., 1998).", "startOffset": 162, "endOffset": 210}, {"referenceID": 23, "context": "However, many approaches to event recognition using HMMs use temporal derivatives of such characteristics to provide object velocity and acceleration information (Siskind and Morris, 1996; Starner et al., 1998).", "startOffset": 162, "endOffset": 210}, {"referenceID": 0, "context": "It would appear possible to co-train object and event models by combining Baum-Welch (Baum, 1972; Baum et al., 1970) with the training procedure for object models (Felzenszwalb et al.", "startOffset": 85, "endOffset": 116}, {"referenceID": 2, "context": "It would appear possible to co-train object and event models by combining Baum-Welch (Baum, 1972; Baum et al., 1970) with the training procedure for object models (Felzenszwalb et al.", "startOffset": 85, "endOffset": 116}, {"referenceID": 3, "context": "Detection-base tracking using dynamic programming has a long history (Castanon, 1990; Wolf et al., 1989), as do motion-profile-based approaches to event recognition using HMMs (Siskind and Morris, 1996; Starner et al.", "startOffset": 69, "endOffset": 104}, {"referenceID": 27, "context": "Detection-base tracking using dynamic programming has a long history (Castanon, 1990; Wolf et al., 1989), as do motion-profile-based approaches to event recognition using HMMs (Siskind and Morris, 1996; Starner et al.", "startOffset": 69, "endOffset": 104}, {"referenceID": 22, "context": ", 1989), as do motion-profile-based approaches to event recognition using HMMs (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005).", "startOffset": 79, "endOffset": 169}, {"referenceID": 23, "context": ", 1989), as do motion-profile-based approaches to event recognition using HMMs (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005).", "startOffset": 79, "endOffset": 169}, {"referenceID": 26, "context": ", 1989), as do motion-profile-based approaches to event recognition using HMMs (Siskind and Morris, 1996; Starner et al., 1998; Wang et al., 2009; Xu et al., 2002, 2005).", "startOffset": 79, "endOffset": 169}, {"referenceID": 14, "context": "Moreover, there have been attempts to integrate object detection and tracking (Li and Nevatia, 2008; Pirsiavash et al., 2011), tracking and event recognition (Li and Chellappa, 2002), and object detection and event recognition (Gupta and Davis, 2007; Moore et al.", "startOffset": 78, "endOffset": 125}, {"referenceID": 18, "context": "Moreover, there have been attempts to integrate object detection and tracking (Li and Nevatia, 2008; Pirsiavash et al., 2011), tracking and event recognition (Li and Chellappa, 2002), and object detection and event recognition (Gupta and Davis, 2007; Moore et al.", "startOffset": 78, "endOffset": 125}, {"referenceID": 13, "context": ", 2011), tracking and event recognition (Li and Chellappa, 2002), and object detection and event recognition (Gupta and Davis, 2007; Moore et al.", "startOffset": 40, "endOffset": 64}, {"referenceID": 12, "context": ", 2011), tracking and event recognition (Li and Chellappa, 2002), and object detection and event recognition (Gupta and Davis, 2007; Moore et al., 1999; Peursum et al., 2005).", "startOffset": 109, "endOffset": 174}, {"referenceID": 15, "context": ", 2011), tracking and event recognition (Li and Chellappa, 2002), and object detection and event recognition (Gupta and Davis, 2007; Moore et al., 1999; Peursum et al., 2005).", "startOffset": 109, "endOffset": 174}, {"referenceID": 17, "context": ", 2011), tracking and event recognition (Li and Chellappa, 2002), and object detection and event recognition (Gupta and Davis, 2007; Moore et al., 1999; Peursum et al., 2005).", "startOffset": 109, "endOffset": 174}], "year": 2012, "abstractText": "The common internal structure and algorithmic organization of object detection, detection-based tracking, and event recognition facilitates a general approach to integrating these three components. This supports multidirectional information flow between these components allowing object detection to influence tracking and event recognition and event recognition to influence tracking and object detection. The performance of the combination can exceed the performance of the components in isolation. This can be done with linear asymptotic complexity.", "creator": "LaTeX with hyperref package"}}}