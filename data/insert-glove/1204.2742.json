{"id": "1204.2742", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2012", "title": "Video In Sentences Out", "abstract": "We present mbogo a system that produces sentential descriptions kempegowda of art-historical video: 700-kilometre who 38-8 did what to whom, and 104-103 where and 52.67 how they eknath did sublimated it. 1000-1200 Action class is agilent rendered maruska as greenspond a pelayo verb, medaled participant objects as noun zver phrases, ronchetti properties marchmain of abkhazian those pinch-hitting objects as greenlanders adjectival modifiers in lancashire those passey noun borge phrases, spatial zag\u00f3rze relations between articular those angmering participants as dialoguing prepositional phrases, and 41.40 characteristics of devta the great-great-granddaughter event leftish as shahrastani prepositional - 41-story phrase adjuncts and adverbial barnes modifiers. Extracting steyne the ogilvie information jogged needed shawon to euro409 render these linguistic 105-101 entities requires non-noble an approach to hollandiae event swarovski recognition that 3,643 recovers 100-pitch object mrkonji\u0107 tracks, the track - tabarka to - 3,293 role farinos assignments, and roraback changing severnside body snowmelt posture.", "histories": [["v1", "Thu, 12 Apr 2012 14:47:44 GMT  (4210kb,D)", "http://arxiv.org/abs/1204.2742v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["andrei barbu", "alexander bridge", "zachary burchill", "dan coroian", "sven dickinson", "sanja fidler", "aaron michaux", "sam mussman", "siddharth narayanaswamy", "dhaval salvi", "lara schmidt", "jiangnan shangguan", "jeffrey mark siskind", "jarrell waggoner", "song wang", "jinlian wei", "yifan yin", "zhiqi zhang"], "accepted": false, "id": "1204.2742"}, "pdf": {"name": "1204.2742.pdf", "metadata": {"source": "CRF", "title": "Video In Sentences Out", "authors": ["Andrei Barbu", "a\u2217Alexander", "Zachary Burchill", "Dan Coroian", "Sven Dickinson", "Sanja Fidler", "Aaron Michaux", "Sam Mussman", "Siddharth Narayanaswamy", "Dhaval Salvi", "Lara Schmidt", "Jiangnan Shangguan", "Jeffrey Mark Siskind", "Jarrell Waggoner", "Song Wang", "Jinlian Wei", "Yifan Yin", "Zhiqi Zhang"], "emails": ["andrei@0xab.com."], "sections": [{"heading": null, "text": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture."}, {"heading": "1 Introduction", "text": "We present a system that produces sentential descriptions of short video clips. These sentences describe who did what to whom, and where and how they did it. This system not only describes the observed action as a verb, it also describes the participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, the spatial relations between those participants as\n\u2217Corresponding author. Email: andrei@0xab.com. Additional images and videos as well as all code and datasets are available at http://engineering.purdue. edu/\u02dcqobi/arxiv2012b.\ncoordination: and verbs: approached, arrived, attached, bounced, buried, carried, caught,\nchased, closed, collided, digging, dropped, entered, exchanged, exited, fell, fled, flew, followed, gave, got, had, handed, hauled, held, hit, jumped, kicked, left, lifted, moved, opened, passed, picked, pushed, put, raised, ran, received, replaced, snatched, stopped, threw, took, touched, turned, walked, went\nnouns: bag, ball, bench, bicycle, box, cage, car, cart, chair, dog, door, ladder, left, mailbox, microwave, motorcycle, object, person, right, skateboard, SUV, table, tripod, truck adjectives: big, black, blue, cardboard, crouched, green, narrow, other, pink, prone, red, short, small, tall, teal, toy, upright, white, wide, yellow prepositions: above, because, below, from, of, over, to, with lexical PPs: downward, leftward, rightward, upward determiners: an, some, that, the particles: away, down, up pronouns: itself, something, themselves adverbs: quickly, slowly auxiliary: was\nTable 1: The vocabulary used to generate sentential descriptions of video.\nprepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. It incorporates a vocabulary of 118 words: 1 coordination, 48 verbs, 24 nouns, 20 adjectives, 8 prepositions, 4 lexical prepositional phrases, 4 determiners, 3 particles, 3 pronouns, 2 adverbs, and 1 auxiliary, as illustrated in Table 1.\nProduction of sentential descriptions requires recognizing the primary action being performed, because such actions are rendered as verbs and verbs serve as the central scaffolding for sentences. However, event recognition alone is insufficient to generate the remaining sentential components. One must recognize object classes in order to render nouns. But even object recognition alone is insufficient to generate meaningful sentences. One must determine the roles that such objects play in the event. The agent, i.e. the\nar X\niv :1\n20 4.\n27 42\nv1 [\ncs .C\ndoer of the action, is typically rendered as the sentential subject while the patient, i.e. the affected object, is typically rendered as the direct object. Detected objects that do not play a role in the observed event, no matter how prominent, should not be incorporated into the description. This means that one cannot use common approaches to event recognition, such as spatiotemporal bags of words (Laptev et al., 2007; Niebles et al., 2008; Scovanner et al., 2007), spatiotemporal volumes (Blank et al., 2005; Laptev et al., 2008; Rodriguez et al., 2008), and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009) that do not determine the class of participant objects and the roles that they play. Even combining such approaches with an object detector would likely detect objects that don\u2019t participate in the event and wouldn\u2019t be able to determine the roles that any detected objects play.\nProducing elaborate sentential descriptions requires more than just event recognition and object detection. Generating a noun phrase with an embedded prepositional phrase, such as the person to the left of the bicycle, requires determining spatial relations between detected objects, as well as knowing which of the two detected objects plays a role in the overall event and which serves just to aid generation of a referring expression to help identify the event participant. Generating a noun phrase with adjectival modifiers, such as the red ball, not only requires determining the properties, such as color, shape, and size, of the observed objects, but also requires determining whether such descriptions are necessary to help disambiguate the referent of a noun phrase. It would be awkward to generate a noun phrase such as the big tall wide red toy cardboard trash can when the trash can would suffice. Moreover, one must track the participants to determine the speed and direction of their motion to generate adverbs such as slowly and prepositional phrases such as leftward. Further, one must track the identity of multiple instances of the same object class to appropriately generate the distinction between Some person hit some other person and The person hit themselves.\nA common assumption in Linguistics (Jackendoff, 1983; Pinker, 1989) is that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants. Object class and image characteristics of the participants are believed to be largely irrelevant to determining the appropriate verb label for an action class. Participants simply fill roles in the spatiotemporal structure of the action class described by a verb. For example, an event where one participant (the agent) picks up another participant (the patient) consists of a sequence of two sub-events, where during the first subevent the agent moves towards the patient while the patient is at rest and during the second sub-event the agent moves together with the patient away from the original location of the patient. While determining whether the agent is a\nperson or a cat, and whether the patient is a ball or a cup, is necessary to generate the noun phrases incorporated into the sentential description, such information is largely irrelevant to determining the verb describing the action. Similarly, while determining the shapes, sizes, colors, textures, etc. of the participants is necessary to generate adjectival modifiers, such information is also largely irrelevant to determining the verb. Common approaches to event recognition, such as spatiotemporal bags of words, spatiotemporal volumes, and tracked feature points, often achieve high accuracy because of correlation with image or video properties exhibited by a particular corpus. These are often artefactual, not defining properties of the verb meaning (e.g. recognizing diving by correlation with blue since it \u2018happens in a pool\u2019 (Liu et al., 2009, p. 2002) or confusing basketball and volleyball \u2018because most of the time the [. . .] sports use very similar courts\u2019 (Ikizler-Cinibis and Sclaroff, 2010, p. 506))."}, {"heading": "2 The mind\u2019s eye corpus", "text": "Many existing video corpora used to evaluate event recognition are ill-suited for evaluating sentential descriptions. For example, the WEIZMANN dataset (Blank et al., 2005) and the KTH dataset (Schuldt et al., 2004) depict events with a single human participant, not ones where people interact with other people or objects. For these datasets, the sentential descriptions would contain no information other than the verb, e.g. The person jumped. Moreover, such datasets, as well as the SPORTS ACTIONS dataset (Rodriguez et al., 2008) and the YOUTUBE dataset (Liu et al., 2009), often make action-class distinctions that are irrelevant to the choice of verb, e.g. wave1 vs. wave2, jump vs. pjump, Golf-Swing-Back vs. Golf-Swing-Front vs. Golf-Swing-Side, Kicking-Front vs. Kicking-Side, Swing-Bench vs. Swing-SideAngle, and golf swing vs. tennis swing vs. swing Other datasets, such as the BALLET dataset (Wang and Mori, 2009) and the UCF50 dataset (Liu et al., 2009), depict larger-scale activities that bear activity-class names that are not well suited to sentential description, e.g. Basketball, Billiards, BreastStroke, CleanAndJerk, HorseRace, HulaHoop, MilitaryParade, TaiChi, and YoYo.\nThe year-one (Y1) corpus produced by DARPA for the Mind\u2019s Eye program, however, was specifically designed to evaluate sentential description. This corpus contains two parts: the development corpus, C-D1, which we use solely for training, and the evaluation corpus, C-E1, which we use solely for testing. Each of the above is further divided into four sections to support the four task goals of the Mind\u2019s Eye program, namely recognition, description, gap filling, and anomaly detection. In this paper, we use only the recognition and description portions and apply our\nentire sentential-description pipeline to the combination of these portions. While portions of C-E1 overlap with C-D1, in this paper we train our methods solely on C-D1 and test our methods solely on the portion of C-E1 that does not overlap with C-D1.\nMoreover, a portion of the corpus was synthetically generated by a variety of means: computer graphics driven by motion capture, pasting foregrounds extracted from green screening onto different backgrounds, and intensity variation introduced by postprocessing. In this paper, we exclude all such synthetic video from our test corpus. Our training set contains 3480 videos and our test set 749 videos. These videos are provided at 720p@30fps and range from 42 to 1727 frames in length, with an average of 435 frames.\nThe videos nominally depict 48 distinct verbs as listed in Table 1. However, the mapping from videos to verbs is not one-to-one. Due to polysemy, a verb may describe more than one action class, e.g. leaving an object on the table vs. leaving the scene. Due to synonymy, an action class may be described by more than one verb, e.g. lift vs. raise. An event described by one verb may contain a component action described by a different verb, e.g. picking up an object vs. touching an object. Many of the events are described by the combination of a verb with other constituents, e.g. have a conversation vs. have a heart attack. And many of the videos depict metaphoric extensions of verbs, e.g. take a puff on a cigarette. Because the mapping from videos to verbs is subjective, the corpus comes labeled with DARPA-collected human judgments in the form of a single present/absent label associated with each video paired with each of the 48 verbs, gathered using Amazon Mechanical Turk. We use these labels for both training and testing as described later."}, {"heading": "3 Overall system architecture", "text": "The overall architecture of our system is depicted in Fig. 1. We first apply detectors (Felzenszwalb et al., 2010a,b) for each object class on each frame of each video. These detectors are biased to yield many false positives but few false negatives. The Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker is then used to project each detection five frames forward to augment the set of detections and further compensate for false negatives in the raw detector output. A dynamicprogramming algorithm (Viterbi, 1971) is then used to select an optimal set of detections that is temporally coherent with optical flow, yielding a set of object tracks for each video. These tracks are then smoothed and used to compute a time-series of feature vectors for each video to describe the relative and absolute motion of event participants. The person detections are then clustered based on part displacements to derive a coarse measure of human\nbody posture in the form of a body-posture codebook. The codebook indices of person detections are then added to the feature vector. Hidden Markov Models (HMMs) are then employed as time-series classifiers to yield verb labels for each video (Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005), together with the object tracks of the participants in the action described by that verb along with the roles they play. These tracks are then processed to produce nouns from object classes, adjectives from object properties, prepositional phrases from spatial relations, and adverbs and prepositional-phrase adjuncts from track properties. Together with the verbs, these are then woven into grammatical sentences. We describe each of the components of this system in detail below: the object detector and tracker in Section 3.1, the body-posture clustering and codebook in Section 3.2, the event classifier in Section 3.3, and the sentential-description component in Section 3.4."}, {"heading": "3.1 Object detection and tracking", "text": "We employ detection-based tracking as described in Section 2 of a parallel submission (id: 568) In detection-based tracking an object detector is applied to each frame of a video to yield a set of candidate detections which are composed into tracks by selecting a single candidate detection from each frame that maximizes temporal coherency of the\ntrack. Felzenszwalb et al. detectors are used for this purpose. Detection-based tracking requires biasing the detector to have high recall at the expense of low precision to allow the tracker to select boxes to yield a temporally coherent track. This is done by depressing the acceptance thresholds. To prevent massive over-generation of false positives, which would severely impact run time, we limit the number of detections produced per-frame to 12.\nTwo practical issues arise when depressing acceptance thresholds. First, it is necessary to reduce the degree of non-maximal suppression incorporated in the Felzenszwalb et al. detectors. Second, with the star detector (Felzenszwalb et al., 2010b), one can simply decrease the single trained acceptance threshold to yield more detections with no increase in computational complexity. However, we prefer to use the star cascade detector (Felzenszwalb et al., 2010a) as it is far faster. With the star cascade detector, though, one must also decrease the trained root- and part-filter thresholds to get more detections. Doing so, however, defeats the computational advantage of the cascade and significantly increases detection time. We thus train a model for the star detector using the standard procedure on human-annotated training data, sample the top detections produced by this model with a decreased acceptance threshold, and train a model for the star cascade detector on these samples. This yields a model that is almost as fast as one trained by the star cascade detector on the original training samples but with the desired bias in acceptance threshold.\nThe Y1 corpus contains approximately 70 different object classes that play a role in the depicted events. Many of these, however, cannot be reliably detected with the Felzenszwalb et al. detectors that we use. We trained models for 25 object classes that can be reliably detected, as listed in Table 2. These object classes account for over 90% of the event participants. Person models were trained with approximately 2000 human-annotated positive samples from C-D1 while nonperson models were trained with approximately 1000 such samples. For each positive training sample, two negative training samples were randomly generated from the same frame constrained to not overlap substantially with the positive samples. We trained three distinct person models to account for body-posture variation and pool these when constructing person tracks. The detection scores were normalized for such pooled detections by a per-model offset computed as follows: A (50 bin) histogram was computed of the scores of the top detection in each frame of a video. The offset is then taken to be the minimum of the value that maximizes the between-class variance (Otsu, 1979) when bipartitioning this histogram and the trained acceptance threshold offset by a fixed, but small, amount (0.4).\nWe employed detection-based tracking for all 25 object models on all 749 videos in our test set. To prune the\nlarge number of tracks thus produced, we discard all tracks corresponding to certain object models on a per-video basis: those that exhibit high detection-score variance over the frames in that video as well as those whose detectionscore distributions are neither unimodal nor bimodal. The parameters governing such pruning were determined solely on the training set. The tracks that remain after this pruning still account for over 90% of the event participants."}, {"heading": "3.2 Body-posture codebook", "text": "We recognize events using a combination of the motion of the event participants and the changing body posture of the human participants. Body-posture information is derived using the part structure produced as a by-product of the Felzenszwalb et al. detectors. While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.3, it suffices to improve event-recognition accuracy. Such information can be extracted from a large unannotated corpus far more robustly than possible with precise articulated models.\nBody-posture information is derived from part structure in two ways. First, we compute a vector of part displacements, each displacement as a vector from the detection center to the part center, normalizing these vectors to unit detection-box area. The time-series of feature vectors is augmented to includes these part displacements and a finite-difference approximation of their temporal derivatives as continuous features for person detections. Second, we vector-quantize the part-displacement vector and include the codebook index as a discrete feature for person detections. Such pose features are included in the timeseries on a per-frame basis. The codebook is trained by running each pose-specific person detector on the positive human-annotated samples used to train that detector and extract the resulting part-displacement vectors. We then pool the part-displacement vectors from the three posespecific person models and employ hierarchical k-means clustering using Euclidean distance to derive a codebook of 49 clusters. Fig. 2 shows sample clusters from our codebook. Codebook indices are derived using Euclidean distance from the means of these clusters."}, {"heading": "3.3 Event classification", "text": "Our tracker produces one or more tracks per object class for each video. We convert such tracks into a time-series of feature vectors. For each video, one track is taken to designate the agent and another track (if present) is taken to designate the patient. During training, we manually specify the track-to-role mapping. During testing, we automatically determine the track-to-role mapping by examining\nall possible such mappings and selecting the one with the highest likelihood (Siskind and Morris, 1996).\nThe feature vector encodes both the motion of the event participants and the changing body posture of the human participants. For each event participant in isolation we incorporate the following single-track features:\n1. x and y coordinates of the detection-box center 2. detection-box aspect ratio and its temporal derivative 3. magnitude and direction of the velocity of the\ndetection-box center 4. magnitude and direction of the acceleration of the\ndetection-box center 5. normalized part displacements and their temporal\nderivatives 6. object class (the object detector yielding the detection) 7. root-filter index 8. body-posture codebook index\nThe last three features are discrete; the remainder are continuous. For each pair of event participants we incorporate the following track-pair features:\n1. distance between the agent and patient detection-box centers and its temporal derivative 2. orientation of the vector from agent detection-box center to patient detection-box center\nOur HMMs assume independent output distributions for each feature. Discrete features are modeled with discrete output distributions. Continuous features denoting linear quantities are modeled with univariate Gaussian output distributions, while those denoting angular quantities are modeled with von Mises output distributions.\nFor each of the 48 action classes, we train two HMMs on two different sets of time-series of feature vectors, one con-\ntaining only single-track features for a single participant and the other containing single-track features for two participants along with the track-pair features. A training set of between 16 and 200 videos was selected manually from C-D1 for each of these 96 HMMs as positive examples depicting each of the 48 action classes. A given video could potentially be included in the training sets for both the onetrack and two-track HMMs for the same action class and even for HMMs for different action classes, if the video was deemed to depict both action classes.\nDuring testing, we generate present/absent judgments for each video in the test set paired with each of the 48 action classes. We do this by thresholding the likelihoods produced by the HMMs. By varying these thresholds, we can produce an ROC curve for each action class, comparing the resulting machine-generated present/absent judgments with the Amazon Mechanical Turk judgments. When doing so, we test videos for which our tracker produces two or more tracks against only the two-track HMMs while we test ones for which our tracker produces a single track against only the one-track HMMs.\nWe performed three experiments, training 96 different 200- state HMMs for each. Experiment I omitted all discrete features and all body-posture related features. Experiment II omitted only the discrete features. Experiment III omitted only the continuous body-posture related features. ROC curves for each experiment are shown in Fig. 3, Fig. 4 and Fig. 5. Note that the incorporation of body-posture information, either in the form of continuous normalized part displacements or discrete codebook indices, improves event-recognition accuracy, despite the fact that the part displacements produced by the Felzenszwalb et al. detectors are noisy and appear unintelligible to the human eye."}, {"heading": "3.4 Generating sentences", "text": "We produce a sentence from a detected action class together with the associated tracks using the templates from Table 3. In these templates, words in italics denote fixed strings, words in bold indicate the action class, X and Y denote subject and object noun phrases, and the categories Adv, PPendo, and PPexo denote adverbs and prepositionalphrase adjuncts to describe the subject motion. The processes for generating these noun phrases, adverbs, and prepositional-phrase adjuncts are described below. Onetrack HMMs take that track to be the agent and thus the subject. For two-track HMMs we choose the mapping from tracks to roles that yields the higher likelihood and take the agent track to be the subject and the patient track to be the object except when the action class is either approached or fled, the agent is (mostly) stationary, and the patient moves more than the agent.\nBrackets in the templates denote optional entities. Optional entities containing Y are generated only for twotrack HMMs. The criteria for generating optional adverbs and prepositional phrases are described below. The optional entity for received is generated when there is a patient track whose category is mailbox, person, person-crouch, or person-down.\nWe use adverbs to describe the velocity of the subject. For some verbs, a velocity adverb would be awkward:\n\u2217X slowly had Y \u2217X had slowly Y\nFurthermore, stylistic considerations dictate the syntactic position of an optional adverb:\nX jumped slowly over Y X slowly jumped over Y X slowly approached Y \u2217X approached slowly Y ?X slowly fell X fell slowly\nThe verb-phrase templates thus indicate whether an adverb is allowed, and if so whether it occurs, preferentially, preverbally or postverbally. Adverbs are chosen subject to three thresholds vaction class1 , v action class 2 , and v action class 3 determined empirically on a per-action-class basis: We select those frames from the subject track where the magnitude of the velocity of the box-detection center is above vaction class1 . An optional adverb is generated by comparing the magnitude of the average velocity v of the subject track boxdetection centers in these frames to the per-action-class thresholds:\nquickly v > vaction class2 slowly vaction class1 \u2264 v \u2264 vaction class3\nWe use prepositional-phrase adjuncts to describe the motion direction of the subject. Again, for some verbs, such adjuncts would be awkward:\n\u2217X had Y leftward \u2217X had Y from the left\nMoreover, for some verbs it is natural to describe the motion direction endogenously, from the perspective of the\nsubject, while for others it is more natural to describe the motion direction exogenously, from the perspective of the viewer:\nX fell leftward X fell from the left X chased Y leftward \u2217X chased Y from the left \u2217X arrived leftward X arrived from the left\nThe verb-phrase templates thus indicate whether an adjunct is allowed, and if so whether it is preferentially endogenous or exogenous. The choice of adjunct is determined from the orientation of v, as computed above and depicted in Fig. 6(a,b). We omit the adjunct when v < vaction class1 .\nWe generate noun phrases X and Y to refer to event participants according to the following grammar:\nNP \u2192 themselves | itself | something | D A\u2217 N [PP] D \u2192 the | that | some\nWhen instantiating a sentential template that has a required object noun-phrase Y for a one-track HMM, we generate a pronoun. A pronoun is also generated when the action class is entered or exited and the patient class is not car, door, suv, or truck. The anaphor themselves is generated if the action class is attached or raised, the anaphor itself if the action class is moved, and something otherwise. As described below, we generate an optional prepositional phrase for the subject noun phrase to describe the spatial relation between the subject and the object. We choose the determiner to handle coreference, generating the when a noun phrase unambiguously refers to the agent or the patient due to the combination of head noun and any adjectives,\nThe person jumped over the ball. The red ball collided with the blue ball.\nthat for an object noun phrase that corefers to a track referred to in a prepositional phrase for the subject,\nThe person to the right of the car approached that car. Some person to the right of some other person approached that other person.\nand some otherwise:\nSome car approached some other car.\nWe generate the head noun of a noun phrase from the object class using the mapping in Table 2(a). Four different kinds of adjectives are generated: color, shape, size, and restrictive modifiers. An optional color adjective is generated based on the average HSV values in the eroded detection boxes for a track: black when V \u2264 0.2, white when V \u2265 0.8, one of red, blue, green, yellow, teal, or pink based on H , when S \u2265 0.7. An optional size adjective is generated in two ways, one from the object class using the mapping in Table 2(c), the other based on per-object-class image statistics. For each object class, a mean object size\na\u0304object class is determined by averaging the detected-box areas over all tracks for that object class in the training set used to train HMMs. An optional size adjective for a track is generated by comparing the average detected-box area a for that track to a\u0304object class:\nbig a \u2265 \u03b2object classa\u0304object class small a \u2264 \u03b1object classa\u0304object class\nThe per-object-class cutoff ratios \u03b1object class and \u03b2object class are computed to equally tripartition the distribution of perobject-class mean object sizes on the training set. Optional shape adjectives are generated in a similar fashion. Per-object-class mean aspect ratios r\u0304object class are determined in addition to the per-object-class mean object sizes a\u0304object class. Optional shape adjectives for a track are generated by comparing the average detected-box aspect ratio r and area a for that track to these means:\ntall r \u2264 0.7r\u0304object class \u2227 a \u2265 \u03b2object classa\u0304object class short r \u2265 1.3r\u0304object class \u2227 a \u2264 \u03b1object classa\u0304object class narrow r \u2264 0.7r\u0304object class \u2227 a \u2264 \u03b1object classa\u0304object class wide r \u2265 1.3r\u0304object class \u2227 a \u2265 \u03b2object classa\u0304object class\nTo avoid generating shape and size adjectives for unstable tracks, they are only generated when the detection-score variance and the detected aspect-ratio variance for the track are below specified thresholds. Optional restrictive modifiers are generated from the object class using the mapping in Table 2(b). Person-pose adjectives are generated from aggregate body-posture information for the track: object\nclass, normalized part displacements, and body-posture codebook indices. We generate all applicable adjectives except for color and person pose. Following the Gricean Maxim of Quantity (Grice, 1975), we only generate color and person-pose adjectives if needed to prevent coreference of nonhuman event participants. Finally, we generate an initial adjective other, as needed to prevent coreference. Generating other does not allow generation of the determiner the in place of that or some. We order any adjectives generated so that other comes first, followed by size, shape, color, and restrictive modifiers, in that order.\nFor two-track HMMs where neither participant moves, a prepositional phrase is generated for subject noun phrases to describe the static 2D spatial relation between the subject X and the reference object Y from the perspective of the viewer, as shown in Fig. 6(c)."}, {"heading": "4 Experimental results", "text": "We used the HMMs generated for Experiment III to compute likelihoods for each video in our test set paired with each of the 48 action classes. For each video, we generated sentences corresponding to the three most-likely action classes. Fig. 7 shows key frames from four videos in our test set along with the sentence generated for the most-likely action class. Human judges rated each videosentence pair to assess whether the sentence was true of the video and whether it described a salient event depicted in that video. 26.7% (601/2247) of the video-sentence pairs\nwere deemed to be true and 7.9% (178/2247) of the videosentence pairs were deemed to be salient. When restricting consideration to only the sentence corresponding to the single most-likely action class for each video, 25.5% (191/749) of the video-sentence pairs were deemed to be true and 8.4% (63/749) of the video-sentence pairs were deemed to be salient. Finally, for 49.4% (370/749) of the videos at least one of the three generated sentences was deemed true and for 18.4% (138/749) of the videos at least one of the three generated sentences was deemed salient."}, {"heading": "5 Conclusion", "text": "Integration of Language and Vision (Aloimonos et al., 2011; Barzialy et al., 2003; Darrell et al., 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time. There has also been work on generating sentential descriptions of static images (Farhadi et al., 2009; Kulkarni et al., 2011; Yao et al., 2010). Yet we are unaware of any prior work that generates as rich sentential video descriptions as we describe here. Producing such rich descriptions requires determining event participants, the mapping of such participants to roles in the event, and their motion and properties. This is incompatible with common approaches to event recognition, such as spatiotemporal bags of words, spatiotemporal volumes, and tracked feature points that cannot determine such information. The approach presented here recovers the information needed to generate rich sentential descriptions by using detectionbased tracking and a body-posture codebook. We demonstrated the efficacy of this approach on a corpus 749 videos."}, {"heading": "Acknowledgments", "text": "This work was supported, in part, by NSF grant CCF0438806, by the Naval Research Laboratory under Contract Number N00173-10-1-G023, by the Army Research Laboratory accomplished under Cooperative Agreement Number W911NF-10-2-0060, and by computational resources provided by Information Technology at Purdue through its Rosen Center for Advanced Computing. Any views, opinions, findings, conclusions, or recommendations contained or expressed in this document or material are those of the author(s) and do not necessarily reflect or represent the views or official policies, either expressed or implied, of NSF, the Naval Research Laboratory, the Office of Naval Research, the Army Research Laboratory, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein."}], "references": [{"title": "AAAI Workshop on Language-Action Tools for Cognitive Artificial Agents: Integrating Vision", "author": ["Y. Aloimonos", "L. Fadiga", "G. Metta", "K. Pastra", "editors"], "venue": "Action and Language,", "citeRegEx": "Aloimonos et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Aloimonos et al\\.", "year": 2011}, {"title": "People-tracking-bydetection and people-detection-by-tracking", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "In CVPR, pages", "citeRegEx": "Andriluka et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Andriluka et al\\.", "year": 2008}, {"title": "Actions as space-time shapes", "author": ["M. Blank", "L. Gorelick", "E. Shechtman", "M. Irani", "R. Basri"], "venue": "In ICCV,", "citeRegEx": "Blank et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blank et al\\.", "year": 2005}, {"title": "Learning and recognizing human dynamics in video sequences", "author": ["Christoph Bregler"], "venue": "In CVPR,", "citeRegEx": "Bregler.,? \\Q1997\\E", "shortCiteRegEx": "Bregler.", "year": 1997}, {"title": "Describing objects by their attributes", "author": ["Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In CVPR,", "citeRegEx": "Farhadi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2009}, {"title": "Cascade object detection with deformable part models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester"], "venue": "In CVPR,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": null, "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Towards 3-d model-based tracking and recognition of human movement", "author": ["D.M. Gavrila", "L.S. Davis"], "venue": "In International Workshop on Face and Gesture Recognition,", "citeRegEx": "Gavrila and Davis.,? \\Q1995\\E", "shortCiteRegEx": "Gavrila and Davis.", "year": 1995}, {"title": "Logic and conversation", "author": ["H.P. Grice"], "venue": "Syntax and Semantics 3: Speech Acts,", "citeRegEx": "Grice.,? \\Q1975\\E", "shortCiteRegEx": "Grice.", "year": 1975}, {"title": "Object, scene and actions: Combining multiple features for human action recognition", "author": ["Nazli Ikizler-Cinibis", "Stan Sclaroff"], "venue": "In ECCV,", "citeRegEx": "Ikizler.Cinibis and Sclaroff.,? \\Q2010\\E", "shortCiteRegEx": "Ikizler.Cinibis and Sclaroff.", "year": 2010}, {"title": "Semantics and Cognition", "author": ["Ray Jackendoff"], "venue": null, "citeRegEx": "Jackendoff.,? \\Q1983\\E", "shortCiteRegEx": "Jackendoff.", "year": 1983}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg"], "venue": "In CVPR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Local velocity-adapted motion events for spatio-temporal recognition", "author": ["I. Laptev", "B. Caputo", "C. Schuldt", "T. Lindeberg"], "venue": "CVIU, 108(3):207\u201329,", "citeRegEx": "Laptev et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Laptev et al\\.", "year": 2007}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "In CVPR,", "citeRegEx": "Laptev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Laptev et al\\.", "year": 2008}, {"title": "Recognizing realistic actions from videos \u201cin the wild", "author": ["J. Liu", "J. Luo", "M. Shah"], "venue": "In CVPR, pages 1996\u20132003,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Integration of Natural Language and Vision Processing, volume I\u2013IV", "author": ["P. McKevitt", "editor"], "venue": null, "citeRegEx": "McKevitt and editor.,? \\Q1996\\E", "shortCiteRegEx": "McKevitt and editor.", "year": 1996}, {"title": "Unsupervised learning of human action categories using spatial-temporal words", "author": ["J.C. Niebles", "H. Wang", "L. Fei-Fei"], "venue": null, "citeRegEx": "Niebles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Niebles et al\\.", "year": 2008}, {"title": "A threshold selection method from gray-level histograms", "author": ["N. Otsu"], "venue": "IEEE Trans. on Systems, Man and Cybernetics,", "citeRegEx": "Otsu.,? \\Q1979\\E", "shortCiteRegEx": "Otsu.", "year": 1979}, {"title": "Learnability and Cognition", "author": ["Steven Pinker"], "venue": null, "citeRegEx": "Pinker.,? \\Q1989\\E", "shortCiteRegEx": "Pinker.", "year": 1989}, {"title": "Action MACH: A spatio-temporal maximum average correlation height filter for action recognition", "author": ["M.D. Rodriguez", "J. Ahmed", "M. Shah"], "venue": "In CVPR,", "citeRegEx": "Rodriguez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2008}, {"title": "Recognizing human actions: A local SVM approach", "author": ["C. Schuldt", "I. Laptev", "B. Caputo"], "venue": "In ICPR, pages 32\u20136,", "citeRegEx": "Schuldt et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schuldt et al\\.", "year": 2004}, {"title": "A 3-dimensional SIFT descriptor and its application to action recognition", "author": ["P. Scovanner", "S. Ali", "M. Shah"], "venue": "In International Conference on Multimedia,", "citeRegEx": "Scovanner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Scovanner et al\\.", "year": 2007}, {"title": "Good features to track", "author": ["J. Shi", "C. Tomasi"], "venue": "In CVPR, pages 593\u2013600,", "citeRegEx": "Shi and Tomasi.,? \\Q1994\\E", "shortCiteRegEx": "Shi and Tomasi.", "year": 1994}, {"title": "HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human", "author": ["L. Sigal", "A. Balan", "M.J. Black"], "venue": "motion. IJCV,", "citeRegEx": "Sigal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sigal et al\\.", "year": 2010}, {"title": "A maximum-likelihood approach to visual event classification", "author": ["J.M. Siskind", "Q. Morris"], "venue": "In ECCV,", "citeRegEx": "Siskind and Morris.,? \\Q1996\\E", "shortCiteRegEx": "Siskind and Morris.", "year": 1996}, {"title": "Realtime American sign language recognition using desk and wearable computer based video", "author": ["Thad Starner", "Joshua Weaver", "Alex Pentland"], "venue": null, "citeRegEx": "Starner et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Starner et al\\.", "year": 1998}, {"title": "Detection and tracking of point features", "author": ["C. Tomasi", "T. Kanade"], "venue": "Technical Report CMU-CS-91-132,", "citeRegEx": "Tomasi and Kanade.,? \\Q1991\\E", "shortCiteRegEx": "Tomasi and Kanade.", "year": 1991}, {"title": "Convolutional codes and their performance in communication systems", "author": ["A.J. Viterbi"], "venue": "IEEE Trans. on Communication,", "citeRegEx": "Viterbi.,? \\Q1971\\E", "shortCiteRegEx": "Viterbi.", "year": 1971}, {"title": "Human action recognition by semilatent topic models", "author": ["Y. Wang", "G. Mori"], "venue": "PAMI, 31(10):1762\u201374,", "citeRegEx": "Wang and Mori.,? \\Q2009\\E", "shortCiteRegEx": "Wang and Mori.", "year": 2009}, {"title": "Motion based event recognition using HMM", "author": ["Gu Xu", "Yu-Fei Ma", "HongJiang Zhang", "Shiqiang Yang"], "venue": "In ICPR,", "citeRegEx": "Xu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2002}, {"title": "Articulated pose estimation using flexible mixtures of parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "In CVPR,", "citeRegEx": "Yang and Ramanan.,? \\Q2011\\E", "shortCiteRegEx": "Yang and Ramanan.", "year": 2011}, {"title": "I2t: Image parsing to text description", "author": ["B.Z. Yao", "Xiong Yang", "Liang Lin", "Mun Wai Lee", "Song-Chun Zhu"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "This means that one cannot use common approaches to event recognition, such as spatiotemporal bags of words (Laptev et al., 2007; Niebles et al., 2008; Scovanner et al., 2007), spatiotemporal volumes (Blank et al.", "startOffset": 108, "endOffset": 175}, {"referenceID": 16, "context": "This means that one cannot use common approaches to event recognition, such as spatiotemporal bags of words (Laptev et al., 2007; Niebles et al., 2008; Scovanner et al., 2007), spatiotemporal volumes (Blank et al.", "startOffset": 108, "endOffset": 175}, {"referenceID": 21, "context": "This means that one cannot use common approaches to event recognition, such as spatiotemporal bags of words (Laptev et al., 2007; Niebles et al., 2008; Scovanner et al., 2007), spatiotemporal volumes (Blank et al.", "startOffset": 108, "endOffset": 175}, {"referenceID": 2, "context": ", 2007), spatiotemporal volumes (Blank et al., 2005; Laptev et al., 2008; Rodriguez et al., 2008), and tracked feature points (Liu et al.", "startOffset": 32, "endOffset": 97}, {"referenceID": 13, "context": ", 2007), spatiotemporal volumes (Blank et al., 2005; Laptev et al., 2008; Rodriguez et al., 2008), and tracked feature points (Liu et al.", "startOffset": 32, "endOffset": 97}, {"referenceID": 19, "context": ", 2007), spatiotemporal volumes (Blank et al., 2005; Laptev et al., 2008; Rodriguez et al., 2008), and tracked feature points (Liu et al.", "startOffset": 32, "endOffset": 97}, {"referenceID": 14, "context": ", 2008), and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009) that do not determine the class of participant objects and the roles that they play.", "startOffset": 36, "endOffset": 97}, {"referenceID": 20, "context": ", 2008), and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009) that do not determine the class of participant objects and the roles that they play.", "startOffset": 36, "endOffset": 97}, {"referenceID": 28, "context": ", 2008), and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009) that do not determine the class of participant objects and the roles that they play.", "startOffset": 36, "endOffset": 97}, {"referenceID": 10, "context": "A common assumption in Linguistics (Jackendoff, 1983; Pinker, 1989) is that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants.", "startOffset": 35, "endOffset": 67}, {"referenceID": 18, "context": "A common assumption in Linguistics (Jackendoff, 1983; Pinker, 1989) is that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants.", "startOffset": 35, "endOffset": 67}, {"referenceID": 2, "context": "For example, the WEIZMANN dataset (Blank et al., 2005) and the KTH dataset (Schuldt et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 20, "context": ", 2005) and the KTH dataset (Schuldt et al., 2004) depict events with a single human participant, not ones where people interact with other people or objects.", "startOffset": 28, "endOffset": 50}, {"referenceID": 19, "context": "Moreover, such datasets, as well as the SPORTS ACTIONS dataset (Rodriguez et al., 2008) and the YOUTUBE dataset (Liu et al.", "startOffset": 63, "endOffset": 87}, {"referenceID": 14, "context": ", 2008) and the YOUTUBE dataset (Liu et al., 2009), often make action-class distinctions that are irrelevant to the choice of verb, e.", "startOffset": 32, "endOffset": 50}, {"referenceID": 28, "context": "swing Other datasets, such as the BALLET dataset (Wang and Mori, 2009) and the UCF50 dataset (Liu et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 14, "context": "swing Other datasets, such as the BALLET dataset (Wang and Mori, 2009) and the UCF50 dataset (Liu et al., 2009), depict larger-scale activities that bear activity-class names that are not well suited to sentential description, e.", "startOffset": 93, "endOffset": 111}, {"referenceID": 22, "context": "The Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker", "startOffset": 30, "endOffset": 77}, {"referenceID": 26, "context": "The Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker", "startOffset": 30, "endOffset": 77}, {"referenceID": 27, "context": "A dynamicprogramming algorithm (Viterbi, 1971) is then used to select an optimal set of detections that is temporally coherent with optical flow, yielding a set of object tracks for each video.", "startOffset": 31, "endOffset": 46}, {"referenceID": 24, "context": "Hidden Markov Models (HMMs) are then employed as time-series classifiers to yield verb labels for each video (Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005), together with the object tracks of the participants in the action described by that verb along with the roles they play.", "startOffset": 109, "endOffset": 201}, {"referenceID": 25, "context": "Hidden Markov Models (HMMs) are then employed as time-series classifiers to yield verb labels for each video (Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005), together with the object tracks of the participants in the action described by that verb along with the roles they play.", "startOffset": 109, "endOffset": 201}, {"referenceID": 28, "context": "Hidden Markov Models (HMMs) are then employed as time-series classifiers to yield verb labels for each video (Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005), together with the object tracks of the participants in the action described by that verb along with the roles they play.", "startOffset": 109, "endOffset": 201}, {"referenceID": 17, "context": "The offset is then taken to be the minimum of the value that maximizes the between-class variance (Otsu, 1979) when bipartitioning this histogram and the trained acceptance threshold offset by a fixed, but small, amount (0.", "startOffset": 98, "endOffset": 110}, {"referenceID": 1, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 3, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 7, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 23, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 30, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 24, "context": "all possible such mappings and selecting the one with the highest likelihood (Siskind and Morris, 1996).", "startOffset": 77, "endOffset": 103}, {"referenceID": 8, "context": "Following the Gricean Maxim of Quantity (Grice, 1975), we only generate color and person-pose adjectives if needed to prevent coreference of nonhuman event participants.", "startOffset": 40, "endOffset": 53}, {"referenceID": 2, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 13, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 14, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 19, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 20, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 24, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 25, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 28, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 4, "context": "There has also been work on generating sentential descriptions of static images (Farhadi et al., 2009; Kulkarni et al., 2011; Yao et al., 2010).", "startOffset": 80, "endOffset": 143}, {"referenceID": 11, "context": "There has also been work on generating sentential descriptions of static images (Farhadi et al., 2009; Kulkarni et al., 2011; Yao et al., 2010).", "startOffset": 80, "endOffset": 143}, {"referenceID": 31, "context": "There has also been work on generating sentential descriptions of static images (Farhadi et al., 2009; Kulkarni et al., 2011; Yao et al., 2010).", "startOffset": 80, "endOffset": 143}], "year": 2012, "abstractText": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture.", "creator": "LaTeX with hyperref package"}}}