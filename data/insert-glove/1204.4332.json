{"id": "1204.4332", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2012", "title": "Designing generalisation evaluation function through human-machine dialogue", "abstract": "Automated nahal generalisation humboldt-universit\u00e4t has forthrightness known important improvements sochua these last c-61 few pbpost.com years. unpopular However, an issue semi-wild that still deserves more dumuzi study mezan concerns polytope the automatic evaluation of eidetic generalised data. 180-million Indeed, multi-billion-dollar many 17,000-square automated generalisation systems pvn require 100-97 the utilisation leakeys of acclaims an 1,240-mile evaluation function submissively to ennes automatically formula assess generalisation outcomes. In nejedl\u00fd this sk\u00f6tkonung paper, we malone propose a parliamentarianism new accruing approach dendrobatidae dedicated someshwar to the sdap design of veritably such \u00e5ngstr\u00f6m a irakere function. This approach radwaniya allows an imperfectly defined htms evaluation samana function tayyiba to be revised rouzi through steffans a 91-yard man - pancrazio machine dialogue. hissed The sinden user arpeggiated gives its maerz preferences rietberg to pashiardis the system faenza by tambour comparing generalisation koppa outcomes. Machine Learning bartolomei techniques reggie are bhimdev then used to danskin improve andretti the evaluation function. An experiment carried 0.52 out on buildings shows 3,383 that storyline our approach significantly ileus improves gold-digger generalisation evaluation skouras functions defined by leinonen users.", "histories": [["v1", "Thu, 19 Apr 2012 12:10:10 GMT  (303kb)", "http://arxiv.org/abs/1204.4332v1", null]], "reviews": [], "SUBJECTS": "cs.HC cs.LG", "authors": ["patrick taillandier", "julien gaffuri"], "accepted": false, "id": "1204.4332"}, "pdf": {"name": "1204.4332.pdf", "metadata": {"source": "CRF", "title": "Designing generalisation evaluation function through human-machine dialogue", "authors": ["Patrick Taillandier", "Julien Gaffuri", "Quang Buu", "Ha Noi", "Viet Nam"], "emails": ["patrick.taillandier@gmail.com", "julien.gaffuri@ign.fr"], "sections": [{"heading": null, "text": "Designing generalisation evaluation function through human-machine dialogue\nPatrick Taillandier1, Julien Gaffuri2\n1IRD, UMI UMMISCO 209,\n32 avenue Henri Varagnat, 93143 Bondy, France Email: patrick.taillandier@gmail.com\n2IFI, MSI, UMI 209,\nngo 42 Ta Quang Buu, Ha Noi, Viet Nam\n3 IGN \u2013 COGIT laboratory \u2013 Paris-Est University 73 avenue de Paris, 94165 Saint-Mand\u00e9 cedex, France\nEmail: julien.gaffuri@ign.fr"}, {"heading": "1. Introduction", "text": "A classic approach in automated generalisation consists in formalising generalisation as an optimisation problem: the goal is to find a state of the data that maximises an evaluation function that is supposed to assess the generalisation state of the data, according to the user need (e.g. Wilson et al., 2003). A key issue of this approach concerns the design of this evaluation function. Unfortunately, designing such a function remains a difficult task. Indeed, while the final user of the generalised data can easily describe his/her need in natural language, it is often far more difficult to express his/her expectations in a formal language that can be used by generalisation systems.\nIn this paper, we propose an approach dedicated to generalisation evaluation functions design. An evaluation function previously designed by a user is improved through a dialogue between the user and a generalisation system. The idea is to collect user preferences by letting him/her compare different generalisation results for a same object.\nIn Section 2, the context of this work is introduced. Section 3 is devoted to the presentation of our approach. Section 4 describes an experiment carried out for building generalisation and Section 5 concludes."}, {"heading": "2. Context", "text": ""}, {"heading": "2.1 Automated evaluation of generalisation results", "text": "If many works focus on the generalisation process automation, only a few deal with automatic evaluation of generalisation outcomes. A classic approach consists in evaluating the generalisation quality by means of a set of constraints translating the expectation towards the generalisation (Beard, 1991). The constraint assessment is often represented by a numeric satisfaction value. The overall generalisation is evaluated by aggregating all the constraint satisfaction values. If the computation of individual constraint satisfaction values is often well-managed, the definition of the aggregating function remains complex (Bard, 2004). This paper focuses on this problem."}, {"heading": "2.2 Design of an evaluation function", "text": "The evaluation function design is a complex problem which was studied in various fields. Many works were interested in the definition of these functions for specific problems (Wimmer et al., 2008) but few proposed general approaches for helping optimisation systems users to define it. A classic approach to solve this problem consists in using supervised machine learning techniques. These techniques consist in inducing a general model from examples labeled by an expert. In this context, it is possible to learn an evaluation function from examples assessed by an expert. This approach was used in several works, like (Wimmer et al., 2008) in the domain of computer vision, and (Clancy et al., 2007) for the learning of cognitive radio. The drawback of this approach is the complexity for experts to quantitatively evaluate the quality of a solution. Indeed, it is sometimes difficult for experts to directly translate the quality of a solution by a numeric value."}, {"heading": "2.3 Formalisation of the evaluation function design", "text": "We assume that a set of constraints, which assessment is represented by a numeric satisfaction value, is defined. The higher the assessment value, the more satisfied the constraint is, thus better the generalisation is. We propose to formulate the aggregation function by a weighted means balanced by a power.\nLet C be the constraint set considered, wi the weight associated to a constraint i, Vali(gen), the assessment value of the constraint i for the generalisation gen, and p, an integer higher or equal to 1. The evaluation function is defined as follows:\np (gen)Valw\nw =n)quality(ge p Ci i p i\nCi\np i\n1\n)(1 \u23a5 \u23a5 \u23a5\n\u23a6\n\u23a4\n\u23a2 \u23a2 \u23a2 \u23a3\n\u23a1\n\u22c5\u22c5\u2211 \u2211 \u2208 \u2208\nThe role of p is to control the relative weight of the most satisfied constraints over the less satisfied ones: the higher is p, the more satisfied constraints are taken into account in the overall quality of the generalisation."}, {"heading": "3. Proposed approach", "text": ""}, {"heading": "3.1. General approach", "text": "We propose an approach to design the generalisation evaluation function based on the presentation of comparisons between generalisations to the user and the learning of an evaluation function from the collected preference data. This approach is close to the one proposed by (Hubert & Ruas, 2003) concerning the parameterisation of the generalisation process. However, a difference is that the user will not just select his/her preferred generalisation among a set, but the user will compare these generalisations. Our approach is composed of 3 steps that are described in the next sections."}, {"heading": "3.2. Initialisation of the comparison set", "text": "The first step concerns the generation of the comparisons, which will be shown to the user to capture his/her needs. A comparison is a pair of different generalisations of the same object. In order to build the comparison set, some geographic objects to generalise are selected. Two different generalisations of these objects are then computed and stored in the comparison set."}, {"heading": "3.3 Capture of the user preferences", "text": "The second step concerns capture of the user preferences: comparisons are successively presented to the user, who gives his/her preference for each of them. This sequence is reiterated until a specific number of comparisons have been presented to the user.\nFor each comparison between two generalisations A and B, the user can choose:\n\u2022 Generalisation A/B is far better than Generalisation B/A \u2022 Generalisation A/B is better than Generalisation B/A \u2022 Generalisation A/B is slightly better than Generalisation B/A \u2022 Generalisation A and Generalisation B are equivalent\nFigure 1 presents the comparison interface of the developed prototype."}, {"heading": "3.4. Evaluation function definition", "text": "The last step consists in learning an evaluation function from the captured user preferences: the parameter values (i.e. the constraint weights wi and the power p) that best fits the preferences given by the user during the previous step are computed. We propose to formulate this problem as a minimisation problem. We define a global error function that represents the inadequacy between an evaluation function (and thus the parameter values assignment) and the user preferences. Our goal is to find the parameter values that minimise the global error function.\nLet feval(gen) be the current evaluation function that evaluates the quality of a generalisation gen; cgen1,gen2 a comparison between two generalisations, gen1 and gen2; pc the user preference for the comparison c. We define the function comp(c, feval, pc) that determines for a comparison c if the user preference pc is compatible with the evaluation function feval, i.e. if the preference is consistent with the quality order obtained by applying the evaluation function. comp(c, feval, pc) is computed as follows:\nThis formula introduces the parameters Valmin FB , Valmin B , Valmax B , Valmin SB , SBmaxVal\nand Valeq that confer a fuzzy aspect to the notion of compatibility. The global error function proposed corresponds to the percentage of comparisons of the comparison sample C that are incompatible with the evaluation function feval:\n| | \u2211\u2208 \u22c5 Cc cevalobj )p,fcomp(c,C\n=C),Error(f 100\nParameter values that minimise Error(fobj, Comp) have to be found. In order to facilitate the search process, we propose to use an evaluation function initially defined by an expert. Indeed, we make the hypothesis that, most of the time, experts \u2013that usually have a good command of the generalisation system-- can design a good generic evaluation function, which can be adapted for some more specific needs. In consequence, we propose to use a local search algorithm and more particularly a tabu search (Glover, 1989). The principle of this kind of algorithm is to start with an initial solution and to attempt to improve it by exploring its neighbourhood. These algorithms are usually very effective for this kind of search problem. Local search algorithms require the definition of the notions of \u2018solution\u2019 and \u2018solution neighbourhood\u2019. For our problem, a solution is a parameters assignment (weights wi, and power p). We define the neighbourhood of a solution as the set of solutions for which only one parameter has its value changed."}, {"heading": "4. Case study: evaluation of building generalisation", "text": ""}, {"heading": "4.1. Context", "text": "Our experiment use a generalisation system based on the AGENT model (Barrault et al., 2001). In this model, the quality of the generalisation is evaluated by a set of constraints. The AGENT model has been the core of numerous research works and is used for map production in several mapping agencies. However, the question of the constraint weight assignment is still asked (Bard, 2004).\nWe propose to experiment our method for building generalisation for a traditional 1:25000 scale topographic map. Five constraints are used. The input data are taken from the BDTopo\u00ae, a one meter resolution topographic database.\nThe initial evaluation function was designed by an expert of the AGENT model. We defined two set of 100 different comparisons each (the learning and the testing set). The learning set is used to learn the evaluation function, the testing to assess the quality of the initial and learnt evaluation functions."}, {"heading": "4.2. Results and discussion", "text": "Table 1 presents the results on the two comparison sets. It shows for each evaluation function and comparison sets the global error (c.f. Section 3.4).\nThese results reveal that the learnt function has allowed an improvement of the global error: for both learning and testing sets, the global errors of the initial function are higher than for the learnt function. However, the quality improvement after the use of the method is only of 11% for the test set. An explanation is the lack of constraints (for example, an orientation constraint). For example, when a comparison composed of two building generalisations, which differ only in term of orientation is shown, the user always prefers the one whose orientation is close to the building initial orientation. Because there is no orientation constraint taken into account into the evaluation function, the difference of the two generalisations can not be measured by the system, and the reason of the different assessment by the user remains ignored. In this context, our approach, through an examination of the incompatible comparisons, can help to determine some important missing constraints and identify faulty ones."}, {"heading": "5. Conclusion", "text": "In this paper, we presented an approach dedicated to the definition of a generalisation evaluation function. We proposed a method based on a human-machine dialogue and the capture of user preferences on generalisation samples. An experiment carried out in the domain of cartographic generalisation showed how our approach can help users to define better evaluation functions.\nThis work is at its beginning. In the near future, we plan to carry out more experiments, in particular to study the impact of the initial evaluation function on the results.\nOur long-term purpose is to provide a method to learn the user preferences concerning all objects and group of objects contained in its data. The last stage of this research would be to automatically learn a user final evaluation method for a complete map piece. Such a system would be able to make an automatic interview of the user, allowing him to give his specific requirements for all characteristics of the map."}], "references": [{"title": "Quality Assessment of Cartographic Generalisation", "author": ["S Bard"], "venue": "Transactions in GIS,", "citeRegEx": "Bard,? \\Q2004\\E", "shortCiteRegEx": "Bard", "year": 2004}, {"title": "Integrating multi-agent, objectoriented, and algorithmic techniques for improved automated map generalization", "author": ["M Barrault", "N Regnauld", "C Duch\u00eane", "K Haire", "C Baeijs", "Y Demazeau", "P Hardy", "W Mackaness", "A Ruas", "R Weibel"], "venue": null, "citeRegEx": "Barrault et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Barrault et al\\.", "year": 2001}, {"title": "Constraints on rule formation", "author": ["K Beard"], "venue": null, "citeRegEx": "Beard,? \\Q1991\\E", "shortCiteRegEx": "Beard", "year": 1991}, {"title": "Applications of Machine Learning to Cognitive Radio Networks", "author": ["C Clancy", "J Hecker", "E Stuntebeck", "T O'Shea"], "venue": "Wireless Communications,", "citeRegEx": "Clancy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Clancy et al\\.", "year": 2007}, {"title": "A method based on samples to capture user needs for generalisation, workshop on progress in automated map generalisation, Paris", "author": ["F Hubert", "A Ruas"], "venue": null, "citeRegEx": "Hubert and Ruas,? \\Q2003\\E", "shortCiteRegEx": "Hubert and Ruas", "year": 2003}, {"title": "Reducing graphic conflict in scale reduced maps using a genetic algorithm. Workshop on progress in automated map generalisation, Paris", "author": ["ID Wilson", "JM Ware", "JA Ware"], "venue": null, "citeRegEx": "Wilson et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2003}, {"title": "Learning local objective functions for robust face model fitting", "author": ["M Wimmer", "F Stulp", "S Pietzsch", "B Radig"], "venue": "IEEE Transactions on Pattern Analysis andMachine Intelligence,", "citeRegEx": "Wimmer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wimmer et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "A classic approach consists in evaluating the generalisation quality by means of a set of constraints translating the expectation towards the generalisation (Beard, 1991).", "startOffset": 157, "endOffset": 170}, {"referenceID": 0, "context": "If the computation of individual constraint satisfaction values is often well-managed, the definition of the aggregating function remains complex (Bard, 2004).", "startOffset": 146, "endOffset": 158}, {"referenceID": 6, "context": "Many works were interested in the definition of these functions for specific problems (Wimmer et al., 2008) but few proposed general approaches for helping optimisation systems users to define it.", "startOffset": 86, "endOffset": 107}, {"referenceID": 6, "context": "This approach was used in several works, like (Wimmer et al., 2008) in the domain of computer vision, and (Clancy et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 3, "context": ", 2008) in the domain of computer vision, and (Clancy et al., 2007) for the learning of cognitive radio.", "startOffset": 46, "endOffset": 67}, {"referenceID": 1, "context": "Context Our experiment use a generalisation system based on the AGENT model (Barrault et al., 2001).", "startOffset": 76, "endOffset": 99}, {"referenceID": 0, "context": "However, the question of the constraint weight assignment is still asked (Bard, 2004).", "startOffset": 73, "endOffset": 85}], "year": 2012, "abstractText": null, "creator": "Word"}}}