{"id": "1205.2661", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs", "abstract": "neier We watashi provide scappoose an algorithm that buck-tick achieves 2-oxidoreductase the optimal ingraham regret aldous rate cherubic in kedron an pricier unknown trenchard weakly communicating Markov Decision Process (stockinged MDP ). The algorithm yelping proceeds joinder in kovas episodes coactivators where, s-adenosyl in each (817) episode, it picks a policy using corbi\u00e8res regularization bookmarking based .342 on rcck the working-age span dello of tnt the wist optimal yilmaz bias vector. For gunas an all-out MDP with S michaelmas states and mukasa A actions whose optimal bias eunice vector mangue has span bounded meretricious by H, we oversubscription show jolyon a goforth regret bound of ~ sorcerers O (HSpAT ). relics We also tregaron relate the span scilly to lacantera various steamy diameter - 287.9 like francioni quantities associated roseli with the pollan MDP, cgas demonstrating how silpa our results improve vajrasattva on sirim previous speared regret pml bounds.", "histories": [["v1", "Wed, 9 May 2012 14:47:06 GMT  (177kb)", "http://arxiv.org/abs/1205.2661v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["peter l bartlett", "ambuj tewari"], "accepted": false, "id": "1205.2661"}, "pdf": {"name": "1205.2661.pdf", "metadata": {"source": "CRF", "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs", "authors": ["Peter L. Bartlett", "Ambuj Tewari"], "emails": [], "sections": [{"heading": null, "text": "We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of O\u0303(HS \u221a AT ). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds."}, {"heading": "1 INTRODUCTION", "text": "In reinforcement learning, an agent interacts with an environment while trying to maximize the total reward it accumulates. Markov Decision Processes (MDPs) are the most commonly used model for the environment. To every MDP is associated a state space S and an action space A. Suppose there are S states and A actions. The parameters of the MDP then consist of S \u00b7 A state transition distributions Ps,a and S \u00b7 A rewards r(s, a). When the agent takes action a in state s, it receives reward r(s, a) and the probability that it moves to state s\u2032 is Ps,a(s\u2032). The agent does not know the parameters Ps,a and r(s, a) of the MDP in advance but has to learn them by directly interacting with the environment. In doing so, it faces the exploration vs. exploitation trade-off that Kearns and Singh [2002] succinctly describe as,\n\u201c. . . should the agent exploit its cumulative experience so far, by executing the action that currently seems best, or should it execute a different action, with the hope of gaining information or experience that could lead\nto higher future payoffs? Too little exploration can prevent the agent from ever converging to the optimal behavior, while too much exploration can prevent the agent from gaining near-optimal payoff in a timely fashion.\u201d\nSuppose the agent uses an algorithm G to choose its actions based on the history of its interactions with the MDP starting from some initial state s1. Denoting the (random) reward obtained at time t by rt, the algorithm\u2019s expected reward until time T is\nRG(s1, T ) = E [ T\u2211 t=1 rt ] .\nSuppose \u03bb? is the optimal per-step reward. An important quantity used to measure how well G is handling the exploration vs. exploitation trade-off is the regret,\n\u2206G(s1, T ) = \u03bb?T \u2212RG(s1, T ) .\nIf \u2206G(s1, T ) is o(T ) then G is indeed learning something useful about the MDP since its expected average reward then converges to the optimal value \u03bb? (which can only be computed with the knowledge of the MDP parameters) in the limit T \u2192\u221e.\nHowever, asymptotic results are of limited value and therefore it is desirable to have finite time bounds on the regret. To obtain such results, one has to work with a restricted class of MDPs. In the theory of MDPs, four fundamental subclasses have been studied. Unless otherwise specified, by a policy, we mean a deterministic stationary policy, i.e. simply a map \u03c0 : S \u2192 A.\nErgodic Every policy induces a single recurrent class, i.e. it is possible to reach any state from any other state.\nUnichain Every policy induces a single recurrent class plus a possibly empty set of transient states.\nCommunicating For every s1, s2 \u2208 S, there is some policy that takes one from s1 to s2.\nWeakly Communicating The state space S decomposes into two sets: in the first, each state is reachable from every other state in the set under some policy; in the second, all states are transient under all policies.\nUnless we modify our criterion, it is clear that regret guarantees cannot be given for general MDPs since the agent might be placed in a state from which reaching the optimally rewarding states is impossible. So, we consider the most general subclass:weakly communicating MDPs. For such MDPs, the optimal gain \u03bb? is state independent and is obtained by solving the following optimality equations,\nh? + \u03bb?e = max a\u2208A\n( r(s, a) + P>s,ah ? ) . (1)\nHere, h? is known as the bias vector. Note that if h? is a bias vector then so is h? + ce where e is the all 1\u2019s vector. If we want to make the dependence of h?(s) and \u03bb? on the underlying MDP M explicit, we will write them as h?(s;M) and \u03bb?(M) respectively.\nIn this paper, we give an algorithm Regal, that achieves O\u0303 ( sp(h?(M))S \u221a AT )\nregret with probability at least 1 \u2212 \u03b4 when started in any state of a weakly communicating MDP M . Here sp(h) is the span of h defined as,\nsp(h) := max s\u2208S h(s)\u2212min s\u2208S h(s) .\nThe O\u0303(\u00b7) notation hides factors that are logarithmic in S,A, T and 1/\u03b4.\nRegal is based on the idea of regularization that has been successfully applied to obtain low regret algorithms in other settings involving decision making under uncertainty. The main idea is simple to describe. Using its experience so far, the agent can build a set M such that with high probability, the true MDP lies in M. If the agent assumes that it is in the best of all possible worlds, it would choose an MDP M \u2032 \u2208M to maximize \u03bb?(M) and follow the optimal policy for M \u2032. Instead, Regal picks M \u2032 to trade-off high gain for low span, by maximizing the following regularized objective,\n\u03bb?(M)\u2212 C sp(h?(M)) ."}, {"heading": "1.1 RELATED WORK", "text": "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar\nand Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities. In the machine learning literature, finite time bounds for undiscounted MDPs were pioneered by Kearns and Singh [2002]. Their seminal work spawned a long thread of research [Brafman and Tennenholtz, 2002, Kakade, 2003, Strehl and Littman, 2005, Strehl et al., 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off.\nThe results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively. Recently, Auer et al. [2009a] gave an algorithm for communicating MDPs whose expected regret after T steps is, with high probability,\nO\u0303(D(M)S \u221a AT ) .\nHere, D(M) is the diameter of the MDP M defined as\nD(M) := max s1 6=s2 min \u03c0 T\u03c0s1\u2192s2 ,\nwhere T\u03c0s1\u2192s2 is the expected number of steps it takes policy \u03c0 to get to s2 from s1. By definition, any communicating MDP has finite diameter. Let O denote the set of average-reward optimal policies. Previous work had considered various notions of \u201cdiameter\u201d, such as,\nDworst(M) := max \u03c0 max s1 6=s2 T\u03c0s1\u2192s2\nDopt(M) := min \u03c0\u2208O max s1 6=s2 T\u03c0s1\u2192s2 .\nClearly D \u2264 Dopt \u2264 Dworst, and so, everything else being the same, an upper bound on regret in terms of D is stronger.\nWe propose another diameter that we call the one-way diameter,\nDow(M) := max s min \u03c0 T\u03c0s1\u2192s\u0304 , (2)\nwhere s\u0304 = argmaxs h?(s). We study the relationship between sp(h?), Dow and D in Section 4 below and prove that sp(h?) \u2264 Dow \u2264 D whenever the rewards are in [0, 1]. So, we not only extend the result of Auer et al. [2009a] to weakly communicating MDPs but also replace D by a smaller quantity, sp(h?)."}, {"heading": "2 PRELIMINARIES", "text": "We make the simplifying assumption that the rewards r(s, a) \u2208 [0, 1] are known and only the transition probabilities Ps,a are unknown. This is not a restrictive assumption as the rewards can also be estimated at the\ncost of increasing the regret bound by some constant factor. The heart of the problem lies in not knowing the transition probabilities.\nRecall the definition of the dynamic programming operator T ,\n(T V )(s) := max a\u2208A\n( r(s, a) + P>s,aV ) .\nLet V n(s) denote the maximum (over all policies, stationary or non-stationary) possible expected reward obtainable in n steps starting from s. This can be computed via the simple recursion,\nV 0(s) = 0\nV n+1 = T V n .\nNote that the optimality equations (1) can be written succinctly in terms of the operator T ,\nh? + \u03bb?e = T h? . (3)"}, {"heading": "3 REGAL", "text": "In this section, we present Regal, an algorithm (see Algorithm 1) inspired by the Ucrl2 algorithm of Auer et al. [2009a]. Before we describe the algorithm, let us set up some notation. Let N(s, a, s\u2032; t) be the number of times the state-action-state triple (s, a, s\u2032) has been visited up to time t. Let N(s, a; t) be defined similarly. Then, an estimate of the state transition probability at time t is\nP\u0302 ts,a(s \u2032) = N(s, a, s\u2032; t) max{N(s, a; t), 1} , (4)\nUsing these estimates, we can define a set M(t) of MDPs such that the true MDP lies in Mk with high probability. The set M(t) consists of all those MDPs whose transition probabilities satisfy,\u2225\u2225\u2225Ps,a \u2212 P\u0302 ts,a\u2225\u2225\u2225\n1 \u2264\n\u221a 12S log(2At/\u03b4)\nmax{N(s, a, t), 1} . (5)\nLike Ucrl2, Regal works in episodes. Let tk denote the start of episode k. We will abbreviate N(s, a, s\u2032; tk) and N(s, a; tk) to Nk(s, a, s\u2032) and Nk(s, a) respectively. Also, let vk(s, a) = Nk+1(s, a)\u2212Nk(s, a) be the number of times (s, a) is visited during episode k.\nFor choosing a policy to follow in episode k, Regal first finds an MDP Mk \u2208 Mk := M(tk) that maximizes a \u201cregularized\u201d average optimal reward,\n\u03bb?(M)\u2212 Ck sp(h?(M)) . (6)\nHere Ck is a regularization parameter that is to be set appropriately at the start of episode k. Regal then\nAlgorithm 1 REGularization based Regret Minimizing ALgorithm (Regal)\nfor episodes k = 1, 2, . . . , do tk \u2190 current time Mk is the set of MDPs whose transition function satisfies (5) with t = tk Choose Mk \u2208 Mk to maximize the following criterion over Mk,\n\u03bb?(M)\u2212 Ck sp(h?(M)) .\n\u03c0k \u2190 average reward optimal policy for Mk Follow \u03c0k until some s, a pair gets visited Nk(s, a) times\nend for\nfollows the average reward optimal policy \u03c0k for Mk until some state-action pair is visited Nk(s, a) times. When this happens, Regal enters the next episode.\nTheorem 1. Suppose Algorithm 1 is run for T steps in a weakly communicating MDP M starting from some state s1. Then, with probability at least 1 \u2212 \u03b4, there exist choices for the regularization parameters Ck such that,\n\u2206(s1, T ) = O ( sp(h?(M))S \u221a AT log(AT/\u03b4) ) .\nThe proof of this theorem is given in Section 6. Unfortunately, as the proof reveals, the choice of the parameter Ck in the theorem requires knowledge of the counts vk(s, a) before episode k begins. Therefore, we cannot run Regal with these choices of Ck. However, if an upper bound H on the span sp(h?(M)) of the true MDP M is known, solving the constrained version of the optimization problem (6) gives Algorithm 2 that enjoys the following guarantee.\nTheorem 2. Suppose Algorithm 2 is run for T steps in a weakly communicating MDP M starting in some state s1. Let the input parameter H be such that H \u2265 sp(h?(M)). Then, with probability at least 1\u2212 \u03b4,\n\u2206(s1, T ) = O ( HS \u221a AT log(AT/\u03b4) ) .\nFinally, we also present a modification of Regal that does not require knowledge of an upper bound on sp(h\u2217)). The downside is that the regret guarantee we can prove becomes O\u0303(sp(h?) \u221a S3AT ). The modification, Regal.D, that is given as Algorithm 3 uses the so called doubling trick to guess the length of an episode. If the guess turns out to be incorrect, the guess is doubled.\nTheorem 3. Suppose Algorithm 3 is run for T steps in a weakly communicating MDP M starting from\nsome state s1. Let the input parameter c be chosen as c = 2S \u221a 12 log(2AT/\u03b4) + \u221a 2 log(1/\u03b4) .\nThen, with probability at least 1\u2212 \u03b4, \u2206(s1, T ) = O ( sp(h?(M)) \u221a S3AT log2(AT/\u03b4) ) .\nAlgorithm 2 Regal.C: Constrained Optimization version of Regal\nInput parameter: H for episodes k = 1, 2, . . . , do tk \u2190 current time Mk is the set of MDPs whose transition function satisfies (5) with t = tk Choose Mk \u2208 Mk by solving the following optimization over M \u2208Mk,\nmax \u03bb?(M) subject to sp(h?(M)) \u2264 H .\n\u03c0k \u2190 average reward optimal policy for Mk Follow \u03c0k until some s, a pair gets visited Nk(s, a) times\nend for\nAlgorithm 3 Regal.D: Regal with doubling trick Input parameter: c for episodes k = 1, 2, . . . , do tk \u2190 current time Mk is the set of MDPs whose transition function satisfies (5) with t = tk j \u2190 1 while every (s, a) has been visited less than Nk(s, a) times in this episode do `k,j \u2190 2j Choose Mk,j \u2208 Mk to maximize the following criterion over Mk,\n\u03bb?(M)\u2212 c\u221a `k,j sp(h?(M)) .\n\u03c0k,j \u2190 average reward optimal policy for Mk,j Follow \u03c0k,j until: `k,j time steps have elapsed OR some s, a pair gets visited Nk(s, a) times\nend while end for"}, {"heading": "4 SPAN AND DIAMETER", "text": "Theorem 4. In any weakly communicating MDP, for any states s1, s2, and any policy \u03c0,\nh?(s2)\u2212 h?(s1) \u2264 \u03bb?T\u03c0s1\u2192s2\nProof. Let us first prove the theorem for all aperiodic weakly communicating MDPs. In such MDPs, value iteration is known to converge [Puterman, 1994]. That is, if we define a sequence of vectors using vn+1 = T vn starting from an arbitrary v0, then\nlim n\u2192\u221e vn(s1)\u2212 vn(s2) = h?(s1)\u2212 h?(s2) ,\nfor any s1, s2. If we choose v0 = 0, then vn = V n and hence we have\nlim n\u2192\u221e V n(s2)\u2212 V n(s1) = h?(s2)\u2212 h?(s1) ,\nfor any s1, s2. Let \u03c0 be a stationary policy. Consider the following n-step non-stationary policy. Follow \u03c0 starting from s1 and wait until s2 is reached. Suppose this happens in \u03c4 steps. Then follow the optimal (n\u2212 \u03c4)-step policy. Note that \u03c4 is a random variable and, by definition,\nE[\u03c4 ] = T\u03c0s1\u2192s2 .\nThe expected reward obtained in this way is at least E[V n\u2212\u03c4 (s2)]. This has to be less than the n-step optimal reward. That is,\nV n(s1) \u2265 E[V n\u2212\u03c4 (s2)] .\nThus, we have\nh?(s2)\u2212 h?(s1) = lim n\u2192\u221e V n(s2)\u2212 V n(s1)\n\u2264 lim n\u2192\u221e V n(s2)\u2212 E[V n\u2212\u03c4 (s2)]\n= lim n\u2192\u221e\nE[V n(s2)\u2212 V n\u2212\u03c4 (s2)]\n= E[ lim n\u2192\u221e\nV n(s2)\u2212 V n\u2212\u03c4 (s2)]\n= E[\u03bb?\u03c4 ] = \u03bb?T\u03c0s1\u2192s2 .\nNow, if the MDP M is periodic apply the following aperiodicity transform [Puterman, 1994] to get a new MDP M\u0303\nr\u0303(s, a) = \u03b8r(s, a)\nP\u0303s,a = (1\u2212 \u03b8)es + \u03b8Ps,a ,\nwhere \u03b8 \u2208 (0, 1). Note that M\u0303 is also weakly communicating. Let h\u0303?, \u03bb\u0303? and T\u0303\u03c0s1\u2192s2 denote the quantities associated with M\u0303 . It is easily verified that these are related to the corresponding quantities for M as follows,\nh\u0303? = h?\n\u03bb\u0303? = \u03b8\u03bb?\nT\u0303\u03c0s1\u2192s2 = T\u03c0s1\u2192s2 \u03b8 .\nUsing these relations and the fact that we have proved the result for M\u0303 gives us,\nh?(s2)\u2212 h?(s1) = h\u0303?(s2)\u2212 h\u0303?(s1) \u2264 \u03bb\u0303?T\u0303\u03c0s1\u2192s2\n= \u03b8\u03bb? T\u03c0s1\u2192s2 \u03b8 = \u03bb?T\u03c0s1\u2192s2 .\nThus, we have proved the result for all weakly communicating MDPs.\nWe can now derive the fact that sp(h?) \u2264 Dow. Corollary 5. For any weakly communicating MDP M with reward in [0, 1], the one-way diameter is an upper bound on the span of the optimal bias vector,\nsp(h?(M)) \u2264 Dow(M) \u2264 D(M) .\nMoreover, both inequalities above can be arbitrarily loose.\nProof. The inequalities follow immediately from Theorem 4. To show that they can be loose, consider a two-state two-action MDP M such that\nP1,1 = (1, 0)> , P1,2 = (1\u2212 , )> , P2,1 = (0, 1)> , P2,2 = (0, 1)> ,\nand r(1, a) = 1 \u2212 \u03b1, r(2, a) = 1 for all a. It is easy to verify that \u03bb? = 1 and h? = (\u2212\u03b1/ , 0)> satisfy the optimality equations (3). Thus, sp(h?) = \u03b1/ and s\u0304 = 2. Therefore, Dow = 1/ . Moreover, D = \u221e. Thus, both inequalities in the corollary can be arbitrarily loose."}, {"heading": "5 LOWER BOUND", "text": "Using Corollary 5, the bound of Theorem 1 can be written as O\u0303 ( Dow(M)S \u221a AT )\n. In this section, we provide a lower bound that matches the upper bound except for the dependence on S.\nUsing the results in Section 4, it is possible to show that the algorithms of Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] enjoy a regret bound of O\u0303(Dow(M) \u221a SAT ) for ergodic MDPs. We therefore conjecture that the lower bound is tight.\nTheorem 6. There exists a universal constant c0 such that for any S,A, dow and any algorithm G, there exists an MDP M with Dow(M) \u2264 dow such that for any T > SA and any s, we have\n\u2206G(s, T ) \u2265 c0dow \u221a SAT .\nProof Sketch. Due to lack of space, we only outline the argument. We modify the MDP used by Auer et al. [2009a] to prove their lower bound. They build a large MDP with S states, A actions by putting together S/2 copies of an MDP with 2 states, A actions. For our lower bound, we modify the construction of their 2- state MDP as follows. For all a \u2208 A, P1,a = (1 \u2212 \u03b1, \u03b1)>, r(1, a) = 0 and r(2, a) = 1. Thus, state 2 is the only rewarding state. For all but a single action a? \u2208 A, P2,a = (\u03b4, 1 \u2212 \u03b4)> and for a?, set P2,a? = (\u03b4 \u2212 , 1\u2212 \u03b4 + ). We will choose \u03b1, \u03b4 and later such that < \u03b4 \u03b1.\nThe idea is that taking a? in state 2 increases the probability of staying in that rewarding state slightly. So, the agent needs to figure out which action is a?. To do this, each action needs to be probed at least c\u03b4/ 2 times. The difference in the average reward of the policy that takes action a? in state 2 and the one that does not is\n\u03b1 \u03b1+ \u03b4 \u2212 \u2212 \u03b1 \u03b1+ \u03b4 > 4\u03b1\nprovided \u03b4 < \u03b1. Now connect the 1 states of the S/2 copies of the MDP using A additional deterministic actions per state in an A-ary tree. Suppose only one of the copies has the good action a? in it. Now, we need to probe at least cSA\u03b4/ 2 times. Choosing \u03b4 \u03b1 means that most of the time, we are in state 2 and number of probes is a good approximation to the time elapsed. Thus, in probing for a? we incur at least\ncSA\u03b4\n2 4\u03b1 = cSA\u03b4 4\u03b1\nregret. Now set = \u03b4 \u221a SA/T and \u03b4 = \u03b12. If T > SA, then < \u03b4 and regret is at least\nc \u221a SAT\n4\u03b1 .\nFinally choose \u03b1 = 1/dow. Noting that Dow for this MDP is 1/\u03b1, proves the theorem (note that the MDP we have constructed has diameter D = D2ow)."}, {"heading": "6 ANALYSIS", "text": "In this section we will prove Theorems 1,2 and 3. We will first set up notation and recall key lemmas from the technical report of Auer et al. [2009b] that will be useful for proving all three theorems.\nLet M denote the true (but unknown) underlying MDP. The quantities \u03bb? and h? will be for this MDP throughout this section. Let `k = \u2211 s,a vk(s, a) be the length of episode k. Let \u03bbk, h?k denote \u03bb(M k), h?(Mk) respectively. The transition matrices of \u03c0k in Mk and\nM will be denoted by P\u0303k and Pk respectively. Further, assume that h? and h?k have been normalized to have their minimum component equal to 0, so that sp(h?) = \u2016h?\u2016\u221e and sp(h?k) = \u2016h?k\u2016\u221e.\nLet vk, rk \u2208 RS denote vectors such that\nvk(s) = vk(s, \u03c0k(s)) ,\nrk(s) = r(s, \u03c0k(s)) .\nNote that, by definition of \u03c0k, we have,\nh?k + \u03bb ? ke = rk + P\u0303kh ? k . (7)\nThe following bound on the numberm of episodes until time T was proved in Auer et al. [2009b].\nLemma 7. The number m of episodes of Algorithms 1,2 and 3 up to step T \u2265 SA is upper bounded as,\nm \u2264 SA log2 8T SA .\nLet \u2206k be the regret incurred in episode k, \u2206k = \u2211 s,a vk(s, a)[\u03bb? \u2212 r(s, a)] .\nNote that the total regret equals,\u2211 k\u2208G \u2206k + \u2211 k\u2208B \u2206k , (8)\nwhere G = {k : M \u2208 Mk} and B = {1, . . . ,m} \u2212G. The following result from Auer et al. [2009b] assures us that the contribution from \u201cbad\u201d episodes (in which the true MDP M does not lie in Mk) is small. Lemma 8. For Algorithms 1,2 and 3, with probability at least 1\u2212 \u03b4, \u2211\nk\u2208B\n\u2206k \u2264 \u221a T ."}, {"heading": "6.1 PROOF OF THEOREM 1", "text": "Consider an episode k \u2208 G. Then, we have\n\u03bb?k \u2212 Ck sp(h?k) \u2265 \u03bb? \u2212 Ck sp(h?) . (9)\nTherefore, \u2206k = \u2211 s,a vk(s, a)[\u03bb? \u2212 r(s, a)]\n\u2264 \u2211 s,a vk(s, a)[\u03bb?k \u2212 Ck sp(h?k) + Ck sp(h?)\u2212 r(s, a)]\n= \u2211 s,a vk(s, a)[\u03bb?k \u2212 r(s, a)]\n\u2212 Ck \u2211 s,a vk(s, a)[sp(h?k)\u2212 sp(h?)]\n= vk(\u03bb?ke\u2212 rk)\u2212 Ck \u2211 s,a vk(s, a)[sp(h?k)\u2212 sp(h?)]\n= vk(P\u0303k \u2212 I)h?k \u2212 Ck \u2211 s,a vk(s, a)[sp(h?k)\u2212 sp(h?)]\n[using (7)]\n= vk(P\u0303k \u2212Pk + Pk \u2212 I)h?k \u2212 Ck \u2211 s,a vk(s, a)[sp(h?k)\u2212 sp(h?)]\n\u2264 \u2016vk(P\u0303k \u2212Pk)\u20161 sp(h?k) + vk(Pk \u2212 I)h?k \u2212 Ck \u2211 s,a vk(s, a)[sp(h?k)\u2212 sp(h?)] . (10)\nLemma 9. With probability at least 1\u2212 \u03b4,\u2211 k\u2208G vk(Pk \u2212 I)h?k\n\u2264 \u2211 k\u2208G sp(h?k) \u221a 2`k log(1/\u03b4)\n+ (sp(h?) + max k\u2208G 1 Ck )(m+ log(1/\u03b4) .\nProof. Using Bernstein\u2019s inequality (see, for example, [Cesa-Bianchi and Lugosi, 2006, Lemma A.8]) instead of Hoeffding-Azuma in the argument of [Auer et al., 2009b, Section B.4], we get, with probability at least 1\u2212 \u03b4,\u2211\nk\u2208G\nvk(Pk \u2212 I)h?k \u2264 \u221a 2 \u2211 k\u2208G sp(h?k)2`k log(1/\u03b4)\n+ max k\u2208G sp(h?k)(m+ log(1/\u03b4) .\nEquation (9) gives sp(h?k) \u2264 sp(h?)+1/Ck. Using this and sub-additivity of square-root gives the lemma.\nIf k \u2208 G then Mk,M \u2208Mk and so we also have\n\u2016vk(P\u0303k \u2212Pk)\u20161 \u2264 2 \u2211 s,a vk(s, a)\n\u221a 12S log(2AT/\u03b4)\nNk(s, a)\n(11)\nUsing this, Lemma 9 and plugging these into (10), we get \u2211 k\u2208G \u2206k \u2264 \u2211 k\u2208G sp(h?k) ( 2 \u2211 s,a vk(s, a) \u221a 12S log(2AT/\u03b4) Nk(s, a)\n+ \u221a 2`k log(1/\u03b4)\u2212 Ck \u2211 s,a vk(s, a) ) + \u2211 k\u2208G Ck`k sp(h?)\n+ (sp(h?) + max k\u2208G 1 Ck )(m+ log(1/\u03b4) .\nEquation (9) in Auer et al. [2009b] gives,\u2211 k \u2211 s,a vk(s, a) Nk(s, a) \u2264 \u221a 8SAT . (12)\nTherefore, choosing Ck to satisfy, Ck = 2 \u2211 s,a vk(s, a) \u221a 12S log 2AT\u03b4 Nk(s,a) + \u221a\n2`k log 1\u03b4 `k\ngives us\u2211 k\u2208G \u2206k \u2264 \u2211 k\u2208G Ck`k sp(h?)\n+ (sp(h?) +\n\u221a T\n2 log(1/\u03b4) (m+ log(1/\u03b4)\n= O ( sp(h?)S \u221a AT log(AT/\u03b4) ) Combining this with (8) and Lemma 8 finishes the proof of Theorem 1."}, {"heading": "6.2 PROOF OF THEOREM 2", "text": "Consider an episode k \u2208 G. Then, we have \u03bb?k \u2265 \u03bb?. Therefore,\n\u2206k = \u2211 s,a vk(s, a)[\u03bb? \u2212 r(s, a)]\n\u2264 \u2211 s,a vk(s, a)[\u03bb?k \u2212 r(s, a)]\n= vk(\u03bb?ke\u2212 rk) = vk(P\u0303k \u2212 I)h?k = vk(P\u0303k \u2212Pk + Pk \u2212 I)h?k \u2264 \u2016vk(P\u0303k \u2212Pk)\u20161 sp(h?k) + vk(Pk \u2212 I)h?k . (13)\nThe following lemma is proved like Lemma 9 (in this case, Hoeffding-Azuma suffices). Lemma 10. With probability at least 1\u2212 \u03b4,\u2211\nk\u2208G\nvk(Pk \u2212 I)h?k \u2264 H (\u221a 2T log(1/\u03b4) +m ) .\nEquation (11) still holds if k \u2208 G. Plugging it and Lemma 10 into (13), we get\n\u2211 k\u2208G \u2206k \u2264 H (\u2211 k\u2208G \u2211 s,a 2vk(s, a) \u221a 12S log(2AT/\u03b4) Nk(s, a)\n+ \u221a 2T log(1/\u03b4) +m ) .\nUsing (12) now gives,\u2211 k\u2208G \u2206k \u2264 O(HS \u221a AT log(AT/\u03b4)) .\nCombining this with (8) and Lemma 8 finishes the proof of Theorem 2."}, {"heading": "6.3 PROOF OF THEOREM 3", "text": "In this case, we have episodes consisting of several sub-episodes whose lengths increase in geometric progression. Let vk,j(s, a) be the number of times (s, a) is visited during sub-episode j of episode k. Thus, `k,j = \u2211 s,a vk,j(s, a). Let \u03bbk,j , h ? k,j denote \u03bb(Mk,j), h?(Mk,j) respectively. The transition matrices of \u03c0k,j in Mk,j and M will be denoted by P\u0303k,j and Pk,j respectively. Let vk,j , rk,j be defined accordingly.\nConsider an episode k \u2208 G. Then, as in the proof of Theorem 1, we have,\n\u03bb?k,j \u2212 Ck,j sp(h?k,j) \u2265 \u03bb? \u2212 Ck,j sp(h?) ,\nand therefore,\n\u2206k,j := \u2211 s,a vk,j(s, a)[\u03bb? \u2212 r(s, a)]\n\u2264 \u2211 s,a vk,j(s, a)[\u03bb?k,j \u2212 Ck,j sp(h?k,j)\n+ Ck,j sp(h?)\u2212 r(s, a)] = vk,j(\u03bb?k,je\u2212 rk,j)\n\u2212 Ck,j \u2211 s,a vk,j(s, a)[sp(h?k,j)\u2212 sp(h?)]\n= vk,j(P\u0303k,j \u2212 I)h?k,j \u2212 Ck,j \u2211 s,a vk,j(s, a)[sp(h?k,j)\u2212 sp(h?)]\n\u2264 \u2016vk,j(P\u0303k,j \u2212Pk,j)\u20161 sp(h?k,j) + vk,j(Pk,j \u2212 I)h?k,j \u2212 Ck,j \u2211 s,a vk,j(s, a)[sp(h?k,j)\u2212 sp(h?)] . (14)\nIf k \u2208 G then Mk,j ,M \u2208Mk. Therefore,\n\u2016vk,j(P\u0303k,j \u2212Pk)\u20161\n\u2264 2 \u2211 s,a vk,j(s, a)\n\u221a 12S log(2AT/\u03b4)\nNk(s, a)\n\u2264 2 \u221a 12S log(2AT/\u03b4) \u221a\u2211\ns,a\nvk,j(s, a) \u00b7 \u221a\u221a\u221a\u221a\u2211 s,a vk,j(s, a) Nk(s, a)\n\u2264 2 \u221a 12S log(2AT/\u03b4) \u221a `k,j \u00b7 \u221a S\n[\u2235 vk,j(s, a) \u2264 vk(s, a) \u2264 Nk(s, a)] = 2S \u221a 12`k,j log(2AT/\u03b4) (15)\nNote that this step differs slightly from its counterpart in the proof of Theorem 1. Here, we used CauchySchwarz (for the second inequality above) to get a bound solely in terms of the length `k,j of the subepisode. This allows us to deal with the problem of not knowing the visit counts vk,j(s, a) in advance.\nThe following lemma is proved exactly like Lemma 9. Lemma 11. With probability at least 1\u2212 \u03b4,\u2211\nk\u2208G \u2211 j vk,j(Pk,j \u2212 I)h?k,j\n\u2264 \u2211 k\u2208G \u2211 j sp(h?k,j) \u221a 2`k,j log(1/\u03b4)\n+ (sp(h?) + max k,j 1 Ck,j )(m log2(T ) + log(1/\u03b4) .\nCombining (15) and Lemma 11 with (14), we have,\u2211 k\u2208G \u2206k\n= \u2211 k\u2208G \u2211 j \u2206k,j\n\u2264 \u2211 k\u2208G \u2211 j sp(h?k,j) ( 2S \u221a 12`k,j log(2AT/\u03b4)\n+ \u221a 2`k,j log(1/\u03b4)\u2212 Ck,j`k,j )\n+ sp(h?) \u2211 k\u2208G \u2211 j Ck,j`k,j\n+ (sp(h?) + max k,j 1 Ck,j )(m log2(T ) + log(1/\u03b4) .\nNow, setting Ck,j = c/ \u221a `k,j for\nc = 2S \u221a 12 log(2AT/\u03b4) + \u221a 2 log(1/\u03b4)\ngives us,\u2211 k\u2208G \u2206k\n\u2264 sp(h?) \u2211 k\u2208G \u2211 j c \u221a `k,j (16)\n+ (sp(h?) + cmax k,j\n\u221a `k,j)(m log2(T ) + log(1/\u03b4) .\nSince `k,j = 2j , \u2211 j \u221a `k,j \u2264 6\n\u221a `k + 2. Therefore,\u2211\nk\u2208G \u2211 j \u221a `k,j \u2264 \u2211 k\u2208G 6 \u221a `k + 2\n\u2264 6 \u221a m \u221a\u2211 k `k + 2m = 6 \u221a m \u221a T + 2m .\nSubstituting this into (16), gives\u2211 k\u2208G \u2206k = O ( sp(h?) \u221a S3AT log2(AT/\u03b4) ) .\nCombining this with (8) and Lemma 8 finishes the proof of Theorem 3."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge the support of DARPA under award FA8750-05-2-0249."}, {"heading": "A. N. Burnetas and M. N. Katehakis. Optimal adaptive", "text": "policies for Markov decision processes. Mathematics of Operations Research, 22(1):222\u2013255, 1997.\nNicolo\u0300 Cesa-Bianchi and Ga\u0301bor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.\nSham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Computational Neuroscience Unit, University College London, 2003.\nMichael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49:209\u2013232, 2002.\nP. R. Kumar and P. P. Varaiya. Stochastic systems: Estimation, identification, and adaptive control. Prentice Hall, 1986.\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 1994.\nAlexander L. Strehl and Michael Littman. A theoretical analysis of model-based interval estimation. In Proceedings of the Twenty-Second International Conference on Machine Learning, pages 857\u2013864. ACM Press, 2005.\nAlexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC model-free reinforcement learning. In Proceedings of the Twenty-Third International Conference on Machine Learning, 2006.\nAmbuj Tewari and Peter L. Bartlett. Optimistic linear programming gives logarithmic regret for irreducible MDPs. In Advances in Neural Information Processing Systems 20, pages 1505\u20131512. MIT Press, 2008."}], "references": [{"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["Peter Auer", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Auer and Ortner.,? \\Q2007\\E", "shortCiteRegEx": "Auer and Ortner.", "year": 2007}, {"title": "Nearoptimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "Near-optimal regret bounds for reinforcement learning (full version), 2009b. URL: http://institute.unileoben.ac.at/infotech/publications/ ucrl2.pdf", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "R-MAX \u2013 a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I. Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2002\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2002}, {"title": "Optimal adaptive policies for Markov decision processes", "author": ["A.N. Burnetas", "M.N. Katehakis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Burnetas and Katehakis.,? \\Q1997\\E", "shortCiteRegEx": "Burnetas and Katehakis.", "year": 1997}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Sham Kakade"], "venue": "PhD thesis, Gatsby Computational Neuroscience Unit,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Stochastic systems: Estimation, identification, and adaptive control", "author": ["P.R. Kumar", "P.P. Varaiya"], "venue": null, "citeRegEx": "Kumar and Varaiya.,? \\Q1986\\E", "shortCiteRegEx": "Kumar and Varaiya.", "year": 1986}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["Martin L. Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L. Strehl", "Michael Littman"], "venue": "In Proceedings of the Twenty-Second International Conference on Machine Learning,", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "PAC model-free reinforcement learning", "author": ["Alexander L. Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L. Littman"], "venue": "In Proceedings of the Twenty-Third International Conference on Machine Learning,", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Optimistic linear programming gives logarithmic regret for irreducible MDPs", "author": ["Ambuj Tewari", "Peter L. Bartlett"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Tewari and Bartlett.,? \\Q2008\\E", "shortCiteRegEx": "Tewari and Bartlett.", "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "exploitation trade-off that Kearns and Singh [2002] succinctly describe as,", "startOffset": 28, "endOffset": 52}, {"referenceID": 8, "context": "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities.", "startOffset": 98, "endOffset": 123}, {"referenceID": 4, "context": "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities.", "startOffset": 145, "endOffset": 175}, {"referenceID": 0, "context": "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities. In the machine learning literature, finite time bounds for undiscounted MDPs were pioneered by Kearns and Singh [2002]. Their seminal work spawned a long thread of research [Brafman and Tennenholtz, 2002, Kakade, 2003, Strehl and Littman, 2005, Strehl et al.", "startOffset": 146, "endOffset": 329}, {"referenceID": 0, "context": ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively.", "startOffset": 8, "endOffset": 283}, {"referenceID": 0, "context": ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively.", "startOffset": 8, "endOffset": 314}, {"referenceID": 0, "context": ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively. Recently, Auer et al. [2009a] gave an algorithm for communicating MDPs whose expected regret after T steps is, with high probability, \u00d5(D(M)S \u221a AT ) .", "startOffset": 8, "endOffset": 395}, {"referenceID": 1, "context": "So, we not only extend the result of Auer et al. [2009a] to weakly communicating MDPs but also replace D by a smaller quantity, sp(h).", "startOffset": 37, "endOffset": 57}, {"referenceID": 1, "context": "In this section, we present Regal, an algorithm (see Algorithm 1) inspired by the Ucrl2 algorithm of Auer et al. [2009a]. Before we describe the algorithm, let us set up some notation.", "startOffset": 101, "endOffset": 121}, {"referenceID": 9, "context": "In such MDPs, value iteration is known to converge [Puterman, 1994].", "startOffset": 51, "endOffset": 67}, {"referenceID": 9, "context": "Now, if the MDP M is periodic apply the following aperiodicity transform [Puterman, 1994] to get a new MDP M\u0303", "startOffset": 73, "endOffset": 89}, {"referenceID": 2, "context": "Using the results in Section 4, it is possible to show that the algorithms of Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] enjoy a regret bound of \u00d5(Dow(M) \u221a SAT ) for ergodic MDPs.", "startOffset": 78, "endOffset": 108}, {"referenceID": 2, "context": "Using the results in Section 4, it is possible to show that the algorithms of Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] enjoy a regret bound of \u00d5(Dow(M) \u221a SAT ) for ergodic MDPs.", "startOffset": 78, "endOffset": 139}, {"referenceID": 1, "context": "We modify the MDP used by Auer et al. [2009a] to prove their lower bound.", "startOffset": 26, "endOffset": 46}, {"referenceID": 1, "context": "We will first set up notation and recall key lemmas from the technical report of Auer et al. [2009b] that will be useful for proving all three theorems.", "startOffset": 81, "endOffset": 101}, {"referenceID": 1, "context": "The following bound on the numberm of episodes until time T was proved in Auer et al. [2009b]. Lemma 7.", "startOffset": 74, "endOffset": 94}, {"referenceID": 1, "context": "The following result from Auer et al. [2009b] assures us that the contribution from \u201cbad\u201d episodes (in which the true MDP M does not lie in M) is small.", "startOffset": 26, "endOffset": 46}, {"referenceID": 1, "context": "Equation (9) in Auer et al. [2009b] gives, \u2211", "startOffset": 16, "endOffset": 36}], "year": 2009, "abstractText": "We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of \u00d5(HS \u221a AT ). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.", "creator": "TeX"}}}