{"id": "1205.3549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2012", "title": "Normalized Maximum Likelihood Coding for Exponential Family with Its Applications to Optimal Clustering", "abstract": "hook-and-line We infalling are concerned with the sibi issue 117.96 of how to calculate the lanphear normalized scragg maximum likelihood ~ (edegganssptimes.com NML) code - c\u01b0\u1eddng length. cheerwine There baynham is hatherley a problem shinjitai that magaluf the peetha normalization gradiska term vickrey of the juventude NML code - conciliazione length phakisa may diverge when it frug is one-block continuous and unbounded zethus and a wintel straightforward computation of it lowfield is baldness highly niu expensive when the victorian/australian data brooke domain is finite. In beachcombers previous works tnpl it rustin has littletons been wacker investigated how 2,000-point to calculate the NML candelabra code - length stixrude for bleekemolen specific bryggen types of distributions. We first muzzatti propose deflecting a general nabj method for ra6 computing the NML soulias code - cancale length for botta the imataca exponential four-season family. silverwing Then dfw we specifically bhujbal focus pray on Gaussian mcroberts mixture zig-zag model ~ (maniakes GMM ), and propose resend a new efficient method yungay for scrawny computing knockin the buscaglia NML adul to them. We develop 85.70 it by clytemnestra generalizing Rissanen ' tramco s ndagijimana re - atn-7 normalizing cipe technique. Then innovia we proven apply otb this method roughest to the affiliative clustering barong issue, in which souad a clustering structure freshly is modeled ukrainian-born using mutualistic a cyberport GMM, 55.45 and the goldberg main task is funicello to jutila estimate terzigno the maruo optimal number herbison of gamefish clusters on the basis of uweinat the 5,657 NML caulkins code - wadie length. We fortiers demonstrate rickettsia using artificial ison data dakini sets rosny the superiority of graves the gunmetal NML - based clustering soo-jung over other criteria eyemouth such as sin-eater AIC, BIC disturbance in terms ebby of cullings the conceicao data size stradlin required icms for high supermall accuracy rate latecomers to 236.5 be thickest achieved.", "histories": [["v1", "Wed, 16 May 2012 03:54:30 GMT  (48kb)", "https://arxiv.org/abs/1205.3549v1", null], ["v2", "Thu, 17 May 2012 01:03:19 GMT  (48kb)", "http://arxiv.org/abs/1205.3549v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["so hirai", "kenji yamanishi"], "accepted": false, "id": "1205.3549"}, "pdf": {"name": "1205.3549.pdf", "metadata": {"source": "CRF", "title": "Normalized Maximum Likelihood Coding for Exponential Family with Its Applications to Optimal Clustering", "authors": ["So Hirai", "Kenji Yamanishi"], "emails": ["Hirai@mist.i.u-tokyo.ac.jp", "yamanishi@mist.i.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n35 49\nv2 [\ncs .L\n\u2217Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN Email: So Hirai@mist.i.u-tokyo.ac.jp He currently belongs to NTT DATA Corporation.\n\u2020Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN Email: yamanishi@mist.i.u-tokyo.ac.jp"}, {"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Motivation and Previous Works", "text": "This paper addresses the issue of how to calculate the normalized maximum likelihood (NML) code-length for a given sequence. Suppose that we are given an n tuple of mdimensional data xn = (x1, \u00b7 \u00b7 \u00b7 ,xn) \u2208 X n, where each xi \u2208 X \u2286 R m. We define the NML distribution fNML relative to a model class M = {f(X n; \u03b8) : \u03b8 \u2208 \u0398} (n = 1, 2, \u00b7 \u00b7 \u00b7 ) by\nfNML(x n;M) =\nf(xn; \u03b8\u0302(xn,M))\nC(M) , (1)\nC(M) =\n\u222b\nf(xn; \u03b8\u0302(xn),M)dxn,\nwhere \u0398 is a parameter spece and \u03b8\u0302 is a maximum likelihood estimator of \u03b8 from xn. The NML code-length for xn relative to M is calculated as follows:\n\u2212 log fNML(x n;M) = \u2212 log f(xn; \u03b8\u0302(xn,M)) + log C(M),\nIt is known from [8] that the NML code-length is optimal in the sense that it achieves the minimum of Shtarkov\u2019s minimax criterion [12]. The NML code-length is called the stochastic complexity [8] and has been employed as a criterion for statistical model selection on the basis of the minimum description length (MDL) principle [10, 4]. However, there is a problem that the normalization term may diverge and a straightforward computation of the normalization term in the NML code-length is highly expensive. The purpose of this paper is twofold. One is to propose a method for efficient computing the NML code-length for the exponential family and Gaussian mixture models. The other is to demonstrate the validity of its applications to optimal clustering.\nRissanen [8] derived a formula of an asymptotic approximation of the NML code-length:\n\u2212 log p(xn; \u03b8\u0302(xn)) + k\n2 log\nn\n2\u03c0 + log\n\u222b\n\u221a\n|I(\u03b8)|d\u03b8 + o(1),\nwhere I(\u03b8) is the Fisher information matrix. Note that this formula takes an asymptotic form. A method for exactly computing the NML code-length has been desired. In the case where the data domain is discrete, there is a problem that the time for a straightforward computation of the normalization term is exponential in data size even for the simplest case where the class of distributions is that of mutinomial distributions. Kontkanen and Myllyma\u0308ki proposed efficient algorithms for the NML code-length for multinomial distributions and Na\u0308ive Bayes model [6, 7]. Meanwhile, in the case where the data domain is continuous and not bounded, there is a problem that the normalization term may diverge for, e.g., Gaussian distributions. Rissanen proposed a method for circumventing this problem for linear regression models by making an elliptic constraint for the data domain so that the normalization term does not diverge [9]. Giurca\u0306neanu et. al. proposed another\nmethod using an rhomboid constraint [3]. Note that all of these works [9, 3] considered 1- dimensional Gaussian distributions. Hirai and Yamanishi [5] applied Rissanen\u2019s technique to the computation of the NML code-length for multi-variate Gaussian distributions.\nWe are specifically concerned with the applications of the NML code-length to clustering. A mixture model may be used as a probabilistic model of clustering where each mixture component corresponds to a cluster. The estimation of the mixture size is one of the most important issues in clustering. Kontkanen and Myllyma\u0308ki [7] proposed an efficient algorithm for NML-based clustering with optimal choices of mixture size for the case where the data domain was discrete. Hirai and Yamanishi [5] proposed an algorithm for efficiently computing the NML code-length for Gaussian mixture models (GMM) for the case where the data domain was continuous."}, {"heading": "1.2 Significance of This Paper", "text": ""}, {"heading": "1) An extension of the computation of the NML code-length to the exponential family.", "text": "We extend Hirai and Yamanishi\u2019s method [5] for computing the NML code-length for Gaussian distributions and GMMs to exponential family including Gamma distributions, logistic distributions, etc. Then we give a method for calculating the NML code-length in a general form.\n2) An improvement of the NML code-length for Gaussian distributions and GMMs using the renormalizing technique. We apply Rissanen\u2019s renormalizing technique [9] into Gaussian distributions and GMMs to derive new formulas for computing the NML codelengths for them. Conventional formulas in [5] depend on the parameters by which the data domain is restricted. The new formulas are obtained by renormalizing the likelihood with respect to the parameters, and are improved in that they are less dependent on hyperparameters than those in [5]. We call the resulting code-length the renormalized maximum likelihood code-length (RNML). Note that the RNML are different from Rissanen\u2019s original one [9] in that they are derived for the case where data is multi-dimensional while Rissanen considered a specific case where it was 1-dimensional.\n3) An empirical demonstration of the superiority of RNML over other criteria in the clustering scenario. We apply the RNML code-length to the clustering scenario in which a GMM is used as a model for clustering. In it we employ artificial data sets to empirically demonstrate the validity of RNML in the estimation of the number of clusters. We show that the number of clusters chosen by the RNML-based criterion converges significantly faster to the true one than those chosen by other criteria such as AIC, BIC, and the original NML."}, {"heading": "2 NML Code-Length for Exponential Family", "text": "In this section, we introduce a method of computing the NML code-length for the exponential family."}, {"heading": "2.1 Exponential Family", "text": "Below we define the exponential family.\nDefinition 1 The probability density function belonging to the exponential family takes the following form:\nf(X ; \u03b8) = h(X) exp { \u03b7(\u03b8)TT (X)\u2212 A(\u03b7(\u03b8)) } , (2)\nwhere \u03b8 \u2208 RD is a real-valued parameter vector (D is the number of parameters) and A(\u03b7) is a normalization term.\nThe joint distribution of data xn is given as follows:\nf(xn; \u03b8) = n \u220f\ni=1\nh(xi) exp { \u03b7(\u03b8)TT (xi)\u2212 A(\u03b7(\u03b8)) } .\nThen the maximum likelihood estimate (MLE): \u03b8\u0302(xn) satisfies:\nE\u03b7(\u03b8\u0302(xn))[T (X)] = 1\nn\nn \u2211\ni=1\nT (xi)."}, {"heading": "2.2 NML Code-Length for Exponential Family", "text": "Below we consider how to calculate the normalization term: C(M) as in (1) for the exponential family. Suppose that for any data, the MLE of \u03b8 from the data can analytically be obtained. It is known that for the exponential family, the MLE can be calculated as a function of sufficient statistics. Hence we may denote the MLE as follows:\n\u03b8\u0302(xn) = \u0398\n(\n1\nn\nn \u2211\ni=1\nT (xi)\n)\n,\nwhere the \u0398(x) is a certain function of x. Below we show how to calculate C(M) by circumventing the problem that it may diverge. The function to be integrated is expanded as follows:\nf(xn; \u03b8) = h(xn|\u03b8\u0302(xn))\u00d7 exp {\nn\u03b7T\u03b8 \u0398 \u22121(\u03b8\u0302(xn))\u2212 nA(\u03b7\u03b8)\n}\n= H(xn|\u03b8\u0302(xn))\u00d7\nD \u220f\nd=1\ngd(\u03b8\u0302d(x n)|\u03b8).\nHere we denote \u03b7\u03b8 = \u03b7(\u03b8) and define the function H(x n|\u03b8\u0302(xn)) def = \u03b4(\u03b8\u0302(xn) = \u03b8\u0302) (\u03b4(\u00b7) is a delta function), and the gd(\u03b8\u0302d(x n)|\u03b8) is the distribution of the MLE for the d-th part of the parameter \u03b8d. Notice here that \u03b8d is not a component of \u03b8 but rather a part of it\u2013 a collection of components. We assume here that parameter parts {\u03b8d} are independent with respect to d. We fix \u03b8\u0302(xn) = \u03b8\u0302 and let\ng(\u03b8\u0302) def =\nD \u220f\nd=1\ngd(\u03b8\u0302d|\u03b8\u0302).\nWe can calculate the normalization term C(M) by integrating g(\u03b8\u0302) with respect to \u03b8\u0302 over the restricted domain as follows:\nC(M) =\n\u222b\nY (\u03b1)\ng(\u03b8\u0302)d\u03b8\u0302,\nwhere we restrict the domain for the integral to be Y (\u03b1) where \u03b1 is a parameter by which the integral \u03b8\u0302 is specified.\nIn summary, for the exponential family, the NML code-length can analytically be obtained provided that the following conditions are fulfilled:\n1. The MLE of \u03b8 can be calculated analytically.\n2. The integral of g(\u03b8\u0302) with respect to \u03b8\u0302 can analytically be obtained."}, {"heading": "2.3 Examples", "text": "Below we give examples of calculation of the NML code-lengths for the exponential family. For the sake of simplicity, we focus on the normalization term C(M) as in (1)."}, {"heading": "2.3.1 Gamma Distributions", "text": "Gamma distributions belong to the exponential family. The density function of xn for a Gamma distribution is defined as follows:\nf(xn; k, \u03b8) = n \u220f\ni=1\n1\n\u0393(k) \u00b7 \u03b8k \u00b7 xk\u22121i \u00b7 exp\n{ \u2212 xi \u03b8 } ,\nwhere k is a shape parameter and \u03b8 is a scale parameter. The MLE of \u03b8 can analytically be obtained. We consider the case where k is known and fixed. The MLE of \u03b8 is given by \u03b8\u0302(xn) = \u2211n\ni=1 xi/(kn). Thus the joint distribution of\nxn is given as follows:\nf(xn; k, \u03b8) = 1\n\u0393(k)n \u00b7 \u03b8kn \u00b7\nn \u220f\ni=1\nxk\u22121i \u00b7 exp\n{\n\u2212 1\n\u03b8\nn \u2211\ni=1\nxi\n}\n= H(xn| k, \u03b8\u0302(xn)) \u00b7 g(\u03b8\u0302(xn); k, \u03b8),\nwhere \u03b8\u0302 is distributed according to the Gamma distribution with a shape parameter kn and a scale parameter \u03b8/(kn). Hence g(\u03b8\u0302(xn); k, \u03b8) is calculated as follows:\ng(\u03b8\u0302(xn); k, \u03b8) = (kn)kn \u00b7 \u03b8\u0302(xn)kn\u22121\n\u0393(kn) \u00b7 \u03b8kn \u00b7 exp\n{\n\u2212 kn\n\u03b8 \u03b8\u0302(xn)\n}\n.\nFix \u03b8\u0302(xn) = \u03b8\u0302 and let H(xn| k, \u03b8\u0302(xn)) = \u03b4(\u03b8\u0302(xn) = \u03b8\u0302). Then we have\ng(\u03b8\u0302; k) def =g(\u03b8\u0302; k, \u03b8\u0302) =\n(kn)kn\n\u0393(kn) \u00b7 ekn \u00b7 1 \u03b8\u0302 .\nLetting hyper-parameters be \u03b8min, \u03b8max and the domain be\nY (\u03b8min, \u03b8max) = { yn|\u03b8min \u2264 \u03b8\u0302(y n) \u2264 \u03b8max } ,\nthe normalization term C(M) is obtained by taking an integral of g(\u03b8\u0302; k) with respect to \u03b8\u0302 over Y (\u03b8min, \u03b8max)as follows:\nC(M) = (kn)kn\n\u0393(kn) \u00b7 ekn\n\u222b \u03b8max\n\u03b8min\n1 \u03b8\u0302 d\u03b8\u0302 =\n(kn)kn\n\u0393(kn) \u00b7 ekn log \u03b8max \u03b8min .\nHence, for fixed k, we obtain a finite value of C(M) for Gamma distributions."}, {"heading": "2.3.2 Logistic Distributions", "text": "The logistic distributions belong to the exponential family. The density function of xn for a logistic distribution with a parameter \u03b8 is defined as\nf(xn; \u03b8) = n \u220f\ni=1\n\u03b8e\u2212xi\n(1 + e\u2212xi)\u03b8+1 .\nThe MLE of \u03b8 is analytically obtained as \u03b8\u0302(xn) = n/( \u2211n i=1 log(1 + e \u2212xi)). Thus the joint density of xn is written as\nf(xn; \u03b8) = \u03b8n \u00b7 exp\n{\n\u2212\nn \u2211\ni=1\nxi \u2212 n(\u03b8 + 1)\n\u03b8\u0302(xn)\n}\n= H(xn|\u03b8\u0302(xn)) \u00b7 g(\u03b8\u0302(xn); \u03b8),\nwhere n/\u03b8\u0302(xn) is distributed according to the Gamma distribution with a shape parameter n and a scale parameter 1/\u03b8. Thus g(\u03b8\u0302(xn); \u03b8) is written as\ng(\u03b8\u0302(xn); \u03b8) = \u03b8n\n\u0393(n) \u00b7\n(\nn\n\u03b8\u0302(xn)\n)n\u22121\n\u00b7 exp\n{\n\u2212 n\u03b8\n\u03b8\u0302(xn)\n}\n.\nFix \u03b8\u0302(xn) = \u03b8\u0302 and let H(xn|\u03b8\u0302(xn)) = \u03b4(\u03b8\u0302(xn) = \u03b8\u0302). Then we have\ng(\u03b8\u0302) def =g(\u03b8\u0302; \u03b8\u0302) =\nnn\u22121\n\u0393(n) \u00b7 en \u00b7 \u03b8\u0302.\nLetting R be a parameter, we define the restricted domain as\nY (R) = { yn|\u03b8\u0302(yn) \u2264 R } . (3)\nThen the normalization term C(M) is obtained by taking an integral of g(\u03b8\u0302) with respect to \u03b8\u0302 as follows:\nC(M) = nn\u22121\n\u0393(n) \u00b7 en\n\u222b R\n0\n\u03b8\u0302 d\u03b8\u0302 = nn\u22121\n\u0393(n) \u00b7 en R2.\nThus we obtain the normalization term C(M) that doesn\u2019t diverge."}, {"heading": "3 Re-normalized Maximum Likelihood", "text": "We show how to compute the RNML code-length for a GMM. Let xn = (x1, \u00b7 \u00b7 \u00b7 ,xn), xi = (xi1, \u00b7 \u00b7 \u00b7 , xim)\n\u22a4 (i = 1, \u00b7 \u00b7 \u00b7 , n) be a given sequence where xi is distributed according to a Gaussian distribution with mean \u00b5 \u2208 Rm and variance-covariance matrix \u03a3 \u2208 Rm\u00d7m for a some positive integer m with density:\nf(x;\u00b5,\u03a3) = 1\n(2\u03c0) m 2 |\u03a3| 1 2\nexp { \u2212 1\n2 (x\u2212 \u00b5)\u22a4\u03a3\u22121(x\u2212 \u00b5)\n}\n.\nNotice here that the normalization term in (1) diverges. Hirai and Yamanishi [5] derived a formula of the NML distribution by restricting the range of data so that the maximum likelihood lies in a bounded range specified by parameters. It is given as follows:\nfNML(x n;R, \u03bbmin) def =\nf(xn; \u00b5\u0302(xn), \u03a3\u0302(xn))\nC(R, \u03bbmin) ,\nwhere\nC(R, \u03bbmin) =\n\u222b\nY (R,\u03bbmin)\nf(yn; \u00b5\u0302(yn), \u03a3\u0302(yn))dyn,\nY (R, \u03bbmin) def = {yn| ||\u00b5\u0302(yn)||2 \u2264 R, \u03bb (j) min \u2264 \u03bb\u0302j(y n)\n(j = 1, \u00b7 \u00b7 \u00b7 , m), yn \u2208 X n}, (4)\nwhere R, \u03bbmin = (\u03bb (1) min, \u00b7 \u00b7 \u00b7 , \u03bb (m) min) are parameters, and \u03bb\u0302j(y n) is the j-th largest eigenvalue of \u03a3\u0302(yn). The normalization term C(R, \u03bbmin) is expanded as follows [5]:\nC(R, \u03bbmin) = 2m+1R\nm 2 \u220fm j=1 \u03bb (j) min\n\u2212 m\n2\nmm+1\u0393(m 2 )\n\u00d7 ( n\n2e\n) mn\n2 1\n\u0393m( n\u22121 2 ) ,\nIf we set the parameters: R, \u03bbmin to be bounded, then the normalization term is also bounded.\nNote here that the value of the normalization term depends on the choice of parameters: R, \u03bbmin. Next we consider the optimization of the NML code-length with respect to the parameters: R, \u03bbmin. That is, we choose the optimal parameters so that they achieve the minimum of the following NML code-length: \u2212 log fNML(x\nn;R, \u03bbmin). The values of R, \u03bbmin that make the NML code-length shortest can be considered as the maximum likelihood (ML) estimates from xn. The terms including R, \u03bbmin in the NML code-length are given as:\nm\n2 logR\u2212\nm\n2\nm \u2211\nj=1\nlog \u03bb (j) min. (5)\nConsidering the range of parameters: (4), the ML estimates of R, \u03bbmin are given as follows:\nR\u0302(yn) = ||\u00b5\u0302(yn)||2,\n\u03bb\u0302 (j) min(y n) = \u03bb\u0302j(y n) (j = 1, \u00b7 \u00b7 \u00b7 , m).\nWe then introduce hyper parameters: \u03b3 = (\u03bb1, \u03bb2, R1, R2) and define the renormalized maximum likelihood (RNML) distribution by\nfRNML(x n; \u03b3) =\nfNML(x n; \u03b3, R\u0302(xn), \u03bb\u0302min(x n))\nC(\u03b3) ,\nwhere the normalization term is expanded as follows:\nC(\u03b3) =\n\u222b\nY (\u03b3)\nfNML(y n; \u03b3, R\u0302(yn), \u03bb\u0302min(y n))dyn,\nY (\u03b3) = {yn| V ( \u221a\nR1) \u2264 V (\n\u221a\nR\u0302(yn)) \u2264 V ( \u221a\nR2),\n\u03bb1 \u2264 \u03bb\u0302 (j) min(y n) \u2264 \u03bb2 (j = 1, \u00b7 \u00b7 \u00b7 , m), y n \u2208 X n},\nwhere V (r) = 2\u03c0 m 2 rm/(m\u0393(m 2 )), which denotes the volume of the m-dimensional ball with radius r. The normalization term C(\u03b3) is rewritten as\nC(\u03b3) = (m\n2\n)m+1 \u00b7 log R2 R1 \u00b7 ( log \u03bb2 \u03bb1 )m .\nThe terms including the hyper-parameters R1, R2, \u03bb1, \u03bb2 in the RNML code-length are given by\nlog log R2 R1 +m log log \u03bb2 \u03bb1 ,\nwhile those including the parameters R, \u03bb (j) min in the NML code-length are given by (5). Comparing them each other, we see that the dependency of the RNML code-length on the hyper parameters is lower than that of the NML code-length on the parameters by logarithmic order.\nWe further give a new formula of the RNML code-length relative to a GMM.\nTheorem 2 The RNML code-length of xn relative to a GMM is expanded as follows:\n\u2113RNML(x n, zn; \u03b3,K)\n= \u2212 log f(xn, zn;K, \u00b5\u0302(xn, zn), \u03a3\u0302(xn, zn)) + log C1(K,n)\n+ log C2(K,n) + logB(x n, zn) +K log I(m,\u03b3),\nwhere\nC1(K,n) = \u2211\nh1+\u00b7\u00b7\u00b7+hK=n\nn!\nh1! \u00b7 \u00b7 \u00b7 hK !\nK \u220f\nk=1\n(hk\nn\n)hk , (6)\nC2(K,n) = \u2211\nh1+\u00b7\u00b7\u00b7+hK=n\nn!\nh1! \u00b7 \u00b7 \u00b7 hK !\nK \u220f\nk=1\n(hk\nn\n)hk \u00b7 J(hk),\n(7)\nB(xn, zn) =\nK \u220f\np=1\n2m+1 \u00b7 ||\u00b5\u0302p(x n, zn)||m \u00b7 |\u03a3\u0302p(x n, zn)|\u2212 m 2\nmm+1\u0393(m2 ) ,\nI(m,\u03b3) = C(\u03b3) = (m\n2\n)m+1 \u00b7 log R2 R1 \u00b7 ( log \u03bb2 \u03bb1 )m ,\nJ(hk) = (hk\n2e\n)mhk \u00b7\n1\n\u0393m( hk\u22121 2 ) . (8)\nHere hk denotes the number of data belonging to the k-th cluster, and \u00b5\u0302p, \u03a3\u0302p denote mean and the ML estimates of the variance-covariance matrix for the p-th cluster.\nNote that straightforward computation of C1(K, n) and C2(K, n) as in (6) and (7) requires O(nK) time. Below we give methods for efficient computation of C1(K, n) and C2(K, n). As for the computation of C1(K, n), Kontkanen and Myllyma\u0308ki proved the following theorem:\nTheorem 3 [6] C1(K, n) satisfies the recursive formula:\nC1(K + 2, n) = C1(K + 1, n) + n\nK C1(K, n). (9)\nHence C1(K, n) is computed in time O(n+K).\nAs for the computation of C2(K, n), we newly give the following result:\nTheorem 4 C2(K, n) satisfies the following formula:\nC2(K + 1, n) = \u2211\nr1+r2=n\nnCr1 (r1 n )r1 (r2 n )r2 C2(K, r1)J(r2), (10)\nwhere J(r2) is as in (8). Hence C2(K, n) is computed in time O(n 2K).\nCombining all of the theorem as above, we see that the RNML code-length of xn relative to a GMM is computed in time O(n2K)."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Comparison with AIC and BIC", "text": "This section gives experimental results showing the validity of the RNML for GMMs. We generated a number of data sequences of size n according to the true GMM M of mixture size K. Each mixture component is a Gaussian distribution with mean \u00b5k and variancecovariance matrix \u03a3k (k = 1, \u00b7 \u00b7 \u00b7 , K). For each data sequence x\nn generated according to the true model M, we also generated their corresponding cluster indices zn using the EM algorithm [2], where zi showed which cluster xi came from (i = 1, . . . , n). In our experiment, we repeated cluster generation using the EM algorithm 100 times by changing initial values of the algorithm. We compared the four criteria: RNML, NML, Akaike\u2019s Information Criterion (AIC) [1] and Bayesian Information Criterion (BIC) [11] for the choice of the number of clusters. We calculated RNML and NML according to the method proposed in the previous sections and [5]. We calculated AIC and BIC as follows:\nAIC(xn, zn;K) = \u22122 log f(xn, zn;K, \u03b8\u0302(xn, zn))\n+m(m+ 3)K +K,\nBIC(xn, zn;K) = \u22122 log f(xn, zn;K, \u03b8\u0302(xn, zn))\n+ m(m+ 3)K\n2\nK \u2211\nk=1\nlog hk +K log n.\nWe measured their performance in terms of the identification probability P (K) and the benefit B(K) defined as follows: Letting K be the true number of clusters and K\u2217 be the one chosen using any criterion,\nP (K) = Prob(K\u2217 = K),\nB(K) = max\n{\n0, 1\u2212 |K\u2217 \u2212K|\nT\n}\n, (11)\nwhere T is a given constant. The identification probability P (K) is the probability that the algorithm outputs the true number of clusters. The benefit is a score assigned to K so that if K = K\u2217 it takes the maximum value 1, and it decreases linearly to zero as |K\u2217\u2212K| increases to T . The resulting benefit is calculated as the average of the benefits taken over all of random generation. We compared RNML, AIC, and BIC in terms of how fast the identification probability and the benefit converge as sample size n increases.\nFig. 1 and Fig. 2 show graphs of accuracy rates and benefit vs data size for the case where the data dimension was m = 5 and the true number of clusters was K = 3. Here we set T = 2 in the calculation of B(K) in (11).\n0 500 1000 1500 2000 0\n0.2\n0.4\n0.6\n0.8\n1\nsample size\nA cc\nur ac\ny R\nat e\n3 clusters, 5 dimension\nAIC BIC NML RNML\nFigure 1: Accuracy Rates\n0 500 1000 1500 2000 0\n0.2\n0.4\n0.6\n0.8\n1\nsample size\nbe ne\nfit\n3 clusters, 5 dimension\nAIC BIC NML RNML\nFigure 2: Benefit\n0 5 10 15 20 25 30 35 40 200\n400\n600\n800\n1000\n1200\n1400\n1600\nparameter 10x\nda ta\ns iz\ne re\nqu ire\nd fo\nr A\nR /b\nen ef\nit to\ne xc\nee d\n0. 8\nparameter and data size required for AR/benefit to exceed 0.8\nAR , NML AR , RNML benefit , NML benefit , RNML\nFigure 3: Data Size Required for Accuracy Rate/benefit to Exceed 0.8 vs Parameter Values\nWe see from these results that RNML achieved the highest identification probability, the highest benefit, and the fastest rate of convergence among all of the criteria: AIC, BIC, NML, and RNML. Specifically this was the case when the data size was not so large. This\nimplies that RNML was effective as a criterion for selecting an optimal number of clusters even when the data size was relatively small.\nTable 1, 2, 3, and 4 show the results on benefit obtained by varying the data dimension and the true number of clusters, where each numerical value in Tables indicates the least data size required for benefit to exceed 0.8. Here Inf shows that benefit did not exceed 0.8.\nWe see from these results that for most of pairs of m and K, RNML achieves high benefit with smaller data size than AIC, BIC, and NML. This implies that the number of clusters estimated by RNML is within \u00b11 of the true one with sufficiently high probability."}, {"heading": "4.2 Dependency of NML and RNML on Parameters", "text": "Fig.3 shows graphs of least data size required for accuracy rate and benefit to achieve 80% and 0.8 versus parameter values, respectively. We define parameter \u03b8 as \u03b8 = R2/R1 = \u03bb2/\u03bb1 in RNML, and \u03b8 = R = \u03bb (j) min \u2212m in NML. We see that the RNML do not depend on parameter values more than NML. It implies that the dependency of RNML on parameter values is much less than that of NML."}, {"heading": "5 Conclusion", "text": "We have proposed a general method for computing the NML code-length for the exponential family.We have developed it by generalizing the existing method for restricting the data domain so that the NML code-length does not diverge. We have specifically focused on Gaussian distributions and GMMs to propose a new efficient method for computing the RNML for them. We have developed it by extending Rissanen\u2019s renormalizing technique into multi-variate Gaussian distributions. We have applied this method to the clustering\nissue, in which we have selected the optimal number of clusters on the basis of the RNML code-length. We have empirically demonstrated using artificial data that our method makes the estimate of the number of clusters converge significantly faster to the true one than AIC, BIC, and NML."}], "references": [{"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "IEEE Trans. on Automatic Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1974}, {"title": "Maximum likelihood from incomplete data via the em", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J.Royal Staitst. Soc.B, 39:1\u201338", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1977}, {"title": "Variable selection in linear regression: Several approaches based on normalized maximum likelihood", "author": ["C.D. Giurc\u0103neanu", "S.A. Razavi", "A. Liski"], "venue": "Signal Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "The Minimum Description Length Principle", "author": ["P.D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Efficient computation of normalized maximum likelihood coding for gaussian mixtures with its applications to optimal clustering", "author": ["S. Hirai", "K. Yamanishi"], "venue": "The IEEE ISIT, pages 1031\u20131035", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "A linear time algorithm for computing the multinomial stochastic complexity", "author": ["P. Kontkanen", "P. Myllym\u00e4ki"], "venue": "Information Processing Letters, 103:227\u2013233", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "An empirical comparison of nml", "author": ["P. Kontkanen", "P. Myllym\u00e4ki"], "venue": "Proceedings of the 2008 International, pages 125\u2013131", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Fisher information and stochastic complexity", "author": ["J. Rissanen"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "MDL denoising", "author": ["J. Rissanen"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Information and Complexity in Statistical Modeling", "author": ["J. Rissanen"], "venue": "Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Annals of Statistics 6 (2), pages 461\u2013464", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1978}, {"title": "Universal sequential coding of single messages", "author": ["M. Shtarkov Yu"], "venue": "Problems of Information Transmission,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}], "referenceMentions": [{"referenceID": 7, "context": "The NML code-length for x relative to M is calculated as follows: \u2212 log fNML(x ;M) = \u2212 log f(x; \u03b8\u0302(x,M)) + log C(M), It is known from [8] that the NML code-length is optimal in the sense that it achieves the minimum of Shtarkov\u2019s minimax criterion [12].", "startOffset": 134, "endOffset": 137}, {"referenceID": 11, "context": "The NML code-length for x relative to M is calculated as follows: \u2212 log fNML(x ;M) = \u2212 log f(x; \u03b8\u0302(x,M)) + log C(M), It is known from [8] that the NML code-length is optimal in the sense that it achieves the minimum of Shtarkov\u2019s minimax criterion [12].", "startOffset": 248, "endOffset": 252}, {"referenceID": 7, "context": "The NML code-length is called the stochastic complexity [8] and has been employed as a criterion for statistical model selection on the basis of the minimum description length (MDL) principle [10, 4].", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "The NML code-length is called the stochastic complexity [8] and has been employed as a criterion for statistical model selection on the basis of the minimum description length (MDL) principle [10, 4].", "startOffset": 192, "endOffset": 199}, {"referenceID": 3, "context": "The NML code-length is called the stochastic complexity [8] and has been employed as a criterion for statistical model selection on the basis of the minimum description length (MDL) principle [10, 4].", "startOffset": 192, "endOffset": 199}, {"referenceID": 7, "context": "Rissanen [8] derived a formula of an asymptotic approximation of the NML code-length: \u2212 log p(x; \u03b8\u0302(x)) + k 2 log n 2\u03c0 + log \u222b", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "Kontkanen and Myllym\u00e4ki proposed efficient algorithms for the NML code-length for multinomial distributions and N\u00e4ive Bayes model [6, 7].", "startOffset": 130, "endOffset": 136}, {"referenceID": 6, "context": "Kontkanen and Myllym\u00e4ki proposed efficient algorithms for the NML code-length for multinomial distributions and N\u00e4ive Bayes model [6, 7].", "startOffset": 130, "endOffset": 136}, {"referenceID": 8, "context": "Rissanen proposed a method for circumventing this problem for linear regression models by making an elliptic constraint for the data domain so that the normalization term does not diverge [9].", "startOffset": 188, "endOffset": 191}, {"referenceID": 2, "context": "method using an rhomboid constraint [3].", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Note that all of these works [9, 3] considered 1dimensional Gaussian distributions.", "startOffset": 29, "endOffset": 35}, {"referenceID": 2, "context": "Note that all of these works [9, 3] considered 1dimensional Gaussian distributions.", "startOffset": 29, "endOffset": 35}, {"referenceID": 4, "context": "Hirai and Yamanishi [5] applied Rissanen\u2019s technique to the computation of the NML code-length for multi-variate Gaussian distributions.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "Kontkanen and Myllym\u00e4ki [7] proposed an efficient algorithm for NML-based clustering with optimal choices of mixture size for the case where the data domain was discrete.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Hirai and Yamanishi [5] proposed an algorithm for efficiently computing the NML code-length for Gaussian mixture models (GMM) for the case where the data domain was continuous.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "We extend Hirai and Yamanishi\u2019s method [5] for computing the NML code-length for Gaussian distributions and GMMs to exponential family including Gamma distributions, logistic distributions, etc.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "We apply Rissanen\u2019s renormalizing technique [9] into Gaussian distributions and GMMs to derive new formulas for computing the NML codelengths for them.", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "Conventional formulas in [5] depend on the parameters by which the data domain is restricted.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "The new formulas are obtained by renormalizing the likelihood with respect to the parameters, and are improved in that they are less dependent on hyperparameters than those in [5].", "startOffset": 176, "endOffset": 179}, {"referenceID": 8, "context": "Note that the RNML are different from Rissanen\u2019s original one [9] in that they are derived for the case where data is multi-dimensional while Rissanen considered a specific case where it was 1-dimensional.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Hirai and Yamanishi [5] derived a formula of the NML distribution by restricting the range of data so that the maximum likelihood lies in a bounded range specified by parameters.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "The normalization term C(R, \u03bbmin) is expanded as follows [5]: C(R, \u03bbmin) = 2R m 2 \u220fm j=1 \u03bb (j) min \u2212 m 2", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "As for the computation of C1(K, n), Kontkanen and Myllym\u00e4ki proved the following theorem: Theorem 3 [6] C1(K, n) satisfies the recursive formula: C1(K + 2, n) = C1(K + 1, n) + n K C1(K, n).", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "For each data sequence x n generated according to the true model M, we also generated their corresponding cluster indices z using the EM algorithm [2], where zi showed which cluster xi came from (i = 1, .", "startOffset": 147, "endOffset": 150}, {"referenceID": 0, "context": "We compared the four criteria: RNML, NML, Akaike\u2019s Information Criterion (AIC) [1] and Bayesian Information Criterion (BIC) [11] for the choice of the number of clusters.", "startOffset": 79, "endOffset": 82}, {"referenceID": 10, "context": "We compared the four criteria: RNML, NML, Akaike\u2019s Information Criterion (AIC) [1] and Bayesian Information Criterion (BIC) [11] for the choice of the number of clusters.", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "We calculated RNML and NML according to the method proposed in the previous sections and [5].", "startOffset": 89, "endOffset": 92}], "year": 2012, "abstractText": "We are concerned with the issue of how to calculate the normalized maximum likelihood (NML) code-length. There is a problem that the normalization term of the NML code-length may diverge when it is continuous and unbounded and a straightforward computation of it is highly expensive when the data domain is finite . In previous works it has been investigated how to calculate the NML code-length for specific types of distributions. We first propose a general method for computing the NML code-length for the exponential family. Then we specifically focus on Gaussian mixture model (GMM), and propose a new efficient method for computing the NML to them. We develop it by generalizing Rissanen\u2019s re-normalizing technique. Then we apply this method to the clustering issue, in which a clustering structure is modeled using a GMM, and the main task is to estimate the optimal number of clusters on the basis of the NML code-length. We demonstrate using artificial data sets the superiority of the NML-based clustering over other criteria such as AIC, BIC in terms of the data size required for high accuracy rate to be achieved. Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN Email: So Hirai@mist.i.u-tokyo.ac.jp He currently belongs to NTT DATA Corporation. Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, JAPAN Email: yamanishi@mist.i.u-tokyo.ac.jp", "creator": "LaTeX with hyperref package"}}}