{"id": "1205.6432", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2012", "title": "Multiclass Learning Approaches: A Theoretical Comparison with Implications", "abstract": "zwilling We kryvyi theoretically analyze hawaiians and anime compare cerva the following five newater popular multiclass classification hairdressers methods: kvasha One vs. tautog All, uncf All Pairs, hypo Tree - espoused based 68.20 classifiers, griaule Error valdese Correcting pipsqueak Output amoroso Codes (ECOC) blumenberg with puehringer randomly 00:01 generated singer-actor code clas matrices, puel and Multiclass SVM. westmalle In the tulloch first four hoga methods, the classification atofina is based gnas on a mathie reduction tudor to binary classification. 45.34 We 10500 consider equipping the guaz\u00fa case where 119.81 the memorias binary semenyih classifier unblocking comes weesen from unsuitably a wbre class of dm VC aor dimension $ d $, morrocco and psychoanalyze in particular http://www.nobel.se from the class of hybrid halfspaces 10:29 over $ \\ intimidating reals ^ d $. We analyze tamassos both the -2.9 estimation error porthole and rathbun the approximation error neurobiological of l'isle these methods. :@ Our amaranthe analysis reveals interesting conclusions of cse practical arpa-e relevance, booneville regarding the desktops success tabubil of saxifrage the different pilkey approaches under clamshell various multiclass conditions. Our dispar proof gedeck technique booklets employs unrepaired tools 20.84 from cocacola VC theory embalo to .621 analyze theosophists the \\ emph {bunde approximation watchtowers error} of hypothesis classes. This is in sharp ngunnawal contrast to galeone most, tauqir if gemlik not pettiness all, 147.4 previous booby-trapped uses of picpus VC partem theory, thammanoon which only deal with estimation error.", "histories": [["v1", "Tue, 29 May 2012 17:40:04 GMT  (29kb,D)", "https://arxiv.org/abs/1205.6432v1", null], ["v2", "Fri, 1 Jun 2012 14:12:58 GMT  (29kb,D)", "http://arxiv.org/abs/1205.6432v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "sivan sabato", "shai shalev-shwartz"], "accepted": true, "id": "1205.6432"}, "pdf": {"name": "1205.6432.pdf", "metadata": {"source": "CRF", "title": "Multiclass Learning Approaches: A Theoretical Comparison with Implications", "authors": ["Amit Daniely", "Sivan Sabato", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this work we consider multiclass prediction: The problem of classifying objects into one of several possible target classes. Applications include, for example, categorizing documents according to topic, and determining which object appears in a given image. We assume that objects (a.k.a. instances) are vectors in X = Rd and the class labels come from the set Y = [k] = {1, . . . , k}. Following the standard PAC model, the learner receives a training set of m examples, drawn i.i.d. from some unknown distribution, and should output a classifier which maps X to Y .\nThe centrality of the multiclass learning problem has spurred the development of various approaches for tackling the task. Perhaps the most straightforward approach is a reduction from multiclass classification to binary classification. For example, the One-vs-All (OvA) method is based on a reduction of the multiclass problem into k binary problems, each of which discriminates between one class to all the rest of the classes (e.g. Rumelhart et al. [1986]). A different reduction is the All-Pairs (AP) approach in which all pairs of classes are compared to each other [Hastie and Tibshirani, 1998]. These two approaches have been unified under the framework of Error Correction Output Codes (ECOC) [Dietterich and Bakiri, 1995, Allwein et al., 2000]. A tree-based classifier (TC) is another reduction in which the prediction is obtained by traversing a binary tree, where at each node of the tree a binary classifier is used to decide on the rest of the path (see for example Beygelzimer et al. [2007]).\nAll of the above methods are based on reductions to binary classification. We pay special attention to the case where the underlying binary classifiers are linear separators (halfspaces). Formally, each w \u2208 Rd+1 defines the linear separator hw(x) = sign(\u3008w, x\u0304\u3009), where x\u0304 = (x, 1) \u2208 Rd+1 is the concatenation of the vector x and the scalar 1. While halfspaces are our primary focus, many of our results hold for any underlying binary hypothesis class of VC dimension d+ 1. \u2217Dept. of Mathematics, The Hebrew University, Jerusalem, Israel \u2020School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel \u2021School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel\nar X\niv :1\n20 5.\n64 32\nv2 [\ncs .L\nG ]\n1 J\nun 2\nOther, more direct approaches to multiclass classification over Rd have also been proposed (e.g. Vapnik [1998], Weston and Watkins [1999], Crammer and Singer [2001]). In this paper we analyze the Multiclass SVM (MSVM) formulation of Crammer and Singer [2001], in which each hypothesis is of the form hW (x) = argmaxi\u2208[k](Wx\u0304)i, where W is a k\u00d7 (d+ 1) matrix and (Wx\u0304)i is the i\u2019th element of the vector Wx\u0304 \u2208 Rk.\nWe theoretically analyze the prediction performance of the aforementioned methods, namely, OvA, AP, ECOC, TC, and MSVM. The error of a multiclass predictor h : Rd \u2192 [k] is defined to be the probability that h(x) 6= y, where (x, y) is sampled from the underlying distributionD over Rd\u00d7 [k], namely, Err(h) = P(x,y)\u223cD[h(x) 6= y]. Our main goal is to understand which method is preferable in terms of the error it will achieve, based on easy-to-verify properties of the problem at hand.\nOur analysis pertains to the type of classifiers each method can potentially find, and does not depend on the specific training algorithm. More precisely, each method corresponds to a hypothesis class, H, which contains the multiclass predictors that may be returned by the method. For example, the hypothesis class of MSVM isH = {x 7\u2192 argmaxi\u2208[k](Wx\u0304)i : W \u2208 Rk\u00d7(d+1)}.\nA learning algorithm, A, receives a training set, S = {(xi, yi)}mi=1, sampled i.i.d. according to D, and returns a multiclass predictor which we denote by A(S) \u2208 H. A learning algorithm is called an Empirical Risk Minimizer (ERM) if it returns a hypothesis inH that minimizes the empirical error on the sample. We denote by h? a hypothesis inH with minimal error,1 that is, h? \u2208 argminh\u2208H Err(h).\nWhen analyzing the error of A(S), it is convenient to decompose this error as a sum of approximation error and estimation error:\nErr(A(S)) = Err(h?)\ufe38 \ufe37\ufe37 \ufe38 approximation + Err(A(S))\u2212 Err(h?)\ufe38 \ufe37\ufe37 \ufe38 estimation . (1)\n\u2022 The approximation error is the minimum error achievable by a predictor in the hypothesis class,H. The approximation error does not depend on the sample size, and is determined solely by the allowed hypothesis class.\n\u2022 The estimation error of an algorithm is the difference between the approximation error, and the error of the classifier the algorithm chose based on the sample. This error exists both for statistical reasons, since the sample may not be large enough to determine the best hypothesis, and for algorithmic reasons, since the learning algorithm may not output the best possible hypothesis given the sample. For the ERM algorithm, the estimation error can be bounded from above by order of \u221a C(H)/m\nwhere C(H) is a complexity measure of H (analogous to the VC dimension) and m is the sample size. A similar term also bounds the estimation error from below for any algorithm. Thus C(H) is an estimate of the best achievable estimation error for the class.\nWhen studying the estimation error of different methods, we follow the standard distribution-free analysis. Namely, we will compare the algorithms based on the worst-case estimation error, where worst-case is over all possible distributions D. Such an analysis can lead us to the following type of conclusion: If two hypothesis classes have roughly the same complexity, C(H1) \u2248 C(H2), and the number of available training examples is significantly larger than this value of complexity, then for both hypothesis classes we are going to have a small estimation error. Hence, in this case the difference in prediction performance between the two methods will be dominated by the approximation error and by the success of the learning algorithm in approaching the best possible estimation error. In our discussion below we disregard possible differences in optimality which stem from algorithmic aspects and implementation details. A rigorous comparison of training heuristics would certainly be of interest and is left to future work.\nFor the approximation error we will provide even stronger results, by comparing the approximation error of classes for any distribution. We rely on the following definition.\n1For simplicity, we assume that the minimum is attainable.\nDefinition 1.1. Given two hypothesis classes, H,H\u2032, we say that H essentially contains H\u2032 if for any distribution, the approximation error ofH is at most the approximation error ofH\u2032. H strictly containsH\u2032 if, in addition, there is a distribution for which the approximation error ofH is strictly smaller than that of H\u2032.\nOur main findings are as follows (see a full comparison in Table 1). The formal statements are given in Section 3.\n\u2022 The estimation errors of OvA, MSVM, and TC are all roughly the same, in the sense that C(H) = \u0398\u0303(dk) for all of the corresponding hypothesis classes. The complexity of AP is \u0398\u0303(dk2). The complexity of ECOC with a code of length l and code-distance \u03b4 is at most O\u0303(dl) and at least d\u03b4/2. It follows that for randomly generated codes, C(H) = \u0398\u0303(dl). Note that this analysis shows that a larger code-distance yields a larger estimation error and might therefore hurt performance. This contrasts with previous \u201creduction-based\u201d analyses of ECOC, which concluded that a larger code distance improves performance.\n\u2022 We prove that the hypothesis class of MSVM essentially contains the hypothesis classes of both OvA and TC. Moreover, these inclusions are strict. Since the estimation errors of these three methods are roughly the same, it follows that the MSVM method dominates both OvA and TC in terms of achievable prediction performance.\n\u2022 In the TC method, one needs to associate each leaf of the tree to a label. If no prior knowledge on how to break the symmetry is known, it is suggested in Beygelzimer et al. [2007] to break symmetry by choosing a random permutation of the labels. We show that whenever d k, for any distribution D, with high probability over the choice of a random permutation, the approximation error of the resulting tree would be close to 1/2. It follows that a random choice of a permutation is likely to yield a poor predictor.\n\u2022 We show that if d k, for any distribution D, the approximation error of ECOC with a randomly generated code matrix is likely to be close to 1/2.\n\u2022 We show that the hypothesis class of AP essentially contains the hypothesis class of MSVM (hence also that of OvA and TC), and that there can be a substantial gap in the containment. Therefore, as expected, the relative performance of AP and MSVM depends on the well-known trade-off between estimation error and approximation error.\nThe above findings suggest that in terms of performance, it may be wiser to choose MSVM over OvA and TC, and especially so when d k. We note, however, that in some situations (e.g. d = k) the prediction success of these methods can be similar, while TC has the advantage of having a testing run-time of d log(k), compared to the testing run-time of dk for OvA and MSVM. In addition, TC and ECOC may be a good choice when there is additional prior knowledge on the distribution or on how to break symmetry between the different labels."}, {"heading": "1.1 Related work", "text": "Allwein et al. [2000] analyzed the multiclass error of ECOC as a function of the binary error. The problem with such a \u201creduction-based\u201d analysis is that such analysis becomes problematic if the underlying binary problems are very hard. Indeed, our analysis reveals that the underlying binary problems would be too hard if d k and the code is randomly generated. The experiments in Allwein et al. [2000] show that when using kernel-based SVM or AdaBoost as the underlying classifier, OvA is inferior to random ECOC. However, in their experiments, the number of classes is small relative to the dimension of the feature space, especially if working with kernels or with combinations of weak learners.\nCrammer and Singer [2001] presented experiments demonstrating that MSVM outperforms OvA on several data sets. Rifkin and Klautau [2004] criticized the experiments of Crammer and Singer [2001], Allwein et al. [2000], and presented another set of experiments demonstrating that all methods perform roughly the same when the underlying binary classifier is very strong (SVM with a Guassian kernel). As our analysis shows, it is not surprising that with enough data and powerful binary classifiers, all methods should perform well. However, in many practical applications, we will prefer not to employ kernels (either because of shortage of examples, which might lead to a large estimation error, or due to computational constraint), and in such cases we expect to see a large difference between the methods.\nBeygelzimer et al. [2007] analyzed the regret of a specific training method for trees, called Filter Tree, as a function of the regret of the binary classifier. The regret is defined to be the difference between the learned classifier and the Bayes-optimal classifier for the problem. Here again we show that the regret values of the underlying binary classifiers are likely to be very large whenever d k and the leaves of the tree are associated to labels in a random way. Thus in this case the regret analysis is problematic. Several authors presented ways to learn better splits, which corresponds to learning the association of leaves to labels (see for example Bengio et al. [2011] and the references therein). Some of our negative results do not hold for such methods, as these do not randomly attach labels to tree leaves.\nDaniely et al. [2011] analyzed the properties of multiclass learning with various ERM learners, and have also provided some bounds on the estimation error of multiclass SVM and of trees. In this paper we both improve these bounds, derive new bounds for other classes, and also analyze the approximation error of the classes. To the best of our knowledge, this is the first case of using VC theory to analyze the approximation error of hypothesis classes."}, {"heading": "2 Definitions and Preliminaries", "text": "We first formally define the hypothesis classes that we analyze in this paper.\nMulticlass SVM (MSVM): For W \u2208 Rk\u00d7(d+1) define hW : Rd \u2192 [k] by hW (x) = argmaxi\u2208[k](Wx\u0304)i and let L = {hW : W \u2208 Rk\u00d7(d+1)}. Though NP-hard in general, solving the ERM problem with respect to L can be done efficiently in the realizable case (namely, whenever exists a hypothesis with zero empirical error on the sample).\nTree-based classifiers (TC): A tree-based multiclass classifier is a full binary tree whose leaves are associated with class labels and whose internal nodes are associated with binary classifiers. To classify an instance, we start with the root node and apply the binary classifier associated with it. If the prediction is 1 we traverse to the right child. Otherwise, we traverse to the left child. This process continues until we reach a leaf, and then we output the label associated with the leaf. Formally, a tree for k classes is a full binary tree T together with a bijection \u03bb : leaf(T )\u2192 [k], which associates a label to each of the leaves. We usually identify T with the pair (T, \u03bb). The set of internal nodes of T is denoted by N(T ). Let H \u2282 {\u00b11}X be a binary hypothesis class. Given a mapping C : N(T )\u2192 H, define a multiclass predictor, hC : X \u2192 [k], by setting hC(x) = \u03bb(v) where v is the last node of the root-to-leaf path v1, . . . vm = v such that vi+1 is the\nleft (resp. right) child of vi if C(vi)(x) = \u22121 (resp. C(vi)(x) = 1). Let HT = {hC | C : N(T ) \u2192 H}. Also, let Htrees = \u222aT is a tree for k classesHT . If H is the class of linear separators over Rd, then for any tree T the ERM problem with respect toHT can be solved efficiently in the realizable case. However, the ERM problem is NP-hard in the non-realizable case.\nError Correcting Output Codes (ECOC): An ECOC is a code M \u2208 Rk\u00d7l along with a bijection \u03bb : [k] \u2192 [k]. We sometimes identify \u03bb with the identity function and M with (M,\u03bb)2. Given a code M , and the result of l binary classifiers represented by a vector u \u2208 {\u22121, 1}l, the code selects a label via M\u0303 : {\u22121, 1}l \u2192 [k], defined by M\u0303(u) = \u03bb ( arg maxi\u2208[k] \u2211l j=1Mijuj ) . Given binary classifiers h1, . . . , hl\nfor each column in the code matrix, the code assigns to the instance x \u2208 X the label M\u0303(h1(x), . . . , hl(x)). Let H \u2282 {\u00b11}X be a binary hypothesis class. Denote by HM \u2286 [k]X the hypotheses class HM = {h : X \u2192 [k] | \u2203(h1, . . . , hl) \u2208 Hl s.t. \u2200x \u2208 X , h(x) = M\u0303(h1(x), . . . , hl(x))}.\nThe distance of a binary code, denoted by \u03b4(M) for M \u2208 {\u00b11}k\u00d7l, is the minimal hamming distance between any two pairs of rows in the code matrix. Formally, the hamming distance between u, v \u2208 {\u22121,+1}l is \u2206h(u, v) = |{r : u[r] 6= v[r]}|, and \u03b4(M) = min1\u2264i<j\u2264k \u2206h(M [i],M [j]). The ECOC paradigm described in [Dietterich and Bakiri, 1995] proposes to choose a code with a large distance.\nOne vs. All (OvA) and All Pairs (AP): Let H \u2282 {\u00b11}X and k \u2265 2. In the OvA method we train k binary problems, each of which discriminates between one class and the rest of the classes. In the AP approach all pairs of classes are compared to each other. This is formally defined as two ECOCs. Define MOvA \u2208 Rk\u00d7k to be the matrix whose (i, j) elements is 1 if i = j and \u22121 if i 6= j. Then, the hypothesis class of OvA is HOvA = HMOvA . For the AP method, let MAP \u2208 Rk\u00d7( k 2) be such that for all i \u2208 [k] and 1 \u2264 j < l \u2264 k, the coordinate corresponding to row i and column (j, l) is defined to be \u22121 if i = j, 1 if i = l, and 0 otherwise. Then, the hypothesis class of AP isHAP = HMAP .\nOur analysis of the estimation error is based on results that bound the sample complexity of multiclass learning. The sample complexity of an algorithm A is the function mA defined as follows: For , \u03b4 > 0, mA( , \u03b4) is the smallest integer such that for every m \u2265 mA( , \u03b4) and every distribution D on X \u00d7Y , with probability of > 1\u2212 \u03b4 over the choice of an i.i.d. sample S of size m,\nErr(A(Sm)) \u2264 min h\u2208H Err(h) + . (2)\nThe first term on the right-hand side is the approximation error of H. Therefore, the sample complexity is the number of examples required to ensure that the estimation error ofA is at most (with high probability). We denote the sample complexity of a class H by mH( , \u03b4) = infAmA( , \u03b4), where the infimum is taken over all learning algorithms.\nTo bound the sample complexity of a hypothesis class we rely on upper and lower bounds on the sample complexity in terms of two generalizations of the VC dimension for multiclass problems, called the Graph dimension and the Natarajan dimension and denoted dG(H) and dN (H). For completeness, these dimensions are formally defined in the appendix.\nTheorem 2.1. Daniely et al. [2011] For every hypothesis classH, and for every ERM rule,\n\u2126\n( dN (H) + ln( 1\u03b4 )\n2\n) \u2264 mH( , \u03b4) \u2264 mERM( , \u03b4) \u2264 O ( min{dN (H) ln(|Y|), dG(H)}+ ln( 1\u03b4 )\n2 ) We note that the constants in the O,\u2126 notations are universal.\n2The use of \u03bb here allows us to later consider codes with random association of rows to labels."}, {"heading": "3 Main Results", "text": "In Section 3.1 we analyze the sample complexity of the different hypothesis classes. We provide lower bounds on the Natarajan dimensions of the various hypothesis classes, thus concluding, in light of Theorem 2.1, a lower bound on the sample complexity of any algorithm. We also provide upper bounds on the graph dimensions of these hypothesis classes, yielding, by the same theorem, an upper bound on the estimation error of ERM. In Section 3.2 we analyze the approximation error of the different hypothesis classes."}, {"heading": "3.1 Sample Complexity", "text": "Together with Theorem 2.1, the following theorems estimate, up to logarithmic factors, the sample complexity of the classes under consideration. We note that these theorems support the rule of thumb that the Natarajan and Graph dimensions are of the same order of the number of parameters. The first theorem shows that the sample complexity of MSVM depends on \u0398\u0303(dk).\nTheorem 3.1. d(k \u2212 1) \u2264 dN (L) \u2264 dG(L) \u2264 O(dk log(dk)).\nNext, we analyze the sample complexities of TC and ECOC. These methods rely on an underlying hypothesis class of binary classifiers. While our main focus is the case in which the binary hypothesis class is halfspaces over Rd, the upper bounds on the sample complexity we derive below holds for any binary hypothesis class of VC dimension d+ 1.\nTheorem 3.2. For every binary hypothesis class of VC dimension d + 1, and for any tree T , dG(HT ) \u2264 dG(Htrees) \u2264 O(dk log(dk)). If the underlying hypothesis class is halfspaces over Rd, then also\nd(k \u2212 1) \u2264 dN (HT ) \u2264 dG(HT ) \u2264 dG(Htrees) \u2264 O(dk log(dk)).\nTheorems 3.1 and 3.2 improve results from Daniely et al. [2011] where it was shown that bd2cb k 2 c \u2264 dN (L) \u2264 O(dk log(dk)), and for every tree dG(HT ) \u2264 O(dk log(dk)). Further it was shown that if H is the set of halfspaces over Rd, then \u2126 ( dk\nlog(k)\n) \u2264 dN (HT ).\nWe next turn to results for ECOC, and its special cases OvA and AP.\nTheorem 3.3. For every M \u2208 Rk\u00d7l and every binary hypothesis class of VC dimension d, dG(HM ) \u2264 O(dl log(dl)). Moreover, if M \u2208 {\u00b11}k\u00d7l and the underlying hypothesis class is halfspaces over Rd, then\nd \u00b7 \u03b4(M)/2 \u2264 dN (HM ) \u2264 dG(HM ) \u2264 O(dl log(dl)) .\nWe note if the code has a large distance, which is the case, for instance, in random codes, then \u03b4(M) = \u2126(l). In this case, the bound is tight up to logarithmic factors.\nTheorem 3.4. For any binary hypothesis class of VC dimension d, dG(HOvA) \u2264 O(dk log(dk)) and dG(HAP) \u2264 O(dk2 log(dk)). If the underlying hypothesis class is halfspaces over Rd we also have:\nd(k \u2212 1) \u2264 dN (HOvA) \u2264 dG(HOvA) \u2264 O(dk log(dk)) and d ( k\u22121\n2\n) \u2264 dN (HAP) \u2264 dG(HAP) \u2264 O(dk2 log(dk))."}, {"heading": "3.2 Approximation error", "text": "We first show that the class L essentially contains HOvA and HT for any tree T , assuming, of course, that H is the class of halfspaces in Rd. We find this result quite surprising, since the sample complexity of all of these classes is of the same order.\nTheorem 3.5. L essentially containsHtrees andHOvA. These inclusions are strict for d \u2265 2 and k \u2265 3.\nOne might suggest that a small increase in the dimension would perhaps allow us to embed L inHT for some tree T or for OvA. The next result shows that this is not the case.\nTheorem 3.6. Any embedding into a higher dimension that allows HOvA or HT (for some tree T for k classes) to essentially contain L, necessarily embeds into a dimension of at least \u2126\u0303(dk).\nThe next theorem shows that the approximation error of AP is better than that of MSVM (and hence also better than OvA and TC). This is expected as the sample complexity of AP is considerably higher, and therefore we face the usual trade-off between approximation and estimation error.\nTheorem 3.7. HAP essentially contains L. Moreover, there is a constant k\u2217 > 0, independent of d, such that the inclusion is strict for all k \u2265 k\u2217.\nFor a random ECOC of length o(k), it is easy to see that it does not contain MSVM, as MSVM has higher complexity. It is also not contained in MSVM, as it generates non-convex regions of labels.\nWe next derive absolute lower bounds on the approximation errors of ECOC and TC when d k. Recall that both methods are built upon binary classifiers that should predict h(x) = 1 if the label of x is in L, for some L \u2282 [k], and should predict h(x) = \u22121 if the label of x is not in L. As the following lemma shows, when the partition of [k] into the two sets L and [k] \\ L is arbitrary and balanced, and k d, such binary classifiers will almost always perform very poorly.\nLemma 3.8. There exists a constant C > 0 for which the following holds. Let H \u2286 {\u00b11}X be any hypothesis class of VC-dimension d, let \u00b5 \u2208 (0, 1/2], and let D be any distribution over X \u00d7 [k] such that \u2200i P(x,y)\u223cD(y = i) \u2264 10k . Let \u03c6 : [k]\u2192 {\u00b11} be a randomly chosen function which is sampled according to one of the following rules: (1) For each i \u2208 [k], each coordinate \u03c6(i) is chosen independently from the other coordinates and P(\u03c6(i) = \u22121) = \u00b5; or (2) \u03c6 is chosen uniformly among all functions satisfying |{i \u2208 [k] : \u03c6(i) = \u22121}| = \u00b5k.\nLet D\u03c6 be the distribution over X \u00d7 {\u00b11} obtained by drawing (x, y) according to D and replacing it with (x, \u03c6(y)). Then, for any \u03bd > 0, if k \u2265 C \u00b7 ( d+ln( 1\u03b4 )\n\u03bd2\n) , then with probability of at least 1\u2212 \u03b4 over the\nchoice of \u03c6, the approximation error ofH with respect to D\u03c6 will be at least \u00b5\u2212 \u03bd.\nAs the corollaries below show, Lemma 3.8 entails that when k d, both random ECOCs with a small code length, and balanced trees with a random labeling of the leaves, are expected to perform very poorly.\nCorollary 3.9. There is a constant C > 0 for which the following holds. Let (T, \u03bb) be a tree for k classes such that \u03bb : leaf(T ) \u2192 [k] is chosen uniformly at random. Denote by kL and kR the number of leaves of the left and right sub-trees (respectively) that descend from root, and let \u00b5 = min{k1k , k2 k }. Let H \u2286 {\u00b11}X be a hypothesis class of VC-dimension d, let \u03bd > 0, and letD be any distribution over X \u00d7 [k] such that \u2200i P(x,y)\u223cD(y = i) \u2264 10k . Then, for k \u2265 C \u00b7 ( d+ln( 1\u03b4 ) \u03bd2 ) , with probability of at least 1 \u2212 \u03b4 over the choice of \u03bb, the approximation error ofHT with respect to D is at least \u00b5\u2212 \u03bd.\nCorollary 3.10. There is a constant C > 0 for which the following holds. Let (M,\u03bb) be an ECOC where M \u2208 Rk\u00d7l, and assume that the bijection \u03bb : [k] \u2192 [k] is chosen uniformly at random. Let H \u2286 {\u00b11}X be a hypothesis class of VC-dimension d, let \u03bd > 0, and let D be any distribution over X \u00d7 [k] such that \u2200i P(x,y)\u223cD(y = i) \u2264 10k . Then, for k \u2265 C \u00b7 ( dl log(dl)+ln( 1\u03b4 ) \u03bd2 ) , with probability of at least 1\u2212 \u03b4 over the choice of \u03bb, the approximation error ofHM with respect to D is at least 1/2\u2212 \u03bd.\nNote that the first corollary holds even if only the top level of the binary tree is balanced and splits the labels randomly to the left and the right sub-trees. The second corollary holds even if the code itself is not random (nor does it have to be binary), and only the association of rows with labels is random. In particular, if the length of the code is O(log(k)), as suggested in Allwein et al. [2000], and the number of classes is \u2126\u0303(d), then the code is expected to perform poorly.\nFor an ECOC with a matrix of length \u2126(k) and d = o(k), we do not have such a negative result as stated in Corollary 3.10. Nonetheless, Lemma 3.8 implies that the prediction of the binary classifiers when d = o(k) is just slightly better than a random guess, thus it seems to indicate that the ECOC method will still perform poorly. Moreover, most current theoretical analyses of ECOC estimate the error of the learned multiclass hypothesis in terms of the average error of the binary classifiers. Alas, when the number of classes is large, Lemma 3.8 shows that this average will be close to 12 .\nFinally, let us briefly discuss the tightness of Lemma 3.8. Let x1, . . . , xd+1 \u2208 Rd be affinely independent and let D be the distribution over Rd \u00d7 [d + 1] defined by P(x,y)\u223cD((x, y) = (xi, i)) = 1d+1 . Is is not hard to see that for every \u03c6 : [d + 1] \u2192 {\u00b11}, the approximation error of the class of halfspaces with respect toD\u03c6 is zero. Thus, in order to ensure a large approximation error for every distribution, the number of classes must be at least linear in the dimension, so in this sense, the lemma is tight. Yet, this example is very simple, since each class is concentrated on a single point and the points are linearly independent. It is possible that in real-world distributions, a large approximation error will be exhibited even when k < d.\nWe note that the phenomenon of a large approximation error, described in Corollaries 3.9 and 3.10, does not reproduce in the classes L,HOvA andHAP , since these classes are symmetric."}, {"heading": "4 Proof Techniques", "text": "Due to lack of space, the proofs for all the results stated above are provided in the appendix. In this section we give a brief description of our main proof techniques.\nMost of our proofs for the estimation error results, stated in Section 3.1, are based on a similar method which we now describe. Let L : {\u00b11}l \u2192 [k] be a multiclass-to-binary reduction (e.g., a tree), and for H \u2286 {\u00b11}X , denote L(H) = {x 7\u2192 L(h1(x), . . . , hl(x)) | h1, . . . , hl \u2208 H}. Our upper bounds for dG(L(H)) are mostly based on the following simple lemma.\nLemma 4.1. If VC(H) = d then dG(L(H)) = O(ld ln(ld)).\nThe technique for the lower bound on dN (L(W)) when W is the class of halfspaces in Rd is more involved, and quite general. We consider a binary hypothesis class G \u2286 {\u00b11}[d]\u00d7[l] which consists of functions having an arbitrary behaviour over [d]\u00d7 {i}, and a very uniform behaviour on other inputs (such as mapping all other inputs to a constant). We show that L(G) N -shatters the set [d]\u00d7 [l]. Since G is quite simple, this is usually not very hard to show. Finally, we show that the class of halfspaces is richer than G, in the sense that the inputs to G can be mapped to points in Rd such that the functions of G can be mapped to halfspaces. We conclude that dN (L(W)) \u2265 dN (L(G)).\nTo prove the approximation error lower bounds stated in Section 3.2, we use the techniques of VC theory in an unconventional way. The idea of this proof is as follows: Using a uniform convergence argument based on the VC dimension of the binary hypothesis class, we show that there exists a small labeled sample S whose approximation error for the hypothesis class is close to the approximation error for the distribution, for all possible label mappings. This allows us to restrict our attention to a finite set of hypotheses, by their restriction to the sample. For these hypotheses, we show that with high probability over the choice of label mapping, the approximation error on the sample is high. A union bound on the finite set of possible hypotheses shows that the approximation error on the distribution will be high, with high probability over the choice of the label mapping."}, {"heading": "5 Implications", "text": "The first immediate implication of our results is that whenever the number of examples in the training set is \u2126\u0303(dk), MSVM should be preferred to OvA and TC. This is certainly true if the hypothesis class of MSVM, L, has a zero approximation error (the realizable case), since the ERM is then solvable with respect to L.\nNote that since the inclusions given in Theorem 3.5 are strict, there are cases where the data is realizable with MSVM but not withHOvA or with respect to any tree.\nIn the non-realizable case, implementing the ERM is intractable for all of these methods. Nonetheless, for each method there are reasonable heuristics to approximate the ERM, which should work well when the approximation error is small. Therefore, we believe that MSVM should be the method of choice in this case as well due to its lower approximation error. However, variations in the optimality of algorithms for different hypothesis classes should also be taken into account in this analysis. We leave this detailed analysis of specific training heuristics for future work. Our analysis also implies that it is highly unrecommended to use TC with a randomly selected \u03bb or ECOC with a random code whenever k > d. Finally, when the number of examples is much larger than dk2, the analysis implies that it is better to choose the AP approach.\nTo conclude this section, we illustrate the relative performance of MSVM, OvA, TC, and ECOC, by considering the simplistic case where d = 2, and each class is concentrated on a single point in R2. In the leftmost graph below, there are two classes in R2, and the approximation error of all algorithms is zero. In the middle graph, there are 9 classes ordered on the unit circle of R2. Here, both MSVM and OvA have a zero approximation error, but the error of TC and of ECOC with a random code will most likely be large. In the rightmost graph, we chose random points in R2. MSVM still has a zero approximation error. However, OvA cannot learn the binary problem of distinguishing between the middle point and the rest of the points and hence has a larger approximation error.\nMSVM 3 3 3 OvA 3 3 7 TC/ECOC 3 7 7"}, {"heading": "A Proofs", "text": ""}, {"heading": "A.1 Notation and Definitions", "text": "Throughout the proofs, we fix d, k \u2265 2. We denote byW = Wd = {hw : w \u2208 Rd+1} the class of linear separators (with bias) over Rd. We assume the following \u201dtie breaking\u201d conventions:\n\u2022 For f : [k]\u2192 R, argmaxi\u2208[k]f(i) is the minimal number i0 \u2208 [k] for which f(i0) = maxi\u2208[k] f(i);\n\u2022 sign(0) = 1.\nGiven a hypotheses classH \u2286 YX , denote its restriction toA \u2286 X byH|A = {f |A : f \u2208 H}. LetH \u2286 YX be a hypothesis class and let \u03c6 : Y \u2192 Y \u2032, \u03b9 : X \u2192 X \u2032 be functions. Denote \u03c6 \u25e6 H = {\u03c6 \u25e6 h : h \u2208 H} and H \u25e6 \u03b9 = {h \u25e6 \u03b9 : h \u2208 H}.\nGiven H \u2286 YX and a distribution D over X \u00d7 Y , denote the approximation error by Err\u2217D(H) = infh\u2208H ErrD(h). Recall that by definition 1.1, H essentially contains H\u2032 \u2286 YX if and only if Err\u2217D(H) \u2264 Err\u2217D(H\u2032) for every distribution D. For a binary hypothesis classH, denote its VC dimension by VC(H).\nLet H \u2286 YX be a hypothesis class and let S \u2286 X . We say that H G-shatters S if there exists an f : S \u2192 Y such that for every T \u2286 S there is a g \u2208 H such that\n\u2200x \u2208 T, g(x) = f(x), and \u2200x \u2208 S \\ T, g(x) 6= f(x).\nWe say that H N-shatters S if there exist f1, f2 : S \u2192 Y such that \u2200y \u2208 S, f1(y) 6= f2(y), and for every T \u2286 S there is a g \u2208 H such that\n\u2200x \u2208 T, g(x) = f1(x), and \u2200x \u2208 S \\ T, g(x) = f2(x).\nThe graph dimension of H, denoted dG(H), is the maximal cardinality of a set that is G-shattered by H. The Natarajan dimension of H, denoted dN (H), is the maximal cardinality of a set that is N-shattered by H. Both of these dimensions coincide with the VC-dimension for |Y| = 2. Note also that we always have dN (H) \u2264 dG(H). As shown in Ben-David et al. [1995], it also holds that dG(H) \u2264 4.67 log2(|Y |)dN (H).\nProof of Lemma 4.1. Let A \u2286 X be a G-shattered set with |A| = dG(L(H)). By Sauer\u2019s Lemma, 2|A| \u2264 |H|A|l \u2264 |A|dl, thus dG(L(H)) = |A| = O(ld log(ld))."}, {"heading": "A.2 Multiclass SVM", "text": "Proof of Theorem 3.1. The lower bound follows from Theorems 3.5 and 3.2. To upper bound dG := dG(L), let S = {x1, . . . , xdG} \u2286 Rd be a set which is G-shattered by L, and let f : S \u2192 [k] be a function that witnesses the shattering. For x \u2208 Rd and j \u2208 [k], denote\n\u03c6(x, j) = (0, . . . 0, x[1], . . . , x[d], 1, 0, . . . , 0) \u2208 R(d+1)k,\nwhere x[1] is in the (d+ 1)(j \u2212 1) coordinate. For every (i, j) \u2208 [dG]\u00d7 [k], define zi,j = \u03c6(xi, f(xi))\u2212 \u03c6(xi, j). Denote Z = {zi,j | (i, j) \u2208 [dG]\u00d7 [k]}. Since VC(W(d+1)k) = (d+ 1)k+ 1, by Sauer\u2019s lemma,\n|W(d+1)k|Z | \u2264 |Z|(d+1)k+1 = (dGk)(d+1)k+1.\nWe now show that there is a one-to-one mapping from subsets of S toW(d+1)k|Z , thus concluding an upper bound on the size of S. For any T \u2286 S, choose W (T ) \u2208 Rk\u00d7(d+1)(R) such that\n{x \u2208 S | hW (T )(x) = f(x)} = T.\nSuch a W (T ) exists because of the G-shattering of S by L using the witness f . Define the vector w(T ) \u2208 Rk(d+1) which is the concatenation of the rows of W (T ), that is\nw(T ) = (W (T )(1,1), . . . ,W (T )(1,d+1), . . . ,W (T )(k,1), . . . ,W (T )(k,d+1))\n. Now, suppose that T1 6= T2 for T1, T2 \u2286 S. We now show that w(T1)|Z 6= w(T2)|Z . Suppose w.l.o.g. that there is some xi \u2208 T1 \\ T2. Thus, f(xi) = hW (T1)(xi) 6= hW (T2)(xi) =: j. It follows that the inner product of xi with row f(xi) of W (T1) is greater than the inner product of xi with row j of W (T1), while for W (T2), the situation is reversed. Therefore, sign(\u3008w(T1), zi,j\u3009) 6= sign(\u3008w(T2), zi,j\u3009), so w(T1) and w(T2) induce different labelings of Z. It follows that the number of subsets of S is bounded by the size of W(d+1)k|Z , thus 2dG \u2264 (kdG)(d+1)k+1. We conclude that dG \u2264 O(dk log(dk)).\nA.3 Simple classes that can be represented by the class of linear separators In this section we define two fairly simple hypothesis classes, and show that the class of linear separators is richer than them. We will later use this observation to prove lower bounds on the Natarajan dimension of various multiclass hypothesis classes.\nLet l \u2265 2. For f \u2208 {\u22121, 1}[d], i \u2208 [l], j \u2208 {\u22121, 1} define f i,j : [d]\u00d7 [l]\u2192 {\u22121, 1} by\nf i,j(u, v) =\n{ f(u) v = i\nj v 6= i,\nAnd define the hypothesis class F l as\nF l = {f i,j : f \u2208 {\u00b11}[d], i \u2208 [l], j \u2208 {\u22121, 1}}.\nFor g \u2208 {\u22121, 1}[d], i \u2208 [l], j \u2208 {\u00b11} define gi,j : [d]\u00d7 [l]\u2192 {\u22121, 1} by\ngi,j(u, v) =  h(u) v = i j v > i\n\u2212j v < i,\nAnd define the hypothesis class Gl as\nGl = {gi,j : g \u2208 {\u22121, 1}[d], i \u2208 [l], j \u2208 {\u00b11}}.\nLet H \u2282 YX ,H\u2032 \u2282 YX \u2032 be two hypotheses classes. We say that H is richer than H\u2032 if there is a mapping \u03b9 : X \u2032 \u2192 X such that H\u2032 = H \u25e6 \u03b9. It is clear that if H is richer than H\u2032 then dN (H\u2032) \u2264 dN (H) and dG(H\u2032) \u2264 dG(H). Thus, the notion of richness can be used to establish lower and upper bounds on the Natarajan and Graph dimension, respectively. The following lemma shows thatW is richer than F l and Gl for every l. This will allow us to use the classes F l, Gl instead ofW when bounding from below the dimension of an ECOC or TC hypothesis class in which the binary classifiers are fromW .\nLemma A.1. For any integer l \u2265 2,W is richer than F l and Gl.\nProof. We shall first prove that W is richer than F l. Choose l unit vectors e1, . . . , el \u2208 Rd. For every i \u2208 [l], choose d affinely independent vectors such that\nx1,i, . . . , xd,i \u2208 {x \u2208 Rd : \u3008x, ei\u3009 = 1, \u2200i\u2032 6= i, \u3008x, ei\u2032\u3009 < 1}.\nThis can be done by choosing d affinely independent vectors in {x \u2208 Rd : \u3008x, ei\u3009 = 1} that are very close to ei. Define \u03b9(m, i) = xm,i. Now fix i \u2208 [l] and j \u2208 {\u22121,+1}, and let f i,j \u2208 F l. We must show that f i,j = h \u25e6 \u03b9 for some h \u2208 W . We will show that there exists an affine map \u039b : Rd \u2192 R for which f i,j = sign \u25e6\u039b \u25e6 \u03b9. This suffices, sinceW is exactly the set of all functions of the form sign \u25e6\u039b where \u039b is an affine map. Define M = {x \u2208 Rd : \u3008x, ei\u3009 = 1}, and let A : M \u2192 R be the affine map defined by\n\u2200m \u2208 [d], A(xm,i) = f(m, i).\nLet P : Rd \u2192M be the orthogonal projection of Rd on M . For \u03b1 \u2208 R, define an affine map \u039b\u03b1 : Rd \u2192 R by\n\u039b\u03b1(x) = A(P (x)) + \u03b1 \u00b7 \u3008x\u2212 ei, ei\u3009.\nNote that, \u2200m \u2208 [d], \u039b\u03b1(xm,i) = f(m, i). Moreover, for every i\u2032 6= i and m \u2208 [d] we have \u3008xm,i\u2032 \u2212 ei, ei\u3009 < 0. Thus, by choosing |\u03b1| sufficiently large and choosing sign(\u03b1) depending on j, we can make sure that f i,j = sign \u25e6 \u039b\u03b1 \u25e6 \u03b9.\nThe proof thatW is richer than Gl is similar and simpler. Let e1, . . . , ed \u2208 Rd\u22121 be affinely independent. Define\n\u03b9(m, i) = (em, i) \u2208 Rd\u22121 \u00d7 R \u223c= Rd,\nGiven gi,j \u2208 Gd,l, let A : Rd\u22121 \u00d7 {i} \u2192 R be the affine map defined by A(em, i) = gi,j(m, i) and let P : Rd \u2192 Rd\u22121 \u00d7 {i} be the orthogonal projection. Define \u039b : Rd \u2192 R by\n\u039b(x, y) = A(P (x, y)) + j \u00b7 10 \u00b7 (y \u2212 i).\nIt is easy to check that sign \u25e6 \u039b \u25e6 \u03b9 = gi,j .\nNote A.2. From Lemma A.1 it follows that VC(F l),VC(Gl) \u2264 d+ 1. On the other hand, both F l and Gl shatter ([d]\u00d7 {1}) \u222a {(1, 2)}. Thus, VC(F l) = VC(Gl) = d+ 1"}, {"heading": "A.4 Trees", "text": "Proof of Theorem 3.2. We first prove the upper bound. Let A \u2286 X be a G-shattered set with |A| = dG(Htrees). By Sauer\u2019s Lemma, and since the number of trees is bounded by kk, we have\n2|A| \u2264 kk \u00b7 |H|A|k \u2264 kk \u00b7 |A|dk,\nthus dG(Htrees) = |A| = O(dk log(dk)). To prove the lower bound, by Lemma A.1, it is enough to show that dN (GlT ) \u2265 d \u00b7 (k \u2212 1) for some l. We will take l = |N(T )| = k \u2212 1. Linearly order N(T ) such that for every node v, the nodes in the left sub-tree emanating from v are smaller than the nodes in the corresponding right sub-tree. We will identify\n[l] with N(T ) by an order-preserving map, thus Gl \u2282 {\u22121, 1}[d]\u00d7N(T ). We also identify the labels with the leaves.\nDefine g1 : [d]\u00d7N(T )\u2192 leaf(T ) by setting g1(i, v) to be the leaf obtained by starting from the node v, going right once and then going left until reaching a leaf. Similarly, define g2 : [d]\u00d7N(T )\u2192 leaf(T ) by setting g2(i, v) to be the leaf obtained by starting from the node v, going left once and then going right until reaching a leaf.\nWe shall show that g1, g2 witness the N -shattering of [d]\u00d7N(T ) by GlT . Given S \u2282 [d]\u00d7N(T ) define C : N(T )\u2192 Gl by\nC(v)(i, u) =  \u22121 u < v 1 u > v\n1 u = v, (i, u) \u2208 S \u22121 u = v, (i, u) /\u2208 S.\nIt is not hard to check that \u2200(i, u) \u2208 S, hC(i, u) = g1(i, u), and \u2200(i, u) /\u2208 S, hC(i, u) = g2(i, u).\nNote A.3. Define G\u0303l = {gi,1 : g \u2208 {\u22121, 1}[d], i \u2208 [l]}. The proof shows that dN (G\u0303lT ) \u2265 d \u00b7 (k \u2212 1). Since VC(G\u0303l) = d, we obtain a simpler proof of Theorem 23 from Daniely et al. [2011], which states that for every tree T there exists a classH of VC dimension d for which dN (HT ) \u2265 d(k \u2212 1)."}, {"heading": "A.5 ECOC, One vs. All and All Pairs", "text": "To prove the results for ECOC and its special cases, we first prove a more general theorem, based on the notion of a sensitive vector for a given code. Fix a code M \u2208 Rk\u00d7l(R). We say that a binary vector u \u2208 {\u00b11}l is q-sensitive for M if there are q indices j \u2208 [l] for which M\u0303(u) 6= M\u0303(u \u2295 ej). Here, u\u2295 ej := (u[1], . . . ,\u2212u[j], . . . , u[l]).\nTheorem A.4. If there exists a q-sensitive vector for a code M \u2208 Rk\u00d7l(R) then dN (WM ) \u2265 d \u00b7 q.\nProof. By Lemma A.1, it suffices to show that dN (F lM ) \u2265 d \u00b7 q. Let u \u2208 {\u00b11}l be a q-sensitive vector. Assume w.l.o.g. that the sensitive coordinates are 1, . . . , q. We shall show that [d] \u00d7 [q] is N -shattered by F lM . Define g1, g2 : [d]\u00d7 [q]\u2192 [k] by\ng1(x, y) = M\u0303(u), g2(x, y) = M\u0303(u\u2295 ey)\nLet T \u2282 [d] \u00d7 [q]. Define h1, . . . , hl \u2208 F l as follows. For every j > q, define hj \u2261 u[j]. For j \u2264 q define\nhj(x, y) =  u[j] y 6= j u[j] y = j, (x, y) \u2208 T \u2212u[j] y = j, (x, y) \u2208 [d]\u00d7 [q] \\ T.\nFor h = (h1, . . . , hl), it is not hard to check that\n\u2200(x, y) \u2208 T, M\u0303(h1(x, y), . . . , hl(x, y)) = g1(x, y), and \u2200(x, y) \u2208 [d]\u00d7 [q] \\ T, M\u0303(h1(x, y), . . . , hl(x, y)) = g2(x, y).\nThe following lemma shows that a code with a large distance is also highly sensitive. In fact, we prove a stronger claim: the sensitivity is actually at least as large as the distance between any row and the row closest to it in Hamming distance. Formally, we consider \u2206(M) = maxi minj 6=i \u2206h(M [i],M [j]) \u2265 \u03b4(M).\nLemma A.5. For any codeM \u2208 Rk\u00d7l(\u00b11), there is a q-sensitive vector forM , where q \u2265 12\u2206(M) \u2265 1 2\u03b4(M).\nProof. Let i1 the row in M such that its hamming distance to the row closest to it is \u2206(M). Denote by i2 the index of the closest row (if there is more than one such row, choose one of them arbitrarily). We have \u2206h(M [i1],M [i2]) = \u2206(M). In addition, \u2200i 6= i1, i2,\u2206h(M [i1],M [i]) \u2265 \u2206(M). Assume w.l.o.g. that the indices in which rows i1 and i2 differ are 1, . . . ,\u2206(M). Consider first the case that i1 < i2. Define u \u2208 {\u00b11}[l] by\nu[j] = { M(i1,j) j \u2264 d\u22062 e M(i2,j) otherwise.\nIs is not hard to check that for every 1 \u2264 j \u2264 d\u22062 e, i1 = M\u0303(u) and M\u0303(u\u2295ej) = i2, thus u is d \u2206 2 e-sensitive. If i1 > i2, the proof is similar except that u is defined as\nu[j] = { M(i2,j) j \u2264 d\u22062 e M(i1,j) otherwise.\nProof of Theorem 3.3. The upper bound follows from Lemma 4.1. The lower bound follows form Theorem A.4 and Lemma A.5.\nProof of Theorem 3.4. The upper bounds follow from Theorem 3.3. To show that dN (WOvA) \u2265 (k \u2212 1)d, we note that the all-negative vector u = (\u22121, . . . ,\u22121) of length k is (k \u2212 1)-sensitive for the code MOvA, and apply Theorem A.4.\nTo show that dN (WAP) \u2265 d ( k\u22121\n2\n) , assume for simplicity that k is odd (a similar analysis can be given\nwhen k is even). Define u \u2208 {\u00b11}( k 2) by\n\u2200i < j, u[i, j] = { 1 j \u2212 i \u2264 k\u221212 \u22121 otherwise.\nFor every n \u2208 [k], we have \u2211\n1\u2264i<j\u2264k u[i, j] \u00b7 MAPn,(i,j) = 0, as the summation counts the number of pairs (i, j) such that n \u2208 {i, j} and MAPn,(i,j) agrees with u[i, j]. Thus, M\u0303 AP(u) = 1, by our tiebreaking assumptions. Moreover, it follows that for every 1 < i < j \u2264 k, we have M\u0303AP(u\u2295e(i,j)) \u2208 {i, j}, since flipping entry [i, j] of u increases (MAPu)j or (MAPu)i by 1 and does not increase the rest of the coordinates of the vector MAPu. This shows that u is ( k\u22121\n2\n) -sensitive."}, {"heading": "A.6 Approximation", "text": "Proof of Theorem 3.5. We first show that for any tree for k classes T , L essentially containsWT . It follows that L essentially containsWtrees as well. Let D a distribution over Rd, let C : N(T )\u2192W be a mapping associating nodes in T to binary classifiers in W , and let > 0. We will show that there exists a matrix W \u2208 Rk\u00d7(d+1) such that Prx\u223cD[hW (x) 6= hC(x)] < .\nFor every v \u2208 N(T ), denote by w(v) \u2208 Rd+1 the linear separator such that C(v) = hw(v). For every w \u2208 Rd+1 define w\u0303 = w + (0, . . . , 0, \u03b3). Recall that for x \u2208 Rd, x\u0304 \u2208 Rd+1 is simply the concatenation (x, 1). Choose r > 0 large enough so that Prx\u223cD[||x\u0304|| > r] < /2 and \u2200v \u2208 N(T ), ||w\u0303(v)|| < r. Choose \u03b3 > 0 small enough so that\nPr x\u223cD [\u2203v \u2208 N(T ), \u3008w\u0303(v), x\u0304\u3009 \u2208 (\u2212\u03b3, \u03b3)] = Pr x\u223cD [\u2203v \u2208 N(T ), \u3008w(v), x\u0304\u3009 \u2208 (\u22122\u03b3, 0)] < /2.\nLet a = 2r2/\u03b3+ 1. For i \u2208 [k], let vi,1, . . . , vi,mi be the path from the root to the leaf associated with label i. For each 1 \u2264 j < mi define bi,j = 1 if vi,j+1 is the right son of vi,j , and bi,j = \u22121 otherwise. Now, define W \u2208 Rk\u00d7(d+1) to be the matrix whose i\u2019th row is wi = \u2211mi\u22121 j=1 a \u2212j \u00b7 bi,jw\u0303(vi,j).\nTo prove that Prx\u223cD[hW (x) 6= hC(x)] < , it suffices to show that hW (x) = hC(x) for every x \u2208 Rd satisfying ||x\u0304|| < r and \u2200v \u2208 N(T ), \u3008w\u0303(v), x\u0304\u3009 /\u2208 (\u2212\u03b3, \u03b3), since the probability mass of the rest of the vectors is less than . Let x \u2208 Rd be a vector that satisfies these assumptions. Denote i1 = hC(x). It suffices to show that for all i2 \u2208 [k] \\ {i1}, \u3008wi1 , x\u0304\u3009 > \u3008wi2 , x\u0304\u3009, since this would imply that hW (x) = i1 as well.\nIndeed, fix i2 6= i1, and let j0 be the length of the joint prefix of the two root-to-leaf paths that match the labels i1 and i2. In other words, \u2200j \u2264 j0, vi1,j = vi2,j and vi1,j0+1 6= vi2,j0+1. Note that\n\u3008x\u0304, (bi1,j0 \u2212 bi2,j0)w\u0303(vi1,j0)\u3009 = \u3008x\u0304, 2bi1,j0w\u0303(vi1,j0)\u3009 = 2|\u3008x\u0304, w\u0303(vi1,j0)\u3009| \u2265 2\u03b3.\nThe last equality holds because bi1,j0 and \u3008x\u0304, w(vi1,j0)\u3009 have the same sign by definition of bi,j . We have\n\u3008wi1 , x\u0304\u3009 \u2212 \u3008wi2 , x\u0304\u3009 = \u3008x\u0304, mi1\u22121\u2211 j=1 a\u2212jbi1,jw\u0303(vi1,j)\u2212 mi2\u22121\u2211 j=1 a\u2212jbi2,jw\u0303(vi2,j)\u3009\n= \u3008x\u0304, a\u2212j0(bi1,j0 \u2212 bi2,j0)w\u0303(vi1,j0)\u3009+ \u3008x\u0304, mi1\u22121\u2211 j=j0+1 a\u2212jbi1,jw\u0303(vi1,j)\u2212 mi2\u22121\u2211 j=j0+1 a\u2212jbi2,jw\u0303(vi2,j)\u3009\n\u2265 \u3008x\u0304, a\u2212j0(bi1,j0 \u2212 bi2,j0)w\u0303(vi1,j0)\u3009 \u2212 \u221e\u2211\nj=j0+1\na\u2212j2r2\n\u2265 2a\u2212j0 ( \u03b3 \u2212 r 2\na\u2212 1\n) > 0.\nSince this holds for all i2 6= i1, it follows that hW (x) = i1. Thus, we have proved that L essentially contains Wtrees.\nNext, we show that L strictly contains Wtrees, by showing a distribution over labeled examples such that the approximation error using L is strictly smaller than the approximation error usingWtrees. Assume w.l.o.g. that d = 2 and k = 3: even if they are larger we can always restrict the support of the distribution to a subspace of dimension 2 and to only three of the labels. Consider the distribution D over R2\u00d7 [3] such that its marginal over R2 is uniform in the unit circle, and Pr(X,Y )\u223cD[Y = i | X = x] = I[x \u2208 Di], where D1, D2, D3 be subsets sectors of equal angle of the unit circle (see Figure 1):\nClearly, by taking the rows ofW to point to the middle of each sector (dashed arrows in the illustration), we get Err\u2217D(L) = 0. In contrast, no linear separator can split the three labels into two gropus without error, thus Err\u2217D(Wtrees) > 0.\nFinally, to see that L essentially contains WOvA, we note that WOvA = WT where T is a tree such that each of its internal nodes has a leaf corresponding to one of the labels as its left son. Thus WOvA is essentially contained inWtrees.\nProof of Theorem 3.7. It is easily seen thatWAP contains L: Let W \u2208 Rd+1\u00d7k, and denote its i\u2019th row by W [i]. For each column (i, j) of MAP, define the binary classifier hi,j \u2208 W such that \u2200x \u2208 Rd, hi,j(x\u0304) = sign(\u3008W [j]\u2212W [i], x\u0304\u3009). Then for all x, hW (x) = M\u0303AP(h1,1(x), . . . , hk\u22121,k(x)).\nTo show that the inclusion is strict, as in the proof of Theorem 3.5, we can and will assume that d = 2. Choose k\u2217 to be the minimal number such that for every k \u2265 k\u2217, dN (WAP ) > dN (L): This number exists by Theorems 3.4 and 3.1 (note that though we chose k\u2217 w.r.t. d = 2, the same k\u2217 is valid for every d). For any k \u2265 k\u2217, it follows that there is a set S \u2286 R2 that is N -shattered byWAP but not by L. Thus, there is a hypothesis h \u2208 WAP such that for every g \u2208 L, g|S 6= h|S . Define the distribution D to be uniform over {(x, h(x)) : x \u2208 S}. Then clearly Err\u2217D(L) > Err \u2217 D(WAP ) = 0.\nNext, we prove Theorem 3.6, which we restate more formally as follows. Note that the result on OvA is implied since there exists a tree that implements OvA.\nTheorem A.6. (Restatement of Theorem 3.6) If there exists an embedding \u03b9 : Rd \u2192 Rd\u2032 and a tree T such thatWd\u2032T \u25e6 \u03b9 essentially contains L, then necessarily d\u2032 \u2265 \u2126\u0303(dk).\nProof. Assume that i \u2208 [k] is the class corresponding to the leaf with the least depth, l. Note that l \u2264 log2(k). Let \u03c6 : [k] \u2192 {\u00b11} be the function that is 1 on {i} and \u22121 otherwise. It is not hard to see that \u03c6 \u25e6 L is the hypothesis class of convex polyhedra in Rd having k \u2212 1 faces. Thus,\nVC(\u03c6 \u25e6 L) \u2265 (k \u2212 1)d, (3)\n[see e.g. Takacs, 2009]. On the other hand, \u03c6 \u25e6 Wd\u2032T , is the class of convex polyhedra in Rd \u2032\nhaving l \u2264 log2(k) faces. Thus, by Lemma 4.1\nVC(\u03c6 \u25e6Wd \u2032 T \u25e6 \u03b9) \u2264 VC(\u03c6 \u25e6Wd \u2032 T ) \u2264 O(ld\u2032 log(ld\u2032)) \u2264 O(log(k)d\u2032 log(log(k)d\u2032)) (4)\nBy the assumption thatWd\u2032T \u25e6 \u03b9 essentially contains L, VC(\u03c6 \u25e6 L) \u2264 VC(\u03c6 \u25e6 Wd \u2032\nT \u25e6 \u03b9). Combining with equations (3) and (4) it follows that d(k \u2212 1) = O(log(k)d\u2032 log(log(k)d\u2032)). Thus, d\u2032 = \u2126\u0303 (dk).\nTo prove Lemma 3.8, we first state the classic VC-dimension theorem, which will be useful to us.\nTheorem A.7 (Vapnik [1998]). There exists a constant C > 0 such that for every hypothesis class H \u2286 {\u00b11}X of VC dimension d, a distribution D over X , , \u03b4 > 0 and m \u2265 C d+ln( 1 \u03b4 ) 2 we have\nPr S\u223cDm\n[ Err\u2217D(H) \u2265 inf\nh\u2208H ErrS(h)\u2212\n] \u2265 1\u2212 \u03b4.\nWe also use the following lemma, which proves a variant of Hoeffding\u2019s inequality.\nLemma A.8. Let \u03b21, . . . , \u03b2k \u2265 0 and let \u03b31, . . . , \u03b3k \u2208 R, such that \u2200i, |\u03b3i| \u2264 \u03b2i. Fix an integer j \u2208 {1, . . . , bk2 c} and let \u00b5 = j/k. Let (X1, . . . , Xk) \u2208 {\u00b11}\nk be a random vector sampled uniformly from the set {(x1, . . . , xk) : \u2211k i=1\nx1+1 2 = \u00b5k}. Define Yi = \u03b2i +Xi\u03b3i and denote \u03b1i = \u03b2i + |\u03b3i|. Assume that\u2211k\ni=1 \u03b1i = 1. Then\nPr [ k\u2211 i=1 Yi \u2264 \u00b5\u2212 ] \u2264 2 exp ( \u2212\n2 2 \u2211k i=1 \u03b1 2 i\n) .\nProof. First, since \u00b5 < 12 , it suffices to prove the claim for the case \u2200i, \u03b3i \u2265 0 since this is the \u201charder\u201d case. Let Z1, . . . , Zk \u2208 {\u00b11} be independent random variables such that Pr[Zi = 1] = \u00b5 \u2212 2 . Denote Wi = \u03b2i + Zi\u03b3i. Further denote W\u0304 = \u2211k i=1Wi and Z\u0304 = \u2211k i=1 Zi+1 2 .\nNote that for every j0 \u2264 j = \u00b5k, given that Z\u0304 = j0, W\u0304 can be described as follows: We start with the value \u2211k i=1 \u03b2i\u2212\u03b3i and then choose j0 indices uniformly from [k]. For each chosen index i, the value of W\u0304\nis increased by 2\u03b3i. \u2211k i=1 Yi can be described in the same way, except that that j \u2265 j0 indices are chosen.\nThus, Pr [\u2211k i=1 Yi \u2264 \u00b5\u2212 ] \u2264 Pr [ W\u0304 \u2264 \u00b5\u2212 | Z\u0304 = j0 ] . Thus, we have\nPr [ k\u2211 i=1 Yi \u2264 \u00b5\u2212 ] \u2264 Pr [ W\u0304 \u2264 \u00b5\u2212 | Z\u0304 \u2264 \u00b5k ] \u2264 Pr [ W\u0304 \u2264 \u00b5\u2212 ] /Pr [ Z\u0304 \u2264 \u00b5k\n] \u2264 2 Pr [ W\u0304 \u2264 \u00b5\u2212\n] \u2264 2 exp ( \u2212 2\n2 \u2211k i=1 \u03b1 2 i\n) .\nThe last inequality follows from Hoeffding\u2019s inequality and noting that\nE[Wi] = \u03b2i + (2(\u00b5\u2212 2 )\u2212 1)\u03b3i = (\u00b5\u2212 2 )(\u03b2i + \u03b3i) + (1\u2212 \u00b5+ 2 )(\u03b2i \u2212 \u03b3i) \u2265 (\u00b5\u2212 2 )\u03b1i.\nSo that \u2211k i=1E[Wi] \u2265 (\u00b5\u2212 2 ) \u2211k i=1 \u03b1i = \u00b5\u2212 2 .\nProof of Lemma 3.8. The idea of this proof is as follows: Using a uniform convergence argument based on the VC dimension of the binary hypothesis class, we show that there exists a labeled sample S such that |S| \u2248 d+k\u03bd2 , and for all possible mappings \u03c6, the approximation error of the hypothesis class on the sample is close to the approximation error on the distribution D\u03c6. This allows us to restrict our attention to a finite set of hypotheses, based on their restriction to the sample. For these hypotheses, we show that with high probability over the choice of \u03c6, the approximation error on the sample is high. Using a union bound on the possible hypotheses, we conclude that the approximation error on the distribution will be high, with high probability over the choice of \u03c6.\nFor i \u2208 [k], denote pi = Prx\u223cD[f(x) = i]. Let S = {(x1, y1), . . . , (xm, ym)} \u2286 X \u00d7 [k] be an i.i.d. sample drawn according to D where m = dC d+(k+2) ln(2)(\u03bd/2)2 e, for the constant from C from Theorem A.7. Given S, denote S\u03c6{(x1, \u03c6(y1)), . . . , (xm, \u03c6(ym))} \u2286 X \u00d7 {\u00b11}. For i \u2208 [k], let p\u0302i = |{j:yj=i}|m .\nFor any fixed \u03c6 : [k] \u2192 {\u00b11}, with probability > 1 \u2212 2\u2212(k+2) over the choice of S we have, by Theorem A.7, that Err\u2217D\u03c6(H) > infh\u2208H ErrS\u03c6(h)\u2212 \u03bd. Since |{\u00b11} [k]| = 2k, w.p. > 1\u2212 14 ,\n\u2200\u03c6 \u2208 {\u00b11}[k], Err\u2217D\u03c6(H) > infh\u2208HErrS\u03c6(h)\u2212 \u03bd 2 . (5)\nMoreover, we have\nE[ k\u2211 i=1 p\u03022i ] = 1 m2 k\u2211 i=1 (( m 2 ) p2i +mpi ) \u2264 k \u00b7 ( m(m\u2212 1) 2m2 100 k2 + 10 mk ) \u2264 60 k .\nThus, by Markov\u2019s inequality, w.p. \u2265 12 we have\nk\u2211 i=1 p\u03022i < 120 k . (6)\nThus, with probability at least 1\u2212 14 \u2212 1 2 > 0, both (6) and (5) holds. In particular, there exists a sample S for which both (6) and (5) hold. Let us fix such an S = {(x1, y1), . . . , (xm, ym)}. Assume now that \u03c6 \u2208 {\u00b11}[k] is sampled according to the first condition. Denote\nYi = |{j : h(xj) 6= \u03c6(yj) and yj = i}|/m.\nFor a fixed h \u2208 H we have\nPr \u03c6\n[ ErrS\u03c6(h) < \u00b5\u2212 \u03bd\n2\n] = Pr\n\u03c6 [ k\u2211 i=1 Yi < \u00b5\u2212 \u03bd 2 ]\nWe note that Yi are independent random variables withE[Yi] \u2265 \u00b5p\u0302i and 0 \u2264 Yi \u2264 p\u0302i. Thus, by Hoeffding\u2019s inequality,\nPr \u03c6\n[ ErrS\u03c6(h) < \u00b5\u2212 \u03bd\n2\n] \u2264 exp ( \u2212 \u03bd 2\n2 \u2211k i=1 p\u0302 2 i\n) \u2264 exp ( \u2212\u03bd 2k\n240\n) .\nBy Sauer\u2019s lemma, |H|{x1,...,xm}| \u2264 ( em d )d . Thus, with probability \u2265 1 \u2212 ( em d )d exp ( \u2212\u03bd 2k 240 ) over the choice of \u03c6, infh\u2208H ErrS\u03c6(h) \u2265 \u00b5\u2212 \u03bd2 and by (5) also\nErr\u2217D\u03c6(H) \u2265 1\n2 \u2212 \u03bd. (7)\nFinally, since m = O ( k+d \u03bd2 ) , if k = \u2126 ( d ln(1/\u03bd)+ln(1/\u03b4) \u03bd2 ) then Eq. (7) holds w.p > 1 \u2212 \u03b4, concluding the proof for the case when the first condition holds. If the second condition holds, the proof is very similar, with the sole difference that Lemma A.8 is used instead of Hoeffding\u2019s inequality.\nProof of Corollary 3.9. The Corollary follows from Lemma 3.8, by noting that Err\u2217D(HT ) \u2265 Err \u2217 D\u03c6(H), where \u03c6 : [k] \u2192 {\u00b11} is defined as \u03c6(i) = 1 if and only if \u03bb\u22121(i) is in the right subtree emanating from the root of T .\nProof of Corollary 3.10. Let \u03c6 : [k] \u2192 {\u00b11} be the function that is \u22121 on [ bk2 c ]\nand 1 otherwise. By Lemma 4.1, applied to L(H) = \u03c6 \u25e6 H(M,Id), VC(\u03c6 \u25e6 H(M,Id)) = O(dl log(dl)), so that, by Lemma 3.8 (applied to a random choice of \u03bb instead of \u03c6), Err\u2217D\u03c6\u25e6\u03bb(\u03c6\u25e6H(M,Id)) \u2265 1 2\u2212\u03bd with probability> 1\u2212\u03b4 over the choice of \u03bb. The proof follows as we note that for every \u03bb, Err\u2217D(H(M,\u03bb\u22121)) = Err \u2217 D\u03bb(H(M,Id)) \u2265 Err\u2217D\u03c6\u25e6\u03bb(\u03c6 \u25e6 H(M,Id))."}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E.L. Allwein", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2000}, {"title": "Characterizations of learnability for classes", "author": ["S. Ben-David", "N. Cesa-Bianchi", "D. Haussler", "P. Long"], "venue": "n}-valued functions. Journal of Computer and System Sciences,", "citeRegEx": "Ben.David et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1995}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2011}, {"title": "Multiclass classification with filter trees", "author": ["A. Beygelzimer", "J. Langford", "P. Ravikumar"], "venue": null, "citeRegEx": "Beygelzimer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2007}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer.,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer.", "year": 2001}, {"title": "Multiclass learnability and the erm principle", "author": ["A. Daniely", "S. Sabato", "S. Ben-David", "S. Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "Daniely et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2011}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Classification by pairwise coupling", "author": ["Trevor Hastie", "Robert Tibshirani"], "venue": "The Annals of Statistics,", "citeRegEx": "Hastie and Tibshirani.,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Tibshirani.", "year": 1998}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rifkin and Klautau.,? \\Q2004\\E", "shortCiteRegEx": "Rifkin and Klautau.", "year": 2004}, {"title": "Learning internal representations by error propagation", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "Parallel Distributed Processing \u2013 Explorations in the Microstructure of Cognition,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Convex polyhedron learning and its applications", "author": ["G. Takacs"], "venue": "PhD thesis, Budapest University of Technology and Economics,", "citeRegEx": "Takacs.,? \\Q2009\\E", "shortCiteRegEx": "Takacs.", "year": 2009}, {"title": "Support vector machines for multi-class pattern recognition", "author": ["J. Weston", "C. Watkins"], "venue": "In Proceedings of the Seventh European Symposium on Artificial Neural Networks,", "citeRegEx": "Weston and Watkins.,? \\Q1999\\E", "shortCiteRegEx": "Weston and Watkins.", "year": 1999}, {"title": "\u03b9). Combining with equations", "author": [], "venue": null, "citeRegEx": "\u25e6,? \\Q1998\\E", "shortCiteRegEx": "\u25e6", "year": 1998}], "referenceMentions": [{"referenceID": 7, "context": "A different reduction is the All-Pairs (AP) approach in which all pairs of classes are compared to each other [Hastie and Tibshirani, 1998].", "startOffset": 110, "endOffset": 139}, {"referenceID": 5, "context": "Rumelhart et al. [1986]).", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "These two approaches have been unified under the framework of Error Correction Output Codes (ECOC) [Dietterich and Bakiri, 1995, Allwein et al., 2000]. A tree-based classifier (TC) is another reduction in which the prediction is obtained by traversing a binary tree, where at each node of the tree a binary classifier is used to decide on the rest of the path (see for example Beygelzimer et al. [2007]).", "startOffset": 129, "endOffset": 403}, {"referenceID": 10, "context": "Vapnik [1998], Weston and Watkins [1999], Crammer and Singer [2001]).", "startOffset": 15, "endOffset": 41}, {"referenceID": 4, "context": "Vapnik [1998], Weston and Watkins [1999], Crammer and Singer [2001]).", "startOffset": 42, "endOffset": 68}, {"referenceID": 4, "context": "Vapnik [1998], Weston and Watkins [1999], Crammer and Singer [2001]). In this paper we analyze the Multiclass SVM (MSVM) formulation of Crammer and Singer [2001], in which each hypothesis is of the form hW (x) = argmaxi\u2208[k](Wx\u0304)i, where W is a k\u00d7 (d+ 1) matrix and (Wx\u0304)i is the i\u2019th element of the vector Wx\u0304 \u2208 R.", "startOffset": 42, "endOffset": 162}, {"referenceID": 3, "context": "If no prior knowledge on how to break the symmetry is known, it is suggested in Beygelzimer et al. [2007] to break symmetry by choosing a random permutation of the labels.", "startOffset": 80, "endOffset": 106}, {"referenceID": 6, "context": "The ECOC paradigm described in [Dietterich and Bakiri, 1995] proposes to choose a code with a large distance.", "startOffset": 31, "endOffset": 60}, {"referenceID": 5, "context": "Daniely et al. [2011] For every hypothesis classH, and for every ERM rule,", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "2 improve results from Daniely et al. [2011] where it was shown that bd2cb k 2 c \u2264 dN (L) \u2264 O(dk log(dk)), and for every tree dG(HT ) \u2264 O(dk log(dk)).", "startOffset": 23, "endOffset": 45}, {"referenceID": 0, "context": "In particular, if the length of the code is O(log(k)), as suggested in Allwein et al. [2000], and the number of classes is \u03a9\u0303(d), then the code is expected to perform poorly.", "startOffset": 71, "endOffset": 93}], "year": 2012, "abstractText": "We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension d, and in particular from the class of halfspaces over R. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the approximation error of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.", "creator": "LaTeX with hyperref package"}}}