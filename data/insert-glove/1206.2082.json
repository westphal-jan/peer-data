{"id": "1206.2082", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2012", "title": "Dimension Independent Similarity Computation", "abstract": "chihana We rackemann present r\u00f8mer a suite of wyggeston algorithms 27-30 for Dimension nev. Independent four-bladed Similarity .448 Computation (DISCO) reller to compute rosneath all africain pairwise similarities between very vicker high dimensional sparse vectors. u.n.-mandated All vijver of our results are provably independent ramayya of xihe dimension, meaning letsion apart from the sageman initial pasiones cost 1.313 of trivially reading in reviser the 68.64 data, fragata all subsequent ultra-modern operations posey are stewardson independent of 1/24 the dimension, preate thus kotla the glenbogle dimension can be p&w very large. enfermo We 18.65 study televoting Cosine, Dice, Overlap, 61.38 Conditional, and 1968-1969 the scruggs Jaccard twort similarity measures. For Jaccard student/faculty similiarity dovercourt we chenin include matzoh an substates improved missan version of MinHash. firmly Our results coppersmith are seifu geared desjoyeaux toward the hamood MapReduce 2,114 framework. 250,000-member We empirically ccf validate rzepczynski our theorems mountbatten at large scale using data from videodisk the juxtapose social ammonius networking 87.32 site dragonflies Twitter.", "histories": [["v1", "Mon, 11 Jun 2012 02:19:27 GMT  (32kb)", "http://arxiv.org/abs/1206.2082v1", null], ["v2", "Thu, 29 Nov 2012 04:40:52 GMT  (35kb)", "http://arxiv.org/abs/1206.2082v2", null], ["v3", "Tue, 2 Apr 2013 03:54:28 GMT  (35kb)", "http://arxiv.org/abs/1206.2082v3", null], ["v4", "Thu, 23 May 2013 07:56:18 GMT  (34kb)", "http://arxiv.org/abs/1206.2082v4", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.DC", "authors": ["reza bosagh zadeh", "ashish goel"], "accepted": false, "id": "1206.2082"}, "pdf": {"name": "1206.2082.pdf", "metadata": {"source": "CRF", "title": "Dimension Independent Similarity Computation", "authors": ["Reza Bosagh Zadeh", "Ashish Goel"], "emails": ["rezab@stanford.edu", "ashishg@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 6.\n20 82\nv1 [\ncs .D\nS] 1\nWe present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high dimensional sparse vectors. All of our results are provably independent of dimension, meaning apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension, thus the dimension can be very large. We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems at large scale using data from the social networking site Twitter.\nKeywords: Cosine, Jaccard, Overlap, Dice, Similarity, MapReduce, dimension independent"}, {"heading": "1. Introduction", "text": "Computing similarity between all pairs of vectors in large scale datasets is a challenge. Traditional approaches of sampling the dataset are limited and linearly dependent on the dimension of the data. We present an approach whose runtime is independent of the data dimension and geared towards modern distributed systems, in particular the MapReduce framework (Dean and Ghemawat, 2008). We focus on 5 similarity similarity measures: Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we present an improved version of the well known MinHash scheme (Broder, 1997). Our framework operates under the following assumptions, each of which we justify.\nFirst, we focus on the case where each dimension is sparse and therefore the natural way to store the data is segmented into dimensions. In our application each dimension is represented by a tweet, thus this assumption was natural. Second, our sampling scheme requires a \u201cbackground model\u201d, meaning the magnitude of each vector is assumed to be known and loaded into memory. In our application this was not a hurdle, since the magnitudes of vectors in our corpus needed to be computed for other tasks. To further address the issue, in the streaming computation model, we can remove the dependence by paying an extra logarithmic factor in memory used. Third, we prove results on highly similar pairs,\nc\u00a92012 Reza Bosagh Zadeh and Ashish Goel.\nsince common applications require thresholding the similarity score with a high threshold value.\nA ubiquitous problem is finding all pairs of objects that are in some sense \u2018similar\u2019 and in particular more similar than a threshold. For such applications of similarity, DISCO is particularly helpful since higher similarity pairs are estimated with provably better accuracy. There are many examples, including\n\u2022 Advertiser keyword suggestions: When targeting advertisements via keywords, it is useful to expand the manually input set of keywords by other similar keywords, requiring finding all keywords more similar than a high threshold (Regelson and Fain, 2006). The vector representing a keyword will often be an indicator vector indicating in which documents the keywords appears.\n\u2022 Document duplicate detection: When deciding if two documents should be declared duplicates, it is very rare that we have the privilege of using simple equality on the text of the document. It is far more common to use similarity measures with a threshold.\n\u2022 Collaborative filtering: Collaborative filtering applications require knowing which users have similar interests. For this reason, given a person or user, it is required to find all other objects more similar than a particular threshold, for which our algorithms are well suited.\n\u2022 Web Search: Rewriting queries given to a search engine is a common trick used to expand the coverage of the search results (Abhishek and Hosanagar, 2007). Adding clusters of similar queries to a given query is a natural query expansion strategy.\nThese applications have been around for many years, but the scale at which we need to solve them keeps steadily increasing. This is particularly true with the rise of the web, and social networks such as Twitter, Facebook, and Google Plus among others. Google currently holds an index of more than 1 trillion webpages, on which duplicate detection must be a daunting task. For our experiments we use the Twitter dataset, where the number of tweets in a single day surpasses 200 million. Many of the applications, including ours involve domains where the dimensionality of the data far exceeds the number of points. In our case the dimensionality is large (more than 200 million), and we can prove that apart from initially reading the data (which cannot be avoided), subsequent MapReduce operations will have input size independent of the number of dimensions. Therefore, as long as the data can be read in, we can ignore the number of dimensions.\nA common technique to deal with large datasets is to simply sample. Indeed, our approach is a sampling scheme too, however we sample in a nontrivial way so that points that have nonzero entries in many dimensions will be sampled with less probability than points which are only present in a few dimensions. Using this idea, we can remove the dependence on dimensionality while being able to mathematically prove \u2013 and empirically verify \u2013 accuracy.\nAlthough we use the MapReduce (Dean and Ghemawat, 2008) framework and discuss shuffle size, the sampling strategy we use can be generalized to other frameworks. We focus on MapReduce because it is the tool of choice for large scale computations at Twitter, Google, and many other companies dealing with large scale data. However, the DISCO\nsampling strategy is potentially useful whenever one is computing a number between 0 and 1 by taking the ratio of an unknown number (the dot product in our case) by some known number (e.g. the magnitude). This is a very high-level description, and we give five very concrete examples, along with proofs, and experiments. Furthermore, DISCO improves the implementation of the well known MinHash (Broder, 1997) scheme in any MapReduce setup."}, {"heading": "2. Formal Preliminaries", "text": "Let T = {t1, . . . , tN} represent N documents, each of length no more than L words. In our context, the documents are tweets from the social networking site Twitter, or could be fixed-window contexts from a large corpus of documents, or any other dataset where dimensions have no more than L nonzero entries and the dataset is available dimensionby-dimension. We are interested in similarity scores between pairs of words in a dictionary containing D words {w1, . . . , wD}. The number of documents in which two words wi and wj co-occur is denoted #(wi, wj). The number of documents in which a single word wi occurs is denoted #(wi).\nTo each word in the dictionary, an N -dimensional indicator vector is assigned, indicating in which documents the word occurs. We operate within a MapReduce framework where each document is an input record. We denote the number of items output by the map phase as the \u2018shuffle size\u2019. Our algorithms will have shuffle sizes that provably do not depend on N , making them particularly attractive for very high dimensional tasks.\nWe focus on 5 different similarity measures, including cosine similarity which is very popular and produces high quality results across different domains (Chien and Immorlica, 2005; Chuang and Chien, 2005; Sahami and Heilman, 2006; Spertus et al., 2005). Cosine similarity is simply the vector normalized dot product: #(x,y)\u221a #(x) \u221a #(y) where #(x) = \u2211N i=1 x[i] and #(x, y) = \u2211N\ni=1 x[i]y[i]. In addition to cosine similarity, we consider many variations of similarity scores that use the dot product, outlined in table 1.\nTo compare the performance of algorithms in a MapReduce framework, we report and analyze shuffle size, which is more reliable than wall time or any other implementationspecific measure. We define shuffle size as the total output of the Map Phase, which is what will need to be \u2018shuffled\u2019 before the Reduce phase can begin. Our results and theorems hold across any MapReduce implementation such as Hadoop (Borthakur, 2007)(Gates et al., 2009) or Google\u2019s MapReduce (Dean and Ghemawat, 2008). We focus on shuffle size because after trivially reading in the data via mappers, the shuffle phase is the bottleneck, since our mappers and reducers are all trivially linear in their input size. In general, MapReduce algorithms are usually judged by two performance measures: largest reduce bucket and shuffle size. Since the largest reduce bucket is not at all a problem for us, we focus on shuffle size.\nFor many applications, and our applications in particular, input vectors are sparse, meaning that the large majority of entries are 0. A sparse vector representation for a vector x is the set of all pairs (i, x[i]) such that x[i] > 0 over all i = 1 . . . N . The size of a vector x, which we denote as #(x) is the number of such pairs. We focus on the case where\neach dimension is sparse and therefore the natural way to store the data is segmented into dimensions.\nThroughout the paper we formally prove results for pairs more similar than a threshold, called \u01eb. Our algorithms will often have a tradeoff between accuracy and shuffle size, where the tradeoff parameter is p.\nWe assume that the dictionary can fit into memory, but that the number of dimensions (documents) is so large that many machines will be needed to even hold the documents on disk. Note that documents and dimensions are the same thing, and we will use these terms interchangeably. We also assume that the magnitudes of the vectors are known and available in all mappers and reducers.\nWe are interested in several similarity measures outlined in Table 1. For each, we prove the shuffle size for the bottleneck step of the pipeline needed to compute all nonzero scores. Our goal is to compute similarity scores between all pairs of words, and prove accuracy results for pairs which have similarity above \u01eb. The naive approach is to first compute the dot product between all pairs of words in a Map-Reduce (Dean and Ghemawat, 2008) style framework. Mappers act on each document, and emit key-value pairs of the form (wi, wj) \u2192 1. These pairs are collected with the sum reducer, which gives the dot product #(wi, wj). The product is then used to trivially compute the formulas in Table 1. The difficulty of computing the similarity scores lies almost entirely in computing #(wi, wj).\nShuffle size is defined as the number of key-value pairs emitted. The shuffle size for the above naive approach is N (\nL 2\n)\n= O(NL2), which can be prohibitive for N too large. Our algorithms remove the dependence on N by dampening the number of times popular words are emitted as part of a pair (e.g. see algorithm 5). Instead of emitting every pair, only some pairs are output, and instead of computing intermediary dot products, we directly compute the similarity score."}, {"heading": "3. Related Work", "text": "Previously the all-pairs similarity search problem has been studied in (Broder et al., 1997; Broder, 1997), in the context of identifying near-duplicate web pages (Broder et al., 1997). We describe MinHash later in section 4.2. We improve the vanilla implementation of MinHash on MapReduce to arrive at better shuffle sizes without (effectively) any loss in accuracy. We prove these results in Section 4.2, and verify them experimentally.\nThere are many papers discussing the all pairs similarity computation problem. In (Elsayed et al., 2008), the MapReduce framework is targeted, but there is still a linear dependence on the dimensionality and no proven results. In (Lin, 2009), all pairs are on computed on MapReduce, but there is a focus on the life sciences domain. The all-pairs similarity search problem has also been addressed in the database community, where it is known as the similarity join problem (Arasu et al., 2006; Chaudhuri et al., 2006; Sarawagi and Kirpal, 2004). These papers are loosely assigned to one of two categories: First, signatures of the points to convert the nonexact matching problem to an exact matching problem followed by hash map and filtering false positives. Second, inverted list based solutions exploit information retrieval techniques.\nThere is large body of work on the nearest neighbors problem, which is the problem of finding the k nearest neighbors of a given query point(Charikar, 2002; Fagin et al., 2003; Gionis et al., 1999; Indyk and Motwani, 1998). Our problem, the all-pairs similarities computation is a generalization of the k nearest neighbor problem.\nIn (Pantel et al., 2009), the authors propose a highly scalable term similarity algorithm, implemented in the MapReduce framework, and deployed over a 200 billion word crawl of the Web to compute pairwise similarities between terms. Their results are still dependent upon the dimension and they only provide experimental evaluations of their algorithms.\nOther related work includes clustering of web data (Beeferman and Berger, 2000; Chien and Immorlica, 2005; Sahami and Heilman, 2006; Spertus et al., 2005). These applications of clustering typically employ relatively straightforward exact algorithms or approximation methods for computing similarity. Our work could be leveraged by these applications for improved performance or higher accuracy through reliance on proven results."}, {"heading": "4. Results", "text": "The Naive algorithm for computing similarities first computes dot products, then simply divides by whatever is necessary to obtain a similarity score. i.e. in a MapReduce implementation:\n1. Given document t, Map using NaiveMapper (Algorithm 1)\n2. Reduce using the NaiveReducer (Algorithm 2)\nAlgorithm 1 NaiveMapper(t)\nfor all pairs (w1, w2) in t do emit ((w1, w2) \u2192 1) end for\nAlgorithm 2 NaiveReducer((w1, w2), \u3008r1, . . . , rR\u3009) a =\n\u2211R i=1 ri\noutput a\u221a #(w1)#(w2)\nThe above steps will compute all dot products, which will then be scaled by appropriate factors for each of the similarity scores. Instead of using naive algorithm, we modify the mapper of the naive algorithm and replace it with Algorithm 3, and replace the reducer with Algorithm 4 to directly compute the actual similarity score, not dot products. The following sections detail how to obtain dimensionality independence for each of the similarity scores.\nAlgorithm 3 DISCOMapper(t)\nfor all pairs (w1, w2) in t do emit using custom Emit function end for\nAlgorithm 4 DISCOReducer((w1, w2), \u3008r1, . . . , rR\u3009) a =\n\u2211R i=1 ri\noutput a \u01eb p"}, {"heading": "4.1 Cosine Similarity", "text": "To remove the dependence on N , we replace the emit function with Algorithm 5.\nAlgorithm 5 CosineSampleEmit(w1, w2)\nWith probability p\n\u01eb\n1 \u221a\n#(w1) \u221a #(w2\nemit ((w1, w2) \u2192 1)\nNote that the more popular a word is, the less likely it is to be output. This is the key observation leading to shuffle size independent of the dimension. We use a slightly different reducer, which instead of calculating #(w1, w2), computes cos(w1, w2) directly without any intermediate steps. The exact estimator is given in the proof of Theorem 1.\nSince the CosineSampleEmit algorithm is only guaranteed to produce the correct similarity score in expectation, we must show that the expected value is highly likely to be obtained. This guarantee is given in Theorem 1. It is worth mentioning that ( e\u03b4\n(1+\u03b4)(1+\u03b4)\n)\nand exp(\u2212\u03b42/2) are always less than 1, thus raising them to the power of p brings them down exponentially in p.\nTheorem 1 For any two words x and y having cos(x, y) \u2265 \u01eb, let X1,X2, . . . ,X#(x,y) represent indicators for the coin flip in calls to CosineSampleEmit with x, y parameters, and\nlet X = \u2211#(x,y)\ni=1 Xi. For any 1 > \u03b4 > 0, we have\nPr\n[\n\u01eb p X > (1 + \u03b4) cos(x, y)\n] \u2264 (\ne\u03b4\n(1 + \u03b4)(1+\u03b4)\n)p\nand\nPr\n[\n\u01eb p X < (1\u2212 \u03b4) cos(x, y)\n]\n< exp(\u2212p\u03b42/2)\nProof We use \u01eb p X as the estimator for cos(x, y). Note that\n\u00b5xy = E[X] = #(x, y) p\n\u01eb\n1 \u221a\n#(x) \u221a #(y) =\np \u01eb cos(x, y) \u2265 p\nThus by the multiplicative form of the Chernoff bound,\nPr\n[\n\u01eb p X > (1 + \u03b4) cos(x, y)\n]\n= Pr\n[\n\u01eb p X > (1 + \u03b4) \u01eb p p \u01eb cos(x, y)\n]\n= Pr [X > (1 + \u03b4)\u00b5xy] <\n(\ne\u03b4\n(1 + \u03b4)(1+\u03b4)\n)\u00b5xy\n\u2264 (\ne\u03b4\n(1 + \u03b4)(1+\u03b4)\n)p\nSimilarly, by the other side of the multiplicative Chernoff bound, we have\nPr\n[\n\u01eb p X < (1\u2212 \u03b4) cos(x, y)\n]\n= Pr[X < (1\u2212 \u03b4)\u00b5xy ] < exp(\u2212\u00b5xy\u03b42/2) \u2264 exp(\u2212p\u03b42/2)\nSince there are ( D 2 ) pairs of words in the dictionary, set p = log(D2) = 2 log(D) and use union bound with theorem 1 to ensure the above bounds hold simultaneously for all pairs x, y having cos(x, y) \u2265 \u01eb.\nNow we show that the shuffle size is independent of N , which is a great improvement over the naive approach when N is large. To see the usefulness of these bounds, it is worth noting that we prove they are almost optimal (i.e. no other algorithm can do much better). In Theorem 2 we prove that any algorithm that purports to accurately calculate highly similar pairs must at least output them, and sometimes there are at least DL such pairs, and so any algorithm that is accurate on highly similar pairs must have at least DL shuffle size. We are off optimal here by only a log(D)/\u01eb factor.\nTheorem 2 The expected shuffle size for CosineSampleEmit is O(DL log(D)/\u01eb) and \u2126(DL).\nProof The expected contribution from each pair of words will constitute the shuffle size:\nD \u2211\ni=1\nD \u2211\nj=i+1\n#(wi,wj) \u2211\nk=1\nPr[CosineSampleEmit(wi, wj)]\n=\nD \u2211\ni=1\nD \u2211\nj=i+1\n#(wi, wj)Pr[CosineSampleEmit(wi, wj)]\n= D \u2211\ni=1\nD \u2211\nj=i+1\np\n\u01eb\n#(wi, wj) \u221a\n#(wi) \u221a #(wj)\n\u2264 p 2\u01eb\nD \u2211\ni=1\nD \u2211\nj=i+1\n#(wi, wj)( 1\n#(wi) +\n1\n#(wj) )\n\u2264 p \u01eb\nD \u2211\ni=1\n1\n#(wi)\nD \u2211\nj=1\n#(wi, wj)\n\u2264 p \u01eb\nD \u2211\ni=1\n1\n#(wi) L#(wi) =\np \u01eb LD = O(DL log(D)/\u01eb)\nThe first inequality holds because of the Arithmetic-Mean-Geometric-Mean inequality applied to {1/#(wi), 1/#(wj)}. The last inequality holds because wi can co-occur with at most #(wi)L other words. It is easy to see via Chernoff bounds that the above shuffle size is obtained with high probability.\nTo see the lowerbound, we construct a dataset consisting of D/L distinct documents of length L, furthermore each document is duplicated L times. To construct this dataset, consider grouping the dictionary into D/L groups, each group containing L words. A document is associated with every group, consisting of all the words in the group. This document is then repeated L times. In each group, it is trivial to check that all pairs of words of have similarity exactly 1. There are (\nL 2\n)\npairs for each group and there are D/L groups, making\nfor a total of (D/L) ( L 2 )\n= \u2126(DL) pairs with similarity 1, and thus also at least \u01eb. Since any algorithm that purports to accurately calculate highly-similar pairs must at least output them, and there are \u2126(DL) such pairs, we have the lowerbound.\nIt is important to observe what happens if the output \u2018probability\u2019 is greater than 1. We certainly Emit, but when the output probability is greater than 1, care must be taken during reducing to scale by the correct factor, since it won\u2019t be correct to divide by p/\u01eb, which is the usual case when the output probability is less than 1. Instead, we must divide by \u221a #(w1) \u221a\n#(w2) because for the pairs where the output probabilty is greater than 1, CosineSampleEmit and Emit are the same. Similar corrections have to be made for the other similarity scores (Dice, Overlap, and Conditional, but not MinHash), so we do not repeat this point. Nonetheless it is an important one which arises during implementation."}, {"heading": "4.2 Jaccard Similarity", "text": "Traditionally MinHash (Broder, 1997) is used to compute Jaccard similiarity scores between all pairs in a dictionary. We improve the MinHash scheme to run much more efficiently with a smaller shuffle size.\nLet h(t) be a hash function that maps documents to distinct numbers in [0, 1], and for any word w define g(w) (called the MinHash of w) to be the minimum value of h(t) over all t that contain w. Then g(w1) = g(w2) exactly when the minimum hash value of the union\n#(w1) + #(w2)\u2212#(w1, w2) lies in the intersection #(w1, w2). Thus\nPr[g(w1) = g(w2)] = #(w1, w2)\n#(w1) + #(w2)\u2212#(w1, w2) = Jac(w1, w2)\nTherefore the indicator random variable that is 1 when g(w1) = g(w2) has expectation equal to the Jaccard similarity between the two words. Unfortunately it has too high a variance to be useful on its own. The idea of the MinHash scheme is to reduce the variance by averaging together k of these variables constructed in the same way with k different hash functions. We index these k functions using the notation gj(w) to denote the MinHash of hash function hj(t). We denote the computation of hashes as \u2018MinHashMap\u2019. Specifically, MinHashMap is defined as Algorithm 6.\nTo estimate Jac(w1, w2) using this version of the scheme, we simply count the number of hash functions for which g(w1) = g(w2), and divide by k to get an estimate of Jac(w1, w2). By the multiplicative Chernoff bound for sums of 0-1 random variables as seen in Theorem 1, setting k = O(1/\u01eb) will ensure that w.h.p. a similarity score that is above \u01eb has relative error no more than \u03b4. Qualitatively, this theorem is the same as given in (Broder, 1997) (where MinHash is introduced) and we do not claim the following as new contribution, however, we include it for completeness. More rigorously,\nTheorem 3 Fix any two words x and y having Jac(x, y) \u2265 \u01eb. Let X1,X2, . . . ,Xk represent indicators for {g1(x) = g1(y), . . . , gk(x) = gk(y)} and X = \u2211k i=1 Xi. For any 1 > \u03b4 > 0 and k = c/\u01eb, we have\nPr [X/k > (1 + \u03b4)Jac(x, y)] \u2264 (\ne\u03b4\n(1 + \u03b4)(1+\u03b4)\n)c\nand Pr [X/k < (1\u2212 \u03b4)Jac(x, y)] \u2264 exp(\u2212c\u03b42/2) Proof We use X/k as the estimator for Jac(x, y). Note that E[X] = kJac(x, y) = (c/\u01eb)Jac(x, y) \u2265 c. Now by standard Chernoff bounds we have\nPr [X/k > (1 + \u03b4)Jac(x, y)] = Pr [X > (1 + \u03b4)E[X]] \u2264 (\ne\u03b4\n(1 + \u03b4)(1+\u03b4)\n)E[X]\n\u2264 (\ne\u03b4\n(1 + \u03b4)(1+\u03b4)\n)c\nSimilarly, by the other side of the multiplicative Chernoff bound, we have\nPr [X/k < (1\u2212 \u03b4)Jac(x, y)] \u2264 exp(\u2212c\u03b42/2)\nThe MapReduce implementation of the above scheme takes in documents and for each unique word in the document outputs k hash values.\nAlgorithm 6 MinHashMap(t, \u3008w1, . . . , wL\u3009) for i = 1 to L do\nfor j = 1 to k do emit ((wi, j) \u2192 hj(t)) end for\nend for\n1. Given document t, Map using MinHashMap (Algorithm 6)\n2. Reduce using the min reducer\nAlgorithm 7 MinHashSampleMap(t, \u3008w1, . . . , wL\u3009) for i = 1 to L do\nfor j = 1 to k do if hj(t) \u2264 c log(Dk)#(wi) then\nemit ((wi, j) \u2192 hj(t)) end if\nend for\nend for\nRecall that a document has at most L words. This naive Mapper will have shuffle size NLk = O(NL/\u01eb), which can be improved upon. After the map phase, for each of the k hash functions, the standard MinReducer is used, which will compute gj(w). These MinHash values are then simply checked for equality. We modify the initial map phase, and prove that the modification brings down shuffle size while maintaining correctness. The modification is seen in algorithm 7, note that c is a small constant we take to be 3.\nWe now prove that MinHashSampleMap will with high probability Emit the minimum hash value for a given word w and hash function h, thus ensuring the steps following MinHashSampleMap will be unaffected.\nTheorem 4 If the hash functions h1, . . . , hk map documents to [0, 1] uniform randomly, and c = 3, then with probability at least 1 \u2212 1\n(Dk)2 , for all words w and hash functions\nh \u2208 {h1, . . . , hk}, MinHashSampleMap will emit the document that realizes g(w).\nProof Fix a word w and hash function h and let z = mint|w\u2208t h(t). Now the probability that MinHashSampleMap will not emit the document that realizes g(w) is\nPr\n[\nz > c log(Dk)\n#(w)\n]\n=\n(\n1\u2212 c log(Dk) #(w)\n)#(w)\n\u2264 e\u2212c log(Dk) = 1 (Dk)c\nThus for a single w and h we have shown MinHashSampleMap will w.h.p. emit the hash that realizes the MinHash. To show the same result for all hash functions and words in the dictionary, set c = 3 and use union bound to get a ( 1\nDk )2 bound on the probability of error\nfor any w1, . . . , wD and h1, . . . , hk.\nNow that we have correctness via theorems 3 and 4, we move onto calculating the shuffle size for MinHashSampleMap.\nTheorem 5 MinHashSampleMap has expected shuffle size O(Dk log(Dk)) = O((D/\u01eb) log(D/\u01eb)).\nProof Simply adding up the expectations for the emissions indicator variables, we see the shuffle size is bounded by:\n\u2211\nh\u2208{h1,...,hk}\n\u2211\nw\u2208{w1,...,wD}\nc log(Dk)\n#(w) #(w) = Dkc log(Dk)\nSetting c = 3 and k = 1/\u01eb gives the desired bound.\nAll of the reducers used in our algorithms are associative and commutative operations (sum and min), and therefore can be combined for optimization. Our results do not change qualitatively when combiners are used, except for one case. A subtle point arises in our claim for improving MinHash. If we combine with m mappers, then the naive MinHash implementation will have a shuffle size of O(mDk) whereas DISCO provides a shuffle size of O(Dk log(Dk)). Since m is usually set to be a very large constant (one can easily use 10,000 mappers in a standard Hadoop implementation), removing the dependence on m is beneficial. For the other similarity measures, combining the Naive mappers can only bring down the shuffle size to O(mD2), which DISCO improves upon asymptotically by obtaining a bound of O(DL/\u01eb log(D)) without even combining, so combining will help even more. In practice, DISCO can be easily combined, ensuring superior performance both theoretically and empirically."}, {"heading": "5. Cosine Similarity in Streaming Model", "text": "We briefly depart from the MapReduce framework and instead work in the \u2018Streaming\u2019 framework. In this setting, data is streamed dimension-by-dimension through a single machine that can at any time answer queries of the form \u201cwhat is the similarity between points x and y considering all the input so far?\u201d. The main performance measure is how much memory the machine uses and queries will be answered in constant time. We describe the algorithm only for cosine similarity, and an almost identical algorithm will work for dice, overlap, and conditional similarity.\nOur algorithm will be very similar to the mapreduce setup, but in replace of emitting pairs to be shuffled for a reduce phase, we instead insert them into a hash map H, keyed by pairs of words, with each entry holding a bag of emissions. H is used to track the emissions by storing them in a bag associated with the pair. Since all data streams through a single machine, for any word x, we can keep a counter for #(x). This will take D counters worth of memory, but as we will see in Theorem 1 (original paper) this memory usage will be dominated by the size of H. Each emission is decorated with the probability of emission. There are two operations to be described: the update that occurs when a new dimension (document) arrives, and the constant time algorithm used to answer similarity queries.\nUpdate. On an update we are given a document. For each pair of words x, y in the document, with independent coin flips of probability q = p\n\u01eb 1\u221a\n#(x)#(y) we lookup the bag\nassociated with (x, y) in H and insert q into it. It is important to note that the emission is being done with probability q and q is computed using the current values of #(x) and #(y). Thus if a query comes after this update, we must take into account all new information. This is done via subsampling and is explained shortly. It remains to show how to use these probabilities to answer queries.\nQuery. We now describe how to answer the only query. Let the query be for the similarity between words x and y. Recall at this point we know both #(x) and #(y) exactly, for the data seen so far. We lookup the pair (x, y) in H, and grab the associated bag of emissions. Recall from above that emission is decorated with the probability of\nemission qi for the i\u2019th entry in the bag. Unfortunately qi will be larger than we need it to be, since it was computed at a previous time, when fewer occurrences of x and y had happened. To remedy this, we independently subsample each of the emissions for the pair x, y with coin flips of probability p\n\u01eb 1 qi \u221a #(x)#(y) . For each pair x, y seen in the input, there\nwill be exactly\nqi p\n\u01eb\n1\nqi \u221a #(x)#(y) =\np\n\u01eb\n1 \u221a\n#(x)#(y)\nprobability of surviving emission and the submsampling. Finally, since the pair x, y is seen exactly #(x, y) times, the same estimator used in Theorem 2 (original paper) will have expectation equal to cos(x, y). Furthermore, since before subsampling we output in expectation more pairs than CosineSampleEmit, the Chernoff bound of Theorem 2 still holds. Finally, to show that H cannot grow too large, we bound its size in Theorem 6.\nTheorem 6 The streaming algorithm uses at most O(DL lg(N) log(D)/\u01eb) memory.\nProof We only need to bound the size of H. Consider a word x and all of its occurrences in documents t1, . . . , t#(x) at final time (i.e. after all N documents have been processed). We conceptually and only for this analysis construct a new larger dataset C \u2032 where each word x is removed and its occurrences are replaced in order with \u230alg #(x)\u230b+ 1 new words x1, . . . , x\u230alg#(x)\u230b+1, so we are effectively segmenting (in time) the occurrences of x. With this in mind, we construct C \u2032 so that each xi will replace 2\ni\u22121 occurrences of x, in time order. i.e. we will have #(xi) = 2 i\u22121.\nNote that our streaming algorithm updates the counters for words with every update. Consider what happens if instead of updating every time, the streaming algorithm somehow in advance knew and used the final #(x) values after all documents have been processed. We call this the \u2018all-knowing\u2019 version. The size of H for such an all-knowing algorithm is the same as the shuffle size for the DISCO sampling scheme analyzed in Theorem 2, simply because there is a bijection between the emits of CosineSampleEmit and inserts into H with exactly the same coin flip probability. We now use this observation and C \u2032.\nWe show that the memory used by H when our algorithm is run on C \u2032 with the allknowing counters dominates (in expectation) the size of H for the original dataset in the streaming model, thus achieving the claimed bound in the current theorem statement.\nLet PrHashMapInsert(x, y) denote the probability of inserting the pair x, y when we run the all-knowing version of the streaming algorithm on C \u2032. Let PrStreamEmit(x, y, a, b) denote the probability of emitting the pair x, y in the streaming model with input C, after observing x, y exactly a, b times, respectively. With these definitions we have\nPrStreamEmit(x, y, a, b) = p\n\u01eb 1\u221a ab\n\u2264 p \u01eb\n1 2\u230alg a\u230b2\u230alg b\u230b \u2264 p \u01eb 1 \u221a\n#(x\u230alg a\u230b) \u221a #(x\u230alg b\u230b)\n= PrHashMapInsert(x\u230alg a\u230b+1, y\u230alg b\u230b+1)\nThe first inequality holds by properties of the floor function. The second inequality holds by definition of C \u2032. The dictionary size for C \u2032 is O(D lg(N)) where D is the original dictionary size for C. Using the same analysis of Theorem 2 (original paper), the shuffle size for C \u2032 is at most O(DL lg(N) log(D)/\u01eb) and therefore so is the size of H for the all-knowing algorithm run on C \u2032, and by the analysis above, so is the hash map size for the original dataset C."}, {"heading": "6. Correctness and Shuffle Size Proofs for other Similarity Measures", "text": ""}, {"heading": "6.1 Overlap Similarity", "text": "Overlap similiarity follows the same pattern as we used for cosine similarity, thus we only explain the parts that are different. The emit function changes to Algorithm 8.\nAlgorithm 8 OverlapSampleEmit(w1, w2)\nWith probability p\n\u01eb\n1\nmin(#(w1),#(w2))\nemit ((w1, w2) \u2192 1)\nThe correctness proof is nearly identical to cosine similarity so we do not restate it. The shuffle size for OverlapSampleEmit is given by the following theorem.\nTheorem 7 The expected shuffle size for OverlapSampleEmit is O(DL log(D)/\u01eb).\nProof The expected contribution from each pair of words will constitute the shuffle size:\nD \u2211\ni=1\nD \u2211\nj=i+1\np\n\u01eb\n#(wi, wj)\nmin(#(wi),#(wj))\n\u2264 p \u01eb\nD \u2211\ni=1\nD \u2211\nj=i+1\n#(wi, wj)( 1\n#(wi) +\n1\n#(wj) )\n\u2264 2p \u01eb\nD \u2211\ni=1\n1\n#(wi)\nD \u2211\nj=1\n#(wi, wj)\n\u2264 2p \u01eb\nD \u2211\ni=1\n1\n#(wi) L#(wi) =\n2p\n\u01eb LD = O(DL log(D)/\u01eb)\nThe first inequality holds trivially. The last inequality holds because wi can co-occur with at most #(wi)L other words. It is easy to see via Chernoff bounds that the above shuffle size is obtained with high probability."}, {"heading": "6.2 Dice Similarity", "text": "Dice similiarity follows the same pattern as we used for cosine similarity, thus we only explain the parts that are different. The emit function changes to Algorithm 9.\nAlgorithm 9 DiceSampleEmit(w1, w2)\nWith probability p\n\u01eb\n2\n#(w1) + #(w2)\nemit ((w1, w2) \u2192 1)\nThe correctness proof is nearly identical to cosine similarity so we do not restate it. The shuffle size for DiceSampleEmit is given by the following theorem.\nTheorem 8 The expected shuffle size for DiceSampleEmit is O(DL log(D)/\u01eb).\nProof The expected contribution from each pair of words will constitute the shuffle size:\n2 D \u2211\ni=1\nD \u2211\nj=i+1\np\n\u01eb\n#(wi, wj)\n#(wi) + #(wj)\n\u2264 2p \u01eb\nD \u2211\ni=1\nD \u2211\nj=1\n#(wi, wj)\n#(wi)\n\u2264 2p \u01eb\nD \u2211\ni=1\n1\n#(wi) L#(wi) =\n2p\n\u01eb LD = O(DL log(D)/\u01eb)\nThe first inequality holds trivially. The last inequality holds because wi can co-occur with at most #(wi)L other words. It is easy to see via Chernoff bounds that the above shuffle size is obtained with high probability."}, {"heading": "6.3 Conditional Similarity", "text": "Conditional similiarity follows the same pattern as we used for cosine similarity, thus we only explain the parts that are different. The emit function changes to Algorithm 10 and the reducer to algorithm 11.\nAlgorithm 10 ConditionalSampleEmit(w1, w2)\nWith probability N\n#(w1)#(w2)\nemit ((w1, w2) \u2192 1)\nThe correctness proof is nearly identical to cosine similarity so we do not restate it. The shuffle size for ConditionalSampleEmit is given by the following theorem, which is slightly\nAlgorithm 11 DISCOCondReducer((w1, w2), \u3008r1, . . . , rR\u3009) a =\n\u2211R i=1 ri\noutput a\ndifferent from the previous similarity scores since it requires an independent co-occurences assumption.\nTheorem 9 Assuming independent co-occurence defined #(w1, w2) = (#(w1)/N)(#(w2)/N)N , the expected shuffle size for ConditionalSampleEmit is O(D2).\nProof The expected contribution from each pair of words will constitute the shuffle size:\nD \u2211\ni=1\nD \u2211\nj=i+1\nN#(wi, wj) #(wi)#(wj)\n= D \u2211\ni=1\nD \u2211\nj=i+1\nN(#(wi)/N)(#(wj)/N)N\n#(wi)#(wj) = O(D2)\nThe first equality holds because of the independent co-occurence assumption. It is easy to see via Chernoff bounds that the above shuffle size is obtained with high probability."}, {"heading": "7. Experiments", "text": "We use data from the social networking site Twitter. Twitter is currently a very popular social network platform. Users interact with Twitter through a web interface, instant messaging clients, or sending mobile text messages. Public updates by users are viewable to the world, and a large majority of Twitter accounts are public. These public tweets provide a large real-time corpus of what is happening in the world. The data we used in this study was created by taking N = 198, 134, 530 public tweets.\nThe number of dimensions N = 198, 134, 530 in our data is equal to the number of tweets and each tweet is a document with size at most 140 characters, providing a small upper bound for L. These documents are ideal for our framework, since our shuffle size upper bound depends on L, which in this case is very small. We used a dictionary of 1000 words advertisers on Twitter are currently targeting to show Promoted Trends, Trends, and Accounts. We also tried a uniformly random sampled dictionary without a qualitative change in results.\nThe reason we only used D = 1000 is because for the purpose of validating our work (i.e. reporting the small errors occurred by our algorithms), we have to compute the true cosine similarities, which means computing true co-occurence for every pair, which is a challenging task computationally. This was a bottleneck only in our experiments for this paper, and does not affect users of our algorithms. We ran experiments with D = 106, but cannot report true error since finding the true cosine similarities are too computationally intensive. In this regime however, our theorems guarantee that the results are good."}, {"heading": "7.1 Shuffle Size vs Accuracy", "text": "We have two parameters to tweak to tradeoff accuracy and shuffle size: \u01eb and p. However, since they only occur in our algorithms as the ratio p/\u01eb, we simply use that as the tradeoff parameter. The reason we separated p and \u01eb was for the theorems to go through nicely, but in reality we only see a single tweaking parameter.\nWe increase p/\u01eb exponentially on the x axis and record the ratio of DISCO shuffle size to the naive implementation. In all cases we can achieve a 90% reduction in shuffle size without sacrificing much accuracy, as see in Figures 2, 4, and 6. The accuracy we report is with respect to true cosine, dice, and overlap similarity."}, {"heading": "7.2 Error vs Similarity Magnitude", "text": "All of our theorems report better accuracy for pairs that have higher similarity than otherwise. To see this empirically, we plot the average error of all pairs that have true similarity above \u01eb. These can be seen in Figures 1, 3, 5, 9, 7, and 8. Note that the reason for large portions of the error being constant in these plots is that there are very few pairs with very high similarities, and therefore the error remains constant while \u01eb is between the difference of two such very high similarity pairs."}, {"heading": "8. Conclusions and Future Directions", "text": "We presented the DISCO suite of algorithms to compute all pairwise similarities between very high dimensional sparse vectors. All of our results are provably independent of di-\nmension, meaning apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension, thus the dimension can be very large.\nAlthough we use the MapReduce (Dean and Ghemawat, 2008) and Streaming computation models to discuss shuffle size and memory, the sampling strategy we use can be generalized to other frameworks. We anticipate the DISCO sampling strategy to be useful whenever one is computing a number between 0 and 1 by taking the ratio of an unknown number (the dot product in our case) by some known number (e.g. \u221a\n#(x)#(y) for cosine similarity). This is a very high-level description, and we give five concrete examples, along with proofs, and experiments."}], "references": [{"title": "Keyword generation for search engine advertising using semantic similarity between terms", "author": ["V. Abhishek", "K. Hosanagar"], "venue": "In EC", "citeRegEx": "Abhishek and Hosanagar.,? \\Q2007\\E", "shortCiteRegEx": "Abhishek and Hosanagar.", "year": 2007}, {"title": "Efficient exact set-similarity joins", "author": ["A. Arasu", "V. Ganti", "R. Kaushik"], "venue": "In VLDB", "citeRegEx": "Arasu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Arasu et al\\.", "year": 2006}, {"title": "Agglomerative clustering of a search engine query log", "author": ["D. Beeferman", "A. Berger"], "venue": "In SIGKDD", "citeRegEx": "Beeferman and Berger.,? \\Q2000\\E", "shortCiteRegEx": "Beeferman and Berger.", "year": 2000}, {"title": "The hadoop distributed file system: Architecture and design", "author": ["D. Borthakur"], "venue": "Hadoop Project Website,", "citeRegEx": "Borthakur.,? \\Q2007\\E", "shortCiteRegEx": "Borthakur.", "year": 2007}, {"title": "Semantic similarity between search engine queries using tem", "author": ["S. Chien", "N. Immorlica"], "venue": null, "citeRegEx": "Chien and Immorlica.,? \\Q2006\\E", "shortCiteRegEx": "Chien and Immorlica.", "year": 2006}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In VLDB", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "STOC", "citeRegEx": "Indyk and Motwani.,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Brute force and indexed approaches to pairwise document similarity comparisons with mapreduce", "author": ["J. Lin"], "venue": "SIGIR", "citeRegEx": "Lin.,? \\Q2009\\E", "shortCiteRegEx": "Lin.", "year": 2009}, {"title": "Web-scale distributional similarity and entity set expansion", "author": ["P. Pantel", "E. Crestan", "A. Borkovsky", "A.M. Popescu", "V. Vyas"], "venue": "EMNLP", "citeRegEx": "Pantel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pantel et al\\.", "year": 2009}, {"title": "Predicting click-through rate using keyword clusters", "author": ["M. Regelson", "D. Fain"], "venue": "In Proceedings of the Second Workshop on Sponsored Search Auctions,", "citeRegEx": "Regelson and Fain.,? \\Q2006\\E", "shortCiteRegEx": "Regelson and Fain.", "year": 2006}, {"title": "A web-based kernel function for measuring the similarity of short text snippets", "author": ["M. Sahami", "T.D. Heilman"], "venue": "WWW", "citeRegEx": "Sahami and Heilman.,? \\Q2006\\E", "shortCiteRegEx": "Sahami and Heilman.", "year": 2006}, {"title": "Efficient set joins on similarity predicates", "author": ["S. Sarawagi", "A. Kirpal"], "venue": "In ACM SIGMOD", "citeRegEx": "Sarawagi and Kirpal.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi and Kirpal.", "year": 2004}, {"title": "Evaluating similarity measures: a large-scale study in the orkut social network", "author": ["E. Spertus", "M. Sahami", "O. Buyukkokten"], "venue": "In SIGKDD", "citeRegEx": "Spertus et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Spertus et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "There are many examples, including \u2022 Advertiser keyword suggestions: When targeting advertisements via keywords, it is useful to expand the manually input set of keywords by other similar keywords, requiring finding all keywords more similar than a high threshold (Regelson and Fain, 2006).", "startOffset": 264, "endOffset": 289}, {"referenceID": 0, "context": "\u2022 Web Search: Rewriting queries given to a search engine is a common trick used to expand the coverage of the search results (Abhishek and Hosanagar, 2007).", "startOffset": 125, "endOffset": 155}, {"referenceID": 10, "context": "We focus on 5 different similarity measures, including cosine similarity which is very popular and produces high quality results across different domains (Chien and Immorlica, 2005; Chuang and Chien, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 154, "endOffset": 253}, {"referenceID": 12, "context": "We focus on 5 different similarity measures, including cosine similarity which is very popular and produces high quality results across different domains (Chien and Immorlica, 2005; Chuang and Chien, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 154, "endOffset": 253}, {"referenceID": 3, "context": "Our results and theorems hold across any MapReduce implementation such as Hadoop (Borthakur, 2007)(Gates et al.", "startOffset": 81, "endOffset": 98}, {"referenceID": 7, "context": "In (Lin, 2009), all pairs are on computed on MapReduce, but there is a focus on the life sciences domain.", "startOffset": 3, "endOffset": 14}, {"referenceID": 1, "context": "The all-pairs similarity search problem has also been addressed in the database community, where it is known as the similarity join problem (Arasu et al., 2006; Chaudhuri et al., 2006; Sarawagi and Kirpal, 2004).", "startOffset": 140, "endOffset": 211}, {"referenceID": 11, "context": "The all-pairs similarity search problem has also been addressed in the database community, where it is known as the similarity join problem (Arasu et al., 2006; Chaudhuri et al., 2006; Sarawagi and Kirpal, 2004).", "startOffset": 140, "endOffset": 211}, {"referenceID": 5, "context": "There is large body of work on the nearest neighbors problem, which is the problem of finding the k nearest neighbors of a given query point(Charikar, 2002; Fagin et al., 2003; Gionis et al., 1999; Indyk and Motwani, 1998).", "startOffset": 140, "endOffset": 222}, {"referenceID": 6, "context": "There is large body of work on the nearest neighbors problem, which is the problem of finding the k nearest neighbors of a given query point(Charikar, 2002; Fagin et al., 2003; Gionis et al., 1999; Indyk and Motwani, 1998).", "startOffset": 140, "endOffset": 222}, {"referenceID": 8, "context": "In (Pantel et al., 2009), the authors propose a highly scalable term similarity algorithm, implemented in the MapReduce framework, and deployed over a 200 billion word crawl of the Web to compute pairwise similarities between terms.", "startOffset": 3, "endOffset": 24}, {"referenceID": 2, "context": "Other related work includes clustering of web data (Beeferman and Berger, 2000; Chien and Immorlica, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 51, "endOffset": 154}, {"referenceID": 10, "context": "Other related work includes clustering of web data (Beeferman and Berger, 2000; Chien and Immorlica, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 51, "endOffset": 154}, {"referenceID": 12, "context": "Other related work includes clustering of web data (Beeferman and Berger, 2000; Chien and Immorlica, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 51, "endOffset": 154}], "year": 2012, "abstractText": "We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high dimensional sparse vectors. All of our results are provably independent of dimension, meaning apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension, thus the dimension can be very large. We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems at large scale using data from the social networking site Twitter.", "creator": "LaTeX with hyperref package"}}}