{"id": "1206.2691", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "IDS: An Incremental Learning Algorithm for Finite Automata", "abstract": "We sekope present shafrir a delude new algorithm IDS for myst\u00e8res incremental glovsky learning of deterministic allusions finite zavis automata (DFA ). tlp This sapphic algorithm diabolus is based anchia on ligota the ising concept smelling of heeren distinguishing stone-campbell sequences tautomer introduced investigatory in (Angluin81 ). perilous We give loughor a rigorous proof that two versions 102.15 of pch this learning tolerance algorithm oblivion correctly learn schoeni in the 55-19 limit. kol\u00e9las Finally we present an safa empirical romantically performance nuku analysis that invermay compares varix these two betteridge algorithms, focussing stovepipe on learning times kushiel and different bythe types of learning queries. carpentier We conclude flaps that mcgartland IDS is vivax an fiddlehead efficient algorithm implementation for jong software engineering hemopure applications of deerhound automata blown-up learning, logical such as testing and isho\u02bfyahb model dearmond inference.", "histories": [["v1", "Wed, 13 Jun 2012 00:27:36 GMT  (143kb,D)", "http://arxiv.org/abs/1206.2691v1", "8 pages, 5 figures"]], "COMMENTS": "8 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.FL", "authors": ["muddassar a sindhu", "karl meinke"], "accepted": false, "id": "1206.2691"}, "pdf": {"name": "1206.2691.pdf", "metadata": {"source": "CRF", "title": "IDS: An Incremental Learning Algorithm for Finite Automata", "authors": ["Muddassar A. Sindhu", "Karl Meinke"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Online learning, model inference, incremental learning, learning in the limit, language inference.\nI. INTRODUCTION\nIn recent years, automata learning algorithms (aka. regular inference algorithms) have found new applications in software engineering such as formal verification (e.g. [2],[3], [4] ) software testing (e.g. [5],[6] ) and model inference (e.g. [7]). These applications mostly centre around learning an abstraction of a complex software system which can then be statically analysed (e.g. by model checking) to determine behavioural correctness. Many of these applications can be improved by the use of learning procedures that are incremental.\nAn automata learning algorithm is incremental if: (i) it constructs a sequence of hypothesis automata H0, H1, ... from a sequence of observations o0, o1, ... about an unknown target automaton A, and this sequence of hypothesis automata finitely converges to A, and (ii) the construction of hypothesis Hi can reuse aspects of the construction of the previous hypothesis Hi\u22121 (such as an equivalence relation on states). The notion of convergence in the limit, as a model of correct incremental learning originates in [8].\nGenerally speaking, much of the literature on automata learning has focussed on offline learning from a fixed preexisting data set describing the target automaton. Other approaches, such as [1] and [9] have considered online learning, where the data set can be extended by constructing and posing new queries. However, little attention has been paid to incremental learning algorithms, which can be seen as a subclass of online algorithms where serial hypothesis construction using a sequence of increasing data sets is emphasized. The much smaller collection of known incremental algorithms includes the RPNI2 algorithm of [10], the IID algorithm of [11] and the algorithm of [12]. However, the motivation for incremental learning from a software engineering perspective is strong, and can be summarised as follows:\n2) the choice of each relevant observation oi about a large unknown software system often needs to be iteratively guided by analysis of the previous hypothesis model Hi\u22121 for efficiency reasons.\nOur research into efficient learning-based testing (LBT) for software systems (see e.g. [13], [14], [6]) has led us to investigate the use of distinguishing sequences to design incremental learning algorithms for DFA. Distinguishing sequences offer a rather minimal and flexible way to construct a state space partition, and hence a quotient automaton that represents a hypothesis H about the target DFA to be learned. Distinguishing sequences were first applied to derive the ID online learning algorithm for DFA in [1].\nIn this paper, we present a new algorithm incremental distinguishing sequences (IDS), which uses the distinguishing sequence technique for incremental learning of DFA. In [6] this algorithm has been successfully applied to learning based testing of reactive systems with demonstrated error discovery rates up to 4000 times faster than using non-incremental learning. Since little seems to have been published about the empirical performance of incremental learning algorithms, we consider this question too. The structure of the paper is as follows. In Section II, we review some essential mathematical preliminaries, including a presentation of Angluin\u2019s original ID algorithm, which is necessary to understand the correctness proof for IDS. In Section III, we present two different versions of the IDS algorithm and prove their correctness. These are called: (1) prefix free IDS, and (2) prefix closed IDS. In Section IV, we compare the empirical performance of our two IDS algorithms with each other. Finally, in Section V, we present some conclusions and discuss future directions for research."}, {"heading": "A. Related Work", "text": "Distinguishing sequences were first applied to derive the ID online learning algorithm for DFA in [1]. The ID algorithm is not incremental, since only a single hypothesis automaton is ever produced. Later an incremental version IID of this algorithm was presented in [11]. Like the IID algorithm, our IDS algorithm is incremental. However in contrast with IID, the IDS algorithm, and its proof of correctness are much simpler, and some technical errors in [11] are also overcome. Distinguishing sequences can be contrasted with the complete consistent table approach to partition construction as represented by the well known online learning algorithm L* of [9]. Unlike L*, distinguishing sequences dispose of the need for\nar X\niv :1\n20 6.\n26 91\nv1 [\ncs .L\nG ]\n1 3\nJu n\n20 12\n2 an equivalence oracle during learning. Instead, we can assume that the observation set P contains a live complete set of input strings (see Section II-B below for a technical definition). Furthermore, unlike L* distinguishing sequences do not require a complete table of queries before building the partition relation. In the context of software testing, both of these differences result in a much more efficient learning algorithm. In particular\nthere is greater scope for using online queries that have been generated by other means (such as model checking). Moreover, since LBT is a black-box approach to software testing, then the use of an equivalence oracle contradicts the black-box methodology. In [10], an incremental version RPNI2 of the RPNI offline learning algorithm of [15] and [16] is presented. The RPNI2 algorithm is much more complex than IDS. It includes a recursive depth first search of a lexicographically ordered state set with backtracking, and computation of a non-deterministic hypothesis automaton that is subsequently rendered deterministic. These operations have no counterpart in IDS. Thus IDS is easier to verify and can be quickly and easily implemented in practise. The incremental learning algorithm introduced in [12] requires a lexicographic ordering on the presentation of online queries, which is less flexible than IDS, and indeed inappropriate for software engineering applications."}, {"heading": "II. PRELIMINARIES", "text": ""}, {"heading": "A. Notations and concepts for DFA", "text": "Let \u03a3 be any set of symbols then \u03a3\u2217 denotes the set of all finite strings over \u03a3 including the empty string \u03bb. The length of a string \u03b1 \u2208 \u03a3\u2217 is denoted by |\u03b1| and |\u03bb| = 0. For strings \u03b1, \u03b2 \u2208 \u03a3\u2217 , \u03b1\u03b2 denotes their concatenation. For \u03b1, \u03b2, \u03b3 \u2208 \u03a3\u2217, if \u03b1 = \u03b2\u03b3 then \u03b2 is termed a prefix of \u03b1 and \u03b3 is termed a suffix of \u03b1. We let Pref(\u03b1) denote the prefix closure of \u03b1, i.e. the set of all prefixes of \u03b1. We can also apply prefix closure pointwise to any set of strings. The set difference operation between two sets U and V denoted by U \u2212 V is the set of elements of U which are not members of V. The symmetric difference operation defined on pairs of sets is defined by U \u2295 V = (U \u2212 V ) \u222a (V \u2212 U). A deterministic finite automaton (DFA) is a quintuple A = \u3008\u03a3, Q, F, q0, \u03b4\u3009, where: \u03a3 is the input alphabet, Q is the state set, F \u2286 Q is the set of final states, q0 \u2208 Q is the initial state and state transition function \u03b4 is a mapping \u03b4 : Q\u00d7 \u03a3\u2192 Q, and \u03b4(qi, b) = qj meaning when in state qi \u2208 Q given input b the automaton A will move to state qj \u2208 Q in one step. We extend the function \u03b4 to a mapping \u03b4\u2217 = Q\u00d7\u03a3\u2217 \u2192 Q inductively defined by \u03b4 = (q, \u03bb) = q and \u03b4\u2217 = (qi, b1, ..., bn) = \u03b4(\u03b4\n\u2217(q, b1, ..., bn\u22121), bn). The language L(A) accepted by A is the set of all strings \u03b1 \u2208 \u03a3\u2217 such that \u03b4\u2217(q0, \u03b1) \u2208 F . As is well known a language L \u2286 \u03a3\u2217is accepted by DFA if and only if L is regular, i.e. L can be defined by a regular grammar. A state q \u2208 Q is said to be live if for some string \u03b1 \u2208 \u03a3\u2217 , \u03b4\u2217(q, \u03b1) \u2208 F , otherwise q is said to be dead. Given a distinguished dead state d0 we define string concatenation modulo the dead state d0, f : \u03a3\u2217\u222a{d0}\u00d7\u03a3\u2192 \u03a3\u2217 \u222a {d0}, by f(d0, \u03c3) = d0 and f(\u03b1, \u03c3) = \u03b1.\u03c3 for \u03b1 \u2208 \u03a3\u2217. This is function is used for automaton learning in Section III. Given any DFA A there exists a minimum state DFA A\u2032 such\ni 0 1\nvi \u03bb b E(d0) \u2205 \u2205 E(\u03bb) \u2205 {b} E(a) \u2205 \u2205 E(b) {\u03bb} {\u03bb, b} E(ba) \u2205 \u2205 E(bb) {\u03bb} {\u03bb}\nTable I FOR(b,+)\nthat L(A) = L(A\u2032) and this automaton is termed the canonical DFA for L(A). A canonical DFA has one dead state at the most. We represent DFA graphically in the usual way using state diagrams. States are represented by small circles labelled by state names and final states among them are marked by concentric double circles. The initial state is represented by attaching a right arrow \u2192 to it. The transitions between the states are represented by directed arrows from state of origin to the destination state. The symbol read from the origin state is attached to the directed arrow as a label. Fig 1 shows state transition diagram of one such DFA."}, {"heading": "B. The ID Algorithm", "text": "Our IDS algorithm is an incremental version of the ID learning algorithm introduced in [1]. The ID algorithm is an online learning algorithm for complete learning of a DFA that starts from a given live complete set P \u2286 \u03a3\u2217 of queries about the target automaton, and generates new queries until a state space partition can be constructed. Since the algorithmic ideas and proof of correctness of IDS are based upon those of ID itself, it is useful to review the ID algorithm here. Algorithm 1 presents the ID algorithm. Since this algorithm has been discussed at length in [1], our own presentation can be brief. A detailed proof of correctness of ID and an analysis of its complexity can be found in [1]. A finite set P \u2286 \u03a3\u2217 of input strings is said to be live complete for a DFA A if for every live state q \u2208 Q there exists a string \u03b1 \u2208 P such that \u03b4\u2217(q0, \u03b1) = q. Given a live complete set P for a target automaton A, the essential idea of the ID algorithm is to first construct the set T \u2032 = P \u222a {f(\u03b1, b)|(\u03b1, b) \u2208 P \u00d7\u03a3} \u222a {d0} of all one element extensions of strings in P as a set of state names for the hypothesis automaton. The symbol d0 is added as a name for the canonical dead state. This set of state names is then iteratively partitioned into sets Ei(\u03b1) \u2286 T \u2032 for i = 0, 1, . . . such that elements \u03b1, \u03b2 of T \u2032 that denote the same state in A will occur in the same partition set, i.e. Ei(\u03b1) = Ei(\u03b2). This partition refinement can be proven to terminate and the resulting collection of sets forms a congruence on T \u2032. Finally the ID algorithm constructs the hypothesis automaton as the resulting quotient automaton. The method used to refine the partition set is to iteratively construct a set V of distinguishing strings, such that no two distinct states of A have the same behaviour on all of V .\nWe will present the ID and IDS algorithms so that similar variables share the same names. This pedagogic device emphasises similarity in the behaviour of both algorithms.\n3 Algorithm 1 The ID Learning Algorithm Input: A live complete set P \u2286 \u03a3\u2217 and a teacher DFA A to answer membership queries \u03b1 \u2208 L(A). Output: A DFA M equivalent to the target DFA A.\n1) begin 2) //Perform Initialization 3) i = 0, vi = \u03bb, V = {vi} 4) P \u2032 = P \u222a{d0}, T = P \u222a{f(\u03b1, b)|(\u03b1, b) \u2208 P\u00d7\u03a3}, T \u2032 = T \u222a{d0} 5) Construct function E0 for v0 = \u03bb, 6) E0(d0) = \u2205 7) \u2200\u03b1 \u2208 T 8) { pose the membership query \u201c\u03b1 \u2208 L(A)?\u201d 9) if the teacher\u2019s response is yes 10) then E0(\u03b1) = {\u03bb} 11) else E0(\u03b1) = \u2205 12) end if 13) } 14) //Refine the partition of the set T \u2032 15) while (\u2203\u03b1, \u03b2 \u2208 P \u2032 and b \u2208 \u03a3 such that Ei(\u03b1) = Ei(\u03b2) but\nEi(f(\u03b1, b)) 66= Ei(f(\u03b2, b))) 16) do 17) Let \u03b3 \u2208 Ei(f(\u03b1, b))\u2295 Ei(f(\u03b2, b)) 18) vi+1 = b\u03b3 19) V = V \u222a {vi+1}, i = i+ 1 20) \u2200\u03b1 \u2208 Tk pose the membership query \u201d\u03b1vi \u2208 L(A)?\u201d 21) { 22) if the teacher\u2019s response is yes 23) then Ei(\u03b1) = Ei\u22121(\u03b1) \u222a {vi} 24) else Ei(\u03b1) = Ei\u22121(\u03b1) 25) end if 26) } 27) end while 28) //Construct the representation M of the target DFA A. 29) The states of M are the sets Ei(\u03b1), where \u03b1 \u2208 T 30) The initial state q0 is the set Ei(\u03bb) 31) The accepting states are the sets Ei(\u03b1) where \u03b1 \u2208 T and \u03bb \u2208 Ei(\u03b1) 32) The transitions of M are defined as follows: 33) \u2200\u03b1 \u2208 P \u2032 34) if Ei(\u03b1) = \u2205 35) then add self loops on the state Ei(\u03b1) for all b \u2208 \u03a3 36) else \u2200b \u2208 \u03a3 set the transition \u03b4(Ei(\u03b1), b) = Ei(f(\u03b1, b)) 37) end if 38) end.\nFigure 2. Hypothesis Automaton M1\nAlgorithm 2 The IDS Learning Algorithm Input: A file S = s1, . . . , sl of input strings si \u2208 \u03a3\u2217 and a teacher DFA A to answer membership queries \u03b1 \u2208 L(A)? Output: A sequence of DFAMt for t = 0, . . . , l as well as the total number of membership queries and book keeping queries asked by the learner.\n1) begin 2) //Perform Initialization 3) i = 0, k = 0, t = 0, vi = \u03bb, V = {vi} 4) //Process the empty string 5) P0 = {\u03bb}, P \u20320 = P0 \u222a {d0}, T0 = P0 \u222a \u03a3 6) E0(d0) = \u2205 7) \u2200\u03b1 \u2208 T0 { 8) pose the membership query \u201c\u03b1 \u2208 L(A)?\u201d, 9) bquery = bquery + 1\n10) if the teacher\u2019s response is yes 11) then E0(\u03b1) = {\u03bb} 12) else E0(\u03b1) = \u2205 13) } 14) //Refine the partition of set T0 as described in Algorithm 3 15) //Construct the current representation M0 of the target DFA 16) //as described in Algorithm 4. 17) 18) //Process the file of examples. 19) while (S 6= empty) 20) do 21) read( S, \u03b1 ) 22) mquery = mquery +1 23) k = k+1, t = t+1 24) Pk = Pk\u22121 \u222a {\u03b1} 25) // Pk = Pk\u22121 \u222a Pref(\u03b1) //prefix closure 26) P \u2032k = Pk \u222a {d0} 27) Tk = Tk\u22121 \u222a {\u03b1} \u222a {f(\u03b1, b)|b \u2208 \u03a3} 28) // Tk = Tk\u22121 \u222a Pref(\u03b1) \u222a {f(\u03b1, b)|\u03b1 \u2208 Pk \u2212 Pk\u22121, b \u2208 \u03a3} 29) //Line 28 for prefix closure 30) T \u2032k = Tk \u222a {d0} 31) \u2200\u03b1 \u2208 Tk \u2212 Tk\u22121 32) { 33) // Fill in the values of Ei(\u03b1) using membership queries: 34) Ei(\u03b1) = {vj |0 \u2264 j \u2264 i, \u03b1vj \u2208 L(A)} 35) bquery = bquery + i 36) } 37) // Refine the partition of the set Tk 38) if \u03b1 is consistent with Mt\u22121 39) then Mt = Mt\u22121 40) else construct Mt as described in Algorithm 4. 41) end while 42) end.\n4\nAlgorithm 3 The Refine Partition Algorithm 1) while (\u2203\u03b1, \u03b2 \u2208 P \u2032k and b \u2208 \u03a3 such that Ei(\u03b1) = Ei(\u03b2)\nbut Ei(f(\u03b1, b)) 66= Ei(f(\u03b2, b))) 2) do 3) Let \u03b3 \u2208 Ei(f(\u03b1, b))\u2295 Ei(f(\u03b2, b)) 4) vi+1 = b\u03b3 5) V = V \u222a {vi+1}, i = i+ 1 6) \u2200\u03b1 \u2208 Tk pose the membership query \u201d\u03b1vi \u2208 L(A)?\u201d 7) { 8) bquery = bquery + 1 9) if the teacher\u2019s response is yes\n10) then Ei(\u03b1) = Ei\u22121(\u03b1) \u222a {vi} 11) else Ei(\u03b1) = Ei\u22121(\u03b1) 12) end if 13) } 14) end while\nAlgorithm 4 The Automata Construction Algorithm 1) The states of Mt are the sets Ei(\u03b1), where \u03b1 \u2208 Tk 2) The initial state q0 is the set Ei(\u03bb) 3) The accepting states are the sets Ei(\u03b1) where \u03b1 \u2208 Tk and \u03bb \u2208 Ei(\u03b1) 4) The transitions of Mt are defined as follows: 5) \u2200\u03b1 \u2208 P \u2032k 6) if Ei(\u03b1) = \u2205 7) then add self loops on the state Ei(\u03b1) for all b \u2208 \u03a3 8) else \u2200b \u2208 \u03a3 set the transition \u03b4(Ei(\u03b1), b) = Ei(f(\u03b1, b)) 9) end if\n10) \u2200\u03b2 \u2208 Tk \u2212 P \u2032k 11) if \u2200\u03b1 \u2208 P \u2032k Ei(\u03b2) 6= Ei(\u03b1) and Ei(\u03b2) 6= \u2205 12) then \u2200b \u2208 \u03a3 set the transition \u03b4(Ei(\u03b2), b) = \u2205 13) end if\nHowever, there are also important differences in behaviour. Thus, when analysing the behavioural properties of program variables we will carefully distinguish their context as e.g. vIDn , E ID n (\u03b1), . . ., and v IDS n , E IDS n (\u03b1), . . . etc. Our proof of correctness for IDS will show how the learning behaviour of IDS on a sequence of input strings s1, . . . , sn \u2208 \u03a3\u2217 can be simulated by the behaviour of ID on the corresponding set of inputs {s1, . . . sn}. Once this is established, one can apply the known correctness of ID to establish the correctness of IDS."}, {"heading": "C. Behavioural differences between IID and IDS", "text": "The IID algorithm of [11] also presents a simulation method for ID. However following points of difference in behaviour of IID and IDS are worth mentioning:\n1) IID starts from a null DFA as hypothesis while IDS constructs the initial hypothesis after reading all \u03c3 \u2208 \u03a3 from the initial state. 2) IID discards all negative examples and waits for the first positive example after the construction of the initial (null) hypothesis to do further construction. IDS on the other hand does construction with all negative and positive examples after building an initial hypothesis which makes it more useful for practical software engineering applications identified in Section I. 3) IID in some cases builds hypotheses which have a partially defined transition function \u03b4 rather than being left total. This will be shown with an example in the next section. IDS fixes this problem due to lines 10-13 of Algorithm 4 described in this paper. 4) Unlike IDS, there is no prefix free version of IID.\n5 5) In addition to the above it is easily shown that IID does not satisfy our Simulation Theorem 2, and thus the two algorithms are quite different. The behavioural properties of ID that are needed to complete this correctness proof can be stated as follows.\nTheorem 1: (i) Let P \u2286 \u03a3\u2217 be a live complete set for a DFA A containing \u03bb. Then given P and A as input, the ID algorithm terminates and the automaton M returned is the canonical automaton for L(A). (ii) Let l \u2208 N be the maximum value of program variable iID given P andA. For all 0 \u2264 n \u2264 l and for all \u03b1 \u2208 T ,\nEIDn (\u03b1) = {vIDj | 0 \u2264 j \u2264 n, \u03b1vIDj \u2208 L(A)}.\nProof: (i) See [1] Theorem 3. (ii) By induction on n. One difference between ID and IID is the frequency of hypothesis automaton construction. With ID this occurs just once, after a single partition refinement is completed. However, with IID this occurs regularly, after each partition refinement. This difference means that the automaton construction algorithm (Algorithm 1, lines 28-37) used for ID can no longer be used for IID, (as asserted in [11]) as we show below. Suppose we want to learn the automaton A shown in Fig 1. A positive example for this automaton is (b,+). If we use it to start learning A using the IID algorithm of [11], we have P0 = Pref(b) = {b, \u03bb} and P \u20320 = {b, \u03bb} \u222a {d0} = {b, \u03bb, d0}. We then obtain the sets T0 = P0 \u222a {f(\u03b1, b)|(\u03b1, b) \u2208 P0 \u00d7 \u03a3} = {b, \u03bb} \u222a {b, \u03bb} \u00d7 {a, b} = {\u03bb, a, b, ba, bb} and T \u20320 = T0 \u222a {d0} = {d0, \u03bb, a, b, ba, bb}. The initial column for the table of partition sets is constructed as shown in Table I.\nFor i = 0 and v0 = \u03bb. From this column we see that two elements of the set P \u20320 have the same value. i.e E(d0) = E(\u03bb) but E(f(d0, b)) 6= E(f(\u03bb, b)). Therefore we have \u03b3 \u2208 E(f(d0, b))\u2295E(f(\u03bb, b)) or \u03b3 \u2208 \u2205\u2295{\u03bb}. We can choose \u03b3 = \u03bb which gives the distinguishing string v1 = b\u03b3 = b\u03bb = b. We then extend Table I for i = 1. Now we can see that all elements of the set P \u20320, which are b, \u03bb and d0, have distinct values in the last column of the table and so no further refinement of the partition is possible. At this stage IID constructs the next hypothesis automaton M1 as shown in Figure 2.\nNow we observe that the transition function \u03b4 for this automaton is only partially defined, since there are no outgoing transitions for the state named {\u03bb}. Therefore the IID algorithm of [11] does not always generate a hypothesis automaton with well defined transition function \u03b4. This created problems when we tried to use this algorithm for practical software engineering applications identified in Section I, e.g. a model checker when used to verify this kind of a hypothesis can get stuck in such a state with no outgoing transitions. Similarly, an automata equivalence checker which is used to terminate a learning-based testing process goes into an infinite loop because of such a state since it will never find its equivalent state in the target automaton.\nThe problem seems to stem from an unclosed table in some executions of IID. The notion of closed and consistent observation table is given in [9] for L* algorithm. The L* algorithm also incrementally builds the observation table but keeps asking queries until it becomes closed and consistent. In this case the table will be closed when \u2200\u03b2 \u2208 T \\ P \u2032 \u2203\u03b1 \u2208 P \u2032\nsuch that Ei(\u03b1) = Ei(\u03b2). If Ei(\u03b1) 6= Ei(\u03b2) as in the above example where E1(bb) = {\u03bb} is not equal to any of E1(d0) = \u2205, E1(\u03bb) = {b} or E1(b) = {\u03bb, b} then the solution in [9] is to move \u03b2 \u2208 T \\ P \u2032 to set P \u2032 and ask more queries to rebuild the congruence that might have been affected by this last addition to set P \u2032. However L* is a complete learning algorithm and it only outputs the description of the hypothesis automaton after learning it completely. Therefore for incremental learning the simplest fix is to set the transitions of such states to the dead state as done in lines 10-13 of Algorithm 4. This doesn\u2019t require any new entries or shuffling the previous entries up in the table thus keeping the congruence intact.\nIII. CORRECTNESS OF IDS ALGORITHM In this section we present our IDS incremental learning algorithm for DFA. In fact, we consider two versions of this algorithm, with and without prefix closure of the set of input strings. We then give a rigorous proof that both algorithms correctly learn an unknown DFA in the limit in the sense of [8]. In Algorithm 2 we present the main IDS algorithm, and in Algorithms 3 and 4 we give its auxiliary algorithms for iterative partition refinement and automaton construction respectively. The version of the IDS algorithm which appears in Algorithm 2 we term the prefix free IDS algorithm, due to lines 24 and 27. Notice that lines 25 and 28 of Algorithm 2 have been commented out. When these latter two lines are uncommented and instead lines 24 and 27 are commented out, we obtain a version of the IDS algorithm that we term prefix closed IDS. We will prove that both prefix closed and prefix free IDS learn correctly in the limit. However, in Section IV we will show that they have quite different performance characteristics in a way that can be expected to influence applications.\nWe will prove the correctness of the prefix free IDS algorithm first, since this proof is somewhat simpler, while the essential proof principles can also be applied to verify the prefix closed IDS algorithm. We begin an analysis of the correctness of prefix free IDS by confirming that the construction of hypothesis automata carried out by Algorithm 4 is well defined.\nProposition 1: For each t \u2265 0 the hypothesis automaton Mt constructed by the automaton construction Algorithm 4 after t input strings have been observed is a well defined DFA.\nProof: The main task is to show \u03b4 to be well defined function and uniquely defined for every state Ei(\u03b1), where \u03b1 \u2208 Tk.\nProposition 1 establishes that Algorithm 2 will generate a sequence of well defined DFA. However, to show that this algorithm learns correctly, we must prove that this sequence of automata converges to the target automaton A given sufficient information about A. It will suffice to show that the behaviour of prefix free IDS can be simulated by the behaviour of ID, since ID is known to learn correctly given a live complete set of input strings (c.f. Theorem 1.(i)). The first step in this proof is to show that the sequences of sets of state names P IDSk and T IDSk generated by prefix free IDS converge to the sets P ID and T ID of ID.\n6 Proposition 2: Let S = s1, . . . , sl be any non-empty sequence of input strings si \u2208 \u03a3\u2217 for prefix free IDS and let P ID = {\u03bb, s1, . . . , sl} be the corresponding input set for ID. (i) For all 0 \u2264 k \u2264 l, P IDSk = {\u03bb, s1, . . . , sk} \u2286 P ID. (ii) For all 0 \u2264 k \u2264 l, T IDSk = P IDSk \u222a{f(\u03b1, b)|\u03b1 \u2208 P IDSk , b \u2208 \u03a3} \u2286 T ID. (iii) P IDSl = P ID and T IDSl = T ID.\nProof: Clearly (iii) follows from (i) and (ii). We prove (i) and (ii) by induction on k.\nNext we turn our attention to proving some fundamental loop invariants for Algorithm 2. Since this algorithm in turn calls the partition refinement Algorithm 3 then we have in effect a doubly nested loop structure to analyse. Clearly the two indexing counters kIDS and iIDS (in the outer and inner loops respectively) both increase on each iteration. However, the relationship between these two variables is not easily defined. Nevertheless, since both variables increase from an initial value of zero, we can assume the existence of a monotone re-indexing function that captures their relationship.\nDefinition 1: Let S = s1, . . . , sl be any non-empty sequence of strings si \u2208 \u03a3\u2217. The re-indexing function KS : N \u2192 N for prefix free IDS on input S is the unique monotonically increasing function such that for each n \u2208 N , KS(n) is the least integer m such that program variable kIDS has value m while the program variable iIDS has value n. Thus, for example, KS(0) = 0. When S is clear from the context, we may write K for KS .\nWith the help of such re-indexing functions we can express important invariant properties of the key program variables vIDSj and E IDS n (\u03b1), and via Proposition 2 their relationship to vIDj and E ID n (\u03b1). Corresponding to the doubly nested loop structure of Algorithm 2, the proof of Theorem 2 below makes use of a doubly nested induction argument.\nTheorem 2: (Simulation Theorem) Let S = s1, . . . , sl be any non-empty sequence of strings si \u2208 \u03a3\u2217. For any execution of prefix free IDS on S there exists an execution of ID on {\u03bb, s1, . . . , sl} such that for all m \u2265 0: (i) For all n \u2265 0 if K(n) = m then: (a) for all 0 \u2264 j \u2264 n, vIDSj = v ID j , (b) for all 0 \u2264 j < n, vIDSn 6= vIDSj , (c) for all \u03b1 \u2208 T IDSm , EIDSn (\u03b1) = {vIDSj |0 \u2264 j \u2264 n, \u03b1vIDSj \u2208 L(A)}. (ii) If m > 0 then let p \u2208 N be the greatest integer such that K(p) = m\u2212 1. Then for all \u03b1 \u2208 T IDSm , EIDSp (\u03b1) = {vIDSj |0 \u2264 j \u2264 p, \u03b1vIDSj \u2208 L(A)}. (iii) The mth partition refinement of IDS terminates.\nProof: By induction on m using Proposition 2(i). Notice that in the statement of Theorem 2 above, since both ID and IDS are non-deterministic algorithms (due to the nondeterministic choice on line 17 of Algorithm 1 and line 3 of Algorithm 3), then we can only talk about the existence of some correct simulation. Clearly there are also simulations of IDS by ID which are not correct, but this does not affect the basic correctness argument.\nCorollary 1: Let S = s1, . . . , sl be any non-empty sequence of strings si \u2208 \u03a3\u2217. Any execution of prefix free IDS on S terminates with the program variable kIDS having value l.\nProof: Follows from Simulation Theorem 2.(iii) since clearly the while loop of Algorithm 2 terminates when the input sequence S is empty.\nUsing the detailed analysis of the invariant properties of the program variables P IDSk and T IDS k in Proposition 2 and v IDS j and EIDSn (\u03b1) in Simulation Theorem 2 it is now a simple matter to establish correctness of learning for the prefix free IDS Algorithm.\nTheorem 3: (Correctness Theorem) Let S = s1, . . . , sl be any non-empty sequence of strings si \u2208 \u03a3\u2217 such that {\u03bb, s1, . . . , sl} is a live complete set for a DFA A. Then prefix free IDS terminates on S and the hypothesis automaton M IDSl is a canonical representation of A.\nProof: By Corollary 1, prefix free IDS terminates on S with the variable kIDS having value l. By Simulation Theorem 2.(i) and Theorem 1.(ii), there exists an execution of ID on {\u03bb, s1, . . . , sl} such that EIDSn (\u03b1) = EIDn (\u03b1) for all \u03b1 \u2208 T IDSl and any n such that K(n) = l. By Proposition 2.(iii), T IDSl = T ID and P \u2032IDSl = P \u2032ID. So letting M ID be the canonical representation of A constructed by ID using {\u03bb, s1, . . . , sl} then M ID and M IDSl have the same state sets, initial states, accepting states and transitions.\nOur next result confirms that the hypothesis automaton M IDSt generated after t input strings have been read is consistent with all currently known observations about the target automaton. This is quite straightforward in the light of Simulation Theorem 2.\nTheorem 4: (Compatibility Theorem) Let S = s1, . . . , sl be any non-empty sequence of strings si \u2208 \u03a3\u2217. For each 0 \u2264 t \u2264 l, M IDSt is compatible with A on {\u03bb, s1, . . . , st}.\nProof: By definition, M IDSt is compatible with A on {\u03bb, s1, . . . , st} if, and only if, for each 0 \u2264 j \u2264 t, sj \u2208 L(A) \u21d4 \u03bb \u2208 EIDSit (sj), where it is the greatest integer such that K(it) = t and the sets EIDSit (\u03b1) for \u03b1 \u2208 T IDS t are the states of M IDSt . Now v IDS 0 = \u03bb. So by Simulation Theorem 2.(i).(c), if sj \u2208 L(A) then sjvIDS0 \u2208 L(A) so vIDS0 \u2208 EIDSit (sj), i.e. \u03bb \u2208 E IDS it\n(sj), and if sj 6\u2208 L(A) then sjv IDS 0 6\u2208 L(A) so vIDS0 6\u2208 EIDSit (sj), i.e. \u03bb 6\u2208 E IDS it (sj). Let us briefly consider the correctness of prefix closed IDS. We begin by observing that the non-sequential ID Algorithm 1 does not compute any prefix closure of input strings. Therefore, Proposition 2 does not hold for prefix closed IDS. In order to obtain a simulation between prefix closed IDS and ID we modify Proposition 2 to the following.\nProposition 3: Let S = s1, . . . , sl be any non-empty sequence of input strings si \u2208 \u03a3\u2217 for prefix closed IDS and let P ID = Pref({\u03bb, s1, . . . , sl}) be the corresponding input set for ID.\n(i) For all 0 \u2264 k \u2264 l, P IDSk = Pref({\u03bb, s1, . . . , sk}) \u2286 P ID. (ii) For all 0 \u2264 k \u2264 l, T IDSk = P IDSk \u222a{f(\u03b1, b)|\u03b1 \u2208 P IIDk , b \u2208 \u03a3} \u2286 T ID. (iii) P IDSl = P ID and T IDSl = T ID\nProof: Similar to the proof of Proposition 2.\nTheorem 5: (Correctness Theorem) Let S = s1, . . . , sl be any non-empty sequence of strings si \u2208 \u03a3\u2217 such that {\u03bb, s1, . . . , sl} is a live complete set for a DFA A. Then prefix closed IDS terminates on S and the hypothesis automaton M IDSl is a canonical representation of A.\nProof: Exercise, following the proof of Theorem 3.\n7"}, {"heading": "IV. EMPIRICAL PERFORMANCE ANALYSIS", "text": "Little seems to have been published about the empirical performance and average time complexity of incremental learning algorithms for DFA in the literature. By the average time complexity of the algorithm we mean the average number of queries needed to completely learn a DFA of a given state space size. This question can be answered experimentally by randomly generating a large number of DFA with a given state space size, and randomly generating a sequence of query strings for each such DFA. From the point of view of software engineering applications such as testing and model inference, we have found that it is important to distinguish between the two types of queries about the target automaton that are used by IDS during the learning procedure. On the one, hand the algorithm uses internally generated queries (we call these book-keeping queries) and on the other hand it uses queries that are supplied externally by the input file (we call these membership queries). From a software engineering applications viewpoint it seems important that the ratio of book-keeping to membership queries should be low. This allows membership queries to have the maximum influence in steering the learning process externally. The average query complexity of the IDS algorithm with respect to the numbers of book-keeping and membership queries needed for complete learning can also be measured by random generation of DFA and query strings. To measure each query type, Algorithm 2 has been instrumented with two integer variables bquery and mquery intended to track the total number of each type of query used during learning (lines 9, 22 and 35). Since two variants of the IDS algorithm were identified, with and without prefix closure of input strings, it was interesting to compare the performance of each of these two variants according to the above two average complexity measures.\nTo empirically measure the average time and query complexity of our two IDS algorithms, two experiments were set up. These measured:\n(1) the average computation time needed to learn a randomly generated DFA (of a given state space size) using randomly generated membership queries, and\n(2) the total number of membership and book-keeping queries needed to learn a randomly generated DFA (of a given state space size) using randomly generated membership queries. We chose randomly generated DFA with state space sizes varying between 5 and 50 states, and an equiprobable distribution of transitions between states. No filtering was applied to remove dead states, so the average effective state space size was therefore somewhat smaller than the nominal state space size.\nThe experimental setup consisted of the following components:\n(1) a random input string generator, (2) a random DFA generator, (3) an instance of the IDS Algorithm (prefix free or prefix closed) , (4) an automaton equivalence checker. The architecture of our evaluation framework and the flow of data between these components are illustrated in Figure 3.\nThe random input string generator constructed strings over the set of alphabet \u03a3 of the target automaton and the length of the generated strings was always \u2264 |Q| of the target automaton. Since IDS begins learning by reading the null string, therefore, null string was only provided externally and wasn\u2019t generated randomly to avoid unnecessary repetition. The random DFA generator started by building a specific sized state set Q. The number of final states |F | \u2264 |Q| was chosen randomly and then these final states were again marked randomly from the state set Q. The initial state was also chosen randomly from the state set. Similarly the transition function \u03b4 was constructed by randomly assigning next states from set Q for each state q \u2208 Q after reading each alphabet \u03c3 \u2208 \u03a3. The IDS algorithms and the entire evaluation framework were implemented in Java. The performance of the input string and DFA generators is dependent on Java\u2019s Random class which generates pseudorandom numbers that depend upon a specific seed. To minimize the chance of generating the same pseudo random strings/automata again the seed was set to the system clock where ever possible.\nThe purpose of the equivalence checker was to terminate the learning procedure as soon as the hypothesis automaton sequence had successfully converged to the target automaton. There are several well known equivalence checking algorithms described in literature. These have runtime complexity ranging from quadratic to nearly linear execution times. We chose an algorithm with nearly linear time performance described in [17]. This was to minimise the overhead of equivalence checking in the overall computation time.\nTen different automata were generated randomly for each state size (ranging between 5 and 50). They were learned by both prefix free and prefix closed IDS and their learning times (in milli-seconds), number of book keeping queries and membership queries asked to reach the target were recorded. The graphs in Figure 4 and Figure 5 show a mean of ten experiments for all these values for both variants of IDS."}, {"heading": "A. Results and Interpretation", "text": "The graphs in Figures 4 and 5 illustrate the outcome of our experiments to measure the average time and average query complexity of both IDS algorithms, as described in Section IV.\nFigure 4 presents the results of estimating the average learning time for the prefix free and prefix closed IDS algorithms as a function of the state space size of the target DFA. For large state space sizes |Q|, the data sets of randomly generated target DFA represent only a small fraction of all possible such DFA of size |Q|. Therefore the two data curves are not smooth for large state space sizes. Nevertheless, there is sufficient data to identify some clear trends. The average learning time for prefix free IDS learning is substantially greater than corresponding time for prefix closed IDS, and this discrepancy increases with state space size. The reason would appear to be that prefix free IDS throws away data about the target DFA that must be regenerated randomly (since input string queries are generated at random). The average time complexity for prefix free IDS learning seems to grow approximately quadratically, while the\n8 average time complexity for prefix closed IDS learning appears to grow almost linearly within the given data range. From this viewpoint, prefix-closed IDS appears to be the superior algorithm.\nFigure 5 presents the results of estimating the average number of membership queries and book-keeping queries as a function of the state space size of the target DFA. Again, we have compared prefix-closed with prefix free IDS learning. Allowance must also be made for the small data set sizes for large state space values. We can see that membership queries grow approximately linearly with the increase in state space size, while book-keeping queries grow approximately quadratically, at least within the data ranges that we considered. There appears to be a small but significant decrease in the number of both book-keeping and membership queries used by the prefix-closed IDS algorithm. The reason for this appears to be similar to the issues identified for average time complexity. Prefix closure seems to be an efficient way to gather data about the target DFA. From the viewpoint of software engineering applications discussed in Section 1, now prefix free IDS appears to be preferable. This is because the decreasing ratio of book-keeping to membership queries improves the possibility to direct the learning process using externally generated queries (e.g. from a model checker)."}, {"heading": "V. CONCLUSIONS", "text": "We have presented two versions of the IDS algorithm which is an incremental algorithm for learning DFA in polynomial time. We have given a rigorous proof that both algorithms correctly learn in the limit. Finally we have presented the results of an empirical study of the average time and query complexity IDS. These empirical results suggest that IDS algorithm is well suited to applications in software engineering, where an incremental approach that allows externally generated online queries is needed. This conclusion is further supported in [6] where we have evaluated the IDS algorithm for learning based testing of reactive systems, and shown that it leads to error discovery up to 4000 times faster than using non-incremental learning. We gratefully acknowledge financial support for this research from the Higher Education Commission (HEC) of Pakistan, the Swedish Research Council (VR) and the EU under project HATS FP7-231620."}], "references": [{"title": "A note on the number of queries needed to identify regular languages", "author": ["D. Angluin"], "venue": "Information and Control, vol. 51, no. 1, pp. 76\u201387, October 1981.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1981}, {"title": "Black-box checking", "author": ["D. Peled", "M. Vardi", "M. Yannakakis"], "venue": "Formal Methods for Protocol Engineering and Distributed Systems FORTE/PSTV. Kluwer, 1999, pp. 225\u2013240.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Sat-based abstraction refinement using ilp and machine learning", "author": ["E. Clarke", "A. Gupta", "J. Kukula", "O. Strichman"], "venue": "Proc. 21st International Conference On Computer Aided Verification (CAV\u201902), 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning meets verification", "author": ["M. Luecker"], "venue": "FMCO, ser. Lecture Notes in Computer Science, F. S. de Boer et al., Ed., vol. 4709. Springer, 2006, pp. 127\u2013151.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic testing via automata learning", "author": ["H. Raffelt", "B. Steffen", "T. Margaria"], "venue": "Hardware and Software: Verification and Testing, ser. LNCS, no. 4899. Springer, 2008, pp. 136\u2013152.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Incremental learning-based testing for reactive systems", "author": ["K. Meinke", "M.A. Sindhu"], "venue": "Tests and Proofs, ser. Lecture Notes in Computer Science, M. Gogolla and B. Wolff, Eds., vol. 6706. Springer, 2011, pp. 134\u2013151.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Regular inference for communication protocol entities", "author": ["T. Bohlin", "B. Jonsson"], "venue": "Dept. of Information Technology, Uppsala University, Technical Report 2008-024, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Language identification in the limit", "author": ["E.M. Gold"], "venue": "Information and Control, vol. 10, pp. 447\u2013474, 1967.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1967}, {"title": "Learning regular sets from queries and counterexamples", "author": ["D. Angluin"], "venue": "Information and Computation, vol. 75, no. 1, pp. 87\u2013106, November 1987.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "Incremental regular inference", "author": ["P. Dupont"], "venue": "Proceedings of the Third ICGI-96, ser. LNAI, no. 1147, 1996.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "A polynomial time incremental algorithm for regular grammar inference", "author": ["R. Parekh", "C. Nichitiu", "V. Honavar"], "venue": "Proc. Fourth Int. Colloq. on Grammatical Inference (ICGI 98), ser. LNAI. Springer, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning automata from ordered examples", "author": ["S. Porat", "J.A. Feldman"], "venue": "Mach. Learn., vol. 7, pp. 109\u2013138, September 1991. [Online]. Available: http://portal.acm.org/citation.cfm?id=125342.125343", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "Automated black-box testing of functional correctness using function approximation", "author": ["K. Meinke"], "venue": "ISSTA \u201904: Proceedings of the 2004 ACM SIGSOFT international symposium on Software testing and analysis. New York, NY, USA: ACM, 2004, pp. 143\u2013153.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "A learning-based approach to unit testing of numerical software", "author": ["K. Meinke", "F. Niu"], "venue": "Proc. Twenty Second IFIP Int. Conf. on Testing Software and Systems (ICTSS 2010), ser. LNCS, no. 6435. Springer, 2010, pp. 221\u2013235.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Inferring regular languages in polynomial update time", "author": ["P. Oncina", "Garc\u00eda"], "venue": "World Scientific Publishing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1991}, {"title": "Random dfa\u2019s can be approximately learned from sparse uniform examples", "author": ["K.J. Lang"], "venue": "Proceedings of the fifth annual workshop on Computational learning theory, ser. COLT \u201992. New York, NY, USA: ACM, 1992, pp. 45\u201352. [Online]. Available: http://doi.acm.org/10.1145/130385.130390", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "A linear algorithm for testing equivalence of finite automata", "author": ["J.E. Hopcroft", "R.M. Karp"], "venue": "Cornell University, Tech. Rep. TR 71-114, 1971. [Online]. Available: http://hdl.handle.net/1813/5958", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1971}], "referenceMentions": [{"referenceID": 0, "context": "This algorithm is based on the concept of distinguishing sequences introduced in [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "[2],[3], [4] ) software testing (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[2],[3], [4] ) software testing (e.", "startOffset": 4, "endOffset": 7}, {"referenceID": 3, "context": "[2],[3], [4] ) software testing (e.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "[5],[6] ) and model inference (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5],[6] ) and model inference (e.", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "[7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The notion of convergence in the limit, as a model of correct incremental learning originates in [8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "Other approaches, such as [1] and [9] have considered online learning,", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "Other approaches, such as [1] and [9] have considered online learning,", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "the RPNI2 algorithm of [10], the IID algorithm of [11] and the algorithm of [12].", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "the RPNI2 algorithm of [10], the IID algorithm of [11] and the algorithm of [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "the RPNI2 algorithm of [10], the IID algorithm of [11] and the algorithm of [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "[13], [14], [6]) has led us to investigate the use of distinguishing sequences to design incremental learning algorithms for DFA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13], [14], [6]) has led us to investigate the use of distinguishing sequences to design incremental learning algorithms for DFA.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "[13], [14], [6]) has led us to investigate the use of distinguishing sequences to design incremental learning algorithms for DFA.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "Distinguishing sequences were first applied to derive the ID online learning algorithm for DFA in [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "In [6] this algorithm has been successfully applied to learning based testing of reactive systems with demonstrated error discovery rates up to 4000 times faster than using non-incremental learning.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Distinguishing sequences were first applied to derive the ID online learning algorithm for DFA in [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "Later an incremental version IID of this algorithm was presented in [11].", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "the IDS algorithm, and its proof of correctness are much simpler, and some technical errors in [11] are also overcome.", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "Distinguishing sequences can be contrasted with the complete consistent table approach to partition construction as represented by the well known online learning algorithm L* of [9].", "startOffset": 178, "endOffset": 181}, {"referenceID": 9, "context": "In [10], an incremental version RPNI2 of the RPNI offline learning algorithm of [15] and [16] is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In [10], an incremental version RPNI2 of the RPNI offline learning algorithm of [15] and [16] is presented.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "In [10], an incremental version RPNI2 of the RPNI offline learning algorithm of [15] and [16] is presented.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "The incremental learning algorithm introduced in [12] requires a lexicographic ordering on the presentation of online queries, which is less flexible than IDS, and indeed inappropriate for software engineering applications.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Our IDS algorithm is an incremental version of the ID learning algorithm introduced in [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "Since this algorithm has been discussed at length in [1], our own presentation can be brief.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "A detailed proof of correctness of ID and an analysis of its complexity can be found in [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "The IID algorithm of [11] also presents a simulation method", "startOffset": 21, "endOffset": 25}, {"referenceID": 0, "context": "Proof: (i) See [1] Theorem 3.", "startOffset": 15, "endOffset": 18}, {"referenceID": 10, "context": "This difference means that the automaton construction algorithm (Algorithm 1, lines 28-37) used for ID can no longer be used for IID, (as asserted in [11]) as we show below.", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "to start learning A using the IID algorithm of [11], we have P0 = Pref(b) = {b, \u03bb} and P \u2032 0 = {b, \u03bb} \u222a {d0} = {b, \u03bb, d0}.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "Therefore the IID algorithm of [11] does not always generate a hypothesis automaton", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "The notion of closed and consistent observation table is given in [9] for L* algorithm.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "If Ei(\u03b1) 6= Ei(\u03b2) as in the above example where E1(bb) = {\u03bb} is not equal to any of E1(d0) = \u2205, E1(\u03bb) = {b} or E1(b) = {\u03bb, b} then the solution in [9] is to move \u03b2 \u2208 T \\ P \u2032 to set P \u2032 and ask more queries to rebuild the congruence that might have been affected by this last addition to set P \u2032.", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "of [8].", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "We chose an algorithm with nearly linear time performance described in [17].", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "This conclusion is further supported in [6] where we have evaluated the IDS algorithm for learning based testing of reactive systems, and shown that it leads to error discovery up to 4000 times faster than using non-incremental", "startOffset": 40, "endOffset": 43}], "year": 2012, "abstractText": "We present a new algorithm IDS for incremental learning of deterministic finite automata (DFA). This algorithm is based on the concept of distinguishing sequences introduced in [1]. We give a rigorous proof that two versions of this learning algorithm correctly learn in the limit. Finally we present an empirical performance analysis that compares these two algorithms, focussing on learning times and different types of learning queries. We conclude that IDS is an efficient algorithm for software engineering applications of automata learning, such as formal software testing and model inference.", "creator": "LaTeX with hyperref package"}}}