{"id": "1206.3714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2012", "title": "How important are Deformable Parts in the Deformable Parts Model?", "abstract": "remissions The main stated contribution pribislav of arsdale the Deformable ferlauto Parts Model (riodinidae DPM) kudzu detector of Felzenszwalb et justin al. (perverted over uncoated the Histogram - erased of - 8,325 Oriented - fuga Gradients nontheistic approach neoplasms of seffner Dalal negotiatior and 1977-79 Triggs) is the use coldblooded of deformable parts. highbush A secondary contribution is the latent discriminative dewdrop learning. inflame Tertiary annuity is arbnori the argentinians use of hikikomori multiple components. A common parsecs belief lawang in ashram the akaba vision community (biti including niet ours, before this study) ismaeel is that 94.49 their arapaima ordering saadia of contributions reflects the performance fireman of kieren detector hcg in practice. mujahidin However, minsmere what chesil we scanned have experimentally justesen found topknot is digambara that the ordering milberg of 6.77 importance might casby actually be the officinarum reverse. bivouacked First, dshk we gr\u00f3dek show stammen that man-at-arms by rohloff increasing the number 12:17 of leung components, and hamzi switching beaconsfield the initialization sawhorses step postmarked from kalmus their dpko aspect - ratio, left - wilkin right comoving flipping feel-good heuristics subdivision to appearance - midgegooroo based 1,624 clustering, considerable improvement norworth in lissy performance third-century is obtained. roissy But more intriguingly, we show that six-volume with sequester these new valore components, the part planai deformations skipjack can mpofu now hnpcc be completely switched goshute off, .253 yet obtaining results dinosauria that ob/gyn are cannington almost on par with jaar the sinotrans original vagif DPM detector. yant Finally, we pujols also masatoshi show initial speusippus results timeliness for using multiple justiciable components on scrabbling a orefice different problem - - reginar scene azaad classification, suggesting reverie that trikora this bridelia idea mealworms might alexandrinus have 1404 wider vou applications skid in wesendonck addition to object wrinkling detection.", "histories": [["v1", "Sat, 16 Jun 2012 23:26:38 GMT  (2049kb,D)", "http://arxiv.org/abs/1206.3714v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["santosh k divvala", "alexei a efros", "martial hebert"], "accepted": false, "id": "1206.3714"}, "pdf": {"name": "1206.3714.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Santosh K. Divvala", "Alexei A. Efros", "Martial Hebert"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Consider the images of category horse in Figure 1 (row1) from the challenging PASCAL VOC dataset [9]. Notice the huge variation in the appearance, shape, pose and camera viewpoint of the different horse instances \u2013 there are left and right-facing horses, horses jumping over a fence in different directions, horses carrying people in different orientations, close-up shots, etc. How can we build a high-performing sliding-window detector that can accommodate the rich diversity amongst the horse instances?\nDeformable Parts Models (DPM) have recently emerged as a useful and popular tool for tackling this challenge. The recent success of the DPM detector of Felzenszwalb et al., [2] has drawn attention from the entire vision community towards this tool, and subsequently it has become an integral component of many classification, segmentation, person layout and action recognition tasks (thus receiving the lifetime achievement award at the PASCAL VOC challenge).\nWhy does the DPM detector [2] perform so well? As the name implies, the main stated contribution of [2] over the HOG detector described in [1] is the idea of deformable parts. Their secondary contribution is latent discriminative\nar X\niv :1\n20 6.\n37 14\nv1 [\ncs .C\nV ]\n1 6\nJu n\n20 12\nlearning. Tertiary is the idea of multiple components (subcategories). The idea behind deformable parts is to represent an object model using a lower-resolution \u2018root\u2019 template, and a set of spatially flexible high-resolution \u2018part\u2019 templates. Each part captures local appearance properties of an object, and the deformations are characterized by links connecting them. Latent discriminative learning involves an iterative procedure that alternates the parameter estimation step between the known variables (e.g., bounding box location of instances) and the unknown i.e., latent variables (e.g., object part locations, instance-component membership). Finally, the idea of subcategories is to segregate object instances into disjoint groups each with a simple (possibly semantically interpretable) theme e.g., frontal vs profile view, or sitting vs standard person, etc, and then learning a separate model per group.\nA common belief in the vision community is that the deformable parts is the most critical contribution, then latent discriminative learning, and then subcategories. Although the ordering somewhat reflects the technical novelty (interestingness) of the corresponding tools and the algorithms involved, is that really the order of importance affecting the performance of the algorithm in practice?\nWhat we have experimentally found from our analysis of the DPM detector is that the ordering might actually be the reverse! First, we show that (i) by increasing the number of subcategories in the mixture model, and (ii) switching from their aspect-ratio, left-right flipping heuristics to appearance-based clustering, considerable improvement in performance is obtained. But more intriguingly, we show that with these new subcategories, the part deformations can be completely turned off, with only minimal performance loss. These observations together highlight that the conceptually simple subcategories idea is indeed an equally important contribution in the DPM detector that can potentially alleviate the need for deformable parts for many practical applications and object classes."}, {"heading": "2 Understanding Subcategories", "text": "In order to deal with significant appearance variations that cannot be tackled by the deformable parts, [2] introduced the notion of multiple components i.e., subcategories into their detector. The first version of their detector [10] only had a single subcategory. The next version [2] had two subcategories that were obtained by splitting the object instances based on aspect ratio heuristic. In the latest version [11], this number was increased to three, with each subcategory comprising of two bilaterally asymmetric i.e., left-right flipped models (effectively resulting in 6 subcategories). The introduction of each additional subcategory has resulted in significant performance gains (e.g., see slide 23 in [12]).\nGiven this observation, what happens if we further increase the number of subcategories in their model? In Section 4, we will see that this does not translate to improvement in performance. This is because the aspect-ratio heuristic does not generalize well to a large number of subcategories, and thus fails to provide a good initialization. Nonetheless, it is possible to explore other ways to\ngenerate subcategories. For example, subcategories for cars can be based either on object pose (e.g., left-facing, right-facing, frontal), or car manufacturer (e.g., Subaru, Ford, Toyota), or some functional attribute (e.g., sports car, utility vehicle, limousine). Figure 1 illustrates a few popular subcategorization schemes for horses.\nWhat is it that the different partitioning schemes are trying to achieve? A closer look at the figures reveals that they are trying to encode the homogeneity in appearance. It is the visual homogeneity of instances within each subcategory that simplifies the learning problem leading to better-performing classifiers (Figure. 2). What this suggests is, instead of using semantics or empirical heuristics, one could directly use appearance-based clustering for generating the subcategories. We use this insight to define new subcategories in the DPM detector, and refer to them as visual subcategories (in contrast to semantic subcategories that involve either human annotations or object-specific heuristics).\nRelated Work The idea of subcategories is inspired by works in machine learning literature [13,14,15,16,17,18] that consider solving a complex (nonlinear) classification problem by using locally linear classification techniques. Several computer vision approaches have explored different strategies for generating subcategories. In [19,5,?], viewpoint annotations associated with instances were used to segregate them into separate left, right, frontal sub-classes. In [3], the size (height) of detection windows was used to cluster them into near and far-scale sub-classes. In [21], co-watch features are used to group videos of a specific category into simpler subcategories. In [4], instances are clustered into poselets using keypoint annotations in the configuration space. In [7], subordinate categories of a basic-level category are constructed using human annotations.\nThe concept of subcategories has also received significant attention in cognitive psychology [22,23]. In the seminal work of [23], the idea of prototypes was\nintroduced. The prototype concept relies on the notion of typicality: its resemblance to the other members of the category and its differences to the members of other categories.\nClosely related are also the recently popular exemplar-based methods [5,24]. While in a global strategy, a single classifier is trained using all instances belonging to a class as positives, in the case of exemplar-based methods, a separate classifier is learned for each individual instance. Although promising results have been demonstrated, exemplar methods are prone to overfitting since too much emphasis is often placed on local irregularities in the data [25]. The global and local learning strategies sit at two extremes of a large spectrum of possible compromises that exploit information from labeled examples. This paper explores intermediate points of this spectrum."}, {"heading": "3 Learning Subcategories", "text": "We first briefly review the key details of using subcategories in the DPM detector, and then explain the details specific to their use in our analysis.\nGiven a set of n labeled instances (e.g., object bounding boxes) D = (< x1, y1 >, . . . , < xn, yn >), with yi \u2208 {\u22121, 1}, the goal is to learn a set of K subcategory classifiers to separate the positive instances from the negative instances, wherein each individual classifier is trained on different subsets of the training data. The assignment of instances to subcategories is modeled as a latent variable z. This binary classification task is formulated as the following (latent SVM) optimization problem that minimizes the trade-off between the l2 regularization term and the hinge loss on the training data [2]:\narg min w\n1\n2 K\u2211 k=1 ||wk||2 + C n\u2211 i=1 i, (1)\nyi.s zi i > 1\u2212 i, i > 0, (2) zi = arg max\nk ski , (3)\nski = wk.\u03c6k(xi) + bk. (4)\nThe parameter C controls the relative weight of the hinge-loss term, wk denotes the separating hyperplane for the kth subclass, and \u03c6k(.) indicates the corresponding feature representation. Since the minimization is semi-convex, the model parameters wk and the latent variable z are learned using an iterative approach [2].\nInitialization As mentioned earlier, a key step for the success of latent subcategory approach is to generate a good initialization of the subcategories. Our initialization method is to warp all the positive instances to a common feature space \u03c6(.), and to perform unsupervised clustering in that space. In our experiments, we found the Kmeans clustering algorithm using Euclidean distance\nfunction to provide a good initialization.\nCalibration One difficulty in merging subcategory classifiers during the testing phase is to ensure that the scores output by individual SVM classifiers (learned with different data distributions) are calibrated appropriately, so as to suppress the influence of noisy ones. We address this problem by transforming the output of each SVM classifier by a sigmoid to yield comparable score distributions [26,27]1(Figure 3). Given a thresholded output score ski for instance i in subcategory k, its calibrated score is defined as\ngki = 1\n1 + exp(Ak.ski +Bk) , (5)\nwhere Ak, Bk are the learned parameters of the following logistic loss function:\narg min Ak,Bk n\u2211 i=1 ti log g k i + (1\u2212 ti) log(1\u2212 gki ), (6)\nti = Or(W k i ,Wi). (7)\nOr(w1, w2) = |w1\u2229w2| |w1\u222aw2| \u2208 [0, 1] indicates the overlap score between two bounding boxes [28], Wi is the ground-truth bounding box for the ith training sample, and W ki indicates the predicted bounding box by the kth subcategory. In our experiments, we found this calibration step to help improve the performance (mean A.P. increase of 0.5% in the detection experiments)."}, {"heading": "4 Experimental Analysis", "text": "We performed our analysis on the PASCAL VOC 2007 comp3 challenge dataset and protocol [9]. Table 1 summarizes our key results. Row1 shows the (baseline) result of the DPM detector [11]. Row2 shows the result obtained by using visual subcategories (with K=15) in the DPM detector. It surpasses the baseline by 2.3% on average across the 20 VOC classes (the mean A.P. improves from 32.3% to 34.6%). Figure 4 shows the top detections obtained per subcategory for horse and train categories. The individual detectors do a good job at localizing instances of their respective subcategories. In Figure 5, the discovered subcategories for symmetric (pottedplant) and deformable (cat) classes are displayed. The subcategories obtained for all of the 20 VOC classes are displayed in the supplementary material.\nRows 3,4 of table 1 show the results obtained by turning off the deformable parts. More specifically, rather than sampling \u2018parts\u2019 from the high-resolution HOG template (sampled at twice the spatial resolution relative to the features captured by the root template) and modeling the deformation amongst them, we directly use all the features from the high-resolution template. This update to the DPM detector results in a simple multi-scale (two-level pyramid) representation with the finer resolution catering towards improved feature localization. We observe that using this two-level pyramid representation for the visual subcategories yields a mean A.P. of 30.9% that is almost on par as the full deformable parts baseline (32.3%). This result becomes intiutive from the observation that instances within each of the subcategory are well-aligned (see figure 5 and supplementary material), and thus simpler models (without deformations) would suffice for training discriminative detectors. For instance, in case of rigid objects such as pottedplants, tvmonitors and trains, the use of part deformations does not offer any improvement over using the multi-scale visual subcategory detector. For a few classes though, such as person and sofa, part deformations seem to be useful, while for some others, such as dining-table and sheep, the multi-scale visual subcategories actually performs better. These observations suggest that, in practice, the relatively simple concept of visual subcategories is as important\n1 Even though the bias term bk is used in equation (1) to make the scores of multiple classifiers comparable, we have found that it is possible for some of the subcategories to be very noisy (specifically when K is large), in which case their output scores cannot be compared directly with other, more reliable ones.\nas the use of deformable parts in the DPM detector.\nComputational Issues. In terms of computational complexity, the two-scale visual subcategory detector (K=15) involves one coarse (root) and one fine resolution template per subcategory, totaling a sum of 30 HOG templates. Whereas the DPM detector has K=6 subcategories each with one root and eight part templates, totaling 54 HOG templates, which need to be convolved at test time. In terms of model learning, the DPM detector has the subcategory, as well as the part deformation parameters (four) as latent variables (for each of the 48 parts), while the visual subcategory detector only has the subcategory label as latent. Therefore it not only requires fewer rounds of latent training than required by the DPM detector (leading to faster convergence), but also is less susceptible to getting stuck in a bad local minima [29]. As emphasized in [2], simpler models are preferable, as they can perform better in practice than rich models, which often suffer from difficulties in training.\nNumber of subcategories. One important parameter is the number of subcategories K. We analyze the influence of K by using different values (K = [3, 6, 9, 12, 15, 20, 25, 50, 100]) for a few classes (\u2018boat\u2019, \u2018dog\u2019, \u2018train\u2019, \u2018tv\u2019) on the validation set. We plot the variation in the performance over different K in figure 6. The performance gradually increases with increasing K, but stabilizes around K=15. We used K = 15 in all the detection experiments.\nInitialization. Proper initialization of clusters is a key requirement for the success of latent variable models. We analyzed the importance of appearancebased initialization by comparing it with the aspect-ratio based initialization of [2]. Simply increasing the number of aspect-ratio based clusters leads to a decrease in performance (mean A.P drops by 1.2%), while for the same number (K=15), appearance-based clustering helps improve the mean A.P. by 2.3%.\nWe noticed minimal variation in the final performance on multiple runs with different Kmeans initialization. We found the (latent) discriminative reclustering step helps in cleaning up any mistakes of the initialization step. (Also we observed that most of the reclustering happens in the first latent update.)"}, {"heading": "5 Application: Visual Subcategories for Scene Classification", "text": "Another scenario where the problem of high intra-class variability is witnessed is scene classification. Scene categories exhibit a large range of visual diversity due to significant variation in camera viewpoint and scene structure. For example, when we refer to the scene category \u2018coast\u2019 (from [30]), it could contain images of rocky shores, sunsets, cloudy beaches, or calm waters. From our analysis of visual subcategories on the object detection dataset, we could expect that their use could also aid in simplifying the learning task for scene classification.\nDataset details. We use the Scene Understanding (SUN) database for our scene classification experiments. The SUN database is a collection of about 100,000 images organized into an exhaustive set of 899 scene categories [30]. For our experiments, we use the subset of 397 well-sampled categories. These 397 fine-grained scene categories are arranged in a 3-level tree: with 397 leaf nodes (subordinate categories) connected to 15 parent nodes at the second level (basic-level categories) that are in turn connected to 3 nodes at the third level (superordinate categories) with the root node at the top. This hierarchy was not considered in the original experimental evaluations in [30] but used as a human organizational tool (in order to facilitate the annotation process e.g., annotators navigate through the three-level hierarchy to arrive at a specific scene type (e.g. \u2018bedroom\u2019) by making relatively easy choices (e.g. \u2018indoor\u2019 versus \u2018outdoor\u2019 at the higher level)).\nOur goal is to train a classifier that can identify images as belonging to one of the 15 basic-level categories.2 We use the images from all the subordinate categories in a basic-level category to build the data corresponding to that basic-level category. The data was split into half training and half testing. The classifiers are all trained in a \u2018one-vs-all\u2019 fashion where instances belonging to a specific category are considered positive examples, and the rest of the instances (belong to all the other categories except the chosen one) serve as negative examples in the training process. While training subcategory classifiers, instances belonging to a particular subcategory (within a category) are treated as positives and the rest of the instances belong to that category are ignored (treated as don\u2019t care examples). The number of subcategories K was set to be 50 (to tackle the larger intra\u2013category diversity in this dataset). We evaluate performance using the A.P. metric as used in PASCAL VOC image classification task [9].\nWe use the GIST feature representation (using the implementation of [31]), that has been well-studied in literature for scene classification experiments (e.g., [8]). We create this descriptor for each image at a 10\u00d7 10 grid resolution where each bin contains that image patch\u2019s average response to steerable filters at 8 ori-\n2 The 15 basic-level categories are \u2018shoppingNdining\u2019, \u2018workplace\u2019, \u2018homeNhotel\u2019, \u2018vehicleInterior\u2019, \u2018sportsNleisure\u2019, \u2018cultural\u2019, \u2018waterNsnow\u2019, \u2018mountainsNdesert\u2019, \u2018forestNfield\u2019, \u2018transportation\u2019, \u2018historicalPlace\u2019, \u2018parks\u2019, \u2018industrial\u2019, \u2018housesNgardens\u2019, \u2018commercialMarkets\u2019.\nentations and 4 scales. We acknowledge that a classification system based on this representation is unlikely to beat the prevailing state-of-the-art. Multiple previous methods have shown that classification performance is significantly improved by the use of BOW-models with densely sampled feature points along with multiple sets of feature descriptors, and the use of spatial pyramids. We chose the simple GIST-based representation for our analysis in this paper as our focus is not to argue in favor of a new classification method, but to show the benefits of the visual subcategory concept using a generic and simple framework."}, {"heading": "5.1 Visual subcategories are semantically interpretable", "text": "Our approach based on visual subcategories achieves a score of 27.1% confidently outperforming the baseline linear SVM 16.9% (see Table in supplementary material for results per category). The utility of our approach becomes more evident as we take a closer look at the classification results of discovered subcategories (see Figure 7). Many of the subcategories discovered correspond to the semantic subordinate categories. For example, the basic-level category \u2018vehicleInterior\u2019 contains clusters for \u2018cockpit\u2019, \u2018bus interior\u2019, \u2018car front seat\u2019, and \u2018car back seat\u2019 that all correspond to the fine-grained categories constituting this basic-level category. Subsequently, this allows deeper reasoning about the image rather than simply assigning the category label. For e.g., instead of simply classifying an image as vehicleInterior, we could now say that it is a \u2018cockpit\u2019 image."}, {"heading": "5.2 Visual subcategories alleviate the need for human supervision", "text": "Given the above result, we seek to quantitatively analyze the benefit of gathering human-annotated subordinate categories over the unsupervisedly discovered visual subcategories. To this end, we ran an experiment where the subcategories in our framework are initialized using the ground-truth subordinate categories. The result obtained using this initialization (mean A.P. of 27.2%) is very similar to that obtained using our unsupervised subcategories of 27.1% (see Table in supplementary material for results per category). This is interesting because it indicates that human supervision for creating the fine-grained subcategories to train a basic-level category classifier may not be of great benefit compared to the unsupervised visual subcategories. Our observations here are also supported by the recent findings in [8], wherein semantic similarity was found to be correlated to visual similiarity at the bottom of the ImageNet [7] hierarchy i.e., when the basic-level category is sliced into extremely small subsets. However to acquire these fine-grained subcategories, one needs to expend significant amount of human annotation effort."}, {"heading": "6 Conclusion", "text": "Contrary to the existing belief that deformable parts is the key contribution for the success of the deformable parts model detector, we have found that\nthe use of subcategories can potentially alleviate their need. The use of visual subcategories not only benefits model learning and performance, but also leads to simpler and more interpretable models. In addition to object detection, their use can also benefit the scene classification task as it can alleviate the need for human supervision in carving the space of fine-grained subordinate categories."}], "references": [{"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. CVPR.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Object detection with discriminatively trained part based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "PAMI", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiresolution models for object detection", "author": ["D. Park", "D. Ramanan", "C. Fowlkes"], "venue": "ECCV.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Detecting people using mutually consistent poselet activations", "author": ["L. Bourdev", "S. Maji", "T. Brox", "J. Malik"], "venue": "ECCV.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "An exemplar model for learning object classes", "author": ["O. Chum", "A. Zisserman"], "venue": "CVPR.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Discriminative mixture-of-templates for viewpoint classification", "author": ["C. Gu", "X. Ren"], "venue": "ECCV.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Visual and semantic similarity in imagenet", "author": ["T. Deselaers", "V. Ferrari"], "venue": "CVPR.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. Felzenszwalb", "D. McAllester", "D. Ramanan"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Adaptive mixture of local experts", "author": ["R. Jacobs", "M. Jordan", "S. Nowlan", "G. Hinton"], "venue": "Neural Computation.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "On locally linear classification by pairwise coupling", "author": ["F. Chen", "C.T. Lu", "A.P. Boedihardjo"], "venue": "ICDM.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Supervised learning with unsupervised output separation", "author": ["N. Japkowicz"], "venue": "ICAISC.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "NIPS.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "On mixtures of linear svms for nonlinear classification", "author": ["Z. Fu", "A. Robles-Kelly"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering inside classes improves performance of linear classifiers", "author": ["D. Fradkin"], "venue": "IEEE International Conference on Tools with Artificial Intelligence.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A statistical method for 3d object detection applied to faces and cars", "author": ["H. Schneiderman", "T. Kanade"], "venue": "Proc. CVPR. Volume 1.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Classification aided two stage localization", "author": ["H. Harzallah", "C. Schmid", "F. Jurie", "A. Gaidon"], "venue": "PASCAL VOC Challenge.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative tag learning on youtube videos with latent sub-tags", "author": ["W. Yang", "G. Toderici"], "venue": "CVPR.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Effects of knowledge and development on subordinate level categorization", "author": ["K.E. Johnson", "A.T. Eilers"], "venue": "Cognitive Dev.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Principles of categorization", "author": ["E. Rosch", "B.B. Lloyd"], "venue": "Cognition and Categorization", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1978}, {"title": "Ensemble of exemplar-svms for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A.A. Efros"], "venue": "ICCV.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Piece-wise model fitting using local data patterns", "author": ["R. Vilalta", "M. Achari", "C. Eick"], "venue": "European Conference on Artificial Intelligence.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J.C. Platt"], "venue": "Advances in Large Margin Classifiers, MIT Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Hidden part models for human action recognition: Probabilistic versus max margin", "author": ["Y. Wang", "G. Mori"], "venue": "PAMI.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting spatial overlap to efficiently compute appearance distances between image windows", "author": ["B. Alexe", "V. Petrescu", "V. Ferrari"], "venue": "NIPS.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Self-paced learning for latent variable models", "author": ["P. Kumar", "B. Packer", "D. Koller"], "venue": "NIPS.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba"], "venue": "CVPR.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV 42", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 1, "context": ", [2] has drawn attention from the entire vision community towards this tool, and subsequently it has become an integral component of many classification, segmentation, person layout and action recognition tasks (thus receiving the lifetime achievement award at the PASCAL VOC challenge).", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Why does the DPM detector [2] perform so well? As the name implies, the main stated contribution of [2] over the HOG detector described in [1] is the idea of deformable parts.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Why does the DPM detector [2] perform so well? As the name implies, the main stated contribution of [2] over the HOG detector described in [1] is the idea of deformable parts.", "startOffset": 100, "endOffset": 103}, {"referenceID": 0, "context": "Why does the DPM detector [2] perform so well? As the name implies, the main stated contribution of [2] over the HOG detector described in [1] is the idea of deformable parts.", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "Monolithic Classifier [1]", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "Aspect-ratio split [2,3] Poselet split [4]", "startOffset": 19, "endOffset": 24}, {"referenceID": 2, "context": "Aspect-ratio split [2,3] Poselet split [4]", "startOffset": 19, "endOffset": 24}, {"referenceID": 3, "context": "Aspect-ratio split [2,3] Poselet split [4]", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "Viewpoint split [5,6] Taxonomy split [7,8]", "startOffset": 16, "endOffset": 21}, {"referenceID": 5, "context": "Viewpoint split [5,6] Taxonomy split [7,8]", "startOffset": 16, "endOffset": 21}, {"referenceID": 6, "context": "Viewpoint split [5,6] Taxonomy split [7,8]", "startOffset": 37, "endOffset": 42}, {"referenceID": 7, "context": "Viewpoint split [5,6] Taxonomy split [7,8]", "startOffset": 37, "endOffset": 42}, {"referenceID": 1, "context": "In order to deal with significant appearance variations that cannot be tackled by the deformable parts, [2] introduced the notion of multiple components i.", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "The first version of their detector [10] only had a single subcategory.", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "The next version [2] had two subcategories that were obtained by splitting the object instances based on aspect ratio heuristic.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "Related Work The idea of subcategories is inspired by works in machine learning literature [13,14,15,16,17,18] that consider solving a complex (nonlinear) classification problem by using locally linear classification techniques.", "startOffset": 91, "endOffset": 110}, {"referenceID": 10, "context": "Related Work The idea of subcategories is inspired by works in machine learning literature [13,14,15,16,17,18] that consider solving a complex (nonlinear) classification problem by using locally linear classification techniques.", "startOffset": 91, "endOffset": 110}, {"referenceID": 11, "context": "Related Work The idea of subcategories is inspired by works in machine learning literature [13,14,15,16,17,18] that consider solving a complex (nonlinear) classification problem by using locally linear classification techniques.", "startOffset": 91, "endOffset": 110}, {"referenceID": 12, "context": "Related Work The idea of subcategories is inspired by works in machine learning literature [13,14,15,16,17,18] that consider solving a complex (nonlinear) classification problem by using locally linear classification techniques.", "startOffset": 91, "endOffset": 110}, {"referenceID": 13, "context": "Related Work The idea of subcategories is inspired by works in machine learning literature [13,14,15,16,17,18] that consider solving a complex (nonlinear) classification problem by using locally linear classification techniques.", "startOffset": 91, "endOffset": 110}, {"referenceID": 14, "context": "Related Work The idea of subcategories is inspired by works in machine learning literature [13,14,15,16,17,18] that consider solving a complex (nonlinear) classification problem by using locally linear classification techniques.", "startOffset": 91, "endOffset": 110}, {"referenceID": 2, "context": "In [3], the size (height) of detection windows was used to cluster them into near and far-scale sub-classes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "In [21], co-watch features are used to group videos of a specific category into simpler subcategories.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In [4], instances are clustered into poselets using keypoint annotations in the configuration space.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], subordinate categories of a basic-level category are constructed using human annotations.", "startOffset": 3, "endOffset": 6}, {"referenceID": 18, "context": "The concept of subcategories has also received significant attention in cognitive psychology [22,23].", "startOffset": 93, "endOffset": 100}, {"referenceID": 19, "context": "The concept of subcategories has also received significant attention in cognitive psychology [22,23].", "startOffset": 93, "endOffset": 100}, {"referenceID": 19, "context": "In the seminal work of [23], the idea of prototypes was", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "Closely related are also the recently popular exemplar-based methods [5,24].", "startOffset": 69, "endOffset": 75}, {"referenceID": 20, "context": "Closely related are also the recently popular exemplar-based methods [5,24].", "startOffset": 69, "endOffset": 75}, {"referenceID": 21, "context": "Although promising results have been demonstrated, exemplar methods are prone to overfitting since too much emphasis is often placed on local irregularities in the data [25].", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "This binary classification task is formulated as the following (latent SVM) optimization problem that minimizes the trade-off between the l2 regularization term and the hinge loss on the training data [2]:", "startOffset": 201, "endOffset": 204}, {"referenceID": 1, "context": "Since the minimization is semi-convex, the model parameters wk and the latent variable z are learned using an iterative approach [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 22, "context": "We address this problem by transforming the output of each SVM classifier by a sigmoid to yield comparable score distributions [26,27](Figure 3).", "startOffset": 127, "endOffset": 134}, {"referenceID": 23, "context": "We address this problem by transforming the output of each SVM classifier by a sigmoid to yield comparable score distributions [26,27](Figure 3).", "startOffset": 127, "endOffset": 134}, {"referenceID": 0, "context": "Or(w1, w2) = |w1\u2229w2| |w1\u222aw2| \u2208 [0, 1] indicates the overlap score between two bounding boxes [28], Wi is the ground-truth bounding box for the ith training sample, and W k i indicates the predicted bounding box by the kth subcategory.", "startOffset": 31, "endOffset": 37}, {"referenceID": 24, "context": "Or(w1, w2) = |w1\u2229w2| |w1\u222aw2| \u2208 [0, 1] indicates the overlap score between two bounding boxes [28], Wi is the ground-truth bounding box for the ith training sample, and W k i indicates the predicted bounding box by the kth subcategory.", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "Therefore it not only requires fewer rounds of latent training than required by the DPM detector (leading to faster convergence), but also is less susceptible to getting stuck in a bad local minima [29].", "startOffset": 198, "endOffset": 202}, {"referenceID": 1, "context": "As emphasized in [2], simpler models are preferable, as they can perform better in practice than rich models, which often suffer from difficulties in training.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "We analyze the influence of K by using different values (K = [3, 6, 9, 12, 15, 20, 25, 50, 100]) for a few classes (\u2018boat\u2019, \u2018dog\u2019, \u2018train\u2019, \u2018tv\u2019) on the validation set.", "startOffset": 61, "endOffset": 95}, {"referenceID": 5, "context": "We analyze the influence of K by using different values (K = [3, 6, 9, 12, 15, 20, 25, 50, 100]) for a few classes (\u2018boat\u2019, \u2018dog\u2019, \u2018train\u2019, \u2018tv\u2019) on the validation set.", "startOffset": 61, "endOffset": 95}, {"referenceID": 11, "context": "We analyze the influence of K by using different values (K = [3, 6, 9, 12, 15, 20, 25, 50, 100]) for a few classes (\u2018boat\u2019, \u2018dog\u2019, \u2018train\u2019, \u2018tv\u2019) on the validation set.", "startOffset": 61, "endOffset": 95}, {"referenceID": 16, "context": "We analyze the influence of K by using different values (K = [3, 6, 9, 12, 15, 20, 25, 50, 100]) for a few classes (\u2018boat\u2019, \u2018dog\u2019, \u2018train\u2019, \u2018tv\u2019) on the validation set.", "startOffset": 61, "endOffset": 95}, {"referenceID": 21, "context": "We analyze the influence of K by using different values (K = [3, 6, 9, 12, 15, 20, 25, 50, 100]) for a few classes (\u2018boat\u2019, \u2018dog\u2019, \u2018train\u2019, \u2018tv\u2019) on the validation set.", "startOffset": 61, "endOffset": 95}, {"referenceID": 1, "context": "We analyzed the importance of appearancebased initialization by comparing it with the aspect-ratio based initialization of [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 26, "context": "For example, when we refer to the scene category \u2018coast\u2019 (from [30]), it could contain images of rocky shores, sunsets, cloudy beaches, or calm waters.", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "The SUN database is a collection of about 100,000 images organized into an exhaustive set of 899 scene categories [30].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "This hierarchy was not considered in the original experimental evaluations in [30] but used as a human organizational tool (in order to facilitate the annotation process e.", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "We use the GIST feature representation (using the implementation of [31]), that has been well-studied in literature for scene classification experiments (e.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "Our observations here are also supported by the recent findings in [8], wherein semantic similarity was found to be correlated to visual similiarity at the bottom of the ImageNet [7] hierarchy i.", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": "Our observations here are also supported by the recent findings in [8], wherein semantic similarity was found to be correlated to visual similiarity at the bottom of the ImageNet [7] hierarchy i.", "startOffset": 179, "endOffset": 182}], "year": 2012, "abstractText": "The main stated contribution of the Deformable Parts Model (DPM) detector of Felzenszwalb et al. (over the Histogram-of-OrientedGradients approach of Dalal and Triggs) is the use of deformable parts. A secondary contribution is the latent discriminative learning. Tertiary is the use of multiple components. A common belief in the vision community (including ours, before this study) is that their ordering of contributions reflects the performance of detector in practice. However, what we have experimentally found is that the ordering of importance might actually be the reverse. First, we show that by increasing the number of components, and switching the initialization step from their aspect-ratio, left-right flipping heuristics to appearance-based clustering, considerable improvement in performance is obtained. But more intriguingly, we show that with these new components, the part deformations can now be completely switched off, yet obtaining results that are almost on par with the original DPM detector. Finally, we also show initial results for using multiple components on a different problem \u2013 scene classification, suggesting that this idea might have wider applications in addition to object detection.", "creator": "LaTeX with hyperref package"}}}