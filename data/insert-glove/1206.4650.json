{"id": "1206.4650", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Analysis of Kernel Mean Matching under Covariate Shift", "abstract": "240-million In real supervised vorbarra learning rijksmonument scenarios, it silvestrii is not sinjin uncommon wriggles that the bergdoll training and test sample d'assisi follow agasse different probability distributions, humongous thus steigerwald rendering the re-wrote necessity lienz to cuspide correct picone the sampling fgf bias. guayule Focusing on focal a zohar particular milsap covariate goodwins shift 1981/82 problem, muzammil we derive high probability mutually confidence b18 bounds omkar for 0.64 the trenched kernel erler mean matching (KMM) jiemin estimator, whose nonresidents convergence rate redescribed turns out millstatt to baralong depend warbonnet on some flashpoint regularity alamoudi measure of ouellette the regression function copulation and maclin also desalegn on sucessor some capacity measure bertani of taekwando the kernel. By comparing ferm\u00edn KMM with ceratosaurus the morissette natural plug - tebogo in tullius estimator, we crossgen establish the finnair superiority of the former hence abadilla provide concrete kvitov\u00e1 evidence / understanding analy to the effectiveness of KMM snubbed under ippy covariate shift.", "histories": [["v1", "Mon, 18 Jun 2012 15:23:37 GMT  (352kb)", "http://arxiv.org/abs/1206.4650v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yaoliang yu", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1206.4650"}, "pdf": {"name": "1206.4650.pdf", "metadata": {"source": "META", "title": "Analysis of Kernel Mean Matching under Covariate Shift", "authors": ["Yao-Liang Yu", "Csaba Szepesv\u00e1ri"], "emails": ["yaoliang@cs.ualberta.ca", "szepesva@cs.ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "In traditional supervised learning, the training and test sample are usually assumed to be drawn from the same probability distribution, however, in practice, this assumption can be easily violated for a variety of reasons, for instance, due to the sampling bias or the nonstationarity of the environment. It is therefore highly desirable to devise algorithms that remain effective under such distribution shifts.\nNeedless to say the problem is hopeless if the training and test distribution share nothing in common. On the other hand, if the two distributions are indeed related in a nontrivial manner, then it is a quite remarkable fact that effective adaptation is possible. Under reasonable assumptions, this problem has been attacked by researchers from statistics (Heckman, 1979; Shimodaira, 2000) and more recently by many researchers from machine learning, see for instance, Zadrozny\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\n(2004); Huang et al. (2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al. (2008); Kanamori et al. (2009). We focus in this paper on the covariate shift assumption which was first formulated by Shimodaira (2000) and has been followed by many others.\nThe assumption that the conditional probability distribution of the output variable given the input variable remains fixed in both the training and test set is termed covariate shift, i.e. the shift happens only for the marginal probability distributions of the covariates. It is well-known that under this setting, the key to correct the sampling bias caused by covariate shift is to estimate the Radon-Nikodym derivative (RND), also called the importance weight or density ratio. A number of methods have been proposed to estimate the RND from finite samples, including kernel mean matching (KMM) (Huang et al., 2007), logistic regression (Bickel et al., 2009), Kullback-Leibler importance estimation (Sugiyama et al., 2008), least-squares (Kanamori et al., 2009), and possibly some others.\nDespite of the many algorithms, our current understanding of covariate shift still seems to be limited. From the analyses we are aware of, such as (Gretton et al., 2009) on the confidence bound of the RND by KMM, (Kanamori et al., 2012) on the convergence rate of the least-squares estimate of the RND, and (Cortes et al., 2008) on the distributional stability, they all assume that certain functions lie in the reproducing kernel Hilbert space (RKHS) induced by some user selected kernel. Since this assumption is impossible to verify (even worse, almost certainly violated in practice), one naturally wonders if we can replace it with something more reasonable. Such goal is pursued in this paper and constitutes our main contribution.\nWe consider the following simple problem: Given the training sample {(Xtri , Y tri )} ntr i=1 and the test sample {Xtei } nte i=1, how well can we estimate the expected value EY te, provided covariate shift has happened? Note that we do not observe the output Y tei on the test sample. This problem, at a first glance, ought to be\n\u201ceasy\u201d, after all we are humbly asking for estimating a scalar. Indeed, under usual assumptions, plus the nearly impossible assumption that the regression function lies in the RKHS, we prove a parametric rate, that is O(n\u2212 1 2 tr +n \u2212 12 te ), for the KMM estimator in Theorem 1 below (to fix ideas, we focus exclusively on KMM in this paper). For a more realistic assumption on the regression function that we borrow from learning theory (Cucker & Zhou, 2007), the convergence rate, proved in Theorem 2, degrades gracefully to O(n \u2212 \u03b8 2(\u03b8+2) tr +n \u2212 \u03b8 2(\u03b8+2)\nte ), where \u03b8 > 0 is a smoothness parameter measuring certain regularity of the regression function (in terms of the kernel). Observe that in the limit when \u03b8 \u2192 \u221e, the regression function eventually lies in the RKHS and we recover the previous parametric rate. In this regard our bound in Theorem 2 is asymptotically optimal. A very nice feature we discovered for the KMM estimator is that it does not require knowledge of the smoothness parameter \u03b8, thus, it is in some sense adaptive.\nOn the negative side, we show that, if the chosen kernel does not interact very well with the unknown regression function, the convergence rate of the KMM estimator could be exceedingly slow, roughly O(log\u2212s ntr\u00b7ntentr+nte ), where s > 0 again measures certain regularity of the regression function. This unfortunate result should draw attention to the importance of selecting which kernel to be used in practice. A thorough comparison between the KMM estimator and the natural plug-in estimator, conducted in Section 4.3, also reveals the superiority of the former.\nWe point out that our results are far from giving a complete picture even for the simple problem we consider here, for instance, it is unclear to us whether or not the rate in Theorem 2 can be improved, eventually, to the parametric rate in Theorem 1? Nevertheless, we hope that our paper will convince others about the importance and possibility to work with more reasonable assumptions under covariate shift, and as an example, suggest relevant tools which can be used to achieve that goal."}, {"heading": "2. Preliminaries", "text": "In this section we formally state the covariate shift problem under our consideration, followed by some relevant discussions."}, {"heading": "2.1. Problem Setup", "text": "Consider the familiar supervised learning setting, where we are given independent and identically distributed (i.i.d.) training samples {(Xtri , Y tri )} ntr i=1 from\nthe joint (Borel) probability measure Ptr(dx, dy) on the (topological) domain X \u00d7 Y, and i.i.d. test samples {Xtei } nte i=1 from the joint probability measure Pte(dx, dy) on the same domain. Notice that we do not observe the output Y tei on the test sample, and more importantly, we do not necessarily assume that the training and test sample are drawn from the same probability measure. The problem we consider in this paper is to estimate the expected value EY te from the training sample {(Xtri , Y tri )} ntr i=1 and the test sample {Xtei } nte i=1. In particular, we would like to determine how fast, say, the 1\u2212 \u03b4 confidence interval for our estimate shrinks to 0 when the sample sizes ntr and nte increase to infinity.\nThis problem, in its full generality, cannot be solved simply because the training probability measure can be completely irrelevant to the test probability measure that we are interested in. However, if the two probability measures are indeed related in a nontrivial way, our problem becomes solvable. One particular example, which we focus on hereafter, is known in the literature as covariate shift (Shimodaira, 2000):\nAssumption 1 (Covariate shift assumption)\nPtr(dy|x) = Pte(dy|x). (1)\nWe use the same notation for the joint, conditional and marginal probability measures, which should cause no confusion as the arguments would reveal which measure is being referred to. Note that the equality P(dx,dy) = P(dy|x)\u00b7P(dx) holds from the definition of the conditional probability measure, whose existence can be confirmed under very mild assumptions.\nUnder the covariate shift assumption, the difficulty of our problem, of course, lies entirely on the potential mismatch between the marginal probability measures Ptr(dx) and Pte(dx). But the Bayes rule already suggests a straightforward approach:\nPte(dx, dy) = Pte(dy|x)\u00b7Pte(dx) = Ptr(dx, dy)\u00b7 dPte dPtr (x),\nwhere the three quantities on the right-hand side can all be estimated from the given samples. However, in order for the above equation to make sense, we need\nAssumption 2 (Continuity assumption) The Radon-Nikodym derivative \u03b2(x) := dPtedPtr (x) is welldefined and bounded from above by B <\u221e. Note that B \u2265 1 due to the normalization constraint\u222b X \u03b2(x)Ptr(dx) = 1. The Radon-Nikodym derivative (RND) is also called the importance weight or the density ratio in the literature. Evidently, if \u03b2(x) is\nnot well-defined, i.e., there exists some measurable set A such that Pte(A) > 0 and Ptr(A) = 0, then in general we cannot infer Pte(dx, dy) from merely Ptr(dx),Pte(dx) and Ptr(dy|x), even under the covariate shift assumption. The bounded from above assumption is more artificial. Recently, in a different setting, (Cortes et al., 2010) managed to replace this assumption with a bounded second moment assumption, at the expense of sacrificing the rate a bit. For us, since the domain X will be assumed to be compact, the bounded from above assumption is not too restrictive (automatically holds when \u03b2(x) is, say, continuous).\nOnce we have the RND \u03b2(x), it becomes easy to correct the sampling bias caused by the mismatch between Ptr(dx) and Pte(dx), hence solving our problem. Formally, let\nm(x) := \u222b Y y Pte(dy|x) (2)\nbe the regression function, then EY te = \u222b X m(x) Pte(dx) = \u222b X m(x)\u03b2(x) Ptr(dx).\nBy the i.i.d. assumption, a reasonable estimator for EY te would then be 1ntr \u2211ntr i=1 \u03b2(X tr i ) \u00b7 Y tri . Hence, similarly to most publications on covariate shift, our problem boils down to estimating the RND \u03b2(x)."}, {"heading": "2.2. A Naive Estimator?", "text": "An immediate solution for estimating \u03b2(x) is to estimate the two marginal measures from the training sample {Xtri } and the test sample {Xtei }, respectively. For instance, if we know a third (Borel) measure Q(dx) (usually the Lebesgue measure on Rd) such that both dPtedQ (x) and dPtr dQ (x) exist, we can employ standard density estimators to estimate them and then set \u03b2\u0302(x) = dPtedQ (x)/ dPtr dQ (x). However, this naive approach is known to be inferior since density estimation in high dimensions is hard, and moreover, small estimation error in dPtrdQ (x) could change \u03b2\u0302(x) significantly. To our knowledge, there is little theoretical analysis on this seemingly naive approach."}, {"heading": "2.3. A Better Estimator?", "text": "It seems more appealing to directly estimate the RND \u03b2(x). Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009). From the many references, we single out the kernel mean matching (KMM) algorithm, first proposed by Huang et al. (2007) and is also the basis of this paper.\nKMM tries to match the mean elements in a feature space induced by a kernel k(\u00b7, \u00b7) on the domain X \u00d7X :\nmin \u03b2\u0302i\n{ L\u0302(\u03b2\u0302) := \u2225\u2225\u2225\u2225\u2225 1ntr ntr\u2211 i=1 \u03b2\u0302i\u03a6(X tr i )\u2212 1 nte nte\u2211 i=1 \u03a6(Xtei ) \u2225\u2225\u2225\u2225\u2225 H } s.t. 0 \u2264 \u03b2\u0302i \u2264 B, (3)\nwhere \u03a6 : X 7\u2192 H denotes the canonical feature map, H is the reproducing kernel Hilbert space1 (RKHS) induced by the kernel k and \u2016\u00b7\u2016H stands for the norm in H. To simplify later analysis, we have chosen to omit the normalization constraint\n\u2223\u2223\u2223 1ntr \u2211ntri=1 \u03b2\u0302i \u2212 1\u2223\u2223\u2223 \u2264 , where is a small positive number, mainly to reflect the fluctuation caused by random samples. It is not hard to verify that (3) is in fact an instance of quadratic programming, hence can be efficiently solved. More details can be found in the paper of Gretton et al. (2009).\nA finite sample 1\u2212\u03b4 confidence bound for L\u0302(\u03b2) (similar as (10) below) is established in Gretton et al. (2009). This bound is further transferred into a confidence bound for the generalization error of some family of loss minimization algorithms in Cortes et al. (2008), under the notion of distributional stability. However, neither results can provide a direct answer to our problem: a finite sample confidence bound on the estimate of EY te."}, {"heading": "2.4. Plug-in Estimator", "text": "Another natural approach is to estimate the regression function from the training sample and then plug into the test set. We postpone the discussion and comparison with respect to this estimator until section 4.3."}, {"heading": "3. Motivation", "text": "We motivate the relevance of our problem in this section.\nSuppose we have an ensemble of classifiers, say, {fj}Nj=1, all trained on the training sample {(Xtri , Y tri )} ntr i=1. A useful task is to compare, hence rank, the classifiers by their generalization errors. This is usually done by assessing the classifiers on some hold out test sample {(Xtei , Y tei )} nte i=1. It is not uncommon that the test sample is drawn from some different probability measure than the training sample, i.e. covariate shift has happened. Since it could be too costly to re-train the classifiers when the test sample is available, we nevertheless still like to\n1A thorough background on the theory of reproducing kernels can be found in Aronszajn (1950).\nhave a principled way to rank the classifiers.\nLet `(\u00b7, \u00b7) be the user\u2019s favourite loss function, and set Ztrij = `(fj(X tr i ), Y tr i ), Z te ij = `(fj(X te i ), Y te i ), then we can use the empirical average of {Zteij } nte i=1 to estimate the generalization error, that is E(Zteij ), of classifier fj . But what if we do not have access to Y te i hence consequently Zteij ? Can we still accomplish the ranking job?\nThe answer is yes, and it is precisely the covariate shift problem under our consideration. To see that, consider the pair {Xtri , Ztrij} ntr i=1 and {Xtei } nte i=1. Under the covariate shift assumption, that is Ptr(dy|x) = Pte(dy|x), it is not hard to see that Ptr(dz|x) = Pte(dz|x), hence the covariate shift assumption holds for the ranking problem, therefore the confidence bounds derived in the next section provide an effective solution.\nWe do not report numerical experiments in this paper for two reasons: 1). Our main interest is on theoretical analysis; 2). Exhaustive experimental results on KMM can already be found in Gretton et al. (2009)."}, {"heading": "4. Theoretical Analysis", "text": "This section contains our main contribution, i.e., a theoretical analysis of the KMM estimator for EY te."}, {"heading": "4.1. The population version", "text": "Let us first take a look at the population version of KMM2, which is much easier to analyze and provides valuable insights:\n\u03b2\u0302\u2217 \u2208 arg min \u03b2\u0302 \u2225\u2225\u2225\u2225\u222b X \u03a6(x)\u03b2\u0302(x)Ptr(dx)\u2212 \u222b X \u03a6(x)Pte(dx) \u2225\u2225\u2225\u2225 H\ns.t. 0 \u2264 \u03b2\u0302 \u2264 B, \u222b X \u03b2\u0302(x)Ptr(dx) = 1.\nThe minimum value is 0 since the true RND \u03b2(x) is apparently feasible, hence at optimum we always have\u222b\nX \u03a6(x)\u03b2\u0302\u2217(x)Ptr(dx) = \u222b X \u03a6(x)Pte(dx). (4)\nThe question is whether the natural estimator\u222b X\u00d7Y \u03b2\u0302\n\u2217(x)y Ptr(dx, dy) is consistent? In other words, is\u222b X m(x)\u03b2\u0302\u2217(x)Ptr(dx) ? = EY te = \u222b X m(x)\u03b2(x)Ptr(dx),\n(5)\n2All Hilbert space valued integrals in this paper are to be understood as the Bochner integral (Yosida, 1980).\nwhere recall that m(x) is the regression function defined in (2) and \u03b2(x) is the true RND.\nThe equality in (5) indeed holds under at least two conditions (respectively). First, if the regression function m \u2208 H, then taking inner products with m in (4) and applying the reproducing property we get (5). Second, if the kernel k is characteristic (Sriperumbudur et al., 2010), meaning that the map \u222b X \u03a6(x)P(dx) from the space of probability measures to the RKHS H is injective, then we conclude \u03b2\u0302\u2217 = \u03b2 from (4) hence follows (5).\nThe above two cases suggest the possibility of solving our problem by KMM. Of course, in reality one only has finite samples from the underlying probability measures, thus calls for a thorough study of the empirical KMM, i.e. (3). Interestingly, our analysis reveals that in the first case above, we indeed can have a parametric rate while in the second case the rate becomes nonparametric, hence inferior (but does not seem to rely on the characteristic property of the kernel)."}, {"heading": "4.2. The empirical version", "text": "In this subsection we analyze KMM in details. The following assumption will be needed:\nAssumption 3 (Compactness assumption) X is a compact metrizable space, Y \u2286 [0, 1], and the kernel k is continuous, whence \u2016k\u2016\u221e \u2264 C2 <\u221e.\nWe use \u2016\u00b7\u2016\u221e for the supremum norm. Under the above assumption, the feature map \u03a6 is continuous hence measurable (with respect to the Borel \u03c3-fields), and the RKHS is separable, therefore the Bochner integrals in the previous subsection are well-defined. Moreover, the conditional probability measure indeed exists under our assumption.\nWe are now ready to derive a finite sample confidence bound for our estimate | 1ntr \u2211ntr i=1 \u03b2\u0302iY tr i \u2212EY te|, where \u03b2\u0302i is a minimizer of (3). We start by splitting the sum:\n1\nntr ntr\u2211 i=1 \u03b2\u0302iY tr i \u2212 EY te =\n1\nntr ntr\u2211 i=1 \u03b2\u0302i(Y tr i \u2212m(Xtri ))\n+ 1\nntr ntr\u2211 i=1 (\u03b2\u0302i \u2212 \u03b2i)(m(Xtri )\u2212 h(Xtri ))\n+ 1\nntr ntr\u2211 i=1 (\u03b2\u0302i \u2212 \u03b2i)h(Xtri )\n+ 1\nntr ntr\u2211 i=1 \u03b2im(X tr i )\u2212 EY te, (6)\nwhere \u03b2i := \u03b2(X tr i ) and h \u2208 H is to be specified later.\nWe bound each term individually. For the last term in (6), we can apply Hoeffding\u2019s inequality (Hoeffding, 1963) to conclude that with probability at least 1\u2212 \u03b4,\u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 \u03b2im(X tr i )\u2212 EY te \u2223\u2223\u2223\u2223\u2223 \u2264 B \u221a 1 2ntr log 2 \u03b4 . (7)\nThe first term in (6) can be bounded similarly. Conditioned on {Xtri } and {Xtei }, we apply again Hoeffding\u2019s inequality. Note that \u03b2\u0302i(Y tr i \u2212 m(Xtri )) \u2208 [\u2212\u03b2\u0302im(Xtri ), \u03b2\u0302i(1 \u2212m(Xtri ))], therefore its range is of size \u03b2\u0302i. With probability at least 1\u2212 \u03b4,\u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 \u03b2\u0302i(Y tr i \u2212m(Xtri )) \u2223\u2223\u2223\u2223\u2223 \u2264 \u221a\u221a\u221a\u221a 1 ntr ntr\u2211 i=1 \u03b2\u03022i \u00b7 \u221a 1 2ntr log 2 \u03b4\n\u2264 B \u221a 1\n2ntr log\n2 \u03b4 . (8)\nThe second and third terms in (6) require more work. Consider first the third term:\u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 (\u03b2\u0302i \u2212 \u03b2i)h(Xtri ) \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 (\u03b2\u0302i \u2212 \u03b2i)\u3008h,\u03a6(Xtri )\u3009 \u2223\u2223\u2223\u2223\u2223 \u2264 \u2016h\u2016H \u00b7\n\u2225\u2225\u2225\u2225\u2225 1ntr ntr\u2211 i=1 (\u03b2\u0302i \u2212 \u03b2i)\u03a6(Xtri ) \u2225\u2225\u2225\u2225\u2225 H\n\u2264 \u2016h\u2016H \u00b7 [L\u0302(\u03b2\u0302) + L\u0302(\u03b21:ntr)] \u2264 \u2016h\u2016H \u00b7 2L\u0302(\u03b21:ntr), (9)\nwhere \u03b21:ntr denotes the restriction of \u03b2 to the training sample {Xtri }, L\u0302(\u00b7) is defined in (3), and the equality is because h \u2208 H (and the reproducing property of the canonical feature map), the first inequality is by the Cauchy-Schwarz inequality, the second inequality is due to the triangle inequality, and the last inequality is by the optimality of \u03b2\u0302 and the feasibility of \u03b21:ntr in problem (3). Next, we bound L\u0302(\u03b21:ntr):\nL\u0302(\u03b21:ntr) := \u2225\u2225\u2225\u2225\u2225 1ntr ntr\u2211 i=1 \u03b2i\u03a6(X tr i )\u2212 1 nte nte\u2211 i=1 \u03a6(Xtei ) \u2225\u2225\u2225\u2225\u2225 H\n\u2264 C \u221a 2 ( B2\nntr +\n1\nnte\n) log 2\n\u03b4 (10)\nwith probability at least 1 \u2212 \u03b4, where the inequality follows from the Hilbert space valued Hoeffding inequality in (Pinelis, 1994, Theorem 3.5). Note that Pinelis proved his inequality for martingales in any 2-smooth separable Banach space (Hilbert spaces are bona fide 2-smooth). We remark that another way, see for instance (Gretton et al., 2009, Lemma 1.5), is to use McDiarmid\u2019s inequality to bound L\u0302(\u03b21:ntr) by its\nexpectation, and then bound the expectation straightforwardly. In general, Pinelis\u2019s inequality will lead to (slightly) tighter bounds due to its known optimality (in certain sense).\nFinally, we come to the second term left in (6), which is roughly the approximation error in learning theory (Cucker & Zhou, 2007). Note that all confidence bounds we have derived so far shrink at the parametric rate O( \u221a 1/ntr + 1/nte). However, from here on we will have to tolerate nonparametric rates. Since we are going to apply different approximation error bounds to the second term in (6), it seems more convenient to collect the results separately. We start with an encouraging result:\nTheorem 1 Under Assumptions 1-3, if the regression function m \u2208 H (the RKHS induced by the kernel k), then with probability at least3 1\u2212 \u03b4,\u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 \u03b2\u0302iY tr i \u2212 EY te \u2223\u2223\u2223\u2223\u2223 \u2264M \u00b7 \u221a 2 ( B2 ntr + 1 nte ) log 6 \u03b4 ,\nwhere M := 1+2C\u2016m\u2016H and \u03b2\u0302i is computed from (3).\nProof: By assumption, setting h = m zeros out the second term in (6). A standard union bound combining (7)-(10) completes the proof (and we simplified the bound by slightly worsening the constant).\nThe confidence bound shrinks at the parametric rate, although the constant depends on \u2016m\u2016H, which in general is not computable, but can be estimated from the training sample {(Xtri , Y tri )} at a rate worse than parametric. Since this estimate inevitably introduces other uncomputable quantities, we omit the relevant discussion. On the other hand, our bound suggests that if a priori information about m is indeed available, one should choose a kernel that minimizes its induced norm on m.\nThe case when m 6\u2208 H is less satisfactory, despite of its practicality. We point out that a denseness argument cannot resolve this difficulty. To be more precise, let us assume for a moment m \u2208 C (X ) (the space of continuous functions on X ) and k be a universal kernel (Steinwart, 2002), meaning that the RKHS induced by k is dense in (C (X ), \u2016 \u00b7 \u2016\u221e). By the assumed universal property of the kernel, there exists suitable h \u2208 H that makes the second term in (6) arbitrarily small (in fact, can be made vanishing), however, on the other hand, recall that the bound (9) on the third term in (6) depends on \u2016h\u2016H hence could blow up. If we trade\n3Throughout this paper, the confidence parameter \u03b4 is always taken arbitrarily in (0, 1).\noff the two terms appropriately, we might get a rate that is acceptable (but worse than parametric). The next theorem concretizes this idea.\nTheorem 2 Under Assumptions 1-3, if A2(m,R) := inf\n\u2016g\u2016H\u2264R \u2016m \u2212 g\u2016L 2Ptr \u2264 C2R \u2212\u03b8/2 for some \u03b8 > 0 and constant C2 \u2265 0, then with probability at least 1\u2212 \u03b4,\u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 \u03b2\u0302iY tr i \u2212 EY te\n\u2223\u2223\u2223\u2223\u2223 \u2264 B \u221a 9\n2ntr log\n8 \u03b4 + C\u03b8(BC2) 2 \u03b8+2D \u03b8 \u03b8+2 2 ,\nwhere D2 := 2C\n\u221a 2 ( B2\nntr + 1nte ) log 8\u03b4+BC \u221a 1 2ntr log 8\u03b4 ,\nC\u03b8 := (1 + 2/\u03b8) ( \u03b8 2 ) 2 \u03b8+2 and \u03b2\u0302i is computed from (3).\nProof: By the triangle inequality,\u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 (\u03b2\u0302i \u2212 \u03b2i)(m(Xtri )\u2212 h(Xtri )) \u2223\u2223\u2223\u2223\u2223 \u2264 B \u00b7 1\nntr ntr\u2211 i=1 |m(Xtri )\u2212 h(Xtri )|.\nNot surprisingly, we apply yet again Hoeffding\u2019s inequality to relate the last term above to its expectation. Since\n\u2016m\u2212 h\u2016\u221e \u2264 1 + \u2016 \u3008h,\u03a6(\u00b7)\u3009 \u2016\u221e \u2264 1 + C\u2016h\u2016H,\nwe have with probability at least 1\u2212 \u03b4,\n1\nntr ntr\u2211 i=1 |m(Xtri )\u2212h(Xtri )|\u2264(1+CR) \u221a 1 2ntr log 2 \u03b4 +A2(m,R),\nwhere R := \u2016h\u2016H . Combining this bound with (7)- (10) and applying our assumption on A2(m,R):\u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 (\u03b2\u0302i \u2212 \u03b2i)(m(Xtri )\u2212 h(Xtri ))\n\u2223\u2223\u2223\u2223\u2223 \u2264 B \u221a 2\nntr log\n8 \u03b4 + 2RC\n\u221a 2 ( B2\nntr +\n1\nnte\n) log 8\n\u03b4\n+BC2R \u2212\u03b8/2 +B(1 + CR)\n\u221a 1\n2ntr log\n8 \u03b4 .\nSetting R = ( \u03b8BC2 2D2 ) 2 \u03b8+2 completes the proof.\nIn Theorem 2 we do not even assume m \u2208 C (X ); all we need is m \u2208 L 2Ptr , the space of Ptr(dx) square integrable functions. The latter condition always holds\nsince 0 \u2264 m \u2264 1 by Assumption 3. The quantity A2(m,R) is called the approximation error in learning theory and its polynomial decay is known to be (almost) equivalent to m \u2208 Range(T \u03b8 2\u03b8+4\nk ), see for instance Theorem 4.1 of Cucker & Zhou (2007). Here Tk is the integral operator (Tkf)(x\u2032) =\u222b X k(x\n\u2032, x)f(x)Ptr(dx) on L 2Ptr . The smoothness parameter \u03b8 > 0 measures the regularity of the regression function, and as it increases, the range space of T \u03b8 2\u03b8+4\nk becomes smaller, hence our decay assumption on A2(m,R) becomes more stringent. Note that the exponent \u03b82\u03b8+4 is necessarily smaller than 1/2 (but approaches 1/2 when \u03b8 \u2192\u221e) because by Mercer\u2019s theorem T 1 2\nk is onto H (in which case the range assumption would bring us back to Theorem 1).\nTheorem 2 shows that the confidence bound now shrinks at a slower rate, roughly O(n \u2212 \u03b8 2(\u03b8+2)\ntr +\nn \u2212 \u03b8 2(\u03b8+2)\nte ), which, as \u03b8 \u2192\u221e, approaches the parametric rate O(n\u2212 1 2 tr + n \u2212 12 te ) derived in Theorem 1 where we assume m \u2208 H. We point out that the source of this slower rate comes from the irregular nature of the regression function (in the eye of the kernel k).\nThe polynomial decay assumption on A2(m,R) is not always satisfied, for instance, it is shown in Theorem 6.2 of Cucker & Zhou (2007) that for C\u221e (indefinite times differentiable) kernels (such as the popular Gaussian kernel), polynomial decay implies that the regression function m \u2208 C\u221e(X ) (under mild assumptions on X and Ptr(dx)). Therefore, as long as one works with smooth kernels but nonsmooth regression functions, the approximation error has to decay logarithmically slowly. We give a logarithmic bound for such cases.\nTheorem 3 Under Assumptions 1-3, if A\u221e(m,R) := inf\n\u2016g\u2016H\u2264R \u2016m\u2212 g\u2016\u221e \u2264 C\u221e(logR)\u2212s for some s > 0 and constant C\u221e \u2265 0 (assuming R \u2265 1), then (for ntr and nte larger than some constant),\u2223\u2223\u2223\u2223\u2223 1ntr ntr\u2211 i=1 \u03b2\u0302iY tr i \u2212 EY te \u2223\u2223\u2223\u2223\u2223 \u2264 ( 1 + 1 s )s BC\u221e ( log sBC\u221e D\u221e\n)\u2212s +B \u221a 2\nntr log\n6 \u03b4 + (sBC\u221e) s s+1D 1 s+1 \u221e\nholds with probability at least 1 \u2212 \u03b4, where D\u221e =\n2C \u221a 2 ( B2\nntr + 1nte ) log 6\u03b4 and \u03b2\u0302i is computed from (3).\nThe proof is similar as that of Theorem 2 except that we set R = ( sBC\u221eD\u221e ) s s+1 .\nTheorem 3 shows that in such unfavourable cases, the confidence bound shrinks at an exceedingly slow\nrate, roughly, O(log\u2212s ntr\u00b7ntentr+nte ). The reason, of course, is due to the slow decay of the approximation error A\u221e(m,R). It is proved in Theorem 6.1 of Cucker & Zhou (2007) that for the Gaussian kernel k(x\u2032, x) = exp(\u2212\u2016x \u2212 x\u2032\u201622/\u03c32), if X \u2286 Rd has smooth boundary and the regression function m \u2208 Hs(X ) with index s > d/2, then the logarithmic decay assumed in Theorem 3 holds. Here Hs(X ) is the Sobolev space (the completion of C\u221e(X ) under the inner product \u3008f, g\u3009s := \u222b X \u2211 |\u03b1|\u2264s d\u03b1f dx d\u03b1g dx , assuming s \u2208 N). Similar bounds also hold for the inverse multiquadrics kernel k(x\u2032, x) = (c2 + \u2016x \u2212 x\u2032\u201622)\u2212\u03b1 with \u03b1 > 0. We remark that in this regard Theorem 3 disrespects the popular Gaussian kernel used ubiquitously in practice and should draw the attention of researchers."}, {"heading": "4.3. Discussion", "text": "It seems worthwhile to devote a subsection to discussing a very natural question that the reader might already have: why not estimate the regression function m on the training set and then plug into the test set, after all m does not change under the covariate shift assumption? Algorithmically, this is perfectly doable, perhaps conceptually even simpler since the algorithm does not need to see the test data beforehand. We note that estimating the regression function from i.i.d. samples has been well studied in the learning theory literature, see for instance, Chapter 8 of Cucker & Zhou (2007) and the many references therein.\nThe difficulty, though, lies in the appropriate error metric on the estimate. Recall that when estimating the regression function from i.i.d. training samples, one usually measures the progress (i.e. the discrepancy between the estimate m\u0302 and m) by the L 2 norm under the training probability measure Ptr(dx), while what we really want is a confidence bound on the term\u2223\u2223\u2223\u2223\u2223 1nte nte\u2211 i=1 m\u0302(Xtei )\u2212 EY te \u2223\u2223\u2223\u2223\u2223 . (11)\nSince Ptr 6= Pte, there is evidently a probability measure mismatch between the bound we have from estimating m and the true interested quantity. Indeed, conditioned on the training sample {(Xtri , Y tri )}, using the triangle inequality we can bound (11) by :\u2223\u2223\u2223\u2223\u2223 1nte nte\u2211 i=1 m\u0302(Xtei )\u2212 \u222b m\u0302(x)Pte(dx)\n\u2223\u2223\u2223\u2223\u2223+ \u2016m\u0302\u2212m\u2016L 2Pte . The first term above can be bounded again through Hoeffding\u2019s inequality, while the second term is close to what we usually have from estimating m: the only difference being that the L 2\nnorm is now under the test probability measure Pte(dx). Fortunately, since the norm of the identity map id : ([\u22121, 1]X , \u2016 \u00b7 \u2016L 2Ptr ) 7\u2192 ([\u22121, 1]\nX , \u2016 \u00b7 \u2016L 2Pte ) is bounded by \u221a B (see Assumption 2), we can deduce a bound for (11) based upon results from estimating m, though less appealingly, a much looser bound than the one given in Theorem 2. We record such a result for the purpose of comparison:\nTheorem 4 Under Assumptions 1-3, if the regression function m \u2208 Range(T \u03b8 2\u03b8+4\nk ) for some \u03b8 > 0, then with probability at least 1\u2212 \u03b4,\u2223\u2223\u2223\u2223\u2223 1nte nte\u2211 i=1 m\u0302(Y tei )\u2212 EY te \u2223\u2223\u2223\u2223\u2223 \u2264 \u221a 1 2nte log 4 \u03b4 + \u221a BC1n \u2212 3\u03b812\u03b8+16 tr ,\nwhere C1 is some constant that does not depend on ntr, nte, and m\u0302 is the (regularized least-squares) estimate of m in Smale & Zhou (2007).\nThe theorem follows from the bound on \u2016m\u0302\u2212m\u2016L 2Ptr in Corollary 3.2 of Sun & Wu (2009), which is an improvement over Smale & Zhou (2007).\nCarefully comparing the current theorem with Theorem 2, we observe: 1). Theorem 4, which is based on the regularized least-squares estimate of the regression function, needs to know in advance the parameter \u03b8 (in order to tune the regularization constant) while Theorem 2, derived for KMM, does not require any such information, hence in some sense KMM is \u201cadaptive\u201d; 2). Theorem 4 has much worse dependence on the training sample size ntr; it does not recover the parametric rate even when the smoothness parameter \u03b8 goes to \u221e (we get n\u22121/4tr , instead of n \u22121/2 tr ). On the other hand, Theorem 4 has better dependence on the test sample size nte, which is, however, probably not so important since usually one has much more test samples than training samples because the lack of labels make the former much easier to acquire; 3). Theorem 4 seems to have better dependence on the parameter B; 4). Given the fact that KMM utilizes both the training data and the test data in the learning phase, it is not entirely a surprise that KMM wins in terms of convergence rate, nevertheless, we find it quite stunning that by sacrificing the rate slightly on nte, KMM is able to improve the rate on ntr so significantly."}, {"heading": "5. Conclusion", "text": "For estimating the expected value of the output on the test set where covariate shift has happened, we have derived high probability confidence bounds for the kernel mean matching (KMM) estimator, which\nconverges, roughly O(n\u2212 1 2 tr + n \u2212 12 te ) when the regression function lies in the RKHS, and more generally O(n \u2212 \u03b8 2(\u03b8+2) tr + n \u2212 \u03b8 2(\u03b8+2)\nte ) when the regression function exhibits certain regularity measured by \u03b8. An extremely slow rate, roughly O(log\u2212s ntr\u00b7ntentr+nte ), is also provided, calling attention of choosing the right kernel. From the comparison of the bounds, KMM proves to be much more superior than the plug-in estimator hence provides concrete evidence/understanding to the effectiveness of KMM under covariate shift.\nAlthough it is unclear to us if it is possible to avoid approximating the regression function, we suspect the bound in Theorem 2 is in some sense optimal and we are currently investigating it. We also plan to generalize our results to the least-squares estimation problem."}, {"heading": "Acknowledgements", "text": "This work was supported by Alberta Innovates Technology Futures and NSERC."}], "references": [{"title": "Analysis of representations for domain adaptation", "author": ["Ben-David", "Shai", "Blitzer", "John", "Crammer", "Koby", "Pereira", "Fernando"], "venue": "In NIPS, pp", "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "Discriminative learning under covariate", "author": ["Bickel", "Steffen", "Br\u00fcckner", "Michael", "Scheffer", "Tobias"], "venue": "shift. JMLR,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Learning bounds for domain adaptation", "author": ["Blitzer", "John", "Crammer", "Koby", "Kulesza", "Alex", "Pereira", "Fernando", "Wortman", "Jennifer"], "venue": "In NIPS, pp", "citeRegEx": "Blitzer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2008}, {"title": "Sample selection bias correction theory", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Riley", "Michael", "Rostamizadeh", "Afshin"], "venue": "In ALT, pp", "citeRegEx": "Cortes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2008}, {"title": "Learning bounds for importance weighting", "author": ["Cortes", "Corinna", "Mansour", "Yishay", "Mohri", "Mehryar"], "venue": "In NIPS, pp", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Learning theory: an approximation theory viewpoint", "author": ["Cucker", "Felipe", "Zhou", "Ding-Xuan"], "venue": null, "citeRegEx": "Cucker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cucker et al\\.", "year": 2007}, {"title": "Covariate Shift by Kernel Mean Matching, pp. 131\u2013160", "author": ["Gretton", "Arthur", "Smola", "Alexander J", "Huang", "Jiayuan", "Schmittfull", "Marcel", "Borgwardt", "Karsten M", "Sch\u00f6lkopf", "Bernhard"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "Sample selection bias as a specification", "author": ["Heckman", "James J"], "venue": "error. Econometrica,", "citeRegEx": "Heckman and J.,? \\Q1979\\E", "shortCiteRegEx": "Heckman and J.", "year": 1979}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["Huang", "Jiayuan", "Smola", "Alexander J", "Gretton", "Arthur", "Borgwardt", "Karsten M", "Sch\u00f6lkopf", "Bernhard"], "venue": "In NIPS,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "A least-squares approach to direct importance estimation", "author": ["Kanamori", "Takafumi", "Hido", "Shohei", "Sugiyama", "Masashi"], "venue": "JMLR, 10:1391\u20131445,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "Statistical analysis of kernel-based leastsquares density-ratio estimation", "author": ["Kanamori", "Takafumi", "Suzuki", "Taiji", "Sugiyama", "Masashi"], "venue": "Machine Learning,", "citeRegEx": "Kanamori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2012}, {"title": "Optimum bounds for the distributions of martingales in Banach spaces", "author": ["Pinelis", "Iosif"], "venue": "The Annals of Probability,", "citeRegEx": "Pinelis and Iosif.,? \\Q1994\\E", "shortCiteRegEx": "Pinelis and Iosif.", "year": 1994}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Shimodaira", "Hidetoshi"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodaira and Hidetoshi.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira and Hidetoshi.", "year": 2000}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["Smale", "Steve", "Zhou", "Ding-Xuan"], "venue": "Constructive Approximation,", "citeRegEx": "Smale et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smale et al\\.", "year": 2007}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["Sriperumbudur", "Bharath K", "Gretton", "Arthur", "Fukumizu", "Kenji", "Sch\u00f6lkopf", "Bernhard", "Lanckriet", "Gert R. G"], "venue": null, "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["Steinwart", "Ingo"], "venue": null, "citeRegEx": "Steinwart and Ingo.,? \\Q2002\\E", "shortCiteRegEx": "Steinwart and Ingo.", "year": 2002}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["Sugiyama", "Masashi", "Nakajima", "Shinichi", "Kashima", "Hisashi", "Buenau", "Paul Von", "Kawanabe", "Motoaki"], "venue": "In NIPS,", "citeRegEx": "Sugiyama et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2008}, {"title": "A note on application of integral operator in learning theory", "author": ["Sun", "Hongwei", "Wu", "Qiang"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Learning and evaluating classifiers under sample selection bias", "author": ["Zadrozny", "Bianca"], "venue": "In ICML", "citeRegEx": "Zadrozny and Bianca.,? \\Q2004\\E", "shortCiteRegEx": "Zadrozny and Bianca.", "year": 2004}], "referenceMentions": [{"referenceID": 5, "context": "(2004); Huang et al. (2007); Bickel et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al.", "startOffset": 8, "endOffset": 53}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al.", "startOffset": 8, "endOffset": 76}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al.", "startOffset": 8, "endOffset": 98}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al. (2008); Kanamori et al.", "startOffset": 8, "endOffset": 122}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al. (2008); Kanamori et al. (2009). We focus in this paper on the covariate shift assumption which was first formulated by Shimodaira (2000) and has been followed by many others.", "startOffset": 8, "endOffset": 146}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al. (2008); Kanamori et al. (2009). We focus in this paper on the covariate shift assumption which was first formulated by Shimodaira (2000) and has been followed by many others.", "startOffset": 8, "endOffset": 252}, {"referenceID": 9, "context": "A number of methods have been proposed to estimate the RND from finite samples, including kernel mean matching (KMM) (Huang et al., 2007), logistic regression (Bickel et al.", "startOffset": 117, "endOffset": 137}, {"referenceID": 1, "context": ", 2007), logistic regression (Bickel et al., 2009), Kullback-Leibler importance estimation (Sugiyama et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 17, "context": ", 2009), Kullback-Leibler importance estimation (Sugiyama et al., 2008), least-squares (Kanamori et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 10, "context": ", 2008), least-squares (Kanamori et al., 2009), and possibly some others.", "startOffset": 23, "endOffset": 46}, {"referenceID": 6, "context": "From the analyses we are aware of, such as (Gretton et al., 2009) on the confidence bound of the RND by KMM, (Kanamori et al.", "startOffset": 43, "endOffset": 65}, {"referenceID": 11, "context": ", 2009) on the confidence bound of the RND by KMM, (Kanamori et al., 2012) on the convergence rate of the least-squares estimate of the RND, and (Cortes et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 3, "context": ", 2012) on the convergence rate of the least-squares estimate of the RND, and (Cortes et al., 2008) on the distributional stability, they all assume that certain functions lie in the reproducing kernel Hilbert space (RKHS) induced by some user selected kernel.", "startOffset": 78, "endOffset": 99}, {"referenceID": 4, "context": "Recently, in a different setting, (Cortes et al., 2010) managed to replace this assumption with a bounded second moment assumption, at the expense of sacrificing the rate a bit.", "startOffset": 34, "endOffset": 55}, {"referenceID": 9, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 17, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 3, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 1, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 10, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 1, "context": ", 2008; Bickel et al., 2009; Kanamori et al., 2009). From the many references, we single out the kernel mean matching (KMM) algorithm, first proposed by Huang et al. (2007) and is also the basis of this paper.", "startOffset": 8, "endOffset": 173}, {"referenceID": 4, "context": "More details can be found in the paper of Gretton et al. (2009). A finite sample 1\u2212\u03b4 confidence bound for L\u0302(\u03b2) (similar as (10) below) is established in Gretton et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 4, "context": "More details can be found in the paper of Gretton et al. (2009). A finite sample 1\u2212\u03b4 confidence bound for L\u0302(\u03b2) (similar as (10) below) is established in Gretton et al. (2009). This bound is further transferred into a confidence bound for the generalization error of some family of loss minimization algorithms in Cortes et al.", "startOffset": 42, "endOffset": 176}, {"referenceID": 3, "context": "This bound is further transferred into a confidence bound for the generalization error of some family of loss minimization algorithms in Cortes et al. (2008), under the notion of distributional stability.", "startOffset": 137, "endOffset": 158}, {"referenceID": 6, "context": "Exhaustive experimental results on KMM can already be found in Gretton et al. (2009).", "startOffset": 63, "endOffset": 85}, {"referenceID": 15, "context": "Second, if the kernel k is characteristic (Sriperumbudur et al., 2010), meaning that the map \u222b X \u03a6(x)P(dx) from the space of probability measures to the RKHS H is injective, then we conclude \u03b2\u0302\u2217 = \u03b2 from (4) hence follows (5).", "startOffset": 42, "endOffset": 70}], "year": 2012, "abstractText": "In real supervised learning scenarios, it is not uncommon that the training and test sample follow different probability distributions, thus rendering the necessity to correct the sampling bias. Focusing on a particular covariate shift problem, we derive high probability confidence bounds for the kernel mean matching (KMM) estimator, whose convergence rate turns out to depend on some regularity measure of the regression function and also on some capacity measure of the kernel. By comparing KMM with the natural plug-in estimator, we establish the superiority of the former hence provide concrete evidence/understanding to the effectiveness of KMM under covariate shift.", "creator": "LaTeX with hyperref package"}}}