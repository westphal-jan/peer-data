{"id": "1206.6410", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "On the Partition Function and Random Maximum A-Posteriori Perturbations", "abstract": "In thamarak this adenoids paper we relate zewe the partition avidius function to daoxin the max - 4,338 statistics bettinger of random epipaleolithic variables. 90-hour In 3-27 particular, 3,295 we s/2004 provide 109-page a novel backlash framework phulbani for gennari approximating omaezaki and bounding australis the partition pure function knauer using samaa MAP warna inference on schlosser randomly marlies perturbed models. palinurus As a result, we astamirov can use efficient rastaq MAP 200-billion solvers engkanto such as graph - glucuronidation cuts to evaluate taj the corresponding big-hearted partition wolf-ferrari function. krakoff We schwamm show non-print that our tuts method excels brigati in the belatedly typical \" www.morningstar.com high 13.19 signal - high norin coupling \" regime lazytown that vernoff results in ragged energy sub-district landscapes difficult for alternative approaches.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (420kb)", "http://arxiv.org/abs/1206.6410v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tamir hazan", "tommi s jaakkola"], "accepted": true, "id": "1206.6410"}, "pdf": {"name": "1206.6410.pdf", "metadata": {"source": "META", "title": "On the Partition Function and Random Maximum A-Posteriori Perturbations", "authors": ["Tamir Hazan", "Tommi Jaakkola"], "emails": ["tamir@ttic.edu", "tommi@csail.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "Learning and inference in complex models drives much of the research in machine learning applications, from computer vision, natural language processing, to computational biology. Examples include object detection (Felzenszwalb et al., 2009), stereo vision (Szeliski et al., 2007), parsing (Koo et al., 2010), or protein design (Sontag et al., 2008). The inference problem in such cases involves assessing the likelihood of possible structures, whether objects, parsers, or molecular structures. The structures are specified by assignments of random variables that need to be maximized or summed over. However, it is often feasible to only find the most likely or maximum a-posteriori (MAP) assignment rather than considering all possible assignments. Indeed, substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity (Kolmogorov, 2006) or by devising approximate methods based on linear programming relaxations (Sontag et al., 2008; Werner, 2008).\nMAP inference is limited when there are other likely assignments. For example, in pose estimation the re-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ncovery of the 3D joint positions from 2D images is often inherently ambiguous. Similarly, in parsing there might be equally likely parse trees for the same sentence. In a fully probabilistic treatment, all possible alternative assignments are considered. This requires summing over the assignments with their respective weights \u2013 evaluating the partition function \u2013 which is considerably harder. In fact, any algorithm for computing the partition function would be able to approximate the MAP value arbitrarily well via a temperature argument (Landau & Lifshitz, 1980). In contrast, MAP inference (maximization) can be tractable even when the problem of evaluating the partition function (weighted counting) is not.\nThe main surprising result of our work is that MAP inference can be used to approximate the partition function. In other words, given an algorithm for computing the MAP value, we can approximate or bound the partition function. The MAP values in our case arise from randomly perturbed models. While models based on random perturbations have been considered recently (Papandreou & Yuille, 2011; Keshet et al., 2011; Tarlow et al., 2012), their relation to the partition function has not. Specifically, we relate the partition function to the expected MAP value of perturbations. This result enables us for the first time to directly use efficient MAP solvers such as graph-cuts or MPLP in calculating the partition function. The approach excels in regimes where there are several but not exponentially many prominent assignments. For example, this happens in cases where observations carry strong signals (local evidence) but are also guided by strong consistency constraints (couplings).\nWe begin by introducing the notation and the counting and maximization problems of interest. We subsequently relate the partition function to the maxstatistics of random variables, and introduce new approximations and bounds on the partition function based on random MAP perturbations. Finally, we describe how to use this method in the context of conditional random fields and demonstrate the effectiveness of the approach."}, {"heading": "2. Background", "text": "Here we briefly define the counting and maximization problems of interest. Throughout the paper we assume real valued potentials \u03c6(y) = \u03c6(y1, ..., yn) <\u221e defined over a discrete product space Y = Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn. The domain is implicitly defined through \u03c6(y) via exclusions \u03c6(y) = \u2212\u221e whenever y 6\u2208 dom(\u03c6).\nThe Gibbs distribution maps the real valued potential functions to the probability scale\np(y1, ..., yn) = 1\nZ exp(\u03c6(y1, ..., yn)) (1) Z = \u2211\ny1,...,yn\nexp(\u03c6(y1, ..., yn)). (2)\nwhere the normalization constant Z is also known as the partition function. The feasibility of using such a distribution for inference and learning is inherently tied to the ability to evaluate the partition function. In the special case, where \u03c6(y) \u2208 {\u2212\u221e, 0} the partition function reduces to counting the number of allowed configurations, namely |dom(\u03c6)|. In general, counting problems are considered very hard, many belonging to the complexity class #P (Valiant, 1979).\nWe can also express the maximum a-posteriori (MAP) inference problem in the same notation as\n(MAP) max y1,...,yn \u03c6(y1, ..., yn). (3)\nwhere, again, the domain of \u03c6(y) is implicitly considered since the maximization avoids all configurations outside the domain. Although the MAP problem is NP-hard in general (Shimony, 1994), it is easier than computing the partition function. It can be solved efficiently in many cases of practical interest, e.g. when \u03c6(y) is a super-modular function. A number of algorithms based on linear programming relaxations have recently been developed for solving MAP problems. Although the run-time of these solvers can be exponential, they are often surprisingly effective in practice."}, {"heading": "3. Max-Statistics", "text": "In the following we describe the basis of our framework. We show how to realize the partition function as the expected value of random MAP perturbations. Analytic expressions for the statistics of a random MAP perturbation can be derived for general discrete sets, whenever independent and identically distributed random perturbations are applied for every assignment y \u2208 Y . Let {\u03b3(y)}y\u2208Y be a collection of random variables. Assume these random\nvariables are independent and identically distributed with F (t) as their cumulative distribution function, i.e. F (t) = P [\u03b3(y) \u2264 t] for each y \u2208 Y . The independence of \u03b3(y) across y \u2208 Y implies that the cumulative distribution function of the random MAP perturbation maxy\u2208Y {\u03c6(y)+\u03b3(y)} is the product of cumulative distribution functions of the individual perturbations \u03c6(y) + \u03b3(y). Therefore P [maxy\u2208Y {\u03c6(y) + \u03b3(y)} \u2264 t] equals to \u220f y\u2208Y F (t \u2212 \u03c6(y)). Below we describe the max-stability of the Gumbel distribution and its expect value.\nLemma 1. Let {\u03b3(y)}y\u2208Y be a collection of independent random variables \u03b3(y) indexed by y \u2208 Y , each following the Gumbel distribution whose cumulative distribution function is F (t) = exp(\u2212 exp(\u2212(t + c))), where c is the Euler constant. Then the random variable maxy\u2208Y {\u03c6(y) + \u03b3(y)} is distributed according to the Gumbel distribution and its expected value is the logarithm of the partition function:\nlogZ = E\u03b3 [ max y\u2208Y {\u03c6(y) + \u03b3(y)} ] . (4)\nProof: The Gumbel cumulative distribution function is closed under multiplication, namely\u220f\ny\u2208Y F (t\u2212 \u03c6(y)) = exp(\u2212 \u2211 y\u2208Y exp(\u2212(t\u2212 \u03c6(y) + c))) = exp(\u2212 exp(\u2212(t+ c))Z) = F (t\u2212 logZ).\nTherefore the random variable maxy\u2208Y {\u03c6(y) + \u03b3(y)} has the Gumbel distribution whose expected value is the logarithm of the partition function.\nIn general each y = (y1, ..., yn) represents an assignment to n variables. In this case the theorem suggests introducing an independent perturbation \u03b3(y) for each such n\u2212dimensional assignment y \u2208 Y . The complexity of evaluating the log-partition function in this manner would be exponential in n. In the following we use lower dimensional random MAP perturbations as the main tool for approximating and bounding the partition function."}, {"heading": "4. Low Dimensional Perturbations", "text": "In this section, we develop efficient approximations and bounds for the partition function based on low dimensional random MAP perturbations. We commence with rewriting the previous result by exploiting the structure of the product space.\nTheorem 1. Let {\u03b3i(yi)}yi\u2208Yi,i=1,...,n, be a collection of independent and identically distributed (i.i.d.) random variables following the Gumbel distribution with\nF (t) = exp(\u2212 exp(\u2212(t+ c))) where c is the Euler constant. Define \u03b3i = {\u03b3i(yi)}yi\u2208Yi . Then\nlogZ = E\u03b31 max y1 \u00b7 \u00b7 \u00b7E\u03b3n max yn {\u03c6(y) + n\u2211 i=1 \u03b3i(yi)}.\nProof: The result follows from applying equation (4) iteratively. Intuitively, Z = \u2211 y1 \u00b7 \u00b7 \u00b7 \u2211 yn\nexp(\u03c6(y)) and equation (4) encodes the correspondence between summation and expectation-maximization, namely\u2211 yi \u2194 E\u03b3i(yi) maxyi . More formally, consider the recursion \u03c6i\u22121(y1, ..., yi\u22121) = E\u03b3i maxyi{\u03c6i(y1, . . . , yi) + \u03b3i(yi)}, where \u03c6n(y1, . . . , yn) = \u03c6(y1, . . . , yn). Equation (4) implies that for each i, \u03c6i\u22121(y1, ..., yi\u22121) = log \u2211 yi\nexp(\u03c6i(y1, ..., yi)). The rest of the proof now follows by induction.\nThe computational complexity of the alternating procedure is still exponential in n. For example, the inner iteration E\u03b3n maxyn{\u03c6(y1, .., yn) + \u03b3n(yn)} needs to be estimated exponentially many times, i.e., for every y1, ..., yn\u22121. Thus from computational perspective the alternating formulation in Theorem 1 is just as inefficient as the formulation in equation (4). We show next how Theorem 1 can be used to derive effective bounds and approximations."}, {"heading": "4.1. Upper Bounds on the Partition Function", "text": "Theorem 1 directly provides easily computable upper bounds on the log-partition function. Intuitively, these bounds correspond to moving expectations outside the maximization operations, each move resulting in an additional bound but also reducing the computational effort needed for the evaluation. For example,\nlogZ \u2264 E\u03b3 max y {\u03c6(y) + \u2211 i \u03b3i(yi)} (5)\nfollows immediately from moving all the expectations in front. In this case the bound is a simple average of MAP values corresponding to models with only single node perturbations {\u03b3i(yi)}yi\u2208Yi,i=1,...,n. If the maximization over \u03c6(y) is feasible (e.g., due to supermodularity), it will typically be feasible after such perturbations as well. The upper bound can thus be evaluated efficiently as a sample average. We generalize this basic result further below.\nCorollary 1. Consider a family of subsets \u03b1 \u2208 A such that \u222a\u03b1\u2208A\u03b1 = {1, ..., n}, and let y\u03b1 be a set of variables {yi}i\u2208\u03b1 restricted to the indexes in \u03b1. Assume that the random variables \u03b3\u03b1(y\u03b1) are i.i.d. according to the Gumbel distribution, for every \u03b1, y\u03b1. Then\nlogZ \u2264 E\u03b3 [\nmax y1,...,yn\n{ \u03c6(y) + \u2211 \u03b1\u2208A \u03b3\u03b1(y\u03b1) }] .\nProof: If the subsets \u03b1 are disjoint, then {y\u03b1}\u03b1\u2208A simply defines a partition of the variables in the model. We can therefore use equation (5) over these grouped variables. In the general case, \u03b1, \u03b1\u2032 \u2208 A may overlap. We lift the variables y1, . . . , yn to a larger set y\u2032 = {y\u2032\u03b1}\u03b1\u2208A where an independent set of variables is introduced for each \u03b1 \u2208 A. We lift the potentials to \u03c6\u2032(y\u2032) by including consistency constraints among the lifted variables\n\u03c6\u2032(y\u2032) = { \u03c6(y1, ..., yn) if \u2200\u03b1, i \u2208 \u03b1 : y\u2032\u03b1,i = yi \u2212\u221e otherwise\nThus, logZ = \u2211 y\u2032 exp(\u03c6\n\u2032(y\u2032)) since inconsistent settings receive zero weight. Moreover, maxy\u2032{\u03c6\u2032(y\u2032) +\u2211 \u03b1 \u03b3\u03b1(y \u2032 \u03b1)} equals maxy{\u03c6(y) + \u2211 \u03b1 \u03b3\u03b1(y\u03b1)} for each realization of the perturbation. This equality holds after expectation over \u03b3 as well. Now, given that the perturbations are independent for each lifted coordinate, the basic result in equation (5), guarantees that E\u03b3 [ maxy\u2032 { \u03c6\u2032(y\u2032) + \u2211 \u03b1 \u03b3\u03b1(y \u2032 \u03b1) }]\nupper bounds logZ. This completes the proof.\nThe structure of \u03b1 \u2282 {1, ..., n} determines the statistical quality and algorithmic efficiency of the method. For example, using a single set \u03b1 = {1, ..., n} the upper bound turns to be the exact characterization in equation (4), but it requires exponentially many independent random variables."}, {"heading": "4.2. Approximating the Partition Function", "text": "We can also use Theorem 1 to derive sampling based approximation schemes for the partition function that also utilize efficient MAP solvers. As a simple step, we could just replace the expectations in Theorem 1 with sampled estimates. The main subtlety lies in how these samples are reused as part of the outer loop maximization steps.\nLet\u2019s begin by considering the n-th dimension alone. For any given setting of y1, ..., yn\u22121, we toss random values \u03b3n(yn), for every yn \u2208 Yn, to estimate log \u2211 yn\nexp(\u03c6(y1, ..., yn)). Since the operation has to be repeated for each different setting of y1, . . . , yn\u22121, we need m \u00b7 |Y | random variables \u03b3n,j(y1, ..., yn\u22121, yn), for every j = 1, ...,m, to ensure that\n1\nm m\u2211 j=1 max yn {\u03c6(y1, ..., yn) + \u03b3n,j(y1, ..., yn)}.\napproximates log \u2211 yn\nexp(\u03c6(y1, ..., yn)) across all y1, . . . , yn\u22121. We can rewrite this expression in terms of a single maximization problem over an extended set of variables. Specifically, we introduce copies yn,j for\neach sample index j = 1, . . . ,m, and pull the maximizations outside the average:\nmax yn,1,...,yn,m\n1\nm m\u2211 j=1 (\u03c6(y1, ..., yn,j) + \u03b3n,j(y1, ..., yn,j)).\nThis result is an approximation to the last expectationmaximization step in Theorem 1. We can now repeat the procedure for dimension n\u22121. Formally, we are using the fact that the partition function is self-reducible, i.e., logZ = log \u2211 y1,...,yn\u22121 exp(log \u2211 yn\nexp(\u03c6(y))). After repeating this step for all the dimensions, we write the resulting approximation as a single but very large MAP program:\nlogZ \u2248 max y\n1\nmn m\u2211 j1,...,jn=1 \u03c6(y1,j1 , ..., yn,jn) +\nn\u2211 i=1 1 mi m\u2211 j1,...,ji=1 \u03b3i,ji(y1,j1 , ..., yi,ji).\nIn this expression, the maximization is over an inflated set of variables where there are m copies of each variable, e.g., yi is expanded into {yi,ji}ji=1,...,m. The problem is that this program uses exponentially many independent random variables \u03b3i,ji(y1,i1 , ..., yi,ji) and thus suffers from the same computational problem as equation (4). However, due to averaging, we expect that most MAP perturbations are now concentrated around the logarithm of the partition function.\nFor computational efficiency we suggest reusing the perturbations, collapsing independent random variables \u03b3i,ji(y1,i1 , ..., yi,ji) to far fewer random variables \u03b3i,ji(yi,ji). This results in the following approximation for the log-partition function:\nmax y1,j1 ,...,yn,jn\n1\nmn \u2211 j1,...,jn \u03c6(y1,j1 , ..., yn,jn) + \u2211 i,ji \u03b3i,ji(yi,ji)\nThis approximation is effective whenever the dominant configurations of variables {yi,ji} were already correlated (few modes) so that the randomness in \u03b3i,ji(y1,i1 , ..., yi,ji) could be compressed with little loss in accuracy. This behavior is typical in models with strong couplings."}, {"heading": "4.3. Lower Bounds on the Partition Function", "text": "For completeness, we also provide a lower bound on the partition function based on randomized MAP computations. Unlike the upper bound discussed earlier, however, the lower bound does not directly exploit Theorem 1.\nTheorem 2. Consider any collection of subsets \u03b1 \u2282 {1, ..., n} and let {\u03b3\u03b1(y\u03b1)} be independent random variables for each setting of \u03b1, y\u03b1. Let K\u03b1,y\u03b1(\u03bb) = logE[exp(\u03bb\u03b3\u03b1(y\u03b1))]. Then logZ \u2265\nsup \u03bb\u22650\n{ logE\u03b3 [ exp(maxy{\u03c6(y) + \u03bb \u2211 \u03b1 \u03b3\u03b1(y\u03b1)}) ] \u2212maxy \u2211 \u03b1K\u03b1,y\u03b1(\u03bb) }\nThe proof appears in the supplementary material. The lower bound is somewhat weaker than the upper bound in Corollary 1. For example, unlike the upper bound, in case of a single \u03b1 = {1, ..., n}, the lower bound is not tight for every \u03c6(y). The parameter \u03bb governs the tradeoff between the perturbed MAP value and the cumulant generating function K\u03b1,y\u03b1(\u03bb). For \u03bb = 0 the cumulant generating function is zero and the lower bound reduces to a trivial bound based on the MAP value. However, we cannot choose arbitrarily large \u03bb since K\u03b1,y\u03b1(\u03bb) can be unbounded depending on the distributions used for perturbations."}, {"heading": "5. Conditional Random Fields", "text": "In a supervised learning problem, we assume training data S of objects x \u2208 X and labels y \u2208 Y such as images and their segmentations. Given a feature vector \u03a6(x, y) < \u221e for each object x \u2208 X and label y \u2208 Y , the learning task is to estimate parameters \u03b8 that maximize the log likelihood of the data under the conditional random field model px(y; \u03b8) = exp(\u03b8T\u03a6(x, y))/Zx(\u03b8). This task can be equivalently stated as a loss minimization problem\nmin \u03b8 \u2211 (x,y)\u2208S ( logZx(\u03b8)\u2212 \u03b8>\u03a6(x, y) ) .\nThe formulation emphasizes the computational cost of using conditional random fields due to the partition function. In the following we make use of a surrogate partition function arising from random MAP perturbations.\nConsider a family of subsets \u03b1 \u2282 {1, ..., n} that cover {1, . . . , n}, and let {\u03b3\u03b1(y\u03b1)}\u03b1,y\u03b1 be i.i.d. random variables with continuous densities. We use the following surrogate criterion for estimating \u03b8: J(\u03b8) =\u2211 (x,y)\u2208S ( E\u03b3 [max y\u0302 {\u03b8>\u03a6(x, y) + \u2211 \u03b1 \u03b3\u03b1(y\u03b1)}]\u2212\u03b8>\u03a6(x, y) )\nThe theorem below establishes some basic properties of this criterion.\nTheorem 3. Under the above assumptions, J(\u03b8) is convex and smooth, and its gradient enforces the mo-\nment matching constraints\n\u2202\n\u2202\u03b8 J(\u03b8) = \u2211 (x,y)\u2208S (\u2211 y\u2032 p\u0302x(y \u2032)\u03a6(x, y\u2032)\u2212 \u03a6(x, y) ) where\np\u0302x(y \u2032) def = P [ y\u2032 \u2208 argmax\ny\u03021,...,y\u0302n {\u03b8>\u03a6(x, y\u0302) + \u2211 \u03b1 \u03b3\u03b1(y\u0302\u03b1)} ] ."}, {"heading": "In particular, if \u03b3\u03b1(y\u03b1) have the Gumbel distribution, J(\u03b8) upper bounds the conditional random field loss.", "text": "Proof: The loss function is convex in \u03b8, cf. (Rockafellar, 1974) Theorem 3. Corollary 1 implies that the surrogate loss upper bounds the partition-loss. To compute the gradient we use Theorem 23 in (Rockafellar, 1974) to differentiate under the integral. The subgradient of the max-function is the indicator function over the maximum argument, cf. Proposition 4.5.1 in (Bertsekas et al., 2003). Since the expectation over the indicator function results in a probability distribution, the surrogate loss is smooth and the gradient takes the above form.\nThe structure of \u03b1 \u2282 {1, ..., n} determines the statistical quality and algorithmic efficiency of the approximation. For example, if we use a single set \u03b1 = {1, ..., n}, this formulation is an exact characterization of the conditional random fields, and its gradient describes the standard moment matching condition."}, {"heading": "6. Empirical Evaluation", "text": "We evaluated our approach on spin glass models \u03c6(y1, ..., yn) = \u2211 i\u2208V \u03c6i(yi) + \u2211 (i,j)\u2208E \u03c6i,j(yi, yj).\nwhere yi \u2208 {\u22121, 1}. Each spin has a local field parameter \u03c6i(yi) = \u03b8iyi and interacts in a grid shaped graphical structure with couplings \u03c6i,j(yi, yj) = \u03b8i,jyiyj . Whenever the coupling parameters are positive the model is called attractive as adjacent variables give higher values to positively correlated configurations. We used low dimensional random perturbations \u03b3i(yi) since such perturbations do not affect the complexity of the MAP solver.\nEvaluating the partition function is challenging when considering strong local field potentials and coupling strengths. The corresponding energy landscape is ragged, and characterized by a relatively small set of dominating configurations. The energy and probability landscapes are presented in the supplementary material. In the following, we show that the random MAP\nperturbations approach performs better than previous approaches in this setting.\nWe evaluated the performance of our method on 10\u00d710 spin glass. The local field parameters \u03b8i were drawn uniformly at random from [\u2212f, f ], where f \u2208 {0.1, 1} to reflect weak and strong potentials. The parameters \u03b8i,j were drawn uniformly from [0, c] or [\u2212c, c] to obtain attractive or mixed coupling potentials. The following algorithms were used to estimate the partition function:\n\u2022 The random MAP perturbation approximation, described in Section 4.2, was executed by inflating the graphical model to 1000\u00d71000 grid. When dealing with attractive potentials this was efficiently evaluated using graph-cuts (Boykov et al., 2001). This approximation method could not be applied to mixed potentials.\n\u2022 The random MAP perturbation upper bound, described in Section 4.1, with perturbations \u03b3i(yi). The expectation was computed using 100 random MAP perturbations, although very similar results were attained after only 10 perturbations. The MAP was computed in the attractive case using graph-cuts and in the mixed case using MPLP (Sontag et al., 2008).\n\u2022 The sum-product form of tree re-weighted belief propagation with uniform distribution over the spanning trees (Wainwright et al., 2005).\n\u2022 Sum-product belief propagation. Whenever the algorithm did not converge we extracted its partition function approximation from its messages.\nWe computed the absolute error in estimating the logarithm of the partition function, averaged over 100 spin glass models, see Fig. 1. One can see that the random MAP perturbation approximation works very well and gives a very accurate estimation using a single MAP value, when considering strong coupling potentials. Also, the random MAP perturbation upper bound is better than the tree re-weighted upper bound in the strong signal domain. Considering the attractive case, the random MAP perturbation approximation and upper bound used the graph-cuts algorithm, therefore were considerably faster than the belief propagation variants. The sum-product belief propagation performs well on the average, but from the plots one can observe its variance. This demonstrates the typical behavior of belief propagation, as it minimizes the non-convex Bethe free energy, thus works well on some instances and does not converge or attain bad local minima on others.\nWe note that the lower bounds of random MAP perturbations, described in Section 4.3, are qualitatively the same as the MAP value, and are outperformed by standard techniques such as the mean-field. We omitted these experiments.\nWe also demonstrated the effectiveness of random MAP perturbations in supervised learning. The training data was composed of ten 100\u00d7 70 binary images, consisting of a man silhouette and a random binary noise, described in Fig. 2. Each image x is described by binary local features \u03c6i(x, yi) which encodes if the i-th pixel is foreground or background, and pairwise features \u03c6i,j(yi, yi) which encourages adjacent features to have the same label. The goal is to estimate the parameters \u03b8i, \u03b8i,j to de-noise the images. Since we are considering 100\u00d770 images, there are about 20, 000 parameters to estimate. Conditional random fields cannot be evaluated on this problem, as the partition function cannot be computed to general graphs with many cycles. However, the MAP can be efficiently estimated using MPLP, thus we applied our approximate conditional random fields described in Section 5. Using the estimated parameters, the pixel based error on the test set was 1.8%. As mentioned before, this approach cannot be compared with conditional random fields. However, without perturbations this program relates to structured-SVM (cf. (Tsochantaridis et al., 2006)) and can be evaluated through MAP solvers. For this case, the pixel based error on the test set was 8.2%."}, {"heading": "7. Related Work", "text": "Throughout this work, we estimate the partition function while computing the max-statistics of collections of random variables. We refer the interested reader to (Kotz & Nadarajah, 2000) for more comprehensive introduction to extreme value statistics.\nTo the best of our knowledge the expected value of\nRandom MAP perturbations over discrete product spaces has not been extensively studied. Talagrand ((Talagrand, 1994), Proposition 4.3) was the first to use random MAP perturbations in discrete settings. However, their approach differs from ours in that their goal was to upper bound the size of dom(\u03c6) \u2286 {0, 1}n using random variables with the Laplace distribution. The proof technique is based on a compression argument, and does not extend to the partition function. Restricting to \u03c6(y) \u2208 {\u2212\u221e, 0}, Corollary 1 presents an alternative technique with weighted assignments. Another upper bound for dom(\u03c6) \u2286 {0, 1}n was described in ((Barvinok & Samorodnitsky, 2007), Theorem 3.1). Their approach used the induction method of (Talagrand, 1995) to prove an upper bound using the logistic distribution. Our Corollary 1 provides an alternative technique for this result. They also extend their upper bound to functions of the form\u2211 y\u2208dom(\u03c6) \u220f i: yi=1 qi, where qi are rational numbers.\nIn this work we also consider parameter estimation using approximate conditional random fields. While computing the gradient we obtained the known result that the Gibbs distribution can be described by the maximal argument of random MAP perturbation. This result is widely used in economics, providing a probabilistic interpretation for choices made by people among a finite set of alternatives. Specifically, the probability of choosing an alternative P [y\u0302 \u2208 argmaxy{\u03c6(y) + \u03b3(y)}] follows the Gibbs distribution whenever \u03b3(y) are independent and distributed according to the Gumbel distribution (McFadden, 1974). This approach is computationally intractable when dealing with discrete product spaces, as it considers n-dimensional independent perturbations. This motivated efficient ways to approximately sample from the Gibbs distribution, through a probability distribution of the form: P [y\u0302 \u2208 argmaxy{\u03c6(y) + \u2211 \u03b1 \u03b3\u03b1(y\u03b1)}], (Papandreou & Yuille, 2011). In particular, the gradient suggested in Theorem 3 was described in (Papandreou & Yuille, 2011). For a more general class of probabilistic models that exploit efficient optimization we refer to (Tarlow et al., 2012). Whenever the perturbations occur in the feature space, random MAP perturbation models relate to PAC-Bayes generalization bounds (Keshet et al., 2011). Other surrogate probability models using computational structures appears in (Papandreou & Yuille, 2010; Kulesza & Taskar, 2010).\nMore broadly, methods for estimating the partition function were subject to extensive research over the past decades. Gibbs sampling, Annealed Importance Sampling and MCMC are typically used for estimating the partition function (cf. (Koller & Friedman, 2009)\nand references therein). These methods are slow when considering ragged energy landscapes, and their mixing time is typically exponential in n. In contrast, perturbed MAP operations are unaffected by ragged energy landscapes provided that the MAP is feasible.\nVariational approaches have been extensively developed to efficiently estimate the partition function in large-scale problems. These are often inner-bound methods where a simpler distribution is optimized as an approximation to the posterior in a KL-divergence sense. The difficulty comes from non-convexity of the set of feasible distributions (e.g., mean field) (Jordan et al., 1999). Variational upper bounds on the other hand are convex, usually derived by replacing the entropy term with a simpler surrogate function and relaxing constraints on sufficient statistics (see, e.g., (Wainwright et al., 2005))."}, {"heading": "8. Discussion", "text": "Evaluating the partition function and computing MAP assignments of variables are key sub-problems in machine learning. While it is well-known that the ability to compute the partition function also leads to a viable MAP algorithm, the reverse is not. We showed here that a randomly perturbed MAP solver can approximate and bound the partition function. The result enables us to take advantage of efficient MAP solvers. Moreover, we demonstrated the effectiveness of our approach in the \u201dhigh-signal high-coupling\u201d regime which dominates machine learning applications and is traditionally hard for current methods. We also applied our approximation to conditional random fields, describing the objective function to the moment matching algorithm of (Papandreou & Yuille, 2011).\nThe bounds we presented hold with expectation. In practice we compute the empirical mean, and standard techniques in measure concentration, e.g. Chebyshev\u2019s inequality, describe how the sampled mean relates to the expected value.\nThe results here can be taken in a number of different directions. The surrogate probability model, emerging from Theorem 3, is based on the maximal argument of perturbed MAP program. This surrogate probability model directly measures the robustness of prediction. Our approach also suggests a new entropy approximation, which can be derived as the conjugate-dual of the surrogate log-partition function. This entropy approximation is used with a valid set of probability distributions induced by perturbations. This is in contrast to current approximations, e.g. based on Bethe entropies, defined over the local marginal polytope."}], "references": [{"title": "Random weighting, asymptotic counting, and inverse isoperimetry", "author": ["A. Barvinok", "A. Samorodnitsky"], "venue": "Israel Journal of Mathematics,", "citeRegEx": "Barvinok and Samorodnitsky,? \\Q2007\\E", "shortCiteRegEx": "Barvinok and Samorodnitsky", "year": 2007}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": null, "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2009}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Pacbayesian approach for minimization of phoneme error rate", "author": ["J. Keshet", "D. McAllester", "T. Hazan"], "venue": "In The International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "Probabilistic graphical models", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": null, "citeRegEx": "Kolmogorov,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov", "year": 2006}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag"], "venue": "In EMNLP,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Extreme value distributions: theory and applications", "author": ["S. Kotz", "S. Nadarajah"], "venue": "World Scientific Publishing Company,", "citeRegEx": "Kotz and Nadarajah,? \\Q2000\\E", "shortCiteRegEx": "Kotz and Nadarajah", "year": 2000}, {"title": "Structured determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In Proc. Neural Information Processing Systems,", "citeRegEx": "Kulesza and Taskar,? \\Q2010\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2010}, {"title": "Statistical physics: Course of theoretical physics, vol", "author": ["L.D. Landau", "Lifshitz", "EM"], "venue": null, "citeRegEx": "Landau et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Landau et al\\.", "year": 1980}, {"title": "Conditional logit analysis of qualitative choice behavior, volume 1, pp. 105\u2013142", "author": ["D. McFadden"], "venue": null, "citeRegEx": "McFadden,? \\Q1974\\E", "shortCiteRegEx": "McFadden", "year": 1974}, {"title": "Gaussian sampling by local perturbations", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. Int. Conf. on Neural Information Processing Systems (NIPS),", "citeRegEx": "Papandreou and Yuille,? \\Q2010\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2010}, {"title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. IEEE Int. Conf. on Computer Vision (ICCV),", "citeRegEx": "Papandreou and Yuille,? \\Q2011\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2011}, {"title": "Conjugate duality and optimization", "author": ["R.T. Rockafellar"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "Rockafellar,? \\Q1974\\E", "shortCiteRegEx": "Rockafellar", "year": 1974}, {"title": "Finding MAPs for belief networks is NP-hard", "author": ["S.E. Shimony"], "venue": "Artificial Intelligence,", "citeRegEx": "Shimony,? \\Q1994\\E", "shortCiteRegEx": "Shimony", "year": 1994}, {"title": "Tightening LP relaxations for MAP using message passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "T. Jaakkola", "Y. Weiss"], "venue": "In Conf. Uncertainty in Artificial Intelligence (UAI). Citeseer,", "citeRegEx": "Sontag et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2008}, {"title": "A comparative study of energy minimization methods for markov random fields with smoothness-based priors", "author": ["R. Szeliski", "R. Zabih", "D. Scharstein", "O. Veksler", "V. Kolmogorov", "A. Agarwala", "M. Tappen", "C. Rother"], "venue": null, "citeRegEx": "Szeliski et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Szeliski et al\\.", "year": 2007}, {"title": "The supremum of some canonical processes", "author": ["M. Talagrand"], "venue": "American Journal of Mathematics,", "citeRegEx": "Talagrand,? \\Q1994\\E", "shortCiteRegEx": "Talagrand", "year": 1994}, {"title": "Concentration of measure and isoperimetric inequalities in product spaces", "author": ["M. Talagrand"], "venue": "Publications Mathe\u0301matiques de l\u2019Institut des Hautes E\u0301tudes Scientifiques,", "citeRegEx": "Talagrand,? \\Q1995\\E", "shortCiteRegEx": "Talagrand", "year": 1995}, {"title": "Randomized Optimum Models for Structured Prediction", "author": ["D. Tarlow", "R.P. Adams", "R.S. Zemel"], "venue": "In AISTATS,", "citeRegEx": "Tarlow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2006}, {"title": "The complexity of computing the permanent", "author": ["L.G. Valiant"], "venue": "Theoretical computer science,", "citeRegEx": "Valiant,? \\Q1979\\E", "shortCiteRegEx": "Valiant", "year": 1979}, {"title": "A new class of upper bounds on the log partition function", "author": ["M.J. Wainwright", "T.S. Jaakkola", "A.S. Willsky"], "venue": "Trans. on Information Theory,", "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf)", "author": ["T. Werner"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Werner,? \\Q2008\\E", "shortCiteRegEx": "Werner", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "Examples include object detection (Felzenszwalb et al., 2009), stereo vision (Szeliski et al.", "startOffset": 34, "endOffset": 61}, {"referenceID": 17, "context": ", 2009), stereo vision (Szeliski et al., 2007), parsing (Koo et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 7, "context": ", 2007), parsing (Koo et al., 2010), or protein design (Sontag et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 16, "context": ", 2010), or protein design (Sontag et al., 2008).", "startOffset": 27, "endOffset": 48}, {"referenceID": 6, "context": "Indeed, substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity (Kolmogorov, 2006) or by devising approximate methods based on linear programming relaxations (Sontag et al.", "startOffset": 169, "endOffset": 187}, {"referenceID": 16, "context": "Indeed, substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity (Kolmogorov, 2006) or by devising approximate methods based on linear programming relaxations (Sontag et al., 2008; Werner, 2008).", "startOffset": 263, "endOffset": 298}, {"referenceID": 24, "context": "Indeed, substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity (Kolmogorov, 2006) or by devising approximate methods based on linear programming relaxations (Sontag et al., 2008; Werner, 2008).", "startOffset": 263, "endOffset": 298}, {"referenceID": 4, "context": "While models based on random perturbations have been considered recently (Papandreou & Yuille, 2011; Keshet et al., 2011; Tarlow et al., 2012), their relation to the partition function has not.", "startOffset": 73, "endOffset": 142}, {"referenceID": 20, "context": "While models based on random perturbations have been considered recently (Papandreou & Yuille, 2011; Keshet et al., 2011; Tarlow et al., 2012), their relation to the partition function has not.", "startOffset": 73, "endOffset": 142}, {"referenceID": 22, "context": "In general, counting problems are considered very hard, many belonging to the complexity class #P (Valiant, 1979).", "startOffset": 98, "endOffset": 113}, {"referenceID": 15, "context": "Although the MAP problem is NP-hard in general (Shimony, 1994), it is easier than computing the partition function.", "startOffset": 47, "endOffset": 62}, {"referenceID": 14, "context": "(Rockafellar, 1974) Theorem 3.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "To compute the gradient we use Theorem 23 in (Rockafellar, 1974) to differentiate under the integral.", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "When dealing with attractive potentials this was efficiently evaluated using graph-cuts (Boykov et al., 2001).", "startOffset": 88, "endOffset": 109}, {"referenceID": 16, "context": "The MAP was computed in the attractive case using graph-cuts and in the mixed case using MPLP (Sontag et al., 2008).", "startOffset": 94, "endOffset": 115}, {"referenceID": 23, "context": "\u2022 The sum-product form of tree re-weighted belief propagation with uniform distribution over the spanning trees (Wainwright et al., 2005).", "startOffset": 112, "endOffset": 137}, {"referenceID": 21, "context": "(Tsochantaridis et al., 2006)) and can be evaluated through MAP solvers.", "startOffset": 0, "endOffset": 29}, {"referenceID": 18, "context": "Talagrand ((Talagrand, 1994), Proposition 4.", "startOffset": 11, "endOffset": 28}, {"referenceID": 19, "context": "Their approach used the induction method of (Talagrand, 1995) to prove an upper bound using the logistic distribution.", "startOffset": 44, "endOffset": 61}, {"referenceID": 11, "context": "Specifically, the probability of choosing an alternative P [\u0177 \u2208 argmaxy{\u03c6(y) + \u03b3(y)}] follows the Gibbs distribution whenever \u03b3(y) are independent and distributed according to the Gumbel distribution (McFadden, 1974).", "startOffset": 200, "endOffset": 216}, {"referenceID": 20, "context": "For a more general class of probabilistic models that exploit efficient optimization we refer to (Tarlow et al., 2012).", "startOffset": 97, "endOffset": 118}, {"referenceID": 4, "context": "Whenever the perturbations occur in the feature space, random MAP perturbation models relate to PAC-Bayes generalization bounds (Keshet et al., 2011).", "startOffset": 128, "endOffset": 149}, {"referenceID": 3, "context": ", mean field) (Jordan et al., 1999).", "startOffset": 14, "endOffset": 35}, {"referenceID": 23, "context": ", (Wainwright et al., 2005)).", "startOffset": 2, "endOffset": 27}], "year": 2012, "abstractText": "In this paper we relate the partition function to the max-statistics of random variables. In particular, we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models. As a result, we can use efficient MAP solvers such as graph-cuts to evaluate the corresponding partition function. We show that our method excels in the typical \u201chigh signal high coupling\u201d regime that results in ragged energy landscapes difficult for alternative approaches.", "creator": "LaTeX with hyperref package"}}}