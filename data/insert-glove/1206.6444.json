{"id": "1206.6444", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Statistical linear estimation with penalized estimators: an application to reinforcement learning", "abstract": "red-headed Motivated bunkmates by worthington value function andrae estimation underdetermined in reinforcement learning, we z2 study blunder statistical linear goal inverse problems, i. e. , problems where koryta the coefficients photocoagulation of pimpinella a linear meco system to be 1.2070 solved warg are pede observed spurgeon in noise. We consider penalized laffont estimators, sermersooq where gyre performance frari is evaluated using fishing a self-restraint matrix - malburg weighted fai two - ltcb norm closeburn of the cherubic defect \u00e1lamo of the cilicia estimator melhem measured 20.1 with loxicha respect award-giving to oros the streett true, verdicts unknown coefficients. alzour Two 21-28 objective functions are marika considered toxicological depending shifnal whether wolinsky the kean error of padukone the defect measured with respect to stadium the 19.25 noisy bristol-myers coefficients is squared or nemorosa unsquared. We suntanning propose simple, f\u00e9es yet overpowers novel yadkin and emusic theoretically well - founded data - kaurismaki dependent dictionnaire choices for sayyad the infectiously regularization parameters for ahoy both cases mcmunn that avoid data - dmr splitting. glendower A distinguishing pseudohistory feature joesbury of our analysis is amirabad-e that p. we derive brise deterministic al-ahsa error cheviot bounds in .95 terms of the loved error lochnagar of the dt coefficients, animerica thus 60.09 allowing 350-page the atchugarry complete margam separation euro356 of warplane the analysis lb8 of terrano the stochastic properties of perricone these phill errors. rm90 We show luneng that our results 5.05 lead bhera to new insights and bounds oesterreichischen for linear value 93.87 function estimation in phylogeography reinforcement learning.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (311kb)", "http://arxiv.org/abs/1206.6444v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["bernardo \u00e1vila pires", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1206.6444"}, "pdf": {"name": "1206.6444.pdf", "metadata": {"source": "META", "title": "Statistical linear estimation with penalized estimators: an application to reinforcement learning", "authors": ["Bernardo \u00c1vila Pires", "Csaba Szepesv\u00e1ri"], "emails": ["bpires@ualberta.ca", "szepesva@cs.ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "Let A be a real-valued m\u00d7d matrix, b be a real-valued m-dimensional vector, M be an m\u00d7m positive semidefinite matrix, and consider the loss function LM : Rd \u2192 R defined by\nLM (\u03b8) . = \u2016A\u03b8 \u2212 b\u2016M ,\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nwhere \u2016\u00b7\u2016M denotes the M matrix-weighted two-norm. We consider the problem of finding a minimizer of this loss when instead of A, b, one has access only to their respective \u201cnoisy\u201d versions, A\u0302, b\u0302. We call this problem a statistical linear inverse problem.\nOur main motivation to study this problem is to better understand the so-called least-squares approach to value function estimation in reinforcement learning, whose goal is to estimate the value function that corresponds to a Markov reward process.1 The leastsquares approach originates from the work of Bradtke and Barto (1996), who proposed to find the parameter-\nvector \u03b8\u0302 of a linear-in-the-parameters value function by solving A\u0302\u03b8 = b\u0302 where the \u201cnoisy\u201d matrix-vector pair, (A\u0302, b\u0302), is computed based on a finite sample.\nThey have proven the almost sure convergence of \u03b8\u0302 to \u03b8\u2217, the solution of A\u03b8 = b, under appropriate conditions on the sample as the sample-size converges to infinity. In particular, they assumed that the sample is generated from either an absorbing or an ergodic Markov chain. More recently, several studies appeared where the finite-sample performance of LSTDlike procedures were investigated (see, e.g., (Antos et al., 2008; Ghavamzadeh et al., 2010; Lazaric et al., 2010; Ghavamzadeh et al., 2011)). The nonparametric variant has also received some attention (Farahmand et al., 2009; Maillard, 2011).\nOne of the difficulties in the analysis of these procedures is that in these problems the sample is correlated, so the standard techniques of supervised learning that assume independence cannot be used. The approach followed by the above-mentioned papers is to extend the existing techniques on an individual basis to deal with correlated samples. However, this\n1For background on this problem the reader may consult, e.g., the books by Bertsekas and Tsitsiklis (1996); Sutton and Barto (1998); Szepesva\u0301ri (2010).\nmight be quite laborious, even only considering the relatively easier case of regression2 (e.g., Farahmand and Szepesva\u0301ri 2011). Thus, a more appealing approach might be to first derive error bounds as a function of the errors A\u0302 \u2212 A, b\u0302 \u2212 b. The advantage of this approach is that it allows one to decouple the technical issue of studying the concentration of the errors A\u0302\u2212A, b\u0302 \u2212 b from the error (or stability) analysis of the estimation procedures. This is the approach that we advocate and follow in this paper. Consequently, our results will always be applicable when one can prove the concentration of the errors A\u0302 \u2212 A, b\u0302 \u2212 b, leading to an overall elegant, modular approach to deriving finite-sample bounds. In some way, our approach parallels the recent trend in learning theory where sharp finite-sample bounds are obtained by first proving deterministic \u201cregret bounds\u201d (e.g., Cesa-Bianchi et al., 2004).\nA second unique feature of our approach is that we derive our results in the above-introduced framework of general statistical linear inverse problems. This allows us to concentrate on the high-level structure of the problem and yields cleaner proofs and results. Furthermore, we think that the problem of linear estimation is interesting on its own due to its mathematical elegance and its applicability beyond value function estimation (a number of specific linear inverse problems, ranging from computer tomography to time series analysis, are discussed in the books by Kirsch (2011) and Alquier et al. (2011)).\nWe will also place special emphasis in statistical linear inverse problems whose underlying system is inconsistent (i.e., when there is no solution to A\u03b8 = b). In value function estimation, such inconsistency may arise in the so-called off-policy version of the problem. Understanding the inconsistent case is important because results that apply to it may shed light on issues arising when learning in badly conditioned systems."}, {"heading": "1.1. Goals", "text": "In this paper, our goal will be to derive exact, uniform, fast, high-probability oracle inequalities for the estimation procedures we study. That is, our goal is to prove that for our choice of an estimator \u03b8\u0302, for any 0 < \u03b4 < 1, with probability 1\u2212 \u03b4,\nLM (\u03b8\u0302) \u2264 inf \u03b8\n{ LM (\u03b8) + cA\u0302,b\u0302(\u03b8, \u03b4) } , (1)\nwhere, for fixed values of \u03b8, \u03b4,\ncA\u0302,b\u0302(\u03b8, \u03b4) = O(max(\u2016A\u0302\u2212A\u2016, \u2016b\u0302\u2212 b\u2016)) (2)\n2Regression is a special case of value function estimation (Szepesva\u0301ri, 2010).\nfor some appropriate norm \u2016\u00b7\u2016. The above is called an oracle inequality since the performance of \u03b8\u0302 (as measured with the loss) is compared to that of an \u201coracle\u201d that has access to the true loss function. The term cA\u0302,b\u0302(\u03b8, \u03b4) expresses the \u201cregret\u201d permitted due to the lack of knowledge of the true loss function. The scaling of this term with \u03b8 (or a norm of it) and \u03b4 will also be of interest.\nLet us now explain the special attributes of the above inequality. We call the \u201crate\u201d in the above inequality \u201cfast\u201d when (2) holds. Such a \u201cfast rate\u201d is possible in simple settings (e.g., when d = 1, A = A\u0302 = 1), hence it is natural to ask whether such rates are still possible in more general settings. The oracle inequality above is called exact because the leading constant (the constant multiplying LM (\u03b8)) equals to 1. When L\u2217 = inf\u03b8 LM (\u03b8) is positive (implying that the system is inconsistent), then only a leading constant of one can guarantee the convergence of the loss to the minimal loss, i.e., the consistency of the estimator. We call the above inequality uniform because it holds for any value of \u03b4. This should be contrasted with inequalities where the range of \u03b4 is lower-bounded and/or the estimator uses its value as input, which may be useful in some cases but falls short of fully characterizing the tail behavior of the loss of the resulting estimator. With some abuse of terminology, an inequality of the above form that holds for all small values of \u03b4 shall be also called uniform. Uniform bounds seem to be harder to prove than their non-uniform counterparts, and we do not know of any uniform, high-probability exact oracle inequality with fast rates, not even in the case of linear regression. Unfortunately, we were also unable to derive such results.\nWhen deriving the estimators, we shall see that a major challenge is to control the magnitude of \u03b8\u0302. Indeed, it follows from our objective function that the size of A\u03b8\u0302 must be controlled, and when A is unknown the magnitude of \u03b8\u0302 must be controlled. This might be difficult when following a naive approach of solving A\u0302\u03b8 = b\u0302 to get \u03b8\u0302, e.g., when A\u0302 is singular, or near-singular (as might be the case frequently in practice). To cope with this issue, in this paper we study procedures built around penalized estimators where a penalty Pen(\u03b8) is combined with the empirical loss L\u0302M (\u03b8) = \u2016A\u0302\u03b8\u2212 b\u0302\u2016M . The penalty is assumed to be some norm of \u03b8. We study two procedures. In the first one, the loss is combined directly with the penalty, in an additive way to get the objective function L\u0302M (\u03b8) + \u03bb\u2016\u03b8\u2016, while in the second one the square of the empirical loss is combined with the penalty: L\u03022M (\u03b8) + \u03bb\u2016\u03b8\u2016. Note that both objective functions are convex. We note in passing that the second objective function when \u2016\u03b8\u2016 is the `1-norm\ngives a Lasso-like procedure, but we postpone further discussion of these choices to later sections of the paper.\nIn the case of both objective functions the main issue becomes selecting the regularization coefficient \u03bb > 0. In this paper we give novel procedures to this end and show that these procedures have advantageous properties: we are able to derive oracle inequalities with fast rates for our procedures, although the inequalities will be either exact or uniform (but not both). To the best of our knowledge our general approach, our procedures, analytic tools and results are novel.\nThe organization of the paper is as follows: in the next section, to motivate the general framework, we briefly describe value function estimation and how it can be put into our general framework. This is followed by a brief section that gives some necessary definitions. Section 3 contains our main results for the two approaches mentioned above. Section 4 discusses the results in the context of value function estimation. The paper is concluded and future work is discussed in Section 5."}, {"heading": "2. Value-estimation in Markov Reward Processes", "text": "The purpose of this section is to show how our results can be applied in the context of valueestimation in Markov Reward Processes. Consider a Markov Reward Process (MRP) (X0, R1, X1, R2, . . .) over a (topological) state space X . By this we mean that (X0, R1, X1, R2, . . .) is a stochastic process, (Xt, Rt+1) \u2208 X \u00d7 R for t \u2265 0 and given the history Ht = (X0, R1, X1, R2, . . . , Xt) up to time t, the distribution of state Xt+1 is completely determined by Xt, while the distribution of the reward Rt+1 is completely determined by Xt and Xt+1 given the history Ht+1. Denote by PM the distribution of (Rt+1, Xt+1) given Xt. We shall call PM a transition kernel. Assume that support of the distribution of X0 covers the whole state space X . Define the value of a state x \u2208 X by V (x) = E [ \u2211\u221e t=0 \u03b3\ntRt+1|X0 = x], where 0 < \u03b3 < 1 is the so-called discount factor.One central problem in reinforcement learning is to estimate the value function V given the trajectory (X0, R1, X1, R2, . . .) (Sutton and Barto, 1998). One popular method is to exploit that the value function is the unique solution to the so-called Bellman equation, which takes the form TW \u2212 W = 0, where W : X \u2192 R and T : RX \u2192 RX is the so-called Bellman operator defined using (TW )(x) = E [Rt+1 + \u03b3W (Xt+1)|Xt = x]. Note that T is affine linear.\nGiven a finite sample (X0, R1, X1, R2, . . . , Xn+1), the LSTD algorithm of Bradtke and Barto (1996) finds an approximate solution to the Bellman equation by solving the linear system\nn\u2211 t=1 (Rt+1 + \u03b3W\u03b8(Xt+1)\u2212W\u03b8(Xt))\u03c6(Xt) = 0 (3)\nin \u03b8 \u2208 Rd. Here \u03c6 = (\u03c61, . . . , \u03c6d)> is a vector of d basis functions, \u03c6i : X \u2192 R, 1 \u2264 i \u2264 d, and W\u03b8 : X \u2192 R is defined using W\u03b8(x) = \u3008\u03b8, \u03c6(x)\u3009. Denoting by \u03b8\u0302 the solution to (3), W\u03b8\u0302 is the approximate value function computed by LSTD. This method can be derived as an instrumental variable method to find an approximate fixed point of T (Bradtke and Barto, 1996) or as a Bubnov-Galerkin method (Yu and Bertsekas, 2010). In any case, the method can be viewed as solving a \u201cnoisy\u201d version of the linear system\nA\u03b8 = b . (4) Here, A = E [ (\u03c6(Xstt )\u2212 \u03b3\u03c6(Xstt+1))\u03c6(Xstt )> ] and\nb = E [ \u03c6(Xstt )R st t+1 ] , where (Xst0 , R st 1 X st 1 , R st 2 , . . .) is a steady-state MRP with transition kernel PM . 3 The linear system (4) can be shown to be consistent (Bertsekas and Tsitsiklis, 1996).4 Note that (3) can also\nbe written in the compact form A\u0302\u03b8 = b\u0302, where A\u0302 = 1/n \u2211n t=1(\u03c6(Xt) \u2212 \u03b3\u03c6(Xt+1))\u03c6(Xt)> and b\u0302 =\n1/n \u2211n t=1Rt+1\u03c6(Xt). By thinking of A\u0302, b\u0302 as \u201cnoisy\u201d versions of A, b and observing that for any M 0 solutions to (4) coincide with the minimizers of LM (\u03b8) = \u2016A\u03b8 \u2212 b\u2016M we see that the least-squares approach to value function estimation can be cast as an instance of statistical linear inverse problems. When M = C\u22121, C = E [ \u03c6(Xt)\u03c6(Xt)\n>], LM (\u00b7) becomes identical to the so-called projected Bellman error loss which can also be written as LM (\u03b8) = \u2016\u03a0\u03c6,\u00b5(TW\u03b8 \u2212 W\u03b8)\u2016\u00b5,2, where \u00b5 is the steady-state distribution underlying PM , \u2016 \u00b7 \u2016\u00b5,2 is the weighted L2(\u00b5)-norm over X and \u03a0 : L2(X , \u00b5) \u2192 L2(X , \u00b5) is the projection on the linear space spanned by \u03c6 with respect to the \u2016\u00b7\u2016\u00b5,2-norm (Antos et al., 2008).\nNote that under mild technical assumptions (to be\ndiscussed later) one can show that (A\u0302n, b\u0302n) = (A\u0302, b\u0302) gets concentrated around (A, b) at the usual parametric rate as the sample size n diverges. Thus, we can indeed view A\u0302, b\u0302 as \u201cnoisy\u201d approximations to (A, b).\n3The MRP is said to be in a steady-state if the distribution of Xt is independent of t.\n4 For a discussion of how well W\u03b8\u2217 approximates V the reader is directed to consult the paper by Scherrer (2010) and the references therein. In this paper, we do not discuss this interesting problem but accept (4) as our starting point.\nOne variation of this problem, the so-called off-policy problem, gives further motivation to recast the problem in terms of a loss function LM (\u00b7) to be minimized. In the off-policy problem the data comes in the form of triplets, ((X0, R\u03031, X\u03031), (X1, R\u03032, X\u03032), . . .), where the distribution of (R\u0303t+1, X\u0303t+1) is again independent of Ht = ((X0, R\u03031, X\u03031), (X1, R\u03032, X\u03032), . . . , (Xt\u22121, R\u0303t, X\u0303t)) given Xt and is equal to the transition kernel PM . Further, it is assumed that (Xt)t\u22650 is a Markov process. The previous setting (also called the on-policy case) is replicated when X\u0303t = Xt, thus this new setting is more general than the previous one. The straightforward generalization of the least-squares approach is\nto define A = E [ (\u03c6(Xstt )\u2212 \u03b3\u03c6(X\u0303stt+1))\u03c6(Xstt )> ] and\nb = E [ R\u0303stt+1\u03c6(X st t ) ] for the \u201csteady-state\u201d process (Xstt , R\u0303 st t+1, X\u0303 st t+1)t\u22650. In this case, the linear system A\u03b8 = b is not necessarily consistent but one can still aim for minimizing (for example) the projected Bellman error. Using A\u0302 = 1/n \u2211n t=1(\u03c6(Xt) \u2212 \u03b3\u03c6(X\u0303t+1))\u03c6(Xt) > and b\u0302 = 1/n \u2211n t=1 R\u0303t+1\u03c6(Xt) we can again cast the problem as a statistical linear inverse problem."}, {"heading": "3. Results", "text": "In this section we give our main results for statistical linear inverse problems. We start with a few definitions. For real numbers a, b, we use a \u2228 b to denote max(a, b). The operator norm of a matrix S with respect to the Euclidean norm \u2016 \u00b7 \u20162 is known to satisfy \u2016S\u20162 = \u03bdmax(S). In what follows, we fix a vector norm \u2016 \u00b7 \u2016. Define the errors of A\u0302 and b\u0302 with the following respective equations: let\n\u2206A . = \u2016M 12 (A\u2212 A\u0302)\u20162,\u2217 , \u2206b . = \u2016M 12 (b\u2212 b\u0302)\u20162 , (5)\nwhere \u2016X\u20162,\u2217 denotes the operator norm of matrix X with respect to the norms \u2016 \u00b7\u20162 and \u2016 \u00b7\u2016, meaning that \u2016X\u20162,\u2217 = supv 6=0 \u2016Xv\u20162/\u2016v\u2016.\nAlthough our main results are oracle inequalities, it will also be interesting to name a minimizer of LM (\u03b8) to explain the structure of some bounds. For this, we introduce \u03b8\u2217 \u2208 Rd as a vector such that \u03b8\u2217 \u2208 arg min\u03b8\u2208Rd LM (\u03b8) where if multiple minimizers exist we choose one with the minimal norm \u2016 \u00b7 \u2016. 5\nIn general, \u2206A,\u2206b are unknown. As it will turn out, in order to properly tune the penalized estimation methods we consider, we need at least upper bounds on these quantities (in particular, on \u2206A). To stay independent of sampling assumptions, we assume that\n5Since our loss function is convex one can always find at least one minimizer.\nsuitable high-probability bounds on \u2206A and \u2206b are available:\nAssumption 3.1. There exist known scaling constants sA, sb > 0 and known \u201ctail\u201d functions zA,\u03b4, zb,\u03b4, \u03b4 \u2208 (0, 1] s.t. for any 0 < \u03b4 < 1, the following hold simultaneously with probability (w.p.) at least 1\u2212 \u03b4:\n\u2206A \u2264 sAzA,\u03b4, \u2206b \u2264 sbzb,\u03b4.\nTo fix the scales of these bounds, we restrict zA,\u03b4, zb,\u03b4 so that zA, 1e = zb, 1 e\n= 1, where e is the base of natural logarithm.\nThe reason to have two terms on the right-hand side in the above inequalities as opposed to having a single term only is because we wish to separate the terms attributable to \u03b4 and the sample size. The intended meaning of sa (and sb) is to capture how the errors behave as a function of the sample size n (typically, we expect sA, sb = O(n\n\u22121/2)), while the terms zA,\u03b4, zb,\u03b4 capture how the errors behave as a function \u03b4 (e.g., they are typically of size O( \u221a ln(1/\u03b4))). In particular, sA, sb should be independent of \u03b4 and zA,\u03b4, zb,\u03b4 should be independent of the sample size. This separation will allow us to distinguish between uniform and nonuniform versions of our oracle inequalities."}, {"heading": "3.1. Minimizing the unsquared penalized loss", "text": "In this section, we present the results for the unsquared penalized loss. Choose \u2016 \u00b7 \u2016 to be some norm of the d-dimensional Euclidean space. For \u03bb > 0, define\n\u03b8\u0302\u03bb \u2208 arg min \u03b8\u2208Rd\n{ L\u0302M (\u03b8) + \u03bb\u2016\u03b8\u2016 } , (6)\nwhere L\u0302M (\u03b8) = \u2016A\u0302\u03b8 \u2212 b\u0302\u2016M . Our first result gives an oracle inequality for \u03b8\u0302\u03bb as a function of \u2206A and \u2206b.\nLemma 3.2. Consider \u03b8\u0302\u03bb as defined in (6). Then,\nLM (\u03b8\u0302\u03bb) \u2264 { 1 \u2228 \u2206A\u03bb }\ninf \u03b8\u2208Rd\n[ LM (\u03b8) + (\u2206A + \u03bb)\u2016\u03b8\u2016 ] + { 2 \u2228 ( 1 + \u2206A\u03bb )} \u2206b.\nThe proof, which is attractively simple and thus elegant, is given in the appendix. The result suggests that the ideal choice for \u03bb is \u2206A. Since \u2206A is unknown, we use its upper bound to choose \u03bb. Depending on whether we allow \u03bb to depend on \u03b4 or not, we get a non-uniform or uniform oracle inequality. In all cases, the rate in the oracle inequality will be fast. We start with the uniform version, non-exact version.\nTheorem 3.3. Let Assumption 3.1 hold and consider \u03b8\u0302\u03bb as defined in (6) where \u03bb = sA. Then, for any\n0 < \u03b4 < 1, w.p. at least 1\u2212 \u03b4 it holds that\nLM (\u03b8\u0302sA) \u2264 zA,\u03b4 \u00b7 inf \u03b8\u2208Rd\n[ LM (\u03b8) + sA(1 + zA,\u03b4)\u2016\u03b8\u2016 ] + sb(1 + zA,\u03b4)zb,\u03b4 .\nBy allowing \u03bb to depend on \u03b4, we get an exact, nonuniform oracle inequality with a fast rate:\nTheorem 3.4. Let Assumption 3.1 hold. Fix 0 < \u03b4 < 1 arbitrarily and choose \u03b8\u0302\u03bb as defined in (6) with \u03bb = sAzA,\u03b4. Then, w.p. at least 1\u2212 \u03b4 it holds that\nLM (\u03b8\u0302sAzA,\u03b4) \u2264 inf \u03b8\u2208Rd\n[ LM (\u03b8) + 2sAzA,\u03b4\u2016\u03b8\u2016 ] + 2sbzb,\u03b4 .\nNote that this bound is as tight as if we had first chosen \u03bb = \u2206A and then applied the stochastic assumptions to obtain a high probability (h.p.) bound.\nWhen the linear system defined by (A, b) is consistent, LM (\u03b8\u2217) = 0. In this case one may prefer Theorem 3.3 to Theorem 3.4. Indeed, focusing on the behavior at \u03b8\u2217 we get from Theorem 3.3 the bound sAzA,\u03b4(1 + zA,\u03b4)\u2016\u03b8\u2217\u2016 + sb(1 + zA,\u03b4)zb,\u03b4 that holds w.p. 1 \u2212 \u03b4 for any value of \u03b4, while from Theorem 3.4 we conclude the bound 2sAzA,\u03b4\u2032\u2016\u03b8\u2217\u2016 + 2sbzb,\u03b4\u2032 , which however, holds only for \u03b4\u2032 \u2265 \u03b4."}, {"heading": "3.2. Minimizing the squared penalized loss", "text": "A more \u201ctraditional\u201d estimator uses the square of the empirical loss function:\n\u03b8\u0302\u03c1 = arg min \u03b8\u2208Rd\n{ L\u03022M (\u03b8) + \u03c1\u2016\u03b8\u2016 } , \u03c1 > 0 . (7)\nTo be able to handle Lasso-like procedures, we decided to avoid squaring the norm of \u03b8. Moreover, not squaring this term is convenient for the proof techniques we used. The extension of our results for other types of penalties, in particular \u2016\u03b8\u20162, is left for future work.\nUnlike the previous case where the loss function and the norm were both unsquared, in this case the selection of the regularization parameter \u03c1 will be more involved. In practice, one often uses a hold-out estimate to choose the best value of \u03c1 amongst a finite number of candidates on an exponential grid. Here, we propose a procedure that avoids splitting the data, but uses the unsquared penalized loss with the same data. The new procedure is defined as follows. For some \u03bb, c > 0 to be chosen later, let\n\u03c1\u0302(\u03bb, c) \u2208 arg min \u03c1\u2208\u039b(\u03bb,c)\n{ L\u0302M (\u03b8\u0302\u03c1) + \u03bb\u2016\u03b8\u0302\u03c1\u2016 } , (8)\nwhere \u039b(\u03bb, c) . = { 2k \u00b7 2c\u03bb : k \u2208 N } and define\n\u03b8\u0303\u03bb,c . = \u03b8\u0302\u03c1\u0302(\u03bb,c) . (9)\nWe now have two parameters that need tuning. However, as we will see, the tuning of these parameters is very similar to what we have seen in the previous section. The reason for this is that \u039b is rich enough to contain a value \u03c1 that makes L\u0302M (\u03b8\u0302\u03c1)+\u03c1\u2016\u03b8\u0302\u03c1\u2016 comparable to (not much larger than) L\u0302M (\u03b8)+\u03bb\u2016\u03b8\u2016 no matter what \u03b8 one selects. This is in fact the key to the proof of the following lemma, which gives a deterministic oracle inequality for \u03b8\u0303\u03bb,c:\nLemma 3.5. Let \u03b8\u0303\u03bb,c be as in (9). Then,\nLM (\u03b8\u0303\u03bb,c) \u2264 { 1 \u2228 \u2206A\u03bb }\ninf \u03b8\u2208Rd\n[ LM (\u03b8) + (\u2206A + 2\u03bb)\u2016\u03b8\u2016 ] + { 2 \u2228 ( 1 + \u2206A\u03bb )} \u2206b + { 1 \u2228 \u2206A\u03bb } c.\nWith the (unattainable) choice \u03bb = \u2206A, c = \u2206b we get\nLM (\u03b8\u0303\u03bb,c) \u2264 inf \u03b8\u2208Rd\n[ LM (\u03b8) + 3\u2206A\u2016\u03b8\u2016 ] + 3\u2206b.\nThese choices are impractical but, as it happened with in the previous section, we can obtain uniform nonexact or non-uniform exact oracle inequalities with fast rates. The non-exact uniform oracle inequality is formalized as follows:\nTheorem 3.6. Let Assumption 3.1 hold and choose \u03b8\u0303\u03bb,c be as in (9) with \u03bb = sA and c = sb. Then, for any 0 < \u03b4 < 1 w.p. at least 1\u2212 \u03b4 it holds that\nLM (\u03b8\u0303\u03bb,c) \u2264 {1 \u2228 zA,\u03b4} inf \u03b8\u2208Rd\n[ LM (\u03b8) + sA(zA,\u03b4 + 2)\u2016\u03b8\u2016 ] + {2 \u2228 (1 + zA,\u03b4)} sbzb,\u03b4 + {1 \u2228 zA,\u03b4} sb .\nThe next theorem gives a non-uniform, exact oracle inequality with fast rates.\nTheorem 3.7. Let Assumption 3.1 hold. Fix 0 < \u03b4 < 1 and choose \u03b8\u0303\u03bb,c be as in (9) with \u03bb = sAzA,\u03b4 and c = sbzb,\u03b4. Then, w.p. at least 1\u2212 \u03b4 it holds that\nLM (\u03b8\u0303\u03bb,c) \u2264 inf \u03b8\u2208Rd\n[ LM (\u03b8) + 3sAzA,\u03b4\u2016\u03b8\u2016 ] + 3sbzb,\u03b4 .\nThe relative merits of the uniform and non-uniform oracle inequalities are unchanged compared to what we have seen in the previous section."}, {"heading": "4. Value-estimation in Markov Reward Processes: Results", "text": "Let us now return to value-estimation in Markov Reward Processes. We consider the projected Bellman error objective, LM (\u03b8) = \u2016A\u03b8\u2212 b\u2016M , where M = C\u22121 (for the definitions see Section 2). Assume that \u2206A, \u2206b are concentrated as in Assumption 3.1, with known\nbounds. This can be arranged for example if the features \u03c6i(Xt) and rewards Rt+1 are a.s. bounded, and if we assume appropriate mixing, such as exponential \u03b2-mixing (Yu, 1994), or when the Markov chain (Xt)t\u22650 forgets its past sufficiently rapidly (Samson, 2000). Note that in these cases (A\u0302, b\u0302) gets concentrated around (A, b) at the usual parametric rate, i.e., sA, sb = O( \u221a 1/n) and zA,\u03b4, zb,\u03b4 = O( \u221a ln(1/\u03b4)).\nFor simplicity, assume first that C is given and consider the on-policy case. As mentioned previously, in this case the system A\u03b8 = b is guaranteed to have a solution and therefore LM (\u03b8\u2217) = 0. Consider the estimator that minimizes the unsquared penalized loss. Then, Theorem 3.3 shows a uniform fast rate when using \u03bb = sA:\nLM (\u03b8\u0302sA) \u2264 (1 + zA,\u03b4) [ sAzA,\u03b4\u2016\u03b8\u2217\u2016+ sbzb,\u03b4 ] .\nWe get a similar inequality for the squared penalized loss using the result Theorem 3.6 with a slightly larger bound.\nIn the off-policy case, the linear system A\u03b8 = b may not have a solution. When it does, the previous bound applies. However, when this linear system does not have a solution, to get an exact oracle inequality we are forced to choose \u03bb (in the case of minimizing the unsquared penalized loss) based on \u03b4. In particular, with the choice \u03bb = sAzA,\u03b4, Theorem 3.4 gives\nLM (\u03b8\u0302sAzA,\u03b4) \u2264 inf \u03b8\u2208Rd\n[ LM (\u03b8) + 2sAzA,\u03b4\u2016\u03b8\u2016 ] + 2sbzb,\u03b4 .\n(10) Again, this inequality gives fast, O( \u221a 1/n) rates when\nsA, sb = O( \u221a\n1/n). Similar results hold for the procedure defined for the squared penalized loss where the bound is given by the inequality of Theorem 3.7.\nWhen C is unknown, one may resort replacing it by M 0. Then, a non-exact oracle inequality can be derived using \u2016x\u20162P \u2264 \u03bdmax(Q\u22121/2PQ\u22121/2)\u2016x\u20162Q. (For a matrix S, we denote by \u03bdmax(S), \u03bdmax(S) its largest and smallest singular values, respectively.) Consider first the unsquared penalized loss. In this case, \u2016A\u03b8\u2212 b\u2016C\u22121 \u2264 \u03bd 1/2 max(M\u22121/2C\u22121M\u22121/2)\u2016A\u03b8 \u2212 b\u2016M . Assume that for an estimator \u03b8\u0302 it holds that \u2016A\u03b8 \u2212 b\u2016M \u2264 inf\u03b8 [ \u2016A\u03b8 \u2212 b\u2016M + cA\u0302,b\u0302(\u03b8) ] . Then, from \u2016A\u03b8 \u2212 b\u2016M \u2264 \u03bd 1/2 max(C1/2MC1/2)\u2016A\u03b8 \u2212 b\u2016C\u22121 we get\n\u2016A\u03b8\u0302 \u2212 b\u2016C\u22121 \u2264 inf \u03b8\n[ \u03ba1/2\u2016A\u03b8 \u2212 b\u2016M + \u03c4\u22121/2cA\u0302,b\u0302(\u03b8) ] .\nwhere \u03ba = \u03bdmax(C 1/2MC1/2)/\u03bdmin(M 1/2CM1/2) is the \u201cconditioning number\u201d of M1/2CM1/2 and \u03c4 =\n\u03bdmin(M 1/2CM1/2). In the on-policy case, for example, this gives bounds of the form\nLM (\u03b8\u0302sA) \u2264 \u03c4\u22121/2(1 + zA,\u03b4) [ sAzA,\u03b4\u2016\u03b8\u2217\u2016+ sbzb,\u03b4 ] .\nThe bound for the off-policy case derived from (10) takes the form\nLM (\u03b8\u0302sAzA,\u03b4) \u2264\ninf \u03b8\u2208Rd\n[ \u03ba1/2LM (\u03b8) + 2\u03c4\u22121/2sAzA,\u03b4\u2016\u03b8\u2016 ] + 2\u03c4\u22121/2sbzb,\u03b4 .\nSimilar inequalities can be derived for our procedures that minimize the squared penalized loss.\nFinally, let us discuss the dependence of our bounds on the choice of the basis functions. This dependence comes through Assumption 3.1. As an example, assume that \u03c6i : X \u2192 [\u22121, 1] and \u2016 \u00b7 \u2016 = \u2016 \u00b7 \u2016p with 1 \u2264 p \u2264 2. In this case, the bound on \u2206A is expected to scale linearly with d, while \u2206b is expected to scale linearly with \u221a d. To see why \u2206A is expected to scale linearly with d note that \u2206A \u2264 \u2016M1/2(A\u0302 \u2212 A)\u20162,2 = \u2016M1/2(A\u0302 \u2212 A)\u2016F , where \u2016 \u00b7 \u2016F denotes the Frobenius norm. Now, the Frobenius norm is the norm underlying the Hilbert-space of square matrices with the inner product \u3008P,Q\u3009 = trace(P>Q) and thus an application of any concentration inequality for Hilbertspace valued random variables (e.g., (Steinwart and Christmann, 2008)) gives a bound that scales with the \u201crange\u201d of N = \u2016M1/2(\u03c6(Xt) \u2212 \u03b3\u03c6(X\u0303t+1))\u03c6(Xt)>\u2016F . Using the rotation property of trace, we get that N = \u2016\u03c6(Xt) \u2212 \u03b3\u03c6(X\u0303t+1)\u2016M\u2016\u03c6(Xt)\u2016. The first term can be bounded using the triangle inequality as a function of \u2016\u03c6(Xt)\u2016M and \u2016\u03c6(X\u0303t+1)\u2016M . Assuming (e.g.,) that M is the identity matrix, we get that both \u2016\u03c6(Xt)\u2016M = \u2016\u03c6(Xt)\u2016 and \u2016\u03c6(X\u0303t+1)\u2016 are of size O( \u221a d). Hence, their product scales linearly with d.\nThe above bound on \u2206A is naive; we believe using \u2206A \u2264 \u03bdmax(A\u0302\u2212A) may yield a tighter dependency on d. E.g., for d \u00d7 d-matrices with i.i.d standard normal entries, the maximum eigenvalue is O( \u221a d) (Vershynin, 2010). Furthermore, note that if the basis functions are correlated, or if they are sparse, the dimension will not necessarily appear linearly in the bound either. For a discussion of when to expect a milder dependence of the norm of \u03c6 on d, the interested reader may consult the paper by Maillard and Munos (2009)."}, {"heading": "4.1. Related work", "text": "Antos et al. (2008) proved a uniform high-probability inequality both for the on-policy and the off-policy cases for LSTD. Their bound takes the form LM (\u03b8\u0302)\u2212 LM (\u03b8\u2217) = O ( d ln(d) ( 1 n ) 1 4 ) , which is a slower rate\nthan the rate we are able to obtain. Further, with our bounding method the ln d factor can be removed from this bound.\nThere are more results available for the on-policy case. As mentioned earlier, in this case the system A\u03b8 = b is consistent and thus our bound, under appropriate mixing conditions, takes the form\nLM (\u03b8\u0302) = O ( L \u221a d\n\u03c4n (1 +R)\n) ,\nwhere \u03c4 . = \u03bdmin(M 1 2CM 1 2 ), L is the worst-case norm of features in the dual norm (L . = supx\u2208X \u2016\u03c6(x)\u2016\u2217;\nas discussed previously, L may be O( \u221a d)) and R is a worst-case bound on the norm of the parameter vector (i.e., \u2016\u03b8\u2217\u2016 \u2264 R). In the next two results, the norm \u2016 \u00b7\u2016 is the 2-norm. Lazaric et al. (2010) for their (unregularized) path-wise LSTD method obtain\nLM (\u03b8\u0302) = O ( L \u221a d log d\nn\u03c4 (1 +R)\n)\n(cf. Theorem 3 in their work). Although this is a fast rate, it also shares the undesirable dependence on 1\u03c4 . Non-uniform, slow rates can be extracted from the paper by Ghavamzadeh et al. (2010) for LSTD with random projections. The result with our notation would look like (cf. Theorem 2)\nLM (\u03b8\u0302) = O ( L2 \u221a log d\n\u03c4\n( 1\nn\n) 1 4\nR+ LR\u221a n\n) .\nMore recently, for the so-called Lasso-TD method, Ghavamzadeh et al. (2011) showed non-uniform O ((\n1 n\n) 1 4 ) -rates, but only for the so-called in-sample\nerror, i.e., the empirical norm at the states used by the algorithm. These rates depend on the `1-norm of \u03b8\u2217 and have no dependence on the minimum eigenvalue, but they are slow in n. At the expense of additional assumptions on the Gram matrix C\u0302 (a sample estimate of C), they have also derived fast rates."}, {"heading": "5. Conclusion and future work", "text": "We have shown performance bounds for two estimators in linear inverse problems. Each of these minimizes one of LM (\u03b8) and L2M (\u03b8), plus a penalty \u03bb\u2016\u03b8\u2016. The penalty weight \u03bb can be chosen a priori without the need for a separate validation data set, and the bounds were presented in a general form that apply to many different instances of statistical linear inverse problems, requiring only that \u2206A and \u2206b concentrate around zero. Our split analysis, into a deterministic\nstep and a stochastic step, allows us to decouple the behavior of \u2206A,\u2206b from that of the estimators.\nWe have recovered `1-penalized variations of LSTD (Bradtke and Barto, 1996) for value function estimation in MRPs. We have shown fast, uniform rates, which, in the on-policy case, are exact and competitive with those existing in the literature. In the off-policy case, the rates are non-exact, and the non-uniform bound is also competitive with existing results.\nFinally, we would like to point out interesting ways to further develop our work.\n`1-penalties. The choice when the norm used in the penalty is the `1-norm has been extensively studied in the supervised learning literature (see, e.g., (Bickel et al., 2009; Koltchinskii, 2011; Bu\u0308hlmann and Geer, 2011) and the references therein), as well as in the reinforcement learning setting (Kolter and Ng, 2009; Ghavamzadeh et al., 2010; 2011; Maillard, 2011), mainly because it allows for non-trivial performance bounds even when the dimension d of the parameter vector is comparable to the sample size n (or even larger than n) provided that the true parameter vector is sparse (i.e., there are many zeroes in it). In this paper we decided not to specialize to this case but rather to focus on the problem of proving fast, exact and (possibly uniform) oracle inequalities. Our results, when applied to the case of an `1-penalty show that in a way adding an `1-penalty does not hurt performance (as we expect that the oracle inequalities with the said properties should hold for a decent method) even if the conditions ideal for the `1-penalty do not hold. We do not know of performance bounds (ours included) for `1-penalized estimation have all of the characteristics we are after in a bound (viz. bounds that are exact, fast and uniform).\nLinear regression. Our results are also worth investigating in the context of linear regression. It is easy to cast regression as a statistical linear estimation problem whose underlying system is always consistent. If we use \u2016 \u00b7\u2016 as the `1-norm, we recover procedures similar to the square-root Lasso (Belloni et al., 2010) and the Lasso (Tibshirani, 1996) for the estimators studied in Sections 3.1 and 3.2, respectively. We believe that confronting the bounds that can be derived from our results with bounds for linear regression in the literature can be very instructive.\nConnection to Inverse Problems. The theory of Inverse Problems is very pertinent to this work, and it is important to study our results under the light of those shown in Chapter 2 of Kirsch (2011); Alquier et al. (2011). The existing knowledge of inverse prob-\nlems may help us better understand which choices of \u2016\u00b7\u2016 allow \u2206A to concentrate around zero, and how fast this concentration occurs. The idea of having learning problems as inverse problems is not new; Rosasco (2006); Vito et al. (2006) study regression in Hilbert spaces as an inverse problem."}, {"heading": "Acknowledgements", "text": "This work was supported by AITF and NSERC."}], "references": [{"title": "Inverse Problems and High-dimensional Estimation", "author": ["P. Alquier", "E. Gautier", "G. Stoltz"], "venue": "SpringerVerlag.", "citeRegEx": "Alquier et al\\.,? 2011", "shortCiteRegEx": "Alquier et al\\.", "year": 2011}, {"title": "Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path", "author": ["A. Antos", "C. Szepesv\u00e1ri", "R. Munos"], "venue": "Machine Learning, 71(1):89\u2013129.", "citeRegEx": "Antos et al\\.,? 2008", "shortCiteRegEx": "Antos et al\\.", "year": 2008}, {"title": "Square-root Lasso: Pivotal recovery of sparse signals via conic programming", "author": ["A. Belloni", "V. Chernozhukov", "L. Wang"], "venue": "arXiv, stat.ME. Published in: Biometrika (2011) 98(4): 791-806.", "citeRegEx": "Belloni et al\\.,? 2010", "shortCiteRegEx": "Belloni et al\\.", "year": 2010}, {"title": "NeuroDynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific, Belmont, MA.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["P. Bickel", "Y. Ritov", "A. Tsybakov"], "venue": "The Annals of Statistics, 37(4):1705\u20131732.", "citeRegEx": "Bickel et al\\.,? 2009", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning, 22:33\u201357.", "citeRegEx": "Bradtke and Barto,? 1996", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "Statistics for HighDimensional Data: Methods, Theory and Applications", "author": ["P. B\u00fchlmann", "S. Geer"], "venue": "Springer Series in Statistics. Springer.", "citeRegEx": "B\u00fchlmann and Geer,? 2011", "shortCiteRegEx": "B\u00fchlmann and Geer", "year": 2011}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory, 50:2050\u2013 2057.", "citeRegEx": "Cesa.Bianchi et al\\.,? 2004", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Regularized policy iteration", "author": ["A. Farahmand", "M. Ghavamzadeh", "C. Szepesv\u00e1ri", "S. Mannor"], "venue": "Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L., editors, NIPS-21, pages 441\u2013448. MIT Press.", "citeRegEx": "Farahmand et al\\.,? 2009", "shortCiteRegEx": "Farahmand et al\\.", "year": 2009}, {"title": "Regularized least-squares regression: Learning from a beta-mixing sequence", "author": ["A. Farahmand", "C. Szepesv\u00e1ri"], "venue": "Journal of Statistical Planning and Inference (in press), 142(2):493\u2013505.", "citeRegEx": "Farahmand and Szepesv\u00e1ri,? 2011", "shortCiteRegEx": "Farahmand and Szepesv\u00e1ri", "year": 2011}, {"title": "LSTD with random projections", "author": ["M. Ghavamzadeh", "A. Lazaric", "Maillard", "O.-A.", "R. Munos"], "venue": "Advances in Neural Information Processing Systems, 23:721\u2013729.", "citeRegEx": "Ghavamzadeh et al\\.,? 2010", "shortCiteRegEx": "Ghavamzadeh et al\\.", "year": 2010}, {"title": "Finite-sample analysis of Lasso-TD", "author": ["M. Ghavamzadeh", "A. Lazaric", "R. Munos", "M. Hoffman"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML), pages 1177\u20131184.", "citeRegEx": "Ghavamzadeh et al\\.,? 2011", "shortCiteRegEx": "Ghavamzadeh et al\\.", "year": 2011}, {"title": "An Introduction to the Mathematical Theory of Inverse Problems", "author": ["A. Kirsch"], "venue": "Springer, 2nd edition.", "citeRegEx": "Kirsch,? 2011", "shortCiteRegEx": "Kirsch", "year": 2011}, {"title": "Oracle inequalities in empirical risk minimization and sparse recovery problems", "author": ["V. Koltchinskii"], "venue": "Springer.", "citeRegEx": "Koltchinskii,? 2011", "shortCiteRegEx": "Koltchinskii", "year": 2011}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Bottou, L. and Littman, M., editors, ICML 2009, pages 521\u2013528. ACM.", "citeRegEx": "Kolter and Ng,? 2009", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "Finite-sample analysis of LSTD", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), pages 615\u2013622.", "citeRegEx": "Lazaric et al\\.,? 2010", "shortCiteRegEx": "Lazaric et al\\.", "year": 2010}, {"title": "Apprentissage S\u00e9quentiel: Bandits, Statistique et Renforcement", "author": ["Maillard", "O.-A."], "venue": "PhD thesis, Universit\u00e9 des Sciences et des Technologies de Lille.", "citeRegEx": "Maillard and O..A.,? 2011", "shortCiteRegEx": "Maillard and O..A.", "year": 2011}, {"title": "Compressed leastsquares regression", "author": ["Maillard", "O.-A.", "R. Munos"], "venue": "NIPS, pages 1213\u20131221.", "citeRegEx": "Maillard et al\\.,? 2009", "shortCiteRegEx": "Maillard et al\\.", "year": 2009}, {"title": "Regularization Approaches in Learning Theory", "author": ["L. Rosasco"], "venue": "PhD thesis, DISI, Universit\u00e0 di Genova.", "citeRegEx": "Rosasco,? 2006", "shortCiteRegEx": "Rosasco", "year": 2006}, {"title": "Concentration of measure inequalities for markov chains and \u03c6-mixing processes", "author": ["Samson", "P.-M."], "venue": "The Annals of Probability, 28(1):416\u2013461.", "citeRegEx": "Samson and P..M.,? 2000", "shortCiteRegEx": "Samson and P..M.", "year": 2000}, {"title": "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view", "author": ["B. Scherrer"], "venue": "F\u00fcrnkranz, J. and Joachims, T., editors, ICML 2010, pages 959\u2013966.", "citeRegEx": "Scherrer,? 2010", "shortCiteRegEx": "Scherrer", "year": 2010}, {"title": "Support Vector Machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Springer, 1st edition.", "citeRegEx": "Steinwart and Christmann,? 2008", "shortCiteRegEx": "Steinwart and Christmann", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Bradford Book. MIT Press, Cambridge, Massachusetts.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers.", "citeRegEx": "Szepesv\u00e1ri,? 2010", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2010}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B, 58(1):267\u2013288.", "citeRegEx": "Tibshirani,? 1996", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "arxiv, math.PR.", "citeRegEx": "Vershynin,? 2010", "shortCiteRegEx": "Vershynin", "year": 2010}, {"title": "Learning from examples as an inverse problem", "author": ["E.D. Vito", "L. Rosasco", "A. Caponnetto", "U.D. Giovannini", "F. Odone"], "venue": "Journal of Machine Learning Research, 6(1):883.", "citeRegEx": "Vito et al\\.,? 2006", "shortCiteRegEx": "Vito et al\\.", "year": 2006}, {"title": "Rates of convergence for empirical processes of stationary mixing sequences", "author": ["B. Yu"], "venue": "The Annals of Probability, 22(1):94\u2013116.", "citeRegEx": "Yu,? 1994", "shortCiteRegEx": "Yu", "year": 1994}, {"title": "Error bounds for approximations from projected linear equations", "author": ["H. Yu", "D. Bertsekas"], "venue": "Mathematics of Operations Research, 35(2):306\u2013329.", "citeRegEx": "Yu and Bertsekas,? 2010", "shortCiteRegEx": "Yu and Bertsekas", "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "The leastsquares approach originates from the work of Bradtke and Barto (1996), who proposed to find the parameter-", "startOffset": 54, "endOffset": 79}, {"referenceID": 1, "context": ", (Antos et al., 2008; Ghavamzadeh et al., 2010; Lazaric et al., 2010; Ghavamzadeh et al., 2011)).", "startOffset": 2, "endOffset": 96}, {"referenceID": 10, "context": ", (Antos et al., 2008; Ghavamzadeh et al., 2010; Lazaric et al., 2010; Ghavamzadeh et al., 2011)).", "startOffset": 2, "endOffset": 96}, {"referenceID": 15, "context": ", (Antos et al., 2008; Ghavamzadeh et al., 2010; Lazaric et al., 2010; Ghavamzadeh et al., 2011)).", "startOffset": 2, "endOffset": 96}, {"referenceID": 11, "context": ", (Antos et al., 2008; Ghavamzadeh et al., 2010; Lazaric et al., 2010; Ghavamzadeh et al., 2011)).", "startOffset": 2, "endOffset": 96}, {"referenceID": 8, "context": "The nonparametric variant has also received some attention (Farahmand et al., 2009; Maillard, 2011).", "startOffset": 59, "endOffset": 99}, {"referenceID": 3, "context": ", the books by Bertsekas and Tsitsiklis (1996); Sutton and Barto (1998); Szepesv\u00e1ri (2010).", "startOffset": 15, "endOffset": 47}, {"referenceID": 3, "context": ", the books by Bertsekas and Tsitsiklis (1996); Sutton and Barto (1998); Szepesv\u00e1ri (2010).", "startOffset": 15, "endOffset": 72}, {"referenceID": 3, "context": ", the books by Bertsekas and Tsitsiklis (1996); Sutton and Barto (1998); Szepesv\u00e1ri (2010).", "startOffset": 15, "endOffset": 91}, {"referenceID": 11, "context": "Furthermore, we think that the problem of linear estimation is interesting on its own due to its mathematical elegance and its applicability beyond value function estimation (a number of specific linear inverse problems, ranging from computer tomography to time series analysis, are discussed in the books by Kirsch (2011) and Alquier et al.", "startOffset": 309, "endOffset": 323}, {"referenceID": 0, "context": "Furthermore, we think that the problem of linear estimation is interesting on its own due to its mathematical elegance and its applicability beyond value function estimation (a number of specific linear inverse problems, ranging from computer tomography to time series analysis, are discussed in the books by Kirsch (2011) and Alquier et al. (2011)).", "startOffset": 327, "endOffset": 349}, {"referenceID": 23, "context": "Regression is a special case of value function estimation (Szepesv\u00e1ri, 2010).", "startOffset": 58, "endOffset": 76}, {"referenceID": 22, "context": ") (Sutton and Barto, 1998).", "startOffset": 2, "endOffset": 26}, {"referenceID": 5, "context": ", Xn+1), the LSTD algorithm of Bradtke and Barto (1996) finds an approximate solution to the Bellman equation by solving the linear system", "startOffset": 31, "endOffset": 56}, {"referenceID": 5, "context": "This method can be derived as an instrumental variable method to find an approximate fixed point of T (Bradtke and Barto, 1996) or as a Bubnov-Galerkin method (Yu and Bertsekas, 2010).", "startOffset": 102, "endOffset": 127}, {"referenceID": 28, "context": "This method can be derived as an instrumental variable method to find an approximate fixed point of T (Bradtke and Barto, 1996) or as a Bubnov-Galerkin method (Yu and Bertsekas, 2010).", "startOffset": 159, "endOffset": 183}, {"referenceID": 3, "context": "3 The linear system (4) can be shown to be consistent (Bertsekas and Tsitsiklis, 1996).", "startOffset": 54, "endOffset": 86}, {"referenceID": 1, "context": "When M = C\u22121, C = E [ \u03c6(Xt)\u03c6(Xt) >], LM (\u00b7) becomes identical to the so-called projected Bellman error loss which can also be written as LM (\u03b8) = \u2016\u03a0\u03c6,\u03bc(TW\u03b8 \u2212 W\u03b8)\u2016\u03bc,2, where \u03bc is the steady-state distribution underlying PM , \u2016 \u00b7 \u2016\u03bc,2 is the weighted L(\u03bc)-norm over X and \u03a0 : L(X , \u03bc) \u2192 L(X , \u03bc) is the projection on the linear space spanned by \u03c6 with respect to the \u2016\u00b7\u2016\u03bc,2-norm (Antos et al., 2008).", "startOffset": 377, "endOffset": 397}, {"referenceID": 20, "context": "4 For a discussion of how well W\u03b8\u2217 approximates V the reader is directed to consult the paper by Scherrer (2010) and the references therein.", "startOffset": 97, "endOffset": 113}, {"referenceID": 27, "context": "bounded, and if we assume appropriate mixing, such as exponential \u03b2-mixing (Yu, 1994), or when the Markov chain (Xt)t\u22650 forgets its past sufficiently rapidly (Samson, 2000).", "startOffset": 75, "endOffset": 85}, {"referenceID": 21, "context": ", (Steinwart and Christmann, 2008)) gives a bound that scales with the \u201crange\u201d of N = \u2016M(\u03c6(Xt) \u2212 \u03b3\u03c6(X\u0303t+1))\u03c6(Xt)\u2016F .", "startOffset": 2, "endOffset": 34}, {"referenceID": 25, "context": "d standard normal entries, the maximum eigenvalue is O( \u221a d) (Vershynin, 2010).", "startOffset": 61, "endOffset": 78}, {"referenceID": 25, "context": "d standard normal entries, the maximum eigenvalue is O( \u221a d) (Vershynin, 2010). Furthermore, note that if the basis functions are correlated, or if they are sparse, the dimension will not necessarily appear linearly in the bound either. For a discussion of when to expect a milder dependence of the norm of \u03c6 on d, the interested reader may consult the paper by Maillard and Munos (2009).", "startOffset": 62, "endOffset": 388}, {"referenceID": 15, "context": "Lazaric et al. (2010) for their (unregularized) path-wise LSTD method obtain", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Non-uniform, slow rates can be extracted from the paper by Ghavamzadeh et al. (2010) for LSTD with random projections.", "startOffset": 59, "endOffset": 85}, {"referenceID": 10, "context": "More recently, for the so-called Lasso-TD method, Ghavamzadeh et al. (2011) showed non-uniform", "startOffset": 50, "endOffset": 76}, {"referenceID": 5, "context": "We have recovered `-penalized variations of LSTD (Bradtke and Barto, 1996) for value function estimation in MRPs.", "startOffset": 49, "endOffset": 74}, {"referenceID": 4, "context": ", (Bickel et al., 2009; Koltchinskii, 2011; B\u00fchlmann and Geer, 2011) and the references therein), as well as in the reinforcement learning setting (Kolter and Ng, 2009; Ghavamzadeh et al.", "startOffset": 2, "endOffset": 68}, {"referenceID": 13, "context": ", (Bickel et al., 2009; Koltchinskii, 2011; B\u00fchlmann and Geer, 2011) and the references therein), as well as in the reinforcement learning setting (Kolter and Ng, 2009; Ghavamzadeh et al.", "startOffset": 2, "endOffset": 68}, {"referenceID": 6, "context": ", (Bickel et al., 2009; Koltchinskii, 2011; B\u00fchlmann and Geer, 2011) and the references therein), as well as in the reinforcement learning setting (Kolter and Ng, 2009; Ghavamzadeh et al.", "startOffset": 2, "endOffset": 68}, {"referenceID": 14, "context": ", 2009; Koltchinskii, 2011; B\u00fchlmann and Geer, 2011) and the references therein), as well as in the reinforcement learning setting (Kolter and Ng, 2009; Ghavamzadeh et al., 2010; 2011; Maillard, 2011), mainly because it allows for non-trivial performance bounds even when the dimension d of the parameter vector is comparable to the sample size n (or even larger than n) provided that the true parameter vector is sparse (i.", "startOffset": 131, "endOffset": 200}, {"referenceID": 10, "context": ", 2009; Koltchinskii, 2011; B\u00fchlmann and Geer, 2011) and the references therein), as well as in the reinforcement learning setting (Kolter and Ng, 2009; Ghavamzadeh et al., 2010; 2011; Maillard, 2011), mainly because it allows for non-trivial performance bounds even when the dimension d of the parameter vector is comparable to the sample size n (or even larger than n) provided that the true parameter vector is sparse (i.", "startOffset": 131, "endOffset": 200}, {"referenceID": 2, "context": "If we use \u2016 \u00b7\u2016 as the `-norm, we recover procedures similar to the square-root Lasso (Belloni et al., 2010) and the Lasso (Tibshirani, 1996) for the estimators studied in Sections 3.", "startOffset": 85, "endOffset": 107}, {"referenceID": 24, "context": ", 2010) and the Lasso (Tibshirani, 1996) for the estimators studied in Sections 3.", "startOffset": 22, "endOffset": 40}, {"referenceID": 11, "context": "The theory of Inverse Problems is very pertinent to this work, and it is important to study our results under the light of those shown in Chapter 2 of Kirsch (2011); Alquier et al.", "startOffset": 151, "endOffset": 165}, {"referenceID": 0, "context": "The theory of Inverse Problems is very pertinent to this work, and it is important to study our results under the light of those shown in Chapter 2 of Kirsch (2011); Alquier et al. (2011). The existing knowledge of inverse prob-", "startOffset": 166, "endOffset": 188}, {"referenceID": 18, "context": "The idea of having learning problems as inverse problems is not new; Rosasco (2006); Vito et al.", "startOffset": 69, "endOffset": 84}, {"referenceID": 18, "context": "The idea of having learning problems as inverse problems is not new; Rosasco (2006); Vito et al. (2006) study regression in Hilbert spaces as an inverse problem.", "startOffset": 69, "endOffset": 104}], "year": 2012, "abstractText": "Motivated by value function estimation in reinforcement learning, we study statistical linear inverse problems, i.e., problems where the coefficients of a linear system to be solved are observed in noise. We consider penalized estimators, where performance is evaluated using a matrix-weighted two-norm of the defect of the estimator measured with respect to the true, unknown coefficients. Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared. We propose simple, yet novel and theoretically well-founded data-dependent choices for the regularization parameters for both cases that avoid datasplitting. A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients, thus allowing the complete separation of the analysis of the stochastic properties of these errors. We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning.", "creator": "LaTeX with hyperref package"}}}