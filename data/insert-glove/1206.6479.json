{"id": "1206.6479", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "The Landmark Selection Method for Multiple Output Prediction", "abstract": "Conditional modeling x \\ noncombat to multimode y is e200 a bjp central seeiso problem in totino machine learning. r.h. A allative substantial borns research effort is devoted to abrahamian such ludu modeling weight-loss when x is high unmc dimensional. dinking We lasso consider, 24700 instead, the fanad case 984,000 of a desecrated high 4-45 dimensional ado.net y, where dmi x ecma is elmdale either low dimensional aka or sundial high hirsutus dimensional. Our unenforced approach is based unlikley on selecting cropp a downgraded small subset baselios y_L of bunnie the dimensions immigrating of y, masonry and rockslides proceed by modeling (i) kaj x \\ tilakraj to dilbert y_L avifauna and (ii) y_L \\ to y. preempts Composing these two models, we obtain a familiarity conditional baradari model x \\ gal\u00e1n to philological y pollert that cromwell possesses convenient statistical spinet properties. faline Multi - areala label pills classification and weipa multivariate liscio regression experiments iqs on enele several datasets magnetism show that sts-117 this m\u00e4rkischer model dorra outperforms hellbent the ot\u00e1vio one player-controlled vs. silurians all approach odel as symbolism well lufisto as germanized several sophisticated percier multiple subtribes output prediction ugadi methods.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (170kb)", "http://arxiv.org/abs/1206.6479v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["krishnakumar balasubramanian", "guy lebanon"], "accepted": true, "id": "1206.6479"}, "pdf": {"name": "1206.6479.pdf", "metadata": {"source": "CRF", "title": "The Landmark Selection Method for Multiple Output Prediction", "authors": ["Krishnakumar Balasubramanian", "Guy Lebanon"], "emails": ["krishnakumar3@gatech.edu", "lebanon@cc.gatech.edu"], "sections": [{"heading": "1. Introduction", "text": "Conditional modeling x 7\u2192 y is a central problem in machine learning. Specific cases include classification, where y is a discrete random variable, and regression, where y is a continuous random variable. Much of the attention in recent years has focused on the case where x is a high dimensional vector. In this case, traditional statistical methods are inefficient due to overfitting. Proposed alternatives for high dimensional x include feature selection and regularized models.\nWe consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. The baseline approach in this case is to independently construct models x 7\u2192 yi \u2208 R for i = 1, . . . , k (assuming y is a k-dimensional real vector). This approach has the advantage of drawing from a wide variety of available single output models, including linear and non-linear regression, logistic regression, and support vector machines. The main disadvantage is that the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nindependent models do not take advantage of a likely correlation between the dimensions of y. Incorporating this correlation becomes especially important when the dimensionality of y is higher or of similar order to the dimensionality of x.\nOur approach is based on selecting a small subset L \u2282 {1, . . . , k} of the dimensions of y, and constructing two models:\nx 7\u2192 yL (1)\nyL 7\u2192 y, (2)\nwhere we use the standard notation yL = {yi : i \u2208 L}. We thus have three problems: selecting the subset L, estimating (1), and estimating (2).\nSpecifically, we estimate model (2) in conjunction with selecting L via least-squares regression with group Lasso based hierarchical regularization. The precise model (1) varies, based on whether y is discrete or continuous. It may be any low-dimensional multiple output model, such as multilabel logistic regression and SVM, or multiple linear regression. If the dimensionality of x is high, regularization for model (1) is also necessary.\nThe underlying assumption of our model is that there exists a subset L of the dimensions of y, called landmark variables, such that the remaining dimensions of y may be expressed as a noisy linear combination y = AyL + \u01eb, with sparse coefficients. Several practical data sets exhibit such a kind of relationship. One example is the prediction of future stock prices y from current stock prices x. The relationship y = AyL + \u01eb is motivated by the identical trends of stock prices of multiple companies with a similar business model, or of multiple investment banks with similar holding portfolio. This phenomenon has been well documented in finance under the term cointegration. Another example is the classification of images (x) depending on what objects appear or do not appear in them (y). Obviously, some objects tend to appear or to not appear simultaneously, such as sky and tree, or car and road.\nThe cardinality s of the subset L is typically orders of magnitude lesser than the actual dimension of the\noutput space making the method scale well to ultrahigh dimensional outputs. For example, the naive one vs. all method requires O(k) independent models that need to be learnt from the data, whereas the number of subproblems selected in the proposed approach scales at the rate of O(s). Assuming s \u226a d we see that there is a huge advantage in terms of number of subproblems selected.\nWe report in this paper experimental results for classification and regression on multiple datasets. Based on our experimental study, we conclude that our model outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods."}, {"heading": "2. Related Work", "text": "Several methods have been proposed for multi-output prediction both in regression and classification setting. In the regression setting, most approaches have focused on penalization of the regression matrix or input space sharing. For example (Izenman, 1975) introduced low-rank penalization of the regression matrix, which was analyzed in (Reinsel & Velu, 1998) in the low-dimensional setting. Recent work focused on analyzing penalized regression in high dimensions (Rohde & Tsybakov, 2011). An alternative approach that is directly applicable to multi-output prediction is group lasso (Yuan & Lin, 2006). Though these methods are popular and widely applicable, they do not directly model correlations in the output dimensions, which can be used to reduce the complexity of the problem. A notable exception is the curds and whey method (Breiman & Friedman, 1997) which uses shrinkage techniques in output space to reduce prediction error.\nIn the classification setting, the popular approach of one-vs-all was proposed by several researchers (see (Rifkin & Klautau, 2004) for a discussion). This method ignores the dependencies between the different dimensions of y, and is inefficient when y is high dimensional. A summary of improvements over the one-versus-all method is available in (Tsoumakas et al., 2010). Alternative approaches assume a class hierarchy on the output space (Cesa-Bianchi et al., 2006), graph structure on the output space (Vens et al., 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al., 2006).\nA paper related to our proposed method is (Hsu et al., 2009), which consider multi-label prediction in a sparse high-dimensional output space. Their proposed method for multi-label classification is to randomly project y and construct a regression model on the reduced subspace. There are two significant dif-\nferences between this paper and our approach: (i) our approach uses data-dependent transformation, rather than a random projection, and (ii) our approach selects a subset of the dimensions of y that contributes to computational efficiency, statistical analysis, and is in line with some practical scenarios (see previous section). Furthermore, the approach by (Hsu et al., 2009) might not be applicable in the regression setting, as output sparsity assumption does not hold for regression in practice.\nRecently proposed variations on (Hsu et al., 2009) include (Tai & Lin, 2012) that propose to reduce the dimensionality of the output space by PCA, and (Bi & Kwok, 2011) that propose to reduce the label space by preserving a graph structure hierarchy on y. While these methods are sub-linear, they still project on to a low-dimensional real subspace, and hence they do not guarantee that the problem in the reduced subspace is easier than the original problem.\nOur approach also has a close connection to sparse PCA (Zou et al., 2006). Two significant differences are: (i) sparse PCA is generally applied to the covariates x rather than y, and (ii) our focus is on identifying the landmarks L and the relationship yL 7\u2192 y rather than estimating the principal components themselves."}, {"heading": "3. The landmark selection method:", "text": "We consider a common scenario where x \u2208 Rd and y \u2208 Y, where Y is either Rk (regression) or {0, 1}k (classification), and k, d are high dimensional. We denote the data matrices, containing n labeled samples, by X \u2208 Rn\u00d7d and Y \u2208 Rn\u00d7k.\nStep 1: Selecting the landmark set L and modeling (2)\nA convenient way to select the set of landmark dimensions L, and to model (2) simultaneously is the following regularized least squares regression model\nA\u0302 = argmin A\u2208Rk\u00d7k\n\u2016Y \u2212 Y A\u20162F + \u03bb1\u2016A\u20161,2 + \u03bb2\u2016A\u20161 (3)\nwhere\n\u2016A\u2016F def = \u221a tr(A\u22a4A)\n\u2016A\u20161,2 def = k \u2211\ni=1\n\u221a \u221a \u221a \u221a k \u2211\nj=1\nA2i,j\n\u2016A\u20161 def =\nk \u2211\ni=1\nk \u2211\nj=1\n|Aij |.\nThe first term in (3) is the least squares empirical risk that is standard in linear regression models. Obvi-\nously, the identity A = I minimizing that term constitutes a trivial solution that is ineffective when y is high dimensional. The second and third terms in (3) promote a \u201csmall\u201d A and thus prevent the estimated model to be the trivial minimizer I of the first term.\nMuch like group lasso, the second term in (3) enforces joint group sparsity across the rows of A. To see this note that \u2016A\u20161,2 is the L1 norm of the L2 norms of the individual rows. Due to the sparsity promoting nature of the L1 minimizer, A\u0302 will have only a few rows that are not identically zero. The resulting effect is the selection of landmark dimensions yL where L corresponds to the non-zero rows. We thus have that the first two terms in (3) simultaneously select the landmark dimensions L, and model yL 7\u2192 y. The third term \u2016A\u20161 promotes sparsity within the coefficients of the model yL 7\u2192 y. This additional sparsity assumption reduces the prediction risk when y is high dimensional.\nThe regularization parameter \u03bb1 controls the number of landmark output dimensions. The regularization parameter \u03bb2 controls the sparsity of the model yL 7\u2192 y. Both \u03bb1 and \u03bb2 should increase with k. When the landmark assumption holds and there exists a landmark set L\u2217 such that y is a noisy sparse linear combination of yL\u2217 , the row sparsity pattern of A\u0302 should coincide with L\u2217 (assuming an appropriate selection of \u03bb1, \u03bb2). As \u03bb1/\u03bb2 increases, the group sparsity constraints become dominant implying that each dimension of y depends on all of the dimensions of yL. As \u03bb1/\u03bb2 decreases, A\u0302 tends to be more sparse within groups, implying that the dimensions of y are sparse linear combinations of the yL.\nFrom a practical point of view, with a proper selection of the regularization parameters \u03bb1, \u03bb2 (for example using cross-validation), the model (3) is quite flexible. It allows handling situations involving large landmark sets L and small landmark sets L, and high or low sparsity for the model yL 7\u2192 y. Empirically, the dependence on the precise value of \u03bb1, \u03bb2 is robust, as small variations in \u03bb1, \u03bb2 do not substantially change the predicted values.\nHandling non-linear output relationship: In order to select and learn non-linear relationships between the outputs and the landmarks, one could use functional joint sparsity models with L1/L\u221e constraints as proposed by (Liu et al., 2008) or with L1+ L1/L2 constraints (appropriately defined on a function space). With this change in step 1, the proposed approach could be used to handle non-linear relationships between the outputs, making the proposed method more flexible. Developing concrete algorithms and analysis for this setting is left as future work.\nStep 2: Estimating (1)\nOnce the landmark outputs L are identified, we can proceed with fitting model (1). In the case of continuous y (regression), model (1) can be estimated using a using multivariate regression model. In the case of a discrete y (classification), a one vs. all classifier may be used for x 7\u2192 yi, i = 1, . . . , s, or alternatively a multiple output classifier may be used for x 7\u2192 yL. Examples include support vector machines and loglinear models. From a statistical perspective, when y is high dimensional the reduction in the number of estimated parameters from kd to sd (in the regression setting) where s \u226a k, contributes to lower prediction risk. If the dimensionality of x is also high, the models x 7\u2192 yL or x 7\u2192 yi should use careful feature selection or regularization to avoid overfitting.\nStep 3: Prediction\nIn many cases, a statistical model for (1) provides not only point estimates, but also a full probabilistic model p(yL|x). Similarly, a statistical model for (2) provides a full probabilistic model for p(y|yL). The implied model\np(y|x) = p(y|yL)p(yL|x)\nsuggests the following procedure for predicting y from x\ny\u2217L = argmax yL p(yL|x) (4)\ny\u2217Lc = argmax yLc\n\u222b\np(y|yL)p(yL|x) dyL. (5)\nAn alternative to (5) is to use the following approximation\nargmax y p(y|x) \u2248 argmax y p\n(\ny| argmax yL p(yL|x)\n)\n.\nIn other words, given a new test sample x, we predict yL using the model from step 2, and then estimate yLc using the model from step 1, operating on the predicted yL. In the case of classification, we follow standard practice and set the components of y to 1 if the corresponding prediction of model (2) is greater than 0.5 and to 0 if it lesser than 0.5. Finally the outputs are concatenated and they represent the prediction for the given sample x. Algorithm 1 summarizes this procedure."}, {"heading": "4. Theory", "text": "In this section, we give a brief theoretical analysis of the proposed approach in the regression setting highlighting the advantage of the proposed approach. We assume that there exist a true landmark subset L\u2217 and provide conditions under which it could be recovered consistently. Specifically, following the analysis\nAlgorithm 1 Landmark selection method\nInput: data {(x1, y1), . . . , (xn, yn)} in the form of X \u2208 Rn\u00d7d and Y \u2208 Rn\u00d7k Step 1: Simultaneously find the landmark set L and solve the optimization problem in step 1 to obtain the model yL \u2192 y and estimate A\u0302. Step 2: Estimate the model x \u2192 yL using independent models for each component of yL or using multiple-output classification or regression algorithms. Step 3: Given a new test point x, estimate y by (4)-(5).\ndeveloped in (Obozinski et al., 2011) for random design linear regression with group Lasso regularization, we can get a lower bound on the number of samples needed for recovering the support of the subset L\u2217 of the landmark labels. For simplicity, we consider the regression setting with the assumption that \u03bb2 = 0.\nWe assume that Y consists of i.i.d. rows sampled from N(0,\u03a3). This distribution could in fact be any subGaussian distribution (which includes any bounded random variable for example the Bernoulli random variable) for which a similar analysis could be carried out. We make the following assumption on the the covariance matrix \u03a3: (1) there exists \u03c1min > 0 and \u03c1max < \u221e such that all the eigenvalues of the s \u00d7 s covariance matrix \u03a3s of the the landmark output yL \u2208 R\ns are contained in the closed interval [\u03c1min, \u03c1max], (2) mutual incoherence: there exist a incoherence parameter \u03b3 \u2208 (0, 1] such that \u2016\u03a3ScSc(\u03a3SS)\n\u22121\u2016\u221e \u2264 1\u2212\u03b3 and (3) self-incoherence: there exists Dmax < \u221e such that \u2016(\u03a3SS)\n\u22121\u2016\u221e \u2264 Dmax. Note that these are standard conditions assumed for support recovery results in the modern sparse recovery analysis. Condition (1) is needed to prevent over-dependency between the landmark outputs. Conditions (2) and (3) are necessary conditions for model selection consistency of sparse recovery problems. For example, several classes of matrices, for example Toeplitz matrices, tree-structured matrices and bounded off-diagonal matrices are shown to satisfy the above conditions (Zhao & Yu, 2006). In the absence of these conditions, landmark recovery might fail even with arbitrarily large training set.\nWe also make the following assumption on the regression matrix. Let amin def\n= mini\u2208L \u2016Ai\u20162 where Ai denote the ith non-zero row of the matrix A. We denote As \u2208 Rs\u00d7k to be the subset of the matrix A with nonzero rows, \u03b6(As) \u2208 R\ns\u00d7k to be the row normalized matrix, and\n\u03c6(A) def = \u03bbmax ( \u03b6(As) \u22a4(\u03a3SS) \u22121\u03b6(As) ) .\nThis quantity characterizes the amount of overlap that\ncould be captured given the output samples. Note that the support overlap function \u03c6(A) satisfies\ns\n\u03c1maxK \u2264 \u03c6(A) \u2264\ns\n\u03c1min\nfor any Y that satisfies assumption (1).\nProposition 1. Consider the label matrix Y with rows i.i.d. drawn from N(0,\u03a3) satisfying assumptions (1)- (3), suppose that a2min decays no more slowly than f(k)min{ 1\ns , 1log(k\u2212s)} for some function f(k) such that\nf(k)/s \u2192 0 and f(k) \u2192 \u221e. Then, as long as n > C \u2032\u03c1max\u03c6(A\n\u2217) log(k \u2212 s), we have with probability greater than 1 \u2212 c1 exp (c2 log s): (1) the optimization problem in 3 (with \u03bb2 = 0) has a unique solution when \u03bb1 = \u221a f(k) log k n and (2) the row support specified by the unique solution of the optimization problem 3 is equal to the row support of the true model.\nProof. The proof follows from the corresponding proof in (Obozinski et al., 2011).\nThe main consequence of the above proposition is that if there exist a set of landmark variable L\u2217 in the output space, the sample complexity is of logarithmic order in the original dimension of the output space k. Using sub-Gaussian assumptions on the label matrix, analogous conditions for classification are possible.\nFollowing (Reinsel & Velu, 1998) we note that for a matrix regression problem y = \u0398x + \u01eb with \u0398 \u2208 R\nm1\u00d7m2 , the Frobenious norm error rate (with n samples, unit noise variance and no assumption on the regression matrix)\n\u2016\u0398\u0302\u2212\u0398\u20162F = O (m1m2\nn\n)\n.\nSince in our case the estimated matrix (1) (assuming linear regression model) is of the dimension s\u00d7 d, the error is of the order of O( sd\nn ) samples (Reinsel & Velu,\n1998), much smaller than the classical setting without the landmark selection method of O(kd\nn ). In particu-\nlar, when s \u226a k, there is a significant gain in efficiency.\nWe conclude that the landmark method makes a structural assumption on the output space in order to facilitate regression in high dimensional setting (n \u226a kd). Other methods, making a different set of structural assumptions (e.g., low-rank regression) try to achieve the same goal, but work under a different set of assumptions. Empirically, the landmark method works better than low-rank regression and group Lasso based multivariate regression on a variety of datasets (see Section 6)."}, {"heading": "5. Optimization procedure", "text": "Here, we provide the optimization procedure required to solve the optimization problem described in step 1. The spaRSA method, proposed recently in (Wright et al., 2009), is a solver for optimization problems of the form\nmin a\u2208Rp f(a) + \u03bb\u03c6(a)\nwhere f is a convex loss function and \u03c6 is a convex regularizer. The main advantage of spaRSA is that when the regularizer is group separable, the problem decomposes over the group.\nUsing vectorization and block-diagonalization, it can be shown that (3) falls under this framework. Upon initial investigation, it appears that the blockdiagonalization operation complicates the solver as it increases the size of the data matrix. However, we describe below a variation on spaRSA that works directly with the Y and A. A similar approach was used in (Sprechmann et al., 2011) for the problem of collaborative dictionary learning with hierarchical penalty. The main advantage of the spaRSA procedure (that the problem decouples across groups) is still preserved and further in our case, each subproblem could be solved via thresholding.\nIn order to solve the optimization problem, the spaRSA procedure generates a sequence of updates that converges to the solution. We refer the reader to (Wright et al., 2009) for a complete description of the general procedure. In our case, we let f(Ai) denote the reconstruction error (the squared loss in our case) for Ai (here and below we denote the i-column of a matrix A as Ai) and define the matrix U (t) \u2208 Rk\u00d7k whose i-column is given by\nU (t) i = A (t) i \u2212 (1/\u03b1 t)\u2207f(A (t) i ).\nThe sequence of spaRSA updates that converge to the true solution is\nA(t+1) = argmin Z\u2208Rk\u00d7k \u2016Z \u2212 U (t)\u2016F + \u03bb1 \u03b1(t) \u2016Z\u2016F + \u03bb2 \u03b1(t) \u2016Z\u20161,\nwhich is group separable into k independent problems as below:\nA (t+1) i = argmin\nZi\u2208Rk \u2016Zi \u2212 U\n(t) i \u20162 + \u03bb1 \u03b1(t) \u2016Zi\u20162 + \u03bb2 \u03b1(t) \u2016Z\u20161.\nThe solutions for each of these sub-problems are available in closed form (similar to (Sprechmann et al., 2011)) as follows:\nA (t+1) i =\n{\nmax{0, \u2016h\u20162 \u2212 \u03bb1}h/\u2016h\u20162 if \u2016h\u20162 > 0 0 if \u2016h\u20162 = 0\nwhere hj = sign(U (t) i,j )max{0, |U (t) i,j |\u2212\u03bb2}. The thresholding require operations that are linear in the dimensionality of the matrix Y . The above procedure is repeated until convergence to obtain the final solution that features row sparsity, and potential sparsity within rows as well."}, {"heading": "6. Experiments", "text": "In this section, we compare our landmark selection method, which we refer to below as moplms, to alternative baselines on classification and regression problems. In our experiments we used code from (Wright et al., 2009) for performing the mixed norm penalty (group lasso and lasso) landmark selection. The regularization parameters were set by cross-validation."}, {"heading": "6.1. Synthetic experiments", "text": "We conducted an experiment on synthetic regression data with k = 500 (dimensionality of y), d = 500 (dimensionality of x). The number of landmark outputs s was varied in the set {50, 100, 200}. The data was simulated from the above model, including the specified landmark outputs. Figure 1 (left) shows the plot of the test MSE prediction error as a function of the sample size for various values of the parameter s/k.\nFrom section 4, we have that if the landmark output selection method is not used, with a linear regression model for x \u2192 y,the Frobenious norm error between the true and estimated matrix scales as O (\nkd n\n)\n. Where as with the landmark output assumption the error for model 1 scales as O (\nsd n\n)\n. This benefit in the estimation error of the regression matrix is reflected in the MSE prediction error. Specifically, as s decreases, the sample complexity decreases. This phenomenon is especially important in high-dimensional cases, when there are fewer samples than the number of parameters to be estimated. We also compared the proposed approach to group-Lasso and low-rank multivariate regression.\nFigure 1 (middle) shows the mse prediction error rate of the moplms method decays faster compared to the other methods.\nWe also experimented with synthetic classification data where y is a 500 dimensional binary vector and the input x \u2208 R500. Similar to the regression setting, the landmark outputs were first generated with s \u2208 {50, 100, 200} and the dependent outputs where generated as sparse linear combination of the landmark outputs. Figure 1 (right) shows the Hamming loss as a function of the sample size. The x 7\u2192 yL model was collection of multiple one-vs-all SVMs. Similar to the regression case, the prediction error decays with the number of landmark outputs s/k. We further compare the proposed approach on synthetic data set against the following methods:\n1. One vs. all: This is a standard baseline approach for multi-label classification, for e.g., (Rifkin & Klautau, 2004).\n2. Multilabel compressive sensing (mlcs): This approach was proposed in (Hsu et al., 2009) where the label vector is projected to a random m dimensional sub-space followed by regression on the compressed subspace.\n3. Multi-label classification via canonical correlation analysis (ml-cca): After performing canonical correlation analysis (CCA) on the input and output variables, a model is learned in the resulting subspace, followed by projection to the original label space.\nFrom Figure 2, we note that the proposed approach has a better rate of decay of hamming loss compared to the other approaches. This phenomenon is further observed in the real world data sets as described in the next section.\nWe conducted an additional experiment to study the number of sub-problems selected. Specifically, we var-\nied the number of sub-problems and the tuning parameters of mlcs, and noted the values achieving the lowest prediction error. We then trained moplms, gradually reducing the regularization parameter until the prediction error matched that of mlcs. The two methods achieved identical prediction error with the following (mean) values of s/k: 0.45 (mlcs) and 0.30 (moplms), indicating moplms selected fewer sub-problems while achieving identical performance. Note, however, that mlcs always uses base regressors and moplms uses base classifiers."}, {"heading": "6.2. Real-world data sets", "text": ""}, {"heading": "6.2.1. Classification", "text": "We experimented with the following two multiple output classification datasets.\n1. del.icio.us This dataset consists of data from del.icio.us, a social bookmarking site where webpages are labeled with multiple contextual tags. The data set contains about 16000 labeled web page and 983 unique labels. We follow the experimental setup followed in (Hsu et al., 2009) and represent web page as a boolean bag-of-words vector, with the vocabulary chosen using a combination of frequency thresholding and \u03c72 feature ranking, resulting in 500 features.\n2. Image data set. This dataset contains 68000 images, with about 22000 unique word tags for each image. Following (Hsu et al., 2009) we retained the 1000 most frequent labels. We represented each image via codes computed with a learned dictionary (of size 1024) via sparse coding (Yang et al., 2009). Specifically, we densely sampled 10\u00d710 patches from the image and computed sparse codes. Finally max-pooling was used to pool the codes obtained for the patches.\nNote that we use thresholding to convert the real output to the binary form of the data. The regulariza-\nDelicious Image Ham. loss F-score Ham. loss F-score\nmlcs 0.0187 0.3732 0.0047 0.3012 ml-cca 0.0164 0.3822 0.0041 0.3183 one.vs.all 0.0144 0.4512 0.0034 0.3923 moplms 0.0142 0.4522 0.0032 0.4031\nTable 1. Test set Hamming loss and F1 measure evaluation of the four classification approaches: mlcs, ml-cca, one vs. all, and moplms. The base classifiers in the reduced space were SVM.\ntion parameters \u03bb1 and \u03bb2 were estimated using crossvalidation. The number of selected landmarks s was 231 for the del.ic.ious data and 278 for the image data set. This was less than the number of sub-problems in both the mlcs and ml-cca approaches, which were also tuned for optimal prediction error.\nTable 1 displays the F1-score and hamming loss that are two standard evaluation metrics for multi-label classification.\nHamming loss = y\u22a41+ y\u0302\u22a41\u2212 2y\u22a4y\u0302\nk\nF1 score = 2y\u22a4y\u0302\n\u2211k i=1 yi + \u2211 i=1 y\u0302i .\nThe landmark selection method performed better in terms of both evaluation metrics. The one-versus-all method was the second best in terms of prediction accuracy, but takes a significantly greater amount of train and test time, compared to the alternative methods.\nFigure 3 (left and middle) shows the decay of the Hamming loss as a function of the sample size for mlcs, moplms and ml-cca method. We omitted the one-vs-all method as it took significantly more amount of time compared to the other approaches, and thus is not computationally attractive. The proposed landmark selection approach has lower prediction error than mlcs and ml-cca."}, {"heading": "6.2.2. Regression", "text": "In the regression setting, we consider predicting the stock prices of several companies based on previous values via the landmark selection approach on the SP 500 data set. More specifically, the data consists of closing stock prices of the 500 companies in the S&P index in the period from August 21, 2009 to August 20, 2010 (a total of 245 entries). We assume the following autoregressive 1 or AR(1) model\nytL = Byt\u22121L + E (6)\nwhere yt = log St\nSt\u22121 represents the log returns (St is\nthe stock price at time t) for day t and E is the noise matrix. The problem is motivated by the observation in finance that multiple companies have stock prices that share identical stochastic trends (cointegration).\nWe compare our landmark selection approach to lowrank multivariate regression (using trace norm regularization) and group lasso based multivariate regression. These two baselines are popular multivariate regression methods. In our case (moplms), we used a multivariate ridge regression for estimating model (1), which is Equation 6 in the current setting. As in the classification setting, the regularization parameter was tuned by cross validation, and resulted in s = 98 landmark outputs.\nTable 2 shows that moplms outperformed the two baselines (group lasso and low-rank multivariate regression). Figure 3 displays the prediction error rate as a function of the sample size. It confirms this conclusion as the prediction error of moplms decays faster than the baselines."}, {"heading": "7. Discussion", "text": "In this paper we propose a framework for multi-output prediction based on parsimonious modeling on the output space. By selecting a subset of the output dimensions (landmarks) and focusing on modeling the dependency of that subset of y on x, we reduce the sample complexity considerably. This is most noticeable when the output dimensionality is high and the different component feature high correlation. Our experiments indicate that the proposed method outperforms standard multi-output methods in both the classification and regression scenarios.\nThe results in this paper raise several interesting questions and follow up directions. First, a detailed analysis is required to characterize the improvements of the proposed methods over competing methods. Second it is interesting to consider cases in which the label vector has a pattern of missingness.\nAcknowledgments: The authors would like to thank Kai Yu and Parikshit Ram for discussions."}], "references": [{"title": "Multi-label classification on tree and dag structured hierarchies", "author": ["W. Bi", "J. Kwok"], "venue": null, "citeRegEx": "Bi and Kwok,? \\Q2011\\E", "shortCiteRegEx": "Bi and Kwok", "year": 2011}, {"title": "Predicting multivariate responses in multiple linear regression", "author": ["L. Breiman", "J.H. Friedman"], "venue": "Journal of the Royal Statistical Society:B,", "citeRegEx": "Breiman and Friedman,? \\Q1997\\E", "shortCiteRegEx": "Breiman and Friedman", "year": 1997}, {"title": "Incremental algorithms for hierarchical classification", "author": ["N. Cesa-Bianchi", "C. Gentile", "L. Zaniboni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": null, "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Reduced-rank regression for the multivariate linear model", "author": ["A.J. Izenman"], "venue": "Journal of multivariate analysis,", "citeRegEx": "Izenman,? \\Q1975\\E", "shortCiteRegEx": "Izenman", "year": 1975}, {"title": "Nonparametric regression and classification with joint sparsity constraints", "author": ["H. Liu", "J. Lafferty", "L. Wasserman"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "I. support union recovery in high-dimensional multivariate regression", "author": ["G. Obozinski", "M.J. Wainwright", "M. Jordan"], "venue": "Annals of statistics,", "citeRegEx": "Obozinski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Obozinski et al\\.", "year": 2011}, {"title": "Multivariate reducedrank regression: theory and applications", "author": ["G.C. Reinsel", "R.P. Velu"], "venue": null, "citeRegEx": "Reinsel and Velu,? \\Q1998\\E", "shortCiteRegEx": "Reinsel and Velu", "year": 1998}, {"title": "In defense of one-vs-all classification", "author": ["R. Rifkin", "A. Klautau"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Rifkin and Klautau,? \\Q2004\\E", "shortCiteRegEx": "Rifkin and Klautau", "year": 2004}, {"title": "Estimation of highdimensional low-rank matrices", "author": ["A. Rohde", "A.B. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Rohde and Tsybakov,? \\Q2011\\E", "shortCiteRegEx": "Rohde and Tsybakov", "year": 2011}, {"title": "C-hilasso: A collaborative hierarchical sparse modeling framework", "author": ["P. Sprechmann", "I. Ram\u0131\u0301rez", "G. Sapiro", "Y.C. Eldar"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Sprechmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sprechmann et al\\.", "year": 2011}, {"title": "Multi-label classification with principle label space transformation", "author": ["F. Tai", "H.T. Lin"], "venue": "Neural Computation,", "citeRegEx": "Tai and Lin,? \\Q2012\\E", "shortCiteRegEx": "Tai and Lin", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2006}, {"title": "Mining multi-label data. Data mining and knowledge discovery handbook", "author": ["G. Tsoumakas", "I. Katakis", "Vlahavas"], "venue": null, "citeRegEx": "Tsoumakas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2010}, {"title": "Decision trees for hierarchical multilabel classification", "author": ["C. Vens", "J. Struyf", "L. Schietgat", "S. D\u017eeroski", "H. Blockeel"], "venue": "Machine Learning,", "citeRegEx": "Vens et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vens et al\\.", "year": 2008}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Wright et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2009}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "For example (Izenman, 1975) introduced low-rank penalization of the regression matrix, which was analyzed in (Reinsel & Velu, 1998) in the low-dimensional setting.", "startOffset": 12, "endOffset": 27}, {"referenceID": 13, "context": "A summary of improvements over the one-versus-all method is available in (Tsoumakas et al., 2010).", "startOffset": 73, "endOffset": 97}, {"referenceID": 2, "context": "Alternative approaches assume a class hierarchy on the output space (Cesa-Bianchi et al., 2006), graph structure on the output space (Vens et al.", "startOffset": 68, "endOffset": 95}, {"referenceID": 14, "context": ", 2006), graph structure on the output space (Vens et al., 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 12, "context": ", 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al., 2006).", "startOffset": 90, "endOffset": 119}, {"referenceID": 3, "context": "A paper related to our proposed method is (Hsu et al., 2009), which consider multi-label prediction in a sparse high-dimensional output space.", "startOffset": 42, "endOffset": 60}, {"referenceID": 3, "context": "Furthermore, the approach by (Hsu et al., 2009) might not be applicable in the regression setting, as output sparsity assumption does not hold for regression in practice.", "startOffset": 29, "endOffset": 47}, {"referenceID": 3, "context": "Recently proposed variations on (Hsu et al., 2009) include (Tai & Lin, 2012) that propose to reduce the dimensionality of the output space by PCA, and (Bi & Kwok, 2011) that propose to reduce the label space by preserving a graph structure hierarchy on y.", "startOffset": 32, "endOffset": 50}, {"referenceID": 19, "context": "Our approach also has a close connection to sparse PCA (Zou et al., 2006).", "startOffset": 55, "endOffset": 73}, {"referenceID": 5, "context": "Handling non-linear output relationship: In order to select and learn non-linear relationships between the outputs and the landmarks, one could use functional joint sparsity models with L1/L\u221e constraints as proposed by (Liu et al., 2008) or with L1+ L1/L2 constraints (appropriately defined on a function space).", "startOffset": 219, "endOffset": 237}, {"referenceID": 6, "context": "developed in (Obozinski et al., 2011) for random design linear regression with group Lasso regularization, we can get a lower bound on the number of samples needed for recovering the support of the subset L of the landmark labels.", "startOffset": 13, "endOffset": 37}, {"referenceID": 6, "context": "The proof follows from the corresponding proof in (Obozinski et al., 2011).", "startOffset": 50, "endOffset": 74}, {"referenceID": 15, "context": "The spaRSA method, proposed recently in (Wright et al., 2009), is a solver for optimization problems of the form", "startOffset": 40, "endOffset": 61}, {"referenceID": 10, "context": "A similar approach was used in (Sprechmann et al., 2011) for the problem of collaborative dictionary learning with hierarchical penalty.", "startOffset": 31, "endOffset": 56}, {"referenceID": 15, "context": "We refer the reader to (Wright et al., 2009) for a complete description of the general procedure.", "startOffset": 23, "endOffset": 44}, {"referenceID": 10, "context": "The solutions for each of these sub-problems are available in closed form (similar to (Sprechmann et al., 2011)) as follows:", "startOffset": 86, "endOffset": 111}, {"referenceID": 15, "context": "In our experiments we used code from (Wright et al., 2009) for performing the mixed norm penalty (group lasso and lasso) landmark selection.", "startOffset": 37, "endOffset": 58}, {"referenceID": 3, "context": "Multilabel compressive sensing (mlcs): This approach was proposed in (Hsu et al., 2009) where the label vector is projected to a random m dimensional sub-space followed by regression on the compressed subspace.", "startOffset": 69, "endOffset": 87}, {"referenceID": 3, "context": "We follow the experimental setup followed in (Hsu et al., 2009) and represent web page as a boolean bag-of-words vector, with the vocabulary chosen using a combination of frequency thresholding and \u03c7 feature ranking, resulting in 500 features.", "startOffset": 45, "endOffset": 63}, {"referenceID": 3, "context": "Following (Hsu et al., 2009) we retained the 1000 most frequent labels.", "startOffset": 10, "endOffset": 28}, {"referenceID": 16, "context": "We represented each image via codes computed with a learned dictionary (of size 1024) via sparse coding (Yang et al., 2009).", "startOffset": 104, "endOffset": 123}], "year": 2012, "abstractText": "Conditional modeling x 7\u2192 y is a central problem in machine learning. A substantial research effort is devoted to such modeling when x is high dimensional. We consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. Our approach is based on selecting a small subset yL of the dimensions of y, and proceed by modeling (i) x 7\u2192 yL and (ii) yL 7\u2192 y. Composing these two models, we obtain a conditional model x 7\u2192 y that possesses convenient statistical properties. Multi-label classification and multivariate regression experiments on several datasets show that this method outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}