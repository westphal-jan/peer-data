{"id": "1206.6856", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Reasoning about Uncertainty in Metric Spaces", "abstract": "adjoined We set up a vignesh model .182 for 497.2 reasoning about paolillo metric spaces telecomasia with underestimates belief vijaynagar theoretic measures. The uncertainty in blackjacks these shikai spaces bas-rhin stems destler from cityscape both probability ventura and metric. meik To helliniko represent mint.com both aspect of rhodin uncertainty, shpetim we choose melan\u00e7on an ballmer expected fatehpur distance 5.47 function padawan as four-bay a tardes measure dr\u00f8bak of krla uncertainty. A haplotype formal isel logical system nbcolympics.com is under-11 constructed for tambi the reasoning about 6,620 expected distance. 611 Soundness 110.12 and completeness robida are shown for this beforu logic. hyperreal For reasoning on blakstad product metric ordaz space formula with uncertainty, a new metric is defined and shown syllogisms to usair have good properties.", "histories": [["v1", "Wed, 27 Jun 2012 16:26:12 GMT  (157kb)", "http://arxiv.org/abs/1206.6856v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["seunghwan lee"], "accepted": false, "id": "1206.6856"}, "pdf": {"name": "1206.6856.pdf", "metadata": {"source": "CRF", "title": "Reasoning about Uncertainty in Metric Spaces", "authors": ["Seunghwan Han Lee"], "emails": ["seunlee@cs.indiana.edu"], "sections": [{"heading": null, "text": "We set up a model for reasoning about metric spaces with belief theoretic measures. The uncertainty in these spaces stems from both probability and metric structures. To represent both aspect of uncertainty, we choose an expected distance function as a measure of uncertainty. A formal logical system is constructed for the reasoning about expected distance. Soundness and completeness are shown for this logic. For reasoning on product metric spaces with uncertainty, a new metric is defined and shown to have good properties."}, {"heading": "1 Introduction", "text": "For formal representation of uncertainty with probabilistic or belief theoretic measures, there exists a wide spectrum of models with complete logical systems (eg. see Halpern 2003, Nilsson 1986, Bacchus 1990, Gerla 1994). There also exists a wide variety of formal systems on spatial reasoning (eg. see Gabelaia et al. 2005, Asher & Vieu 1995, Lemon & Pratt 1998) and especially on reasoning about metric spaces (eg. see Kutz et al. 2003, Wolter & Zakharyaschev 2005). But it is hard to find a formal system when probabilistic uncertainty is present on metric spaces.\nIn probabilistic or statistical analysis, it is usually assumed that values of random variables are real numbers. Since most of statistical inference is related to reasoning about expectation and variance, restricting the range of random variables to the spaces where those values are well defined is considered as an acceptable sacrifice. A formal reasoning system on expectations with respect to probability measures and other belief theoretic measures can be found in (Halpern & Pucella 2002).\nEven though expectations of an event cannot be defined on arbitrary metric spaces, the expectation of distances with respect to a fixed set is well defined in any metric space. For example, an expectation of a random location on a sphere cannot be defined, but an expected distance of a random location from the south hemisphere is a well defined notion. The expected distance is often interpreted as a loss function in statistical decision theory (Berger 1985).\nLoss(a) = EP (d(X, a)).\nIn this case, the distance function d(x, a) with respected to a fixed point a is used to calculate the cost for predicting x when a is a true state. A complimentary concept of a cost function in decision theory is a utility function. The expected utility function has been an important topic in economics for a long time (Von Neumann & Morgenstern 1944; Savage 1954; Schmeidler 1989). We can also find more generalized expected utility functions defined for plausibility measures and generalized utility functions. (Chu & Halpern 2003; 2004).\nThe objective of a decision problem is to find an action that minimizes a loss function. For example, if we have a uniform prior probability distribution on [0, 1] where d(x, y) = |x\u2212 y|, the loss minimizing prediction would be X = 0.5. But in inference problems, the objective is to derive more information from given information. As an example, for the previous distance function, if we know that EP (d(X, 0)) = 1, then we can deduce that EP (d(X, 1)) = 0. For inference problems, a viewpoint from fuzzy logic is also meaningful. Let\u2019s assume that a fuzzy set Da represents a fuzzy concept \u201cdissimilar from a\u201d for a point a in a 1-bounded metric space \u2126. A natural candidate for a membership function of Da would be d(x, a). Given a probability distribution P , Zadeh (1968) introduces the probability of a fuzzy concept.\nP (Da) = \u222b\n\u2126\n\u00b5DadP = EP (d(X, a)).\nWe can interpret this as an expected dissimilarity of a from \u2126 \u2212 {a}. Even though probability of fuzzy events is one possible representation of uncertainty in metric spaces, no formal reasoning system was provided yet. There has been various attempts to generalize Shannon\u2019s entropy to the metric spaces using expected similarity and expected distance (Yager 1992; Lee 2006).\nIn this paper, we will generate a formal axiomatic reasoning system using expected distance measures and prove soundness and completeness of the system. Since the expected distance function turns out to be a doubt function, we can also define various dual measures. For reasoning on product spaces, we will introduce a new product metric and define mutual independence with respect to expected distance."}, {"heading": "2 Expected distance", "text": "A probability space is a tuple (\u2126,F , P ) where \u2126 is a set, F is a \u03c3-algebra of subsets of \u2126 and P : F \u2192 [0, 1] is a function satisfying the properties of probability\n\u2022 Prob1. P (A) \u2265 0 for any A \u2208 F . \u2022 Prob2. P (\u2126) = 1. \u2022 Prob3. P (\u22c2\u221ei=1 Ai) = \u2211\u221e i=1 P (Ai)\nif Ai \u2282 F are disjoint.\nTo exclude problems with measurability, we will further assume that F = P\u2126. In most of applications \u2126 is a finite set and P is defined at every point.\nA 1-bounded pseudo metric space is a tuple (\u2126, d) where \u2126 is a set and d is a function on \u2126 \u00d7 \u2126 that satisfies\n\u2022 PMet1. d(x, x) = 0 (reflexive) \u2022 PMet2. d(x, y) = d(y, x) (symmetry) \u2022 PMet3. d(x, y) + d(y, z) \u2265 d(x, z) (triangular in-\nequality)\n\u2022 PMet4. d(x, y) \u2264 1 (1-bounded)\nfor all x, y, z \u2208 \u2126. To become a metric space, d should satisfy an extra condition\nIf d(x, y) = 0 then x = y.\nIn practice, this condition is too strong for a reasoning system. Allowing a zero distance between two instances is a common approach in spatial reasoning.\nWe further assume that the metric in our system is bounded by 1 because of following reasons. First,\nin most applications distance functions are practically bounded. Even if it is not bounded, there are conventional methods that normalize a metric space (\u2126, d) into a 1 bounded metric space such as (\u2126, d1+d ). Especially if d is bounded by M , (\u2126, dM ) is a straightforward conversion. Second, if metrics are not bounded by a certain number, it is hard to compare distances from multiple metrics. To get a normalized degree of uncertainty, a bound is a necessity. Third, 1 bounded metrics works well with probability or belief functions that are also 1 bounded. As an example, since Pr(A) is bounded by 1, for independent events A,B, we have Pr(A and B) \u2264 Pr(A). Similar reasoning for expected distances when the distance functions are bounded.\nDefinition 1. Let\u2019s assume that (\u2126,F , P, d) is a metric probability space with a probability measure P on a \u03c3-algebra F = P\u2126 and a 1 bounded pseudometric d. For a given subset U \u2282 \u2126, the expected distance of U is defined as\nedP,d(U) = \u222b\n\u2126\nd(x,U)dP\n= \u222b\nU\nd(x,U)dP\nwhere the distance between a point and a set is conventionally defined as\nd(x,U) = min y\u2208U d(x, y).\nSince d(x, \u2205) = 1 and d(x, \u2126) = 0 we have\nedP,d(\u2205) = 1 , edP,d(\u2126) = 0 .\nEven though an expected distance of a set U is defined using P , when we perform an inference on edP,d(U) we do not assume any knowledge of P . Therefore, we can safely omit P from the notation in inference problems under unknown probability distribution P . We will also omit d if there is no confusion for a base metric. Since we assumed that F = P\u2126, d(x,U) is always a measurable function.\nWe can also define expected distance functions with respect to any kind of belief theoretic measures (Shafer 1976). The only difference is that the probability measure P is changed into one of belief theoretic functions. A belief function Bel is a function that satisfies\n\u2022 B1. Bel(A) \u2265 0 for any A \u2208 F . \u2022 B2. Bel(\u2126) = 1. \u2022 B3. Bel(\u22c3ni=1 Ai) \u2265\u2211n\nr=1 (\u22121)r+1 \u2211 {I\u2032\u2282{1,\u00b7\u00b7\u00b7 ,n}||I\u2032|=r} Bel( \u22c2 i\u2208I\u2032 Ai).\nA doubt function and a plausibility function are defined from Bel.\nDoubt(A) = Bel(Ac)\nPl(A) = 1\u2212Bel(Ac).\nBut they can be also defined as set functions on \u2126 that satisfy the following properties.\nDoubt(\u2205) = 1, Doubt(\u2126) = 0\nDoubt( n\u22c2\ni=1\nAi)\n\u2265 n\u2211\nr=1\n(\u22121)r+1 \u2211\n{I\u2282{1,2,\u00b7\u00b7\u00b7 ,n}||I|=r} Doubt(\n\u22c3 i\u2208I Ai)\nPlaus(\u2205) = 0, P laus(\u2126) = 1\nPlaus( n\u22c2\ni=1\nAi)\n\u2264 n\u2211\nr=1\n(\u22121)r+1 \u2211\n{I\u2282{1,2,\u00b7\u00b7\u00b7 ,n}||I|=r} Plaus(\n\u22c3 i\u2208I Ai)"}, {"heading": "3 Inclusion-exclusion principle", "text": "The inclusion-exclusion principle about set union and intersection is one of the most important properties in combinatorics, and has applications in diverse areas. Especially belief theoretic functions of DempsterShafer theory are defined by modifying the equality of the inclusion exclusion principle (Shafer 1976). We will propose a theorem on minimum and maximum that is similar to the inclusion-exclusion principle.\nTheorem 1 (Alternating min-max). Let (G,\u2264, \u2217) be a commutative group with a linear order \u2264 and a group operation \u2217. For d1, \u00b7 \u00b7 \u00b7 , dn \u2208 G we have\nmax(d1, \u00b7 \u00b7 \u00b7 , dn) = n\u2211\nr=1\n[ \u2211\n{I\u2282{1,\u00b7\u00b7\u00b7 ,n}||I|=r} min i\u2208I di\n](\u22121)r+1\nmin(d1, \u00b7 \u00b7 \u00b7 , dn) = n\u2211\nr=1\n[ \u2211\n{I\u2282{1,\u00b7\u00b7\u00b7 ,n}||I|=r} max i\u2208I di\n](\u22121)r+1\nwhere \u22121 represents the inverse in the group and the summation is for the group operation.\nThe proofs can be found in (Lee 2006). Let\u2019s consider a special case when G = R and \u2217 = + to get the following theorem.\nTheorem 2. Let (\u2126, d) be a metric space. For a finite class {Ai}ni=1 of subsets of \u2126 we have\nd(x, n\u22c2\ni=1\nAi) \u2265 n\u2211\nr=1\n(\u22121)r+1 \u2211\n{I\u2282{1,\u00b7\u00b7\u00b7 ,n}||I|=r} d(x,\n\u22c3 i\u2208I Ai)\nProof. From the definition of set distance, we have\nd(x, \u22c2\ni\u2208I Ai) \u2265 max i\u2208I d(x,Ai),\nd(x, \u22c3\ni\u2208I Ai) = min i\u2208I d(x, Ai).\nThe theorem follows from theorem 1.\nWe have the following general fact of expectations for any belief theoretic measures since finite sums can be interchangeable with integrals.\nTheorem 3. Let (\u2126, P ), be a space with a belief theoretic measure P . If measurable functions f , fi, i \u2208 I satisfy an (in)equality\nf 4 \u2211\ni\u2208I aifi\nwhere 4 is an (in)equality and ai, i \u2208 I are constants, then\nEP (f) 4 \u2211\ni\u2208I aiEP (fi).\nWe have the following property of the expected distance function as a special case of previous two theorems.\nTheorem 4. Let P be a belief theoretic function. Given Ai \u2282 (\u2126, P, d), 1 \u2264 i \u2264 n, we have\nedP,d( n\u22c2\ni=1\nAi)\n\u2265 n\u2211\nr=1\n(\u22121)r+1 \u2211\n{I\u2282{1,2,\u00b7\u00b7\u00b7 ,n}||I|=r} edP,d(\n\u22c3 i\u2208I Ai)\nThis theorem shows that expected distance functions become doubt functions. An important property of belief theoretic functions is the Mo\u0308bius inversion (Shafer 1976). A mass function m is defined to be a set function on \u2126 that satisfies\n1. m(\u2205) = 0 2. \u2211 U\u2282\u2126 m(U) = 1.\nIt is known that the following inversion theorem holds for any doubt function.\nTheorem 5. If ed is an expected distance function, there is a mass function m such that\ned(A) = \u2211\nU\u2282Ac m(U)\nm(A) = \u2211\nU\u2282A (\u22121)|A|\u2212|U |ed(U)\nAt this point we note that our inequalities are fundamentally different from those in Halpern and Pucella\u2019s paper (2002) that can be stated as\nEBel( n\u2228\ni=1\nXi)\n\u2265 n\u2211\nr=1\n(\u22121)r+1 \u2211\n{I\u2282{1,2,\u00b7\u00b7\u00b7 ,n}||I|=r} EBel(\n\u2227 i\u2208I Xi)\nwhere Xi\u2019s are gambles (random variables). Our inequality is derived from the inequality of distance functions. It does not change the direction regardless of base measures. The above inequality came from the belief theoretic measure upon which the expectation is defined. Therefore, different inequalities hold for different base measures. For example, the above inequality becomes an equality when the base measure is a probability measure."}, {"heading": "4 Dual measures of the expected distance function", "text": "Since expected distance function is a doubt function, we can also consider dual functions of an expected distance function.\nDefinition 2. The expected similarity, expected absoluteness, and expected relativeness are defined as\nes(A) = 1\u2212 ed(A) ea(A) = ed(Ac) er(A) = 1\u2212 ed(AC)\nBecause ed(A) + ed(Ac) \u2264 1 we have ed(A) \u2264 er(A) ea(A) \u2264 es(A)\nThe equality holds when the distance function is crisp (values are 0 or 1) or when the probability measure is crisp. Let\u2019s see ea in its integration form.\nea(A) = \u222b\nA\nd(x,Ac)dP\nThis measures the expected distance between A and Ac. The bigger ea is, the more A becomes an absolute\nchoice. Therefore the bigger er is, the less A becomes an absolute choice. We can compare the meaning of ed and er in the following example.\nExample 1. Let\u2019s assume that a search team for a lost child needs to pick up one of two places A, B as a starting point. If d(A,B) is small, it is not meaningful to spend time to optimize the starting point since the other point will be reached soon. This can be represented with expected distance and expected relativeness. Let d(A,B) = 0.2, and the real probability of the lost child being in A and B be P(A) = 0.1 and P(B) = 0.9. Then, ed({A}) = 0.2 \u00b7 0.9 = 0.18 . Because the cost of wrong prediction is low, even a relatively improbable place \u201cA\u201d has low expected distance. But the expected relativeness is big because the alternative \u201cB\u201d also has low expected distance.\ner({A}) = 1\u2212 ed({A}c) = 1\u2212 0.2 \u00b7 0.1 = 0.98 .\nThis represents that even though predicting \u201cA\u201d has little risk, it need not be the best choice since predicting {A}c = {B} also has little risk.\nAs in belief theory we can consider a tuple\n[ed(A), er(A)]\nto represent the uncertainty of a probabilistic event A in metric space. The interval becomes smaller when the distance function is crisp with respect to A,Ac. It means that the set A becomes a more distinctive category with respect to the distance. If the interval is very small, an expected distance behaves like a probability measure. ed(A) + ed(Ac) \u2248 1 . Since ed(A) \u2264 P (Ac), actual values can be derived from probabilities.\ned(A) \u2248 P (Ac)."}, {"heading": "5 Inference on product of metric spaces", "text": "When we deal with inference problems with a large number of variables, it is almost impossible to maintain the data set without factoring the whole space into product spaces. To reduce the dimension of product spaces, statistical analysis often assumes that variables are independent. Events A1, \u00b7 \u00b7 \u00b7 , An are mutually independent relative to probability if and only if\nPr( \u22c2\ni\u2208I Ai) =\n\u220f i\u2208I Pr(Ai)\nfor any I \u2282 {1, \u00b7 \u00b7 \u00b7 , n}. A set of \u03c3-fields F1, \u00b7 \u00b7 \u00b7 ,Fn are said to be mutually independent when for any given Ai \u2208 Fi, A1, \u00b7 \u00b7 \u00b7 , An are mutually independent. We can extend this condition to expected distance functions.\nDefinition 3. Events A1, \u00b7 \u00b7 \u00b7 , An are mutually independent relative to expected distance if and only if\n1\u2212 ed( \u22c2\ni\u2208I Ai) =\n\u220f i\u2208I (1\u2212 ed(Ai))\nfor any I \u2282 {1, \u00b7 \u00b7 \u00b7 , n}.\nIn probability theory, the product measure space and the product probability are defined as\nn\u220f\ni=1\nFi = \u03c3-field generated by\n{ n\u220f\ni=1\nAi|Ai \u2208 Fi} ,\n(P1 \u00d7 P2)(E) = \u222b\nX\nP2(Ex) dP1(x)\n= \u222b\nY\nP1(Ey) dP2(y) .\nwhere Ex = {y : (x, y) \u2208 E} and Ey = {x : (x, y) \u2208 E}. This product probability makes Ex, Ey independent events. We will construct a product expected distance function so that it also preserves the mutual independence. For that purpose we need a special product metric.\nDefinition 4. If {(\u2126i, di)}1\u2264i\u2264n are 1 bounded metric spaces, a metric \u039bn ~d on \u220fn i=1 \u2126i is defined as\n\u039bn ~d(~x, ~y)\n= n\u2211\nr=1\n(\u22121)r+1 \u220f\n{I\u2282{1,2,\u00b7\u00b7\u00b7 ,n}||I|=r},i\u2208I di(xi, yi)\n= 1\u2212 n\u220f\ni=1\n(1\u2212 di(xi, yi))\nwhere ~xn = (x1, \u00b7 \u00b7 \u00b7 , xn). Theorem 6. If {(\u2126i, di)}1\u2264i\u2264n are 1 bounded metric spaces, then ( \u220fn i=1 \u2126i, \u039b\nn ~d) becomes a 1 bounded metric space.\nBecause of restricted space, refer to (Lee 2006) for omitted proofs. Most of known metrics on product spaces such as Euclidean metric, product metric, supremum metric, etc. are not appropriate for reasoning. The Euclidean metric does not even become a 1- bounded metric on a product space. Other metrics are dependent on the order of spaces or not constructed\nrecursively. \u039bn ~d satisfies all such requirements. Furthermore, it is a unique distance function that satisfies following conditions. Theorem 7. \u039bn ~d is unique under the following conditions.\n1. \u039bn+1(d1, \u00b7 \u00b7 \u00b7 , dn, dn+1) = \u039b2(\u039bn(d1, \u00b7 \u00b7 \u00b7 , dn), dn+1)\n2. \u039bn(d1, \u00b7 \u00b7 \u00b7 , 1, \u00b7 \u00b7 \u00b7 , dn) = 1 3. \u039bn(0, \u00b7 \u00b7 \u00b7 , 0) = 0 4. \u039bn(d1, \u00b7 \u00b7 \u00b7 , di, \u00b7 \u00b7 \u00b7 , dj , \u00b7 \u00b7 \u00b7 dn)\n= \u039bn(d1, \u00b7 \u00b7 \u00b7 , dj , \u00b7 \u00b7 \u00b7 , di, \u00b7 \u00b7 \u00b7 , dn) 5. \u2202\u039b n\n\u2202di (d1, \u00b7 \u00b7 \u00b7 , dn) = f(d1, \u00b7 \u00b7 \u00b7 , d\u0302i \u00b7 \u00b7 \u00b7 , dn)\nThe following theorem shows that if more different aspects are known, the distance between two concepts becomes bigger. Theorem 8. Given (\u2126, di), 1 \u2264 i \u2264 n + 1,\n\u039bn(d1, \u00b7 \u00b7 \u00b7 dn) \u2264 \u039bn+1(d1, \u00b7 \u00b7 \u00b7 , dn, dn+1)\nA partial order < on [0, 1]n can be defined as ~x < ~y if and only if xi < yi for some i, and not xi > yi for any 0 \u2264 i \u2264 n. \u039bn ~d preserves this order in the following sense. Theorem 9. If ~x < ~y < ~z, then \u039bn ~d(~x, ~y) < \u039bn ~d(~x, ~z).\nMetrics such as sup(d1, \u00b7 \u00b7 \u00b7 , dn) does not preserve this order. Now we will prove the mutual independence under \u039bn ~d. Lemma 10. For Ai \u2282 (\u2126i, di) we have\n1\u2212 \u039bn ~d(~x, n\u220f\ni=1\nAi) = n\u220f\ni=1\n(1\u2212 di(xi, Ai)).\nThe following theorem for mutual independence follows from this lemma. Theorem 11. Let Ai \u2282 (\u2126i,Fi, Pi, di) for each i. If F1, \u00b7 \u00b7 \u00b7 ,Fn are mutually independent, then on ( \u220fn i=1 \u2126i, \u220fn\ni=1 Fi, P, \u039bn ~d) 1\u2212 edP,\u039bn ~d( \u220f\ni\u2208I Ai) =\n\u220f i\u2208I (1\u2212 edPi,di(Ai))\nGiven Ai \u2282 \u2126i, it is a common convention to denote Ai \u2282 \u220fn i=1 \u2126i for \u21261\u00d7\u00b7 \u00b7 \u00b7\u00d7\u2126i\u22121\u00d7Ai\u00d7\u2126i+1\u00d7\u00b7 \u00b7 \u00b7\u00d7\u2126n. Lemma 12. If Ai, Bi \u2282 (\u2126i, di) for i = 1, \u00b7 \u00b7 \u00b7 , n then \u039bn ~d(A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2126i \u00d7 \u00b7 \u00b7 \u00b7 \u00d7An,\nB1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Bi \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Bn) = \u039bn\u22121~d(A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 A\u0302i \u00d7 \u00b7 \u00b7 \u00b7 \u00d7An,\nB1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 B\u0302i \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Bn).\nBecause of this lemma, for Ai, Bi \u2282 (\u2126i, di) we have\ndi(Ai, Bi) = \u039bn ~d(Ai, Bi).\nTheorem 13. Let (\u2126i,Fi, Pi, di) be the probability space with metric. If Ai \u2282 \u2126i then\nedP,\u039bn ~d(\u21261 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Ai \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2126n) = edPi,di(Ai) ."}, {"heading": "6 Axiomatizing expected distance", "text": ""}, {"heading": "6.1 Syntax and semantics", "text": "The syntax of a logic of expected distance LED is defined as follows.\nDefinition 5. Let \u03a0 = {P1, P2, \u00b7 \u00b7 \u00b7 } be a set of primitive propositions. The set of propositional formulas is the closure of \u03a0 under the Boolean operations \u2227,\u00ac as in the propositional logic. An expected distance term is an expression of the form a1ED(\u03d51) + \u00b7 \u00b7 \u00b7 + anED(\u03d5n), where \u03d5i is a propositional formula and ai\u2019s are real numbers. A basic expected distance formula is a statement of the form t \u2265 \u03b1 where t is a expected distance term and \u03b1 is a real number. An expected distance formula or just a formula of the language LE is a Boolean combination of basic expected distance formulas.\nAs an example, (2ED(P \u2227Q) + 0.23ED(\u00acQ) \u2265 0.2) \u2228 (ED(\u00acP ) < 0.1) is a formula. We will use abbreviations such as t < \u03b1 for \u00ac(t \u2265 \u03b1), t \u2264 \u03b1 for (\u22121)t \u2265 \u2212\u03b1, t = \u03b1 for (t \u2264 \u03b1) \u2227 (t \u2265 \u03b1). The semantics of LED is defined on a probability structure with a metric, which is a tuple M = (\u2126,F , P, d, \u03c0). (\u2126,F , P ) is a probability space where all subsets of \u2126 are measurable. \u03c0 : \u03a0 \u2192 {0, 1} is a truth assignment function of atomic propositions. d is a metric on \u2126. We first define the interpretation of a propositional formula \u03c6.\n\u03d5M = {\u03c9 \u2208 \u2126|\u03c0(\u03c9)(\u03d5) = 1}. For a given model M ,\nM |= a1ED(\u03d51) + \u00b7 \u00b7 \u00b7+ anED(\u03d5n) \u2265 \u03b1 iff a1ed(\u03d5M1 ) + \u00b7 \u00b7 \u00b7+ aned(\u03d5Mn ) \u2265 \u03b1\nWe extend |= to arbitrary formulas as"}, {"heading": "M |= f1 \u2227 f2 iff M |= f1 and M |= f2", "text": "M |= \u00acf iff M 6|= f\nNote that even though we defined a semantics for probability structures, since an expected distance can be taken with respect to any other belief theoretic measures, we can make logics for those structures just by changing the probability structure into those.\n6.2 Axioms of LED\nNow we will construct a sound and complete axioms for our logic. First, we use the axioms AxioL that is defined in (Fagin, Halpern, & Megiddo 1990), (Fagin & Halpern 1994), and shown to be sound and complete.\nI. AxiomP : Axioms for propositional logic. II. AxiomL : Axioms for linear inequalities including AxiomP .\nThe following axioms characterize the logic of expected distances.\nIII. AxiomED: Axioms for expected distance including AxiomL.\n1. (true) ED(true) = 0.\n2. (false) ED(false) = 1.\n3. (nonnegative) ED(\u03d5) \u2265 0.\n4. (inclusion-exclusion) ED( \u2227n\ni=1 \u03d5i) \u2265 \u2211nr=1(\u22121)r+1 \u2211 {I\u2282{1,2,\u00b7\u00b7\u00b7 ,n}||I|=r}ED( \u2228 i\u2208I \u03d5i).\n5. (substitution) \u03d5 \u21d4 \u03c8 in propositional logic, then ED(\u03d5) = ED(\u03c8).\nThe inclusion exclusion axiom is the generalization of the simple case\nED(\u03d5 \u2227 \u03c8) \u2265 ED(\u03d5) + ED(\u03c8)\u2212 ED(\u03d5 \u2228 \u03c8) .\nEven though these axioms are for probability measures, the axioms for other belief theoretic measures would be the same. We can prove the following properties of expected distance from AxiomED.\nTheorem 14. 1. ED(\u00ac\u03d5) \u2264 1\u2212 ED(\u03d5).\n2. ED(\u03d5) \u2265 ED(\u03d5 \u2227 \u03c8) + ED(\u03d5 \u2227 \u00ac\u03c8)\u2212 1.\n3. If \u03d5 \u21d2 \u03c8 then ED(\u03d5) \u2264 ED(\u03c8)."}, {"heading": "6.3 Soundness and Completeness", "text": "Now we will prove the soundness and completeness theorems for the logic of expected distance. Even though an expected distance function is a doubt function, the axioms for doubt function need not be complete for the system of expected distance. For example, a probability measure is a plausibility measure, but the axiom set of plausibility measure is not complete for probabilistic systems.\nTheorem 15. AxiomED is sound and weakly complete.\nProof. As for soundness, all axioms except for inclusion-exclusion axiom can be easily proved to be sound. The inclusion-exclusion axiom comes from theorem 4.\nTo prove the weak completeness we will build a model for any given consistent formula f \u2208 LED. Once we have the model existence, the completeness is proved as following. Let\u2019s assume that \u0393 |= \u03d5 for a finite \u0393. If \u0393 6` \u03c6 then \u2227 \u0393 \u2228 \u00ac\u03c6 is consistent. By model existence we have a model M such that M |= \u2227 \u0393 \u2228 \u00ac\u03d5. Therefore \u0393 6|= \u03d5. This contradiction solves the completeness.\nLet\u2019s represent f in a disjunctive normal form g1\u2228\u00b7 \u00b7 \u00b7\u2228 gn where each gi is a conjunction of basic expected distance formulas and their negations. If f is consistent, then some gi is consistent. Moreover, any model that satisfies gi also satisfies f . Therefore we only need to make a model for the formula gi.\nLet {P1, \u00b7 \u00b7 \u00b7 , Pk} be the set of all primitive propositions that appears in gi. Let {\u03d51, \u00b7 \u00b7 \u00b7 , \u03d52k} be the set of all possible formulas where \u03d5i = \u00b1P1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 \u00b1Pk. (\u00b1Pj represents Pj or \u00acPj). Let n = 2k. For any propositional formula \u03c8 that appear in gi, we have a set I \u2282 {1, \u00b7 \u00b7 \u00b7 , n} such that\n|= \u03c8 \u21d4 \u2228\ni\u2208I \u03d5i .\nLet g\u2032i be the formula made from gi by substituting all occurrences of propositional formula \u03c8 with equivalent\u2228\ni\u2208I \u03d5i\u2019s. Because of the soundness of the axioms, it is enough to construct a model for the formula g\u2032i.\nNow consider the formula h that is the conjunction of g\u2032i with the following formulas that represents the axioms. The idea is to add enough restrictions so that any solution of the linear equation satisfy conditions of expected distance.\nWe first conjunct a formula for true. Since true \u2261\u2228n i=1 \u03d5i,\nED( n\u2228\ni=1\n\u03d5i) = 0 .\nSecond, we conjunct a formula for the empty set. Since false \u2261 \u2228\u2205 \u03d5i,\nED( \u2228\n\u2205 \u03d5i) = 1 .\nThird, we conjunct 2n formulas that represent the nonnegative conditions for all I \u2282 {1, \u00b7 \u00b7 \u00b7 , n}.\nED( \u2228\ni\u2208I \u03d5i) \u2265 0 .\nFinally, we conjunct formulas that represent the inclusion-exclusion conditions. For all K \u2282 P{0, \u00b7 \u00b7 \u00b7 , n},\nED([ \u2228\ni\u2208\u22c2 K \u03d5i])\n\u2265 |K|\u2211 r=1 (\u22121)r+1 \u2211 {K\u2032\u2282K||K\u2032|=r} ED( \u2228 i\u2208\u22c3 K\u2032 \u03d5i) .\nSince g\u2032i is consistent in AxiomED, h is also consistent in AxiomED. Therefore it should be consistent in AxiomL. In AxiomL, the expected distance terms ED( \u2228 i\u2208I \u03d5i)\u2019s are considered as variables. Since we know the completeness of the AxiomL, h should have a model. A formula in the logic of linear inequalities is a system of linear inequalities. A model of a formula has an interpretation map that assigns a solution for the linear inequality system of the given formula. Therefore, the model of h assigns a number eI for each variable ED( \u2228 i\u2208I \u03d5i)}. So that if the eI \u2019s are substituted into the variables, the linear inequality system is satisfied. So we have\ne\u2205 = 1, e{1,\u00b7\u00b7\u00b7 ,n} = 0,\neI \u2265 0,\ne\u22c2 K \u2265 |K|\u2211 r=1 (\u22121)r+1 \u2211 {K\u2032\u2282K||K\u2032|=r} e\u22c3 K\u2032 .\nFrom Lemma 16 there is a model M = (\u2126,F , P, d, \u03c0) such that\nedP,d([ \u2228\ni\u2208I \u03d5i]M ) = edP,d(\n\u22c3 i\u2208I \u03d5Mi ) = eI .\nTherefore, M |= h and the theorem is proved. Lemma 16. For {eI \u2208 R|I \u2282 {1, \u00b7 \u00b7 \u00b7 , n}} such that\ne\u2205 = 1, e{1,\u00b7\u00b7\u00b7 ,n} = 0,\neI \u2265 0,\ne\u22c2 K \u2265 |K|\u2211 r=1 (\u22121)r+1 \u2211 {K\u2032\u2282K||K\u2032|=r} e\u22c3 K\u2032 .\nwhere K \u2282 P{1, \u00b7 \u00b7 \u00b7 , n}, we have a probability metric space M = (\u2126,F , P, d, \u03c0) such that\nedP,d([ \u2228\ni\u2208I \u03d5i]M ) = edP,d(\n\u22c3 i\u2208I \u03d5Mi ) = eI .\nProof. An underlying space \u2126 and a propositional interpretation \u03c0 is constructed as follows.\n\u2126 = \u22c3\ni\n\u03d5Mi\n\u03d5Mi = \u03c0(\u03d5i) = Xi \u222a Yi Xi = {xi,J}J\u2282{1,\u00b7\u00b7\u00b7\u0302i\u00b7\u00b7\u00b7 ,n} Yi = \u22c3\nj 6=i {yi,j,K}K\u2282{1,\u00b7\u00b7\u00b7 ,j\u0302,\u00b7\u00b7\u00b7 ,n}.\nBefore constructing a probability measure P , let\u2019s consider a discrete space of n-points, \u2126\u2032 = {a\u03d5M1 , \u00b7 \u00b7 \u00b7 , a\u03d5Mn }. A set function ed\u2032 : P\u2126\u2032 \u2192 R is defined as ed\u2032(\n\u22c3 i\u2208I{a\u03d5Mi }) = eI . Because of the prop-\nerties of eI , ee\u2032 satisfies all properties of doubt function. Therefore we can apply Mo\u0308bius transformation to get the mass function m : P\u2126\u2032 \u2192 R. If we represent m( \u22c3 i\u2208I \u03d5 M i ) = mI , we have the following equality from the definition of Mo\u0308bius transformation.\nmI = \u2211\nJ\u2282I (\u22121)|I|\u2212|J|eJ .\neI = \u2211\nJ\u2282Ic mJ .\nConsider the measurable space F = P\u2126. Since \u2126 is finite, a probability distribution on F is determined by the probability at each point of \u2126. So, we can construct the probability distribution P on F as\nP (xi,J ) = mJc\n|Jc| , P (yi,J,K) = 0 .\nNow we will define a pseudo distance function d on \u2126 as\nd(xj\u2032,K\u2032 , yi,j,K)\n= { 0 if j = j\u2032 and K = K \u2032 and i 6\u2208 K 1 else\nd(yi,j,K , yi\u2032,j\u2032,K\u2032) = max(d(xj,K , yi,j,K), d(xj\u2032,K\u2032 , yi\u2032,j\u2032,K\u2032))\nd(xi,J , xi\u2032,J \u2032) = 1\nLet\u2019s show the triangular inequality. Since all values are 0 or 1, the only cases that we are concerned is the case of 0 + 0 \u2265 1. Since 0 distance can not happen between xi,J \u2019s, the remaining cases are as follows. First,\nd(xj,K , yi,j,K) + d(xj,K , yi\u2032,j,K) \u2265 d(yi,j,K , yi\u2032,j,K) ,\nd(xj,K , yi,j,K) + d(yi,j,K , yi\u2032,j,K) \u2265 d(xj,K , yi\u2032,j,K) follows from the definition of d. Finally,\nd(yi,j,K , yi\u2032,j\u2032,K\u2032) + d(yi\u2032,j\u2032,K\u2032 , yi\u2032\u2032,j\u2032\u2032,K\u2032\u2032) \u2265 d(yi,j,K , yi\u2032\u2032,j\u2032\u2032,K\u2032\u2032)\nholds since\nmax(d(xj,K , yi,j,K), d(xj\u2032,K\u2032 , yi\u2032,j\u2032,K\u2032)) +max(d(xj\u2032,K\u2032 , yi\u2032,j\u2032,K\u2032), d(xj\u2032\u2032,K\u2032\u2032 , yi\u2032\u2032,j\u2032\u2032,K\u2032\u2032))\n\u2265 max(d(xj,K , yi,j,K), d(xj\u2032\u2032,K\u2032\u2032 , yi\u2032\u2032,j\u2032\u2032,K\u2032\u2032)) .\nThis proves that d is a pseudo distance function.\nSince we constructed M , we will check that the expected distance edP,d satisfies edP,d( \u22c3 i\u2208I \u03d5 M i ) = eI .\nThe distance between xj,K and \u22c3 i\u2208I \u03d5 M i is as follows.\nd(xj,K , \u22c3\ni\u2208I \u03d5Mi ) = min i\u2208I min j\u2032,K\u2032 d(xj,K , yi,j\u2032,K\u2032)\n= { 1 if I \u2282 K 0 else .\nwhere j 6\u2208 K by definition. Therefore,\ned( \u22c3\ni\u2208I \u03d5Mi )\n= \u2211\n\u03c9\u2208\u2126 d(\u03c9,\n\u22c3 i\u2208I \u03d5Mi ) \u00b7 P (\u03c9)\n= \u2211\nj\u2208Kc\n[ \u2211\nK\u2282{1,\u00b7\u00b7\u00b7 ,n} d(xj,K ,\n\u22c3 i\u2208I \u03d5Mi ) \u00b7 P (xj,K) ]\n= \u2211\nj\u2208Kc\n[ \u2211\nK\u2282{1,\u00b7\u00b7\u00b7 ,n},I\u2282K P (xj,K)\n]\n= \u2211\nK\u2282{1,\u00b7\u00b7\u00b7 ,n},I\u2282K\n[ \u2211\nj\u2208Kc\nmKc |Kc| ]\n= \u2211\nK\u2282{1,\u00b7\u00b7\u00b7 ,n},I\u2282K mKc\n= \u2211\nKc\u2282Ic mKc =\n\u2211\nJ\u2282Ic mJ = eI ."}, {"heading": "7 Discussion", "text": "We constructed reasoning systems on metric spaces using expected distance functions. These systems could be built on a metric space equipped with any kind of belief theoretic measures. The axioms we adopted for these systems were sound and complete. But symbols like \u201cPr\u201d or \u201cD\u201d that represent probability or distance are not included in LED. We showed that probability logic and the logic of expected distance are not more expressive than each other (Lee 2006). Furthermore, extended languages including some of those symbols\nare strictly more expressive than LED. It is an interesting question whether there exist sets of complete axioms for those extended systems.\nReasoning with expected distance has many potential applications. One interesting application is a reasoning system for second order uncertainties (Gaifman 1986). Since uncertainty degrees are usually represented in metric spaces, we can adopt expected distance functions to represent second order uncertainties. As an example, an expression such as ED(\u201cProb(\u03d5) = 0.5\u201d) = 0.1 is an efficient representations when we represent a probability distribution over probabilities of \u03d5. Since Prob(\u201cProb(\u03d5) = 0.5\u201d) would always become zero, a purely probabilistic second order reasoning system should chose an interval such as Prob(Prob(\u03d5) \u2208 [0.4, 0.6]) = 0.8. The reasoning for this choice is not clear, and tends to lose more information in the process. There is no explicit reason why we should choose the previous expression instead of Prob(Prob(\u03d5) \u2208 [0.2, 0.9]) = 0.9. The representation with expected distance is more natural and intuitive.\nAcknowledgement I thank Lawrence Moss for his support and guidance for this paper."}], "references": [{"title": "Toward a geometry of common sense: A semantics and a complete axiomatization of mereotopology", "author": ["N. Asher", "L. Vieu"], "venue": "IJCAI-95, 846\u2013852.", "citeRegEx": "Asher and Vieu,? 1995", "shortCiteRegEx": "Asher and Vieu", "year": 1995}, {"title": "Representing and reasoning with probabilistic knowledge: a logical approach to probabilities", "author": ["F. Bacchus"], "venue": "Cambridge, Mass.: MIT Press.", "citeRegEx": "Bacchus,? 1990", "shortCiteRegEx": "Bacchus", "year": 1990}, {"title": "Statistical decision theory and Bayesian analysis", "author": ["J.O. Berger"], "venue": "Springer series in statistics. New York: Springer-Verlag, 2nd edition.", "citeRegEx": "Berger,? 1985", "shortCiteRegEx": "Berger", "year": 1985}, {"title": "Great expectations", "author": ["F.C. Chu", "J.Y. Halpern"], "venue": "part i: On the customizability of generalized expected utility. In Eighteenth International Joint Conference ofn Aritificial Intelligence (IJCAI03), 279\u2013302.", "citeRegEx": "Chu and Halpern,? 2003", "shortCiteRegEx": "Chu and Halpern", "year": 2003}, {"title": "Great expectations", "author": ["F.C. Chu", "J.Y. Halpern"], "venue": "part ii: generalized expected utility as a universal decision rule. Artificial Intelligence 159(12):207\u2013229.", "citeRegEx": "Chu and Halpern,? 2004", "shortCiteRegEx": "Chu and Halpern", "year": 2004}, {"title": "Reasoning about knowledge and probability", "author": ["R. Fagin", "J.Y. Halpern"], "venue": "J. ACM 41(2):340\u2013367.", "citeRegEx": "Fagin and Halpern,? 1994", "shortCiteRegEx": "Fagin and Halpern", "year": 1994}, {"title": "A logic for reasoning about probabilities", "author": ["R. Fagin", "J.Y. Halpern", "N. Megiddo"], "venue": "Information and Computation 87(1-2):78\u2013128.", "citeRegEx": "Fagin et al\\.,? 1990", "shortCiteRegEx": "Fagin et al\\.", "year": 1990}, {"title": "Combining spatial and temporal logics: Expressiveness vs", "author": ["D. Gabelaia", "R. Kontchakov", "A. Kurucz", "F. Wolter", "M. Zakharyaschev"], "venue": "complexity. Journal of Artificial Intelligence Research 23:167\u2013 243.", "citeRegEx": "Gabelaia et al\\.,? 2005", "shortCiteRegEx": "Gabelaia et al\\.", "year": 2005}, {"title": "A theory of higher order probabilities", "author": ["H. Gaifman"], "venue": "1986 Conference on Theoretical aspects of reasoning about knowledge. Monterey, California, United States: Morgan Kaufmann Publishers Inc.", "citeRegEx": "Gaifman,? 1986", "shortCiteRegEx": "Gaifman", "year": 1986}, {"title": "Inferences in probability logic", "author": ["G. Gerla"], "venue": "Artificial Intelligence 70(1-2):33\u201352.", "citeRegEx": "Gerla,? 1994", "shortCiteRegEx": "Gerla", "year": 1994}, {"title": "Reasoning about expectation", "author": ["J.Y. Halpern", "R. Pucella"], "venue": "UAI\u201902, 18th Conference in Uncertainty in Artificial Intelligence, 217\u2013215.", "citeRegEx": "Halpern and Pucella,? 2002", "shortCiteRegEx": "Halpern and Pucella", "year": 2002}, {"title": "Reasoning about uncertainty", "author": ["J.Y. Halpern"], "venue": "Cambridge, Mass.: MIT Press.", "citeRegEx": "Halpern,? 2003", "shortCiteRegEx": "Halpern", "year": 2003}, {"title": "Logics of metric spaces", "author": ["O. Kutz", "F. Wolter", "H. Sturm", "N.-Y. Suzuki", "M. Zakharyaschev"], "venue": "ACM Trans. Comput. Logic 4(2):260\u2013294.", "citeRegEx": "Kutz et al\\.,? 2003", "shortCiteRegEx": "Kutz et al\\.", "year": 2003}, {"title": "Probabilistic reasoning on metric spaces", "author": ["S.H. Lee"], "venue": "Ph.D. Dissertation, Indiana University Bloomington.", "citeRegEx": "Lee,? 2006", "shortCiteRegEx": "Lee", "year": 2006}, {"title": "On the incompleteness of modal logics of space: advancing complete modal logics of place", "author": ["O. Lemon", "I. Pratt"], "venue": "Advances in Modal Logic, volume 1. CSLI Publications, Stanford. 115\u2013132.", "citeRegEx": "Lemon and Pratt,? 1998", "shortCiteRegEx": "Lemon and Pratt", "year": 1998}, {"title": "Probabilistic logic", "author": ["N.J. Nilsson"], "venue": "Artificial Intelligence 28(1):71\u201387.", "citeRegEx": "Nilsson,? 1986", "shortCiteRegEx": "Nilsson", "year": 1986}, {"title": "The foundations of statistics", "author": ["L.J. Savage"], "venue": "New York,: Wiley.", "citeRegEx": "Savage,? 1954", "shortCiteRegEx": "Savage", "year": 1954}, {"title": "Subjective-probability and expected utility without additivity", "author": ["D. Schmeidler"], "venue": "Econometrica 57(3):571\u2013587.", "citeRegEx": "Schmeidler,? 1989", "shortCiteRegEx": "Schmeidler", "year": 1989}, {"title": "A mathematical theory of evidence", "author": ["G. Shafer"], "venue": "Princeton, N.J.: Princeton University Press.", "citeRegEx": "Shafer,? 1976", "shortCiteRegEx": "Shafer", "year": 1976}, {"title": "Theory of games and economic behavior", "author": ["J. Von Neumann", "O. Morgenstern"], "venue": "Princeton,: Princeton university press.", "citeRegEx": "Neumann and Morgenstern,? 1944", "shortCiteRegEx": "Neumann and Morgenstern", "year": 1944}, {"title": "A logic for metric and topology", "author": ["F. Wolter", "M. Zakharyaschev"], "venue": "Journal of Symbolic Logic 70(3):795\u2013828.", "citeRegEx": "Wolter and Zakharyaschev,? 2005", "shortCiteRegEx": "Wolter and Zakharyaschev", "year": 2005}, {"title": "Entropy measures under similarity relations", "author": ["R.R. Yager"], "venue": "International Journal of General Systems 20(4):341\u2013358.", "citeRegEx": "Yager,? 1992", "shortCiteRegEx": "Yager", "year": 1992}, {"title": "Probability measures of fuzzy events", "author": ["L.A. Zadeh"], "venue": "Jour. of Math. anal. and appl. 23:412\u2013427.", "citeRegEx": "Zadeh,? 1968", "shortCiteRegEx": "Zadeh", "year": 1968}], "referenceMentions": [{"referenceID": 2, "context": "The expected distance is often interpreted as a loss function in statistical decision theory (Berger 1985).", "startOffset": 93, "endOffset": 106}, {"referenceID": 16, "context": "The expected utility function has been an important topic in economics for a long time (Von Neumann & Morgenstern 1944; Savage 1954; Schmeidler 1989).", "startOffset": 87, "endOffset": 149}, {"referenceID": 17, "context": "The expected utility function has been an important topic in economics for a long time (Von Neumann & Morgenstern 1944; Savage 1954; Schmeidler 1989).", "startOffset": 87, "endOffset": 149}, {"referenceID": 1, "context": "see Halpern 2003, Nilsson 1986, Bacchus 1990, Gerla 1994). There also exists a wide variety of formal systems on spatial reasoning (eg. see Gabelaia et al. 2005, Asher & Vieu 1995, Lemon & Pratt 1998) and especially on reasoning about metric spaces (eg. see Kutz et al. 2003, Wolter & Zakharyaschev 2005). But it is hard to find a formal system when probabilistic uncertainty is present on metric spaces. In probabilistic or statistical analysis, it is usually assumed that values of random variables are real numbers. Since most of statistical inference is related to reasoning about expectation and variance, restricting the range of random variables to the spaces where those values are well defined is considered as an acceptable sacrifice. A formal reasoning system on expectations with respect to probability measures and other belief theoretic measures can be found in (Halpern & Pucella 2002). Even though expectations of an event cannot be defined on arbitrary metric spaces, the expectation of distances with respect to a fixed set is well defined in any metric space. For example, an expectation of a random location on a sphere cannot be defined, but an expected distance of a random location from the south hemisphere is a well defined notion. The expected distance is often interpreted as a loss function in statistical decision theory (Berger 1985). Loss(a) = EP (d(X, a)). In this case, the distance function d(x, a) with respected to a fixed point a is used to calculate the cost for predicting x when a is a true state. A complimentary concept of a cost function in decision theory is a utility function. The expected utility function has been an important topic in economics for a long time (Von Neumann & Morgenstern 1944; Savage 1954; Schmeidler 1989). We can also find more generalized expected utility functions defined for plausibility measures and generalized utility functions. (Chu & Halpern 2003; 2004). The objective of a decision problem is to find an action that minimizes a loss function. For example, if we have a uniform prior probability distribution on [0, 1] where d(x, y) = |x\u2212 y|, the loss minimizing prediction would be X = 0.5. But in inference problems, the objective is to derive more information from given information. As an example, for the previous distance function, if we know that EP (d(X, 0)) = 1, then we can deduce that EP (d(X, 1)) = 0. For inference problems, a viewpoint from fuzzy logic is also meaningful. Let\u2019s assume that a fuzzy set Da represents a fuzzy concept \u201cdissimilar from a\u201d for a point a in a 1-bounded metric space \u03a9. A natural candidate for a membership function of Da would be d(x, a). Given a probability distribution P , Zadeh (1968) introduces the probability of a fuzzy concept.", "startOffset": 32, "endOffset": 2709}, {"referenceID": 21, "context": "There has been various attempts to generalize Shannon\u2019s entropy to the metric spaces using expected similarity and expected distance (Yager 1992; Lee 2006).", "startOffset": 133, "endOffset": 155}, {"referenceID": 13, "context": "There has been various attempts to generalize Shannon\u2019s entropy to the metric spaces using expected similarity and expected distance (Yager 1992; Lee 2006).", "startOffset": 133, "endOffset": 155}, {"referenceID": 18, "context": "We can also define expected distance functions with respect to any kind of belief theoretic measures (Shafer 1976).", "startOffset": 101, "endOffset": 114}, {"referenceID": 18, "context": "Especially belief theoretic functions of DempsterShafer theory are defined by modifying the equality of the inclusion exclusion principle (Shafer 1976).", "startOffset": 138, "endOffset": 151}, {"referenceID": 13, "context": "The proofs can be found in (Lee 2006).", "startOffset": 27, "endOffset": 37}, {"referenceID": 18, "context": "An important property of belief theoretic functions is the M\u00f6bius inversion (Shafer 1976).", "startOffset": 76, "endOffset": 89}, {"referenceID": 10, "context": "At this point we note that our inequalities are fundamentally different from those in Halpern and Pucella\u2019s paper (2002) that can be stated as", "startOffset": 86, "endOffset": 121}, {"referenceID": 13, "context": "Because of restricted space, refer to (Lee 2006) for omitted proofs.", "startOffset": 38, "endOffset": 48}, {"referenceID": 13, "context": "We showed that probability logic and the logic of expected distance are not more expressive than each other (Lee 2006).", "startOffset": 108, "endOffset": 118}, {"referenceID": 8, "context": "One interesting application is a reasoning system for second order uncertainties (Gaifman 1986).", "startOffset": 81, "endOffset": 95}], "year": 2006, "abstractText": "We set up a model for reasoning about metric spaces with belief theoretic measures. The uncertainty in these spaces stems from both probability and metric structures. To represent both aspect of uncertainty, we choose an expected distance function as a measure of uncertainty. A formal logical system is constructed for the reasoning about expected distance. Soundness and completeness are shown for this logic. For reasoning on product metric spaces with uncertainty, a new metric is defined and shown to have good properties.", "creator": " TeX output 2006.05.21:2316"}}}