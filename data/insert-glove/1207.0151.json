{"id": "1207.0151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2012", "title": "Differentiable Pooling for Hierarchical Feature Learning", "abstract": "netlibrary We arbuscula introduce a ms2 parametric conch form khok of jintropin pooling, donaghue based playwriting on a staf Gaussian, 13.5-meter which can six-legged be filariasis optimized jarosz alongside nooke the 39.71 features cerv in a alirio single global rehe objective airstaff function. By hopa contrast, existing pooling schemes are inquirer based on heuristics (multifactor e. g. sigmodon local raghunathpur maximum) and have vrbica no clear link sicartsa to ravishing the deplane cost function ashtar of the model. wenceslaus Furthermore, the h5ni variables of the ecusa Gaussian explicitly wbai store \u017ealgiris location information, distinct from bagong the dominions appearance captured by the features, ankang thus providing seimei a nisei what / 30-32 where komisja decomposition of the input d'urf\u00e9 signal. qualifier Although coruna the differentiable bentivolio pooling scheme can visting be incorporated in odio a 10 wide profesores range 103,000 of pentecostalists hierarchical models, liselotte we liebeler demonstrate 1814 it relishes in the slifer context anarcho-syndicalists of a iwan Deconvolutional Network model (Zeiler 29.37 et day-night al. ICCV 1992-2004 2011 ). We also yiddish explore a rsis number 1640.4 of secondary pricing issues nakhlestan within 3,893 this confided model glenbervie and present 57.87 detailed dehydrating experiments sulu on MNIST fued digits.", "histories": [["v1", "Sat, 30 Jun 2012 21:04:13 GMT  (1997kb,D)", "http://arxiv.org/abs/1207.0151v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["matthew d zeiler", "rob fergus"], "accepted": false, "id": "1207.0151"}, "pdf": {"name": "1207.0151.pdf", "metadata": {"source": "CRF", "title": "Differentiable Pooling for Hierarchical Feature Learning", "authors": ["Matthew D. Zeiler"], "emails": ["zeiler@cs.nyu.edu", "fergus@cs.nyu.edu"], "sections": [{"heading": "1. Introduction", "text": "A number of recent approaches in vision and machine learning have explored hierarchical representations for images and video, with the goal of learning features for object recognition. One class of methods, for example Convolutional Neural Networks [13] or the recent RICA model of Le et al. [12], use a purely feed-forward hierarchy that maps the input image to a set of features which are presented to a simple classifier. Another class of models attempts to build hierarchical generative models of the data. These include Deep Belief Networks [9], Deep Boltzmann Machines [19] and the Compositional Models of Zhu et al. [23, 4].\nSpatial pooling is a key mechanism in all these hierarchical image representations, giving invariance to local perturbations of the input and allowing higher-level features to model large portions of the image. Sum and max pooling are the most common forms, with max being typically preferred (see Boureau et al. [3] for an analysis).\nIn this paper we introduce a parametric form of pooling that can be directly integrated into the overall objective function of many hierarchical models. Using a Gaussian parametric model, we can directly optimize the mean and variance of each Gaussian pooling region during inference to minimize a global objective function. This contrasts with\nexisting pooling methods that just optimize a local criterion (e.g. max over a region). Adjusting the variance of each Gaussian allows a smooth transition between selecting a single element (akin to max pooling) over the pooling region, or averaging over it (like a sum operation).\nIntegrating pooling into the objective facilitates joint training and inference across all layers of the hierarchy, something that is often a major issue in many deep models. During training, most approaches build up layer-by-layer, holding the output of the layer beneath fixed. However, this is sub-optimal, since the features in the low-layers cannot use top-down information from a higher layer to improve them. A few approaches do perform full joint training of the layers, notably the Deep Boltzmann Machine [19], and Eslami et al. [5], as applied to images, and the Deep Energy Models of Ngiam et al. [15]. We demonstrate our differentiable pooling in a third model with this capability, the Deconvolutional Networks of Zeiler et al. [22]. This is a simple sparse-coding model that can be easily stacked and we show how joint inference and training of all layers is possible, using the differentiable pooling. However, differentiable pooling is not confined to the Deconvolutional Network model \u2013 it is capable of being incorporated into many existing hierarchical models.\nThe latent variables that control the Gaussians in our pooling scheme store location information (\u201cwhere\u201d), distinct from the features that capture appearance (\u201cwhat\u201d). This separation of what/where is also present in Ranzato et al. [17], the transforming auto-encoders of Hinton et al. [7], and Zeiler et al. [22].\nIn this paper, we also explore a number of secondary issues that help with training deep models: non-negativity constraints; different forms of sparsity; overcoming local minima during inference and different sparsity levels during training and testing."}, {"heading": "2. Model Overview", "text": "We explain our contributions in the context of a Deconvolutional Network, introduced by Zeiler et al. [22]. This model is a hierarchical form of convolutional sparse coding that can learn invariant image features in an unsupervised\nar X\niv :1\n20 7.\n01 51\nv1 [\ncs .C\nV ]\n3 0\nJu n\n20 12\n* \u00a0\n\u03a3 \u00a0 Input image y\nConv Layer 1 Conv Layer 2 U npool Layer 1 Unpooled Map z1,1 Feature Map p1,1\n1st Layer Filters\n2nd Layer Filters\n|.| 0.5 \u00a0\nz1,2 \u00a0\np1,2\nU npool Layer 2 \u03b81,2\n\u03b81,1\nzB,2 \u00a0\nf 1 1,1 \u00a0 f C 1,1 \u00a0 f 1 B,1 \u00a0 f C B,1 \u00a0\nLayer 1 features p1\nReconstructed input y\nLayer 2 unpooled features z2\nL0.5 Sparsity\nLayer 1 unpooled features z1\nUnpool Vars \u03b81 U\u03b81 \u00a0 P\u03b81 \u00a0\nF1 \u00a0 F T 1 \u00a0\nF2 \u00a0 F T 2 \u00a0\n^ \u00a0\nR2 \u00a0 R2 \u00a0 T \u00a0\nUnpooling Vars \u00a0\n* \u00a0\nUnpooled Map zB,1\nFeature Map pB,1\n|.| 0.5 \u00a0 \u03b8B,1\nUnpooling Vars \u00a0\n* \u00a0* \u00a0\n\u03a3 \u00a0 \u03a3 \u00a0 \u03a3 \u00a0 \u03a3 \u00a0\n* \u00a0* \u00a0 * \u00a0 * \u00a0* \u00a0 * \u00a0\npB,2\n\u03b8B,2\nf 1 1,2 \u00a0 f 1 B,2 \u00a0\nLayer 2 features p2\nUnpool Vars \u03b82 U\u03b82 \u00a0 P\u03b82 \u00a0\n \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0  \u00a0 Feature Map p\nNeighborhood N1\nN2 N4\nUnpooled feature map z\nx\ny\n\u03b8(1): \u00b5x, \u00b5y, \u03b3x , \u03b3y \u03b8(2): \u00b5x, \u00b5y, \u03b3x , \u03b3y \u03b8(3): \u00b5x, \u00b5y, \u03b3x , \u03b3y \u03b8(4): \u00b5x, \u00b5y, \u03b3x , \u03b3y (Un)pooling Variables \u03b8\n(a) (b) (c)\nFigure 1. (a): A 2-layer model architecture. (b): Schematic of inference in a two layer model. (c): Illustration of the Gaussian parameterization used in our differentiable pooling.\nmanner. Its simplicity allows the easy integration of differentiable pooling and is amenable to joint inference over all layers.\nLet us start by reviewing a single Deconvolutional Network layer, presented with an input image vc (having c color channels). The goal is to produce a reconstruction v\u0302 from sparse features p, that is close to v. We achieve this by minimizing:\n\u03bb\n2 \u2211 c \u2016v\u0302c \u2212 vc\u201622 + |p|\u03b1 (1)\nwhere \u03bb is a hyper-parameter that controls the influence of the reconstruction term. p consists of a set of B 2-D feature maps, thus forming an over complete-basis. To give a unique solution, a sparsity constraint on p is needed and we use an element-wise pseudo-norm where 0.5 \u2264 \u03b1 \u2264 1. The reconstruction v\u0302 is produced from p by two sub-layers: Unpooling and Convolution."}, {"heading": "2.1. Unpooling", "text": "In the unpooling sub-stage, each 2D feature map pb undergoes an unpooling operation to produce a larger 2D unpooled feature map zb1. Each element j in pb influences a small neighborhood Nj (typically 2 \u00d7 2 or 3 \u00d7 3) in the unpooled map zb, via a set of weightsw(i) within the neighborhood:\nzb(i) = w(i)pk(j) \u2200i \u2208 Nj (2)\n13D (un)pooling is also possible, as explored in [22].\nWe constrain the weights w(i) to have unit `2-norm, as this makes the unpooling operation invertible2. The inverse pooling operation computes each element j in pb as the sum of weights w(i) in neighborhood Nj of the unpooled map zb:\npb(j) = \u2211 i\u2208Nj w(i)zb(j) (3)\nIn Zeiler et al. [22], max (un)pooling was used, equivalent to w(i) being all zero, except for a single element set to 1. In this work, we consider more general w(i)\u2019s, as detailed in Section 2.5, treating them as latent variables which will be inferred for each input image. Note that each element in p has its own set of w\u2019s.\nFor the rest of the paper, we consider the neighborhoods Nj to be non-overlapping, but the above formulation generalizes to overlapping regions as well. For brevity, we write the unpooling operation as a single linear matrix, parameterized by weights w: z = Uwp."}, {"heading": "2.2. Convolution", "text": "In the convolution sub-stage, the reconstruction v\u0302c is formed by convolving 2D unpooled feature maps zb with filters f cb and summing them:\nv\u0302c = B\u2211 b=1 zb \u2217 f cb (4)\nwhere \u2217 is the 2D convolution operator. The filters f are the parameters of the model common to all images. The feature\n2Combining Eqs. 2 and 3, we have pb(j) = \u2211 i w 2(i)pb(j), hence\u2211\ni w 2(i)=1.\nmaps z are latent variables, specific to each image. For notational brevity, we combine the convolution and summing operations into a single convolution matrix F and convert the multiple 2D maps zb into a single vector z: v\u0302 = Fz."}, {"heading": "2.3. Discussion of Single Layer", "text": "The combination of unpooling and convolution operations gives the reconstruction v\u0302 from p:\n\u03bb 2 \u2016FUwp\u2212 v\u201622 + |p|\u03b1 (5)\nA single layer of the model is shown in the lower part of Fig. 1(a). This integrated formulation allows the straightforward optimization of the filters f , features p and the (un)pooling weights w to minimize a single objective function. While most other models also learn filters and features, the pooling operation is typically fixed. Direct optimization of Eqn. 5 with respect to w is one the main contributions of this work and is described in Section 2.5.\nNote that, given fixed weights w, the reconstruction is linear in p, thus Eqn. 5 describes a tri-linear model, with w coding position (where) information about the (what) features p.\nEqn. 5 differs from the original Deconvolutional Network formulation [22] in several important ways. First, sparsity is imposed directly on p, as opposed to z. This integrates pooling into the objective function, allowing it to become part of the inference. Second, [22] considers only \u03b1 = 1, rather than the hyper-Laplacian (\u03b1 < 1) sparsity we employ. Third, p is non-negative, as opposed to [22] where there was no such constraint. Fourth, and most importantly, by inferring the optimal (un)pooling weights w we directly minimize the objective function of the model. Fixed sum or max pooling, employed by other approaches, is a local heuristic that has no clear relationship to the overall cost."}, {"heading": "2.4. Multiple Layers", "text": "Multi-layer models are constructed by stacking the single layer model described above in the same manner as Zeiler et al. [22]. The feature maps p from one layer become the input maps to the layer above (which now has B \u201ccolor channels\u201d).\nAn important property of the model is that feature maps exist solely at the top of the model (there are no explicit features in intermediate layers), thus the only variables at the intermediate layers are filters F and unpooling weights w. For an l layer model, the reconstruction v\u0302 is:\nv\u0302 = F1Uw1F2Uw2 . . . FlUwlpl = Rlpl (6)\nwhere Fk and Uwk are the convolutional and unpooling operations from each layer k. We condense the sequence of unpooling and convolution operations into a single reconstruction operator Rl, which lets us write the overall object for a multi-layer model (shown here for a single image, v,\nbut optimized over a set of images v1, . . . , vI during training):\nCl(v) = \u03bb\n2 \u2016Rlpl \u2212 v\u201622 + |pl|\u03b1 (7)\nA multi-layer model is shown in Fig. 1(a). Note that since Rl is linear, given the (un)pooling weights w, the reconstruction term is easily differentiable. The derivative of Rl is simply RTl = U T wl FTl . . . U T w1F T 1 , which is a forward propagation operator. This takes a signal at the input and repeatedly convolves (using with flipped versions of the filters at each layer) and pools (using weights wl) all the way up to the features. This is a key operation for both inference and learning, as described in Section 3 and Section 4 respectively. Fig. 1(b) illustrates the reconstruction and forward propagation operations."}, {"heading": "2.5. Differentiable Pooling", "text": "We impose a parametric form on the (un)pooling weights w to ensure that the features are invariant to small changes in the input. The pooling would otherwise be able to memorize perfectly the unpooled features, giving \u201clossless\u201d pooling which would not generalize at all.\nThe parametric model we use is a 2D axis-aligned Gaussian, with mean (\u00b5x, \u00b5y) and precision (\u03b3x, \u03b3y) over the pooling neighborhood Nj , introduced in Section 2.1. The Gaussian is normalized within the extent of the pooling region to give weights w whose square sums to 1 (thus giving unit `2 norm):\nw(i) = \u221a a(i)\u221a\u2211 i\u2032 a(i \u2032) (8)\nwhere a(i) is value of the Gaussian for element i, at location x(i), y(i) within the neighborhood Nj :\na(i) = e\u2212[ \u03b3x 2 (x(i)\u2212\u00b5x) 2+ \u03b3y 2 (y(i)\u2212\u00b5y) 2] (9)\nFig. 1(c) shows an illustration of this parameterization. For brevity, we let \u03b8j = {\u00b5x, \u00b5y, \u03b3x, \u03b3y} be the parameters for neighborhood Nj . We thus rewrite the unpooling operation in Uwl as U\u03b8l . The Gaussian representation has several advantages over existing sum or max pooling:\n\u2022 Varying the mean of the Gaussian selects a particular region in the unpooled feature map, just like max pooling. This makes the feature invariant to small translations within the unpooled maps.\n\u2022 Varying the precision of the Gaussian allows a smooth variation between max and sum operations (high and low precision respectively).\n\u2022 Changes in precision allow invariance to small scale changes in the unpooled features. For example, the width of an edge can easily be altered by adjusting the variance (see Fig. 2(c)).\n\u2022 The continuous nature of the Gaussian allows subpixel reconstruction that avoids aliasing artifacts, which can occur with max pooling. See Fig. 5 for an illustration of this.\n\u2022 The Gaussian representation is differentiable, i.e. the gradient of Eqn. 5 with respect to \u03b8j has analytic form, as detailed in Section 3.2."}, {"heading": "2.6. Non-Negativity", "text": "In standard sparse coding and other learning methods both the feature activations and the learned parameters can be positive or negative. This contrasts with our model, in which we enforce non-negativity.\nThis is motivated by several factors. First, there is no notion of a negative intensities or objects in the visual world. Second, the Gaussian parameterization used in the differentiable pooling scheme, described in Section 2.5 has positive weights, so cannot represent individual negative values in the unpooled feature maps. Third, there is some biological evidence for non-negative representations within the brain [10]. Finally, we find experimentally that non-negativity reduces the flexibility of the model, encouraging it to learn good representations. The features computed at test-time have improved classification performance, compared with models without this constraint (see Section 6.4)."}, {"heading": "2.7. Hyper-Laplacian Sparsity", "text": "Most sparse coding models utilize the `1-norm to enforce a sparsity constraint on the features [16], as a proxy for optimizing `0 sparsity [21]. However, a drawback of this form of regularization is that it gives the same cost to two elements being 0.5 versus a single elements at 1 and the other at 0, even though the latter has a lower `0 cost.\nTo encourage features with lower `0 cost, we use a pseudo-norm `0.5 (i.e. \u03b1 = 0.5 in Eqn. 5) inspired by Krishnan and Fergus [11], which aggressively pushes small elements toward zero. To optimize this, we experimented with techniques in [11], but settled on gradient descent for simplicity."}, {"heading": "3. Inference", "text": "During inference, the filters f at all layers are fixed and the objective is to find the features p and (un)pooling variables \u03b8 for all neighborhoods and all layers that minimize Eqn. 7. We do this by alternating between updating the features p and the Gaussian variables \u03b8, while holding the other fixed."}, {"heading": "3.1. Feature Updates", "text": "For a given layer l, we seek the features pl that minimize Cl(v) (Eqn. 7), given an input image v, filters f1, . . . , fl and unpooling variables \u03b81, . . . , \u03b8l. This is a large convolutional\nsparse coding problem and we adapt the ISTA scheme of Beck and Teboulle [1]. This uses an iterative framework of gradient and shrinkage steps.\nGradient step: The gradient of Cl(v) with respect to pl is:\n\u2207pl = \u2202Cl(v)\n\u2202pl = RTl (Rlpl \u2212 v) (10)\nThis involves first reconstructing the input from the current features: v\u0302 = Rlpl, computing the error signal e = (v\u0302\u2212 v), and then forward propagating this up to compute the top layer gradient\u2207pl = RTl e. Given the gradient, we then can update pl: pl = pl \u2212 \u03bbl\u03b2pl\u2207pl (11) where the \u03b2pl parameter sets the size of the gradient step.\nShrinkage step: Following the gradient step, we perform a per-element shrinkage operation that clamps small elements in pl to zero, increasing its sparsity. For \u03b1 = 1, we use the standard `1 shrinkage:\npl = max(|pl| \u2212 \u03b2pl , 0) \u00b7 sign(pl) (12)\nFor \u03b1 = 0.5, we step in the direction of the gradient:\npl = pl \u2212 \u03b2pl 1\n2\n\u221a |pl| \u22121 \u00b7 sign(pl) (13)\nProjection step: After shrinking small elements away, the solution is then projected onto the non-negative set:\npl = max(pl, 0) (14)\nStep size calculation: In order to set a learning rate for the feature map optimization, we employ an estimation technique for steepest descent problems [20] which uses the gradients\u2207pl = \u2202C\u2202pl :\n\u03b2pl = \u2207pTl \u2207pl\n\u2207pTl RTl Rl\u2207pl (15)\nAutomating the step-size computation has two advantages. First, each layer requires a significantly different learning rate on account of the differences in architecture, making it hard to set manually. Second, by computing the step-size before each gradient step, each ISTA iteration makes good progress at reducing the overall cost. In practice, we find fixed step-sizes to be significantly inferior. \u2207pl is computed once per mini-batch. For efficiency, instead of computing the denominator in Eqn. 15 for each image, we estimate it by selecting a small portion (\u223c10%) of each mini-batch.\nReset step: Repeated optimization of the objective function tends to get stuck in local minima as it proceeds over the dataset for several epochs. We found a simple and effective way to overcome this problem. By setting all feature maps pl to 0 every few epochs (essentially re-initializing inference), cleaner filters and better performing features can be learned, as demonstrated in Section 6.5.\nThis reset may be explained as follows. During alternating inference and learning stages, the model can overfit a\nmini-batch of data by optimizing either the filters or feature maps too much. This causes the model to lock up in a state where no new feature map element can turn on because the reconstruction performance is sufficient to have only a small error propagating forward to the feature level. Since no new features turn on after shrinkage, the filters remain fixed as they continue to get the same gradients. This can happen early in the learning procedure when the filters are still not optimal and therefore the learned representation suffers. By resetting the feature maps, at the next epoch the model has to reevaluate how to reconstruct the image from scratch, and can therefore turn on the optimal feature elements and continue to optimize the filters."}, {"heading": "3.2. (Un)pooling Variable Updates", "text": "Given a model with l layers, we wish to update the (un)pooling variables \u03b8k at each intermediate layer k to optimize the objective Cl(v). We assume that the filters f1, . . . , fl and features pl are fixed.\nThe gradients for the pooling variables \u03b8k involve combining, at layer k, the forward propagated error signal with the top down reconstruction signal. This combined signal then drives the update of the pooling variables. More formally:\n\u2202Cl(v)\n\u2202U\u03b8k = RTk (Rlpl \u2212 v) \u00b7 (R(l\u2192k)pl) (16)\nwhereRl\u2192k is the top down reconstruction from layer l feature maps to layer k feature maps and RTk is the error propagation up to zk.\nWith the chosen Gaussian parameterization of the pooling regions, the chain rule can be used to compute the gradient for each parameter \u03b8k = {\u00b5x, \u00b5y, \u03b3x, \u03b3y}:\n\u2207\u03b8k = \u2202Cl(v)\n\u2202\u03b8k(j) = \u2211 i\u2032\u2208Nj \u2202Cl(v) \u2202U\u03b8k(i \u2032) \u2202U\u03b8k(i \u2032) \u2202w(i\u2032) \u2211 i\u2208Nj \u2202w(i\u2032) \u2202a(i) \u2202a(i) \u2202\u03b8k(j)\n(17) where j is the neighborhood index,\n\u2202U\u03b8k(i \u2032)\n\u2202w(i\u2032) = p\u0302k(i\n\u2032) = (R(l\u2192k)pl)(i \u2032) (18)\n\u2202w(i\u2032)\n\u2202a(i) = ( \u2211 n\u2208N a(n))\u22121[w(i)] (19)\n\u2202w(i\u2032) \u2202a(i\u2032) = ( \u2211 n\u2208N a(n))\u22121[1\u2212 w(i\u2032)] (20)\n\u2202a(i)\n\u2202\u00b5x(j) = \u03b3x(j)(x(i)\u2212 \u00b5x(j))a(i) (21)\n\u2202a(i)\n\u2202\u00b5y(j) = \u03b3y(j)(y(i)\u2212 \u00b5y(j))a(i) (22)\n\u2202a(i) \u2202\u03b3x(j) = \u22121 2 (x(i)\u2212 \u00b5x(j))2a(i) (23)\n\u2202a(i) \u2202\u03b3y(j) = \u22121 2 (y(i)\u2212 \u00b5y(j))2a(i) (24)\nwhere x(i) and y(i) are the coordinates within the pooling neighborhood Nj .\nAlgorithm 1 Learning with Differentiable Pooling in Deconvolutional Networks Require: Training set Y , # layers L, # epochs E, # ISTA steps T Require: Regularization coefficients \u03bbl, # feature maps Bl Require: Pooling step sizes \u03b2Ul\n1: for l = 1 : L do %% Loop over layers 2: Init. features/filters: pil \u223c 0, fl \u223c N (0, ) 3: Init. switches: \u03b8il = Fit(R T l yi) \u2200i 4: for epoch = 1 : E do %% Epoch iteration 5: for i = 1 : N do %% Loop over images 6: for t = 1 : T do %% ISTA iteration 7: Reconstruct input: v\u0302li = Rlpil 8: Compute reconstruction error: e = v\u0302li \u2212 vi 9: Propagate error up to layer l: \u2207pl = RTl e\n10: Estimate step size \u03b2pl as in Eqn. 15 11: Take gradient step on p: pil = p i l \u2212 \u03bbl\u03b2pl\u2207pl 12: Perform shrink: pil = max(|pil| \u2212 \u03b2pl , 0)sign(p i l) 13: Project to positive: pil = max(p i l, 0) 14: for k = 1 : l do %% Loop over lower layers 15: Take gradient step on \u03b8: \u03b8ik = \u03b8 i k \u2212 \u03bbl\u03b2Uk\u2207\u03b8k 16: end for 17: end for 18: end for 19: Update fl by solving Eqn. 26 using CG 20: Project fl to positive and unit length 21: end for 22: end for 23: Output: filters f , feature maps p and pooling variables \u03b8.\nOnce the complete gradient is computed as in Eqn. 17, we do a gradient step on each pooling variable:\n\u03b8k = \u03b8k \u2212 \u03bbl\u03b2Uk\u2207\u03b8k (25) using a fixed step size \u03b2Uk . We experimented with a similar step size to Eqn. 15 for the pooling parameters, however found the estimates to be unstable, likely due to the nonlinear derivatives involved in the Gaussian pooling."}, {"heading": "4. Learning", "text": "After inference of the feature maps for the top layer and (un)pooling variables for all layers is complete, the filters in each layer are updated. This is done using the gradient with respect to each layer\u2019s filters:\n\u2202Cl \u2202(f bc )k = \u03bbl[R T k\u22121(Rlpl \u2212 v)]c \u2217 [(U\u03b8kR(l\u2192k)pl)]b (26) where the left term is the bottom up error signal propagated up to the feature maps below the given filters, pk\u22121 and the right term is the top down reconstruction to the unpooled feature maps zk. The gradient is therefore the convolution between all combinations of input error maps to the layer (indexed by c) and the unpooled feature maps reconstructed from above (indexed by b), resulting in updates of each filter plane f bc , for each layer k.\nIn practice we use batch conjugate gradient updates for learning the filters as the model is linear in Fk once the fea-\nture maps and pooling parameters are inferred. After 2 steps of conjugate gradients, the filters are projected to be nonnegative and renormalized to unit `2 length."}, {"heading": "4.1. Joint Inference", "text": "The objective function explicitly constrains the reconstruction from the top layer features to be close to the input image. From this we can calculate gradients for each layer\u2019s filters and pooling variables while optimizing the top level features maps. Therefore for each image we can infer the local shifts and scalings of low level features as the high level concepts develop.\nWe have found that pre-training the first layer in one phase of training and then using the pooling variables and learned layer 1 filters to initialize a second phase of training works best. The second phase of training optimizes the second layer objective from which we can update p2, Uw2 , Uw1 , F2, and F1 jointly. If care is not taken in this joint update, the first layer features can trade off representation power with the second layer filters. This can result in the second layer filters capturing the details while the first layer filters become dots. To avoid this problem, after the first phase of training we hold F1 fixed and optimize the remaining variables jointly. Thus, while the filters are learned layer-by-layer, inference is always performed jointly across all layers. This has the nice property that these low level parts can move and scale as the Uw1 variables are optimized while the high level concepts are learned."}, {"heading": "5. Initialization of Parameters", "text": "Before training, the filter parameters are initialized to Gaussian distributed random values. After this random initialization, the filters are projected to be non-negative and normalized to unit length before training begins.\nBefore inference, either at the start of training or at test time, we initialize the features maps to 0. This creates a reconstruction of 0 in the pixel space, therefore the initial gradient being propagated up the network is \u2212y. This is similar to a feedforward network for the first iteration of inference. While forward propagating this signal up the network we can leverage the Gaussian parameterization of the pooling regions to fit these pooling parameters using moment matching. That is, at each layer, we extract the optimal pooling parameter that fit this bottom up signal. This provides a natural initialization to both the pooling variables at each layer and the top level feature activations given the input image and the filter initialization."}, {"heading": "6. Experiments", "text": "Evaluation on MNIST We choose to evaluate our model on the MNIST handwritten digit classification task. This dataset provides a relatively large number of training instances per class, has many other results to compare to, and\nallows easy interpretation of how a trained model is decomposing each image.\nPre-processing: The inputs were the unprocessed MNIST digits at 28x28 resolution. Since no preprocessing was done, the elements remained nonnegative.\nModel architecture: We trained a 2 layer model with 5x5 filters in each layer and 2x2 non-overlapping pooling regions. The first layer contained 16 feature maps and the second layer contained 48 features maps. Each of these 48 feature maps connect randomly to 8 different layer 1 feature maps through the second layer filters. These sizes were chosen comparable to [22] while being more amenable to GPU processing. The receptive fields of the second layer features are 14x14 pixels with this configuration, or one quarter the input image size.\nClassification: One motivation of this paper was to analyze how the classification pipeline of Zeiler et al. [22] could be simplified by making the top level features of the network more informative. Therefore, in this paper we simply treat the top level activations inferred for each image as input to a linear SVM [6].\nThe only post processing done to these high level activations is that overlapping patches are extracted and pooled, analogous to the dense SIFT processing which is shown by many computer vision researchers to improve results [2]. This step provides an expansion in the number of inputs, allowing the linear SVM to operate in a higher dimensional space. For layer 1 classification these patches were 9x9 elements of the layer 1 features maps. For layer 2 they were 6x6 patches, roughly the same ratio to the feature map size as for layer 1. These patches were concatenated as input to the classifier. Throughout the experiments we did not combine features from multiple layers, concatenating only layer 1 patches together for layer 1 classification and only layer 2 features together for layer 2 classification. These final inputs to the classifier were each normalized to unit length.\nHyperparameters: By cross validating on a 50,000 train and 10,000 validation set of MNIST images, we found that \u03bb1 = 2 and \u03bb2 = 0.5 gave optimal classification performance. Each layer was trained with 100 ISTA steps/epoch for 50 epochs (passes through the dataset). After epoch 25, the feature maps were reset to 0 during training. At test time, we found higher \u03bb1 = 5 and \u03bb2 = 5 improved classification, as did optimizing for only 50 ISTA steps of inference."}, {"heading": "6.1. Model visualization", "text": "By visualizing the filters and features maps of the model, we can easily understand what it has learned. In Fig. 2 (a) we demonstrate sharp reconstructions of the input images from the second layer features maps. In Fig. 2 (b) we display the raw filter coefficients for layer 1 which have learned small pieces of strokes. By incorporating the pooling parameters into the layer, these filters are robust to small\nchanges in the input. Visualizing these invariances of a model can be helpful in understanding the inputs the model is sensitive to. Searching through the dataset of inferred feature map activations and selecting the maximum element per feature map to project downward into the pixel space as in [22] is one way of visualizing these invariances. However, these selected elements are only exemplars of inputs that most strongly activated that feature. In Fig. 2(c) we show a more representative selection of invariances by instead selecting a feature activation to be projected down based on sampling from the distribution of activations for that feature inf the dataset. This gives a less biased view of what activates that feature than selecting the largest few activations from the dataset. Once a sample is selected for a given feature map, the pooling variables corresponding to the image from which the activation was selected are used in the unpooling stages to do the top down visualization.\nExamining the 16 sample visualizations for each feature in Fig. 2(c) shows the scale and shifts that the Gaussian pooling provides to these relatively simple first layer filters. We can continue to analyze the model by viewing the layer 2 filters planes in Fig. 2(d). Each of the 48 second layer features has 16 filter planes (shown in separate groups), one connecting to each of the layer 1 feature maps. While the second layer filters are difficult to understand directly, we can visualize the learned representation of the second layer by projecting down all the way to the pixel space through layer 1. Fig. 2(e) shows for each of the 48 feature maps a 4x4 grid of pixel space projections obtained by sampling 16 activations from the distribution of activations of each layer 2 feature and projecting down via alternating convolution and unpooling with the corresponding pooling variables separately for each activation.\nWhile analyzing the features in pixel space is informative, we have also found it is useful to view the features as decompositions of an input image to know how the model is representing the data. One possible method of displaying the decomposition is by coloring each pixel of the reconstruction according to which feature it came from. Each feature is assigned a hue (in no particular order) and the associated reconstruction produced then defines the saturation of that color. The resulting image therefore depicts the high level feature assignments. Pixels with brownish colors indicate a summation of several colors (features) together. Note that the input images themselves are grayscale \u2013 the colors are just for visualization purposes.\nIn Fig. 3(d) we show such a reconstruction from layer 1 for the original image in (e). To understand the model we also show the layer 1 feature map activations in (a) with their corresponding color assignment around them. Notice the sparse distribution of activations can reconstruct the entire image by utilizing the Gaussian pooling and layer 1 fil-\nters in (c). Fig. 3(b) shows the result of this unpooling operation on the feature maps. Notice in the orange and purple boxes the elongated lines in the unpooled maps, made possible by a low precision in one dimension.\nFig. 4 takes this analysis one step further by using the second layer of the model. Starting from 3 features in the layer 2 feature maps as shown in (a), they are unpooled (as shown in (b)) and then convolved with the second layer filters to reconstruct many elements down on to the first layer features maps (c). These are further unpooled to (d) where again you can see the benefits of the Gaussian pooling smoothly transitioning between non-overlapping pooling regions. These are finally convolved with first layer filters (e) to give the decomposition shown in (f). Notice how long range structures are grouped into common features in the higher layer compared to the layer 1 decomposition of Fig. 3."}, {"heading": "6.2. Max Pooling vs Gaussian Pooling", "text": "The discrete locations that max pooling allows within a region are a limiting factor in the reconstruction quality of the model. Fig. 5 (bottom) shows a significant aliasing effect is present in the visualizations of the model when Max pooling is used. With the complex interactions between positive and negative elements removed, the model is not able to form smooth transitions between non overlapping pooling regions even though the filters used in the succeeding convolution sublayer have overlap between regions. Using the Gaussian pooling, the model can infer the desired precisions and means in order to optimize the reconstruction quality from high layers of the model.\nThis fine tuning of reconstruction allows for improvements without significantly varying the features activations (ie. maintains or decreases the sparsity while adjusting the pooling parameters). This is confirmed in Fig. 6 where we break down the cost function into the reconstruction and regularization terms. In this figure we also display the `0 sparsity of each model as this can directly be used for comparison.\nThe Gaussian pooling significantly outperforms Max pooling in terms of optimizing the objective. By not being able to adjust the pooling variables to optimize the overall cost, Max pooling plateaus despite running for many epochs. Additionally it has a much higher `0 cost throughout training. In contrast, the `0 cost with Gaussian pooling decreases smoothly throughout training because the model can fine tune the pooling parameters to explain much more with each feature activation. This property is shown in Table 1 to significantly improve classification performance compared to Max pooling when stacking."}, {"heading": "6.3. Joint Inference", "text": "One of the main criticisms of sparse coding methods is that inference must be conducted even at test time due to the lack of a feedforward connection to encode the features. In our approach we discovered two fundamental techniques that mitigate this drawback.\nThe first is that running a joint inference procedure over both layers of our network improves the classification performance compared to running each layer separately. Instead of inferring the feature maps and pooling variables for the first layer and then using these pooling variables to initialize the second layer inference (2 phases), we can directly run inference with a two layer model. The differentiable pooling allows us to infer the pooling variables of both layers in addition to the layer 2 feature values simultaneously in 1 phase. At the first iteration of inference we leverage the ability to fit the Gaussian pooling parameters in a feed forward way as mentioned in Section 5. This halves the number of inference iterations needed by not requiring any first layer inference prior to inferring the second layer.\nTo examine this first discovery in depth we considered several combinations of how to joint train and then run inference at test time with this model. During training we have found both qualitatively in terms of feature diversity and quantitatively in terms of classification performance that training in separate phases, one for each layer of the model, works better than jointly training both layers from scratch. In the second phase of training, when optimizing for reconstruction from the second layer feature maps, the first layer pooling variables and filters can either be updated or held fixed. Each row of Table 2 examines each combi-\nnation of these updates during training. We can see that the optimal training scheme was with fixed first layer filters but pooling updates on both layers. This made the system more stable while still allowing these first layer filters to move and scale as needed by updating the first layer pooling variables.\nIn all cases we see a significant reduction in error rates when doing inference in 1 phase. The middle column of the table shows this 1 phase inference, but without optimizing the first layer pooling parameters U1 whereas the last column does optimize U1. We see an improvement in updating U1 for all but the last row which was trained without U1 and so is used to that type of inference. This improvement with joint inference of U1, U2, and p2 is a key finding which is only possible with differentiable pooling.\nThe second discovery that reduces evaluation time is that running the same number of ISTA iterations as was done during training does not give optimal classification performance, possibly due to over-sparsification of the features. Similarly running with too few iterations also reduces performance. Fig. 7 shows a plot comparing the number of ISTA iterations to the classification performance with an optimum at 50 ISTA steps, half the number used during training."}, {"heading": "6.4. Effects of Non-Negativity", "text": "With negative elements present in the system, many possible solutions can be found during optimization. This happens because subtractions allow the removal of portions of high level features. This has the effect of making them less discriminative because the model can change parameters inbetween the high level feature activations and the input image in order to reconstruct better while assigning less meaning to the feature activations themselves.\nTo show this is not an artifact of the Gaussian pooling being more suited to nonnegative systems (due to the summation over the pooling region possibly leading to cancellations if negatives are present), we include comparison in Table 3 to Max pooling. In both cases, enforcing positivity via projected gradient descent improves the discriminative information preserved in the features."}, {"heading": "6.5. Effects of Feature Reset", "text": "When training the model on MNIST, some less than optimal filters are learned when not resetting the feature maps. For example, in Fig. 8 (c) many of these layer 1 filters are block-like such as the 3rd row, 2nd column. However this same feature in (a) improves if the feature maps are reset to 0 once half way through training. This single reset is enough to encourage the filters to specialize and improve. Similarly, the layer 2 pixel visualizations in (b) have much more variation due to the reset compared to (d) which did not have the reset. In particular, notice many blob-like features learned in (d) without reset such as the 2nd and 5th rows of the 1st column that improve in (b). These larger, more varied features learned with the reset help improve classification performance as shown in Table 4."}, {"heading": "6.6. Effects of Hyper-Laplacian Sparsity", "text": "It has previously been shown that sparsity encourages learning of distinctive features, however it is not necessarily useful for classification [18] [22]. We analyze this in the context of hyper-laplacian sparsity applied to both training and inference. In this comparison we trained two models, one with a `1 prior on the feature maps and the other with\na `0.5 prior. Once trained, we took each model and ran inference with both `1 and `0.5 priors. For reference the `0 sparsity for the training runs was 4.2 for the `0.5 regularized training and 20.2 for the `1 regularized training with the same \u03bb2 = 0.5 setting. Since the amount of sparsity can also be controlled during inference by the \u03bb2 parameter, we plot in Fig. 9 the classification performance for various \u03bb2 settings in these four model combinations.\nInterestingly, utilizing the added sparsity during training enforced by the `0.5 while using the more relaxed `1 prior for inference is the optimal combination for all \u03bb settings. This suggests sparsity is useful during training to learn meaningful features, but is not as useful for inference at test time."}, {"heading": "6.7. Comparison to Other Methods", "text": "We chose the MNIST dataset for it\u2019s large number of results to compare to. Of these, deep learning methods typically fall into one of two categories, 1) those that are completely unsupervised and have a simple classifier on top, or 2) those that are fine-tune discriminatively with labels. Our method falls into the first category as it is completely unsu-\npervised during training, and only the linear SVM applied on top has access to the label information of the training set. We do not back propagate this information through the network, but this would be an interesting future direction to pursue. Table 5 shows our method is competitive with other deep generative models, even surpassing several which use discriminative fine tuning."}, {"heading": "7. Discussion", "text": "In this work we introduced the concept of differentiable pooling for deep learning methods. Also, we demonstrated that joint training the model improves performance, positivity encourages the model to learn better representations, and that there is an optimal amount of sparsity to be used during training and inference. Finally, we introduced a simple resetting scheme to avoid local minimum and learn better features. We believe many of the approaches and findings in this work are applicable not only to Deconvolutional Networks but also to sparse coding and other deep learning methods in general."}], "references": [{"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning midlevel features for recognition", "author": ["Y. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "In CVPR. IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "A theoretical analysis of feature pooling in vision algorithms", "author": ["Y. Boureau", "J. Ponce", "Y. LeCun"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Rapid inference on a novel and/or graph for object detection, segmentation and parsing", "author": ["Y. Chen", "L. Zhu", "C. Lin", "A. Yuille", "Z. H"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "The shape boltzmann machine: a strong model of object shape", "author": ["S. Eslami", "N. Heess", "J. Winn"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S. Wang"], "venue": "In ICANN-11,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. The"], "venue": "Neuro Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Modeling receptive fields with non-negative sparse coding", "author": ["P.O. Hoyer"], "venue": "Neurocomputing, 52-54:547\u2013552,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Analytic Hyper-Laplacian Priors for Fast Image Deconvolution", "author": ["D. Krishnan", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Comput.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Learning deep energy models", "author": ["J. Ngiam", "Z. Chen", "P. Koh", "A. Ng"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object reocgnition", "author": ["M. Ranzato", "F. Huang", "Y. Boureau", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Are sparse representations really relevant for image classification", "author": ["R. Rigamonti", "M. Brown", "V. Lepetit"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In AISTATS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["J.R. Shewchuk"], "venue": "Neural Comput.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M. Zeiler", "G. Taylor", "R. Fergus"], "venue": "In ICCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Latent hierarchical structural learning for object detection", "author": ["L. Zhu", "Y. Chen", "A. Yuille", "W. Freeman"], "venue": "In CVPR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}], "referenceMentions": [{"referenceID": 21, "context": "[22]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "One class of methods, for example Convolutional Neural Networks [13] or the recent RICA model of Le et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "[12], use a purely feed-forward hierarchy that maps the input image to a set of features which are presented to a simple classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "These include Deep Belief Networks [9], Deep Boltzmann Machines [19] and the Compositional Models of Zhu et al.", "startOffset": 35, "endOffset": 38}, {"referenceID": 18, "context": "These include Deep Belief Networks [9], Deep Boltzmann Machines [19] and the Compositional Models of Zhu et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "[23, 4].", "startOffset": 0, "endOffset": 7}, {"referenceID": 3, "context": "[23, 4].", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "[3] for an analysis).", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "A few approaches do perform full joint training of the layers, notably the Deep Boltzmann Machine [19], and Eslami et al.", "startOffset": 98, "endOffset": 102}, {"referenceID": 4, "context": "[5], as applied to images, and the Deep Energy Models of Ngiam et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17], the transforming auto-encoders of Hinton et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7], and Zeiler et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "13D (un)pooling is also possible, as explored in [22].", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "[22], max (un)pooling was used, equivalent to w(i) being all zero, except for a single element set to 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "5 differs from the original Deconvolutional Network formulation [22] in several important ways.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "Second, [22] considers only \u03b1 = 1, rather than the hyper-Laplacian (\u03b1 < 1) sparsity we employ.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Third, p is non-negative, as opposed to [22] where there was no such constraint.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Third, there is some biological evidence for non-negative representations within the brain [10].", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "Most sparse coding models utilize the `1-norm to enforce a sparsity constraint on the features [16], as a proxy for optimizing `0 sparsity [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "Most sparse coding models utilize the `1-norm to enforce a sparsity constraint on the features [16], as a proxy for optimizing `0 sparsity [21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "5) inspired by Krishnan and Fergus [11], which aggressively pushes small elements toward zero.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "To optimize this, we experimented with techniques in [11], but settled on gradient descent for simplicity.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "This is a large convolutional sparse coding problem and we adapt the ISTA scheme of Beck and Teboulle [1].", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "Step size calculation: In order to set a learning rate for the feature map optimization, we employ an estimation technique for steepest descent problems [20] which uses the gradients\u2207pl = \u2202C \u2202pl : \u03b2pl = \u2207pl \u2207pl \u2207pl RT l Rl\u2207pl (15)", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "These sizes were chosen comparable to [22] while being more amenable to GPU processing.", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "[22] could be simplified by making the top level features of the network more informative.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Therefore, in this paper we simply treat the top level activations inferred for each image as input to a linear SVM [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "The only post processing done to these high level activations is that overlapping patches are extracted and pooled, analogous to the dense SIFT processing which is shown by many computer vision researchers to improve results [2].", "startOffset": 225, "endOffset": 228}, {"referenceID": 21, "context": "Searching through the dataset of inferred feature map activations and selecting the maximum element per feature map to project downward into the pixel space as in [22] is one way of visualizing these invariances.", "startOffset": 163, "endOffset": 167}, {"referenceID": 17, "context": "It has previously been shown that sparsity encourages learning of distinctive features, however it is not necessarily useful for classification [18] [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 21, "context": "It has previously been shown that sparsity encourages learning of distinctive features, however it is not necessarily useful for classification [18] [22].", "startOffset": 149, "endOffset": 153}, {"referenceID": 13, "context": "84% \u2013 CDBN (1+2 layers) [14] 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "82% \u2013 DBN (3 layers) [8] [9] 2.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "82% \u2013 DBN (3 layers) [8] [9] 2.", "startOffset": 25, "endOffset": 28}, {"referenceID": 18, "context": "DBM (2 layers) [19] \u2013 0.", "startOffset": 15, "endOffset": 19}], "year": 2012, "abstractText": "We introduce a parametric form of pooling, based on a Gaussian, which can be optimized alongside the features in a single global objective function. By contrast, existing pooling schemes are based on heuristics (e.g. local maximum) and have no clear link to the cost function of the model. Furthermore, the variables of the Gaussian explicitly store location information, distinct from the appearance captured by the features, thus providing a what/where decomposition of the input signal. Although the differentiable pooling scheme can be incorporated in a wide range of hierarchical models, we demonstrate it in the context of a Deconvolutional Network model (Zeiler et al. [22]). We also explore a number of secondary issues within this model and present detailed experiments on MNIST digits.", "creator": "LaTeX with hyperref package"}}}