{"id": "1211.3966", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2012", "title": "Lasso Screening Rules via Dual Polytope Projection", "abstract": "Lasso perrino is stora a porkpie widely used regression keokuk technique to non-adjacent find sparse 09:30 representations. bergdolmo When become the wis. dimension of fondant the feature space 45.37 and preservation the wqew number of bhanjyang samples neos are shooto extremely wondrously large, solving the Lasso problem remains challenging. sembilan To improve the efficiency rondeau of solving large - opposition scale Lasso muasher problems, El dille Ghaoui and his colleagues have proposed interhostel the SAFE divesting rules 73.70 which are doges able to quickly identify maindy the inactive predictors, i. e. , w8 predictors jicai that malipiero have breuil 0 components kahar in the solution vector. Then, the yeshurun inactive predictors rbh or sakda features najamudeen can be sawston removed euskaltel from immunocompetent the optimization dooling problem to telescript reduce its scale. By saturniidae transforming 8-minute the standard magnitude Lasso to filmex its dual 40.03 form, it can be shown hydrous that ambiorix the newari inactive bavasi predictors include semi-solid the set delury of bodiford inactive qirbi constraints line-item on the oladapo optimal dual norval solution. In rs-68 this resta paper, 12,000-member we propose a fiction-writing fast morgellons and vioxx efficient maldito screening rule elmes via chagall Dual tutuila Polytope Projections (7,655 DPP ), which is mainly misspelling based on the nunberg uniqueness babati and mccoughtry nonexpansiveness of the colwell optimal dual solution due to frw the 376th fact punya that filmstrips the fitts feasible orkanger set safia in the bures dual space tipos is a raucus convex rosebud and closed inkers polytope. Moreover, pollina we leverage show that 239.7 our 2,558 screening laudes rule 101-85 can williamston be klei extended 20:10 to singaporean identify inactive groups firouzabadi in group boophis Lasso. 77.46 To the pre-1970 best krims of our cartoonishly knowledge, there are marini currently unattainable no \" toccata exact \" screening mpenza rules kabardino for group Lasso. immunizations We lubet have delfouneso evaluated our hindquarters screening canicoba rule using zeeshan both paynter synthetic hightone and custoza real data dufton sets. gehenna Results low-rent show okemo that our detente rule is 26-0 more 34-inch effective to 1.204 identify inactive predictors poea than notability existing eigenstates state - ronaldsay of - kosheh the - hashana art screening casuistry rules.", "histories": [["v1", "Fri, 16 Nov 2012 17:48:42 GMT  (479kb,D)", "https://arxiv.org/abs/1211.3966v1", null], ["v2", "Sat, 1 Feb 2014 00:12:10 GMT  (911kb,D)", "http://arxiv.org/abs/1211.3966v2", null], ["v3", "Wed, 15 Oct 2014 20:18:33 GMT  (445kb,D)", "http://arxiv.org/abs/1211.3966v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jie wang", "jiayu zhou", "peter wonka", "jieping ye"], "accepted": true, "id": "1211.3966"}, "pdf": {"name": "1211.3966.pdf", "metadata": {"source": "CRF", "title": "Lasso Screening Rules via Dual Polytope Projection", "authors": ["Jie Wang", "Peter Wonka", "Jieping Ye"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Data with various structures and scales comes from almost every aspect of daily life. To effectively extract patterns in the data and build interpretable models with high prediction accuracy is always desirable. One popular technique to identify important explanatory features is by sparse regularization. For instance, consider the widely used `1-regularized least squares regression problem known as Lasso [31]. The most appealing property of Lasso is the sparsity of the solutions, which is equivalent to feature selection. Suppose we have N observations and p features. Let y denote the N dimensional response vector and X = [x1,x2, . . . ,xp] be the N \u00d7p feature matrix. Let \u03bb \u2265 0 be the regularization parameter. The Lasso problem is formulated as the following optimization problem:\ninf \u03b2\u2208Rp\n1 2 \u2016y \u2212X\u03b2\u201622 + \u03bb\u2016\u03b2\u20161. (1)\nLasso has achieved great success in a wide range of applications [13, 12, 38, 9, 34] and in recent years many algorithms have been developed to efficiently solve the Lasso problem [15, 20, 26, 14, 18, 4, 19]. However, when the dimension of feature space and the number of samples are very large, solving the Lasso problem remains challenging because we may not even be able to load the data matrix into main memory. The idea of screening has been shown very promising in solving Lasso for\nar X\niv :1\n21 1.\n39 66\nv3 [\ncs .L\nG ]\n1 5\nO ct\n2 01\n4\nlarge-scale problems. Essentially, screening aims to quickly identify the inactive features that have 0 components in the solution and then remove them from the optimization. Therefore, we can work on a reduced feature matrix to solve the Lasso problem, which may lead to substantial savings in computational cost and memory usage.\nExisting screening methods for Lasso can be roughly divided into two categories: the Heuristic Screening Methods and the Safe Screening Methods. As the name indicated, the heuristic screening methods can not guarantee that the discarded features have zero coefficients in the solution vector. In other words, they may mistakenly discard the active features which have nonzero coefficients in the sparse representations. Well-known heuristic screening methods for Lasso include SIS [17] and strong rules [32]. SIS is based on the associations between features and the prediction task, but not from an optimization point of view. Strong rules rely on the assumption that the absolute values of the inner products between features and the residue are nonexpansive [3] with respect to the parameter values. Notice that, in real applications, this assumption is not always true. In order to ensure the correctness of the solutions, strong rules check the KKT conditions for violations. In case of violations, they weaken the screened set and repeat this process. In contrast to the heuristic screening methods, the safe screening methods for Lasso can guarantee that the discarded features are absent from the resulting sparse models. Existing safe screening methods for Lasso includes SAFE [16] and DOME [36, 35], which are based on an estimation of the dual optimal solution. The key challenge of searching for effective safe screening rules is how to accurately estimate the dual optimal solution. The more accurate the estimation is, the more effective the resulting screening rule is in discarding the inactive features. Moreover, Xiang et al. [36] have shown that the SAFE rule for Lasso can be read as a special case of their testing rules.\nIn this paper, we develop novel efficient and effective screening rules for the Lasso problem; our screening rules are safe in the sense that no active features will be discarded. As the name indicated (DPP), the proposed approaches heavily rely on the geometric properties of the Lasso problem. Indeed, the dual problem of problem (1) can be formulated as a projection problem. More specifically, the dual optimal solution of the Lasso problem is the projection of the scaled response vector onto a nonempty closed and convex polytope (the feasible set of the dual problem). This nice property provides us many elegant approaches to accurately estimate the dual optimal solutions, e.g., nonexpansiveness, firmly nonexpansiveness [3]. In fact, the estimation of the dual optimal solution in DPP is a direct application of the nonexpansiveness of the projection operators. Moreover, by further exploiting the properties of the projection operators, we can significantly improve the estimation of the dual optimal solution. Based on this estimation, we develop the so called enhanced DPP (EDPP) rules which are able to detect far more inactive features than DPP. Therefore, the speedup gained by EDPP is much higher than the one by DPP.\nIn real applications, the optimal parameter value of \u03bb is generally unknown and needs to be estimated. To determine an appropriate value of \u03bb, commonly used approaches such as cross validation and stability selection involve solving the Lasso problems over a grid of tuning parameters \u03bb1 > \u03bb2 > . . . > \u03bbK. Thus, the process can be very time consuming. To address this challenge, we develop the sequential version of the DPP families. Briefly speaking, for the Lasso problem, suppose we are given the solution \u03b2\u2217(\u03bbk\u22121) at \u03bbk\u22121. We then apply the screening rules to identify the inactive features of problem (1) at \u03bbk by making use of \u03b2\n\u2217(\u03bbk\u22121). The idea of the sequential screening rules is proposed by [16] and [32] and has been shown to be very effective for the aforementioned scenario. In [32], the authors demonstrate that the sequential strong rules are very effective in discarding inactive features especially for very small parameter values and achieve the state-of-\nthe-art performance. However, in contrast to the recursive SAFE (the sequential version of SAFE rules) and the sequential version of DPP rules, it is worthwhile to mention that the sequential strong rules may mistakenly discard active features because they are heuristic methods. Moreover, it is worthwhile to mention that, for the existing screening rules including SAFE and strong rules, the basic versions are usually special cases of their sequential versions, and the same applies to our DPP and EDPP rules. For the DOME rule [36, 35], it is unclear whether its sequential version exists.\nThe rest of this paper is organized as follows. We present the family of DPP screening rules, i.e., DPP and EDPP, in detail for the Lasso problem in Section 2. Section 3 extends the idea of DPP screening rules to identify inactive groups in group Lasso [37]. We evaluate our screening rules on synthetic and real data sets from many different applications. In Section 4, the experimental results demonstrate that our rules are more effective in discarding inactive features than existing state-of-the-art screening rules. We show that the efficiency of the solver can be improved by several orders of magnitude with the enhanced DPP rules, especially for the high-dimensional data sets (notice that, the screening methods can be integrated with any existing solvers for the Lasso problem). Some concluding remarks are given in Section 5."}, {"heading": "2 Screening Rules for Lasso via Dual Polytope Projections", "text": "In this section, we present the details of the proposed DPP and EDPP screening rules for the Lasso problem. We first review some basics of the dual problem of Lasso including its geometric properties in Section 2.1; we also briefly discuss some basic guidelines for developing safe screening rules for Lasso. Based on the geometric properties discussed in Section 2.1, we then develop the basic DPP screening rule in Section 2.2. As a straightforward extension in dealing with the model selection problems, we also develop the sequential version of DPP rules. In Section 2.3, by exploiting more geometric properties of the dual problem of Lasso, we further improve the DPP rules by developing the so called enhanced DPP (EDPP) rules. The EDPP screening rules significantly outperform DPP rules in identifying the inactive features for the Lasso problem."}, {"heading": "2.1 Basics", "text": "Different from [36, 35], we do not assume y and all xi have unit length.The dual problem of problem (1) takes the form of (to make the paper self-contained, we provide the detailed derivation of the dual form in the appendix):\nsup \u03b8\n{ 1\n2 \u2016y\u201622 \u2212\n\u03bb2\n2 \u2225\u2225\u2225\u03b8 \u2212 y \u03bb \u2225\u2225\u22252 2 : |xTi \u03b8| \u2264 1, i = 1, 2, . . . , p } , (2)\nwhere \u03b8 is the dual variable. For notational convenience, let the optimal solution of problem (2) be \u03b8\u2217(\u03bb) [recall that the optimal solution of problem (1) with parameter \u03bb is denoted by \u03b2\u2217(\u03bb)]. Then, the KKT conditions are given by:\ny = X\u03b2\u2217(\u03bb) + \u03bb\u03b8\u2217(\u03bb), (3)\nxTi \u03b8 \u2217(\u03bb) \u2208\n{ sign([\u03b2\u2217(\u03bb)]i), if [\u03b2\n\u2217(\u03bb)]i 6= 0, [\u22121, 1], if [\u03b2\u2217(\u03bb)]i = 0, i = 1, . . . , p, (4)\nwhere [\u00b7]k denotes the kth component. In view of the KKT condition in (4), we have\n|xTi (\u03b8\u2217(\u03bb))T | < 1\u21d2 [\u03b2\u2217(\u03bb)]i = 0\u21d2 xi is an inactive feature. (R1)\nIn other words, we can potentially make use of (R1) to identify the inactive features for the Lasso problem. However, since \u03b8\u2217(\u03bb) is generally unknown, we can not directly apply (R1) to identify the inactive features. Inspired by the SAFE rules [16], we can first estimate a region \u0398 which contains \u03b8\u2217(\u03bb\u2032\u2032). Then, (R1) can be relaxed as follows:\nsup \u03b8\u2208\u0398 |xTi \u03b8| < 1\u21d2 [\u03b2\u2217(\u03bb)]i = 0\u21d2 xi is an inactive feature. (R1\u2019)\nClearly, as long as we can find a region \u0398 which contains \u03b8\u2217(\u03bb), (R1\u2019) will lead to a screening rule to detect the inactive features for the Lasso problem. Moreover, in view of (R1) and (R1\u2019), we can see that the smaller the region \u0398 is, the more accurate the estimation of \u03b8\u2217(\u03bb) is. As a result, more inactive features can be identified by the resulting screening rules.\nGeometric Interpretations of the Dual Problem By a closer look at the dual problem (2), we can observe that the dual optimal solution is the feasible point which is closest to y/\u03bb. For notational convenience, let the feasible set of problem (2) be F . Clearly, F is the intersection of 2p closed half-spaces, and thus a closed and convex polytope. (Notice that, F is also nonempty since 0 \u2208 F .) In other words, \u03b8\u2217(\u03bb) is the projection of y/\u03bb onto the polytope F . Mathematically, for an arbitrary vector w and a convex set C in a Hilbert space H, let us define the projection operator as\nPC(w) = argmin u\u2208C\n\u2016u\u2212w\u20162. (5)\nThen, the dual optimal solution \u03b8\u2217(\u03bb) can be expressed by\n\u03b8\u2217(\u03bb) = PF (y/\u03bb) = argmin \u03b8\u2208F \u2225\u2225\u2225\u03b8 \u2212 y \u03bb \u2225\u2225\u2225 2 . (6)\nIndeed, the nice property of problem (2) illustrated by Eq. (6) leads to many interesting results. For example, it is easy to see that y/\u03bb would be an interior point of F when \u03bb is large enough. If this is the case, we immediately have the following assertions: 1) y/\u03bb is an interior point of F implies that none of the constraints of problem (2) would be active on y/\u03bb, i.e., |xTi (y/(\u03bb)|) < 1 for all i = 1, . . . , p; 2) \u03b8\u2217(\u03bb) is an interior point of F as well since \u03b8\u2217(\u03bb) = PF (y/\u03bb) = y/\u03bb by Eq. (6) and the fact y/\u03bb \u2208 F . Combining the results in 1) and 2), it is easy to see that |xTi \u03b8\u2217(\u03bb)| < 1 for all i = 1, . . . , p. By (R1), we can conclude that \u03b2\u2217(\u03bb) = 0, under the assumption that \u03bb is large enough.\nThe above analysis may naturally lead to a question: does there exist a specific parameter value \u03bbmax such that the optimal solution of problem (1) is 0 whenever \u03bb > \u03bbmax? The answer is affirmative. Indeed, let us define\n\u03bbmax = max i |xTi y|. (7)\nIt is well known [32] that \u03bbmax defined by Eq. (7) is the smallest parameter such that problem (1) has a trivial solution, i.e.,\n\u03b2\u2217(\u03bb) = 0, \u2200 \u03bb \u2208 [\u03bbmax,\u221e). (8)\nCombining the results in (8) and Eq. (3), we immediately have\n\u03b8\u2217(\u03bb) = y\n\u03bb , \u2200 \u03bb \u2208 [\u03bbmax,\u221e). (9)\nTherefore, through out the rest of this paper, we will focus on the cases with \u03bb \u2208 (0, \u03bbmax).\nIn the subsequent sections, we will follow (R1\u2019) to develop our screening rules. More specifically, the derivation of the proposed screening rules can be divided into the following three steps:\nStep 1. We first estimate a region \u0398 which contains the dual optimal solution \u03b8\u2217(\u03bb).\nStep 2. We solve the maximization problem in (R1\u2019), i.e., sup\u03b8\u2208\u0398 |xTi \u03b8|.\nStep 3. By plugging in the upper bound we find in Step 2, it is straightforward to develop the screening rule based on (R1\u2019).\nThe geometric property of the dual problem illustrated by Eq. (6) serves as a fundamentally important role in developing our DPP and EDPP screening rules."}, {"heading": "2.2 Fundamental Screening Rules via Dual Polytope Projections (DPP)", "text": "In this Section, we propose the so called DPP screening rules for discarding the inactive features for Lasso. As the name indicates, the idea of DPP heavily relies on the properties of projection operators, e.g., the nonexpansiveness [5]. We will follow the three steps stated in Section 2.1 to develop the DPP screening rules.\nFirst, we need to find a region \u0398 which contains the dual optimal solution \u03b8\u2217(\u03bb). Indeed, the result in (9) provides us an important clue. That is, we may be able to estimate a possible region for \u03b8\u2217(\u03bb) in terms of a known \u03b8\u2217(\u03bb0) with \u03bb < \u03bb0. Notice that, we can always set \u03bb0 = \u03bbmax and make use of the fact that \u03b8\u2217(\u03bbmax) = y/\u03bbmax implied by (9). Another key ingredient comes from Eq. (6), i.e., the dual optimal solution \u03b8\u2217(\u03bb) is the projection of y/\u03bb onto the feasible set F , which is nonempty closed and convex. A nice property of the projection operators defined in a Hilbert space with respect to a nonempty closed and convex set is the so called nonexpansiveness. For convenience, we restate the definition of nonexpansiveness in the following theorem.\nTheorem 1. Let C be a nonempty closed convex subset of a Hilbert space H. Then the projection operator defined in Eq. (5) is continuous and nonexpansive, i.e.,\n\u2016PC(w2)\u2212 PC(w1)\u20162 \u2264 \u2016w2 \u2212w1\u20162, \u2200w2,w1 \u2208 H. (10)\nIn view of Eq. (6), a direct application of Theorem 1 leads to the following result:\nTheorem 2. For the Lasso problem, let \u03bb, \u03bb0 > 0 be two regularization parameters. Then, \u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u20162 \u2264 \u2223\u2223\u2223\u2223 1\u03bb \u2212 1\u03bb0 \u2223\u2223\u2223\u2223 \u2016y\u20162. (11) For notational convenience, let a ball centered at c with radius \u03c1 be denoted by B(c, \u03c1). Theorem 2 actually implies that the dual optimal solution must be inside a ball centered at \u03b8\u2217(\u03bb0) with radius |1/\u03bb\u2212 1/\u03bb0| \u2016y\u20162, i.e.,\n\u03b8\u2217(\u03bb) \u2208 B ( \u03b8\u2217(\u03bb0), \u2223\u2223\u2223\u2223 1\u03bb \u2212 1\u03bb0 \u2223\u2223\u2223\u2223 \u2016y\u20162) . (12)\nWe thus complete the first step for developing DPP. Because it is easy to find the upper bound of a linear functional over a ball, we combine the remaining two steps as follows.\nTheorem 3. For the Lasso problem, assume we are given the solution of its dual problem \u03b8\u2217(\u03bb0) for a specific \u03bb0. Let \u03bb be a positive value different from \u03bb0. Then [\u03b2\n\u2217(\u03bb)]i = 0 if\u2223\u2223xTi \u03b8\u2217(\u03bb)\u2223\u2223 < 1\u2212 \u2016xi\u20162\u2016y\u20162 \u2223\u2223\u2223\u2223 1\u03bb \u2212 1\u03bb0 \u2223\u2223\u2223\u2223 . (13)\nProof. The dual optimal solution \u03b8\u2217(\u03bb) is estimated to be inside the ball given by Eq. (12). To simplify notations, let c = \u03b8\u2217(\u03bb0) and \u03c1 = |1/\u03bb\u2212 1/\u03bb0| \u2016y\u20162. To develop a screening rule based on (R1\u2019), we need to solve the optimization problem: sup\u03b8\u2208B(c,\u03c1) |xTi \u03b8|.\nIndeed, for any \u03b8 \u2208 B(c, \u03c1), it can be expressed by:\n\u03b8 = \u03b8\u2217(\u03bb0) + v, \u2016v\u20162 \u2264 \u03c1.\nTherefore, the optimization problem can be easily solved as follows:\nsup \u03b8\u2208B(c,\u03c1) \u2223\u2223xTi \u03b8\u2223\u2223 = sup \u2016v\u20162\u2264\u03c1 \u2223\u2223xTi (\u03b8\u2217(\u03bb0) + v)\u2223\u2223 = \u2223\u2223xTi \u03b8\u2217(\u03bb0)\u2223\u2223+ \u03c1\u2016xi\u20162. (14) By plugging the upper bound in Eq. (14) to (R1\u2019), we obtain the statement in Theorem 3, which completes the proof.\nTheorem 3 implies that we can develop applicable screening rules for Lasso as long as the dual optimal solution \u03b8\u2217(\u00b7) is known for a certain parameter value \u03bb0. By simply setting \u03bb0 = \u03bbmax and noting that \u03b8\u2217(\u03bbmax) = y/\u03bbmax [please refer to Eq. (9)], Theorem 3 immediately leads to the following result.\nCorollary 4. Basic DPP: For the Lasso problem (1), let \u03bbmax = maxi |xTi y|. If \u03bb \u2265 \u03bbmax, then [\u03b2\u2217]i = 0, \u2200i \u2208 I. Otherwise, [\u03b2\u2217(\u03bb)]i = 0 if\u2223\u2223\u2223\u2223xTi y\u03bbmax \u2223\u2223\u2223\u2223 < 1\u2212 ( 1\u03bb \u2212 1\u03bbmax ) \u2016xi\u20162\u2016y\u20162.\nRemark 1. Notice that, DPP is not the same as ST1 [36] and SAFE [16], which discards the ith feature if\n|xTi y| < \u03bb\u2212 \u2016xi\u20162\u2016y\u20162 \u03bbmax \u2212 \u03bb \u03bbmax . (15)\nFrom the perspective of the sphere test, the radius of ST1/SAFE and DPP are the same. But the centers of ST1 and DPP are y/\u03bb and y/\u03bbmax respectively, which leads to different formulas, i.e., Eq. (15) and Corollary 4.\nIn real applications, the optimal parameter value of \u03bb is generally unknown and needs to be estimated. To determine an appropriate value of \u03bb, commonly used approaches such as cross validation and stability selection involve solving the Lasso problem over a grid of tuning parameters \u03bb1 > \u03bb2 > . . . > \u03bbK, which is very time consuming. Motivated by the ideas of [32] and [16], we develop a sequential version of DPP rules. We first apply the DPP screening rule in Corollary 4\nto discard inactive features for the Lasso problem (1) with parameter being \u03bb1. After solving the reduced optimization problem for \u03bb1, we obtain the exact solution \u03b2\n\u2217(\u03bb1). Hence by Eq. (3), we can find \u03b8\u2217(\u03bb1). According to Theorem 3, once we know the optimal dual solution \u03b8\n\u2217(\u03bb1), we can construct a new screening rule by setting \u03bb0 = \u03bb1 to identify inactive features for problem (1) with parameter being \u03bb2. By repeating the above process, we obtain the sequential version of the DPP rule as in the following corollary.\nCorollary 5. Sequential DPP: For the Lasso problem (1), suppose we are given a sequence of parameter values \u03bbmax = \u03bb0 > \u03bb1 > . . . > \u03bbm. Then for any integer 0 \u2264 k < m, we have [\u03b2\u2217(\u03bbk+1)]i = 0 if \u03b2\n\u2217(\u03bbk) is known and the following holds:\u2223\u2223\u2223\u2223xTi y \u2212X\u03b2\u2217(\u03bbk)\u03bbk \u2223\u2223\u2223\u2223 < 1\u2212 ( 1\u03bbk+1 \u2212 1\u03bbk ) \u2016xi\u20162\u2016y\u20162.\nRemark 2. From Corollaries 4 and 5, we can see that both of the DPP and sequential DPP rules discard the inactive features for the Lasso problem with a smaller parameter value by assuming a known dual optimal solution at a larger parameter value. This is in fact a standard way to construct screening rules for Lasso [32, 16, 36, 35].\nRemark 3. For illustration purpose, we present both the basic and sequential version of the DPP screening rules. However, it is easy to see that the basic DPP rule can be easily derived from its sequential version by simply setting \u03bbk = \u03bbmax and \u03bbk+1 = \u03bb. Therefore, in this paper, we will focus on the development and evaluation of the sequential version of the proposed screening rules. To avoid any confusions, DPP and EDPP all refer to the corresponding sequential versions."}, {"heading": "2.3 Enhanced DPP Rules for Lasso", "text": "In this section, we further improve the DPP rules presented in Section 2.2 by a more careful analysis of the projection operators. Indeed, from the three steps by which we develop the DPP rules, we can see that the first step is a key. In other words, the estimation of the dual optimal solution serves as a fundamentally important role in developing the DPP rules. Moreover, (R1\u2019) implies that the more accurate the estimation is, the more effective the resulting screening rule is in discarding the inactive features. The estimation of the dual optimal solution in DPP rules is in fact a direct consequence of the nonexpansiveness of the projection operators. Therefore, in order to improve the performance of the DPP rules in discarding the inactive features, we propose two different approaches to find more accurate estimations of the dual optimal solution. These two approaches are presented in detail in Sections 2.3.1 and 2.3.2 respectively. By combining the ideas of these two approaches, we can further improve the estimation of the dual optimal solution. Based on this estimation, we develop the enhanced DPP rules (EDPP) in Section 2.3.3. Again, we will follow the three steps in Section 2.1 to develop the proposed screening rules."}, {"heading": "2.3.1 Improving the DPP rules via Projections of Rays", "text": "In the DPP screening rules, the dual optimal solution \u03b8\u2217(\u03bb) is estimated to be inside the ball B (\u03b8\u2217(\u03bb0), |1/\u03bb\u2212 1/\u03bb0|\u2016y\u20162) with \u03b8\u2217(\u03bb0) given. In this section, we show that \u03b8\u2217(\u03bb) lies inside a ball centered at \u03b8\u2217(\u03bb0) with a smaller radius.\nIndeed, it is well known that the projection of an arbitrary point onto a nonempty closed convex set C in a Hilbert space H always exists and is unique [3]. However, the converse is not true, i.e.,\nthere may exist w1,w2 \u2208 H such that w1 6= w2 and PC(w1) = PC(w2). In fact, it is known that the following result holds:\nLemma 6. [3] Let C be a nonempty closed convex subset of a Hilbert space H. For a point w \u2208 H, let w(t) = PC(w) + t(w \u2212 PC(w)). Then, the projection of the point w(t) is PC(w) for all t \u2265 0, i.e.,\nPC(w(t)) = PC(w),\u2200t \u2265 0. (16)\nClearly, when w 6= PC(w), i.e., w /\u2208 C, w(t) with t \u2265 0 is the ray starting from PC(w) and pointing in the same direction as w \u2212 PC(w). By Lemma 6, we know that the projection of the ray w(t) with t \u2265 0 onto the set C is a single point PC(w). [When w = PC(w), i.e., w \u2208 C, w(t) with t \u2265 0 becomes a single point and the statement in Lemma 6 is trivial.]\nBy making use of Lemma 6 and the nonexpansiveness of the projection operators, we can improve the estimation of the dual optimal solution in DPP [please refer to Theorem 2 and Eq. (12)]. More specifically, we have the following result:\nTheorem 7. For the Lasso problem, suppose the dual optimal solution \u03b8\u2217(\u00b7) at \u03bb0 \u2208 (0, \u03bbmax] is known. For any \u03bb \u2208 (0, \u03bb0], let us define\nv1(\u03bb0) =\n{ y \u03bb0 \u2212 \u03b8\u2217(\u03bb0), if \u03bb0 \u2208 (0, \u03bbmax),\nsign(xT\u2217 y)x\u2217, if \u03bb0 = \u03bbmax, where x\u2217 = argmaxxi |x T i y|, (17)\nv2(\u03bb, \u03bb0) = y \u03bb \u2212 \u03b8\u2217(\u03bb0), (18)\nv\u22a52 (\u03bb, \u03bb0) = v2(\u03bb, \u03bb0)\u2212 \u3008v1(\u03bb0),v2(\u03bb, \u03bb0)\u3009 \u2016v1(\u03bb0)\u201622 v1(\u03bb0). (19)\nThen, the dual optimal solution \u03b8\u2217(\u03bb) can be estimated as follows:\n\u03b8\u2217(\u03bb) \u2208 B ( \u03b8\u2217(\u03bb0), \u2016v\u22a52 (\u03bb, \u03bb0)\u20162 ) \u2286 B ( \u03b8\u2217(\u03bb0), \u2223\u2223\u2223\u2223 1\u03bb \u2212 1\u03bb0 \u2223\u2223\u2223\u2223 \u2016y\u20162) . (20)\nProof. By making use of Lemma 6, we present the proof of the statement for the cases with \u03bb0 \u2208 (0, \u03bbmax). We postpone the proof of the statement for the case with \u03bb0 = \u03bbmax after we introduce more general technical results.\nIn view of the assumption \u03bb0 \u2208 (0, \u03bbmax), it is easy to see that\ny \u03bb0 /\u2208 F \u21d2 y \u03bb0 6= PF\n( y\n\u03bb0\n) = \u03b8\u2217(\u03bb0)\u21d2 y\n\u03bb0 \u2212 \u03b8\u2217(\u03bb0) 6= 0. (21)\nFor each \u03bb0 \u2208 (0, \u03bbmax), let us define\n\u03b8\u03bb0(t) = \u03b8 \u2217(\u03bb0) + tv1(\u03bb0) = \u03b8 \u2217(\u03bb0) + t\n( y\n\u03bb0 \u2212 \u03b8\u2217(\u03bb0)\n) , t \u2265 0. (22)\nBy the result in (21), we can see that \u03b8\u03bb0(\u00b7) defined by Eq. (22) is a ray which starts at \u03b8\u2217(\u03bb0) and points in the same direction as y/\u03bb0 \u2212 \u03b8\u2217(\u03bb0). In view of Eq. (6), a direct application of Lemma 6 leads to that:\nPF (\u03b8\u03bb0(t)) = \u03b8 \u2217(\u03bb0), \u2200 t \u2265 0. (23)\nBy applying Theorem 1 again, we have \u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u20162 = \u2225\u2225\u2225PF (y\n\u03bb\n) \u2212 PF (\u03b8\u03bb0(t)) \u2225\u2225\u2225 2\n(24) \u2264 \u2225\u2225\u2225y \u03bb \u2212 \u03b8\u03bb0(t) \u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225t( y\u03bb0 \u2212 \u03b8\u2217(\u03bb0) ) \u2212 (y \u03bb \u2212 \u03b8\u2217(\u03bb0) )\u2225\u2225\u2225\u2225 2 = \u2016tv1(\u03bb0)\u2212 v2(\u03bb, \u03bb0)\u20162, \u2200 t \u2265 0.\nBecause the inequality in (24) holds for all t \u2265 0, it is easy to see that\n\u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u20162 \u2264 min t\u22650 \u2016tv1(\u03bb0)\u2212 v2(\u03bb, \u03bb0)\u20162 (25)\n= { \u2016v2(\u03bb, \u03bb0)\u20162, if \u3008v1(\u03bb0),v2(\u03bb, \u03bb0)\u3009 < 0,\u2225\u2225v\u22a52 (\u03bb, \u03bb0)\u2225\u22252 , otherwise.\nThe inequality in (25) implies that, to prove the first half of the statement, i.e.,\n\u03b8\u2217(\u03bb) \u2208 B(\u03b8\u2217(\u03bb0), \u2016v\u22a52 (\u03bb, \u03bb0)\u20162),\nwe only need to show that \u3008v1(\u03bb0),v2(\u03bb, \u03bb0)\u3009 \u2265 0. Indeed, it is easy to see that 0 \u2208 F . Therefore, in view of Eq. (23), the distance between \u03b8\u03bb0(t) and \u03b8\u2217(\u03bb0) must be shorter than the one between \u03b8\u03bb0(t) and 0 for all t \u2265 0, i.e.,\n\u2016\u03b8\u03bb0(t)\u2212 \u03b8\u2217(\u03bb0)\u201622 \u2264 \u2016\u03b8\u03bb0(t)\u2212 0\u201622 (26) \u21d2 0 \u2264 \u2016\u03b8\u2217(\u03bb0)\u201622 + 2t (\u2329 \u03b8\u2217(\u03bb0), y\n\u03bb0\n\u232a \u2212 \u2016\u03b8\u2217(\u03bb0)\u201622 ) , \u2200 t \u2265 0.\nSince the inequality in (26) holds for all t \u2265 0, we can conclude that:\u2329 \u03b8\u2217(\u03bb0), y\n\u03bb0\n\u232a \u2212 \u2016\u03b8\u2217(\u03bb0)\u201622 \u2265 0\u21d2\n\u2016y\u20162 \u03bb0 \u2265 \u2016\u03b8\u2217(\u03bb0)\u20162. (27)\nTherefore, we can see that: \u3008v1(\u03bb0),v2(\u03bb, \u03bb0)\u3009 = \u2329 y\n\u03bb0 \u2212 \u03b8\u2217(\u03bb0),\ny \u03bb \u2212 y \u03bb0 + y \u03bb0 \u2212 \u03b8\u2217(\u03bb0)\n\u232a (28)\n\u2265 ( 1\n\u03bb \u2212 1 \u03bb0\n)\u2329 y\n\u03bb0 \u2212 \u03b8\u2217(\u03bb0),y \u232a = ( 1\n\u03bb \u2212 1 \u03bb0 )( \u2016y\u201622 \u03bb0 \u2212 \u3008\u03b8\u2217(\u03bb0),y\u3009 ) \u2265 ( 1\n\u03bb \u2212 1 \u03bb0 )( \u2016y\u201622 \u03bb0 \u2212 \u2016\u03b8\u2217(\u03bb0)\u20162\u2016y\u20162 ) \u2265 0.\nThe last inequality in (28) is due to the result in (27). Clearly, in view of (25) and (28), we can see that the first half of the statement holds, i.e., \u03b8\u2217(\u03bb) \u2208 B(\u03b8\u2217(\u03bb0), \u2016v\u22a52 (\u03bb, \u03bb0)\u20162). The second half of the statement, i.e., B(\u03b8\u2217(\u03bb0), \u2016v\u22a52 (\u03bb, \u03bb0)\u20162) \u2286 B(\u03b8\u2217(\u03bb0), |1/\u03bb \u2212 1/\u03bb0|\u2016y\u20162), can be easily obtained by noting that the inequality in (24) reduces to the one in (12) when t = 1. This completes the proof of the statement with \u03bb0 \u2208 (0, \u03bbmax).\nBefore we present the proof of Theorem 7 for the case with \u03bb0 = \u03bbmax, let us briefly review some technical results from convex analysis first.\nDefinition 8. [28] Let C be a nonempty closed convex subset of a Hilbert space H and w \u2208 C. The set\nNC(w) := {v : \u3008v,u\u2212w\u3009 \u2264 0, \u2200u \u2208 C} (29)\nis called the normal cone to C at w.\nIn terms of the normal cones, the following theorem provides an elegant and useful characterization of the projections onto nonempty closed convex subsets of a Hilbert space.\nTheorem 9. [3] Let C be a nonempty closed convex subset of a Hilbert space H. Then, for every w \u2208 H and w0 \u2208 C, w0 is the projection of w onto C if and only if w \u2212w0 \u2208 NC(w0), i.e.,\nw0 = PC(w)\u21d4 \u3008w \u2212w0,u\u2212w0\u3009 \u2264 0, \u2200u \u2208 C. (30)\nIn view of the proof of Theorem 7, we can see that Eq. (23) is a key step. When \u03bb0 = \u03bbmax, similar to Eq. (22), let us define\n\u03b8\u03bbmax(t) = \u03b8 \u2217(\u03bbmax) + tv1(\u03bbmax), \u2200 t \u2265 0. (31)\nBy Theorem 9, the following lemma shows that Eq. (23) also holds for \u03bb0 = \u03bbmax.\nLemma 10. For the Lasso problem, let v1(\u00b7) and \u03b8\u03bbmax(\u00b7) be given by Eq. (17) and Eq. (31), then the following result holds:\nPF (\u03b8\u03bbmax(t)) = \u03b8 \u2217(\u03bbmax), \u2200 t \u2265 0. (32)\nProof. To prove the statement, Theorem 9 implies that we only need to show:\n\u3008v1(\u03bbmax), \u03b8 \u2212 \u03b8\u2217(\u03bbmax)\u3009 \u2264 0, \u2200 \u03b8 \u2208 F. (33)\nRecall that v1(\u03bbmax) = sign(x T \u2217 y)x\u2217, x\u2217 = argmaxxi |x T i y| [Eq. (17)], and \u03b8\u2217(\u03bbmax) = y/\u03bbmax [Eq. (9)]. It is easy to see that\n\u3008v1(\u03bbmax), \u03b8\u2217(\u03bbmax)\u3009 = \u2329 sign(xT\u2217 y)x\u2217, y\n\u03bbmax \u232a = |xT\u2217 y| \u03bbmax = 1. (34)\nMoreover, assume \u03b8 is an arbitrary point of F . Then, we have |\u3008x\u2217, \u03b8\u3009| \u2264 1, and thus\n\u3008v1(\u03bbmax), \u03b8\u3009 = \u3008sign(xT\u2217 y)x\u2217, \u03b8\u3009 \u2264 |\u3008x\u2217, \u03b8\u3009| \u2264 1. (35)\nTherefore, the inequality in (33) easily follows by combing the results in (34) and (35), which completes the proof.\nWe are now ready to give the proof of Theorem 7 for the case with \u03bb0 = \u03bbmax.\nProof. In view of Theorem 1 and Lemma 10, we have \u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bbmax)\u20162 = \u2225\u2225\u2225PF (y\n\u03bb\n) \u2212 PF (\u03b8\u03bbmax(t)) \u2225\u2225\u2225 2\n(36) \u2264 \u2225\u2225\u2225y \u03bb \u2212 \u03b8\u03bbmax(t) \u2225\u2225\u2225 2 = \u2225\u2225\u2225tv1(\u03bbmax)\u2212 (y \u03bb \u2212 \u03b8\u2217(\u03bbmax) )\u2225\u2225\u2225 2 = \u2016tv1(\u03bbmax)\u2212 v2(\u03bb, \u03bbmax)\u20162, \u2200 t \u2265 0.\nBecause the inequality in (36) holds for all t \u2265 0, we can see that\n\u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bbmax)\u20162 \u2264 min t\u22650 \u2016tv1(\u03bbmax)\u2212 v2(\u03bb, \u03bbmax)\u20162 (37)\n= { \u2016v2(\u03bb, \u03bbmax)\u20162, if \u3008v1(\u03bbmax),v2(\u03bb, \u03bbmax)\u3009 < 0,\u2225\u2225v\u22a52 (\u03bb, \u03bbmax)\u2225\u22252 , otherwise.\nClearly, we only need to show that \u3008v1(\u03bbmax),v2(\u03bb, \u03bbmax)\u3009 \u2265 0. Indeed, Lemma 10 implies that v1(\u03bbmax) \u2208 NF (\u03b8\u2217(\u03bbmax)) [please refer to the inequality in (33)]. By noting that 0 \u2208 F , we have\u2329 v1(\u03bbmax), 0\u2212 y\n\u03bbmax\n\u232a \u2264 0\u21d2 \u3008v1(\u03bbmax),y\u3009 \u2265 0. (38)\nMoreover, because y/\u03bbmax = \u03b8 \u2217(\u03bbmax), it is easy to see that\n\u3008v1(\u03bbmax),v2(\u03bb, \u03bbmax)\u3009 = \u2329 v1(\u03bbmax), y\n\u03bb \u2212 y \u03bbmax\n\u232a (39)\n=\n( 1\n\u03bb \u2212 1 \u03bbmax\n) \u3008v1(\u03bbmax),y\u3009 \u2265 0.\nTherefore, in view of (37) and (39), we can see that the first half of the statement holds, i.e., \u03b8\u2217(\u03bb) \u2208 B(\u03b8\u2217(\u03bbmax), \u2016v\u22a52 (\u03bb, \u03bbmax)\u20162). The second half of the statement, i.e.,\nB(\u03b8\u2217(\u03bbmax), \u2016v\u22a52 (\u03bb, \u03bbmax)\u20162) \u2286 B(\u03b8\u2217(\u03bbmax), |1/\u03bb\u2212 1/\u03bbmax|\u2016y\u20162),\ncan be easily obtained by noting that the inequality in (37) reduces to the one in (12) when t = 0. This completes the proof of the statement with \u03bb0 = \u03bbmax. Thus, the proof of Theorem 7 is completed.\nTheorem 7 in fact provides a more accurate estimation of the dual optimal solution than the one in DPP, i.e., \u03b8\u2217(\u03bb) lies inside a ball centered at \u03b8\u2217(\u03bb0) with a radius \u2016v\u22a52 (\u03bb, \u03bb0)\u20162. Based on this improved estimation and (R1\u2019), we can develop the following screening rule to discard the inactive features for Lasso.\nTheorem 11. For the Lasso problem, assume the dual optimal solution \u03b8\u2217(\u00b7) at \u03bb0 \u2208 (0, \u03bbmax] is known. Then, for each \u03bb \u2208 (0, \u03bb0), we have [\u03b2\u2217(\u03bb)]i = 0 if\n|xTi \u03b8\u2217(\u03bb0)| < 1\u2212 \u2016v\u22a52 (\u03bb, \u03bb0)\u20162\u2016xi\u20162.\nWe omit the proof of Theorem 11 since it is very similar to the one of Theorem 3. By Theorem 11, we can easily develop the following sequential screening rule.\nImprovement 1: For the Lasso problem (1), suppose we are given a sequence of parameter values \u03bbmax = \u03bb0 > \u03bb1 > . . . > \u03bbK. Then for any integer 0 \u2264 k < K, we have [\u03b2\u2217(\u03bbk+1)]i = 0 if \u03b2\u2217(\u03bbk) is known and the following holds:\u2223\u2223\u2223\u2223xTi y \u2212X\u03b2\u2217(\u03bbk)\u03bbk\n\u2223\u2223\u2223\u2223 < 1\u2212 \u2016v\u22a52 (\u03bbk+1, \u03bbk)\u20162\u2016xi\u20162. The screening rule in Improvement 1 is developed based on (R1\u2019) and the estimation of the dual optimal solution in Theorem 7, which is more accurate than the one in DPP. Therefore, in view of (R1\u2019), the screening rule in Improvement 1 are more effective in discarding the inactive features than the DPP rule."}, {"heading": "2.3.2 Improving the DPP rules via Firmly Nonexpansiveness", "text": "In Section 2.3.1, we improve the estimation of the dual optimal solution in DPP by making use of the projections of properly chosen rays. (R1\u2019) implies that the resulting screening rule stated in Improvement 1 is more effective in discarding the inactive features than DPP. In this Section, we present another approach to improve the estimation of the dual optimal solution in DPP by making use of the so called firmly nonexpansiveness of the projections onto nonempty closed convex subset of a Hilbert space.\nTheorem 12. [3] Let C be a nonempty closed convex subset of a Hilbert space H. Then the projection operator defined in Eq. (5) is continuous and firmly nonexpansive. In other words, for any w1,w2 \u2208 H, we have\n\u2016PC(w1)\u2212 PC(w2)\u201622 + \u2016(Id\u2212 PC)(w1)\u2212 (Id\u2212 PC)(w2)\u201622 \u2264 \u2016w1 \u2212w2\u201622, (40)\nwhere Id is the identity operator.\nIn view of the inequalities in (40) and (10), it is easy to see that firmly nonexpansiveness implies nonexpansiveness. But the converse is not true. Therefore, firmly nonexpansiveness of the projection operators is a stronger property than the nonexpansiveness. A direct application of Theorem 12 leads to the following result.\nTheorem 13. For the Lasso problem, let \u03bb, \u03bb0 > 0 be two parameter values. Then \u03b8\u2217(\u03bb) \u2208 B ( \u03b8\u2217(\u03bb0) + 1\n2\n( 1\n\u03bb \u2212 1 \u03bb0\n) y, 1\n2 \u2223\u2223\u2223\u2223 1\u03bb \u2212 1\u03bb0 \u2223\u2223\u2223\u2223 \u2016y\u20162) \u2282 B(\u03b8\u2217(\u03bb0), \u2223\u2223\u2223\u2223 1\u03bb \u2212 1\u03bb0 \u2223\u2223\u2223\u2223 \u2016y\u20162) . (41) Proof. In view of Eq. (6) and the firmly nonexpansiveness in (40), we have\n\u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u201622 + \u2225\u2225\u2225\u2225(y\u03bb \u2212 \u03b8\u2217(\u03bb))\u2212 ( y \u03bb0 \u2212 \u03b8\u2217(\u03bb0) )\u2225\u2225\u2225\u22252 2 \u2264 \u2225\u2225\u2225\u2225y\u03bb \u2212 y\u03bb0 \u2225\u2225\u2225\u22252 2\n(42) \u21d4 \u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u201622 \u2264 \u2329 \u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0), y\n\u03bb \u2212 y \u03bb0 \u232a \u21d4 \u2225\u2225\u2225\u2225\u03b8\u2217(\u03bb)\u2212 (\u03b8\u2217(\u03bb0) + 12 ( 1 \u03bb \u2212 1 \u03bb0 ) y )\u2225\u2225\u2225\u2225 2 \u2264 1 2 \u2223\u2223\u2223\u2223 1\u03bb \u2212 1\u03bb0 \u2223\u2223\u2223\u2223 \u2016y\u20162,\nwhich completes the proof of the first half of the statement. The second half of the statement is trivial by noting that the first inequality in (42) (firmly nonexpansiveness) implies the inequality in (11) (nonexpansiveness) but not vice versa. Indeed, it is easy to see that the ball in the middle of (41) is inside the right one and has only a half radius.\nClearly, Theorem 13 provides a more accurate estimation of the dual optimal solution than the one in DPP, i.e., the dual optimal solution must be inside a ball which is a subset of the one in DPP and has only a half radius. Again, based on the estimation in Theorem 13 and (R1\u2019), we have the following result.\nTheorem 14. For the Lasso problem, assume the dual optimal solution \u03b8\u2217(\u00b7) at \u03bb0 \u2208 (0, \u03bbmax] is known. Then, for each \u03bb \u2208 (0, \u03bb0), we have [\u03b2\u2217(\u03bb)]i = 0 if\u2223\u2223\u2223\u2223xTi (\u03b8\u2217(\u03bb0) + 12 ( 1 \u03bb \u2212 1 \u03bb0 ) y )\u2223\u2223\u2223\u2223 < 1\u2212 12 ( 1 \u03bb \u2212 1 \u03bb0 ) \u2016y\u20162\u2016xi\u20162.\nWe omit the proof of Theorem 14 since it is very similar to the proof of Theorem 3. A direct application of Theorem 14 leads to the following sequential screening rule.\nImprovement 2: For the Lasso problem (1), suppose we are given a sequence of parameter values \u03bbmax = \u03bb0 > \u03bb1 > . . . > \u03bbK. Then for any integer 0 \u2264 k < K, we have [\u03b2\u2217(\u03bbk+1)]i = 0 if \u03b2\u2217(\u03bbk) is known and the following holds:\u2223\u2223\u2223\u2223xTi (y \u2212X\u03b2\u2217(\u03bbk)\u03bbk + 12 ( 1 \u03bbk+1 \u2212 1 \u03bbk ) y )\u2223\u2223\u2223\u2223 < 1\u2212 12 ( 1 \u03bbk+1 \u2212 1 \u03bbk ) \u2016y\u20162\u2016xi\u20162.\nBecause the screening rule in Improvement 2 is developed based on (R1\u2019) and the estimation in Theorem 13, it is easy to see that Improvement 2 is more effective in discarding the inactive features than DPP."}, {"heading": "2.3.3 The Proposed Enhanced DPP Rules", "text": "In Sections 2.3.1 and 2.3.2, we present two different approaches to improve the estimation of the dual optimal solution in DPP. In view of (R1\u2019), we can see that the resulting screening rules, i.e., Improvements 1 and 2, are more effective in discarding the inactive features than DPP. In this section, we give a more accurate estimation of the dual optimal solution than the ones in Theorems 7 and 13 by combining the aforementioned two approaches together. The resulting screening rule for Lasso is the so called enhanced DPP rule (EDPP). Again, (R1\u2019) implies that EDPP is more effective in discarding the inactive features than the screening rules in Improvements 1 and 2. We also present several experiments to demonstrate that EDPP is able to identify more inactive features than the screening rules in Improvements 1 and 2. Therefore, in the subsequent sections, we will focus on the generalizations and evaluations of EDPP.\nTo develop the EDPP rules, we still follow the three steps in Section 2.1. Indeed, by combining the two approaches proposed in Sections 2.3.1 and 2.3.2, we can further improve the estimation of the dual optimal solution in the following theorem.\nTheorem 15. For the Lasso problem, suppose the dual optimal solution \u03b8\u2217(\u00b7) at \u03bb0 \u2208 (0, \u03bbmax] is known, and \u2200 \u03bb \u2208 (0, \u03bb0], let v\u22a52 (\u03bb, \u03bb0) be given by Eq. (19). Then, we have\u2225\u2225\u2225\u2225\u03b8\u2217(\u03bb)\u2212 (\u03b8\u2217(\u03bb0) + 12v\u22a52 (\u03bb, \u03bb0) )\u2225\u2225\u2225\u2225 2 \u2264 1 2 \u2016v\u22a52 (\u03bb, \u03bb0)\u20162. (43)\nProof. Recall that \u03b8\u03bb0(t) is defined by Eq. (22) and Eq. (31). In view of (40), we have\u2225\u2225\u2225PF (y \u03bb ) \u2212 PF (\u03b8\u03bb0(t)) \u2225\u2225\u22252 2 + \u2225\u2225\u2225(Id\u2212 PF )(y \u03bb ) \u2212 (Id\u2212 PF ) (\u03b8\u03bb0(t)) \u2225\u2225\u22252 2 \u2264 \u2225\u2225\u2225y \u03bb \u2212 \u03b8\u03bb0(t) \u2225\u2225\u22252 2 . (44)\nBy expanding the second term on the left hand side of (44) and rearranging the terms, we obtain the following equivalent form:\u2225\u2225\u2225PF (y\n\u03bb\n) \u2212 PF (\u03b8\u03bb0(t)) \u2225\u2225\u22252 2 \u2264 \u2329y \u03bb \u2212 \u03b8\u03bb0(t), PF (y \u03bb ) \u2212 PF (\u03b8\u03bb0(t)) \u232a . (45)\nIn view of Eq. (6), Eq. (23) and Eq. (32), the inequality in (45) can be rewritten as\n\u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u201622 \u2264 \u2329y \u03bb \u2212 \u03b8\u03bb0(t), \u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0) \u232a (46)\n= \u2329y \u03bb \u2212 \u03b8\u2217(\u03bb0)\u2212 tv1(\u03bb0), \u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0) \u232a = \u3008v2(\u03bb, \u03bb0)\u2212 tv1(\u03bb0), \u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u3009, \u2200t \u2265 0.\n[Recall that v1(\u03bb0) and v2(\u03bb, \u03bb0) are defined by Eq. (17) and Eq. (18) respectively.] Clearly, the inequality in (46) is equivalent to\u2225\u2225\u2225\u2225\u03b8\u2217(\u03bb)\u2212 (\u03b8\u2217(\u03bb0) + 12(v2(\u03bb, \u03bb0)\u2212 tv1(\u03bb0)) )\u2225\u2225\u2225\u22252 2 \u2264 1 4 \u2016v2(\u03bb, \u03bb0)\u2212 tv1(\u03bb0)\u201622, \u2200t \u2265 0. (47)\nThe statement follows easily by minimizing the right hand side of the inequality in (47), which has been done in the proof of Theorem 7.\nIndeed, Theorem 15 is equivalent to bounding \u03b8\u2217(\u03bb) in a ball as follows:\n\u03b8\u2217(\u03bb) \u2208 B ( \u03b8\u2217(\u03bb0) + 1\n2 v\u22a52 (\u03bb, \u03bb0),\n1 2 \u2016v\u22a52 (\u03bb, \u03bb0)\u20162\n) . (48)\nBased on this estimation and (R1\u2019), we immediately have the following result.\nTheorem 16. For the Lasso problem, assume the dual optimal problem \u03b8\u2217(\u00b7) at \u03bb0 \u2208 (0, \u03bbmax] is known, and \u03bb \u2208 (0, \u03bb0]. Then [\u03b2\u2217(\u03bb)]i = 0 if the following holds:\u2223\u2223\u2223\u2223xTi (\u03b8\u2217(\u03bb0) + 12v\u22a52 (\u03bb, \u03bb0)\n)\u2223\u2223\u2223\u2223 < 1\u2212 12\u2016v\u22a52 (\u03bb, \u03bb0)\u20162\u2016xi\u20162. We omit the proof of Theorem 16 since it is very similar to the one of Theorem 3. Based on\nTheorem 16, we can develop the EDPP rules as follows.\nCorollary 17. EDPP: For the Lasso problem, suppose we are given a sequence of parameter values \u03bbmax = \u03bb0 > \u03bb1 > . . . > \u03bbK. Then for any integer 0 \u2264 k < K, we have [\u03b2\u2217(\u03bbk+1)]i = 0 if \u03b2\u2217(\u03bbk) is known and the following holds:\u2223\u2223\u2223\u2223xTi (y \u2212X\u03b2\u2217(\u03bbk)\u03bbk + 12v\u22a52 (\u03bbk+1, \u03bbk)\n)\u2223\u2223\u2223\u2223 < 1\u2212 12\u2016v\u22a52 (\u03bbk+1, \u03bbk)\u20162\u2016xi\u20162. (49) It is easy to see that the ball in (48) has the smallest radius compared to the ones in Theorems 7 and 13, and thus it provides the most accurate estimation of the dual optimal solution. According to (R1\u2019), EDPP is more effective in discarding the inactive features than DPP, Improvements 1 and 2.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nDPP Improvement1 Improvement2 EDPP\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nDPP Improvement1 Improvement2 EDPP\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nDPP Improvement1 Improvement2 EDPP\nSpeedup\nSpeedup\nSpeedup\nComparisons of the Family of DPP rules We evaluate the performance of the family of DPP screening rules, i.e., DPP, Improvement 1, Improvement 2 and EDPP, on three real data sets: a) the Prostate Cancer [27]; b) the PIE face image data set [30]; c) the MNIST handwritten digit data set [21]. To measure the performance of the screening rules, we compute the following two quantities:\n1. the rejection ratio, i.e., the ratio of the number of features discarded by screening rules to the actual number of zero features in the ground truth;\n2. the speedup, i.e., the ratio of the running time of the solver with screening rules to the running time of the solver without screening.\nFor each data set, we run the solver with or without the screening rules to solve the Lasso problem along a sequence of 100 parameter values equally spaced on the \u03bb/\u03bbmax scale from 0.05 to 1.0. Fig. 1 presents the rejection ratios and speedup by the family of DPP screening rules. Table 1 reports the running time of the solver with or without the screening rules for solving the 100 Lasso problems, as well as the time for running the screening rules.\nThe Prostate Cancer Data Set The Prostate Cancer data set [27] is obtained by protein mass spectrometry. The features are indexed by time-of-flight values, which are related to the mass over charge ratios of the constituent proteins in the blood. The data set has 15154 measurements of 132 patients. 69 of the patients have prostate cancer and the rest are healthy. Therefore, the data matrix X is of size 132 \u00d7 15154, and the response vector y \u2208 {1,\u22121}132 contains the binary labels of the patients.\nThe PIE Face Image Data Set The PIE face image data set used in this experiment1 [10] contains 11554 gray face images of 68 people, taken under different poses, illumination conditions and expressions. Each of the images has 32\u00d7 32 pixels. Therefore, in each trial, we first randomly pick an image as the response y \u2208 R1024, and then use the remaining images to form the data matrix X \u2208 R1024\u00d711553. We run 100 trials and report the average performance of the screening rules.\nThe MNIST Handwritten Digit Data Set This data set contains grey images of scanned handwritten digits, including 60, 000 for training and 10, 000 for testing. The dimension of each image is 28 \u00d7 28. We first randomly select 5000 images for each digit from the training set (and in total we have 50000 images) and get a data matrix X \u2208 R784\u00d750000. Then in each trial, we randomly select an image from the testing set as the response y \u2208 R784. We run 100 trials and report the average performance of the screening rules.\nFrom Fig. 1, we can see that both Improvements 1 and 2 are able to discard more inactive features than DPP, and thus lead to a higher speedup. Compared to Improvement 2, we can also observe that Improvement 1 is more effective in discarding the inactive features. For the three data sets, the second row of Fig. 1 shows that Improvement 1 leads to about 20, 60, 70 times speedup respectively, which are much higher than the ones gained by Improvement 1 (roughly 10 times for all the three cases).\nMoreover, the EDPP rule, which combines the ideas of both Improvements 1 and 2, is even more effective in discarding the inactive features than Improvement 1. We can see that, for all of\n1http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html\nthe three data sets and most of the 100 parameter values, the rejection ratios of EDPP are very close to 100%. In other words, EDPP is able to discard almost all of the inactive features. Thus, the resulting speedup of EDPP is significantly better than the ones gained by the other three DPP rules. For the PIE and MNIST data sets, we can see that the speedup gained EDPP is about 150 and 230 times, which are two orders of magnitude. In view of Table 1, for the MNIST data set, the solver without screening needs about 2566.26 seconds to solve the 100 Lasso problems. In contrast, the solver with EDPP only needs 11.12 seconds, leading to substantial savings in the computational cost. Moreover, from the last four columns of Table 1, we can also observe that the computational cost of the family of DPP rules are very low. Compared to that of the solver without screening, the computational cost of the family of DPP rules is negligible.\nIn Section 4, we will only compare the performance of EDPP against several other state-of-theart screening rules."}, {"heading": "3 Extensions to Group Lasso", "text": "To demonstrate the flexibility of the family of DPP rules, we extend the idea of EDPP to the group Lasso problem [37] in this section. Although the Lasso and group Lasso problems are very different from each other, we will see that their dual problems share a lot of similarities. For example, both of the dual problems can be formulated as looking for projections onto nonempty closed convex subsets of a Hilbert space. Recall that, the EDPP rule for the Lasso problem is entirely based on the properties of the projection operators. Therefore, the framework of the EDPP screening rule we developed for Lasso is also applicable for the group Lasso problem. In Section 3.1, we briefly review some basics of the group Lasso problem and explore the geometric properties of its dual problem. In Section 3.2, we develop the EDPP rule for the group Lasso problem."}, {"heading": "3.1 Basics", "text": "With the group information available, the group Lasso problem takes the form of:\ninf \u03b2\u2208Rp\n1\n2 \u2225\u2225\u2225\u2225y \u2212\u2211Gg=1 Xg\u03b2g \u2225\u2225\u2225\u22252 2 + \u03bb \u2211G g=1 \u221a ng\u2016\u03b2g\u20162, (50)\nwhere Xg \u2208 RN\u00d7ng is the data matrix for the gth group and p = \u2211G\ng=1 ng. The dual problem of (50) is (see detailed derivation in the appendix):\nsup \u03b8\n{ 1\n2 \u2016y\u201622 \u2212\n\u03bb2\n2 \u2225\u2225\u2225\u03b8 \u2212 y \u03bb \u2225\u2225\u22252 2 : \u2016XTg \u03b8\u20162 \u2264 \u221a ng, g = 1, 2, . . . , G } (51)\nThe KKT conditions are given by y = \u2211G\ng=1 Xg\u03b2\n\u2217 g (\u03bb) + \u03bb\u03b8 \u2217(\u03bb), (52)\n(\u03b8\u2217(\u03bb))TXg \u2208\n{\u221a ng \u03b2\u2217g (\u03bb)\n\u2016\u03b2\u2217g (\u03bb)\u20162 , if\u03b2\u2217g (\u03bb) 6= 0,\n\u221a ngu, \u2016u\u20162 \u2264 1, if\u03b2\u2217g (\u03bb) = 0.\n(53)\nfor g = 1, 2, . . . , G. Clearly, in view of Eq. (53), we can see that\n\u2016(\u03b8\u2217(\u03bb))TXg\u20162 < \u221a ng \u21d2 \u03b2\u2217g (\u03bb) = 0 (R2)\nHowever, since \u03b8\u2217(\u03bb) is generally unknown, (R2) is not applicable to identify the inactive groups, i.e., the groups which have 0 coefficients in the solution vector, for the group Lasso problem. Therefore, similar to the Lasso problem, we can first find a region \u0398 which contains \u03b8\u2217(\u03bb), and then (R2) can be relaxed as follows:\nsup \u03b8\u2208\u0398 \u2016(\u03b8)TXg\u20162 <\n\u221a ng \u21d2 \u03b2\u2217g (\u03bb) = 0. (R2\u2032)\nTherefore, to develop screening rules for the group Lasso problem, we only need to estimate the region \u0398 which contains \u03b8\u2217(\u03bb), solve the maximization problem in (R2\u2032), and plug it into (R2\u2032). In other words, the three steps proposed in Section 2.1 can also be applied to develop screening rules for the group Lasso problem. Moreover, (R2\u2032) also implies that the smaller the region \u0398 is, the more accurate the estimation of the dual optimal solution is. As a result, the more effective the resulting screening rule is in discarding the inactive features.\nGeometric Interpretations For notational convenience, let F be the feasible set of problem (51). Similar to the case of Lasso, problem (51)implies that the dual optimal \u03b8\u2217(\u03bb) is the projection of y/\u03bb onto the feasible set F , i.e.,\n\u03b8\u2217(\u03bb) = PF (y \u03bb ) , \u2200 \u03bb > 0. (54)\nCompared to Eq. (6), the only difference in Eq. (54) is that the feasible set F is the intersection of a set of ellipsoids, and thus not a polytope. However, similar to F , F is also a nonempty closed and convex (notice that 0 is a feasible point). Therefore, we can make use of all the aforementioned properties of the projection operators, e.g., Lemmas 6 and 10, Theorems 9 and 12, to develop screening rules for the group Lasso problem.\nMoreover, similar to the case of Lasso, we also have a specific parameter value [32] for the group Lasso problem, i.e.,\n\u03bbmax = max g \u2016XTg y\u20162\u221a ng . (55)\nIndeed, \u03bbmax is the smallest parameter value such that the optimal solution of problem (50) is 0. More specifically, we have:\n\u03b2\u2217(\u03bb) = 0, \u2200 \u03bb \u2208 [\u03bbmax,\u221e). (56)\nCombining the result in (56) and Eq. (52), we immediately have\n\u03b8\u2217(\u03bb) = y\n\u03bb , \u2200 \u03bb \u2208 [\u03bbmax,\u221e). (57)\nTherefore, all through the subsequent sections, we will focus on the cases with \u03bb \u2208 (0, \u03bbmax)."}, {"heading": "3.2 Enhanced DPP rule for Group Lasso", "text": "In view of (R2\u2032), we can see that the estimation of the dual optimal solution is the key step to develop a screening rule for the group Lasso problem. Because \u03b8\u2217(\u03bb) is the projection of y/\u03bb onto the nonempty closed convex set F [please refer to Eq. (54)], we can make use of all the properties of projection operators, e.g., Lemmas 6 and 10, Theorems 9 and 12, to estimate the dual optimal solution. First, let us develop a useful technical result as follows.\nLemma 18. For the group Lasso problem, let \u03bbmax be given by Eq. (55) and\nX\u2217 := argmaxXg \u2016XTg y\u20162\u221a\nng . (58)\nSuppose the dual optimal solution \u03b8\u2217(\u00b7) is known at \u03bb0 \u2208 (0, \u03bbmax], let us define\nv1(\u03bb0) =\n{ y \u03bb0 \u2212 \u03b8\u2217(\u03bb0), if \u03bb0 \u2208 (0, \u03bbmax),\nX\u2217X T \u2217 y, if \u03bb0 = \u03bbmax.\n(59)\n\u03b8\u03bb0(t) = \u03b8 \u2217(\u03bb0) + tv1(\u03bb0), t \u2265 0. (60)\nThen, we have the following result holds\nPF (\u03b8\u03bb0(t)) = \u03b8 \u2217(\u03bb0), \u2200 t \u2265 0. (61)\nProof. Let us first consider the cases with \u03bb0 \u2208 (0, \u03bbmax). In view of the definition of \u03bbmax, it is easy to see that y/\u03bb0 /\u2208 F . Therefore, in view of Eq. (54) and Lemma 6, the statement in Eq. (61) follows immediately.\nWe next consider the case with \u03bb0 = \u03bbmax. By Theorem 9, we only need to check if\nv1(\u03bbmax) \u2208 NF (\u03b8 \u2217(\u03bbmax))\u21d4 \u2329 v1(\u03bbmax), \u03b8 \u2212 \u03b8\u2217(\u03bbmax) \u232a \u2264 0, \u2200 \u03b8 \u2208 F . (62)\nIndeed, in view of Eq. (55) and Eq. (57), we can see that \u3008v1(\u03bbmax), \u03b8\u2217(\u03bbmax)\u3009 = \u2329 X\u2217X T \u2217 y, y\n\u03bbmax \u232a = \u2016XT\u2217 y\u201622 \u03bbmax . (63)\nOn the other hand, by Eq. (55) and Eq. (58), we can see that\n\u2016XT\u2217 y\u20162 = \u03bbmax \u221a n\u2217, (64)\nwhere n\u2217 is the number of columns of X\u2217. By plugging Eq. (64) into Eq. (63), we have\n\u3008v1(\u03bbmax), \u03b8\u2217(\u03bbmax)\u3009 = \u03bbmax \u00b7 n\u2217. (65)\nMoreover, for any feasible point \u03b8 \u2208 F , we can see that\n\u2016XT\u2217 \u03b8\u20162 \u2264 \u221a n\u2217. (66)\nIn view of the result in (66) and Eq. (64), it is easy to see that\u2329 v1(\u03bbmax), \u03b8 \u232a = \u2329 X\u2217X T \u2217 y, \u03b8 \u232a = \u2329 XT\u2217 y,X T \u2217 \u03b8 \u232a \u2264 \u2016XT\u2217 y\u20162\u2016XT\u2217 \u03b8\u20162 = \u03bbmax \u00b7 n\u2217. (67)\nCombining the result in Eq. (63) and (67), it is easy to see that the inequality in (62) holds for all \u03b8 \u2208 F , which completes the proof.\nBy Lemma 18, we can accurately estimate the dual optimal solution of the group Lasso problem in the following theorem. It is easy to see that the result in Theorem 19 is very similar to the one in Theorem 15 for the Lasso problem.\nTheorem 19. For the group Lasso problem, suppose the dual optimal solution \u03b8\u2217(\u00b7) at \u03b80 \u2208 (0, \u03bbmax] is known, and v1(\u03bb0) is given by Eq. (59). For any \u03bb \u2208 (0, \u03bb0], let us define\nv2(\u03bb, \u03bb0) = y \u03bb \u2212 \u03b8\u2217(\u03bb0), (68)\nv\u22a52 (\u03bb, \u03bb0) = v2(\u03bb, \u03bb0)\u2212 \u3008v1(\u03bb0),v2(\u03bb, \u03bb0)\u3009 \u2016v1(\u03bb0)\u201622 v1(\u03bb0). (69)\nThen, the dual optimal solution \u03b8\u2217(\u03bb) can be estimated as follows:\u2225\u2225\u2225\u2225\u03b8\u2217(\u03bb)\u2212 (\u03b8\u2217(\u03bb0) + 12v\u22a52 (\u03bb, \u03bb0) )\u2225\u2225\u2225\u2225\n2\n\u2264 1 2 \u2016v\u22a52 (\u03bb, \u03bb0)\u20162. (70)\nWe omit the proof of Theorem 19 since it is exactly the same as the one of Theorem 15. Indeed, Theorem 19 is equivalent to estimating \u03b8\u2217(\u03bb) in a ball as follows:\n\u03b8\u2217(\u03bb) \u2208 B ( \u03b8\u2217(\u03bb0) + 1\n2 v\u22a52 (\u03bb, \u03bb0),\n1 2 \u2016v\u22a52 (\u03bb, \u03bb0)\u20162\n) . (71)\nBased on this estimation and (R2\u2032), we immediately have the following result.\nTheorem 20. For the group Lasso problem, assume the dual optimal solution \u03b8\u2217(\u00b7) is known at \u03bb0 \u2208 (0, \u03bbmax], and \u03bb \u2208 (0, \u03bb0]. Then \u03b2\u2217g (\u03bb) = 0 if the following holds\u2225\u2225\u2225\u2225XTg (\u03b8\u2217(\u03bb0) + 12v\u22a52 (\u03bb, \u03bb0) )\u2225\u2225\u2225\u2225 2 < \u221a ng \u2212 1 2 \u2016v\u22a52 (\u03bb, \u03bb0)\u20162\u2016Xg\u20162. (72) Proof. In view of (R2\u2032), we only need to check if\u2225\u2225XTg \u03b8\u2217(\u03bb)\u2225\u22252 < \u221ang. To simplify notations, let\no = \u03b8\u2217(\u03bb0) + 1 2 v\u22a52 (\u03bb, \u03bb0), r = 1 2 \u2016v\u22a52 (\u03bb, \u03bb0)\u20162."}, {"heading": "It is easy to see that \u2225\u2225XTg \u03b8\u2217(\u03bb)\u2225\u22252 \u2264 \u2016XTg (\u03b8\u2217(\u03bb)\u2212 o)\u20162 + \u2016XTg o\u20162 (73)", "text": "< \u2016Xg\u20162\u2016\u03b8\u2217(\u03bb)\u2212 o\u20162 + \u221a ng \u2212 r\u2016Xg\u20162\n\u2264 r\u2016Xg\u20162 + \u221a ng \u2212 r\u2016Xg\u20162 = \u221a ng,\nwhich completes the proof. The second and third inequalities in (73) are due to (72) and Theorem 19, respectively.\nIn view of Eq. (52) and Theorem 20, we can derive the EDPP rule to discard the inactive groups for the group Lasso problem as follows.\nCorollary 21. EDPP: For the group Lasso problem (50), suppose we are given a sequence of parameter values \u03bbmax = \u03bb0 > \u03bb1 > . . . > \u03bbK. For any integer 0 \u2264 k < K, we have \u03b2\u2217g (\u03bbk+1) = 0 if \u03b2\u2217(\u03bbk) is known and the following holds:\u2225\u2225\u2225\u2225\u2225XTg ( y \u2212 \u2211G g=1 Xg\u03b2 \u2217 g (\u03bbk) \u03bbk + 1 2 v\u22a52 (\u03bbk+1, \u03bbk) )\u2225\u2225\u2225\u2225\u2225 2 < \u221a ng \u2212 1 2 \u2016v\u22a52 (\u03bbk+1, \u03bbk)\u20162\u2016Xg\u20162."}, {"heading": "4 Experiments", "text": "In this section, we evaluate the proposed EDPP rules for Lasso and group Lasso on both synthetic and real data sets. To measure the performance of our screening rules, we compute the rejection ratio and speedup (please refer to Section 2.3.3 for details). Because the EDPP rule is safe, i.e., no active features/groups will be mistakenly discarded, the rejection ratio will be less than one.\nIn Section 4.1, we conduct two sets of experiments to compare the performance of EDPP against several state-of-the-art screening methods. We first compare the performance of the basic versions of EDPP, DOME, SAFE, and strong rule. Then, we focus on the sequential versions of EDPP, SAFE, and strong rule. Notice that, SAFE and EDPP are safe. However, strong rule may mistakenly discard features with nonzero coefficients in the solution. Although DOME is also safe for the Lasso problem, it is unclear if there exists a sequential version of DOME. Recall that, real applications usually favor the sequential screening rules because we need to solve a sequence of of Lasso problems to determine an appropriate parameter value [32]. Moreover, DOME assumes special structure on the data, i.e., each feature and the response vector should be normalized to have unit length.\nIn Section 4.2, we compare EDPP with strong rule for the group Lasso problem on synthetic data sets. We are not aware of any safe screening rules for the group Lasso problem at this point. For SAFE and Dome, it is not straightforward to extend them to the group Lasso problem."}, {"heading": "4.1 EDPP for the Lasso Problem", "text": "For the Lasso problem, we first compare the performance of the basic versions of EDPP, DOME, SAFE and strong rule in Section 4.1.1. Then, we compare the performance of the sequential versions of EDPP, SAFE and strong rule in Section 4.1.2."}, {"heading": "4.1.1 Evaluation of the Basic EDPP Rule", "text": "In this section, we perform experiments on six real data sets to compare the performance of the basic versions of SAFE, DOME, strong rule and EDPP. Briefly speaking, suppose that we are given a parameter value \u03bb. Basic versions of the aforementioned screening rules always make use of \u03b2\u2217(\u03bbmax) to identify the zero components of \u03b2\u2217(\u03bb). Take EDPP for example. The basic version of EDPP can be obtained by replacing \u03b2\u2217(\u03bbk) and v \u22a5 2 (\u03bbk+1, \u03bbk) with \u03b2 \u2217(\u03bb0) and v \u22a5 2 (\u03bbk, \u03bb0), respectively, in (49) for all k = 1, . . . ,K. In this experiment, we report the rejection ratios of the basic SAFE, DOME, strong rule and EDPP along a sequence of 100 parameter values equally spaced on the \u03bb/\u03bbmax scale from 0.05 to 1.0. We note that DOME requires that all features of the data sets have unit length. Therefore, to compare the performance of DOME with SAFE, strong rule and EDPP, we normalize the features of all the data sets used in this section. However, it is worthwhile to mention that SAFE, strong rule and EDPP do not assume any specific structures on the data set. The data sets used in this section are listed as follows:\na) Colon Cancer data set [1];\nb) Lung Cancer data set [6];\nc) Prostate Cancer data set [27];\nd) PIE face image data set [30, 10];\ne) MNIST handwritten digit data set [21];\nf) COIL-100 image data set [24, 11].\nThe Colon Cancer Data Set This data set contains gene expression information of 22 normal tissues and 40 colon cancer tissues, and each has 2000 gene expression values.\nThe Lung Cancer Data Set This data set contains gene expression information of 186 lung tumors and 17 normal lung specimens. Each specimen has 12600 expression values.\nThe COIL-100 Image Data Set The data set consists of images of 100 objects. The images of each object are taken every 5 degree by rotating the object, yielding 72 images per object. The dimension of each image is 32 \u00d7 32. In each trial, we randomly select one image as the response vector and use the remaining ones as the data matrix. We run 100 trials and report the average performance of the screening rules.\nThe description and the experimental settings for the Prostate Cancer data set, the PIE face image data set and the MNIST handwritten digit data set are given in Section 2.3.3.\nFig. 2 reports the rejection ratios of the basic versions of SAFE, DOME, strong rule and EDPP. We can see that EDPP significantly outperforms the other three screening methods on five of the\nsix data sets, i.e., the Colon Cancer, Lung Cancer, Prostate Cancer, MNIST, and COIL-100 data sets. On the PIE face image data set, EDPP and DOME provide similar performance and both significantly outperform SAFE and strong rule.\nHowever, as pointed out by Tibshirani et al. [32], the real strength of screening methods stems from their sequential versions. The reason is because the optimal parameter value is unknown in real applications. Typical approaches for model selection usually involve solving the Lasso problems many times along a sequence of parameter values. Thus, the sequential screening methods are more suitable in facilitating the aforementioned scenario and more useful than their basic-version counterparts in practice [32]."}, {"heading": "4.1.2 Evaluation of the Sequential EDPP Rule", "text": "In this section, we compare the performance of the sequential versions of SAFE, strong rule and EDPP by the rejection ratio and speedup. We first perform experiments on two synthetic data sets. We then apply the three screening rules to six real data sets.\nSynthetic Data Sets\nFirst, we perform experiments on several synthetic problems, which have been commonly used in the sparse learning literature [7, 39, 31]. We simulate data from the true model\ny = X\u03b2\u2217 + \u03c3 , \u223c N(0, 1). (74)\nWe generate two data sets with 250\u00d7 10000 entries: Synthetic 1 and Synthetic 2. For Synthetic 1, the entries of the data matrix X are i.i.d. standard Gaussian with pairwise correlation zero, i.e., corr(xi,xi) = 0. For Synthetic 2, the entries of the data matrix X are drawn from i.i.d. standard Gaussian with pairwise correlation 0.5|i\u2212j|, i.e., corr(xi,xj) = 0.5\n|i\u2212j|. To generate the response vector y \u2208 R250 by the model in (74), we need to set the parameter \u03c3 and construct the ground truth \u03b2\u2217 \u2208 R10000. Throughout this section, \u03c3 is set to be 0.1. To construct \u03b2\u2217, we randomly select p components which are populated from a uniform [\u22121, 1] distribution, and set the remaining ones as 0. After we generate the data matrix X and the response vector y, we run the solver with or without screening rules to solve the Lasso problems along a sequence of 100 parameter values equally spaced on the \u03bb/\u03bbmax scale from 0.05 to 1.0. We then run 100 trials and report the average performance.\nWe first apply the screening rules, i.e., SAFE, strong rule and EDPP to Synthetic 1 with p = 100, 1000, 5000 respectively. Fig. 3(a), Fig. 3(b) and Fig. 3(c) present the corresponding rejection ratios and speedup of SAFE, strong rule and EDPP. We can see that the rejection ratios of strong rule and EDPP are comparable to each other, and both of them are more effective in discarding inactive features than SAFE. In terms of the speedup, EDPP provides better performance than strong rule. The reason is because strong rule is a heuristic screening method, i.e., it may mistakenly discard active features which have nonzero components in the solution. Thus, strong rule needs to check the KKT conditions to ensure the correctness of the screening result. In contrast, the EDPP rule does not need to check the KKT conditions since the discarded features are guaranteed to be absent from the resulting sparse representation. From the last two columns of Table 2, we can observe that the running time of strong rule is about twice of that of EDPP.\nFig. 3(d), Fig. 3(e) and Fig. 3(f) present the rejection ratios and speedup of SAFE, strong rule and EDPP on Synthetic 2 with p = 100, 1000, 5000 respectively. We can observe patterns similar to\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nSAFE Strong Rule EDPP\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nSAFE Strong Rule EDPP\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nSAFE Strong Rule EDPP\nSpeedup\nSpeedup\nSpeedup\nSynthetic 1. Clearly, our method, EDPP, is very robust to the variations of the intrinsic structures of the data sets and the sparsity of the ground truth.\nReal Data Sets In this section, we compare the performance of the EDPP rule with SAFE and strong rule on six real data sets along a sequence of 100 parameter values equally spaced on the \u03bb/\u03bbmax scale from 0.05 to 1.0. The data sets are listed as follows:\na) Breast Cancer data set [33, 29];\nb) Leukemia data set [2];\nc) Prostate Cancer data set [27];\nd) PIE face image data set [30, 10];\ne) MNIST handwritten digit data set [21];\nf) Street View House Number (SVHN) data set [25].\nWe present the rejection ratios and speedup of EDPP, SAFE and strong rule in Fig. 4. Table 3 reports the running time of the solver with or without screening for solving the 100 Lasso problems, and that of the screening rules.\nThe Breast Cancer Data Set This data set contains 44 tumor samples, each of which is represented by 7129 genes. Therefore, the data matrix X is of 44 \u00d7 7129. The response vector y \u2208 {1,\u22121}44 contains the binary label of each sample.\nThe Leukemia Data Set This data set is a DNA microarray data set, containing 52 samples and 11225 genes. Therefore, the data matrix X is of 55\u00d711225. The response vector y \u2208 {1,\u22121}52 contains the binary label of each sample.\nThe SVHN Data set The SVHN data set contains color images of street view house numbers, including 73257 images for training and 26032 for testing. The dimension of each image is 32\u00d7 32. In each trial, we first randomly select an image as the response y \u2208 R3072, and then use the remaining ones to form the data matrix X \u2208 R3072\u00d799288. We run 100 trials and report the average performance.\nThe description and the experiment settings for the Prostate Cancer data set, the PIE face image data set and the MNIST handwritten digit data set are given in Section 2.3.3.\nFrom Fig. 4, we can see that the rejection ratios of strong rule and EDPP are comparable to each other. Compared to SAFE, both of strong rule and EDPP are able to identify far more inactive features, leading to a much higher speedup. However, because strong rule needs to check the KKT conditions to ensure the correctness of the screening results, the speedup gained by EDPP is higher than that by strong rule. When the size of the data matrix is not very large, e.g., the\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nSAFE Strong Rule EDPP\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nSAFE Strong Rule EDPP\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nSAFE Strong Rule EDPP\nSpeedup\nSpeedup\nSpeedup\nBreast Cancer and Leukemia data sets, the speedup gained by EDPP are slightly higher than that by strong rule. However, when the size of the data matrix is large, e.g., the MNIST and SVHN data sets, the speedup gained by EDPP are significantly higher than that by strong rule. Moreover, we can also observe from Fig. 4 that, the larger the data matrix is, the higher the speedup can be gained by EDPP. More specifically, for the small data sets, e.g., the Breast Cancer, Leukemia and Prostate Cancer data sets, the speedup gained by EDPP is about 10, 17 and 30 times. In contrast, for the large data sets, e.g., the PIE, MNIST and SVHN data sets, the speedup gained by EDPP is two orders of magnitude. Take the SVHN data set for example. The solver without screening needs about 3 hours to solve the 100 Lasso problems. Combined with the EDPP rule, the solver\nonly needs less than 1 minute to complete the task.\nClearly, the proposed EDPP screening rule is very effective in accelerating the computation of Lasso especially for large-scale problems, and outperforms the state-of-the-art approaches like SAFE and strong rule. Notice that, the EDPP method is safe in the sense that the discarded features are guaranteed to have zero coefficients in the solution.\nEDPP with Least-Angle Regression (LARS)\nAs we mentioned in the introduction, we can combine EDPP with any existing solver. In this experiment, we integrate EDPP and strong rule with another state-of-the-art solver for Lasso, i.e., Least-Angle Regression (LARS) [15]. We perform experiments on the same real data sets used in the last section with the same experiment settings. Because the rejection ratios of screening methods are irrelevant to the solvers, we only report the speedup. Table 4 reports the running time of LARS with or without screening for solving the 100 Lasso problems, and that of the screening methods. Fig. 5 shows the speedup of these two methods. We can still observe a substantial speedup gained by EDPP. The reason is that EDPP has a very low computational cost (see Table 4) and it is very effective in discarding inactive features (see Fig. 4).\n21.67\n32.50\n0 10 20 30 40\nStrong Rule\nEDPP\nSpeedup\n(a) Breast Cancer, X \u2208 R44\u00d77129\n16.22\n29.20\n0 10 20 30 40\nStrong Rule\nEDPP\nSpeedup\n(b) Leukemia, X \u2208 R55\u00d711225\n5.54\n15.57\n0 5 10 15 20\nStrong Rule\nEDPP\nSpeedup\n(c) Prostate Cancer, X \u2208 R132\u00d715154\nSpeedup\nSpeedup\nSpeedup"}, {"heading": "4.2 EDPP for the Group Lasso Problem", "text": "In this experiment, we evaluate the performance of EDPP and strong rule with different numbers of groups. The data matrix X is fixed to be 250 \u00d7 200000. The entries of the response vector y and the data matrix X are generated i.i.d. from a standard Gaussian distribution. For each experiment, we repeat the computation 20 times and report the average results. Moreover, let ng denote the number of groups and sg be the average group size. For example, if ng is 10000, then sg = p/ng = 20.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nStrong Rule EDPP\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nStrong Rule EDPP\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n0.2\n0.4\n0.6\n0.8\n1\n\u03bb/\u03bbmax\nR ej\nec tio\nn R\nat io\nStrong Rule EDPP\nSpeedup\nSpeedup\nSpeedup\nFrom Figure 6, we can see that EDPP and strong rule are able to discard more inactive groups when the number of groups ng increases. The intuition behind this observation is that the estimation of the dual optimal solution is more accurate with a smaller group size. Notice that, a large ng implies a small average group size. Figure 6 also implies that compared to strong rule, EDPP is\nable to discard more inactive groups and is more robust with respect to different values of ng. Table 5 further demonstrates the effectiveness of EDPP in improving the efficiency of the solver. When ng = 10000, the efficiency of the solver is improved by about 80 times. When ng = 20000 and 40000, the efficiency of the solver is boosted by about 120 and 160 times with EDPP respectively."}, {"heading": "5 Conclusion", "text": "In this paper, we develop new screening rules for the Lasso problem by making use of the properties of the projection operators with respect to a closed convex set. Our proposed methods, i.e., DPP screening rules, are able to effectively identify inactive predictors of the Lasso problem, thus greatly reducing the size of the optimization problem. Moreover, we further improve DPP rule and propose the enhanced DPP rule, which is more effective in discarding inactive features than DPP rule. The idea of the family of DPP rules can be easily generalized to identify the inactive groups of the group Lasso problem. Extensive numerical experiments on both synthetic and real data demonstrate the effectiveness of the proposed rules. It is worthwhile to mention that the family of DPP rules can be combined with any Lasso solver as a speedup tool. In the future, we plan to generalize our ideas to other sparse formulations consisting of more general structured sparse penalties, e.g., tree/graph Lasso, fused Lasso."}, {"heading": "Appendix A.", "text": "In this appendix, we give the detailed derivation of the dual problem of Lasso."}, {"heading": "A1. Dual Formulation", "text": "Assuming the data matrix is X \u2208 RN\u00d7p, the standard Lasso problem is given by:\ninf \u03b2\u2208Rp\n1 2 \u2016y \u2212X\u03b2\u201622 + \u03bb\u2016\u03b2\u20161. (75)\nFor completeness, we give a detailed deviation of the dual formulation of (75) in this section. Note that problem (75) has no constraints. Therefore the dual problem is trivial and useless. A common trick [8] is to introduce a new set of variables z = y \u2212X\u03b2 such that problem (75) becomes:\ninf \u03b2\n1 2 \u2016z\u201622 + \u03bb\u2016\u03b2\u20161, (76)\nsubject to z = y \u2212X\u03b2.\nBy introducing the dual variables \u03b7 \u2208 RN , we get the Lagrangian of problem (76):\nL(\u03b2, z, \u03b7) = 1\n2 \u2016z\u201622 + \u03bb\u2016\u03b2\u20161 + \u03b7T \u00b7 (y \u2212X\u03b2 \u2212 z). (77)\nFor the Lagrangian, the primal variables are \u03b2 and z. And the dual function g(\u03b7) is:\ng(\u03b7) = inf \u03b2,z L(\u03b2, z, \u03b7) = \u03b7Ty + inf \u03b2 (\u2212\u03b7TX\u03b2 + \u03bb\u2016\u03b2\u20161) + inf z (1 2 \u2016z\u201622 \u2212 \u03b7T z ) . (78)\nIn order to get g(\u03b7), we need to solve the following two optimization problems.\ninf \u03b2 \u2212\u03b7TX\u03b2 + \u03bb\u2016\u03b2\u20161, (79)\nand\ninf z\n1 2 \u2016z\u201622 \u2212 \u03b7T z. (80)\nLet us first consider problem (79). Denote the objective function of problem (79) as\nf1(\u03b2) = \u2212\u03b7TX\u03b2 + \u03bb\u2016\u03b2\u20161. (81)\nf1(\u03b2) is convex but not smooth. Therefore let us consider its subgradient\n\u2202f1(\u03b2) = \u2212XT \u03b7 + \u03bbv,\nin which \u2016v\u2016\u221e \u2264 1 and vT\u03b2 = \u2016\u03b2\u20161, i.e., v is the subgradient of \u2016\u03b2\u20161. The necessary condition for f1 to attain an optimum is\n\u2203\u03b2\u2032, such that 0 \u2208 \u2202f1(\u03b2\u2032) = {\u2212XT \u03b7 + \u03bbv\u2032},\nwhere v\u2032 \u2208 \u2202\u2016\u03b2\u2032\u20161. In other words, \u03b2\u2032,v\u2032 should satisfy\nv\u2032 = XT \u03b7\n\u03bb , \u2016v\u2032\u2016\u221e \u2264 1,v\u2032T\u03b2\u2032 = \u2016\u03b2\u2032\u20161,\nwhich is equivalent to |xTi \u03b7| \u2264 \u03bb, i = 1, 2, . . . , p. (82) Then we plug v\u2032 = X T \u03b7 \u03bb and v \u2032T\u03b2\u2032 = \u2016\u03b2\u2032\u20161 into Eq. (81):\nf1(\u03b2 \u2032) = inf \u03b2 f1(\u03b2) = \u2212\u03b7TX\u03b2\u2032 + \u03bb (XT \u03b7 \u03bb )T \u03b2\u2032 = 0. (83)\nTherefore, the optimum value of problem (79) is 0. Next, let us consider problem (80). Denote the objective function of problem (80) as f2(z). Let us rewrite f2(z) as:\nf2(z) = 1\n2 (\u2016z\u2212 \u03b7\u201622 \u2212 \u2016\u03b7\u201622). (84)\nClearly, z\u2032 = argmin\nz f2(z) = \u03b7,\nand\ninf z f2(z) = \u2212\n1 2 \u2016\u03b7\u201622.\nCombining everything above, we get the dual problem:\nsup \u03b7 g(\u03b7) = \u03b7Ty \u2212 1 2 \u2016\u03b7\u201622, (85)\nsubject to |xTi \u03b7| \u2264 \u03bb, i = 1, 2, . . . , p.\nwhich is equivalent to\nsup \u03b7\ng(\u03b7) = 1\n2 \u2016y\u201622 \u2212\n1 2 \u2016\u03b7 \u2212 y\u201622, (86)\nsubject to |xTi \u03b7| \u2264 \u03bb, i = 1, 2, . . . , p.\nBy a simple re-scaling of the dual variables \u03b7, i.e., let \u03b8 = \u03b7\u03bb , problem (86) transforms to:\nsup \u03b8\ng(\u03b8) = 1\n2 \u2016y\u201622 \u2212\n\u03bb2\n2 \u2016\u03b8 \u2212 y \u03bb \u201622, (87)\nsubject to |xTi \u03b8| \u2264 1, i = 1, 2, . . . , p."}, {"heading": "A2. The KKT Conditions", "text": "Problem (76) is clearly convex and its constraints are all affine. By Slater\u2019s condition, as long as problem (76) is feasible we will have strong duality. Denote \u03b2\u2217, z\u2217 and \u03b8\u2217 as optimal primal and dual variables. The Lagrangian is\nL(\u03b2, z, \u03b8) = 1\n2 \u2016z\u201622 + \u03bb\u2016\u03b2\u20161 + \u03bb\u03b8T \u00b7 (y \u2212X\u03b2 \u2212 z). (88)\nFrom the KKT condition, we have\n0 \u2208 \u2202\u03b2L(\u03b2\u2217, z\u2217, \u03b8\u2217) = \u2212\u03bbXT \u03b8\u2217 + \u03bbv, in which \u2016v\u2016\u221e \u2264 1 and vT\u03b2\u2217 = \u2016\u03b2\u2217\u20161, (89)\n\u2207zL(\u03b2\u2217, z\u2217, \u03b8\u2217) = z\u2217 \u2212 \u03bb\u03b8\u2217 = 0, (90)\n\u2207\u03b8L(\u03b2\u2217, z\u2217, \u03b8\u2217) = \u03bb(y \u2212X\u03b2\u2217 \u2212 z\u2217) = 0. (91)\nFrom Eq. (90) and (91), we have:\ny = X\u03b2\u2217 + \u03bb\u03b8\u2217. (92)\nFrom Eq. (89), we know there exists v\u2217 \u2208 \u2202\u2016\u03b2\u2217\u20161 such that"}, {"heading": "XT \u03b8\u2217 = v\u2217, \u2016v\u2217\u2016\u221e \u2264 1 and (v\u2217)T\u03b2\u2217 = \u2016\u03b2\u2217\u20161,", "text": "which is equivalent to\n|xTi \u03b8\u2217| \u2264 1, i = 1, 2, . . . , p, and (\u03b8\u2217)TX\u03b2\u2217 = \u2016\u03b2\u2217\u20161. (93)\nFrom Eq. (93), it is easy to conclude:\n(\u03b8\u2217)Txi \u2208\n{ sign(\u03b2\u2217i ) if \u03b2 \u2217 i 6= 0,\n[\u22121, 1] if \u03b2\u2217i = 0. (94)"}, {"heading": "Appendix B.", "text": "In this appendix, we present the detailed derivation of the dual problem of group Lasso."}, {"heading": "B1. Dual Formulation", "text": "Assuming the data matrix is Xg \u2208 RN\u00d7ng and p = \u2211G g=1 ng, the group Lasso problem is given by:\ninf \u03b2\u2208Rp\n1 2 \u2016y \u2212 G\u2211 g=1 Xg\u03b2g\u201622 + \u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u20162. (95)\nLet z = y \u2212 \u2211G\ng=1 Xg\u03b2g and problem (95) becomes:\ninf \u03b2\n1 2 \u2016z\u201622 + \u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u20162, (96)\nsubject to z = y \u2212 G\u2211 g=1 Xg\u03b2g.\nBy introducing the dual variables \u03b7 \u2208 RN , the Lagrangian of problem (96) is:\nL(\u03b2, z, \u03b7) = 1\n2 \u2016z\u201622 + \u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u20162 + \u03b7T \u00b7 (y \u2212 G\u2211 g=1 Xg\u03b2g \u2212 z). (97)\nand the dual function g(\u03b7) is:\ng(\u03b7) = inf \u03b2,z L(\u03b2, z, \u03b7) = \u03b7Ty + inf \u03b2\n( \u2212 \u03b7T G\u2211 g=1 Xg\u03b2g + \u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u20162 ) + inf z (1 2 \u2016z\u201622 \u2212 \u03b7T z ) . (98)\nIn order to get g(\u03b7), let us solve the following two optimization problems.\ninf \u03b2 \u2212\u03b7T G\u2211 g=1 Xg\u03b2g + \u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u20162, (99)\nand\ninf z\n1 2 \u2016z\u201622 \u2212 \u03b7T z. (100)\nLet us first consider problem (99). Denote the objective function of problem (99) as\nf\u0302(\u03b2) = \u2212\u03b7T G\u2211 g=1 Xg\u03b2g + \u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u20162, (101)\nLet f\u0302g(\u03b2g) = \u2212\u03b7TXg\u03b2g + \u03bb \u221a ng\u2016\u03b2g\u20162, g = 1, 2, . . . , G.\nthen we can split problem (99) into a set of subproblems. Clearly f\u0302g(\u03b2g) is convex but not smooth because it has a singular point at 0. Consider the subgradient of f\u0302g,\n\u2202f\u0302g(\u03b2g) = \u2212XTg \u03b7 + \u03bb \u221a ngvg, g = 1, 2, . . . , G,\nwhere vg is the subgradient of \u2016\u03b2g\u20162:\nvg \u2208 { \u03b2g \u2016\u03b2g\u20162 if \u03b2g 6= 0, u, \u2016u\u20162 \u2264 1 if \u03b2g = 0.\n(102)\nLet \u03b2\u2032g be the optimal solution of f\u0302g, then \u03b2 \u2032 g satisfy\n\u2203v\u2032g \u2208 \u2202\u2016\u03b2\u2032g\u20162, \u2212XTg \u03b7 + \u03bb \u221a ngv \u2032 g = 0.\nIf \u03b2\u2032g = 0, clearly, f\u0302g(\u03b2 \u2032 g) = 0. Otherwise, since \u03bb \u221a ngv \u2032 g = X T g \u03b7 and v \u2032 g = \u03b2\u2032g \u2016\u03b2\u2032g\u20162 , we have\nf\u0302g(\u03b2 \u2032 g) = \u2212\u03bb \u221a ng\n(\u03b2\u2032g) T\n\u2016\u03b2\u2032g\u20162 \u03b2\u2032g + \u03bb\n\u221a ng\u2016\u03b2\u2032g\u20162 = 0.\nAll together, we can conclude the\ninf \u03b2g f\u0302g(\u03b2g) = 0, g = 1, 2, . . . , G\nand thus\ninf \u03b2 f\u0302(\u03b2) = inf \u03b2 G\u2211 g=1 f\u0302g(\u03b2g) = G\u2211 g=1 inf \u03b2g f\u0302g(\u03b2g) = 0.\nThe second equality is due to the fact that \u03b2g\u2019s are independent.\nNote, from Eq. (102), it is easy to see \u2016vg\u20162 \u2264 1. Since \u03bb \u221a ngv \u2032 g = X T g \u03b7, we get a constraint on\n\u03b7, i.e., \u03b7 should satisfy:\n\u2016XTg \u03b7\u20162 \u2264 \u03bb \u221a ng, g = 1, 2, . . . , G.\nNext, let us consider problem (100). Since problem (100) is exactly the same as problem (80), we conclude:\nz\u2032 = argmin z\n1 2 \u2016z\u201622 \u2212 \u03b7T z = \u03b7,\nand\ninf z\n1 2 \u2016z\u201622 \u2212 \u03b7T z = \u2212 1 2 \u2016\u03b7\u201622.\nTherefore the dual function g(\u03b7) is:\ng(\u03b7) = \u03b7Ty \u2212 1 2 \u2016\u03b7\u201622.\nCombining everything above, we get the dual formulation of the group Lasso:\nsup \u03b7 g(\u03b7) = \u03b7Ty \u2212 1 2 \u2016\u03b7\u201622, (103)\nsubject to \u2016XTg \u03b7\u20162 \u2264 \u03bb \u221a ng, g = 1, 2, . . . , G.\nwhich is equivalent to\nsup \u03b7\ng(\u03b7) = 1\n2 \u2016y\u201622 \u2212\n1 2 \u2016\u03b7 \u2212 y\u201622, (104)\nsubject to \u2016XTg \u03b7\u20162 \u2264 \u03bb \u221a ng, g = 1, 2, . . . , G.\nBy a simple re-scaling of the dual variables \u03b7, i.e., let \u03b8 = \u03b7\u03bb , problem (104) transforms to:\nsup \u03b8\ng(\u03b8) = 1\n2 \u2016y\u201622 \u2212\n\u03bb2\n2 \u2016\u03b8 \u2212 y \u03bb \u201622, (105)\nsubject to \u2016XTg \u03b8\u20162 \u2264 \u221a ng, g = 1, 2, . . . , G."}, {"heading": "B2. The KKT Conditions", "text": "Clearly, problem (96) is convex and its constraints are all affine. By Slater\u2019s condition, as long as problem (96) is feasible we will have strong duality. Denote \u03b2\u2217, z\u2217 and \u03b8\u2217 as optimal primal and dual variables. The Lagrangian is\nL(\u03b2, z, \u03b8) = 1\n2 \u2016z\u201622 + \u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u20162 + \u03bb\u03b8T \u00b7 (y \u2212 G\u2211 g=1 Xg\u03b2g \u2212 z). (106)\nFrom the KKT condition, we have\n0 \u2208 \u2202\u03b2gL(\u03b2\u2217, z\u2217, \u03b8\u2217) = \u2212\u03bbXTg \u03b8\u2217 + \u03bb \u221a ngvg, in which vg \u2208 \u2202\u2016\u03b2\u2217g\u20162, g = 1, 2, . . . , G, (107)\n\u2207zL(\u03b2\u2217, z\u2217, \u03b8\u2217) = z\u2217 \u2212 \u03bb\u03b8\u2217 = 0, (108)\n\u2207\u03b8L(\u03b2\u2217, z\u2217, \u03b8\u2217) = \u03bb \u00b7 (y \u2212 G\u2211 g=1 Xg\u03b2 \u2217 g \u2212 z\u2217) = 0. (109)\nFrom Eq. (108) and (109), we have:\ny = G\u2211 g=1 Xg\u03b2 \u2217 g + \u03bb\u03b8 \u2217. (110)\nFrom Eq. (107), we know there exists v\u2032g \u2208 \u2202\u2016\u03b2\u2217g\u20162 such that\nXTg \u03b8 \u2217 = \u221a ngv \u2032 g\nand\nv\u2032g \u2208\n{ \u03b2\u2217g \u2016\u03b2\u2217g\u20162\nif \u03b2\u2217g 6= 0, u, \u2016u\u20162 \u2264 1 if \u03b2\u2217g = 0,\nThen the following holds:\nXTg \u03b8 \u2217 \u2208\n{\u221a ng\n\u03b2\u2217g \u2016\u03b2\u2217g\u20162\nif \u03b2\u2217g 6= 0, \u221a ngu, \u2016u\u20162 \u2264 1 if \u03b2\u2217g = 0,\n(111)\nfor g = 1, 2, . . . , G. Clearly, if \u2016XTg \u03b8\u2217\u20162 < \u221a ng, we can conclude \u03b2 \u2217 g = 0."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>Lasso is a widely used regression technique to find sparse representations. When the di-<lb>mension of the feature space and the number of samples are extremely large, solving the Lasso<lb>problem remains challenging. To improve the efficiency of solving large-scale Lasso problems,<lb>El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify<lb>the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then,<lb>the inactive predictors or features can be removed from the optimization problem to reduce its<lb>scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive<lb>predictors include the set of inactive constraints on the optimal dual solution. In this paper,<lb>we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which<lb>is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to<lb>the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we<lb>show that our screening rule can be extended to identify inactive groups in group Lasso. To<lb>the best of our knowledge, there is currently no exact screening rule for group Lasso. We have<lb>evaluated our screening rule using synthetic and real data sets. Results show that our rule is<lb>more effective in identifying inactive predictors than existing state-of-the-art screening rules for<lb>Lasso.", "creator": "LaTeX with hyperref package"}}}