{"id": "1301.2268", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables", "abstract": "nazirite Global variational chronos approximation methods in bunche graphical models allow internments efficient approximate inference lethbridge of nondual complex cultivos posterior chapman.htm distributions by using trogons a simpler kingson model. The nccc choice 3-74 of the recurring approximating model microfinancing determines dery a citzens tradeoff charioteers between the complexity of palmeiro the approximation dragonet procedure re5 and the quality of mccrystal the bukovyna approximation. upazilla In milanov this paper, h\u00e9 we siler consider variational popovi\u0107 approximations udyog based on two bunt classes takura of o-methylation models brilliantly that are modabber richer than frafjord standard intrawest Bayesian 117.32 networks, Markov merseybeat networks rokot or bebington mixture models. antell As such, brinkley these nucleic classes distillers allow cybercafe to find sptrans better tradeoffs in codepage the spectrum mccamant of approximations. The first bassanio class aller of mahmoudi models are chain graphs, idale which yankers capture distributions ironically that akf are partially 2,047 directed. The second trin class irritatingly of models are disbursement directed graphs (Bayesian networks) with additional fumarate latent variables. asne Both classes margerie allow representation possessions of .256 multi - variable tabby dependencies muniswamy that krasts cannot be v.o. easily mise-en-sc\u00e8ne represented 130-ton within a peals Bayesian network.", "histories": [["v1", "Thu, 10 Jan 2013 16:23:26 GMT  (1178kb)", "http://arxiv.org/abs/1301.2268v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tal el-hay", "nir friedman"], "accepted": false, "id": "1301.2268"}, "pdf": {"name": "1301.2268.pdf", "metadata": {"source": "CRF", "title": "Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables", "authors": ["Tal El-Hay", "Nir Friedman"], "emails": ["@cs.huji.ac.il"], "sections": null, "references": [{"title": "Tractable variational structures for approximating graphical models", "author": ["D. Barber", "W. Wiegerinck"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Elements of Informa\u00ad", "author": ["T.M. Cover", "J. A Thomas"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "Saul and Jordan [ 1 0] sug\u00ad gest to circumvent this problem by using structured variationa! approximation.", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 1, "context": "A common measure of distance is the KL divergence [2] between Q(T : 8) and the posterior distribution P(T I o).", "startOffset": 50, "endOffset": 53}], "year": 2011, "abstractText": "Global variational approximation methods in graphical models allow efficient approximate inference of com\u00ad plex posterior distributions by using a simpler model. The choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation. In this paper, we consider variational approximations based on two classes of models that are richer than standard Bayesian networks, Markov networks or mixture mod\u00ad els. As such, these classes allow to find better tradeoffs in the spectrum of approximations. The first class of models are elwin graphs, which capture distributions that are partially directed. The second class of mod\u00ad els are directed graphs (Bayesian networks) with addi\u00ad tional latent variables. Both classes allow representa\u00ad tion of multi-variable dependencies that cannot be eas\u00ad ily represented within a Bayesian network.", "creator": "pdftk 1.41 - www.pdftk.com"}}}