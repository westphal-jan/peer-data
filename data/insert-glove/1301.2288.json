{"id": "1301.2288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Inference in Hybrid Networks: Theoretical Limits and Practical Algorithms", "abstract": "zagorec An important sema subclass of convene hybrid marcoussis Bayesian networks il-12 are those peopling that xircom represent merges Conditional tekirdag Linear reprocessors Gaussian (muelleri CLG) merc\u0153ur distributions - - - whom a distribution sorrenti with sharbi a bookmakers multivariate v\u00f6gele Gaussian component for rundschau each seasonless instantiation chocked of the discrete variables. triticale In 33.62 this paper 1948-1951 we explore bruceville the lafayette problem of inference commisssion in CLGs. xochitl We show that mjp inference graysmith in normales CLGs can investement be rivetted significantly harder vr6 than inference in hovhannisyan Bayes r.newman Nets. maharana In self-learning particular, stromboli we 9.69 prove conductivities that even if the CLG nocioni is dushyanta restricted to non-repudiation an barnack extremely simple structure carport of balkline a polytree in kibungo which every continuous node has zinzan at most one dkny discrete alief ancestor, fudong the observatory inference task nawan is goldhaber NP - hard. To deal furthering with the rane often prohibitive computational cost exudates of slaughterhouse the exact tagammu inference algorithm for suing CLGs, microtubule-associated we explore several approximate zazie inference gitanos algorithms. These algorithms inglaterra try niida to find sabraw a small disalvo subset al-bank of Gaussians four-and-a-half-year which are a good 205.6 approximation arenicola to on-court the full mixture distribution. '84 We consider quantz two cablesystems Monte Carlo quoit approaches ifcc and 30-year a novel approach that enumerates galdino mixture r\u014dj\u016b components dyp in polyheme order of gymnasiums prior re-appearing probability. We compare hightailed these methods redwood on jugo a variety of problems osteopath and 4/4 show gianvito that our liveried novel algorithm elin is tecmo very authored promising for large, hybrid 27-acre diagnosis circuitry problems.", "histories": [["v1", "Thu, 10 Jan 2013 16:24:54 GMT  (1206kb)", "http://arxiv.org/abs/1301.2288v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["uri lerner", "ron parr"], "accepted": false, "id": "1301.2288"}, "pdf": {"name": "1301.2288.pdf", "metadata": {"source": "CRF", "title": "Inference in Hybrid Networks: Theoretical Limits and Practical Algorithms", "authors": ["Uri Lerner", "Ronald Parr"], "emails": ["uri@cs.stanford.edu", "parr@cs.duke.edu"], "sections": null, "references": [{"title": "Stable local compuation with con\u00ad ditional Gaussian distributions", "author": ["F. Jensen"], "venue": "Technical Report R-99-2014,", "citeRegEx": "Jensen.,? \\Q2014\\E", "shortCiteRegEx": "Jensen.", "year": 2014}], "referenceMentions": [], "year": 2011, "abstractText": "An important subclass of hybrid Bayesian networks are those that represent Conditional Linear Gaussian (CLG) distributionsa distribution with a multivari\u00ad ate Gaussian component for each instantiation of the discrete variables. In this paper we explore the prob\u00ad lem of inference in CLGs, and provide complexity re\u00ad sults for an important class of CLGs, which includes Switching Kalman Filters. In particular, we prove that even if the CLG is restricted to an extremely simple structure of a polytree, the inference task is NP-hard. Furthermore, we show that, unless P=NP, even ap\u00ad proximate inference on these simple networks is in\u00ad tractable. Given the often prohibitive computational cost of even approximate inference, we must take advantage of spe\u00ad cial domain properties which may enable efficient in\u00ad ference. We concentrate on the fault diagnosis domain, and explore several approximate inference algorithms. These algorithms try to find a small subset of Gaus\u00ad sians which are a good approximation to the full mix\u00ad ture distribution. We consider two Monte Carlo ap\u00ad proaches and a novel approach that enumerates mix\u00ad ture components in order of prior probability. We com\u00ad pare these methods on a variety of problems and show that our novel algorithm is very promising for large, hybrid diagnosis problems.", "creator": "pdftk 1.41 - www.pdftk.com"}}}