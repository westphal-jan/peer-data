{"id": "1301.2293", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Aggregating Learned Probabilistic Beliefs", "abstract": "We 1pt consider the task diagramming of foudre aggregating beliefs ambacang of koppett severalexperts. 123.90 We assume that cantey these spirochetes beliefs wlosowicz are jaque represented as probabilitydistributions. hollowed-out We tuned argue that matchbooks the \u00f8rland evaluation hargreaves of alvito any akito aggregationtechnique jailhouse depends on the pioneering semantic ulufa'alu context of uncharacteristic this derision task. We haughley propose aframework, laz in co-morbid which satwant we suazo assume dulger that condensed nature azarian generates bellcote samples dsl from a ` true ' kye distribution and different experts 2-83 form kurze their flappers beliefs 76101 based braddock onthe subsets cenotaphs of the recombinant data they tadano have a myogenesis chance to observe. 3,958 Naturally, ecoregion theideal aggregate maximinus distribution seau would n'gotty be the jacinth one big-name learned 81-76 from thecombined kamila sample zavadil sets. poborsky Such a formulation leads case-insensitive to 26,700 a natural yvaine way tomeasure the accuracy of the aggregation mechanism. We show humanitarians that apbnews.com the wadleigh well - known aggregation operator LinOP is zeughaus ideallysuited ingreso for heatwole that task. twinkled We propose 100.65 a LinOP - based yukaghir learning algorithm, inspired by the pukan techniques developed for acors Bayesian learning, whichaggregates the lemba experts ' distributions apcom represented g-17 as bilis Bayesiannetworks. \u03bcmol Our sportsplex preliminary obtusifolia experiments show chernokozovo that ccdac this upside-down algorithmperforms artemio well heckling in practice.", "histories": [["v1", "Thu, 10 Jan 2013 16:25:16 GMT  (914kb)", "http://arxiv.org/abs/1301.2293v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pedrito maynard-reid ii", "urszula chajewska"], "accepted": false, "id": "1301.2293"}, "pdf": {"name": "1301.2293.pdf", "metadata": {"source": "CRF", "title": "Aggregating Learned Probabilistic Beliefs", "authors": ["Pedrito Maynard-Reid"], "emails": ["urszula@cs.stanford.edu"], "sections": null, "references": [{"title": "Theory refinement on bayesian net\u00ad", "author": ["W. Buntine"], "venue": null, "citeRegEx": "Buntine.,? \\Q1991\\E", "shortCiteRegEx": "Buntine.", "year": 1991}, {"title": "A normative examination of ensemble learning algorithms", "author": ["D.M. Pennock", "P.E. Horvitz"], "venue": "In Proc. ICML'OO,", "citeRegEx": "Pennock and Horvitz.,? \\Q2000\\E", "shortCiteRegEx": "Pennock and Horvitz.", "year": 2000}], "referenceMentions": [], "year": 2011, "abstractText": "We consider the task of aggregating beliefs of sev\u00ad eral experts. We assume that these beliefs are rep\u00ad resented as probability distributions. We argue that the evaluation of any aggregation technique depends on the semantic context of this task. We propose a framework, in which we assume that nature generates samples from a 'true' distribution and different experts form their beliefs based on the subsets of the data they have a chance to observe. Naturally, the optimal ag\u00ad gregate distribution would be the one learned from the combined sample sets. Such a formulation leads to a natural way to measure the accuracy of the aggregation mechanism. We show that the well-known aggregation operator LinOP is ideally suited for that task. We propose a LinOP-based learning algorithm, inspired by the techniques developed for Bayesian learning, which aggregates the experts' distributions represented as Bayesian networks. We show experimentally that this algorithm performs well in practice.", "creator": "pdftk 1.41 - www.pdftk.com"}}}