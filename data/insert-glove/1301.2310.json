{"id": "1301.2310", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Policy Improvement for POMDPs Using Normalized Importance Sampling", "abstract": "waltzed We muskoka present a feltsman new http://marsrovers.jpl.nasa.gov method quivira for estimating koblet the expected return gorbenko of a POMDP colpoys from experience. pob The method conjointly does not austronesians assume non-observant any knowledge faretta of the jewish-owned POMDP cotgrave and gorda allows contorted the mintzberg experience deutschen to be gathered from as\u014d an arbitrary puszta sequence 98-mile of kanze policies. The banpot return austins is 29,035-foot estimated for devor any invigorates new policy of beatify the POMDP. We discomforting motivate the estimator from devillers function - approximation seydi and preloaded importance sampling v-8 points - affordable of - temeka view epidote and mcconal derive its ingleby theoretical kensaku properties. giardino Although the hatf estimator is biased, presently it has dayrell low manilius variance and acclimatize the bias is often irrelevant when the racketeering estimator detoxing is used delville for shifflet pair - wise unzen comparisons. 97.10 We conclude by sionil extending the xlt estimator re-publication to contrabands policies with redacted memory ravening and co-chaired compare its performance elaborateness in siriusxm a dawood greedy search algorithm greffulhe to REINFORCE algorithms non-being showing foulger an order inter-college of viviparidae magnitude reduction in anang the number of trials required.", "histories": [["v1", "Thu, 10 Jan 2013 16:26:30 GMT  (1326kb)", "http://arxiv.org/abs/1301.2310v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["christian r shelton"], "accepted": false, "id": "1301.2310"}, "pdf": {"name": "1301.2310.pdf", "metadata": {"source": "CRF", "title": "Policy Improvement for POMDPs using Normalized Importance Sampling", "authors": ["Christian R. Shelton"], "emails": [], "sections": null, "references": [{"title": "Approximate planning in large POMDPs via reusable trajectories", "author": ["Kearns et al", "M. 1999] Kearns", "Y. Mansour", "A. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "Bayesian estimates of equation system param\u00ad eters: An application of integration by monte carlo", "author": ["Kloek", "van Dijk", "T. 1978] Kloek", "H.K. van Dijk"], "venue": null, "citeRegEx": "Kloek et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Kloek et al\\.", "year": 1978}, {"title": "Exploration in gradient-based reinforce\u00ad ment learning", "author": ["Meuleau et al", "N. 2001] Meuleau", "L. Peshkin", "Kim", "K.-E"], "venue": "Technical Report AI-MEMO 2001-003,", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "Learning policies with external mem\u00ad ory", "author": ["P. L"], "venue": "In Proceedings of the Sixteenth International Con\u00ad ference on Machine Learning", "citeRegEx": "L.,? \\Q1999\\E", "shortCiteRegEx": "L.", "year": 1999}, {"title": "Bounds on sample size for policy eval\u00ad uation in markov environments", "author": ["Peshkin", "Mukherjee", "L. 2001] Peshkin", "S. Mukher\u00ad jee"], "venue": "In Fourteenth Annual Conference on Computational Learning Theory", "citeRegEx": "Peshkin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2001}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["Precup et al", "D. 2001] Precup", "R.S. Sutton", "S. Das\u00ad gupta"], "venue": "In Proceedings of the Eigh\u00ad teenth International Conference on Machine Learning", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "Eligibility traces for off-polcy policy evalua\u00ad tion", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "[Precup et a!.,", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Numerical Recipes in C", "author": ["Press et al", "W.H. 1992] Press", "S.A. Teukolsky", "Vet\u00ad terling", "W. T", "B.P. Flannery"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1992\\E", "shortCiteRegEx": "al. et al\\.", "year": 1992}], "referenceMentions": [], "year": 2011, "abstractText": "We present a new method for estimating the expected return of a POMDP from experi\u00ad ence. The estimator does not assume any knowledge of the POMDP, can estimate the returns for finite state controllers, allows ex\u00ad perience to be gathered from arbitrary se\u00ad quences of policies, and estimates the return for any new policy. We motivate the estima\u00ad tor from function-approximation and impor\u00ad tance sampling points-of-view and derive its bias and variance. Although the estimator is biased, it has low variance and the bias is of\u00ad ten irrelevant when the estimator is used for pair-wise comparisons. We conclude by ex\u00ad tending the estimator to policies with mem\u00ad ory and compare its performance in a greedy search algorithm to the REINFORCE algo\u00ad rithm showing an order of magnitude reduc\u00ad tion in the number of trials required.", "creator": "pdftk 1.41 - www.pdftk.com"}}}