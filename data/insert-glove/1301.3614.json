{"id": "1301.3614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine Translation", "abstract": "overboard A neural retributions probabilistic toun language model (ascheim NPLM) tiebreak provide 81501 an idea to achieve the better meishan perplexity than n - made-up gram otoe language model maliku and shaktar their poddar smoothed achutha language models. This menard paper investigates application nakonde area wrey in bilingual ersten NLP, specifically wynd Statistical haft Machine Translation (SMT ). 2719 We g\u00fcl focus x\u00e1 on the tyhypko perspectives that NPLM has g\u00f6tterd\u00e4mmerung potential sixth-seeded to bated open 6.075 the pickerell possibility ulmo to haak complement potentially ` barrish huge ' monolingual heptathlete resources almudena into the ` tamie resource - usatoday constraint ' schumpeter bilingual furr resources. fedak In order to facilitate the application bartke to various lawful tasks, 158.1 we propose 2,411 the ente joint dohb space model of macartney ngram - manweb HMM motu language erzulie model. We obstetrical show two yevgeniy experiments paralamas in sahli SMT: romas system kunika combination and sidelined word laboratories alignment.", "histories": [["v1", "Wed, 16 Jan 2013 07:56:20 GMT  (280kb)", "https://arxiv.org/abs/1301.3614v1", null], ["v2", "Sun, 20 Jan 2013 22:58:41 GMT  (56kb)", "http://arxiv.org/abs/1301.3614v2", null], ["v3", "Fri, 21 Apr 2017 02:42:35 GMT  (56kb)", "http://arxiv.org/abs/1301.3614v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tsuyoshi okita"], "accepted": false, "id": "1301.3614"}, "pdf": {"name": "1301.3614.pdf", "metadata": {"source": "CRF", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine Translation", "authors": ["Tsuyoshi Okita"], "emails": ["tokita@computing.dcu.ie"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 1.\n36 14\nv3 [\ncs .C\nL ]\n2 1"}, {"heading": "1 Introduction", "text": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48]. Recently, the latter one, i.e. smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38]. A NPLM considers the representation of data in order to make the probability distribution of word sequences more compact where we focus on the similar semantical and syntactical roles of words. For example, when we have two sentences \u201cThe cat is walking in the bedroom\u201d and \u201cA dog was running in a room\u201d, these sentences can be more compactly stored than the n-gram language model if we focus on the similarity between (the, a), (bedroom, room), (is, was), and (running, walking). Thus, a NPLM provides the semantical and syntactical roles of words as a language model. A NPLM of [3] implemented this using the multi-layer neural network and yielded 20% to 35% better perplexity than the language model with the modified Kneser-Ney methods [9].\nThere are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43]. First, one category of applications include POS tagging, NER tagging, and parsing [12, 7]. This category uses the features provided by a NPLM in the limited window size. It is often the case that there is no such long range effects that the decision cannot be made beyond the limited windows which requires to look carefully the elements in a long distance. Second, the other category of applications include Semantic Role Labeling (SRL) task [12, 14]. This category uses the features within a sentence. A typical element is the predicate in a SRL task which requires the information which sometimes in a long distance but within a sentence. Both of these approaches do not require to obtain the best tag sequence, but these tags are independent. Third, the final category includes MERT process [42]\nand possibly many others where most of them remain undeveloped. The objective of this learning in this category is not to search the best tag for a word but the best sequence for a sentence. Hence, we need to apply the sequential learning approach. Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components. For example, the training corpus of word aligner is often strictly restricted to the given parallel corpus. However, a NPLM allows this training with huge monolingual corpus. Although most of this line has not been even tested mostly due to the problem of computational complexity of training NPLM, [43] applied this to MERT process which reranks the n-best lists using NPLM. This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35]. This category of tasks employs the sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding) [27, 44, 33] on Conditional Random Fields (CRFs) / Markov Random Fields (MRFs).\nAlthough this paper discusses an ngram-HMM language model which we introduce as one model of NPLM where we borrow many of the mechanism from infinite HMM [19] and hierarchical PitmanYor LM [48], one main contribution would be to show one new application area of NPLM in SMT. Although several applications of NPLM have been presented, there have been no application to the task of system combination as far as we know.\nThe remainder of this paper is organized as follows. Section 2 describes ngram-HMM language model while Section 3 introduces a joint space model of ngram-HMM language model. In Section 4, our intrinsic experimental results are presented, while in Section 5 our extrinsic experimental results are presented. We conclude in Section 5."}, {"heading": "2 Ngram-HMM Language Model", "text": "Generative model Figure 1 depicted an example of ngram-HMM language model, i.e. 4-gramHMM language model in this case, in blue (in the center). We consider a Hidden Markov Model (HMM) [40, 21, 2] of size K which emits n-gram word sequence wi, . . . , wi\u2212K+1 where hi, . . . , hi\u2212K+1 denote corresponding hidden states. The arcs from wi\u22123 to wi, \u00b7 \u00b7 \u00b7 , wi\u22121 to wi show the back-off relations appeared in language model smoothing, such as Kneser-Ney smoothing [26], Good-Turing smoothing [24], and hierarchical Pitman-Yor LM smoothing [48].\nIn the left side in Figure 1, we place one Dirichlet Process prior DP(\u03b1,H), with concentration parameter \u03b1 and base measure H , for the transition probabilities going out from each hidden state. This construction is borrowed from the infinite HMM [2, 19]. The observation likelihood for the hidden word ht are parameterized as in wt|ht \u223c F (\u03c6ht) since the hidden variables of HMM is limited in its representation power where \u03c6ht denotes output parameters. This is since the observations can be regarded as being generated from a dynamic mixture model [19] as in (1), the Dirichlet priors\non the rows have a shared parameter.\np(wi|hi\u22121 = k) =\nK\u2211\nhi=1\np(hi|hi\u22121 = k)p(wi|hi)\n=\nK\u2211\nhi=1\n\u03c0k,hip(wi|\u03c6hi) (1)\nIn the right side in Figure 1, we place Pitman-Yor prior PY, which has advantage in its power-law behavior as our target is NLP, as in (2):\nwi|w1:i\u22121 \u223c PY(di, \u03b8i, Gi) (2)\nwhere \u03b1 is a concentration parameter, \u03b8 is a strength parameter, and Gi is a base measure. This construction is borrowed from hierarchical Pitman-Yor language model [48].\nInference We compute the expected value of the posterior distribution of the hidden variables with a beam search [19]. This blocked Gibbs sampler alternate samples the parameters (transition matrix, output parameters), the state sequence, hyper-parameters, and the parameters related to language model smoothing. As is mentioned in [19], this sampler has characteristic in that it adaptively truncates the state space and run dynamic programming as in (3):\np(ht|w1:t, u1:t) = p(wt|ht) \u2211\nht\u22121:ut<\u03c0 (ht\u22121,ht)\np(ht\u22121|w1:t\u22121, u1:t\u22121) (3)\nwhere ut is only valid if this is smaller than the transition probabilities of the hidden word sequence h1, . . . , hK . Note that we use an auxiliary variable ui which samples for each word in the sequence from the distribution ui \u223c Uniform(0, \u03c0 (hi\u22121,hi)). The implementation of the beam sampler consists of preprocessing the transition matrix \u03c0 and sorting its elements in descending order.\nInitialization First, we obtain the parameters for hierarchical Pitman-Yor process-based language model [48, 23], which can be obtained using a block Gibbs sampling [32].\nSecond, in order to obtain a better initialization value h for the above inference, we perform the following EM algorithm instead of giving the distribution of h randomly. This EM algorithm incorporates the above mentioned truncation [19]. In the E-step, we compute the expected value of the posterior distribution of the hidden variables. For every position hi, we send a forward message \u03b1(hi\u2212n+1:i\u22121) in a single path from the start to the end of the chain (which is the standard forward recursion in HMM; Hence we use \u03b1). Here we normalize the sum of \u03b1 considering the truncated variables ui\u2212n+1:i\u22121.\n\u03b1(hi\u2212n+2:i) = \u2211 \u03b1(hi\u2212n+1:i\u22121)\u2211 \u03b1(ui\u2212n+1:i\u22121) P (wi|hi) \u2211 \u03b1(ui\u2212n+1:i\u22121)P (hi|hi\u2212n+1:i\u22121) (4)\nThen, for every position hj , we send a message \u03b2(hi\u2212n+2:i, hj) in multiple paths from the start to the end of the chain as in (5),\n\u03b2(hi\u2212n+2:i, hj) = \u2211 \u03b1(hi\u2212n+1:i\u22121)\u2211 \u03b1(ui\u2212n+1:i\u22121) P (wi|hi) \u2211 \u03b2(hi\u2212n+1:i\u22121, hj)P (hi|hi\u2212n+1:i\u22121) (5)\nThis step aims at obtaining the expected value of the posterior distribution (Similar construction to use expectation can be seen in factored HMM [22]). In the M-step, using this expected value of the posterior distribution obtained in the E-step to evaluate the expectation of the logarithm of the complete-data likelihood."}, {"heading": "3 Joint Space Model", "text": "In this paper, we mechanically introduce a joint space model. Other than the ngram-HMM language model obtained in the previous section, we will often encounter the situation where we have another hidden variables h1 which is irrelevant to h0 which is depicted in Figure 2. Suppose that we have\nthe ngram-HMM language model yielded the hidden variables suggesting semantic and syntactical role of words. Adding to this, we may have another hidden variables suggesting, say, a genre ID. This genre ID can be considered as the second context which is often not closely related to the first context. This also has an advantage in this mechanical construction that the resulted languagemodel often has the perplexity smaller than the original ngram-HMM language model. Note that we do not intend to learn this model jointly using the universal criteria, but we just concatenate the labels by different tasks on the same sequence. By this formulation, we intend to facilitate the use of this language model.\nIt is noted that those two contexts may not be derived in a single learning algorithm. For example, language model with the sentence context may be derived in the same way with that with the word context. In the above example, a hidden semantics over sentence is not a sequential object. Hence, this can be only considering all the sentence are independent. Then, we can obtain this using, say, LDA."}, {"heading": "4 Intrinsic Evaluation", "text": "We compared the perplexity of ngram-HMMLM (1 feature), ngram-HMMLM (2 features, the same as in this paper and genre ID is 4 class), modified Kneser-Ney smoothing (irstlm) [18], and hierarchical Pitman Yor LM [48]. We used news2011 English testset. We trained LM using Europarl."}, {"heading": "5 Extrinsic Evaluation: Task of System Combination", "text": "We applied ngram-HMM language model to the task of system combination. For given multiple Machine Translation (MT) outputs, this task essentially combines the best fragments among given MT outputs to recreate a new MT output. The standard procedure consists of three steps: Minimum Bayes Risk decoding, monolingual word alignment, and monotonic consensus decoding. Although these procedures themselves will need explanations in order to understand the following, we keep the main text in minimum, moving some explanations (but not sufficient) in appendices. Note that although this experiment was done using the ngram-HMM language model, any NPLM may be sufficient for this purpose. In this sense, we use the term NPLM instead of ngram-HMM language model.\nFeatures in Joint Space The first feature of NPLM is the semantically and syntactically similar words of roles, which can be derived from the original NPLM. We introduce the second feature in this paragraph, which is a genre ID.\nThe motivation to use this feature comes from the study of domain adaptation for SMT where it becomes popular to consider the effect of genre in testset. This paper uses Latent Dirichlet Allocation\n(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset. And then, we place these labels on a joint space.\nLDA represents topics as multinomial distributions over theW unique word-types in the corpus and represents documents as a mixture of topics. Let C be the number of unique labels in the corpus. Each label c is represented by a W -dimensional multinomial distribution \u03c6c over the vocabulary. For document d, we observe both the words in the document w(d) as well as the document labels c(d). Given the distribution over topics \u03b8d, the generation of words in the document is captured by the following generative model.The parameters \u03b1 and \u03b2 relate to the corpus level, the variables \u03b8d belong to the document level, and finally the variables zdn and wdn correspond to the word level, which are sampled once for each word in each document.\nUsing topic modeling in the second step, we propose the overall algorithm to obtain genre IDs for testset as in (5).\n1. Fix the number of clusters C, we explore values from small to big where the optimal value will be searched on tuning set.\n2. Do unsupervised document classification (or LDA) on the source side of the tuning and test sets.\n(a) For each label c \u2208 {1, . . .C}, sample a distribution over word-types \u03c6c \u223c Dirichlet(\u00b7|\u03b2)\n(b) For each document d \u2208 {1, . . . , D}\ni. Sample a distribution over its observed labels \u03b8d \u223c Dirichlet(\u00b7|\u03b1) ii. For each word i \u2208 {1, . . . , NWd }\nA. Sample a label z (d) i \u223c Multinomial(\u03b8d) B. Sample a word w (d) i \u223c Multinomial(\u03c6c) from the label c = z (d) i\n3. Separate each class of tuning and test sets (keep the original index and new index in the allocated separated dataset).\n4. (Run system combination on each class.)\n5. (Reconstruct the system combined results of each class preserving the original index.)\nModified Process in System Combination Given a joint space of NPLM, we need to specify in which process of the task of system combination among three processes use this NPLM. We only discuss here the standard system combination using confusion-network. This strategy takes the following three steps (Very brief explanation of these three is available in Appendix):\n\u2022 Minimum Bayes Risk decoding [28] (with Minimum Error Rate Training (MERT) process [34])\nE\u0302MBRbest = argminE\u2032\u2208ER(E \u2032) = argminE\u2032\u2208E\n\u2211\nE\u2032\u2208EE\nL(E,E\u2032)P (E|F )\n= argminE\u2032\u2208E \u2211\nE\u2032\u2208EE\n(1 \u2212BLEUE(E \u2032))P (E|F )\n\u2022 Monolingual word alignment\n\u2022 (Monotone) consensus decoding (with MERT process)\nEbest = argmax e\nI\u220f\ni=1\n\u03c6(i|e\u0304i)pLM (e)\nSimilar to the task of n-best reranking in MERT process [43], we consider the reranking of nbest lists in the third step of above, i.e. (monotone) consensus decoding (with MERT process). We do not discuss the other two processes in this paper.\nOn one hand, we intend to use the first feature of NPLM, i.e. the semantically and syntactically similar role of words, for paraphrases. The n-best reranking in MERT process [43] alternate the\nprobability suggested by word sense disambiguation task using the feature of NPLM, while we intend to add a sentence which replaces the words using NPLM. On the other hand, we intend to use the second feature of NPLM, i.e. the genre ID, to split a single system combination system into multiple system combination systems based on the genre ID clusters. In this perspective, the role of these two feature can be seen as independent. We conducted four kinds of settings below.\n(A) \u2014First Feature: N-Best Reranking in Monotonic Consensus Decoding without Noise \u2013 NPLM plain In the first setting for the experiments, we used the first feature without considering noise. The original aim of NPLM is to capture the semantically and syntactically similar words in a way that a latent word depends on the context. We will be able to get variety of words if we condition on the fixed context, which would form paraphrases in theory.\nWe introduce our algorithm via a word sense disambiguation (WSD) task which selects the right disambiguated sense for the word in question. This task is necessary due to the fact that a text is natively ambiguous accommodating with several different meanings. The task of WSD [14] can be written as in (6):\nP (synseti|featuresi, \u03b8) = 1\nZ(features)\n\u220f\nm\ng(synseti, k) f(featureki ) (6)\nwhere k ranges over all possible features, f(featureki ) is an indicator function whose value is 1 if the feature exists, and 0 otherwise, g(synseti, k) is a parameter for a given synset and feature, \u03b8 is a collection of all these parameters in g(synseti, k), and Z is a normalization constant. Note that we use the term \u201csynset\u201d as an analogy of the WordNet [30]: this is equivalent to \u201csense\u201d or \u201cmeaning\u201d. Note also that NPLM will be included as one of the features in this equation. If features include sufficient statistics, a task of WSD will succeed. Otherwise, it will fail. We do reranking of the outcome of this WSD task.\nOn the one hand, the paraphrases obtained in this way have attractive aspects that can be called \u201ca creative word\u201d [50]. This is since the traditional resource that can be used when building a translation model by SMT are constrained on parallel corpus. However, NPLM can be trained on huge monolingual corpus. On the other hand, unfortunately in practice, the notorious training time of NPLM only allows us to use fairly small monolingual corpus although many papers made an effort to reduce it [31]. Due to this, we cannot ignore the fact that NPLM trained not on a huge corpus may be affected by noise. Conversely, we have no guarantee that such noise will be reduced if we train NPLM on a huge corpus. It is quite likely that NPLM has a lot of noise for small corpora. Hence, this paper also needs to provide the way to overcome difficulties of noisy data. In order to avoid this difficulty, we limit the paraphrase only when it includes itself in high probability.\n(B)\u2014 First Feature: N-Best Reranking in Monotonic Consensus Decoding with Noise \u2013 NPLM dep In the second setting for our experiment, we used the first feature considering noise. Although we modified a suggested paraphrase without any intervention in the above algorithm, it is also possible to examine whether such suggestion should be adopted or not. If we add paraphrases and the resulted sentence has a higher score in terms of the modified dependency score [39] (See Figure 3), this means that the addition of paraphrases is a good choice. If the resulted score decreases, we do not need to add them. One difficulty in this approach is that we do not have a reference which allows us to score it in the usual manner. For this reason, we adopt the naive way to deploy the above and we deploy this with pseudo references. (This formulation is equivalent that we decode these inputs by MBR decoding.) First, if we add paraphrases and the resulted sentence does not have a very bad score, we add these paraphrases since these paraphrase are not very bad (naive way). Second, we do scoring between the sentence in question with all the other candidates (pseudo references) and calculate an average of them. Thus, our second algorithm is to select a paraphrase which may not achieve a very bad score in terms of the modified dependency score using NPLM.\n(C) \u2014 Second Feature: Genre ID \u2014 DA (Domain Adaptation) In the third setting of our experiment, we used only the second feature. As is mentioned in the explanation about this feature, we intend to splits a single module of system combination into multiple modules of system combi-\nnation according to the genre ID. Hence, we will use the module of system combination tuned for the specific genre ID, 1.\n(D) \u2014 First and Second Feature \u2014 COMBINED In the fourth setting we used both features. In this setting, (1) we used modules of system combination which are tuned for the specific genre ID, and (2) we prepared NPLM whose context can be switched based on the specific genre of the sentence in test set. The latter was straightforward since these two features are stored in joint space in our case.\nExperimental Results ML4HMT-2012 provides four translation outputs (s1 to s4) which are MT outputs by two RBMT systems, APERTIUM and LUCY, PB-SMT (MOSES) and HPB-SMT (MOSES), respectively. The tuning data consists of 20,000 sentence pairs, while the test data consists of 3,003 sentence pairs.\nOur experimental setting is as follows. We use our system combination module [16, 17, 35], which has its own language modeling tool, MERT process, and MBR decoding. We use the BLEU metric as loss function in MBR decoding. We use TERP2 as alignment metrics in monolingual word alignment. We trained NPLM using 500,000 sentence pairs from English side of EN-ES corpus of EUROPARL3.\nThe results show that the first setting of NPLM-based paraphrased augmentation, that is NPLM plain, achieved 25.61 BLEU points, which lost 0.39 BLEU points absolute over the standard system combination. The second setting, NPLM dep, achieved slightly better results of 25.81 BLEU points, which lost 0.19 BLEU points absolute over the standard system combination. Note that the baseline achieved 26.00 BLEU points, the best single system in terms of BLEU was s4 which achieved 25.31 BLEU points, and the best single system in terms of METEOR was s2 which achieved 0.5853. The third setting achieved 26.33 BLEU points, which was the best among our four settings. The fourth setting achieved 25.95, which is again lost 0.05 BLEU points over the standard system combination.\nOther than our four settings where these settings differ which features to use, we run several different settings of system combination in order to understand the performance of four settings. Standard system combination using BLEU loss function (line 5 in Table 2), standard system combination using TER loss function (line 6), system combination whose backbone is unanamously taken from the RBMT outputs (MT input s2 in this case; line 11), and system combination whose backbone is selected by the modified dependency score (which has three variations in the figure; modDep preci-\n1E.g., we translate newswire with system combination module tuned with newswire tuning set, while we translate medical text with system combination module tuned with medical text tuning set.\n2http://www.cs.umd.edu/\u223csnover/terp 3http://www.statmt.org/europarl\nsion, recall and Fscore; line 12, 13 and 14). One interesting characteristics is that the s2 backbone (line 11) achieved the best score among all of these variations. Then, the score of the modified dependency measure-selected backbone follows. From these runs, we cannot say that the runs related to NPLM, i.e. (A), (B) and (D), were not particularly successful. The possible reason for this was that our interface with NPLM was only limited to paraphrases, which was not very successfuly chosen by reranking.\nConclusion and Perspectives\nThis paper proposes a non-parametric Bayesian way to interpret NPLM, which we call ngramHMM language model. Then, we add a small extension to this by concatenating other context in the same model, which we call a joint space ngram-HMM language model. The main issues investigated in this paper was an application of NPLM in bilingual NLP, specifically Statistical Machine Translation (SMT). We focused on the perspectives that NPLM has potential to open the possibility to complement potentially \u2018huge\u2019 monolingual resources into the \u2018resource-constraint\u2019 bilingual resources. We compared our proposed algorithms and others. One discovery was that when we use a fairly small NPLM, noise reduction may be one way to improve the quality. In our case, the noise reduced version obtained 0.2 BLEU points better.\nFurther work would be to apply this NPLM in various other tasks in SMT: word alignment, hierarchical phrase-based decoding, and semantic incorporated MT systems in order to discover the merit of \u2018depth\u2019 of architecture in Machine Learning."}], "references": [{"title": "Computing consensus translation from multiple machine translation systems", "author": ["S. BANGALORE", "G. BORDEL", "G. RICCARDI"], "venue": "In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Variational algorithms for approximate bayesian inference", "author": ["M.J. BEAL"], "venue": "PhD Thesis at Gatsby Computational Neuroscience Unit, University College London", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A neural probabilistic language model", "author": ["Y. BENGIO", "R. DUCHARME", "P. VINCENT"], "venue": "In Proceedings of Neural Information Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Neural probabilistic language models", "author": ["Y. BENGIO", "H. SCHWENK", "SEN\u00c9CAL", "J.-S", "F. MORIN", "GAUVAIN", "J.-L"], "venue": "Innovations in Machine Learning: Theory and Applications Edited", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Latent dirichlet allocation", "author": ["D. BLEI", "A.Y. NG", "M.I. JORDAN"], "venue": "Journal of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Introduction to probabilistic topic models", "author": ["D.M. BLEI"], "venue": "Communications of the ACM", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Towards open-text semantic parsing via multi-task learning of structured embeddings", "author": ["A. BORDES", "X. GLOROT", "J. WESTON", "Y. BENGIO"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S. CHEN", "J. GOODMAN"], "venue": "Technical report TR-10-98 Harvard University", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Deep learning for efficient discriminative parsing", "author": ["R. COLLOBERT"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. COLLOBERT", "J. WESTON"], "venue": "In International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. COLLOBERT", "J. WESTON", "L. BOTTOU", "M. KARLEN", "K. KAVUKCUOGLU", "P. KUKSA"], "venue": "Journal of Machine Learning Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Fast consensus decoding over translation forests", "author": ["J. DENERO", "D. CHIANG", "K. KNIGHT"], "venue": "In proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "The latent words language model", "author": ["K. DESCHACHT", "J.D. BELDER", "MOENS", "M.-F"], "venue": "Computer Speech and Language", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "MaTrEx: the DCU MT System for WMT", "author": ["DU J", "HE Y", "PENKALE S", "WAY"], "venue": "In Proceedings of the Third EACL Workshop on Statistical Machine Translation", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "An incremental three-pass system combination framework by combining multiple hypothesis alignment methods", "author": ["DU J", "WAY"], "venue": "International Journal of Asian Language Processing 20,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Using terp to augment the system combination for smt", "author": ["DU J", "WAY"], "venue": "In Proceedings of the Ninth Conference of the Association for Machine Translation", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Irstlm: an open source toolkit for handling large scale language models", "author": ["M. FEDERICO", "N. BERTOLDI", "M. CETTOLO"], "venue": "Proceedings of Interspeech", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "The infinite hmm for unsupervised pos tagging", "author": ["J.V. GAEL", "A. VLACHOS", "Z. GHAHRAMANI"], "venue": "The 2009 Conference on Empirical Methods on Natural Language Processing (EMNLP", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Lossless compression based on the sequence memoizer", "author": ["J. GASTHAUS", "F. WOOD", "TEH", "Y. W"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "An introduction to hidden markov models and bayesian networks", "author": ["Z. GHAHRAMANI"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence 15,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Factorial hidden markov models", "author": ["Z. GHAHRAMANI", "M.I. JORDAN", "P. SMYTH"], "venue": "Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Contextual dependencies in unsupervised word segmentation", "author": ["S. GOLDWATER", "T.L. GRIFFITHS", "M. JOHNSON"], "venue": "In Proceedings of Conference on Computational Linguistics / Association for Computational Linguistics", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "The population frequencies of species and the estimation of population paramters", "author": ["I.J. GOOD"], "venue": "Biometrika 40,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1953}, {"title": "Distributed representations. Parallel Distributed Processing: Explorations in the Microstructure of Cognition(Edited by D.E", "author": ["G.E. HINTON", "J.L. MCCLELLAND", "D. RUMELHART"], "venue": "Rumelhart and J.L. McClelland) MIT Press", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1986}, {"title": "Improved backing-off for n-gram language modeling", "author": ["KNESER R", "NEY"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Probabilistic graphical models: Principles and techniques", "author": ["D. KOLLER", "N. FRIEDMAN"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Minimum Bayes-Risk word alignment of bilingual texts", "author": ["S. KUMAR", "W. BYRNE"], "venue": "In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Computing consensus translation frommultiple machine translation systems using enhanced hypotheses alignment", "author": ["E. MATUSOV", "N. UEFFING", "NEY"], "venue": "In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. MILLER"], "venue": "Communications of the ACM 38,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1995}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["MNIH A", "TEH", "W. Y"], "venue": "In Proceedings of the International Conference on Machine Learning", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Bayesian unsupervised word segmentation with nested pitman-yor language modeling", "author": ["D. MOCHIHASHI", "T. YAMADA", "N. UEDA"], "venue": "In Proceedings of Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Machine learning: A probabilistic perspective", "author": ["K.P. MURPHY"], "venue": "The MIT Press", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models", "author": ["OCH F", "NEY"], "venue": "Computational Linguistics 29,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Minimum bayes risk decoding with enlarged hypothesis space in system combination", "author": ["T. OKITA", "J. VAN GENABITH"], "venue": "In Proceedings of the 13th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Hierarchical pitman-yor language model in machine translation", "author": ["OKITA T", "WAY"], "venue": "In Proceedings of the International Conference on Asian Language Processing (IALP", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "AND WAY, A. Pitman-Yor process-based language model for Machine Translation", "author": ["T. OKITA"], "venue": "International Journal on Asian Language Processing 21,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "AND WAY, A. Given bilingual terminology in statistical machine translation: Mwe-sensitve word alignment and hierarchical pitman-yor processbased translation model smoothing", "author": ["T. OKITA"], "venue": "In Proceedings of the 24th International Florida Artificial Intelligence Research Society Conference", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Evaluating machine translation with LFG dependencies", "author": ["K. OWCZARZAK", "J. VAN GENABITH", "WAY"], "venue": "Machine Translation 21,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. RABINER"], "venue": "Proceedings of the IEEE 77,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1989}, {"title": "Continuous space language models", "author": ["H. SCHWENK"], "venue": "Computer Speech and Language", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Continuous space language models for statistical machine translation", "author": ["H. SCHWENK"], "venue": "The Prague Bulletin of Mathematical Linguistics", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Large, pruned or continuous space language models on a gpu for statistical machine translation", "author": ["H. SCHWENK", "A. ROUSSEAU", "M. ATTIK"], "venue": "In Proceeding of the NAACL workshop on the Future of Language Modeling", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Approximate inference in graphical models using LP relaxations", "author": ["D. SONTAG"], "venue": "Massachusetts Institute of Technology (Ph.D. thesis)", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "The complexity of inference in latent dirichlet allocation", "author": ["SONTAG D", "ROY", "M. D"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Probabilistic topic models", "author": ["M. STEYVERS", "T. GRIFFITHS"], "venue": "Handbook of Latent Semantic Analysis. Psychology", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "SRILM \u2013 An extensible language modeling toolkit", "author": ["A. STOLCKE"], "venue": "In Proceedings of the International Conference on Spoken Language Processing", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2002}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["TEH Y. W"], "venue": "In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL-06),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2006}, {"title": "Lattice minimum bayes-risk decoding for statistical machine translation", "author": ["R. TROMBLE", "S. KUMAR", "F. OCH", "W. MACHEREY"], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2008}, {"title": "Exploding the creativity myth: The computational foundations of linguistic creativity. London: Bloomsbury", "author": ["T. VEALE"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "A stochastic memoizer for sequence data", "author": ["F. WOOD", "C. ARCHAMBEAU", "J. GASTHAUS", "L. JAMES", "TEH", "Y. W"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 45, "endOffset": 51}, {"referenceID": 3, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 45, "endOffset": 51}, {"referenceID": 23, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 88, "endOffset": 92}, {"referenceID": 45, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 169, "endOffset": 173}, {"referenceID": 24, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 209, "endOffset": 220}, {"referenceID": 7, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 209, "endOffset": 220}, {"referenceID": 46, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 209, "endOffset": 220}, {"referenceID": 46, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 156, "endOffset": 160}, {"referenceID": 49, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 188, "endOffset": 196}, {"referenceID": 18, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 188, "endOffset": 196}, {"referenceID": 34, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 230, "endOffset": 242}, {"referenceID": 35, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 230, "endOffset": 242}, {"referenceID": 36, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 230, "endOffset": 242}, {"referenceID": 2, "context": "A NPLM of [3] implemented this using the multi-layer neural network and yielded 20% to 35% better perplexity than the language model with the modified Kneser-Ney methods [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "A NPLM of [3] implemented this using the multi-layer neural network and yielded 20% to 35% better perplexity than the language model with the modified Kneser-Ney methods [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 39, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 9, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 40, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 8, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 10, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 12, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 41, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 10, "context": "First, one category of applications include POS tagging, NER tagging, and parsing [12, 7].", "startOffset": 82, "endOffset": 89}, {"referenceID": 6, "context": "First, one category of applications include POS tagging, NER tagging, and parsing [12, 7].", "startOffset": 82, "endOffset": 89}, {"referenceID": 10, "context": "Second, the other category of applications include Semantic Role Labeling (SRL) task [12, 14].", "startOffset": 85, "endOffset": 93}, {"referenceID": 12, "context": "Second, the other category of applications include Semantic Role Labeling (SRL) task [12, 14].", "startOffset": 85, "endOffset": 93}, {"referenceID": 40, "context": "Third, the final category includes MERT process [42]", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 47, "endOffset": 63}, {"referenceID": 8, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 47, "endOffset": 63}, {"referenceID": 10, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 47, "endOffset": 63}, {"referenceID": 12, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 47, "endOffset": 63}, {"referenceID": 48, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 214, "endOffset": 218}, {"referenceID": 41, "context": "Although most of this line has not been even tested mostly due to the problem of computational complexity of training NPLM, [43] applied this to MERT process which reranks the n-best lists using NPLM.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 27, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 47, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 13, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 11, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 33, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 25, "context": "This category of tasks employs the sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding) [27, 44, 33] on Conditional Random Fields (CRFs) / Markov Random Fields (MRFs).", "startOffset": 117, "endOffset": 129}, {"referenceID": 42, "context": "This category of tasks employs the sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding) [27, 44, 33] on Conditional Random Fields (CRFs) / Markov Random Fields (MRFs).", "startOffset": 117, "endOffset": 129}, {"referenceID": 31, "context": "This category of tasks employs the sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding) [27, 44, 33] on Conditional Random Fields (CRFs) / Markov Random Fields (MRFs).", "startOffset": 117, "endOffset": 129}, {"referenceID": 17, "context": "Although this paper discusses an ngram-HMM language model which we introduce as one model of NPLM where we borrow many of the mechanism from infinite HMM [19] and hierarchical PitmanYor LM [48], one main contribution would be to show one new application area of NPLM in SMT.", "startOffset": 154, "endOffset": 158}, {"referenceID": 46, "context": "Although this paper discusses an ngram-HMM language model which we introduce as one model of NPLM where we borrow many of the mechanism from infinite HMM [19] and hierarchical PitmanYor LM [48], one main contribution would be to show one new application area of NPLM in SMT.", "startOffset": 189, "endOffset": 193}, {"referenceID": 38, "context": "We consider a Hidden Markov Model (HMM) [40, 21, 2] of size K which emits n-gram word sequence wi, .", "startOffset": 40, "endOffset": 51}, {"referenceID": 19, "context": "We consider a Hidden Markov Model (HMM) [40, 21, 2] of size K which emits n-gram word sequence wi, .", "startOffset": 40, "endOffset": 51}, {"referenceID": 1, "context": "We consider a Hidden Markov Model (HMM) [40, 21, 2] of size K which emits n-gram word sequence wi, .", "startOffset": 40, "endOffset": 51}, {"referenceID": 24, "context": "The arcs from wi\u22123 to wi, \u00b7 \u00b7 \u00b7 , wi\u22121 to wi show the back-off relations appeared in language model smoothing, such as Kneser-Ney smoothing [26], Good-Turing smoothing [24], and hierarchical Pitman-Yor LM smoothing [48].", "startOffset": 140, "endOffset": 144}, {"referenceID": 22, "context": "The arcs from wi\u22123 to wi, \u00b7 \u00b7 \u00b7 , wi\u22121 to wi show the back-off relations appeared in language model smoothing, such as Kneser-Ney smoothing [26], Good-Turing smoothing [24], and hierarchical Pitman-Yor LM smoothing [48].", "startOffset": 168, "endOffset": 172}, {"referenceID": 46, "context": "The arcs from wi\u22123 to wi, \u00b7 \u00b7 \u00b7 , wi\u22121 to wi show the back-off relations appeared in language model smoothing, such as Kneser-Ney smoothing [26], Good-Turing smoothing [24], and hierarchical Pitman-Yor LM smoothing [48].", "startOffset": 215, "endOffset": 219}, {"referenceID": 1, "context": "This construction is borrowed from the infinite HMM [2, 19].", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "This construction is borrowed from the infinite HMM [2, 19].", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "This is since the observations can be regarded as being generated from a dynamic mixture model [19] as in (1), the Dirichlet priors", "startOffset": 95, "endOffset": 99}, {"referenceID": 46, "context": "This construction is borrowed from hierarchical Pitman-Yor language model [48].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Inference We compute the expected value of the posterior distribution of the hidden variables with a beam search [19].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "As is mentioned in [19], this sampler has characteristic in that it adaptively truncates the state space and run dynamic programming as in (3):", "startOffset": 19, "endOffset": 23}, {"referenceID": 46, "context": "Initialization First, we obtain the parameters for hierarchical Pitman-Yor process-based language model [48, 23], which can be obtained using a block Gibbs sampling [32].", "startOffset": 104, "endOffset": 112}, {"referenceID": 21, "context": "Initialization First, we obtain the parameters for hierarchical Pitman-Yor process-based language model [48, 23], which can be obtained using a block Gibbs sampling [32].", "startOffset": 104, "endOffset": 112}, {"referenceID": 30, "context": "Initialization First, we obtain the parameters for hierarchical Pitman-Yor process-based language model [48, 23], which can be obtained using a block Gibbs sampling [32].", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "This EM algorithm incorporates the above mentioned truncation [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "This step aims at obtaining the expected value of the posterior distribution (Similar construction to use expectation can be seen in factored HMM [22]).", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "We compared the perplexity of ngram-HMMLM (1 feature), ngram-HMMLM (2 features, the same as in this paper and genre ID is 4 class), modified Kneser-Ney smoothing (irstlm) [18], and hierarchical Pitman Yor LM [48].", "startOffset": 171, "endOffset": 175}, {"referenceID": 46, "context": "We compared the perplexity of ngram-HMMLM (1 feature), ngram-HMMLM (2 features, the same as in this paper and genre ID is 4 class), modified Kneser-Ney smoothing (irstlm) [18], and hierarchical Pitman Yor LM [48].", "startOffset": 208, "endOffset": 212}, {"referenceID": 4, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 44, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 5, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 43, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 31, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 26, "context": "\u2022 Minimum Bayes Risk decoding [28] (with Minimum Error Rate Training (MERT) process [34])", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "\u2022 Minimum Bayes Risk decoding [28] (with Minimum Error Rate Training (MERT) process [34])", "startOffset": 84, "endOffset": 88}, {"referenceID": 41, "context": "Similar to the task of n-best reranking in MERT process [43], we consider the reranking of nbest lists in the third step of above, i.", "startOffset": 56, "endOffset": 60}, {"referenceID": 41, "context": "The n-best reranking in MERT process [43] alternate the", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "The task of WSD [14] can be written as in (6):", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "Note that we use the term \u201csynset\u201d as an analogy of the WordNet [30]: this is equivalent to \u201csense\u201d or \u201cmeaning\u201d.", "startOffset": 64, "endOffset": 68}, {"referenceID": 48, "context": "On the one hand, the paraphrases obtained in this way have attractive aspects that can be called \u201ca creative word\u201d [50].", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "On the other hand, unfortunately in practice, the notorious training time of NPLM only allows us to use fairly small monolingual corpus although many papers made an effort to reduce it [31].", "startOffset": 185, "endOffset": 189}, {"referenceID": 37, "context": "If we add paraphrases and the resulted sentence has a higher score in terms of the modified dependency score [39] (See Figure 3), this means that the addition of paraphrases is a good choice.", "startOffset": 109, "endOffset": 113}, {"referenceID": 37, "context": "Figure 3: By the modified dependency score [39], the score of these two sentences, \u201cJohn resigned yesterday\u201d and \u201cYesterday John resigned\u201d, are the same.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "We use our system combination module [16, 17, 35], which has its own language modeling tool, MERT process, and MBR decoding.", "startOffset": 37, "endOffset": 49}, {"referenceID": 15, "context": "We use our system combination module [16, 17, 35], which has its own language modeling tool, MERT process, and MBR decoding.", "startOffset": 37, "endOffset": 49}, {"referenceID": 33, "context": "We use our system combination module [16, 17, 35], which has its own language modeling tool, MERT process, and MBR decoding.", "startOffset": 37, "endOffset": 49}], "year": 2017, "abstractText": "A neural probabilistic language model (NPLM) provides an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially \u2018huge\u2019 monolingual resources into the \u2018resource-constraint\u2019 bilingual resources. We introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian construction. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show an experiment of system combination in the area of SMT. One discovery was that our treatment of noise improved the results 0.20 BLEU points if NPLM is trained in relatively small corpus, in our case 500,000 sentence pairs, which is often the case due to the long training time of NPLM.", "creator": "LaTeX with hyperref package"}}}