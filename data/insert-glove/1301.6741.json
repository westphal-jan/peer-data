{"id": "1301.6741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Practical Uses of Belief Functions", "abstract": "buildable We gar\u00e7ons present lulle examples where d'amor the use 5dn of belief 98.02 functions qvale provided sound s-cup and leons elegant moinian solutions oslobodenje to real brachialis life problems. foremothers These are danah essentially characterized by? supressed missing ' phumaphi information. fredrix The anglo-boer examples meiwa deal lapu with mweka 1) capa discriminant analysis using quarterhorse a amoud learning conveniens set p.a.o.k. where taddei classes 0421 are thinkfilm only partially known; unnao 2) 88.90 an information retrieval systems handling cha\u00eene inter - documents 2,004 relationships; dullness 3) the combination 1,876 of army data \u0142\u00f3d\u017a from beatallica sensors competent on partially pyrokinetic overlapping frames; 4) 51-7 the 6.98 determination of huayin the kddi number of \u05e8 sources in 1hr a multi - 3,435 sensor comedown environment eulima by tainos studying 10.57 the inter - sensors 9.63 contradiction. undriveable The purpose aloofness of the balcarres paper is to myrmecia report morgana on such applications halki where the humaid use of belief functions trots provides republica a cd-rs convenient pettus tool to air/fuel handle? messy ' data problems.", "histories": [["v1", "Wed, 23 Jan 2013 16:01:05 GMT  (354kb)", "http://arxiv.org/abs/1301.6741v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["philippe smets"], "accepted": false, "id": "1301.6741"}, "pdf": {"name": "1301.6741.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Philippe Smets"], "emails": [], "sections": [{"heading": null, "text": "Keywords: belief function, hints model, transferable belief model.\n1. INTRODUCTION.\nThe models proposed to represent quantified uncertainty are based on probability, possibility or belief functions. They are complementary, as they cover different forms of uncertainty (Smets, 1998a). We consider here only those based on belief functions, in particular the hint model and the transferable belief model (TBM).\nShowing one theory is better than another is often just impossible, as it requires a clear definition of 'better'. Better in what sense? Usually there are no definitive and absolute quality criteria, and most used criteria are either ad hoc or biased toward one theory.\nWhat can then be done in order to compare models? l. One can compare the underlying axioms and evaluate their respective adequacy and naturalness. Such axioms exist for each model, but there is no criterion that tells which one is really adequate. 2. One can compare the consequences of the various models and discard those leading to inadequate conclusions. But the conclusions to which they lead, are\nusually defendable, even when they don't agree, and there is no golden standard to select the 'winner'. 3. One can compare their abilities to solve small artificial delicate problems. But toy examples like the 3 prisoners, the 3 doors, the 'Peter, Paul and Mary Saga' don't lead to clear conclusions, as the merits of the solutions cannot be assessed definitively. 4. One can compare their usefulness in solving 'real' problems. This is what we try to do here by presenting some problems where the belief function approach was quite convenient.\nA very important point when modeling uncertainty is to be clear about what is modeled, an obvious preliminary step that some skip too easily. Both probability and belief functions based models represented the weighted opinion of an agent that the actual world belongs to a given set of possible worlds, or equivalently that a given proposition is true in the actual world. Thus there is something called the 'actual world' and it has to be made clear what is really meant before even applying any model. In the actual world, as considered here, every proposition is either true or false: there is no fuzziness (belief on fuzzy events has been defined, but is not considered here).\nIt would be nice to compare the results obtained with belief functions with those one could obtain with a probabilistic approach. Some comparisons are presented here. But we realize the difficulty encountered when trying such a comparison. In fact belief functions are used when some of the data needed for a probability analysis are missing. If all such data were available, they should be fed into the belief function analysis ... in which case the model reduces itself into the probability model. The whole argument about using belief functions centers on how the missing information is handled. Probabilists usually solve the problem by introducing some 'natural' assumptions like equi-probabilities, independence, or a modelization of the missing-ness. If these assumptions are 'close' to reality, the probability solution is often optimal, in which case using a belief functions approach is useless (we just hope that belief functions produce results almost as good as those obtained with probability functions). The real interest of the belief functions approach is to be found in its robustness to discrepancies between the assumptions and the reality. E.g., Appriou (1997) shows an analysis dealing with missile recognition where the belief functions approach was much more robust to these discrepancies than the probabilistic approach.\nBesides, comparing the two approaches is difficult, as there are many methods to handle the missing information in probability theory. Bad results observed with probability functions could then be explained either by a weakness of the probability approach or by an inadequate choice of the method. Deciding which one applies is delicate.\nWe describe four examples where belief functions models produce nice and efficient solutions. These examples are more or less 'real life' examples. We nevertheless limit our presentation to simplified illustrations; their generalization to large-scale applications is immediate. These examples were essentially developed by Thierry Denoeux, University of Compiegne, France (Denoeux, 1995), by Johan Schubert, Royal Institute of Technology, Stockholm, Sweden (Schubert, 1995), by Justin Picard, Universite de Neuchatel, Switzerland (Picard, 1998) and by Janez, ONERA, Paris, France, (Janez, 1996). Another application of the TBM is presented in the paper 'Assessing the value of a candidate' by Dubois et al. (1999, see in this proceeding). The first application is presented in some detail and includes a comparison with probabilistic approach. For lack of space, the other three examples are just shortly described. Their authors describe in full detail the methods used and their interest.\nThis paper only reports on the use of belief functions in a few real applications recently developed. In depth comparisons with other methods are still missing. Benchmark exercises should be organized.\n2. UNCERTAINTY AND BELIEF FUNCTIONS.\nShafer (1976) introduces a model to represent quantified beliefs based on so-called belief functions. Since, many new results have been obtained that we survey here. We neglect the computational issue: valuation based system, fast Mobius transform and approximation methods are detailed in (Gabbay and Smets, 1998-99, vol. 5).\nIn AI, Shafer's model was called 'Dempster-Shafer theory' (Gordon and Shortliffe, 1984). Unfortunately what this name covers varies widely from authors to authors (Smets, 1994). It can correspond to: 1. a lower probability model, 2 . Dempster's model derived from probability theory\n(Dempster, 1967) and represented by the hints theory of Kohlas (Kohlas and Monney, 1995), 3 . Shafer's model unrelated to probability theory (Shafer, 1977, 1992) and represented by the transferable belief model (Smets and Kennes, 1994, Smets 1997a, 1998). The confusion between these interpretations explains most errors encountered in the literature where authors analyze Shafer's ideas.\n2.1. The lower probability model.\nPractical Uses of Belief Functions 613\nLet a set Q, and let IT be a set of probability functions defined on Q. The lower probability of a subsets A of Q is defined as: P\u00b7(A) = min Pen P(A) for every A\ufffdQ. The P. function is also called the lower envelope of IT. Under certain weak constraints P. is a Choquet capacity monotone of order 2, and it might even be monotone of order infinite, in which case P. is a belief function. In all these cases, P\u2022 and IT are in one-to-one correspondence.\nThere are at least two ways to get IT. 1. There exist a P function on Q band the agent knows\nonly that P belongs to IT. It can also be obtained by studying betting behaviors and calling P\u2022(A) the maximal price the agent, called the player, would pay to a banker to enter a game where the player gets from the banker $1 if A occurs, and nothing otherwise (Walley, 1991). (The difference with the Bayesian definition is that the agent cannot be forced to be the banker). 2 . Beliefs are represented by families of probability functions that can be defined through their lower envelop p. (Kyburg, 1987, Voorbraak, 1993).\nAn important issue when developing a model to represent beliefs is to explain its behavior when new pieces of evidence are introduced like in the conditioning process. In the first interpretation, the solution is obvious: every probability function in IT is conditioned by Bayes rule on A\ufffdQ, and p. A is the lower envelop of this new set of probability functions (Jaffray, 1992). These results were often used to criticize Dempster's rule of conditioning, and indirectly Shafer's work. This comparison is inappropriate, as Dempster's rule of conditioning is not justified in this context. This approach is not further considered here after as belief functions are only marginally concerned.\n2.2. The theory of hints.\nHistorically, the use of belief functions was initiated by Dempster while justifying fiducial inference (Dempster, 1967, 1968). Today its most developed model is the hint theory of Kohlas and Monney (1995). They assume Dempster's ori\nB inal structure (Q, P, r. 8) where Q and e\nare two sets, P is a probability measure on Q and r is a one-to-many mapping from Q to e. The set e is the set of possible answers to a question whose answer is unknown. One and only one element of e is the correct answer to the question. 'The goal is to make assertions about the answer in the light of the available information. We assume that this information allows for several different interpretations, depending on some unknown circumstances. These interpretations are regrouped into the set Q and there is exactly one correct interpretation. Not all interpretations are equally likely and the known probability measure pil reflects our information in that respect. Furthermore, if the interpretation roe Q is the correct one, then the answer is known to be in the subset\n614 Smets\nf(w)\ufffde. Such a structure H = (Q, P, I', 8) is called a hint. .. An interpretation WE Q supports the hypothesis H if f(w)\ufffdH because in that case the answer is necessarily in H. The degree of support of H is defined as the probability of all supporting interpretation of H' (Kohlas and Mooney, 1995, page vi).\nThe hints theory is similar to the probability of provability theory (Ruspini, 1986, Pearl, 1988, Smets, 199la, 1993b), a theory that extends the domain of the probability functions from propositional logic to modal logic.\n2.3. The transferable belief model.\nShafer (1976) proposes to quantify beliefs with belief functions, instead of probability functions as classically used for that purpose. He introduces the model and both Dempster's rule of conditioning that correspond to the Bayes conditioning rule, and Dempster's rule of combination that correspond to the probability aggregation rule.\nFrom these ideas, we develop the TBM, a non probabilistic model for representing the quantified beliefs held by an agent.\nLet You denote the agent whose beliefs are considered, but it should be realized that an agent can also be a piece of equipment, a computer program, a sensor, etc ... Subjectivity is not essential. Your beliefs manifest themselves at two mental levels: the credal level where beliefs are entertained and represented by belief functions, and the pignistic level where beliefs are used to act and represented by probability functions.\nSuppose a frame of discernment Q on which Your beliefs are considered. One world in Q is the actual world, denoted w0. You can only express the strength of Your opinion, Your belief, that Wo belongs to this or that subset of Q. You allocate parts of Your belief to the fact that WoE A for every A\ufffdQ. The part of belief, denoted m(A), given to A\ufffdQ represents the part of Your belief that specifically supports the fact that oo0E A and no set more specific than A. The total amount of belief, denoted bel(B), that supports WoE B is obtained by adding the parts of belief m(A) given to the sets A, At0, A\ufffd.\nWhen You must take decisions, the belief held at the credal level, and represented by the basic belief assignment m defined on Q, induces a probability function at the 'pignistic' level, denoted BetP and also defined on Q. The transformation is called the pignistic transformation (Smets, 1989): m(A) 1 BetP(x) = L for every xE Q. A!;;;Q,xeA 1- m(0) I A I This probability function can be used in order to make decisions using expected utilities theory. Its justification\nis based on rationality requirements detailed in (Smets and Kennes, 1994)\nThe operational definition of a degree of belief is based on the agent's betting behaviors and its assessment is based on exchangeable bets just as it is done for subjective probabilities (Smets and Kennes, 1994)\nThe axiomatic of the TBM as a model to represent quantified beliefs is detailed in Smets (1993A, 1997a), (see also Wong et a!., 1990). Axiomatic justification for combination rules is given in (Smets, 1990, Dubois and Prade, 1986, Hajek, 1992, Klawonn and Schwecke 1992). The generalization of the Bayesian Theorem to the TBM is presented in Smets 1978, 1993b). Decision process based on lower expectations are explained in Strat, ( 1990) Jaffray (1989), whereas Wilson (1993) studies the properties of the pignistic probabilities and Smets (1993c) examines what become the pignistic transformations in a dynamic decision making context.\nRevision of beliefs by specialization are described in (Kruse and Schwecke, 1990, Klawonn and Smets, 1992) and several general combination rules have been developed (Dempster's rule of combination is hardly the only rule for combining two belief functions. There are many other rules, like the disjunctive rule of combination (Smets, 1993d), the a-combination rules (Smets, 1997b, the cautious combination rules, etc . . . ).\nPrinciples of information content have been developed. Measures extending entropy measure are detailed in (Klir and Wierman, 1998), whereas we develop a measure adapted to the TBM (Smets, 1983). Principle of minimal commitment that states 'never give more support to a set that necessary' replaces the maximum entropy principle used in probability theory (Hsia, 1991)\nClassically bel(A) quantifies 'I have good reason to believe A' and bel(A) is the strength of these 'good reasons'. In (Smets, 1995), we show how to represents concepts like 'good reasons not to believe', a concept similar in logic to the retraction a Ia Giirdenfors. Similarly, we can also express concepts like 'I still have some reasons to believe', and 'I still have some reasons not to believe'.\n2.4. Future developments.\nMany new developments have been achieved since Shafer's seminal work. Limiting oneself to the theory as presented in Shafer's book is no more acceptable. The distinction between the three interpretations for belief functions seems essential but it deserves further work to validate or invalidate it. Works on belief revisions - finding their nature and the adequate rules for representing their effect - are necessary. Dempster's rule of conditioning fits just one kind of revision. There are still open theoretical issues but it is obvious that real life applications are needed before the interest of the model\ncan be assessed. In Europe there are already quite a few applications under development. They usually concern problems where some information essential for a probability approach is missing and cannot be obtained. The way belief functions can adequately represent partial or total ignorance is usually acknowledge. Belief functions are used for pattern recognition, multi -sensor data fusion, diagnosis... A nice property of belief functions is that only what is known is used.\n3. THE TBM CLASSIFIER.\nDiscriminant analysis is probably the most classical tool used for classifying cases into one of several categories given the values of some measurement variables. Normally, we use a set of data, called the learning set (LS). For each case in LS, we know the values taken for each measurement variable and the classification variable that tells the class to which the case belongs. The classes are finite and unordered. Let Q denote the set of possible classes: Q = {CJ, Cz,oo., cnl\u00b7\nA learning set with N cases and p measurement variables is the set {(Cj, xu. x2;, oo\u2022 xp;): i = I , 2 ... N} where X; is the 'name' of the i'th case, c; is the class to which X; belongs, and Xji is the value of the measurement variable j for X;. The data of a new case, denoted X?. is collected, but the class to which X? belongs, denoted C?, is unknown. We want to predict the value of c? given the observed values of the measurement variables of X?\u00b7 Solutions to this problem are well established. One of them, called discriminant analysis, is fully described in most textbooks of statistics.\nLet us now suppose that instead of the ideal learning set LS as described here above, we have a learning set PKLS where the classes of the cases are only partially known. For instance suppose we only know that case X1 belongs either to CJ or Cz class, that case Xz does not belong to class CJo case X3 belongs either to Cz or cs or c7 class ... Can we adapt the discriminant analysis method to such 'messy' data case? In fact we face a problem of 'partially supervised learning'. For some cases, classes are known as in the supervised learning approach, for some cases, class in completely as in the unsupervised approach. But here we also have all the cases where we know partially their class. Probabilistic solutions could be based on: 1. a Bayesian approach where we assess for each case a\nprobability function that describes the class to which it belongs. We then allocate every case to a class (and get the probability to get that learning set), compute the needed parameters as in a supervised learning approach and average the results weighted by the probability of the learning sets. 2. a maximum likelihood approach where we estimate the unknown parameters, including the probability with which the case belong to a given class.\nPractical Uses of Belief Functions 615\n3 . an adaptation of cluster analysis where partial constraints are introduced that represent the knowledge about the class to which each case belong. Whatever method is used, the computational complexity is a serious problem and an adequate tuning of some parameters is not a small matter. The transferable belief model provides another approach that can handle elegantly and efficiently such a messy case. The method was invented by Denoeux (1995). We present results of the method - called the TBM classifier - and compare them with those obtained by the classical discriminant analysis applied to the same data base but using the exact value for the classes, a method that is then optimal. Details about these results are given in Denoeux (1995), Zouhal (1997), De Smet (1998).\n3.1. Discriminant Analysis with Partially Known Classes\nLet pkc; denote the subset of Q that represents what we know about the class to which case X; belongs. The learning set PKLS is now the set {(pkc;, xu. xz;, 000 Xp;): i = 1, 2 ... N}\nIntuitively the method can be described by an anthropomorphic model. Each case X; in PKLS is considered as an individual. Let c;o denoted the true class to which X; belongs. All X; knows about cw is that c;oE pkc; (Denoeux andZouhal ( 1999) generalizes to the case where this knowledge is represented by a belief function or possibility function on Q). Then X; looks at the unknown case and expresses 'his' belief bel; about C?. If X? is 'close' to X;, X; would defend that C? = c;o. As all what X; knows about c;o is that ewE pkc;, then all what X; can express about case X? is that c ?E pkc;. If X? is not 'close' to X;, X; cannot say anything about c;o.\nThis description is formalized as follows. X; can only states: case X? belongs to the same set of classes as myself, what is represented by a belief function with m;o(pkc;) = 1. Let d(X;,X?) be the 'distance' between X; and X?. If d(X;.X?) is small, then what X; stated is reliable, if d(X;,X?) is large, it is not reliable, the largest d(X;,X? ), the less reliable. The impact of this reliability is represented by a discounting on m;o into m;. So lllj(pkc;) = f(d(X;,X?)) and m;(Q) = 1-f(d(X;,X?)) where f(d)E [0,1] and is decreasing with d. Thus every case X; generates such a simple support function be\\ on Q that concerns the value of C?\u00b7\nConsider now what information X? collects. Case X7 receives all these simple support functions bel;, and combines them by Dempster's rule of combination into a new belief function bel on Q that represents the belief\n616 Smets\nheld by case X? about c? and induced by the collected belief functions beli:\nbel? = Etli;J...N beli. If a decision must be made on the value of C?, we build the pignistic probability BetP? on Q from bel7 by the application of the pignistic transformation (described and justified in Smets and Kennes, 1994) and use the classical expected utility theory in order to take the optimal decision.\nIn the comparison study presented here after (and done by Y. De Smet 1998), we use the next solutions. Each measurement variable in PKLS is linearly re-scaled so that their 5th percentile is 0 and their 95th percentile is l. So measurement variables share similar scales, and the method is robust to outliers.\nFor f, he uses: f(d) = max(l - a d, 0) with a>O. More elaborated formulas were useless. For d, he uses the D2 of Mahalanobis using a covariance matrix :Ei that depends on\nXi and which parameters are based on the cases in the neighborhood of Xi.\nDe Smet applied this approach to many sets of data. We present only six case studies. The quality criterion used in all comparisons is the classical PCC (percent of correct classification). The predicted class is always the class with the highest pignistic probability (the most probable class). Furthermore in every artificial case study, the pkc is never erroneous, i.e. the true class of Xi belongs always\nto pkci.\nCase Study 1. Isosceles triangle, AB/AC/BC. Suppose a two dimensional (p=2) trigroup classification problem with group labels A, B, C. Data in the three groups are normally distributed, their means are at the comer of an isosceles triangle with coordinate (0,0), ( 4,0) and (2,2), and the covariance matrix is the unit matrix. 200 cases are randomly generated in the 3 groups. Each set of 200 cases is split in two subsets of 100 cases, those in the first subsets having their label transformed into { A,B } , the others into { A,C}. The same is done for the other two groups. The learning set is made out of 20 A, 20 B, 20 C cases (randomly selected), the other case making the testing set. Table 1 presents the PCC obtained for 5 unrelated sets of data by the TBM -classifier (denoted TBM with pkc). For comparison purpose, we also present the PCC obtained by linear discriminant analysis, denoted DA, applied to the same data sets but using the true classes for the data in the learning set. Both methods produce similar results, an excellent result for what concerns the TBM-classifier. Indeed it only uses the partially known classes whereas the DA uses precisely known classes, a much richer information, and furthermore DA is the optimal method for these data as they satisfy exactly the requirements underlying the use of DA.\nCase Study 2. Collinear, A/B/AC-BC. Data are generated as in case study 1, except the means are collinear, located at ( -3,0), (3,Q) and (0,0). The pkc of the A cases is A, and the pkc of the B cases is B (there is no missing information for these two sets of data). The C cases where all classified are either {A,C} or {B,C}. The difficulty comes from the fact that the C cases are essentially located between the A and B cases. The results in table 2 support the conclusions of case study l .\nCase Study 3. 5 classes, in &5 We use p=5 and 5 classes, denoted A, B ... E. The mean of group 1 is located on the first axis at 2 .fi, for group 2, on the second axis at 2 .fi , etc ... The covariance matrix = I. The pkc are build as follows. Suppose a case in group 2 as indicated by the vector (0, 1, 0, 0, 0). Then for every 0 in the vector, we toss a fair coin: if heads we put a 1, if tails we leave the 0. Then the 1 's in the resulting vector indicate those subsets included in the pkc of that case. So if the end vector is (1, 1, 0, 1, 0), the pkc is {A, B, D}. The learning set is made out of 30 cases from each class. Even though the knowledge about the class was quite poor, the TBM-classifier provided results (see table 3) almost as good as the discriminant analysis approach applied on perfectly known classes (again optimal here). That the PCC with the TBM-classifier are lower is normal, no miracle can be expected, the TBM-classifier used a very imprecise information, and a method using more information should provided better results.\nCase Study 4. Triangle, No pkc. In order to see if the TBM-classifier behaves well when the classes are precisely known, we use the isosceles triangle of case study 1, with side length = 10, and a covariance matrix cr2J with cr2 varying from 10 to 50. The linear discriminant is against the optimal method in such a\ncase. Table 2 shows that the TBM-classifier performs as well as the linear discriminant method, whatever a2.\nCase Study 5. Collinear, No pkc. As in cased study 2, we use 3 groups with 100 cases per group and the means are collinear located at ( 10,0) (20,0) (30,0). With the covariance matrices LA = LB = Lc = 50.1, both the TBM and the linear discriminant classifiers produce PCC of 69%. When LA = LC = 10.1 and LB =\n('\ufffd \ufffd), the PCC are 73% for the TBM classifier and 51% for the linear discriminant classifier (which condition for optimality are unsatisfied here, but in real life it is not obvious to realize it and linear discriminant classifiers are often applied in such cases). The nice conclusion is that the TBM classifier is robust against such bad data.\nCase Study 6: Real Data, no pkc. We move now to real data where the classes are precisely known, just to show that the TBM-classifier behaves similarly to some of its competitors (De Caestekere, 1997). She uses a !-Nearest Neighbor classifier, a multi layers perceptron method, a prototype Nearest Neighbor method and we apply to the same data sets the TBM classifier. these classifiers are applied to the too famous Iris data set, the diabetes data set and the wave data sets presented in De Caestekere. Their major characteristics are presented in table 3. Data where used as given, or with added white noises. Results of the TBM-classifier (table 4) are as good as those obtained with the other three approaches that are usually acknowledged as being among the best.\nThe conclusions of the comparisons based on the six case studies illustrated here (and many others done buy De Smet) are: 4. when the classes are precisely known, the TBM\nclassifier performs almost as well as the classical classifiers.\nPractical Uses of Belief Functions 617\n5. the TBM-classifier can be used in cases where classes are partially known, in which cases performance is still very good. That the TBM-classifier can be applied in the case of partially known classes provides its real interest, as such messy data situations can hardly be handled with the classical tools as available today.\n3.2. Partial knowledge is real life.\nIt seems the TBM -classifier provides a nice tool, but does it fill a real need. The answer is affirmative. Real life hardly complies with the perfect knowledge usually required by classical statistical tools. Real life is messy data, not idealized data as one hopes for. As an example, consider the clinician who collect during the 1980's the data from 300 hundred patients suffering from a given disease Dx. In the 80's such patients were classified as A or B? Then as science advances, a new category C is described for patients with disease Dx. So during the 90's, our clinician collect 200 data classified as A, B or C. The clinician comes to you and asks for a computerized classifier. How to handle the first 300 cases, the A cases were in fact A or C, and the B cases were B or C, and their exact classes cannot be re-assessed. Are you going to throw away the 300 cases as useless . . . With the TBM classifier, you can proceed with all the 500 cases, whereas a plain statistical analysis would have serious troubles with the learning set.\nSuppose another clinician who collects data in 4 classes denoted A, B, C and D. Then regulation or knowledge changes and these cases are supposed to be classified into three classes, denoted X, Y and Z, where all A cases are now X cases, all D cases are now Z cases, and the B cases turn out to be either X or Y cases, and the C cases turn out to be either Y or Z cases. How to handle the old database? This is exactly what is illustrated by case study 2.\nSuppose a disease with 3 forms denoted A, B and C and three clinicians, denoted a, b, and c. Dr. a can only differentiate between A and not-A cases. The A cases are treated by Dr. a, the not-A cases are send to the hospital H. The same scenario holds for Dr. b where A is replaced by B, and for Dr. c where A is replaced by C. So at hospital H, the only cases they treat where classified as not-A, not E or not-C, and there is no way to find out what was the exact class of these patients (as if the only available information is the name of the sending doctor). This is exactly what is illustrated by case study 1.\nThis shows that 'artificial' examples are not that artificial.\nIt would be interesting to compare these results when classes are only partially known with other techniques. The major difficulty is of course in the construction of alternate methods based on probability theory as extra assumptions will have to be fed into the models, and the quality of the results will strongly depends on the\n618 Smets\nadequacy of these assumptions. In practice the user is not aware of this adequacy before using the classifier.\n4. PAS FOR INFORMATION RETRIEVAL.\nJustin Picard (1998) applies the Probabilistic Argumentation Systems, denoted PAS, (Kohlas and Haenni, 1999), an adaptation of the hint model, and a generalization of the ATMS of Laskey and Lehner (1989) to a problem of information retrieval, using in particular the CACM collection (3204 documents, 50 queries). Let a query Q, a set of documents Di and the citation links between them, denoted Citing(Di, Dj). The citation link reflects the idea that if a document Di is relevant to Q and cites Dj, then Dj is probably also relevant to Q. For each document Di, he introduces an assumption ai. When assumption ai holds then Di is relevant to Q. When assumption ai does not hold, then nothing can be concluded about the relevance of Di to Q. To assess the probability Ui that assumption ai holds, he uses the rank of Di as provided by the search engines present on the Web. He fits Ui by a logistic regression.\nexp -2.42ln(ranlq )+ 1.11 Uj = --'-----,..,..,.,....,\ufffd.,......;\"77 -,-;-\n1+exp 2.42ln(ranlq)+l.ll\nPicard then builds the PAS for modeling document relationship like the one in figure 5. The numerous cycles in the graph should be enhanced; they do not create any problem when using the PAS methodology. Another assumption Iij is introduced. If there is a citation link between Di and Dj, if Di is relevant to Q and if Iij holds then document Dj is also relevant to Q. If Iij does not hold, noting can be concluded about the relevance of Dj to Q that would result from the citation link between them. He accepts that the probability A. that Iij holds does not depend on the documents involved. The fitted value for A. is .2644.\n165 \ufffd 135 r 154 as 34\nThe figure represents 'graphically a PAS for a collection of six documents having some document relationships.\nRules are represented by arrows and assumptions by w h i t e d o t t e d c i r c l e s ( e . g . , a 1 \ufffd D\ufffdo az\ufffdDz, ... DIAI12\ufffdDz ... ) One can see that the support of D6 (or any document) corresponds to all existing path from any a priori assumption to D6: the support of 06 is 36V(a1Al16)\u00b7 D6 can thus be \"proven\" either by the retrieval system (argument a6) or by document D1 through the link from D1 to D6 (argument a1Al16)\u00b7\nDocument D4 illustrates how PAS deal with cycles. There are links from D4 to D3, D:> to Ds and Ds to D4. Even if there is a cycle here, evidence is counted only once: recall that the support of D4 is the disjunction of all arguments for which D4 becomes true. asAls4 is one such argument. Since it implicitly contains (asAls4Al43Al3sAls4) ( =\nasAls4Al43Al3s) which would correspond to the cycle D4D3-Ds-D4, this last argument is not considered. Anyway, the algorithm for determining the symbolic support eliminates cycles.' (Quotations are from Picard, 1998).\n5. SENSORS ON PARTIALLY OVERLAPPING FRAMES.\nSuppose a sensor S 1 that has been trained to recognize A objects and B objects and another sensor Sz that has been trained to recognize B objects and C objects (like A = airplanes, B = helicopters and C = rockets). Sensor S1 never saw a C object, and we know nothing on how S1 would react if to a C object. Beliefs provides by S 1 are always on the frame of discernment {A, B}. The same holds for Sz with A and C permuted. A new object X is presented to the two sensors. Both sensor S1 and Sz express their belief m1 and m2, the first on the frame {A, B}, the second on the frame{B, C}. How to combine these two beliefs on a common frame Q = {A, B, C}? Solutions have been proposed in Janez (1996).\nSolutions are based on the next constraint. If both m1 and mz are conditioned on {B}, and combined b y Dempster's rule of combination (unnormalized), the resulting belief function should be the same as the one obtained after 'combining' the original m1 and mz on [A, B, C}, and conditioning the result on { B } . The problem is of course how to 'combine' m1 and m2. The original Dempster's rule of combination is inadequate as it requires that both belief functions are defined on the same frame of discernment, what is not the case here.\nA general solution is as follows. Let Q 1 and 02 be the frame of discernment of m1 and mz, respectively. Let 0 = 01nOz. For all A \ufffd 01uOz, let A1 = An01, Az = An02, A0 = AnQ, and\nm(A) = ml (AJ) m2(A2) (mJ[Q]$m2[QJ)(AJ2)\nmJ[Q](Ao) m2[Q](Ao)\nwhere mi[Q] and mz[Q] are the basic belief assignments obtained by conditioning m1 and mz on Q. In table 5, we illustrate the computation. We have mi[B]$mz[B](B) =\n(.1+.3)*(.7+.1) = .32. This mass is distributed on {B}, (A, B}, {A, C} and {A, B, C} according to the next ratios: (.11.4).(.7/.8), (.3/.4).(.7/.8), (.11.4).(.11.8), and (.3/.4).(.11.8). In this example the first sensor supports that X is an A, whereas the second claims that X is a B. If X had been a B, how comes the first did not say so? So the second sensor is probably facing an A and just states B because it does not know what an A is. So we feel that the most plausible solution is X = A, what is confirmed by BetP12 being the largest for A: BetP12(A) = .455.\nJust to enhance the simplicity of the belief function solution, we examine what this problem would be when expressed within probability theory. Suppose two sensors S1 and Sz. Sensor S1 generates a probability function on {A, B, C), denoted P*1, but we only know P1 =\nP*I(.J(A,B }), the value of P*1 after conditioning it on {A, B }. The same holds for sensor Sz with Pz = P*z(./{B,C}). Aggregate P1 and Pz in order to derive a probability function on {A, B, C). The major issue is on how to reconstruct P*1 and P*z from P1 and Pz. It means how to 'de-condition' a probability function on Q when all you know is the result of its conditioning on some strict subset of Q. Suppose Q = {a, b, c, d) and you know P({a}/{a,b)). What would be P({c}) and P({c,d})? There are infinitely many solutions. Introducing the maximum entropy principle leads to P( { c}) = .P( { d}) = .25 and P((a}) = .5 * P(a/{a,b}). Such a solution is strongly linked to the insufficient reason principle and suffers of all its weaknesses.\n6. ANALYZING CONTRADICTION AND THE NUMBER OF SOURCES.\nSuppose a piece of equipment has failed. We collect data from four sensors S1, S2, S3 and S4. Each sensor produces a belief function on the set of possible component that might have failed. Table 6 presents a highly simplified example where each sensor produces a simple support function pointing toward one component. S 1 and Sz both point toward component c 1, whereas s3 and s4 point\nPractical Uses of Belief Functions 619\ntoward component Cz. If the four sources S1 to S4 were highly reliable, you would conclude that both C1 and Cz are broken. Indeed if only one has failed, the source are contradictory, whereas if two components have failed, results are coherent if S 1 and Sz report on one broken component and S3 and S4 report on a second broken component.\nHow do we translate this problem into belief functions language? The solution is obtained by considering the mass m(0) given to 0 that may be positive in the transferable belief model. When applying Dempster's rule of combination to two basic belief assignments m1 and mz, the result is given by: mu(A) = L, mi(X)mz(Y) for all A 0}.. X.Y\ufffdO.XnY=A We do not normalize the resulting basic belief assignment m12, m(0) is among the computed masses and it does not have to be 0 like in Shafer's original presentation. The mass m(0) quantifies the amount of contradiction between the various sources of belief functions.\nSchubert (1995) has proposed a strategy to decide the number of events under consideration by the various sensors producing the several collected belief functions. He analyses m(0) and finds the association between sensors and events that somehow brings the total conflict to an acceptable level .\nSuppose the data of table 6. If there is only one broken component the four sensors are speaking about the same event. The contradiction computed after combining the four basic belief assignments is 90, what reflect an enormous conflict between the four sources. If there is two broken components, then some sources might speak about one, the other about the second. So we split the four sensors into two groups, compute what is the contradiction within each group, and sum these contradictions. For instance, suppose sensors SI> S2 and S 3 speak about one component, then the contradiction is 0.56, whereas there is no contradiction for sensor 4. Total contradiction is thus 0.56. Now if we consider that sensor S1 and S3 speak about one component, whereas S2 and S4 speak about the other, the total contradiction is 1.14. Contradiction completely disappears when S1 and S2 are grouped as reporting on one component, and S3 and S4 on the second. This result fits with common sense analysis of the data. In real life applications, the basic belief assignments are usually quite elaborated, and finding an\n620 Smets\nadequate grouping is not obvious. The technique of 'peeling' the mass given to the empty set (to the contradiction) is nevertheless still applicable. The level of 'tolerable contradiction' is itself determined by the analysis of the conflict present in the given belief functions (and obtained by the use of the canonical decomposition of the belief functions (Smets, 1995)).\nThe mass m(0) acts in fact as a measure of discrepancy between several belief functions. The proposed algorithm leads to grouping sources which belief functions are 'close' to each other. In probability theory using cross entropy or chi-square coefficients can achieve this purpose. Comparisons between these approaches are not available (as far as we know). The advantage of the belief function approach resides in the well-founded nature of the approach. The mass m(0) is part of the transferable belief model, whereas the cross-entropy, the chi-square and the likes need always extra assumptions in order to justify their use.\n7. FINAL REMARKS.\nWe have presented a few 'real' life problems where the use of belief functions is interesting. These problems are characterized by the presence of some missing information that are needed to apply the probability approach. Probabilists normally try to obtain some 'estimation' of the values for these missing data and apply their model with these data (using sensitivity analysis in order to check the robustness of their conclusions to reasonable variations around the guessed 'estimation'). Usually the belief function models do not need such assumptions and is well adapted to work with the information as really available. This power comes from the ability of belief functions to represent any form of uncertainty: full knowledge, partial ignorance, total ignorance (and even probability knowledge). Probability functions do not have such expressiveness power. \u00a3qui probability is not full ignorance, it is already a quite precise form of knowledge.\nIn practice, the major interest of the belief function approach as presented here comes from its robustness (Appriou, 1997, Picard, 1998). When the 'estimations' of\nthe missing data are close to what they are in reality, the probability model is normally perfect. But once differences increase between the 'estimations' and reality, the probability models deteriorate much faster then the belief function models. It is amazing to note that long before belief functions had been introduced, HUber ( 1973) had developed robust methods in statistics and his results have some similarity with those of the belief functions approach.\nThe computational issue is real but as shown in these examples, it seems manageable. No serious studies are available on that issue. We feel that the computational complexity will be similar to those encountered in probability theory, but of course brute force approaches must be avoided. E.g., in the Schubert's example, it is obvious that the sensor clustering will not be achieved by testing every partition, and that some stepwise approaches have to be used. The fact that belief functions are defined on the power set, whereas probability functions are defined on the set has often been used as an argument against the use of belief functions. Theoretically the argument is correct, but in practice situations will hardly be so complex and there are even cases where the complexity is smaller with belief functions. In any case approximations will be used.\nAcknowledgement. Research work has been partly supported by the ESPRIT IV, Working Group FUSION funded by a grant from the European Union.\nBibliography. Appriou A. (1997) Multisensor data fusion in situation\nassessment processes. in Qualitative and quantitative practical reasoning. Gabbay D. M. , Kruse R., Nonnengart A. and Ohlbach H. J. eds., Springer, Berlin, pg. 1-15. De Smet Y. (1998) Application de Ia theorie des fonctions de croyance aux problemes de classification. Memoire. Faculte des Sciences, Universite Libre de Bruxelles. Decaestecker C. (1997) Finding prototypes for nearest neighbour classifiacftion by means of gradient descemt and deterministic annealing. Pattern Recognition 30:281-288. Dempster A. P. (1997) Upper and lower probabilities induced by a multiple valued mapping. Ann. Math. Statistics 38: 325- 339. Dempster A.P. ( 1968) A generalization of Bayesian inference. J. Roy. Statist. Soc. B.30:205-247. Denoeux T. (1995) A k-nearest neighbor classification rule based on Dempster-Shafer theory. IEEE Trans. SMC: 25:804- 813. Denoeux T. and Zouhal L. M. (1999) Handling possibilistic labels in pattern classification using evidential reasoning. Submitted for publication. Dubois D. and Prade H. (1986) On the unicity of Dempster's rule of combination. Int. J. Intell. Systems 1:133-142. Gabbay D. M. and Smets Ph. Eds. (1998-99) Handbook of defeasible reasoning and uncertainty management systems. Vol. 1: Quantified representation of uncertainty and imprecision, Vol. 2: Reasoning with actual and potential contradictions. Vol. 3: Belief changes, Vol. 4: Abductive reasoning and learning. Vol. 5: Algorithms for uncertainty and defeasible reasoning .. Kluwer, Doordrecht.\nGordon J. and Shortliffe E. H. ( 1984) The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project. Addison Wesley, Reading, MA. 272-292. Hajek P. (1992) Deriving Dempster's rule. in IPMU '92 proceedings, pg. 73-75. Hsia Y.-T. (1991) Characterizing Belief with Minimum Commitment. DCAI-91:1184-1189. Huber P. J. (1973) The use of Choquet capacities in statistics. Bull. Internal. Statist. Inst. 45:181-191 Jaffray J.Y. (1992) Bayesian updating and belief functions. IEEE Trans. SMC, 22:1144-1152. Jaffray J. Y. (1989) Coherent bets under partially resolving uncertainty and belief functions. Theory and Decision 26:99- 105. Janez F. (1996) Fusion de sources d'information definies sur des referentials non exhaustifs differents. These. Universite d' Angers, France. Klawonn F. and Schwecke E. (1992) On the axiomatic justification of Dempster's rule of combination. Int. J. Intel. Systems 7:469-478. Klawonn F. and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems. in Gabbay and Smets (1998-99), Vol. 5. Kohlas J. and Monney P.A. (1995) A Mathematical Theory of Hints. An Approach to Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathematical Systems No. 425. Springer-Verlag. Kruse R. and Schwecke E. ( 1990) Specialization: a new concept for uncertainty handling with belief functions. Int. J. Gen. Systems 18:49-60. Kyburg H.E. Jr. (1987) Bayesian and non-Bayesian evidential updating. Artificial Intelligence, 31:271-294. Laskey K. and Lehner P.E., ( 1989) Assumptions, Beliefs and Probabilities. Artif. Intel. 41: 65-77. Pearl J. ( 1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Pub. San Mateo, Ca, USA. Picard J. ( 1998) Modeling and combining evidence provided by documents relationships using probabilistic argumentation systems. Proc. ACM-SIGIR, Melbourne, Australia. Ruspini E.H. (1986) The logical foundations of evidential reasoning. Technical note 408, SRI International, Menlo Park, Ca. Schubert, J., (1995) Cluster-based Specification Techniques in Dempster-Shafer Theory, Proc. European Conf. Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Springer-Verlag, Berlin. Shafer G. (1976) A mathematical theory of evidence. Princeton Univ. Press. Princeton, NJ. Shafer G. (1992) Rejoinder to Comments on \"Perspectives in the theory and practice of belief functions\". Intern. J. Approx. Reasoning, 6:445-480. Smets Ph. (1978) Un modele mathematico-statistique simulant le processus du diagnostic medical. These d'agn!gation de l'Enseignement Superieur. Presses Universitaires. 269 pages. (available through University Microfilm International, 30-32 Mortimer street, London WIN 7RA, thesis 80-70,003).\nPractical Uses of Belief Functions 621\nSmets Ph. (1983) Information Content of an Evidence. Int. J. Man Machine Studies, 19: 33-43. Smets Ph. (1989) Constructing the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5, Henrion M., Shachter R.D., Kana! L.N. and Lemmer J.F. eds, North Holland, Amsterdam, 29-40 Smets Ph. (1990) The combination of evidence in the transferable belief model. IEEE Pattern analysis and Machine Intelligence, 12:447-458. Smets Ph. (1991) Probability of provability and belief functions. Logique et Analyse, 133-134:177-195. Smets Ph. ( 1993a) An axiomatic justifiaction for the use of belief function to quantify beliefs. DCAI'93 (Inter. Joint Conf. on AI), Chambery. pg. 598-603. Smets Ph. (1993b) Probability of Deductibility and Belief Functions. ECSQARU 93. Smets Ph. (1993c) No Dutch Book can be built against the TBM even though update is not obtained by Bayes rule of conditioning. SIS, Workshop on Probabilisitic Expert Systems, (ed. R. Scozzafava), Roma, pg. 181-204 .. Smets Ph. (1993d) Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem. Int. J. Approximate Reasoning 9:1-35. Smets Ph. (1994) What is Dempster-Shafer's model? in Advances in the Dempster-Shafer Theory of Evidence. Yager R.R., Kacprzyk J. and Fedrizzi M., eds, Wiley, New York, pg. 5-34 .. Smets Ph. ( 1995) The Canonical Decomposition of a Weighted Belief. DCAI'95, Montreal. pg.1896-1901. Smets Ph. (1997a) The axiomatic justification of the transferable belief model. Artificial Intelligence 92: 229-242. Smets Ph. (1997b) The alpha-junctions: combination operators applicable to belief functions. in Qualitative and quantitative practical reasoning. Gabbay D. M. , Kruse R., Nonnengart A. and Ohlbach H. J. eds., Springer, Berlin, pg. 131-153. Smets Ph. (1998a) Probability, Possibility, Belief: Which and Where ? in Gabbay and Smets (1998-99) Vol. 1, pg. 1-24. Smets Ph. (1998b) The Transferable Belief Model for Quantified Belief Representation. in Gabbay and Smets (1998-99) Vol. 1, pg. 267-301. Smets Ph. and Kennes R. ( 1994) The transferable belief model. Artificial Intelligence 66: 191-234. Smets Ph. and Kruse R. (1997) The transferable belief model for belief representation. in Uncertainty management in information systems: froin needs to solutions. Motto A. and Smets Ph. eds. Kluwer, Boston. Strat T.M. (1990) Decision analysis using belief functions. Int. J. Approx. Reasoning 4: 391-418 Voorbraak F. (1993) As Far as I Know: Epistemic Logic and Uncertainty. Dissertation, Utrecht University. Walley P. (1991) Statistical reasoning with imprecise probabilities. Chapman and Hall, London .. Wilson N. (1993) Decision making with belief functions and pignistic probabilities. in Symbolic and Quantitative Approaches to Reasoning and Uncertainty. Clarke M., Kruse R. and Moral S. (eds.), Springer Verlag, Berlin, pg. 364-371. Wong S.K.M., Yao Y.Y., Bollmann P. and Burger H.C. (1990) Axiomatization of qualitative belief structure. IEEE Trans. SMC, 21:726-734. Yager R.R., Kacprzyk J. and Fedrizzi M., eds. (1994) Advances in the Dempster-Shafer Theory of Evidence. Wiley, New York, Zouhal L. M. (1997) Contribution a !'application de Ia theorie des fonctions de croyances en reconnaissance des formes. Universite de Technologie de Compiegne, France"}], "references": [{"title": "Multisensor data fusion in situation assessment processes. in Qualitative and quantitative practical reasoning", "author": ["A. Appriou"], "venue": null, "citeRegEx": "Appriou,? \\Q1997\\E", "shortCiteRegEx": "Appriou", "year": 1997}, {"title": "Application de Ia theorie des fonctions de croyance aux problemes de", "author": ["Y. De Smet"], "venue": null, "citeRegEx": "Smet,? \\Q1998\\E", "shortCiteRegEx": "Smet", "year": 1998}, {"title": "A k-nearest neighbor classification rule based on Dempster-Shafer theory", "author": ["T. Denoeux"], "venue": "IEEE Trans", "citeRegEx": "Denoeux,? \\Q1995\\E", "shortCiteRegEx": "Denoeux", "year": 1995}, {"title": "Handling possibilistic labels in pattern classification using evidential reasoning", "author": ["T. Denoeux", "M. Zouhal L"], "venue": null, "citeRegEx": "Denoeux and L.,? \\Q1999\\E", "shortCiteRegEx": "Denoeux and L.", "year": 1999}, {"title": "On the unicity of Dempster's rule of combination", "author": ["D. Dubois", "H. Prade"], "venue": "Int. J. Intell", "citeRegEx": "Dubois and Prade,? \\Q1986\\E", "shortCiteRegEx": "Dubois and Prade", "year": 1986}, {"title": "Handbook of defeasible reasoning and uncertainty management systems. Vol. 1: Quantified representation of uncertainty and imprecision, Vol. 2: Reasoning with actual and potential", "author": ["Gabbay D. M", "Smets Ph"], "venue": "Eds", "citeRegEx": "M. and Ph.,? \\Q1998\\E", "shortCiteRegEx": "M. and Ph.", "year": 1998}, {"title": "The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project", "author": [], "venue": "Addison\u00ad Wesley,", "citeRegEx": "H.,? \\Q1984\\E", "shortCiteRegEx": "H.", "year": 1984}, {"title": "Deriving Dempster's rule", "author": ["P. Hajek"], "venue": "IPMU '92 proceedings,", "citeRegEx": "Hajek,? \\Q1992\\E", "shortCiteRegEx": "Hajek", "year": 1992}, {"title": "Characterizing Belief with Minimum Commitment. DCAI-91:1184-1189", "author": ["Hsia Y.-T"], "venue": null, "citeRegEx": "Y..T.,? \\Q1991\\E", "shortCiteRegEx": "Y..T.", "year": 1991}, {"title": "The use of Choquet capacities in statistics", "author": ["J. Huber P"], "venue": "Bull. Internal. Statist. Inst", "citeRegEx": "P.,? \\Q1973\\E", "shortCiteRegEx": "P.", "year": 1973}, {"title": "Bayesian updating and belief functions", "author": ["J.Y. Jaffray"], "venue": "IEEE Trans. SMC,", "citeRegEx": "Jaffray,? \\Q1992\\E", "shortCiteRegEx": "Jaffray", "year": 1992}, {"title": "Coherent bets under partially resolving uncertainty and belief functions. Theory and Decision 26:99105", "author": ["Y. Jaffray J"], "venue": null, "citeRegEx": "J.,? \\Q1989\\E", "shortCiteRegEx": "J.", "year": 1989}, {"title": "Fusion de sources d'information definies sur des referentials non exhaustifs differents", "author": ["F. Janez"], "venue": null, "citeRegEx": "Janez,? \\Q1996\\E", "shortCiteRegEx": "Janez", "year": 1996}, {"title": "On the axiomatic justification of Dempster's rule of combination", "author": ["F. Klawonn", "E. Schwecke"], "venue": "Int. J. Intel. Systems", "citeRegEx": "Klawonn and Schwecke,? \\Q1992\\E", "shortCiteRegEx": "Klawonn and Schwecke", "year": 1992}, {"title": "The dynammic of belief in the transferable belief model and specialization-generalization matrices", "author": ["Klawonn F", "Smets Ph"], "venue": null, "citeRegEx": "F. and Ph.,? \\Q1992\\E", "shortCiteRegEx": "F. and Ph.", "year": 1992}, {"title": "Uncertainty based information", "author": ["G.J. Klir", "M.J. Wierman"], "venue": null, "citeRegEx": "Klir and Wierman,? \\Q1998\\E", "shortCiteRegEx": "Klir and Wierman", "year": 1998}, {"title": "Assumption-based reasoning and probabilistic argumentation systems", "author": ["J. Kohlas", "R. Haenni"], "venue": "in Gabbay and Smets (1998-99),", "citeRegEx": "Kohlas and Haenni,? \\Q1999\\E", "shortCiteRegEx": "Kohlas and Haenni", "year": 1999}, {"title": "A Mathematical Theory of Hints. An Approach to Dempster-Shafer Theory of Evidence", "author": ["J. Kohlas", "P.A. Monney"], "venue": "Lecture Notes in Economics and Mathematical Systems", "citeRegEx": "Kohlas and Monney,? \\Q1995\\E", "shortCiteRegEx": "Kohlas and Monney", "year": 1995}, {"title": "Specialization: a new concept for uncertainty handling with belief functions", "author": ["R. Kruse", "E. Schwecke"], "venue": "Int. J. Gen. Systems", "citeRegEx": "Kruse and Schwecke,? \\Q1990\\E", "shortCiteRegEx": "Kruse and Schwecke", "year": 1990}, {"title": "Bayesian and non-Bayesian evidential updating", "author": ["Kyburg H.E. Jr."], "venue": "Artificial Intelligence,", "citeRegEx": "Jr.,? \\Q1987\\E", "shortCiteRegEx": "Jr.", "year": 1987}, {"title": "Assumptions, Beliefs and Probabilities", "author": ["K. Laskey", "P.E. Lehner"], "venue": "Artif. Intel", "citeRegEx": "Laskey and Lehner,? \\Q1989\\E", "shortCiteRegEx": "Laskey and Lehner", "year": 1989}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Modeling and combining evidence provided by documents relationships using probabilistic argumentation systems", "author": ["J. Picard"], "venue": "Proc. ACM-SIGIR,", "citeRegEx": "Picard,? \\Q1998\\E", "shortCiteRegEx": "Picard", "year": 1998}, {"title": "The logical foundations of evidential reasoning. Technical note 408", "author": ["E.H. Ruspini"], "venue": "SRI International,", "citeRegEx": "Ruspini,? \\Q1986\\E", "shortCiteRegEx": "Ruspini", "year": 1986}, {"title": "Cluster-based Specification Techniques in Dempster-Shafer Theory, Proc. European Conf. Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Springer-Verlag, Berlin", "author": ["J. Schubert"], "venue": null, "citeRegEx": "Schubert,? \\Q1995\\E", "shortCiteRegEx": "Schubert", "year": 1995}, {"title": "A mathematical theory of evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "Shafer,? \\Q1976\\E", "shortCiteRegEx": "Shafer", "year": 1976}, {"title": "Rejoinder to Comments on \"Perspectives in the theory and practice of belief functions", "author": ["G. Shafer"], "venue": "Intern. J. Approx. Reasoning,", "citeRegEx": "Shafer,? \\Q1992\\E", "shortCiteRegEx": "Shafer", "year": 1992}, {"title": "Un modele mathematico-statistique simulant le processus du diagnostic medical", "author": ["Smets Ph"], "venue": "These d'agn!gation de l'Enseignement Superieur. Presses Universitaires. 269 pages. (available through University Microfilm International,", "citeRegEx": "Ph.,? \\Q1978\\E", "shortCiteRegEx": "Ph.", "year": 1978}, {"title": "Information Content of an Evidence", "author": ["Smets Ph"], "venue": "Int. J. Man Machine Studies,", "citeRegEx": "Ph.,? \\Q1983\\E", "shortCiteRegEx": "Ph.", "year": 1983}, {"title": "Constructing the pignistic probability function in a context of uncertainty", "author": ["Smets Ph"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "Ph.,? \\Q1989\\E", "shortCiteRegEx": "Ph.", "year": 1989}, {"title": "The combination of evidence in the transferable belief model", "author": ["Smets Ph"], "venue": "IEEE Pattern analysis and Machine Intelligence,", "citeRegEx": "Ph.,? \\Q1990\\E", "shortCiteRegEx": "Ph.", "year": 1990}, {"title": "Probability of provability and belief functions", "author": ["Smets Ph"], "venue": "Logique et Analyse,", "citeRegEx": "Ph.,? \\Q1991\\E", "shortCiteRegEx": "Ph.", "year": 1991}, {"title": "An axiomatic justifiaction for the use of belief function to quantify beliefs. DCAI'93 (Inter", "author": ["Smets Ph"], "venue": "Joint Conf. on AI),", "citeRegEx": "Ph.,? \\Q1993\\E", "shortCiteRegEx": "Ph.", "year": 1993}, {"title": "Probability of Deductibility and Belief Functions", "author": ["Smets Ph"], "venue": "ECSQARU", "citeRegEx": "Ph.,? \\Q1993\\E", "shortCiteRegEx": "Ph.", "year": 1993}, {"title": "No Dutch Book can be built against the TBM even though update is not obtained by Bayes rule of conditioning", "author": ["Smets Ph"], "venue": "SIS, Workshop on Probabilisitic Expert Systems,", "citeRegEx": "Ph.,? \\Q1993\\E", "shortCiteRegEx": "Ph.", "year": 1993}, {"title": "Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem", "author": ["Smets Ph"], "venue": "Int. J. Approximate Reasoning", "citeRegEx": "Ph.,? \\Q1993\\E", "shortCiteRegEx": "Ph.", "year": 1993}, {"title": "What is Dempster-Shafer's model? in Advances in the Dempster-Shafer Theory of Evidence", "author": ["Smets Ph"], "venue": "Yager R.R., Kacprzyk J. and Fedrizzi M.,", "citeRegEx": "Ph.,? \\Q1994\\E", "shortCiteRegEx": "Ph.", "year": 1994}, {"title": "The Canonical Decomposition of a Weighted Belief", "author": ["Smets Ph"], "venue": "DCAI'95, Montreal", "citeRegEx": "Ph.,? \\Q1995\\E", "shortCiteRegEx": "Ph.", "year": 1995}, {"title": "The axiomatic justification of the transferable belief model", "author": ["Smets Ph"], "venue": "Artificial Intelligence", "citeRegEx": "Ph.,? \\Q1997\\E", "shortCiteRegEx": "Ph.", "year": 1997}, {"title": "The alpha-junctions: combination operators applicable to belief functions. in Qualitative and quantitative practical", "author": ["Smets Ph"], "venue": null, "citeRegEx": "Ph.,? \\Q1997\\E", "shortCiteRegEx": "Ph.", "year": 1997}, {"title": "Probability, Possibility, Belief: Which and Where", "author": ["Smets Ph"], "venue": "Gabbay and Smets (1998-99)", "citeRegEx": "Ph.,? \\Q1998\\E", "shortCiteRegEx": "Ph.", "year": 1998}, {"title": "The Transferable Belief Model for Quantified Belief Representation", "author": ["Smets Ph"], "venue": "in Gabbay and Smets (1998-99)", "citeRegEx": "Ph.,? \\Q1998\\E", "shortCiteRegEx": "Ph.", "year": 1998}, {"title": "The transferable belief model", "author": ["Smets Ph", "Kennes R"], "venue": "Artificial Intelligence", "citeRegEx": "Ph. and R.,? \\Q1994\\E", "shortCiteRegEx": "Ph. and R.", "year": 1994}, {"title": "The transferable belief model for belief representation. in Uncertainty management in information systems: froin needs to solutions", "author": ["Smets Ph", "Kruse R"], "venue": "Motto A. and Smets Ph. eds. Kluwer,", "citeRegEx": "Ph. and R.,? \\Q1997\\E", "shortCiteRegEx": "Ph. and R.", "year": 1997}, {"title": "Decision analysis using belief functions", "author": ["T.M. Strat"], "venue": "Int. J. Approx. Reasoning", "citeRegEx": "Strat,? \\Q1990\\E", "shortCiteRegEx": "Strat", "year": 1990}, {"title": "Statistical reasoning with imprecise probabilities", "author": ["P. Walley"], "venue": null, "citeRegEx": "Walley,? \\Q1991\\E", "shortCiteRegEx": "Walley", "year": 1991}, {"title": "Decision making with belief functions and pignistic probabilities. in Symbolic and Quantitative Approaches to Reasoning and Uncertainty", "author": ["N. Wilson"], "venue": null, "citeRegEx": "Wilson,? \\Q1993\\E", "shortCiteRegEx": "Wilson", "year": 1993}, {"title": "Axiomatization of qualitative belief structure", "author": ["S.K.M. Wong", "Y.Y. Yao", "P. Bollmann", "H.C. Burger"], "venue": "IEEE Trans. SMC,", "citeRegEx": "Wong et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Wong et al\\.", "year": 1990}, {"title": "Advances in the Dempster-Shafer Theory of Evidence", "author": ["R.R. Yager", "J. Kacprzyk", "M. Fedrizzi", "eds"], "venue": null, "citeRegEx": "Yager et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Yager et al\\.", "year": 1994}, {"title": "Contribution a !'application de Ia theorie des fonctions de croyances en reconnaissance des formes", "author": ["M. Zouhal L"], "venue": "Universite de Technologie de Compiegne,", "citeRegEx": "L.,? \\Q1997\\E", "shortCiteRegEx": "L.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": ", Appriou (1997) shows an analysis", "startOffset": 2, "endOffset": 17}, {"referenceID": 2, "context": "These examples were essentially developed by Thierry Denoeux, University of Compiegne, France (Denoeux, 1995), by Johan Schubert, Royal Institute of Technology, Stockholm, Sweden (Schubert, 1995), by Justin Picard, Universite de Neuchatel, Switzerland (Picard, 1998) and by Janez, ONERA, Paris, France, (Janez, 1996).", "startOffset": 94, "endOffset": 109}, {"referenceID": 24, "context": "These examples were essentially developed by Thierry Denoeux, University of Compiegne, France (Denoeux, 1995), by Johan Schubert, Royal Institute of Technology, Stockholm, Sweden (Schubert, 1995), by Justin Picard, Universite de Neuchatel, Switzerland (Picard, 1998) and by Janez, ONERA, Paris, France, (Janez, 1996).", "startOffset": 179, "endOffset": 195}, {"referenceID": 22, "context": "These examples were essentially developed by Thierry Denoeux, University of Compiegne, France (Denoeux, 1995), by Johan Schubert, Royal Institute of Technology, Stockholm, Sweden (Schubert, 1995), by Justin Picard, Universite de Neuchatel, Switzerland (Picard, 1998) and by Janez, ONERA, Paris, France, (Janez, 1996).", "startOffset": 252, "endOffset": 266}, {"referenceID": 12, "context": "These examples were essentially developed by Thierry Denoeux, University of Compiegne, France (Denoeux, 1995), by Johan Schubert, Royal Institute of Technology, Stockholm, Sweden (Schubert, 1995), by Justin Picard, Universite de Neuchatel, Switzerland (Picard, 1998) and by Janez, ONERA, Paris, France, (Janez, 1996).", "startOffset": 303, "endOffset": 316}, {"referenceID": 17, "context": "Dempster's model derived from probability theory (Dempster, 1967) and represented by the hints theory of Kohlas (Kohlas and Monney, 1995), 3 .", "startOffset": 112, "endOffset": 137}, {"referenceID": 45, "context": "maximal price the agent, called the player, would pay to a banker to enter a game where the player gets from the banker $1 if A occurs, and nothing otherwise (Walley, 1991).", "startOffset": 158, "endOffset": 172}, {"referenceID": 10, "context": "probability functions (Jaffray, 1992).", "startOffset": 22, "endOffset": 37}, {"referenceID": 1, "context": "The axiomatic of the TBM as a model to represent quantified beliefs is detailed in Smets (1993A, 1997a), (see also Wong et a!., 1990). Axiomatic justification for combination rules is given in (Smets, 1990, Dubois and Prade, 1986, Hajek, 1992, Klawonn and Schwecke 1992). The generalization of the Bayesian Theorem to the TBM is presented in Smets 1978, 1993b). Decision process based on lower expectations are explained in Strat, ( 1990) Jaffray (1989), whereas Wilson (1993) studies the properties of the pignistic probabilities and Smets (1993c) examines what become the pignistic transformations in a dynamic decision making context.", "startOffset": 83, "endOffset": 454}, {"referenceID": 1, "context": "The axiomatic of the TBM as a model to represent quantified beliefs is detailed in Smets (1993A, 1997a), (see also Wong et a!., 1990). Axiomatic justification for combination rules is given in (Smets, 1990, Dubois and Prade, 1986, Hajek, 1992, Klawonn and Schwecke 1992). The generalization of the Bayesian Theorem to the TBM is presented in Smets 1978, 1993b). Decision process based on lower expectations are explained in Strat, ( 1990) Jaffray (1989), whereas Wilson (1993) studies the properties of the pignistic probabilities and Smets (1993c) examines what become the pignistic transformations in a dynamic decision making context.", "startOffset": 83, "endOffset": 477}, {"referenceID": 1, "context": "The axiomatic of the TBM as a model to represent quantified beliefs is detailed in Smets (1993A, 1997a), (see also Wong et a!., 1990). Axiomatic justification for combination rules is given in (Smets, 1990, Dubois and Prade, 1986, Hajek, 1992, Klawonn and Schwecke 1992). The generalization of the Bayesian Theorem to the TBM is presented in Smets 1978, 1993b). Decision process based on lower expectations are explained in Strat, ( 1990) Jaffray (1989), whereas Wilson (1993) studies the properties of the pignistic probabilities and Smets (1993c) examines what become the pignistic transformations in a dynamic decision making context.", "startOffset": 83, "endOffset": 549}, {"referenceID": 15, "context": "Measures extending entropy measure are detailed in (Klir and Wierman, 1998), whereas we develop a measure adapted to the TBM (Smets, 1983).", "startOffset": 51, "endOffset": 75}, {"referenceID": 1, "context": "The method was invented by Denoeux (1995). We present results of the method - called the TBM classifier - and compare them with those obtained by the classical discriminant analysis applied to the same data base but using the exact value for the classes, a method that is then optimal.", "startOffset": 27, "endOffset": 42}, {"referenceID": 1, "context": "The method was invented by Denoeux (1995). We present results of the method - called the TBM classifier - and compare them with those obtained by the classical discriminant analysis applied to the same data base but using the exact value for the classes, a method that is then optimal. Details about these results are given in Denoeux (1995), Zouhal (1997), De Smet (1998).", "startOffset": 27, "endOffset": 342}, {"referenceID": 1, "context": "The method was invented by Denoeux (1995). We present results of the method - called the TBM classifier - and compare them with those obtained by the classical discriminant analysis applied to the same data base but using the exact value for the classes, a method that is then optimal. Details about these results are given in Denoeux (1995), Zouhal (1997), De Smet (1998).", "startOffset": 27, "endOffset": 357}, {"referenceID": 1, "context": "Details about these results are given in Denoeux (1995), Zouhal (1997), De Smet (1998).", "startOffset": 75, "endOffset": 87}, {"referenceID": 16, "context": "Justin Picard (1998) applies the Probabilistic Argumentation Systems, denoted PAS, (Kohlas and Haenni, 1999), an adaptation of the hint model, and a generalization of the ATMS of Laskey and Lehner (1989) to a problem of information retrieval, using in particular the CACM collection (3204 documents, 50 queries).", "startOffset": 83, "endOffset": 108}, {"referenceID": 8, "context": "Justin Picard (1998) applies the Probabilistic Argumentation Systems, denoted PAS, (Kohlas and Haenni, 1999), an adaptation of the hint model, and a generalization of the ATMS of Laskey and Lehner (1989) to a problem of information retrieval, using in particular the CACM collection (3204 documents, 50 queries).", "startOffset": 7, "endOffset": 21}, {"referenceID": 6, "context": "Justin Picard (1998) applies the Probabilistic Argumentation Systems, denoted PAS, (Kohlas and Haenni, 1999), an adaptation of the hint model, and a generalization of the ATMS of Laskey and Lehner (1989) to a problem of information retrieval, using in particular the CACM collection (3204 documents, 50 queries).", "startOffset": 95, "endOffset": 204}, {"referenceID": 6, "context": "How to combine these two beliefs on a common frame Q = {A, B, C}? Solutions have been proposed in Janez (1996).", "startOffset": 0, "endOffset": 111}, {"referenceID": 1, "context": "De Smet Y. (1998) Application de Ia theorie des fonctions de croyance aux problemes de classification.", "startOffset": 3, "endOffset": 18}, {"referenceID": 9, "context": "P. (1997) Upper and lower probabilities induced by a multiple valued mapping.", "startOffset": 0, "endOffset": 10}, {"referenceID": 6, "context": "and Prade H. (1986) On the unicity of Dempster's rule of combination.", "startOffset": 10, "endOffset": 20}, {"referenceID": 5, "context": "H. ( 1984) The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project. Addison\u00ad Wesley, Reading, MA. 272-292. Hajek P. (1992) Deriving Dempster's rule.", "startOffset": 0, "endOffset": 246}, {"referenceID": 5, "context": "H. ( 1984) The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project. Addison\u00ad Wesley, Reading, MA. 272-292. Hajek P. (1992) Deriving Dempster's rule. in IPMU '92 proceedings, pg. 73-75. Hsia Y.-T. (1991) Characterizing Belief with Minimum Commitment.", "startOffset": 0, "endOffset": 326}, {"referenceID": 5, "context": "H. ( 1984) The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project. Addison\u00ad Wesley, Reading, MA. 272-292. Hajek P. (1992) Deriving Dempster's rule. in IPMU '92 proceedings, pg. 73-75. Hsia Y.-T. (1991) Characterizing Belief with Minimum Commitment. DCAI-91:1184-1189. Huber P. J. (1973) The use of Choquet capacities in statistics.", "startOffset": 0, "endOffset": 411}, {"referenceID": 5, "context": "H. ( 1984) The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project. Addison\u00ad Wesley, Reading, MA. 272-292. Hajek P. (1992) Deriving Dempster's rule. in IPMU '92 proceedings, pg. 73-75. Hsia Y.-T. (1991) Characterizing Belief with Minimum Commitment. DCAI-91:1184-1189. Huber P. J. (1973) The use of Choquet capacities in statistics. Bull. Internal. Statist. Inst. 45:181-191 Jaffray J.Y. (1992) Bayesian updating and belief functions.", "startOffset": 0, "endOffset": 518}, {"referenceID": 5, "context": "H. ( 1984) The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project. Addison\u00ad Wesley, Reading, MA. 272-292. Hajek P. (1992) Deriving Dempster's rule. in IPMU '92 proceedings, pg. 73-75. Hsia Y.-T. (1991) Characterizing Belief with Minimum Commitment. DCAI-91:1184-1189. Huber P. J. (1973) The use of Choquet capacities in statistics. Bull. Internal. Statist. Inst. 45:181-191 Jaffray J.Y. (1992) Bayesian updating and belief functions. IEEE Trans. SMC, 22:1144-1152. Jaffray J. Y. (1989) Coherent bets under partially resolving uncertainty and belief functions.", "startOffset": 0, "endOffset": 610}, {"referenceID": 5, "context": "H. ( 1984) The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project. Addison\u00ad Wesley, Reading, MA. 272-292. Hajek P. (1992) Deriving Dempster's rule. in IPMU '92 proceedings, pg. 73-75. Hsia Y.-T. (1991) Characterizing Belief with Minimum Commitment. DCAI-91:1184-1189. Huber P. J. (1973) The use of Choquet capacities in statistics. Bull. Internal. Statist. Inst. 45:181-191 Jaffray J.Y. (1992) Bayesian updating and belief functions. IEEE Trans. SMC, 22:1144-1152. Jaffray J. Y. (1989) Coherent bets under partially resolving uncertainty and belief functions. Theory and Decision 26:99105. Janez F. (1996) Fusion de sources d'information definies sur des referentials non exhaustifs differents.", "startOffset": 0, "endOffset": 730}, {"referenceID": 5, "context": "H. ( 1984) The Dempster-Shafer theory of evidence, in Buchanan B.G. and Shortliffe E.H. (Eds), Rule-Based Expert Systems: the MYCIN experiments of the Stanford Heuristic Programming Project. Addison\u00ad Wesley, Reading, MA. 272-292. Hajek P. (1992) Deriving Dempster's rule. in IPMU '92 proceedings, pg. 73-75. Hsia Y.-T. (1991) Characterizing Belief with Minimum Commitment. DCAI-91:1184-1189. Huber P. J. (1973) The use of Choquet capacities in statistics. Bull. Internal. Statist. Inst. 45:181-191 Jaffray J.Y. (1992) Bayesian updating and belief functions. IEEE Trans. SMC, 22:1144-1152. Jaffray J. Y. (1989) Coherent bets under partially resolving uncertainty and belief functions. Theory and Decision 26:99105. Janez F. (1996) Fusion de sources d'information definies sur des referentials non exhaustifs differents. These. Universite d' Angers, France. Klawonn F. and Schwecke E. (1992) On the axiomatic justification of Dempster's rule of combination.", "startOffset": 0, "endOffset": 890}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information.", "startOffset": 4, "endOffset": 288}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems.", "startOffset": 4, "endOffset": 378}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems. in Gabbay and Smets (1998-99), Vol. 5. Kohlas J. and Monney P.A. (1995) A Mathematical Theory of Hints.", "startOffset": 4, "endOffset": 518}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems. in Gabbay and Smets (1998-99), Vol. 5. Kohlas J. and Monney P.A. (1995) A Mathematical Theory of Hints. An Approach to Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathematical Systems No. 425. Springer-Verlag. Kruse R. and Schwecke E. ( 1990) Specialization: a new concept for uncertainty handling with belief functions. Int. J. Gen. Systems 18:49-60. Kyburg H.E. Jr. (1987) Bayesian and non-Bayesian evidential updating.", "startOffset": 4, "endOffset": 844}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems. in Gabbay and Smets (1998-99), Vol. 5. Kohlas J. and Monney P.A. (1995) A Mathematical Theory of Hints. An Approach to Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathematical Systems No. 425. Springer-Verlag. Kruse R. and Schwecke E. ( 1990) Specialization: a new concept for uncertainty handling with belief functions. Int. J. Gen. Systems 18:49-60. Kyburg H.E. Jr. (1987) Bayesian and non-Bayesian evidential updating. Artificial Intelligence, 31:271-294. Laskey K. and Lehner P.E., ( 1989) Assumptions, Beliefs and Probabilities. Artif. Intel. 41: 65-77. Pearl J. ( 1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Pub. San Mateo, Ca, USA. Picard J. ( 1998) Modeling and combining evidence provided by documents relationships using probabilistic argumentation systems. Proc. ACM-SIGIR, Melbourne, Australia. Ruspini E.H. (1986) The logical foundations of evidential reasoning.", "startOffset": 4, "endOffset": 1355}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems. in Gabbay and Smets (1998-99), Vol. 5. Kohlas J. and Monney P.A. (1995) A Mathematical Theory of Hints. An Approach to Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathematical Systems No. 425. Springer-Verlag. Kruse R. and Schwecke E. ( 1990) Specialization: a new concept for uncertainty handling with belief functions. Int. J. Gen. Systems 18:49-60. Kyburg H.E. Jr. (1987) Bayesian and non-Bayesian evidential updating. Artificial Intelligence, 31:271-294. Laskey K. and Lehner P.E., ( 1989) Assumptions, Beliefs and Probabilities. Artif. Intel. 41: 65-77. Pearl J. ( 1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Pub. San Mateo, Ca, USA. Picard J. ( 1998) Modeling and combining evidence provided by documents relationships using probabilistic argumentation systems. Proc. ACM-SIGIR, Melbourne, Australia. Ruspini E.H. (1986) The logical foundations of evidential reasoning. Technical note 408, SRI International, Menlo Park, Ca. Schubert, J., (1995) Cluster-based Specification Techniques in Dempster-Shafer Theory, Proc.", "startOffset": 4, "endOffset": 1480}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems. in Gabbay and Smets (1998-99), Vol. 5. Kohlas J. and Monney P.A. (1995) A Mathematical Theory of Hints. An Approach to Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathematical Systems No. 425. Springer-Verlag. Kruse R. and Schwecke E. ( 1990) Specialization: a new concept for uncertainty handling with belief functions. Int. J. Gen. Systems 18:49-60. Kyburg H.E. Jr. (1987) Bayesian and non-Bayesian evidential updating. Artificial Intelligence, 31:271-294. Laskey K. and Lehner P.E., ( 1989) Assumptions, Beliefs and Probabilities. Artif. Intel. 41: 65-77. Pearl J. ( 1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Pub. San Mateo, Ca, USA. Picard J. ( 1998) Modeling and combining evidence provided by documents relationships using probabilistic argumentation systems. Proc. ACM-SIGIR, Melbourne, Australia. Ruspini E.H. (1986) The logical foundations of evidential reasoning. Technical note 408, SRI International, Menlo Park, Ca. Schubert, J., (1995) Cluster-based Specification Techniques in Dempster-Shafer Theory, Proc. European Conf. Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Springer-Verlag, Berlin. Shafer G. (1976) A mathematical theory of evidence.", "startOffset": 4, "endOffset": 1676}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems. in Gabbay and Smets (1998-99), Vol. 5. Kohlas J. and Monney P.A. (1995) A Mathematical Theory of Hints. An Approach to Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathematical Systems No. 425. Springer-Verlag. Kruse R. and Schwecke E. ( 1990) Specialization: a new concept for uncertainty handling with belief functions. Int. J. Gen. Systems 18:49-60. Kyburg H.E. Jr. (1987) Bayesian and non-Bayesian evidential updating. Artificial Intelligence, 31:271-294. Laskey K. and Lehner P.E., ( 1989) Assumptions, Beliefs and Probabilities. Artif. Intel. 41: 65-77. Pearl J. ( 1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Pub. San Mateo, Ca, USA. Picard J. ( 1998) Modeling and combining evidence provided by documents relationships using probabilistic argumentation systems. Proc. ACM-SIGIR, Melbourne, Australia. Ruspini E.H. (1986) The logical foundations of evidential reasoning. Technical note 408, SRI International, Menlo Park, Ca. Schubert, J., (1995) Cluster-based Specification Techniques in Dempster-Shafer Theory, Proc. European Conf. Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Springer-Verlag, Berlin. Shafer G. (1976) A mathematical theory of evidence. Princeton Univ. Press. Princeton, NJ. Shafer G. (1992) Rejoinder to Comments on \"Perspectives in the theory and practice of belief functions\".", "startOffset": 4, "endOffset": 1766}, {"referenceID": 1, "context": "and Smets Ph. ( 1992) The dynammic of belief in the transferable belief model and specialization-generalization matrices. in Dubois D., Wellman M.P., d'Ambrosio B. and Smets P. Uncertainty in AI 92. Morgan Kaufmann, San Mateo, Ca, USA, 1992, pg.130-137. Klir G.J. and Wierman M.J. (1998) Uncertainty based information. Physica-Verlag, Heidelberg. Kohlas J. and Haenni R. (1999) Assumption-based reasoning and probabilistic argumentation systems. in Gabbay and Smets (1998-99), Vol. 5. Kohlas J. and Monney P.A. (1995) A Mathematical Theory of Hints. An Approach to Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathematical Systems No. 425. Springer-Verlag. Kruse R. and Schwecke E. ( 1990) Specialization: a new concept for uncertainty handling with belief functions. Int. J. Gen. Systems 18:49-60. Kyburg H.E. Jr. (1987) Bayesian and non-Bayesian evidential updating. Artificial Intelligence, 31:271-294. Laskey K. and Lehner P.E., ( 1989) Assumptions, Beliefs and Probabilities. Artif. Intel. 41: 65-77. Pearl J. ( 1988) Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Pub. San Mateo, Ca, USA. Picard J. ( 1998) Modeling and combining evidence provided by documents relationships using probabilistic argumentation systems. Proc. ACM-SIGIR, Melbourne, Australia. Ruspini E.H. (1986) The logical foundations of evidential reasoning. Technical note 408, SRI International, Menlo Park, Ca. Schubert, J., (1995) Cluster-based Specification Techniques in Dempster-Shafer Theory, Proc. European Conf. Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Springer-Verlag, Berlin. Shafer G. (1976) A mathematical theory of evidence. Princeton Univ. Press. Princeton, NJ. Shafer G. (1992) Rejoinder to Comments on \"Perspectives in the theory and practice of belief functions\". Intern. J. Approx. Reasoning, 6:445-480. Smets Ph. (1978) Un modele mathematico-statistique simulant le processus du diagnostic medical.", "startOffset": 4, "endOffset": 1912}], "year": 2011, "abstractText": "We present examples where the use of belief functions provided sound and elegant solutions to real life problems. These are essentially characterized by 'missing' information. The examples deal with 1) discriminant analysis using a learning set where classes are only partially known; 2) an information retrieval systems handling inter-documents relationships; 3) the combination of data from sensors competent on partially overlapping frames; 4) the determination of the number of sources in a multi-sensor environment by studying the inter\u00ad sensors contradiction. The purpose of the paper is to report on such applications where the use of belief functions provides a convenient tool to handle 'messy' data problems.", "creator": "pdftk 1.41 - www.pdftk.com"}}}