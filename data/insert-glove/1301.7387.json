{"id": "1301.7387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Measure Selection: Notions of Rationality and Representation Independence", "abstract": "glanbia We synced take another ibew look cuadra at the kainer general bustos problem of warsak selecting sterilizes a preferred probability misdirect measure among those that comply husnu with some fistulas given constraints. e-r The archaism dominant gigaflops role that 4-1-1 entropy maximization has obtained in this tartar context 42.91 is 265.8 questioned 79.2 by ick arguing luke that nityanandam the otaqvar minimum information principle on rhubarb which it typecast is m\u00eame based unsa could mse be supplanted by kinross an pritchardia at least sacred as walls plausible \" oestrus likelihood currywurst of l\u00e9gende evidence \" squeezebox principle. We then review a method for turning given vocalion selection cardroom functions into grundfos representation returnable independent variants, excitons and num\u00e9ro discuss the sickest tradeoffs involved in this deprivations transformation.", "histories": [["v1", "Wed, 30 Jan 2013 15:04:45 GMT  (290kb)", "http://arxiv.org/abs/1301.7387v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["manfred jaeger"], "accepted": false, "id": "1301.7387"}, "pdf": {"name": "1301.7387.pdf", "metadata": {"source": "CRF", "title": "Measure Selection: Notions of Rationality and Representation Independence", "authors": ["Manfred Jaeger"], "emails": ["jaeger@mpi-sb.mpg.de"], "sections": [{"heading": null, "text": "We take another look at the general problem of selecting a preferred probability measure among those that comply with some given constraints. The dominant role that entropy maximization has obtained in this context is questioned by argu ing that the minimum information principle on which it is based could be supplanted by an at least as plausible \"likelihood of evidence\" prin ciple. We then review a method for turning given selection functions into representation indepen dent variants, and discuss the tradeoffs involved in this transformation.\n1 INTRODUCTION\nAn ever recurring theme in probabilistic inference is the se lection of preferred probability measures from some set of possible choices: we are given a state space A, a subset J of the set \ufffdA of probability measures on A, and are asked to identify a set I ( J) \ufffd J of measures that fulfill certain desiderata.\nOne example that instantiates this abstract schema is the se lection of a prior probability distribution in Bayesian statis tics. In this case, J is the set of all probability measures (usually restricted to a suitable parametric family) consis tent with our prior information, and I ( J) is the element se lected as our prior, usually on the basis of some minimum information principle. Another example is default seman tics for probabilistic knowledge bases. In this case A is the set of all models of some propositional language, J \ufffd \ufffdA is the set of models of some knowledge base KB in some probabilistic extension of propositional logic, and I(J) is a set of default models of KB that reflect certain common sense inferences to be drawn from KB.\nA number of studies (Shore & Johnson 1980, Paris & Ven covska 1990, Paris 1994) have addressed the question of what general rationality principles should guide our choice\nof I(J), and which formal method can be used to imple ment these principles. From these considerations, entropy maximization emerges as the unique selection rule that sat isfies our needs in general. Another selection rule, called center of mass by Paris (1994), against this background, does not seem to be a serious competitor of entropy max imization.\nThe purpose of the present paper is twofold. In the first part it is argued that center of mass inference, in spite of its poor performance with respect to theoretical rationality princi ples, is deeply entrenched in commonsense probabilistic reasoning. As an explanation for this phenomenon we argue that whereas entropy maximization is derived from a gen eral minimum information principle, center of mass infer ence might be justified by a no less viable likelihood of evi dence principle. In the second part of the paper we discuss the problem of representation dependence, both of center of mass and of maximum entropy inference. In (Jaeger 1996) a method was proposed that transforms selection rules I into variants i that are representation independent. In the current paper, results are presented that show what useful properties of I (particularly as expressed by rationality prin ciples) we have to trade in for representation independence, and which of these properties will be preserved in i.\n2 PRINCIPLES OF MEASURE SELECTION\nIn this section the basic definitions for measure selection functions and rationality principles are provided. We here present a purely semantic set of definitions. This is to say, we introduce the selection function I as operating on sets of probability measures \ufffdA. and that we formulate rational ity principles as conditions on the geometric form of I(J) given the geometric form of J. The alternative approach is syntactic: in that approach one focuses on a specific prob abilistic representation language, the class A of its models, and sets J \ufffd \ufffdA that are described by knowledge bases in the language. The selection function I is then seen as oper ating on know ledge bases KB, and rationality principles im-\npose conditions on the syntactic form of statements valid in I(KB) in terms of the syntactic form of KB. This approach is taken by Paris (1994).\nFor a finite state space A of size n, after ordering the ele ments of A in an arbitrary way, we can identify .6-A with\nA measure selection function I is any function that for every n E N maps \u00a39!(.6-n) (the powerset of .6-n) into \u00a39!(.6-n), such that I(J) \ufffd J for all J \ufffd .6-n, and I(rr(J)) = rr(I( J)) for all permutations 7f of (1, . . . , n) . This last con dition makes sure that a measure selection function defined on { .6. n I n E N} can be applied unambiguously to .6-A for arbitrary finite A, because it then does not matter what par ticular order we use on A. Note that in contrast with Shore and Johnson (1980) and Paris and Vencovska (1990) we do not demand I to be point-valued, i.e. I(J) to be a single measure in .6. n.\nIn this paper we are mostly concerned with two particular selection functions. The first is entropy maximization, de noted Ime\u2022 where from J \ufffd .6-n we select those P E J, P = (pl, ... ,Pn), for which H(P) :=- L:\ufffd=l Piln(pi) is maximal. Ime(J) is a singleton for closed and convex J; it is nonempty for closed J. When J is not closed, there need not exist an entropy maximal element in J, in which case I(J) = 0.\nThe second selection function we here consider is Icm. the center of mass selection function. The center of mass (jh, . . . , Pn) of J \ufffd .6. n is determined by\nwhere J(E) is the \"E-hull\" around J, i.e. the set containing all points of R n with a Euclidean distance smaller than E to some element of J; .\\ n is the n-dimensional Lebesgue measure. Taking the limit over the J (E ) in ( 1) is necessary, because J \ufffd .6. n has Lebesgue measure zero, so that both integrals in (1) would be zero when taken over J. Icm(J) is either a singleton (when the limit (1) exists, and the cen ter of mass so defined lies inside J - this is guaranteed for convex J), or else empty.\nNext, we briefly formulate the most important formal con ditions for I that were introduced as consistency axioms by Shore and Johnson (1980), and as (rationality) principles by Paris and Vencovska (1990). We state these conditions in a form that is semantic and generalizes the previous ver sions (given for point-valued selection rules I) to set-valued I. For intuitive motivations of the principles the reader is referred to (Shore & Johnson 1980) and (Paris 1994). Us ing the terminology of Paris and Vencovska, we here con sider the principles of relativization, obstinacy, indepen dence, and irrelevant information.\nMeasure Selection 275\nTo define the relativization principle we need the follow ing notation: if A is a state space, E \ufffd A, and P E .6-A, then PIE E .6-E denotes the conditional distribution of P on E. The notation Ec is used for the complement of E. I satisfies the relativization principle, if the following holds: whenever J is of the form\n(2)\nI(J)IE :={PIE I P E I(J), P(E) > 0} = I(JB).\nThe obstinacy principle says that whenever I(J) \ufffd J* \ufffd J, then I(J*) = I(J).\nThe independence principle does not generalize as unam biguously from point-valued I to set-valued I, as the pre vious two principles. We therefore here introduce two vari ants, strong independence and weak independence. To de fine these principles, let A = E x C be a product space; for P E .6-A let P f E, P f C denote the marginal distributions induced by PonE and C, respectively. Consider J E .6-A of the form\nfor some JB \ufffd .6-E, J0 \ufffd .6-C. We say that I satisfies the strong independence principle if for such J we have\nI(J) = I(JB) l8l I(J0) (4)\n:= {P l8l Q I p E I(JB), Q E I(JQ)},\nwhere l8l is the standard product of measures.\nConditions ( 4) is very strong, as it encodes an independence assumption not expressible by linear constraints, and that, in general, can be satisfied only by selecting non-convex I(J), even for convex J. A \"linear approximation\" to (4) is weak independence: we say that I satisfies the weak inde pendence principle if for J of the form (3), and all E' \ufffd E, C' \ufffd C we have\ninfi(J)(E' x C') = infi(JB)(E') \u00b7 infi(J 0)(C')\nsupi(J)(E' x C') = supi(JB)(E') \u00b7 supi(J 0)(C')\nwhere, e.g. infi(J)(E' x C') := inf{P(E' x C') I P E I(J)}.\nClosely related to the independence principles is the irrele vant information principle: this principle is satisfied by I if for J of the form (3) we get\n(5)\nTo these familiar rationality principles, we add one more technical property that will be needed below: I is called di mension independentifl(J x {0}) = I(J) x {0}.\n276 Jaeger\n3 CENTER OF MASS VS. MAXIMUM\nENTROPY\nTable 1 lists a number of results on which selection func tions satisfy the rationality principles listed in the previous section. For the time being, we are only concerned with the first two lines in the table, which list (for the most part well known) results about maximum entropy and center of mass. I me\u2022 of course, satisfies all of the listed principles, and the fact that (under some mild additional assumptions) it is the only selection function that will achieve this has been pro posed as a reason to prefer entropy maximization over all other selection rules (Shore & Johnson 1980, Paris & Ven covska 1990). Paris (1994) is careful to point out that such a justification of entropy maximization hinges on what he calls Watts assumption: the set J of possible measures en codes everything that is known to the expert whose judg ments the selection function is supposed to model. Given that J is all we know, and that nonetheless we are forced to choose some smaller (or unique) I(J) \ufffd J, it then is natu ral to let this choice be guided by the\nMinimal information desideratum (MID): through the selection of I( J) as little as possible additional information beyond J should be assumed.\nThe rationality principles now simply are rigorous condi tions capturing several aspects of the MID.\nEven though the MID is rather plausible, and persuasively leads us to the maximum entropy principle, it is not hard to find evidence that center of mass inference has an intuitive appeal as a model for commonsense probabilistic inference that is at least as great as that of maximum entropy. Recent works in which center of mass inference is applied in some guise are (Druzdzel & van der Gaag 1995) and (Grove & Halpern 1997). One source from which more or less ex plicit instances of center of mass inference originate is the fact that this selection rule arises naturally from an analysis of our selection problem from the point of view of Bayesian statistics. Let us briefly tum to this connection.\nIn Bayesian statistics a belief state over a state space A is maintained by a probability distribution P9 on some pa-\nrameter space e. Each() E e determines a distribution Po on A, so that a distribution pA on A is given by\nPA(B) = l Po(B)dP9. (6) The distribution defined by (6) is known as the predictive distribution. The probability distribution P9 is updated on observations of elements of A according to Bayes' rule.\nAt first sight, this Bayesian procedure seems to have a dif ferent field of application than measure selection rules like maximum entropy, because it takes as input an observed event, rather than a set of admissible probability distribu tions. It is well known, however, that by suitable model ing, Bayesian conditioning and entropy maximization can be made to bear on the same problems, and that they then often yield incompatible results (see (Uffink 1996) for a re view of results).\nOne perspective we can adopt in order to process informa tion presented as admissible subsets J \ufffd .6.A of distribu tions within the Bayesian framework is to view J as an ob served event J9 : = {B E e I Po E J} in e. We can then simply condition P9 on J9, and obtain via the poste rior distribution pe a new predictive distribution on A.\nIn general, e will not parameterize the whole set .6.A, i.e. there are P E .6.A with P -:/= Po for all (). For the case of finite A that we are here concerned with, however, we can choose e so that {Po I () E e} = .6.A. A canonical choice is e = .6. n' for which we then get the trivial identity Po = B.\nBefore the constraints J have been obtained, our distribu tion pe one would be chosen as a non-informative prior, which here is the Lebesgue measure on .6. n (normalized, so as to yield a probability measure). The posterior pe, after conditioning on J (or on J (E) ( E -t 0), if J has measure 0), then is simply the Lebesgue measure restricted to J, and the new predictive distribution on A is given by (1). Thus, we have seen that center of mass inference really can be under stood as an instance of Bayesian conditioning and marginal ization. Since these are inference techniques that we would certainly not consider irrational, and yet center of mass fails the rationality principles emenating from the MID, we have to question the MID as the exclusive notion of rationality, and look for alternatives. We motivate a proposal for a dif-\nferent principle on which to base our notion of rationality by two very simple examples.\nFirst, consider the scenario where we are told that a cer tain coin is biased, and yields heads with probability in the interval [0.6, 0.9). What should our assumption be on the exact value of P (heads)? Maximum entropy prescribes P(heads) = 0.6. How can we then justify the (arguably more intuitive) center of mass solution P(heads) = 0. 75? In this case, we might reason as follows. We will first con struct a likely scenario for how the original information P(heads) E [0.6, 0.9) was obtained. The most plausible such scenario here is to assume that the coin in question has been tossed a number of times, that the relative frequency of heads has been observed, and then has been imbedded in the confidence interval [0.6,0.9] chosen wide enough to render the statement P(heads) E [0.6, 0.9) a virtual certainty. Un der these assumptions, it would be most reasonable to take for our inferred value of P(heads) the originally observed frequency, which would be assumed to have been 0.75.\nIn a second example, suppose that we are told that in the first democratic parliamentary elections in the newly inde pendent Republic of Transcaucasia the Progressive Demo cratic Party (PDP) has gained at least 5% of the votes, the National Unity Party (NUP) has gained at least 55% of the votes, and that these were the only two parties on the ballot. What should our belief be about the actual result of the elec tion, i.e. what is our estimate of the probability P(PDP) of a random voter having voted for PDP? Maximum entropy says P(PDP ) = 0.45, but a probably much more sensible estimate is gained when we assume that the given constraint P(PDP ) E [0.05, 0.45) reflects the intermediate result af ter 60% of the votes have been counted, and that the final result is obtained by extrapolating this partial count to all votes cast, which would yield approximately 8% for PDP, i.e. P(PDP) = 0.08. This is different from the center of mass solution P(PDP) = 0.25, but shares with it the im portant characteristic of choosing a value in the interior of the constraint set, rather than on the boundary, as maximum entropy will do.\nThe two examples illustrate a form of probabilistic infer ence not accounted for by entropy maximization: when we are given the information that the \"true\" distribution P be longs to J, we are not limited to take this infomiation at its face value only, i.e. as a constraint on which P E D. n we may choose, but we can also take advantage of the \"meta information\" that \"P E J\" is exactly what we got to know. In the examples above we have used this meta-information together with plausible assumptions on how information J typically is generated to arrive at our results. More fo\ufffd mally, J has been interpreted as the value of a random vari able whose distribution is determined by the \"true\" value P E D. n we want to infer. Thus, the selection procedure J \ufffd I( J) essentially becomes a statistical parameter esti mation problem, and the guiding principle for this selection\ncan be formulated as the\nMeasure Selection 277\nMaximum likelihood of evidence desideratum (MLED): the set I(J) should contain those dis tributions that are most likely to produce the in formation J.\nOne might object that in the discussion of our examples we have blatantly violated Watts assumption, because our ar guments made use of the semantic content of the variables heads and PDP, which is information not contained in J. This is true as far as the two specific results argued for in the examples are concerned. It does not, however, compro mise the MLED as a general principle that we can aim for even when such semantic background information is miss ing. The question now, of course, is how the philosophi cal MLED can be sharpened into formal conditions in the same way that the rationality principles sharpen the MID, and what, if any, selection functions satisfy these condi tions. A first such condition that one might consider is to require that for convex J the selected set I(J) lies in the (relative) interior of J. This very weak condition already eliminates maximum entropy, but retains center of mass as a possible choice.\n4 REPRESENTATION INDEPENDENCE\n4.1 THE ISSUE\nIn the previous section it was argued that the failure of the rationality principle for center of mass need not be con strued as a conclusive argument against this selection rule. There is one rationality principle, however, whose violation by center of mass really is quite disturbing: center of mass is not language invariant (Paris 1994). This means that the result obtained by applying Icm to some knowledge base KB depends on our assumptions of what additional propo sitional variables there exist in our probability space except those actually mentioned in KB. If, for instance, in the coin tossing example of section 3 we had assumed that besides the variables heads there also is a variable quarter in our vo cabulary (standing for the fact that the coin in question is a quarter), then the mere presence of this additional variable will change our results for P(heads), even though there is no information given about quarter, let alone any informa tion linking heads to quarter.\nMaximum entropy satisfies language independence, but it fails to satisfy another property that can be seen as a fur ther rationality principle: representation independence. As a very simple illustration of the problem, we may com pare the results obtained by applying entropy maximiza tion to the two knowledge bases KB := P(A) \ufffd 0.9 and KB1 ::= P(A1 V A2) \ufffd 0.9, where A, A1, A2 are propositional variables. In the first case entropy maxi mazation yields P(A) = 0.5; in the second case P(A1 V\n278 Jaeger\nAz) = 0.75. That this is an often undesirable behav ior becomes clear when we substitute e.g. A=. \"there ex ists life on Mars\", A1 =.\"there exists plant life on Mars\", Az =.\"there exists animal life on Mars\". Based on examples like these, we can roughly describe representation depen dence of entropy maximization (and other selection func tions) as the property of returning results that depend on non-essential choices of language and syntax, rather than on semantic content only. It was not until recently, that infor mal, example-based, descriptions of representation depen dence received more rigorous underpinnings. Paris (1994) appears to have been the first to supply a precise concept of representation independence by introducing his atomic ity principle. This principle requires that inferred probabil ity values do not change when a knowledge base is trans lated by replacing a propositional variable by a formula in a new language,just as A was replaced by A1 V Az in our ex ample above. Paris then shows that this principle can not be satisfied by a selection function I that yields unique values I ( J) for closed and convex J. Paris and Vencovska ( 1997) later argue that atomicity is not a reasonable principle in the first place, because the fact whether representations A or A1 V A2 are used is relevant information that may very well influence our inferences.\nHalpern and Koller ( 1995) give a semantic definition of rep resentation independence, based on embeddings f : A -+ B of state spaces. This definition subsumes the atomicity principle, but can also be applied to selection problems not framed within the context of a propositional probabilistic logic.\nIn (Jaeger 1996) a generalization of atomicity along a differ ent line is provided by developing a concept of representa tion independence for arbitrary nonmonotonic logics. That concept, like atomicitiy, is syntactic, based on interpreta tions between formal languages.\nIn the remainder of this section, a definition of representa tion independence is given that combines elements from the ones found in (Halpern & Koller 1995) and (Jaeger 1996). In order to stay in line with the other definitions used in the present paper, the definitions we provide are semantic, but our motivation for these definitions very much derives from syntactic considerations.\nA definition of representation independence essentially hinges on a definition of what constitutes a representation change. In our first example, a representation change was given by the syntactic interpretation f : A H A1 V Az, which induces the mapping f : KB H KB1. Here the repre sentation KB1 is a refinement of the representation KB, ob tained by moving from a simpler to a richer language. In general, we will also want to deal with alternative represen tations of the same information, none of which is a strict refinement of the other. For instance, let KBz = P(Bt 1\\ 82) ::; 0.9. KB1 and KBz now represent the same informa-\ntion with respect to the correspondence A1 V Az ++ 81 /\\Bz. It is convenient to model such a correspondence as medi ated by a third \"common ground\" language, so that both representations are interpretations from this (poorer) lan guage. Here we can choose {A} as the common ground lan guage, and obtain KB2 = g(KB) under the interpretation g : A H 81 1\\ 82. In the terminology of (Jaeger 1996), KB1 and KB2 would be called representational variants with re spect to f and g.\nThe semantic analogues of syntactic interpretations are em beddings of state spaces (Halpern & Koller 1995).\nDefinition 4.1 Let A, B be finite state spaces. An embed ding of A in B is any function f : A -+ &(B) with a1 =I az =>/(at) n f(az) = 0, and B = UaEAf(a).\nIn the case where A and B are the sets of models of propo sitional languages LA and LB. any syntactic interpretation f : X H \u00a2 x (X ranging over the propositional variables of LA, \u00a2x being a formula in LB) induces an embedding of A in B: a model a for LA is mapped to the set of models f (a) \ufffd B (possibly empty) in which the formulas \u00a2 x have the same truth values as the variables X have in a.\nEmbeddings f : A -+ B induce a mapping f : AA -+ &(AB) via f(P) = {Q E AB I Va E A : Q(f(a)) = P(a)}. Note that we get f(P) = 0 exactly when there ex ists a E A with P(a) > 0 and f(a) = 0. For J \ufffd AA we write f(J) for UpEJf(P). For Q E AB we define /(Q) E AA via Q(a) := Q(f(a)). Finally, for H \ufffd AB, let /(H):= {/(Q) I Q E H}.\nFrom our informal discussion of when two knowledge bases are representational variants, and the definition of em beddings, we now derive a formal semantic definition of representational variants.\nDefinition 4.2 Let A, B, C be state spaces, f : C -+ A, g : C -+ B be embeddings. Let J \ufffd AA, H \ufffd AB. We say that J and H are representational variants with respect to f and g, written J \ufffd H, iff\n/(J) = g(H). (7)\nand\nJ = f(/(J)), H = g(g(H)) (8)\nCondition (7) says that J and H contain the same informa tion about the common ground state space C. Condition (8) essentially means that J and H do not contain any addi tional information about A and B, respectively, that is not given as a translated constraint on C. Condition (8) is quite restrictive, and in (Jaeger 1996) was not part of the defini tion of representational variants. We include it here solely for convenience, because our subsequent results only apply to this restricted notion.\nDefinition 4.3 A measure selection function I is called representation independent, iff for any A, B, C, j, g, J, H as in definition 4.2, we have that J \ufffd H implies\nf(I(J)) = g(I(H)). (9)\nDefinition 4.3 is very similar to the one given by Halpern and Koller ( 1995), only that their unidirectional notion of representation shifts is replaced by the symmetrical notion of representational variants. The definition of Halpern and Koller is here covered by the special case B = C, g the identity function, and f a faithful embedding (i.e. f(c) \"I 0 for all c E C). Also, it is easy to see that representation independence in the sense of definition 4.3 implies language independence.\n4.2 RE PRE SE NTATION INDE PE NDE NT SE LE CTION FUNCTIONS\nIn this section we study representation independent selec tion functions. First, we review a construction presented in (Jaeger 1996) that allows us to transform a given selection function I into a representation independent variant i. To motivate this construction, first assume that we are given the situation of definition 4.2, i.e. we have the three state spaces A, B, C, the em beddings j, g, and the sets J, H with J t-4 H. We want to select subsets i ( J), i (H) such that (9) holds. Here there is an obvious way of doing this: we simply use any selection function I to choose I(j(J)) = I(g(H)), and then let i(J) := f(I(f(J))),i(H) := g(I(g(H))). Because of (8) we have i(J) \ufffd J, i(H) \ufffd H.\nThe problem, of course, is that in general we are not given a special scenario J \ufffd H for which (9) has to be satis fied, but only some J \ufffd AA from which we have to choose i( J) such that (9) holds for every possible instantiation of the schema J \ufffd H. The key to defining i is the observa tion that there is a simplest state space C and an embedding f : C -+ A such that J = j(f(J)). For any selection function I we can therefore define i(J) := f(I(f(J))), which then defines a representation independent selection function i. The following definitions and results taken from (Jaeger 1996) (here somewhat reformulated to fit into our semantic framework) describe the construction.\nTheorem 4.4 Let J \ufffd AA. There exists a smallest state space S J and an embedding h : S J -+ A, such that J = h(!J(J)). SJ and hare unique up to renaming the elements of SJ.\nThe embedding h induces a partition {h ( s) I s E SJ} on A. In fact, we can choose this partition itself as a canonical representation of SJ. The embedding h then simply is the identity. The function fJ : AA -+ ASJ becomes the re striction !J(P) = Pf SJ, and the mapping h : ASJ -+\nMeasure Selection 279\nAA induced by h is the extension operator:\nExt(Q) := !J(Q) = {P E AA I Pf SJ = Q}.\nThus, theorem 4.4 can be restated as follows: for J \ufffd AA there exists a unique coarsest partition SJ of A, s.t.\n(10)\nIn the following, we will understand SJ to refer to this par tition.\nDefinition 4.5 Let I be a measure selection function. The measure selection function i is defined by\ni(J) := Ext(I(Jf SJ)). (11)\nTheorem 4.6 When I is a dimension independent selec tion function, then i is representation independent.\nThe following short example illustrates the definitions and theorems formulated so far.\nE xample4.7 Let A = {aba2,a3,a4},B {b1>b2},C = {cbc2,c3}. Letf: C-+ A, g: C-+ B be embeddings that map c1, c2, c3 to {a1, a2}, { a3}, { a4}, and {bl}, {b2}, 0, respectively. Let J \ufffd AA be defined by the constraints P( { a1, a2}) \ufffd 0.6 and P( { a4}) = 0. Let H \ufffd ABbe defined by the constraint P( {bl}) \ufffd 0.6. Then\nJ(J) = g(H) = {P E AC I P(c1) \ufffd 0.6,P({cg}) = 0},\nand J = f(f(J)), H = g(g(H)). Hence J t-4 H. The partition SJ is { { a1, a2}, { a3}, { a4} }; the partition SH is { {b!}, {b2} }.\nNext, we computeime(J) andime(H). For lme(J) we first determine Jf SJ, which is {P E ASJ I P({a1,a2}) \ufffd 0.6,P({a4}) = 0} (unlike here, it need not always be the case that the constraints defining J r s J are identi cal to the original constraints defining J). Thus, we get Ime(Jf SJ) = {(0.6, 0.4, 0)}, and\nlme(J) = Ext(Ime(Jf SJ)) = {P E AA I P({a1,a2} = 0.6,P(a4) = 0}\nNote that Ime(J) = (\ufffd, \ufffd' \ufffd' 0) tJ. lme(J). Similarly, we computeime(H) = {P E AB I P({bl}) = 0.6}. Finally, we can check that\nf(ime(J)) = {P E AC I P({c!}) = 0.6,P(c4) = 0}\n= g(fme(H)),\nso that (9) is satisfied.\nThe proof of theorem 4.4, which was given in (Jaeger 1995), is not constructive. The following results show that at least in the case of J being a polytope, SJ can be effectively con structed.\n280 Jaeger\nTheorem 4.8 Let A = { ai, . . . , an}, let J \ufffd \ufffdA be a polytope defined by k linear inequality constraints ci = \"\u00a37=I rijP(aj) s; Si (i = 1, . . . ,k;rij,Si E R). Then the partition SJ of A can be computed in time polynomial in kn.\nProof: In the sequel, we denote P( aj) by Pi. Computation of SJ amounts to deciding the equivalence relation\na\u00b7\"' a\u00b7\u00b7++ {a\u00b7 a\u00b7} C B \u2022 J \u2022 ,, J - for some BE SJ. (12)\nFor notational convenience, take i = 1, j = 2. For P = (PI,P2, .. . ,pn) E \ufffdA we abbreviatep3, .. . ,pn byp. We then get: ai \"'a2 iff VP = (pi,P2, p) E \ufffdA:\nP E J ++(PI+ P2,0,p) E J and (O,pi + P2,p) E J. (13)\nNow define for i = 1, ... , k: fi := max{ril, ri2}, and let Ci be the constraint defined by replacing ril and ri2 by fi in Ci. Let J be the polytope defined by the Ci. We show that\n(14)\nThe left to right direction in (14) is immediate: J = J im plies that J is definable by a set of constraints in which PI and P2 only appear within terms of the form f(pi + P2), so that (13) clearly is satisfied.\nFor the right to left direction, assume that a1 \"' a2\u2022 J \ufffd J trivially holds, because J is obtained from J by sharpening each defining constraint. Let P = (PI,P2,p) E J. Con sider i E {1, . . . , k }, and assume without loss of general ity thatfi = ril. By ( l3) we have that (PI +P2,0,p) E J. Since (PI + P2, 0, p) satisfies Ci iff (PI, P2, p) satisfies Ci, we obtain that P satisfies Ci. This holds for all i, so that J\ufffd J.\nA test for J \ufffd J can be conducted via k satisfiability tests for systems of k + 1 linear constraints on n variables. Each such test can be conducted in time polynomial in nk (e.g. (Chvatal 1983)). Finally, we have to do s; n(n- 1) tests of the relation \"', giving an overall runtime for the construction of SJ polynomial in nk. 0\nSo far, we only have presented some limited empirical ev idence that the selection functions i can still be interesting and useful. Example 4.7 shows that with lme nontrivial in ferences can be obtained. Also, if we apply lme to our ex ample of section 4.1, we find that we now infer P(A) = 0. 5, P(AI V A2) = 0.5, and P(BI 1\\ 82) = 0. 5, which ar guably is the most reasonable result. Beyond the evidence provided by examples like these, the net result of defini tion 4.5 and theorem 4.6, so far, only is that we know rep resentation independent selection functions to exist. This, in itself, is not exciting, because the trivial selection func tion I, with I(J) = J for all J, already is representation\nindependent. In order to evaluate the significance of the orem 4.6, we therefore have to look for general results on the properties of i. More specifically, the question of inter est is: how many of the original useful properties of I are preserved under the transformation I ---+ i, and what do we have to trade in for gaining representation independence?\nThe first major concession we have to make is precision: even where I returns point values, i, in general, will only return intervals. By the impossibility result of Paris (1994) that was mentioned in section 4.1, this is an unavoidable weakness of representation independent selection func tions. Another property that often holds for I, but usually is lost in i is continuity (see (Paris 1994) for a formal defi nition). To see why this is the case, consider the state space A = {a I, a2}, and Je \ufffd \ufffdA defined by the constraint P(ai) s; 1 - \u20ac. For every E > 0 we then have SJ. = {{a!}, {a2}}, and obtain, e.g., lme(Je) = {(0.5,0.5)}. ForE= 0, however, we have J0 = \ufffdA, SJ0 = { {ai,a2} }, and fme(Jo) = \ufffdA.\nWhile the loss of precision and of continuity run counter conventional desiderata for a selection function, they both can be justified to some extent by arguing that the relevant state space we should consider is not so much the more or less arbitrarily specified underlying set A, as the set of \"se mantic concepts\" implicit in our knowledge J. The ele ments of SJ are just the formalization of such a notion of semantic concept, and the use of i corresponds to using this set as the truly relevant state space. Introducing a constraint P(ai) s; 1 - E, then, more than establishing a numeri cal constraint, carries the impact of introducing {a I} as a semantic concept that we have to account for in our state space.\nThe following theorem provides some positive results on what is preserved under the transformation I ---+ i.\nTheorem 4.9 The principles of weak independence and ir relevant information are preserved under the transformation I ---+ i. If I is dimension independent, then the relativiza tion principle also is preserved under I ---+ i.\nProof: (Sketch) We only prove the statement for weak inde pendence and irrelevant information, omitting the (simpler) proof for relativization.\nThe key to the proof is the following observation: if J is of the form (3), then SJ is essentially the product of SJB and SJc. There is a minor complication: SJB [SJc] may con tain the element B0 := {b E B J VP E JB : P(b) = 0} [the similarly defined C0]. Then SJ will contain the \"irreg ular\" set Ao := B x Co UBo x C. The exact claim we want to prove is\nWe denote the right hand side of ( 15) by S*. It is straightfor ward to verify that J = Ext(Jf S*), which shows that S*\nis a refinement of S J. It remains to show, conversely, that S* is a refinement of SJ. This means that we have to show that there do not exist distinct elements si, s2 E S* with si \"\" s2 (since S* is a refinement of SJ, the equivalence re lation \"\" also is well-defined on S* by: si \"\" s2 iff a1 \"\" az for some a1 E si, a2 E s2). We proceed indirectly, and as sume that such si, s2 exist. The case where either si or s2 is equal to A0 easily leads to a contradiction, so we assume that si = (sf, sf), s2 = (sr, sf) with sf E SJB \\ Bo, sf E SJc \\Co (i = 1, 2).\nIt follows that there exists P E J with P(si) > 0. From si \"\" Sz, using criterion (13), it then follows that there also isQ E JwithQ(si) > O andQ(s2) > 0. Marginalizing on B yields the result: there exists Q8 E J B with Q8 (sf) > 0 and Q8(sf) > 0. Let E := min{Q8(sf ),Q8(sr ) }. From sf f sf (with \"\" defined by JC) it follows that there exists Qf E JC, Qf E D.C \\ JC that agree on all elements of S JC except sf and sf, and such that I Qf (sf) - Qf (sf) I = 8 < E. Without loss of gener ality, assume that Qf(sf)(= Qf(sf)) = Qf(sf) + 8. It is readily verified, that the two marginals Q8 and Qf can be extended to a measure 0 on S* with O(si) = min{ Qc (sf), Q8 (sf)} \ufffd 8. From si \"\"s2 it follows that 0' obtained from 0 by shifting probability mass 8 from si to s2 again is in J. But now the marginal of 0' on C is just Qf, which thus would have to belong to JC, contradicting our assumption. This completes the proof that S* \ufffd S J.\nHaving established (15), the remainder of the proof for weak independence and irrelevant information is simple. From the fact that Jf SJ is of the form (3), and the assump tion that I satisfies the respective principles, we obtain (2) and (5), respectively, for i(J)f SJ = I(Jf SJ). It is readily verified that the validity of (2) and (5) is preserved when then I ( J)f S J is extended to A via Ext( I ( J)f S J).\n0\nThe third line in table 1 summarizes the contents of theo rem 4.9 and the negative results mentioned above. Line 4 and 5 also explicitly list the properties of lme and lcm\u2022 some of which can be derived directly from the entries in lines 1- 3; others require short, separate, proofs.\n5 CONCLUSION\nRationality criteria for measure selection functions mostly have been formulated in terms of formal principles, or ax ioms. Entropy maximization has the best track record with respect to these principles; it is therefore often regarded as the one most reasonable selection rule. In this paper we have pointed out that rationality might also be based on the statistical criterion of identifying the most likely source for the information we are given, and that under such a changed perspective center of mass may look much more attractive than maximum entropy.\nMeasure Selection 281\nRepresentation independence is one intuitively reasonable formal principle that even entropy maximization fails to sat isfy. It has been shown that we can gain representation in dependence when we are willing to forfeit some of the de cisiveness of our inferences. This result is equally relevant for center of mass and for maximum entropy inference, as in particular it yields a language independent variant of center of mass. We have presented results that show that the mod ification of a selection rule I to its representation indepen dent variant i preserves at least some of those characteristic features of I that made I attractive in the first place.\nReferences\nChvatal, V. (1983), Linear programming, W.H. Freeman and Co., New York, NY.\nDruzdzel, M. J. & van der Gaag, L. C. (1995), Elicitation of probabilities for belief networks: Combining qualita tive and quantitative information, in 'Proceedings of UAI-95', pp. 141-148.\nGrove, A. J. & Halpern, J. Y. (1997), Probability update: Conditioning vs. cross-entropy, in 'Proceedings of UAI-97', pp. 208-214.\nHalpern, J. Y. & Koller, D. (1995), Representation depen dence in probabilistic inference, in 'Proceedings of IJCAI-95', pp. 1853-1860.\nJaeger, M. (1995), Default Reasoning about Probabilities, PhD thesis, UniversiHit des Saarlandes.\nJaeger, M. (1996), Representation independence of non monotonic inference relations, in 'Proceedings of KR'96', pp. 461-472.\nParis, J. B. (1994), The Uncertain Reasoner's Companion, Cambridge University Press.\nParis, I. & Vencovska, A. (1990), 'A note on the inevitabil ity of maximum entropy', International Journal of Approximate Reasoning 4, 183-223.\nParis, J. & Vencovska, A. (1997), 'In defense of the maxi mum entropy inference process', International Jour nal of Approximate Reasoning 17,77-103.\nShore, J. & Johnson, R. (1980), 'Axiomatic derivation of ilie principle of maximum entropy and the principle of minimum cross-entropy', IEEE Transactions on In formation Theory IT-26(1), 26-37.\nUffink, J. (1996), 'The constraint rule of the maximum en tropy principle', Studies in History and Philosophy of Modem Physics 27."}], "references": [{"title": "Linear programming, W.H", "author": ["V. Chvatal"], "venue": null, "citeRegEx": "Chvatal,? \\Q1983\\E", "shortCiteRegEx": "Chvatal", "year": 1983}, {"title": "Probability update: Conditioning vs. cross-entropy, in 'Proceedings of UAI-97", "author": ["A.J. Grove", "J.Y. Halpern"], "venue": null, "citeRegEx": "Grove and Halpern,? \\Q1997\\E", "shortCiteRegEx": "Grove and Halpern", "year": 1997}, {"title": "Representation depen\u00ad dence in probabilistic inference, in 'Proceedings of IJCAI-95", "author": ["J.Y. Halpern", "D. Koller"], "venue": null, "citeRegEx": "Halpern and Koller,? \\Q1995\\E", "shortCiteRegEx": "Halpern and Koller", "year": 1995}, {"title": "Default Reasoning about Probabilities", "author": ["M. Jaeger"], "venue": "PhD thesis,", "citeRegEx": "Jaeger,? \\Q1995\\E", "shortCiteRegEx": "Jaeger", "year": 1995}, {"title": "Representation independence of non\u00ad monotonic inference relations, in 'Proceedings of KR'96", "author": ["M. Jaeger"], "venue": null, "citeRegEx": "Jaeger,? \\Q1996\\E", "shortCiteRegEx": "Jaeger", "year": 1996}, {"title": "The Uncertain Reasoner's Companion", "author": ["J.B. Paris"], "venue": null, "citeRegEx": "Paris,? \\Q1994\\E", "shortCiteRegEx": "Paris", "year": 1994}, {"title": "A note on the inevitabil\u00ad ity of maximum entropy", "author": ["A. Vencovska"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "I. and Vencovska,? \\Q1990\\E", "shortCiteRegEx": "I. and Vencovska", "year": 1990}, {"title": "In defense of the maxi\u00ad mum entropy inference process", "author": ["J. Paris", "A. Vencovska"], "venue": null, "citeRegEx": "Paris and Vencovska,? \\Q1997\\E", "shortCiteRegEx": "Paris and Vencovska", "year": 1997}, {"title": "Axiomatic derivation of ilie principle of maximum entropy and the principle of minimum cross-entropy", "author": ["J. Shore", "R. Johnson"], "venue": null, "citeRegEx": "Shore and Johnson,? \\Q1980\\E", "shortCiteRegEx": "Shore and Johnson", "year": 1980}], "referenceMentions": [{"referenceID": 5, "context": "center of mass by Paris (1994), against this background, does not seem to be a serious competitor of entropy max\u00ad imization.", "startOffset": 18, "endOffset": 31}, {"referenceID": 4, "context": "In (Jaeger 1996) a method was proposed that transforms selection rules I into variants i that are representation independent.", "startOffset": 3, "endOffset": 16}, {"referenceID": 5, "context": "This approach is taken by Paris (1994).", "startOffset": 26, "endOffset": 39}, {"referenceID": 6, "context": "Note that in contrast with Shore and Johnson (1980) and Paris and Vencovska (1990) we do not demand I to be point-valued, i.", "startOffset": 27, "endOffset": 52}, {"referenceID": 5, "context": "Note that in contrast with Shore and Johnson (1980) and Paris and Vencovska (1990) we do not demand I to be point-valued, i.", "startOffset": 56, "endOffset": 83}, {"referenceID": 5, "context": "For intuitive motivations of the principles the reader is referred to (Shore & Johnson 1980) and (Paris 1994).", "startOffset": 97, "endOffset": 109}, {"referenceID": 5, "context": "Shore and Johnson (1980), and as (rationality) principles by Paris and Vencovska (1990). We state these conditions in a form that is semantic and generalizes the previous ver\u00ad sions (given for point-valued selection rules I) to set-valued I.", "startOffset": 61, "endOffset": 88}, {"referenceID": 5, "context": "I me\u2022 of course, satisfies all of the listed principles, and the fact that (under some mild additional assumptions) it is the only selection function that will achieve this has been pro\u00ad posed as a reason to prefer entropy maximization over all other selection rules (Shore & Johnson 1980, Paris & Ven\u00ad covska 1990). Paris (1994) is careful to point out that such a justification of entropy maximization hinges on what he calls Watts assumption: the set J of possible measures en\u00ad codes everything that is known to the expert whose judg\u00ad ments the selection function is supposed to model.", "startOffset": 290, "endOffset": 330}, {"referenceID": 5, "context": "There is one rationality principle, however, whose violation by center of mass really is quite disturbing: center of mass is not language invariant (Paris 1994).", "startOffset": 148, "endOffset": 160}, {"referenceID": 5, "context": "Paris (1994) appears to have been the first to supply a precise concept of representation independence by introducing his atomic\u00ad ity principle.", "startOffset": 0, "endOffset": 13}, {"referenceID": 4, "context": "In (Jaeger 1996) a generalization of atomicity along a differ\u00ad ent line is provided by developing a concept of representa\u00ad tion independence for arbitrary nonmonotonic logics.", "startOffset": 3, "endOffset": 16}, {"referenceID": 4, "context": "In the remainder of this section, a definition of representa\u00ad tion independence is given that combines elements from the ones found in (Halpern & Koller 1995) and (Jaeger 1996).", "startOffset": 163, "endOffset": 176}, {"referenceID": 4, "context": "In the terminology of (Jaeger 1996), KB1 and KB2 would be called representational variants with re\u00ad spect to f and g.", "startOffset": 22, "endOffset": 35}, {"referenceID": 4, "context": "Condition (8) is quite restrictive, and in (Jaeger 1996) was not part of the defini\u00ad tion of representational variants.", "startOffset": 43, "endOffset": 56}, {"referenceID": 4, "context": "First, we review a construction presented in (Jaeger 1996) that allows us to transform a given selection function I into a representation independent variant i.", "startOffset": 45, "endOffset": 58}, {"referenceID": 4, "context": "The following definitions and results taken from (Jaeger 1996) (here somewhat reformulated to fit into our semantic framework) describe the construction.", "startOffset": 49, "endOffset": 62}, {"referenceID": 3, "context": "4, which was given in (Jaeger 1995), is not constructive.", "startOffset": 22, "endOffset": 35}, {"referenceID": 0, "context": "(Chvatal 1983)).", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "By the impossibility result of Paris (1994) that was mentioned in section 4.", "startOffset": 31, "endOffset": 44}, {"referenceID": 5, "context": "Another property that often holds for I, but usually is lost in i is continuity (see (Paris 1994) for a formal defi\u00ad", "startOffset": 85, "endOffset": 97}], "year": 2011, "abstractText": "We take another look at the general problem of selecting a preferred probability measure among those that comply with some given constraints. The dominant role that entropy maximization has obtained in this context is questioned by argu\u00ad ing that the minimum information principle on which it is based could be supplanted by an at least as plausible \"likelihood of evidence\" prin\u00ad ciple. We then review a method for turning given selection functions into representation indepen\u00ad dent variants, and discuss the tradeoffs involved in this transformation.", "creator": "pdftk 1.41 - www.pdftk.com"}}}