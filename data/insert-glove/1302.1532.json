{"id": "1302.1532", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "A Standard Approach for Optimizing Belief Network Inference using Query DAGs", "abstract": "resenting This interlaced paper proposes a novel, algorithm - independent shut approach to optimizing belief hylsamex network leytonstone inference. rather than astyages designing abera optimizations 227.6 on sps an akademia algorithm by non-productive algorithm basis, we argue that chimps one ttwb should hazle use reconquered an unoptimized hagar algorithm 100.41 to avgi generate a yashin Q - sloganeering DAG, nightshirts a compiled graecae graphical representation hwan of dionisia the belief nikkan network, bajakian and chengue then tillakaratne optimize writer/director the Q - DAG marjon and 51-member its evaluator starkest instead. We longmen present romain a interment set przemy\u015bl of branes Q - DAG optimizations that supplant blaise optimizations sub-caste designed for traditional inference algorithms, poundcake including dung zero eadgyth compression, mwinkeljohn network hinterstoder pruning mossi and caching. We show streltsy that flavelle our chorrillos Q - 2,908 DAG starrs optimizations require time linear tamir in drop-in the ras Q - DAG size, 55.10 and significantly paneriai simplify vaishno the rudraprayag process akcha of smiley designing atic algorithms for 6.79 optimizing belief network inference.", "histories": [["v1", "Wed, 6 Feb 2013 15:54:47 GMT  (1020kb)", "http://arxiv.org/abs/1302.1532v1", "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)"]], "COMMENTS": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adnan darwiche", "gregory m provan"], "accepted": false, "id": "1302.1532"}, "pdf": {"name": "1302.1532.pdf", "metadata": {"source": "CRF", "title": "A Standard Approach for Optimizing Belief Network Inference using Query DAGs", "authors": ["Adnan Darwiche"], "emails": ["darwiche@aub.", "@rise."], "sections": null, "references": [{"title": "Query DAGs: A practical paradigm for implementing belief network inference", "author": ["Adnan Darwiche", "Gregory Provan"], "venue": "In Proceedings of the 12th Conference on Uncertainty in Artificial In\u00ad telligence (UAI),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Query DAGs: A practical paradigm for implementing belief-network inference", "author": ["Adnan Darwiche", "Gregory Provan"], "venue": "Journal of Artificial In\u00ad telligence Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Inference in belief networks: A procedural guide", "author": ["Cecil Huang", "Adnan Darwiche"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Bayesian updating in recursive graphical models by local computation", "author": ["F.V. Jensen", "S.L. Lauritzen", "K.G. Olesen"], "venue": "Computational Statistics Quar\u00ad terly,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Approxima\u00ad tions in Bayesian belief universes for knowledge based systems", "author": ["Frank Jensen", "Stig K. Andersen"], "venue": "In Proceedings of the Sixth Con\u00ad ference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "is the same as the time complexity of the underlying belief network algorithm; and (c) a Q-DAG evaluator is a very simple piece of software [1, 2] .", "startOffset": 140, "endOffset": 146}, {"referenceID": 1, "context": "is the same as the time complexity of the underlying belief network algorithm; and (c) a Q-DAG evaluator is a very simple piece of software [1, 2] .", "startOffset": 140, "endOffset": 146}, {"referenceID": 2, "context": "In a nutshell, when us\u00ad ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero\u00ad compression, and network-pruning [3, 4, 5].", "startOffset": 211, "endOffset": 220}, {"referenceID": 3, "context": "In a nutshell, when us\u00ad ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero\u00ad compression, and network-pruning [3, 4, 5].", "startOffset": 211, "endOffset": 220}, {"referenceID": 4, "context": "In a nutshell, when us\u00ad ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero\u00ad compression, and network-pruning [3, 4, 5].", "startOffset": 211, "endOffset": 220}, {"referenceID": 0, "context": "Details of this generation pro\u00ad cess can be found in [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 1, "context": "Details of this generation pro\u00ad cess can be found in [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "Each root node in a Q-DAG is either a nu\u00ad meric node, Num, which is labeled with a number p in [0, 1], or an evidence-specific node, Esn, which is la\u00ad beled with a pair (V, v) where V is a variable and v is a value of the variable.", "startOffset": 95, "endOffset": 101}, {"referenceID": 4, "context": "Zero compression is an optimization technique that is typically implemented in algorithms based on join trees [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "Zero compression, as presented in [5], addresses this wasteful propagation by visiting entries in cliques to identify and annihilate the zero entries.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].", "startOffset": 120, "endOffset": 129}, {"referenceID": 3, "context": "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].", "startOffset": 120, "endOffset": 129}, {"referenceID": 4, "context": "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].", "startOffset": 120, "endOffset": 129}, {"referenceID": 2, "context": "Figure 15, which is borrowed from [3], depicts the over\u00ad all control of the join tree algorithm.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Details of these operations are beyond the scope of this paper, but see [3] for a relatively comprehen\u00ad sive discussion.", "startOffset": 72, "endOffset": 75}], "year": 2011, "abstractText": "This paper proposes a novel, algorithm\u00ad independent approach to optimizing belief network inference. Rather than designing op\u00ad timizations on an algorithm by algorithm ba\u00ad sis, we argue that one should use an unop\u00ad timized algorithm to generate a Q-DAG, a compiled graphical representation of the be\u00ad lief network, and then optimize the Q-DAG and its evaluator instead. We present a set of Q-DAG optimizations that supplant opti\u00ad mizations designed for traditional inference algorithms, including zero compression, net\u00ad work pruning and caching. We show that our Q-DAG optimizations require time linear in the Q-DAG size, and significantly simplify the process of designing algorithms for opti\u00ad mizing belief network inference.", "creator": "pdftk 1.41 - www.pdftk.com"}}}