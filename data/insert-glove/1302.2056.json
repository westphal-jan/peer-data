{"id": "1302.2056", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2013", "title": "Complexity distribution of agent policies", "abstract": "We gstaad analyse catchwords the complexity -5000 of environments golds according to the policies carcieri that rezzonico need cuitl\u00e1huac to be euro39 used tuymans to achieve terraza high performance. apex The performance senecal results vrr for a connecticut-based population 240.7 of policies leads rockall to eekelen a 1,000-a distribution that is 75.2 examined schlumbergera in terms michaels of guben policy complexity dakkak and finnish-language analysed through reuilly several macur diagrams and indicators. The notion prescriber of wehrmachtbericht environment response hardanger curve is also introduced, by taida inverting tamu the 5-simplex performance results starachowice into magnitudes an ability pinkard scale. maestro We apply vtu all these concepts, diagrams berengaria and judicially indicators hiestand to torkel a fortean minimalistic 41:49 environment class, 1.48 agent - sigfrido populated elementary bajhang cellular wilayah automata, showing how the difficulty, autonomies discriminating belushi power and phosphite ranges (previous catana to ony normalisation) may vary malahat for ss14 several matsuzakaya environments.", "histories": [["v1", "Fri, 8 Feb 2013 15:01:20 GMT  (725kb,D)", "http://arxiv.org/abs/1302.2056v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jose hernandez-orallo"], "accepted": false, "id": "1302.2056"}, "pdf": {"name": "1302.2056.pdf", "metadata": {"source": "CRF", "title": "Complexity distribution of agent policies", "authors": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "emails": ["(jorallo@dsic.upv.es)"], "sections": [{"heading": null, "text": "Keywords: Algorithmic information theory, reinforcement learning, environment difficulty, discriminating power, agent policy, task difficulty, psychometrics, elementary cellular automata.\nar X\niv :1\n30 2.\nContents"}, {"heading": "1 Introduction 2", "text": ""}, {"heading": "2 Where to look at? 4", "text": "2.1 Looking at the problem . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Looking at the search space . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Looking at the population of solvers . . . . . . . . . . . . . . . . . . 8"}, {"heading": "3 The complexity of environments, actions and policies 10", "text": ""}, {"heading": "4 A distribution of agent policies 13", "text": "4.1 Graphical analysis and indicators . . . . . . . . . . . . . . . . . . . . 14 4.2 Environment response functions . . . . . . . . . . . . . . . . . . . . . 17"}, {"heading": "5 Case study: agent-populated elementary cellular automaton 22", "text": "5.1 Agent-populated elementary cellular automata: definition and examples 22 5.2 Experimental setting . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.3 Analysis of the distribution . . . . . . . . . . . . . . . . . . . . . . . 29 5.4 Estimation of the environment response curves . . . . . . . . . . . . 33"}, {"heading": "6 Discussion 38", "text": "6.1 Comparing to other kinds of complexity . . . . . . . . . . . . . . . . 38 6.2 Taking performance-guided heuristics into account . . . . . . . . . . 41 6.3 Applications, limitations and future work . . . . . . . . . . . . . . . 42"}, {"heading": "1 Introduction", "text": "Many scientific disciplines must evaluate natural and artificial systems as a daily basis, such as psychometrics, animal cognition and artificial intelligence. A fundamental issue in these disciplines is then to determine the difficulty and the discriminating power of a task or problem instance. The capabilities of a system are given by the kinds and difficulty of the tasks it can solve. In this sense, tasks become discriminating if they can be solved by some agents but not by others. As a result, it is of utmost importance to precisely determine not only the types of tasks that are used but also their difficulty and their discriminating power. Informally, difficulty is usually associated with the expected result for a set of individuals while discriminative power is usually associated with the variability of the results for a set of individuals.\nPsychometrics has dealt with the analysis of task difficulty and discriminating power. Item Response Theory (IRT) [14], for instance, is a well-founded approach in psychometrics where the difficulty (in terms of response curves) for each item is used to construct more effective tests, in order to derive scores and to obtain reliability measures. The \u201cdifficulty\u201d of an item or task is generally (but not always) calculated with the results of previous tests on the same population, and not as a result of an underlying theory of the intrinsic problem difficulty of the task at hand. In artificial intelligence, the emphasis has not usually been put on evaluation, but on the development of intelligent systems. Nonetheless, some domain-specific tests and benchmarks have usually been designed by including problem instances of various difficulty levels. In the end, the use of tasks of previously known difficulty and discriminating power is crucial to make testing effective, and get an accurate result with a minimum amount of tasks and time.\nIn more general terms, the concepts of complexity and difficulty appear in almost every field of science: physics, biology, psychology, (algorithmic) information theory, complex system theory, evolutionary computation, cryptography and many other fields. The analyses and measures are so diverse that any comprehensive account is far beyond of any survey, although we will give some pointers in the next section. The reason of such a diversity is due to the very different elements whose complexity we may be interested in (physical phenomena, life forms, bit strings, algorithms, reasoning processes, etc.) and also the different purposes (understanding phenomena, developing new methods or just measuring individuals). In fact, it is when we talk about agents (or any other kind of cognitive system) when we use the word \u2018difficulty\u2019 instead of \u2018complexity\u2019, as suggesting that \u2018difficulty\u2019 is a relative (cognitive) issue. In other words, complexity is usually associated with the problem (statement and solution) and difficulty is usually associated with the way or method to go from statement to solution. It is thus not strange that a very complex task (in terms of how statement and solution are expressed) may have an easy solution. Conversely, some sets of very simple rules (e.g., games, automata or mathemat-\nical conjectures) develop into (or emerge to) extremely intricate phenomena and solutions.\nIn this paper, we focus on the analysis of difficulty (and discriminating power) for a particular (but ultimately general1) setting. We concentrate on an interactive scenario, where an agent interacts with an environment through actions and observations. Instead of considering tasks with a particular episodic goal, we consider (possibly infinite) tasks where performance is evaluated in terms of rewards. This is in line with many approaches for cognitive system evaluation in psychology, animal cognition and artificial intelligence, most especially in the reinforcement learning setting. In the context of agent-environment interaction, the task is given by the environment and the good \u2018solutions\u2019 are given by sequences of actions that maximise rewards. These actions are the result of a policy, which determines a behaviour in general. The central idea of this paper is to put the emphasis on behaviours, rather than actions. We highlight the issue that the aggregated rewards for the possibly infinite set of policies may be distributed in many different ways. As we will see, the study of these distributions plays a crucial role in the estimation of task difficulty. Another distinctive feature of our approach is that we will assess environment difficulty using algorithmic information theory [41] (by estimating the Kolmogorov complexity of each policy) in order to (1) evaluate the information content of the policy, (2) make the complexity measure more independent of the representation language, due to the invariance theorem2, and (3) consider all (or many of) the alternative (but equivalent) expressions of the same policy, so making the estimation more robust, as with Solomonoff\u2019s algorithmic probability (this is exploited by the coding theorem method [10, 53]).\nSo, the main idea of this paper is that we see this difficulty from the standpoint of the policies, i.e., the agents, by calculating their complexity. We do not estimate the complexity of the environment. We do not estimate the complexity of the solution either (the sequence of actions). Instead, we estimate the complexity of the description of the policies, using a policy description language (an agent language) and compressing each program. As a result, we evaluate the difficulty of an environment by observing the distribution of policies in terms of their Kolmogorov complexity and their aggregated reward. The most straightforward analysis of the distribution is just looking at the frequency of simple policies achieving good results. In other words, a starting point can be the following \u2018maxim\u2019: \u201cIf the environment has many simple policies achieving good results then it is easy. Otherwise, it is difficult\u201d. From this starting point, we will also derive other more robust indicators, functions and graphical tools from this distribution, in order to say when an environment is more or less difficult. Also, we will also investigate whether the environment\n1Many other problems can be formulated using restricted environment classes (see, e.g., the hierarchy of environments in [37, ch. 3]).\n2The Kolmogorov complexity of the same object using two different description mechanism is the same, up to a constant that is independent of the object [41].\nis discriminating, not only in the beginning but after a random walk or after other policies.\nThe rest of this paper is distributed as follows. Section 2 shortly reviews some of the many different ways of measuring complexity appeared in the literature, distinguishing those that look at the problem (statement or solution), at the search space or at the population of solvers. Also, several notions of difficulty and discriminating power are introduced. Section 3 sets the focus on environments and gives definitions of their Kolmogorov complexity, the complexity of optimal action sequences and the complexity of their policies, and some connections between them. Section 4 analyses the whole distribution of policy complexity and aggregated reward, using graphical tools, summary indicators and adapting the notion of response function, which finally leads to some general indicators of complexity and discriminating power. Section 5 introduces minimalistic environments based on agent-populated elementary cellular automata and a very simple agent policy language. Using them, we show the distribution of rewards according to policy complexity, calculate their indicators and plot their response functions. Section 6 discusses how this approach relates to other kinds of complexity, analyses the applicability and limitations of the approach and explores avenues for future work."}, {"heading": "2 Where to look at?", "text": "There is a plethora of terms around the notion of \u2018problem difficulty\u2019, such as task difficulty, problem hardness or simply complexity. We will shortly review some of them. Instead of arranging them according to their origin: psychometrics, computer science, complexity theory, biology, etc., we will group the approaches in the literature according to what they look at in order to establish parameters such as difficulty or discriminating power."}, {"heading": "2.1 Looking at the problem", "text": "The first distinction that we need to do is between problem class difficulty and problem instance difficulty. In computational complexity theory [13], the focus is usually put on class complexity, e.g., the behaviour of an algorithm for all possible inputs, in terms of the time (or space) that an algorithm takes to solve a problem or to decide whether an object belongs to a set. Nonetheless, the notion of instance complexity has also been considered, related to the concept of average-case computational complexity, developed by Leonid Levin in the 1980s [40], which is of course related to the more general notion of average-case performance of algorithms (see, e.g., [35]). However, the point of view is the time (or space) that a given algorithm requires to solve a problem instance, such as quicksort being faster for an almost sorted instance than a randomly sorted one, not about the intrinsic complexity of the problem instance.\nHere we are interested in instance difficulty, from the point of a view of cognitive agent evaluation (running whatever algorithm or policy). This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5]. In this line, and closely related to the emerging field of artificial intelligence, Simon and Kotovsky explored, for decades, \u201cthe problem space of difficulty\u201d [48, 36]. However, most of these views of difficulty have been anthropocentric. The use of these measures for non-human subjects (especially for machines) may be misleading since it has been shown that many tasks that are very difficult for humans are very easy for machines and vice versa [11].\nA more mathematical (and computational) approach for associating complexity with the number (or the size) of items that are necessary to explain a concept (or solve a problem) is now known as algorithmic information theory (also known as Kolmogorov complexity [41]). As a result, in the past forty decades, the difficulty of solving a particular task has been occasionally related to its Kolmogorov complexity. This relation has been especially advocated for in inductive inference, because it makes sense to look at the length of the shortest pattern explaining the evidence, and this length seems to determine the difficulty of the problem. However, the relation is, at most, unidirectional3, and not always intuitive4. Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap. 7]), have been proposed instead, where not only the solution of the problem (the pattern) is analysed but also how difficult is to extract that solution from the evidence. These proposals are generally conceived for evaluating objects but some of them can also be used for quantifying difficulty in sequential inductive problems. For instance, a variant of Kt, known as intensional complexity, accurately captured the difficulty humans found on IQ test series problems [31, 21]. Outside induction, the idea of instance complexity [46][41, sec. 7.4] has also been developed with the use of algorithmic information theory, in front of the classical view of problem class complexity mentioned above. For other kinds of problems, such as general inductive and deductive problems, [20, 23, 22] also explored the use of algorithmic information theory, by considering the information gain from the problem to the solution.\nThe above approaches have mostly focussed on static or (at most) sequential problems. When we move to the evaluation of natural or artificial agents, we need to address the estimation of the difficulty of interactive tasks, or environments, where subsequent input is affected by the agent\u2019s actions. Here, we usually consider other elements as well, such as a score or a reward. In reinforcement learning, for instance, the difficulty of a problem depends on the goal and how rewards are\n3Some repetitive, but long, patterns (hence having high Kolmogorov complexity) may just require memory to see and store the repetition.\n4True random strings are incompressible, so leading to no \u2018solution\u2019 at all, either easy or difficult.\nassigned (according or not to this goal). Also, rewards are rarely normalised, and the aggregation of results for several environments could be biassed in favour of those environments with higher reward magnitudes. Another problem of an interactive setting is that each action leads to a different (sub)environment, so the results may be very sensitive to an early wrong action. This is why some works have advocated for ergodic environments [37], where the agent can always recover from a local \u2018hell\u2019 or \u2018heaven\u2019. Yet another issue is that estimating the reward of a policy may take many steps in the environment, so the mere checking of a solution takes a lot of time. Also, the choice of the aggregate reward function is crucial, especially if time is taken into account [26]. Finally, the evaluated agent can evolve during the process and become a different agent [29].\nDespite all these problems, many works on reinforcement learning, agents and robotics have considered specific ways of approximating difficulty. Typically, the mere \u2018size\u2019 of the problem (e.g., maze size, [51]) or the state space [43], have been used as approximations of the \u2018difficulty\u2019 of an environment. However, these approaches are usually domain-specific and are prone to a series of problem because difficulty is just handled in an informal or ad-hoc way.\nAlternatively, the application of Kolmogorov complexity to estimate the difficulty of an environment has also been explored, but becomes much more cumbersome. In [27], another variant of Kolmogorov complexity (Ktmax) was suggested as a measure of complexity for the environments, but it was already stated that this approximation was unidirectional, since some very complex environments might be easy, i.e., high rewards could be obtained by very simple policies ([27, sec. 4.1]). Also, the problem of whether some environments are more discriminating than others is also discussed there, and some notions (such as reward-sensitiveness) were introduced [27, sec. 4.2]. Following these ideas, an environment class was introduced in [25], where the difficulty of an environment can be approximated by the Kolmogorov complexity of the rules (as a Markov algorithm) that describe the environment. The class is used to create intelligence tests in [34], but the approximation of difficulty is not satisfactory. In fact, some other related works also mention the importance of being able to assess the difficulty of the environment, such [12] and to achieve discriminating power [24].\nEven more challenging, though, is when we consider other agents in the environment. In games (and multi-agent systems in general), the difficulty of a problem depends on the rules, but most especially on the opponents. For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].\nA more minimalistic approach, starting with very basic components is the study of the emergence of complexity (and other properties) in cellular automata and other artificial life universes (see, e.g., [50]). Algorithmic information theory has also been applied here; an interesting approach is the calculation of the complexity\nof the patterns that an elementary (1-dimensional) cellular automaton generates [54]. However, the purpose of Zenil et al. is not to measure the difficulty for an agent (not even a \u2018glider\u2019), but whether the 2-dimensional patterns have low or high complexity. In fact, it is very unlikely (or only likely after billions of generations) that in any of these settings or any other artificial life universe, we find \u2018agents\u2019 whose cognitive abilities may be interesting to evaluate."}, {"heading": "2.2 Looking at the search space", "text": "Instead of looking at the complexity of the environment, a natural way to look at the difficulty of a problem relies on analysing the search space. For instance, in a maze, the search space is the set of trajectories that can be performed in the maze. Each trajectory is a sequence of actions. Analysing how likely the solutions are in the space of trajectories may work because, in this particular case, the environment is static (the maze does not change after the agent\u2019s actions). However, things are different for other kinds of environment, where the environment really reacts to the agent\u2019s actions. In general, the same ideas and principles used in the areas of optimisation and heuristics apply here. For instance, it is said that a problem is difficult if there is a \u201chigh density of well-separated almost solutions (local minima)\u201d [9] or we have a \u201crugged solution space\u201d [32].\nEvolutionary computation is an area where the search space and its relation to difficulty has been studied more exhaustively. While fitness and reward functions could be considered parallel, most approaches consider a solution as achieving the problem goal, while we have advocated above for a case where have an aggregated reward, not a final goal. Nonetheless, the first big difference with a reinforcement learning case is that in evolutionary computation we consider the notion of operator, which converts (mutates) one solution into a different solution. The solutions are the nodes of the search space and the operators are edges. This makes up the \u2018landscape\u2019 in evolutionary approaches, but it is not clear how to adapt this to other settings. Another difference between environments and evolutionary problems is that for many approaches the size of the individual codes (genotype) is constant. Only when the genotype size is variable (e.g., evolutionary programming), the size of the solution is a factor of study. Taking these (and other) differences) into account, it is still useful to learn from the experience in this field. [19] includes an account of approaches for problem difficulty measures in evolutionary computation, appeared in the past twenty years. There are several interesting concepts such as \u201crugged fitness landscape\u201d (problems where the solution space is full of local minima and maxima), the \u2018needle in a haystack\u201d metaphor, the existence of one (or a few) way to the solution, deceptive ways (promising ways finally leading nowhere), the appearance of \u201cepistatic interactions\u201d between parts of the solution (gene co-influence), the notions of building blocks, and many others."}, {"heading": "2.3 Looking at the population of solvers", "text": "In psychometrics, items are usually classified into several difficulty categories, and a variety of items of different difficulty are used in order to cover a wide range of the ability that is to be measured. Item response theory (IRT) [14] is a paradigm for the study of items (tasks) and a well-grounded way of designing tests and other instruments that measure abilities, especially in the area of (computerised) adaptive testing. IRT is based on mathematical functions and associated probability and informativeness estimations for each item, according to several models. One very common model for discrete-score problems is the three-parameter logistic model, where the item response function (or curve) corresponds to the probability that an agent with ability \u03b8 gives a correct response to an item. This model is characterised as follows:\np(\u03b8) , c+ 1\u2212 c\n1 + e\u2212a(\u03b8\u2212b)\nwhere a is the discrimination (the maximum slope of the curve), b is the difficulty or item location (the value of \u03b8 leading to a probability half-way between c and 1), and c is the chance or asymptotic minimum (the value that is obtained by random guess, i.e., (1 + c)/2. as in multiple choice items). The zero-ability expected result is given when \u03b8 = 0, which is exactly z = c + 1\u2212c\n1+eab . Figure 1 (left) shows an example of a\nlogistic item response curve. For continuous score items, a very frequent approach is the linear model [44, 15]:\nX(\u03b8) , z + \u03bb\u03b8 +\nwhere z is the intercept (zero-ability expected result), \u03bb is the loading or slope, and is the measurement error. Again, the slope \u03bb is positively related to most measures of discriminating power [16]. Figure 1 (right) shows and example of a linear item response curve. Note that for continuous score items, if they are bounded, the logistic model may be more appropriate.\nWorking with item response models is very useful for the design of tests, because if we have a collection of items, we can choose the most suited one for the subject (or population) we want to evaluate. According to the results the subject has obtained on previous items, we may choose more difficult items if the subject has succeeded on the easy ones, we may look for those items that are more discriminative in the area we have doubts at, etc. Note that discrimination is not a global issue: a curve may have a very high slope at a given point, so it is highly discriminating in this area, but the curve will almost be flat when we are far from this point. Conversely, if we have a low slope, then the item covers a wide range of difficulties but the result of the item will not be so informative as for a higher slope.\nOne important question about these curves is how the parameters are estimated. This is usually done by applying the item to a population of subjects for which we already know their ability. Since the ability of subjects is usually determined by tests, leads to certain circularity, which is usually sorted out in an incremental way in psychometrics: the first tests were devised by comparing on informal assessments\nof both the ability of subjects and the difficulty of tasks, the next attempt refined them using the results from the previous ones, etc. It is important to note that there is no theoretical assessment of the intrinsic difficulty or discriminating power of items: this is done empirically by looking at the population.\nOverall, there have been approaches to derive problem difficulty (and other parameters) from the intrinsic complexity of the problem statement (or the solution), from the complexity of the way from the problem statement to the solution, i.e., the search space, and, finally, extrinsically, from a population. Some of them are informal and other are more rigorously defined in terms of the representation of problem statement, solution or search space. The main drawback of any of these, but most especially, the ones based on the search space, is that the measure of difficulty highly depends on the representation. For instance, if two evolutionary settings use different solution representation and different operators, the estimated difficulties for the same problem could highly differ. The approaches based on algorithmic information theory are less prone to this drawback because the use of Kolmogorov complexity may reduce this dependency due to the invariance theorem. Let us see next how to apply algorithmic information theory to the general case when an agent interacts with an environment, and how it leads to a population-oriented approach."}, {"heading": "3 The complexity of environments, actions and policies", "text": "The main goal of this paper is to study the difficulty that one agent faces on a given environment. In order to consider a very general setting (including games, for instance), we will consider that there might be other agents in the environment. Multi-agent environments are usually based on some common space, transition rules and reward system, which are shared by all of them. A more formal definition follows:\nDefinition 1. A multi-agent environment \u00b5 is a tuple \u3008\u03c3, \u03c4, \u03c9, \u03c1,\u03a0\u3009 where \u03c3 is a (state) space, \u03c4 is a state transition function, \u03a0 is a set of agents \u03c01, \u03c02, . . . , \u03c0n as policy functions and \u03c9 and \u03c1 are observation and reward functions (respectively) for all the agents.\nThe previous definition does not completely specify how the transition rules or the reward system work and the topology of the space, the possible contents and the movements of the agents. We will see a specific case in section 5. For the moment, it is enough to analyse the issue of difficulty in a general, but still formal way, with the following definitions and notation. Given any of the agents \u03c0i running for t steps on a deterministic environment, we denote by \u03c7ti its interaction history, which is a sequence of tuples of the form \u3008a, o, r\u3009, representing action, observation and reward respectively, and where a and o are elements in finite discrete sets A and O respectively, and r is a bounded rational number. The policy \u03c0i is then a function \u03c0i(\u03c7 t i) \u2192 a t+1 i . Similarly, the transition function takes the previous state and the actions of all the agents leading to another state: \u03c4(\u03c3t, at1, a t 2, . . . , a t n)\u2192 \u03c3t+1, from which observations and rewards are produced for all of them: \u03c9(st+1, i) = ot+1i and \u03c1(st+1, i) = rt+1i . Given an agent, we can just project over its history \u03c7 t i to get just the sequence of actions, denoted by \u03b1ti. We assume all transition function to be deterministic. We denote by Rti(\u03c0, \u00b5) an aggregation function of rewards which is applied to the sequence of rewards until time t (e.g., the average of all rewards so far for agent i).\nAs discussed in the previous section, we can try to estimate the difficulty of the environment in different ways: by looking at the problem, the solution or the search space. Here, the problem is given by the environment \u00b5, the solution is a sequence of actions \u03b1 and the search space is given by the exploration of all the sequences of actions.\nFirst, if we consider the problem statement (i.e., the environment), we can define its complexity K as5:\nK(\u00b5) , K(\u3008\u03c3, \u03c4, \u03c9, \u03c1,\u03a0\u3009)\nIf we take the point of view (or role) of agent i, then the complexity of the environ-\n5Unless precisely specified, K can be the Kolmogorov complexity, an approximation or other related function.\nment is defined as: K\u0307i(\u00b5) , K(\u3008\u03c3, \u03c4, \u03c9, \u03c1,\u03a0\u2212 {\u03c0i}\u3009) (1)\nThe use of K\u0307i(\u00b5) as an approximation of the difficulty of the environment (from the point of view of role i) is then the first possibility and would boil down to estimating the difficulty of an environment as the complexity of its space, transition function, observation function, reward function and other agents.\nAs mentioned above, we can consider a second approach: calculating the complexity of the solution. Focussing on one role i and a given episode length t, let us define Ati(\u00b5) as the set of sequences of actions made by any policy \u03c0 such that Rti(\u00b5, \u03c0) is maximised for role i.\nIt follows that |Ati(\u00b5)| \u2265 1 (there might be one or more sequences maximising the aggregated reward). From here, we can define the best-action difficulty (or hardness) h as follows:\nhti(\u00b5) , min \u03b1\u2208Ati(\u00b5) K(\u03b1) (2)\nHere, h(\u00b5) represents the complexity of the simplest action sequence leading to optimal aggregated reward.\nWhat is the relation of the two previous approaches? We can show that if K is Kolmogorov complexity then we have:\nProposition 1. Assuming an environment \u00b5 with computable space, transition, observation and reward functions, and also a computable reward aggregation function, we have:\n\u2200i \u2208 {1 . . . n} hti(\u00b5) \u2264+ K\u0307i(\u00b5) +K(t)\nwhere \u2264+ means that the inequality holds up to a constant which is independent of both terms.\nProof. In this case, consider one of the shortest descriptions (coded as binary strings) d for \u3008\u03c3, \u03c4, \u03c9, \u03c1,\u03a0\u2212 {\u03c0i}\u3009. Clearly l(d) = K\u0307i(\u00b5). If we know this shortest description we can simulate the environment \u00b5 and the interaction with all the agents (except \u03c0i). We now simulate (e.g., in parallel) all possible actions for agent \u03c0i until time t and record its histories and rewards. We just select those which maximise Rti and get exactly A t i(\u00b5). The description of how to make this simulation will have an overhead of c + K(t), which is independent of the environment (it only depends on the aggregation function Rti, the descriptional language being used, and t). Note that any computable function can be described with a finite program. Consequently, describing Ati(\u00b5) takes at most l(d) + c+K(t) bits. Naturally, hti(\u00b5) = min\u03b1\u2208Ati(\u00b5)K(\u03b1) \u2264 l(d) + c+K(t) + c \u2032 bits, where c\u2032 is the number of bits needed to describe what a minimum is and how to select it from the set Ati. Then: \u2200i \u2208 {1 . . . n} hti(\u00b5) \u2264 l(d) + c+K(t) + c\u2032 = K\u0307i(\u00b5) + c+K(t) + c\u2032, which completes the proof.\nNote that the previous rationale also leads toK(Ati(\u00b5)|\u00b5) \u2264+ K(t). The previous proposition implies that simple environments (with low K) cannot have complex optimal action sequences. However, the opposite implication is not true and we cannot find a lower bound. Also, the previous proposition relies on the use of K, which is incomputable. In fact, this upper bound would not be true in case we used some approximation of K taking time into account (such as Levin\u2019s Kt). Nonetheless, in many cases, it may still be true that the cost of describing a sequence of actions leading to (and calculating) the optimal result is smaller (in general) than the \u2018cost\u2019 of describing the environment and the rest of \u2018ingredients\u2019 of the problem. In other cases, however, this will not be true, as, e.g., in environments with very complex emergence behaviour. For example, in chess, it is still not trivial (in computational terms) what to do when knowing the rules and the moves of the opponent in order to, e.g., finish the match in least moves. Given infinite time, however, the optimal solution can be found.\nSo we have analysed the complexity of the problem formulation (the environment) and the solution (the sequence of actions). Now, we are going to consider agent policies. There are several reasons for this. First, the sequence of actions depends on the environment. For instance, consider a very complex environment which outputs a sequence of non-compressible observations and consider that the best reward is just attained by performing an action which is a simple function of the observation (e.g., if observations and actions were binary, just replicating the input as an output). Then the complexity of the sequence of actions would be high, but the solution to the problem is intuitively easy6. Second, action sequences depend on t and may become very long for long episodes. In contrast, policies are finite, which makes the distribution analysis much easier even with rough approximations of Kolmogorov complexity. Third, policies represent agents, i.e., behaviours, not sequences of actions. In the end, we are interested in knowing whether there are simple agents (i.e., simple policies) solving the problem. In fact, the same sequence of actions can be performed by two different agents, with very different complexities of their policies.\nBefore pushing forward the approach based on policies, we need a few definitions. Given a class of policies or agents \u2126, we define:\nRmax t i(\u00b5) , max \u03c0\u2208\u2126 Rti(\u03c0, \u00b5) (3)\nRmin t i(\u00b5) , min \u03c0\u2208\u2126 Rti(\u03c0, \u00b5) (4)\nRmean t i(\u00b5) ,\n1 |\u2126| \u2211 \u03c0\u2208\u2126 Rti(\u03c0, \u00b5) (5)\n6In fact, this is true in general, at least if K is Kolmogorov complexity, since K(Ati(\u00b5)|\u00b5) \u2264+ K(t).\nas the maximum, minimum, and average (respectively) aggregated reward attainable in t steps. In other words, this is the best (respectively worst, or average) score for any possible agent performing as agent \u03c0i in the environment. We can calculate the complexity of the best policy or, more precisely, the lowest complexity of any best policy, known as the best-policy difficulty (or hardness) ~ as follows:\n~ti(\u00b5) , min \u03c0:Rti(\u03c0)=Rmax t i(\u00b5) K(\u03c0) (6)\nAnd we get a similar result to the one above:\nProposition 2. Assuming an environment \u00b5 with computable space, transition and reward functions, and also a computable reward aggregation function:\n\u2200i \u2208 {1 . . . n} ~ti(\u00b5) \u2264+ K\u0307ti (\u00b5) +K(t)\nwhere \u2264+ means that the inequality holds up to a constant which is independent of both terms.\nProof. The proof is very similar to the one for proposition 1, but just choosing a policy that gets the best results given the environment.\nSo we have a very loose upper bound to ~ti(\u00b5). Since the actual value depends on a minimum, this suggests that we need to explore many agent policies in terms of their aggregated reward in order to estimate this. We take a look at the whole agent policy distribution below. From now on, we will drop i when it is clear which role we are using (or there is only one agent in the environment, i.e., if it is not a multi-agent environment) and drop t when clear from the context (or irrelevant for the matter at hand). Similarly, we will also drop \u00b5 when working with just one environment."}, {"heading": "4 A distribution of agent policies", "text": "The value ~(\u00b5) seen in Eq. 6 of the previous section looks like a good candidate to problem difficulty, as the lowest complexity of any optimal policy. If the best policy (or one of the best policies) has low complexity, the problem could be said to be easy. While this is a straightforward interpretation, it is too narrow because of several reasons. First, it might be the case that the best policy \u03c0 leads to R(\u03c0) = r while the second best policy \u03c0\u2032 leads to R(\u03c0\u2032) = r \u2212 . If is (comparatively) very small but K(\u03c0\u2032) << K(\u03c0), we may even say that an almost equally good (but much simpler) policy can be found. Intuitively, the environment would look even easier. Second, focussing on just one value is too risky if we plan to do a robust approximation of the value of difficulty, especially taking into account that in practice we will be forced to analyse samples of the agent population \u2126, and not the whole population itself.\nHence, the idea is to analyse R(\u03c0) against the complexity of \u03c0. This is what we see next."}, {"heading": "4.1 Graphical analysis and indicators", "text": "In order to study R(\u03c0) in terms of the complexity of \u03c0, we define slices of the whole distribution (conditional distributions actually) according to its complexity. Given a class of agents \u2126, we define:\n\u2126[k] , {\u03c0 \u2208 \u2126 : K(\u03c0) = k}\nFrom here, we can parametrise R as follows:\nDefinition 2. The aggregated reward per difficulty k is given by the following distribution:\nR[k] , {R(\u03c0) : \u03c0 \u2208 \u2126[k]}\nBy varying k we have a series of distributions. We will use the notation [\u2264 k] to represent all values of complexity from 1 to k. For instance R[\u2264 k] = \u2211 j=1..k R[j], known as the \u2018accumulated\u20197 version of R[k]. What R[k] looks like? This will highly depend on the environment so its shape can give us information about its difficulty. An example is shown in Figure 2. If we look at the distributions in this figure, we see that the distribution typically widens with increasing values of k. This does not mean that the expected reward increases (it remains constant in this case), but that the higher variability given by more complex programs make it possible to consider more diverse policies and find some that may be better.\nWe can derive some indicators of difficulty as a summarisation of the distribution. The first idea for difficulty is to consider the maximum. The maximum envelope is just defined as:\nRmax[k] , max \u03c0\u2208\u2126[k] R[k] (7)\nClearly, Rmax, as was already defined (see Eq. 3), can also be calculated as maxk Rmax[k]. In the example of Figure 2, this is 10.8. The previous Eq. 7 can be similarly defined for Rmin and Rmean. By looking at Rmax, Rmin and Rmean we can have an idea of the normalisation of the environment. If Rmean is not centred, it may indicate some kind of bias (i.e., the environment can be benevolent or malevolent, or simply that the rewards are not balanced). This suggests that results need to be normalised, by getting a new plot where the mean is centred. Normalisation is important because it makes possible to aggregate the results of one (or more agents) on several environments, by making them commensurable.\n7An \u2018accumulated\u2019 distribution is not a cumulative distribution.\nIf we look for one simple indicator of difficulty, yet again our first approach of a measure of difficulty could be to choose where Rmax is found:\nargmink=1..\u221e(Rmax[k])\nwhich is exactly equivalent to ~, as seen in Eq. 6. In the environment of Figure 2, we see that the maximum value is reached at complexity 30, so we have that ~ = 30. However, there are other policies with almost-as-good results at complexity 14. This shows that this indicator is not robust. We could also calculate the smallest value of k for a given value of R or other indicators. Nonetheless, all these indicators based on a single number are prone to problems for real situations, where we can only get a sample of the points, and not all of them. An alternative option then is to calculate similar values taking some quantile of the data into account, e.g., 99%, and get the lowest value of k such that we get 1% of the data above a given value (or, as a percentage of the maximum, e.g., 95% of Rmax). Many other statistical indicators can be used, including a comparison of the distributions. For instance, we could determine for which value of k the distributions stabilise (consecutive distributions become very similar) in order to determine when a more robust maximum has been reached. Another interesting indicator is to calculate the correlation between reward and complexity. If we consider the correlation of all the values, i.e., Cor\u03c0\u2208\u2126(K(\u03c0), R(\u03c0)), we should expect to have no correlation (as in Figure 2). Quite differently, we can calculate the correlation by slice using the maximum value, i.e., Cori=1..kmax(i, Rmax[i]) (with kmax being the maximum complexity which is considered), which in this case is 0.73 (Spearman correlation). Here, a high positive correlation is expected. Otherwise, the environment may even be penalising complex policies. We will see the use of some of these indicators in section 5.\nNonetheless, considering the envelope (either as given by the maximum points or by a quantile of the data) may not capture the difficulty of finding good rewards in general. This issue is related to how the points are distributed for each slice, which is, in turn, related to the notion of discriminating power. In order to clarify this, we need to be more precise about the meaning of being discriminating. This was initially stated, in an informal way, as a property of problems or tasks that can be solved by some agents but not by others, i.e., showing high variability in results. An interesting plot for understanding this would be to show the distribution by slice, or the distribution of R for the whole data. The degree of dispersion (measured by kurtosis or other indicators) could be useful here. For instance, if the values between Rmax and Rmin are distributed almost uniformly we have that there are many policies closer to the optimal result. If it has a more peaked shape (for instance like a beta distribution with \u03b1 = \u03b2 = 3), then the extreme values may be more difficult to reach. While this may give clear findings occasionally (we will look at several cases in section 5), this analysis may be unable, in general, to disentangle difficulty and discriminating power. Let us look for a different approach next."}, {"heading": "4.2 Environment response functions", "text": "As we mentioned in section 2, item response theory (IRT) [14] is a paradigm for the study of items (tasks) and a more principled way of designing tests and other instruments that measure abilities, especially in the area of (computerised) adaptive testing. The most distinctive feature about IRT is that each item (task) is categorised and analysed according to its difficulty in terms of the response that subjects of different ability degrees may show at the task.\nWe saw an example of the three-parameter logistic model and a linear model in Figure 1. Typically, for tasks which are led by bounded rewards, the curves should look something in between of the two models. Nonetheless, we do not even need to define a parametric model: what we really need is to determine a function that returns the expected reward given an ability level. In order to do this, we need to look at our policy (agent) distribution again.\nConsider a population of agents \u2126. We can define a probability distribution over them.\nDefinition 3. Given a population of agents, \u2126, the a priori policy probability, denoted w(\u03c0), is defined as a distribution over \u2126.\nThis gives more or less weight to policies and represents the probability of finding, using or exploring a policy. We can get a sample S of size N (without replacement) from the set \u2126 by using the probability w(\u03c0). This is denoted by \u2126||w,N .\nNow let us consider a tolerance value \u03b3, with 0 \u2264 \u03b3 \u2264 1. Given a random sample according to w and a tolerance value \u03b3, the probability that the sample of size N contains a value sufficiently closer (\u03b3) to the maximum aggregated reward of the population \u2126 is given by:\nqpos(\u03b3,\u2126, w,N) , Pr\n[ max\n\u03c0\u2208\u2126||w,N {R(\u03c0)} \u2265 (1\u2212 \u03b3)(Rmax \u2212Rmean) +Rmean\n] (8)\nThere is a baseline probability, even for extreme tolerance \u03b3 = 1:\nProposition 3. If Rmean is equal to the median of the aggregated reward in \u2126 and w(\u03c0) is independent of R(\u03c0) then q(1,\u2126, w, 1) = 1/2.\nProof. If N = 1 we just draw one element, and with \u03b3 = 1 we have:\nqpos(1,\u2126, w, 1) = Pr\n[ max\n\u03c0\u2208\u2126||w,N {R(\u03c0)} \u2265 (1\u2212 1)(Rmax \u2212Rmean) +Rmean ] = Pr [ max\n\u03c0\u2208\u2126||w,N {R(\u03c0)} \u2265 Rmean ] If the mean and the median match then there are exactly |\u2126|/2 elements above Rmean and the other half would be below, so the probability that taking just one is greater than or equal to Rmean is exactly 1/2.\nInterestingly, we can define qneg in a very similar way to qpos:\nqneg(\u03b3,\u2126, w,N) , Pr\n[ min\n\u03c0\u2208\u2126||w,N {R(\u03c0)} \u2264 \u03b3(Rmean \u2212Rmin) +Rmin ] Similar results as proposition 3 can be obtained for qneg. Note that the maximum (or minimum) is unique but there might be more than one policy reaching that maximum (minimum).\nWe now look for the minimum size of the sample such that the above probability (either qpos or qneg) is at least 1/2. In other words, we want to know how large the sample has to be to have probability 1/2 (or larger) of getting the maximum (up to a tolerance level).\nNpos(\u03b3,\u2126, w) , min{N : qpos(\u03b3,\u2126, w,N) \u2265 1/2} (9)\nNneg(\u03b3,\u2126, w) , min{N : qneg(\u03b3,\u2126, w,N) \u2265 1/2} (10) From here, we have:\nProposition 4. Under the same conditions of proposition 3:\nNpos(1,\u2126, w) = 1\nNpos(0,\u2126, w) = |\u2126|/2 For the second equality we also assume that the policy that produces Rmax is unique.\nProof. The first one follows directly from proposition 3. For \u03b3 = 0, we have that qpos simplifies to Pr(Rmax \u2208 \u2126||w,N ). Since w is independent of Rmax, and there is only one policy for Rmax, we need to draw half of the elements of \u2126 in order to have exactly probability 1/2.\nSimilarly for Nneg. By varying \u03b3 between 0 and 1 we have a function of the expected value of policies that we need to explore to approach a given value (in this case the actual maximum reward of the population, i.e., the best result). Depending on the choice of w, the functions Npos and Nneg will have different shapes. Since \u2126, the set of agent policies, can be very large (or even infinite), and this set is discrete, we need to consider a distribution that is appropriate for infinitely many discrete objects. A good choice is a universal distribution, using some measure of (Kolmogorov) complexity K, as follows:\nw(\u03c0) , 2\u2212K(\u03c0)\u2211\n\u03c0\u2032\u2208\u2126 2 \u2212K(\u03c0\u2032) (11)\nThis probability represents that simple policies will be sampled (explored) with much higher probability, which is a very reasonable choice as an a priori probability. Note that if w(\u03c0) is independent of R(\u00b5), as the assumptions of propositions 3 and 4, we necessarily have that Cor\u03c0\u2208\u2126(K(\u03c0), R(\u03c0)) = 0. However, the reverse is not true. If we use the previous w in Eq. 9 we have the following property:\nProposition 5. Consider \u03c0\u2217pos as any of the shortest policies with maximum reward, i.e., any element in the set argmin\u03c0{K(\u03c0) : R(\u03c0) = Rmax}. We have that:\nNpos(0,\u2126, w) \u2264 2K(\u03c0 \u2217 pos) \u2211 \u03c0\u2032\u2208\u2126 2\u2212K(\u03c0 \u2032)\nProof. Let us use the shorthand notation k\u2217 = K(\u03c0\u2217pos) and wall = \u2211 \u03c0\u2032\u2208\u2126 2 \u2212K(\u03c0\u2032). The probability of \u03c0\u2217pos appearing in a sample of size N is greater than or equal to 1 \u2212 (1 \u2212 w)N (this would be the probability with replacement). Then, since this policy gives the maximum in \u2126, we have that:\nqpos(0,\u2126, w,N) = Pr\n[ max\n\u03c0\u2208\u2126||w,N {R(\u03c0)} \u2265 (1\u2212 \u03b3)(Rmax \u2212Rmean) +Rmean ] \u2265 1\u2212 (1\u2212 w)N\nSince qpos is non-decreasing with N , in order to calculate N we make qpos equal to 1/2 in order to have Nmin :\nqpos = 1/2 \u2265 1\u2212 (1\u2212 w)N\n(1\u2212 w)N \u2265 1/2\n(1\u2212 2\u2212k \u2217\nwall )N \u2265 1/2\nN \u2264 log 1\u2212 2\u2212k \u2217 wall (1/2) N \u2264 log2(1/2) log2(1\u2212 2 \u2212k\u2217\nwall )\nN \u2264 \u22121 log2(1\u2212 2 \u2212k\u2217\nwall )\n(12)\nNow we use the following variable change t = 1\u2212 2\u2212k \u2217\nwall . Since k\u2217 > 0 we have that\n0 < t < 1. So the above inequality becomes:\nN \u2264 \u22121 log2(t)\n(13)\nWe see now that\n\u22121 log2(t) \u2264 1 1\u2212 t log2(t) \u2264 \u22121 + t ln(t)\nln(2) \u2264 \u22121 + t\nln(t) \u2264 (\u22121 + t) ln(2)\nwall\n)\n, we use fr = (2k)wall and fl = (2\nk\u22121)wall, fm lies tightly in between.\nThe plot shows these threes functions (fm solid, fr dashed, fl dotted), all with wall = 1.\nis true because for 0 < t < 1 we have that ln(t) \u2264 \u22121+t and (\u22121+t) < (\u22121+t) ln(2) since (\u22121 + t) is negative. Using this in Eq. 13 and undoing the variable change we have:\nN \u2264 \u22121 log2(t) \u2264 1 1\u2212 t N \u2264 1 2\u2212k\u2217\nwall\nN \u2264 (2k\u2217)wall N \u2264 2k\u2217 \u2211 \u03c0\u2032\u2208\u2126 2\u2212K(\u03c0 \u2032)\nIn fact, it can also be shown that this bound would be tight (if we used replacement in the sample), because if 1/2 \u2265 (1\u2212 2\u2212(k \u2217\u22121)\nwall )N then Eq. 12 would be an equality.\nFigure 3 illustrates how these two functions would make tight bounds.\nNote that the normalisation factor is 1 when K is a complexity function such that the universal measure derived from it is normalised. In any case, this is a constant value which only depends on the environment.\nThe previous result is also applicable to the shortest policies with minimum reward, \u03c0\u2217neg. In both cases, the expression suggests that in order to have a measure\nwhich is commensurable with the value of complexity k from which the probability w is used, we need to apply a logarithm to N . This leads us to propose the following metric:\nDpos(\u03b3,\u2126, w) , log2Npos(\u03b3,\u2126, w) (14)\nDneg(\u03b3,\u2126, w) , log2Nneg(\u03b3,\u2126, w) (15)\nAssuming w and \u2126 are fixed in what follows, this gives functions of difficulty in terms of \u03b3, i.e., Dpos(\u03b3) and Dneg(\u03b3). Note that, from propositions 3 and 4, we have that Dpos(1) = log2 1 = 0, i.e., the difficulty with tolerance \u03b3 = 1 is just 0. Similarly for Dneg. The maximum value of difficulty under independence of w(\u03c0) and R(\u03c0) would be log2 |\u2126|/2 = (log2 |\u2126|) \u2212 1. Note that we will usually calculate Dpos with a sample. If this sample is small, the estimation will be below the actual value of Dpos. As the sample becomes larger, the value will converge to its actual value (note that rewards are bounded).\nNow, if we are able to estimate Dpos, mapping \u03b3 into k, we can derive (Dpos) \u22121, which returns a value of \u03b3 that ensures 1/2 probability of finding that level of aggregated reward using a sample size related to 2k. Instead of interpreting the input as a difficulty, we can see as an ability of the agent, leading to curve very much like the item response curve. Also, we can just translate the output of (Dpos) \u22121 to an aggregated reward by applying the resulting tolerance expression, and defining <pos(\u03b8) , (1 \u2212 (Dpos)\u22121(\u03b1))(Rmax \u2212 Rmean) + Rmean, for \u03b8 \u2265 0 with output between Rmean and Rmax. This non-decreasing function can be plotted in the form of environment response curves. We can do similarly for Dneg leading to <neg(\u03b8) , (Dneg)\u22121(\u2212\u03b8)(Rmean \u2212 Rmin) + Rmin, for \u03b8 \u2264 0 with output between Rmean and Rmin. Putting both things together gives the following function:\nDefinition 4. The environment response function (or curve) is defined as:\n<(\u03b8) , <neg(\u03b8) if \u03b8 < 0 <pos(\u03b8) otherwise\nNote that the above curves are not normalised, because we recover the original scale given by Rmax, Rmin and Rmean. By just setting these terms to 1, \u22121 and 0 respectively, we can attain a normalised environment response curve. An illustrative example is shown in Figure 4.\nWe have used \u03b8 for the ability, as in Item Response Theory. The use of a negative ability may look strange initially, but it has a good interpretation. If we see the value of \u03b8 = 0 as the ability that is expected to score as the average of all policies, then performing worse than that can only happen by chance or because the agent is making an effort to get bad rewards. This leads to a duality in these plots."}, {"heading": "5 Case study: agent-populated elementary cellular au-", "text": "tomaton\nIn theory, we can analyse difficulty and discriminating power for any possible kind of environment which is compatible with definition 1, i.e., any discrete-time deterministic environment. In this section, we are going to choose a simplistic setting for practical reasons. First, we are interested in minimalistic environments where the number of observations and actions are extremely reduced, so having some relatively rich phenomena with very simple transition functions. Second, we are interested in simplistic policy languages in order to be able to evaluate a large amount of agents quickly. Third, we want to elaborate on settings that are well known and previously studied in terms of emergence and complexity. The configuration we present next follows these three conditions."}, {"heading": "5.1 Agent-populated elementary cellular automata: definition and examples", "text": "The environments we will work with are will use elementary cellular automata (ECA) [50] for the space \u03c3 and the transition function \u03c4 , and will let the agent see and modify part of the usual behaviour of the automaton. The following definition\nspecifies the complete behaviour of this kind of the environment:\nDefinition 5. A single-agent elementary cellular automaton (SAECA) is a special kind of environment \u3008\u03c3, \u03c4, \u03c9, \u03c1,\u03a0\u3009, as seen in definition 1, with the following extra parameters \u2329 \u03c30, \u03bd, p0 \u232a . The state \u03c3 is represented by a one-dimensional array of bits or cells \u03c31, \u03c32, . . . , \u03c3n, also known as configuration. We consider the array to be finite (length |\u03c3| = n) but circular in terms of neighbourhood (\u03c30 = \u03c3n and \u03c3n+1 = \u03c31). There is an initial array \u03c3\n0, also know as seed. The transition function \u03c4 is given by \u03bd, as any of the 22 3 = 256 rules that can be defined looking at each cell and its two neighbours according to the numbering scheme convention introduced in [50]. Given the behaviour of the space, we consider just one agent in \u03a0. The agent is located at one cell (its position p) with 1 \u2264 p \u2264 n, which is initially p0. The set of observations O is given by three bits \u3008c, l, r\u3009 representing the contents of the left and right neighbouring cells respectively, i.e., \u03c3p\u22121 and \u03c3p+1. The actions A are given by a pair of \u2018move\u2019 and \u2018upshot\u2019, denoted by \u3008V,U\u3009. The ordered set of moves is given by { stay, right, left }, and the ordered set of upshots is { keep, swap, set0, set1 }, which respectively mean that the content of the cell where the agent is does not change, the content of the cell is swapped (0 \u2192 1, 1 \u2192 0), the content is set to 0 and the content is set to 1. The rewards are calculated in the following way. If the agent is at position p at time t, then we use this formula:\nrt \u2190 \u2211\ni=1..bn/2c\n\u03c3tpos+i + \u03c3 t pos\u2212i\n2i+1\nwhich counts the number of 1s which are in the neighbourhood of the agent, weighted by their proximity. It is easy to see that 0 \u2264 rt \u2264 1.\nThe order of events for each step in the system is: observations are produced, actions are performed, the automaton is updated and finally, rewards are produced.\nNote that the environment is parametrised by the original contents of the array \u03c30 (including its size), the ECA rule number \u03bd, and the original position of the agent p0. Given an environment and a computable agent, the evolution of the system is computable and deterministic.\nIn order to explore the results for different policies, we need a language for expressing them. There are a few agent languages in the literature, but they are too oriented towards the architecture, are too focussed on Markov Decision Processes or are not sufficiently minimalistic for an exhaustive search over the policies (see, e.g., [1]). As a result, we just a introduce a new language, APL:\nDefinition 6. The agent policy language APL is given by a memory (or history) binary array m, initially empty (and not circular), and an ordered set of instructions I = { back=0, fwd=1, Xaddm=2, Xadd1=3, Yaddm=4, Yadd1=5 }. A program or policy \u03c0 is a sequence of instructions \u03b91, \u03b92, ..., \u03b9m in I. The interpreter works on its memory by using two accumulators V and U , and the action is given by the result of the accumulators at the end of the process. Namely:\n1. Read the observation \u3008c, l, r\u3009 and append it to the history array m.\n2. Place the memory pointer b at the end of m.\n3. V \u2190 stay\n4. U \u2190 keep\n5. forall \u03b9j\n6. case \u03b9j:\n7. back : b\u2190 max(b\u2212 1, 1)\n8. fwd : b\u2190 min(b+ 1, |m|)\n9. Vaddm : V \u2190 (V +mb) mod 3\n10. Vadd1 : V \u2190 (V + 1) mod 3\n11. Uaddm : U \u2190 (U +mb) mod 4\n12. Uadd1 : U \u2190 (U + 1) mod 4\n13. end case\n14. endfor\n15. return \u3008V,U\u3009\nThe language is clearly not universal, and all programs end. The goal of this language is not to be easily programmable but to be able to express some simple policies that may be useful in the environment.\nLet us see a few examples of how these environments and agents work. First, Figure 5 shows the evolution of several environments with seed \u201c010101010101010101010\u201d, p0= 11 and several values of \u03bd. We include an inert agent, i.e., an agent that does not affect the environment (i.e., an empty policy \u03c0). As a result, the resulting matrix after 200 iterations is the same as a classical elementary cellular automaton with each number \u03bd, with patterns that are well-known (see, e.g., [50]).\nThings change when we incorporate an active agent in the environment. Figure 6 shows how the environment with elementary cellular automaton number 110 varies for several agent policies. The resulting matrix patterns are different. Similar things (where differences are more visible) happen with rule number 164 (Figure 7).\nRule 26\nRule 73\nRule 122\nRule 110\nRule 150\nRule 164\nRule 184\nRule 110\nPolicy\nRule 110\nPolicy 42222404223555\nRule 110\nPolicy 1114442\nRule 110\nPolicy 24\nRule 110\nPolicy 2242555\nRule 110\nPolicy 425\nRule 110\nPolicy 23555\nRule 110\nPolicy random\nRule 164\nPolicy\nRule 164\nPolicy 22142335\nRule 164\nPolicy 24\nRule 164\nPolicy 21211240223555\nRule 164\nPolicy 425\nRule 164\nPolicy 42222404223555\nRule 164\nPolicy 23555\nRule 164\nPolicy random"}, {"heading": "5.2 Experimental setting", "text": "And now let us see how the plots, indicators and curve introduced in section 4 can be applied to this particular environment class. The idea is to evaluate a population of policies \u2126 against several environments. Of course, in order to make this evaluation finite, we set a fixed number of steps t. This is meant to resemble a single agent trying different behaviours until it finds the optimal one. As usual, an agent trying to learn the optimal policy will do a mixture of exploration, exploitation and also \u2018mind practising\u2019. By \u2018mind practising\u2019 we mean that an agent can construct a model of the environment and try some policies on that model \u2018mentally\u2019 without really implementing them into real actions. Taking into account this variety of strategies for a learning agent, we may think in at least four possible ways of evaluating policies in an environment:\n1. Fresh: We could think that whenever an agent tries a new policy in \u2126, it starts as if the environment were brand new, so the seed configuration is reinitialised for each policy. This is only realistic in environments which are ergodic or where bad actions do not lead to local heavens or hells. On the contrary, this option would be completely unrealistic if we just try a couple of policies and fall into a trap, making it impossible to try other policies.\n2. Random walk: We start with a brand new environment each time, but we execute a sequence of random actions before trying each policy. In other words, we calculate Rt from the environment after a random walk of t\u2032 steps.\n3. Chained: given a set of policies \u2126 we want to consider, we just choose a random order and evaluate them sequentially, without resetting the environment to its initial state. We calculate Rt for each policy as usual.\n4. Levin search: same as above (Chained policies) but the policies are ordered ascendingly by the length of their description.\nThe three last strategies are able to \u2018detect\u2019 when the environment is \u2018spoilt\u2019 by a bad policy and set into an unresponsive state where rewards cannot be modified any more. If this is the case, this will be shown in the distribution plots. For instance, in some ECAs, a previous policy may leave the configuration without 1s and the environment will not be able to easily recover from there.\nThe two last strategies can possibly use their memory and discard some policies according to their models, without actually trying them, as if their weight (their a priori policy probability w as for definition 3). Although we think that the first strategy is unrealistic, we have done experiments with the four of them. In what follows, we only show results with the Fresh and the Chained strategies, as these are more representative. In both cases, we interweave a 5% of random-walk policies (note that random-walk policies are non-deterministic and cannot be expressed by\nthe language APL). This random actions will also be useful to get information about any bias of the environment. Finally, in the case of the Chained strategy, this percentage of random walks actually makes the experiment a hybrid between Random walk and Chained.\nOnce decided how the search for policies is modelled, we now describe more details about the experiments. We set the number of steps to 300. The seed configuration is always \u201c010101010101010101010\u201d and the start-up agent position is p0 = 11. The set of policies \u2126 contains 2,000 policies. As mentioned above, 100 are random-walk policies (these are interwoven regularly between the others in the Chained strategy). The remaining 1,900 policies are randomly generated using a uniform distribution (since \u2126 is finite we can do this for w in definition 3) for the size of the APL program (between 1 and 20 instructions), so making up about 95 policies each. Given the size of the program, we choose each instruction with a uniform distribution among the six instructions in APL. Except for small lengths, policies are rarely equal, but they can be equivalent. For instance, a back instruction followed by a forward instruction cancel, many instructions at the end are useless if there is no Vaddm, Vadd1, Uaddm, Uadd1 after them, as well as some other simplifications. Because of this, we built a simplifier that transforms programs (when possible) into shorter ones. The simplifier is not exhaustive and some programs may have shorter equivalent version that the simplifier is not able to detect. Nonetheless, we estimate that it covers most cases. The application of the simplifier makes that the frequencies of programs of short size is slightly increased and the number of programs of high size (e.g., > 15) is reduced considerably. This phenomenon has to be taken into account when looking at the plots and results.\nFinally, in order to approximate the Kolmogorov complexity for each policy, we applied a compression algorithm to all the programs. In particular, we used a common approach [52, 34]: we coded the program as a character string and compressed it (using the memCompress function in R [47], a GNU project implementation of Lempel-Ziv coding). As a normalisation, from the resulting complexity we subtracted the length of the compressed size of an empty string. This makes the complexities range between kmin = 0 and kmax = 20. Given that Lempel-Ziv works with bytes and APL has only 6 instructions, the complexity has to be understood as being multiplied by log2(6) = 2.58 and not by log2(256) = 8. In any case, this factor is the same for all policies so it is irrelevant for our study. The important thing is that the joint use of the simplification and the compression processes can give a relatively acceptable (and efficient) approximation of Kolmogorov complexity for these short strings."}, {"heading": "5.3 Analysis of the distribution", "text": "As we have mentioned above, we performed experiments using a Fresh strategy and a Chained strategy, with a 5% of random-walk policies. Figure 8 shows the distributions R[k] for several environments (rules 184, 110, 122, 164) using the Fresh\nstrategy. In the Figure, each small circle (in grey) shows a policy. The box plots and the envelopes are shown for the accumulated values ([\u2264 k]). We show complexity (k) on the x-axis and aggregated reward (R) on the y-axis. As mentioned above, there is a higher concentration of values of k between 5 and 10, because policies are compressed from a uniform distribution between 0 and 20.\nWe see that the plots are very different in terms of average, range and evolution. The environment with rule 110 evolves very quickly for the maximum envelope, while rule 122 has a slower trend. The environment with rule 184 has very wide range and high dispersion. On the contrary, rule 164 has a small range (especially for rewards below the average). And there are also very important differences in terms of average aggregated reward (Rmean) as well. Some environments show some clusters. For instance, the environment with rule 184 shows a cluster of values around R = 0.95, as well as at 0.7, at 0.5, 0.3, and 0.05. There are some areas that are almost empty, even for high values of k, such as the range between 0.8 and 0.9. Finally, the results for the random-walk policies (shown on the left of the plots) are usually very compact, showing that an analysis using random actions (instead of policies) is not able to show the structure of the problem.\nThe values for Rmax for the four environments (184, 110, 122, 164) are 0.99, 0.73, 0.77 and 0.54 and the smallest complexities to reach these values (~) are 6, 3, 6 and 11. The Rmean values are 0.40, 0.56, 0.58 and 0.12 respectively, and the Rmin values are 0.01, 0.27, 0.30 and 0.01 respectively. The (Spearman) correlations Cor\u03c0\u2208\u2126(K(\u03c0), R(\u03c0)) are 0.12, 0.03, 0.17 and 0.09 respectively\n8, and the (Spearman) correlations Cori=1..kmax(i, Rmax[\u2264 i]) are 0.84, 0.70, 0.85 and 0.94 respectively. If we take a look at normalisation, we see that the average results for random-walk policies are 0.49, 0.57, 0.59 and 0.12, which are very similar to the Rmean values except for environment 184. This suggests that environments can be normalised to have 0 expected aggregated reward for a random-walk agent (random actions), as we did with the notion of \u2018balanced\u2019 environments in [27, 25, sec. 4.2], or we can do this to get 0 for a random policy, as we will do here, leading to different results.\nNow we analyse similar experiments with the Chained strategy in Figure 9. The shapes of the distributions are similar to the previous case, but there are some differences. For instance, the environment with rule 164 is very sensitive to previous policies, suggesting that many policies lead to a low number of 1s from where it is difficult to recover. In fact, the existence of non-recoverable configurations full of 0s cannot be ruled out. In fact, it is symptomatic that we see a higher average performance with random-walk agents (action-random) than with a set of random policies.\nWe now include some indicators for Figure 9 as well, shown in Table 1. While the values of Rmax are similar to the ones for the Fresh strategy, the ~ values are very different, confirming that this value is not robust to small changes (in\n8This small bias may be originated by some instructions setting 1s coming before the instructions setting 0s in the APL instruction set.\n31\nthe sample or the strategy). We see some relevant changes with respect to the Fresh strategy case in Rmin, which again indicates that some policies may lead to malevolent environment states from where it is difficult to recover.\nWe also did some experiments with a larger working sample \u2126 of 9,500 policies (10,000 minus the random walks). The results are also shown in Table 1. As we can see, while there is a strong similarity in results for many indicators, some other indicators vary slightly, and one, ~, is clearly not robust.\nFinally, we show the whole distribution without slicing by complexity in Figure 10. The narrow bars (usually higher and placed about in the middle) of the histograms show the 100 random-walk policies. The wide bars show the remaining 1,900 policies. We see that the histograms for the random-walk policies are peaked in the middle and with a normal-like shape. Apart from their location, randomwalk policies do not provide too much information. However, this is not the case for the other policies. In the case of environments with rules 110 and 122 we see a normal-like shape, which suggests that, in their ranges, discriminating power is quite regular, with more policies around the average and very few on the extremes of the distribution. On the contrary, environments with rules 184 and 164 show a very different picture. Rule 184 has a high concentration of policies for aggregated rewards close to 0, 0.5 and 1, and valleys for the rest. This makes it difficult to understand whether the discriminating power is high or low. Also the notion of difficulty is blurred here, because it seems difficult to reach 0 and 1 because of their location, but then there are many policies with these scores. Rule 164 gives a very asymmetric picture, and many policies behave worse than random.\nIn practice, we need a clearer way of determining the actual difficulty, normalisation and discriminating power of each problem. We introduced the environment response curves for that in section 4.2 and we see them next for this setting."}, {"heading": "5.4 Estimation of the environment response curves", "text": "First of all, we give a few indications of how the environment response curves are calculated. We estimate w(\u03c0) in Eq. 11, by using k as an approximation of K(\u03c0)\nRule: 184\nRule: 110\nand then we sample over the previously built population of \u2126, with 1,900 policies. Since we need to invert the functions Dpos and Dneg, we calculate 101 different values of \u03b3 at equal intervals from 0 to 1. For each value of \u03b3, we construct samples S (without replacement, i.e., S \u2190 \u2126||w,N ) with values of N = 1..1900, and we calculate max\u03c0\u2208S R(\u03c0) for each sample S. We calculate whether this value is greater than or equal to (1 \u2212 \u03b3)(Rmax \u2212 Rmean) + Rmean as in Eq. 8 for each of these samples, leading to an array of Boolean values denoted by B, whose size9 is 1,900. Note that this is not an estimation of qpos for each N , but just one case. Since we want to calculate Npos and Nneg (see equations 9 and 10), we need to estimate when qpos is greater than or equal to 1/2. We do this by summing how many falses there are in B. This is exactly Npos. Since our value for qpos was not a probability but just one case, we repeat the process 400 times in order to get a good estimate of Npos. Finally, by calculating the binary logarithm we have Dpos as for Eq. 14. This leads to a table of 101 values of \u03b3 and \u03b8. During the same procedure, we also calculate the neg versions (Nneg, Dneg, etc.) and get a second table. From these two tables, we can just get the inverted functions <pos, <neg and their joint version <.\nFigure 11 shows this function <. The tolerance levels can be easily seen on the y-axis (right), which is a (linearly) normalised version of the aggregated rewards (yaxis, left). Apart from the convenient way of showing how the values are normalised (so all the environments become commensurable if we look at the right y-axis), we can also get simple and clear indicators of difficulty and discriminating power. For instance, from these curves we can get estimations of the difficulty of each environment for different levels of tolerance as calculated in the caption of Figure 11. Also, the notion of discrimination can be analysed by looking at the shape of the response curves. For instance, rules 184, 122 and 164 have convex curves for the positive part (top right part of the curves) while 110 is concave. This indicates how difficulty varies for several values of tolerance. In fact, difficulty plunges from 8.90 to 3.10 when changing from 0% to 25% tolerance for rule 110. Also, it is interesting to look at these plots in a dual way, because just by changing the rewards r to 1\u2212 r in our environments, we would get environments with the structure given by the bottom left part of the curves.\nFinally, Table 2 shows the differences in the estimation of difficulties using a larger working sample: 9,500 (10,000 without the random walks). We see that these measures are more robust and convergent, especially for tolerance levels 0.05 and 0.10. The environment response curves are shown in Figure 12, and also have the same shape as those in Figure 11.\n9In the implementation, we do a trick to make things much faster: we check whether there are more than 10 \u2018trues\u2019 in a row. In this case, we consider that the process has stabilised and we stop, filling the rest of values with \u2018true\u2019."}, {"heading": "6 Discussion", "text": "In section 2 we mentioned many other approaches to the notions of difficulty and discriminating power. We will now compare our proposal and results with some of these previous notions and will put the contributions, limitations and future work in this context."}, {"heading": "6.1 Comparing to other kinds of complexity", "text": "We have used approximations of Kolmogorov complexity to analyse the policies. In section 3 we derived some upper bounds by evaluating the complexity of the whole environment. In the setting seen in section 5, this would have implied the compression of the definition of the configuration rules, the reward rules and the observation function. Approximating this as given by Eq. 1 would lead to a large number. For instance, the implementation of the environments and automata takes about hundreds lines of code in the language R. More compressed implementations are possible, but it is difficult to conceive an implementation whose length is less than a few thousand bits. So, proposition 1 is not useful as an approximation of difficulty in this case.\nA way-out might have been to consider some underlying setting or class of environments, from which we just add some parameters or rules. For instance, we could assume that the reference machine contains the basics for evolutionary cellular automata as well as the way agents and rewards are handled in this setting. With this assumption, we would only need to calculate how complex each rule is. The problem here, and one of the reasons we have chosen elementary cellular automata, is that there are only 256 rules. Consequently, it is very unlikely that any analysis of the complexity of the description of the environment like this would lead to differences in complexity greater than log2 256 = 8 bits.\nAnother possibility might have been to look at the patterns each ECA rule\ngenerates and see whether this correlates with the difficulty we can find on them. Fortunately, a deep and insightful analysis of the space-time diagrams have recently been performed by several studies [52, 54]. Note that if we analyse and try to compress their space-time diagrams (e.g., those depicted in Figure 5) we are looking at some of their emergence properties, not at the automaton itself. In other words, we only need to code part of it (of its emergent properties), and not the whole of it. As a result, the complexity may be much lower than the whole automaton (Eq. 1). If we just focus on the four rules we have used throughout the experimental section: 184, 110, 122 and 164, we have that [52], which uses the compression method to approximate Kolmogorov complexity, sorts them (from lowest to highest complexity) as follows: 164, 184, 122, 110. A related study, [54], which uses the coding theorem method through a \u2018block matrix decomposition\u2019, reaches slightly different values but exactly the same order. If we compare this order with the results of difficulty (with 0 tolerance) as shown in Figure 11, we have that the order is 184, 122, 164 and 110, and for (0.25 tolerance) it is 110, 184, 122, 164. In other words, the Kolmogorov complexity of how the ECA evolves in space-time does not seem to correlate with the difficulty that the agent may find in it. This is of course expected, since there are many other things (such as rewards or the influence of the agent in the ECA) that may lead to simple policies occasionally succeeding even if the configuration of the automaton becomes too messy (complex).\nThe take-away message of all this is something that we already pointed out in the introduction: the emergence of complexity is a very different thing to the emergence of difficulty. While some approaches to difficulty can work (and have worked) by looking at the environment complexity, any general account of difficulty must take an agent-centred point of view.\nAs discussed in previous sections, there are several options for this. One of them is to look at action sequences, as in Eq. 2. Figure 13 shows the aggregated reward results for 2,000 agents arranged by the complexity of their sequences of actions K(\u03b1) (each sequence is composed of 300 pairs of move and upshot, \u3008V,U\u3009). As we see, the complexity of the actions does not correlate with the maximum envelope (not shown in the figure). Moreover, we see that the correlation (Cori=1..kmax(i, Rmax[\u2264 i])) is clearly negative. As more complex the actions are, the variance is lower. This is partially caused by the number of policies leading to high-complexity action sequences is relatively smaller than the number of policies leading to low-complexity action sequences.\nAlthough Figure 13 cannot be used as a good source to identify the difficulty or discriminating power of an environment, it confirms some of the findings shown in other plots. For instance, the distribution for rules 110 and 122 is quite regular and matches the most regular histograms in Figure 10. Clearly, rules 164 and 184 lead to less complex space-time diagrams (they are the simplest according to [52, 54] and intuitively in Figure 5), so simple policies in these cases are not expected to create random-like (i.e., complex) action sequences.\nThis does not mean at all that Kolmogorov complexity and other tools from algorithmic information theory should not be applied for determining the difficulty of an environment. The take-away message is that for understanding difficulty we need to apply them on the policies, as we have done in this paper. In fact, the good thing about Kolmogorov complexity is that the results should be more independent of the language used to represent policies (our agent policy language APL here)."}, {"heading": "6.2 Taking performance-guided heuristics into account", "text": "One of the features of the approach introduced in this paper is that it is mostly independent of what heuristic a learning agent might consider to evaluate the policies, i.e., to explore the policy space. In fact, even sticking to one of the two last strategies (Chained and Levin) made in section 5.2, we do a sample of policies. Any reasonable (and feasible) sample will typically build short programs first instead of long ones, so this will be closely related to Levin\u2019s optimal search [39] any way, which is known to asymptotically dominate any other search. This assumes a finite episode length t and that the a priori probability w in definition 3 as set in Eq. 11 is not modified.\nHowever, it is of course highly debatable that having the opportunity of attempting several policies and seeing the ongoing reward for each of them, the exploration of the solution space still behaves blind on this, trying one policy after the other without modifying the a priori probability w. Also, we need to consider that some policies can be discarded by our mind models, as we also discussed under the term \u2018mind practising\u2019. It looks reasonable that if we try a policy and gets high reward, we typically modify that policy instead of attempting another one completely from scratch. This is in the end what most heuristics do, guided by gradient ascent or some other kind of maximisation process.\nLet us consider an example of this. Imagine a binary policy representation, i.e., strings over \u03a3 = {0, 1} and consider that we can just modify policies by adding or removing a bit at any position (Levenshtein distance d). Also consider that we explore each policy for a long time, so we can get a fairly good approximation of its performance. From here we can construct a graph, by connecting all the strings s and t such that d(s, t) = 1. Imagine that we have a way to determine whether some strings correspond to the same policy, allowing us to merge them into equivalence classes, with d(s, t) = 0. This leads to a graph, where for each node in the graph we have an estimated reward R and each edge requires the modification of one bit. These graphs can be explored by some kind of gradient-descent or any other heuristic. Without setting all of the details about how the heuristics work it is hard to figure out a criterion to determine a measure of difficulty from a graph like this: the length of the shortest non-decreasing path (if it exists), the best policy after a breadth-first, with or without backtracking, ... There are infinitely many heuristics taking the information into account. Some of them are hybrids, by trying to maximise results but also keeping solutions simple (e.g., MML-guided\nheuristics [49]). In fact, this binary representation is just one of the infinitely many possibilities, and the resulting graph would highly depend on it and, as a result, our indicators (and possible plots) of difficulty. We could even define a distribution of heuristics and calculate difficulty by an appropriate weighting of all of them. However, this would be infeasible in practice and could possibly boil down to some of the approaches, such as the one used in the paper, that try to ignore heuristics or assume a very general one such as Levin\u2019s search.\nNonetheless, we think that the specialisation of some of the plots, indicators and curves for families of heuristics could be a useful area of research, especially for the analysis of agent algorithms and other learning techniques. This would also connect this work to all the previous analysis on difficulty made in the area of evolutionary computation [19]. In this sense, some recent approaches based on the notion of motif, such as \u2018motif difficulty\u2019 [42] would be worth being investigated in relation to this work."}, {"heading": "6.3 Applications, limitations and future work", "text": "The main contribution of this paper is the view of difficulty in terms of how a population of policies (i.e., behaviours) perform on a given task. While this is a traditional view in psychometrics, especially in Item Response Theory, here we propose to construct the policies, i.e., the population, according to some distribution over a policy language, and evaluate their results in terms of the (Kolmogorov) complexity of the policy. By developing these ideas we have reached several findings. The alternative of using the (Kolmogorov) complexity of the environment leads to very loose upper bounds, which are useless in practice. We have also seen that analysing a problem by how random actions perform (the random-walk policy) is not able to unveil the structure of the problem. Using policies instead leads to a much better understanding. The distribution of policies in terms of complexity, either by the use of graphical tools or statistical indicators is informative. However, it is sometimes hard to understand and does not yield clear notions of difficulty and discriminating power, or how several environments can be normalised so their results become commensurable. The development of the environment response curves gives a much cleaner picture of the notions of difficulty and discriminating power and make the analysis directly operative for constructing tests and benchmarks, as Item Response Theory does.\nWhile the notion of difficulty derived here has intuitive interpretations, it is certainly difficult to expect that this could lead to a consensus about what difficulty is and how it can be measured. However, there seems to be more agreement in that assessing the difficulty of problems and tasks is a very important issue when evaluating agents and systems (either artificial or natural) and also when designing intelligent systems, artificial intelligence artefacts, heuristics, algorithms and almost any engineered thing that requires to solve a (computational) problem.\nThe first application of the tools introduced in this paper is the one that mo-\ntivated it: the construction of cognitive tests of different abilities (for artificial or natural systems). With the tools developed here we can now choose an environment class representing a cognitive ability, and select a subset of environments to integrate a test. This selection can now be done in a much more principled way. First, we can normalise the results of many environments, so the aggregation of results of several environments (tasks) become more meaningful. Second, we know their difficulty at any tolerance level, so we can choose those tasks that are more appropriate to the subject we want to evaluate. Third, by using their discriminating power, we can get maximum information from a single environment, and hence accelerate the evaluation. In fact, we can adapt techniques from (computerised) adaptive testing to do this. The generality of this approach makes it applicable to any kind of subject or system, since environments are a most general approach, which can be instantiated to almost any problem presentation in computer science (see [37, ch. 3] for an environment hierarchy). For instance, by using appropriate environment classes we can evaluate learning abilities, planning abilities, visual abilities, deductive abilities, etc. Note that a meaningful choice of each environment is much more effective than choosing them arbitrarily, can replace or complement those evaluations based on repositories and it is certainly more effective than choosing all the environments or selecting them by a (universal) distribution, as in [38].\nA second application is the assessment of ill-specified problems and interfaces. Throughout the paper we have been assuming that we knew the complete description of the environment or interactive task at hand. This is necessary for K\u0307 (Eq. 1), but not for the other indicators, plots and measures of difficulty based on (actions or) policies. This means that we can apply this assessment to multi-agent systems, where the behaviour or the other agents are unknown, possibly some of them being natural agents (animals or humans). This makes it possible to evaluate any environment, provided we can establish an appropriate interface, including real environments, games, robotic environments, etc. In fact, we can also check whether the use of different interfaces for the same task leads to different policy distributions, curves and indicators.\nA third application is to help develop better heuristics, especially in reinforcement learning. This can be done in two ways. First, we can analyse the kinds of environments (according to their response curves) that a given heuristic can achieve. This can provide useful information about the exact problems where the heuristic is struggling and also whether there is a neat correlation with the indicators of difficulty. Second, we can customise the techniques used here by changing the probability w of each policy according to some of the features of the heuristic and see how the results change. In any of both ways, the relation of our approach with Levin\u2019s search must be fundamental here.\nA fourth application is a better understanding of the notion of difficulty and its relation to existing problem solvers, especially in the area of natural and artificial evolution. Since our approach is based on the evaluation of population of\nagents, we can specialise this population according to a given natural population (as psychometrics does) or to some theoretical population (in theoretical biology, artificial life or evolutionary computation). How the distributions and measures of difficulty evolve generation after generation (when facing the same or different problems) could provide valuable information about the evolutionary process itself.\nThe contributions and applications mentioned above also carry some limitations. One limitation is the use of Kolmogorov complexity, which has two important problems: it is incomputable (and needs to be approximated) and it involves constants that depend on the reference machine, which may be very important (relatively) for small objects. Fortunately, there has been a significant improvement in the empirical approximation of Kolmogorov complexity [10, 53], which has also devised methods which are less sensitive to the reference machine. A second limitation is the computational effort required to calculate the distributions. Although the plots and measures for the minimalistic setting used in this paper can be calculated in a matter of minutes with a personal computer, things may grow exponentially for more elaborated environment classes and policy languages. Nonetheless, it is important to state that these evaluations do not have to be done in real time. They can be done beforehand and the results can be stored for successive applications of each environment. This is what psychometricians do; they construct a library or catalogue of items, whose response curves have been previously estimated (not effortless), and they can use them repeatedly on demand. Since our approach is not specialised to any kind of heuristic, environments that have already been evaluated can be used for a broader range of situations, also making the initial investment more beneficial.\nThere are also many new things to explore, some of them related to the issues above. For instance, we could use better approximations to Kolmogorov complexity or some other variants (such as Kt, already used in [31, 21]), leading to possibly more accurate views of difficulty, since time must be taken into account more explicitly from the beginning. The way in which the generation of policies is done could be rethought in order to resemble the way the coding theorem method works and get the estimation of Kolmogorov complexity (or any variant) directly, without further processing.\nOther settings have to be explored, such as the use of other kinds of agents, described with different (and universal) policy languages, using other kinds of environments, etc. For instance, we are considering applying this to mazes (in comparison with [51]), the matching pennies game (in relation to [30]) or generalisations of cellular automata, such as Random Boolean Networks [17], in a setting that would highly resemble the one introduced in [25].\nThere is also an interesting work to be done in terms of indicators and graphical representations. For instance, we have used an empirical approach to the derivation of the environment response curve, but some other IRT models of difficulty and discriminating power ([15, 16]) could be adapted to our case.\nOverall, while there has been an enormous effort and significant progress in understanding what complexity is and how it emerges, the question about what difficulty is and how it emerges is more elusive. We hope that this work is a step towards a better understanding and measurement of difficulty."}, {"heading": "Acknowledgements", "text": "The implementation of the elementary cellular automata used in the environments is based on the library \u2018CellularAutomaton\u2019 by John Hughes for R [47]. I am grateful to Fernando Soler-Toscano for letting me know about their work [54] on the complexity of 2D objects generated by elementary cellular automata."}], "references": [{"title": "Programmable reinforcement learning agents", "author": ["D. Andre"], "venue": "PhD thesis, University of California,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Computational depth: Concept and applications", "author": ["L. Antunes", "L. Fortnow", "D. van Melkebeek", "N.V. Vinodchandran"], "venue": "Theoretical Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Chapter 8 working memory, automaticity, and problem difficulty", "author": ["M.H. Ashcraft", "R.D. Donley", "M.A. Halas", "M. Vakali"], "venue": "Jamie I.D. Campbell, editor, The Nature and Origins of Mathematical Skills, volume 91 of Advances in Psychology, pages 301 \u2013 329. North-Holland,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Effective complexity and its relation to logical depth", "author": ["N. Ay", "M. Mueller", "A. Szkola"], "venue": "ArXiv e-prints, 0810.5663, October", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Dissociating working memory from task difficulty in human prefrontal cortex", "author": ["D.M. Barch", "T.S. Braver", "L.E. Nystrom", "S.D. Forman", "D.C. Noll", "J.D. Cohen"], "venue": "Neuropsychologia, 35(10):1373\u20131380,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["L. Busoniu", "R. Babuska", "B. De Schutter"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 38(2):156\u2013172,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Algorithmic information theory", "author": ["G.J. Chaitin"], "venue": "IBM J. Res. Develop., 21:350\u2013 359,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1977}, {"title": "Sophistication and logical depth revisited", "author": ["F.B. Chedid"], "venue": "Computer Systems and Applications (AICCSA), 2010 IEEE/ACS International Conference on, pages 1\u20134. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Where the really hard problems are", "author": ["P. Cheeseman", "B. Kanefsky", "W.M. Taylor"], "venue": "Proceedings of the 12th IJCAI, pages 331\u2013337,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}, {"title": "Numerical evaluation of algorithmic complexity for short strings: A glance into the innermost structure of randomness", "author": ["J.P. Delahaye", "H. Zenil"], "venue": "Applied Mathematics and Computation,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "IQ tests are not for machines, yet", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "Intelligence, 40(2):77 \u2013 81,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Compression and intelligence: social environments and communication", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo", "P.K. Das"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks (eds), editors, Artificial General Intelligence 2011, volume 6830, pages 204\u2013211. LNAI series, Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Theory of computational complexity, volume 58", "author": ["D.Z. Du", "K.I. Ko"], "venue": "Wiley- Interscience,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Item response theory for psychologists", "author": ["S.E. Embretson", "S.P. Reise"], "venue": "Lawrence Erlbaum,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Difficulty, discrimination, and information indices in the linear factor analysis model for continuous item responses", "author": ["P.J. Ferrando"], "venue": "Applied Psychological Measurement, 33(1):9\u201324,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Assessing the discriminating power of item and test scores in the linear factor-analysis model", "author": ["P.J. Ferrando"], "venue": "Psicol\u00f3gica, 33:111\u2013139,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Complexity and information: Measuring emergence, self-organization, and homeostasis at multiple scales", "author": ["C. Gershenson", "N. Fernandez"], "venue": "Complexity,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Problem difficulty and response format in syllogistic reasoning", "author": ["D.K. Hardman", "S.J. Payne"], "venue": "The Quarterly Journal of Experimental Psychology, 48(4):945\u2013 975,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "A note on problem difficulty measures in black-box optimization: Classification, realizations and predictability", "author": ["J. He", "C. Reeves", "C. Witt", "X. Yao"], "venue": "Evolutionary Computation, 15(4):435\u2013443,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Computational measures of information gain and reinforcement in inference processes", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "PhD thesis, DSIC, Universitat de Valencia,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Beyond the Turing Test", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "J. Logic, Language & Information, 9(4):447\u2013466,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "On the computational measurement of intelligence factors", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "A. Meystel, editor, Performance metrics for intelligent systems workshop, pages 1\u20138. National Institute of Standards and Technology, Gaithersburg, MD, U.S.A.,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Thesis: Computational measures of information gain and reinforcement in inference processes", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "AI Communications, 13(1):49\u201350,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "On discriminative environments, randomness, two-part compression and MML", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "Technical Report, available at http://users.dsic.upv.es/proy/anynt/,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "M. Hutter et al., editor, Artificial General Intelligence, 3rd Intl Conf, pages 182\u2013183. Atlantis Press, Extended report at http://users.dsic.upv.es/proy/anynt/unbiased.pdf,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "On evaluating agent performance in a fixed period of time", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "M. Hutter et al., editor, Artificial General Intelligence, 3rd Intl Conf, pages 25\u201330. Atlantis Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Measuring universal intelligence: Towards an anytime intelligence test", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Artificial Intelligence, 174(18):1508 \u2013 1539,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "On more realistic environment distributions for defining, evaluating and developing intelligence", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Insa-Cabrera"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks (eds), editors, Artificial General Intelligence 2011, volume 6830, pages 82\u201391. LNAI series, Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "On potential cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Minds and Machines,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Turing tests with Turing machines", "author": ["J. Hern\u00e1ndez-Orallo", "J. Insa", "D.L. Dowe", "B. Hibbard"], "venue": "Andrei Voronkov, editor, The Alan Turing Centenary Conference, Turing-100, Manchester, volume 10 of EPiC Series, pages 140\u2013 156,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity", "author": ["J. Hern\u00e1ndez-Orallo", "N. Minaya-Collado"], "venue": "Proc. Intl Symposium of Engineering of Intelligent Systems (EIS\u201998), pages 146\u2013163. ICSC Press,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "Sat-encodings, search space structure, and local search performance", "author": ["H.H. Hoos"], "venue": "International Joint Conference on Artificial Intelligence, volume 16, pages 296\u2013303. Citeseer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}, {"title": "On measuring social intelligence: Experiments on competition and cooperation", "author": ["J. Insa-Cabrera", "J.L. Benacloch-Ayuso", "Hern\u00e1ndez-Orallo J"], "venue": "AGI, volume 7716 of Lecture Notes in Computer Science,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Comparing humans and AI agents", "author": ["J. Insa-Cabrera", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Hern\u00e1ndez-Orallo"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks (eds), editors, Artificial General Intelligence 2011, volume 6830, pages 122\u2013132. LNAI series, Springer,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Sorting and searching, volume 3 of the art of computer programming", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1973}, {"title": "What makes some problems really hard: Explorations in the problem space of difficulty", "author": ["K. Kotovsky", "H.A. Simon"], "venue": "Cognitive Psychology, 22(2):143\u2013183,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1990}, {"title": "Machine Super Intelligence", "author": ["S. Legg"], "venue": "Department of Informatics, University of Lugano, June", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["S. Legg", "M. Hutter"], "venue": "Minds and Machines, 17(4):391\u2013444,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal sequential search problems", "author": ["L.A. Levin"], "venue": "Problems of Information Transmission, 9(3):265\u2013266,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1973}, {"title": "Average case complete problems", "author": ["L.A. Levin"], "venue": "SIAM Journal on Computing, 15:285,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1986}, {"title": "An introduction to Kolmogorov complexity and its applications (3rd ed.)", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Motif difficulty (md): a predictive measure of problem difficulty for evolutionary algorithms using network motifs", "author": ["J. Liu", "H.A. Abbass", "D.G. Green", "W. Zhong"], "venue": "Evolutionary Computation, 20(3):321\u2013347,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer of experience between reinforcement learning environments with progressive difficulty", "author": ["M.G. Madden", "T. Howley"], "venue": "Artificial Intelligence Review, 21(3):375\u2013398,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "Generalized linear item response theory", "author": ["G.J. Mellenbergh"], "venue": "Psychological Bulletin, 115(2):300,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1994}, {"title": "The magical number seven, plus or minus two: some limits on our capacity for processing information", "author": ["G.A. Miller"], "venue": "Psychological review, 63(2):81,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1956}, {"title": "Instance complexity", "author": ["P. Orponen", "K.I. Ko", "U. Sch\u00f6ning", "O. Watanabe"], "venue": "Journal of the ACM (JACM), 41(1):96\u2013121,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1994}, {"title": "R: A language and environment for statistical computing", "author": ["R Team"], "venue": "R Foundation for Statistical Computing, Vienna,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Human acquisition of concepts for sequential patterns", "author": ["H.A. Simon", "K. Kotovsky"], "venue": "Psychological Review, 70(6):534,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1963}, {"title": "Statistical and Inductive Inference by Minimum Message Length", "author": ["C.S. Wallace"], "venue": "Springer-Verlag,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "A new kind of science", "author": ["S. Wolfram"], "venue": "Wolfram media Champaign, IL,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning mazes with aliasing states: An LCS algorithm with associative perception", "author": ["Z. Zatuchna", "A. Bagnall"], "venue": "Adaptive Behavior, 17(1):28\u201357,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Compression-based investigation of the dynamical properties of cellular automata and other systems", "author": ["H. Zenil"], "venue": "Complex Systems, 19(1):1\u201328,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Une approche exp\u00e9rimentale \u00e0 la th\u00e9orie algorithmique de la complexit\u00e9", "author": ["H. Zenil"], "venue": "PhD thesis, Dissertation in fulfilment of the degree of Doctor in Computer Science, Universit\u00e9 de Lille,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "Two-dimensional kolmogorov complexity and validation of the coding theorem method by compressibility", "author": ["H. Zenil", "F. Soler-Toscano", "J.P. Delahaye", "N. Gauvrit"], "venue": "arXiv preprint arXiv:1212.6745,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Item Response Theory (IRT) [14], for instance, is a well-founded approach in psychometrics where the difficulty (in terms of response curves) for each item is used to construct more effective tests, in order to derive scores and to obtain reliability measures.", "startOffset": 27, "endOffset": 31}, {"referenceID": 40, "context": "Another distinctive feature of our approach is that we will assess environment difficulty using algorithmic information theory [41] (by estimating the Kolmogorov complexity of each policy) in order to (1) evaluate the information content of the policy, (2) make the complexity measure more independent of the representation language, due to the invariance theorem2, and (3) consider all (or many of) the alternative (but equivalent) expressions of the same policy, so making the estimation more robust, as with Solomonoff\u2019s algorithmic probability (this is exploited by the coding theorem method [10, 53]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 9, "context": "Another distinctive feature of our approach is that we will assess environment difficulty using algorithmic information theory [41] (by estimating the Kolmogorov complexity of each policy) in order to (1) evaluate the information content of the policy, (2) make the complexity measure more independent of the representation language, due to the invariance theorem2, and (3) consider all (or many of) the alternative (but equivalent) expressions of the same policy, so making the estimation more robust, as with Solomonoff\u2019s algorithmic probability (this is exploited by the coding theorem method [10, 53]).", "startOffset": 596, "endOffset": 604}, {"referenceID": 52, "context": "Another distinctive feature of our approach is that we will assess environment difficulty using algorithmic information theory [41] (by estimating the Kolmogorov complexity of each policy) in order to (1) evaluate the information content of the policy, (2) make the complexity measure more independent of the representation language, due to the invariance theorem2, and (3) consider all (or many of) the alternative (but equivalent) expressions of the same policy, so making the estimation more robust, as with Solomonoff\u2019s algorithmic probability (this is exploited by the coding theorem method [10, 53]).", "startOffset": 596, "endOffset": 604}, {"referenceID": 40, "context": "The Kolmogorov complexity of the same object using two different description mechanism is the same, up to a constant that is independent of the object [41].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "In computational complexity theory [13], the focus is usually put on class complexity, e.", "startOffset": 35, "endOffset": 39}, {"referenceID": 39, "context": "Nonetheless, the notion of instance complexity has also been considered, related to the concept of average-case computational complexity, developed by Leonid Levin in the 1980s [40], which is of course related to the more general notion of average-case performance of algorithms (see, e.", "startOffset": 177, "endOffset": 181}, {"referenceID": 34, "context": ", [35]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 44, "context": "This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5].", "startOffset": 246, "endOffset": 260}, {"referenceID": 2, "context": "This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5].", "startOffset": 246, "endOffset": 260}, {"referenceID": 17, "context": "This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5].", "startOffset": 246, "endOffset": 260}, {"referenceID": 4, "context": "This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5].", "startOffset": 246, "endOffset": 260}, {"referenceID": 47, "context": "In this line, and closely related to the emerging field of artificial intelligence, Simon and Kotovsky explored, for decades, \u201cthe problem space of difficulty\u201d [48, 36].", "startOffset": 160, "endOffset": 168}, {"referenceID": 35, "context": "In this line, and closely related to the emerging field of artificial intelligence, Simon and Kotovsky explored, for decades, \u201cthe problem space of difficulty\u201d [48, 36].", "startOffset": 160, "endOffset": 168}, {"referenceID": 10, "context": "The use of these measures for non-human subjects (especially for machines) may be misleading since it has been shown that many tasks that are very difficult for humans are very easy for machines and vice versa [11].", "startOffset": 210, "endOffset": 214}, {"referenceID": 40, "context": "A more mathematical (and computational) approach for associating complexity with the number (or the size) of items that are necessary to explain a concept (or solve a problem) is now known as algorithmic information theory (also known as Kolmogorov complexity [41]).", "startOffset": 260, "endOffset": 264}, {"referenceID": 38, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 112, "endOffset": 115}, {"referenceID": 7, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 132, "endOffset": 135}, {"referenceID": 30, "context": "For instance, a variant of Kt, known as intensional complexity, accurately captured the difficulty humans found on IQ test series problems [31, 21].", "startOffset": 139, "endOffset": 147}, {"referenceID": 20, "context": "For instance, a variant of Kt, known as intensional complexity, accurately captured the difficulty humans found on IQ test series problems [31, 21].", "startOffset": 139, "endOffset": 147}, {"referenceID": 45, "context": "Outside induction, the idea of instance complexity [46][41, sec.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "For other kinds of problems, such as general inductive and deductive problems, [20, 23, 22] also explored the use of algorithmic information theory, by considering the information gain from the problem to the solution.", "startOffset": 79, "endOffset": 91}, {"referenceID": 22, "context": "For other kinds of problems, such as general inductive and deductive problems, [20, 23, 22] also explored the use of algorithmic information theory, by considering the information gain from the problem to the solution.", "startOffset": 79, "endOffset": 91}, {"referenceID": 21, "context": "For other kinds of problems, such as general inductive and deductive problems, [20, 23, 22] also explored the use of algorithmic information theory, by considering the information gain from the problem to the solution.", "startOffset": 79, "endOffset": 91}, {"referenceID": 36, "context": "This is why some works have advocated for ergodic environments [37], where the agent can always recover from a local \u2018hell\u2019 or \u2018heaven\u2019.", "startOffset": 63, "endOffset": 67}, {"referenceID": 25, "context": "Also, the choice of the aggregate reward function is crucial, especially if time is taken into account [26].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "Finally, the evaluated agent can evolve during the process and become a different agent [29].", "startOffset": 88, "endOffset": 92}, {"referenceID": 50, "context": ", maze size, [51]) or the state space [43], have been used as approximations of the \u2018difficulty\u2019 of an environment.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": ", maze size, [51]) or the state space [43], have been used as approximations of the \u2018difficulty\u2019 of an environment.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "In [27], another variant of Kolmogorov complexity (Ktmax) was suggested as a measure of complexity for the environments, but it was already stated that this approximation was unidirectional, since some very complex environments might be easy, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Following these ideas, an environment class was introduced in [25], where the difficulty of an environment can be approximated by the Kolmogorov complexity of the rules (as a Markov algorithm) that describe the environment.", "startOffset": 62, "endOffset": 66}, {"referenceID": 33, "context": "The class is used to create intelligence tests in [34], but the approximation of difficulty is not satisfactory.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "In fact, some other related works also mention the importance of being able to assess the difficulty of the environment, such [12] and to achieve discriminating power [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 23, "context": "In fact, some other related works also mention the importance of being able to assess the difficulty of the environment, such [12] and to achieve discriminating power [24].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].", "startOffset": 70, "endOffset": 73}, {"referenceID": 32, "context": "For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].", "startOffset": 159, "endOffset": 163}, {"referenceID": 27, "context": "For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].", "startOffset": 188, "endOffset": 196}, {"referenceID": 29, "context": "For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].", "startOffset": 188, "endOffset": 196}, {"referenceID": 49, "context": ", [50]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 53, "context": "of the patterns that an elementary (1-dimensional) cellular automaton generates [54].", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "For instance, it is said that a problem is difficult if there is a \u201chigh density of well-separated almost solutions (local minima)\u201d [9] or we have a \u201crugged solution space\u201d [32].", "startOffset": 132, "endOffset": 135}, {"referenceID": 31, "context": "For instance, it is said that a problem is difficult if there is a \u201chigh density of well-separated almost solutions (local minima)\u201d [9] or we have a \u201crugged solution space\u201d [32].", "startOffset": 173, "endOffset": 177}, {"referenceID": 18, "context": "[19] includes an account of approaches for problem difficulty measures in evolutionary computation, appeared in the past twenty years.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Item response theory (IRT) [14] is a paradigm for the study of items (tasks) and a well-grounded way of designing tests and other instruments that measure abilities, especially in the area of (computerised) adaptive testing.", "startOffset": 27, "endOffset": 31}, {"referenceID": 43, "context": "For continuous score items, a very frequent approach is the linear model [44, 15]: X(\u03b8) , z + \u03bb\u03b8 + where z is the intercept (zero-ability expected result), \u03bb is the loading or slope, and is the measurement error.", "startOffset": 73, "endOffset": 81}, {"referenceID": 14, "context": "For continuous score items, a very frequent approach is the linear model [44, 15]: X(\u03b8) , z + \u03bb\u03b8 + where z is the intercept (zero-ability expected result), \u03bb is the loading or slope, and is the measurement error.", "startOffset": 73, "endOffset": 81}, {"referenceID": 15, "context": "Again, the slope \u03bb is positively related to most measures of discriminating power [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "2 Environment response functions As we mentioned in section 2, item response theory (IRT) [14] is a paradigm for the study of items (tasks) and a more principled way of designing tests and other instruments that measure abilities, especially in the area of (computerised) adaptive testing.", "startOffset": 90, "endOffset": 94}, {"referenceID": 49, "context": "1 Agent-populated elementary cellular automata: definition and examples The environments we will work with are will use elementary cellular automata (ECA) [50] for the space \u03c3 and the transition function \u03c4 , and will let the agent see and modify part of the usual behaviour of the automaton.", "startOffset": 155, "endOffset": 159}, {"referenceID": 49, "context": "The transition function \u03c4 is given by \u03bd, as any of the 22 3 = 256 rules that can be defined looking at each cell and its two neighbours according to the numbering scheme convention introduced in [50].", "startOffset": 195, "endOffset": 199}, {"referenceID": 0, "context": ", [1]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 49, "context": ", [50]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 51, "context": "In particular, we used a common approach [52, 34]: we coded the program as a character string and compressed it (using the memCompress function in R [47], a GNU project implementation of Lempel-Ziv coding).", "startOffset": 41, "endOffset": 49}, {"referenceID": 33, "context": "In particular, we used a common approach [52, 34]: we coded the program as a character string and compressed it (using the memCompress function in R [47], a GNU project implementation of Lempel-Ziv coding).", "startOffset": 41, "endOffset": 49}, {"referenceID": 46, "context": "In particular, we used a common approach [52, 34]: we coded the program as a character string and compressed it (using the memCompress function in R [47], a GNU project implementation of Lempel-Ziv coding).", "startOffset": 149, "endOffset": 153}, {"referenceID": 51, "context": "Fortunately, a deep and insightful analysis of the space-time diagrams have recently been performed by several studies [52, 54].", "startOffset": 119, "endOffset": 127}, {"referenceID": 53, "context": "Fortunately, a deep and insightful analysis of the space-time diagrams have recently been performed by several studies [52, 54].", "startOffset": 119, "endOffset": 127}, {"referenceID": 51, "context": "If we just focus on the four rules we have used throughout the experimental section: 184, 110, 122 and 164, we have that [52], which uses the compression method to approximate Kolmogorov complexity, sorts them (from lowest to highest complexity) as follows: 164, 184, 122, 110.", "startOffset": 121, "endOffset": 125}, {"referenceID": 53, "context": "A related study, [54], which uses the coding theorem method through a \u2018block matrix decomposition\u2019, reaches slightly different values but exactly the same order.", "startOffset": 17, "endOffset": 21}, {"referenceID": 51, "context": "Clearly, rules 164 and 184 lead to less complex space-time diagrams (they are the simplest according to [52, 54] and intuitively in Figure 5), so simple policies in these cases are not expected to create random-like (i.", "startOffset": 104, "endOffset": 112}, {"referenceID": 53, "context": "Clearly, rules 164 and 184 lead to less complex space-time diagrams (they are the simplest according to [52, 54] and intuitively in Figure 5), so simple policies in these cases are not expected to create random-like (i.", "startOffset": 104, "endOffset": 112}, {"referenceID": 38, "context": "Any reasonable (and feasible) sample will typically build short programs first instead of long ones, so this will be closely related to Levin\u2019s optimal search [39] any way, which is known to asymptotically dominate any other search.", "startOffset": 159, "endOffset": 163}, {"referenceID": 48, "context": "heuristics [49]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "This would also connect this work to all the previous analysis on difficulty made in the area of evolutionary computation [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 41, "context": "In this sense, some recent approaches based on the notion of motif, such as \u2018motif difficulty\u2019 [42] would be worth being investigated in relation to this work.", "startOffset": 95, "endOffset": 99}, {"referenceID": 37, "context": "Note that a meaningful choice of each environment is much more effective than choosing them arbitrarily, can replace or complement those evaluations based on repositories and it is certainly more effective than choosing all the environments or selecting them by a (universal) distribution, as in [38].", "startOffset": 296, "endOffset": 300}], "year": 2013, "abstractText": "We analyse the complexity of environments according to the policies that need to be used to achieve high performance. The performance results for a population of policies leads to a distribution that is examined in terms of policy complexity and analysed through several diagrams and indicators. The notion of environment response curve is also introduced, by inverting the performance results into an ability scale. We apply all these concepts, diagrams and indicators to a minimalistic environment class, agent-populated elementary cellular automata, showing how the difficulty, discriminating power and ranges (previous to normalisation) may vary for several environments.", "creator": "LaTeX with hyperref package"}}}