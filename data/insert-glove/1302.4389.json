{"id": "1302.4389", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2013", "title": "Maxout Networks", "abstract": "We 1,541 consider golgotha the staden problem momoiro of mnsii designing 5.6875 models metalliferous to leverage 2.71 a recently dewey introduced approximate model averaging richbourg technique salameh called clappers dropout. cashbox We define a humanoid simple new kr\u00f6d model celje called sheedy maxout (26.84 so named minneapolis-st because darkroom its {\\ ozric em out} put is the sani max prapawadee of exequiel a set of mindbenders inputs, c3i and because it silicic is abdy a natural companion to dropout) bhadrapur designed kapamilya to chart-topping both facilitate townies optimization piacenza by dropout santel and improve foltin the abc/wb accuracy withdrawals of mabini dropout ' s kandivali fast approximate model averaging mateljan technique. chelsy We nilly empirically i.coast verify cherubini that tzeng the model successfully accomplishes yokohama both realm of these grapes tasks. twe We br02 use txu maxout yorkin and dropout macias to demonstrate state of the art classification paas performance programmer on antigay four myun benchmark bakersville datasets: MNIST, CIFAR - 10, eddo CIFAR - 100, 1.5515 and inordinately SVHN.", "histories": [["v1", "Mon, 18 Feb 2013 18:59:07 GMT  (517kb,D)", "http://arxiv.org/abs/1302.4389v1", null], ["v2", "Tue, 19 Feb 2013 04:39:48 GMT  (484kb,D)", "http://arxiv.org/abs/1302.4389v2", "Revision; fixed a broken reference and removed a latex command from the abstract on the metadata page"], ["v3", "Wed, 20 Feb 2013 22:33:13 GMT  (485kb,D)", "http://arxiv.org/abs/1302.4389v3", "Corrected a mistake in the \"Review of dropout\" section"], ["v4", "Fri, 20 Sep 2013 08:54:35 GMT  (1433kb,D)", "http://arxiv.org/abs/1302.4389v4", "This is the version of the paper that appears in ICML 2013"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ian j goodfellow", "david warde-farley", "mehdi mirza", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1302.4389"}, "pdf": {"name": "1302.4389.pdf", "metadata": {"source": "META", "title": "Maxout Networks", "authors": ["Ian J. Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville"], "emails": ["goodfeli@iro.umontreal.ca", "wardefar@iro.umontreal.ca", "mirzamom@iro.umontreal.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": "1. Introduction", "text": "A recently introduced technique known as dropout (Hinton et al., 2012) provides an inexpensive and simple means of training a large ensemble of models that share parameters, as well as an inexpensive and simple means of approximately averaging together these models to make a prediction. Dropout has been used to improve the performance of multilayer perceptrons and deep convolutional networks, redefining the state of the art on tasks ranging from audio classification to very large scale object recognition (Hinton et al., 2012; Krizhevsky et al., 2012). While dropout is known to work well in practice, it has not previously been demonstrated to actually perform model averaging for deep architectures. Moreover, dropout is generally\nCode associated with this paper is available at https://github.com/lisa-lab/pylearn2 in the module pylearn2.models.maxout.\nviewed as an indiscriminately applicable tool that will reliably yield a modest improvement in performance when applied to almost any model.\nWe argue that rather than using dropout as a slight performance enhancement applied to arbitrary models, the best performance may be obtained by directly considering how to use dropout as a model averaging technique, and designing a model that enhances dropout\u2019s abilities in this respect. Training using dropout differs significantly from previous approaches such as ordinary stochastic gradient descent. Dropout is most effective when taking relatively large steps in parameter space. In this regime, each update can be seen as making a significant update to a different model on a different subset of the training set. The ideal operating regime for dropout is when the overall training procedure resembles training an ensemble with bagging under parameter sharing constraints. This differs radically from the ideal stochastic gradient operating regime in which a single model makes steady progress via small steps. Another important consideration is that dropout model averaging is only an approximation when applied to deep models. Models that are explicitly designed to minimize this approximation error may thus enhance dropout\u2019s performance as well.\nWe propose a simple model that we call maxout that has beneficial characteristics both for optimization and model averaging with dropout. We use this model in conjunction with dropout to set the state of the art on four benchmark datasets."}, {"heading": "2. Review of dropout", "text": "Dropout is a technique that can be applied to deterministic feedforward architectures that predict an output y given input vector v. These architectures contain\nar X\niv :1\n30 2.\n43 89\nv1 [\nst at\n.M L\n] 1\n8 Fe\nb 20\na series of hidden layers h = {h(1), . . . , h(L)}. Dropout trains an ensemble of models consisting of the set of all models that contain a subset of the variables in both v and h. The same set of parameters \u03b8 is used to parameterize a family of distributions p(y | v; \u03b8, \u00b5) where \u00b5 \u2208 M is a binary mask determining which variables to include in the model. If the model contains N total input and hidden variables, and M denotes the set of all 2N possible masks, then assuming the masks are sampled uniformly at random, dropout maximizes the likelihood of the model defined by taking the geometric mean over all sub-models:\npensemble(y | v; \u03b8) \u221d 2 N \u221a \u03a0\u00b5\u2208Mp(y|v; \u03b8, \u00b5).\nThis can be seen by taking the log and sampling \u00b5 uniformly to obtain an unbiased estimator. Gradient ascent on the log likelihood can thus be accomplished by randomly sampling a different dropout mask \u00b5 for each example during stochastic gradient training. In order for this training procedure to behave as if it is training an ensemble rather than a single model, each update must have a large effect, so that it makes the sub-model corresponding to that \u00b5 fit the current input v well. This resembles using bagging, except that all of the models in the ensemble share parameters so they can never be completely independent of one another. Note that regardless of the form of p(y | v; \u03b8, \u00b5) this procedure follows an unbiased estimate of the gradient of the likelihood of the ensemble.\nThe functional form becomes important when it comes time to make a prediction by averaging together all models. In the case where p(y | v; \u03b8) = softmax(vTW + b), the prediction defined by renormalizing the geometric mean of p(y | v; \u03b8, \u00b5) overM is simply given by softmax(vTW/2 + b). In other words, the average over exponentially many models can be computed simply by running a single model with the weights divided by 2. This result holds exactly in this case, but previous work on dropout applies the same scheme in deeper architectures such as multilayer perceptrons and convolutional neural networks. In this case, the approximation has not been characterized mathematically, but performs well in practice."}, {"heading": "3. Description of maxout", "text": "The maxout model is simply a feed-forward achitecture, such as a multilayer perceptron or deep convolutional neural network, that uses a new type of activation function: the maxout unit. Given an input x \u2208 Rd, a maxout hidden layer implements the function\nhi(x) = max j\u2208[1,k] zij\nwhere\nzij = x TW\u00b7\u00b7\u00b7ij + bij\nfor learned parameters W \u2208 Rd\u00d7m\u00d7k and b \u2208 Rm\u00d7k. In the context of convolutional networks, a maxout feature map can be constructed by taking the maximum across k affine feature maps (i.e., pool across channels, rather than over spatial locations). A single maxout unit can be interpreted as making a piecewise linear approximation to an arbitrary convex function. In other words, as the training algorithm optimizes the parameters, it learns not just the relationship between hidden units, but also the activation function of each hidden unit. See Fig. 1 for a graphical depiction of how this works.\nMaxout abandons many of the mainstays of traditional activation function design. The representation it produces is not sparse at all (see Fig. 2), though the gradient is highly sparse and dropout will artificially sparsify the effective representation during training. While maxout may learn to saturate on one side or the other this is a measure zero event. While a significant proportion of parameter space corresponds to the function being bounded from below, maxout is not constrained to learn to be bounded at all. In fact, being bounded from above is also a measure zero event. Maxout is locally linear almost everywhere, while many popular activation functions have signficant curvature. Given all of these departures from standard practice, it may seem surprising that maxout activation functions work at all, but we find that they are very robust and easy to train with dropout, and achieve excellent performance."}, {"heading": "4. Maxout is a universal approximator", "text": "A standard MLP with enough hidden units is a universal approximator. Surprisingly, the maxout network requires only two maxout hidden units to be a universal approximator. The key is that each hidden unit may require arbitrary many affine components. In particular, we show below that a maxout model with just two hidden units can approximate, arbitrarily closely, any continuous function of x \u2208 Rd. A diagram illustrating the basic idea of the proof is presented in Fig. 3.\nConsider the continuous piecewise linear (PWL) function g(x) consisting of k locally linear (affine) regions on Rd.\nProposition 4.1 (From Theorem 2.1 in (Wang, 2004)) For any positive integers m and d, there always exists two groups of d + 1-dimensional real-valued parameter vectors [W1j , b1j ], j \u2208 [1, k] and [W2j , b2j ], j \u2208 [1, k] such that:\ng(x) = h1(x)\u2212 h2(x) (1)\nThat is, any continuous PWL function can be expressed as a difference of two convex PWL functions. The proof is given in (Wang, 2004) and omitted here for brevity.\nProposition 4.2 From the Stone-Weierstrass approximation theorem, let f : C \u2192 R be a continuous function, C be a compact domain C \u2282 Rd and > 0 be\nany positive real number. Then there exists a continuous PWL function g, (depending upon ), such that for all x \u2208 C, |f(x)\u2212 g(x)| < .\nTheorem 4.3 Universal approximator theorem. Any continuous function f can be approximated arbitrarily well on a compact domain C \u2282 Rd by a maxout network with two maxout hidden units.\nSketch of Proof By Proposition 4.2, any continuous function can be approximated arbitrarily well (up to ), by a piecewise linear function. We now note that the representation of piecewise linear functions given in Proposition 4.1 exactly matches a maxout net with two hidden units h1(x) and h2(x), with sufficiently large k to achieve the desired degree of approximation . Combining these, we conclude that a two hidden unit maxout network can approximate any continuous function f(x) arbitrarily well on the compact domain C. In general as \u2192 0, we have k \u2192\u221e."}, {"heading": "5. Benchmark results", "text": "We evaluated the maxout model on four benchmark datasets and set the state of the art on all of them. Maxout shows not only a clear improvement over previous methods but also shows a strong enhancement of dropout\u2019s abilities."}, {"heading": "5.1. MNIST", "text": "The MNIST (LeCun et al., 1998) dataset consists of 28 \u00d7 28 pixel greyscale images of handwritten digits\n0-9, with 60,000 training examples and 10,000 test examples.\nTraditionally methods are evaluated in separate categories depending on whether they exploit the fact that the examples have image structure or not. For the permutation invariant version of the MNIST task, only methods unaware of the 2D structure of the data are permitted. In this case, we trained a model consisting of two densely connected maxout layers followed by a softmax layer. Besides using dropout, we further regularized the model by imposing a constraint on the norm of each weight vector. All constraint values, learning rate and momentum schedule parameters, layer sizes, etc. were selected by minimizing the error on a validation set consisting of the last 10,000 training examples. In order to make use of the full training set, we record the value of the log likelihood on the first 50,000 examples at the point of minimal validation error. We then continue training on the full 60,000 example training set until the validation set log likelihood matches this number. We obtained a test set error of 0.94%, which is the best result of which we are aware that does not use unsupervised pretraining. We summarize the state of the art results on permutation invariant MNIST in Table 1.\nWe also considered the MNIST dataset without the permutation invariance restriction. In this case, we used three convolutional maxout hidden layers (with spatial max pooling on top of the maxout layers) followed by a densely connected softmax layer. We were able to rapidly explore parameter space thanks to the extremely fast GPU convolution library developed by Krizhevsky et al. (2012). We obtained a test set error rate of 0.45%, which sets a new state of the art in this\ncategory. Note that it is possible to get better results on MNIST by augmenting the dataset with transformations of the standard set of images (Ciresan et al., 2010) . A summary of the best methods on the general MNIST dataset is provided in Table 2."}, {"heading": "5.2. CIFAR-10", "text": "The CIFAR-10 dataset (Krizhevsky & Hinton, 2009) consists of 32 \u00d7 32 color images drawn from 10 classes. The training set contains 50,000 images and the test set contains 10,000. We preprocessed the data using global contrast normalization and ZCA whitening. This is the same preprocessing applied by (Coates et al., 2011) to individual patches of the dataset in the context of unsupervised modeling of patches.\nWe follow a similar procedure as with the MNIST dataset, but with one change. On MNIST, we find the best number of training epochs in terms of validation set error, then record the training set log likelihood\nand continue training using the entire training set until the validation set log likelihood has reached this value. On CIFAR-10, continuing training like this is infeasible because the final value of the learning rate is very small and the validation set error is very high. Training until the validation set likelihood matches the cross-validated value of the training likelihood would thus take prohibitively long. Instead, we retrain the model from scratch, and stop when the new validation set likelihood matches the value of the training set likelihood selected by cross validation. As with MNIST, we do not use data augmentation (such as training on translated and reflected versions of the images) and compare our results only to methods that do not use data augmentation.\nOur best model consists of three convolutional maxout layers followed by a fully connected maxout layer, then finally a softmax layer. This is similar to the architecture used by (Hinton et al., 2012) except that our penultimate layer is fully connected instead of locally connected. Using this approach we obtain a test set error of 12.93%, which improves upon the state of the art by over two percentage points. (If we do not train on the validation set, we obtain a test set error of 14.05%, which also improves over the previous state of the art). A summary of the best CIFAR-10 methods is provided in Table 3. A visualization of the convolution kernels is shown in Fig. 5.\nAs shown in Fig. 6, dropout was critical for obtaining good generalization error. Unlike previous results in which dropout reduces the generalization error by about 10%, maxout is specifically designed to enhance the effect of dropout, resulting here in a greater than 25% reduction in the generalization error."}, {"heading": "5.3. CIFAR-100", "text": "The CIFAR-100 (Krizhevsky & Hinton, 2009) dataset is the same size and format as the CIFAR-10 dataset, but contains 100 classes, with only one tenth as many\nlabeled examples per class. Due to lack of time we did not cross-validate hyperparameters on CIFAR-100 but simply applied the hyperparameters that yielded the best validation set performance on CIFAR-10. We obtained a test set error of 38.57%, which is state of the art (if we do not retrain using the entire training set, we obtain a test set error of 41.48%, which also surpasses the current state of the art) . A summary of the best methods on CIFAR-100 is provided in Table 4."}, {"heading": "5.4. Street View House Numbers", "text": "The SVHN (Netzer et al., 2011) dataset consists of color images of house numbers collected by Google Street View. The dataset comes in two formats. We consider the second format, in which each image is of size 32 \u00d7 32 and the task is to classify the digit in the center of the image. Additional digits may appear beside it but must be ignored. This is a difficult unsolved real-world task with potential commercial applications\nfor systems that achieve under 1% error. There are 73,257 digits in the training set, 26,032 digits in the test set and 531,131 additional, somewhat less difficult examples, to use as an extra training set. Following Sermanet et al. (2012b) to build a validation set, we select 400 samples per class from the training set and 200 samples per class from the extra set. The remaining digits of the train and extra sets are used for training.\nFor this dataset, we did not train on the validation set at all. We used it only to find the best hyperparameters. We preprocessed the data in the same way as (Zeiler & Fergus, 2013), by applying local contrast normalization on each of the RGB channels. Otherwise, we followed the same approach as on MNIST. Our best model consists of three convolutional maxout hidden layers (with spatial pooling on top of maxout layers as for MNIST) followed by a densely connected softmax layer. The hyper-parameters for maxout layers are the following. The number of channels are 128, 128, 256 for each layer. The maxout pool sizes were chosen as 2, 2, and 4, respectively. The spatial pooling shapes are respectively (4, 4), (4, 4), (2, 2) with stride sizes of 2. We obtained a test set error rate of 2.72%, which sets the state of the art. A summary of comparable methods is provided in Table 5."}, {"heading": "6. Model Averaging", "text": "Having demonstrated that maxout networks are effective learning algorithms, we turn to analyzing the reasons for their success. We first identify reasons that maxout networks are highly compatible with dropout\u2019s approximate model averaging technique.\nThe intuitive justification for averaging together dropout models by dividing the weights by 2 given by (Hinton et al., 2012) is that this does exact model averaging for a single layer model, softmax regression. To this characterization, we add the observation that the model averaging remains exact if the model is extended to multiple linear layers. While this has the same representational power as a single layer the expression of the weights as a product of several matrices could have a different inductive bias. More importantly, it indicates that dropout does exact model averaging in deeper architectures provided that they are locally linear among the space of inputs to each layer that are visited by applying different dropout masks.\nWe argue that the ensemble style training used in dropout encourages maxout units to be locally linear. Because each subset of the model (corresponding to one model in the ensemble) must make a good prediction of the output, each unit should learn to have roughly the same activation regardless of which of its inputs are dropped out. Thus, while a maxout net-\nwork with arbitrary parameters will be far from locally linear in this space, a maxout network trained with dropout may have the identity of the maximal filter in each unit change relatively rarely as the dropout masks change. Thus networks consisting of linear operations and the max(\u00b7) could learn to exploit dropout\u2019s approximate model averaging technique well.\nMany popular activation functions have significant curvature nearly everywhere. These observations suggest that the approximate model averaging of dropout will not be as accurate for such activation functions. To test this we compared the best maxout model trained on MNIST with dropout to a hyperbolic tangent network trained on MNIST with dropout. We sampled several subsets of each model and compared the geometric mean of these sampled models\u2019 predictions to the prediction made using the dropout technique of dividing the weights by 2. We found evidence that dropout is indeed performing model averaging, even in multilayer networks, and that it is more accurate in the case of maxout networks. See Fig. 7 and Fig. 8 for details."}, {"heading": "7. Optimization", "text": "The second key reason that maxout performs well is that it improves the bagging style training phase of the dropout algorithm.\nNote that the arguments in section 6 motivating the use of maxout also apply equally to rectified linear units (???). Maxout seems superficially similar to max pooling over a set of rectified linear units, which is equivalent to including a constant 0 in the set from which maxout selects the max. However, we find that including this constant 0 is very harmful to optimization in the context of dropout. For instance, on MNIST our best validation set error with an MLP is 1.04%. If we include a 0 in the max, this rises to over 1.2%. In the context of dropout, we argue that maxout has superior optimization properties relative to max pooling over rectified linear units."}, {"heading": "7.1. Optimization experiments", "text": "To verify that maxout yields better optimization performance than max pooled rectified linear units when training with dropout, we carried out two experiments. First, we stressed the optimization capabilities of the training algorithm by training a small (two hidden convolutional layers with k = 2 and sixteen kernels) model on the large (600,000 example) SVHN dataset. When training with rectifier units the training error gets stuck at 7.3%. If we train instead with maxout units, we obtain 5.1% training error. As another optimization stress test, we tried training very deep and narrow models on MNIST, and found that maxout networks cope better with increasing depth than rectifiers. See Fig. 9 for details."}, {"heading": "7.2. Saturation", "text": "Optimization proceeds very differently when using dropout than when using ordinary stochastic gradient descent. SGD usually works best with a small learning rate that results in a smoothly decreasing objective function, while dropout works best with a large learning rate, resulting in a constantly fluctuating objective function. Dropout rapidly explores many different directions and rejects the ones that worsen performance, while SGD moves slowly and steadily in the most promising direction. We find empirically that these very different operating regimes result in very different outcomes for rectifier units. When training with SGD, we find that the rectifier units saturate at 0 less than 5% of the time. When training with dropout, this increases to 60% of the time. Because the 0 in the max(0, z) activation function is a constant, and not a parameter as when a maxout unit is 0, this blocks the gradient from flowing through the unit. In the absence of gradient through the unit, it is difficult for training to change this unit to become active again. Maxout does not suffer from this problem because gradient always flows through every maxout unit. Units that take on negative activations may be steered to become positive again later. Fig. 10 illustrates how active rectifier units become inactive at a greater rate than inactive units become active when training with dropout, but maxout units, which are always active, transition between positive and negative activations at about equal rates in each direction. We hypothesize that the high proportion of zeros and the difficulty of escaping them impairs the optimization performance of rectifiers relative to maxout.\nIn order to investigate this hypothesis, we trained two MLPs on MNIST with the same architecture of 1200 filters per layer pooled in groups of 5. When we include a constant 0 in the max pooling, the resulting trained model fails to make use of 17.6% of the filters in the second layer and 39.2% of the filters in the second layer. A small minority of the filters usually took on the maximal value in the pool, and the rest of the time the maximal value was a constant 0. Maxout, on the other hand, used all but 2 of the 2400 filters in the network. Each filter in each maxout unit in the network was maximal for some training example. All filters had been utilised and tuned to the classification task.\n7.3. Lower layer gradients and the bagging effect\nIn order to behave differently from SGD, dropout requires that the gradient change noticeably when the choice of which units to drop changes. If the gra-\ndient is approximately constant with respect to the dropout mask, then dropout simplifies to SGD training. We tested the hypothesis that rectifier networks suffer from diminished gradient flow to the lower layers of the network by monitoring the variance with respect to dropout masks for fixed data during training of two different MLPs on MNIST. The variance on the gradient of the output weights was about 1.4 times larger for maxout on an average training epoch step, while the variance on the gradient of the first layer weights was 3.4 times larger for maxout than for rectifiers. In concordance with our previous result showing that maxout with dropouts allows training deeper networks, this greater variance suggests that maxout better propagates varying information downward to the lower layers and helps dropout training to better resemble bagging for these lower-layer parameters. Rectifier networks, with more of their gradient lost to saturation, presumably cause dropout training to behave more like regular SGD toward the bottom of the network."}, {"heading": "8. Conclusion", "text": "In this paper, we have proposed a new family of functions called maxout that is particularly well suited for training with dropout, and for which we have proven a universal approximation theorem. We have shown empirical evidence that dropout attains a good approximation to model averaging in deep models. We have shown that maxout exploits this model averaging behavior because the approximation is more accurate for maxout units than for tanh units. We have\ndemonstrated that optimization behaves very differently in the context of dropout than in the pure SGD case. By designing the maxout gradient to avoid pitfalls such as failing to use many of a model\u2019s filters, we are able to train maxout networks on much larger training sets and with much deeper networks than is possible using rectifier units. We have also shown that maxout propagates variations in the gradient due to different choices of dropout masks to the lowest layers of a network, thereby ensuring that every parameter in the model can enjoy the full benefit of dropout rather than SGD training and more faithfully emulate bagging training. More broadly, the state of the art performance of our approach on five different benchmark tasks motivates the design of further models that are explicitly intended to perform well when combined with inexpensive approximations to model averaging."}, {"heading": "9. Acknowledgements", "text": "The authors would like to thank the developers of Theano, in particular Fre\u0301de\u0301ric Bastien and Pascal Lamblin for their assistance with infrastructure development and performance optimization. We would also like to thank Yann Dauphin for helpful discussions."}], "references": [{"title": "Deep big simple neural nets for handwritten digit recognition", "author": ["D.C. Ciresan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "In Proc. International Conference on Computer Vision (ICCV\u201909),", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Jia", "Yangqing", "Huang", "Chang"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "The manifold tangent classifier", "author": ["Rifai", "Salah", "Dauphin", "Yann", "Vincent", "Pascal", "Bengio", "Yoshua", "Muller", "Xavier"], "venue": "In NIPS\u20192011,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009),", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["Sermanet", "Pierre", "Chintala", "Soumith", "LeCun", "Yann"], "venue": "CoRR, abs/1204.3968,", "citeRegEx": "Sermanet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2012}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["Sermanet", "Pierre", "Chintala", "Soumith", "LeCun", "Yann"], "venue": "In International Conference on Pattern Recognition (ICPR", "citeRegEx": "Sermanet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2012}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan Prescott"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "General constructive representations for continuous piecewise-linear functions", "author": ["Wang", "Shuning"], "venue": "IEEE Trans. Circuits Systems,", "citeRegEx": "Wang and Shuning.,? \\Q2004\\E", "shortCiteRegEx": "Wang and Shuning.", "year": 2004}, {"title": "Deep convex net: A scalable architecture for speech pattern classification", "author": ["Yu", "Dong", "Deng", "Li"], "venue": "In INTERSPEECH,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "CoRR, abs/1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "A recently introduced technique known as dropout (Hinton et al., 2012) provides an inexpensive and simple means of training a large ensemble of models that share parameters, as well as an inexpensive and simple means of approximately averaging together these models to make a prediction.", "startOffset": 49, "endOffset": 70}, {"referenceID": 2, "context": "Dropout has been used to improve the performance of multilayer perceptrons and deep convolutional networks, redefining the state of the art on tasks ranging from audio classification to very large scale object recognition (Hinton et al., 2012; Krizhevsky et al., 2012).", "startOffset": 222, "endOffset": 268}, {"referenceID": 6, "context": "Dropout has been used to improve the performance of multilayer perceptrons and deep convolutional networks, redefining the state of the art on tasks ranging from audio classification to very large scale object recognition (Hinton et al., 2012; Krizhevsky et al., 2012).", "startOffset": 222, "endOffset": 268}, {"referenceID": 7, "context": "The MNIST (LeCun et al., 1998) dataset consists of 28 \u00d7 28 pixel greyscale images of handwritten digits", "startOffset": 10, "endOffset": 30}, {"referenceID": 2, "context": "Rectifier MLP + dropout (Hinton et al., 2012) 1.", "startOffset": 24, "endOffset": 45}, {"referenceID": 9, "context": "Manifold Tangent Classifier (Rifai et al., 2011) 0.", "startOffset": 28, "endOffset": 48}, {"referenceID": 2, "context": "DBM + dropout (Hinton et al., 2012) 0.", "startOffset": 14, "endOffset": 35}, {"referenceID": 5, "context": "We were able to rapidly explore parameter space thanks to the extremely fast GPU convolution library developed by Krizhevsky et al. (2012). We obtained a test set error rate of 0.", "startOffset": 114, "endOffset": 139}, {"referenceID": 3, "context": "2-layer CNN+2-layer NN (Jarrett et al., 2009) 0.", "startOffset": 23, "endOffset": 45}, {"referenceID": 0, "context": "Note that it is possible to get better results on MNIST by augmenting the dataset with transformations of the standard set of images (Ciresan et al., 2010) .", "startOffset": 133, "endOffset": 155}, {"referenceID": 1, "context": "This is the same preprocessing applied by (Coates et al., 2011) to individual patches of the dataset in the context of unsupervised modeling of patches.", "startOffset": 42, "endOffset": 63}, {"referenceID": 13, "context": "CNN + Spearmint (Snoek et al., 2012) 14.", "startOffset": 16, "endOffset": 36}, {"referenceID": 2, "context": "This is similar to the architecture used by (Hinton et al., 2012) except that our penultimate layer is fully connected instead of locally connected.", "startOffset": 44, "endOffset": 65}, {"referenceID": 8, "context": "The SVHN (Netzer et al., 2011) dataset consists of color images of house numbers collected by Google Street View.", "startOffset": 9, "endOffset": 30}, {"referenceID": 11, "context": "Following Sermanet et al. (2012b) to build a validation set, we select 400 samples per class from the training set and 200 samples per class from the extra set.", "startOffset": 10, "endOffset": 34}, {"referenceID": 2, "context": "The intuitive justification for averaging together dropout models by dividing the weights by 2 given by (Hinton et al., 2012) is that this does exact model averaging for a single layer model, softmax regression.", "startOffset": 104, "endOffset": 125}], "year": 2017, "abstractText": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout\u2019s fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR100, and SVHN.", "creator": "LaTeX with hyperref package"}}}