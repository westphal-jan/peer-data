{"id": "1302.6794", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Efficient Estimation of the Value of Information in Monte Carlo Models", "abstract": "The expected tarty value of information (petroluem EVI) is the tarquinii most psoas powerful spokespersons measure of dario sensitivity 740,000 to uncertainty in a decision nebbi model: tiantong it measures haselton the potential fortified of 121.40 information farimex to recede improve the 1.3185 decision, uhrig and hence measures the expected 5,920 value 21.32 of klea outcome. Standard 11.0 methods allwyn for 99.04 computing tyseley EVI 18-yarder use discrete variables darnielle and conjugate are computationally intractable for models that contain zalmay more westons than aleta a few variables. Monte overstrained Carlo jetro simulation mickelberg provides predestination the godmen basis for 1,566 more tractable 19.21 evaluation skimboarding of provost large sajna predictive models formula with continuous malco and discrete variables, sub-projects but so 7.34 far computation soyer of 62.32 EVI in a hilaly Monte Carlo setting hiltz also compunctions has 1298 appeared stereoscopy impractical. We najdorf introduce cbeltran an gr\u00eamio approximate ahmetovic approach based on pre - posterior analysis for estimating javelinas EVI manas in Monte Carlo models. Our maliks method uses a 69.93 linear lamphu approximation assiduously to the 42.15 value imara function xilong and scor multiple stumblebum linear horror-comedy regression pessotto to venizelos estimate 31.53 the unlicenced linear narba model from forward-center the samples. torhout The lipinski approach is autotransfusion efficient diandra and olek practical for extremely large vivo models. stammerer It halbish allows easy estimation of 177.2 EVI f10 for pizzaexpress perfect shipholding or partial early-19th-century information on individual naxakis variables kubo or on combinations geli of variables. We illustrate khamtay its implementation 106-95 within tornero Demos (a multinomial decision ballcaps modeling system ), and j\u00f5geva its northop application to firoz a 100.28 large model legnago for crisis transportation salave'a planning.", "histories": [["v1", "Wed, 27 Feb 2013 14:14:43 GMT  (1511kb)", "http://arxiv.org/abs/1302.6794v1", "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)"]], "COMMENTS": "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom chavez", "max henrion"], "accepted": false, "id": "1302.6794"}, "pdf": {"name": "1302.6794.pdf", "metadata": {"source": "CRF", "title": "Efficient Estimation of the Value of Information in Monte Carlo Models", "authors": ["Tom Chavez", "Max Henrion"], "emails": [], "sections": null, "references": [{"title": "Recovering Value oflnformation from Pairwise Peeks,", "author": ["Chavez", "Tom"], "venue": "Rockwell Palo Alto Science Lab. Technical Memorandum", "citeRegEx": "Chavez and Tom.,? \\Q1994\\E", "shortCiteRegEx": "Chavez and Tom.", "year": 1994}, {"title": "Uncertainty: A Guilk ro Dealing with Uncertainry in Quantitative Risk and Policy Analysis", "author": ["Hennon. Max", "Morgan", "Granger"], "venue": null, "citeRegEx": "Max et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Max et al\\.", "year": 1990}, {"title": "Proximal Decision Analysis.", "author": ["A. Howard. R"], "venue": "Management Science,", "citeRegEx": "R.,? \\Q1970\\E", "shortCiteRegEx": "R.", "year": 1970}, {"title": "Applied Statistical Decision Th\ufffdory", "author": ["Raiffa", "Howard", "Schlaifer", "Raben"], "venue": null, "citeRegEx": "Raiffa et al\\.,? \\Q1961\\E", "shortCiteRegEx": "Raiffa et al\\.", "year": 1961}, {"title": "Statistical Reasoning for IM Behavioral Sciences", "author": ["Shavelson", "Richard"], "venue": null, "citeRegEx": "Shavelson and Richard.,? \\Q1988\\E", "shortCiteRegEx": "Shavelson and Richard.", "year": 1988}], "referenceMentions": [], "year": 2011, "abstractText": "The expected value of information (EVI) is the most powerful measure of sensitivity to uncer\u00ad tainty in a decision model: it measures the potential of information to improve the decision, and hence measures the expected value of the outcome. Stan\u00ad dard methods for computing EVI use discrete vari\u00ad ables and are computationally intractable for models that contain more than a few variables. Monte Carlo simulation provides the basis for more tractable evaluation of large predictive models with continu\u00ad ous and discrete variables, but so far computation of EVI in a Monte Carlo setting also has appeared impractical. We introduce an approximate approach based on preposterior analysis for estimating EVI in Monte Carlo models. Our method uses a linear approximation to the value function and multiple linear regression to estimate the linear model from the samples. The approach is efficient and practical for extremely large models. It allows easy estima\u00ad tion of EVI for perfect or panial information on individual variables or on combinations of variables. We illustrate its implementation within Demos (a decision modeling system), and its application to a large model for crisis transponation planning. 1.0 EVI: What's so, and What's New A ny model is inevitably a simplification of reality, and most of its input quantities are invariably uncertain. Sensi\u00ad tivity analysis identifies which sources of uncertainty in a model affect its outputs most significantly. In this way, it helps a decision maker focus attention on what assump\u00ad tions really matter. It also helps a decision modeler to assign priorities to his efforts to improve, refine, or extend his model by identifying those variables for which it will be most valuable to find more complete data, to interview more knowledgeable experts, or to build more elaborate submodels. The expected value of information (EVI) on a variable xi measures the expected increase in value y if we learn new information about xi and make a decision with higher\u00ad expected value in light of that information. It is the most powerful method of sensitivity analysis because it ana\u00ad lyzes a variable's importance in terms of the overall pre\u00ad scription for action, and it expresses that importance in the utility or value units of the problem. Other methods, such as rank-order correlation, express importance in terms of the correlation between an uncertain variable and the out\u00ad put of the decision model. There are many cases where a variable can show high sensitivity in this way, yet still have no effect on the selection of an optimal decision. Deterministic perturbation measures importance in utility or value units, but it ignores nonlinearities and interactions among variables, and also fails to measure a variable's importance in terms of that variable's ability to change the recommended decision. 120 Chavez and Henrion One calculates EVPI (Expected Value of Perfect Infor\u00b7 mation) in discrete models by rolling back the decision tree. The computation itself is straightforward in the sense that, to compute EVI, one simply places at the front of the tree the chance variables to be observed. The EVPI is computed as the difference between the expected value computed for this scenario and the expected value for the regular tree, without observations. Computing EVI with continuous variables is less intuitive, because we have no tidy way of reversing the uncertainty, unlike the discrete case. Yet continuous models are increasingly the norm for risk and decision analysis, first because discretizing inherently continuous variables intro\u00ad duces unnecessary approximation, and second because Monte Carlo methods and their variants (e.g., Latin hyper\u00ad cube) generate tractable, highly efficient solutions to pre\u00ad dictive models that contain thousands of variables. An especially useful feature of the Monte Carlo method is that, for a specified error, the computational complexity increases linearly in the number of uncertain variables [Morgan and Henrion, 1990]. Exact methods require com\u00ad putation time that is exponential in the number of vari\u00ad ables. There is thus a need to develop flexible, efficient methods for computing EVI on continuous variables in a Monte Carlo setting. A ftexible method has (I) the ability to com\u00ad pute EVI on single variables or on any combination of variables, and (2) the ability to compute both perfect and partial values of information. Perfect information removes uncertainty entirely. Partial information reduces uncertainty. We present a general framework for calculating EVI based on preposterior analysis. Using that framework, we develop a technique for computing EVI that depends on a linear approximation to the value function and on multiple linear regression to estimate the constants for the linear function. We also discuss a heuristic method for measuring the value of partial information in terms of what we call the relative information multiple (RIM). We have implemented these methods in detachable computational modules using Demos, a decision modeling system from Lumina, Inc., Palo Alto, CA. We demonstrate their use on a large model to aid in military transportation crisis plan\u00ad ning. 2.0 Framework A decision model consists of a set of n state variables x1, \u2022\u2022 ,xn, which we will denote by X. The decision maker has control of a decision variable D, which can assume one of m possible values d1 , .. ,dm. The value or utility func\u00ad tion v(X,di) expresses the payoff to the decision maker when X obtains and decision di is chosen. In a typical decision model, the state variables are uncer\u00ad tain. We express prior knowledge about X in the form of a probability distribution, denoted {XI \ufffd},where \ufffddenotes a prior state of information. The optimal Bayes' decision maximizing the expected value1 is given by l = Arg max ( X d I l' ) d v ( ' ) .., . The optimal decision given perfect information on state variable x , denoted d* _., is d\u2022 x __ Arg max d (v(X,d)Jx,\ufffd) We define EVPI on x as EVPI (x) = (v (X, l x) I \ufffd){v (X, d*) I \ufffd). In a similar fashion, we define the optimal expected\u00ad value decision given the revelation of evidence e, d* e\u2022 as le= Arg d max (v(X,d)Je.\ufffd) Then the EVI for evidence e is EVI(e) = (v(X,d.e)l\ufffd)-(v(X,l)l\ufffd) 2.1 Binary decisions and Function z Let us consider a simplified decision problem with two decision alternatives: one of them is the optimal Bayes' decision d*; the other we denote tr\". In view of the uncertainty in the state variables, there must exist uncertainty in the outputs as well. Thus, for each 1. We use Howard's inferential notation (see, for example, Howard, 1970). {XIS) denotes the probability density of X condi\u00ad tional on S; (XIS) denotes the expectation of X conditional on S. Efficient Estimation of the Value of Information in Monte Carlo Models 121 decision d;, there exists a unique probability distribution on value { v(X,d;)l \ufffd} (see Figure 1). For notational conve\u00ad nience we let v(d) = (v(X, d) I\ufffd). We now define z = v(X,l) -v(X,tf). Function z is the pivotal element in our framework for computing EVI because it describes the difference in value between the best and second-best decisions. In Figure 2, we have graphed the probability distribution of z. The shaded area represents the total probability of making a bad decision, i.e., doing d' when cY would yield higher value. Exploiting information encoded in the shaded, neg\u00ad ative portion of the z distribution's curve will provide the necessary clues to compute EVPI and EVI. FIGURE 1. Probability \ufffdistributions on value for the two decisions d and cr-. Pro density {v(X,tf)[\ufffd} {v(X,l)[s} FIGURE 2. Function z: the difference in value between the best and second-best decisions.", "creator": "pdftk 1.41 - www.pdftk.com"}}}