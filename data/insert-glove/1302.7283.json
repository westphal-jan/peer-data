{"id": "1302.7283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2013", "title": "Source Separation using Regularized NMF with MMSE Estimates under GMM Priors with Online Learning for The Uncertainties", "abstract": "hansie We aayiram propose carrickfergus a savers new method horwits to huseyin enforce priors on the michavila solution of 202.9 the nonnegative matrix factorization (bella NMF ). The proposed algorithm discriminating can devensian be used for precio denoising bbc4 or renyu single - cartouche channel denmon source separation (mccaul SCSS) pieridae applications. hootch The 2,269 NMF wojnowski solution is leehom guided graymail to borg follow 99.48 the collapsing Minimum Mean 17000 Square continuous-flow Error (MMSE) viviendas estimates under Gaussian yakumo mixture balthasar prior post-colonialism models (GMM) points-based for credi the source wintemute signal. In 33.86 SCSS applications, the tanjevic spectra sambourne of afge the observed welling mixed mocker signal moorside are safranchuk decomposed 24-28 as a ladki weighted linear 528.5 combination soga of nov\u00e9 trained basis vectors for each rose-marie source heister using NMF. janco In 66.21 this work, thiersch the NMF faired decomposition weight matrices vagankovo are keilor treated as a distorted 78.7 image by cantering a distortion performative operator, which bambang is sanderstead learned directly shetlands from gleich the moskowitz observed counterattacking signals. resourcing The MMSE estimate codice of the non-roman weights matrix agility under yamany GMM 84.76 prior and log - normal distribution gulalai for the distortion wyness is then macritchie found to improve friede the mespilus NMF decomposition awwad results. ifpi The MMSE estimate is dalgarno embedded within wisest the airheads optimization demchugdongrub objective cabi to form italicus a novel jeweller regularized NMF perception cost hayasaki function. datelined The corresponding update multi-tracked rules for the new g200 objectives riemannian are courthouses derived in 101.04 this reengineering paper. Experimental poinsett results martynov show that, 283.4 the isopod proposed buttstock regularized 83.99 NMF algorithm improves the wuerzburg source 5am separation performance herrmann compared liger with endoderm using dellacamera NMF without prior or with other prior models.", "histories": [["v1", "Thu, 28 Feb 2013 18:56:56 GMT  (164kb,D)", "http://arxiv.org/abs/1302.7283v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["emad m grais", "hakan erdogan"], "accepted": false, "id": "1302.7283"}, "pdf": {"name": "1302.7283.pdf", "metadata": {"source": "CRF", "title": "Source Separation using Regularized NMF with MMSE Estimates under GMM Priors with Online Learning for The Uncertainties", "authors": ["Emad M. Grais", "Hakan Erdogan"], "emails": ["haerdogan}@sabanciuniv.edu"], "sections": [{"heading": null, "text": "We propose a new method to enforce priors on the solution of the nonnegative matrix factorization (NMF). The proposed algorithm can be used for denoising or single-channel source separation (SCSS) applications. The NMF solution is guided to follow the Minimum Mean Square Error (MMSE) estimates under Gaussian mixture prior models (GMM) for the source signal. In SCSS applications, the spectra of the observed mixed signal are decomposed as a weighted linear combination of trained basis vectors for each source using NMF. In this work, the NMF decomposition weight matrices are treated as a distorted image by a distortion operator, which is learned directly from the observed signals. The MMSE estimate of the weights matrix under GMM prior and log-normal distribution for the distortion is then found to improve the NMF decomposition results. The MMSE estimate is embedded within the optimization objective to form a novel regularized NMF cost function. The corresponding update rules for the new objectives are derived in this paper. Experimental results show that, the proposed regularized NMF al-\nPreprint submitted to Elsevier March 1, 2013\nar X\niv :1\n30 2.\n72 83\nv1 [\ncs .L\nG ]\ngorithm improves the source separation performance compared with using NMF without prior or with other prior models. Keywords: Single channel source separation, nonnegative matrix factorization, minimum mean square error estimates, and Gaussian mixture models."}, {"heading": "1. Introduction", "text": "Nonnegative matrix factorization (Lee and Seung, 2001) is an important tool for source separation applications, especially when only one observation of the mixed signal is available. NMF is used to decompose a nonnegative matrix into a multiplication of two nonnegative matrices, a basis matrix and a gains matrix. The basis matrix contains a set of basis vectors and the gains matrix contains the weights corresponding to the basis vectors in the basis matrix. The NMF solutions are found by solving an optimization problem based on minimizing a predefined cost function. As most optimization problems, the main goal in NMF is to find the solutions that minimize the cost function without considering any prior information rather than the nonnegativity constraint. There have been many works that tried to enforce prior information related to the nature of the application on the NMF decomposition results. For audio source separation applications, the continuity and sparsity priors were enforced in the NMF decomposition weights (Virtanen, 2007). In Bertin et al. (2009), and Bertin et al. (2010), smoothness and harmonicity priors were enforced on the NMF solution in Bayesian framework and applied to music transcription. In Wilson et al. (2008b), and Wilson et al. (2008a) the regularized NMF was used to increase the NMF decomposition\nweights matrix likelihood under a prior Gaussian distribution. In Fevotte et al. (2009), Markov chain prior model for smoothness was used within a Bayesian framework in regularized NMF with Itakura-Saito (IS-NMF) divergence. In Virtanen et al. (2008), the conjugate prior distributions on the NMF weights and basis matrices solutions with the Poisson observation model within Bayesian framework was introduced. The Gamma distribution and the Gamma Markov chain (Cemgil and Dikmen, 2007) were used as priors for the basis and weights/gains matrices respectively in Virtanen et al. (2008). A mixture of Gamma prior model was used as a prior for the basis matrix in Virtanen and Cemgil (2009). The regularized NMF with smoothness and spatial decorrelation constraints was used in Chen et al. (2006) for EEG applications. In Cichocki et al. (2006), and Chen et al. (2006), a variety of constrained NMF algorithms were used for different applications.\nIn supervised single channel source separation (SCSS), NMF is used in two main stages, the training stage and the separation stage (Schmidt and Olsson, 2006; Grais and Erdogan, 2011a,b,c; Grais et al., 2012; Grais and Erdogan, 2012c). In the training stage, NMF is used to decompose the spectrogram of clean training data for the source signals into a multiplication of trained basis and weights/gains matrices for each source. The trained basis matrix is used as a representative model for the training data of each source and the trained gains matrices are usually ignored. In the separation stage, NMF is used to decompose the mixed signal spectrogram as a nonnegative weighted linear combination of the columns in the trained basis matrices. The spectrogram estimate for each source in the mixed signal can be found by summing its corresponding trained basis terms from the NMF decomposition during the\nseparation stage. One of the main problems of this framework is that, the estimate for each source spectrogram is affected by the other sources in the mixed signal. The NMF decomposition of the weight combinations in the separation stage needs to be improved. To improve the NMF decomposition during the separation stage, prior information about the weight combinations for each source can be considered.\nIn this work, we introduce a new method of enforcing the NMF solution of the weights matrix in the separation stage to follow certain estimated patterns. We assume we have prior statistical informations about the solution of the NMF weights matrix. The Gaussian mixture model (GMM) is used as a prior model for the valid/expected weight combination patterns that can exist in the columns of the weights matrix that are related to the nature of the source signals. Here, in the training stage, NMF is also used to decompose the spectrogram of the training data into trained basis and weights/gains matrices for each source. In this work, the trained gains matrix is used along with the trained basis matrix to represent each source. We can see the columns of the trained gains matrix as valid weight combinations that their corresponding bases in the basis matrix can jointly receive for a certain type of source signal. The columns of the trained gains matrix can be used to train a prior model that captures the statistics of the valid weight combinations that the bases can receive. The prior gain model and the trained basis matrix for each source can be used to represent each source in the separation stage. During the separation stage, the prior model can guide the NMF solution to prefer these valid weight patterns. The multivariate Gaussian mixture model (GMM) can be used to model the trained gains matrix (Grais and Erdogan,\n2012b). The GMM is a rich model which captures the statistics and the correlations of the valid gain combinations for each source signal. GMMs are extensively used in speech processing applications like speech recognition and speaker verification. GMMs are used to model the multi-modal nature in speech feature vectors due to phonetic differences, speaking styles, gender, accents (Rabiner and Juang, 1993). We are conjecturing that the weight vectors of the NMF gains matrix can be considered as a feature extracted from the signal in a frame so that it can be modeled well with a GMM. The columns in the trained weights matrix are normalized, and their logarithm is then calculated and used to train the GMM prior. The basis matrix and the trained GMM prior model for the weights are jointly used as a trained representative model for the source training signals.\nIn the separation stage and after observing the mixed signal, NMF is used again to decompose the mixed signal spectrogram as a weighted linear combination of the trained basis vectors for the sources that are involved in the observed mixed signal. The conventional NMF solution for the weight combinations is found to minimize a predefined NMF cost function ignoring that, for each set of trained basis vectors of a certain source signal there is a set of corresponding valid weight combinations that the bases can possibly receive. In Grais and Erdogan (2012b), the prior GMM that models the valid weight combinations for each source is used to guide the NMF solution for the gains matrix during the separation stage. The priors in Grais and Erdogan (2012b) are enforced by maximizing the log-likelihood of the NMF solution with the trained prior GMMs. The priors in Grais and Erdogan (2012b) are enforced without evaluating how good the NMF solution is without using the\npriors. For example, if the NMF solution without prior is not satisfactory, we would like to rely more on the priors and vice versa.\nIn this work, we introduce a new strategy of applying the priors on the NMF solutions of the gain matrix during the separation stage. The new strategy is based on evaluating how much the solution of the NMF gains matrix needs to rely on the prior GMMs. The NMF solutions without using priors for the weights matrix for each source during the separation stage can be seen as a deformed image, and its corresponding valid weights/gains matrix is needed to be estimated under the GMM prior. The deformation operator parameters which measure the uncertainty of the NMF solution of the weights matrix are learned directly from the observed mixed signal. The uncertainty in this work is a measurement of how far the NMF solution of the weights matrix during the separation stage is from being a valid weight pattern that is modeled in the prior GMM. The learned uncertainties are used with the minimum mean square error (MMSE) estimator to find the estimate of the valid weights matrix. The estimated valid weights matrix should also consider the minimization of the NMF cost function. To achieve these two goals, a regularized NMF is used to consider the valid weight patterns that can appear in the columns of the weights matrix while decreasing the NMF cost function. The uncertainties within MMSE estimates of the valid weight combinations are embedded in the regularized NMF cost function for this purpose. The uncertainty measurements play very important role in this work as we will show in next sections. If the uncertainty of the NMF solution of the weights matrix is high, that means the regularized NMF needs more support from the prior term. In case of low uncertainty, the regularized\nNMF needs less support from the prior term. Including the uncertainty measurements in the regularization term using MMSE estimate makes the proposed regularized NMF algorithm decide automatically how much the solution should rely on the prior GMM term. This is the main advantage of the proposed regularized NMF compared to the regularization using the loglikelihood of the GMM prior or other prior distributions (Grais and Erdogan, 2012a,b; Canny, 2004). Incorporation of the uncertainties that measure the extent of distortion in the NMF weights matrix solutions in the regularization term is a main novelty of this work, which has not been seen before in the regularization literature.\nThe remainder of this paper is organized as follows: In section 2, we give a brief explanation about NMF. In section 3, we discuss the problem of single channel source separation and its formulation. In Section 4, we show the conventional usage of NMF in SCSS problems. Section 5 describes the needs for a regularized NMF. Sections 6 to 9 introduce the new regularized NMF and how it is used in the SCSS problem, which is the main contribution of this paper. Section 10 indicates the source signal reconstruction after NMF decomposition. In the remaining sections, we present our observations and the results of our experiments."}, {"heading": "2. Nonnegative matrix factorization", "text": "Nonnegative matrix factorization is used to decompose any nonnegative matrix V into a multiplication of a nonnegative basis matrix B and a nonnegative gains or weights matrix G as follows:\nV \u2248 BG. (1)\nThe columns of matrix B contain nonnegative basis or dictionary vectors that are optimized to allow the data in V to be approximated as a nonnegative linear combination of its constituent vectors. Each column in the gains/weights matrix G contains the set of weight combinations that the basis vectors in the basis matrix have for its corresponding column in the V matrix. To solve for matrix B and G, different NMF cost functions can be used. For audio source separation applications, the Itakura-Saito (IS-NMF) divergence cost function (Fevotte et al., 2009) is usually used. This cost function is found to be a good measurement for the perceptual differences between different audio signals (Fevotte et al., 2009; Jaureguiberry et al., 2011). The IS-NMF cost function is defined as:\nmin B,G\nDIS (V ||BG) , (2)\nwhere\nDIS (V ||BG) = \u2211 m,n\n( V m,n\n(BG)m,n \u2212 log V m,n (BG)m,n \u2212 1\n) .\nThe IS-NMF solutions for equation (2) can be computed by alternating multiplicative updates of B and G (Fevotte et al., 2009; Jaureguiberry et al., 2011) as:\nB \u2190 B \u2297\nV (BG) 2G T\n1 BGG\nT , (3)\nG\u2190 G\u2297 BT V (BG) 2\nBT 1BG\n, (4)\nwhere the operation \u2297 is an element-wise multiplication, all divisions and (.)2 are element-wise operations. In source separation applications, IS-NMF is\nused with matrices of power spectral densities of the source signals (Fevotte et al., 2009; Jaureguiberry et al., 2011)."}, {"heading": "3. Problem formulation for SCSS", "text": "In single channel source separation (SCSS) problems, the aim is to find estimates of source signals that are mixed on a single observation channel y(t). This problem is usually solved in the short time Fourier transform (STFT) domain. Let Y (t, f) be the STFT of y(t), where t represents the frame index and f is the frequency-index. Due to the linearity of the STFT, we have:\nY (t, f) = S1(t, f) + S2(t, f), (5)\nwhere S1(t, f) and S2(t, f) are the unknown STFT of the first and second sources in the mixed signal. Assuming independence of the sources, we can write the power spectral density (PSD) of the measured signal as the sum of source signal PSDs as follows:\n\u03c32y(t, f) = \u03c3 2 1(t, f) + \u03c3 2 2(t, f), (6)\nwhere \u03c32y(t, f) = E(|Y (t, f)|2). We can write the PSDs for all frames as a spectrogram matrix as follows:\nY = S1 + S2, (7)\nwhere S1 and S2 are the unknown spectrograms of the source signals, and they need to be estimated using the observed mixed signal and training data for each source. The spectrogram of the measured signal Y is calculated by taking the squared magnitude of the STFT of the measured signal y(t)."}, {"heading": "4. Conventional NMF for SCSS", "text": "In conventional single channel source separation using NMF without regularization (Grais et al., 2012), there are two main stages to find estimates for S1 and S2 in equation (7). The first stage is the training stage and the second stage is the separation/testing stage. In the training stage, the spectrogram Strain for each source is calculated by computing the squared magnitude of the STFT of each source training signal. NMF is used to decompose the spectrogram into basis and gains matrices as follows:\nStrain1 \u2248 B1Gtrain1 , Strain2 \u2248 B2Gtrain2 , (8)\nthe multiplicative update rules in equations (3) and (4) are used to solve for B1,B2,G train 1 and G train 2 for both sources. Within each iteration, the columns of B1 and B2 are normalized and the matrices G train 1 and G train 2 are computed accordingly. The initialization of all matrices B1,B2,G train 1 and Gtrain2 is done using positive random noise. After finding basis and gains matrices for each source training data, the basis matrices are used in the mixed signal decomposition as shown in the following sections. All the basis matrices B1 and B2 are kept fixed in the remaining sections in this paper.\nIn the separation stage after observing the mixed signal y(t), NMF is used to decompose the mixed signal spectrogram Y with the trained bases matrices B1 and B2 that were found from solving equation (8) as follows:\nY \u2248 [B1,B2]G, or Y \u2248 [B1 B2]  G1 G2  , (9) then the corresponding spectrogram estimate for each source can be found\nas:\nS\u03031 = B1G1, S\u03032 = B2G2. (10)\nLet Btrain = [B1,B2]. The only unknown here is the gains matrix G since the matrix Btrain was found during the training stage and it is fixed in the separation stage. The matrix G is a combination of two submatrices as in equation (9). NMF is used to solve for G in (9) using the update rule in equation (4) and G is initialized with positive random numbers."}, {"heading": "5. Motivation for regularized NMF", "text": "The solution of the gains submatrix G1 in (9) is affected by the existence of the second source in the mixed signal. Also, G2 is affected by the first source in the mixed signal. The effect of one source into the gains matrix solution of the other source strongly depends on the energy level of each source in the mixed signal. Therefore, the estimated spectrograms S\u03031 and S\u03032 in equation (10) that are found from solving G using the update rule in (4) may contain residual contribution from each other and other distortions. To fix this problem, more discriminative constraints must be added to the solution of each gains submatrix. The columns of the solution gains submatrix G1 and G2 should form a valid/expected weight combinations for its corresponding basis matrix of its corresponding source signal. The information about the valid weight combinations that can exits in the gains matrix for a source signal can be found in the gains matrix that was computed from the clean training data of the same source. For example, the information about valid weight combinations that can exist in the gains matrix G1 in equation (9) can be found in its training gains matrix Gtrain1 in equation (8).\nThe columns of the trained gains matrix Gtrain1 represent the valid weight combinations that the basis matrix B1 can receive for the first source. Note that, the basis matrix B1 is common in the training and separation stages. The solution of the gains submatrix G1 in equation (9) should consider the prior about the valid combination that is present in its corresponding trained gains matrix Gtrain1 in equation (8) for the same source.\nIn our previous work (Grais and Erdogan, 2012b), data in the training gains matrixGtraini for source i was modeled using a GMM. The NMF solution of the gains matrix during the separation stage was guided by the prior GMM. The GMM was learned using the logarithm of the normalized columns of the training gains matrix. The NMF solution for the gains matrix during the separation stage was enforced to increase its log-likelihood with the trained GMM prior using regularized NMF as follows:\nCold = DIS (Y ||BtrainG)\u2212Rold(G), (11)\nwhere Rold(G) is the weighted sum of the log-likelihoods of the log-normalized columns of the gains matrix G. Rold(G) was defined as follows:\nRold(G) = 2\u2211 i=1 \u03b7i\u0393old(Gi), (12)\nwhere \u0393old(Gi) is the log-likelihood for the submatrix Gi, and \u03b7i is the regularization parameter for source i. The regularization parameter in Grais and Erdogan (2012b) was playing two important roles. The first role was to match the scale of the IS-NMF divergence term with the scale of the log-likelihood prior term. The second role was to decide how much the regularized NMF cost function needs to rely on the prior term. The results in Grais and Erdogan (2012b) show that, when the source i has higher energy level than\nthe other sources, the value of its corresponding regularization parameter \u03b7i becomes smaller than the values of other regularization parameters for the other sources. That can be reformed as follows: when the source has high energy level, the gains matrix solution of the regularized NMF in (11) rely less on the prior model and vice versa. The values of the regularization parameters in Grais and Erdogan (2012b) was chosen manually for every energy level for each source. In the cases when the conjugate prior models of the NMF solutions were used (Virtanen et al., 2008; Canny, 2004), the hyperparameters of the prior models were also chosen manually. The conjugate prior models usually enforced on NMF solutions using a Bayesian framework (Fevotte et al., 2009; Virtanen et al., 2008; Canny, 2004). In Grais and Erdogan (2012b), it was also shown that, the hyper-parameter choices for the conjugate prior models can also depend on the energy level differences of the source signals in the mixed signal."}, {"heading": "6. Motivation for the proposed regularized NMF", "text": "In this work, we try to use prior GMMs to guide the solution of the gains matrix during the separation stage using regularized NMF as in Grais and Erdogan (2012b) but following a totally different regularization strategy. We also try to find a way to estimate how much the solution of the regularized NMF needs to rely on the prior GMMs automatically not manually as in Grais and Erdogan (2012b). The way of finding how much the regularized NMF solution of the gains matrix needs to rely on the prior GMM is by measuring how far the statistics of the solution of the gains matrix Gi in (9) is from the statistics of the solution of the valid gains matrix solution\nGtraini in (8) for source i. Recall that, the matrix G train i in (8) contains the weight combinations that the columns in the basis matrix Bi can jointly receive for the clean data of source i. The data in Gtraini can be used as a prior information for what kinds of weight combinations that should exist in Gi in (9) since the matrix Bi is the same in (8) and (9). The matrix Gtraini in (8) is used to train a prior GMM for the expected (valid) weight combinations that can exist in the gains matrix for source i as in Grais and Erdogan (2012b). The solution of the gains submatrix Gi in (9) can be seen as a deformed observation that needs to be restored using MMSE estimate under its corresponding GMM prior for source i. How far the statistics of the solution of the gains matrix Gi is from the statistics of the solution of the valid gains matrix solution Gtraini can be seen as how much the gains submatrix Gi is deformed. How much deformation exists in the gains matrix Gi can be learned directly and the logarithm of this deformation is modeled using a Gaussian distribution with zero mean and a diagonal covariance matrix \u03a8i. When the deformation or the uncertainty measurement \u03a8i of the gain submatrix Gi is high, we expect our target regularized NMF cost function to rely more on the prior GMM for source i and vice versa. Based on the measurement \u03a8i, the proposed NMF cost function decides automatically how much the solution of the regularized NMF needs to rely on the prior GMMs, which is a main advantage of the proposed regularized NMF over our previous work (Grais and Erdogan, 2012b). Applying the prior information on the gains matrix Gi in (9) using MMSE estimate under a GMM prior using regularized NMF is the new strategy that we introduce in this paper.\nIn the following sections, we give more details about training the prior\nGMM for the gains matrix for each source. Then, we give more details about our proposed regularized NMF using MMSE estimate to find better solution for the gains matrix in (9). In Section 8, we present our proposed regularized NMF in a general manner. In Section 8, we assume we have a trained basis matrix B, a trained prior GMM for a clean gains matrix, and a gains matrix G that inherited some distortion from the original matrix V from solving equation (2). We introduce our proposed regularized NMF in a general fashion in Section 8 to make the idea clearer for different NMF applications like, dimensionality reduction, denosing, and other applications. The update rules that solve the proposed regularized NMF are also derived in Section 8 in a general fashion regardless of the application. The GMM in Section 8 is the trained prior GMM that captures the statistics of the valid weights combinations that should have been existed in the gains matrixG. In section 9, we show how we use the proposed regularized NMF to find better solutions for the gain submatrices in equation (9) for our single channel source separation problem."}, {"heading": "7. Training the GMM prior models", "text": "We use the gains matrices Gtrain1 and G train 2 in equation (8) to train prior models for the expected/valid weight patterns in the gains matrix for each source. For each matrix Gtrain1 and G train 2 , we normalize their columns and then calculate their logarithm. The normalization in this paper is done using the Euclidean norm. The log-normalized columns are then used to train a gains prior GMM for each source. The GMM for a random variable x is\ndefined as:\np(x) = K\u2211 k=1\n\u03c0k\n(2\u03c0)d/2 |\u03a3k|1/2 exp\n{ \u22121\n2 (x\u2212 \u00b5k)\nT \u03a3\u22121k (x\u2212 \u00b5k) } , (13)\nwhere K is the number of Gaussian mixture components, \u03c0k is the mixture weight, d is the vector dimension, \u00b5k is the mean vector and \u03a3k is the diagonal covariance matrix of the kth Gaussian model. In training GMM, the expectation maximization (EM) algorithm (Dempster et al., 1977) is used to learn the GMM parameters (\u03c0k,\u00b5k,\u03a3k, \u2200k = {1, 2, ..., K}) for each source given its trained gain matrixGtrain. The suitable value for K usually depends on the nature, dimension and the size of the available training data. We use the logarithm because it has been shown that the logarithm of a variable taking values between 0 and 1 can be modeled well by a GMM (Wessel et al., 2000). Since the main goal of the prior model is to capture the statistics of the patterns in the trained gains matrix, we use normalization to make the prior models insensitive to the energy level of the training data. The normalization makes the same prior models applicable for a wide range of energy levels and avoids the need to train a different prior model for different energy levels. By normalization we are modeling the ratio and correlation between the combination of the weights that the bases can jointly receive."}, {"heading": "8. The proposed regularized NMF", "text": "The goal of regularized NMF is to incorporate prior information on the solution matrices B and G. In this work, we enforce a statistical prior information on the solution of the gains/weights matrix G only. We need the solution of the gains matrixG to minimize the IS-divergence cost function\nin equation (2), and the columns of the gains matrix G should form valid weight combinations under a prior GMM model.\nThe most used strategy for incorporating a prior is by maximizing the likelihood of the solution under the prior model while minimizing the NMF divergence at the same time. To achieve this, we usually add these two objectives in a single cost function. In Grais and Erdogan (2012b), a GMM was used as the prior model for the gains matrix, and the solution of the gains matrix was encouraged to increase its log-likelihood with the prior model using this regularized NMF cost function. The regularization parameters in Grais and Erdogan (2012b) were the only tools to control how much the regularized NMF relies on the prior models based on the energy differences of the sources in the mixed signal. The values of the regularization parameters were changed manually in that work.\nGaussian mixture model is a very general prior model where we can see the means of the GMM mixture components as \u201cvalid templates\u201d that were observed in the training data. Even, Parzen density priors (Kim et al., 2007) can be seen under the same framework. In Parzen density prior estimation, training examples are seen as \u201cvalid templates\u201d and a fixed variance is assigned to each example. In GMM priors, we learn the templates as cluster means from training data and we can also estimate the cluster variances from the data. We can think of the GMM prior as a way to encourage the use of valid templates or cluster means in the NMF solution during the separation stage. This view of the GMM prior will be helpful in understanding the MMSE estimate method we introduce in this paper.\nWe can find a way of measuring how far the conventional NMF (NMF\nwithout prior) solution is from the trained templates in the prior GMM and call this the error term. Based on this error, the regularized NMF can decide automatically how much the solution of the NMF needs help from the prior model. If the conventional NMF solution is far from the templates then the regularized NMF will rely more on the prior model. If the conventional NMF solution is close to the templates then the regularized NMF will rely less on the prior model. By deciding automatically how much the regularized NMF needs to rely on the prior we conjecture that, we do not need to manually change the values for the regularization parameter based on the energy differences of the sources in the mixed signal 1 to improve the performance of NMF as in Grais and Erdogan (2012b).\nWe use the following way of measuring how far the conventional NMF solution is from the prior templates: We can see the solution of the conventional NMF as distorted observations of a true/valid template. Given the prior GMM templates, we can learn a probability distribution model for the distortion that captures how far the observations in the conventional gains matrix is from the prior GMM. The distortion or the error model can be seen as a summary of the distortion that exists in all columns in the gains matrix of the NMF solution.\nBased on the prior GMM and the trained distortion model, we can find a better estimate for the desired observation for each column in the distorted gains matrix. We can mathematically formulate this by seeing the solution matrix G that only minimizes the cost function in equation (2) as a distorted\n1In this paper, the regularization parameters are chosen once and kept fixed regardless\nof the energy differences of the source signals\nimage where its restored image needs to be estimated. The columns of the matrix G are normalized using the `2 norm and their logarithm is then calculated. Let the log-normalized column n namely (log gn \u2016gn\u20162 ) of the gains matrix be qn. The vector qn is treated as a distorted observation as:\nqn = xn + e, (14)\nwhere xn is the logarithm of the unknown desired pattern that corresponds to the observation qn and needs to be estimated under a prior GMM, e is the logarithm of the deformation operator, which is modeled by a Gaussian distribution with zero mean and diagonal covariance matrix \u03a8 as N (e|0,\u03a8). The GMM prior model for the gains matrix is trained using log-normalized columns of the trained gains matrix from training data as shown for example in Section 7. The uncertainty \u03a8 is trained directly from all the log-normalized columns of the gains matrix q = {q1, .., qn, .., qN}, where N is the number of columns in the matrix G. The uncertainty \u03a8 can be iteratively learned using the expectation maximization (EM) algorithm. Given the prior GMM parameters which are considered fixed here, the update of \u03a8 is found based on the sufficient statistics z\u0302n and R\u0302n as follows (Rosti and Gales, 2001, 2004; Ghahramani and Hinton, 1997) [Appendix A]:\n\u03a8 = diag\n{ 1\nN N\u2211 n=1 ( qnq T n \u2212 qnz\u0302 T n \u2212 z\u0302nqTn + R\u0302n )} , (15)\nwhere the \u201cdiag\u201d operator sets all the off-diagonal elements of a matrix to zero, N is the number of columns in matrix G, and the sufficient statistics z\u0302n and R\u0302n can be updated using \u03a8 from the previous iteration as follows:\nz\u0302n = K\u2211 k=1 \u03b3knz\u0302kn, (16)\nand\nR\u0302n = K\u2211 k=1 \u03b3knR\u0302kn, (17)\nwhere\n\u03b3kn = [ \u03c0kN (qn|\u00b5k,\u03a3k + \u03a8)\u2211K j=1 \u03c0jN ( qn|\u00b5j,\u03a3j + \u03a8 )] , (18) R\u0302kn = \u03a3k \u2212\u03a3k (\u03a3k + \u03a8)\u22121 \u03a3Tk + z\u0302knz\u0302 T kn, (19)\nand\nz\u0302kn = \u00b5k + \u03a3k (\u03a3k + \u03a8) \u22121 (qn \u2212 \u00b5k) . (20)\n\u03a8 is considered as a general uncertainty measurement over all the observations in matrixG. \u03a8 can be seen as a model that summarizes the deformation that exists in all columns in the gains matrix G.\nGiven the GMM prior parameters and the uncertainty measurement \u03a8, the MMSE estimate of each pattern xn given its observation qn under the observation model in equation (14) can be found similar to Rosti and Gales (2001, 2004), and Ghahramani and Hinton (1997) as in Appendix A as follows:\nf (qn) = K\u2211 k=1 \u03b3kn [ \u00b5k + \u03a3k (\u03a3k + \u03a8) \u22121 (qn \u2212 \u00b5k) ] = x\u0302n, (21)\nwhere\n\u03b3kn = [ \u03c0kN (qn|\u00b5k,\u03a3k + \u03a8)\u2211K j=1 \u03c0jN ( qn|\u00b5j,\u03a3j + \u03a8 )] . (22) The value of \u03a8 in the term \u03a3k (\u03a3k + \u03a8) \u22121 in equation (21) plays an important role in this framework. When the entries of the uncertainty \u03a8 are very small comparing to their corresponding entries in \u03a3k for a certain active GMM component k, the term \u03a3k (\u03a3k + \u03a8) \u22121 tends to be the identity matrix, and MMSE estimate in (21) will be the observation qn. When the entries of\nthe uncertainty \u03a8 are very high comparing to their corresponding entries in \u03a3k for a certain active GMM component k, the term \u03a3k (\u03a3k + \u03a8) \u22121 tends to be a zeros matrix, and MMSE estimate will be the weighted sum of prior\ntemplates \u2211K\nk=1 \u03b3kn\u00b5k. In most cases \u03b3kn tends to be close to one for one\nGaussian component, and close to zero for the other components in a large dimension space. This makes the MMSE estimate in the case of high \u03a8 to be one of the mean vectors in the prior GMM, which is considered as a template pattern for the valid observation. We can rephrase this as follows: When the uncertainty of the observations q is high, the MMSE estimate of x, relies more on the prior GMM of x. When the uncertainty of the observations q is low, the MMSE estimate of x, relies more on the observation qn. In general, the MMSE solution of x lies between the observation qn and one of the templates in the prior GMM. The term \u03a3k (\u03a3k + \u03a8) \u22121 controls the distance between x\u0302n and qn and also the distance between x\u0302n and one of the template \u00b5k assuming that \u03b3kn \u2248 1 for a Gaussian component k.\nThe model in equation (14) expresses the normalized columns of the gains matrix as a distorted image with a multiplicative deformation diagonal matrix. For the normalized gain columns gn \u2016gn\u20162 of G there is a deformation matrix E with log-normal distribution that is applied to the correct pattern that we need to estimate g\u0302n as follows:\ngn \u2016gn\u20162 = Eg\u0302n. (23)\nThe uncertainty for E is represented in its covariance matrix \u03a8. For the distorted matrix G we find its corresponding MMSE estimate for its lognormalized columns G\u0302. Another reason for working in logarithm domain is that, the gains are constrained to be nonnegative and the MMSE estimate\ncan be negative so the logarithm of the normalized gains is an unconstrained variable that we can work with. The estimated weight patterns in G\u0302 that are corresponding to the MMSE estimates for the correct patterns do not consider minimizing the NMF cost function in equation (2), which is still the main goal. We need the solution of G to consider the pattern shape priors on the solution of the gains matrix, and also considers the reconstruction error of the NMF cost function. To consider the combination of the two objectives, we consider using the regularized NMF. We add a penalty term to the NMF-divergence cost function. The penalty term tries to minimize the distance between the solution of log-normalized columns of gn with its corresponding MMSE estimate f(gn) as follows:\nlog gn \u2016gn\u20162\n\u2248 f (\nlog gn \u2016gn\u20162\n) or\ngn \u2016gn\u20162\n\u2248 exp ( f ( log\ngn \u2016gn\u20162\n)) . (24)\nThe regularized IS-NMF cost function is defined as follows:\nC = DIS (V ||BG) + \u03b1L(G), (25)\nwhere\nL(G) = N\u2211 n=1 \u2225\u2225\u2225\u2225 gn\u2016gn\u20162 \u2212 exp ( f ( log gn \u2016gn\u20162 ))\u2225\u2225\u2225\u22252 2 , (26)\nf ( log\ngn \u2016gn\u20162\n) is the MMSE estimate defined in equation (21), and \u03b1 is a\nregularization parameter. The regularized NMF can be rewritten in more details as C = \u2211 m,n ( V m,n (BG)m,n \u2212 log V m,n (BG)m,n \u2212 1 ) + \u03b1 N\u2211 n=1 \u2225\u2225\u2225\u2225\u2225 gn\u2016gn\u20162 \u2212 exp ( K\u2211 k=1 \u03b3kn [ \u00b5k +\u03a3k (\u03a3k +\u03a8) \u22121 ( log gn \u2016gn\u20162 \u2212 \u00b5k )])\u2225\u2225\u2225\u2225\u2225 2\n2\n.\n(27)\nIn equation (27), the MMSE estimate of the desired patterns of the gains matrix is embedded in the regularized NMF cost function. The first term\nin (27), decreases the reconstruction error between V and BG. Given \u03a8, we can forget for a while the MMSE estimate concept that leaded us to our target regularized NMF cost function in (27) and see equation (27) as an optimization problem. We can see from (27) that, if the distortion measurement parameter \u03a8 is high, the regularized nonnegative matrix factorization solution for the gains matrix will rely more on the prior GMM for the gains matrix. If the distortion parameter \u03a8 is low, the regularized nonnegative matrix factorization solution for the gains matrix will be close to the ordinary NMF solution for the gains matrix without considering any prior. The second term in equation (27) is ignored in the case of zero uncertainty \u03a8. In case of high values of \u03a8, the second term encourages to decrease the distance between each normalized column gn \u2016gn\u20162 in G with a corresponding prior template exp (\u00b5k) assuming that \u03b3kn \u2248 1 for a certain Gaussian component k. For different values \u03a8, the penalty term decreases the distance between each gn \u2016gn\u20162 and an estimated pattern that lies between a prior template and gn \u2016gn\u20162 . The term (log gn \u2016gn\u20162 \u2212 \u00b5k) in (27) measures how far each log-normalized column in the gains matrix is from a valid template \u00b5k. Under the assumption \u03b3kn \u2248 1 for a certain Gaussian component k, the second term in (27) is also ignored when the observation log gn \u2016gn\u20162 form a valid pattern (log gn \u2016gn\u20162 = \u00b5k). How far each log-normalized column in the gains matrix is from a valid template decides how much influence the MMSE estimate prior term has to the solution of (27) for each observation.\nThe multiplicative update rule forB in (27) is still the same as in equation (3). The multiplicative update rule for G can be found by following the same procedures as in Virtanen (2007); Bertin et al. (2010); Grais and Erdogan\n(2012b). The gradient with respect to G of the cost function \u2207GC can be expressed as a difference of two positive terms \u2207+GC and \u2207 \u2212 GC as follows:\n\u2207GC = \u2207+GC \u2212\u2207 \u2212 GC. (28)\nThe cost function is shown to be nonincreasing under the update rule (Virtanen, 2007; Bertin et al., 2010):\nG\u2190 G\u2297 \u2207 \u2212 GC\n\u2207+GC , (29)\nwhere the operations \u2297 and division are element-wise as in equation (4). We can write the gradients as:\n\u2207GC = \u2207GDIS + \u03b1\u2207GL(G), (30)\nwhere \u2207GL(G) is a matrix with the same size of G. The gradient for the ISNMF and the gradient of the prior term can also be expressed as a difference of two positive terms as follows:\n\u2207GDIS = \u2207+GDIS \u2212\u2207 \u2212 GDIS, (31)\nand\n\u2207GL(G) = \u2207+GL(G)\u2212\u2207 \u2212 GL(G). (32)\nWe can rewrite equations (28, 30) as:\n\u2207GC = ( \u2207+GDIS + \u03b1\u2207 + GL(G) ) \u2212 ( \u2207\u2212GDIS + \u03b1\u2207 \u2212 GL(G) ) . (33)\nThe final update rule in equation (29) can be written as follows:\nG\u2190 G\u2297 \u2207 \u2212 GDIS + \u03b1\u2207 \u2212 GL(G)\n\u2207+GDIS + \u03b1\u2207 + GL(G)\n, (34)\nwhere\n\u2207GDIS = BT 1 BG \u2212BT V (BG)2 , (35)\n\u2207\u2212GDIS = B T V\n(BG)2 , and \u2207+GDIS = B\nT 1\nBG . (36)\nNote that, in calculating the gradients \u2207+GL(G) and \u2207 \u2212 GL(G), the term \u03b3kn is also a function of G. The gradients \u2207+GL(G) and \u2207 \u2212 GL(G) are calculated in Appendix B. Since all the terms in equation (34) are nonnegative, then the values of G of the update rule (34) are nonnegative."}, {"heading": "9. The proposed regularized NMF for SCSS", "text": "In this section, we are back to the single channel source separation problem to find a better solution to equation (9). Figure 1 shows the flow chart that summarizes all stages of applying our proposed regularized NMF method for SCSS problems. Given the trained basis matrices B1, B2 that were computed from solving (8), and the trained gain prior GMM for each source from Section 7, we try to apply the proposed regularized NMF cost function in Section 8 to find better solution for the gain submatrices in equation (9). The bases matrix Btrain = [B1,B2] is still fixed here, we just need to update the gains matrix G in (9). The normalized columns of the submatrices G1 and G2 in equation (9) can be seen as deformed images as in equation (23) and their restored images are needed to be estimated. First, we need to learn the uncertainties parameters \u03a81 and \u03a82 for the deformation operators E1 and E2 respectively for each image as shown in learning the uncertainties stage in Figure 1. The columns of the submatrix G1 are normalized and\ntheir logarithm are calculated and used with the trained GMM prior parameters for the first source to estimate \u03a81 iteratively using the EM algorithm in equations (15) to (20). The log-normalized columns \u201clog gn \u2016gn\u20162 \u201d of G1 can be seen as qn in equations (15) to (20). We repeat the same procedures to calculate \u03a82 using the log-normalized columns of G2 and the prior GMM for the second source. The uncertainties \u03a81 and \u03a82 can also be seen as measurements of the remaining distortion from one source into another source, which also depends on the mixing ratio between the two sources. For example, if\nthe first source has higher energy than the second source in the mixed signal, we expect the values of \u03a82 to be higher than the values in \u03a81 and vice versa. After calculating the uncertainty parameters for both sources \u03a81 and \u03a82, we use the regularized NMF in (25) to solve for G with the prior GMMs for both sources and the estimated uncertainties \u03a81 and \u03a82 as follows:\nC = DIS (Y ||BtrainG) +R(G), (37)\nwhere\nR(G) = \u03b11L1(G1) + \u03b12L2(G2), (38)\nL1(G1) is defined as in equation (26) for the first source, L2(G2) is for the second source, \u03b11, and \u03b12 are their corresponding regularization parameters. The update rule in equation (34) can be used to solve for G after modifying it as follows:\nG\u2190 G\u2297 \u2207 \u2212 GDIS +\u2207 \u2212 GR(G)\n\u2207+GDIS +\u2207 + GR(G)\n, (39)\nwhere \u2207+GR(G) and \u2207 \u2212 GR(G) are nonnegative matrices with the same size of G and they are combinations of two submatrices as follows:\n\u2207\u2212GR(G) =  \u03b11\u2207\u2212GL(G1) \u03b12\u2207\u2212GL(G2)  , \u2207+GR(G) =  \u03b11\u2207+GL(G1) \u03b12\u2207+GL(G2)  , (40) where \u2207+GL(G1),\u2207 \u2212 GL(G1),\u2207 + GL(G2), and \u2207 \u2212 GL(G2) are calculated as in section 8 for each source.\nThe normalization of the columns of the gain matrices are used in the prior term R(G) and its gradient terms only. The general solution for the gains matrix of equation (37) at each iteration is not normalized. The normalization is done only in the prior term since the prior models have been\ntrained by normalized data before. Normalization is also useful in cases where the source signals occur with different energy levels from each other in the mixed signal. Normalizing the training and testing gain matrices gives the prior models a chance to work with any energy level that the source signals can take in the mixed signal regardless of the energy levels of the training signals.\nThe regularization parameters in (38) have only one role. They are chosen to match the scale between the NMF divergence term and the MMSE estimate prior term in the regularized NMF cost function in (37). There is no need to change the values of the regularization parameters according to the energy differences of the source signals in the mixed signal as in Grais and Erdogan (2012b). Reasonable values for the regularization parameters are chosen manually and kept fixed in this work. Another main difference between the regularized NMF in Grais and Erdogan (2012b) that is shown in equation (11) and the proposed regularized NMF in this paper is related to the training procedures for the source models. In both works, the main aim of the training stage is to train the basis matrices and the gains prior GMMs for the source signals. In Grais and Erdogan (2012b), to match between the way the trained models were used during training with the way they were used during separation, the basis matrices and the prior GMM parameters were learned jointly using the regularized NMF cost function in (11). The joint training for the source models was introduced in Grais and Erdogan (2012b) to improve the separation performance. In joint training, after updating the gains matrix at each NMF iteration using the gain update rule for the regularized NMF in (11), the GMM parameters were then updated (re-\ntrained). Since, we needed to update (retrain) the GMM parameters at each NMF iteration, joint training slowed down the training of the source models in Grais and Erdogan (2012b). Another problem of using joint training is that, we had other regularization parameters during the training stage that needed to be chosen. Using joint training duplicates the number of the regularization parameters that need to be chosen. Choosing the regularization parameters in Grais and Erdogan (2012b) was done using validation data. That means, in Grais and Erdogan (2012b) we had to train many source models (basis matrix and prior GMM) for different regularization parameter values. Then, we chose the best combination for the regularization parameter values in training and separation stages that gave the best results during the separation stage. In the case of using MMSE estimate regularization for NMF, we do not need to use joint training. In this paper, we do not need to consider solving the regularized NMF in (27) during the training stage to solve (8). In the training stage, the training data for each source is assumed to be clean data. Since the spectrogram of each source training data represents clean source data, the NMF solution for the gains matrix can not be seen as a distorted image. Therefore, the deformation measurement parameter \u03a8train is a matrix of zeros. When \u03a8train = 0, the MMSE estimates prior\nterm in (27) will disappear because \u2211K\nk=1 \u03b3kn = 1. Then, the regularized\nNMF (27) becomes just NMF. That means, we do not need to use the regularized NMF during the training stage which is not the case in Grais and Erdogan (2012b). Here in the training stage, we just need to use IS-NMF to decompose the spectrogram of the training data into trained basis and gains matrices. After the trained gains matrix is computed, it is used to train the\nprior GMM as shown in Sections 4 and 7."}, {"heading": "10. Source signals reconstruction", "text": "After finding the suitable solution for the gains matrix G in Section 9, the initial estimated spectrograms S\u03031 and S\u03032 can be calculated from (10) and then used to build spectral masks as follows:\nH1 = S\u03031\nS\u03031 + S\u03032 , H2 =\nS\u03032\nS\u03031 + S\u03032 , (41)\nwhere the divisions are done element-wise. The final estimate of each source STFT can be obtained as follows:\nS\u03021 (t, f) = H1 (t, f)Y (t, f) , S\u03022 (t, f) = H2 (t, f)Y (t, f) , (42)\nwhere Y (t, f) is the STFT of the observed mixed signal in equation (5), H1 (t, f) and H2 (t, f) are the entries at row f and column t of the spectral masks H1 and H2 respectively. The spectral mask entries scale the observed mixed signal STFT entries according to the contribution of each source in the mixed signal. The spectral masks can be seen as the Wiener filter as in Fevotte et al. (2009). The estimated source signals s\u03021(t) and s\u03022(t) can be found by using inverse STFT of their corresponding STFTs S\u03021(t, f) and S\u03022(t, f)."}, {"heading": "11. Experiments and Discussion", "text": "We applied the proposed algorithm to separate a speech signal from a background piano music signal. Our main goal was to get a clean speech\nsignal from a mixture of speech and piano signals. We simulated our algorithm on a collection of speech and piano data at 16kHz sampling rate. For speech data, we used the training and testing male speech data from the TIMIT database. For music data, we downloaded piano music data from the piano society web site (URL, 2009a). We used 12 pieces with approximate 50 minutes total duration from different composers but from a single artist for training and left out one piece for testing. The PSD for the speech and music data were calculated by using the STFT: A Hamming window with 480 points length and 60% overlap was used and the FFT was taken at 512 points, the first 257 FFT points only were used since the conjugate of the remaining 255 points are involved in the first points. We trained 128 basis vectors for each source, which makes the size of Bspeech and Bmusic matrices to be 257 \u00d7 128, hence, the vector dimension d = 128 in equation (13) for both sources. The mixed data was formed by adding random portions of the test music file to 20 speech files from the test data of the TIMIT database at different speech-to-music ratio (SMR) values in dB. The audio power levels of each file were found using the \u201caudio voltmeter\u201d program from the G.191 ITU-T STL software suite (URL, 2009b). For each SMR value, we obtained 20 mixed utterances this way. We used the first 10 utterances as a validation set to choose reasonable values for the regularization parameters \u03b1speech and \u03b1music and the number of Gaussian mixture components K. The other 10 mixed utterances were used for testing.\nPerformance measurement of the separation algorithm was done using the signal to noise ratio (SNR). The average SNR over the 10 test utterances for each SMR case are reported. We also used signal to interference ratio\n(SIR), which is defined as the ratio of the target energy to the interference error due to the music signal only (Vincent et al., 2006).\nTable 1 shows SNR and SIR of the separated speech signal using NMF with different values of the number of Gaussian mixture components K and fixed regularization parameters \u03b1speech = \u03b1music = 1. The first column of the Table, shows the separation results of using just NMF without any prior.\nAs we can see from the Table, the proposed regularized NMF algorithm improves the separation performance for challenging SMR cases compared with using just NMF without priors. Increasing the number of Gaussian mixture components K improves the separation performance until K = 16. From the shown results, K = 16 seems to be a good choice for the given data sets. The best choice for K usually depends on the nature and the size of the training data. For example, for speech signal in general there are variety of phonetic differences, gender, speaking styles, accents, which raises the necessity for using many Gaussian components.\nComparison with other priors\nIn this section we compared our proposed method of using MMSE estimates under GMM prior on the solution of NMF with two other prior methods. The first prior is the sparsity prior and the second prior is enforced by maximizing the loglikelihood under GMM prior distribution.\nIn the sparsity prior, the NMF solution of the gains matrix was enforced to be sparse (Virtanen and Cemgil, 2009; Schmidt and Olsson, 2006). The sparse NMF is defined as\nC (G) = DIS (Y ||BG) + \u03bb \u2211 m,n Gm,n, (43)\nwhere \u03bb is the regularization parameter. The gain update rule of G can be found as follows:\nG\u2190 G\u2297 BT Y (BG) 2\nBT 1BG + \u03bb . (44)\nEnforcing sparsity on the NMF solution of the gains matrix is equivalent to model the prior of the gains matrix using exponential distribution with parameter \u03bb (Virtanen and Cemgil, 2009). The update rule in equation (44) is found based on maximizing the likelihood of the gains matrix under the exponential prior distribution.\nThe second method of enforcing prior on the NMF solution is by using GMM gain prior (Grais and Erdogan, 2012a,b). The NMF solution for the gains matrix is enforced to increase its log-likelihood with the trained GMM prior as follows:\nC = DIS (Y ||BG)\u2212R2(G), (45)\nwhere R2(G) is the weighted sum of the log-likelihoods of the log-normalized columns of the gains matrix G. R2(G) can be written as follows:\nR2(G) = 2\u2211 i=1 \u03b7i\u0393(Gi), (46)\nwhere \u0393(Gi) is the log-likelihood for the submatrix Gi for source i.\nIn sparsity and GMM based log-likelihood prior methods, to match between the used update rule for the gains matrix during training and separation, the priors were enforced during both training and separation stages. In sparse NMF we used sparsity constraints during training and separation stages. In regularized NMF with GMM based log-likelihood prior we trained the NMF bases and the prior GMM parameters jointly as shown in Grais and Erdogan (2012b).\nIn the sparse NMF case, we got best results when \u03bb = 0.0001 for both sources in the training and separation stages. In the case of enforcing the gains matrix to increase the log-likelihood under GMM prior (Grais and Erdogan, 2012b) we got the best results when \u03b7 = 1 in the training and \u03b7 = 0.1 in the separation stage. The number of Gaussian components was K = 4 for both sources. It is important to note that, in the case of using MMSE under GMM prior there is no need to enforce prior during training since the uncertainty measurements during training are assumed to be zeros since the training data are clean signals. When the uncertainty is zero, then the regularized NMF in case of MMSE under GMM prior is the same as the NMF cost function, then the update rule for the gains matrix in the training stage is the same as the update rule in the case of using just NMF.\nFigures 2 and 3 show the SNR and SIR for the different type of prior\nmodels. The black line shows the separation performance in the case of no prior is used. The red line shows the performance for the case of using sparse NMF. The green line shows the performance in the case of enforcing the gains matrix to increase its likelihood with the prior GMM. The blue line shows the separation performance in the case of using MMSE estimate under GMM prior that is proposed in this paper. As we can see, the proposed method of enforcing prior on the gains matrix using MMSE estimate under GMM\nprior gives the best performance comparing with the other methods. The used MMSE estimates prior in this work gives better results than the GMM likelihood method (Grais and Erdogan, 2012b) because of the measurements of the uncertainties in the MMSE under GMM case. The uncertainties work as feedback measurements that adjust the needs to the prior based on the amount of distortion in the gains matrix during the separation stage.\nComparing the relative improvements in dB that we got in this paper with the achieved improvements in other works (Wilson et al., 2008b,a; Virtanen and Cemgil, 2009; Virtanen, 2007) we can see that the, improvements in this\npaper can be considered to be high."}, {"heading": "12. CONCLUSION", "text": "In this work, we introduced a new regularized NMF algorithm. The NMF solution for the gains matrix was guided by the MMSE estimate under a GMM prior where the uncertainty of the observed mixed signal was learned online from the observed data. The proposed algorithm can be extended for better measurements of the distortion in the observed signal by embedding more parameters in equation (14) that can be learned online from the observed signal."}, {"heading": "13. Acknowledgements", "text": "This research is partially supported by Turk Telekom Group Research and Development, project entitled \u201cSingle Channel Source Separation\u201d, grant number 3014-06, support year 2012."}, {"heading": "APPENDIX A", "text": "In this appendix, we show the MMSE estimate and the parameter \u03a8 learning similar to Rosti and Gales (2001), Ghahramani and Hinton (1997), and Rosti and Gales (2004). Assume we have a noisy observation y as shown in the graphical model in Figure 4, which can be formulated as follows:\ny = x+ e, (47)\nwhere e is the noise term, and x is the unknown underlying correct signal\nwhich needs to be estimated under a GMM prior distribution:\np (x) = K\u2211 k=1 \u03c0kN (x|\u00b5k,\u03a3k) , (48)\nthe error term e has a Gaussian distribution with zero mean and diagonal covariance matrix \u03a8:\np (e) = N (e|0,\u03a8) . (49)\nThe conditional distribution of y is a Gaussian with mean x and diagonal covariance matrix \u03a8:\np(y|x, k) = N (y|x,\u03a8) . (50)\nThe distribution of y given the Gaussian component k is a Gaussian with mean \u00b5k and diagonal covariance matrix \u03a3k + \u03a8:\np(y|k) = N (y|\u00b5k,\u03a3k + \u03a8) . (51)\nThe marginal probability distribution of y is a GMM:\np(y) = K\u2211 k=1 \u03c0kN (y|\u00b5k,\u03a3k + \u03a8) , (52)\nwhere the expectations E (x) = E (y), and E (e) = 0. Note that, this observation model has some mathematical similarities but different concepts with factor analysis models assuming the load matrix is the identity matrix (Rosti and Gales, 2001; Ghahramani and Hinton, 1997; Rosti and Gales, 2004; Jordan and Bishop).\nThe MMSE estimate of x can be found by calculating the conditional expectation of x given the observation y. Given the Gaussian component k, the joint distribution of x and y is a multivariate Gaussian distribution with conditional expectation and conditional variance as follows (Rosti and Gales, 2001; Leon-Garcia, 1994):\nE (x|y, k) = \u00b5k + \u03a3kxy\u03a3 \u22121 ky (y \u2212 \u00b5k) , (53)\nvar (x|y, k) = \u03a3k \u2212\u03a3kxy\u03a3 \u22121 ky\u03a3 T kxy , (54)\nwe know that\n\u03a3ky = \u03a3k + \u03a8, (55)\nand\n\u03a3kxy = cov (x,y) = E ( xyT ) \u2212 E (x)E ( yT )\n= E [ x ( xT + eT )] \u2212 E (x)E ( yT )\n= E ( xxT ) + E (x)E ( eT ) \u2212 E (x)E ( yT )\n= var (x) + E (x)E ( xT ) \u2212 E (x)E ( yT )\n= var (x) = \u03a3k. (56)\nThe conditional expectation given the Gaussian component k of the prior model is\nE (x|y, k) = \u00b5k + \u03a3k (\u03a3k + \u03a8) \u22121 (y \u2212 \u00b5k)\n= x\u0302k. (57)\nWe also can find the following conditional expectation given only the observation y as follows:\nE (x|y) = K\u2211 k=1 E (k|y)E (x|y, k)\n= K\u2211 k=1 \u03b3kE (x|y, k) = x\u0302, (58)\nwhere\nE (k|y) = \u03c0kp (y|k)\u2211K j=1 \u03c0jp (y|j) = \u03b3k. (59)\nFrom equations (57, 58, 59) we can write the final MMSE estimate of x\ngiven the model parameters as follows:\nx\u0302 = K\u2211 k=1 \u03b3k [ \u00b5k + \u03a3k (\u03a3k + \u03a8) \u22121 (y \u2212 \u00b5k) ] . (60)\nWe need also to find the following sufficient statistics to be used in esti-\nmating the model parameters:\nvar (x|y, k) = \u03a3k \u2212\u03a3k (\u03a3k + \u03a8)\u22121 \u03a3Tk , (61)\nE ( xxT |y, k ) = var (x|y, k) + E (x|y, k)E (x|y, k)T\n= \u03a3k \u2212\u03a3k (\u03a3k + \u03a8)\u22121 \u03a3Tk + x\u0302kx\u0302 T k = R\u0302k, (62)\nand\nE ( xxT |y ) = K\u2211 k=1 E (k|y)E ( xxT |y, k ) =\nK\u2211 k=1 \u03b3kE ( xxT |y, k ) =\nK\u2211 k=1 \u03b3kR\u0302k\n= R\u0302. (63)\nParameters learning using the EM algorithm\nIn the training stage, we assume we have clean data with e = 0. The prior GMM parameters \u03c0,\u00b5,\u03a3 are learned as regular GMM models. The only parameter that need to be estimated is \u03a8, which is learned from the deformed signal \u201cqn\u201d in the paper. The parameter \u03a8 is learned iteratively using maximum likelihood estimation. Given the data points q = q1, q2, .., qn, ...., qN , and the GMM parameters, we need to find an estimate for \u03a8. We follow the same procedures as in Rosti and Gales (2001), Ghahramani and Hinton (1997), and Rosti and Gales (2004).\nLets rewrite the sufficient statistics in equations (59, 57, 60, 62, 63) after replacing x with z (to avoid confusion between calculating MMSE and training the model parameters) as follows:\n\u03b3kn = \u03c0kN (qn|\u00b5k,\u03a3k + \u03a8)\u2211K j=1 \u03c0jN ( qn|\u00b5j,\u03a3j + \u03a8 ) , (64) z\u0302kn = E (z|qn, k) = \u00b5k + \u03a3k (\u03a3k + \u03a8) \u22121 (qn \u2212 \u00b5k) , (65)\nz\u0302n = E (z|qn) = K\u2211 k=1 \u03b3knz\u0302kn, (66)\nR\u0302kn = E ( zzT |qn, k ) = \u03a3k \u2212\u03a3k (\u03a3k + \u03a8)\u22121 \u03a3Tk + z\u0302knz\u0302 T kn, (67)\nand\nR\u0302n = E ( zzT |qn ) = K\u2211 k=1 \u03b3knR\u0302kn. (68)\nThe complete log-likelihood can be written in a product form as follows:\nl (q, z, k|\u00b5,\u03a3, \u03c0,\u03a8) = log N\u220f n=1 K\u220f k=1 p(k)p(z|k)p(qn|z, k),\n= log N\u220f n=1 K\u220f k=1 [\u03c0kN (z|\u00b5k,\u03a3k) N (qn|z,\u03a8)] k , (69)\nl (q, z, k|\u00b5,\u03a3, \u03c0,\u03a8) = N\u2211 n=1 K\u2211 k=1 k log \u03c0k+ N\u2211 n=1 K\u2211 k=1 k log N (z|\u00b5k,\u03a3k)+ N\u2211 n=1 K\u2211 k=1 k log N (qn|z,\u03a8) .\n(70)\nThe conditional expectation of the complete log likelihood, which is con-\nditioning on the observed data qn can be written as:\nQ = N\u2211 n=1 K\u2211 k=1 Eqn (k|qn) log \u03c0k + N\u2211 n=1 K\u2211 k=1 Eqn (k|qn)Eqn (log N (z|\u00b5k,\u03a3k) |qn)\n+ N\u2211 n=1 K\u2211 k=1 Eqn (k|qn)Eqn (log N (qn|z,\u03a8) |qn) , (71)\ngiven that\nEqn (k|qn) = \u03c0kN (qn|\u00b5k,\u03a3k + \u03a8)\u2211K j=1 \u03c0jN ( qn|\u00b5j,\u03a3j + \u03a8 ) = \u03b3kn. (72)\nWe can write the complete log-likelihood as follows:\nQ = N\u2211 n=1 K\u2211 k=1 \u03b3kn log \u03c0k + N\u2211 n=1 K\u2211 k=1 \u03b3knEqn (log N (z|\u00b5k,\u03a3k) |qn)\n+ N\u2211 n=1 K\u2211 k=1 \u03b3knEqn (log N (qn|z,\u03a8) |qn) . (73)\nFor the parameter \u03a8, we need to maximize the third part of equation\n(73) with respect to \u03a8: Qqn = N\u2211 n=1 K\u2211 k=1 \u03b3knEqn (log N (qn|z,\u03a8) |qn)\n= N\u2211 n=1 K\u2211 k=1 \u03b3knEqn\n( log\n1\n(2\u03c0) d 2 |\u03a8| 1 2\nexp { \u22121 2 (qn \u2212 z) T \u03a8\u22121 (qn \u2212 z) } |qn, k ) ,\n= N\u2211 n=1 K\u2211 k=1 \u03b3knEqn ( \u2212d 2 log (2\u03c0)\u2212 1 2 log |\u03a8| \u2212 1 2 (qn \u2212 z) T \u03a8\u22121 (qn \u2212 z) |qn, k ) ,\n(74)\nthe derivative of Qqn with respect to \u03a8 \u22121 is set to zero:\n\u2202Qqn \u2202\u03a8\u22121 = N\u2211 n=1 K\u2211 k=1 \u03b3knEqn ( 1 2 \u03a8\u2212 1 2 (qn \u2212 z) (qn \u2212 z) T |qn, k ) = 0, (75)\n\u03a8 N\u2211 n=1 K\u2211 k=1 \u03b3kn = N\u2211 n=1 K\u2211 k=1 \u03b3knqnq T n \u2212 N\u2211 n=1 qn K\u2211 k=1 \u03b3knEqn (z|qn, k) T\n\u2212 ( N\u2211 n=1 qn K\u2211 k=1 \u03b3knEqn (z|qn, k) T )T + N\u2211 n=1 K\u2211 k=1 \u03b3knEqn ( zzT |qn, k ) ,\n(76)\nwe know that N\u2211 n=1 K\u2211 k=1 \u03b3kn = N and K\u2211 k=1 \u03b3kn = 1,\nthen N\u2211 n=1 qnq T n K\u2211 k=1 \u03b3kn = N\u2211 n=1 qnq T n , and\n\u03a8 N\u2211 n=1 K\u2211 k=1 \u03b3kn = N\u03a8.\nWe can use the values of \u2211K k=1 \u03b3knEqn (z|qn, k) and \u2211K k=1 \u03b3knEqn ( zzT |qn, k ) from equations (66, 68) to find the estimate of \u03a8 as follows:\n\u03a8\u0302 = diag\n{ 1\nN N\u2211 n=1 ( qnq T n \u2212 qnz\u0302Tn \u2212 z\u0302nqTn + R\u0302n )} , (77)\nwhere the \u201cdiag\u201d operator sets all the off-diagonal elements of a matrix to zero."}, {"heading": "APPENDIX B", "text": "In this appendix, we show the gradients of the penalty term in the regularized NMF cost function in section 2.1. To calculate the update rule for the gains matrix G, the gradients \u2207+GL(G) and \u2207 \u2212 GL(G) are needed to be calculated. Lets recall the regularized NMF cost function\nC (G) = DIS (V ||BG) + \u03b1L(G), (78)\nwhere\nL(G) = N\u2211 n \u2225\u2225\u2225\u2225 gn\u2016gn\u20162 \u2212 exp (f (gn)) \u2225\u2225\u2225\u22252 2 , (79)\nf (gn) = K\u2211 k=1 \u03b3kn [ \u00b5k + \u03a3k (\u03a3k + \u03a8) \u22121 ( log gn \u2016gn\u20162 \u2212 \u00b5k )] , (80)\nand\n\u03b3kn =  \u03c0kN ( log gn \u2016gn\u20162 |\u00b5k,\u03a3k + \u03a8 )\n\u2211K j=1 \u03c0jN ( log\ngn \u2016gn\u20162\n|\u00b5j,\u03a3j + \u03a8 )  . (81)\nSince the training data for the GMM models are the logarithm of the normalized vectors, then the mean vectors of the GMM are always not positive, also the values of log gn \u2016gn\u20162 are also not positive, and gn is always nonnegative.\nLet gn = x, and its component a is gna = xa, and f(gn) = f(x). We can\nwrite the constraint in equation (79) as:\nL(x) = \u2225\u2225\u2225\u2225 x\u2016x\u20162 \u2212 exp (f(x)) \u2225\u2225\u2225\u22252 2 . (82)\nThe a component of the gradient of L(x) is\n\u2202L(x)\n\u2202xa = 2 ( xa \u2016x\u20162 \u2212 exp (f(xa)) )( 1 \u2016x\u20162 \u2212 x 2 a \u2016x\u201632 \u2212\u2207f(xa) exp (f(xa)) ) = \u2207L(xa), (83)\nwhich can be written as a difference of two positive terms\n\u2207L(xa) = \u2207+L(xa)\u2212\u2207\u2212L(xa). (84)\nThe component a of the gradient of f (x) can be written as a difference of two positive terms:\n\u2202f (x)\n\u2202xa = \u2207+f (xa)\u2212\u2207\u2212f (xa) . (85)\nThe component a of the gradient of L (x) in equation (84) can be written as:\n\u2207+L(xa) = 2 { xa \u2016x\u20162 ( 1 \u2016x\u20162 + exp (f(xa))\u2207\u2212f(xa) ) + exp (f(xa)) ( x2a \u2016x\u201632 + exp (f(xa))\u2207+f(xa) )} ,\n(86)\nand\n\u2207\u2212L(xa) = 2 { xa \u2016x\u20162 ( x2a \u2016x\u201632 + exp (f(xa))\u2207+f(xa) ) + exp (f(xa)) ( 1 \u2016x\u20162 + exp (f(xa))\u2207\u2212f(xa) )} .\n(87)\nWe need to find the values of \u2207+f(xa) and \u2207\u2212f(xa). Note that, the term\n\u03a3k (\u03a3k + \u03a8) \u22121 forms a diagonal matrix.\nLet\nH(xa) = \u00b5ka + \u03a3kaa (\u03a3kaa + \u03a8aa) \u22121 (\nlog xa \u2016x\u20162 \u2212 \u00b5ka\n) , (88)\nthen f(x) in equation (80) can be written as:\nf(x) = K\u2211 k=1 \u03b3k(x)H(x). (89)\nThe gradient of f(x) in equation (89) can be written as:\n\u2207f(xa) = K\u2211 k=1 [\u03b3k(x)\u2207H(xa) +H(xa)\u2207\u03b3k(xa)] , (90)\nwhere\n\u03b3k(x) =\n \u03c0kN ( log x\u2016x\u20162 |\u00b5k,\u03a3k + \u03a8 ) \u2211K\nj=1 \u03c0jN ( log x\u2016x\u20162 |\u00b5j,\u03a3j + \u03a8\n)  = Mk(x)\nNk(x) . (91)\nWe can also write the gradient components of H(xa) and \u03b3k(x) as a difference of two positive terms\n\u2207H(xa) = \u2207+H(xa)\u2212\u2207\u2212H(xa), (92)\nand\n\u2207\u03b3k(xa) = \u2207+\u03b3k(xa)\u2212\u2207\u2212\u03b3k(xa). (93)\nThe gradient of f(xa) in equations (85, 90) can be written as:\n\u2207+f(xa) = K\u2211 k=1 [ \u03b3k(x)\u2207+H(xa) +H+(xa)\u2207+\u03b3k(xa) +H\u2212(xa)\u2207\u2212\u03b3k(xa) ] ,\n(94)\n\u2207\u2212f(xa) = K\u2211 k=1 [ \u03b3k(x)\u2207\u2212H(xa) +H\u2212(xa)\u2207+\u03b3k(xa) +H+(xa)\u2207\u2212\u03b3k(xa) ] ,\n(95)\nwhere\n\u2207+H(xa) = \u03a3kaa (\u03a3kaa + \u03a8aa) \u22121 1\nxa , (96)\n\u2207\u2212H(xa) = \u03a3kaa (\u03a3kaa + \u03a8aa) \u22121 xa\n\u2016x\u201622 , (97)\nand H(xa) can be written as a difference of two positive terms:\nH(xa) = H +(xa)\u2212H\u2212(xa), (98)\nwhere\nH+(xa) = \u2212\u03a3kaa (\u03a3kaa + \u03a8aa) \u22121\u00b5ka , (99)\nand\nH\u2212(xa) = \u2212 [ \u00b5ka + \u03a3kaa (\u03a3kaa + \u03a8aa)\n\u22121 log xa \u2016x\u20162\n] . (100)\nWe can rewrite \u03b3k(x) in equation (91) as:\n\u03b3k(x) = Mk(x)\nNk(x) , (101)\nnote that \u03b3k(x),Mk(x), Nk(x) \u2265 0.\nThe component a of the gradient of \u03b3k(x) can be written as:\n\u2207\u03b3k(xa) = Nk(x)\u2207Mk(xa)\u2212Mk(x)\u2207Nk(xa)\nN2k (x) . (102)\nWe can write the gradients of Mk(x) and Nk(x) as a difference of two positive terms\n\u2207Mk(xa) = \u2207+Mk(xa)\u2212\u2207\u2212Mk(xa), (103)\nand\n\u2207Nk(xa) = K\u2211 k=1 \u2207+Mk(xa)\u2212 K\u2211 k=1 \u2207\u2212Mk(xa). (104)\nThe gradient of \u03b3k(xa) in equation (93) can be written as:\n\u2207+\u03b3k(xa) = Nk(x)\u2207M+k (xa) +Mk(x)\n\u2211K k=1\u2207\u2212Mk(xa)\nN2k (x) , (105)\n\u2207\u2212\u03b3k(xa) = Nk(x)\u2207M\u2212k (xa) +Mk(x)\n\u2211K k=1\u2207+Mk(xa)\nN2k (x) , (106)\nwhere\n\u2207+Mk(xa) = Mk(x) (\u03a3kaa + \u03a8aa) \u22121 [ \u22121 xa log xa \u2016x\u20162 \u2212 \u00b5kaxa \u2016x\u201622 ] , (107)\nand\n\u2207\u2212Mk(xa) = Mk(x) (\u03a3kaa + \u03a8aa) \u22121 [ \u2212\u00b5ka xa \u2212 xa \u2016x\u201622 log xa \u2016x\u20162 ] . (108)\nAfter finding\u2207+\u03b3k(xa), and\u2207\u2212\u03b3k(xa) from equations (105, 106), and\u2207+H(xa), and \u2207\u2212H(xa) from equations (96, 97), we can find the gradients \u2207+f(xa), and\u2207\u2212f(xa) in equations (94, 95), which complete our solution for\u2207+L(xa), and \u2207\u2212L(xa) in equations (86, 87)."}], "references": [{"title": "Fast Bayesian NMF algorithms enforcing harmonicity and temporal continuity in polyphonic music transcription, in: IEEE workshop on applications of signal processing to audio and acoustics", "author": ["N. Bertin", "R. Badeau", "E. Vincent"], "venue": null, "citeRegEx": "Bertin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bertin et al\\.", "year": 2009}, {"title": "Enforcing harmonicity and smoothness in bayesian nonnegative matrix factorization applied to polyphonic music transcription", "author": ["N. Bertin", "R. Badeau", "E. Vincent"], "venue": "IEEE Transactions,", "citeRegEx": "Bertin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bertin et al\\.", "year": 2010}, {"title": "GaP: a factor model for discrete data", "author": ["J. Canny"], "venue": "in: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "Canny,? \\Q2004\\E", "shortCiteRegEx": "Canny", "year": 2004}, {"title": "Conjugate Gamma Markov random fields for modelling nonstationary sources, in: International Conference on Independent Component Analysis and Signal Separation", "author": ["A.T. Cemgil", "O. Dikmen"], "venue": null, "citeRegEx": "Cemgil and Dikmen,? \\Q2007\\E", "shortCiteRegEx": "Cemgil and Dikmen", "year": 2007}, {"title": "Constrained non-negative matrix factorization method for EEG analysis in early detection of alzheimers disease", "author": ["Z. Chen", "A. Cichocki", "T.M. Rutkowski"], "venue": "in: IEEE International Conference on Acoustics,", "citeRegEx": "Chen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2006}, {"title": "New algorithms for nonnegative matrix factorization in applications to blind source separation", "author": ["A. Cichocki", "R. Zdunek", "S. Amari"], "venue": "in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)", "citeRegEx": "Cichocki et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2006}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Nonnegative matrix factorization with the itakura-saito divergence. With application to music analysis", "author": ["C. Fevotte", "N. Bertin", "J.L. Durrieu"], "venue": "Neural Computation", "citeRegEx": "Fevotte et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fevotte et al\\.", "year": 2009}, {"title": "The EM algorithm for mixtures of factor analyzers", "author": ["Z. Ghahramani", "G.E. Hinton"], "venue": "Technical Report. CRG-TR-96-1,", "citeRegEx": "Ghahramani and Hinton,? \\Q1997\\E", "shortCiteRegEx": "Ghahramani and Hinton", "year": 1997}, {"title": "Adaptation of speaker-specific bases in non-negative matrix factorization for single channel speech-music separation, in: Annual Conference of the International Speech Communication Association (INTERSPEECH)", "author": ["E.M. Grais", "H. Erdogan"], "venue": null, "citeRegEx": "Grais and Erdogan,? \\Q2011\\E", "shortCiteRegEx": "Grais and Erdogan", "year": 2011}, {"title": "Single channel speech music separation using nonnegative matrix factorization and spectral masks", "author": ["E.M. Grais", "H. Erdogan"], "venue": "in: International Conference on Digital Signal Processing", "citeRegEx": "Grais and Erdogan,? \\Q2011\\E", "shortCiteRegEx": "Grais and Erdogan", "year": 2011}, {"title": "2011c. Single channel speech music separation using nonnegative matrix factorization with sliding window and spectral masks, in: Annual Conference of the International Speech Communication Association (INTERSPEECH)", "author": ["E.M. Grais", "H. Erdogan"], "venue": null, "citeRegEx": "Grais and Erdogan,? \\Q2011\\E", "shortCiteRegEx": "Grais and Erdogan", "year": 2011}, {"title": "Gaussian mixture gain priors for regularized nonnegative matrix factorization in single-channel source separation, in: Annual Conference of the International Speech Communication Association (INTERSPEECH)", "author": ["E.M. Grais", "H. Erdogan"], "venue": null, "citeRegEx": "Grais and Erdogan,? \\Q2012\\E", "shortCiteRegEx": "Grais and Erdogan", "year": 2012}, {"title": "Regularized nonnegative matrix factorization using gaussian mixture priors for supervised single channel source separation. Computer Speech and Language http://dx.doi.org/10.1016/j.csl.2012.09.002", "author": ["E.M. Grais", "H. Erdogan"], "venue": null, "citeRegEx": "Grais and Erdogan,? \\Q2012\\E", "shortCiteRegEx": "Grais and Erdogan", "year": 2012}, {"title": "Spectro-temporal post-smoothing in NMF based single-channel source separation", "author": ["E.M. Grais", "H. Erdogan"], "venue": "in: European Signal Processing Conference (EUSIPCO)", "citeRegEx": "Grais and Erdogan,? \\Q2012\\E", "shortCiteRegEx": "Grais and Erdogan", "year": 2012}, {"title": "Audio-Visual speech recognition with background music using single-channel source separation", "author": ["E.M. Grais", "I.S. Topkaya", "H. Erdogan"], "venue": "in: IEEE Conference on Signal Processing and Communications Applications (SIU)", "citeRegEx": "Grais et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grais et al\\.", "year": 2012}, {"title": "Adaptation of source-specific dictionaries in non-negative matrix factorization for source separation", "author": ["X. Jaureguiberry", "P. Leveau", "S. Maller", "J.J. Burred"], "venue": "in: IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP)", "citeRegEx": "Jaureguiberry et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jaureguiberry et al\\.", "year": 2011}, {"title": "Nonparametric shape priors for active contour-based image segmentation", "author": ["J. Kim", "M. cetin", "A.S. Willsky"], "venue": "Signal Processing", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Lee and Seung,? \\Q2001\\E", "shortCiteRegEx": "Lee and Seung", "year": 2001}, {"title": "Probability and random processing for electrical engineering", "author": ["A. Leon-Garcia"], "venue": null, "citeRegEx": "Leon.Garcia,? \\Q1994\\E", "shortCiteRegEx": "Leon.Garcia", "year": 1994}, {"title": "Fundamentals of speech recognition", "author": ["L. Rabiner", "B.H. Juang"], "venue": null, "citeRegEx": "Rabiner and Juang,? \\Q1993\\E", "shortCiteRegEx": "Rabiner and Juang", "year": 1993}, {"title": "Generalised linear Gaussian models", "author": ["A.V. Rosti", "M. Gales"], "venue": "Technical Report", "citeRegEx": "Rosti and Gales,? \\Q2001\\E", "shortCiteRegEx": "Rosti and Gales", "year": 2001}, {"title": "Factor analysed hidden markov models for speech recognition", "author": ["A.V.I. Rosti", "M.J.F. Gales"], "venue": "Computer Speech and Language, Issue", "citeRegEx": "Rosti and Gales,? \\Q2004\\E", "shortCiteRegEx": "Rosti and Gales", "year": 2004}, {"title": "Single-channel speech separation using sparse non-negative matrix factorization", "author": ["M.N. Schmidt", "R.K. Olsson"], "venue": "in: International Conference on Spoken Language Processing (INTERSPEECH)", "citeRegEx": "Schmidt and Olsson,? \\Q2006\\E", "shortCiteRegEx": "Schmidt and Olsson", "year": 2006}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. Fevotte"], "venue": "IEEE Transactions, Audio, speech, and language processing", "citeRegEx": "Vincent et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2006}, {"title": "Monaural sound source separation by non-negative matrix factorization with temporal continuity and sparseness criteria", "author": ["T. Virtanen"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "Virtanen,? \\Q2007\\E", "shortCiteRegEx": "Virtanen", "year": 2007}, {"title": "Mixtures of gamma priors for non-negative matrix factorization based speech separation, in: International Conference on Independent Component Analysis and Blind Signal Separation", "author": ["T. Virtanen", "A.T. Cemgil"], "venue": null, "citeRegEx": "Virtanen and Cemgil,? \\Q2009\\E", "shortCiteRegEx": "Virtanen and Cemgil", "year": 2009}, {"title": "Bayesian extensions to nonnegative matrix factorization for audio signal modeling, in: IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP)", "author": ["T. Virtanen", "A.T. Cemgil", "S. Godsill"], "venue": null, "citeRegEx": "Virtanen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Virtanen et al\\.", "year": 2008}, {"title": "Using posterior word probabilities for improved speech recognition, in: IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP)", "author": ["F. Wessel", "R. Schluter", "H. Ney"], "venue": null, "citeRegEx": "Wessel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wessel et al\\.", "year": 2000}, {"title": "Regularized non-negative matrix factorization with temporal dependencies for speech denoising", "author": ["K.W. Wilson", "B. Raj", "P. Smaragdis"], "venue": null, "citeRegEx": "Wilson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2008}, {"title": "The MMSE estimate of x can be found by calculating the conditional expectation of x given the observation y. Given the Gaussian component k, the joint distribution of x and y is a multivariate Gaussian distribution with conditional expectation and conditional variance as follows (Rosti and", "author": ["Jordan", "Bishop"], "venue": null, "citeRegEx": "Jordan and Bishop..,? \\Q2004\\E", "shortCiteRegEx": "Jordan and Bishop..", "year": 2004}, {"title": "E (x|y, k) = \u03bck + \u03a3kxy\u03a3", "author": ["Gales"], "venue": null, "citeRegEx": "Gales,? \\Q1994\\E", "shortCiteRegEx": "Gales", "year": 1994}], "referenceMentions": [{"referenceID": 18, "context": "Introduction Nonnegative matrix factorization (Lee and Seung, 2001) is an important tool for source separation applications, especially when only one observation of the mixed signal is available.", "startOffset": 46, "endOffset": 67}, {"referenceID": 25, "context": "For audio source separation applications, the continuity and sparsity priors were enforced in the NMF decomposition weights (Virtanen, 2007).", "startOffset": 124, "endOffset": 140}, {"referenceID": 0, "context": "In Bertin et al. (2009), and Bertin et al.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "In Bertin et al. (2009), and Bertin et al. (2010), smoothness and harmonicity priors were enforced on the NMF solution in Bayesian framework and applied to music transcription.", "startOffset": 3, "endOffset": 50}, {"referenceID": 0, "context": "In Bertin et al. (2009), and Bertin et al. (2010), smoothness and harmonicity priors were enforced on the NMF solution in Bayesian framework and applied to music transcription. In Wilson et al. (2008b), and Wilson et al.", "startOffset": 3, "endOffset": 202}, {"referenceID": 0, "context": "In Bertin et al. (2009), and Bertin et al. (2010), smoothness and harmonicity priors were enforced on the NMF solution in Bayesian framework and applied to music transcription. In Wilson et al. (2008b), and Wilson et al. (2008a) the regularized NMF was used to increase the NMF decomposition", "startOffset": 3, "endOffset": 229}, {"referenceID": 3, "context": "The Gamma distribution and the Gamma Markov chain (Cemgil and Dikmen, 2007) were used as priors for the basis and weights/gains matrices respectively in Virtanen et al.", "startOffset": 50, "endOffset": 75}, {"referenceID": 23, "context": "In supervised single channel source separation (SCSS), NMF is used in two main stages, the training stage and the separation stage (Schmidt and Olsson, 2006; Grais and Erdogan, 2011a,b,c; Grais et al., 2012; Grais and Erdogan, 2012c).", "startOffset": 131, "endOffset": 233}, {"referenceID": 15, "context": "In supervised single channel source separation (SCSS), NMF is used in two main stages, the training stage and the separation stage (Schmidt and Olsson, 2006; Grais and Erdogan, 2011a,b,c; Grais et al., 2012; Grais and Erdogan, 2012c).", "startOffset": 131, "endOffset": 233}, {"referenceID": 4, "context": "In Fevotte et al. (2009), Markov chain prior model for smoothness was used within a Bayesian framework in regularized NMF with Itakura-Saito (IS-NMF) divergence.", "startOffset": 3, "endOffset": 25}, {"referenceID": 4, "context": "In Fevotte et al. (2009), Markov chain prior model for smoothness was used within a Bayesian framework in regularized NMF with Itakura-Saito (IS-NMF) divergence. In Virtanen et al. (2008), the conjugate prior distributions on the NMF weights and basis matrices solutions with the Poisson observation model within Bayesian framework was introduced.", "startOffset": 3, "endOffset": 188}, {"referenceID": 3, "context": "The Gamma distribution and the Gamma Markov chain (Cemgil and Dikmen, 2007) were used as priors for the basis and weights/gains matrices respectively in Virtanen et al. (2008). A mixture of Gamma prior model was used as a prior for the basis matrix in Virtanen and Cemgil (2009).", "startOffset": 51, "endOffset": 176}, {"referenceID": 3, "context": "The Gamma distribution and the Gamma Markov chain (Cemgil and Dikmen, 2007) were used as priors for the basis and weights/gains matrices respectively in Virtanen et al. (2008). A mixture of Gamma prior model was used as a prior for the basis matrix in Virtanen and Cemgil (2009). The regularized NMF with smoothness and spatial decorrelation constraints was used in Chen et al.", "startOffset": 51, "endOffset": 279}, {"referenceID": 3, "context": "The Gamma distribution and the Gamma Markov chain (Cemgil and Dikmen, 2007) were used as priors for the basis and weights/gains matrices respectively in Virtanen et al. (2008). A mixture of Gamma prior model was used as a prior for the basis matrix in Virtanen and Cemgil (2009). The regularized NMF with smoothness and spatial decorrelation constraints was used in Chen et al. (2006) for EEG applications.", "startOffset": 51, "endOffset": 385}, {"referenceID": 3, "context": "The Gamma distribution and the Gamma Markov chain (Cemgil and Dikmen, 2007) were used as priors for the basis and weights/gains matrices respectively in Virtanen et al. (2008). A mixture of Gamma prior model was used as a prior for the basis matrix in Virtanen and Cemgil (2009). The regularized NMF with smoothness and spatial decorrelation constraints was used in Chen et al. (2006) for EEG applications. In Cichocki et al. (2006), and Chen et al.", "startOffset": 51, "endOffset": 433}, {"referenceID": 3, "context": "The Gamma distribution and the Gamma Markov chain (Cemgil and Dikmen, 2007) were used as priors for the basis and weights/gains matrices respectively in Virtanen et al. (2008). A mixture of Gamma prior model was used as a prior for the basis matrix in Virtanen and Cemgil (2009). The regularized NMF with smoothness and spatial decorrelation constraints was used in Chen et al. (2006) for EEG applications. In Cichocki et al. (2006), and Chen et al. (2006), a variety of constrained NMF algorithms were used for different applications.", "startOffset": 51, "endOffset": 457}, {"referenceID": 20, "context": "GMMs are used to model the multi-modal nature in speech feature vectors due to phonetic differences, speaking styles, gender, accents (Rabiner and Juang, 1993).", "startOffset": 134, "endOffset": 159}, {"referenceID": 9, "context": "In Grais and Erdogan (2012b), the prior GMM that models the valid weight combinations for each source is used to guide the NMF solution for the gains matrix during the separation stage.", "startOffset": 3, "endOffset": 29}, {"referenceID": 9, "context": "In Grais and Erdogan (2012b), the prior GMM that models the valid weight combinations for each source is used to guide the NMF solution for the gains matrix during the separation stage. The priors in Grais and Erdogan (2012b) are enforced by maximizing the log-likelihood of the NMF solution with the trained prior GMMs.", "startOffset": 3, "endOffset": 226}, {"referenceID": 9, "context": "In Grais and Erdogan (2012b), the prior GMM that models the valid weight combinations for each source is used to guide the NMF solution for the gains matrix during the separation stage. The priors in Grais and Erdogan (2012b) are enforced by maximizing the log-likelihood of the NMF solution with the trained prior GMMs. The priors in Grais and Erdogan (2012b) are enforced without evaluating how good the NMF solution is without using the", "startOffset": 3, "endOffset": 361}, {"referenceID": 2, "context": "This is the main advantage of the proposed regularized NMF compared to the regularization using the loglikelihood of the GMM prior or other prior distributions (Grais and Erdogan, 2012a,b; Canny, 2004).", "startOffset": 160, "endOffset": 201}, {"referenceID": 7, "context": "For audio source separation applications, the Itakura-Saito (IS-NMF) divergence cost function (Fevotte et al., 2009) is usually used.", "startOffset": 94, "endOffset": 116}, {"referenceID": 7, "context": "This cost function is found to be a good measurement for the perceptual differences between different audio signals (Fevotte et al., 2009; Jaureguiberry et al., 2011).", "startOffset": 116, "endOffset": 166}, {"referenceID": 16, "context": "This cost function is found to be a good measurement for the perceptual differences between different audio signals (Fevotte et al., 2009; Jaureguiberry et al., 2011).", "startOffset": 116, "endOffset": 166}, {"referenceID": 7, "context": "The IS-NMF solutions for equation (2) can be computed by alternating multiplicative updates of B and G (Fevotte et al., 2009; Jaureguiberry et al., 2011) as:", "startOffset": 103, "endOffset": 153}, {"referenceID": 16, "context": "The IS-NMF solutions for equation (2) can be computed by alternating multiplicative updates of B and G (Fevotte et al., 2009; Jaureguiberry et al., 2011) as:", "startOffset": 103, "endOffset": 153}, {"referenceID": 7, "context": "used with matrices of power spectral densities of the source signals (Fevotte et al., 2009; Jaureguiberry et al., 2011).", "startOffset": 69, "endOffset": 119}, {"referenceID": 16, "context": "used with matrices of power spectral densities of the source signals (Fevotte et al., 2009; Jaureguiberry et al., 2011).", "startOffset": 69, "endOffset": 119}, {"referenceID": 15, "context": "In conventional single channel source separation using NMF without regularization (Grais et al., 2012), there are two main stages to find estimates for S1 and S2 in equation (7).", "startOffset": 82, "endOffset": 102}, {"referenceID": 9, "context": "The regularization parameter in Grais and Erdogan (2012b) was playing two important roles.", "startOffset": 32, "endOffset": 58}, {"referenceID": 9, "context": "The regularization parameter in Grais and Erdogan (2012b) was playing two important roles. The first role was to match the scale of the IS-NMF divergence term with the scale of the log-likelihood prior term. The second role was to decide how much the regularized NMF cost function needs to rely on the prior term. The results in Grais and Erdogan (2012b) show that, when the source i has higher energy level than 12", "startOffset": 32, "endOffset": 355}, {"referenceID": 27, "context": "In the cases when the conjugate prior models of the NMF solutions were used (Virtanen et al., 2008; Canny, 2004), the hyperparameters of the prior models were also chosen manually.", "startOffset": 76, "endOffset": 112}, {"referenceID": 2, "context": "In the cases when the conjugate prior models of the NMF solutions were used (Virtanen et al., 2008; Canny, 2004), the hyperparameters of the prior models were also chosen manually.", "startOffset": 76, "endOffset": 112}, {"referenceID": 7, "context": "The conjugate prior models usually enforced on NMF solutions using a Bayesian framework (Fevotte et al., 2009; Virtanen et al., 2008; Canny, 2004).", "startOffset": 88, "endOffset": 146}, {"referenceID": 27, "context": "The conjugate prior models usually enforced on NMF solutions using a Bayesian framework (Fevotte et al., 2009; Virtanen et al., 2008; Canny, 2004).", "startOffset": 88, "endOffset": 146}, {"referenceID": 2, "context": "The conjugate prior models usually enforced on NMF solutions using a Bayesian framework (Fevotte et al., 2009; Virtanen et al., 2008; Canny, 2004).", "startOffset": 88, "endOffset": 146}, {"referenceID": 7, "context": "The values of the regularization parameters in Grais and Erdogan (2012b) was chosen manually for every energy level for each source.", "startOffset": 47, "endOffset": 73}, {"referenceID": 2, "context": ", 2008; Canny, 2004), the hyperparameters of the prior models were also chosen manually. The conjugate prior models usually enforced on NMF solutions using a Bayesian framework (Fevotte et al., 2009; Virtanen et al., 2008; Canny, 2004). In Grais and Erdogan (2012b), it was also shown that, the hyper-parameter choices for the conjugate prior models can also depend on the energy level differences of the source signals in the mixed signal.", "startOffset": 8, "endOffset": 266}, {"referenceID": 9, "context": "Motivation for the proposed regularized NMF In this work, we try to use prior GMMs to guide the solution of the gains matrix during the separation stage using regularized NMF as in Grais and Erdogan (2012b) but following a totally different regularization strategy.", "startOffset": 181, "endOffset": 207}, {"referenceID": 9, "context": "Motivation for the proposed regularized NMF In this work, we try to use prior GMMs to guide the solution of the gains matrix during the separation stage using regularized NMF as in Grais and Erdogan (2012b) but following a totally different regularization strategy. We also try to find a way to estimate how much the solution of the regularized NMF needs to rely on the prior GMMs automatically not manually as in Grais and Erdogan (2012b). The way of finding how much the regularized NMF solution of the gains matrix needs to rely on the prior GMM is by measuring how far the statistics of the solution of the gains matrix Gi in (9) is from the statistics of the solution of the valid gains matrix solution", "startOffset": 181, "endOffset": 440}, {"referenceID": 9, "context": "The matrix G i in (8) is used to train a prior GMM for the expected (valid) weight combinations that can exist in the gains matrix for source i as in Grais and Erdogan (2012b). The solution of the gains submatrix Gi in (9) can be seen as a deformed observation that needs to be restored using MMSE estimate under its corresponding GMM prior for source i.", "startOffset": 150, "endOffset": 176}, {"referenceID": 6, "context": "In training GMM, the expectation maximization (EM) algorithm (Dempster et al., 1977) is used to learn the GMM parameters (\u03c0k,\u03bck,\u03a3k, \u2200k = {1, 2, .", "startOffset": 61, "endOffset": 84}, {"referenceID": 28, "context": "We use the logarithm because it has been shown that the logarithm of a variable taking values between 0 and 1 can be modeled well by a GMM (Wessel et al., 2000).", "startOffset": 139, "endOffset": 160}, {"referenceID": 17, "context": "Even, Parzen density priors (Kim et al., 2007) can be seen under the same framework.", "startOffset": 28, "endOffset": 46}, {"referenceID": 9, "context": "In Grais and Erdogan (2012b), a GMM was used as the prior model for the gains matrix, and the solution of the gains matrix was encouraged to increase its log-likelihood with the prior model using this regularized NMF cost function.", "startOffset": 3, "endOffset": 29}, {"referenceID": 9, "context": "In Grais and Erdogan (2012b), a GMM was used as the prior model for the gains matrix, and the solution of the gains matrix was encouraged to increase its log-likelihood with the prior model using this regularized NMF cost function. The regularization parameters in Grais and Erdogan (2012b) were the only tools to control how much the regularized NMF relies on the prior models based on the energy differences of the sources in the mixed signal.", "startOffset": 3, "endOffset": 291}, {"referenceID": 9, "context": "By deciding automatically how much the regularized NMF needs to rely on the prior we conjecture that, we do not need to manually change the values for the regularization parameter based on the energy differences of the sources in the mixed signal 1 to improve the performance of NMF as in Grais and Erdogan (2012b). We use the following way of measuring how far the conventional NMF solution is from the prior templates: We can see the solution of the conventional NMF as distorted observations of a true/valid template.", "startOffset": 289, "endOffset": 315}, {"referenceID": 8, "context": "Given the prior GMM parameters which are considered fixed here, the update of \u03a8 is found based on the sufficient statistics \u1e91n and R\u0302n as follows (Rosti and Gales, 2001, 2004; Ghahramani and Hinton, 1997) [Appendix A]:", "startOffset": 146, "endOffset": 204}, {"referenceID": 8, "context": "Given the GMM prior parameters and the uncertainty measurement \u03a8, the MMSE estimate of each pattern xn given its observation qn under the observation model in equation (14) can be found similar to Rosti and Gales (2001, 2004), and Ghahramani and Hinton (1997) as in Appendix A as follows: f (qn) = K \u2211", "startOffset": 231, "endOffset": 260}, {"referenceID": 17, "context": "The multiplicative update rule for G can be found by following the same procedures as in Virtanen (2007); Bertin et al.", "startOffset": 89, "endOffset": 105}, {"referenceID": 0, "context": "The multiplicative update rule for G can be found by following the same procedures as in Virtanen (2007); Bertin et al. (2010); Grais and Erdogan", "startOffset": 106, "endOffset": 127}, {"referenceID": 25, "context": "The cost function is shown to be nonincreasing under the update rule (Virtanen, 2007; Bertin et al., 2010): G\u2190 G\u2297 \u2207 \u2212 GC \u2207+GC , (29)", "startOffset": 69, "endOffset": 106}, {"referenceID": 1, "context": "The cost function is shown to be nonincreasing under the update rule (Virtanen, 2007; Bertin et al., 2010): G\u2190 G\u2297 \u2207 \u2212 GC \u2207+GC , (29)", "startOffset": 69, "endOffset": 106}, {"referenceID": 9, "context": "There is no need to change the values of the regularization parameters according to the energy differences of the source signals in the mixed signal as in Grais and Erdogan (2012b). Reasonable values for the regularization parameters are chosen manually and kept fixed in this work.", "startOffset": 155, "endOffset": 181}, {"referenceID": 9, "context": "There is no need to change the values of the regularization parameters according to the energy differences of the source signals in the mixed signal as in Grais and Erdogan (2012b). Reasonable values for the regularization parameters are chosen manually and kept fixed in this work. Another main difference between the regularized NMF in Grais and Erdogan (2012b) that is shown in equation (11) and the proposed regularized NMF in this paper is related to the training procedures for the source models.", "startOffset": 155, "endOffset": 364}, {"referenceID": 9, "context": "There is no need to change the values of the regularization parameters according to the energy differences of the source signals in the mixed signal as in Grais and Erdogan (2012b). Reasonable values for the regularization parameters are chosen manually and kept fixed in this work. Another main difference between the regularized NMF in Grais and Erdogan (2012b) that is shown in equation (11) and the proposed regularized NMF in this paper is related to the training procedures for the source models. In both works, the main aim of the training stage is to train the basis matrices and the gains prior GMMs for the source signals. In Grais and Erdogan (2012b), to match between the way the trained models were used during training with the way they were used during separation, the basis matrices and the prior GMM parameters were learned jointly using the regularized NMF cost function in (11).", "startOffset": 155, "endOffset": 662}, {"referenceID": 9, "context": "There is no need to change the values of the regularization parameters according to the energy differences of the source signals in the mixed signal as in Grais and Erdogan (2012b). Reasonable values for the regularization parameters are chosen manually and kept fixed in this work. Another main difference between the regularized NMF in Grais and Erdogan (2012b) that is shown in equation (11) and the proposed regularized NMF in this paper is related to the training procedures for the source models. In both works, the main aim of the training stage is to train the basis matrices and the gains prior GMMs for the source signals. In Grais and Erdogan (2012b), to match between the way the trained models were used during training with the way they were used during separation, the basis matrices and the prior GMM parameters were learned jointly using the regularized NMF cost function in (11). The joint training for the source models was introduced in Grais and Erdogan (2012b) to improve the separation performance.", "startOffset": 155, "endOffset": 983}, {"referenceID": 9, "context": "Since, we needed to update (retrain) the GMM parameters at each NMF iteration, joint training slowed down the training of the source models in Grais and Erdogan (2012b). Another problem of using joint training is that, we had other regularization parameters during the training stage that needed to be chosen.", "startOffset": 143, "endOffset": 169}, {"referenceID": 9, "context": "Since, we needed to update (retrain) the GMM parameters at each NMF iteration, joint training slowed down the training of the source models in Grais and Erdogan (2012b). Another problem of using joint training is that, we had other regularization parameters during the training stage that needed to be chosen. Using joint training duplicates the number of the regularization parameters that need to be chosen. Choosing the regularization parameters in Grais and Erdogan (2012b) was done using validation data.", "startOffset": 143, "endOffset": 478}, {"referenceID": 9, "context": "Since, we needed to update (retrain) the GMM parameters at each NMF iteration, joint training slowed down the training of the source models in Grais and Erdogan (2012b). Another problem of using joint training is that, we had other regularization parameters during the training stage that needed to be chosen. Using joint training duplicates the number of the regularization parameters that need to be chosen. Choosing the regularization parameters in Grais and Erdogan (2012b) was done using validation data. That means, in Grais and Erdogan (2012b) we had to train many source models (basis matrix and prior GMM) for different regularization parameter values.", "startOffset": 143, "endOffset": 551}, {"referenceID": 9, "context": "Since, we needed to update (retrain) the GMM parameters at each NMF iteration, joint training slowed down the training of the source models in Grais and Erdogan (2012b). Another problem of using joint training is that, we had other regularization parameters during the training stage that needed to be chosen. Using joint training duplicates the number of the regularization parameters that need to be chosen. Choosing the regularization parameters in Grais and Erdogan (2012b) was done using validation data. That means, in Grais and Erdogan (2012b) we had to train many source models (basis matrix and prior GMM) for different regularization parameter values. Then, we chose the best combination for the regularization parameter values in training and separation stages that gave the best results during the separation stage. In the case of using MMSE estimate regularization for NMF, we do not need to use joint training. In this paper, we do not need to consider solving the regularized NMF in (27) during the training stage to solve (8). In the training stage, the training data for each source is assumed to be clean data. Since the spectrogram of each source training data represents clean source data, the NMF solution for the gains matrix can not be seen as a distorted image. Therefore, the deformation measurement parameter \u03a8 is a matrix of zeros. When \u03a8 = 0, the MMSE estimates prior term in (27) will disappear because \u2211K k=1 \u03b3kn = 1. Then, the regularized NMF (27) becomes just NMF. That means, we do not need to use the regularized NMF during the training stage which is not the case in Grais and Erdogan (2012b). Here in the training stage, we just need to use IS-NMF to decompose the spectrogram of the training data into trained basis and gains matrices.", "startOffset": 143, "endOffset": 1628}, {"referenceID": 7, "context": "The spectral masks can be seen as the Wiener filter as in Fevotte et al. (2009). The estimated source signals \u015d1(t) and \u015d2(t) can be found by using inverse STFT of their corresponding STFTs \u015c1(t, f) and \u015c2(t, f).", "startOffset": 58, "endOffset": 80}, {"referenceID": 24, "context": "(SIR), which is defined as the ratio of the target energy to the interference error due to the music signal only (Vincent et al., 2006).", "startOffset": 113, "endOffset": 135}, {"referenceID": 26, "context": "In the sparsity prior, the NMF solution of the gains matrix was enforced to be sparse (Virtanen and Cemgil, 2009; Schmidt and Olsson, 2006).", "startOffset": 86, "endOffset": 139}, {"referenceID": 23, "context": "In the sparsity prior, the NMF solution of the gains matrix was enforced to be sparse (Virtanen and Cemgil, 2009; Schmidt and Olsson, 2006).", "startOffset": 86, "endOffset": 139}, {"referenceID": 26, "context": "Enforcing sparsity on the NMF solution of the gains matrix is equivalent to model the prior of the gains matrix using exponential distribution with parameter \u03bb (Virtanen and Cemgil, 2009).", "startOffset": 160, "endOffset": 187}, {"referenceID": 9, "context": "In regularized NMF with GMM based log-likelihood prior we trained the NMF bases and the prior GMM parameters jointly as shown in Grais and Erdogan (2012b). In the sparse NMF case, we got best results when \u03bb = 0.", "startOffset": 129, "endOffset": 155}, {"referenceID": 26, "context": "Comparing the relative improvements in dB that we got in this paper with the achieved improvements in other works (Wilson et al., 2008b,a; Virtanen and Cemgil, 2009; Virtanen, 2007) we can see that the, improvements in this", "startOffset": 114, "endOffset": 181}, {"referenceID": 25, "context": "Comparing the relative improvements in dB that we got in this paper with the achieved improvements in other works (Wilson et al., 2008b,a; Virtanen and Cemgil, 2009; Virtanen, 2007) we can see that the, improvements in this", "startOffset": 114, "endOffset": 181}], "year": 2013, "abstractText": "We propose a new method to enforce priors on the solution of the nonnegative matrix factorization (NMF). The proposed algorithm can be used for denoising or single-channel source separation (SCSS) applications. The NMF solution is guided to follow the Minimum Mean Square Error (MMSE) estimates under Gaussian mixture prior models (GMM) for the source signal. In SCSS applications, the spectra of the observed mixed signal are decomposed as a weighted linear combination of trained basis vectors for each source using NMF. In this work, the NMF decomposition weight matrices are treated as a distorted image by a distortion operator, which is learned directly from the observed signals. The MMSE estimate of the weights matrix under GMM prior and log-normal distribution for the distortion is then found to improve the NMF decomposition results. The MMSE estimate is embedded within the optimization objective to form a novel regularized NMF cost function. The corresponding update rules for the new objectives are derived in this paper. Experimental results show that, the proposed regularized NMF alPreprint submitted to Elsevier March 1, 2013 ar X iv :1 30 2. 72 83 v1 [ cs .L G ] 2 8 Fe b 20 13 gorithm improves the source separation performance compared with using NMF without prior or with other prior models.", "creator": "LaTeX with hyperref package"}}}