{"id": "1303.2826", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2013", "title": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA", "abstract": "lyubomir This revengers article 12:37 presents a probabilistic soccorso generative caneghem model for sheeba text jpegs based on katelyn semantic topics decandido and syntactic classes 998 called Part - of - Speech LDA (tto POSLDA ). roskosmos POSLDA fatuma simultaneously rawah uncovers short - co-presenter range syntactic patterns (dinhata syntax) exorcised and tut long - colbeck range non-repudiation semantic joelle patterns (topics) symplectic that infarction exist 59.1 in document collections. cauberg This sub-zones results in word distributions ankara that hirko are specific 948 to selna both corvalan topics (baike sports, education, .. .) age-related and manuals parts - ieri of - speech (105.13 nouns, verbs, .. . ). calvani For example, multinomial urc distributions over sporran words bocheng are uncovered lactuca that sajidul can schuessler be understood 2/7th as \" nouns cybercafes about 1.720 weather \" acerca or \" 200-ton verbs about maekyung law \". We aggar describe kobayakawa the interestingly model passive and setups an juster approximate habr\u00e9 inference runt algorithm scandisk and 23.08 then colyer demonstrate the quality of maniacally the ascendant learned topics kmorales both finials qualitatively cohabiting and alene quantitatively. Then, barmore we gentzen discuss decriminalization an NLP mca/universal application where the siteswap output dollar-denominated of POSLDA can lead to leben strong winsberg improvements mockingbirds in flashover quality: unsupervised agapova part - of - dehiscence speech tagging. We trounced describe algorithms kartvelian for tryscoring this capulina task 454.5 that make 159.4 use of 1lb POSLDA - in\u00e1cio learned malone distributions guney that 711,000 result dastaan in fron improved huntington-whiteley performance beyond 25/32 the melissanidis state of adulthood the 119.36 art.", "histories": [["v1", "Tue, 12 Mar 2013 10:20:50 GMT  (154kb,D)", "http://arxiv.org/abs/1303.2826v1", "Currently under review for the journal Computational Linguistics"]], "COMMENTS": "Currently under review for the journal Computational Linguistics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["william m darling", "fei song"], "accepted": false, "id": "1303.2826"}, "pdf": {"name": "1303.2826.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA", "authors": ["William Darling", "Fei Song"], "emails": ["william.darling@xrce.xerox.com.", "fsong@uoguelph.ca."], "sections": [{"heading": null, "text": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA\nWilliam Darling\u2217 Xerox Research Centre Europe\nFei Song\u2217\u2217 University of Guelph\nThis article presents a probabilistic generative model for text based on semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA). POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and long-range semantic patterns (topics) that exist in document collections. This results in word distributions that are specific to both topics (sports, education, ...) and parts-of-speech (nouns, verbs, ...). For example, multinomial distributions over words are uncovered that can be understood as \u201cnouns about weather\u201d or \u201cverbs about law\u201d. We describe the model and an approximate inference algorithm and then demonstrate the quality of the learned topics both qualitatively and quantitatively. Then, we discuss an NLP application where the output of POSLDA can lead to strong improvements in quality: unsupervised partof-speech tagging. We describe algorithms for this task that make use of POSLDA-learned distributions that result in improved performance beyond the state of the art."}, {"heading": "1. Introduction", "text": "Two highly related phenomena are resulting in a renaissance for the study of computational linguistics. The first is the increasing level of access to textual data sources over the Internet such as classic works of literature (The Gutenberg Project), structured explanatory knowledge of the world (Wikipedia), people\u2019s every thought (Twitter), governments\u2019 every desire (laws and legal decisions), and ongoing triumphs, defeats, changes, and breaking information (online news). The second, concomitant with the first, is the growth of interest in, and the power of, machine learning algorithms which can exploit the vast amounts of data that are being made available and help make sense of them.\nLike language itself, machine learning techniques can be described and contrasted with each other along a number of different axes of understanding and dichotomies. One of the most important of these dichotomies is the division between supervised and unsupervised learning approaches (Blei, Griffiths, and Jordan 2010). While supervised approaches are concerned with generalizing a function that predicts an output y given some input x learned from examining labeled example pairs (xi, yi), unsupervised approaches involve uncovering hidden patterns and associations that exist in data (Hastie, Tibshirani, and Friedman 2001). Clustering algorithms, for example, are unsupervised machine learning techniques that attempt to group data together because they are similar in some way. A logical definition of similarity \u2013 especially with respect to linguistics \u2013 is, informally, that two texts are similar because they discuss the same topics.\n\u2217 6 chemin de Maupertuis, 38240 Meylan, France. E-mail: william.darling@xrce.xerox.com. \u2217\u2217 50 Stone Road East, Guelph, Ontario, N1G 2W1, Canada. E-mail: fsong@uoguelph.ca.\n\u00a9 2005 Association for Computational Linguistics\nar X\niv :1\n30 3.\n28 26\nv1 [\ncs .C\nL ]\n1 2\nM ar\n2 01\n3\nAnother canonical example of unsupervised learning is dimensionality reduction. In reducing the dimensionality of a dataset, an algorithm seeks to find a simpler or more condensed representation of the data while preserving (or bringing forth) some kind of meaning. An apt dimensionally-reduced representation of texts \u2013 collections of words \u2013 is the topics that they represent. In the standard bag-of-words representation used for many natural language processing tasks, the cardinality of the dimensional representation is the number of distinct words that can be used in the texts, that is, the vocabulary. This number is typically on the order of several thousand, while a text\u2019s content, in a broad sense, can also be described by the topics that it addresses out of a finite number of generic topics on the order of hundreds or less.\nIt is therefore no surprise that probabilistic topic models, which are generative models of text (based on unsupervised machine learning algorithms), are continually growing in interest. They provide a means to uncover the hidden thematic structure that underlies large document collections and therefore allow us to explore, summarize, and understand what a collection is about with ease and efficiency. Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003), the original topic model, describes the generative story that lays the foundation for this kind of model. It posits that a document can be created through a random process of drawing words from a mixedmembership model. At a high level, a document is created by first selecting the topics that it will address, and then randomly generating words from distributions associated with those topics. Numerous more complex models have been presented that build on this basic idea and several introductory papers exist that cover the area in detail (Blei 2012).\nMore specifically, the LDA generative process works as follows. For each document d, a document-specific topic portion \u03b8d is drawn from a Dirichlet distribution. \u03b8d is a discrete distribution over K topics and corresponds to the weight that each topic will have in the document. Then, for each word wi, a topic index zi is drawn from \u03b8d. To generate the word, a token is drawn from a topic-specific word distribution \u03c6(zi). There areK topic-specific word distributions, each of which corresponds to a distribution over words specific to the given topic. To generate a document, however, one would require the word distributions specific to each topic, and for each document, one would also need the topic portion. Neither of these are readily available from the input, but they can be learned from a document collection by reversing the generative process through posterior inference.\nIn unsupervised learning, we cannot simply write a general algorithm to find what we hope to be interesting patterns. The \u201cno free lunch\u201d theorem tells us essentially that no interesting patterns can be uncovered if we do not assume that certain kinds of patterns must exist (Wolpert and Macready 1997). It turns out that assuming that documents reflect latent topics and that words from the same topics will co-occur is a good assumption and thus interesting results can be learned through reversing the assumed model such as the words that are most important to each topic, and a dimensionally-reduced representation of each document in topic space. However, the correct number \u2013 or the type of assumptions \u2013 is important. If we make too many, the data may not fit those assumptions and the output will be nonsense. Conversely, if we make too few, interesting patterns may be missed.\nThe standard document representation for topic modeling is the bag-of-words (Salton and McGill 1983). Each document is represented by the number of times that each word in a fixed vocabulary appears. Because word order is ignored, a great deal of meaning is lost, but the representation is efficient and has proved to be successful. However, word order \u2013 ignored in canonical topic models \u2013 is clearly of importance to\nlanguage because though some sense may be extracted from a text whose words have been scrambled, the full meaning can only come about through a parsing of the words based on their order and relation to each other. It has also been shown that different parts of the brain are used to understand semantics and syntax (Boyd-Graber and Blei 2010). Traditional topic models miss this kind of syntactical information because they are never exposed to word-order patterns in the first place. While the bag-of-words approach is efficient, further advances in NLP will require algorithms to be able to have the full understanding that humans are afforded.\nIn this article we introduce a new probabilistic generative model called Part-ofSpeech LDA (POSLDA) which considers both the semantic topic that a word is associated with (if any) and its syntactic purpose in a sentence. This approach allows a more structured view of language creation and as a result, the posterior distributions that are learned are more specific and meaningful than in other topic models. It also allows NLP applications to make better predictions and deductions because each piece of information \u2013 syntactic and semantic \u2013 provides evidence that can help disambiguate the purpose and meaning of a word.\nThe article is organized as follows. In section 2, we discuss previous work that has focused on bringing syntactic information into probabilistic topic models. In section3, we explain our model, POSLDA, in detail. We then present the results of three sets of experiments in section 4: first, we demonstrate qualitatively the interpretability of the uncovered posterior distributions with example syntax-specific topics on a number of diverse datasets; second, we report quantitative results on the model\u2019s ability to generalize on unseen texts and to uncover high quality topics; and third, we show how the model\u2019s ability to disambiguate word use through the joint influences of semantics and syntax can lead to better results in unsupervised part-of-speech (POS) tagging than a Bayesian Hidden Markov Model (HMM). Finally, in section 5, we conclude with thoughts on future work."}, {"heading": "2. Topics and Syntax", "text": "The constraints that are imposed by language on phrase structure and word order are called syntax (Manning and Sch\u00fctze 1999). The syntactic meaning of a word helps to explain its functional purpose in a sentence, whereas the semantic meaning is related to its lexical-thematic purpose. The former is based on short-range dependencies at the sentence level, while the latter realizes long-range dependencies at the document level. LDA and other topic models uncover patterns by exploiting the long-range dependencies of words co-occurring. Here, we want to add the short-range dependency structures to the model and we do so by focusing on modeling the functional purpose of a word in a sentence. We are therefore interested in the part-of-speech category that a word \u2013 in its given context \u2013 belongs to. These include nouns, verbs, adjectives, adverbs, prepositions, conjunctions, etc. The canonical tool for unsupervised word syntax modeling is the hidden Markov model (HMM) (Rabiner 1990) and it is therefore a natural place to begin in adding syntax information to a semantic topic model.\nThe first work in combining syntactic notions of language with probabilistic topic models is based on embedding an LDA-like model in a single state of an HMM (Griffiths et al. 2005). This model \u2013 dubbed HMMLDA \u2013 represents an asymmetric composite model where all generated words follow short-range syntactic dependencies, but only \u201csemantic\u201d words that are generated from a single state obey long-range dependencies. Words that carry long-range dependencies will be generated given the documentspecific topic distribution, and other words will be generated independent of the current\ntheme. This framework is different from the traditional use of the HMM in syntax modeling as each state will not correspond to a discrete part-of-speech. The content class in particular \u2013 which is designated as the sole class that can generate \u201csemantic\u201d words \u2013 will need to subsume nouns, verbs, adjectives, and other words that are topic-dependent. We will return to this simplifying assumption when we present our POSLDA model.\nMore formally, the HMMLDA model is defined by two sets of sequential latent variables (zn)Nn=1, which represent the latent topics for each word, and (cn)Nn=1, which represent the latent syntactic classes for these words. One state in the model, s0 \u2208 S, is designated as the semantic class where the LDA-like topic model is embedded. Each topic k is associated with a discrete topic-word distribution \u03c6(k) and each class s 6= s0 is associated with a syntax word distribution \u03c6(s). Like LDA, each document d can be described by a distribution over topics \u03b8(d). However, unlike LDA, each word wi only depends on its topic zi if its class ci = s0. A word\u2019s class is modeled with the embedded HMM and transitions between classes si and si+1 are encoded in a transition matrix \u03c0. The generative process by which a document is created under the HMMLDA model is as follows.\n1. Draw \u03b8(d) \u223c Dirichlet(\u03b1) 2. For each word wi in document d\n(a) Draw topic zi \u223c \u03b8(d) (b) Draw class ci \u223c \u03c0(ci\u22121) (c) If ci = s0:\ni. Draw wi \u223c \u03c6(zi)\n(d) Else: i. Draw wi \u223c \u03c6(ci)\nLike LDA, exact posterior inference is intractable for the HMMLDA model (Griffiths et al. 2005). Griffiths, et al. therefore turn to Gibbs sampling. The HMM in the HMMLDA is a Bayesian HMM meaning that the transition rows \u03c0r and the emission probabilities \u03c6(c) are multinomial random variables with Dirichlet priors. The same framework for collapsed Gibbs sampling can therefore be used as with the original LDA.\nOne of the most interesting qualities of the HMMLDA model is that stop-words and other \u201csyntax-only\u201d words are pulled to the non-semantic classes so that the learned topics are interpretable and noise-free without any need for pre-processing or a priori stop-word removal. This defines one of the key motivations in the development of POSLDA. While the model seems to learn distributions that are noticeably useful, it is still far from perfect. It almost exclusively finds nouns as content words when verbs are often equally as important in semantic topics. This could be due to the use of the HMM that learns to discern nouns from other parts-of-speech and simply sees other semantically-important types of words as outliers that are pushed to the syntactic classes.\nAnother recent approach at combining syntax and semantics into a coherent probabilistic generative model is the Syntactic Topic Model (STM) (Boyd-Graber and Blei 2010). Unlike the HMMLDA model, where a word is deemed to either come from a corpus-wide syntax class or a semantic-based topic, the STM discovers topics that are both syntactically and semantically coherent. This is more directly in line with our goals for the POSLDA model. As for the motivation in combining these notions of word information, Boyd-Graber and Blei provide an edifying example:\nNext weekend, you could be relaxing in ________.\nThere are two distinct text-modeling based approaches one could use to reason about what word might be used to fill in the blank. With a topic model such as LDA, where this document is about travel, high probability words might include \u201csailing\u201d, \u201cRome\u201d, or \u201cflight\u201d. With a syntax model, it might determine that the missing word should be a noun, and thus high probability words could include \u201cbed\u201d, \u201cchurch\u201d, or \u201cschool\u201d. However, as is explained in (Boyd-Graber and Blei 2010), the best candidate to fill in the blank is an intersection of these two types of reasoning. The word \u201csailing\u201d matches the topic related aspect of the sentence (travel), but does not fit syntactically (verb). The opposite is the case for the noun \u201cschool\u201d. However, when both syntactic and semantic notions of language are taken into account, a word such as \u201cRome\u201d, which has high probability both in the travel topic and in the noun syntax class, will be selected with high probability.\nThe STM is a non-parametric model where the number of topics is not set a priori but is determined, through posterior inference, by the data. While the standard LDA model draws the document topic portions for a document from a K-dimensional Dirichlet distribution with a fixed value of K topics, here the transition distributions \u03c0 and document topic portions \u03b8d are drawn from a Dirichlet Process (DP), where a vector \u03b2 of infinite length is a global weight that is drawn from a stick-breaking distribution and is used as a base measure for the DP. This approach frees users of the model from having to determine themselves how many topics a corpus might contain. Our POSLDA model can also easily become a nonparametric Bayesian model by incorporating a hierarchical Diriclet Process (HDP) prior (Teh et al. 2006).1 In this formulation, the number of topics can be learned from the data in the sense that through inference we can learn the optimal number of topics K that will maximize the likelihood of the data.\nThe generative process for the STM is as follows:\n1. Draw global weights \u03b2 \u223c GEM(\u03b1) 2. For each topic index k = {1, ...}:\n(a) Draw topic \u03c4k \u223c Dir(\u03c3\u03c1u) (b) Draw transition distribution \u03c0k \u223c DP(\u03b1T , \u03b2)\n3. For each document d = {1, ...,M}: (a) Draw document weights \u03b8d \u223c DP(\u03b1D, \u03b2) (b) For each sentence root node with index (d, r) \u2208 SENTENCE-ROOTSd:\ni. Draw topic assignment zd,r \u221d \u03b8d\u03c0start ii. Draw root word wd,r \u223c \u03c4zr\n1 The approach to do so specifically for POSLDA is outlined in \u00a73.4 of (Darling 2012).\n(c) For each additional child with index (d, c) and parent with index (d, p): i. Draw topic assignment zd,c \u221d \u03b8d\u03c0zd,p\nii. Draw word wd,c \u223c \u03c4zd,n\nWhile this generative process is clearly much more complex than the corresponding one for simpler models such as LDA and HMMLDA, it is also more powerful. The key steps are 3(b)(i) and 3(c)(i) where the topic assignment for a word is chosen to be a convolution of the long-range semantic topic portion \u03b8d and the short-range syntactic probability \u03c0zd,p . Some example semantically and syntactically coherent topics learned from the STM are shown in Table 1. Note that in each case, the high probability words are both syntactically equivalent (noun, verb, etc.) and semantically related in a topic modeling sense.\nWhile the posterior distributions learned by the STM appear to fall in line with our goals of a syntactically-cognizant generative topic model, it suffers from certain limitations that we would like to address. First, the generative story depends on a form of meta-sentence structure that exists before the words have been generated. That is, texts are generated based on the probabilities in sentence-specific dependency parse trees (Boyd-Graber and Blei 2010). Therefore, to perform inference with the STM \u2013 and recover the posterior distributions like those in Table 1 \u2013 the data must be separately pre-processed into sentence dependency parses. This means that the model is not learning the syntax patterns itself, but is using information that must be supplied by a separate algorithm before inference is performed. Conversely, we are interested in a fully generative model that is consistent across syntax and semantics where inferring the short-range sentence-wide dependencies forms part of the model.\nIn the next section we will present our model, POSLDA. It is a strong generalization of HMMLDA in that it follows the idea that we can combine an HMM with an LDAlike topic model, but it takes the idea further so that topic-dependent words can also be influenced by different syntax classes and thus learn part-of-speech specific posterior distributions."}, {"heading": "3. POSLDA", "text": "While LDA is in some sense a simple extension of probabilistic latent semantic analysis (Hofmann 2001), it can be seen as the first fully generative topic model by virtue of the Dirichlet prior that is placed on the document-topic portions which in effect free the model from specific training data. Since its inception, LDA has been extended in numerous ways and particularly by infusing the model with additional factors. Word distributions can become more specific if we consider that generated words are dependent on not only the current topic, but also other latent aspects such as the sentiment of the writing and the writer\u2019s personality or ideological perspective (Paul and Girju 2010; Ahmed and Xing 2010). This allows one to uncover such word distributions as \u201cpositive/negative words about films\u201d or \u201cwords about weather from the perspective of Americans/Swedes/Australians\u201d. In fact, this approach is so powerful that it has been generalized into techniques that can easily add specific factors to topic models through the use of strong prior information (Paul and Dredze 2012). However, to include word syntax, we need a different approach because this factor does not come out of the types of words that are used, but their order.\nPart-of-Speech LDA (POSLDA) is an extension and generalization of LDA and HMMLDA that is designed to understand the long- and short-range dependencies between words, and as a tool for more complex NLP tasks that require both seman-\ntic and syntactic information to attain optimal performance. In an HMM, words are considered independent of their wider context within a document, but depend on the classes of the words that appear before them. Therefore, the word order in a syntax model is important and the bag-of-words representation used in canonical topic models is no longer appropriate. Because both types of word information are important, and modeling each separately entails certain restrictions, we seek to bridge these restrictions with a unified model of language, POSLDA.\nUnder POSLDA, each word token is now associated with two latent variables: a topic z and a syntactic class c. We posit that the topics are generated through the LDA process, while the classes are generated through a Bayesian HMM. The observed word tokens are then generally dependent on both the topic and the class: rather than a single discrete distribution for a particular topic z or a particular class c, there are distributions for each topic-class pair (z, c) from which we assume words are sampled. However, there also exists a set of \u201csyntactic-only\u201d words that do not depend on the thematic context of a document (Griffiths et al. 2005). These words \u2013 such as determiners, prepositions, and conjunctions \u2013 are often called \u201cfunction\u201d words and should be modeled as \u201cuniversal\u201d syntax classes that are not affected by \u2013 and are not assigned \u2013 a latent topic.\nWe therefore are interested in a generative model of text where all generated words depend on the function that they perform in a sentence, and a subset of these words also depend on the current semantic topic. For the HMM-like portion of the model, we denote the set of classes C = CSEM \u222a CSYN, which includes the set of semantic classes CSEM and the set of syntactic (function word) classes CSYN. If a word is generated from a function word class, it does not depend on the topic. This allows our model to accommodate functional words that appear independently of the topical content of a document.\nWe use a similar notation to LDA, where \u03b8d is a document-topic portion and \u03c6\u03b7 is a word distribution. Additionally, we denote the HMM transition matrix \u03c0, which we assume has rows that are drawn from a Dirichlet distribution with hyperparameter \u03b3. Denote S = |C| and T = |Z|, the numbers of classes and topics, respectively. There are\nSSYN word distributions \u03c6(SYN) for function word classes and T \u00d7 SSEM word distributions \u03c6(SEM) for semantic classes. A graphical model depiction of POSLDA is shown in Figure 1. This figure also denotes a slight variation to the model called Labeled POSLDA which is analogous to Labeled LDA for the original LDA (Ramage et al. 2009). Here, an observed dictionary denoted by \u039b restricts the classes that certain words can take on. We will make use of this model for dictionary-based unsupervised part-of-speech tagging.\nThe generative process for a corpus of documents D under the POSLDA model is described as follows:\n1. For each row \u03c0r \u2208 \u03c0: (a) Draw \u03c0r \u223c Dirichlet(\u03b3) 2. For each word distribution \u03c6\u03b7 \u2208 \u03c6: (a) Draw \u03c6\u03b7 \u223c Dirichlet(\u03b2) 3. For each document d \u2208 D: (a) Draw \u03b8d \u223c Dirichlet(\u03b1) (b) For each word token wi \u2208 d:\ni. Draw ci \u223c \u03c0ci\u22121,...,ci\u2212n ii. If ci /\u2208 CSEM:\nA. Draw wi \u223c \u03c6(SYN)ci iii. Else:\nA. Draw zi \u223c \u03b8d B. Draw wi \u223c \u03c6(SEM)ci,zi\nIn traditional topic models, it is generally the case that common function words will overwhelm the word distributions, leading to suboptimal results and learned word distributions that are difficult to interpret. This problem is often skirted by either data pre-processing (e.g. removing stop words from a domain-dependent list) (Blei, Ng, and Jordan 2003), backing off to \u201cbackground\u201d word models (Chemudugunta, Smyth, and Steyvers 2006; Paul and Girju 2010), or by performing term re-weighting (Wilson and Chew 2010). In the case of POSLDA, these common words are naturally explained by the corresponding function word classes and are pushed to these distributions rather than the topic-specific distributions during learning."}, {"heading": "3.1 Relations to Other Models", "text": "The idea of having discrete word distributions for the cross product of topics and classes is related to multi-faceted topic models where word tokens are associated with multiple latent variables (Paul and Girju 2010; Ahmed and Xing 2010; Paul and Dredze 2012). Under such models, words can be explained by a latent topic as well as a second (or nth) underlying variable such as the perspective or dialect of the author, and words may depend on both (or multiple) factors. In our case, the second variable is the partof-speech \u2013 or functional purpose \u2013 of the token.\nPOSLDA is also similar to a recent model called Nested HMM-LDA (nHMMLDA) (Jiang 2009). The model described is very similar to POSLDA but contains certain limitations. Principally, rather than allowing each word to be generated from any of K topics, all words from a sentence must share the same topic. This is a strong assumption since it will not allow a sentence to discuss more than a single topic.\nPOSLDA is constructed in a generalized manner and contains many existing models as special cases. For example, POSLDA reduces to a Bayesian HMM when the\nnumber of topics K = 1, the original LDA model when the number of classes S = 1, or the HMMLDA model when the number of semantic classes SSEM = 1. One of the key benefits of these reductions is that one can easily experiment with any of these models using a single POSLDA implementation by simply altering the necessary parameters. POSLDA can also easily reduce to the nHMMLDA model by forcing all words in a sentence to share the same topic."}, {"heading": "3.2 Approximate Inference", "text": "The principal computational problem in probabilistic topic/syntax models is posterior inference (Blei and Lafferty 2009). As it is based on LDA and the Bayesian HMM, exact inference in the POSLDA model is also intractable. Therefore, following many others, we make use of the MCMC-based approximate inference technique collapsed Gibbs sampling (Griffiths and Steyvers 2004; Heinrich 2004). Here, the multinomial parameters are first integrated out and we directly sample the indexing latent variables ci and zi. In POSLDA, if the class is designated as syntactic, then it only depends on the class. We therefore introduce the counts n(ci,zi)wi which correspond to the number of times that word wi is assigned to class ci and topic zi. Our sampling equation is then as follows:\np(ci, zi|c\u2212i, z\u2212i,w) \u221d \u03c1ci \u00d7 n (d) zi +\u03b1zi n(d). +\u03b1. n (ci,zi) wi +\u03b2 n(ci,zi). +W\u03b2 ci \u2208 CSEM\n\u03c1ci \u00d7 n (ci) wi +\u03b2\nn(ci). +W\u03b2 ci \u2208 CSYN\n(1)\nwhere\n\u03c1ci = n(ci\u22122,ci\u22121,ci) + \u03b3ci n(ci\u22122,ci\u22121) + \u03b3. \u00b7 n(ci\u22121,ci,ci+1) + \u03b3ci n(ci\u22121,ci) + \u03b3. \u00b7 n(ci,ci+1,ci+2) + \u03b3ci n(ci,ci+1) + \u03b3.\n(2)\nNote that we sample the pair (ci, zi) jointly as a block, which requires computing a sampling distribution over SSYN + T \u00d7 SSEM. It would also be valid to sample ci and zi separately, which would require only S + T computations, in which case, the sampling procedure would be somewhat different. Despite the lower number of computations per iteration, however, the sampler is likely to converge faster with our blocked approach because the two variables are tightly coupled. The intuition is that a non-block-based sampler could have difficulty escaping local optima because we are interested in the most probable pair; a highly probable class c sampled on its own, for example, could prevent the sampler from choosing a more likely pair (c\u2032, z).\nOne problem with MCMC-based methods is that assessing convergence can be difficult. We do not address specific approaches to get around this issue, but we note that a stabilizing likelihood can be used to infer convergence. Generally this happens after several hundred iterations (depending on the size of the dataset), and for the datasets used in this article, the likelihood will converge at about 2,000 iterations. Finally, we are interested in deriving point estimates for the topics \u03c6(c,z) from the sampled statistics. \u03c6 is a 3-dimensional array where \u03c6(c,z)wi = p(wi|c, z). Therefore, following (1), we get\np(wi|c, z) = n (c,z) wi + \u03b2\nn(c,z). +W\u03b2 (3)"}, {"heading": "4. Experiments and Results", "text": "In this section we present a set of experiments on the POSLDA model to demonstrate its capabilities as a topic and syntax model of language. We demonstrate both qualitatively and quantitatively the model\u2019s ability to capture the semantic and syntactic axes of information prevalent in a corpus. We begin qualitatively with topic interpretability and then present quantitative results on the ability of POSLDA as a predictive language model. Following this, we show its ability as a model for performing unsupervised POS tagging."}, {"heading": "4.1 Topic Interpretability", "text": "Judging the interpretability of a set of topics is highly subjective. Chang, et al. look at \u201cword intrusion\u201d where a user determines an intruding word from a set of words that does not thematically fit with the other words, and \u201ctopic intrusion\u201d where a user determines whether the learned document-topic portion \u03b8d appropriately describes the semantic theme of the document (Chang et al. 2009). Here, we are mostly interested in subjectively demonstrating the low incidence of \u201cword intrusion\u201d both in terms of semantics (theme) and syntax (part-of-speech). We subjectively demonstrate that our\nmodel learns semantic and syntactic word distributions that are likely robust towards problems of word intrusion.2\nTo demonstrate the effectiveness of POSLDA\u2019s semantic-syntactic pattern recognition ability, we fit a number of models to different datasets. We demonstrate results both on more \u201ctraditional\u201d news corpora such as TREC AP and the WSJ, and on more esoteric datasets such as collections of tweets from the microblogging website Twitter and collections of legal decisions from the Supreme Court of Canada.3 We begin with a standard demonstration of topic interpretability on news data from the Associated Press.\n4.1.1 Traditional News Data. Table 2 shows three topics \u2013 manually labeled as \u201claw\u201d, \u201cfinance\u201d, and \u201chealth\u201d \u2013 learned from a 2,250 document subset of the TREC AP corpus (Harman 1992). We set the number of topics K = 30, the number of classes S = 17, and the number of semantic classes SSEM = 7.4 We show the top ten words from three POSspecific topics labeled manually as adjective, verb, and noun. The interpretability of the topics and the cohesiveness of the terms with high probabilities is clear. All three topics assign high probabilities to words that one would expect to have high importance. More importantly, however, the POS-specific topics also clearly reflect their syntactic roles. Each of the verbs is assuredly (even without the proper context) a verb, and the same thing for the nouns. The adjectives seem to fit as well; though many of the words could be considered nouns depending on the context, it is clear how given the topic each of the words can very well act as an adjective. A final point worth mentioning is that, unlike LDA, we do not perform stop-word removal. Instead, the POSLDA model has pushed stop-words to their own syntactic classes (rather than semantic) freeing us from having to perform pre- or post-processing steps to ensure interpretable topics. The top words in four of these topic-independent syntactic classes are shown in Table 3 with manually-labeled class names.\n2 This is in line with our approach of viewing topic models as tools for performing other tasks. We are most interested in objective quantitative results learned from applying our models to NLP tasks such as POS tagging which we demonstrate later in this section. 3 http://scc.lexum.org/en/index.html. 4 We choose this number based on intuition. We imagine that of the 17 often-delineated parts-of-speech\n(Goldwater and Griffiths 2007), 6 or 7 will generally be theme-specific. These include adjectives, verbs, gerund or present participle verbs, adverbs, nouns, and past participle verbs. It can be helpful to include an \u201cextra\u201d semantic class to see if the model can find patterns of semantics-syntax that we might miss.\nNext we look at the ACL_DCI release of the WSJ treebank dataset which contains approximately 3 million words over 6,058 documents. We turn to this dataset both to show further results of POSLDA\u2019s pattern recognition ability along the axes of both semantics and syntax, and to demonstrate the scalability of the model to a larger corpus. Because this is a much larger dataset than the TREC AP corpus, we set the number of topics K = 50, but leave the class parameters untouched. Note that this approach to \u201cguessing\u201d the number of topics represented by a dataset is a typical way to begin understanding the make-up of a collection of documents. With the parametric version of POSLDA, we can use perplexity as calculated on a held-out test set to help determine the best number of topics and this is explored in the following subsection. For a more principled approach, we can use an HDP prior over the number of topics (Teh et al. 2006).\nTable 4 shows two topics learned on the WSJ dataset with the parameters described above. Again, we show each topic as three POS-specific topics learned from POSLDA: verb, noun, and adjective. We show some of the more interpretable part-of-speechspecific topics, but in general the learned distributions appear noisier than those found on the smaller AP dataset. Nevertheless, the found topics are subjectively interpretable.\n4.1.2 Domain Specific Data. While corpora of newswire documents are prevalent in NLP research due to their contained articles\u2019 proclivity, there are a number of other\ndomains with large collections of data that will benefit from new statistical models for corpus exploration, data mining, and other text-related tasks. These areas may include, inter alia, public health, economics, and law. Studying domain-specific corpora can be illuminating in text modeling research because often domains are filled with eccentricities that do not show up in general writing. These include domain-specific vocabularies and specialized writing structures. Law is a particularly relevant field because of the abundance of textual data that is produced in the field. Here, we demonstrate the qualitative effectiveness of modeling a collection of Supreme Court of Canada decisions with the POSLDA model.\nWe choose K = 40, S = 17, and SSEM = 7, as above. Four of the uncovered topics broken into adjective, verb, and noun are displayed in Table 5. Once again, the POSspecific topics are clear and interpretable. In the subtopic for \u201ccriminal law\u201d adjectives, for example, there are a number of first-words from some common criminal law phrases. These include \u201cmens\u201d from the common phrase mens rea (the mental component required to commit a crime), \u201cactus\u201d from the common phrase actus reus (the physical act component required to commit a crime), and \u201creasonable\u201d which often modifies the word \u201cdoubt\u201d to form the phrase reasonable doubt. Furthermore, the nouns in the criminal law topic help make clear that domain specific data have been understood properly. The most probable word in the subtopic for \u201ccriminal law\u201d nouns is \u201caccused\u201d which might na\u00efvely be tagged as a verb in a dictionary-only based method\nthat has little understanding of context. Here, it is correctly placed in the noun subtopic because the accused is the person that stands accused of a crime.5\nAs with other datasets there is some noisiness in the results such as the word \u201csection\u201d appearing in the verb subtopic for the criminal, labour, and family law topics. This is likely due to the fact that in legal decisions phrases describing the origin of laws typically have their own sort of quasi-grammar and \u201csection\u201d is a common word in this context. An example is found in Winters v. Legal Services Society6 where our sentence splitter found the sentence \u201cThe Requirements of Section 3(2)\u201d. For this to be a proper sentence it requires a verb. The model has likely decided that the word \u201csection\u201d would work the best as a verb in this context and it is therefore likely an error attributable to the sentence splitting algorithm as opposed to the POSLDA model. Another problem is that the verb \u201cappeal\u201d has shown up as the most important verb in every topic. This is an interesting issue because typically we have seen indistinguishing words pushed to the syntax-only classes. One likely reason that this has not happened is that, though the word is important in many topics, it is not important in all of the uncovered latent topics. Appeal is also an interesting word in this domain because it is used to a great extent as both a verb (to appeal a decision) and as a noun (the appeal in question). While appeal is a common noun, it does not show up as any of the top ten nouns in any of the displayed noun subtopics. Despite these slight inconsistencies, however, the POSLDAlearned topics from the SCC dataset are interpretable and clear.\n4.1.3 Noisy Data. Recently, there has been a large interest in mining the vast quantity of text data created every day though the popular microblogging service Twitter.7 There are a number of unique challenges associated with analyzing this kind of data, however, that make it different from the datasets studied above. First, unlike the highly-structured news articles in corpora such as TREC AP and WSJ, Twitter messages (\u201ctweets\u201d) are rarely well-formed sentences. Proper grammar (or even anything close to it) is all but abandoned on Twitter and the rules of punctuation have been seemingly reinvented (Ramage, Dumais, and Liebling 2010). One of the principal reasons for this style of writing is that tweets are constrained to a maximum of 140 characters. This of course poses its own issues with respect to text modelling as short documents will contain less thematic information. In addition, partially due to character-limit constraints and partially because Twitter is very popular amongst young people, proper spelling is rare in such a dataset (Ritter, Cherry, and Dolan 2010). Each of these issues poses unique problems for modelling topics in this area. Because the long-range thematic dependencies in POSLDA are determined by word co-occurrence, multiple spellings of the same word can hinder unsupervised topic recognition. On the other hand, because the shortrange syntactic dependencies in POSLDA are learned by understanding common wordclass transitions, structureless grammar also causes problems in determining parts-ofspeech.\nDespite the issues outlined above, however, Twitter is a very interesting resource. It represents the up-to-the-minute thoughts of millions of people across the world and the knowledge that can be learned from this data is likely immense. One of the most interesting analyses of Twitter data so far is to use a supervised topic model to learn current issues about public health such as what health problems are being experienced\n5 See, e.g. Black\u2019s Law Dictionary (2d Pocket ed. 2001). 6 Winters v. Legal Services Society, [1999] 3 S.C.R. 160. 7 http://www.twitter.com.\nby whom (and where) and how people are treating those problems (Paul and Dredze 2011). Here, we simply demonstrate that without any additional machinery, POSLDA can learn semantically and syntactically consistent topics from a collection of tweets. We use the Twitter POS dataset released at ACL 2011 by Gimpel, et al. which consists of approximately 26,000 words across 1,827 tweets (Gimpel et al. 2011). While this is a fairly limited collection of data, our model is nevertheless able to uncover some interesting POS-specific topics. Table 6 shows three manually-labeled topics learned with the settings of K = 20, S = 17, and SSEM = 7.\nThe topics outlined in Table 6 are by far the noisiest and least interpretable demonstrated thus far. Because of the non-standard text structure and issues with incorrectly spelled words, it is difficult for the algorithm to uncover the patterns of interest. Nevertheless, of the 20 topics, three fairly interpretable topics are demonstrated. As it is difficult to tell exactly which part-of-speech each subtopic represents, we only list two distinct subtopics per topic: verb and noun. The first topic \u2013 \u201cmedia\u201d \u2013 is likely the most coherent while the other two have been labelled with trailing question marks to convey that it is difficult to name the topics. It is for this reason that Twitter-specific topic models (or at least Twitter-specific alterations to existing topic models) may be required (Ramage, Dumais, and Liebling 2010; Ritter, Cherry, and Dolan 2010). Analyzing the listings in Table 6 does show, however, that POSLDA can uncover interesting semantic and syntactic patterns even in this highly noisy source."}, {"heading": "4.2 Quantitative Results", "text": "There are several methods that are commonly employed to evaluate novel probabilistic models in the literature (Wallach et al. 2009). The original LDA paper \u2013 and many others \u2013 use the perplexity which is a standard metric in the information retrieval literature (Blei, Ng, and Jordan 2003; Teh et al. 2006). A probabilistic model can also be evaluated by considering its performance on an extrinsic task. Here, we first focus on the perplexity and use it to measure POSLDA\u2019s performance as a predictive language model, and then discuss using the model for unsupervised POS tagging.\n4.2.1 Predictive Language Modelling. Following the standard practice in topic modeling research (Blei, Ng, and Jordan 2003; Griffiths et al. 2005; Teh et al. 2006; Zhu, Blei, and Lafferty 2006), we fit a model to a training set and compute the perplexity of a held-out test set. The perplexity can be defined as the predicted average number of words that\nare equally likely to be generated for a given position (Zhu, Blei, and Lafferty 2006). It is also a monotonically decreasing function of the log likelihood.\nThe perplexity of a held-out test set Dtest = (wd)Md=1 is given as:\nppx(Dtest) = exp ( \u2212 \u2211M d=1 log p(wd|M)\u2211M\nd=1Nd\n) (4)\nwhereM represents the model parameters learned from the training data, p(wd|M) is the probability (likelihood) of document wd given the learned model parameters, and Nd is the number of words in document wd.\nWe test the POSLDA model on a subset of the AP TREC dataset. We use ten-fold cross-validation where the data is split into 10 subsets of equal size. We conduct 10 experiments where one of the subsets is held out for testing and the model is trained on the remaining 9 subsets. We report the average over the 10 experiments. We compare the perplexity of POSLDA to the original LDA model, a Bayesian HMM, and Griffiths, et al.\u2019s HMMLDA. Each model is trained using 10,000 iterations of Gibbs sampling. We use asymmetric priors on the document-topic portions \u03b8 and the rows of the HMM transition matrix \u03c0. Following (Wallach, Mimno, and McCallum 2009), we use a symmetric prior on the topic-word distributions \u03c6 because having asymmetric values on the topics themselves does not seem to lead to improved performance. The prior\u2019s hyperparameters are optimized using Minka\u2019s fixed-point method (Wallach 2008). For these experiments we use S = 10 classes of which 5 are designated as semantic. By definition, the HMMLDA model has SS = 1 and the LDA model has S = SSEM = 1. The HMM model does not consider the number of topics K.\nFigure 3 shows the average perplexity values on a held-out test set for a number of models in the same family as POSLDA over a range of topic values. The HMM achieves\nlower perplexity values than LDA at all topic settings K = {5, 10, 15, 20, 25, 30}. All three topic-based models realize perplexity improvements as the number of topics K increases. Both HMMLDA and POSLDA \u2013 which combine the benefits of the HMM and LDA models \u2013 result in lower perplexity values. POSLDA\u2019s additional flexibility in terms of the additional semantic classes allows it to record the lowest perplexity values of all the models tested for each topic setting.\nWhile Figure 3 shows how the POSLDA model\u2019s ability to generalize on unseen data is affected by the number of topics, Figure 4 illustrates the changes in the perplexity when the number of classes S is the independent variable. While it may not reflect the best possible values for the POSLDA model, we set the number of semantic classes SSEM = S. Interestingly, the HMM\u2019s perplexity starts to shoot up when S = 15, and both HMMLDA and POSLDA show poor perplexity when S = 20. This likely reflects too much variation in the model (especially with no disambiguation between semantic and syntactic classes). Next, we look at how the perplexity varies when S is held fixed but SSEM is free to vary.\nFigure 5 shows the average perplexity as the number of semantic states SSEM is varied. We set S = 10 and investigate how different settings of SSEM affect the model\u2019s ability to generalize. When SSEM = 0 the model reduces to a Bayesian HMM and when SSEM = 1 it becomes the HMMLDA model. Adding some semantic information with HMMLDA improves the perplexity considerably (1172 to 957) and further distinguishing semantic information in POSLDA continues to improve the perplexity until it reaches a minimum at SSEM = 6. This reflects the fact that certain classes of words such as conjunctions are not aided by thematic information.\nAbove we have demonstrated both that POSLDA tends to learn interpretable topics that are part-of-speech specific, and that it can lead to better predictive performance than other similar generative probabilistic models. The purposes of the models are clearly different, however. POSLDA is more flexible, but it requires a more expensive\nrepresentation of text (a sequence of words rather than a bag). HMMs are also extremely useful and versatile for a number of applications not necessarily related to text. Next we are interested in applications where the POSLDA model is truly needed or can truly make a difference. In (Darling and Song 2011) we showed how a POSLDA-like model can improve the results in a text summarization task. In the next section, we concentrate on unsupervised part-of-speech tagging."}, {"heading": "4.3 Unsupervised POS Tagging", "text": "Goldwater and Griffiths show that Bayesian HMMs increase the accuracy of unsupervised POS tagging by up to 14 percentage points over the MLE approach (Goldwater and Griffiths 2007). While these results are impressive, unsupervised approaches continue to fall well short of the accuracy obtained with supervised taggers. Nevertheless, unsupervised approaches are preferred in many situations especially when there is no access to large quantities of training data in a specific domain or particular language. We therefore aim to continue improving accuracy with unsupervised approaches by introducing semantics as an additional source of information for this task.\nThe word \u201cseal\u201d appears both as a verb (to seal a jar or a leak) and as a noun (the marine mammal Pinniped) in the WSJ treebank dataset. The HMM approach can often tag each of these occurrences appropriately given the context, but there are cases where it will fail. However, if the topic being discussed is marine biology, we have another piece of evidence that increases the likelihood that this occurrence of the word \u201cseal\u201d is a noun about the marine mammal. If the topic is about pickling or roofing, however, \u201cseal\u201d as a verb is given more evidence. Another example is the word \u201cbook\u201d. In a literary context it will almost always take the form of a noun. However, in a topic about promotions or services, the word is more likely to function as a verb: \u201cto book a hotel\u201d.\nFollowing this intuition, we use the POSLDA model with the number of HMM states set to the number of possible tags in the tag set and use the state index learned through posterior inference as the predicted tag for each word.\nWe take two approaches to perform unsupervised POS tagging using the POSLDA model. The first approach uses a tag dictionary containing varying amounts of information on the possible tags that certain words have taken on in the training data. This renders the problem to a case of POS disambiguation rather than pure unsupervised tagging. It is, however, the most common approach to demonstrating results in unsupervised tagging (Goldwater and Griffiths 2007). The second approach is a pure unsupervised method that implements POS clustering. Because we cannot know which classes represent which parts-of-speech, we instead compare the learned clusters to the correct clustering where clusters are exchangeable. Accordingly, we use the variation of information (VI) for evaluating this task (Meila\u030c 2007).\nWe showed in (Darling, Paul, and Song 2012) that this model consistently beats the Bayesian HMM approach in the Twitter domain. Here, we show that these improvements hold in a more traditional domain: the ACL_DCI release of the Penn Treebank\u2019s collection of Wall Street Journal newswire articles. This dataset contains approximately 3M words over 6K documents. We condense the tag set to the more standard for unsupervised POS tagging 17-tag set introduced by (Smith and Eisner 2005).\nWe follow the established form of (Merialdo 1993) and (Goldwater and Griffiths 2007) for unsupervised POS tagging by making use of a tag dictionary to constrain the possible tag choices for each word and therefore render the problem closer to disambiguation. Like in (Goldwater and Griffiths 2007), we employ a number of dictionaries with varying degrees of knowledge. A dictionary contains the tag information for a word only when it appears more than d times in the training corpus. We ran experiments for d = 1, 2, 3, 5, 10, and\u221e where the problem becomes POS clustering. We report both tagging accuracy and the variation of information (VI), which computes the information lost in moving from one clustering C to another C \u2032: V I(C,C \u2032) = H(C) +H(C \u2032)\u2212 2I(C,C \u2032) (Meila\u030c 2007). This can be interpreted as a measure of similarity between the clusterings, where a smaller value indicates higher similarity.\nTo properly make use of a tag dictionary, we slightly alter the POSLDA model by adding an additional observed random variable to the model as in Labeled LDA (Ramage et al. 2009). The tag dictionary is encoded in the model by a list of binary class indicators for each word \u039b(w) = (\u03bbw1 , ..., \u03bbwV ) where \u03bbw = (l1, ..., lC) for each class. The model then becomes as depicted in Figure 1 (right). The sampling equation is then updated simply to p(zi, ci|c\u2212i, z\u2212i,w) \u221d p(zi, ci|c\u2212i, z\u2212i,w)\u00d7 \u03bbw,ci .\nWe run our Gibbs sampler for 20,000 iterations and obtain a maximum a posteriori (MAP) estimate for each word\u2019s tag by employing simulated annealing. Each posterior probability p(c, z|\u00b7) in the sampling distribution is raised to the power of 1\u03c4 where \u03c4 is a value (in traditional annealing used in physics \u03c4 would be a temperature) that approaches 0 as the sampler converges. This approach is akin to bringing a system from an arbitrary state to one with the lowest energy thus viewing the Gibbs sampling procedure as a random search whose goal is to identify the MAP tag sequence, as employed in (Goldwater and Griffiths 2007). Finally, we run each experiment 5 times from random initializations and report the average accuracy and variation of information. All improved results reported for the POSLDA model are statistically significantly different from those achieved with the BHMM model as determined by a Student\u2019s t-test with p 0.01.\nThe achieved results are shown in Table 7 for a random tag assignment, the Bayesian HMM (BHMM) described in (Goldwater and Griffiths 2007), and our POSLDA tagging\napproach. For both BHMM and POSLDA, we optimize the asymmetric hyperparameters \u03b1 and \u03b3 and the symmetric prior \u03b2 using either direct optimization or sampling with similar results. We use 6 semantic classes \u2013 adjective, verb, gerund or present participle verb, adverb, noun, and past participle verb \u2013 leaving 11 remaining for syntax words, and we report results for K = 10 topics.\nThese POS tagging results are also shown graphically in Figure 6. Our model outperforms the Bayesian HMM for all values of d in accuracy and all values of d but one for variation of information. In fact, POSLDA maintains higher tagging accuracy for d = 5 than the Bayesian HMM for d = 1. Our approach consistently surpasses the results achieved with BHMM by approximately 1.5 percentage points for each value of\nd. The one case where POSLDA did not improve upon BHMM is where d =\u221e for POS clustering."}, {"heading": "5. Conclusions and Future Work", "text": "In this article we presented the combined topic and syntax model, Part-of-Speech LDA or POSLDA. We have also demonstrated its use as an improved model for performing unsupervised POS tagging. Our overarching goal is to demonstrate that combining the two axes of word meaning \u2013 syntax and semantics \u2013 into a coherent model can result in improvements to both learned topic distributions and to NLP tasks such as POS tagging. We showed that incorporating semantic information into the HMM model led to improved results for this task. Additionally, we showed that combining the two axes of word information results in a language model that achieves lower perplexity \u2013 and therefore better predictive capability \u2013 than other similar probabilistic models.\nIn future work we would like to apply the POSLDA model to other NLP tasks that also rely upon learned word distributions. These include text summarization, text segmentation, and translation. An interesting avenue for further research is POSLDA\u2019s ability to serve as the base of a language generation system. While performing the LDA generative process would perhaps result in documents that contain words that are semantically related, we cannot say that it is generating language. POSLDA on the other hand \u2013 with some strong prior knowledge \u2013 may be able to generate coherent natural language because it exhibits more structure in the language generation process. We hope to explore this direction of research and apply it to tasks such as database population and abstract text summarization."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the Natural Sciences and Engineering Research Council of Canada for partially funding this work through a Doctoral Post-Graduate Scholarship for William Darling. The authors would also like to acknowledge the financial support provided by Ontario Centres of Excellence (OCE) through the OCE/Precarn Alliance Program."}], "references": [{"title": "Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective", "author": ["Ahmed", "Xing2010]Ahmed", "Amr", "Eric P. Xing"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ahmed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2010}, {"title": "Probabilistic topic models", "author": ["M. David"], "venue": "Commun. ACM,", "citeRegEx": "David,? \\Q2012\\E", "shortCiteRegEx": "David", "year": 2012}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["Blei", "Griffiths", "Jordan2010]Blei", "David M", "Thomas L. Griffiths", "Michael I. Jordan"], "venue": "J. ACM,", "citeRegEx": "Blei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["Blei", "Ng", "Jordan2003]Blei", "David M", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["Chang et al.2009]Chang", "Jonathan", "Jordan Boyd-Graber", "Chong Wang", "Sean Gerrish", "David M. Blei"], "venue": "In Neural Information Processing Systems", "citeRegEx": "al.2009.Chang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Chang et al\\.", "year": 2009}, {"title": "Modeling general and specific aspects of documents with a probabilistic topic model", "author": ["Chemudugunta", "Smyth", "Steyvers2006]Chemudugunta", "Chaitanya", "Padhraic Smyth", "Mark Steyvers"], "venue": "In NIPS,", "citeRegEx": "Chemudugunta et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chemudugunta et al\\.", "year": 2006}, {"title": "Generalized Probabilistic Topic and Syntax Models for Natural Language Processing", "author": ["M. William"], "venue": "Ph.D. thesis,", "citeRegEx": "William,? \\Q2012\\E", "shortCiteRegEx": "William", "year": 2012}, {"title": "Unsupervised part-of-speech tagging in noisy and esoteric domains with a syntactic-semantic bayesian hmm", "author": ["Darling", "Paul", "Song2012]Darling", "William M", "Michael J. Paul", "Fei Song"], "venue": "In Proceedings of the EACL 2012 Workshop on Semantic Analysis in Social Media", "citeRegEx": "Darling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Darling et al\\.", "year": 2012}, {"title": "Probabilistic document modeling for syntax removal in text summarization", "author": ["Darling", "Song2011]Darling", "William M", "Fei Song"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Darling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Darling et al\\.", "year": 2011}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments", "author": ["Gimpel et al.2011]Gimpel", "Kevin", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "al.2011.Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Gimpel et al\\.", "year": 2011}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["Goldwater", "Griffiths2007]Goldwater", "Sharon", "Tom Griffiths"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Goldwater et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2007}, {"title": "Integrating topics and syntax", "author": ["Griffiths et al.2005]Griffiths", "Thomas L", "Mark Steyvers", "David M. Blei", "Joshua B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al.2005.Griffiths et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al.2005.Griffiths et al\\.", "year": 2005}, {"title": "Overview of the first text retrieval conference (trec-1)", "author": ["Harman1992]Harman", "Donna"], "venue": "In TREC,", "citeRegEx": "Harman1992.Harman and Donna.,? \\Q1992\\E", "shortCiteRegEx": "Harman1992.Harman and Donna.", "year": 1992}, {"title": "The elements of statistical learning: data mining, inference, and prediction: with 200 full-color illustrations", "author": ["Hastie", "Tibshirani", "Friedman2001]Hastie", "Trevor", "Robert Tibshirani", "J.H. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["Hofmann2001]Hofmann", "Thomas"], "venue": null, "citeRegEx": "Hofmann2001.Hofmann and Thomas.,? \\Q2001\\E", "shortCiteRegEx": "Hofmann2001.Hofmann and Thomas.", "year": 2001}, {"title": "Modeling syntactic structures of topics with a nested hmm-lda", "author": ["Jiang2009]Jiang", "Jing"], "venue": "In Proceedings of the 2009 Ninth IEEE International Conference on Data Mining,", "citeRegEx": "Jiang2009.Jiang and Jing.,? \\Q2009\\E", "shortCiteRegEx": "Jiang2009.Jiang and Jing.", "year": 2009}, {"title": "Foundations of Statistical Natural Language Processing. MIT Press, Cambridge (Mass.) and London", "author": ["Manning", "Sch\u00fctze1999]Manning", "Christopher D", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Comparing clusterings\u00e2\u0102\u0164an information based distance", "author": ["M. Meil\u01ce2007]Meil\u01ce"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Meil\u01ce2007.Meil\u01ce,? \\Q2007\\E", "shortCiteRegEx": "Meil\u01ce2007.Meil\u01ce", "year": 2007}, {"title": "Tagging english text with a probabilistic model", "author": ["Merialdo1993]Merialdo", "Bernard"], "venue": "Computational Linguistics,", "citeRegEx": "Merialdo1993.Merialdo and Bernard.,? \\Q1993\\E", "shortCiteRegEx": "Merialdo1993.Merialdo and Bernard.", "year": 1993}, {"title": "Factorial lda: Sparse multi-dimensional text models", "author": ["Paul", "Dredze2012]Paul", "Michael", "Mark Dredze"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Paul et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2012}, {"title": "You are what you tweet: Analyzing twitter for public health", "author": ["Paul", "Dredze2011]Paul", "Michael J", "Mark Dredze"], "venue": "In 5th International AAAI Conference on Weblogs and Social Media (ICWSM", "citeRegEx": "Paul et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2011}, {"title": "A two-dimensional topic-aspect model for discovering multi-faceted topics", "author": ["Paul", "Girju2010]Paul", "Michael J", "Roxana Girju"], "venue": null, "citeRegEx": "Paul et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2010}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["R. Lawrence"], "venue": "Readings in speech recognition", "citeRegEx": "Lawrence,? \\Q1990\\E", "shortCiteRegEx": "Lawrence", "year": 1990}, {"title": "Characterizing microblogs with topic models", "author": ["Ramage", "Dumais", "Liebling2010]Ramage", "Daniel", "Susan Dumais", "Dan Liebling"], "venue": null, "citeRegEx": "Ramage et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2010}, {"title": "Labeled lda: a supervised topic model for credit attribution in multi-labeled corpora", "author": ["Ramage et al.2009]Ramage", "Daniel", "David Hall", "Ramesh Nallapati", "Christopher D. Manning"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1,", "citeRegEx": "al.2009.Ramage et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Ramage et al\\.", "year": 2009}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Ritter", "Cherry", "Dolan2010]Ritter", "Alan", "Colin Cherry", "Bill Dolan"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Introduction to Modern Information Retrieval", "author": ["Salton", "G. McGill1983]Salton", "M. McGill"], "venue": null, "citeRegEx": "Salton et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1983}, {"title": "Contrastive estimation: training log-linear models on unlabeled data", "author": ["Smith", "Eisner2005]Smith", "Noah A", "Jason Eisner"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Rethinking lda: Why priors matter", "author": ["Wallach", "Mimno", "McCallum2009]Wallach", "Hanna", "David Mimno", "Andrew McCallum"], "venue": null, "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Structured Topic Models for Language", "author": ["M. Hanna"], "venue": "Ph.D. thesis,", "citeRegEx": "Hanna,? \\Q2008\\E", "shortCiteRegEx": "Hanna", "year": 2008}, {"title": "Evaluation methods for topic models", "author": ["Wallach et al.2009]Wallach", "Hanna M", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "al.2009.Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Wallach et al\\.", "year": 2009}, {"title": "Term weighting schemes for latent dirichlet allocation", "author": ["Wilson", "Chew2010]Wilson", "Andrew T", "Peter A. Chew"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Wilson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2010}, {"title": "No free lunch theorems for optimization", "author": ["Wolpert", "D.H. Macready1997]Wolpert", "W.G. Macready"], "venue": "Trans. Evol. Comp,", "citeRegEx": "Wolpert et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wolpert et al\\.", "year": 1997}, {"title": "Taglda: Bringing document structure knowledge into topic models", "author": ["Zhu", "Blei", "Lafferty2006]Zhu", "Xiaojin", "David M. Blei", "John Lafferty"], "venue": "Technical Report TR-1553,", "citeRegEx": "Zhu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 28, "context": "There are several methods that are commonly employed to evaluate novel probabilistic models in the literature (Wallach et al. 2009).", "startOffset": 110, "endOffset": 131}], "year": 2013, "abstractText": "This article presents a probabilistic generative model for text based on semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA). POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and long-range semantic patterns (topics) that exist in document collections. This results in word distributions that are specific to both topics (sports, education, ...) and parts-of-speech (nouns, verbs, ...). For example, multinomial distributions over words are uncovered that can be understood as \u201cnouns about weather\u201d or \u201cverbs about law\u201d. We describe the model and an approximate inference algorithm and then demonstrate the quality of the learned topics both qualitatively and quantitatively. Then, we discuss an NLP application where the output of POSLDA can lead to strong improvements in quality: unsupervised partof-speech tagging. We describe algorithms for this task that make use of POSLDA-learned distributions that result in improved performance beyond the state of the art.", "creator": "LaTeX with hyperref package"}}}