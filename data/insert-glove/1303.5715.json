{"id": "1303.5715", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Local Expression Languages for Probabilistic Dependence: a Preliminary Report", "abstract": "combusting We present a kintanar generalization kepov of escartin the stinkers local expression surroundings language suppress used monolinguals in 3-5-2 the Symbolic Probabilistic Inference (SPI) approach polymaths to cross-town inference 19:03 in voice belief o-methylation nets [96-minute 1l, [kupang 8 ]. The local expression hannum language in dayspring SPI is the cutt language toothed in which the labus dependence sunshade of a yzaga node on its kaddish antecedents horizontally-opposed is described. 16-19 The original language imangali represented 4x the mohannad dependence kakhk as salga a single monolithic conditional probability distribution. sherin The chapoy extended govou language provides short-faced a set of jamundi operators (*, +, s\u00f6derstr\u00f6m and -) rouxel which go-karts can be kouma used senga to specify methods arsan for franprix combining partial intifadeh conditional 2504 distributions. As winden one jakubec instance of divan the whitehaven utility of this laccadive extension, we walikale show how 8-point this sheshunoff extended dominion language jcrc can wissam be stevia used to ctla-4 capture the gargoyle semantics, representational cbgb advantages, martella and inferential complexity calichman advantages sumaye of w\u00fcrzburg the \" comeaux noisy frate or \" keylock relationship.", "histories": [["v1", "Wed, 20 Mar 2013 15:30:27 GMT  (463kb)", "http://arxiv.org/abs/1303.5715v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bruce d'ambrosio"], "accepted": false, "id": "1303.5715"}, "pdf": {"name": "1303.5715.pdf", "metadata": {"source": "CRF", "title": "Local Expression Languages for Probabilistic Dependence: a preliminary report", "authors": ["Bruce D'Ambrosio"], "emails": [], "sections": [{"heading": null, "text": "1 Introduction\nA belief net [5] is a compact, localized representation of a probabilistic model. The key to its locality is that, given a graphical structure representing the de p\n_ endencies (and, implicitly, conditional independen cies) among a set of variables, the joint probability distribution over that set can be completely described by specifying the appropriate set of marginal and con ditional distributions over the nodes involved. When the graph is sparse, this will involve a much smaller set of nur\ufffders than the full joint. Equally important, the graphical structure can be used to guide process ing to find efficient ways to evaluate queries against the model. For more details, see [5], [7], [1]. All is not as rosy at it might seem, though. The graphical level is not capable of representing all interesting structural information which might simplify representation or in ference. The only mechanism available for describing antecedent interactions in typical general purpose be lief net inference algorithms is the full conditional dis-\ntribution across all antecedents. However, a number of restricted interaction models have been identified which have lower space and time complexity than the full conditional. The noisy-or [5], [6J, [4] for example, can be used to model independent causes of an event, and is linear in both space and time in the number of antecedents. In this paper we show an extension to the local expression language used in Symbolic Probabilis tic Inference (SPI) [8] which is capable of directly ex pressing a noisy-or interaction model, which captures both the space and time advantages of the model and which permits use of the model within arbitrary belief nets. In the remainder of this paper we first present a very brief overview of SPI. We then present an exten sion to the representation used to describe the depen dence of a node on its antecedents in SPI, and show how it can be used to capture the noisy-or relation ship. We then discuss the nature of the changes which must be made to support this extended local expres sion language. A key issue is the determination of how to distribute conformal product operations over addi tion and subtraction. We close with some remaining questions.\n2 Overview of SPI\nIn this section we briefly review the essential aspects of the SPI approach to inference in belief nets. For further details, see [1] or [8].\n2.1 Overview\nComputation of probabilities in a belief net can be done quite straightforwardly, albeit somewhat inefficiently1. I illustrate this process with a simple network, shown in figure 1. F irst, the prior probabili ties according to the chain rule:\np(A) = p(A) p(B) = EA p(BIA) * p(A)\n1we ignore evidence for purposes of this introduction. It introduces only minor complications, see [1], [8] for details.\n96 D'Ambrosio\nFigure 1: A Simple Belief Net\np(C) =LAP( CIA)* p(A) p(D) =LAB c E pp(DIB,C, F)* p(FIE) * p(E) '* p(CIA) * p(BIA) * p(A) p(E) = p(E) p(F) = LEP(FJE) * p(E) Now suppose we wish to compute p( D). The procedure is quite simple. We begin by evaluating the above ex pression for p(D) from right to left. Once all distribu tions are combined, we have computed the joint across all six variables, and can derive the marginal over D by summing over all other variables. The actual computa tion can be optimized somewhat by retaining each di mension only until we have combined with all terms in which the dimension appears (that dimension is a goal of the evaluation, in which case it must be retained throughout the computation). For example, we can sum over A, since it is not needed in the final result, immediately after combining with p( BIA) and p( C lA). Conjunctive queries are easily computed, simply by evaluating the union of the symbolic expressions for the corresponding nodes. SPI essentially follows this process, but can be viewed as a heuristic procedure for developing factorings which minimize the dimension of intermediate results. The factoring is developed incre mentally, and factoring is intermixed with expression evaluation. We briefly sketch the process used in the following.\nSPI uses a more compact representation, expressing each node marginal only in terms of its conditional dis tribution and (implicitly) the immediate antecedents needed for computation. For our sample belief net this yields the following expressions:\nexp(A) = p(A) exp(B) = p(BJA) exp(C) = p(CIA) exp(D) = p(D]BCF) exp(E) p(E) exp(F) p(FlE)\nIt may not be obvious how we can reconstruct the earlier computations from this representation. I will describe the evaluation algorithm shortly.\nThe next component of the representation is a parti tioning of the set of nodes. The partitions are arranged in a tree subject to constraints as described in [8], al though note that we permit partitions to contain more than one node in this paper. One valid partition of the example belief-net is shown in fig. 22.\nNow consider how we might compute p(D), given this information. The marginal probability of any node can be computed by multiplying the probability ex pression for the node of interest by distributions across sets of antecedents, subject to the following decompo sition constraint: a joint distribution must be com puted for antecedents whose corresponding nodes lie in a subtree rooted by the same child partition. Thus, the partitioning describes how to factor any \ufffdxpres sion at each stage of computation. The expression for D in the root partition, for example, will be decom posed into three groups, {p(DlBC F)}, {p(BC)}, and {p(F)P. p(DlBCF) is known, p(BC)\n. and p(F) will\nbe computed by querying the appropnate ch1ld par titions. The identification of the two subqueries that can be processed independently (p(BC) and p(F)) is central to the efficiency of SPI. See [8] for proofs of the\n2The root of the partition tree need not contain only belief net leaf nodes and subtrees need uot contain only antecedent nodes. We present here what we consider the minima.! description of SPI needed to understand the ex tensions described in this paper.\n3It should be noted that the value returned from a query to a. child partition will be a distribution conditioned on a.ll evidence in the subtree rooted by that child, but not con ditioned on evidence elsewhere in the partition tree. At tempting to distinguish the various states of conditioning would clutter the representation, so we will not attempt to indicate the set of evidence a distribution has been condi tioned with respect to in this paper.\nLocal Expression Languages for Probabilistic Dependence 97\nproperties which make this possible.\nSince the partition graph is a tree, the recursion will terminate (and be evaluable, since all leaf node marginals are defined in the original belief net).\nBelow I detail this process for evaluating the marginal probability p( D):\n1. In the root partition, determine the expansion for p(D):\nexp(D) = p(DIBCF)\n2. The following child partition queries are formed according to the antecedent set decomposition cri terion:\n\u2022 Query to Cl: p(BC)? \u2022 Query to C2: p(F)?\n3. C1 expands p(BC) to:\np(BIA)p(CIA)\n4. This in turn generates a query to C3: p(A)?\n5. C3 returns p(A).\n6. C1 evaluates I:;A p(BIA)p(CIA)p(A) and returns p(BC).\n7. C2 expands p(F) to: p(F IE).\n8. This in turn generates the query to C4: p(E)?\n9. C4 returns p(E).\n10. C3 evaluates EEP(FIE)p(E) and returns p(F).\n11. Root evaluates EBcFP(DIBCF)p(BC)p(F) and returns p(D).\nThis simple example demonstrates a key feature of the algorithm: each partition deals with a low dimensional subspace of the overall probability space. While six variables are involved, the factoring keeps the dimen sionality of intermediate computations down to four. We have made several simplifications in this presen tation: we consider only partition trees with all belief net root nodes in partition tree leaves, and we do not consider evidence. See the cited papers for a more complete treatment of the basic algorithm.\n3 Local Expression Languages for Probabilistic Knowledge\nIn this section we extend the local representation in SPI. This extended expression language is useful for compact representation of a number of canonical in teraction models among antecedents. In particular, we demonstrate its use in capturing the noisy-or model. The next section describes the extensions needed in the SPI evaluation algorithm.\nFigure 3: Noisy Or Sample Net\nThe local expression attached to a node in SPI as de scribed in the previous section is a particularly simple one: it is either a marginal or conditional probabil ity distribution. While this representation is complete (that is, is capable of expressing any coherent prob ability model), it suffers from both space and time complexity limitations: both the space and time re quired are exponential in the number of antecedents. However, computation of child probabilities using the noisy-or interaction model is linear in the number of (independent) antecedents in both space and time. When evidence is available on child nodes, computa tion of the posterior probability of parents is expo nential in the number of positive pieces of evidence, but linear in the number of pieces of negative evi dence. Heckerman [3] has developed an algorithm, called Quickscore, which provides this efficiency for two level bipartite graphs. However, the author is un aware of any implemented system other than the one reported here which can efficiently incorporate a noisy or within an arbitrary belief net. If the interaction be tween the effects of A and B on D in example 3 can be modelled as a noisy-or interaction, then we might write the following expression for the dependence of D on A and B, following Pearl [5]:\nP(D = t) F(D = j)\n1- (1- cA(D))(l- cB(D)) (1- CA(D))(1- CB(D))\nWhere cA(D) is the probability that D is true given that A is true and B is false. We use c rather than p to emphasize that these are not standard conditional probabilities. Nonetheless, in the following we will show that we can compute with these distribution.st using an extension to the same mechanisms already in SPI. There are three components of SPI that must be extended:\n1. The expression language must be extended to per mit direct encoding of those aspects of interaction\n\u2022 As will be noted later, all elements being combined are represented as generalized distributions over some domain. In the case of cA(D), the domain is the singleton A = t, D = t, and A is an antecedent.\n98 D'Ambrosio\nmodels which provide space or time advantages.\n2. The symbolic composition operator used to com bine local expressions must be extended. This op erator previously had one task: to rearrange con formal products of expressions into an efficiently evaluable form, using the commutativity and as sociativity of the operation. For extended expres sions we have distributivity of conformal product over addition and subtraction as an option as well as commutativity and associativity.\n3. The numeric evaluation procedure must be ex tended. First, we must define semantics for the addition and subtraction operators. Second, we must decide how completely to evaluate. The al gorithm presented in [8] presumed that it was al ways appropriate to reduce an expression to the joint distribution over the variables needed in the result. For example, given\nLF(DIB,C)F(B)F(C) B\nnumeric evaluation will yield F ( D, B) rather than F(DIB)F (B). While we believe this to be an op timal choice, the situation is less clear for the extended local expression language. Should the goal of expression evaluation always to be return a joint across the target variables, or are there cases in which it is better to return a partially evaluated expression?\nIn addition, computational complexity of both sym bolic and numeric evaluation stages must remain lin ear, or at least low order polynomial, in the num ber of independent antecedents (otherwise what is the . ') pomt .. In the following we first present a description of the local expression language we have developed. We then proceed to describe the evaluation of queries refer encing nodes whose probabilistic dependency on other nodes is defined using expressions from this language. Not all expressions in this language represent coher ent probabilistic relationships. We presume that the user starts with a well understood interaction model and simply needs a computational framework that can perform inference with that model.\n3.1 BNF for a simple expression language\nWe present below the BNF for a simple expression language capable of representing noisy-or and a variety of other special-case interaction models:\narithmetic-exp -+ term I (+term term-set) -+ I (- term term-set) -+ I (* term term-set)\nterm -+ arithmetic-exp I distribution. term-set -+ term I term term-set. distribution -+ namedimensions\u00b7 dimensions -+ conditioned \"I\" conditioning. conditioned -+ node-name domain\n-+ node-namedomainconditioned. conditioning -+ node-name domain\n-+ node-name domain I conditioning. domain -+ I value I {value-set}.\nvalue-set -+ value. I value value-set.\nNotice that every term eventually must reduce to one or more distributions defined over some domain. As an example, the local expressions for D and E for our sample noisy-or figure are as follows:\nexp(D)\nexp(E)\nlv,- (lv,- cv,JA,) * (lv,- cv,IB,) +(1v1- cv,IA,) * (lv1- cv,IB,)\nlE,- (lE,- CE,IB,) * (lE,- CE,IC,) +(1E1- CEJIB,) * (1E1- CE1!C,)\nThe above notation may seem a bit obscure. It is perhaps further obscured by the fact that the actual numbers are not represented. The notation lv, de notes a distribution named\"!,\" which is defined over the subspace of the joint probability distribution for the network for which node D holds the value t. This distribution contains the single value 1.0 (the actual value is not a necessary consequent of the above no tation, but it is convenient to give constants names which correspond to their values.). The expression for D, then, can be read as a straightforward recoding of the noisy-or model. It specifies that the distribution for D can be computed as the sum of two components. The first component computes a value for F(D = t), and the second, on the next line, computes the value for P(D = j). Since these two terms are mutually exclusive (as is obvious in this case, since they are de fined over disjoint elements of the domain of D), they can be combined using simple addition. We specify the following semantics for the operators:\n* Conformal product. We assign the same semantics as for standard SPI. When combining distribu tions defined over differing subspaces of the do main for a node, only those values in the domain for which both distributions are defined need be considered. That is, distributions are implicitly extended with 0.0 in all values for which they are not defined. Thus, lvT can be seen to specify the distribution {1.0,0.0} over {D = t,D = f}.\n+ /- sum/ difference. Simple sum or difference of the two terms. This assumes that the terms being combined are mutually exclusive. As before, dis-\nLocal Expression Languages for Probabilistic Dependence 99\ntributions are extended with zeros for values in the domain over which they are not defined. How ever, note the following interesting case: how do we compute 1D, -cD,[A,? The first distribution is defined over D, but the second is defined over D conditioned on A. Unlike the conformal product case, we cannot directly add or subtract distribu tions over non-identical sets of variables. We first \"normalize\" the domains by multiplying both by p(A). Supposep(A) = {.1,.9}, and cD,[A, = .7. Then the above computation would yield:\nA-t A-f A-t \ufffd-f A_;; A-f I \ufffd/A I 1D, I cD,IA I \ufffd esult\nI -t .1 .9 .07 .0 .0 .9 The above is strictly necessary only if A is needed in the result. If it is not, the alternative is to mul tiply cDdA, by p(A) and then sum over A before subtracting it from 1 D,.\nNote that cD,IA, and cD,IAt represent the same noisy or parameter. Two copies of this parameter are needed in our current expression language to denote its par ticipation in the computation for D = t and in the computation for D = j. We have not had time to investigate ways to eliminate this redundancy. There are doubtless other notations that would serve equally well. The above notation is the one used in our imple mentation, and serves to una\u00aeiguously specify the subspace over which each term is defined.\n3.2 Symbolic composition of extended local expressions\nIn general, query evaluation in each partition consists of three stages, each of which will require modification. The three stages are:\n1. Composition of local expressions for all partition nodes involved in the query.\n2. Generation of subqueries to each child partition from which information is needed.\n3. Numerical evaluation of the results.\nIn this section we discuss the first of these points, the symbolic composition, and present an algorithm for distributing conformal product over addition and sub traction. This algorithm yields an efficiently evaluable expression under the following restrictions:\n1. There are many ways to construct valid partition ing of nodes. The way we currently use, and which we assume here, is to construct a tree with belief net root nodes at the leaves, and with multiple nodes in a partition where permissible under the partitioning constraints. One such partitioning algorithm is described in [1], and the partition tree constructed by that algorithm is shown in figure 4.\n2. Queries to child partitions always return a single joint distribution across query nodes.\nWe will discuss later research in progress on relaxing these restrictions. The algorithm for distribution of conformal product over addition and subtraction be gins with the outermost expression, and is as follows:\n1. If the operation in the current expression is a con formal product, then\n(a) group terms with overlapping sets of child partitions from which information is needed.\n(b) distribute conformal product one level down in each group, over those terms in the group either separable into subterms which need an tecedents from disjoint subtrees, or which re quire information from only a subset of the child partition set associated with the group.\n(c) repeat this step on the rewritten expression if the operation is still a conformal product. (can occur when terms are themselves con formal products)\n2. Recursively apply step 1 to each term if the cur rent expression is a result of performing a distri bution operation.\nThis procedure assumes that the expressions being combined are initially in efficiently evaluable form, as are ezp(D) and ezp(E) above.\nFigure 4: Noisy Or Sample Net Partition\nAJ; a simple example, consider the evaluation of the query for p(D, E) in the net shown in Fig.3, where we use the noisy-or model for {A, B} and for { B, C} (since the queries to child partitions for p( A) through p(C) are trivial, we will ignore them and concentrate on processing in the partitions containing D and E). We concentrate on the processing in the root partition.\nQuery to root: p(D,E)\n100 D'Ambrosio\nComposition of local expressions for query nodes:\np(D , E) = ((lv,- (lv,- cvo\ufffdA,) * lv,- cv,,B.)) +(lv1 - cv,IA,) * (lv,- cv,IB,))\n*((ls,- (ls,- cEtiB,) * (ls,- cs,,c,))\n+(is, - cE,IB,) * (lE, * cs11c,))\nApplying the distribution procedure to the above ex pression for p(D, E) yields, at the first step (for sake of space we list only the terms involving Dt and Et): p(Dt, Et) = (lv, - (lv, - cvtiA,) * (lv, - cv,,B,))\n*(lEt- (let- cs,IB,) * (ls,- cs,,c,))\nSince the top level conformal product in this result contains terms which are separable, it is distributed over those terms. This yields:\n((lv, * 1E,) + ((lv, - CDtiA,) * (lv, - CDt/B,)) *((ls, - cs,IB,) * (ls, - cE,Ic,)))\n-((ls, * ((lv,- cv,,A,) * (lv,- cv,IB,))) +(lv, * ((ls,- cE,IB,) * (let- cs,,c,))))\nThere are still conformal products which have not been fully distributed. Only one of these, however, contains terms which group together. Distributing that confor mal product one level deeper yields:\n((lv, * ls,) + ((1v,- cvtiA,) * (lv, - cv,,B,) *(ls,- cs,,B,) * (ls,- cs,lc,))) -((lE, * ((lv,- CDtiA,) * (lv, - CDtiB,))) +(lv, * ((ls,- cE,IB,) * (lEt- cs,,c,))))\nWe are done. We can now apply commutativity and associativity to yield the final evaluation form:\n((lv, * ls,) + ((lv,- cv,IA,) *((lv,- cv,IB,) * (ls,- cEtiB,)) *(ls, - cstlc,)))\n-((ls, * ((lv,- cDtiA,) * (lv, - cv,IB,))) +(1v, * ((lE,- cE,IB,) * (lEt- cE,Ic,))))\nNone of the conformal products in this final expression meet the criteria for distribution, so we are done, and the expression can be evaluated according to normal SPI methods (the standard SPI local ordering heuris tic will group terms appropriately for efficient evalu ation, see (lj. The distribution procedure is efficient, and performs well (although not perfectly) at gener ating an expression which can be efficiently evaluated. We discuss each of these following description of the remaining processing needed.\n3.3 Subquery Generation:\nIt should be clear that during the above process the information needed from each child partition has al ready been identified. Subquery processing proceeds as in standard SPI. It may seem at first that some sav ings could be achieved by not computing the full dis tribution for a node, but only the distribution over the\nreferenced subrange. However, at this time we always generate sub-queries for the full distribution across a node.\n3.4 Expression Evaluation:\nWe have already discussed the semantics of the opera tors in the local expression language. The only remain ing issue is whether evaluation should always reduce an expression to a single joint distribution across the desired set of result variables or stop short of com plete evaluation. Our current implementation always reduces expressions to a single joint distribution.\nDistribution Procedure Complexity The distri bution procedure includes the following steps:\n\u2022 Grouping of terms - this can be done in O(nm) time, where n is the number of distributions in the expression, and m is the number of child par titions.\n\u2022 \"Separable\" test - this can be done inO(nm) time. \u2022 Repetition of these two steps can occur up to\nO(ln) times, where 1 is the length of a node ex pression, and n is the number of node expressions being composed. For combination of noisy-or ex pressions as shown above, the overall time will be exponential in the number of expressions being combined which share antecedents.\nEvaluation Complexity For noisy-or the above procedure preserves the property that the complexity of numeric evaluation is linear in the number of in dependent antecedents, since independent antecedents will reside in disjoint subtrees below the partition con taining the expression being evaluated. As a result, terms referencing them will not group together, and therefore will not force distribution of the conformal product operator. This property is satisfied for the individual noisy or expressions for D and E. How ever, is not true for the initial composition of the local expressions for D and E. Correct evaluation of that expression would require that the summation over B be delayed until the subexpressions for D and E had been evaluated and combined. In general, the compu tation would be exponential in both space and time in the number of shared antecedents. By distributing the conformal product using the algorithm specified above, we reduce the space and time complexity of evalua tion of the final expression to linear in the number of shared antecedents. The price we pay, however, is that the expression size, and therefore also evaluation com plexity, becomes exponential in the number of nodes being combined. An alternate distribution heuristic might weigh more carefully the costs and benefits of distribution. A few further notes:\n1. The \"separable\" criterion is not perfect. Con sider, for example, the expression ( (p( D IB, C) +\nLocal Expression Languages for Probabilistic Dependence 101\np(EjA, B))*(p(jjA,C)+p(GjB, C))) where A, B, and C reside in disjoint partition subtrees. Ac cording to our current test, both terms are sep arable, since no single distribution requires all of the partitions needed to evaluate an entire term. Nonetheless, distributing the conformal product does not yield a more efficiently evaluable expres sion.\n2. It is not always possible to distribute conformal product operators down to a level which permits independent evaluation of each term (for example, terms might be distributions with overlapping sets of antecedents). In this case the same local eval uation heuristic used in [ 1 J is used to group terms and sequence evaluation.\n3. Full distribution of * over + and - would not permit efficient evaluation. Were we to fully dis tribute conformal product, the result would be correct, but we would need to evaluate a num ber of terms exponential in both the number of antecedents and the number of nodes.\n4. Consideration of the example we presented should make it clear that the algorithm reproduces the essential results of Quickscore when applied to two level bipartite (BN20) graphs: numeric evalua tion is linear in the number of antecedents, linear in the number of negative findings, and exponen tial in the number of positive findings.\n4 Discussion\nThe above procedure is not optimal. It is, however, correct, and therefore provides a method for perform ing inference using standard interaction models such as noisy-or within SPI. Further, it correctly handles non independence of antecedents. A review of the example above will reveal that the identification and grouping of the terms involving B did not in any way depend on either the fact that both terms named the same node (B), nor that the terms carne from separate lo cal expressions. Similar grouping and distribution of the conformal product operator would have occurred in processing a query for p(D) if A and B were in the same partition subtree below D. We have therefore presented a general method for evaluating arbitrary belief nets which contain noisy or models of antecedent interaction.\nNoisy-or is traditionally considered to be of restricted applicability since standard presentations restrict to the case where all nodes take only two values. How ever, there is a straightforward generalization to the multi valued case which requires ( v - 1 )2 parameters for each antecedent, where v is the number of values a variable can take. The methods presented here sup port this generalization as well as the simple two-value case.\nThe work presented here is far from complete. Two major extensions are needed to provide efficient sup port for the local expression language we describe. First, we must extend the distribution heuristic to cover the case where child partitions contain conse quent (child) nodes. We believe this to be a minor ex tension. More difficult is the question of whether it is always appropriate to reduce an expression to a single joint distribution over the query nodes when perform ing numeric evaluation. In general we have no reason to believe this is the case. The general problem being solved is to find a factoring of the global expression for the query, as described in [1]. The partition tree indicates how to decompose queries and when nodes can be summed over, but contains little further infor mation to guide evaluation. We are therefore investi gating techniques which delay expression reduction as long as possible, only performing in each partition the evaluation necessary to perform summing over nodes not needed higher in the tree.\nAlso, we do not consider the local expression language to be complete. We have begun to explore further ex tensions to the local expression language. For example, we are pursuing, in conjunction with R. Fung and R. Shachter, the use of a CASE statement to represent contingencies in belief nets [9].\nWe began our exploration of probabilistic inference in the context of truth maintenance systems, and at that time used symbolic representation at the level of indi vidual probability mass elements (2]. Later, motivated by efficiency concerns, we changed to a symbolic rep resentation at the distribution level [8]. We now seem to have come full circle: the implementation described here again performs symbolic reasoning on elements as small as individual probabilities. The difference is that we now have a choice of representation grain-size, and can select the grain-size appropriate for the de pendence model being described.\n5 Conclusion\nBelief nets are a compact, intuitive representation for general probabilistic models, but suffer from inabil ity to efficiently represent low level structural details such as asymmetries and noisy-or relationships. We have shown how the SPI framework can be extended to support a wide class of antecedent interaction models. This permits free use of these models within an arbi trary belief net, and provides efficient processing of ar bitrary marginal and conditional queries on the result ing belief net. This facility also provides for easy ex perimentation on new interaction models, since there is no need to write code to perform inference using the new model: one directly describes the interaction us ing a simple algebraic local expression language. The full expression language has been implemented and is in use at Intel Corp. in a chip fabrication process di-\n102 D'Ambrosio\nagnosis project.\nAcknowledgements"}, {"heading": "T hanks to Bob Fung and Peter Raulefs for many useful discussions. T hanks to NSF (IRI88-21660) for provid ing the support w hich made this work possible.", "text": "References\n[1] B. D'Ambrosio. Symbolic probabilistic inference. Technical report, CS Dept., Oregon State Univer sity, 1989.\n[2] B. D'Ambrosio. Incremental evaluation and con struction of defeasible probabilistic models. In ternational Journal of Approximate Reasoning, To Appear 1990.\n[3] D. Heckerman. A tractable inference algorithm for diagnosing multiple diseases\u00b7 In Proceedings of the Fifth Conference on Uncertainty in AI, pages 17 4- 181, August 1989.\n[4] M. Henrion. Towards efficient probabilistic diag nosis with a very large knowledge-base. In AAAI Workshop on the Principles of Diagnosis, 1990.\n[5] J. Pearl. Probabilistic Reasoning in Intelligent Sys tems. Morgan Kaufmann, Palo Alto, 1988.\n[6] Y. Peng and J. Reggia. A probabilistic causal model for diagnostic problem solving - part 1: In tegrating symbolic causal inference with numeric probabilistic inference. IEEE Trans. on Systems, Man, and Cybernetics: special issue on diagnosis, SMC-17(2):146-162, 1987.\n[7] R. Shachter. Evaluating influence diagrams. Op emtions Research, 34( 6):871 - 882, November December 1986.\n[8] R. Shachter, B. D'Ambrosio, and B. DelFavero. Symbolic probabilistic inference in belief networks. In Proceedings Eighth National Conference on AI, pages 126-131. AAAI, August 1990.\n[9] R. Shachter and R. Fung. Contingent influence di agrams. Tech report, Dept. of Engineering Eco nomic Systems, Stanford University, September 1990. In preparation."}], "references": [{"title": "Symbolic probabilistic inference", "author": ["B. D'Ambrosio"], "venue": "Technical report, CS Dept., Oregon State Univer\u00ad sity,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Incremental evaluation and con\u00ad struction of defeasible probabilistic models", "author": ["B. D'Ambrosio"], "venue": "In\u00ad ternational Journal of Approximate Reasoning, To Appear", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["D. Heckerman"], "venue": "In Proceedings of the Fifth Conference on Uncertainty in AI, pages", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1989}, {"title": "Towards efficient probabilistic diag\u00ad nosis with a very large knowledge-base", "author": ["M. Henrion"], "venue": "In AAAI Workshop on the Principles of Diagnosis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Probabilistic Reasoning in Intelligent Sys\u00ad tems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "A probabilistic causal model for diagnostic problem solving - part 1: In\u00ad tegrating symbolic causal inference with numeric probabilistic inference", "author": ["Y. Peng", "J. Reggia"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics: special issue on diagnosis,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1987}, {"title": "Evaluating influence diagrams", "author": ["R. Shachter"], "venue": "Op\u00ad emtions Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1986}, {"title": "Symbolic probabilistic inference in belief networks", "author": ["R. Shachter", "B. D'Ambrosio", "B. DelFavero"], "venue": "In Proceedings Eighth National Conference on AI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Contingent influence di\u00ad agrams", "author": ["R. Shachter", "R. Fung"], "venue": "Tech report, Dept. of Engineering Eco\u00ad nomic Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "We present a generalization of the local ex\u00ad pression language used in the Symbolic Prob\u00ad abilistic Inference (SPI) approach to infer\u00ad ence in belief nets [1], [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 7, "context": "We present a generalization of the local ex\u00ad pression language used in the Symbolic Prob\u00ad abilistic Inference (SPI) approach to infer\u00ad ence in belief nets [1], [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 4, "context": "A belief net [5] is a compact, localized representation of a probabilistic model.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "For more details, see [5], [7], [1].", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "For more details, see [5], [7], [1].", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "For more details, see [5], [7], [1].", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "The noisy-or [5], [6J, [4] for example, can be used to model independent causes of an event, and is linear in both space and time in the number of antecedents.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "The noisy-or [5], [6J, [4] for example, can be used to model independent causes of an event, and is linear in both space and time in the number of antecedents.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "In this paper we show an extension to the local expression language used in Symbolic Probabilis\u00ad tic Inference (SPI) [8] which is capable of directly ex\u00ad pressing a noisy-or interaction model, which captures both the space and time advantages of the model and which permits use of the model within arbitrary belief nets.", "startOffset": 117, "endOffset": 120}, {"referenceID": 0, "context": "For further details, see [1] or [8].", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "For further details, see [1] or [8].", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "It introduces only minor complications, see [1], [8] for details.", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "It introduces only minor complications, see [1], [8] for details.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "The partitions are arranged in a tree subject to constraints as described in [8], al\u00ad though note that we permit partitions to contain more than one node in this paper.", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "See [8] for proofs of the", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "Heckerman [3] has developed an algorithm, called Quickscore, which provides this efficiency for two level bipartite graphs.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "If the interaction be\u00ad tween the effects of A and B on D in example 3 can be modelled as a noisy-or interaction, then we might write the following expression for the dependence of D on A and B, following Pearl [5]:", "startOffset": 210, "endOffset": 213}, {"referenceID": 7, "context": "The al\u00ad gorithm presented in [8] presumed that it was al\u00ad ways appropriate to reduce an expression to the joint distribution over the variables needed in the result.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "One such partitioning algorithm is described in [1], and the partition tree constructed by that algorithm is shown in figure 4.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "The general problem being solved is to find a factoring of the global expression for the query, as described in [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "Shachter, the use of a CASE statement to represent contingencies in belief nets [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "Later, motivated by efficiency concerns, we changed to a symbolic rep\u00ad resentation at the distribution level [8].", "startOffset": 109, "endOffset": 112}], "year": 2011, "abstractText": "We present a generalization of the local ex\u00ad pression language used in the Symbolic Prob\u00ad abilistic Inference (SPI) approach to infer\u00ad ence in belief nets [1], [8]. The local expres\u00ad sion language in SPI is the language in which the dependence of a node on its antecedents is described. The original language represented the dependence as a single monolithic condi\u00ad tional probability distribution. The extended language provides a set of operators (*, +, and -) which can be used to specify meth\u00ad ods for combining partial conditional distri\u00ad butions. AJJ one instance of the utility of this extension, we show how this extended lan\u00ad guage can be used to capture the semantics, representational advantages, and inferential complexity advantages of the \"noisy or\" re\u00ad lationship.", "creator": "pdftk 1.41 - www.pdftk.com"}}}