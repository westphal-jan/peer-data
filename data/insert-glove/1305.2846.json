{"id": "1305.2846", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2013", "title": "Opportunities & Challenges In Automatic Speech Recognition", "abstract": "Automatic wangled speech 3.8-liter recognition necesitan enables samia a confessin wide range harley of gigante current and silda emerging applications such flac\u0103ra as automatic papagos transcription, multimedia reconsider content analysis, and spokewoman natural vexes human - computer interfaces. chaplin This paper provides communitarian a aliker glimpse of the opportunities and paramhans challenges estabilidad that provider parallelism praetorians provides for automatic dogrib speech recognition kolesnikova and ikegami related clavicle application research from pirita the point of view hideyoshi of speech secord researchers. jupe The tpms increasing mishka parallelism haseltine in 35.11 computing .210 platforms opens niazi three major possibilities 70-man for speech frightens recognition 25.07 systems: hawkishness improving firstfed recognition accuracy in kiera non - mandir ideal, matouq everyday neurotic noisy gunbuster environments; miqati increasing recognition throughput gazidede in kloten batch guerrini processing of quedan speech data; 2001/2 and reducing recognition 0350 latency monavie in grammis realtime usage scenarios. taniec This ohlone paper tanco describes 4.34 technical vingaard challenges, approaches taken, and possible directions for shaozhong future bcf research to guide llega the aretino design panmunjon of efficient paracelsus parallel perls software and hardware infrastructures.", "histories": [["v1", "Thu, 9 May 2013 08:42:26 GMT  (66kb)", "http://arxiv.org/abs/1305.2846v1", "Pages: 05 Figures : 01 Proceedings of the International Conference BEATS 2010, NIT Jalandhar, INDIA"]], "COMMENTS": "Pages: 05 Figures : 01 Proceedings of the International Conference BEATS 2010, NIT Jalandhar, INDIA", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["rashmi makhijani", "urmila shrawankar", "v m thakare"], "accepted": false, "id": "1305.2846"}, "pdf": {"name": "1305.2846.pdf", "metadata": {"source": "CRF", "title": "Opportunities and Challenges in Automatic Speech Recognition", "authors": ["Rashmi Makhijani", "Urmila Shrawankar", "V. M. Thakare"], "emails": [], "sections": [{"heading": null, "text": "Applications in today\u2019s world can no longer rely on significant increases in processor clock rate for performance improvements, as clock rate is now limited by factors such as power dissipation [4]. Rather, parallel scalability (the ability for an application to efficiently utilize an increasing number of processing elements) is now required for software to obtain sustained performance improvements on successive generations of processors. Automatic Speech Recognition (ASR) is an application that consistently exploits advances in computation capabilities. With the availability of a new generation of highly parallel single-chip computation platforms, ASR researchers are faced with the question of unlimited computing to make speech recognition better. The goal of the work reported here is to explore plausible approaches to improve ASR in three ways:\n1. Improve Accuracy: Account for noisy and reverberant environments in which current systems perform poorly, thereby increasing the range of scenarios where speech technology can be an effective solution.\n2. Improve Throughput: Allow batch processing of the speech recognition task to execute as efficiently as possible,\nthereby increasing the utility for multimedia search and retrieval.\n3. Improve Latency: Allow speech-based applications, such as speech-to-speech translation, to achieve real-time performance, where speech recognition is just one component of the application. This paper discusses current work as well as opportunities and challenges in these areas with regard to parallelization from the point of view of speech researchers.\n2. Improving Accuracy\nSpeech recognition systems can be sufficiently accurate when trained with enough data having similar characteristics to the test conditions. However, there still remain many circumstances in which recognition accuracy is quite poor. These include moderately to seriously noisy or reverberant noise conditions, and any variability between training and recognition conditions with respect to channel and speaker characteristics (such as style, emotion, topic, accent, and language). One approach that is both \u201cembarrassingly\u201d parallel and effective in improving ASR robustness is the socalled multistream approach. As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks. And as noted in [23], recent results have demonstrated that a larger number of feature representations can be particularly effective in the case of noisy speech. In order to conduct research on a massively parallel front end, a large feature space is desired. One approach that found to be useful is to compute spectro-temporal features. These features correspond to the output of filters that are tuned to certain rates of change in the time and frequency dimensions.\nVarious approaches have been devised to combine and select the inherently large number of potential spectrotemporal features because processing them entirely is currently considered computationally intractable."}, {"heading": "2.1 Current Approach", "text": "Current preferred approach to robust feature extraction\nis to generate many feature streams with different spectrotemporal properties. For instance, some streams might be more sensitive to speech that varies at a slow syllabic rate (e.g., 2 per second) and others might be more sensitive to\nsignals that vary at a higher rate (such as 6 syllables per second). The streams are processed by neural networks (MultiLayer Perceptrons, or MLPs) trained for discrimination between phones and generate estimates of posterior phone probability distributions.\nFor MLP-based feature streams, the most common combining techniques are: (1) appending all features to a single stream; (2) combining posterior distributions by a product rule, with or without scaling; (3) combining posterior distributions by an additive rule, with or without scaling; and (4) combining posterior distributions by another MLP, which may also use other features. Current best approach to combination is to train an additional Neural Network to generate combination weights by incorporating entropies from the streams as well as overall spectral information. A 28- stream system is used, including 16 streams from division of temporal modulation frequencies, 8 streams from division by spectral modulation frequencies, and 4 streams from a division by both [23]. Using this method, for the Numbers 95 corpus with the Aurora noises added [12] the average word error rate was 8.1%, reduced from 15.3% for MFCCs and first and second order time derivatives. While robustness to environmental acoustics is the main focus, four equally weighted streams, with quasi-tonotopically divided spectro-temporal features were used. The system yielded a 13.3% relative improvement on the baseline, lowering word error rate from 25.5% to 22.1%."}, {"heading": "2.2 Future Directions", "text": "In the current approach, the same modulation filters are\napplied to the entire spectrum. Within this one feature stream, a pipe-and-filter parallel pattern can be used to distribute work across processing elements. Since the MLPs used within the stream depend on dense linear algebra, the wealth of methods to parallelize matrix operations can be exploited. The 28 streams can also be potentially expanded to hundreds or thousands of streams by applying the Gabor filters to different parts of the spectrum as separate streams using a map-reduce parallel pattern. These techniques will be even more important to analyze speech from distant microphones at meetings, a task that naturally provides challenges due to noise and reverberation. Finally, there will be more parallelization considerations in combining the many stream methods with conventional approaches to noise robustness. As many stream feature combination naturally adapt to parallel computing architectures, the improvement will be significant.\n3. Improving Throughput\nBatch speech transcription can be \u201cembarrassingly parallel\u201d by distributing different speech utterances to different machines. However, there is significant value in improving compute efficiency, which is increasingly relevant in today\u2019s energy limited and form-factor limited devices and compute facilities.\nThe many components of an ASR system can be partitioned into a feature extractor and an inference engine. The speech feature extractor collects feature vectors from input audio waveforms using a sequence of signal processing steps in a data flow framework. Many levels of parallelism can be exploited within a step, as well as across steps, as described in section 2.1. Thus feature extraction is highly scalable with respect to the parallel platform advances. However, parallelizing the inference engine requires surmounting significant challenges. The inference engine traverses a graph-based recognition network based on the Viterbi search algorithm [17] and infers the most likely word sequence based on the extracted speech features and the recognition network. In a typical recognition process, there are significant parallelization challenges in concurrently evaluating thousands of alternative interpretations of a speech utterance to find the most likely interpretation. The traversal is conducted over an irregular graph-based knowledge network and is controlled by a sequence of audio features known only at run time. Furthermore, the data working set changes dynamically during the traversal process and the algorithm requires frequent communication between concurrent tasks. These problem characteristics lead to unpredictable memory accesses and poor data locality and cause significant challenges in load balancing and efficient synchronization between processor cores. There have been many attempts to parallelize speech recognition on emerging platforms, leveraging both fine grained and coarse-grained concurrency in the application.\nFine-grained concurrency was mapped onto the multiprocessor with distributed memory in [20]. The implementation statically mapped a carefully partitioned recognition network onto the multiprocessors to minimize load imbalance. [14] explored coarse-grained concurrency in speech recognition and implemented a pipeline of tasks on a cellphone-oriented multicore architecture. [22] proposed a parallel speech recognizer implementation on a commodity multicore system using OpenMP. The Viterbi search was parallelized by statically partitioning a tree-lexical search network across cores. The parallel recognition system proposed in [19] also uses a weighted finite state transducer (WFST) and data parallelism when traversing the recognition network. Prior works such as [10, 7] leveraged many core processors and focused on speeding up the compute-intensive phase (i.e., observation probability computation) of ASR on many core accelerators. Both [10, 7] demonstrated\napproximately 5x speedups in the compute-intensive phase and mapped the communication intensive phases (i.e., Viterbi search) onto the host processor."}, {"heading": "3.1 Current Approach", "text": "More recently, a data-parallel automatic speech\nrecognition inference engine was implemented on the graphics processing unit (GPU), achieving over 11x speedup compared to SIMD optimized sequential implementation on an Intel core i7 CPU. With less than 8% sequential overhead, the solution promises more speedup on future more parallel platforms [8]. The speedup was enabled by constructing the recognition engine\u2019s software architecture to efficiently execute on singlechip manycore processors. There are four key implementation decisions that contributed to the speedup:"}, {"heading": "3.1.1 Exposing fine-grained parallelism", "text": "The software architecture of the inference engine is\nillustrated in Figure 1. The Hidden Markov model (HMM) based inference algorithm dictates that there is an outer iteration processing one input feature vector at a time. Within each iteration, there is a sequence of algorithmic steps implementing maximal-likelihood inference process. The parallelism of the application is inside each algorithmic steps, where the inference engine keeps track of thousands to tens of thousands of alternative interpretations of the input waveform. The challenge is that each algorithmic step only performs tens to hundreds of instructions on each alternative interpretation, thus synchronizations between the algorithmic steps impose sequential overheads. In multi-chip parallel platforms, the synchronization overhead significantly degrades parallel speedup. The opportunity brought by single-chip manycore parallel processors is that the synchronization overhead is significantly reduced to the point that the finegrained parallelism can be exposed and the application speedup potentials can be realized."}, {"heading": "3.1.2 Implementing all parts of an algorithm on the GPU", "text": "Current GPUs are accelerator subsystems managed by a\nCPU over the PCIe data bus. As shown in Figure 1. In the inference engine, there is a compute intensive phase and a communication intensive phase of execution in each inference iteration. The compute intensive phase calculates the sum of differences of a feature vector against Gaussian mixtures in the acoustic model and can be readily parallelized. The communication intensive phase keeps track of thousands of alternative interpretations and manages their traversal through a complex finite state transducer representing the pronunciation and language models. While 17.7x speedup for the compute-intensive phase compared to sequential execution on the CPU was achieved , the communication-intensive phase is much more difficult to parallelize and received a 4.4x speedup. However, because the algorithm is completely\nimplemented on the GPU, it has achieved a 11.3x speedup of the overall inference engine."}, {"heading": "3.1.3 Leveraging fast hardware atomic operation support:", "text": "The inference process is composed of data-parallel graph\ntraversals on the recognition network. The graph traversal routines are executing in parallel on difference cores and frequently have to update the same memory location. This causes race conditions as the same piece of data must be read and conditionally written by multiple instruction streams at the same time. The race condition can be resolved using a sequence of data parallel algorithmic steps in the application software or by using hardware-based atomic operation support. When leveraging hardware-based atomic operation support, however, the operations must be carefully managed as atomic operations to the same memory address are sequentialized."}, {"heading": "3.1.4 Construct runtime data buffers to maximally regularize data access patterns:", "text": "The recognition network is an irregular network and the\ntraversal through the network is guided by user input available only at runtime. In each iteration of the inference engine, to maximally utilize the memory load and store bandwidth, the data to be accessed is gathered during the iteration into a consecutive vector acting as runtime data buffers, such that the algorithmic steps in the iteration are able to load and store results one cache line at a time. This maximizes the utilization of the available data bandwidth to memory. With these four key implementation decisions, it is possible to overcome the parallelization challenges imposed by the application, and architect and implement a scalable parallel solution for speech recognition inference decoding."}, {"heading": "3.2 Future Directions", "text": "The current work established an efficient software\narchitecture for speech recognition targeting the highly parallel manycore platforms. The ongoing work is constructing an application framework that allows many additional features to be extended without jeopardizing the efficiency and throughput of the implementation. One example of such additional feature can be an alternative observation likelihood computation that reduces the amount of computation necessary. Other improvements to the software architecture include producing word lattices or confusionnetworks in the context of multiple pass recognition systems. The improvements in recognition throughput could also be used to trade off speed with accuracy, making viable approaches.\n4. Improving Latency\nFor speech recognition to be useful in multispeaker scenarios, it is also important to determine \u201cwho is speaking\nwhen\u201d, a process called \u201cspeaker diarization\u201d, and to further segment the speech in a way that is reasonable for human consumption.The ongoing research is by no means complete but speaker diarization is a good example for explaining the opportunities to improve latency. Most speaker diarization systems use agglomerative hierarchical clustering as a core approach to perform diarization. At a high-level, systems extract MFCC features from a given audio track, discriminate between speech and nonspeech regions (speech activity detection), and use the agglomerative clustering approach to perform both segmentation of the audio track into speaker-homogeneous time segments and the grouping of these segments into speaker-homogeneous clusters in one step. Speech activity regions are determined using a speech/non-speech detector, e.g., [21]. The nonspeech regions are then excluded from the agglomerative clustering where the clustering is initialized using k clusters, with k larger than the number of speakers that are assumed to appear in the recording. Every cluster is modeled with a Gaussian Mixture Model containing g Gaussians. In order to train initial GMMs for the k speaker clusters an initial segmentation is generated by uniformly partitioning the audio into k segments of the same length. The ICSI system [1, 3] then performs the following iterations:\nRe-Segmentation: Run Viterbi alignment to find the optimal path of frames and models. In the ICSI system, a minimum duration of 2.5 seconds is assumed for each speech segment. Re-Training: Given the new segmentation of the audio track, compute new Gaussian Mixture Models for each of the clusters. Cluster Merging: Given the new GMMs, try to find the two clusters that most likely represent the same speaker. This is done by computing a score based on the Bayesian Information Criterion (BIC) of each of the clusters and the BIC score of a new GMM trained on the merged segments for two clusters. If the BIC score of the merged GMM is larger than or equal to the sum of the individual BIC scores, the two models are merged and the algorithm continues at the re-segmentation step using the merged GMM. If no pair is found, the algorithm stops. As a result of different sequential optimization approaches [13], the current implementation runs at about 0.6\u00d7 realtime, i.e., for 10 minutes of audio data, diarization finishes in roughly 6 minutes. The main problem with the approach is that it requires the complete recording of a meeting file and so the latency is the time of the meeting + 0.6\u00d7 realtime of the meeting duration. There are many applications where online diarization is desirable and batch processing impractical."}, {"heading": "4.1 Current Approach", "text": "An initial approach to online diarization was presented in the NIST Rich Transcription 2009 evaluations. The system consisted of a training step and an online recognition step. For the training step, the first 1000 seconds of the input are taken and performed offline speaker diarization using the system described above. Then speaker models are trained and a speech/non-speech model are taken from the output of the\nsystem. This is done by concatenating a random 60 second chunk of each speaker\u2019s segmented data and another one for the non-speech segments. In the online recognition step, the remainder of the meeting are recognized using the trained models. The sampled audio data is noise-reduced and converted into MFCC features. For every frame, the likelihood for each set of features is computed against each set of Gaussian Mixtures obtained in the training step, i.e. each speaker model and the non-speech model. A total of 250 ten ms frames is used for a majority vote on the likelihood values to determine the classification result. Therefore the latency totals at t + 2.5 s per decision (plus the portion of the offline training). Such a system can significantly benefit from parallelism. If the offline diarization were two orders-of magnitude faster than realtime, the offline diarization could process one minute of meeting in less than a second."}, {"heading": "4.2 Future Directions", "text": "Parallelism can be leveraged for low latency on different levels. The training of Gaussian Mixture Modes primarily requires matrix computation. If matrix computation is sped up by parallelism, more training can be run in the background at reduced wait times, resulting in both higher accuracy and lower latency. Also, giving models more iterations often leads them to converge with even less data, which also reduces latency. In the concrete example of diarization, lower runtime and therefore lower latency can be achieved by speeding up the cluster merge process, which might be parallelized on a thread level or using data parallelism by distributing each speaker model to a different core. With incoming data arriving through a sound card, USB device, or hard drive, I/O operations are likely to become a significant part of the runtime once parallelism is used intensively. Also, in the it was found that caching of highly repeated low-level operations (e.g., logarithm computations) helps runtime significantly. Therefore, a central cache for repeated operations seems highly desirable.\n5. Conclusions\nAutomatic Speech Recognition (ASR) is an application that consistently benefits from more powerful computation platforms. With the increasing adoption of parallel multicore and manycore processors, significant opportunities for speech recognition can be seen in increasing recognition accuracy, increasing batch-recognition throughput, and reducing recognition latency. Here on-going work on these directions, focusing on the opportunities and challenges with regard to parallelization is described. The proposed directions for future research may serve to guide future designs of efficient parallel software and hardware infrastructures for speech recognition.\n6. References [1]J. Ajmera and C. Wooters. A Robust Speaker Clustering Algorithm. In In Proceedings of IEEE Workshop on Automatic Speech Recognition Understanding, pages 411\u2013 416, 2003. [2] J. B. Allen. How do humans process and recognize speech? IEEE Transactions on Speech and Audio Processing, 2(4):567\u2013577, October 1994. [3] X. Anguera, C.Wooters, B. Peskin, and M. Aguilo. Robust speaker segmentation for meetings: The ICSI-SRI spring 2005 diarization system. In Proceeding of the NIST MLMI Meeting Recognition Workshop, Edinburgh, 2005. Springer. [4] K. Asanovic, R. Bodik, B. C. Catanzaro, J. J. Gebis, P. Husbands, K. Keutzer, D. A. Patterson, W. L. Plishker, J. Shalf, S. W. Williams, and K. A. Yelick. The landscape of parallel computing research: A view from Berkeley. Technical Report UCB/EECS-2006-183, EECS Department, University of California, Berkeley, Dec 2006. [5] H. Bourlard and S. Dupont. A new ASR approach based on independent processing and recombination of partial frequency bands. In Proceedings of the International Conference on Speech and Language Processing (ICSLP), pages 426\u2013 429, Philadelphia, USA, October 1996. [6] H. Bourlard, S. Dupont, and C. Ris. Multi-stream speech recognition. Technical Report RR 96-07, IDIAP research Institute, Martigny, Switzerland, December 1996. [7] P. Cardinal, P. Dumouchel, G. Boulianne, and M. Comeau. GPU accelerated acoustic likelihood computations. In Proc. Interspeech, 2008. [8] J. Chong, E. Gonina, Y. Yi, and K. Keutzer. A fully data parallel WFST-based large vocabulary continuous speech recognition on a graphics processing unit. In Proceeding of the 10th Annual Conference of the International Speech Communication Association, page 1183 1186, September 2009. [9] D. Depireux, J. Simon, D. Klein, and S. Shamma. Spectrotemporal response field characterization with dynamic ripples in ferret primary auditory cortexa. Journal of Neurophysiology, 85(3), 2001. [10] P. R. Dixon, T. Oonishi, and S. Furui. Fast acoustic computations using graphics processors. In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan, 2009. [11] V. Gadde, A. Stolcke, D. Vergyri, J. Zheng, K. Sonmez, and A. Venkataraman. Building an ASR system for noisy environments: SRI\u2019s 2001 SPINE evaluation system. In\nProceedings of the International Conference on Speech and Language Processing (ICSLP), pages 1577\u20131580, Denver, USA, September 2002. [12] D. Gelbart. Noisy numbers data and numbers speech recognizer. http://www.icsi.berkeley.edu/speech/papers/gelbartms/ numbers. [13] Y. Huang, O. Vinyals, G. Friedland, C. M\u00a8uller, N. Mirghafori, and C. Wooters. A fast-match approach for robust, faster than real-time speaker diarization. In Proceedings of the IEEE Automatic Speech Recognition Understanding Workshop, 2007. [14] S. Ishikawa, K. Yamabana, R. Isotani, and A. Okumura. Parallel LVCSR algorithm for cellphoneoriented multicore processors. In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Toulouse, France, 2006. [15] A. Janin, D.W. Ellis, and N. Morgan. Multistream: Ready for prime-time? In Proceedings of the European Conference on Speech Communication and Technology (Eurospeech), volume 2, pages 591\u2013 594, Budapest, September 1999. [16] D. Klein, D. Depireux, J. Simon, and S. Shamma. Robust spectrotemporal reverse correlation for the auditory system: optimizing stimulus design. Journal of Computational Neuroscience, 9(1):85\u2013111, 2000. [17] H. Ney and S. Ortmanns. Dynamic programming search for continuous speech recognition. IEEE Signal Processing Magazine, 16:64\u201383, 1999. [18] ParLab web site. http://parlab.eecs.berkeley.edu. [19] S. Phillips and A. Rogers. Parallel speech recognition. Intl. Journal of Parallel Programming, 27(4):257\u2013288, 1999. [20] M. Ravishankar. Parallel implementation of fast beam search for speaker-independent continuous speech recognition. Technical report, Computer Science and Automation, Indian Institute of Science, Bangalore, India, 1993. [21] C. Wooters and M. Huijbregts. The ICSI RT07s Speaker Diarization System. In Multimodal Technologies for Perception of Humans: International Ev aluation Workshops CLEAR 2007 and RT 2007, pages 509\u2013519, Baltimore, MD, USA, May 2007. Springer-Verlag. [22] K. You, Y. Lee, and W. Sung. OpenMP-based parallel implementation of a continuous speech recognizer on a multicore system. In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan, 2009. [23] S. Zhao, S. Ravuri, and N. Morgan. Multi-stream to many-stream: Using spectro-temporal features for ASR."}], "references": [{"title": "A Robust Speaker Clustering Algorithm", "author": ["J. Ajmera", "C. Wooters"], "venue": "Proceedings of IEEE Workshop on Automatic Speech Recognition Understanding,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "How do humans process and recognize speech", "author": ["J.B. Allen"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Robust speaker segmentation for meetings: The ICSI-SRI spring 2005 diarization system", "author": ["X. Anguera", "C.Wooters", "B. Peskin", "M. Aguilo"], "venue": "In Proceeding of the NIST MLMI Meeting Recognition Workshop,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "The landscape of parallel computing research: A view from Berkeley", "author": ["K. Asanovic", "R. Bodik", "B.C. Catanzaro", "J.J. Gebis", "P. Husbands", "K. Keutzer", "D.A. Patterson", "W.L. Plishker", "J. Shalf", "S.W. Williams", "K.A. Yelick"], "venue": "Technical Report UCB/EECS-2006-183,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "A new ASR approach based on independent processing and recombination of partial frequency bands", "author": ["H. Bourlard", "S. Dupont"], "venue": "In Proceedings of the International Conference on Speech and Language Processing (ICSLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Multi-stream speech recognition", "author": ["H. Bourlard", "S. Dupont", "C. Ris"], "venue": "Technical Report RR 96-07,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "GPU accelerated acoustic likelihood computations", "author": ["P. Cardinal", "P. Dumouchel", "G. Boulianne", "M. Comeau"], "venue": "Proc. Interspeech", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "A fully data parallel WFST-based large vocabulary continuous speech recognition on a graphics processing unit", "author": ["J. Chong", "E. Gonina", "Y. Yi", "K. Keutzer"], "venue": "In Proceeding of the 10th Annual Conference of the International Speech Communication Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Spectrotemporal response field characterization with dynamic ripples in ferret primary auditory cortexa", "author": ["D. Depireux", "J. Simon", "D. Klein", "S. Shamma"], "venue": "Journal of Neurophysiology, 85(3)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast acoustic computations using graphics processors", "author": ["P.R. Dixon", "T. Oonishi", "S. Furui"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Building an ASR system for noisy environments: SRI\u2019s 2001 SPINE evaluation system", "author": ["V. Gadde", "A. Stolcke", "D. Vergyri", "J. Zheng", "K. Sonmez", "A. Venkataraman"], "venue": "Proceedings of the International Conference on Speech and Language Processing (ICSLP),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "C", "author": ["Y. Huang", "O. Vinyals", "G. Friedland"], "venue": "M \u0308uller, N. Mirghafori, and C. Wooters. A fast-match approach for robust, faster than real-time speaker diarization. In Proceedings of the IEEE Automatic Speech Recognition Understanding Workshop", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Parallel LVCSR algorithm for cellphoneoriented multicore processors", "author": ["S. Ishikawa", "K. Yamabana", "R. Isotani", "A. Okumura"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Toulouse, France", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Multistream: Ready for prime-time", "author": ["A. Janin", "D.W. Ellis", "N. Morgan"], "venue": "In Proceedings of the European Conference on Speech Communication and Technology (Eurospeech),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Robust spectrotemporal reverse correlation for the auditory system: optimizing stimulus design", "author": ["D. Klein", "D. Depireux", "J. Simon", "S. Shamma"], "venue": "Journal of Computational Neuroscience, 9(1):85\u2013111", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Dynamic programming search for continuous speech recognition", "author": ["H. Ney", "S. Ortmanns"], "venue": "IEEE Signal Processing Magazine, 16:64\u201383", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Parallel speech recognition", "author": ["S. Phillips", "A. Rogers"], "venue": "Intl. Journal of Parallel Programming, 27(4):257\u2013288", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Parallel implementation of fast beam search for speaker-independent continuous speech recognition", "author": ["M. Ravishankar"], "venue": "Technical report, Computer Science and Automation, Indian Institute of Science, Bangalore, India", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "The ICSI RT07s Speaker Diarization System", "author": ["C. Wooters", "M. Huijbregts"], "venue": "CLEAR 2007 and RT", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "OpenMP-based parallel implementation of a continuous speech recognizer on a multicore system", "author": ["K. You", "Y. Lee", "W. Sung"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Applications in today\u2019s world can no longer rely on significant increases in processor clock rate for performance improvements, as clock rate is now limited by factors such as power dissipation [4].", "startOffset": 194, "endOffset": 197}, {"referenceID": 4, "context": "As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks.", "startOffset": 40, "endOffset": 54}, {"referenceID": 5, "context": "As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks.", "startOffset": 40, "endOffset": 54}, {"referenceID": 13, "context": "As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks.", "startOffset": 40, "endOffset": 54}, {"referenceID": 10, "context": "As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks.", "startOffset": 40, "endOffset": 54}, {"referenceID": 15, "context": "The inference engine traverses a graph-based recognition network based on the Viterbi search algorithm [17] and infers the most likely word sequence based on the extracted speech features and the recognition network.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "Fine-grained concurrency was mapped onto the multiprocessor with distributed memory in [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "[14] explored coarse-grained concurrency in speech recognition and implemented a pipeline of tasks on a cellphone-oriented multicore architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] proposed a parallel speech recognizer implementation on a commodity multicore system using OpenMP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The parallel recognition system proposed in [19] also uses a weighted finite state transducer (WFST) and data parallelism when traversing the recognition network.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "Prior works such as [10, 7] leveraged many core processors and focused on speeding up the compute-intensive phase (i.", "startOffset": 20, "endOffset": 27}, {"referenceID": 6, "context": "Prior works such as [10, 7] leveraged many core processors and focused on speeding up the compute-intensive phase (i.", "startOffset": 20, "endOffset": 27}, {"referenceID": 9, "context": "Both [10, 7] demonstrated", "startOffset": 5, "endOffset": 12}, {"referenceID": 6, "context": "Both [10, 7] demonstrated", "startOffset": 5, "endOffset": 12}, {"referenceID": 7, "context": "With less than 8% sequential overhead, the solution promises more speedup on future more parallel platforms [8].", "startOffset": 108, "endOffset": 111}, {"referenceID": 18, "context": ", [21].", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "The ICSI system [1, 3] then performs the following iterations: Re-Segmentation: Run Viterbi alignment to find the optimal path of frames and models.", "startOffset": 16, "endOffset": 22}, {"referenceID": 2, "context": "The ICSI system [1, 3] then performs the following iterations: Re-Segmentation: Run Viterbi alignment to find the optimal path of frames and models.", "startOffset": 16, "endOffset": 22}, {"referenceID": 11, "context": "As a result of different sequential optimization approaches [13], the current implementation runs at about 0.", "startOffset": 60, "endOffset": 64}], "year": 2013, "abstractText": "Automatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures.", "creator": "PScript5.dll Version 5.2"}}}