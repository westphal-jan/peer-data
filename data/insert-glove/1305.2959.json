{"id": "1305.2959", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2013", "title": "Automatic Speech Recognition Using Template Model for Man-Machine Interface", "abstract": "Speech is 48min a trainees natural ayoade form yasemin of rigeur communication kempson for cliffs human zentralbahn beings, obliquity and fedoseyev computers with the ability brute to idrs understand turkman speech and speak marstrand with 25.17 a khasekhemwy human voice bene are expected to contribute to dannemeyer the development of more rowson natural man - machine cit interfaces. qixia Computers with 76-year this yoann kind of ability 2258 are okui gradually chones becoming ashmun a reality, through heged\u0171s the assigned evolution of fudd speech shushing recognition cazypedia technologies. Speech ciller is enviar being an yisheng important tarand mode soeken of radars interaction with babbin computers. powerbook In padishah this paper polyphase Feature camogie extraction analyst is implemented clegane using well - known ilyukhin Mel - Frequency trade-related Cepstral kishore Coefficients (MFCC ). Pattern matching is done 3,832 using jetton Dynamic chch-tv time courtemanche warping (DTW) algorithm.", "histories": [["v1", "Thu, 9 May 2013 08:47:47 GMT  (62kb)", "http://arxiv.org/abs/1305.2959v1", "Pages: 05 Figures : 01 Tables : 03 Proceedings of the International Conference ICAET 2010, Chennai, India. arXiv admin note: text overlap witharXiv:1305.2847"]], "COMMENTS": "Pages: 05 Figures : 01 Tables : 03 Proceedings of the International Conference ICAET 2010, Chennai, India. arXiv admin note: text overlap witharXiv:1305.2847", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["neema mishra", "urmila shrawankar", "v m thakare"], "accepted": false, "id": "1305.2959"}, "pdf": {"name": "1305.2959.pdf", "metadata": {"source": "CRF", "title": "Automatic Speech Recognition Using Template Model for Man-Machine Interface", "authors": ["Neema Mishra", "Urmila Shrawankar"], "emails": ["neema.mishra@gmail.com", "urmila@ieee.org"], "sections": [{"heading": "1. Introduction", "text": "We always needed to minimize human effort to get things done. A convenient and user-friendly interface for Human Computer Interaction is an important technology issue. The prevalent computer interface is via keyboard or a pointing device for input and a visual display unit or a printer for output. Spoken languages dominate communication among human being and hence people expect speech interface with computers [1]. Automatic Speech Recognition (ASR) methods can be categories into two types; a) Textindependent (TI) and b) Text dependant (TD).Text-independent methods [2-6] assume that passwords users are uttering can be anything.TI methods not pay much attention to features dynamics and treat the sequence of extracted features from the speech utterance not as a sequence of symbols. Text-dependant speaker recognition methods [7-9] exploits the feature dynamic to capture the identity of the speaker\u2019s methods compare the features vectors sequence of the test utterance with the \u201cfeature-dynamicsmodel\u201d of all the speakers. Machine recognition of speech involves generating a sequence of words which best matches the given speech signal. In the speaker independent mode of speech recognition, the computer should ignore the specific characteristic of the speech signal and extract the intended message. To make an Automatic Speech Recognition (ASR) system, we used speech recognition by pattern matching of whole words. In this method if the same person repeats the same isolated word on separate occasion, the pattern is likely to be generally similar because the same phonetic relationships will apply. A wellestablished approach to ASR is to store in the machine example acoustic patterns (called template) for the words to be recognized, usually spoken by the person who will subsequently used the machine. Any incoming word is compared in turn with all words in the store and the one, which is most similar, is assumed the correct one. Exploiting this similarity is, however, critically depend on how the word patterns are compared i.e how the distance between words is calculated. The most commonly used feature extraction methods are Linear Predictive Cepstral Coefficients (LPCC), Perceptual Linear Prediction (PLP) Cepstra, Mel-Frequency Cepstral Coefficients (MFCC).Compared to LPCC, speech recognition based on MFCC can produce higher recognition rate and suppressed noise effectively.So,MFCC featured is utilized more effectively than LPCC.Standard MFCC is very sensitive to noise for the feature of speech is static.So,here we used MFCC extraction method\nfor better result. In this paper for pattern matching Dynamic time warping algorithm is used. The paper organized as follows: 2.discuss about Feature Extraction 3.Pattern Recognition 4.Discuss about data collection and experimental result .5.Conclusion and future work."}, {"heading": "2. Feature Extraction", "text": "The common step in feature extraction is frequency or spectral analysis. The signal processing techniques aim to extract features that are related to identify the characteristics [10].The goal of feature extraction is to find a set of properties of an utterance that have acoustic correlations in the speech signal i.e. parameters that can some how estimated through processing of signal waveform. Such parameters are termed as features [11]. Several different feature extraction algorithm exist, namely\n\u2022 Linear Predictive Cepstral Coefficients(LPCC)\n\u2022 Perceptual Linear Prediction(PLP) Cepstra\n\u2022 Mel-Frequency Cepstral Coefficients (MFCC)\nLPCC computes Spectral envelop before converting it into Cepstral coefficient. The LPCC are LP-derived cepstral coefficient. The PLP integrates critical bands, equal loudness pre emphasis and intensity to compressed loudness. The PLP is based on the Nonlinear Bark scale. The PLP is designed to speech recognition with removing of speaker dependant characteristics. MFCC are extensively in ASR.MFCC is based on signal decomposition with the help of a filter bank, which uses the Mel scale. The MFCC results on Discrete Cosine Transform (DCT) of a real logarithm of a short-time energy expressed on the Mel frequency sacle.This paper work considered 12MFCC.The Cepstral coefficients are set of feature reported to be robust in some different pattern recognition tasks concerning with human voice. Human voice is very well adapted to the ear sensitivity. In speech recognition, tasks 12 coefficients are retained which represent the slow variation of the spectrum of the signal, which characterizes the vocal tract shape of the uttered words [12].\nThe Mel-frequency cepstrum coefficient technique is often used to create the fingerprint of the sound files. The mfcc are based on the known variation of the human ear\u2019s critical bandwidth frequencies with filter spaced linearly at low frequencies and logarithmically at high frequencies used to capture important characteristics of speech. Studies have shown that human perception of the frequencies contents of sound for speech signal does not follow a linear sacle.Thus, for each store with an actual frequency, measures in Hz; a subjective pitch is measured on a scale called the Mel-scale. The Mel frequency scale is below 1000 Hz and a logarithmic spacing above 100Hz.As reference point the pitch of 1 KHz, tone, 40db above the perceptual hearing threshold is defined as 1000 Mels.The following formula is used to compute the Mels for particular frequency: Mel (f) = 2595*log10(1+f/700)\nIn frame, blocking the speech waveform is cropped to remove silence or acoustical interference that may be present in the beginning or end of the sound file. The windowing block minimizes the discontinuities of the signal by tapering the beginning and end of each frame to zero. The Fast Fourier Transform (FFT) block converts each frame from the time domain to the frequency domain. In the Mel Scale Filter block, the signal is filtered using band-pass filter whose bandwidths and spacing are roughly equal to critical bands and whose range of center frequencies covers frequencies most important for speech perception (300-5000Hz).In Inverse Discrete Fourier Transform (IDFT) block, Cepstrum is generated. Where Cepstrum is the spectrum of the log of the spectrum. MFCC feature is considered for speaker \u2013independent"}, {"heading": "3. Pattern Recognition", "text": "The dynamic time warping is a technique of finding optimal non-linear warping of test patterns so as to obtained good match with a reference pattern(model) with a reasonable computational load [14]. In this paper, DTW based on pattern matching is done.DTW is an algorithm for measuring similarity between two sequences which may very in time or speed.DTW is a method that allow a computer to find an optimal match between two given sequences with certain restriction. In speech recognition technique the test data is converted to templates. The recognition process then consists of matching the incoming speech pattern with stored templates pattern. The template with the lowest distance measure from the input pattern is the recognized word. The optimal match (lowest distance measure) is based upon dynamic programming. This is called a Dynamic Time Warping (DTW) [1]. Let D (i,j) be the cumulative distance along the optimum path from the beginning of the word to point i,j. ij D (i, j) = \u2211 d(x, y) x,y=1, 1 Along the best path There are only three possibilities for the point before i,j it follows that If D (i, j) is the global distance up to (i, j) and the local distance at (i, j) is given by d (i, j) D(i,j)= min (i-1,j),D(i-1,j-1), D(i,j-1)) + d(i,j) \u2026\u2026. (1) The value (1, 1) must be equal to d (1, 1) as this point is the beginning of all possible paths. The final global distance D (i,j) gives us the overall\nmatching score of the template with the input. The input word is then recognized as the word corresponding to the template with the lowest matching score.\n4. Experimental Setup\n4.1 Data Collection The database consists of four repetitions of ten words by 4 person (3 male and 1 female).The set of names being same for all speakers. The data colleted from the microphone contains the effect of room acoustics and the background noise in the computer room. The speech data was sampled at 16 kHz, single channel (mono) in MS wav format. The close talking microphone was used in the experiment for data collection."}, {"heading": "4.2 Experiment Details", "text": "We conducted three different experiments dealing with different aspects of speech recognition system for better accuracy and practical implementation. In order to match the Test template to the reference template of the same speaker, we have record voices of speakers, converted them to sequence vectors of Mel cepstral coefficient and stored them into binary file as reference template. Now, for matching the stored reference template with the test template, we use DTW algorithm. Experiment 1: Matching test and reference template of same speaker. In this experiment first repetition of 10 words are used as the reference and next 3 repetitions of the words are used as test template. The 30(3*10) template were matched using DTW algorithm implemented in mat lab. The accuracy for other speakers in the similar manner was:"}, {"heading": "SPEAKER ACCURACY", "text": "The average accuracy when test and reverence template of same speaker were matched is 97.5%. Experiment 2: Matching reference template of one speaker with test template of another speaker. In this experiment, first repetition of 10 names by one speaker was taken as a reference template and all templates of all others speakers was taken as test template matched against them. For example, taking 3rd speaker as reference speaker and 2nd speaker as test 35 out of 40 templates got matched. Therefore the accuracy is calculated as 35/40*100 =87.5%. Here the voices being matched are different speakers, the accuracy decreased as expected. In case of mismatch of gender the accuracy decreased further (speaker 1 is female while rest are male.) All possible combination was tested and the few results are tabulated as follows:"}, {"heading": "REFERNCE TEST ACCURACY", "text": "The average accuracy when test and reference templates of different speakers were matched was 70.6%."}, {"heading": "5. CONCLUSION AND FUTURE DIRECTION", "text": "Automatic recognition of speech by machine has been a goal of research for more than five decades [15].This work proposed an approach to implement an ASR with MFCC feature extraction and for pattern matching DTW is used which match test and reference template for same speaker with accuracy 97.5%.In the other experiment for keeping matching reference template of one speaker with test template of another speaker the accuracy is 70.6%.Till now we use rectangular windowing because it is\nsimple to understand. One could use hamming windowing to increase the accuracy rate. Scope for future work would concentrate on incorporating the features as from limited database size to large, from speaker dependant to speaker independent. In addition, we can apply (HIDDEN MARKOV MODEL) HMM to find out the most probable word from the database, if no corresponding template is stored in the database."}, {"heading": "6. REFERNCES", "text": "[1] Holmes, J.N. (1988).Speech Synthesis And Recognition, Van Nostrand Reinfold (UK) Co, Ltd, pp.102-113. [2] Bimbot et al, \u201cA Tutorial on Text-Independent Speaker Verification\u201d, Eurasip J.Appl. Speech Proc.4 (2004). [3] D.Reynolds, et al., \u201cSpeaker Verification using adapted GMM\u201d, Digital Signal Processing, 10(1- 3), 2000. [4] F.K.Soong, et al, \u201cA vector quantization approach to speaker recognition\u201d, AT&T Tech.J., Vol 66(1987). [5] A.Das & P.Ghosh, \u201cAudio-Video Biometric Recognition by Vector Quantization\u201d,Proc.IEEE SLT-06. [6] T.Kinnunen, E.Karpov, and P.Franti, \u201cRealTime Speaker Identification and"}, {"heading": "Verification\u201d,IEEE Trans.On Audio, Speech and", "text": "Language Processing, V14-1, Jan 2006. [7] T.Matsui and S.Furui, \u201cConcatenated phoneme models for text-variable speaker recognition\u201d, Proc.ICASSP-93. [8] D.Falavigna, \u201cComparison of Different HMM Based Methods for Speaker Verification\u201d, Proc.Eurospeech-95. [9] Higgins et al., \u201cSpeaker Verification using randomized phrase prompting\u201d, Digital Signal Processing, 1(2): (1991). [10] Sandipan Chakroborthy and Goutam Saha, \u201cImproved Text-Independent Speaker Identification Using Fused MFCC & IMFCC Feature Sets Based on Gaussian Filter,\u201d International Journal of Signal Processing, vol.5, no.1, pp.11-19, 2009. [11] R. K. Aggarwal and Mayank Dave \u201cImplementing a Speech Recognition System Interface for Indian Languages\u201d IJCNLP-08 Workshop on NLP for Less Privileged, IIIT, Hyderabad, 11 January 2008.\n[12] L.Rabiner and B.H. Juang, \u201cFundamental of Speech Recognition,\u201dPTRPrentice Hall, Englewood Cliffs, New Jersey, 1993. [13] Murty & Yegna, \u201cCombining Evidence from Residual Phase and MFCC features for Speaker Recognition, IEEE Signal Processing Letters, V13-1, (2006). [14] Hiroaki Sakoe and Seibi Chiba, \u201cDynamic Programming Algorithms Optimization for Spoken Word Recognition\u201d,IEEE Trans on acoustic, speech and signal processing,vol.ASSP26,no.1,December 1978. [15] Lawrence Rabiner, Biing-Hwang Juang and B.Yegnanarayana, \u201cFundamentals of Speech Recognition\u201d, Pearson Education, 2009."}], "references": [{"title": "Synthesis And Recognition, Van Nostrand Reinfold (UK) Co, Ltd, pp.102-113", "author": ["J.N. Holmes"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "A Tutorial on Text-Independent Speaker Verification", "author": ["Bimbot"], "venue": "Eurasip J.Appl. Speech", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "A vector quantization approach to speaker recognition", "author": ["F.K.Soong"], "venue": "AT&T Tech.J.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1987}, {"title": "Speaker Verification using randomized phrase prompting", "author": ["Higgins"], "venue": "Digital Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "Improved Text-Independent Speaker Identification Using Fused MFCC & IMFCC Feature Sets Based on Gaussian Filter", "author": ["Sandipan Chakroborthy", "Goutam Saha"], "venue": "International Journal of Signal Processing, vol.5, no.1, pp.11-19, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Implementing a Speech Recognition System Interface for Indian Languages", "author": ["R.K. Aggarwal", "Mayank Dave"], "venue": "IJCNLP-08 Workshop on NLP for Less Privileged, IIIT, Hyderabad,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Fundamental of Speech Recognition,\u201dPTRPrentice", "author": ["L.Rabiner", "B.H. Juang"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Combining Evidence from Residual Phase and MFCC features for Speaker Recognition", "author": ["Murty", "Yegna"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Dynamic Programming Algorithms Optimization for Spoken Word Recognition\u201d,IEEE Trans on acoustic, speech and signal processing,vol.ASSP", "author": ["Hiroaki Sakoe", "Seibi Chiba"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1978}], "referenceMentions": [{"referenceID": 0, "context": "Spoken languages dominate communication among human being and hence people expect speech interface with computers [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "Text-independent methods [2-6] assume that passwords users are uttering can be anything.", "startOffset": 25, "endOffset": 30}, {"referenceID": 2, "context": "Text-independent methods [2-6] assume that passwords users are uttering can be anything.", "startOffset": 25, "endOffset": 30}, {"referenceID": 3, "context": "Text-dependant speaker recognition methods [7-9] exploits the feature dynamic to capture the identity of the speaker\u2019s methods compare the features vectors sequence of the test utterance with the \u201cfeature-dynamicsmodel\u201d of all the speakers.", "startOffset": 43, "endOffset": 48}, {"referenceID": 4, "context": "The signal processing techniques aim to extract features that are related to identify the characteristics [10].", "startOffset": 106, "endOffset": 110}, {"referenceID": 5, "context": "Such parameters are termed as features [11].", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "In speech recognition, tasks 12 coefficients are retained which represent the slow variation of the spectrum of the signal, which characterizes the vocal tract shape of the uttered words [12].", "startOffset": 187, "endOffset": 191}, {"referenceID": 7, "context": "speech recognition and for the speaker recognition task as well [13].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Pattern Recognition The dynamic time warping is a technique of finding optimal non-linear warping of test patterns so as to obtained good match with a reference pattern(model) with a reasonable computational load [14].", "startOffset": 213, "endOffset": 217}, {"referenceID": 0, "context": "This is called a Dynamic Time Warping (DTW) [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "[1] Holmes, J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Bimbot et al, \u201cA Tutorial on Text-Independent Speaker Verification\u201d, Eurasip J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[9] Higgins et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[10] Sandipan Chakroborthy and Goutam Saha, \u201cImproved Text-Independent Speaker Identification Using Fused MFCC & IMFCC Feature Sets Based on Gaussian Filter,\u201d International Journal of Signal Processing, vol.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[11] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[12] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[13] Murty & Yegna, \u201cCombining Evidence from Residual Phase and MFCC features for Speaker Recognition, IEEE Signal Processing Letters, V13-1, (2006).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[14] Hiroaki Sakoe and Seibi Chiba, \u201cDynamic Programming Algorithms Optimization for Spoken Word Recognition\u201d,IEEE Trans on acoustic, speech and signal processing,vol.", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "Speech is a natural form of communication for human beings, and computers with the ability to understand speech and speak with a human voice are expected to contribute to the development of more natural man-machine interfaces. Computers with this kind of ability are gradually becoming a reality, through the evolution of speech recognition technologies. Speech being an important mode of interaction with computers. In this paper Feature extraction is implemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Pattern matching is done using Dynamic time warping (DTW) algorithm.", "creator": "PScript5.dll Version 5.2"}}}