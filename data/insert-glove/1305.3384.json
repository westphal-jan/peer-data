{"id": "1305.3384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2013", "title": "Transfer Learning for Content-Based Recommender Systems using Tree Matching", "abstract": "25a In rodong-1 this paper we dimensions present a marmon new epithelioid approach to espoo content - loquaciousness based reconvert transfer learning 3f for battlezone solving mccubbin the minakov data sparsity problem galette in cases when 6:47 the users ' mailings preferences shattered in lendoiro the target east-northeast domain are either scarce konlive or fist unavailable, 60.76 but averil the necessary sacchi information on inferiorly the ie9 preferences exists pollino in tetrahedral another domain. jankelowitz We show that training 4-9 a hegen system to amdh use kazan such information across domains can diocles produce better huasun performance. kottakkal Specifically, we represent calyptraeidae users ' behavior eutrophic patterns based abcp on topological graph pwrs structures. Each behavior sahibganj pattern bearings represents the tawe behavior traveling of bunnie a set of users, muffuletta when the users ' 522,000 behavior is defined culbro as the luciferase items godowsky they z-93 rated moated and itogi the items ' hsiao-hsien rating debs values. re-assessed In shekau the alfons\u00edn next nawi step we find a correlation spore-bearing between behavior patterns kvitfjell in u.s.-turkey the source antineutrinos domain taghiyev and behavior patterns in greenup the scurrying target muhammadan domain. spiros This mapping naohiko is considered melkonian a bridge between the beam@globe.com two schulken domains. 12th Based 86.78 on the correlation 14:56 and 112.42 content - attributes seyf of anglicanum the items, we train a dotage machine guarantors learning sip model university-manila to predict jetpack users ' cyberporn ratings cargol in self-hatred the target domain. When hiss we compare puttkamer our dongling approach carrigan to the arbeiter-zeitung popularity 2-34 approach tinplate and KNN - cross - david-lloyd domain on 54-43 a 95.00 real world dataset, the results chateaux show that manesar on tarkwa an yome average kundera of minkowski 83 $% $ kusuo of the janak cases our tempestuous approach cowpoke outperforms 1,913 both f.k. methods.", "histories": [["v1", "Wed, 15 May 2013 08:00:54 GMT  (46kb,D)", "http://arxiv.org/abs/1305.3384v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["naseem biadsy", "lior rokach", "armin shmilovici"], "accepted": false, "id": "1305.3384"}, "pdf": {"name": "1305.3384.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning for Content-Based Recommender Systems using Tree Matching", "authors": ["Naseem Biadsy", "Lior Rokach", "Armin Shmilovici"], "emails": ["naseem@cs.bgu.ac.il", "liorrk@bgu.ac.il", "armin@bgu.ac.il"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.4 [Information Systems Applications]: Miscellaneous\nGeneral Terms Recommendations, Experimentation\nKeywords Recommender-Systems, Transfer Learning, Content-based, Behavior Patterns"}, {"heading": "1. INTRODUCTION", "text": "Cross domain recommenders [1] aim to improve recommendation in one domain (hereafter: the target) based on knowledge in another domain (hereafter: the source). There are two approaches for cross domain: 1) those that use a mediator to construct and initialize an empty user model, or that enrich the existing user model by using provided data or a partial user model in another domain or service; 2) those that do not use a mediator. In this paper we propose a content-based cross domain RS that uses a mediator to suggest items to a new user in a target domain who has only rated items in the source domain. We assume that: 1) the target domain has very high sparsity; 2) the source domain has low sparsity; and 3) the new user has already rated some items in the source domain. Based on these assumptions, traditional recommendation algorithms do not work well in the target domain for two reasons: first, high sparsity; second, the new user and cold-start issues.\nThe problem: Given a set of items T in a target domain, and given a user u who has already rated certain items in the source domain but not in the target, what are the most N preferred items in T by user u?\nWe begin solving this problem by presenting a new graphbased transfer learning method for content-base recommendation. We define the concept of behavior tree as a topological representation of users\u2019 behavior patterns. By users\u2019 behavior we mean the items rated by a user and the rating values the user has assigned to these items. Based on these trees, we find correlated behavior patterns in the source and the target domains which are considered bridges between the two domains. Then, we combine the data of each bridge with the content data of the items in a one features vector. This vector is used as a sample in the training data for a machine learning algorithm. Here we assume that common users exist between the source and target domains.\nThe paper\u2019s innovation is (1) the graph structure that we employ for dealing with the problem and (2) the items\u2019 content data with transfer learning recommenders that we employ to improve classifier performance. The trees structure provides a rich representation of the users\u2019 behavior. First, this structure enables items to be clustered according to the users\u2019 behavior, and the clusters to be represented by a forest of behavior trees. Second, the tree\u2019s structure defines a hierarchy among the items, this hierarchy is important because it shows how much the item represents the behavior pattern it belongs to. Finally, we find a correlation (mapping) between items in the source domain and items in the target domain, so by running a process of a tree matching between trees in the source and trees in the target we get this mapping. Furthermore, from the content data we gain more features to represent the training samples for building the recommendation model."}, {"heading": "2. THE RECOMMENDATION FRAMEWORK", "text": "Our framework deals with the abovementioned problem in three phases: The first is described in (2.1) and it aims to preprocess the users\u2019 data in the source and target domains. Preprocessing consists of three steps: 1) building behavior trees for each domain; 2) graph matching; 3) building training samples. The second phase (2.2) is for training a model on the training set. The third phase, described in (2.3), is for recommending items to a new user based on his/her behavior in the source domain."}, {"heading": "2.1 Preparing Training Data", "text": "2.1.1 Building Behavior Graphs Constructing the forests is done in four steps. First we sep-\narate the same item with different rating values by expanding each item into k items, where k equals the number of possible\nar X\niv :1\n30 5.\n33 84\nv1 [\ncs .L\nG ]\n1 5\nM ay\n2 01\n3\nrating values in the domain. For example, if item i was rated as r1 by one group of users and the same item got rating r2 by a different group of users then we consider them as different items and represent them by ir1 and ir2. Note: the number of items after this step is k \u00d7 number of origin items. The next step. Here we sort the separated items by their popularity, that is, by the number of users who rated it. In table (1) we give an example of a rating matrix, where the popularity of item 1 2 (item 1 with rating 2) in this matrix is 3.\nThe third step: We adopt a topological representation by taking the sorted items set of each domain and representing them by a forest of weighted graphs. These graphs are constructed as follows: Nodes: Each item on the sorted items list is represented by one node (Note: the node represents a pair of an item and one rating value). Edges: An edge is found between two nodes if there are common users who rated both items represented by both nodes (Note: Rating must be the same as in the node). Weight: The weight of the edge between two nodes is defined as the Jaccard coefficient between the the sets of users who rated the items represented by the nodes. Jaccard coefficient measures similarity between sample sets, and defined as the size of the intersection divided by the size of the union of the sample sets (Wikipedia):\nSimilarity(A, B) = J(A, B) = |A \u22c2\nB| |A \u22c3 B| . If the two sets A and\nB are empty then we define J(A,B) = 0. The max value of this similarity when A,B are finite is 1, and the minimal value is 0.\nIf the weight of edge is less than a given threshold 1 we drop the edge. Note, a one-to-one mapping exists between the nodes and the items, thus we can use the term node or item to refer the same element. Figure (1) shows the behavior graphs (forest) that we get from table (1) when threshold = 0.5.\nThe fourth step: We convert each graph in the forest into a topological tree which we refer as a behavior tree. This is another representation of the users\u2019 behavior in the graph based on the graph\u2019s structure. This process is done by adding an artificial node to be the root for each tree, and connecting all the popular 2 nodes in the graph to this node. Then we use a recursive greedy algorithm for completing the tree by adding all the other nodes on the graph to the tree. This is done by connecting each child to the parent who is connected to the highest edge weight in the original graph.\n1This threshold depends on different parameters such as the number of users in the domain, the number of items, the number of ratings, etc. We set it as 0.5 in our experiments. 2The most popular items in each graph are those have greater popularity than the average popularities in the same graph.\nFigure 1: Behavior graphs (forest) of table (1) when threshold = 0.5.\nThe motivation for moving from graphs to trees is the simpler hierarchical structure of the trees compared to graphs, and the ability to represent sufficient data of the behavior graphs by behavior trees. Figure (1) represents the graphs we get from the rating matrix in Table (1). and Figure (2) illustrates the behavior trees based on the behavior graphs in figure (1), where Ti is the behavior tree of graph Gi, i : 1 \u2192 6, and the nodes with labels A1-A6 are artificial nodes.\n2.1.2 Tree Matching The first task is to decide for each tree in the source domain\nwhich tree in the target domain is the most appropriate to be matched with. Here we have the advantage of the common users, so we define similarity between two trees in the same way that we defined similarity between two items in (2.1.1), but here we refer to it on a set of users instead of on a set of items: Tree-Similarity(T1, T2) = J(U(T1), U(T2)) = |U(T1) \u22c2 U(T2)| |U(T1) \u22c3 U(T2)|\nU(T) = \u22c3|T |\ni=1 (u | user u rated item i in T ). After we find for each tree in the source the tree in the target that it should be matched with, we run the tree matching process to match the nodes in both trees. Eventually we arrive at a set of pairs of matched nodes. These pairs are the bridges between the items in the source and the items in the target.\n2.1.3 Building the Training Samples This is the final step in preparing training data. Here we\nprepare the training set for a supervised machine learning task. We find a special structure for features vectors that combines the data of the users\u2019 behavior in the source and target domains, and the items content data. This is done by taking all the matched nodes from the previous step. Then, for each two matched nodes ns and ntr that represent item ts in the source with rate rs and item ttr in the target with rating rtr respectively, we define one features vector as following: [f1(ts), ..., fn(ts), rs; f1(ttr), ..., fm(ttr), rtr], where fi(t) is the value of feature i for item t. This structure provides a full\nimage for the user\u2019s behavior from the previous step and the features of the items they rated."}, {"heading": "2.2 Model Training", "text": "Here we build a model of a multiclass classifier on the training samples which combine features of items in the source, ratings in the source, features of items in the target, and ratings in the target. Since our goal is to predict the user\u2019s rating of an item in the target domain, the class of the classifier is the last attribute in the features vector, where all the other attributes in the vector represent the features. We expect that this model to predict rating for an item t in the target domain for a new user u that has rated only items in the source domain. The number of the classes equals the number of possible ratings values in the target domain."}, {"heading": "2.3 Recommendation", "text": "The recommendation task is based on the model we described in the previous task (2.2). When the system is asked to recommend the top N items in the target domain for a new user u, the system ranks each item tr in the target domain based on items that the user has rated in the source domain. The ranking is carried out by building a features matrix for each item tr in the target as follows:\n\u2022 The number of the rows in the matrix equals the number of items the user rated in the source.\n\u2022 The number of the column in the matrix equals n + m + 2, where n is the number of the item\u2019s features in the source, m is the number of the item\u2019s features in the target domain and + 2 for the rating in the source and the class (rating in target).\n\u2022 The value in row i and column j equals:\nIf j < n + 1 Then: The value of feature j in item i (in the source)\nIf j = n + 1 Then: The rating value that the user u rated item i (in the source)\nIf j > n + 1 Then: The value of feature j\u2212(n+1) of the item tr (in the target)\nIf j = n + 2 Then: The value is \u201d?\u201d (missing)\nTo find the rank of a features matrix M , we first run the classifier on each row in the matrix in order to return the vector of the predicted probability for each class. This vector is called the distribution vector and represents the probability for each class, so the value in entry i equals to the probability that this samples is classified as i. Also, the vector\u2019s size equals the number of the classes (possible ratings). For each row we take the distribution vector P returned by the classifier and compute the expected rating3 as follows: expected rating =\u2211k\nj=1 j\u00d7P [j], when k is the size of V . The rank of the features matrix M is defined as the average of the expected ratings of\nall the rows in M : Rank(M) = \u2211z j=1 ER(j)\nz , where ER(j) is\nthe expected rating for row j and z is the number of the rows in M . Then we sort these matrices by the rank value, and return the target items represented by the top N matrices as the recommended items for the user u.\n3We assume that the ratings are natural continuos numbers that start from 1 as the minimal value."}, {"heading": "3. EXPERIMENT", "text": "In this chapter we investigate whether the additional knowledge gained from the source domain and the content of the items can improve the recommendation in the target domain. We compare our approach with the Popularity, which is a single domain approach and usually used for recommendation to new users, and with the KNN-cross-domain approach that uses both domains but dose not use the content data."}, {"heading": "3.1 Dataset", "text": "We used the Loads data-set in our experiments. Loads data is a real-world content dataset that includes different domains such as videos, music, games (common users among the domains). We chose music-loads as the source domain and gameloads as the target domain, and extracted 600 common users from both domains, 817 items from the music domain with 18,552 ratings, and 1264 items from the games domain with 17,640 ratings. This dataset is event-based, thus we manipulated it and converted the events to ratings by weighting the events by their type. For example the event user buys an item that was converted to the max rate value. For this experiment we used a binary ratings."}, {"heading": "3.2 Evaluation and Metrics", "text": "Our goal is to recommend a set of n items that may find the interest of the new user in the target domain. This kind of recommendation refers to recommending some good items [2] or top-N items. The correct method of evaluating the recommender in this case is by measuring the precision at N (or Top-N precision) which is the number of interesting items from the recommended items [2]. Since we have content data, we consider a recommended item as a true positive if it is similar to 80% of the positively rated items"}, {"heading": "3.3 Baseline", "text": "We compare our method, referred to as BGM (Behavior Graph Matching), with two base-line recommenders: KNN-cross-domain: This recommender is a cross-domain recommender based on collaborative filtering with Pearson\u2019s correlation coefficient method. The main idea behind the method is to find the K-nearest-neighbors of the active user in the source domain who also rated items in the target domain, and to consider them as the K-nearest-neighbors of the active user, who is also in the target domain, and then predict the user\u2019s ratings in the target domain based on the ratings in the domain.\nPopularity: This is a naive method that recommends the popular items to the new users. This method is the simplest to implement and sometimes outperforms other algorithms [3] especially in a domain where most of the users have similar preferences."}, {"heading": "3.4 BGM versus Popularity", "text": "We evaluate the two methods by 10-fold cross validation on the Loads dataset, when each fold includes 60 users for test and 540 users for train. The test set was considered as the new users\u2019 set in the target domain who have just ratings in the source domain. The behavior patterns were based on all the users in the source domain, and users belong to the training set in the target domain. For each user in the test set we asked the recommender to recommend the Top-N items when N= 5, 10, 15, 20, 50, and 100, and for each set of recommended items we measured the top-N precision per user. The popularity algorithm uses the training set in the target domain to find the popular item that was recommend for each of the test users,\nthen we computed the top-N precision for each N, and for each N we found the average of the top-N precision for all of the users.\nFigure (3) shows the results, where we divided into 6 groups, each group representing a different N value and containing three columns, the first one representing the average Top-N precision value for the BGM, and the second one representing the average of Top-N precision value for the popularity algorithm. We note that BGM outperforms the popularity in most of the cases. Thus, our conclusion, which is based on results, is that it is better to use the source domain to improve recommendation in the target domain."}, {"heading": "3.5 BGM Vs. KNN cross-domain", "text": "Tis experiment was conducted in order to compare the performance of our approach with the collaborative-filtering crossdomain approach which also uses the knowledge in the source domain to return recommendations to the target domain. The main goal was to determine whether the BGM method makes maximum use of the content data of the items, and to check whether the proposed behavior patterns work well for representing the users\u2019 behavior and transferring knowledge. As in the first experiment, this experiment, too, was conducted with a 10-fold cross validation on Loads dataset.\nWe can compare the results in Figure (3), where the third column in each group represents the value of top-N precision of the KNN-cross-domain. Note: BGM outperforms the KNNcross-domain in 83% of the cases."}, {"heading": "3.6 Statistical Analysis", "text": "We performed paired t-tests on the same results we get by running 10-fold cross-validation with BGM, Popularity and KNNcross-domain on Loads dataset, and we checked whether they were statistically significant. So each user was considered a participant, and each of the recommenders was considered a different method of the test.\nWe received a table of 600 users (rows), and for each user we received the top-N precision value when TopN= 5, 10, 15, and 20 with each of the methods (12 columns per user). Then the t-test was performed for every two relevant columns (with the same N ) . All the results are statistically significant, which means that our method performance is significantly better than the two other methods when the number of recommended items is 5, 10, 15, and 20, and the size of the sample is 600 participants."}, {"heading": "4. CONCLUSIONS", "text": "In this paper we presented and evaluated a novel method of transfer learning in content-based recommenders by using a new topological structure that we called a behavior graph. The\nmain idea of using such structure is its ability of representing rich data about the users\u2019 behavior, and the relation between items in a structure. By employing tree matching methods we discovered a correlation between items in the source and items in the target.\nWe compared our method with the popularity approach, which is generally used with recommendations to new users, and with the KNN-cross-domain method. Our comparison was based on a real-world dataset called Loads dataset. We evaluated the Top-N precision metric by 10-fold cross validation.\nThe results show that our method (referred to as BGM) outperforms the popularity and KNN-cross-domain methods in most cases. Our conclusion: it is preferable to use the data in the source domain and the item\u2019s content data when dealing with this kind of recommendation problem."}, {"heading": "5. REFERENCES", "text": "[1] S. Berkovsky, T. Kuflik, and F. Ricci. Cross-domain\nmediation in collaborative filtering. User Modeling 2007, pages 355\u2013359, 2007.\n[2] J. L. Herlocker, J. A. Konstan, L. G. Terveen, John, and T. Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems, 22:5\u201353, 2004.\n[3] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock. CROC: A New Evaluation Criterion for Recommender Systems."}], "references": [{"title": "Cross-domain mediation in collaborative filtering", "author": ["S. Berkovsky", "T. Kuflik", "F. Ricci"], "venue": "User Modeling", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["J.L. Herlocker", "J.A. Konstan", "L.G. Terveen", "John", "T. Riedl"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Cross domain recommenders [1] aim to improve recommendation in one domain (hereafter: the target) based on knowledge in another domain (hereafter: the source).", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "Table 1: Rating matrix, possible ratings [1, 2, 3] (k=3).", "startOffset": 41, "endOffset": 50}, {"referenceID": 1, "context": "Table 1: Rating matrix, possible ratings [1, 2, 3] (k=3).", "startOffset": 41, "endOffset": 50}, {"referenceID": 1, "context": "This kind of recommendation refers to recommending some good items [2] or top-N items.", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "The correct method of evaluating the recommender in this case is by measuring the precision at N (or Top-N precision) which is the number of interesting items from the recommended items [2].", "startOffset": 186, "endOffset": 189}], "year": 2013, "abstractText": "In this paper we present a new approach to content-based transfer learning for solving the data sparsity problem in cases when the users\u2019 preferences in the target domain are either scarce or unavailable, but the necessary information on the preferences exists in another domain. We show that training a system to use such information across domains can produce better performance. Specifically, we represent users\u2019 behavior patterns based on topological graph structures. Each behavior pattern represents the behavior of a set of users, when the users\u2019 behavior is defined as the items they rated and the items\u2019 rating values. In the next step we find a correlation between behavior patterns in the source domain and behavior patterns in the target domain. This mapping is considered a bridge between the two domains. Based on the correlation and content-attributes of the items, we train a machine learning model to predict users\u2019 ratings in the target domain. When we compare our approach to the popularity approach and KNN-cross-domain on a real world dataset, the results show that on an average of 83% of the cases our approach outperforms both methods.", "creator": "LaTeX with hyperref package"}}}