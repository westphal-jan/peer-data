{"id": "1306.2663", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2013", "title": "Large Margin Low Rank Tensor Analysis", "abstract": "Other consecuencias than vector representations, the westerhoff direct objects \u00e9tendards of human cognition re-elected are generally semi-auto high - harnam order tensors, henrot such mukerjee as 2D ghg images tumens and 28-30 3D textures. From this accommodative fact, two vaillancourt interesting comperes questions urbanist naturally arise: blunderbuss How does the korrodi human \u014duchi brain dunrobin represent these tensor creativity perceptions in bamboozle a \" manifold \" way, bj\u00f6rnsson and how can they cpd be dialectology recognized drought-stricken on claudication the \" manifold \"? shaziman In 1870-72 this paper, we multifocal present a t40 supervised chikamatsu model loewner to learn the mellows intrinsic structure of the tensors trihk embedded risottos in a '62 high dimensional knudtzon Euclidean hillquit space. With the pfc fixed point starner continuation poetarum procedures, our vakhayev model willet automatically 62,100 and tecumsehs jointly 45-14 discovers the hatin optimal kakawin dimensionality 13-april and vils the fowey representations of the vatuvei low kirchhundem dimensional embeddings. This cytyc makes cdpd it nichinan an effective sumfest simulation commsec of the spahi cognitive alemannic process anti-royalist of tti human spectacularly brain. martinelli Furthermore, debes the pravind generalization duped of avinashi our model 19.64 based hoechst on soloff similarity cinzia between booterstown the learned low low-winged dimensional embeddings can be jaiswal viewed as cordite counterpart of linares recognition tamsui of human brain. Experiments on applications for boehmer object recognition +0.25 and face filibuster recognition irianese demonstrate the nzrl superiority adni of left our tawfik proposed model over chiquet state - of - higgs the - mpci art t\u00edmea approaches.", "histories": [["v1", "Tue, 11 Jun 2013 21:39:56 GMT  (1379kb,D)", "http://arxiv.org/abs/1306.2663v1", "30 pages"]], "COMMENTS": "30 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["guoqiang zhong", "mohamed cheriet"], "accepted": false, "id": "1306.2663"}, "pdf": {"name": "1306.2663.pdf", "metadata": {"source": "CRF", "title": "Large Margin Low Rank Tensor Analysis", "authors": ["Guoqiang Zhong", "Mohamed Cheriet"], "emails": ["guoqiang.zhong@synchromedia.ca,", "mohamed.cheriet@etsmtl.ca."], "sections": [{"heading": null, "text": "Other than vector representations, the direct objects of human cognition are generally high-order tensors, such as 2D images and 3D textures. From this fact, two interesting questions naturally arise: How does the human brain represent these tensor perceptions in a \u201cmanifold\u201d way, and how can they be recognized on the \u201cmanifold\u201d? In this paper, we present a supervised model to learn the intrinsic structure of the tensors embedded in a high dimensional Euclidean space. With the fixed point continuation procedures, our model automatically and jointly discovers the optimal dimensionality and the rep-\nar X\niv :1\nresentations of the low dimensional embeddings. This makes it an effective simulation of the cognitive process of human brain. Furthermore, the generalization of our model based on similarity between the learned low dimensional embeddings can be viewed as counterpart of recognition of human brain. Experiments on applications for object recognition and face recognition demonstrate the superiority of our proposed model over state-of-the-art approaches."}, {"heading": "1 Introduction", "text": "In one paper by Seung and Lee (2000), the authors state that the human brain represents real world perceptual stimuli in a manifold way \u2013 encoding high dimensional signals in an intrinsically low dimensional structure. At the same time of their work and later on, numerous manifold learning algorithms, such as isometric feature mapping (Isomap) (Tenenbaum et al., 2000) and locally linear embedding (LLE) (Roweis and Saul, 2000), were proposed for discovering the manifold structure of data embedded in a high dimensional space. Most of these manifold learning methods can be applied to vector representations of signals, and yield acceptable performance for visualization and recognition. However, in contrast, humans can perceive not only vector representations of signals (one-order tensors), but also high order representations (highorder tensors), such as 2D images and 3D textures. More importantly, humans can in general perform high accuracy recognition based on learned patterns, i.e. recognizing objects and faces. From this fact, two questions naturally arise: How does the human brain learn the intrinsic manifold structure of the tensor representations, and how does\nit recognize new patterns based on the learned manifold structure?\nTo solve these two questions, some researchers try to extend traditional vector representation-based dimensionality reduction approaches to the applications related to high order tensors. Specifically, some representative tensor dimensionality reduction approaches include (Yang et al., 2004; Ye et al., 2004) and (Wang et al., 2007). These approaches can learn the low dimensional representations of tensors in either an unsupervised or a supervised way. In particular, the approach presented in (Wang et al., 2007) is theoretically guaranteed to converge to a local optimal solution of the learning problem. However, one common issue of these approaches exists: the dimensionality of the low dimensional tensor space must be manually specified before these approaches are applied. Therefore, these approaches may not necessarily lead to the genuine manifold structure of the tensors.\nTo exploit the questions above and overcome the shortage of previous approaches, in this paper, we propose a novel tensor dimensionality reduction method, called large margin low rank tensor analysis (LMLRTA). LMLRTA is aimed at learning the low dimensional representations of tensors using techniques of multi-linear algebra (Northcott, 1984) and graph theories (Bondy and Murty, 1976). Compared to traditional vector representation-based dimensionality reduction approaches, LMLRTA can take any order of tensors as input, including 1D vectors (one-order tensor), 2D matrices (twoorder tensor), and more. This guarantees the feasibility of that one can use LMLRTA to simulate the way how human brain represents perceived signals, such as speech, images and textures. Furthermore, unlike previous tensor dimensionality reduction approaches (Yang et al., 2004; Ye et al., 2004; Wang et al., 2007), which can only learn\nthe low dimensional embeddings with a priori specified dimensionality, LMLRTA can automatically learn the optimal dimensionality of the tensor subspace. This guarantees LMLRTA to be an intelligent method to simulate the way of human perception. Besides, for the recognition of new coming patterns, we employ similarity between the learned low dimensional representations as measurement, which corresponds to the way how the human brain recognize new objects (Rosch, 1973).\nThe rest of this paper is organized as follows. In Section 2, we provide an brief overview of previous work on dimensionality reduction. In Section 3, we present our proposed model, LMLRTA, in detail, including its formulation and optimization. Particularly, we theoretically prove that LMLRTA can converge to a local optimal solution of the optimization problem. Section 4 shows the experimental results on real world applications, including object recognition and face recognition, which are related to problems with respect to 2D tensors and 3D tensors, respectively. We conclude this paper in Section 5 with remarks and future work."}, {"heading": "2 Previous work", "text": "In order to find the effective low dimensional representations of data, many dimensionality reduction approaches have been proposed in the areas of pattern recognition and machine learning. The most representative approaches are principal component analysis (PCA) and linear discriminant analysis (LDA) for the unsupervised and supervised learning paradigms, respectively. They are widely used in many applications due to their simplicity and efficiency. However, it is well known that both of them are optimal\nonly if the relation between the latent and the observed space can be described with a linear function. To address this issue, nonlinear extensions based on kernel method have been proposed to provide nonlinear formulations, i.e. kernel principal component analysis (KPCA) (Scho\u0308lkopf et al., 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000).\nSince about a decade ago, many manifold learning approaches have been proposed.\nThese manifold learning approaches, including isometric feature mapping (Isomap) (Tenenbaum et al., 2000) and locally linear embedding (LLE) (Roweis and Saul, 2000), can faithfully preserve global or local geometrical properties of the nonlinear structure of data. However, these methods only work on a given set of data points, and cannot be easily extended to out-of-sample data (Bengio et al., 2003). To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points. In particular, Yan et al. proposed a general framework known as graph embedding for dimensionality reduction (Yan et al., 2007). Most of the spectral learning-based approaches, either linear or nonlinear, either supervised or unsupervised, are contained in this framework. Furthermore, based on this framework, the authors proposed the marginal Fisher analysis (MFA) algorithm for supervised linear dimensionality reduction. In the research of probabilistic learning models, Lawrence (2005) proposed the Gaussian process latent variable models (GPLVM), which extends PCA to a probabilistic nonlinear formulation. Combining a Gaussian Markov random field prior with GPLVM, Zhong et al. (2010) proposed the Gaussian process latent random field model, which can be\nconsidered as a supervised variant of GPLVM. In the area of neural network research, Hinton and Salakhutdinov (2006) proposed a deep neural network model called autoencoder for dimensionality reduction. To exploit the effect of deep architecture for dimensionality reduction, some other deep neural network models were also introduced, such as deep belief nets (DBN) (Hinton et al., 2006), stacked autoencoder (SAE) (Bengio et al., 2006) and stacked denoise autoencoder (SDAE) (Vincent et al., 2010). These studies show that deep neural networks can generally learn high level representations of data, which can benefit subsequent recognition tasks.\nAll of the above approaches assume that the input data are in the form of vectors. In many real world applications, however, the objects are essentially represented as highorder tensors, such as 2D images or 3D textures. One have to unfold these tensors into one-dimensional vectors first before the dimensionality reduction approaches can be applied. In this case, some useful information in the original data may not be sufficiently preserved. Moreover, high-dimensional vectorized representations suffer from the curse of dimensionality, as well as high computational cost. To alleviate these problems, 2DPCA (Yang et al., 2004) and 2DLDA (Ye et al., 2004) were proposed to extend the original PCA and LDA algorithms to work directly on 2D matrices rather than 1D vectors. In recent years, many other approaches (Yan et al., 2007; Tao et al., 2007; Fu and Huang, 2008; Liu et al., 2012, 2010) were also proposed to deal with high-order tensor problems. In particular, Wang et al. (2007) proposed a tensor dimensionality reduction method based on the graph embedding framework, which is proved that it is the first method to give a convergent solution. However, as described before, all these previous tensor dimensionality reduction approaches have a common shortage:\nthe dimensionality of the low dimensional representations must be specified manually before the approaches can be applied.\nTo address the above issues existing in both vector representation-based and tensor representation-based dimensionality reduction approaches, in this paper, we propose our novel method for tensor dimensionality reduction, called large margin low rank tensor analysis (LMLRTA). LMLRTA is able to take any order of tensors as input, and automatically learn the dimensionality of the low dimensional representations. More importantly, these merits make it an effective model to simulate the way how human brain represents and recognizes perceived signals."}, {"heading": "3 Large margin low rank tensor analysis (LMLRTA)", "text": "In this section, we first introduce the used notation and some basic terminologies on tensor operations (Kolda and Bader, 2009; Dai and Yeung, 2006). And then, we detail our model, LMLRTA, including its formulation and optimization. Theoretical analyses to LMLRTA, such as its convergence, are also presented."}, {"heading": "3.1 Notation and terminologies", "text": "We denote vector by using bold lowercase letter, such as v, matrix by using bold uppercase letter, such as M, and tensor by using calligraphic capital letter, such as A. Suppose A is a tensor of size I1 \u00d7 I2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 IL, the order of A is L and the lth dimension (or mode) of A is of size Il. In addition, we denote the index of a single entry within a tensor by subscripts, such as Ai1,...,iL .\nDefinition 1 The scalar product \u3008A,B\u3009 of two tensors A,B \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IL is defined as \u3008A,B\u3009 = \u2211\ni1 \u00b7 \u00b7 \u00b7 \u2211 iL Ai1,...,iLB\u2217i1,...,iL , where \u2217 denotes complex conjugation. Fur-\nthermore, the Frobenius norm of a tensor A is defined as \u2016A\u2016F = \u221a \u3008A,A\u3009.\nDefinition 2 The l-mode product of a tensor A \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IL and a matrix U \u2208 RJl\u00d7Il is an I1\u00d7\u00b7 \u00b7 \u00b7\u00d7Il\u22121\u00d7Jl\u00d7Il+1\u00d7\u00b7 \u00b7 \u00b7\u00d7IL tensor denoted asA\u00d7lU, where the corre-\nsponding entries are given by (A\u00d7lU)i1,...,il\u22121,jl,il+1,...,iL = \u2211 il Ai1,...,il\u22121,il,il+1,...,iLUjlil .\nDefinition 3 LetA be an I1\u00d7I2\u00d7\u00b7 \u00b7 \u00b7\u00d7IL tensor and (\u03c01, . . . , \u03c0L\u22121) be any permutation of the entries of the set {1, . . . , l\u2212 1, l+ 1, . . . , L}. The l-mode unfolding of the tensor A into an Il \u00d7 \u220fL\u22121 k=1 I\u03c0k matrix, denoted as A (l), is defined by A \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IL \u21d2l A(l) \u2208 RIl\u00d7 \u220fL\u22121 k=1 I\u03c0k , where A(l)ilj = Ai1,...,iL with j = 1 + \u2211L\u22121 k=1 (i\u03c0k \u2212 1) \u220fk\u22121 k\u0302=1 I\u03c0k\u0302.\nDefinition 4 The multi-linear rank of a tensor is a set of nonnegative numbers, (r1, r2, . . . , rL), such that\nrl = dim(R(A(l)) = rank(A(l)), l = 1, 2, . . . , L\nwhere R(A) = {f |f = Az} is the range space of the matrix A, and rank(A) is the matrix rank.\nMulti-linear rank of tensors is elegantly discussed in (de Silva and Lim, 2008), as well as other rank concepts. In this paper, we only focus on multi-linear rank of tensors and call it \u201crank\u201d for short."}, {"heading": "3.2 Formulation of LMLRTA", "text": "As pointed out by researchers in the area of cognitive psychology that humans learn based on the similarity of examples (Rosch, 1973), here, we formulate our model based\non the local similarity between tensor data. In addition, thanks to the existence of many \u201cteachers\u201d, we can generally obtain the categorical information of the examples before or during learning. Take, for example, the moment when someone introduces an individual to his friend. His friend will probably remember the name of the individual first, and then her or his face and voice. In this case, name of the individual corresponds to a categorical label, whilst her or his face and voice are features to perceive. In the same way, we formulate our learning model in a supervised scheme.\nGiven a set of N tensor data, {A1, . . . ,AN} \u2208 RI1\u00d7...\u00d7IL , with the associated class labels {y1, . . . ,yN} \u2208 {1, 2, . . . , C}, where L is the order of the tensors and C is the number of classes, we learn L low rank projection matrix Ul \u2208 RJl\u00d7Il (Jl \u2264 Il, l = 1, . . . , L), such that N embedded data points {B1, . . . ,BN} \u2208 RJ1\u00d7...\u00d7JL can be obtained as Bi = Ai \u00d71 U1 \u00d72 . . .\u00d7L UL. The objective function can be written as\nmin L(\u03bb, \u00b5,Ul|Ll=1) = \u00b5 L\u2211 l=1 rank(Ul) + \u03bb 2NL \u2211 i,j \u03b7ij\u2016Bi \u2212 Bj\u20162F\n+ 1\n2NL \u2211 i,j,p \u03b7ij(1\u2212 \u03c8ij)[1 + \u2016Bi \u2212 Bj\u20162F \u2212 \u2016Bi \u2212 Bp\u20162F ]\u2020 (1)\nwhere rank(Ul) is the rank of matrix Ul, \u2016A\u2016F is the Frobenius norm of a tensor A, and [z]\u2020 = max(0, z) is the so-called hinge loss, which is aimed at maximizing the margin between classes. If Ai and Aj have the same class label, and Ai is one of the k1-nearest neighbors ofAj orAj is one of the k1-nearest neighbors ofAi, then \u03b7ij = 1, otherwise \u03b7ij = 0. If Ai and Aj have different class labels, and Ai is one of the k2nearest neighbors of Aj or Aj is one of the k2-nearest neighbors of Ai, then \u03c8ij = 0,\notherwise \u03c8ij = 1, i.e.\n\u03c8ij =  0, yi 6= yj and Aj \u2208 Nk2(Ai) or Ai \u2208 Nk2(Aj); 1, otherwise, (2)\nwhere Nk(Ai) stands for k-nearest neighbor of Ai. Like the binary matrix {\u03b7ij} , the matrix {\u03c8ij} is fixed and does not change during learning.\nThe minimization of the first term of the objective function, \u2211L\nl=1 rank(Ul), is to\nlearn low rank Ul\u2019s and further the low dimensional representations of the tensors. The second term of the objective function is to enforce the neighboring data in each class to be close in the low dimensional tensor subspace. It can be considered as a graph Laplacian-parameterized loss function with respect to the low dimensional embeddings (Chung, 1997; Belkin and Niyogi, 2003; Tenenbaum et al., 2011), where each node corresponds to one tensor datum in the given data set. For each tensor datum Ai, the hinge loss in the third term will be incurred by a differently labeled datum within k2-nearest neighbors of Ai, if whose distance to Ai does not exceed, by 1, the distance from Ai to any of its k1-nearest neighbors within the class of Ai. This third term thereby favors projection matrices in which different classes maintain a large margin of distance. Furthermore, it encourages nearby data in different classes far apart in the low dimensional tensor subspace.\nrank(Ul) is a non-convex function with respect to Ul and difficult to optimize. Following recent work in matrix completion (Cande\u0300s and Tao, 2010; Cande\u0300s and Recht, 2012), we replace it with its convex envelope \u2014 the nuclear norm of Ul, which is de-\nfined as the sum of its singular values, i.e. \u2016Ul\u2016\u2217 = r\u2211 s=1 \u03c3s(Ul) , where \u03c3s(Ul)\u2019s are the singular values of Ul, and r is the rank of Ul. Thus, the resulting formulation of our\nmodel can be written as\nmin L(\u03bb, \u00b5,Ul|Ll=1) = \u00b5 L\u2211 l=1 \u2016Ul\u2016\u2217 + \u03bb 2NL \u2211 i,j \u03b7ij\u2016Bi \u2212 Bj\u20162F\n+ 1\n2NL \u2211 i,j,p \u03b7ij(1\u2212 \u03c8ip)[1 + \u2016Bi \u2212 Bj\u20162F \u2212 \u2016Bi \u2212 Bp\u20162F ]\u2020 (3)\nSince Problem (3) is not convex with respect to Ul, we transform it into a convex problem with respect to Wl = UTl Ul. Meanwhile, using the slack variables, Problem (3) can be rewritten as\nmin L(\u03bb, \u00b5, \u03be,Wl|Ll=1) = \u00b5 L\u2211\nl=1\n\u2016Wl\u2016\u2217 + \u03bb\n2NL \u2211 i,j \u03b7ijtr((Y (l) i \u2212Y (l) j )(Y (l) i \u2212Y (l) j ) TWl)\n+ 1\n2NL \u2211 i,j,p \u03b7ij(1\u2212 \u03c8ip)\u03beijp\ns.t. tr((Y (l) i \u2212Y (l) p )(Y (l) i \u2212Y (l) p ) TWl)\u2212 tr((Y(l)i \u2212Y (l) j )(Y (l) i \u2212Y (l) j ) TWl) \u2265 1\u2212 \u03beijp,\n\u03beijp \u2265 0, i, j, p = 1, 2, . . . , N, (4)\nwhere Y(l)i is the l-mode unfolding matrix of the tensor Yi = Ai \u00d71 U1 \u00d72 . . . \u00d7l\u22121 Ul\u22121 \u00d7l+1 Ul+1 \u00d7l+2 . . . \u00d7L UL. For the second term of the objective function and the first constraint in Problem (4), we have used the property of the trace function: tr(Ul(Y (l) i \u2212Y (l) j )(Y (l) i \u2212Y (l) j ) TUTl ) = tr((Y (l) i \u2212Y (l) j )(Y (l) i \u2212Y (l) j ) TUTl Ul).\nThe equivalence between Problem (3) and Problem (4) can be guaranteed by the\nfollowing lemma.\nLemma 1 Based on the notation above, Problem (3) and Problem (4) are equivalent.\nProof: Based on simple computation, we know that the second term of the objective function in Problem (3) is equal to that in Problem (4), while the third term of the objective function in Problem (3) is equivalent to that in Problem (4) with the constraints. As \u03c3s(Ul) = \u221a \u03c3s(Wl), the optimal solution of Problem (3) must correspond to the optimal solution of Problem (4), and vice versa, where \u03c3s(Ul) and \u03c3s(Wl) are the singular\nvalues of Ul and Wl, respectively. Thus, the lemma is proved.\nProblem (4) is not jointly convex with respect to all the Wl\u2019s. However, it\u2019s convex\nwith respect to each of them. This is guaranteed by the following lemma.\nLemma 2 Problem (4) is convex with respect to each Wl.\nProof: First, the nuclear norm of Wl, \u2016Wl\u2016\u2217, is a convex function with respect to Wl. Second, the other terms of the objective function and the constraints in Problem (4) are all linear function with respect to Wl. Hence, Problem (4) is convex with respect to each Wl.\nRemark 1 (Relation to previous works) 1) LMLRTA can be considered as a su-\npervised multi-linear extension of locality preserving projections (LPP) (He and Niyogi, 2003), in that the second term of the objective function in Problem (4) forces neighboring data in a same class to be close in the low dimensional tensor subspace;\n2) LMLRTA can also be considered as a reformulation of tensor marginal Fisher\nanalysis (TMFA) (Yan et al., 2007). However, TMFA is not guaranteed to converge to a local optimum of the optimization problem (Wang et al., 2007), but LMLRTA is guaranteed as proved in Section 3.3;\n3) For Problem (4), we can consider it as a variant of the Large Margin Nearest\nNeighbor (LMNN) algorithm (Weinberger et al., 2005) for distance metric learning in tensor space. Moreover, we can learn low rank distance matrices via the formulation of Problem (4), which the LMNN algorithm is not endowed;\n4) In contrast to previous approaches for tensor dimensionality reduction, which\ncan only learn project matrices with pre-specified dimensionality of the low dimensional representations, LMLRTA can automatically learn the dimensionality of the low dimensional representations from the given data. This will be shown in Section 3.3.\n5) Unlike deep neural network models (Hinton et al., 2006; Bengio et al., 2006; Vin-\ncent et al., 2010), which simulate human brain\u2019s hierarchical structure, LMLRTA mimics the way of human perception. On one hand, LMLRTA can take any order of tensors as input, but most deep neural networks only take vectorized representations of data. On the other hand, with large number of parameters, the learning of deep neural network models in general needs many training data. If the size of the training set is small, deep neural network models may fail to learn the intrinsic structure of data. However, in this case, LMLRTA can perform much better than deep neural network models. Experimental results in Section 4 demonstrate this effect."}, {"heading": "3.3 Optimization", "text": "Similar to previous approaches on tensor dimensionality reduction (Dai and Yeung, 2006; Wang et al., 2007), here we solve Problem (4) using an iterative optimization algorithm. In each iteration, we refine one projection matrix by fixing the others. Here, for each Wl, problem (4) is a semi-definite programming problem, which can be solved using off-the-shelf algorithms, such as SeDuMi1 and CVX (Grant and Boyd, 2008).\n1http://sedumi.ie.lehigh.edu/\nHowever, the computational cost of semi-definite programming approaches is in general very high. Here, we solve the problem by means of a modified fixed point continuation (MFPC) method (Ma et al., 2011).\nMFPC is an iterative optimization method. In the t-th iteration, it involves two\nalternating steps:\na) Gradient step: Ztl = W t l \u2212 \u03c4g(Wtl);\nb) Shrinkage step: Wt+1l = S\u03c4\u00b5(Z t l).\nIn the gradient step, g(Wtl) is the sub-gradient of the objective function in problem (4) with respect to Wtl (excluding the nuclear norm term), and \u03c4 is the step size. Here, we can express \u03beijp as a function with respect to Wtl :\n\u03beijp(W t l) = [1 + tr((Y (l) i \u2212Y (l) j )(Y (l) i \u2212Y (l) j ) TWl)\u2212 tr((Y(l)i \u2212Y (l) p )(Y (l) i \u2212Y (l) p ) TWl)]\u2020 i, j, p = 1, 2, . . . , N. (5)\nNote that the hinge loss is not differentiable, but we can compute its sub-gradient and use a standard descent algorithm to optimize the problem. Thus we can calculate g(Wtl) as\ng(Wtl) = \u03bb\n2NL \u2211 i,j \u03b7ij(Y (l) i \u2212Y (l) j )(Y (l) i \u2212Y (l) j ) T\n+ 1\n2NL \u2211 {i,j,p}\u2208S \u03b7ij(1\u2212 \u03c8ip)((Y(l)i \u2212Y (l) j )(Y (l) i \u2212Y (l) j ) T \u2212 (Y(l)i \u2212Y (l) p )(Y (l) i \u2212Y (l) p ) T ),(6)\nwhere S is the set of triplets whose corresponding slack variable exceeds zero, i.e.,\n\u03beijp(W t l) > 0.\nIn the shrinkage step, S\u03c4\u00b5(Ztl) = V max{0,\u039b\u2212diag(\u03c4\u00b5)}VT is a matrix shrinkage operator on Ztl = V\u039bV T , where max is element-wise and diag(\u03c4\u00b5) is a diagonal matrix with all the diagonal elements set to \u03c4\u00b5. Here, since Wtl is supposed to be a symmetric and positive semi-definite matrix, its eigenvalues should be nonnegative. Therefore,\nAlgorithm 1 The MPFC algorithm. 1: Input:\n2: \u03bb, Tmax, W0l , \u00b5\u0304 > 0; % Tmax is the maximum number of iterations. 3: Initialization: 4: \u00b51 > \u00b52 > . . . > \u00b5K = \u00b5\u0304; 5: Steps: 6: for \u00b5 = \u00b51, \u00b52, . . . , \u00b5K do 7: while t < Tmax and not converge do 8: Compute Ztl = W t l \u2212 \u03c4g(Wtl) and eigenvalue decomposition of Ztl , Ztl =\nV\u039bVT ;\n9: Compute Wt+1l = S\u03c4\u00b5(Z t l);\n10: end while 11: end for 12: Output: 13: The learned Wl.\nwe adapt the eigenvalue decomposition method to shrink the rank of Ztl . To this end, the shrinkage operator shifts the eigenvalues down, and truncates any eigenvalue less than \u03c4\u00b5 to zero. This step reduces the nuclear norm of Wtl . If some eigenvalues are truncated to zeros, this step reduces the rank of Wtl as well. In our experiments, we use relative error as the stopping criterion of the MFPC algorithm.\nFor clarity, we present the procedure of the MPFC algorithm in Algorithm 1. For the convergence of the MFPC algorithm, we present a theorem as below.\nTheorem 1 For fixed Wk, k = 1, . . . , l\u22121, l+1, . . . , L, the sequence {Wtl} generated\nby the MPFC algorithm with \u03c4 \u2208 (0, 2/\u03bbmax(g(Wl))) converges to the optimal solution, W\u2217l , of Problem (4), where \u03bbmax(g(Wl)) is the maximum eigenvalue of g(Wl).\nThe proof of this theorem is similar to that of theorem 4 in (Ma et al., 2011). A minor difference is, we use eigenvalue decomposition here instead of singular value decomposition as used in the proof of theorem 4 in (Ma et al., 2011). However, the derivation and results are the same.\nBased on the above lemmas and Theorem 1, we can have the following theorem on\nthe convergence of our proposed method, LMLRTA.\nTheorem 2 LMLRTA converges to a local optimal solution of Problem (4).\nProof: To prove Theorem 2, we only need to prove that the objective function has a lower bound, as well as the iterative optimization procedures monotonically decrease the value of the objective function.\nFirst of all, it\u2019s easy to see that the value of the objective function in Problem (4) is always larger than or equal to 0. Hence, 0 is a lower bound of this objective function. Secondly, for the optimization of each Wl, l = 1, . . . , L, from Theorem 1, we know that the MPFC algorithm minimizes the value of the objective function in Problem (4). Therefore, the iterative procedures of LMLRTA monotonically decrease the value of the objective function, and LMLRTA is guaranteed to converge to a local optimal solution of Problem (4).\nBased on Lemma 2 and Theorem 2, we can easily obtain a corollary as below:\nCorollary 1 If the given data are one-order tensors, the LMLRTA algorithm converges to the optimal solution of Problem (4)."}, {"heading": "3.4 Generalization to new tensor data", "text": "For the recognition of unseen test tensors, we employ the tensor Frobenius norm-based k-nearest neighbor classifier as recognizer, in that it measures the local similarity between training data and test data in the low dimensional tensor subspace (Rosch, 1973)."}, {"heading": "4 Experiments", "text": "In this section, we report the experimental results obtained on two real world applications: object recognition and face recognition. Particularly, for the face recognition task on the ORL data set, we used 3D Gabor transformation of the face images as input signals. This is mainly based on the fact that the kernels of the Gabor filters resemble the receptive field profiles of the mammalian cortical simple cells (Daugman, 1988), which enhances our learning model to better mimic the way of human perception. In the following, we report the parameter settings and experimental results in detail."}, {"heading": "4.1 Parameter settings", "text": "To demonstrate the effectiveness of our method for the intrinsic representation learning and recognition, we conducted experiments on the COIL-20 data set2 and the ORL face data set3. The COIL-20 data set includes 20 classes of objects, and 72 samples within each class. The size of the images is 32 \u00d7 32. The ORL data set contains 400 images of 40 subjects, where each image was normalized to a size of 32 \u00d7 32. For each face image, we used 28 Gabor filters to extract textural features. To the end, each face image\n2http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php. 3http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html.\nwas represented as a 32 \u00d7 32 \u00d7 28 tensor. On the COIL-20 data set, we used 5-fold cross validation to evaluate the performance of the compared methods. The average classification results were reported. As each subject only has 10 images in the ORL data set, we evaluated the compared methods based on the average over 5 times random partition of the data. Here, variety of scenarios \u2014 different numbers of training data from each class, were tested.\nTo show the advantage of our proposed method, LMLRTA, we compared it with two classic vector representation-based dimensionality reduction approaches \u2013 linear discriminant analysis (LDA) (Fisher, 1936) and marginal Fisher analysis (MFA) (Yan et al., 2007), one deep neural networks model called stacked denoising autoencoder (SDAE) (Vincent et al., 2010), and two state-of-the-art tensor dimensionality reduction methods \u2013 convergent multi-linear discriminant analysis (CMDA) and convergent tensor margin Fisher analysis (CTMFA) (Wang et al., 2007). For comparison, we also provided the classification results obtained in the original data space. In the LMLRTA algorithm, k1 and k2 were set to 7 and 15 respectively for the COIL-20 data set, while for the ORL data set, they were set to ntrain \u2212 1 and 2 \u00d7 ntrain respectively, where ntrain is the number of training data from each class. Furthermore, \u03bb was selected from {0.001, 0.01, 0.1, 1, 10}, and the one resulting best classification result was used. For CMDA and CTMFA, we adopted the best setting learned by LMLRTA to specify the dimensionality of the low dimensional tensor subspace. We used the code of SDAE from a public deep learning toolbox4. For all the methods but SDAE, tensor Frobenius norm-based 1-nearest neighbor classifier was used for the recognition of test data.\n4https://github.com/rasmusbergpalm/DeepLearnToolbox."}, {"heading": "4.2 Visualization", "text": "Figure 1 (a) and Figure 1 (b) illustrate the 2D embeddings of the object images from the COIL-20 data set and that of the 3D Gabor transformation of the face images from the ORL data set, respectively. The t-distribution-based stochastic neighbor embedding (t-SNE) algorithm (van der Maaten and Hinton, 2008) was employed to learn these 2D embeddings, where the distances between data were measured based on tensor Frobenius norm. From Figure 1 (a) and Figure 1 (b), we can see that, in the original space of these two data sets, most of the classes align on a sub-manifold embedded in the ambient space. However, for some classes, the data are scattered in a large area of the data space, and alternatively, close to data of other classes. As a result, similarity-based classifiers may predict the labels of some unseen data incorrectly in both of these two\noriginal representation spaces. Hence, it\u2019s necessary to learn the intrinsic and informative representations of the given tensor data.\nFigure 2 (a) and Figure 2 (b) illustrate the 2D embeddings of the low dimensional tensor representations for the COIL-20 and the ORL data set, respectively. Here, LMLRTA was used to learn the low dimensional tensor representations, while the t-SNE algorithm was used to generate the 2D embeddings. It is easy to see, LMLRTA successfully discovered the manifold structure of these two data sets. In both Figure 2 (a) and Figure 2 (b), the similarity between data of the same class are faithfully preserved, whilst the discrimination between classes are maximized.\nFigure 3 shows some low dimensional tensor representations of the images from the COIL-20 data set, which were learned by CMDA (a), CTMFA (b) and LMLRTA (c), respectively. Five classes were randomly selected, and low dimensional representations of five images were further randomly selected to show for each class. Particularly, in each sub-figure of Figure 3, each row shows the low dimensional tensor representations of images from one class. In contrast to the dimensionality of the original image, 32\u00d7 32, the dimensionality of the low dimensional representations here is 12 \u00d7 11. We can see that, all three methods can preserve the similarity between data of the same class faithfully. However, the discrimination between classes in the low dimensional tensor subspace learned by LMLRTA is much better than those learned by CMDA and CTMFA. Recognition results shown in Section 4.3 also demonstrate this observation."}, {"heading": "4.3 Object recognition results on the COIL-20 data set (2D tensors)", "text": "In this experiment, we compare LMLRTA with some related approaches on the object recognition application. The compared approaches include LDA, MFA, SDAE, CMDA, CTMFA and classification in the original space. We implemented experiment on the COIL-20 data set. To conduct this experiment, we empirically tested the dimensionality of the LDA subspace and that of the MFA subspace, and fixed them to 19 and 33, respectively. For the SDAE algorithm, we used a 6-layer neural network model. The sizes of the layers were 1024, 512, 256, 64, 32 and 20, respectively. For LMLRTA, CMDA and CTMFA, we just followed the settings as introduced in Section 4.1.\nFigure 4 shows the classification accuracy and standard deviation obtained by the compared methods. It is easy to see that, LMLRTA performed best among all the com-\npared methods, as it achieved 100% accuracy over all 5 folds of cross validation. Due to the loss of local structural information of the images, vector representation-based approaches, LDA and MFA, performed worst on this problem. Because of the limitation of training sample size, deep neural network model, SDAE, can not outperform LMLRTA on this problem and shew a large standard deviation. State-of-the-art tensor dimensionality reduction approaches, CMDA and CTMFA, can converge to a local optimal solution of the learning problem, but not perform as well as LMLRTA.\nTo show the convergence process of the MPFC algorithm during learning of the\nprojection matrices, Figure 5 illustrates the values of the objective function against iterations during the optimization of LMLRTA on the COIL-20 data set. As we can see, the MPFC algorithm converges to a stationary point of the problem as the iteration continues."}, {"heading": "4.4 Face recognition results on the ORL data set (3D tensors)", "text": "Figure 6 shows the classification accuracy and standard deviation obtained on the ORL data set. Due to high computational complexity problems of LDA, MFA and SDAE (the vector representations of the tensors is of dimensionality 32\u00d7 32\u00d7 28 = 28672), here we only compared LMLRTA to CMDA, CTMFA and the classification in the original data space. From Figure 6, we can see that LMLRTA consistently outperforms the\ncompared convergent tensor dimensionality reduction approaches. More importantly, as LMLRTA gradually reduces the ranks of the projection matrices during optimization, it can learn the dimensionality of the intrinsic low dimensional tensor space automatically from data. However, for traditional tensor dimensionality reduction algorithms, the parameter must be manually specified before they can be applied. This may result in unsatisfactory results on the applications."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a supervised tensor dimensionality reduction method, called large margin low rank tensor analysis (LMLRTA). LMLRTA can be utilized to auto-\nmatically and jointly learn the dimensionality and representations of low dimensional embeddings of tensors. This property makes it an effective simulation of the way how human brain represents perceived signals. To recognize new coming data, we employ similarity based classifiers in the learned tensor subspace, which corresponds to the recognition procedure of human brain (Rosch, 1973). Experiments on object recognition and face recognition show the superiority of LMLRTA over classic vector representation-based dimensionality reduction approaches, deep neural network models and existing tensor dimensionality reduction approaches. In future work, we attempt to extend LMLRTA to the scenarios of transfer learning (Pan and Yang, 2010) and active learning (Cohn et al., 1994), to simulate the way how human brain transfers knowledge from some source domains to a target domain, and the way how human brain actively generates questions and learns knowledge. Furthermore, we plan to combine LMLRTA with deep neural networks (LeCun et al., 2001) and non-negative matrix factorization\nmodels (Lee and Seung, 1999), to solve challenging large scale problems."}, {"heading": "Acknowledgments", "text": "We thank the Social Sciences and Humanities Research Council of Canada (SSHRC) as well as the Natural Sciences and Engineering Research Council of Canada (NSERC) for their financial support."}], "references": [{"title": "Generalized Discriminant Analysis Using a Kernel Approach", "author": ["G. Baudat", "F. Anouar"], "venue": "Neural Comput., 12(10):2385\u20132404.", "citeRegEx": "Baudat and Anouar,? 2000", "shortCiteRegEx": "Baudat and Anouar", "year": 2000}, {"title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, 15(6):1373\u20131396.", "citeRegEx": "Belkin and Niyogi,? 2003", "shortCiteRegEx": "Belkin and Niyogi", "year": 2003}, {"title": "Greedy Layer-Wise Training of Deep Networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "NIPS, pages 153\u2013160.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering", "author": ["Y. Bengio", "Paiement", "J.-F.", "P. Vincent", "O. Delalleau", "N.L. Roux", "M. Ouimet"], "venue": "NIPS.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Graph Theory with Applications", "author": ["J.A. Bondy", "U.S.R. Murty"], "venue": "Elsevier, North-Holland.", "citeRegEx": "Bondy and Murty,? 1976", "shortCiteRegEx": "Bondy and Murty", "year": 1976}, {"title": "Exact Matrix Completion via Convex Optimization", "author": ["E. Cand\u00e8s", "B. Recht"], "venue": "Commun. ACM, 55(6):111\u2013119. 26", "citeRegEx": "Cand\u00e8s and Recht,? 2012", "shortCiteRegEx": "Cand\u00e8s and Recht", "year": 2012}, {"title": "The Power of Convex Relaxation: Near-optimal Matrix Completion", "author": ["E. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory, 56(5):2053\u20132080.", "citeRegEx": "Cand\u00e8s and Tao,? 2010", "shortCiteRegEx": "Cand\u00e8s and Tao", "year": 2010}, {"title": "Spectral Graph Theory", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society.", "citeRegEx": "Chung,? 1997", "shortCiteRegEx": "Chung", "year": 1997}, {"title": "Improving Generalization with Active Learning", "author": ["D. Cohn", "R. Ladner", "A. Waibel"], "venue": "Machine Learning, pages 201\u2013221.", "citeRegEx": "Cohn et al\\.,? 1994", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Tensor Embedding Methods", "author": ["G. Dai", "Yeung", "D.-Y."], "venue": "AAAI, pages 330\u2013 335.", "citeRegEx": "Dai et al\\.,? 2006", "shortCiteRegEx": "Dai et al\\.", "year": 2006}, {"title": "Complete Discrete 2D Gabor Transforms by Neural Networks for Image Analysis and Compression", "author": ["J.G. Daugman"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, 36(7):1169\u20131179.", "citeRegEx": "Daugman,? 1988", "shortCiteRegEx": "Daugman", "year": 1988}, {"title": "Tensor Rank and the Ill-Posedness of the Best LowRank Approximation Problem", "author": ["V. de Silva", "Lim", "L.-H"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Silva et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2008}, {"title": "The Use of Multiple Measurements in Taxonomic Problems", "author": ["R.A. Fisher"], "venue": "Annals of Eugenics, 7(7):179\u2013188.", "citeRegEx": "Fisher,? 1936", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Image Classification Using Correlation Tensor Analysis", "author": ["Y. Fu", "T.S. Huang"], "venue": "Image Processing, IEEE Transactions on, 17(2):226\u2013234.", "citeRegEx": "Fu and Huang,? 2008", "shortCiteRegEx": "Fu and Huang", "year": 2008}, {"title": "Graph Implementations for Nonsmooth Convex Programs", "author": ["M. Grant", "S. Boyd"], "venue": "Blondel, V., Boyd, S., and Kimura, H., editors, Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95\u2013110. Springer-Verlag Limited. 27", "citeRegEx": "Grant and Boyd,? 2008", "shortCiteRegEx": "Grant and Boyd", "year": 2008}, {"title": "Locality Preserving Projections", "author": ["X. He", "P. Niyogi"], "venue": "NIPS.", "citeRegEx": "He and Niyogi,? 2003", "shortCiteRegEx": "He and Niyogi", "year": 2003}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Reducing the Dimensionality of Data with Neural Networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507.", "citeRegEx": "Hinton and Salakhutdinov,? 2006", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Tensor Decompositions and Applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Review, 51(3):455\u2013500.", "citeRegEx": "Kolda and Bader,? 2009", "shortCiteRegEx": "Kolda and Bader", "year": 2009}, {"title": "Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models", "author": ["N.D. Lawrence"], "venue": "Journal of Machine Learning Research, 6:1783\u20131816.", "citeRegEx": "Lawrence,? 2005", "shortCiteRegEx": "Lawrence", "year": 2005}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Intelligent Signal Processing, pages 306\u2013351. IEEE Press.", "citeRegEx": "LeCun et al\\.,? 2001", "shortCiteRegEx": "LeCun et al\\.", "year": 2001}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401(6755):788\u2013791.", "citeRegEx": "Lee and Seung,? 1999", "shortCiteRegEx": "Lee and Seung", "year": 1999}, {"title": "Sparse Non-negative Tensor Factorization Using Columnwise Coordinate Descent", "author": ["J. Liu", "J. Liu", "P. Wonka", "J. Ye"], "venue": "Pattern Recognition, 45(1):649\u2013656.", "citeRegEx": "Liu et al\\.,? 2012", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Tensor Distance Based Multilinear LocalityPreserved Maximum Information Embedding", "author": ["Y. Liu", "Y. Liu", "K.C.C. Chan"], "venue": "IEEE Transactions on Neural Networks, 21(11):1848\u20131854. 28", "citeRegEx": "Liu et al\\.,? 2010", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Nonlinear Component Analysis", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "M\u00fcller", "K.-R"], "venue": "Linear Embedding. Science,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Dimensionality Reduction of Multimodal Labeled Data by Local", "author": ["M. Sugiyama"], "venue": null, "citeRegEx": "Sugiyama,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama", "year": 2007}, {"title": "General Tensor Discriminant", "author": ["X. Li", "X. Wu", "S.J. Maybank"], "venue": "Fisher Discriminant Analysis. Journal of Machine Learning Research,", "citeRegEx": "D. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "D. et al\\.", "year": 2007}, {"title": "How to Grow a Mind: Statistics, Structure, and Abstraction", "author": ["J.B. Tenenbaum", "C. Kemp", "T.L. Griffiths", "N.D. Goodman"], "venue": "Science, 331(6022):1279\u20131285.", "citeRegEx": "Tenenbaum et al\\.,? 2011", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2011}, {"title": "Visualizing Data using t-SNE", "author": ["L. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "Journal of Machine Learning Research, 11:3371\u20133408.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "A Convengent Solution to Tensor Subspace Learning", "author": ["H. Wang", "S. Yan", "T.S. Huang", "X. Tang"], "venue": "IJCAI, pages 629\u2013634.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "NIPS.", "citeRegEx": "Weinberger et al\\.,? 2005", "shortCiteRegEx": "Weinberger et al\\.", "year": 2005}, {"title": "Graph Embedding and Extensions: A General Framework for Dimensionality Reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "Zhang", "H.-J.", "Q. Yang", "S. Lin"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 29(1):40\u201351.", "citeRegEx": "Yan et al\\.,? 2007", "shortCiteRegEx": "Yan et al\\.", "year": 2007}, {"title": "Two-Dimensional PCA: A New Approach to Appearance-Based Face Representation and Recognition", "author": ["J. Yang", "D. Zhang", "A.F. Frangi", "Yang", "J.-Y."], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 26(1):131\u2013137.", "citeRegEx": "Yang et al\\.,? 2004", "shortCiteRegEx": "Yang et al\\.", "year": 2004}, {"title": "Two-Dimensional Linear Discriminant Analysis", "author": ["J. Ye", "R. Janardan", "Q. Li"], "venue": "NIPS.", "citeRegEx": "Ye et al\\.,? 2004", "shortCiteRegEx": "Ye et al\\.", "year": 2004}, {"title": "Gaussian Process Latent Random Field", "author": ["G. Zhong", "Li", "W.-J.", "Yeung", "D.-Y.", "X. Hou", "Liu", "C.-L."], "venue": "AAAI. 30", "citeRegEx": "Zhong et al\\.,? 2010", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 33, "context": "Specifically, some representative tensor dimensionality reduction approaches include (Yang et al., 2004; Ye et al., 2004) and (Wang et al.", "startOffset": 85, "endOffset": 121}, {"referenceID": 34, "context": "Specifically, some representative tensor dimensionality reduction approaches include (Yang et al., 2004; Ye et al., 2004) and (Wang et al.", "startOffset": 85, "endOffset": 121}, {"referenceID": 30, "context": ", 2004) and (Wang et al., 2007).", "startOffset": 12, "endOffset": 31}, {"referenceID": 30, "context": "In particular, the approach presented in (Wang et al., 2007) is theoretically guaranteed to converge to a local optimal solution of the learning problem.", "startOffset": 41, "endOffset": 60}, {"referenceID": 4, "context": "LMLRTA is aimed at learning the low dimensional representations of tensors using techniques of multi-linear algebra (Northcott, 1984) and graph theories (Bondy and Murty, 1976).", "startOffset": 153, "endOffset": 176}, {"referenceID": 33, "context": "Furthermore, unlike previous tensor dimensionality reduction approaches (Yang et al., 2004; Ye et al., 2004; Wang et al., 2007), which can only learn", "startOffset": 72, "endOffset": 127}, {"referenceID": 34, "context": "Furthermore, unlike previous tensor dimensionality reduction approaches (Yang et al., 2004; Ye et al., 2004; Wang et al., 2007), which can only learn", "startOffset": 72, "endOffset": 127}, {"referenceID": 30, "context": "Furthermore, unlike previous tensor dimensionality reduction approaches (Yang et al., 2004; Ye et al., 2004; Wang et al., 2007), which can only learn", "startOffset": 72, "endOffset": 127}, {"referenceID": 24, "context": "kernel principal component analysis (KPCA) (Sch\u00f6lkopf et al., 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000).", "startOffset": 43, "endOffset": 67}, {"referenceID": 0, "context": ", 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000).", "startOffset": 52, "endOffset": 77}, {"referenceID": 3, "context": "However, these methods only work on a given set of data points, and cannot be easily extended to out-of-sample data (Bengio et al., 2003).", "startOffset": 116, "endOffset": 137}, {"referenceID": 15, "context": "To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points.", "startOffset": 65, "endOffset": 86}, {"referenceID": 25, "context": "To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points.", "startOffset": 133, "endOffset": 149}, {"referenceID": 32, "context": "proposed a general framework known as graph embedding for dimensionality reduction (Yan et al., 2007).", "startOffset": 83, "endOffset": 101}, {"referenceID": 0, "context": ", 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000). Since about a decade ago, many manifold learning approaches have been proposed. These manifold learning approaches, including isometric feature mapping (Isomap) (Tenenbaum et al., 2000) and locally linear embedding (LLE) (Roweis and Saul, 2000), can faithfully preserve global or local geometrical properties of the nonlinear structure of data. However, these methods only work on a given set of data points, and cannot be easily extended to out-of-sample data (Bengio et al., 2003). To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points. In particular, Yan et al. proposed a general framework known as graph embedding for dimensionality reduction (Yan et al., 2007). Most of the spectral learning-based approaches, either linear or nonlinear, either supervised or unsupervised, are contained in this framework. Furthermore, based on this framework, the authors proposed the marginal Fisher analysis (MFA) algorithm for supervised linear dimensionality reduction. In the research of probabilistic learning models, Lawrence (2005) proposed the Gaussian process latent variable models (GPLVM), which extends PCA to a probabilistic nonlinear formulation.", "startOffset": 53, "endOffset": 1329}, {"referenceID": 0, "context": ", 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000). Since about a decade ago, many manifold learning approaches have been proposed. These manifold learning approaches, including isometric feature mapping (Isomap) (Tenenbaum et al., 2000) and locally linear embedding (LLE) (Roweis and Saul, 2000), can faithfully preserve global or local geometrical properties of the nonlinear structure of data. However, these methods only work on a given set of data points, and cannot be easily extended to out-of-sample data (Bengio et al., 2003). To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points. In particular, Yan et al. proposed a general framework known as graph embedding for dimensionality reduction (Yan et al., 2007). Most of the spectral learning-based approaches, either linear or nonlinear, either supervised or unsupervised, are contained in this framework. Furthermore, based on this framework, the authors proposed the marginal Fisher analysis (MFA) algorithm for supervised linear dimensionality reduction. In the research of probabilistic learning models, Lawrence (2005) proposed the Gaussian process latent variable models (GPLVM), which extends PCA to a probabilistic nonlinear formulation. Combining a Gaussian Markov random field prior with GPLVM, Zhong et al. (2010) proposed the Gaussian process latent random field model, which can be", "startOffset": 53, "endOffset": 1530}, {"referenceID": 16, "context": "To exploit the effect of deep architecture for dimensionality reduction, some other deep neural network models were also introduced, such as deep belief nets (DBN) (Hinton et al., 2006), stacked autoencoder (SAE) (Bengio et al.", "startOffset": 164, "endOffset": 185}, {"referenceID": 2, "context": ", 2006), stacked autoencoder (SAE) (Bengio et al., 2006) and stacked denoise autoencoder (SDAE) (Vincent et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 29, "context": ", 2006) and stacked denoise autoencoder (SDAE) (Vincent et al., 2010).", "startOffset": 47, "endOffset": 69}, {"referenceID": 33, "context": "To alleviate these problems, 2DPCA (Yang et al., 2004) and 2DLDA (Ye et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 34, "context": ", 2004) and 2DLDA (Ye et al., 2004) were proposed to extend the original PCA and LDA algorithms to work directly on 2D matrices rather than 1D vectors.", "startOffset": 18, "endOffset": 35}, {"referenceID": 32, "context": "In recent years, many other approaches (Yan et al., 2007; Tao et al., 2007; Fu and Huang, 2008; Liu et al., 2012, 2010) were also proposed to deal with high-order tensor problems.", "startOffset": 39, "endOffset": 119}, {"referenceID": 13, "context": "In recent years, many other approaches (Yan et al., 2007; Tao et al., 2007; Fu and Huang, 2008; Liu et al., 2012, 2010) were also proposed to deal with high-order tensor problems.", "startOffset": 39, "endOffset": 119}, {"referenceID": 13, "context": "In the area of neural network research, Hinton and Salakhutdinov (2006) proposed a deep neural network model called autoencoder for dimensionality reduction.", "startOffset": 40, "endOffset": 72}, {"referenceID": 2, "context": ", 2006), stacked autoencoder (SAE) (Bengio et al., 2006) and stacked denoise autoencoder (SDAE) (Vincent et al., 2010). These studies show that deep neural networks can generally learn high level representations of data, which can benefit subsequent recognition tasks. All of the above approaches assume that the input data are in the form of vectors. In many real world applications, however, the objects are essentially represented as highorder tensors, such as 2D images or 3D textures. One have to unfold these tensors into one-dimensional vectors first before the dimensionality reduction approaches can be applied. In this case, some useful information in the original data may not be sufficiently preserved. Moreover, high-dimensional vectorized representations suffer from the curse of dimensionality, as well as high computational cost. To alleviate these problems, 2DPCA (Yang et al., 2004) and 2DLDA (Ye et al., 2004) were proposed to extend the original PCA and LDA algorithms to work directly on 2D matrices rather than 1D vectors. In recent years, many other approaches (Yan et al., 2007; Tao et al., 2007; Fu and Huang, 2008; Liu et al., 2012, 2010) were also proposed to deal with high-order tensor problems. In particular, Wang et al. (2007) proposed a tensor dimensionality reduction method based on the graph embedding framework, which is proved that it is the first method to give a convergent solution.", "startOffset": 36, "endOffset": 1259}, {"referenceID": 18, "context": "In this section, we first introduce the used notation and some basic terminologies on tensor operations (Kolda and Bader, 2009; Dai and Yeung, 2006).", "startOffset": 104, "endOffset": 148}, {"referenceID": 7, "context": "It can be considered as a graph Laplacian-parameterized loss function with respect to the low dimensional embeddings (Chung, 1997; Belkin and Niyogi, 2003; Tenenbaum et al., 2011), where each node corresponds to one tensor datum in the given data set.", "startOffset": 117, "endOffset": 179}, {"referenceID": 1, "context": "It can be considered as a graph Laplacian-parameterized loss function with respect to the low dimensional embeddings (Chung, 1997; Belkin and Niyogi, 2003; Tenenbaum et al., 2011), where each node corresponds to one tensor datum in the given data set.", "startOffset": 117, "endOffset": 179}, {"referenceID": 27, "context": "It can be considered as a graph Laplacian-parameterized loss function with respect to the low dimensional embeddings (Chung, 1997; Belkin and Niyogi, 2003; Tenenbaum et al., 2011), where each node corresponds to one tensor datum in the given data set.", "startOffset": 117, "endOffset": 179}, {"referenceID": 6, "context": "Following recent work in matrix completion (Cand\u00e8s and Tao, 2010; Cand\u00e8s and Recht, 2012), we replace it with its convex envelope \u2014 the nuclear norm of Ul, which is defined as the sum of its singular values, i.", "startOffset": 43, "endOffset": 89}, {"referenceID": 5, "context": "Following recent work in matrix completion (Cand\u00e8s and Tao, 2010; Cand\u00e8s and Recht, 2012), we replace it with its convex envelope \u2014 the nuclear norm of Ul, which is defined as the sum of its singular values, i.", "startOffset": 43, "endOffset": 89}, {"referenceID": 15, "context": "Remark 1 (Relation to previous works) 1) LMLRTA can be considered as a supervised multi-linear extension of locality preserving projections (LPP) (He and Niyogi, 2003), in that the second term of the objective function in Problem (4) forces neighboring data in a same class to be close in the low dimensional tensor subspace;", "startOffset": 146, "endOffset": 167}, {"referenceID": 32, "context": "2) LMLRTA can also be considered as a reformulation of tensor marginal Fisher analysis (TMFA) (Yan et al., 2007).", "startOffset": 94, "endOffset": 112}, {"referenceID": 30, "context": "However, TMFA is not guaranteed to converge to a local optimum of the optimization problem (Wang et al., 2007), but LMLRTA is guaranteed as proved in Section 3.", "startOffset": 91, "endOffset": 110}, {"referenceID": 31, "context": "3) For Problem (4), we can consider it as a variant of the Large Margin Nearest Neighbor (LMNN) algorithm (Weinberger et al., 2005) for distance metric learning in tensor space.", "startOffset": 106, "endOffset": 131}, {"referenceID": 16, "context": "5) Unlike deep neural network models (Hinton et al., 2006; Bengio et al., 2006; Vincent et al., 2010), which simulate human brain\u2019s hierarchical structure, LMLRTA mimics the way of human perception.", "startOffset": 37, "endOffset": 101}, {"referenceID": 2, "context": "5) Unlike deep neural network models (Hinton et al., 2006; Bengio et al., 2006; Vincent et al., 2010), which simulate human brain\u2019s hierarchical structure, LMLRTA mimics the way of human perception.", "startOffset": 37, "endOffset": 101}, {"referenceID": 29, "context": "5) Unlike deep neural network models (Hinton et al., 2006; Bengio et al., 2006; Vincent et al., 2010), which simulate human brain\u2019s hierarchical structure, LMLRTA mimics the way of human perception.", "startOffset": 37, "endOffset": 101}, {"referenceID": 30, "context": "Similar to previous approaches on tensor dimensionality reduction (Dai and Yeung, 2006; Wang et al., 2007), here we solve Problem (4) using an iterative optimization algorithm.", "startOffset": 66, "endOffset": 106}, {"referenceID": 14, "context": "Here, for each Wl, problem (4) is a semi-definite programming problem, which can be solved using off-the-shelf algorithms, such as SeDuMi1 and CVX (Grant and Boyd, 2008).", "startOffset": 147, "endOffset": 169}, {"referenceID": 10, "context": "This is mainly based on the fact that the kernels of the Gabor filters resemble the receptive field profiles of the mammalian cortical simple cells (Daugman, 1988), which enhances our learning model to better mimic the way of human perception.", "startOffset": 148, "endOffset": 163}, {"referenceID": 12, "context": "To show the advantage of our proposed method, LMLRTA, we compared it with two classic vector representation-based dimensionality reduction approaches \u2013 linear discriminant analysis (LDA) (Fisher, 1936) and marginal Fisher analysis (MFA) (Yan et al.", "startOffset": 187, "endOffset": 201}, {"referenceID": 32, "context": "To show the advantage of our proposed method, LMLRTA, we compared it with two classic vector representation-based dimensionality reduction approaches \u2013 linear discriminant analysis (LDA) (Fisher, 1936) and marginal Fisher analysis (MFA) (Yan et al., 2007), one deep neural networks model called stacked denoising autoencoder (SDAE) (Vincent et al.", "startOffset": 237, "endOffset": 255}, {"referenceID": 29, "context": ", 2007), one deep neural networks model called stacked denoising autoencoder (SDAE) (Vincent et al., 2010), and two state-of-the-art tensor dimensionality reduction methods \u2013 convergent multi-linear discriminant analysis (CMDA) and convergent tensor margin Fisher analysis (CTMFA) (Wang et al.", "startOffset": 84, "endOffset": 106}, {"referenceID": 30, "context": ", 2010), and two state-of-the-art tensor dimensionality reduction methods \u2013 convergent multi-linear discriminant analysis (CMDA) and convergent tensor margin Fisher analysis (CTMFA) (Wang et al., 2007).", "startOffset": 182, "endOffset": 201}, {"referenceID": 8, "context": "In future work, we attempt to extend LMLRTA to the scenarios of transfer learning (Pan and Yang, 2010) and active learning (Cohn et al., 1994), to simulate the way how human brain transfers knowledge from some source domains to a target domain, and the way how human brain actively generates questions and learns knowledge.", "startOffset": 123, "endOffset": 142}, {"referenceID": 20, "context": "Furthermore, we plan to combine LMLRTA with deep neural networks (LeCun et al., 2001) and non-negative matrix factorization", "startOffset": 65, "endOffset": 85}, {"referenceID": 21, "context": "models (Lee and Seung, 1999), to solve challenging large scale problems.", "startOffset": 7, "endOffset": 28}], "year": 2013, "abstractText": "Other than vector representations, the direct objects of human cognition are generally high-order tensors, such as 2D images and 3D textures. From this fact, two interesting questions naturally arise: How does the human brain represent these tensor perceptions in a \u201cmanifold\u201d way, and how can they be recognized on the \u201cmanifold\u201d? In this paper, we present a supervised model to learn the intrinsic structure of the tensors embedded in a high dimensional Euclidean space. With the fixed point continuation procedures, our model automatically and jointly discovers the optimal dimensionality and the repar X iv :1 30 6. 26 63 v1 [ cs .L G ] 1 1 Ju n 20 13 resentations of the low dimensional embeddings. This makes it an effective simulation of the cognitive process of human brain. Furthermore, the generalization of our model based on similarity between the learned low dimensional embeddings can be viewed as counterpart of recognition of human brain. Experiments on applications for object recognition and face recognition demonstrate the superiority of our proposed model over state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}