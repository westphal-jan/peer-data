{"id": "1306.3212", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2013", "title": "Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation", "abstract": "The 1/8th L1 - regularized plattsburgh Gaussian maximum likelihood smilon estimator (MLE) theorized has rwenzori been galyani shown waggled to have anti-coalition strong zeytinburnu statistical guarantees in recovering designated a sparse inverse valjavec covariance massaging matrix, rearden or alternatively the underlying ekuban graph structure of tiwana a radian Gaussian Markov Random strummer Field, shahram from very limited samples. zeuner We wamidh propose arino a novel algorithm \u0622\u0628\u0627\u062f for solving the stepdaughter resulting optimization high-handed problem which is fouls a jajpur regularized log - determinant 37-31 program. In punji contrast nikbakht to mouthwashes recent wasim state - criddle of - the - art spacetimes methods hunua that ashtian largely watercolor use first burrowing order 12-13 gradient morto information, t\u00fcrkic our freegan algorithm blackarachnia is based on 1272 Newton ' hillbillies s karavan method yesh and reoccurs employs a decamped quadratic atlus approximation, betweem but with some arnis modifications that bowlus leverage culverted the rfp structure arulanantham of meisner the sparse holmesburg Gaussian MLE problem. freefalling We show that mini-albums our landowski method is superlinearly light-year convergent, aldehyde and present experimental moyale results using bloemendaal synthetic 1,000-odd and relatives real - assynt world owens-illinois application sq. data that 23.19 demonstrate kivelson the considerable improvements in 2,983 performance pyles of our reiterate method reducer when tinashe compared to post-production other state - segredo of - the - p50 art methods.", "histories": [["v1", "Thu, 13 Jun 2013 19:51:59 GMT  (160kb)", "http://arxiv.org/abs/1306.3212v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["cho-jui hsieh", "m\u00e1ty\u00e1s a sustik", "inderjit s dhillon", "pradeep ravikumar"], "accepted": true, "id": "1306.3212"}, "pdf": {"name": "1306.3212.pdf", "metadata": {"source": "CRF", "title": "Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation", "authors": ["Cho-Jui Hsieh", "M\u00e1ty\u00e1s A. Sustik", "Inderjit S. Dhillon", "Pradeep Ravikumar"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n32 12\nv1 [\ncs .L\nG ]\n1 3"}, {"heading": "1 Introduction", "text": "Statistical problems in modern data settings are increasingly high-dimensional, where the number of parameters is very large, potentially outnumbering even the number of observations. An important class of such problems involves estimating the graph structure of a Gaussian Markov random field (GMRF), with applications ranging from biological inference in gene networks, analysis of fMRI brain connectivity data and analysis of interactions in social networks. Specifically, given n independently drawn samples {y1,y2, . . . ,yn} from a p-variate Gaussian distribution, so that yi \u223c N (\u00b5,\u03a3), the task is to estimate its inverse covariance matrix \u03a3\u22121, also referred to as the precision or concentration matrix. The non-zero pattern of this inverse covariance matrix \u03a3\u22121 can be shown to correspond to the underlying graph structure of the GMRF. An active line of work in high-dimensional settings, where p \u226b n, is based on imposing constraints on the model space; in the GMRF case a common structured constraint is that of sparsity of the inverse covariance matrix. Accordingly, recent papers by Banerjee et al. [2008], Friedman et al. [2008], Yuan and Lin [2007] have proposed an estimator that minimizes the Gaussian negative log-likelihood regularized by the \u21131 norm of the entries (typically restricted to those on the off-diagonal) of the inverse covariance matrix, which encourages sparsity in its entries. The resulting\noptimization problem is a log-determinant program, which is convex, and can be solved in polynomial time.\nFor such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the \u21131-regularized Gaussian MLE?\nFor superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner.\nIn this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the \u21131-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al. [2010]), which we then solve using coordinate descent. A key facet of our method is that we are able to reduce the computational cost of a coordinate descent update from the naive O(p2) to O(p) complexity by exploiting the structure present in the problem, and by a careful arrangement and caching of the computations. Furthermore, an Armijo-rule based step size selection rule ensures sufficient descent and positive definiteness of the intermediate iterates. Finally, we use the form of the stationary condition characterizing the optimal solution to focus the Newton direction computation on a small subset of free variables, but in a manner that preserves the strong convergence guarantees of second-order descent. We note that when the solution has a block-diagonal structure as described in Mazumder and Hastie [2012], Witten et al. [2011], the fixed/free set selection in QUIC can automatically identify this sparsity structure and avoid updates to the off-diagonal block elements.\nA preliminary version of this paper appeared in the NIPS 2011 conference [Hsieh et al., 2011]. This paper contains the following enhancements to the conference version: (i) inclusion of generalized matrix regularized in the problem statement and subsequent analysis, (ii) detailed convergence analysis of our proposed method, QUIC, (iii) detailed experimental comparisons under different parameters and precision/recall curves for synthetic data, and (iv) new material on the relationship of the free and fixed sets in QUIC with recent covariance thresholding techniques (see Section 3.4 for more details).\nThe outline of the paper is as follows. We start with a review of related work and the problem setup in Section 2. In Section 3, we present our algorithm that combines quadratic approximation, Newton\u2019s method and coordinate descent. In Section 4, we show superlinear convergence of our method. We summarize the experimental results in Section 5, where we compare the algorithm using both real data and synthetic examples from Li and Toh [2010]. We observe that our algorithm performs overwhelmingly better (quadratic instead of linear convergence) than existing solutions described in the literature.\nNotation. In this paper, boldfaced lowercase letters denote vectors and uppercase letters denote p \u00d7 p real matrices. Sp++ denotes the space of p \u00d7 p symmetric positive definite matrices and X \u227b 0 and X 0 means that X is positive definite or positive semidefinite, respectively. The vectorized listing of the elements of a p \u00d7 p matrix X is denoted by vec(X) \u2208 Rp 2\nand the Kronecker product of the matrices X and Y is denoted by X \u2297 Y . For a real-valued function f(X), \u2207f(X) is a p \u00d7 p matrix with (i, j) element equal to \u2202\u2202Xij f(X) and denoted by \u2207ijf(X), while \u2207 2f(X) is the p2 \u00d7 p2 Hessian matrix. We will use the \u21131 and \u2113\u221e norms defined on the vectorized form of matrix X: \u2016X\u20161 := \u2211\ni,j |Xij | and \u2016X\u2016\u221e := maxi,j |Xij |. We also employ elementwise \u21131-regularization, \u2016X\u20161,\u039b := \u2211\ni,j \u03bbij |Xij |, where \u039b = [\u03bbij] with \u03bbij > 0 for off-diagonal elements, and \u03bbii \u2265 0 for diagonal elements."}, {"heading": "2 Background and Related work", "text": "Let y be a p-variate Gaussian random vector, with distribution N (\u00b5,\u03a3). Given n independently drawn samples {y1, . . . ,yn} of this random vector, the sample covariance matrix can be written as\nS = 1\nn\u2212 1\nn \u2211\nk=1\n(yk \u2212 \u00b5\u0302)(yk \u2212 \u00b5\u0302) T , where \u00b5\u0302 =\n1\nn\nn \u2211\nk=1\nyk. (1)\nGiven a regularization penalty \u03bb > 0, the \u21131-regularized Gaussian MLE for the inverse covariance matrix can be written as the solution of the following regularized log-determinant program:\nargmin X\u227b0\n{\n\u2212 log detX + tr(SX) + \u03bb\np \u2211\ni,j=1\n|Xij |\n}\n. (2)\nThe \u21131 regularization promotes sparsity in the inverse covariance matrix, and thus encourages a sparse graphical model structure. We consider a generalized weighted \u21131 regularization, where given a symmetric nonnegative weight matrix \u039b = [\u03bbij], we can assign different nonnegative weights to different entries, obtaining the regularization term \u2016X\u20161,\u039b = \u2211p i,j=1 \u03bbij |Xij |. In this paper we will focus on solving the following generalized sparse inverse covariance estimation problem:\nargmin X\u227b0\n{\n\u2212 log detX + tr(SX) + \u2016X\u20161,\u039b\n}\n= argmin X\u227b0 f(X), (3)\nwhere X\u2217 = (\u03a3\u2217)\u22121. In order to ensure that problem (3) has a unique minimizer, as we show later, it is sufficient to require that \u03bbij > 0 for off-diagonal entries, and \u03bbii \u2265 0 for diagonal entries. The standard off-diagonal \u21131 regularization variant \u03bb \u2211 I 6=j |Xij | is a special\ncase of this weighted regularization function. For further details on the background and utility of \u21131 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al. [2008], Ravikumar et al. [2011], Duchi et al. [2008].\nDue in part to its importance, there has been an active line of work on efficient optimization methods for solving (2) and (3). Since the regularization term is non-smooth and hard to solve, many methods aim to solve the dual problem of (3):\n\u03a3\u2217 = argmax |Wij\u2212Sij |\u2264\u03bbij log detW, (4)\nwhich has a smooth objective function with bounded constraints. Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM.\nOther first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO. However, each coordinate update of SINCO has a time complexity of O(p2), which becomes computationally prohibitive when handling large problems. We will show in this paper that after forming the quadratic approximation, the time complexity of one coordinate update can be performed in O(p) operations. This trick is one of the key advantages of our proposed method, QUIC.\nOne common characteristic of the above methods is that they are first-order iterative methods that mainly use gradient information at each step. Such first-order methods have become increasingly popular in recent years for high-dimensional problems in part due to their ease of implementation, and because they require very little computation and memory at each step. The caveat is that they have at most linear rates of convergence (Bertsekas [1995]). To achieve superlinear convergence rates, one has to consider second-order methods, which have attracted some attention only recently for the sparse inverse covariance estimation problem. Li and Toh [2010] handle the non-smoothness of the \u21131 regularization in the objective function by doubling the number of variables, and solving the resulting constrained optimization problem by an inexact interior point method. Schmidt et al. [2009] propose a second order Projected Quasi-Newton method (PQN) that solves the dual problem (4), since the dual objective function is smooth. The key difference of our method when compared to these recent second order solvers is that we directly solve the \u21131-regularized primal objective using a second-order method. As we show, this allows us\nto leverage structure in the problem, and efficiently approximate the generalized Newton direction using coordinate descent. Subsequent to the preliminary version of this paper (see [Hsieh et al., 2011]), Olsen et al. [2012] have proposed generalizations to our framework to allow various inner solvers such as FISTA, conjugate gradient (CG), and LBFGS to be used, in addition to our proposed coordinate descent scheme."}, {"heading": "3 Quadratic Approximation Method", "text": "We first note that the objective f(X) in the non-differentiable optimization problem (3), can be written as the sum of two parts, f(X) = g(X) + h(X), where\ng(X) = \u2212 log detX + tr(SX) and h(X) = \u2016X\u20161,\u039b. (5)\nThe first component g(X) is twice differentiable, and strictly convex. The second part, h(X), is convex but non-differentiable. Following the approach of Tseng and Yun [2007] and Yun and Toh [2011], we build a quadratic approximation around any iterate Xt for this composite function by first considering the second-order Taylor expansion of the smooth component g(X):\ng\u0304Xt(\u2206) \u2261 g(Xt) + vec(\u2207g(Xt)) T vec(\u2206) +\n1 2 vec(\u2206)T\u22072g(Xt) vec(\u2206). (6)\nThe Newton direction Dt for the entire objective f(X) can then be written as the solution of the regularized quadratic program:\nDt = argmin \u2206\n{ g\u0304Xt(\u2206) + h(Xt +\u2206) } . (7)\nWe use this Newton direction to compute our iterative estimates {Xt} for the solution of the optimization problem (3). This variant of Newton method for such composite objectives is also referred to as a \u201cproximal Newton-type method,\u201d and was empirically studied in Schmidt [2010]. Tseng and Yun [2007] considered the more general case where the Hessian \u22072g(Xt) is replaced by any positive definite matrix. See also the recent paper by Lee et al. [2012], where convergence properties of such general proximal Newton-type methods are discussed. We note that a key caveat to applying such second-order methods in high-dimensional settings is that the computation of the Newton direction appears to have a large time complexity, which is one reason why first-order methods have been so popular for solving the high-dimensional \u21131-regularized Gaussian MLE.\nLet us delve into the Newton direction computation in (7). Note that it can be rewritten as a standard Lasso regression problem [Tibshirani, 1996]:\nargmin \u2206\n1 2 \u2016H 1 2 vec(\u2206) +H\u2212 1 2b\u201622 + \u2016vec(Xt +\u2206)\u20161,\u039b, (8)\nwhere H = \u22072g(Xt) and b = vec(\u2207g(Xt)). Many efficient optimization methods exist that solve Lasso regression problems, such as the coordinate descent method [Meier et al., 2008], the gradient projection method [Polyak, 1969], and iterative shrinking methods [Daubechies et al.,\n2004, Beck and Teboulle, 2009]. When applied to the Lasso problem of (7), most of these optimization methods would require the computation of the gradient of g\u0304Xt(\u2206):\n\u2207g\u0304Xt(\u2206) = H vec(\u2206) + b. (9)\nThe straightforward approach for computing (9) for a general p2 \u00d7 p2 Hessian matrix H would take O(p4) time, making it impractical for large problems. Fortunately, for the sparse inverse covariance problem (3), the Hessian matrix H has the following special form (see for instance [Boyd and Vandenberghe, 2009, Chapter A.4.3]):\nH = \u22072g(Xt) = X \u22121 t \u2297X \u22121 t .\nWe show how to exploit this special form of the Hessian matrix to perform one coordinate descent step that updates one element of \u2206 in O(p) time. Hence a full sweep of coordinate descent steps over all the variables requires O(p3) time. This key observation is one of the reasons that makes our Newton-like method viable for solving the inverse covariance estimation problem.\nThere exist other functions which allow efficient computation of Hessian times vector. As an example, we consider the case of \u21131-regularized logistic regression. Suppose we are given n samples with feature vectors x1, . . . ,xn \u2208 R\np and labels y1, . . . , yn, and we solve the following optimization problem to compute the model parameter w:\narg min w\u2208Rp\nn \u2211\ni=1\nlog(1 + e\u2212yiw T xi) + \u03bb\u2016w\u20161.\nFollowing our earlier approach, we can decompose this objective function into smooth and non-smooth parts, g(w) + h(w), where\ng(w) =\nn \u2211\ni=1\nlog(1 + e\u2212yiw T xi) and h(w) = \u03bb\u2016w\u20161.\nIn order to apply coordinate descent to solve the quadratic approximation, we have to compute the gradient as in (9). The Hessian matrix \u22072g(w) is a p \u00d7 p matrix, so direct computation of this gradient costs O(p2) flops. However, the Hessian matrix for logistic regression has the following simple form\nH = \u22072g(w) = XDXT ,\nwhere D is a diagonal matrix with Dii = e\u2212yiw\nT xi\n(1+e\u2212yiw T xi )2\nand X = [x1, x2, . . . , xn].\nTherefore we can write\n\u2207g\u0304Xt(\u2206) = (\u2207 2g(w)) vec(\u2206) + b = XD(XT vec(\u2206)) + b. (10)\nThe time complexity to compute (10) is only proportional to the number of nonzero elements in the data matrix X, which can be much smaller than O(p2) for sparse datasets. Therefore similar quadratic approximation approaches are also efficient for solving the\n\u21131-regularized logistic regression problem as shown by [Friedman et al., 2010, Yuan et al., 2012].\nIn the sequel, we detail three innovations which make our quadratic approximation algorithm feasible for solving large sparse inverse covariance problems. First, we approximate the Newton direction computation using an efficient coordinate descent method that exploits the structure of Hessian matrix, so that we reduce the time cost of each coordinate descent update step from O(p2) to O(p). Second, we employ an Armijo-rule based step size selection to ensure sufficient descent and positive-definiteness of the next iterate. Finally, we use the form of the stationary condition characterizing the optimal solution, to focus the Newton direction computation to a small subset of free variables, in a manner that preserves the strong convergence guarantees of second-order descent. We outline each of these three innovations in the following three subsections. A high level overview of our method is presented in Algorithm 1.\nAlgorithm 1: Quadratic Approximation method for Sparse Inverse Covariance Learning (QUIC overview)\nInput : Empirical covariance matrix S (positive semi-definite, p\u00d7 p), regularization parameter matrix \u039b, initial iterate X0, inner stopping tolerance \u01eb. Output: Sequence {Xt} that converges to argminX\u227b0 f(X), where f(X) = g(X)+h(X), where g(X) = \u2212 log detX +tr(SX), h(X) = \u2016X\u20161,\u039b.\n1 for t = 0, 1, . . . do 2 Compute Wt = X \u22121 t . 3 Form the second order approximation f\u0304Xt(\u2206) := g\u0304Xt(\u2206) + h(Xt +\u2206) to f(Xt +\u2206). 4 Partition the variables into free and fixed sets based on the gradient, see Section 3.3. 5 Use coordinate descent to find the Newton direction Dt = argmin\u2206 f\u0304Xt(Xt +\u2206) over the set of free variables, see (13) and (16) in Section 3.1. (A Lasso problem.) 6 Use an Armijo-rule based step-size selection to get \u03b1 such that Xt+1 = Xt + \u03b1Dt is positive definite and there is sufficient decrease in the objective function, see (21) in Section 3.2.\n7 end"}, {"heading": "3.1 Computing the Newton Direction", "text": "In order to compute the Newton direction, we have to solve the Lasso problem (7). From [Boyd and Vandenberghe, 2009, Chapter A.4.3], the gradient and Hessian for g(X) = \u2212 log detX + tr(SX) are\n\u2207g(X) = S \u2212X\u22121 and \u22072g(X) = X\u22121 \u2297X\u22121. (11)\nIn order to formulate our problem accordingly, we can verify that for a symmetric matrix \u2206 we have tr(X\u22121t \u2206X \u22121 t \u2206) = vec(\u2206) T (X\u22121t \u2297X \u22121 t ) vec(\u2206), so that g\u0304Xt(\u2206) in (7) can be\nrewritten as\ng\u0304Xt(\u2206) = \u2212 log detXt + tr(SXt) + tr((S \u2212Wt) T\u2206) +\n1 2 tr(Wt\u2206Wt\u2206), (12)\nwhere Wt = X \u22121 t .\nIn Friedman et al. [2007], Wu and Lange [2008], the authors show that coordinate descent methods are very efficient for solving Lasso type problems. An obvious way to update each element of \u2206 (to solve (7)) requires O(p2) floating point operations since Wt \u2297 Wt is a p\n2 \u00d7 p2 matrix, thus yielding an O(p4) procedure. As we show below, our implementation reduces the cost of one variable update to O(p) by exploiting the structure of the specific form of the second order term tr(Wt\u2206Wt\u2206).\nFor notational simplicity, we will omit the iteration index t in the derivations below where we only discuss a single Newton iteration; this applies to the rest of the this section and section 3.2 as well. (Hence, the notation for g\u0304Xt is also simplified to g\u0304.) Furthermore, we omit the use of a separate index for the coordinate descent updates. Thus, we simply use D to denote the current iterate approximating the Newton direction and use D\u2032 for the updated direction. Consider the coordinate descent update for the variable Xij , with i < j that preserves symmetry: D\u2032 = D+\u00b5(eie T j +eje T i ). The solution of the one-variable problem corresponding to (7) yields \u00b5:\nargmin \u00b5\ng\u0304(D + \u00b5(eie T j + eje T i )) + 2\u03bbij |Xij +Dij + \u00b5|. (13)\nWe expand the terms appearing in the definition of g\u0304 after substituting D\u2032 = D+\u00b5(eie T j + eje T i ) for \u2206 in (12) and omit the terms not dependent on \u00b5. The contribution of tr(SD\n\u2032)\u2212 tr(WD\u2032) yields 2\u00b5(Sij\u2212Wij), while the regularization term contributes 2\u03bbij |Xij+Dij+\u00b5|, as seen from (13). The quadratic term can be rewritten (using the fact that tr(AB) = tr(BA) and the symmetry of D and W ) to yield:\ntr(WD\u2032WD\u2032) = tr(WDWD) + 4\u00b5wTi Dwj + 2\u00b5 2(W 2ij +WiiWjj), (14)\nwhere wi refers to the i-th column of W . In order to compute the single variable update we seek the minimum of the following quadratic function of \u00b5:\n1 2 (W 2ij +WiiWjj)\u00b5 2 + (Sij \u2212Wij +w T i Dwj)\u00b5 + \u03bbij|Xij +Dij + \u00b5|. (15)\nLetting a = W 2ij +WiiWjj, b = Sij \u2212Wij +w T i Dwj, and c = Xij +Dij the minimum is achieved for:\n\u00b5 = \u2212c+ S(c\u2212 b/a, \u03bbij/a), (16)\nwhere\nS(z, r) = sign(z)max{|z| \u2212 r, 0} (17)\nis the soft-thresholding function. Similarly, when i = j, for D\u2032 = D + \u00b5eie T i , we get\ntr(WD\u2032WD\u2032) = tr(WDWD) + 2\u00b5wTi Dwi + \u00b5 2(W 2ii). (18)\nTherefore the update rule for Dii can be computed by (16) with a = W 2 ii, b = Sii \u2212Wii + wTi Dwi, and c = Xii +Dii. Since a and c are easy to compute, the main computational cost arises while evaluating wTi Dwj, the third term contributing to coefficient b above. Direct computation requires O(p2) time. Instead, we maintain U = DW by updating two rows of the matrix U for every variable update in D costing O(p) flops, and then compute wTi uj using O(p) flops. Another way to view this arrangement is that we maintain a decomposition WDW = \u2211p\nk=1wku T k throughout the process by storing the vectors uk, the columns of matrix U . The representation allows us to compute wTi Dwj, the (i, j) element of WDW , using only O(p) flops, enabling fast computation of \u00b5 in (16). In order to maintain the matrix U , we also need to update 2p elements, namely two coordinates of each uk when Dij is modified. We can compactly write the row updates of U as follows: ui\u00b7 \u2190 ui\u00b7 + \u00b5wj\u00b7 and uj\u00b7 \u2190 uj\u00b7 + \u00b5wi\u00b7, where ui\u00b7 refers to the i-th row vector of U .\nUpdate rule when X is diagonal\nThe calculation of the Newton direction can be simplified if X is a diagonal matrix. For example, this occurs in the first Newton iteration when we initialize QUIC using the identity (or diagonal) matrix. When X is diagonal, the Hessian \u22072g(X) = X\u22121 \u2297X\u22121 is a diagonal matrix, which indicates that all one variable sub-problems are independent of each other. Therefore, we only need to update each variable once to reach the optimum of (7). In particular, by examining (16), the optimal solution for D\u2032ij is\nD\u2032ij =\n\n\n\nS ( \u2212 Sij\nWiiWjj , \u03bbij WiiWjj\n)\nif i 6= j,\n\u2212Xii + S ( Xii \u2212 Sii\u2212Wii\nW 2 ii\n, \u03bbii W 2\nii\n) if i = j, (19)\nwhere, as a reminder, Wii = 1/Xii. Thus, in this case, the closed form solution for each variable can be computed in O(1) time, so the time complexity for the first Newton direction is further reduced from O(p3) to O(p2).\nUpdating only a subset of variables\nIn our QUIC algorithm we compute the Newton direction using only a subset of the variables we call the free set. We identify these variables in each Newton iteration based on the value of the gradient (we will discuss the details of the selection in Section 3.3). In the following, we define the Newton direction restricted to a subset J of variables as the solution of a quadratic approximation.\nDefinition 1. Let J denote a (symmetric) subset of variables. The Newton direction restricted to J is defined as:\nD\u2217J(X) \u2261 arg min D:Dij=0 \u2200(i,j)/\u2208J\ntr(\u2207g(X)TD) + 1\n2 vec(D)T\u22072g(X) vec(D) + \u2016X +D\u20161,\u039b. (20)\nThe cost to compute the Newton direction is thus substantially reduced when the free set is small, which as we will show in Section 3.3, occurs when the optimal solution of the \u21131-regularized Gaussian MLE is sparse."}, {"heading": "3.2 Computing the Step Size", "text": "Following the computation of the Newton direction D\u2217 = D\u2217J(X) (restricted to the subset of variables J), we need to find a step size \u03b1 \u2208 (0, 1] that ensures positive definiteness of the next iterate X + \u03b1D\u2217 and leads to a sufficient decrease of the objective function.\nWe adopt Armijo\u2019s rule (Bertsekas [1995],Tseng and Yun [2007]) and try step-sizes \u03b1 \u2208 {\u03b20, \u03b21, \u03b22, . . . } with a constant decrease rate 0 < \u03b2 < 1 (typically \u03b2 = 0.5), until we find the smallest k \u2208 N with \u03b1 = \u03b2k such that X + \u03b1D\u2217 is (a) positive-definite, and (b) satisfies the following sufficient decrease condition:\nf(X + \u03b1D\u2217) \u2264 f(X) + \u03b1\u03c3\u03b4, \u03b4 = tr(\u2207g(X)TD\u2217) + \u2016X +D\u2217\u20161,\u039b \u2212 \u2016X\u20161,\u039b, (21)\nwhere 0 < \u03c3 < 0.5. We verify positive definiteness while we compute the Cholesky factorization (costs O(p3) flops) needed for the objective function evaluation that computes log det(X + \u03b1D\u2217). This step dominates the computational cost in the step-size computations. We use the standard convention in convex analysis that f(X) = +\u221e when X is not in the effective domain of f , i.e., X is not positive definite. Using this convention, (21) enforces positive definiteness of X +\u03b1D\u2217. Next, we prove three important properties (P1\u2013P3) that the line search procedure governed by (21) satisfies:\nP1. The condition (21) is satisfied for some (sufficiently small) \u03b1, establishing that the algorithm does not enter into an infinite line search step. We note that in Proposition 1 below we show that the line search condition (21) can be satisfied for any symmetric matrix D (even one which is not the Newton direction).\nP2. For the Newton direction D\u2217, the quantity \u03b4 in (21) is negative, which ensures that the objective function decreases. Moreover, to guarantee that Xt converges to the global optimum, \u03b4 should be small enough when the current iterate Xt is far from the optimal solution. In Proposition 2 we will prove the stronger condition that \u03b4 \u2264 \u2212(1/M2)\u2016D\u20162F for some constant M . \u2016D\u2016 2 F can be viewed as a measure of the\ndistance from optimality of the current iterate Xt, and this bound ensures that the objective function decrease is proportional to \u2016D\u20162F .\nP3. When X is close enough to the global optimum, the step size \u03b1 = 1 will satisfy the line search condition (21). We will show this property in Proposition 3. Moreover, combined with the global convergence of QUIC proved in Theorem 2, this property suggests that after a finite number of iterations \u03b1 will always be 1; this also implies that only one Cholesky factorization is needed per iteration (to evaluate log det(X+ \u03b1D\u2217) for computing f(X + \u03b1D)).\nWe first prove a useful lemma.\nLemma 1. For X,D symmetric and 1 \u2265 \u03b1 \u2265 0:\n\u2016X + \u03b1D\u20161,\u039b \u2264 \u03b1\u2016X +D\u20161,\u039b + (1\u2212 \u03b1)\u2016X\u20161,\u039b.\nProof. Since \u2016\u00b7\u20161,\u039b is convex for \u039b \u2265 0, the following holds for 0 \u2264 \u03b1 \u2264 1:\n\u2016X + \u03b1D\u20161,\u039b = \u2016\u03b1(X +D) + (1\u2212 \u03b1)X\u20161,\u039b \u2264 \u03b1\u2016X +D\u20161,\u039b + (1\u2212 \u03b1)\u2016X\u20161,\u039b. (22)\nProposition 1 (corresponds to Property P1). For any X \u227b 0 and symmetric D, there exists an \u03b1\u0304 > 0 such that for all \u03b1 < \u03b1\u0304, the matrix X + \u03b1D satisfies the line search condition (21).\nProof. When \u03b1 < \u03c3n(X)/\u2016D\u20162 (where \u03c3n(X) stands for the smallest eigenvalue of X and \u2016D\u20162 is the induced 2-norm of D, i.e., the largest eigenvalue of D), we have \u2016\u03b1D\u20162 < \u03c3n(X), which implies that X + \u03b1D \u227b 0. So we can write:\nf(X + \u03b1D)\u2212 f(X) = g(X + \u03b1D)\u2212 g(X) + \u2016X + \u03b1D\u20161,\u039b \u2212 \u2016X\u20161,\u039b\n\u2264 g(X + \u03b1D)\u2212 g(X) + \u03b1(\u2016X +D\u20161,\u039b \u2212 \u2016X\u20161,\u039b), (by Lemma 1) = \u03b1 tr((\u2207g(X))TD) +O(\u03b12) + \u03b1(\u2016X +D\u20161,\u039b \u2212 \u2016X\u20161,\u039b) = \u03b1\u03b4 +O(\u03b12).\nTherefore for any fixed 0 < \u03c3 < 1 and sufficiently small \u03b1, the line search condition (21) must hold.\nBefore proving properties P2 and P3, we first state a few lemmas that will be useful in the sequel.\nLemma 2. \u03b4 = \u03b4J(X) in the line search condition (21) satisfies\n\u03b4 = tr((\u2207g(X))TD\u2217) + \u2016X +D\u2217\u20161,\u039b \u2212 \u2016X\u20161,\u039b \u2264 \u2212 vec(D \u2217)T\u22072g(X) vec(D\u2217), (23)\nwhere D\u2217 = D\u2217J(X) is the minimizer of the \u21131-regularized quadratic approximation defined in (20).\nProof. According to the definition of D\u2217 \u2261 D\u2217J(X) in (20), for all 0 < \u03b1 < 1 we have:\ntr(\u2207g(X)TD\u2217) + 1\n2 vec(D\u2217)T\u22072g(X) vec(D\u2217) + \u2016X +D\u2217\u20161,\u039b \u2264\ntr(\u2207g(X)T\u03b1D\u2217) + 1\n2 vec(\u03b1D\u2217)T\u22072g(X) vec(\u03b1D\u2217) + \u2016X + \u03b1D\u2217\u20161,\u039b. (24)\nWe combine (24) and Lemma 1 to yield:\ntr(\u2207g(X)TD\u2217) + 1\n2 vec(D\u2217)T\u22072g(X) vec(D\u2217) + \u2016X +D\u2217\u20161,\u039b \u2264\n\u03b1 tr(\u2207g(X)TD\u2217) + 1\n2 \u03b12 vec(D\u2217)T\u22072g(X) vec(D\u2217) + \u03b1\u2016X +D\u2217\u20161,\u039b + (1\u2212 \u03b1)\u2016X\u20161,\u039b.\nTherefore\n(1\u2212\u03b1)[tr(\u2207g(X)TD\u2217)+\u2016X +D\u2217\u20161,\u039b\u2212\u2016X\u20161,\u039b]+ 1\n2 (1\u2212\u03b12) vec(D\u2217)T\u22072g(X) vec(D\u2217) \u2264 0.\nDivide both sides by 1\u2212 \u03b1 > 0 to get:\ntr(\u2207g(X)TD\u2217) + \u2016X +D\u2217\u20161,\u039b \u2212 \u2016X\u20161,\u039b + 1\n2 (1 + \u03b1) vec(D\u2217)T\u22072g(X) vec(D\u2217) \u2264 0.\nBy taking the limit as \u03b1 \u2191 1, we get:\ntr(\u2207g(X)TD\u2217) + \u2016X +D\u2217\u20161,\u039b \u2212 \u2016X\u20161,\u039b \u2264 \u2212 vec(D \u2217)T\u22072g(X) vec(D\u2217),\nwhich proves (23).\nSince \u22072g(X) = X\u22121 \u2297 X\u22121 is positive definite, Lemma 2 ensures that \u03b4 < 0 for all X \u227b 0. Since the updates in our algorithm satisfy the line search condition (21), we have established that the function value is decreasing. It also follows that all the iterates {Xt}t=0,1,... belong to the level set U defined by:\nU = {X | f(X) \u2264 f(X0) and X \u2208 S p ++}. (25)\nLemma 3. The level set U defined in (25) is contained in the set {X | mI X MI} for some constants m,M > 0, if we assume that the off-diagonal elements of \u039b and the diagonal elements of S are positive.\nProof. We begin the proof by showing that the largest eigenvalue of X is less then M , a well chosen constant that depends only on \u039b, f(X0) and the matrix S. We note that S 0 and X \u227b 0 implies tr(SX) \u2265 0 and therefore:\nf(X0) > f(X) \u2265 \u2212 log detX + \u2016X\u20161,\u039b. (26)\nSince \u2016X\u20162 is the largest eigenvalue of the p\u00d7pmatrixX, we have log detX \u2264 p log(\u2016X\u20162). Combine with (26) and the fact that the off-diagonal elements of \u039b are no smaller than some \u03bb > 0:\n\u03bb \u2211\ni 6=j\n|Xij | \u2264 \u2016X\u20161,\u039b \u2264 f(X0) + p log(||X||2). (27)\nSimilarly, \u2016X\u20161,\u039b \u2265 0 implies that:\ntr(SX) \u2264 f(X0) + p log(||X||2). (28)\nNext, we introduce \u03b1 = mini Sii and \u03b2 = maxi 6=j |Sij | and split tr(SX) into diagonal and off-diagonal terms in order to bound it:\ntr(SX) = \u2211\ni\nSiiXii + \u2211\ni 6=j\nSijXij \u2265 \u03b1 tr(X)\u2212 \u03b2 \u2211\ni 6=j\n|Xij |.\nSince ||X||2 \u2264 tr(X),\n\u03b1||X||2 \u2264 \u03b1 tr(X) \u2264 tr(XS) + \u03b2 \u2211\ni 6=j\n|Xij |.\nCombine with (27) and (28) to get:\n\u03b1||X||2 \u2264 (1 + \u03b2/\u03bb)(f(X0) + p log(||X||2)). (29)\nThe left hand side of inequality (29), as a function of ||X||2, grows much faster than the right hand side (note \u03b1 > 0), and therefore ||X||2 can be bounded depending only on the values of f(X0), \u03b1, \u03b2 and \u03bb.\nIn order to prove the lower bound, we consider the smallest eigenvalue of X denoted by a and use the upper bound on the other eigenvalues to get:\nf(X0) > f(X) > \u2212 log detX \u2265 \u2212 log a\u2212 (p\u2212 1) logM, (30)\nwhich shows that m = e\u2212f(X0)M\u2212(p\u22121) is a lower bound for a.\nWe note that the conclusion of the lemma also holds if the conditions on \u039b and S are replaced by only the requirement that the diagonal elements of \u039b are positive, see Banerjee et al. [2008]. We emphasize that Lemma 3 allows the extension of the convergence results to the practically important case when the regularization does not penalize the diagonal. In subsequent arguments we will continue to refer to the minimum and maximum eigenvalues m and M established in the proof.\nProposition 2 (corresponds to Property P2). \u03b4 = \u03b4J (X) as defined in the line search condition (21) satisfies\n\u03b4 \u2264 \u2212(1/\u2016X\u201622)\u2016D \u2217\u20162F \u2264 \u2212(1/M 2)\u2016D\u2217\u20162F , (31)\nwhere M is as in Lemma 3.\nProof. We want to further bound the right hand side of (23). Since \u22072g(X) = X\u22121 \u2297 X\u22121, the smallest eigenvalue of \u22072g(X) is 1/\u2016X\u201622, and we combine with Lemma 3 to get (31).\nThe eigenvalues of any iterate X are bounded by Lemma 3, and therefore \u22072g(X) = X\u22121 \u2297 X\u22121 is Lipschitz continuous. Next, we prove that \u03b1 = 1 satisfies the line search condition when we are close to the global optimum.\nProposition 3 (corresponds to Property P3). Assume that \u22072g is Lipschitz continuous, i.e., \u2203L > 0 such that \u2200t > 0,\n\u2016\u22072g(X + tD)\u2212\u22072g(X)\u2016F \u2264 L\u2016tD\u2016F = tL\u2016D\u2016F . (32)\nThen, if X is close enough to X\u2217, the line search condition (21) will be satisfied with step size \u03b1 = 1.\nProof. We need to derive a bound for the decrease in the objective function value. We define g\u0303(t) = g(X + tD), which yields g\u0303\u2032\u2032(t) = vec(D)T\u22072g(X + tD) vec(D). First, we bound |g\u0303\u2032\u2032(t)\u2212 g\u0303\u2032\u2032(0)|:\n|g\u0303\u2032\u2032(t)\u2212 g\u0303\u2032\u2032(0)| = | vec(D)T (\u22072g(X + tD)\u2212\u22072g(X)) vec(D)|\n\u2264 \u2016 vec(D)T (\u22072g(X + tD)\u2212\u22072g(X))\u20162\u2016 vec(D)\u20162 (by Cauchy-Schwartz) \u2264 \u2016 vec(D)\u201622\u2016\u2207 2g(X + tD)\u2212\u22072g(X)\u20162 (by definition of \u2016 \u00b7 \u20162 norm) \u2264 \u2016D\u20162F \u2016\u2207 2g(X + tD)\u2212\u22072g(X)\u2016F (since \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u2016f for any matrix) \u2264 \u2016D\u20162F tL\u2016D\u2016F (by (32)) = tL\u2016D\u20163F .\nTherefore, an upper bound for g\u0303\u2032\u2032(t):\ng\u0303\u2032\u2032(t) \u2264 g\u0303\u2032\u2032(0) + tL\u2016D\u20163F = vec(D) T\u22072g(X) vec(D) + tL\u2016D\u20163F .\nIntegrate both sides to get\ng\u0303\u2032(t) \u2264 g\u0303\u2032(0) + t vec(D)T\u22072g(X) vec(D) + 1\n2 t2L\u2016D\u20163F\n= tr((\u2207g(X))TD) + t vec(D)T\u22072g(X) vec(D) + 1\n2 t2L\u2016D\u20163F .\nIntegrate both sides again:\ng\u0303(t) \u2264 g\u0303(0) + t tr((\u2207g(X))TD) + 1\n2 t2 vec(D)T\u22072g(X) vec(D) +\n1 6 t3L\u2016D\u20163F .\nTaking t = 1 we have\ng(X +D) \u2264 g(X) + tr(\u2207g(X)TD) + 1\n2 vec(D)T\u22072g(X) vec(D) +\n1 6 L\u2016D\u20163F\nf(X +D) \u2264 g(X) + \u2016X\u20161,\u039b + (tr(\u2207g(X) TD) + \u2016X +D\u20161,\u039b \u2212 \u2016X\u20161,\u039b)\n+ 1\n2 vec(D)T\u22072g(X) vec(D) +\n1 6 L\u2016D\u20163F\n\u2264f(X) + \u03b4 + 1\n2 vec(D)T\u22072g(X) vec(D) +\n1 6 L\u2016D\u20163F\n\u2264f(X) + \u03b4\n2 +\n1 6 L\u2016D\u20163F (by Lemma 2)\n\u2264f(X) + ( 1\n2 \u2212\n1 6 LM2\u2016D\u2016F )\u03b4 (by Proposition 2).\nWhen X is close to X\u2217, D is close to 0; therefore when k is large enough, the second term must be smaller than \u03c3\u03b4 ( 0 < \u03c3 < 0.5), which implies that the line search condition (21) holds with \u03b1 = 1."}, {"heading": "3.3 Identifying which variables to update", "text": "In this section, we use the stationary condition of the Gaussian MLE problem to select a subset of variables to update in any Newton direction computation. Specifically, we partition the variables into free and fixed sets based on the value of the gradient at the start of the outer loop that computes the Newton direction. We define the free set Sfree and fixed set Sfixed as:\nXij \u2208 Sfixed if |\u2207ijg(X)| \u2264 \u03bbij , and Xij = 0,\nXij \u2208 Sfree otherwise. (33)\nWe will now show that a Newton update restricted to the fixed set of variables would not change any of the coordinates in that set. In brief, the gradient condition |\u2207ijg(X)| < \u03bbij entails that the inner coordinate descent steps, according to the update in (16), would set these coordinates to zero, so they would not change since they were zero to begin with.\nAt non-differentiable points of \u2016X\u20161, only sub-gradient can be defined. To derive the optimality condition, we begin by introducing the minimum-norm subgradient for f and relate it to the optimal solution X\u2217 of (3).\nDefinition 2. We define the minimum-norm subgradient gradSij f(X) as follows:\ngradSij f(X) =\n\n \n \n\u2207ijg(X) + \u03bbij if Xij > 0,\n\u2207ijg(X) \u2212 \u03bbij if Xij < 0,\nsign(\u2207ijg(X))max(|\u2207ijg(X)| \u2212 \u03bbij, 0) if Xij = 0.\nLemma 4. For any index set J , gradSijf(X) = 0 \u2200(i, j) \u2208 J if and only if \u2206 \u2217 = 0 is a solution of the following optimization problem:\nargmin \u2206 f(X +\u2206) such that \u2206ij = 0 \u2200(i, j) /\u2208 J. (34)\nProof. Any optimal solution \u2206\u2217 for (34) must satisfy the following, for all (i, j) \u2208 J ,\n\u2207ijg(X +\u2206 \u2217)\n\n \n \n= \u2212\u03bbij if Xij > 0,\n= \u03bbij if Xij < 0,\n\u2208 [\u2212\u03bbij \u03bbij] if Xij = 0.\n(35)\nIt can be seen immediately that \u2206\u2217 = 0 satisfies (35) if and only if gradSij f(X) = 0 for all (i, j) \u2208 J .\nIn our case, \u2207g(X) = S \u2212X\u22121 and therefore\ngradSijf(X) =\n\n \n \n(S \u2212X\u22121)ij + \u03bbij if Xij > 0, (S \u2212X\u22121)ij \u2212 \u03bbij if Xij < 0, sign((S \u2212X\u22121)ij)max(|(S \u2212X \u22121)ij | \u2212 \u03bbij, 0) if Xij = 0.\nThe definition of the minimum-norm sub-gradient is closely related to the definition of the fixed and free sets. A variable Xij belongs to the fixed set if and only if Xij = 0 and gradSijf(X) = 0. Therefore, taking J = Sfixed in Lemma 4 we arrive at the following crucial property of the fixed set.\nProposition 4. For any Xt and corresponding fixed and free sets Sfixed and Sfree as defined by (33), \u2206\u2217 = 0 is the solution of the following optimization problem:\nargmin \u2206 f(Xt +\u2206) such that \u2206ij = 0 \u2200(i, j) \u2208 Sfree.\nBased on the above proposition, if we perform block coordinate descent restricted to the fixed set, then no updates would occur. We then perform the inner loop coordinate descent updates restricted to only the free set to find the Newton direction. With this modification, the number of variables over which we perform the coordinate descent update of (16) can be potentially reduced from p2 to the number of non-zeros in Xt. When the solution is sparse (depending on the value of \u039b) the number of free variables is much smaller than p2 and we can obtain huge computational gains as a result. In essence, we very efficiently select a subset of the coordinates that need to be updated.\nThe attractive facet of this modification is that it leverages the sparsity of the solution and intermediate iterates in a manner that falls within the block coordinate descent framework of Tseng and Yun [2007]. The index sets J1, J2, . . . corresponding to the block coordinate descent steps in the general setting of Tseng and Yun [2007][p. 392] need to satisfy a Gauss-Seidel type of condition:\n\u22c3\nj=0,...,T\u22121\nJt+j \u2287 N \u2200t = 1, 2, . . . (36)\nfor some fixed T , where N denotes the full index set. In our framework J1, J3, . . . denote the fixed sets at various iterations, and J2, J4, . . . denote the free sets. Since J2i+1 and J2i+2 is a partitioning of N the choice T = 3 will suffice. But will the size of the free set be small? We initialize X0 to a diagonal matrix, which is sparse. The following lemma shows that after a finite number of iterations, the iterates Xt will have a similar sparsity pattern as the limit X\u2217. Lemma 5 is actually an immediate consequence of Lemma 8 in Section 4.\nLemma 5. Assume that {Xt} converges to X \u2217, the optimal solution of (3). If for some index pair (i, j), |\u2207ijg(X \u2217)| < \u03bbij (so that X \u2217 ij = 0), then there exists a constant t\u0304 > 0 such that for all t > t\u0304, the iterates Xt satisfy\n|\u2207ijg(Xt)| \u2264 \u03bbij and (Xt)ij = 0. (37)\nNote that |\u2207ijg(X \u2217)| < \u03bbij implies X \u2217 ij = 0 from from the optimality condition of (3). A similar (so called shrinking) strategy is used in SVM and \u21131-regularized logistic regression problems as mentioned in Yuan et al. [2010]. In our experiments, we demonstrate that this strategy reduces the size of the free set very quickly."}, {"heading": "3.4 The block-diagonal structure of X\u2217", "text": "It has been shown recently by (Mazumder and Hastie [2012],Witten et al. [2011]) that when the thresholded covariance matrixE defined byEij = S(Sij , \u03bb) = sign(Sij)max(|Sij |\u2212 \u03bb, 0) has the following block-diagonal structure:\nE =\n\n    E1 0 . . . 0 0 E2 . . . 0 ... ... ...\n... 0 0 0 Ek\n\n    , (38)\nthen the solution X\u2217 of the inverse covariance estimation problem (2) also has the same block-diagonal structure:\nX\u2217 =\n\n    X\u22171 0 . . . 0 0 X\u22172 . . . 0 ... ... ...\n... 0 0 0 X\u2217k\n\n    .\nThis result can be extended to the case when the elements are penalized differently, i.e., \u03bbij \u2019s are different. Then, if Eij = S(Sij , \u03bbij) is block diagonal, so is the solution X\n\u2217 of (3), see Hsieh et al. [2012]. Thus each X\u2217i can be computed independently. Based on this observation one can decompose the problem into sub-problems of smaller sizes. Since the complexity of solving (3) is O(p3), solving several smaller sub-problems is much faster. In the following, we show that our updating rule and fixed/free set selection technique can automatically detect this block-diagonal structure for free.\nRecall that we have a closed form solution in the first iteration when the input is a diagonal matrix. Based on (19), since Xij = 0 for all i 6= j in this step, we have\nDij = XiiXjjS(\u2212Sij, \u03bbij) = \u2212XiiXjjS(Sij, \u03bbij) for all i 6= j.\nWe see that after the first iteration the nonzero pattern of X will be exactly the same as the nonzero pattern of the thresholded covariance matrix E as depicted in (38). In order to establish that the same is true at each subsequent step, we complete our argument using induction, by showing that the structure is preserved.\nMore precisely, we show that the off-diagonal blocks always belong to the fixed set if |Sij| \u2264 \u03bbij . Recall the definition of the fixed set in (33). We need to check whether |\u2207ijg(X)| \u2264 \u03bbij for all (i, j) in the off-diagonal blocks of E. Taking the inverse preserves the diagonal structure, and therefore \u2207ijg(X) = Sij \u2212 X \u22121 ij = Sij. We conclude noting that Eij = 0 implies that |\u2207ijg(X)| \u2264 \u03bbij, meaning that (i, j) will belong to the fixed set.\nWe decompose the matrix into smaller blocks prior to running Cholesky factorization to avoid the O(p3) time complexity. The connected components of X can be detected in O(\u2016X\u20160) time, which is very efficient when X is sparse. The detailed description of QUIC is presented in Algorithm 2."}, {"heading": "4 Convergence Analysis", "text": "In Section 3, we introduced the main ideas behind our QUIC algorithm. In this section, we first prove that QUIC converges to the global optimum, and then show that the convergence rate is quadratic. Banerjee et al. [2008] showed that for the special case where \u039bij = \u03bb the optimization problem (2) has a unique global optimum and that the eigenvalues of the primal optimal solutionX\u2217 are bound. In the following theorem we show this result for more general \u039b where only the off-diagonal elements need to be positive.\nTheorem 1. There exists a unique minimizer X\u2217 for the optimization problem (3).\nProof. According to Lemma 3, the level set U defined in (25) contains all the iterates, and it is in turn contained in the compact set S \u2261 {X | mI X MI}. According to the Weierstrass extreme value theorem [Apostol, 1974], any continuous function in a compact set attains its minimum. Furthermore, \u22072g(X) = X\u22121 \u2297X\u22121 implies \u22072g(X) M\u22122I. Since \u2016X\u20161,\u039b is convex and \u2212 log det(X) is strongly convex, we have that f(X) is strongly convex on the compact set S, and therefore the minimizerX\u2217 is unique [Apostol, 1974]."}, {"heading": "4.1 Convergence Guarantee", "text": "In order to show that QUIC converges to the optimal solution, we consider a more general setting of the quadratic approximation algorithm: at each iteration, the iterate Yt is updated by Yt = Yt+\u03b1tDJt(Yt) where Jt is a subset of variables chosen to update at iteration t, DJt(Yt) is the Newton direction restricted to Jt defined by (20), and \u03b1t is the step size selected by the Armijo rule mentioned in Section 3.2. The algorithm is summarized in Algorithm 3. Similar to the block coordinate descent framework of Tseng and Yun [2007], we assume the index set Jt satisfies a Gauss-Seidel type of condition:\n\u22c3\nj=0,...,T\u22121\nJt+j \u2287 N \u2200t = 1, 2, . . . . (39)\nAlgorithm 2:QUadratic approximation method for sparse Inverse Covariance learning (QUIC)\nInput : Empirical covariance matrix S (positive semi-definite p\u00d7 p), regularization parameter matrix \u039b, initial X0, inner stopping tolerance \u01eb, parameters 0 < \u03c3 < 0.5, 0 < \u03b2 < 1 Output: Sequence of Xt converging to argminX\u227b0 f(X), where f(X) = g(X)+h(X), where g(X) = \u2212 log detX +tr(SX), h(X) = \u2016X\u20161,\u039b.\n1 Compute W0 = X \u22121 0 . 2 for t = 0, 1, . . . do 3 D = 0, U = 0 4 while not converged do 5 Partition the variables into fixed and free sets: 6 Sfixed := {(i, j) | |\u2207ijg(Xt)| < \u03bbij and (Xt)ij = 0}, Sfree := {(i, j) | |\u2207ijg(Xt)| \u2265 \u03bbij or (Xt)ij 6= 0}. 7 for (i, j) \u2208 Sfree do 8 a = w2ij + wiiwjj 9 b = sij \u2212 wij +w T \u00b7iu\u00b7j\n10 c = xij + dij 11 \u00b5 = \u2212c+ S(c\u2212 b/a, \u03bbij/a) 12 dij \u2190 dij + \u00b5 13 ui\u00b7 \u2190 ui\u00b7 + \u00b5wj\u00b7 14 uj\u00b7 \u2190 uj\u00b7 + \u00b5wi\u00b7 15 end\n16 end 17 for \u03b1 = 1, \u03b2, \u03b22, . . . do 18 Compute the Cholesky factorization LLT = Xt + \u03b1D. 19 if Xt + \u03b1D 6\u227b 0 then 20 continue 21 end 22 Compute f(Xt + \u03b1D) from L and Xt + \u03b1D 23 if f(Xt + \u03b1D) \u2264 f(Xt) + \u03b1\u03c3 [tr(\u2207g(Xt)D) + \u2016Xt +D\u20161,\u039b \u2212 \u2016X\u20161,\u039b] then 24 break 25 end\n26 end 27 Xt+1 = Xt + \u03b1D 28 Compute Wt+1 = X \u22121 t+1 reusing the Cholesky factor. 29 end\nAlgorithm 3: General Block Quadratic Approximation method for Sparse Inverse Covariance Learning\nInput : Empirical covariance matrix S (positive semi-definite p\u00d7 p), regularization parameter matrix \u039b, initial Y0, inner stopping tolerance \u01eb\nOutput: Sequence of Yt. 1 for t = 0, 1, . . . do 2 Generate a variable subset Jt. 3 Compute the Newton direction Dt \u2261 DJt(Yt) by (20). 4 Compute the step-size \u03b1t using an Armijo-rule based step-size selection in (21). 5 Update Yt+1 = Yt + \u03b1tDt. 6 end\nIn QUIC, J1, J3, . . . denote the fixed sets and J2, J4, . . . denote the free sets. If {Xt}t=1,2,... denotes the sequence generated by QUIC, then\nY1 = Y2 = X1, Y3 = Y4 = X2, . . . , Y2i\u22121 = Y2i = Xi.\nMoreover, since each J2i\u22121 and J2i is a partitioning of N , the choice T = 3 will satisfy (39). In the rest of this section, we will show that {Yt}t=1,2,... converges to the global optimum, thus {Xt}t=1,2,... generated by QUIC also converges to the global optimum.\nOur first step towards the convergence proof is a lemma on convergent subsequences of Xt.\nLemma 6. For any convergent subsequence Yst \u2192 Y \u2217, we have Dst \u2261 DJst (Yst) \u2192 0.\nProof. The objective value decreases according to the line search condition (21) and Proposition 2. According to Lemma 3, f(Yst) cannot converge to negative infinity, so f(Xst)\u2212 f(Xst+1) \u2192 0. The line search condition (21) implies that we have \u03b1st\u03b4st \u2192 0.\nWe proceed to prove the statement by contradiction. If Dst does not converge to 0, then there exist an infinite index set T \u2286 {s1, s2, . . .} and \u03b7 > 0 such that \u2016Dt\u2016F > \u03b7 for all t \u2208 T . According to Proposition 2, \u03b4st is bounded away from 0, therefore \u03b4st 6\u2192 0, while \u03b1st \u2192 0. We can assume without loss of generality that \u03b1st < 1, that is the line search condition is not satisfied in the first attempt. We will work in this index set T in the derivations that follow.\nThe line search step size \u03b1t < 1 (t \u2208 T ) satisfies (21), but \u03b1t = \u03b1t/\u03b2 does not satisfy (21) by the minimality of our line search procedure. So we have:\nf(Yt + \u03b1tDt)\u2212 f(Yt) \u2265 \u03c3\u03b1t\u03b4t. (40)\nIf Yt + \u03b1tDt is not positive definite, then we define f(Yt + \u03b1tDt) to be \u221e, so (40) still holds. We expand the definition of f and apply Lemma 1:\n\u03c3\u03b1t\u2206t \u2264 g(Yt + \u03b1tDt)\u2212 g(Yt) + \u2016Yt + \u03b1tDt\u20161,\u039b \u2212 \u2016Yt\u20161,\u039b\n\u2264 g(Yt + \u03b1tDt)\u2212 g(Yt) + \u03b1t\u2016|Yt +Dt\u20161,\u039b + (1\u2212 \u03b1t)\u2016Yt\u20161,\u039b \u2212 \u2016Yt\u20161,\u039b\n= g(Yt + \u03b1tDt)\u2212 g(Yt) + \u03b1t(\u2016Yt +Dt\u20161,\u039b \u2212 \u2016Yt\u20161,\u039b),\u2200t \u2208 T .\nBy the definition of \u03b4t we have:\n\u03c3\u03b4t \u2264 g(Yt + \u03b1tDt)\u2212 g(Yt)\n\u03b1t + \u03b4t \u2212 tr(\u2207g(Yt)\nTDt),\n(1\u2212 \u03c3)(\u2212\u03b4t) \u2264 g(Yt + \u03b1tDt)\u2212 g(Yt)\n\u03b1t \u2212 tr(\u2207g(Yt)\nTDt).\nBy Proposition 2 we have:\n(1\u2212 \u03c3)M\u22122\u2016Dt\u2016 2 F \u2264\ng(Yt + \u03b1tDt)\u2212 g(Yt)\n\u03b1t \u2212 tr(\u2207g(Yt)\nTDt)\n(1\u2212 \u03c3)M\u22122\u2016Dt\u2016F \u2264 g(Yt + \u03b1t\u2016Dt\u2016F Dt \u2016Dt\u2016F )\u2212 g(Yt)\n\u03b1t\u2016Dt\u2016F \u2212 tr(\u2207g(Yt) T Dt \u2016Dt\u2016F ).\nWe set \u03b1\u0302t = \u03b1t\u2016Dt\u2016F . Since \u2016Dt\u2016F > \u03b7 for all t \u2208 T we have:\n(1\u2212 \u03c3)M\u22122\u03b7 \u2264 g(Yt + \u03b1\u0302t\nDt \u2016Dt\u2016F )\u2212 g(Yt)\n\u03b1\u0302t \u2212 tr(\u2207g(Yt) T Dt \u2016Dt\u2016F )\n= \u03b1\u0302t tr(\u2207g(Yt) Dt \u2016Dt\u2016F ) +O(\u03b1\u03022t )\n\u03b1\u0302t \u2212 tr(\u2207g(Yt) T Dt \u2016Dt\u2016F )\n= O(\u03b1\u0302t) (41)\nAgain, by Proposition 2,\n\u2212\u03b1t\u03b4t \u2265 \u03b1tM \u22122\u2016Dt\u2016 2 F \u2265 M \u22122\u03b1t\u2016Dt\u2016F \u03b7.\nSince {\u03b1t\u03b4t}t \u2192 0, it follows that {\u03b1t\u2016Dt\u2016F }t \u2192 0 and {\u03b1\u0302 k}t \u2192 0. Taking limit of (41) as t \u2208 T\u0304 and t \u2192 \u221e, we have\n(1\u2212 \u03c3)M\u22122\u03b7 \u2264 0,\na contradiction, finishing the proof.\nIn Lemma 6, we prove that DJt converges to zero for the converging subsequence. Next we show that DJ is closely related to grad\nSf(Y ) defined in Definition 2, which in turn is an indicator of optimality as proved in Lemma 4.\nLemma 7. For any index set J and positive definite Y , DJ(Y ) = 0 if and only if gradSij f(Y ) = 0 for all (i, j) \u2208 J .\nProof. DJ(Y ) = 0 if and only if D = 0 satisfies the optimality condition of (20). The condition can be written as (35) restricted to (i, j) \u2208 J , which in turn is equivalent to the optimality condition of f . Therefore DJ(Y ) = 0 iff grad S ij f(Y ) = 0 for all (i, j) \u2208 J .\nBased on these lemmas, we are now able to prove our main convergence theorem.\nTheorem 2. Algorithm 3 converges to a unique global optimum Y \u2217.\nProof. Assume a subsequence {Yt}T converges to Y\u0304 . Since the choice of the index set Jt selected at each step is finite, we can further assume that Jt = J\u03040 for all t \u2208 T , considering a subsequence of T if necessary. From Lemma 6, DJ\u03040(Yt) \u2192 0. By the continuity of \u2207g(Y ) and \u22072g(Y ), it is easy to show DJ\u03040(Yt) \u2192 DJ\u03040(Y\u0304 ). Therefore DJ\u03040(Y\u0304 ) = 0. Based on Lemma 7, we have\ngradSijf(Y ) = 0 for all (i, j) \u2208 J\u03040.\nFurthermore, {DJ\u03040(Yt)}T \u2192 0 and \u2016Yt \u2212 Yt+1\u2016F \u2264 \u2016DJ\u03040(Yt)\u2016F , so {Yt+1}t\u2208T also converges to Y\u0304 . By considering a subsequence of T if necessary, we can further assume that Jt+1 = J\u03041 for all t \u2208 T . By the same argument, we can show that {DJt+1(Yt)}T \u2192 0, so DJ\u03041(Y\u0304 ) = 0. Similarly, we can show that DJ\u0304t(Y\u0304 ) = 0 \u2200t = 0, . . . , T \u2212 1 can be assumed for an appropriate subset of T . With assumption (39) and Lemma 7 we have\ngradSij f(Y\u0304 ) = 0 \u2200i, j. (42)\nUsing Lemma 4 with J is the set of all variables, we can show that (42) implies Y\u0304 is the global optimum."}, {"heading": "4.2 Asymptotic Convergence Rate", "text": "Newton methods on constrained minimization problems: The convergence rate of the Newton method on bounded constrained minimization has been studied in Levitin and Polyak [1966] and Dunn [1980]. Here, we briefly mention their results.\nAssume we want to solve a constrained minimization problem\nmin x\u2208\u2126 F (x),\nwhere \u2126 is a nonempty subset of Rn denoting the constraint set and F : Rn \u2192 R has a second derivative \u22072F (x). Then beginning from x0, the natural Newton updates entail computing the (k + 1)th iterate xk+1 as\nxk+1 = argmin x\u2208\u2126\n\u2207F (xk) T (x\u2212 xk) +\n1 2 (x\u2212 xk) T\u22072F (xk)(x\u2212 xk). (43)\nFor simplicity, we assume F is strictly convex, and has a unique minimizer x\u2217 in \u2126. Then the following theorem holds\nTheorem 3 (From Theorem 3.1 in Dunn [1980]). Assume F is strictly convex, has a unique minimizer x\u2217 in \u2126, and that \u22072F (x) is Lipschitz continuous. Then for all x0 sufficiently close to x\u2217, the sequence {xk} generated by (43) converges quadratically to x \u2217.\nThis theorem is proved in Dunn [1980]. In our case, the objective function f(X) is non-smooth so that Theorem 3 does not directly apply. Instead, we will first show that after a finite number of iterations the sign of the iterates {Xt} generated by QUIC will not change, so that we can then use Theorem 3 to establish asymptotic quadratic convergence.\nQuadratic convergence rate for QUIC:\nUnlike the previous section, our Algorithm 3 does not perform an unrestricted Newton update: it iteratively selects variable subsets {Jt}t=1,... in the manner of fixed and free sets, and performs Newton directions restricted to the free sets. In the following, we show that the sequence {Xt}t=1,2,... generated by QUIC does converge quadratically to the global optimum.\nAssume X\u2217 is the optimal solution, then we can divide the index set with \u03bbij 6= 0 into three subsets:\nP = {(i, j) | X\u2217ij > 0}, N = {(i, j) | X\u2217ij < 0}, (44) Z = {(i, j) | X\u2217ij = 0}.\nFrom the optimality condition of X\u2217,\n\u2207ijg(X \u2217)\n\n \n \n= \u2212\u03bbij if (i, j) \u2208 P,\n= \u03bbij if (i, j) \u2208 N,\n\u2208 [\u2212\u03bbij , \u03bbij ] if (i, j) \u2208 Z.\n(45)\nLemma 8. Assume that the sequence {Xt} converges to the global optimum X \u2217. Then there exists a t\u0304 such that\n(Xt)ij\n\n \n \n\u2265 0 if (i, j) \u2208 P\n\u2264 0 if (i, j) \u2208 N\n= 0 if (i, j) \u2208 Z\n(46)\nfor all t > t\u0304.\nProof. We prove the case for (i, j) \u2208 P by contradiction, the other two cases can be handled similarly. If we cannot find a t\u0304 satisfying (46), then there exists an infinite subsequence {Xst} such that (Xst)ij < 0. We consider the update from Xst\u22121 to Xst . From Lemma 3, we can assume that st is large enough so that the step size equals 1, therefore Xst = Xst\u22121 + D(Xst\u22121) where D(Xst\u22121) is defined in (20). Since (Xst)ij = (Xst\u22121)ij + (D(Xst\u22121))ij < 0, from the optimality condition of (20) we have\n(\n\u2207g(Xst\u22121) +\u2207 2g(Xst\u22121) vec(D(Xst\u22121))\n)\nij = \u03bbij . (47)\nSince D(Xst\u22121) converges to 0, (47) implies that {\u2207ijg(Xst\u22121)} will converge to \u03bbij. However, (45) implies \u2207ijg(X\n\u2217) = \u2212\u03bbij, and by the continuity of \u2207g we get that {\u2207ijg(Xt)} converges to \u2207ijg(X \u2217) = \u2212\u03bbij, a contradiction, finishing the proof.\nThe following lemma shows that the coordinates from the fixed set remain zero after a finite number of iterations.\nLemma 9. Assume Xt \u2192 X \u2217. There exists a t\u0304 > 0 such that variables in P or N will not be selected to be in the fixed set Sfixed, when t > t\u0304. That is,\nSfixed \u2286 Z.\nProof. Since Xt converges to X \u2217, (Xt)ij converges to X \u2217 ij > 0 if (i, j) \u2208 P and to X \u2217 ij < 0 if (i, j) \u2208 N . Recall that (i, j) belongs to the fixed set only if (Xt)ij = 0. When t is large enough, (Xt)ij 6= 0 when Xt \u2208 P \u222a N , therefore P and N will be disjoint from the fixed set. Moreover, by the definition of the fixed set (33), indexes with \u03bbij = 0 will never be selected. We proved that the fixed set will be a subset of Z when t is large enough.\nTheorem 4. The sequence {Xt} generated by the QUIC algorithm converges quadratically to X\u2217.\nProof. First, if the index sets P,N and Z (related to the optimal solution) are given, the optimum of (2) is the same as the optimum of the following constrained minimization problem:\nmin X\n\u2212 log det(X) + tr(SX) + \u2211\n(i,j)\u2208P\n\u03bbijXij \u2212 \u2211\n(i,j)\u2208N\n\u03bbijXij\ns.t. Xij \u2265 0 \u2200(i, j) \u2208 P, (48)\nXij \u2264 0 \u2200(i, j) \u2208 N,\nXij = 0 \u2200(i, j) \u2208 Z.\nIn the following, we show that when t is large enough, QUIC solves the minimization problem described by (48).\n1. The constraints in (48) are satisfied by QUIC iterates after a finite number of steps, as shown in Lemma 8. Thus, the \u21131-regularized Gaussian MLE (3) is equivalent to the smooth constrained objective (48), since the constraints in (48) are satisfied when solving (3).\n2. Since the optimization problem in (48) is smooth, it can be solved using constrained Newton updates as in (43). The QUIC update direction DJ (Xt) is restricted to a set of free variables in J . This is exactly equal to the unrestricted Newton update as in (43), after a finite number of steps, as established by Lemma 9. In particular, at each iteration the fixed set is contained in Z, which is the set which always satisfies (Dt)Z = 0 for large enough t.\n3. Moreover, by Lemma 3 the step size is \u03b1 = 1 when t is large enough.\nTherefore our algorithm is equivalent to the constrained Newton method in (43), which in turn converges quadratically to the optimal solution of (48). Since the revised problem (48) and our original problem (3) has the same minimum, we have shown that QUIC converges quadratically to the optimum of (3).\nIn the next section, we show that this asymptotic convergence behavior of QUIC is corroborated empirically as well."}, {"heading": "5 Experimental Results", "text": ""}, {"heading": "5.1 Stopping condition for solving the sub-problems", "text": "In the convergence analysis of Section 4, we assumed that each Newton direction Dt is computed exactly by solving the Lasso subproblem (20). In our implementation we use an iterative solver to compute Dt, which after a finite set of iterations only solves the problem to some accuracy. In the first experiment we explore how varying the accuracy to which we compute the Newton direction affects overall performance. In Figure 1 we plot the total run times for the ER biology dataset from [Li and Toh, 2010] correspond to different numbers of inner iterations used in the coordinate descent solver of QUIC.\nWe can observe that QUIC with one inner iteration converges faster in the beginning, but eventually achieves just a linear convergence rate, whileQUIC with 20 inner iterations converges more slowly in the beginning, but eventually achieves quadratic convergence. Based on this observation, we propose an adaptive stopping condition: we set the number of coordinate descent steps to be \u03b1t for the t-th outer iteration, where \u03b1 is a constant; we use \u03b1 = 1/3 in our experiments. Figure 1(b) shows that by using this adaptive stopping condition, QUIC is not only efficient in the beginning, but also achieves quadratic convergence."}, {"heading": "5.2 Comparisons with other methods", "text": "In this section, we compare the performance of QUIC on both synthetic and real datasets with other state-of-the-art methods. We have implemented QUIC in C++, and all ex-\nperiments were executed on 2.83GHz Xeon X5440 machines with 32G RAM and Linux OS.\nWe include the following algorithms in our comparisons:\n\u2022 ALM: the Alternating Linearization Method proposed by Scheinberg et al. [2010]. We use their MATLAB source code for the experiments.\n\u2022 ADMM: another implementation of the alternating linearization method implemented by Boyd et al. [2012]. The matlab code can be downloaded from http://www.stanford.edu/~boyd/papers/admm/. We found that the default parameters (which we note are independent of the regularization penalty) yielded slow convergence; we set the augmented Lagrangian parameter to \u03c1 = 50 and the overrelaxation parameter to \u03b1 = 1.5. These parameters achieved the best speed on the ER dataset.\n\u2022 glasso: the block coordinate descent method proposed by Friedman et al. [2008]. We use the latest version glasso 1.7 downloaded from http://www-stat.stanford.edu/~tibs/glasso/.\n\u2022 PSM: the Projected Subgradient Method proposed by Duchi et al. [2008]. We use the MATLAB source code available at http://www.cs.ubc.ca/~schmidtm/Software/PQN.html.\n\u2022 SINCO: the greedy coordinate descent method proposed by Scheinberg and Rish [2010]. The code can be downloaded from https://projects.coin-or.org/OptiML/browser/trunk/sinco.\n\u2022 IPM: An inexact interior point method proposed by Li and Toh [2010]. The source code can be downloaded from http://www.math.nus.edu.sg/~mattohkc/Covsel-0.zip.\n\u2022 PQN: the projected quasi-Newton method proposed by Schmidt et al. [2009]. The source code can be downloaded from http://www.di.ens.fr/~mschmidt/Software/PQN.html.\nIn the following, we compare QUIC and the above state-of-the-art methods on synthetic and real datasets with various settings of \u03bb."}, {"heading": "5.2.1 Experiments on synthetic datasets", "text": "We first compare the run times of the different methods on synthetic data. We generate the two following types of graph structures for the underlying Gaussian Markov Random Fields:\n\u2022 Chain Graphs: The ground truth inverse covariance matrix \u03a3\u22121 is set to be \u03a3\u22121i,i\u22121 =\n\u22120.5 and \u03a3\u22121i,i = 1.25.\nGiven the inverse covariance matrix \u03a3\u22121, we draw a limited number, n = p/2 i.i.d. samples from the corresponding GMRF distribution, in order to simulate the high-dimensional setting.\nTable 1 shows the attributes of the synthetic datasets that we used in the timing comparisons. The dimensionality varies from {1000, 4000, 10000}. For chain graphs, we select \u03bb so that the solution has (approximately) the correct number of nonzero elements. In order to test the performance of the algorithms under different values of \u03bb, for the case of random-structured graphs we considered two \u03bb values; one of which resulted in the discovery of the correct number of non-zeros and one which resulted in five-times thereof. We measured the accuracy of the graph structure recovered by the true positive rate (TPR) and false positive rate (FPR) defined as\nTPR = |{(i, j) | (X\u2217)ij > 0 and Qij > 0}|\n|{(i, j) | Qij > 0}| ,FPR =\n|{(i, j) | (X\u2217)ij > 0 and Qij = 0}|\n|{(i, j) | Qij = 0}| ,\n(49)\nwhere Q is the ground truth sparse inverse covariance. Since QUIC does not natively compute a dual solution, the duality gap cannot be used as a stopping condition.1 In practice, we can use the minimum-norm sub-gradient\n1Note, that W = X\u22121 cannot be expected to satisfy the dual constraints requiring |Wij \u2212 Sij | \u2264 \u03bbij . One could project X\u22121 in order to enforce the constraints and use the resulting matrix to compute the duality gap. Our implementation provides this computation only if the user requests it.\ndefined in Definition 2 as the stopping condition. There is no additional computational cost to this approach because X\u22121 is computed as part of the QUIC algorithm. In the experiments, we report the time for each algorithm to achieve \u01eb-accurate solution defined by f(Xk)\u2212 f(X\u2217) < \u01ebf(X\u2217).\nTable 2 shows the results for \u01eb = 10\u22122 and 10\u22126, where \u01eb = 10\u22122 tests the ability for an algorithm to get a good initial guess (the nonzero structure), and \u01eb = 10\u22126 tests whether an algorithm can achieve an accurate solution. Table 2 shows that QUIC is consistently and overwhelmingly faster than other methods, both initially with \u01eb = 10\u22122, and at \u01eb = 10\u22126. Moreover, for the p = 10000 random pattern, there are p2 = 100 million variables and the selection of fixed/free sets helpsQUIC to focus only on very small subset of them. We converge to the solution in about 15 minutes, while other methods fail to even have an initial guess within 8 hours.\nIn some applications, researchers are primarily interested in the obtained graphical structure represented by the solution. Therefore, in addition to the objective function value, we further compare the true positive rate and false positive rate of the obtained nonzero pattern in Xt by each algorithm. In Figure 2, we use two synthetic datasets, chain1000 and random1000, as examples. For each algorithm, we plot the objective function value, true positive rate, and false positive rate of the iterates Xt versus run time. For\nthe methods that solve the dual problem, the sparse inverse covariance matrix Xt = W \u22121 t is usually dense, so we consider elements with absolute value larger than 10\u22126 as nonzero elements. We can see thatQUIC not only obtains lower objective function value efficiently, but also recover the ground truth structure of GMRF faster than other methods."}, {"heading": "5.2.2 Experiments on real datasets", "text": "We use the real world biology datasets preprocessed by Li and Toh [2010] to compare the performance of our method with other state-of-the-art methods. In the first set of experiments, we set the regularization parameter \u03bb to be 0.5, which achieves reasonable sparsity for the following datasets: Estrogen (p = 692), Arabidopsis (p = 834), Leukemia (p = 1, 225), Hereditary (p = 1, 869). In Figure 3 we plot the relative error (f(Xt) \u2212 f(X\u2217))/f(X\u2217) (on a log scale) against time in seconds. We can observe from Figure 3 that under this setting \u2013 large \u03bb and sparse solution \u2013 QUIC can be seen to achieve superlinear convergence while other methods exhibit at most a linear convergence. Overall, we see that QUIC can be five times faster than other methods, and can be expected to be even faster if a higher accuracy is desired.\nIn the second set of experiments, we compare the algorithms under different values of the regularization parameter \u03bb on the ER dataset. In Figure 3(a) we show the results for \u03bb = 0.5. We then decrease \u03bb to 0.1, 0.05, 0.01 using the same datasets and show the results in Figure 5. A smaller \u03bb yields a denser solution, and we list the density of the convergence point X\u2217 in Figure 5. From Figure 5 we can see that QUIC is the most efficient method when \u03bb is large (solution is sparse), but IPM and PSM outperforms QUIC when \u03bb is small (solution is dense). However, those cases are usually not useful in practice because when solving the \u21131-regularized MLE problem one usually wants a sparse graphical structure for the GMRF. The main reason that QUIC is very efficient under large \u03bb is that with fixed/free set selection, the coordinate descent method can focus on a small portion of variables, while in PSM and IPM the whole matrix is considered at each iteration.\nTo further demonstrate the power of fixed/free set selection, we use Hereditarybc dataset as an example. In Figure 4, we plot the size of the free set versus iterations for Hereditarybc dataset. Starting from a total of 18692 = 3, 493, 161 variables, the size of the free set progressively drops, in fact to less than 120, 000 in the very first iteration. We can see the super-linear convergence of QUIC even more clearly when we plot it against the number of iterations."}, {"heading": "5.3 Block-diagonal structure", "text": "As discussed earlier, Mazumder and Hastie [2012], Witten et al. [2011] showed that when the thresholded covariance matrix E = max(|S|\u2212\u03bb, 0) is block-diagonal, then the problem can be naturally decomposed into sub-problems. This observation has been implemented in the latest version of glasso. In the end of Section 3, we discuss that the fixed/free set selection can automatically identify the block-diagonal structure of the thresholded matrix, and thus QUIC can benefit from block-diagonal structure even when we do not explicitly decompose the matrix. In the following experiment we will show that with input sample\ncovariance S with block-diagonal structure represented by E (see Section 3.4), QUIC still outperforms glasso. Moreover, we will show that when some off-diagonal elements are added into the problem, while QUIC is still efficient because of its fixed/free set selection, glasso on the other hand suddenly becomes very slow.\nWe generate synthetic data with block-diagonal structure as follows. We generate a sparse 150 \u00d7 150 inverse covariance matrix \u0398\u0304 as discussed in Section 5.2.1, and then replicate \u0398\u0304 eight times on the diagonal blocks to form a 1200 \u00d7 1200 block-diagonal matrix. Using this inverse covariance matrix to generate samples, we compare the following methods:\n\u2022 QUIC: our proposed algorithm.\n\u2022 glasso: In the latest version of glasso, the matrix is first decomposed into connected components based on the thresholded covariance matrix max(|S| \u2212 \u03bb), and then each sub-problem is solved individually.\nWe then test the two algorithms for regularization parameter \u03bb taking values from the set {0.017, . . . , 0.011}. When \u03bb = 0.017, the thresholded covariance matrix E has eight blocks, while when \u03bb = 0.011 the block structure reduces to a single block. For each single \u03bb trial, we compare the time taken by QUIC and glasso to achieve (f(Xt)\u2212 f(X\u2217))/f(X\u2217) < 10\u22125. Figure 6 shows the experimental results. We can see that both methods are very fast for the case where the problem can be decomposed into 8 subproblems (large \u03bb); however, when we slightly increase \u03bb so that there is only 1 connected component, QUIC becomes much faster than glasso. This is because even for the nondecomposable case, QUIC can still keep most of the very sparse off-diagonal blocks in the fixed set to speedup the process, while glasso cannot benefit from this sparse structure."}, {"heading": "Acknowledgements", "text": "This research was supported by NSF grants IIS-1018426 and CCF-0728879. ISD acknowledges support from the Moncrief Grand Challenge Award. We would like to thank Kim-Chuan Toh for providing data sets and the IPM code as well as Katya Scheinberg and Shiqian Ma for providing the ALM implementation."}], "references": [{"title": "Convergence rates of gradient methods for high-dimensional statistical recovery", "author": ["A. Agarwal", "S. Negahban", "M. Wainwright"], "venue": "In NIPS,", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data", "author": ["O. Banerjee", "L.E. Ghaoui"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Banerjee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Nonlinear programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas.", "year": 1995}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2009\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2009}, {"title": "Matlab scripts for alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Technical report,", "citeRegEx": "Boyd et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2012}, {"title": "First-order methods for sparse covariance selection", "author": ["A. d\u2019Aspremont", "O. Banerjee", "L. El Ghaoui"], "venue": "SIAM Journal on Matrix Analysis and its Applications,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Daubechies et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2004}, {"title": "Projected subgradient methods for learning sparse Gaussians", "author": ["J. Duchi", "S. Gould", "D. Koller"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Newton\u2019s method and the Goldstein step-length rule for constrained minimization problems", "author": ["J.C. Dunn"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Dunn.,? \\Q1980\\E", "shortCiteRegEx": "Dunn.", "year": 1980}, {"title": "Pathwise coordinate optimization", "author": ["J. Friedman", "T. Hastie", "H. H\u00f6fling", "R. Tibshirani"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "URL http://dx.doi.org/10.1093/biostatistics/kxm045", "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Sparse inverse covariance matrix estimation using quadratic approximation", "author": ["C.-J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P. Ravikumar"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hsieh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2011}, {"title": "A divide-and-conquer method for sparse inverse covariance estimation", "author": ["C.-J. Hsieh", "I.S. Dhillon", "P. Ravikumar", "A. Banerjee"], "venue": "In NIPS,", "citeRegEx": "Hsieh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2012}, {"title": "Proximal Newton-type methods for convex optimization", "author": ["J.D. Lee", "Y. Sun", "M.A. Saunders"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Constrained minimization methods. U.S.S.R", "author": ["E.S. Levitin", "B.T. Polyak"], "venue": "Computational Math. and Math. Phys.,", "citeRegEx": "Levitin and Polyak.,? \\Q1966\\E", "shortCiteRegEx": "Levitin and Polyak.", "year": 1966}, {"title": "An inexact interior point method for L1-reguarlized sparse covariance selection", "author": ["L. Li", "K.-C. Toh"], "venue": "Mathematical Programming Computation,", "citeRegEx": "Li and Toh.,? \\Q2010\\E", "shortCiteRegEx": "Li and Toh.", "year": 2010}, {"title": "Smooth optimization approach for sparse covariance selection", "author": ["Z. Lu"], "venue": "SIAM Journal of Optimization,", "citeRegEx": "Lu.,? \\Q2009\\E", "shortCiteRegEx": "Lu.", "year": 2009}, {"title": "Exact covariance thresholding into connected components for large-scale graphical lasso", "author": ["R. Mazumder", "T. Hastie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mazumder and Hastie.,? \\Q2012\\E", "shortCiteRegEx": "Mazumder and Hastie.", "year": 2012}, {"title": "The group lasso for logistic regression", "author": ["L. Meier", "S. Van de Geer", "P. B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Meier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2008}, {"title": "Newton-like methods for sparse inverse covariance estimation", "author": ["P. Olsen", "F. Oztoprak", "J. Nocedal", "S. Rennie"], "venue": "Technical report,", "citeRegEx": "Olsen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Olsen et al\\.", "year": 2012}, {"title": "The conjugate gradient method in extremal problems. U.S.S.R", "author": ["B.T. Polyak"], "venue": "Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak.,? \\Q1969\\E", "shortCiteRegEx": "Polyak.", "year": 1969}, {"title": "High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence", "author": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"], "venue": null, "citeRegEx": "Ravikumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2011}, {"title": "Learning sparse Gaussian Markov networks using a greedy coordinate ascent approach", "author": ["K. Scheinberg", "I. Rish"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Scheinberg and Rish.,? \\Q2010\\E", "shortCiteRegEx": "Scheinberg and Rish.", "year": 2010}, {"title": "Sparse inverse covariance selection via alternating linearization methods", "author": ["K. Scheinberg", "S. Ma", "D. Glodfarb"], "venue": null, "citeRegEx": "Scheinberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Scheinberg et al\\.", "year": 2010}, {"title": "Graphical model structure learning with l1-regularization", "author": ["M. Schmidt"], "venue": "PhD thesis, University of British Columbia,", "citeRegEx": "Schmidt.,? \\Q2010\\E", "shortCiteRegEx": "Schmidt.", "year": 2010}, {"title": "Optimizing costly functions with simple constraints: A limited-memory projected quasi-Newton algorithm", "author": ["M. Schmidt", "E. Van Den Berg", "M.P. Friedl", "K. Murphy"], "venue": "In Proc. of Conf. on Artificial Intelligence and Statistics,", "citeRegEx": "Schmidt et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "A coordinate gradient descent method for nonsmooth separable minimization", "author": ["P. Tseng", "S. Yun"], "venue": "Mathematical Programming,", "citeRegEx": "Tseng and Yun.,? \\Q2007\\E", "shortCiteRegEx": "Tseng and Yun.", "year": 2007}, {"title": "New insights and faster computations for the graphical lasso", "author": ["D.M. Witten", "J.H. Friedman", "N. Simon"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Witten et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2011}, {"title": "Coordinate descent algorithms for lasso penalized regression", "author": ["T.T. Wu", "K. Lange"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Wu and Lange.,? \\Q2008\\E", "shortCiteRegEx": "Wu and Lange.", "year": 2008}, {"title": "A comparison of optimization methods and software for large-scale L1-regularized linear classification", "author": ["G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2010}, {"title": "An improved GLMNET for L1-regularized logistic regression", "author": ["G.-X. Yuan", "C.-H. Ho", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "Model selection and estimation in the Gaussian graphical model", "author": ["M. Yuan", "Y. Lin"], "venue": null, "citeRegEx": "Yuan and Lin.,? \\Q2007\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2007}, {"title": "A coordinate gradient descent method for L1-regularized convex minimization", "author": ["S. Yun", "K.-C. Toh"], "venue": "Computational Optimizations and Applications,", "citeRegEx": "Yun and Toh.,? \\Q2011\\E", "shortCiteRegEx": "Yun and Toh.", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Accordingly, recent papers by Banerjee et al. [2008], Friedman et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 1, "context": "Accordingly, recent papers by Banerjee et al. [2008], Friedman et al. [2008], Yuan and Lin [2007] have proposed an estimator that minimizes the Gaussian negative log-likelihood regularized by the l1 norm of the entries (typically restricted to those on the off-diagonal) of the inverse covariance matrix, which encourages sparsity in its entries.", "startOffset": 30, "endOffset": 77}, {"referenceID": 1, "context": "Accordingly, recent papers by Banerjee et al. [2008], Friedman et al. [2008], Yuan and Lin [2007] have proposed an estimator that minimizes the Gaussian negative log-likelihood regularized by the l1 norm of the entries (typically restricted to those on the off-diagonal) of the inverse covariance matrix, which encourages sparsity in its entries.", "startOffset": 30, "endOffset": 98}, {"referenceID": 13, "context": "A preliminary version of this paper appeared in the NIPS 2011 conference [Hsieh et al., 2011].", "startOffset": 73, "endOffset": 93}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]).", "startOffset": 177, "endOffset": 199}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the l1-regularized Gaussian MLE? For superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner. In this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the l1-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al.", "startOffset": 177, "endOffset": 1946}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the l1-regularized Gaussian MLE? For superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner. In this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the l1-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al. [2010]), which we then solve using coordinate descent.", "startOffset": 177, "endOffset": 1970}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the l1-regularized Gaussian MLE? For superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner. In this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the l1-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al. [2010]), which we then solve using coordinate descent. A key facet of our method is that we are able to reduce the computational cost of a coordinate descent update from the naive O(p2) to O(p) complexity by exploiting the structure present in the problem, and by a careful arrangement and caching of the computations. Furthermore, an Armijo-rule based step size selection rule ensures sufficient descent and positive definiteness of the intermediate iterates. Finally, we use the form of the stationary condition characterizing the optimal solution to focus the Newton direction computation on a small subset of free variables, but in a manner that preserves the strong convergence guarantees of second-order descent. We note that when the solution has a block-diagonal structure as described in Mazumder and Hastie [2012], Witten et al.", "startOffset": 177, "endOffset": 2787}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the l1-regularized Gaussian MLE? For superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner. In this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the l1-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al. [2010]), which we then solve using coordinate descent. A key facet of our method is that we are able to reduce the computational cost of a coordinate descent update from the naive O(p2) to O(p) complexity by exploiting the structure present in the problem, and by a careful arrangement and caching of the computations. Furthermore, an Armijo-rule based step size selection rule ensures sufficient descent and positive definiteness of the intermediate iterates. Finally, we use the form of the stationary condition characterizing the optimal solution to focus the Newton direction computation on a small subset of free variables, but in a manner that preserves the strong convergence guarantees of second-order descent. We note that when the solution has a block-diagonal structure as described in Mazumder and Hastie [2012], Witten et al. [2011], the fixed/free set selection in QUIC can automatically identify this sparsity structure and avoid updates to the off-diagonal block elements.", "startOffset": 177, "endOffset": 2809}, {"referenceID": 17, "context": "We summarize the experimental results in Section 5, where we compare the algorithm using both real data and synthetic examples from Li and Toh [2010]. We observe that our algorithm performs overwhelmingly better (quadratic instead of linear convergence) than existing solutions described in the literature.", "startOffset": 132, "endOffset": 150}, {"referenceID": 28, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al.", "startOffset": 119, "endOffset": 139}, {"referenceID": 1, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al.", "startOffset": 140, "endOffset": 163}, {"referenceID": 1, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al. [2008], Ravikumar et al.", "startOffset": 140, "endOffset": 187}, {"referenceID": 1, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al. [2008], Ravikumar et al. [2011], Duchi et al.", "startOffset": 140, "endOffset": 212}, {"referenceID": 1, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al. [2008], Ravikumar et al. [2011], Duchi et al. [2008]. Due in part to its importance, there has been an active line of work on efficient optimization methods for solving (2) and (3).", "startOffset": 140, "endOffset": 233}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso.", "startOffset": 0, "endOffset": 326}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM.", "startOffset": 0, "endOffset": 631}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM.", "startOffset": 0, "endOffset": 696}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al.", "startOffset": 0, "endOffset": 872}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package.", "startOffset": 0, "endOffset": 980}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO.", "startOffset": 0, "endOffset": 1168}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO. However, each coordinate update of SINCO has a time complexity of O(p2), which becomes computationally prohibitive when handling large problems. We will show in this paper that after forming the quadratic approximation, the time complexity of one coordinate update can be performed in O(p) operations. This trick is one of the key advantages of our proposed method, QUIC. One common characteristic of the above methods is that they are first-order iterative methods that mainly use gradient information at each step. Such first-order methods have become increasingly popular in recent years for high-dimensional problems in part due to their ease of implementation, and because they require very little computation and memory at each step. The caveat is that they have at most linear rates of convergence (Bertsekas [1995]).", "startOffset": 0, "endOffset": 2101}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO. However, each coordinate update of SINCO has a time complexity of O(p2), which becomes computationally prohibitive when handling large problems. We will show in this paper that after forming the quadratic approximation, the time complexity of one coordinate update can be performed in O(p) operations. This trick is one of the key advantages of our proposed method, QUIC. One common characteristic of the above methods is that they are first-order iterative methods that mainly use gradient information at each step. Such first-order methods have become increasingly popular in recent years for high-dimensional problems in part due to their ease of implementation, and because they require very little computation and memory at each step. The caveat is that they have at most linear rates of convergence (Bertsekas [1995]). To achieve superlinear convergence rates, one has to consider second-order methods, which have attracted some attention only recently for the sparse inverse covariance estimation problem. Li and Toh [2010] handle the non-smoothness of the l1 regularization in the objective function by doubling the number of variables, and solving the resulting constrained optimization problem by an inexact interior point method.", "startOffset": 0, "endOffset": 2309}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO. However, each coordinate update of SINCO has a time complexity of O(p2), which becomes computationally prohibitive when handling large problems. We will show in this paper that after forming the quadratic approximation, the time complexity of one coordinate update can be performed in O(p) operations. This trick is one of the key advantages of our proposed method, QUIC. One common characteristic of the above methods is that they are first-order iterative methods that mainly use gradient information at each step. Such first-order methods have become increasingly popular in recent years for high-dimensional problems in part due to their ease of implementation, and because they require very little computation and memory at each step. The caveat is that they have at most linear rates of convergence (Bertsekas [1995]). To achieve superlinear convergence rates, one has to consider second-order methods, which have attracted some attention only recently for the sparse inverse covariance estimation problem. Li and Toh [2010] handle the non-smoothness of the l1 regularization in the objective function by doubling the number of variables, and solving the resulting constrained optimization problem by an inexact interior point method. Schmidt et al. [2009] propose a second order Projected Quasi-Newton method (PQN) that solves the dual problem (4), since the dual objective function is smooth.", "startOffset": 0, "endOffset": 2541}, {"referenceID": 13, "context": "Subsequent to the preliminary version of this paper (see [Hsieh et al., 2011]), Olsen et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 13, "context": "Subsequent to the preliminary version of this paper (see [Hsieh et al., 2011]), Olsen et al. [2012] have proposed generalizations to our framework to allow various inner solvers such as FISTA, conjugate gradient (CG), and LBFGS to be used, in addition to our proposed coordinate descent scheme.", "startOffset": 58, "endOffset": 100}, {"referenceID": 29, "context": "Following the approach of Tseng and Yun [2007] and Yun and Toh [2011], we build a quadratic approximation around any iterate Xt for this composite function by first considering the second-order Taylor expansion of the smooth component g(X):", "startOffset": 26, "endOffset": 47}, {"referenceID": 29, "context": "Following the approach of Tseng and Yun [2007] and Yun and Toh [2011], we build a quadratic approximation around any iterate Xt for this composite function by first considering the second-order Taylor expansion of the smooth component g(X):", "startOffset": 26, "endOffset": 70}, {"referenceID": 28, "context": "Note that it can be rewritten as a standard Lasso regression problem [Tibshirani, 1996]:", "startOffset": 69, "endOffset": 87}, {"referenceID": 25, "context": "This variant of Newton method for such composite objectives is also referred to as a \u201cproximal Newton-type method,\u201d and was empirically studied in Schmidt [2010]. Tseng and Yun [2007] considered the more general case where the Hessian \u2207g(Xt) is replaced by any positive definite matrix.", "startOffset": 147, "endOffset": 162}, {"referenceID": 25, "context": "This variant of Newton method for such composite objectives is also referred to as a \u201cproximal Newton-type method,\u201d and was empirically studied in Schmidt [2010]. Tseng and Yun [2007] considered the more general case where the Hessian \u2207g(Xt) is replaced by any positive definite matrix.", "startOffset": 147, "endOffset": 184}, {"referenceID": 15, "context": "See also the recent paper by Lee et al. [2012], where convergence properties of such general proximal Newton-type methods are discussed.", "startOffset": 29, "endOffset": 47}, {"referenceID": 20, "context": "Many efficient optimization methods exist that solve Lasso regression problems, such as the coordinate descent method [Meier et al., 2008], the gradient projection method [Polyak, 1969], and iterative shrinking methods [Daubechies et al.", "startOffset": 118, "endOffset": 138}, {"referenceID": 22, "context": ", 2008], the gradient projection method [Polyak, 1969], and iterative shrinking methods [Daubechies et al.", "startOffset": 40, "endOffset": 54}, {"referenceID": 10, "context": "In Friedman et al. [2007], Wu and Lange [2008], the authors show that coordinate descent methods are very efficient for solving Lasso type problems.", "startOffset": 3, "endOffset": 26}, {"referenceID": 10, "context": "In Friedman et al. [2007], Wu and Lange [2008], the authors show that coordinate descent methods are very efficient for solving Lasso type problems.", "startOffset": 3, "endOffset": 47}, {"referenceID": 3, "context": "We adopt Armijo\u2019s rule (Bertsekas [1995],Tseng and Yun [2007]) and try step-sizes \u03b1 \u2208 {\u03b20, \u03b21, \u03b22, .", "startOffset": 24, "endOffset": 41}, {"referenceID": 3, "context": "We adopt Armijo\u2019s rule (Bertsekas [1995],Tseng and Yun [2007]) and try step-sizes \u03b1 \u2208 {\u03b20, \u03b21, \u03b22, .", "startOffset": 24, "endOffset": 62}, {"referenceID": 1, "context": "We note that the conclusion of the lemma also holds if the conditions on \u039b and S are replaced by only the requirement that the diagonal elements of \u039b are positive, see Banerjee et al. [2008]. We emphasize that Lemma 3 allows the extension of the convergence results to the practically important case when the regularization does not penalize the diagonal.", "startOffset": 168, "endOffset": 191}, {"referenceID": 29, "context": "The attractive facet of this modification is that it leverages the sparsity of the solution and intermediate iterates in a manner that falls within the block coordinate descent framework of Tseng and Yun [2007]. The index sets J1, J2, .", "startOffset": 190, "endOffset": 211}, {"referenceID": 29, "context": "The attractive facet of this modification is that it leverages the sparsity of the solution and intermediate iterates in a manner that falls within the block coordinate descent framework of Tseng and Yun [2007]. The index sets J1, J2, . . . corresponding to the block coordinate descent steps in the general setting of Tseng and Yun [2007][p.", "startOffset": 190, "endOffset": 340}, {"referenceID": 32, "context": "A similar (so called shrinking) strategy is used in SVM and l1-regularized logistic regression problems as mentioned in Yuan et al. [2010]. In our experiments, we demonstrate that this strategy reduces the size of the free set very quickly.", "startOffset": 120, "endOffset": 139}, {"referenceID": 19, "context": "4 The block-diagonal structure of X It has been shown recently by (Mazumder and Hastie [2012],Witten et al.", "startOffset": 67, "endOffset": 94}, {"referenceID": 19, "context": "4 The block-diagonal structure of X It has been shown recently by (Mazumder and Hastie [2012],Witten et al. [2011]) that when the thresholded covariance matrixE defined byEij = S(Sij , \u03bb) = sign(Sij)max(|Sij |\u2212 \u03bb, 0) has the following block-diagonal structure:", "startOffset": 67, "endOffset": 115}, {"referenceID": 13, "context": "Then, if Eij = S(Sij , \u03bbij) is block diagonal, so is the solution X \u2217 of (3), see Hsieh et al. [2012]. Thus each X\u2217 i can be computed independently.", "startOffset": 82, "endOffset": 102}, {"referenceID": 1, "context": "Banerjee et al. [2008] showed that for the special case where \u039bij = \u03bb the optimization problem (2) has a unique global optimum and that the eigenvalues of the primal optimal solutionX\u2217 are bound.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "Similar to the block coordinate descent framework of Tseng and Yun [2007], we assume the index set Jt satisfies a Gauss-Seidel type of condition:", "startOffset": 53, "endOffset": 74}, {"referenceID": 15, "context": "2 Asymptotic Convergence Rate Newton methods on constrained minimization problems: The convergence rate of the Newton method on bounded constrained minimization has been studied in Levitin and Polyak [1966] and Dunn [1980].", "startOffset": 181, "endOffset": 207}, {"referenceID": 9, "context": "2 Asymptotic Convergence Rate Newton methods on constrained minimization problems: The convergence rate of the Newton method on bounded constrained minimization has been studied in Levitin and Polyak [1966] and Dunn [1980]. Here, we briefly mention their results.", "startOffset": 211, "endOffset": 223}, {"referenceID": 9, "context": "1 in Dunn [1980]).", "startOffset": 5, "endOffset": 17}, {"referenceID": 9, "context": "1 in Dunn [1980]). Assume F is strictly convex, has a unique minimizer x\u2217 in \u03a9, and that \u22072F (x) is Lipschitz continuous. Then for all x0 sufficiently close to x\u2217, the sequence {xk} generated by (43) converges quadratically to x \u2217. This theorem is proved in Dunn [1980]. In our case, the objective function f(X) is non-smooth so that Theorem 3 does not directly apply.", "startOffset": 5, "endOffset": 270}, {"referenceID": 17, "context": "In Figure 1 we plot the total run times for the ER biology dataset from [Li and Toh, 2010] correspond to different numbers of inner iterations used in the coordinate descent solver of QUIC.", "startOffset": 72, "endOffset": 90}, {"referenceID": 25, "context": "\u2022 ALM: the Alternating Linearization Method proposed by Scheinberg et al. [2010]. We use their MATLAB source code for the experiments.", "startOffset": 56, "endOffset": 81}, {"referenceID": 5, "context": "\u2022 ADMM: another implementation of the alternating linearization method implemented by Boyd et al. [2012]. The matlab code can be downloaded from http://www.", "startOffset": 86, "endOffset": 105}, {"referenceID": 9, "context": "\u2022 glasso: the block coordinate descent method proposed by Friedman et al. [2008]. We use the latest version glasso 1.", "startOffset": 58, "endOffset": 81}, {"referenceID": 8, "context": "\u2022 PSM: the Projected Subgradient Method proposed by Duchi et al. [2008]. We use the MATLAB source code available at http://www.", "startOffset": 52, "endOffset": 72}, {"referenceID": 8, "context": "\u2022 PSM: the Projected Subgradient Method proposed by Duchi et al. [2008]. We use the MATLAB source code available at http://www.cs.ubc.ca/~schmidtm/Software/PQN.html. \u2022 SINCO: the greedy coordinate descent method proposed by Scheinberg and Rish [2010]. The code can be downloaded from https://projects.", "startOffset": 52, "endOffset": 251}, {"referenceID": 17, "context": "\u2022 IPM: An inexact interior point method proposed by Li and Toh [2010]. The source code can be downloaded from http://www.", "startOffset": 52, "endOffset": 70}, {"referenceID": 17, "context": "\u2022 IPM: An inexact interior point method proposed by Li and Toh [2010]. The source code can be downloaded from http://www.math.nus.edu.sg/~mattohkc/Covsel-0.zip. \u2022 PQN: the projected quasi-Newton method proposed by Schmidt et al. [2009]. The source code can be downloaded from http://www.", "startOffset": 52, "endOffset": 236}, {"referenceID": 17, "context": "\u2022 Graphs with Random Sparsity Structures: We use the procedure mentioned in Example 1 in Li and Toh [2010] to generate inverse covariance matrices with random non-zero patterns.", "startOffset": 89, "endOffset": 107}, {"referenceID": 17, "context": "We use the real world biology datasets preprocessed by Li and Toh [2010] to compare the performance of our method with other state-of-the-art methods.", "startOffset": 55, "endOffset": 73}, {"referenceID": 19, "context": "3 Block-diagonal structure As discussed earlier, Mazumder and Hastie [2012], Witten et al.", "startOffset": 49, "endOffset": 76}, {"referenceID": 19, "context": "3 Block-diagonal structure As discussed earlier, Mazumder and Hastie [2012], Witten et al. [2011] showed that when the thresholded covariance matrix E = max(|S|\u2212\u03bb, 0) is block-diagonal, then the problem can be naturally decomposed into sub-problems.", "startOffset": 49, "endOffset": 98}], "year": 2013, "abstractText": "The l1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to recent state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton\u2019s method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and present experimental results using synthetic and real-world application data that demonstrate the considerable improvements in performance of our method when compared to other state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}