{"id": "1306.4633", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2013", "title": "A Fuzzy Based Approach to Text Mining and Document Clustering", "abstract": "Fuzzy logvinenko logic deals with spartiates degrees of woroniecki truth. In this lawgivers paper, helvetii we have shown prince-archbishopric how to tofu apply hornaday fuzzy logic 74.23 in obelisk text mining in order jitters to pomerania-wolgast perform document clustering. basemen We took an example of document muqataa clustering atout where the documents a330-300 had kenrick to be gay clustered into two categories. The rhinestone method udonis involved cleaning adulterants up precipitately the funk text and through stemming 264th of words. Then, fond we chose abiding m number of yahiya features bahawal which differ hamao significantly in 112.3 their bridgeless word frequencies (WF ), septuagenarians normalized by document length, between courtside documents humperdink belonging to these 25.46 two 1,472 clusters. 15.13 The harry documents to be barha clustered non-family were thornbill represented bojkov as a calypsos collection visiters of gatemouth m 1946/47 normalized WF values. 41-9 Fuzzy c - means (FCM) doull algorithm alyce was mirifica used to cluster hodgdon these gammie documents aviron into genna two clusters. mccumbee After the FCM ambacang execution wauwatosa finished, the pancrace documents \u00e9tienne in testimonium the 1983 two 1-iron clusters were pinta analysed for skylstad the unwedded values of mq-9 their nordheim respective erase m features. It warhol was known 189.5 that fuhl documents belonging qawwal to a bimonthly document type, maozhen say X, 58.90 tend headend to have basken higher WF values for some non-university particular foulke features. If flamingo the 8-tracks documents 5.32 belonging to zueva a cluster had 86.22 higher WF spittin values lubricator for pennypack those malouh same kaleida features, 106.89 then that 26.4 cluster yusufov was aidan said unrecovered to katwe represent eudaimonia X. By osbat fuzzy artfully logic, we not 30.8 only familiaris get discounting the 189,000 cluster name, but also vexing the receded degree spotkick to asphyxiation which tadlock a hummelstown document belongs toril to partizan a caller cluster.", "histories": [["v1", "Thu, 6 Jun 2013 07:35:23 GMT  (359kb)", "http://arxiv.org/abs/1306.4633v1", "10 pages, 6 tables, 1 figure, review paper, International Journal of Data Mining &amp; Knowledge Management Process (IJDKP) ISSN : 2230 - 9608[Online] ; 2231 - 007X [Print]. Paper can be found atthis http URL"]], "COMMENTS": "10 pages, 6 tables, 1 figure, review paper, International Journal of Data Mining &amp; Knowledge Management Process (IJDKP) ISSN : 2230 - 9608[Online] ; 2231 - 007X [Print]. Paper can be found atthis http URL", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["sumit goswami", "mayank singh shishodia"], "accepted": false, "id": "1306.4633"}, "pdf": {"name": "1306.4633.pdf", "metadata": {"source": "CRF", "title": "A FUZZY BASED APPROACH TO TEXT MINING AND DOCUMENT CLUSTERING", "authors": ["Sumit Goswami", "Mayank Singh Shishodia"], "emails": ["sumit_13@yahoo.com", "mayanksshishodia@gmail.com"], "sections": [{"heading": null, "text": "Fuzzy logic deals with degrees of truth. In this paper, we have shown how to apply fuzzy logic in text mining in order to perform document clustering. We took an example of document clustering where the documents had to be clustered into two categories. The method involved cleaning up the text and stemming of words. Then, we chose \u2018m\u2019 features which differ significantly in their word frequencies (WF), normalized by document length, between documents belonging to these two clusters. The documents to be clustered were represented as a collection of \u2018m\u2019 normalized WF values. Fuzzy c-means (FCM) algorithm was used to cluster these documents into two clusters. After the FCM execution finished, the documents in the two clusters were analysed for the values of their respective \u2018m\u2019 features. It was known that documents belonging to a document type \u2018X\u2019 tend to have higher WF values for some particular features. If the documents belonging to a cluster had higher WF values for those same features, then that cluster was said to represent \u2018X\u2019. By fuzzy logic, we not only get the cluster name, but also the degree to which a document belongs to a cluster.\nKEYWORDS\nFuzzy Logic, Text Mining, Fuzzy c-means, Document Clustering"}, {"heading": "1. INTRODUCTION", "text": "There is an increasing amount of textual data available every day. Text Mining is the automated process of extracting some previously unknown information from analysis of these unstructured texts [1]. Although Text Mining is similar to Data Mining, yet they are quite different. Data mining is related to pattern-extraction from databases while for text-mining, it is natural language. It is also different from search, web search in particular, where the information being searched already exists. In Text Mining, the computer is trying to extract something new from the already available resources.\nIn this review paper, we have applied fuzzy logic to text-mining to perform document clustering into a number of pre-specified clusters. As an example, we have shown how to classify given documents into two categories \u2013 sports and politics. First, the documents are cleaned by removal of advertisements and tags. The hyphens are dealt with and the stop words are removed. Word stemming is carried out to represent each word by its root [2] . Each document is then represented as a bag of words, a method which assigns weights to words on basis of their significance in the document which we considered to be their frequency of occurrence. We calculated word frequency as\nWF = (Word Count/ (Total Words in the Document)) x10000 (1)\n\u201em\u201f number of words are chosen [3][4][5] and each document is represented as a set of \u201em\u201f values which\nare the WF values of those corresponding \u201em\u201f words in that document. We then perform FCM on this representation to cluster the documents into required number of categories. Pre-known knowledge is used to name these clusters. In the forthcoming sections, we explain what fuzzy logic is, how it differs from probability, the FCM algorithm and how it can be used to perform document clustering with an example. We tell why fuzzy logic should be used for this purpose."}, {"heading": "2. FUZZY LOGIC", "text": "Fuzzy logic is a mathematical logic model in which truth can be partial i.e. it can have value between 0 and 1, that is completely false and completely true [6] . It is based on approximate reasoning instead of exact reasoning."}, {"heading": "2.1 Fuzzy Logic vs Probability", "text": "Probability can be divided into two broad categories \u2013 Evidential and Physical. Consider an event E with probability 80%. Evidential Probability says that given the current available evidence, the likelihood of E occurring is 80%. On the other hand, Physical Probability says that if the experiment involving E as an outcome was repeated an infinite number of times, then 80% times, E will occur [7] .\nFor example, consider a car \u201eA\u201f that goes 150 Kilometres per hour. Now this car would definitely be considered fast. Now consider another car \u201eB\u201f that goes 180 Kilometres per hour. In a repeatable experiment, it is not that the car \u201eB\u201f will be fast more often than A [Physical Probability] or that given the current information; it is more likely that Car \u201fB\u201f is faster than Car \u201eA\u201f [Evidential Probability]. Both the cars possess the property fast, but in different degrees.\nIf we use Fuzzy Logic for this problem and there is a class called \u201cFast Cars\u201d that contains the cars with the property \u201cfast\u201d , then the respective membership values of both Car A and Car B can be compared. The higher membership value means that the corresponding car possesses the property \u201cfast\u201d to a higher degree."}, {"heading": "2.2 Degrees of Truth", "text": "The degrees of truth are obtained from the membership function. An example is shown in Figure1:\nSuppose, we are given a statement that high frequency of occurrence of the word \u201cBall\u201d in a document means that the document is related to Sports. In order to associate numerical values with these \u201chigh\u201d or similarly \u201clow\u201d, \u201cmedium\u201d frequency of words, we use a membership function. Linguistic hedging can be used to create more categories by adding adjectives like \u201cVery\u201d, \u201cFairly\u201d, \u201dNot so\u201d, \u201cLess\u201d, \u201cMore\u201d etc.\nFigure1 shows a membership function which decides when the frequency of occurrence of the word \u201cball\u201d in a document is said to be low, medium or high. But it should be noted that the membership function depends on the situation and problem as perceived by the designer. For example, someone else may design the membership function where the word frequency of even 500 is considered low. This can be the case for a document containing the ball-by-ball commentary of a cricket match. A sample portion of this document could be like \u201cThe captain has handed over the ball to Player A for the final over. He runs in to bowl the first ball. It\u2019s a short ball outside the off stump. It\u2019s a no-ball! The batsman smashes the ball and it goes for a sixer\u201d. Here, the word ball occurs 5 times in total of 44 words. This is a 1136 word frequency (per 10000 words) value."}, {"heading": "3. CLUSTERING", "text": "Using Fuzzy Logic and text-mining we can cluster similar documents together. Document Clustering is used by the computer to group documents into meaningful groups. We will use c-means algorithm. This algorithm is classified into two types- Hard c-means and FCM [6] .\nHard c-means algorithm is used to cluster \u201em\u201f observations into \u201ec\u201f clusters. Each cluster has its cluster centre. The observation belongs to the cluster which has the least distance from it. The clustering is crisp, which means that each observation is clustered into one and only one cluster.\nFCM is a variation of the hard c-means clustering algorithm. Every observation here has a membership value associated with each of the clusters which is related inversely to the distance of that observation from the centre of the cluster."}, {"heading": "3.1 The FCM Algorithm", "text": "The main aim of FCM algorithm is to minimize the objective function given by [6] :\n, (2)\nWhere\nm denotes any real number greater than 1\nuij denotes membership degree of xi in Jth cluster\nxi is ith dimension of the measured data\nci is the ith dimension of the cluster centre.\nThe formula for updating the cluster centre ci is:\n(3)\nThe Partition Matrix values are updated using the formula:\n(4)\nThe FCM algorithm\u201fs iteration stops when the maximum change in the values of Fuzzy c-partition matrix is less than E, where E is a termination criterion with value between 0 and 1.\nThe steps followed in FCM Algorithm are given below:\n1. Initialize the fuzzy partition matrix, U=[uij] matrix, U (0)\n2. At k-step: calculate the centre\u201fs vectors C (k) = [cj] with U (k) using equation (3).\n3. Update U (k),U (k+1) using equation (4).\n4. If || U (k+1) - U (k) ||< then STOP; otherwise return to 2.\nThe documents are clustered based on a representation called \u201cbag of words\u201d."}, {"heading": "4. Bag of Words Representation of a Document", "text": "Words are assigned weights according to significance in the document which can be the number of occurrences of that word in that document.\nConsider a paragraph:\n\u201cIt is a variation of the Hard C-Means clustering algorithm. Each observation here has a membership value associated with each of the clusters which is related inversely to the distance of that observation from the centre of the cluster.\u201d\nThe Bag of Words representation for the above paragraph will look similar to Table1.\nApart from single words, we can also use phrases better results. Few examples of these are \u201cengineering student\u201d, \u201ccomputer games\u201d, \u201cTourist attraction\u201d, \u201cgold medal\u201d etc. The same Bag of Words technique can be modified to treat the phrases as single words. In our method, we assigned the weights as the number of occurrences of the word in the document normalized by the document length and multiplied by 10,000."}, {"heading": "5. STEPS INVOLVED IN APPLYING FUZZY LOGIC TO TEXT MINING", "text": "The following steps are followed in order to apply fuzzy logic to text mining."}, {"heading": "5.1 Pre-processing of Text", "text": "This phase involves cleaning up the text like removing advertisements etc. Also we have to\ndeal with hyphens, remove tags from documents like HTML, XML etc. Removal of this unwanted text helps improve the efficiency of our algorithm."}, {"heading": "5.2 Feature Generation", "text": "The documents are represented in the \u201cBag of Words\u201d method. Stop words like \u201cthe\u201d, \u201ca\u201d,\n\u201dan\u201d etc. are removed. Word Stemming [2] is carried out. Word Stemming refers to representing the word by its root, example the words - driving, drove and driven should be represented by drive. This is important because not all the documents are in the same active or passive voice, or the same tense."}, {"heading": "5.3 Feature Selection", "text": "Here, the features that we will use are selected. This selection of features can either be done\nbefore use or based on use. Number of features that will be used for our process is further reduced. We choose only the features which will help us in our process [3][4][5] . In order to cluster separately, the documents relating to sports and politics, presence of names of people in a document will not help much. There can be names in both of those. However, presence of words like \u201cstadium\u201d , \u201cball\u201d ,\u201dcheerleaders\u201d etc. will help relate the documents to the field of sports. Similarly, words like \u201celection\u201d, \u201cdemocracy\u201d, \u201cparliament\u201d will relate the documents to politics category.\nIn some situations, however, it will be hard to manually point out the features which distinguish documents that should belong to different types. In such situations, we need to find out what features we should use for our clustering. To do this, we can take sample documents which are already known to belong to our required categories. Then the information gained from the analysis of these documents can be applied to point out the differences among documents belonging to different clusters. The word frequency, for various words (normalized by document length), can be calculated for these documents. We choose the words with a significant word frequency variation between different document types."}, {"heading": "5.4 Clustering", "text": "The method used here is FCM clustering algorithm. Each document to be clustered has\nalready been represented as a \u201cbag of words\u201d, and from that \u201cbag\u201d only the essential \u201cwords\u201d (features) have been kept. The aim now is to cluster the documents into categories (called classes in FCM).Using FCM, the documents having similar frequencies (normalized by document length) of various selected features are clustered together. The number of clusters to be made can be specified at an early stage but this might result in inaccurate results later if this number is not appropriate. This problem is known as Cluster Validity.\nOnce the FCM execution finishes, either due to accomplishing the pre-specified number of iterations or because of max-change in the Fuzzy Partition Matrix being lesser than the threshold, we obtain the required number of clusters and all the documents have been clustered among them."}, {"heading": "5.5 Evaluation and Interpretation of Results", "text": "From the Fuzzy Partition Matrix, one can find to what degree a given document belongs to\neach cluster. If the membership value of a document for a given cluster is high, the document can be said to strongly belong to that cluster. However, if all the membership values corresponding to a specific document are almost same (Example .35, .35, .30 among three clusters), it would imply that the document does not quite strongly belong to any of those clusters."}, {"heading": "6. EXAMPLE", "text": "Suppose we are given some documents which we have to cluster into two categories -\u201csports\u201d and \u201cpolitics\u201d.\nWe will not present the first two steps that we followed as they are pretty simple. We will start from the third \u2013 feature selection. We had no idea about which words (features) we could use to cluster our documents. Hence, we had to find such features.\nWe took some documents that we knew were related to Sports and Politics respectively. We applied step1 and step 2 on them and then counted the word frequency of various words using eq(1). Then we analysed the differences in these WF values for same words between documents relating to Sports category and the documents relating to Politics category.\nThe partial result of our analysis is presented in Table 2.\nBy looking at the Table 2, it was apparent that Words like \u201cWin\u201d, \u201cCampaign\u201d are used in similar amounts in documents belonging to both groups. However, words like \u201cStadium\u201d (203 v/s 7), \u201cBall\u201d (501 v/s 30), \u201cTeam\u201d (250 v/s 80) and \u201cDemocracy\u201d (1 v/s 140) could be used to differentiate among documents belonging to Sports and Politics respectively. For the sake of simplicity and an easy example, we chose only 4 words \u2013 \u201cStadium\u201d, \u201cBall\u201d, \u201cTeam\u201d and \u201cDemocracy\u201d. On basis of these 4 words, we clustered the given documents into Politics and Sports categories.\nNow that we had the documents to be clustered and had selected the features we wanted to use for the clustering process; we could start our clustering process.\nLet D= {d1, d2, d3\u2026dn} represent our \u201en\u201f documents to be clustered.\nNow each of these documents, di, is defined by the \u201em\u201f selected features i.e.\ndi= {di1, di2, di3..dim}\nHere, each di in the universe D is a m-dimensional vector representing the m selected features which will be normalised in the same way we did for feature selecting by using eq(1). An alternate way to look at this problem is considering each di as a point in m-dimensional space representing the features. D, the universe, is a set of points with n elements (documents) in the same sample space.\nNow we have to initialise the fuzzy partition matrix. Keeping in mind the constraints that all entries should be in the interval [0, 1] and the columns should add to a total of 1 and considering that we are given 4 documents to cluster:\nOur initial assumption is that Doc1, Doc2, Doc5 and Doc6 belong to cluster 1 and Doc3, Doc4, Doc7 and Doc8 belong to cluster 2. Now we have to calculate the initial cluster centres. For c=1, that is cluster1, the centre (V1) can be calculated using eq (3). Since, the membership value of Doc3, Doc4, Doc7 and Doc8 in Cluster 1 is 0, we have\nV1j =((1)x1j+(1)x2j+(1)x5j+(1)x6j) / ( 1 2 +1 2 +1 2 +1 2 )= (x1j+x2j+ x5j + x6j)/4\nNow, say our word frequencies were like"}, {"heading": "V11= (180+200+210+7)/4=149.25", "text": "Similarly, Calculating all V1j we get\nV1= {149.25, 300, 162.5, 7.5} is the centre of cluster 1.\nSimilarly, V2= {50,110.75, 67.75, 33.25} is the centre of cluster 2.\nNow calculating the Euclidian distances of each Document vector from both centre clusters:\nD11 = ((180-149.25) 2 + (400-300) 2 + (200-162.5) 2 + (1-7.75) 2 ) 1/2 = 111.32\nSimilarly,"}, {"heading": "D12 = 149.5, D13 = 339.5, D14 = 352.5, D15 = 102.2, D16 = 353.4, D17 = 109.2, D18 = 351.1", "text": "Similarly, from the cluster two:\nD21 = ((180-50) 2 + (400-110.75) 2 + (200-67.75) 2 + (1-33.25) 2 ) 1/2 = 345.10"}, {"heading": "D22 = 382.2, D23 = 105.1, D24 = 118.3, D25 = 334.4, D26 = 119.6, D27 = 339.7, D28 = 116.9", "text": "Now that we have distance of each vector from both cluster centres, we will now update the Fuzzy Partition Matrix, by using eq(4).\nNote that we have already decided the value of the \u201em\u201f in this formula, earlier in the example.\nU11= [1+ (111.32 / 345.10) 2 ] -1 =.90, U12= .867, U23= .913, U24= .898, U15=.9146, U26=.897, U17=.906, U28=.901\nThe remaining values of the fuzzy partition matrix are updated such that each column adds to 1. We get the updated Fuzzy Partition Matrix as\nSay our threshold change (stopping condition) was 0.001. Our max change here is 0.133 which is greater than 0.001. Hence we will continue. Now we will repeat the process by again calculating the new cluster centres. But now we will use the updated fuzzy partition matrix values. So, now our centre for cluster 1 will be calculated using\nV1j = (0.900 x1j +0.867 x2j + 0.087 x3j + 0.102 x4j + 0.915 x5j + 0.103 x6j + 0.906 x7j + 0.099 x8j)/ (0.900 2 +0.867 2 +0.087 2 +0.102 2 +0.915 2 +0.103 2 +0.906 2 +0.099 2 )"}, {"heading": "7. INTERPRETING THE RESULTS", "text": "Suppose that our final fuzzy partition matrix for 8 documents looks something like this:\nWe see that Doc1, Doc2, Doc5, Doc7 belong to cluster 1 whereas Doc3, Doc4, Doc6, Doc8 belong to cluster 2 on basis of high membership values in those respective clusters.\nIn the example we took, the documents Doc1, Doc2, Doc5, and Doc7 had higher frequency of words like stadium, ball and team (which are related to sports). Since these documents have been clustered together, one can say that Cluster 1 is Sports. At the same time, Cluster2 will contain documents with higher frequency of words like democracy (which are related to politics). Hence Cluster2 represents Politics.\nHere, Doc1 relates to Sports to a large degree (due to its very high membership value). If we set the criteria that membership values greater than 0.85 with respect to a given cluster can be called \u201cstrong membership\u201d, then Doc1 and Doc5 can be said to \u201cstrongly\u201d belong to Cluster1 which is sports. Moreover, we can say that Doc1 relates to Sports more \u201cstrongly\u201d than Doc5 does. This interpretation of results in linguistic form [8] is what gives advantage to usage of Fuzzy Logic over Probability models like Bayesian in Text Mining."}, {"heading": "8. CONCLUSION", "text": "In this paper, we showed how one can use fuzzy logic in text mining to cluster documents by taking an example where the documents were clustered into two topics :- \u201csports\u201d and \u201cpolitics\u201d. The advantage of using fuzzy logic over probability was that in the former, we could calculate the degree to which a given document belonged to either categories- \u201csports\u201d as well as \u201cpolitics\u201d. This was not possible in the probability model. In other words, instead of simply saying whether the document belonged to \u201csports\u201d or \u201cpolitics\u201d, we could now tell the degree to which the document characteristics resembled that of a sports-related document as well as a politics-related document. By doing this for all documents in the data-set, we could also compare two documents and tell which one belongs \u201cmore\u201d to which topic. Also, because every document will have some membership values in each of the clusters, no useful document will ever excluded from search results [1][5] ."}, {"heading": "9. REFERENCES", "text": "1. Vishal Gupta, Gurpreet S. Lehal; \u201cA Survey of Text Mining Techniques and Applications\u201d;\nJournal of Emerging Technologies in Web Intelligence, Vol.1, No.1, August 2009\n2. K.Sathiyakumari, V.Preamsudha, G.Manimekalai; \u201cUnsupervised Approach for Document Clustering Using Modified Fuzzy C mean Algorithm\u201d; International Journal of Computer &\nOrganization Trends \u2013Volume 11 Issue3-2011.\n3. R. Rajendra Prasath, Sudeshna Sarkar: Unsupervised Feature Generation using Knowledge\nRepositories for Effective Text Categorization. ECAI 2010: 1101-1102\n4. Sumit Goswami, Sudeshna Sarkar, Mayur Rustagi: Stylometric Analysis of Bloggers' Age\nand Gender. ICWSM 2009"}, {"heading": "5. Sumit Goswami, Mayank Singh Shishodia; \u201cA fuzzy based approach to stylometric analysis", "text": "of blogger\u201fs age and gender\u201d; HIS 2012: 47-51"}, {"heading": "6. Ross, T. J. (2010); \u201cFuzzy Logic with Engineering Applications\u201d, Third Edition, John Wiley", "text": "& Sons, Ltd, Chichester, UK 7. Fuzzy logic vs Proobability (Good Math, Bad Math); http://scientopia.org/blogs /goodmath/ 2011 /02 /02 / fuzzy-logic-vs-probability/, last checked on 28th July 2012. 8. Nogueira, T.M. ; \u201c On The Use of Fuzzy Rules to Text Document Classification \u201d; 2010 10th\nInternational Conference on Hybrid Intelligent Systems (HIS),; 23-25 Aug 2010 Atlanta, US."}], "references": [{"title": "A Survey of Text Mining Techniques and Applications\u201d", "author": ["Vishal Gupta", "Gurpreet S. Lehal"], "venue": "Journal of Emerging Technologies in Web Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Unsupervised Feature Generation using Knowledge Repositories for Effective Text Categorization", "author": ["R. Rajendra Prasath", "Sudeshna Sarkar"], "venue": "ECAI", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Stylometric Analysis of Bloggers' Age and Gender", "author": ["Sumit Goswami", "Sudeshna Sarkar", "Mayur Rustagi"], "venue": "ICWSM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A fuzzy based approach to stylometric analysis of blogger\u201fs age and gender\u201d", "author": ["Sumit Goswami", "Mayank Singh Shishodia"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Fuzzy Logic with Engineering Applications", "author": ["T.J. Ross"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "On The Use of Fuzzy Rules to Text Document Classification \u201d", "author": ["T.M. Nogueira"], "venue": "International Conference on Hybrid Intelligent Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Text Mining is the automated process of extracting some previously unknown information from analysis of these unstructured texts [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "WF = (Word Count/ (Total Words in the Document)) x10000 (1) \u201em\u201f number of words are chosen [3][4][5] and each document is represented as a set of \u201em\u201f values which are the WF values of those corresponding \u201em\u201f words in that document.", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "WF = (Word Count/ (Total Words in the Document)) x10000 (1) \u201em\u201f number of words are chosen [3][4][5] and each document is represented as a set of \u201em\u201f values which are the WF values of those corresponding \u201em\u201f words in that document.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "WF = (Word Count/ (Total Words in the Document)) x10000 (1) \u201em\u201f number of words are chosen [3][4][5] and each document is represented as a set of \u201em\u201f values which are the WF values of those corresponding \u201em\u201f words in that document.", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "it can have value between 0 and 1, that is completely false and completely true [6] .", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "This algorithm is classified into two types- Hard c-means and FCM [6] .", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "1 The FCM Algorithm The main aim of FCM algorithm is to minimize the objective function given by [6] :", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "We choose only the features which will help us in our process [3][4][5] .", "startOffset": 62, "endOffset": 65}, {"referenceID": 2, "context": "We choose only the features which will help us in our process [3][4][5] .", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "We choose only the features which will help us in our process [3][4][5] .", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "Keeping in mind the constraints that all entries should be in the interval [0, 1] and the columns should add to a total of 1 and considering that we are given 4 documents to cluster:", "startOffset": 75, "endOffset": 81}, {"referenceID": 5, "context": "This interpretation of results in linguistic form [8] is what gives advantage to usage of Fuzzy Logic over Probability models like Bayesian in Text Mining.", "startOffset": 50, "endOffset": 53}], "year": 2013, "abstractText": "Fuzzy logic deals with degrees of truth. In this paper, we have shown how to apply fuzzy logic in text mining in order to perform document clustering. We took an example of document clustering where the documents had to be clustered into two categories. The method involved cleaning up the text and stemming of words. Then, we chose \u2018m\u2019 features which differ significantly in their word frequencies (WF), normalized by document length, between documents belonging to these two clusters. The documents to be clustered were represented as a collection of \u2018m\u2019 normalized WF values. Fuzzy c-means (FCM) algorithm was used to cluster these documents into two clusters. After the FCM execution finished, the documents in the two clusters were analysed for the values of their respective \u2018m\u2019 features. It was known that documents belonging to a document type \u2018X\u2019 tend to have higher WF values for some particular features. If the documents belonging to a cluster had higher WF values for those same features, then that cluster was said to represent \u2018X\u2019. By fuzzy logic, we not only get the cluster name, but also the degree to which a document belongs to a cluster.", "creator": "Microsoft\u00ae Word 2010"}}}