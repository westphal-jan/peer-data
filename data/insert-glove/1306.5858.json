{"id": "1306.5858", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2013", "title": "Distributed Heuristic Forward Search for Multi-Agent Systems", "abstract": "lovegren This kronprinz paper http://www.kennedy describes a abba number s.o. of jokl distributed nusatenggara forward search 1-foot algorithms helary for 10.07 solving (907) multi - agent clitics planning lehya problems. 37.49 We introduce knaap a distributed bookshop formulation of erudite non - 48.52 optimal cannizaro forward tgb search, 118.98 as counter-battery well mullens as chos\u014fn an optimal 50.8 version, MAD - sumeidi A *. valborg Our shraddha algorithms eulex exploit dactyls the structure of multi - luketic agent problems to not inhibitors only distribute pampers the tlatilco work efficiently scpa among connemara different audiological agents, but spilsby also overeaters to singace remove symmetries puteri and westerngeco reduce the alexiou overall post-industrial workload. The algorithms bertoldo ensure that private bese information is dumbadze not tif shared 1,367 among sainte-foy agents, yet computation is still efficient - - choppy outperforming current state - of - the - art distributed planners, tfr and gideon in some cases moustached even infringements centralized search - - despite cruchaga the 43.56 fact vaup\u00e9s that chromebook each heyne agent has access lignotuber only taiyuan to partial merill information.", "histories": [["v1", "Tue, 25 Jun 2013 06:58:31 GMT  (232kb,D)", "http://arxiv.org/abs/1306.5858v1", "26 pages"]], "COMMENTS": "26 pages", "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["raz nissim", "ronen brafman"], "accepted": false, "id": "1306.5858"}, "pdf": {"name": "1306.5858.pdf", "metadata": {"source": "CRF", "title": "Distributed Heuristic Forward Search for Multi-Agent Systems", "authors": ["Raz Nissim", "Ronen Brafman"], "emails": ["raznis@cs.bgu.ac.il", "brafman@cs.bgu.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Interest in multi-agent systems is constantly rising, and examples of virtual and real systems abound, with virtual social communities providing many such instances. The ability to plan for such systems and the ability of such systems to autonomously plan for themselves is an important challenge for AI, especially as the size of these systems can be quite large. In this context, a fundamental question is how to perform distributed planning for a distributed multi-agent system efficiently, and in many cases, how to do it while preserving privacy.\nDistributed planning is interesting for a number of reasons. Scientifically and intellectually, it is interesting to seek distributed algorithms for fundamental computational tasks, such as classical planning. Similarly, it is interesting (and likely very useful in the long term) to seek distributed versions of fundamental tools in computer science, and search is definitely such a tool. Moreover, there are pragmatic reasons for seeking distributed algorithms. As an example, imagine a setting in which different manufacturers or service providers can publish their capabilities and then collaborate with each other to provide new products or services that none of them can provide alone. Such providers will certainly need to reveal some sort of public interface, describing what they can contribute to others, as well as what they require from others. But most likely, they will not want to describe their inner workings: their internal state and how they can manipulate it (e.g., their current stock levels, machinery, logistics capabilities, personnel, other commitments, etc.). This is usually confidential proprietary information that an agent would not want to reveal, although clearly one must reason about it during the planing process.\nIn principle, the above problem can be addressed using a central trusted party running a suitable planning algorithm. However, such a trusted party may not exist in all settings. Moreover, centralized planning puts the entire computational burden on a single agent,\nar X\niv :1\n30 6.\n58 58\nv1 [\ncs .A\nI] 2\n5 Ju\nn 20\n13\nrather than distribute it across the system. Thus, centralized algorithms are less robust to agent failures, and sometimes less efficient. For these reasons, distributed algorithms are often sought, and in our case in particular, distributed, privacy preserving algorithms. Indeed, this is the main motivation for the field of distributed algorithms, and in particular, the work on distributed constraint satisfaction problems (CSP) (Yokoo, Durfee, Ishida, & Kuwabara, 1998; Meisels, 2007).\nYet another motivation for distributed algorithms is provided by planning domains in which search operators that correspond to actions are implemented using complex simulation software that is accessible to the relevant agent only because that agent is not interested in sharing it (due to privacy concerns or commercial interests) or because it is not realistic to transfer, implement, and appropriately execute such software as part of a planning algorithm. As an example, consider planning by a group of robotic agents, each with different capabilities. Each agent has a simulator that can compute the effect of its actions, which the agents do not want to share with each other. Thus, the application of each agent\u2019s actions during search can only be done by the agents themselves\nMoreover, it is often the case that good distributed algorithms formulated for a cooperative team provide the foundation for algorithms and mechanisms for solving similar problems for teams of self-interested agents. For example, the work on planning games (Brafman, Domshlak, Engel, & Tennenholtz, 2009, 2010) suggests modified versions of an earlier algorithm for cooperative multi-agent systems (Brafman & Domshlak, 2008) and work on mechanism design for solving distributed CSPs by self-interested agents (Petcu, Faltings, & Parkes, 2008) is based on earlier work in distributed CSPs for cooperative teams (Petcu & Faltings, 2005). Finally, work on distributed algorithms can lead to insights on problem factoring and abstraction, as we shall demonstrate later on in this paper \u2013 presenting a new effective pruning technique for centralized planning that is an outgrowth of our work on distributed search.\nThere is a long tradition of work on multi-agent planning for cooperative and noncooperative agent teams involving centralized and distributed algorithms, often using involved models that model uncertainty, resources, and more (Nilsson, 1980; Hansen & Zilberstein, 2001; Bernstein, Givan, Immerman, & Zilberstein, 2002; Szer, Charpillet, & Zilberstein, 2005), and much work on how to coordinate the local plans of agents or to allow agents to plan locally under certain constraints (Cox & Durfee, 2005; Steenhuisen, Witteveen, ter Mors, & Valk, 2006; ter Mors, Valk, & Witteveen, 2004; ter Mors & Witteveen, 2005). However, our starting point is a more basic, and hence, we believe, more fundamental model introduced by Brafman and Domshlak (BD) which offers what is possibly the simplest model of MA planning \u2013 ma-strips (Brafman & Domshlak, 2008). ma-strips minimally extends standard strips (or PDDL) models by specifying a set of agent ids, and associating each action in the domain with one of the agents. Thus, essentially, it partitions the set of actions among the set of agents. We believe that this model serves as a skeleton model for most work on multi-agent planning, and that the insights gained from it can help us address more involved models as well.\nDistributed planning can easily be performed in ma-strips using existing distributed planning algorithms (Vrakas, Refanidis, & Vlahavas, 2001; Kishimoto, Fukunaga, & Botea, 2009; Burns, Lemons, Ruml, & Zhou, 2010) applied to the underlying strips planning problem (i.e., where we ignore agent identities). However, these algorithms were devised\nto speed-up the solution of centralized planning problems given access to a distributed computing environment, such as a large cluster. However, these algorithms do not \u201drespect\u201d the inherent distributed form of the problem, giving all agents access to all actions and hence do not preserve privacy. Nor can they be used when search operators cannot be shared by agents, as in the scenario described earlier.\nRecently, a number of algorithms that maintain agent privacy and utilize the inherent distributed structure of the system have emerged. The most natural approach is based on distributed CSP techniques and was introduced in BD\u2019s original work. BD formulate a particular CSP that is particularly suited for ma-strips problems whose solution is a plan. This algorithm can be transformed into a fully distributed algorithm simply by using a distributed CSP solver. Unfortunately, distributed CSP solvers cannot handle even the smallest instances of MA planning problems. Consequently, a dedicated algorithm, based on the ideas of BD, Planning-First, was developed (Nissim, Brafman, & Domshlak, 2010). While performing well on some domains, this algorithm had trouble scaling up to problems in which each agent had to execute more than a small number of actions. (Indeed, BD\u2019s algorithm scales exponentially with the minimal number of actions per agent in the solution plan.) Recently, a new, improved algorithm, based on partial-order planning, map-pop, was developed by (Torren\u0303o, Onaindia, & Sapena, 2012). Yet, this algorithm, too, leaves a serious gap between what we can solve using a distributed planner and what can solved using a centralized planner. Moreover, neither algorithm attempts to generate a cost-optimal plan.\nIn single-agent planning, constraint-based and partial-order planning techniques are currently dominated by heuristic forward search techniques. Thus, it is natural to ask whether it is possible to formulate a distributed heuristic forward search algorithm for distributed planning. This paper provides a positive answer to this question in the form of a general approach to distributed search in which each agent performs only the part of the state expansion relevant to it. The resulting algorithms are very simple and very efficient \u2013 outperforming previous algorithms by orders of magnitude \u2013 and offer similar flexibility to that of forward-search based algorithms for single-agent planning. They respect the natural distributed structure of the system, and thus allow us to formulate privacy preserving versions.\nOne particular variant of our general approach yields a distributed version of the a* algorithm, called mad-a*, using which we obtain a general, efficient distributed algorithm for optimal planning. mad-a* solves a more difficult problem than centralized search because in the privacy preserving setting, each agent has less knowledge than a centralized solver. Yet, it is able to solve some problems centralized a* cannot solve. As we will show, the main reason for this speed-up is an interesting optimality preserving pruning technique that is naturally built into our search approach. This insight has led to to a new effective pruning technique for centralized search that we shall describe later on.\nThe rest of this paper is organized as follows. The next section presents the model we use and related work. Section 3 describes our MA forward search algorithm, and Section 4 describes mad-a*, a modified version which maintains optimality. We next present the MA planning framework ma-fd, and empirical results for both mafs and mad-a*. Section 7 shows how we can exploit insights from our distributed search methods to obtain a new effective pruning methods for centralized search. Section 8 concludes the paper with a discussion."}, {"heading": "2. Background", "text": "A ma-strips problem (Brafman & Domshlak, 2008) for a set of agents \u03a6 = {\u03d5i}ki=1, is given by a 4-tuple \u03a0 = \u3008P, {Ai}ki=1, I, G\u3009, where P is a finite set of propositions, I \u2286 P and G \u2286 P encode the initial state and goal, respectively, and for 1 \u2264 i \u2264 k, Ai is the set of actions agent \u03d5i is capable of performing. Each action a = \u3008pre(a), eff(a)\u3009 is given by its preconditions and effects. A plan is a solution to \u03a0 iff it is a solution to the underlying strips problem obtained by ignoring the identities of the agent associated with each action. Since each action is associated with an agent, a plan tells each agent what to do and when to do it. In different planning contexts, one might seek special types of solutions. For example, in the context of planning games (Brafman et al., 2009), stable solutions are sought. We focus on cooperative multi-agent systems, seeking either a (standard) solution or a cost-optimal solution.\nThe partitioning of the actions to agents yields a distinction between private and public propositions and actions. A private proposition of agent \u03d5 is required and affected only by the actions of \u03d5. An action is private if all its preconditions and effects are private. All other actions are classified as public. That is, \u03d5\u2019s private actions affect and are affected only by \u03d5\u2019s actions, while its public actions may require or affect the actions of other agents. For ease of the presentation of our algorithms and their proofs, we assume that all actions that achieve a goal condition are considered public. Our methods are easily modified to remove this assumption.\nWe note that while the notion of private/public is natural to the ma-strips encoding, it can easily be applied in models having multi-valued variables. For example, in SAS+, where each variable may have multiple values, the analogous of a (boolean) proposition in ma-strips is a \u3008variable, value\u3009 pair. Such a pair is considered private if it is required, achieved or destroyed only by the actions of a single agent. Consequently, actions which require, achieve or destroy only private \u3008variable, value\u3009 pairs are considered private. For clarity and consistency with previous work we use ma-strips notation when discussing the theoretical aspects of our work. However, the examples given, as well the practical framework we present for MA planning, use the more concise multi-valued variables SAS+ encoding.\nIn a distributed system of fully-cooperative agents privacy is not an issue, and so the distinction between private and public actions is not essential, although it can be exploited for computational gains (Brafman & Domshlak, 2008). However, there are settings in which agents collaborate on a specific task, but prefer not to reveal private information about their local states, their private actions, and the cost of these private actions. They wish only to make their public interface known \u2013 i.e., the public preconditions and effects of their actions.\nThis setting is the planning equivalent to the area of distributed CSPs, where agents must coordinate (e.g., schedule a meeting) while keeping certain constraints and private variables private. We will refer to algorithms that plan without revealing this information as privacy preserving (distributed) planning algorithms. More specifically, in a privacypreserving algorithm the only information available about an agent to others is its set of public actions, projected onto public propositions. This can be viewed as the interface between the agents. Information about an agent\u2019s private actions and private aspects of a public action are known to the agent only.\nGiven a model of a distributed system such as ma-strips, it is natural to ask how to search for a solution. The best known example of distributed search is that of distributed CSPs (Yokoo et al., 1998), and various search techniques and heuristics have been developed for it (Meisels, 2007). Planning problems can be cast as CSP problems (given some bound on the number of actions), and the first attempt to solve ma-strips problems was based on a reduction to distributed CSPs. More specifically, Brafman and Domshlak introduced the Planning as CSP+Planning methodology for planning by a system of cooperative agents with private information. This approach separates the public aspect of the problem, which involves finding public action sequences that satisfy a certain distributed CSP, from the private aspect, which ensures that each agent can actually execute these public actions in a sequence. Solutions found are locally optimal, in the sense that they minimize \u03b4, the maximal number of public actions performed by an agent. This methodology was later extended to the first fully distributed MA algorithm for ma-strips planning, PlanningFirst (Nissim et al., 2010). Planning First was shown to be efficient in solving problems where the agents are very loosely coupled, and where \u03b4 is very low. However, it does not scale up as \u03b4 rises, mostly due to the large search space of the distributed CSP. Recently, a distributed planner based on partial order planning was introduced (Torren\u0303o et al., 2012), which outperforms Planning First, effectively solving more tightly coupled problems. Both methods are privacy preserving, but do not guarantee cost-optimal solutions."}, {"heading": "3. Multi-Agent Forward Search", "text": "This section describes our distributed variant of forward best-first search, which we call mafs. We begin with the algorithm itself, including an overview and pseudo-code. We next provide an example of the flow of mafs, and a discussion of its finer points."}, {"heading": "3.1 The MAFS Algorithm", "text": "Algorithms 1-3 depict the mafs algorithm for agent \u03d5i. In mafs, a separate search space is maintained for each agent. Each agent maintains an open list of states that are candidates for expansion and a closed list of already expanded states. It expands the state with the minimal f value in its open list. When an agent expands state s, it uses its own operators only. This means two agents expanding the same state will generate different successor states.\nSince no agent expands all relevant search nodes, messages must be sent between agents, informing one agent of open search nodes relevant to it expanded by another agent. Agent \u03d5i characterizes state s as relevant to agent \u03d5j if \u03d5j has a public operator whose public preconditions (the preconditions \u03d5i is aware of) hold in s, and the creating action of s is public. In that case, Agent \u03d5i will send s to Agent \u03d5j .\nThe messages sent between agents contain the full state s, i.e. including both public and private variable values, as well as the cost of the best plan from the initial state to s found so far, and the sending agent\u2019s heuristic estimate of s. When agent \u03d5 receives a state via a message, it checks whether this state exists in its open or closed lists. If it does not appear in these lists, it is inserted into the open list. If a copy of this state with higher g value exists, its g value is updated, and if it is in the closed list, it is reopened. Otherwise, it is discarded. Whenever a received state is (re)inserted into the open list, the\nAlgorithm 1 mafs for agent \u03d5i 1: while did not receive true from a solution verification procedure do 2: for all messages m in message queue do 3: process-message(m) 4: s\u2190 extract-min(open list) 5: expand(s)\nAlgorithm 2 process-message(m = \u3008s, g\u03d5j (s), h\u03d5j (s)\u3009) 1: if s is not in open or closed list or g\u03d5i(s) > g\u03d5j (s) then 2: add s to open list and calculate h\u03d5i(s) 3: g\u03d5i(s)\u2190 g\u03d5j (s) 4: h\u03d5i(s)\u2190 max(h\u03d5i(s), h\u03d5j (s))\nAlgorithm 3 expand(s)\n1: move s to closed list 2: if s is a goal state then 3: broadcast s to all agents 4: initiate verification of s as a solution 5: return 6: for all agents \u03d5j \u2208 \u03a6 do 7: if the last action leading to s was public and \u03d5j has a public action for which all public preconditions hold in s then 8: send s to \u03d5j 9: apply \u03d5i\u2019s successor operator to s\n10: for all successors s\u2032 do 11: update g\u03d5i(s \u2032) and calculate h\u03d5i(s \u2032) 12: if s\u2032 is not in closed list or f\u03d5i(s \u2032) is now smaller than it was when s\u2032 was moved to closed list then 13: move s\u2032 to open list\nagent computes its local h value for this state, and then can choose between/combine the value it has calculated and the h value in the received message. If both heuristics are known to be admissible, for example, the agent could choose the maximal of the two estimates, as is done in Line 4 of Algorithm 2.\nOnce an agent expands a solution state s, it sends s to all agents and awaits their confirmation. For simplicity, and in order to avoid deadlock, once an agent either broadcasts or confirms a solution, it is not allowed to create new solutions. If a solution is found by more than one agent, the one with lower cost is chosen, and ties are broken by choosing the solution of the agent having the lower ID. When the solution is confirmed by all agents, the agent initiates the trace-back of the solution plan. This is also a distributed process, which involves all agents that perform some action in the optimal plan. The initiating agent begins the trace-back, and when arriving at a state received via a message, it sends a trace-back message to the sending agent. This continues until arriving at the initial state. When the trace-back phase is done, a terminating message is broadcasted and the solution is outputted.\nAs we will see, this general and simple scheme \u2013 apply your own actions/operators only and send relevant generated nodes to other agents \u2013 can be used to distribute other search algorithms. However, there are various subtle points pertaining to message sending and termination that influence the correctness and efficiency of the distributed algorithm, which we discuss later.\nTo better demonstrate the flow of the algorithm, consider the example given in Figure 1. In this example, we have two agents who must cooperate in order to achieve the goal. The agents\u2019 actions are described on the left-hand side, where every node in the graph depicts an action, and an edge (u, v) indicates that u either achieves or destroys a precondition of v. There are two public actions a5, a8, which affect/depend on the only public variable, v4, while the rest of the actions are private. In the initial state, all variable values are zero (i.e., I = 0000), and the goal is G = {v4 = 2}. When the agents begin searching, each applies its own actions only. Therefore, agent 2 quickly exhausts its search space, since as far as it\u2019s concerned, state 0020 is a dead end. Agent 1 generates its search space, until it applies public action a5, which results in state s = 2201. s is then sent to agent 2, since all the public preconditions of a8 hold in s (Line 7 of Algorithm 3). Upon receiving s, agent 2 continues applying its actions, eventually reaching the goal state, which is then broadcasted."}, {"heading": "3.2 Discussion", "text": "We now discuss some of the more subtle points of mafs."}, {"heading": "3.2.1 Preserving Agent Privacy", "text": "If our goal is to preserve privacy, it may appear that mafs agents are revealing their private data because they transmit their private state in their messages. Yet, in fact, this information is not used by any of the other agents, nor is it altered. It is simply copied to future states to be used only by the agent. Since private state data is used only as an ID, the agents can encrypt this data and keep a table locally, which maps IDs to private states. If this encryption can generate multiple possible IDs for each private state, other\nagents cannot identify other agents\u2019 private states.The issue of privacy is discussed further in Section 8.3.\nTo compute heuristic estimates of states it receives, an agent must assess the effort required to achieve the goal from them. To do this, it needs some information about the effort required of other agents to construct their part of the plan. In a fully cooperative setting, an agent can have access to the full description of other agents\u2019 actions. In the privacy preserving setting, two issues arise. First, agents have only partial information about other agents\u2019 capabilities \u2013 they only have access to their public interface. Second, different agents may compute different heuristic estimates of the same state because each agent has full information about its capabilities, but not about those of the others. This issue does not affect the actual algorithm, which is agnostic to how agents compute their heuristic estimate, although the fact that agents have less information can lead to poorer heuristic estimates. On the other hand, agents are free to use different heuristic functions, and as we will demonstrate empirically, using the public interfaces only, we are still able to efficiently solve planning problems.\nWhen a state is reached via a message, it includes the sending agent\u2019s heuristic estimate. Therefore, the receiving agent now has two (possibly different) estimates it can use. If the heuristics are known to be admissible, then clearly the maximal (most accurate) value is taken, as in line 4 of Algorithm 2. If not, the agent is free to decide how to use these estimates, depending on their known qualities."}, {"heading": "3.2.2 Relevancy and Timing of the Messages", "text": "State s is considered relevant to agent \u03d5j if it has a public action for which all public preconditions hold in s and the last action leading to s was public (line 7 of Algorithm 3). This means that all states that are products of private actions are considered irrelevant to other agents. As it turns out, since private actions do not affect other agent\u2019s capability to perform actions, an agent must send only states in which the last action performed was public, in order to maintain completeness (and optimality, as proved in the next section). Regarding states that are products of private actions as irrelevant decreases communication, while effectively pruning large, symmetrical parts of the search space. In fact, we will show in Section 7 how this property of mafs can be used to obtain state-space pruning in centralized planning algorithms, using a method called Partition-based pruning.\nAs was hinted earlier, there exists some flexibility regarding when these relevant states are sent. Centralized search can be viewed as essentially \u201csending\u201d every state (i.e., inserting it to its open list) once it is generated. In mafs, relevant states can be sent when they are expanded (as in the pseudo-code) or once they are generated (changing Algorithm 3 by moving the for-loop on line 6 inside the for-loop on line 10). The timing of the messages is especially important in the distributed setting since agents may have different heuristic estimations. Sending the messages once they are generated increases communication, but allows for states that are not considered promising by some agent to be expanded by another agent in an earlier stage. Sending relevant states when they are expanded, on the other hand, decreases communication, but delays the sending of states not viewed as promising. Experimenting with the two options, we found that the lazy approach, of sending the messages only when they are expanded, dominates the other, most likely because communication can be costly."}, {"heading": "3.2.3 Robustness", "text": "One of the main motivations for distributed problem solving is robustness. We discuss robustness with respect to agent failure, or the ability of the algorithm to handle the failure of one or more of the computing agents. In the centralized case, where only one computing agent exists, its failure means the inability to solve the problem. In the case of mafs, the algorithm can still find solutions even if a group of agents fails to perform their computation. To do this, the agents simply need to ignore all states in which the failing agent participated in the plan leading up to them. If this is done, the algorithm will find a solution excluding the failed agent.\nFor this to be done, the agents must additionally identify each state s with the set of agents participating in plans leading to up to s. If s can be reached via two paths having different participating agents sets, s is duplicated, in order to maintain completeness. If agent \u03d5 fails, all agents remove from their open lists all states in which \u03d5 is a participating agent, and ignore any such states arriving in future messages. This simple alteration of mafs guarantees that if a solution excluding the failed agent exists, it is found."}, {"heading": "3.2.4 Search Using Complex Actions", "text": "In Section 1, we mentioned a scenario where search operators corresponding to real-world actions are implemented using complex simulation software. This situation can arise, for\nexample, with a team of heterogeneous robotic agents, each of which has a dedicated simulator of its actions. There are natural cases in which such agents are unlikely to want to share such generative models. For example, imagine a robotic team where different robots are supplied by different manufacturers, e.g., some are autonomous heavy equipment, while others are humanoid robots, and yet others are drones. Simulation software for such robots is usually complex and proprietary. It is unlikely that agents would want to share it, although they most likely have no problem advertising their capabilities. Moreover, in the case of ad-hoc teams, transferring and installing such software is unlikely to work online. (Any person who attempted to install such simulators knows how sensitive they can be to the particular computing environment they run on).\nOur approach is well suited for such settings: First, forward search methods are capable of using generative, rather than declarative models of the agent\u2019s actions, as their central steps involves the generation of successor states and their insertion into appropriate queues. They are oblivious as to how the operators are described or implemented, as long as successor states can be generated. Second, our approach respects the natural system structure, and each agent need only apply its own operators. Thus, there is no need to share the generative models amongst the agents.\nOne problem, however, with generative operators is the fact that most contemporary methods for generating heuristic functions for generated states require either a declarative strips-like description or a generative model (in the case of sampling methods). Fortunately, our empirical results indicate that the use of an approximate model works quite well in practice. Indeed, our approach assumes that other agents use only the public part of an agent\u2019s action model, which is only an approximation. Even if the original action model is generative, a declarative approximate model can be constructed using learning techniques (Yang, Wu, & Jiang, 2007). Alternatively, sampling methods could use a suitably developed simplified simulator."}, {"heading": "4. Optimal MAFS", "text": "mafs as presented, is not an optimal planning algorithm. It can, however, be slightly modified in order to achieve optimality. We now describe these modifications, which result in a MA variation of a* we refer to as Multi-Agent Distributed a* (mad-a*).\nAs in a*, the state chosen for expansion by each agent must be the one with the lowest f = g + h value in its open list, where the heuristic estimates are admissible. In mad-a*, therefore, extract-min (Line 4 in Algorithm 1) must return this state."}, {"heading": "4.1 Termination Detection", "text": "Unlike in a*, expansion of a goal state in mafs does not necessarily mean an optimal solution has been found. In our case, a solution is known to be optimal only if all agents prove it so. Intuitively, a solution state s having solution cost f\u2217 is known to be optimal if there exists no state s\u2032 in the open list or the input channel of some agent, such that f(s\u2032) < f\u2217. In other words, solution state s is known to be optimal if f(s) \u2264 flower\u2212bound, where flower\u2212bound is a lower bound on the f -value of the entire system (which includes all states in all open lists, as well as states in messages that have not been processed, yet).\nTo detect this situation, we use Chandy and Lamport\u2019s snapshot algorithm (Chandy & Lamport, 1985), which enables a process to create an approximation of the global state of the system, without \u201cfreezing\u201d the distributed computation. Although there is no guarantee that the computed global state actually occurred, the approximation is good enough to determine whether a stable property currently holds in the system. A property of the system is stable if it is a global predicate which remains true once it becomes true. Specifically, properties of the form flower\u2212bound \u2265 c for some fixed value c, are stable when h is a globally consistent heuristic function. That is, when f values cannot decrease along a path. In our case, this path may involve a number of agents, each with its h values. If each of the local functions h\u03d5 are consistent, and agents apply the max operator when receiving a state via a message (known as pathmax ), this property holds1.\nWe note that for simplicity of the pseudo-code we omitted the detection of a situation where a goal state does not exist. This can be done by determining whether the stable property \u201cthere are no open states in the system\u201d holds, using the same snapshot algorithm."}, {"heading": "4.2 Proof of Optimality", "text": "We now prove the optimality of mad-a*. We must note that as it is presented, mada* maintains completeness (and optimality) only if all actions which achieve some goal condition are considered public. This property is assumed throughout this section, but the algorithm is easily modified to remove it. We begin by proving the following lemmas regarding the solution structure of a MA planning problem.\nLemma 1. Let P = (a1, a2 . . . , ak) be a legal plan for a MA planning problem \u03a0. Let ai, ai+1 be two consecutive actions taken in P by different agents, of which at least one is private. Then P \u2032 = (a1, . . . , ai+1, ai, . . . , ak) is a legal plan for \u03a0 and P (I) = P \u2032(I).\nProof. By definition of private and public actions, and because ai, ai+1 are actions belonging to different agents, varset(ai)\u2229varset(ai+1) = \u2205, where varset(a) is the set of variables which affect and are affected by a. Therefore, ai does not achieve any of ai+1\u2019s preconditions, and ai+1 does not destroy any of ai\u2019s preconditions. Therefore, if s is the state in which ai is executed in P , ai+1 is executable in s, ai is executable in ai+1(s), and ai(ai+1(s)) = ai+1(ai(s)). Therefore, P\n\u2032 = (a1, . . . , ai+1, ai, . . . , ak) is a legal plan for \u03a0. Since the suffix (ai+2, ai+3, . . . , ak) remains unchanged in P \u2032, P (I) = P \u2032(I), completing the proof.\nCorollary 1. For a MA planning problem \u03a0 for which an optimal plan P = (a1, a2, . . . , ak) exists, there exists an optimal plan P \u2032 = (a\u20321, a \u2032 2, . . . , a \u2032 k) for which the following restrictions apply:\n1. If ai is the first public action in P \u2032, then a1, . . . , ai belong to the same agent.\n2. For each pair of consecutive public actions ai, aj in P \u2032, all actions al, i < l \u2264 j belong\nto the same agent.\nProof. Using repeated application of Lemma 1, we can move any ordered sequence of private actions performed by agent \u03d5, so that it would be immediately before \u03d5\u2019s subsequent public\n1. Although recent work (Holte, 2010) shows that pathmax does not necessarily make a bona-fide consistent heuristic, pathmax does ensure that f -values along a path are non-decreasing.\naction and maintain legality of the plan. Since application of Lemma 1 does not change the cost of plan, the resulting plan is cost-optimal as well.\nNext, we prove the following lemma, which is a MA extension to a well known result for a*. In what follows, we have tacitly assumed a liveness property with the conditions that every sent message eventually arrives at its destination and that all agent operations take a finite amount of time. Also, for the clarity of the proof, we assume the atomicity of the expand and process-message procedures.\nLemma 2. For any non-closed node s and for any optimal path P from I to s which follows the restrictions of Lemma 1, there exists an agent \u03d5 which either has an open node s\u2032 or has an incoming message containing s\u2032, such that s\u2032 is on P and g\u03d5(s \u2032) = g\u2217(s\u2032) .\nProof. : Let P = (I = n0, n1, . . . , nk = s). If I is in the open list of some agent \u03d5 (\u03d5 did not finish the algorithm\u2019s first iteration), let s\u2032 = I and the lemma is trivially true since g\u03d5(I) = g\n\u2217(I) = 0. Suppose I is closed for all agents. Let \u2206 be the set of all nodes ni in P that are closed by some agent \u03d5, such that g\u03d5(ni) = g\n\u2217(ni). \u2206 is not empty, since by assumption, I \u2208 \u2206. Let nj be the element of \u2206 with the highest index, closed by agent \u03d5. Clearly, nj 6= s since s is non-closed. Let a be the action causing the transition nj \u2192 nj+1 in P . Therefore, g\u2217(nj+1) = g\u03d5(nj) + cost(a).\nIf \u03d5 is the agent performing a, then nj+1 is generated and moved to \u03d5\u2019s open list in lines 9-13 of Algorithm 3, where g\u03d5(nj+1) is assigned the value g\u03d5(nj) + cost(a) = g\n\u2217(nj+1) and the claim holds.\nOtherwise, a is performed by agent \u03d5\u2032 6= \u03d5. If a is a public action, then all its preconditions hold in nj , and therefore nj is sent to \u03d5\n\u2032 by \u03d5 in line 8 in Algorithm 3. If a is a private action, by the definition of P , the next public action a\u2032 in P is performed by \u03d5\u2032. Since private actions do not change the values of public variables, the public preconditions of a\u2032 must hold in nj , and therefore nj is sent to \u03d5\n\u2032 by \u03d5 in line 8 in Algorithm 3. Now, if the message containing nj has been processed by \u03d5\n\u2032, nj has been added to the open list of \u03d5\u2032 in Algorithm 2 and the claim holds since g\u03d5\u2032(nj) = g\u03d5(nj) = g\n\u2217(nj). Otherwise, \u03d5\u2032 has an incoming (unprocessed) message containing nj and the claim holds since g\u03d5(nj) = g \u2217(nj).\nCorollary 2. Suppose h\u03d5 is admissible for every \u03d5 \u2208 \u03a6, and suppose the algorithm has not terminated. Then, for any optimal solution path P which follows the restrictions of Lemma 1 from I to any goal node s?, there exists an agent \u03d5i which either has an open node s or has an incoming message containing s, such that s is on P and f\u03d5i(s) \u2264 h\u2217(I).\nProof. : By Lemma 2, for every restricted optimal path P , there exists an agent \u03d5i which either has an open node s or has an incoming message containing s, such that s is on P and g\u03d5i(s) = g\n\u2217(s) . By the definition of f , and since h\u03d5i is admissible, we have in both cases:\nf\u03d5i(s) = g\u03d5i(s) + h\u03d5i(s) = g \u2217(s) + h\u03d5i(s)\n\u2264 g\u2217(s) + h\u2217(s) = f\u2217(s)\nBut since P is an optimal path, f\u2217(n) = h\u2217(I), for all n \u2208 P , which completes the proof.\nAnother lemma must be proved regarding the solution verification process. We assume global consistency of all heuristic functions, since all admissible heuristics can be made consistent by locally using the pathmax equation (Me\u0301ro, 1984), and by using the max operator as in line 4 of Algorithm 2 on heuristic values of different agents. This is required since flower\u2212bound must be non-decreasing.\nLemma 3. Let \u03d5 be an agent which either has an open node s or has an incoming message containing s. Then, the solution verification procedure for state s\u2217 with f(s\u2217) > f\u03d5(s) will return false.\nProof. Let \u03d5 be an agent which either has an open node s or has an incoming message containing s, such that f\u03d5(s) < f(s\n\u2217) for some solution node s\u2217. The solution verification procedure for state s\u2217 verifies the stable property p = \u201cf(s\u2217) \u2264 flower\u2212bound\u201d. Since flower\u2212bound represents the lowest f -value of any open or unprocessed state in the system, we have flower\u2212bound \u2264 f\u03d5(s) < f(s\u2217), contradicting p. Relying on the correctness of the snapshot algorithm, this means that the solution verification procedure will return false, proving the claim.\nWe can now prove the optimality of our algorithm.\nTheorem 1. mad-a* terminates by finding a cost-optimal path to a goal node, if one exists.\nProof. : We prove this theorem by assuming the contrary - the algorithm does not terminate by finding a cost-optimal path to a goal node. 3 cases are to be considered:\n1. The algorithm terminates at a non-goal node. This contradicts the termination condition, since solution verification is initiated only when a goal state is expanded.\n2. The algorithm does not terminate. Since we are dealing with a finite search space, let \u03c7(\u03a0) denote the number of possible non-goal states. Since there are only a finite number of paths from I to any node s in the search space, s can be reopened a finite number of times. Let \u03c1(\u03a0) be the maximum number of times any non-goal node s can be reopened by any agent. Let t be the time point when all non-goal nodes s with f\u03d5(s) < h\n\u2217(I) have been closed forever by all agents \u03d5. This t exists since a) we assume liveness of message passing and agent computations; b) after at most \u03c7(\u03a0)\u00d7\u03c1(\u03a0) expansions of non-goal nodes by \u03d5, all non-goal nodes of the search space must be closed forever by \u03d5; and c) no goal node s\u2217 with f(s\u2217) < h\u2217(I) exists2.\nBy Corollary 2 and since an optimal path from I to some goal state s\u2217 exists, some agent \u03d5 expanded state s\u2217 at time t\u2032, such that f\u03d5(s\n\u2217) \u2264 h\u2217(I). Since s\u2217 is an optimal solution, if t\u2032 \u2265 t, flower\u2212bound \u2265 f\u03d5(s\u2217) at time t\u2032. Therefore, \u03d5\u2019s verification procedure of s\u2217 will return true, and the algorithm terminates.\nOtherwise, t\u2032 < t. Let \u03d5\u2032 be the last agent to close a non-goal state s with f\u03d5\u2032(s) < f\u03d5(s \u2217). \u03d5\u2032 has s\u2217 in its open list or as an incoming message. This is true because s\u2217 has been broad-casted to all agents by \u03d5, and because every time s\u2217 is closed by some agent (when it expands it), it is immediately broad-casted again, ending up in the\n2. This is needed since goal node expansions are not bounded.\nagent\u2019s open list or in its message queue. Now, \u03d5\u2032 has no more open nodes with f - value lower than s\u2217, so it will eventually expand s\u2217, initiating the solution verification procedure which will return true, since flower\u2212bound \u2265 f\u03d5(s\u2217). This contradicts the assumption of non-termination.\n3. The algorithm terminates at a goal node without achieving optimal cost. Suppose the algorithm terminates at some goal node s with f(s) > h\u2217(I). By Corollary 2, there existed just before termination an agent \u03d5 having an open node s\u2032, or having an incoming message containing s\u2032, such that s\u2032 is on an optimal path and f\u03d5(s\n\u2032) \u2264 h\u2217(I). Therefore, by Lemma 3, the solution verification procedure for state s will return false, contradicting the assumption that the algorithm terminated.\nThis concludes the proof."}, {"heading": "5. MA Planning Framework", "text": "One of the main goals of this work is to provide a general and scalable framework for solving the MA planning problem. We believe that such a framework will provide researchers with fertile ground for developing new search techniques and heuristics for MA planning, and extensions to richer planning formalisms.\nWe chose Fast Downward (Helmert, 2006) (FD) as the basis for our MA framework \u2013 MA-FD. FD is currently the leading framework for planning, both in the number of algorithms and heuristics that it provides, and in terms of performance \u2013 winners of the past three international planning competitions were implemented on top of it. FD is also well documented and supported, so implementing and testing new ideas is relatively easy.\nma-fd uses FD\u2019s translator and preprocessor, with minor changes to support the distribution of operators to agents. In addition to the PDDL files describing the domain and the problem instance, ma-fd receives a file detailing the number of agents, their names, and their IP addresses. The agents do not have shared memory, and all information is relayed between agents using messages. Inter-agent communication is performed using the TCP/IP protocol, which enables running multiple ma-fd agents as processes on multi-core systems, networked computers/robots, or even the cloud. ma-fd is therefore fit to run as is on any number of (networked) processors, in both its optimal and satisficing setting.\nBoth settings are currently implemented and available3, and since there is full flexibility regarding the heuristics used by agents, all heuristics available on FD are also available on ma-fd. New heuristics are easily implementable, as in FD, and creating new search algorithms can also be done with minimal effort, since ma-fd provides the ground-work (parsing, communication, etc.)."}, {"heading": "6. Empirical Results", "text": "To evaluate mafs in its non-optimal setting, we compare it to the state-of-the-art distributed planner map-pop (Torren\u0303o et al., 2012), and to the Planning-First algorithm (Nissim et al., 2010). As noted in the Introduction, another available algorithm for distributed MA planning is via reduction to distributed CSPs using an off-the-shelf dis-CSP solver. We found\n3. The code is available at https://github.com/raznis/dist-selfish-fd .\nthis approach was incapable of solving even small planning problems, and therefore we omitted its results from the tables. The problems used are benchmarks from the International Planning Competition (IPC) where tasks can naturally be cast as MA problems. The Satellites and Rovers domains where motivated by real MA applications used by NASA. Satellites requires planning and scheduling observation tasks between multiple satellites, each equipped with different imaging tools. Rovers involves multiple rovers navigating a planetary surface, finding samples and communicating them back to a Lander. Logistics, Transport and Zenotravel are transportation domains, where multiple vehicles transport packages to their destination. The Transport domain generalizes Logistics, adding a capacity to each vehicle (i.e., a limit on the number of packages it may carry) and different move action costs depending on road length. We consider problems from the Rovers and Satellites domains as loosely-coupled, i.e., problems where agents have many private actions (e.g., instrument warm-up and placement in Rovers, which does not affect other agents). On the other hand, we consider the transportation domains as tightly-coupled, having few private actions (only move actions in Logistics) and many public actions (all load/unload actions).\nFor each planning problem, we ran mafs, using eager best-first search and an alternation open list with one queue for each of the two heuristic functions ff (Hoffmann & Nebel, 2001) and the context-enhanced additive heuristic (Helmert & Geffner, 2008). Table 1 depicts results of mafs, map-pop and Planning-First, on all IPC domains supported by map-pop. We compare the algorithms across three categories \u2013 1) solution qualit,y which reports the cost of the outputted plan, 2) running time, and 3) the number of messages sent during the planning process. Experiments were run on a AMD Phenom 9550 2.2GHZ processor, time limit was set at 60 minutes, and memory usage was limited to 4GB. An \u201cX\u201d signifies that the problem was not solved within the time-limit, or exceeded memory constraints.\nIt is clear that mafs overwhelmingly dominates both map-pop and Planning-First (denoted p-f), with respect to running time and communication, solving all problems faster while sending less messages. All problems were solved at least 70X faster than map-pop, with several Logistics and Rovers problems being solved over 1000X faster, and the largest Satellites instance being solved over 400X faster. The low communication complexity of mafs is important, since in distributed systems message passing could be more costly and time-consuming than local computation. Moreover, as messages in mafs are essentially a state description, message size is linear in the number of propositions. Although for some problems map-pop finds lower-cost solutions, in most cases mafs outputs better solution quality. We believe that when mafs finds lower quality solutions, this is mostly because message-passing takes longer than local computation \u2013 a subset of agents that have the ability to achieve the goal \u201con their own\u201d, will do so before being made aware of other, less costly solutions including other agents. One possible way of improving solution quality further would be using anytime search methods, which improve solution quality over time.\nTo evaluate mad-a* with respect to centralized optimal search (a*), we ran both algorithms using the state-of-the-art Merge&Shrink heuristic4 (Helmert, Haslum, & Hoffmann, 2007). Both configurations were run on the same multi-core machine, where for mad-a*, each agent was allocated a single processor, and a* was run on a single processor. Time\n4. We used exact bisimulation with abstraction size limit 10K (DFP-bop) as the shrink strategy of Merge&Shrink (Nissim, Hoffmann, & Helmert, 2011).\nlimit was set at 30 minutes, and memory usage was limited to 4GB, regardless of the number of cores used. Table 2 depicts the runtime, efficiency (speedup divided by the number of processors), number of expanded nodes and the average of the agents\u2019 initial state hvalues. When comparing mad-a* to centralized a*, our intuition is that efficiency will be\nlow, due to the inaccuracy of the agents\u2019 heuristic estimates, and the overhead incurred by communication. In fact, the local estimates of the agents are much less accurate than those of the global heuristic, as is apparent from the lower average h values of the initial state, given as approximate measures of heuristic quality. In the tightly-coupled domains \u2013 Logistics, Transport and Zenotravel, we do notice very low efficiency values, mostly due to the large number of public actions, which result in many messages being passed between agents. However, in the more loosely-coupled domains Satellites and Rovers, mad-a* exhibits nearly linear and super-linear speedup, solving 2 problems not solved by centralized a*. We elaborate on this important issue in the next section."}, {"heading": "7. Partition-Based Path Pruning", "text": "The empirical results presented in Table 2 raise an interesting question: how does mada* achieve > 1 efficiency in weakly coupled environments? It is known that when using a consistent heuristic, a* is optimal in the number of nodes it expands to recognize an optimal solution. In principle, it appears that mad-a* should expand at least the same search tree, so it is not clear, a-priori, why we reach super-linear speedup when comparing to a*. The main reason for this is mad-a*\u2019s inherent exploitation of symmetry, resulting in the pruning of effect-equivalent paths.\nSymmetry exploitation utilizes the notion of public and private actions. As we noted in Corollary 1, the existence of private actions implies the existence of multiple effectequivalent permutations of certain action sequences. a* does not recognize or exploit this fact, and mafs does. Specifically, imagine that agent \u03d5i just generated state s using one of its public actions, and s satisfies the preconditions of some public action a of agent \u03d5j . Agent \u03d5i will eventually send s to agent \u03d5j , and the latter will eventually apply a to it. Now, imagine that agent \u03d5i has a private action a\n\u2032 applicable at state s, resulting in the state s\u2032 = a\u2032(s). Because a\u2032 is private to \u03d5i, from the fact that a is applicable at s we deduce that a is applicable at s\u2032 as well. Hence, a* would apply a at s\u2032. However, in mafs, agent \u03d5j would not apply a at s\n\u2032 because it will not receive s\u2032 from agent \u03d5i. Thus, mafs does not explore all possible action sequences. This fact can also be clearly seen in the example given in Figure 1 \u2013 The reachable search space in this example has 31 states, while the number of reachable states using mafs is only 16.\nSince mafs\u2019s inherent pruning of action sequences requires only a partitioning of the actions, it does not pertain only to MA systems, but to any factored system having internal operators. Since the only difference between a ma-strips planning problem and a strips one is the fact that actions are partitioned between agents, why not re-factor the centralized problem into an \u201cartificial\u201d MA one? By mapping all actions into disjoint sets such that\u22c3k\ni Ai = A, each representing an \u201cagent\u201d, we can now distinguish between private and public operators. Given this distinction, the pruning rule used is simple:\nPartition-Based (PB) Pruning Rule: Following a private action a \u2208 Ai, prune all actions not in Ai.\nThe fact that this pruning rule is optimality-preserving (i.e., does not prune all optimal solutions) follows immediately from Corollary 1, as if there exists an optimal solution \u03c0?, it can be permuted into a legal, optimal plan which is not pruned. This, however, is not enough to maintain the optimality of a* search. We now present a slight modification of the a* algorithm, which allows the application of optimality preserving pruning methods (such as PB-pruning) for the purpose of optimal planning."}, {"heading": "7.1 Path Pruning A*", "text": "The path pruning a*, (denoted pp-a*), is a search algorithm which receives a planning problem \u03a0 and a pruning method \u03c1 as input, and produces a plan \u03c0, which is guaranteed to be optimal provided that \u03c1 respects the following properties: (i) \u03c1 is optimality preserving, and (ii) \u03c1 prunes only according to the last action. It is easy to see, for example, that PB pruning respects the second condition, since it fires only according to the last action."}, {"heading": "7.1.1 pp-a* versus a*", "text": "pp-a* is identical to a* except for the following three changes. First, a different data-type is used for recording an open node. In pp-a*, an open list node is a pair (A, s), where s is the state and A is a set of actions, recording various possible ways to reach s from a previous state. Second, node expansion is subject to the pruning rules of method \u03c1. Namely, ppa* executes an applicable action a\u2032 in state (A, s) only if there is at least one action a \u2208 A s.t. the execution of a\u2032 is allowed after a under \u03c1\u2019s pruning rules. Third, duplicate states are handled differently. In a*, when a state s which is already open is reached by another search path, the open list node is updated with the action of the lower g value, and in case of a tie \u2013 drops the competing path. In contrast, ties in pp-a* are handled by preserving the last actions which led to s in each of the paths. Hence, if action a led to an open state s via a path of cost g, and if the existing open list node (A, s) has the same g value, then the node is updated to (A\u222a{a}, s), thus all actions leading to s with path cost g are saved. Tie breaking also affects the criterion under which closed nodes are reopened. In a*, nodes are reopened only when reached via paths of lower g values. In pp-a*, if an action a leading to state s of some closed node (A, s) is not contained in A, and if the g values are equal, then the node reopens as ({A\u222a{a}}, s). However, when the node is expanded, only actions that are now allowed by \u03c1 and were previously pruned, are executed. We now move to prove the correctness of pp-a*."}, {"heading": "7.1.2 Proof of Correctness and Optimality", "text": "The next lemma refers to pp-a*, and assumes \u03c1 to be an optimality preserving pruning method, which prunes according to the last action. We say that node (A, s) is optimal on path P , if A contains an action a which leads to state s on path P , and g(s) = g\u2217(s). The notation s \u227aP s\u2032 denotes the fact that state s precedes state s\u2032 in optimal path P .\nLemma 4. In pp-a*, for any non-closed state sk and for any optimal non-\u03c1-pruned path P from I to sk, there exists an open list node (A \u2032, s\u2032) which is optimal on P .\nProof. Let P be an optimal non-\u03c1-pruned path from I to sk. If I is in the open list, let s\u2032 = I and the lemma is trivially true since g(I) = g\u2217(I) = 0. Suppose I is closed. Let \u2206 be the set of all nodes (Ai, si) optimal on P , that were closed. \u2206 is not empty, since by assumption, I is in \u2206. Let the nodes in \u2206 be ordered such that si \u227aP sj for i < j, and let j be the highest index of any si in \u2206.\nSince the closed node (Aj , sj) has an optimal g value, it had been expanded prior to closing. From the properties of pp-a*, it follows that the expansion of (Aj , sj), which is optimal on P , is followed with an attempt to generate a node (Aj+1, sj+1) which is optimal on P as well. Generation of (Aj+1, sj+1) must be allowed, since under the highest index assumption there can be no closed node containing s which is optimal on P . Naturally, sj \u227aP sj+1.\nAt this point, we note that actions in Aj+1 cannot be removed by any competing path from I to sj+1, since (Aj+1, sj+1) has an optimal g value. It is possible, though, that additional actions leading to sj+1 are added to the node. The updated node can be represented by (A\u2032j+1 \u2287 Aj+1, sj+1), and the property of optimality on P holds. Additionally, node (A\u2032j+1, sj+1) cannot be closed after its generation, since again, this contradicts the highest\nindex property. Hence, there exists an open list node (A\u2032, s\u2032) which is optimal on P . This concludes the proof.\nCorollary 3. If h is admissible and \u03c1 is optimality-preserving, pp-a* using \u03c1 is optimal.\nProof. This follows directly from Lemma 4, the optimality preserving property of \u03c1 and the properties of pp-a*, which allow every optimal, non-\u03c1-pruned path to be generated."}, {"heading": "7.2 Empirical Analysis of PB-Pruning", "text": "We set out to check the effect of mad-a*\u2019s inherent exploitation of symmetry on its efficiency compared to a*. The hypothesis that this is mad-a*\u2019s main advantage over a* is well\nsupported by the results in Table 3, which shows a comparison of mad-a* and centralized a* using PB pruning. Here, we see that in all problems where mad-a* achieves superlinear speedup w.r.t. a*, applying partition-based pruning where partition=agent reduces runtime and expansions dramatically. In all cases, mad-a*\u2019s efficiency w.r.t. a* using PB pruning is sublinear. This is, of course, also due to the fact that mad-a* solves a more difficult problem \u2013 having incomplete information has a negative effect on the quality of heuristics computed by the agents.\nFinally, we note that although MA structure is evident in some benchmark planning domains (e.g. Logistics, Rovers, Satellites, Zenotravel etc.), in general there isn\u2019t always an obvious way of decomposing the problem. In work further exploring PB pruning (Nissim, Apsel, & Brafman, 2012), we describe an automated method for decomposing a general planning problem, making PB pruning applicable in the general setting."}, {"heading": "8. Discussion", "text": "We presented a formulation of heuristic forward search for distributed systems that respects the natural distributed structure of the system. mafs dominates the state-of-the-art w.r.t. runtime and communication, as well as solution quality in most cases. In this class of privacy preserving algorithms, mad-a*, is the first cost-optimal distributed planning algorithm, and it is competitive with its centralized counterpart, despite having partial information. Our work raises a number of research challenges and opportunities, which we now discuss."}, {"heading": "8.1 Accurate Heuristics Given Incomplete Information", "text": "The empirical results presented lead us to what is perhaps the greatest practical challenge suggested by mafs and mad-a* \u2013 computing an accurate heuristic in a distributed (privacypreserving) system. In some domains, the existence of private information that is not shared leads to serious deterioration in the quality of the heuristic function, greatly increasing the number of nodes expanded, and/or affecting solution quality. We believe that there are techniques that can be used to alleviate this problem. As a simple example, consider a public action apub that can be applied only after a private action apriv. For example, in the rover domain, a send message can only be applied after various private actions required to collect data are executed. If the cost of apub known to other agents would reflect the cost of apriv as well, the heuristic estimates would be more accurate. Another possibility for improving heuristic estimates is using an additive heuristic. In that case, rather than taking the maximum of the agent\u2019s own heuristic estimate and the estimate of the sending agent, the two could be added. To maintain admissibility, this would require using something like cost partitioning (Katz & Domshlak, 2008). One obvious way of doing this would be to give each agent the full cost of its actions and zero cost for other actions. The problem with this approach is that initially, when the state is generated and the only estimate available is that of the generating agent, this estimate is very inaccurate, since it assigns 0 to all other actions. In fact, the agent will be inclined to prefer actions performed by other agents, as they appear very cheap, and we see especially poor results in domains where different agents can achieve the same goal, as in the Rovers domain, resulting in estimates of 0 for many non-goal states. Therefore, how to effectively compute accurate heuristics in the distributed setting remains an open challenge."}, {"heading": "8.2 Secure Multi-Party Computation", "text": "Secure Multi-Party Computation (Yao, 1982, 1986) is a subfield of Cryptography which relates closely to distributed planning, as well as to distributed problem solving in general. The goal of methods for secure multi-party computation is to enable multiple agents to compute a function over their inputs, while keeping these inputs private. More specifically, agents \u03d51, . . . , \u03d5n, having private data x1, . . . , xn, would like to jointly compute some function f(x1, . . . , xn), without revealing any information about their private information, other than what can be reasonably deduced from the value of f(x1, . . . , xn).\nWhile in principle, it appears that these techniques can be extended to our setting of distributed planning, their complexity quickly becomes unmanageable. For example, a common approach for secure multiparty computation uses cryptographic circuits. When solving the shortest path problem (e.g., network routing, Gupta et al., 2012), the size of the circuits created is polynomial in the size of the graph. In our setting the function f computes a shortest path in the implicit graph induced by the descriptions of the agents\u2019 actions. As this graph is exponential in the problem description size, it quickly becomes infeasible to construct these circuits given time and memory limitations. While it is true that planning is NP-hard and forward search algorithms do, in general, require exponential time/memory, the purpose of heuristic search is to reduce the search space and to solve large problems in low-polynomial time. Requiring the construction of exponential-sized circuits a-priori contradicts the goal of efficiency and feasibility. Another difference between our model and the ones used for secure multiparty computation, is that these methods assume that some (\u2265 1) of the agents are honest, and the other agents are adversaries which are determined to uncover the private information. In distributed planning, the distinction between honest agents and adversaries is not as clear-cut. Despite faithfully participating in the distributed protocol, all agents might benefit from discovering other agents\u2019 private information (e.g., competing companies or contractors), and therefore can all be viewed as adversaries where privacy is concerned. Therefore, the assumptions usually made for secure multiparty computation regarding the limited number of adversaries do not fit our models as well."}, {"heading": "8.3 Privacy", "text": "Work in distributed CSPs (Yokoo, Suzuki, & Hirayama, 2002; Silaghi & Mitra, 2004) identified that although a key motivation for distributed computation is preservation of agent privacy, some private information may leak during the search process. For example, in DisCSP each agent has a single variable, and there exist both binary and unary constraints. Binary constraints are public since more than one agent knows of their existence, while unary constraints are considered private information. In meeting scheduling, an agent has a single variable whose values are possible meeting time slots. A binary constraint could be an equality constraint between the values of two variables belonging to different agents, while a unary constraint represents slots in which the agent cannot hold meetings. During search, whenever an agent sends some assignment of its variable to other agents, they can deduce that that value has no unary constraint forbidding it. If this value does not end up being assigned in the solution, the agent revealed some private information that could not have been deduced from only viewing the solution. In the field of DisCSPs, there has been\nwork focusing of how to measure this privacy loss (Franzin, Rossi, Freuder, & Wallace, 2004; Maheswaran, Pearce, Bowring, Varakantham, & Tambe, 2006), as well as work on analyzing how much information specific algorithms lose (Greenstadt, Pearce, & Tambe, 2006). More recently, further work has emerged on how to alter existing DisCSP algorithms to handle stricter privacy demands (Greenstadt, Grosz, & Smith, 2007; Le\u0301aute\u0301 & Faltings, 2009).\nIn our model of distributed planning, things are a bit different. To consider privacy loss during search, first we must examine what type of information could leak during distributed search. Our model considers the private preconditions of public actions, private actions, and private action costs as private information which the agents do not want to disclose.\nWe begin by discussing the cost of private actions. When running mad-a*, messages sent by the agents contain g-values, or the currently minimal cost of arriving at a state. Given this information throughout the search procedure, the agents can deduce an upper bound on the minimal cost of applying public action a, given a public state s. Consider an example of a system consisting of two agents \u03d51,2. During the search procedure, \u03d51 sends public state s to \u03d52 multiple times, each with different private states, which are indistinguishable to \u03d52, by applying methods discussed in Section 3.2.1. Upon receiving s, \u03d52 continues searching until applying public action a, and then sending the resulting state s\u2032 back to \u03d51, which can now compute g(s\n\u2032)\u2212 g(s), or the total (including private) cost of applying a. If \u03d51 minimizes this value for every s\n\u2032, it can now deduce an upper bound on the minimal cost of applying a given public state s.\nAnother possible leak of information can be the existence of private actions or private preconditions of public actions. These affect whether or not a public action can be applied at certain states. When running mafs, the first bit of information which can easily be deduced is whether a public action is applicable in some reachable state. Clearly, if an agent sends a state for which the creating operator is public action a, then other agents now know that there exist some reachable state in which a is applicable. However, this information is apparent from the public description of public actions, and hence is not private.\nHowever, there exists a potentially more serious leak of information. Given all the knowledge accumulated during the search process, agents can attempt to recreate some model of other agent\u2019s private states, and the possible transitions between these private states. For example, given every public state, agents can see which actions were applicable and which actions where not applicable. If the actions are different, agents can deduce that the states are different. This information can later be used in order to reconstruct a model of the agent\u2019s private state using techniques for learning with hidden values or techniques for learning hidden states. Of course, there is no guarantee that this information will be correct or useful, obtaining it requires collaboration between different agents (that need to share which public states they received from the agent), and algorithms for learning in the context of hidden variables/states can be weak. Nevertheless, clearly some information could leak.\nThe discussion above indicates that careful investigation of information leaks and development of algorithms that have better privacy guarantees is an important avenue for future research. First, it would be interesting to see work that empirically investigates the significance of privacy loss. For example, our empirical results indicate that many problems\ncan be solved quickly using distributed forward search, without expanding too many nodes. Is it possible to build reasonable models of agent\u2019s private states in such cases?\nSecond, one can develop variants of current algorithms that have stronger privacy preserving properties. For example, consider the problem of inferring upper bounds on the cost the minimal cost of applying action a in public state s. In general, private actions which achieve preconditions of a public action do not have to be applied immediately before that public action \u2013 an agent can perform some of the private actions required for a public action before a previous public action. In other words, an agent can \u201cdistribute\u201d the private cost of a public action between different \u201csegments\u201d, or parts of the plan between two public actions, making the cost of the first action appear higher and the cost of the second action lower, although with some potential impact on optimality. In the case of non-optimal search, g-values are not disclosed, so this is not an issue.\nThe above example illustrates a general idea: one can trade-off efficiency for privacy. A similar tradeoff is explored in the area of differential privacy (Dwork, 2006). There, some noise is inserted into a database before statistical queries are evaluated, such that the answer to the statistical query is correct to within some given tolerance, , yet one cannot infer information about a particular entry in the database (e.g., describing the medical record of an individual). Similarly, in our context, one can consider algorithms in which agents refrain from sending certain public states with some probability, or send it with some random delay, or even possibly, generate bogus, intermediate public states. Such changes are likely to have some impact on running time and solution quality, and these tradeoffs would be interesting to explore.\nWe believe that as this area matures, much like in the area of DisCSP, more attention will be given to the problem of precise quantification of privacy and privacy loss. Our work brings us closer to this stage. It offers algorithms for distributed search that start to match that of centralized search, and perhaps more importantly, a general methodology for distributed forward search that respects the natural distributed structure of the system, that can form a basis for such extensions."}], "references": [{"title": "The complexity of decentralized control of markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Math. Oper. Res.,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "From one to many: Planning for loosely coupled multi-agent systems", "author": ["R.I. Brafman", "C. Domshlak"], "venue": "In ICAPS,", "citeRegEx": "Brafman and Domshlak,? \\Q2008\\E", "shortCiteRegEx": "Brafman and Domshlak", "year": 2008}, {"title": "Transferable utility planning games", "author": ["R.I. Brafman", "C. Domshlak", "Y. Engel", "M. Tennenholtz"], "venue": null, "citeRegEx": "Brafman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2010}, {"title": "Best-first heuristic search for multicore machines", "author": ["E. Burns", "S. Lemons", "W. Ruml", "R. Zhou"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Burns et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Burns et al\\.", "year": 2010}, {"title": "Distributed snapshots: Determining global states of distributed systems", "author": ["K.M. Chandy", "L. Lamport"], "venue": "ACM Trans. Comput. Syst.,", "citeRegEx": "Chandy and Lamport,? \\Q1985\\E", "shortCiteRegEx": "Chandy and Lamport", "year": 1985}, {"title": "An efficient algorithm for multiagent plan coordination", "author": ["J.S. Cox", "E.H. Durfee"], "venue": "In AAMAS,", "citeRegEx": "Cox and Durfee,? \\Q2005\\E", "shortCiteRegEx": "Cox and Durfee", "year": 2005}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "In ICALP", "citeRegEx": "Dwork,? \\Q2006\\E", "shortCiteRegEx": "Dwork", "year": 2006}, {"title": "Multi-agent constraint systems with preferences: Efficiency, solution quality, and privacy loss", "author": ["M.S. Franzin", "F. Rossi", "E.C. Freuder", "R.J. Wallace"], "venue": "Computational Intelligence,", "citeRegEx": "Franzin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Franzin et al\\.", "year": 2004}, {"title": "Ssdpop: improving the privacy of dcop with secret sharing", "author": ["R. Greenstadt", "B.J. Grosz", "M.D. Smith"], "venue": "In AAMAS,", "citeRegEx": "Greenstadt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Greenstadt et al\\.", "year": 2007}, {"title": "Analysis of privacy loss in distributed constraint optimization", "author": ["R. Greenstadt", "J.P. Pearce", "M. Tambe"], "venue": "In AAAI,", "citeRegEx": "Greenstadt et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Greenstadt et al\\.", "year": 2006}, {"title": "A new approach to interdomain routing based on secure multiparty computation", "author": ["D. Gupta", "A. Segal", "A. Panda", "G. Segev", "M. Schapira", "J. Feigenbaum", "J. Rexford", "S. Shenker"], "venue": "In HotNets,", "citeRegEx": "Gupta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2012}, {"title": "The fast downward planning system", "author": ["M. Helmert"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Helmert,? \\Q2006\\E", "shortCiteRegEx": "Helmert", "year": 2006}, {"title": "Unifying the causal graph and additive heuristics", "author": ["M. Helmert", "H. Geffner"], "venue": "In ICAPS,", "citeRegEx": "Helmert and Geffner,? \\Q2008\\E", "shortCiteRegEx": "Helmert and Geffner", "year": 2008}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "In ICAPS,", "citeRegEx": "Helmert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Helmert et al\\.", "year": 2007}, {"title": "The FF planning system: fast plan generation through heuristic search", "author": ["J. Hoffmann", "B. Nebel"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Hoffmann and Nebel,? \\Q2001\\E", "shortCiteRegEx": "Hoffmann and Nebel", "year": 2001}, {"title": "Common misconceptions concerning heuristic search. In SOCS", "author": ["R.C. Holte"], "venue": null, "citeRegEx": "Holte,? \\Q2010\\E", "shortCiteRegEx": "Holte", "year": 2010}, {"title": "Optimal additive composition of abstraction-based admissible heuristics", "author": ["M. Katz", "C. Domshlak"], "venue": "In ICAPS,", "citeRegEx": "Katz and Domshlak,? \\Q2008\\E", "shortCiteRegEx": "Katz and Domshlak", "year": 2008}, {"title": "Scalable, parallel best-first search for optimal sequential planning", "author": ["A. Kishimoto", "A.S. Fukunaga", "A. Botea"], "venue": "In ICAPS", "citeRegEx": "Kishimoto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kishimoto et al\\.", "year": 2009}, {"title": "Privacy-preserving multi-agent constraint satisfaction", "author": ["T. L\u00e9aut\u00e9", "B. Faltings"], "venue": "In CSE", "citeRegEx": "L\u00e9aut\u00e9 and Faltings,? \\Q2009\\E", "shortCiteRegEx": "L\u00e9aut\u00e9 and Faltings", "year": 2009}, {"title": "Privacy loss in distributed constraint reasoning: A quantitative framework for analysis and its applications", "author": ["R.T. Maheswaran", "J.P. Pearce", "E. Bowring", "P. Varakantham", "M. Tambe"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Maheswaran et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Maheswaran et al\\.", "year": 2006}, {"title": "Distributed Search by Constrained Agents: Algorithms, Performance, Communication (Advanced Information and Knowledge Processing)", "author": ["A. Meisels"], "venue": null, "citeRegEx": "Meisels,? \\Q2007\\E", "shortCiteRegEx": "Meisels", "year": 2007}, {"title": "A heuristic search algorithm with modifiable estimate", "author": ["L. M\u00e9ro"], "venue": "Artif. Intell.,", "citeRegEx": "M\u00e9ro,? \\Q1984\\E", "shortCiteRegEx": "M\u00e9ro", "year": 1984}, {"title": "Principles of artificial intelligence", "author": ["N.J. Nilsson"], "venue": null, "citeRegEx": "Nilsson,? \\Q1980\\E", "shortCiteRegEx": "Nilsson", "year": 1980}, {"title": "Tunneling and decomposition-based state reduction for optimal planning", "author": ["R. Nissim", "U. Apsel", "R.I. Brafman"], "venue": "In ECAI,", "citeRegEx": "Nissim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2012}, {"title": "A general, fully distributed multi-agent planning algorithm", "author": ["R. Nissim", "R.I. Brafman", "C. Domshlak"], "venue": "In AAMAS,", "citeRegEx": "Nissim et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2010}, {"title": "Computing perfect heuristics in polynomial time: On bisimulation and merge-and-shrink abstraction in optimal planning", "author": ["R. Nissim", "J. Hoffmann", "M. Helmert"], "venue": "In IJCAI,", "citeRegEx": "Nissim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2011}, {"title": "A scalable method for multiagent constraint optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "In IJCAI,", "citeRegEx": "Petcu and Faltings,? \\Q2005\\E", "shortCiteRegEx": "Petcu and Faltings", "year": 2005}, {"title": "M-DPOP: Faithful distributed implementation of efficient social choice problems", "author": ["A. Petcu", "B. Faltings", "D.C. Parkes"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Petcu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Petcu et al\\.", "year": 2008}, {"title": "Distributed constraint satisfaction and optimization with privacy enforcement", "author": ["Silaghi", "M.-C", "D. Mitra"], "venue": "In IAT,", "citeRegEx": "Silaghi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Silaghi et al\\.", "year": 2004}, {"title": "Framework and complexity results for coordinating non-cooperative planning agents", "author": ["J.R. Steenhuisen", "C. Witteveen", "A. ter Mors", "J. Valk"], "venue": "In MATES,", "citeRegEx": "Steenhuisen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steenhuisen et al\\.", "year": 2006}, {"title": "Maa*: A heuristic search algorithm for solving decentralized pomdps", "author": ["D. Szer", "F. Charpillet", "S. Zilberstein"], "venue": "In UAI,", "citeRegEx": "Szer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Szer et al\\.", "year": 2005}, {"title": "Coordinating autonomous planners", "author": ["A. ter Mors", "J. Valk", "C. Witteveen"], "venue": "In IC-AI,", "citeRegEx": "Mors et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mors et al\\.", "year": 2004}, {"title": "Coordinating self interested autonomous planning agents", "author": ["A. ter Mors", "C. Witteveen"], "venue": "In BNAIC,", "citeRegEx": "Mors and Witteveen,? \\Q2005\\E", "shortCiteRegEx": "Mors and Witteveen", "year": 2005}, {"title": "An approach to multi-agent planning with incomplete information", "author": ["A. Torre\u00f1o", "E. Onaindia", "O. Sapena"], "venue": "In ECAI,", "citeRegEx": "Torre\u00f1o et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Torre\u00f1o et al\\.", "year": 2012}, {"title": "Parallel planning via the distribution of operators", "author": ["D. Vrakas", "I. Refanidis", "I.P. Vlahavas"], "venue": "J. Exp. Theor. Artif. Intell.,", "citeRegEx": "Vrakas et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Vrakas et al\\.", "year": 2001}, {"title": "Learning action models from plan examples using weighted max-sat", "author": ["Q. Yang", "K. Wu", "Y. Jiang"], "venue": "Artif. Intell.,", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}, {"title": "Protocols for secure computations (extended abstract)", "author": ["Yao", "A.C.-C"], "venue": "In FOCS,", "citeRegEx": "Yao and C..C.,? \\Q1982\\E", "shortCiteRegEx": "Yao and C..C.", "year": 1982}, {"title": "How to generate and exchange secrets (extended abstract)", "author": ["Yao", "A.C.-C"], "venue": "In FOCS,", "citeRegEx": "Yao and C..C.,? \\Q1986\\E", "shortCiteRegEx": "Yao and C..C.", "year": 1986}, {"title": "The distributed constraint satisfaction problem: Formalization and algorithms", "author": ["M. Yokoo", "E.H. Durfee", "T. Ishida", "K. Kuwabara"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "Yokoo et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Yokoo et al\\.", "year": 1998}, {"title": "Secure distributed constraint satisfaction: Reaching agreement without revealing private information", "author": ["M. Yokoo", "K. Suzuki", "K. Hirayama"], "venue": "In CP,", "citeRegEx": "Yokoo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Yokoo et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 20, "context": "Indeed, this is the main motivation for the field of distributed algorithms, and in particular, the work on distributed constraint satisfaction problems (CSP) (Yokoo, Durfee, Ishida, & Kuwabara, 1998; Meisels, 2007).", "startOffset": 159, "endOffset": 215}, {"referenceID": 22, "context": "There is a long tradition of work on multi-agent planning for cooperative and noncooperative agent teams involving centralized and distributed algorithms, often using involved models that model uncertainty, resources, and more (Nilsson, 1980; Hansen & Zilberstein, 2001; Bernstein, Givan, Immerman, & Zilberstein, 2002; Szer, Charpillet, & Zilberstein, 2005), and much work on how to coordinate the local plans of agents or to allow agents to plan locally under certain constraints (Cox & Durfee, 2005; Steenhuisen, Witteveen, ter Mors, & Valk, 2006; ter Mors, Valk, & Witteveen, 2004; ter Mors & Witteveen, 2005).", "startOffset": 227, "endOffset": 358}, {"referenceID": 38, "context": "The best known example of distributed search is that of distributed CSPs (Yokoo et al., 1998), and various search techniques and heuristics have been developed for it (Meisels, 2007).", "startOffset": 73, "endOffset": 93}, {"referenceID": 20, "context": ", 1998), and various search techniques and heuristics have been developed for it (Meisels, 2007).", "startOffset": 81, "endOffset": 96}, {"referenceID": 24, "context": "This methodology was later extended to the first fully distributed MA algorithm for ma-strips planning, PlanningFirst (Nissim et al., 2010).", "startOffset": 118, "endOffset": 139}, {"referenceID": 33, "context": "Recently, a distributed planner based on partial order planning was introduced (Torre\u00f1o et al., 2012), which outperforms Planning First, effectively solving more tightly coupled problems.", "startOffset": 79, "endOffset": 101}, {"referenceID": 15, "context": "Although recent work (Holte, 2010) shows that pathmax does not necessarily make a bona-fide consistent heuristic, pathmax does ensure that f -values along a path are non-decreasing.", "startOffset": 21, "endOffset": 34}, {"referenceID": 21, "context": "We assume global consistency of all heuristic functions, since all admissible heuristics can be made consistent by locally using the pathmax equation (M\u00e9ro, 1984), and by using the max operator as in line 4 of Algorithm 2 on heuristic values of different agents.", "startOffset": 150, "endOffset": 162}, {"referenceID": 11, "context": "We chose Fast Downward (Helmert, 2006) (FD) as the basis for our MA framework \u2013 MA-FD.", "startOffset": 23, "endOffset": 38}, {"referenceID": 33, "context": "To evaluate mafs in its non-optimal setting, we compare it to the state-of-the-art distributed planner map-pop (Torre\u00f1o et al., 2012), and to the Planning-First algorithm (Nissim et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 24, "context": ", 2012), and to the Planning-First algorithm (Nissim et al., 2010).", "startOffset": 45, "endOffset": 66}, {"referenceID": 6, "context": "A similar tradeoff is explored in the area of differential privacy (Dwork, 2006).", "startOffset": 67, "endOffset": 80}], "year": 2013, "abstractText": "This paper describes a number of distributed forward search algorithms for solving multi-agent planning problems. We introduce a distributed formulation of non-optimal forward search, as well as an optimal version, mad-a*. Our algorithms exploit the structure of multi-agent problems to not only distribute the work efficiently among different agents, but also to remove symmetries and reduce the overall workload. The algorithms ensure that private information is not shared among agents, yet computation is still efficient \u2013 outperforming current state-of-the-art distributed planners, and in some cases even centralized search \u2013 despite the fact that each agent has access only to partial information.", "creator": "TeX"}}}