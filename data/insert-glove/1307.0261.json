{"id": "1307.0261", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2013", "title": "WebSets: Extracting Sets of Entities from the Web Using Unsupervised Information Extraction", "abstract": "We burtynsky describe erie a betfred open - domain resulted information irreducibility extraction maestas method for banyo extracting halicarnassus concept - kuralt instance empyema pairs vinica from an envisage HTML corpus. rd6 Most safi earlier knucklehead approaches to centrifuged this problem rely on raveena combining clusters of distributionally indecisiveness similar terms and oil concept - instance drug pairs guterman obtained 339.7 with pescia Hearst patterns. In contrast, our halutz method relies clanricarde on chatelain a novel approach chodkiewicz for bhoys clustering terms tent-like found in alekseyevich HTML chockablock tables, outside-left and then disinhibited assigning concept abgar names to bradninch these 12pm clusters using Hearst anglicanism patterns. The wijngaarden method 98.03 can acum be efficiently verdadero applied to semipalatinsk a large gladiators corpus, mahv and experimental confiscating results chao'an on nocturnal several datasets show that our 20-a method budgen can accurately extract kasey large weiming numbers of moncho concept - instance venevisi\u00f3n pairs.", "histories": [["v1", "Mon, 1 Jul 2013 02:49:08 GMT  (75kb,D)", "http://arxiv.org/abs/1307.0261v1", "10 pages; International Conference on Web Search and Data Mining 2012"]], "COMMENTS": "10 pages; International Conference on Web Search and Data Mining 2012", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["bhavana dalvi", "william w cohen", "jamie callan"], "accepted": false, "id": "1307.0261"}, "pdf": {"name": "1307.0261.pdf", "metadata": {"source": "CRF", "title": "WebSets: Extracting Sets of Entities from the Web Using Unsupervised Information Extraction", "authors": ["Bhavana Dalvi", "William W. Cohen", "Jamie Callan"], "emails": ["bbd@cs.cmu.edu", "wcohen@cs.cmu.edu", "callan@cs.cmu.edu"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors: I.2.6[Artificial Intelligence]: Learning - Knowledge acquisition\nGeneral Terms: Algorithms, Experimentation.\nKeywords: Web Mining, Clustering, Hyponymy Relation Acquisition."}, {"heading": "1. INTRODUCTION", "text": "Many NLP tasks\u2014include summarization, co-reference resolution, and named entity extraction\u2014are made easier by acquiring large sets of concept-instance pairs (such as \u201cgoddess,Venus\u201d and \u201cUS President\u201d,\u201dBill Clinton\u201d). Ontologies that include many such pairs (e.g., FreeBase and WordNet) exist, but are often incomplete. Here we consider the problem of automatically harvesting concept-instance pairs from a large corpus of HTML tables.\nPast approaches to this problem have primarily been based detecting coordinate terms and hyponym patterns. Hyponym patterns, sometimes called Hearst patterns [11], are surface patterns (like \u201cXs such as Y\u201d) indicating that X,Y are a concept-instance pair. Two terms i and j are coordinate terms if i and j are instances of the same concept: for instance, \u201cBill Clinton\u201d and \u201cRichard Nixon\u201d are coordinate terms, since both are instances of the concept \u201cUS President\u201d. Coordinate terms are most frequently detected by clustering terms based on distributional similarity [15]\u2014i.e.,\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WSDM\u201912, February 8\u201312, 2012, Seattle, Washingtion, USA. Copyright 2012 ACM 978-1-4503-0747-5/12/02 ...$10.00.\nthe similarity of their contexts in free text. Various techniques can be used to combine these coordinate-term and hyponym information to generate additional concept-instance pairs [26, 22].\nOur approach is novel in that it relies solely on HTML tables to detect coordinate terms. We present a novel clustering method that finds extremely precise coordinate-term clusters by merging table columns that contain overlapping triplets of instances, and show that this clustering method outperforms k-means, while still being scalable enough to apply to large corpora. We also present a new method for combining hyponym and coordinate-term information, and show that on table-rich corpora, this method improves on previously-published techniques [26], obtaining higher accuracy while generating nearly hundred times the number of concept-instance pairs. In a final set of experiments, we show that allowing a small amount of user input for each coordinate-term cluster can produce concept-instance pairs with accuracy in the high 90\u2019s for four different corpora.\nThe experiments in this paper are conducted on several different HTML corpora and a collection of Hearst pattern [11] instances that have been extracted from ClueWeb09, all of which are made available for future researchers.\nThe rest of this paper is organized as follows. Section 2 presents a more detailed overview of the related work. Section 3 describes our unsupervised IE technique, which we call WebSets. Section 4 describes the evaluation methodology and experimental results, and section 6 concludes."}, {"heading": "2. RELATED WORK", "text": "Information extraction from unstructured and semi-structured information sources on the Web is an active area of research in recent years. In this section we summarize the existing techniques after categorizing them, by the inputs they are using and the goals they are trying to reach."}, {"heading": "2.1 Exploiting Tables on the Web", "text": "Gatterbauer et al. [8] focus on extracting tabular data from various kinds of pages that have table-like visual representations when rendered in a browser. Although we do not make use of these techniques, they could be employed by WebSets to collect more tabular information from a corpus.\nThe WebTables [2] system extracted schema information from a huge corpus of 14.1 billion HTML tables from Google\u2019s general-purpose web crawl. They built an attribute correlation statistics database (AcsDB), which can be used to create an attribute name thesaurus and a schema auto-completion system. Unfortunately the WebTables corpora is not publi-\nar X\niv :1\n30 7.\n02 61\nv1 [\ncs .L\nG ]\n1 J\nul 2\n01 3\ncally available; it would be an interesting project to apply WebSets to a corpus of this size.\nGupta et al. [9] focuses on the task of extending a table given a few seed rows. Their technique consolidates HTML lists relevant to the example rows to build a result table. Similarly, SEAL (Set Expansion for Any Language) [28] is a set expansion system which starts with a few seed examples and extends them using lists detected using character-based heuristics. Unlike these systems, WebSets does not require seed examples, but instead extracts concept-instance pairs in an unsupervised manner from a corpus.\nGupta and Sarawagi [10] consider jointly training structured extraction models from overlapping web source (primarily in tables), thus avoiding the need for labeled data. WebSets also depends on content overlap across table columns and domains, but generates concept-instance pairs instead of building an extractor.\nLimaye et al. [14] proposed a system to use an existing catalog and type hierarchy for annotating table columns and cells. They also propose better indexing techniques to improve the search engine response to table queries. WebSets differs in that the Hearst-pattern data it uses is noisier than a catalog or type hierarchy."}, {"heading": "2.2 Information Extraction Systems", "text": "Many systems perform semi-supervised information extraction from free text on the web, using only a few seed examples or seed rules. These systems include KnowItAll [6, 7], ASIA [27], and Coupled Pattern Learning (CPL) [4]. Along similar lines, Parameswaran et al. [18] propose a concept extraction algorithm which can identify a canonical form of a concept, filtering out sub-concepts or superconcepts; and Ritter et al. [20] describe a scheme for filtering concept-instance pairs, using a SVM classifier which uses as features frequency statistics for several Hearst patterns on a large corpus. Given a training set of text containing known concept-instance pairs, Snow et al. [22] learns \u201cdependency path\u201d features, which can further expand the set of conceptinstance pairs. WebSets is different from most of these approaches in that it builds all sets of entities from a given corpus, and does not require seed sets, a starting ontology, or a set of target concept names.\nTextRunner [30] is an open-domain IE system which makes a single pass over the corpus of unstructured text and extracts a large set of relational tuples, without requiring any human input. Unlike WebSets, however, it does not build coherent sets of category or relation instances. Pantel and Ravichandran [17] proposes a method to automatically label distributional term clusters using Hearst-like patterns, and Van Durme and Pasca [26] proposed an alternative approach method to extract labeled classes of instances from unstructured text. All of these approaches use only unstructured text to find coordinate terms and assigning hypernyms, whereas WebSets uses HTML tables. As we show in the experimental results, WebSets uses a novel method for combining coordinate-term clusters and hypernyms data that quantitatively improves over Van Durme and Pasca\u2019s method for combining coordinate-term clusters and hypernym data.\nThere also exist some systems that use both free-text and tabular information. Talukdar et al. [24] proposed a graph random walk based semi-supervised label propagation technique for open domain class instance extractions,\nwhich extends the system of Van Durme and Pasca by using table data (from WebTables) as well as free-text distributional clusters. However, unlike Talukdar et al.\u2019s system, WebSets does not require distributional clusters. Coupled SEAL (CSEAL) [4] use of mutual exclusion, containment and type checking relationships to extend SEAL, and CPL [4] and CSEAL are the two components of NELL [25], a multi-strategy semi-supervised learning system. However, unlike NELL, WebSets does not require seed instances or an ontology as input.\nShinzato and Torisawa [21] showed that coordinate terms can be extracted from itemizations in structured web documents. Their method finds concept-instance pairs pertaining to a single query list, and finds candidate hypernyms by querying the web on-the-fly to collect documents. In contrast, WebSets processes all lists in a corpora simultaneously, and makes no web queries."}, {"heading": "3. WEBSETS", "text": "In this section we describe an unsupervised information extraction technique named WebSets which extracts conceptinstance pairs from HTML tables in a given corpus. It builds coordinate term clusters using co-occurrence in table columns and assigns hypernyms to these clusters using Hearst pattern data extracted form text corpus. To build term clusters, system should first extract tables by parsing HTML pages, then decide which tables have useful relational data. Following the hypothesis that entities appearing in a table column possibly belong to the same concept, each table column in this extracted data is a candidate entity set; the system hence needs to have a mechanism to cluster those table columns. Clustering the table columns will yield sets of entities, each of which potentially belongs to a coherent concept. These sets will become more useful if they are labeled with appropriate concept-names. To summarize, the technique we develop needs to solve following sub-problems:\n1. Table Identification: Extracting tables from the corpus that are likely to have relational data.\n2. Entity Clustering: Efficiently clustering the extracted table cells to generate coherent sets of entities.\n3. Hypernym Recommendation: Labeling each cluster with an appropriate concept-name (hyperym).\nIn this section, we describe our approach, WebSets, which solves each of the above mentioned sub-problems in an effective way."}, {"heading": "3.1 Table Identification", "text": "Currently WebSets parses tables defined by<table> tags1. This is only a fraction of structured data available on the Web. Use of other techniques like Gatterbauer et al. [8] can provide more input data to learn sets from.\nFurther only a small fraction of HTML tables actually contain useful relational data(see Section 4). Remaining tables are used for formatting or rendering purposes rather than to present relational data. To filter out useful tables, WebSets uses the following set of features: (1) the number of rows (2)\n1We found that large number of HTML pages have broken syntax. We use the HTML syntax cleaning software Tidy [1] to fix the syntax in these pages.\nthe number of non-link columns (3) the length of cells after removing formatting tags (4) whether table contains other HTML tables. The thresholds set for our experiments are explained in Section 4."}, {"heading": "3.2 Entity Clustering", "text": "At the end of the table identification step, we have a collection of HTML tables which are likely to contain relational data. Each of the table-cells is a candidate entity and each table-column is a candidate set of entities. However many columns have information that is useful only within a site (e.g., navigational links) and many are overlapping. To solve this problem we use the redundancy of information on the Web. If a set of entities co-occur in multiple table-columns across the Web, then it is more likely to be an important entity set. Since these tables come from different domains and are created by different authors, they will typically only partially overlap. To cluster the entities in these columns into coherent sets, the first essential step will be to represent this data in a way that reveals the co-occurrence of entities across multiple table columns. In this section, we will first discuss the data representation, and then algorithms for entity clustering.\n3.2.1 Data Representation We considered two ways to represent table data. With the \u201cEntity record representation\u201d, one record per entity is created, and it contains information about which table-columns and URL domains the entity was mentioned in. These entity records can then be clustered, based on the overlap of table-columns and domains, to yield sets of entities. One advantage of this representation is that it is compact. Another advantage is that the number of distinct domains an entity occurred in can be used as an indicator of how \u201cimportant\u201d it is. A disadvantage of this representation is that if some entity-name is ambiguous (has multiple senses), it will collapse all senses of the entity into one record. E.g., consider a corpus which contains mentions of the entity \u201cApple\u201d in two senses, \u201cApple as a fruit\u201d and \u201cApple as a company\u201d. This representation will create a single record for the entity \u201cApple\u201d with its occurrence as a fruit and as a company confounded. An unsupervised IE system might find it difficult to decompose multiple senses from this single record.\nTo solve this problem, we propose a novel representation of the table data called \u201cEntity-Triplet records\u201d. In this representation, there is a record for each triplet of adjacent entities instead of for individual entities. Each record contains information about which table-columns and URL domains the triplet occurred in. Hence in case of an ambiguous entity like \u201cApple\u201d, its occurrences as a fruit will be separate from its occurrences as a company, e.g., {Apple, Avocado, Banana} will be one triplet record and {Apple, Microsoft, DELL} will be another triplet record. Thus entities within a triplet disambiguate each other. Since we have a list of all domains a triplet occurred in, only those triplets which appear in enough domains can be considered as important.\nIn this way, we represent the table data as a triplet store, which contains a record for each entity triplet. The triplet store can be built in one single pass over the corpus. If the system considers all possible triplets that can be created from each table column, then number of triplet records will be quadratic in the total size of all tables. This can be a serious concern for a web-scale dataset. Hence our system\nconstructs onlty those triplets which are adjacent i.e. subsequences of entities in a table-column. This ensures that number of triplet records are linear in total size of all tables, making the system scalable. The system keeps track of all table-columns a triplet occurred in, which makes it possible to reconstruct a column by joining triplets on columnId. Hence this storage method does not result in any loss of information.\nConsider an example of tables containing countries and their capitals. Original tables are shown in Table 1, 2 and the triplet records created by WebSets are shown in Table 3. Second row in Table 3, indicates that the triplet (China, Canada, France) occurred in column 1 of tableId 21 and column 1 of tableId 34. Also these entities are retrieved from webpages which reside in the domains\u201cwww.dom1.com\u201dand \u201cwww.dom2.com\u201d.\nThese triplets are canonicalized by converting entity strings to lower case and arranging the constituent entities in alphabetical order. The triplets are then ranked in descending order of number of domains.\nWe create O(n) triplets from a table column of size n. Adding each triplet to the Triplet Store using hashmap takes O(1) time. Given a set of T HTML tables with a total of N entities in them, the Triplet Store can be created in O(N) time. Ranking the triplets using any common sorting technique will takeO(N\u2217logN) time. Hence the total complexity of building the Triplet Store is O(N \u2217 logN).\n3.2.2 Building entity clusters The next task is to cluster these triplet records into mean-\ningful sets. The system does not know how many clusters are present in the underlying dataset; and since our dataset will be constructed from a huge HTML web corpus, the clustering algorithm needs to be very efficient. Given these requirements, it can be easily seen that parametric clustering algorithms like K-means may not be effective due to the unknown number of clusters. Non-parametric algorithms like agglomerative clustering [5] fit most of our requirements. The most efficient agglomerative clustering al-\ngorithm is the single-link clustering algorithm, but even that would require computing the similarity of every pair of entity triplets returned. This will be very expensive for the web scale datasets we are aiming to handle. Hence we develop a new bottom-up clustering algorithm which is efficient in terms of both space and time. Experiments to compare our algorithm with a standard K-means clustering algorithm are described in Section 4.\n3.2.3 Bottom-Up Clustering Algorithm The clustering algorithm (named \u201cBottom-Up Clusterer\u201d)\nis described formally in Algorithm 1. The clusterer scans through each triplet record t which has occurred in at least minUniqueDomain distinct domains. A triplet and a cluster are represented with the same data-structure: (1) a set of entities, (2) a set of columnIds in which the entities co-occurred and (3) a set of domains in which the entities occurred.\nThe clusterer compares the overlap of triplet t against each cluster Ci. The triplet t is added to the first Ci so that either of the following two cases is true: (1) at least 2 entities from t appear in cluster Ci . (2) at least 2 columnIds from t appear in cluster Ci. (i.e. minEntityOverlap = 2 and minColumnOverlap = 2) In both these cases, intuitively there is a high probability that t belongs to the same category as cluster Ci. If no such overlap is found with existing clusters, the algorithm creates a new cluster and initializes it with the triplet t.\nThis clustering algorithm is order dependent, i.e., if the order in which records are processed changes, it might return a different set of clusters. Finding the optimal ordering of triplets is a hard problem, but a reasonably good ordering can be easily generated by ordering the triplets in the descending order of number of distinct domains. We discard triplets that appear in less than minUniqueDomain domains.\nAlgorithm 1 Bottom-Up Clustering Algorithm\n1: function Bottom-Up-Clusterer(TripletStore):Clusters {Triplet records are ordered in descending order of number of distinct domain.} 2: Initialize Clusters = \u03c6; max = 0 3: for (every t \u2208 TripletStore : such that |t.domains| >=minUniqueDomain) do 4: assigned = false 5: for every Ci \u2208 Clusters do 6: if |t.entities \u2229 Ci.entities| >= minEntityOverlap OR |t.col \u2229 Ci.col| >= minColumnOverlap then 7: Ci = Ci \u222a t 8: assigned = true 9: break;\n10: end if 11: end for 12: if not assigned then 13: increment max 14: Create new cluster Cmax = t 15: Clusters = Clusters \u222a Cmax 16: end if 17: end for 18: end function\n3.2.4 Computational complexity Suppose that our dataset has total T tables. Let these\ntables have in total N cells. For each triplet t, Algorithm 1 finds entity and column overlap with all existing clusters. This operation can be implemented efficiently by keeping two inverted indices: (1) from each entity to all clusterIds it belongs to and (2) from each columnId to all clusterIds it belongs to. ClusterIds in each \u201cpostings list \u201d 4 will be kept in sorted order.\nMerging k sorted lists, with resultant list size of n takes O(n \u2217 logk) time. Now let us compute worst case time complexity of Algorithm 1. To compute entity overlap of each triplet, the algorithm merges 3 postings lists with total size of O(N). This step will take O(N) time. To compute TableId:ColumnId overlap of each triplet, it merges O(T ) postings lists with total size of O(N). This step will take O(N \u2217 logT ) time. Hence for each triplet, finding a clusterId to merge the triplet with takes O(N \u2217 logT ) time. There are O(N) triplets. So the Bottom-Up Clusterer will have worst case time complexity of O(N2 \u2217 logT ). In practice, it is much less than this. If N is total number of table-cells in the corpus then the total number of triplet occurrences is also O(N). Hence all the postings list merges can be amortized to be O(N). Hence amortized complexity of this clustering algorithm is O(N \u2217 logT ). Considering the time complexity to sort the triplet store, total time complexity is O(N \u2217 logN), which is much better than the complexity of a naive single-link clustering algorithm, which is O(N2\u2217logN)."}, {"heading": "3.3 Hypernym Recommendation", "text": "Previous sections described how do we obtain coordinate term clusters using co-occurrence of terms in the table columns. In this section we label these term clusters with the help of Hyponym Concept dataset.\n3.3.1 Building The Hyponym-Concept Dataset The Hyponym Concept Dataset is built by acquiring concept-\ninstance pairs from unstructured text using Hearst patterns. For this task we used the data extracted from the ClueWeb09 corpus [3] by the developers of the NELL KB [25]. They used heuristics to identify and then shallow-parse approximately 2 billion sentences, and then extracted from this all patterns of the form \u201c word1 .. wordk \u201d where the \u2018filler\u2019 word1 .. wordk is between one and five tokens long, and this filler appears at least once between two base noun phrases in the corpus. Each filler is paired with all pairs of noun phrases that bracket it, together with the count of the total number of times this sequence occurred. For instance, the filler \u2018 and vacations in \u2019 occurs with the pair \u2018Holidays, Thailand\u2019 with a count of one and \u2018Hotels,Italy\u2019 with a count of six, indicating that the phrase \u201cHotels and vacations in Italy\u201d occurred six times in the corpus . The hyponym dataset was constructed by finding all fillers that match one of the regular expressions in Table 4. These correspond to a subset of the Hearst patterns used in ASIA [27] together with some \u201cdoubly anchored\u201d versions of these patterns [13].\nEach record in the Hyponym Concept Dataset contains an entity and all concepts it co-occurred with. Table 5 shows an example of records in this dataset. According to this table, entity \u201cUSA\u201d appeared with the concept \u201ccountry\u201d 1000\n4In the information retrieval terminology, an inverted index has a record per word that contains the list of all documentids the word occurred in. This list is referred to as the \u201cpostings list\u201d. In this paper, a \u201cpostings list\u201d refers to the list of all clusterIds that a triplet or a columnId belongs to.\ntimes. Similarly, \u201cMonkey\u201d appeared with two different concepts, 100 times with\u201canimal\u201dand 60 times with\u201cmammal\u201d.\n3.3.2 Assigning Hypernyms to Clusters Assigning a meaningful concept-name to each entity set\nis important for two reasons. It enables us to systematically evaluate entity sets; e.g., it is easier for an evaluator to answer the question: \u201cIs Boston a city?\u201d than \u201cIs Boston a member of set #37?\u201d) It also makes the system more useful for summarizing the data in a corpus (see Section 4.3.5). This section describes how WebSets recommends candidate hypernyms for each entity set produced by Algorithm 1.\nFor this task, we use the coordinate term clusters extracted from tables and Hyponym Concept Dataset extracted using Hearst patterns. Note that this part of the system uses information extracted from unstructured text from the Web, to recommend category names to sets extracted from tables on the Web.\nAlgorithm 2 Hypernym Recommendation Algorithm\n1: function GenerateHypernyms 2: Given: c: An entity cluster generated by Algorithm 1, I: Set of all entities , L: Set of all labels, H \u2286 L\u00d7 I: Hyponym-concept dataset, 3: Returns: RLc : Ranked list of hypernyms for c. 4: Algorithm: 5: RLc = \u03c6 6: for every label l \u2208 L do 7: Hl = Set of entities which co-occurred with l in H 8: Score(l) = |Hl \u2229 c| 9: RLc = RLc \u222a < l, Score(l) >\n10: end for 11: Sort RLc in descending order of Score(l) 12: Output RLc 13: end function\nThe algorithm is formally described in Algorithm 2. For each set produced at the end of clustering, we find which entities from the set belong to Hyponym Concept Dataset and collect all concepts they co-occur with. Then these concepts are ranked by number of unique entities in the set it co-occurred with. This ranked list serves as hypernym recommendations for the set. The output of this stage can be used in two ways. We can assign the topmost hypernym in the rank list as the class label for a cluster (used in Section 4.2.4). Another possibility is to present a ranked list of\nhypernyms for each cluster to a user, who can then select the one which is the best for a given entity cluster (refer to Section 4.3.2).\nOur method scores labels differently from Van Durme and Pasca method [26] It is also different in the sense that they output a concept-instance pair < x, y > only when < x, y > appears in the set of candidate concept-instance pairs, whereas we extend the labels to the whole cluster. Hence even if some pair < x, y > is not present in the Hyponym Concept dataset, it can be produced as output."}, {"heading": "4. EXPERIMENTAL EVALUATION", "text": "In this section we first discuss the datasets we worked on. Then we evaluate each step of our extraction process separately. These experiments assume that our method extracts a list of concept-instance pairs from the HTML corpus. Later part of this section discusses how well WebSets perform end to end, as a tool to process a large HTML corpus, and build coherent sets of entities along with labeling each of them. The datasets and evaluations done for these experiments are posted online at http://rtw.ml.cmu.edu/ wk/WebSets/wsdm_2012_online/index.html."}, {"heading": "4.1 Datasets", "text": "To test the performance of WebSets, we created several webpage datasets that are likely to have coherent sets of entities. An evaluation can then be done to check whether the system extracts expected entity sets from those datasets. Some of these datasets are created using SEAL, CSEAL and ASIA systems (refer to Section 2) that extract information from semi-structured pages on the Web. Each of these systems, takes a name or seed examples of a category as input and finds possible instances of that category using set expansion techniques. It queries a web search engine during this process and stores all the pages downloaded from the Web in a cache, so that they can be reused for any similar queries in the future. The cache contents generated by these systems help us build reasonable sized datasets for our experiments. We expect that WebSets will find reasonable number of HTML tables in these datasets.\n1. Toy Apple: This is a small toy dataset created with the help of multiple SEAL queries with \u201cApple\u201d as a fruit and as a company. It is created to demonstrate the entity disambiguation effect of using triplet records and to compare various clustering algorithms.\n2. Delicious Sports: This dataset is a subset of DAILabor Delicious corpus [29], created by taking only those URLs which are tagged as \u201csports\u201d.\n3. Delicious Music: This dataset is a subset of DAILabor Delicious corpus [29], created by taking only those URLs which are tagged as \u201cmusic\u201d.\n4. CSEAL Useful: CSEAL is one of the methods which suggest new category and relation instances to NELL KB. CSEAL mostly extracts entities out of semi-structured information on the Web. For each instance in the KB, we can retrieve the information about which methods supported the existence of the instance. The webpages from which these methods derived the support are also recorded. CSEAL Useful dataset is a collection of those HTML pages from which CSEAL gathered information about entities in the NELL KB.\n5. ASIA NELL: This dataset is collected using hypernyms associated with entities in the NELL KB as queries for ASIA. Examples of such hypernyms are \u201cCity\u201d, \u201cBird\u201d, \u201cSports team\u201d etc.\n6. ASIA INT: This dataset is also collected using the ASIA system but with another set of category names as input. These category names come from \u201cIntelligence domain\u201d. Examples of categories in this domain are \u201cgovernment types\u201d, \u201cinternational organizations\u201d, \u201cfederal agencies\u201d, \u201creligions\u201d etc.\n7. Clueweb HPR: This dataset is collected by randomly sampling high pagerank pages in the Clueweb dataset [3]. For this purpose we used the Fusion spam scores[12] provided by Waterloo university and used pages with spam-rank score higher than 60%.\nTable 6 shows the number of HTML pages and tables present in each of the above mentioned datasets. Note that Clueweb HPR contains random sample of the Clueweb dataset, and hence does not contain large number of tables like other datasets. Datasets derived using SEAL or ASIA are expected to have higher concentration of semi-structured data in them. Toy Apple and both Delicious datasets are specifically designed for certain domains, whereas CSEAL Useful and ASIA NELL are relatively heterogeneous, Clueweb HPR being the most heterogeneous dataset among all. In each of the following experiments, we work on one or more of these datasets to evaluate different aspects of WebSets."}, {"heading": "4.2 Evaluation of Individual Stages", "text": "In this section we evaluate each stage of WebSets system and measure the performance. Here we consider WebSets as a technique to generate large number of concept-instance pairs given a HTML corpus.\n4.2.1 Evaluation: Table Identification Table 7 shows the statistics of table identification for each\ndataset. Based on the features described in Section 3.1, we filter out only those tables as useful which cross some predefined thresholds. These thresholds were derived from the intuitions after manually going through some samples of data, and are kept constant for all datasets and experiments described in this paper. When we evaluated a random sample of recursive tables, 93% of them were useless for our purposes, and only 7% tables contained relational data. Hence we decide to ignore the recursive tables. We construct triplets of entities in a table column, hence a table should have at least 3 rows. As we are not using any link data in our system, we consider only those columns which do not have links. WebSets is looking for tables containing relational data, hence if a table has at least 2 non-link columns, probability of it having relational data increases. While counting\nthese rows and columns, we are looking for named entities. So from each cell, all HTML tags and links are removed. A very coarse filtering by length is then applied. We consider only those table cells which are 2-50 characters in length. Table 7 shows that percentage of relational tables increases by orders of magnitude due to this filtering step.\n4.2.2 Evaluation: Clustering Algorithm In these set of experiments we first compare WebSets with\nbaseline K-means clustering algorithm. Later we see how \u201cEntity-Triplet record\u201d representation is better than \u201cEntity record\u201d representation. We compare the clustering algorithms in terms of commonly used clustering metrics: cluster purity (Purity), normalized mutual information (NMI), rand index (RI) [16] and Fowlkes-Mallows index (FM) which are defined as follows:\n\u2022 Cluster Purity: To compute cluster purity, for each cluster the class which is most frequent in it gets assigned. The accuracy of this assignment is then measured by counting the number of correctly assigned documents and dividing by total number of documents. purity(\u2126, C) = 1\nN \u2211 k maxj |\u03c9k \u22c2 cj | where\nN is total number of documents, \u2126 = \u03c91, \u03c92, ..., \u03c9K is the set of clusters and C = c1, c2, ..., cJ is the set of classes. We interpret \u03c9k as the set of documents in cluster \u03c9k and cj as the set of documents in cluster cj .\n\u2022 Normalized Mutual Information: High purity is easy to achieve when the number of clusters is large, hence purity cannot be used to trade off the quality of the clustering against the number of clusters. NMI is a measure that allows us to make such tradeoff. NMI(\u2126, C) = I(\u2126;C)\n[H(\u2126)+H(C)]/2\nwhere maximum likelihood estimates for mutual information and entropy are computed as follows:\nI(\u2126, C) = \u2211\nk \u2211 j |\u03c9k \u22c2 cj | N , H(\u2126) = \u2212 \u2211 k |\u03c9k| N \u2217log |\u03c9k| N .\n\u2022 Rand Index: This metric is based on informationtheoretic interpretation of clustering. Clustering is a series of decisions, one for each of the N(N\u22121)/2 pairs of documents in the collection. True label denotes the pair belongs to same class or not. Predicted label denotes whether the pair belongs to same cluster or not. This way we can count True Positive(TP ), False positive (FP ), True Negative (TN) and False Negative (FN) score for the clustering. Rand Index is then defined as follows: RI = TP+TN\nTP+FP+FN+TN .\n\u2022 Fowlkes-Mallows index: While the previous three metrics are applicable only for hard-clustering, this metric is defined for soft clustering of data where clusters can be overlapping but classes are non-overlapping.\nLet nij be the size of intersection of cluster \u03c9i and class cj , ni\u2217 = \u2211 j nij and n\u2217j = \u2211 i nij . FM = ( \u2211\nij ( nij 2 ) )/ \u221a\u221a\u221a\u221a\u2211 i ( ni\u2217 2 )\u2211 j ( n\u2217j 2 ) The derivation of this formula can be found in Ramirez et al. [19].\nHere each triplet is considered as a document. \u2126 refers to clusters of these triplets generated by WebSets or K-means. C refers to actual classes these triplets belong to. To compare quality of clusters across algorithms, we manually labeled each table-column of Toy Apple and Delicious Sports datasets. These labels are then extended to triplets within the table column. This experiment is not repeated for remaining datasets because manual labeling of the whole dataset is very expensive.\nThe performance of K-means depends on the input parameter K and random initialization of cluster centroids to start the clustering process. We run K-means with cosine distance function, a range of values of K and multiple starting points for each value of K. Figure 1 shows the plot of various runs of K-means vs. WebSets on Delicious Sports dataset. Table 8 shows the comparison of WebSets vs. best run of K-means on Toy Apple and Delicious Sports datasets. We can see that WebSets performs better or comparable to K-means in terms of purity, NMI, RI, and FM. Through manual labeling we found that there are 27 and 29 distinct category sets in Toy Apple and Delicious Sports datasets respectively. We can see that WebSets defined 25 and 32 distinct clusters which are very close to actual number of meaningful sets, compared to 40 and 50 clusters defined by K-means.\nStandard K-means algorithm has time complexity of O(I\u2217 K \u2217 N \u2217 T ), where I is the number of iterations, K is the number of clusters, N is the number of table-cells and T is the number of dimensions (here number of table-columns). As seen in Section 3, WebSets has time complexity of O(N \u2217 logN). Hence our bottom-up clustering algorithm is more efficient than K-means in terms of time-complexity.\nTo study entity disambiguation effect of triplet records, we generated both \u201cEntity record\u201d and \u201cEntity-Triplet record\u201d representations of Toy Apple dataset. When we ran our clustering algorithm on \u201cEntity-Triplet record\u201d dataset, we found \u201cApple\u201d in two different clusters, one in which it was clustered with other fruits and had supporting evidence from table-columns talking about fruits; other cluster contained companies and evidence from related table-columns. Running the same clustering algorithm on\u201cEntity record\u201ddataset, resulted in a huge cluster containing\u201cApple\u201d, fruits and companies all combined. Thus we can say that entity triplets do help WebSets to disambiguate multiple senses of the same entity-string.\nNow we compare the performance of clustering algorithms on entity record vs. triplet record representation. In this ex-\nperiment we use Toy Apple dataset. A table-column is considered as document and clusters produced are soft-clustering of this document set. Hence each table-column can be present in multiple clusters, but belongs to only one class. Purity, NMI and RI metrics are not applicable for soft clustering, however FM metric is valid. We run K-Means algorithm for different values of K. Table 9 shows the best performing results of K-Means. WebSets produced 25 clusters on entity record representation and 34 clusters using triplet representation. In terms of FM index, each method gives better performance on triplet record representation when compared to entity record representation. Hence triplet record representation does improve these clustering methods.\n4.2.3 Evaluation: Hyponym-Concept Dataset Note that the Hyponym-concept dataset is created in an\nunsupervised way, hence we cannot guarantee that the conceptinstance pairs in this dataset are accurate. To get an idea of quality of concept-instance pairs in this dataset, we randomly sampled 100 pairs. 55% of them were accurate. Hence hypernym recommendation step is dealing with noisy conceptinstance pairs as input.\n4.2.4 Evaluation: Hypernym Recommendation We compare our hypernym recommendation technique with\nVan Durme and Pasca technique [26]. Our method is different from Van Durme and Pasca Method (DPM) in the sense that, they output a concept-instance pair only when it appears in a set of candidate concept-instance pairs i.e. it exists in Hyponym-concept dataset. In our method, based on overlap of coordinate term cluster with the Hyponym Concept dataset, we extend the labels to whole cluster. Another difference is the coordinate term clusters we are dealing with. DPM assumes term clusters are semantic partitions of terms present in the text corpus. The clusters generated by WebSets are clusters of table columns. There is higher possibility of having multiple small size clusters which all belong to same semantic class, but were not merged due to our single pass bottom-up clusterer. Hence the same method may not work equally well on these clusters.\nWe sample 100 concept-instance pairs randomly from output of each method to measure accuracy. The results of different methods are presented in Table 10. As we can see, DPM generates concept-instance pairs with 50% accuracy even when run with conservative thresholds like K = 5 and J = 0.2. We also tried an extension of DPM called DPMExt which outputs a label for each entity in the cluster, when the label satisfies thresholds defined by J and K. This extension increases coverage of concept-instance pairs orders of magnitude (0.4K to 1.2K) at the cost of slight decrease in accuracy (50% to 44%). Hypernym recommendation of WebSets (WS) is described in Algorithm 2, and only topmost hypernym in the ranked list is produced for each cluster. Table 10 shows that WS has a reasonable accuracy (62.2%) and yield compared to DPM and DPMExt. As discussed in Section\n4.2.3, Hyponym-concept dataset has noisy pairs. We also tried an extension of WebSets called WSExt which overcomes this problem by considering only those class-instance pairs which have occurred at least 5 times in the corpus. Adding this simple constraint, improves accuracy from 67% to 78% and correct pairs yield from 45K to 51K."}, {"heading": "4.3 WebSets as an IE technique", "text": "In this section, we evaluate the whole WebSets system as an IE technique, which generates coherent sets of entities and labels each set with appropriate hypernym. At the end we demonstrate how these entity clusters can summarize the corpus.\n4.3.1 Experimental Methodology Its very expensive to label every table-column of each\ndataset. Here we present a sampling based evaluation method to evaluate WebSets on datasets: CSEAL Useful, ASIA NELL, ASIA INT and Clueweb HPR. We chose these four datasets to cover the different types i.e., domain-specific, open-domain and completely heterogeneous datasets. There are two parts of the evaluation: evaluating labels of each cluster and checking whether each entity of that cluster is coherent with the assigned label.\nThe subjective evaluation is of the form\u201cdeciding whether a cluster is meaningful or noisy\u201d, \u201cassigning label to an unlabeled cluster\u201d. This is done by us (referred to as evaluators). The objective evaluation of the kind \u201cwhether X belongs to category Y\u201d is done using Amazon Mechanical Turk [23]. We created yes/no questions of the form \u201cIs X of type Y?\u201d.\nTo evaluate precision of clusters created by WebSets, we uniformly sampled maximum 100 clusters per dataset, with maximum of 100 samples per cluster and gave them to the Mechanical Turk in the form of yes/no questions. Each question was answered by three different individuals. The ma-\njority vote for each question was considered as a decision for that question. To evaluate quality of Mechanical Turk labels, we sampled 100 questions at random, and manually answered the questions. Then we checked whether majority vote by the Mechanical Turk matches with our answers. We specifically checked majority votes for some confusing questions which were likely to get labeled wrong. We found that majority vote of three individuals was correct more than 90% times and in case of ambiguous questions precision estimates are biased low.\nWe evaluate WebSets using three criteria: (1) Are the clusters generated by WebSets meaningful? (2) How good are the recommended hypernyms? (3) What is precision of the meaningful clusters? Subsequent sections discuss these experiments in detail.\n4.3.2 Meaningfulness of Clusters In this experiment, we did manual evaluation of mean-\ningfulness of clusters, with the help of evaluators. We uniformly sampled maximum 100 clusters from each dataset. We showed following details of each cluster to the evaluator: (1) top 5 hypernyms per cluster (2) maximum 100 entities sampled uniformly from the cluster.\nAn evaluator was asked to look at the entities and check whether any of the hypernyms suggested by the system is correct. If any one of them is correct then he labels the cluster with that hypernym. If none of the hypernym is correct, he can label cluster with any other hypernym that represents the cluster. If entities in a cluster are noisy or do not form any meaningful set, then the cluster is marked as \u201cnoisy\u201d. If the evaluator picks any of the candidate hypernyms as label or gives his own label then the cluster is considered as meaningful, else it is considered as noisy.\nTable 11 shows that 63-73% of the clusters were labeled as meaningful. Note that number of triplets used by the clustering algorithm (Table 11) is different from total number of triplets in the triplet store (Table 6), because only those triplets that occur in at least minUniqueDomain (set to 2 for all experiments) distinct domains are clustered.\n4.3.3 Performance of Hypernym Recommendation In this experiment, we evaluate the performance of the\nhypernym recommendation using following criterion: (1) What fraction of total clusters were assigned some hypernym?: This can be directly computed by looking at the outputs generated by hypernym recommendation.\n(2) For what fraction of clusters evaluator chose the label from the recommended hypernyms?: This can be computed by checking whether each of the manually assigned labels was one of the recommended labels. (3) What is Mean Reciprocal Rank (MRR) of the hypernym ranking?: The evaluator gets to see ranked list of top 5 labels suggested by the hypernym recommender. We compute MRR based on rank of the label selected by the evaluator. While calculating MRR, we consider all meaningful clusters(including the ones for which label does not come from the recommended hypernyms).\nTable 12 shows the results of this evaluation. Out of the random sample of clusters evaluated, hypernym recommendation could label 50-60% of them correctly. The MRR of labels is 0.56-0.59 for all the datasets.\n4.3.4 Precision of Meaningful Clusters In this experiment we want to evaluate the coherency of\nthe clusters; i.e., whether all entities in a cluster are coherent with the label assigned to the cluster. To verify this, we evaluated the meaningful clusters found in previous experiment, using the Mechanical Turk. This evaluation procedure is already discussed in Section 4.3.1. Table 13 shows that the meaningful clusters6 have precision in the range 97-99%. This indicates that WebSets generates coherent entity clusters and hypernym assignment is reasonable.\n4.3.5 Application: Summary of the Corpus The sets of entities produced by Websets can be consid-\nered as a summary of the HTML corpus in terms of what concepts and entities are discussed in the corpus. Tables\n6First three values in column 2 of Table 13 are same as percentage values in column 5 of Table 11. This is because we sample maximum 100 clusters per dataset for the evaluation, hence percentage of meaningful clusters equals the actual number. For the Clueweb HPR dataset, there are only 47 clusters in total, so all are evaluated.\n14 and 15 show such summaries for the datasets ASIA INT and Delicious Music respectively. We choose these datasets because they are domain-specific and hence we have some idea of what entity sets are expected to be found. Looking at the summary we can verify if those expectations are met. Due to space constraints only few clusters with ten entities from each of them are presented here. The labels shown here are generated by Algorithm 2.\nASIA INT is a dataset from Intelligence domain, and the sets shown in Table 14 indicate that corpus contains frequent mentions of officer ranks, international organizations, government types, religions etc. These clusters are related to Intelligence domain and give an idea of what the corpus is about. Similarly Table 14 shows the sets of entities discussed very frequently in Delicious Music i.e., music domain. All of them are important concepts in the music domain. Furthermore our tool generates links to tables and domains from which this data was gathered. While looking at these sets one can click and browse the tables and pages which mention these entities. Hence it can serve as a good exploratory tool for individuals who analyze large datasets."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "This work is supported in part by the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory (AFRL) contract number FA8650-10C-7058. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. This work is also partially supported by the Google Research Grant.\nThe views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either ex-\npressed or implied, of Google, IARPA, AFRL, or the U.S. Government."}, {"heading": "6. CONCLUSION", "text": "We described a open-domain information extraction technique for extracting concept-instance pairs from an HTML corpus. Our approach is novel in that it relies solely on HTML tables to detect coordinate terms. We presented a novel clustering method that finds extremely precise (cluster purity 83-99%) coordinate-term clusters by merging table columns that contain overlapping triplets of instances. This clustering method outperforms k-means in terms of Purity, Rand Index and FM index. We showed that the time complexity of our clustering algorithm is O(N \u2217 logN), making it more efficient than K-means or agglomerative clustering algorithms. We also presented a new method for combining candidate concept-instance pairs and coordinate-term clusters, and showed that on table-rich corpora, this method improved on Van Durme and Pasca method [26]. Our method increased the accuracy from 50% to 78% while generating nearly hundred times the number of concept-instance pairs.\nWe also showed that allowing a small amount of user input for labeling each coordinate-term cluster can produce concept-instance pairs with accuracy in the range 97-99% for four different corpora. Finally we demonstrated that the labeled entity sets produced by WebSets can act as summary of a HTML corpus. The datasets and manual evaluations generated by this work will be made available for future researchers. An interesting direction for future research can be to extend this technique to extract the relations between the entity sets and naming them."}, {"heading": "7. REFERENCES", "text": "[1] Html TIDY project. http://tidy.sourceforge.net/.\n[2] M. J. Cafarella, E. Wu, A. Halevy, Y. Zhang, and D. Z. Wang. Webtables: Exploring the power of tables on the web. PVLDB, 2008.\n[3] J. Callan. The clueweb09 dataset. http://boston.lti.cs.cmu.edu/Data/clueweb09/.\n[4] A. Carlson, J. Betteridge, R. C. Wang, E. R. Hruschka, Jr., and T. M. Mitchell. Coupled semi-supervised learning for information extraction. In WSDM, 2010.\n[5] W. H. E. Day and H. Edelsbrunner. Efficient algorithms for agglomerative hierarchical clustering methods. In Journal of Classification, 1984.\n[6] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A.-M. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. Web-scale information extraction in knowitall: (preliminary results). In WWW, 2004.\n[7] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yate. Unsupervised named-entity extraction from the web: An experimental study. In AI, 2005.\n[8] W. Gatterbauer, P. Bohunsky, M. Herzog, B. Kru\u0308pl, and B. Pollak. Towards domain-independent information extraction from web tables. In WWW, 2007.\n[9] R. Gupta and S. Sarawagi. Answering table augmentation queries from unstructured lists on the web. In VLDB, 2009.\n[10] R. Gupta and S. Sarawagi. Joint training for open-domain extraction on the web: exploiting overlap when supervision is limited. In WSDM, 2011.\n[11] M. A. Hearst. Automatic acquisition of hyponyms from large text corpora. In ACL, 1992.\n[12] J. Kamps, R. Kaptein, and M. Koolen. Using anchor text, spam filtering and wikipedia for web search and entity ranking. TREC, 2010.\n[13] Z. Kozareva and E. Hovy. A semi-supervised method to learn and construct taxonomies using the web. In EMNLP, 2010.\n[14] G. Limaye, S. Sarawagi, and S. Chakrabarti. Annotating and searching web tables using entities, types and relationships. PVLDB, 2010.\n[15] D. Lin and P. Pantel. Concept discovery from text. In COLING, 2002.\n[16] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to information retrieval. In Cambridge University Press, 2008.\n[17] P. Pantel and D. Ravichandran. Automatically labeling semantic classes. In HLT-NAACL, 2004.\n[18] A. Parameswaran, H. Garcia-Molina, and A. Rajaraman. Towards the web of concepts: Extracting concepts from large datasets. In VLDB, 2010.\n[19] E. Ramirez, R. Brena, D. Magatti, and F. Stella. Probabilistic metrics for soft-clustering and topic model validation. In Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010.\n[20] A. Ritter, S. Soderland, and O. Etzioni. What is this, anyway: Automatic hypernym discovery. In AAAI, 2009.\n[21] K. Shinzato and K. Torisawa. Acquiring hyponymy relations from web documents. In HLT-NAACL, 2004.\n[22] R. Snow, D. Jurafsky, and A. Y. Ng. Learning syntactic patterns for automatic hypernym discovery. In NIPS, 2004.\n[23] R. Snow, B. O\u2019Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast - but is it good? evaluating non-expert annotations for natural language tasks. In EMNLP, 2008.\n[24] P. P. Talukdar, J. Reisinger, M. Pas\u0327ca, D. Ravichandran, R. Bhagat, and F. Pereira. Weakly-supervised acquisition of labeled class instances using graph random walks. In EMNLP, 2008.\n[25] M. Tom. Nell: Never-ending language learning. http://rtw.ml.cmu.edu/rtw/.\n[26] B. Van Durme and M. Pasca. Finding cars, goddesses and enzymes: parametrizable acquisition of labeled instances for open-domain information extraction. In AAAI, 2008.\n[27] R. C. Wang and W. W. Cohen. Automatic set instance extraction using the web. In ACL, 2009.\n[28] R. C. Wang and W. W. Cohen. Character-level analysis of semi-structured documents for set expansion. In EMNLP, 2009.\n[29] R. Wetzker, C. Zimmermann, and C. Bauckhage. Analyzing social bookmarking systems: A del.icio.us cookbook. Mining Social Data (MSoDa) Workshop Proceedings, ECAI, 2008. http://www.dai-labor.de/ en/competence_centers/irml/datasets/.\n[30] A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broadhead, and S. Soderland. Textrunner: Open information extraction on the web. In NAACL, 2007."}], "references": [{"title": "Webtables: Exploring the power of tables on the web", "author": ["M.J. Cafarella", "E. Wu", "A. Halevy", "Y. Zhang", "D.Z. Wang"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Coupled semi-supervised learning for information extraction", "author": ["A. Carlson", "J. Betteridge", "R.C. Wang", "E.R. Hruschka", "Jr.", "T.M. Mitchell"], "venue": "In WSDM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Efficient algorithms for agglomerative hierarchical clustering methods", "author": ["W.H.E. Day", "H. Edelsbrunner"], "venue": "In Journal of Classification,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1984}, {"title": "Web-scale information extraction in knowitall: (preliminary results)", "author": ["O. Etzioni", "M. Cafarella", "D. Downey", "S. Kok", "A.-M. Popescu", "T. Shaked", "S. Soderland", "D.S. Weld", "A. Yates"], "venue": "In WWW,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Unsupervised named-entity extraction from the web: An experimental study", "author": ["O. Etzioni", "M. Cafarella", "D. Downey", "A.-M. Popescu", "T. Shaked", "S. Soderland", "D.S. Weld", "A. Yate"], "venue": "In AI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Towards domain-independent information extraction from web tables", "author": ["W. Gatterbauer", "P. Bohunsky", "M. Herzog", "B. Kr\u00fcpl", "B. Pollak"], "venue": "In WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Answering table augmentation queries from unstructured lists on the web", "author": ["R. Gupta", "S. Sarawagi"], "venue": "In VLDB,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Joint training for open-domain extraction on the web: exploiting overlap when supervision is limited", "author": ["R. Gupta", "S. Sarawagi"], "venue": "In WSDM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["M.A. Hearst"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Using anchor text, spam filtering and wikipedia for web search and entity", "author": ["J. Kamps", "R. Kaptein", "M. Koolen"], "venue": "ranking. TREC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "A semi-supervised method to learn and construct taxonomies using the web", "author": ["Z. Kozareva", "E. Hovy"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Annotating and searching web tables using entities, types and relationships", "author": ["G. Limaye", "S. Sarawagi", "S. Chakrabarti"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Concept discovery from text", "author": ["D. Lin", "P. Pantel"], "venue": "In COLING,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schtze"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Automatically labeling semantic classes", "author": ["P. Pantel", "D. Ravichandran"], "venue": "In HLT-NAACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Towards the web of concepts: Extracting concepts from large datasets", "author": ["A. Parameswaran", "H. Garcia-Molina", "A. Rajaraman"], "venue": "In VLDB,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Probabilistic metrics for soft-clustering and topic model validation", "author": ["E. Ramirez", "R. Brena", "D. Magatti", "F. Stella"], "venue": "In Web Intelligence and Intelligent Agent Technology (WI-IAT),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "What is this, anyway: Automatic hypernym discovery", "author": ["A. Ritter", "S. Soderland", "O. Etzioni"], "venue": "In AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Acquiring hyponymy relations from web documents", "author": ["K. Shinzato", "K. Torisawa"], "venue": "In HLT-NAACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["R. Snow", "D. Jurafsky", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Cheap and fast - but is it good? evaluating non-expert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In EMNLP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Weakly-supervised acquisition of labeled class instances using graph random walks", "author": ["P.P. Talukdar", "J. Reisinger", "M. Pa\u015fca", "D. Ravichandran", "R. Bhagat", "F. Pereira"], "venue": "In EMNLP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Finding cars, goddesses and enzymes: parametrizable acquisition of labeled instances for open-domain information extraction", "author": ["B. Van Durme", "M. Pasca"], "venue": "In AAAI,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Automatic set instance extraction using the web", "author": ["R.C. Wang", "W.W. Cohen"], "venue": "In ACL,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Character-level analysis of semi-structured documents for set expansion", "author": ["R.C. Wang", "W.W. Cohen"], "venue": "In EMNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Analyzing social bookmarking systems: A del.icio.us cookbook", "author": ["R. Wetzker", "C. Zimmermann", "C. Bauckhage"], "venue": "Mining Social Data (MSoDa) Workshop Proceedings,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Textrunner: Open information extraction on the web", "author": ["A. Yates", "M. Cafarella", "M. Banko", "O. Etzioni", "M. Broadhead", "S. Soderland"], "venue": "In NAACL,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}], "referenceMentions": [{"referenceID": 8, "context": "Hyponym patterns, sometimes called Hearst patterns [11], are surface patterns (like \u201cXs such as Y\u201d) indicating that X,Y are a concept-instance pair.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "Coordinate terms are most frequently detected by clustering terms based on distributional similarity [15]\u2014i.", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "Various techniques can be used to combine these coordinate-term and hyponym information to generate additional concept-instance pairs [26, 22].", "startOffset": 134, "endOffset": 142}, {"referenceID": 19, "context": "Various techniques can be used to combine these coordinate-term and hyponym information to generate additional concept-instance pairs [26, 22].", "startOffset": 134, "endOffset": 142}, {"referenceID": 22, "context": "We also present a new method for combining hyponym and coordinate-term information, and show that on table-rich corpora, this method improves on previously-published techniques [26], obtaining higher accuracy while generating nearly hundred times the number of concept-instance pairs.", "startOffset": 177, "endOffset": 181}, {"referenceID": 8, "context": "The experiments in this paper are conducted on several different HTML corpora and a collection of Hearst pattern [11] instances that have been extracted from ClueWeb09, all of which are made available for future researchers.", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "[8] focus on extracting tabular data from various kinds of pages that have table-like visual representations when rendered in a browser.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The WebTables [2] system extracted schema information from a huge corpus of 14.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "[9] focuses on the task of extending a table given a few seed rows.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Similarly, SEAL (Set Expansion for Any Language) [28] is a set expansion system which starts with a few seed examples and extends them using lists detected using character-based heuristics.", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "Gupta and Sarawagi [10] consider jointly training structured extraction models from overlapping web source (primarily in tables), thus avoiding the need for labeled data.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "[14] proposed a system to use an existing catalog and type hierarchy for annotating table columns and cells.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "These systems include KnowItAll [6, 7], ASIA [27], and Coupled Pattern Learning (CPL) [4].", "startOffset": 32, "endOffset": 38}, {"referenceID": 4, "context": "These systems include KnowItAll [6, 7], ASIA [27], and Coupled Pattern Learning (CPL) [4].", "startOffset": 32, "endOffset": 38}, {"referenceID": 23, "context": "These systems include KnowItAll [6, 7], ASIA [27], and Coupled Pattern Learning (CPL) [4].", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "These systems include KnowItAll [6, 7], ASIA [27], and Coupled Pattern Learning (CPL) [4].", "startOffset": 86, "endOffset": 89}, {"referenceID": 15, "context": "[18] propose a concept extraction algorithm which can identify a canonical form of a concept, filtering out sub-concepts or superconcepts; and Ritter et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] describe a scheme for filtering concept-instance pairs, using a SVM classifier which uses as features frequency statistics for several Hearst patterns on a large corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] learns \u201cdependency path\u201d features, which can further expand the set of conceptinstance pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "TextRunner [30] is an open-domain IE system which makes a single pass over the corpus of unstructured text and extracts a large set of relational tuples, without requiring any human input.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "Pantel and Ravichandran [17] proposes a method to automatically label distributional term clusters using Hearst-like patterns, and Van Durme and Pasca [26] proposed an alternative approach method to extract labeled classes of instances from unstructured text.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "Pantel and Ravichandran [17] proposes a method to automatically label distributional term clusters using Hearst-like patterns, and Van Durme and Pasca [26] proposed an alternative approach method to extract labeled classes of instances from unstructured text.", "startOffset": 151, "endOffset": 155}, {"referenceID": 21, "context": "[24] proposed a graph random walk based semi-supervised label propagation technique for open domain class instance extractions, which extends the system of Van Durme and Pasca by using table data (from WebTables) as well as free-text distributional clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Coupled SEAL (CSEAL) [4] use of mutual exclusion, containment and type checking relationships to extend SEAL, and CPL [4] and CSEAL are the two components of NELL [25], a multi-strategy semi-supervised learning system.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Coupled SEAL (CSEAL) [4] use of mutual exclusion, containment and type checking relationships to extend SEAL, and CPL [4] and CSEAL are the two components of NELL [25], a multi-strategy semi-supervised learning system.", "startOffset": 118, "endOffset": 121}, {"referenceID": 18, "context": "Shinzato and Torisawa [21] showed that coordinate terms can be extracted from itemizations in structured web documents.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "[8] can provide more input data to learn sets from.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Non-parametric algorithms like agglomerative clustering [5] fit most of our requirements.", "startOffset": 56, "endOffset": 59}, {"referenceID": 23, "context": "These correspond to a subset of the Hearst patterns used in ASIA [27] together with some \u201cdoubly anchored\u201d versions of these patterns [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "These correspond to a subset of the Hearst patterns used in ASIA [27] together with some \u201cdoubly anchored\u201d versions of these patterns [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "Our method scores labels differently from Van Durme and Pasca method [26] It is also different in the sense that they output a concept-instance pair < x, y > only when < x, y > appears in the set of candidate concept-instance pairs, whereas we extend the labels to the whole cluster.", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "Delicious Sports: This dataset is a subset of DAILabor Delicious corpus [29], created by taking only those URLs which are tagged as \u201csports\u201d.", "startOffset": 72, "endOffset": 76}, {"referenceID": 25, "context": "Delicious Music: This dataset is a subset of DAILabor Delicious corpus [29], created by taking only those URLs which are tagged as \u201cmusic\u201d.", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "For this purpose we used the Fusion spam scores[12] provided by Waterloo university and used pages with spam-rank score higher than 60%.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "We compare the clustering algorithms in terms of commonly used clustering metrics: cluster purity (Purity), normalized mutual information (NMI), rand index (RI) [16] and Fowlkes-Mallows index (FM) which are defined as follows:", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "We compare our hypernym recommendation technique with Van Durme and Pasca technique [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "The objective evaluation of the kind \u201cwhether X belongs to category Y\u201d is done using Amazon Mechanical Turk [23].", "startOffset": 108, "endOffset": 112}, {"referenceID": 22, "context": "We also presented a new method for combining candidate concept-instance pairs and coordinate-term clusters, and showed that on table-rich corpora, this method improved on Van Durme and Pasca method [26].", "startOffset": 198, "endOffset": 202}], "year": 2013, "abstractText": "We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and conceptinstance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs.", "creator": "LaTeX with hyperref package"}}}