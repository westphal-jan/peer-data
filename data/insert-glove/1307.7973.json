{"id": "1307.7973", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jul-2013", "title": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction", "abstract": "koseki This paper proposes kakar a m\u00e1scara novel approach 1.032 for geekiness relation extraction layered from cerbaill free slote text which yaroslavl is trained mirnawan to gerede jointly use dehaven information 4,000-kilometer from the 3,447 text and from existing poya knowledge. Our 60,000-70 model is 80.30 based jellicle on wust two scoring jan/feb functions railbird that indology operate by learning hort low - dimensional embeddings of words and of entities and tecmo relationships from tamara a knowledge base. We empirically lambiel show bbw on New agbayani York progressives Times articles walkovers aligned with Freebase relations coxe that reublin our approach paregoric is pashaluk able kadans to 3,079 efficiently use the esses extra cubensis information provided chakiris by a twenty-six large subset wols of cybi Freebase data (slimline 4M entities, 23k octuplets relationships) wily to kdwb improve mozersky over 400,000-dollar existing methods rosaline that rely enniss on text davydova features hohenberg alone.", "histories": [["v1", "Tue, 30 Jul 2013 13:37:09 GMT  (78kb)", "http://arxiv.org/abs/1307.7973v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["jason weston", "antoine bordes", "oksana yakhnenko", "nicolas usunier"], "accepted": true, "id": "1307.7973"}, "pdf": {"name": "1307.7973.pdf", "metadata": {"source": "CRF", "title": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction", "authors": ["Jason Weston", "Oksana Yakhnenko", "Nicolas Usunier"], "emails": ["jweston@google.com", "bordesan@utc.fr", "oksana@google.com", "usuniern@utc.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n79 73\nv1 [\ncs .C\nL ]\n3 0\nJu l 2"}, {"heading": "1 Introduction", "text": "Information extraction (IE) aims at generating structured data from free text in order to populate Knowledge Bases (KBs). Hence, one is given an incomplete KB composed of a set of triples of the form (h, r , t); h is the left-hand side entity (or head), t the right-hand side entity (or tail) and r the relationship linking them. An example from the Freebase KB1 is (/m/2d3rf ,<director-of>, /m/3/324), where /m/2d3rf refers to the director \u201cAlfred Hitchcock\" and /m/3/324 to the movie \u201cThe Birds\".\nThis paper focuses on the problem of learning to perform relation extraction (RE) under weak supervision from a KB. RE is sub-task of IE that considers that entities have already been detected by a different process, such as a named-entity recognizer. RE then aims at assigning to a relation mention m\n1www.freebase.com\n(i.e. a sequence of text which states that some relation is true) the corresponding relationship from the KB, given a pair of extracted entities (h, t) as context. For example, given the triplet (/m/2d3rf ,\u201cwrote and directed\", /m/3/324), a system should predict <director-of>. The task is said to be weakly supervised because for each pair of entities (h, t) detected in the text, all relation mentions m associated with them are labeled with all the relationships connecting h and t in the KB, whether they are actually expressed by m or not.\nOur key contribution is a novel model that employs not only weakly labeled text mention data, as most approaches do, but also leverages triples from the known KB. The model thus learns the plausibility of new (h, r , t) triples by generalizing from the KB, even though this triple is not present. A ranking-based embedding framework is used to train such a model. Thereby, relation mentions, entities and relationships are all embedded into a common low-dimensional vector space, where scores are computed. We show that our system can successfully take into account information from a largescale KB (Freebase: 4M entities, 23k relationships) to improve over existing systems, which are only using text features.\nPrevious work Learning under weak supervision is common in Natural language processing, especially for tasks where the annotations costs are important such as semantic parsing (Kate and Mooney, 2007; Liang et al., 2009; Bordes et al., 2010; Matuszek et al., 2012). This is also naturally used in IE, since it allows to train large-scale systems without requiring to la-\nbel numerous texts. The idea was introduced by (Craven et al., 1999), which matched the Yeast Protein Database with PubMed abstracts. It was also used to train open extractors based on Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010). Large-scale open IE projects (Banko et al., 2007; Carlson et al., 2010) also rely on weak supervision, since they learn models from a seed KB in order to extend it.\nWeak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multiinstance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these works only use textual information to perform extraction.\nRecently, Riedel et al. (2013) proposed an approach to model jointly KB data and text by relying on collaborative filtering. Unlike our model, this method can not directly connect text mentions and KB relationships, but does it indirectly through joint learning of shared embeddings for entities in text and in the KB. We did not compare with this recent approach, since it uses a different evaluation protocol than previous work in RE."}, {"heading": "2 Embedding-based Framework", "text": "Our work concerns energy-based methods, which learn low-dimensional vector representations (embeddings) of atomic symbols (words, entities, relationships, etc.). In this framework, we learn two models: one for predicting relationships given relation mentions and another one to encode the interactions among entities and relationships from the KB. The joint action of both models in prediction allows us to use the connection between the KB and text to perform relation extraction. One could also share parameters between models (via shared embeddings), but this is not implemented in this work. This approach is inspired by previous work designed to connect words and Wordnet (Bordes et al., 2012).\nBoth submodels end up learning vector embeddings of symbols, either for entities or relationships in the KB, or for each word/feature of the vocabulary (denoted V). The set of entities and relationships in the KB are denoted by E and R, and nv , ne and nr\ndenote the size of V , E and R respectively. Given a triplet (h, r , t) the embeddings of the entities and the relationship (vectors in Rk ) are denoted with the same letter, in boldface characters (i.e. h, r, t)."}, {"heading": "2.1 Connecting text and relationships", "text": "The first part of the framework concerns the learning of a function Sm2r (m, r), based on embeddings, that is designed to score the similarity of a relation mention m and a relationship r .\nOur approach is inspired by previous work for connecting word labels and images (Weston et al., 2010), which we adapted, replacing images by mentions and word labels by relationships. Intuitively, it consists of first projecting windows of words into the embedding space and then computing a similarity measure (the dot product in this paper) between this projection and a relationship embedding. The scoring function is then:\nSm2r (m, r) = f(m) \u22a4 r\nwith f a function mapping a window of words into R k , f(m) = W\u22a4\u03a6(m); W is the matrix of Rnv\u00d7k containing all word embeddings w; \u03a6(m) is the (sparse) binary representation of m (\u2208 Rnv ) and r \u2208 Rk is the embedding of the relationship r .\nThis approach can be easily applied at test time to score (mention, relationship) pairs. Since this learning problem is weakly supervised, Bordes et al. (2010) showed that a convenient way to train it is by using a ranking loss. Hence, given a data set D = {(mi , ri ), i = 1, ... , |D|} consisting of (mention, relationship) training pairs, one could learn the embeddings using constraints of the form:\n\u2200i , \u2200r \u2032 6= ri , f(mi ) \u22a4 ri > 1 + f(mi ) \u22a4 r \u2032 , (1)\nwhere 1 is the margin. Given any mention m one can predict the corresponding relationship r\u0302(m) with:\nr\u0302(m) = arg max r \u2032\u2208R\nSm2r (m, r \u2032) = arg max\nr \u2032\u2208R\n( f(m)\u22a4r\u2032 ) .\nLearning Sm2r (\u00b7) under constraints (1) is well suited when one is interested in building a a permention prediction system. However, performance metrics of relation extraction are sometimes measured using precision recall curves aggregated for all mentions concerning the same pair of entities,\nas in (Riedel et al., 2010). In that case the scores across predictions for different mentions need to be calibrated so that the most confident ones have the higher scores. This can be better encoded with constraints of the following form:\n\u2200i , j , \u2200r \u2032 6= ri , f(mi ) \u22a4 ri > 1 + f(mj) \u22a4 r \u2032 .\nIn this setup, scores of pairs observed in the training set should be larger than that of any other prediction across all mentions. In practice, we use \u201csoft\u201d ranking constraints (optimizing the hinge loss), and enforce a (hard) constraint on the norms of the columns of W and r, i.e. \u2200i , ||Wi ||2 \u2264 1 and \u2200j , ||rj ||2 \u2264 1. Training is carried out by stochastic gradient descent (SGD), updating W and r at each step. See (Weston et al., 2010; Bordes et al., 2013) for details."}, {"heading": "2.2 Encoding structured data of KBs", "text": "Using only weakly labeled text mentions for training ignores much of the prior knowledge we can leverage from a large KB such as Freebase. In order to connect this relational data with our model, we propose to encode its information into entity and relationship embeddings. This allows us to build a model which can score the plausibility of new entity relationship triples which are missing from Freebase. Several models have been recently developed for that purpose (e.g. in (Nickel et al., 2011; Bordes et al., 2011; Bordes et al., 2012)): we chose in this work to use the approach of (Bordes et al., 2013), which is simple, flexible and has shown very promising results on Freebase data.\nGiven a training set S = {(hi , ri , ti ), i = 1, ... , |S|} of relations extracted from the KB, this model learns vector embeddings of the entities and of the relationships using the idea that the functional relation induced by the r -labeled arcs of the KB should correspond to a translation of the embeddings. Hence, this method enforces that h + r \u2248 t when (h, r , t) holds, while h+ r should be far away from t otherwise. Hence such a model gives the following score for the plausibility of a relation:\nSkb(h, r , t) = \u2212||h+ r\u2212 t|| 2 2 .\nA ranking loss is also used for training Skb. The ranking objective is designed to assign higher scores\nto existing relations versus any other possibility:\n\u2200i , \u2200h\u2032 6= hi , Skb(hi , ri , ti ) \u2265 1 + Skb(h \u2032, ri , ti), \u2200i , \u2200r \u2032 6= ri , Skb(hi , ri , ti ) \u2265 1 + Skb(hi , r \u2032, ti ), \u2200i , \u2200t \u2032 6= ti , Skb(hi , ri , ti ) \u2265 1 + Skb(hi , ri , t \u2032).\nAs in section 2.1 we use soft constraints, enforce constraints on the norm of embeddings, i.e. \u2200h,r ,t , ||h||2 \u2264 1, ||r ||2 \u2264 1, ||t||2 \u2264 1, and training is performed using SGD, as in (Bordes et al., 2013).\nAt test time, one may again need to calibrate the scores Skb across entity pairs. We propose a simple approach: we convert the scores by ranking all relationships R by Skb and instead output:\nS\u0303kb(h, r , t) = \u03a6 (\n\u2211\nr \u2032 6=r\n\u03b4(Skb(h, r , t) > Skb(h, r \u2032, t))\n)\n,\ni.e. a function of the rank of r . We chose the simplified model \u03a6(x) = 1 if x < t and 0 otherwise."}, {"heading": "2.3 Implementation for relation extraction", "text": "Our framework can be used for relation extraction in the following way. First, for each pair of entities (h, t) that appear in the test set, all the corresponding mentions Mh,t in the test set are collected and a prediction is performed with:\nr\u0302h,t = argmax r\u2208R\n\u2211\nm\u2208Mh,t\nSm2r (m, r) .\nThe predicted relationship can either be a valid relationship or NA \u2013 a marker that means that there is no relation between h and t (NA is added to R during training and is treated like other relationships). If r\u0302h,t is a relationship, a composite score is defined:\nSm2r+kb(h, r\u0302h,t , t)= \u2211\nm\u2208Mh,t\nSm2r (m, r\u0302h,t)+S\u0303kb(h, r\u0302h,t , t)\nHence, the composite model favors predictions that agree with both the mentions and the KB. If r\u0302h,t is NA, the score is unchanged."}, {"heading": "3 Experiments", "text": "We use the training and test data, evaluation framework and baselines from (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).\nNYT+FB This dataset, developed by (Riedel et al., 2010), aligns Freebase relations with the New York Times corpus. Entities were found using the Stanford named entity tagger (Finkel et al., 2005), and were matched to their name in Freebase. For each mention, sentence level features are extracted which include part of speech, named entity and dependency tree path properties. Unlike some of the previous methods, we do not use features that aggregate properties across multiple mentions. We kept the 100,000 most frequent features.There are 52 possible relationships and 121,034 training mentions of which most are labeled as no relation (labeled \u201cNA\u201d) \u2013 there are 4700 Freebase relations mentioned in the training set, and 1950 in the test set.\nFreebase Freebase is a large-scale KB that has around 80M entities, 23k relationships and 1.2B relations. We used a subset restricted to the top 4M entities (with the largest number of relations in a preprocessed subset) for scalability reasons. We used all the 23k relationships. To make a realistic setting, we did not choose the entity set using the NYT+FB data set, so it may not overlap completely. For that reason, we needed to keep the set rather large. Keeping the top 4M entities gives an overlap of 80% with the entities in the NYT+FB test set. Most importantly, we then removed all the entity pairs present in the NYT+FB test set from Freebase, i.e. all relations they are involved in independent of the relationship. This ensures that we cannot just memorize the true relations for an entity pair \u2013 we have to learn to generalize from other entities and relations.\nAs the NYT+FB dataset was built on an earlier version of Freebase we also had to translate the deprecated relationships into their new variants (e.g. \u201c/p/business/company/place_founded \u201d \u2192 \u201c/organization/organization/place_founded\u201d) to make the two datasets link (the 52 relationships in NYT+FB are now a subset of the 23k from Freebase). We then trained the Skb model on the remaining triples.\nModeling Following (Bordes et al., 2013) we set the embedding dimension k to 50. The learning rate for SGD was selected using a validation set: we obtained 0.001 for Sm2r , and 0.1 for Skb. For the calibration of S\u0302kb, t = 10 (note, here we are ranking all 23k Freebase relationships). Training Sm2r took\n5 minutes, whilst training Skb took 2 days due to the large scale of the data set.\nResults Figure 1 displays the aggregate precision / recall curves of our approach WSABIEM2R+FB which uses the combination of Sm2r+Skb, as well as WSABIEM2R , which only uses Sm2r , and state-ofthe-art: HOFFMANN (Hoffmann et al., 2011)2, MIMLRE (Surdeanu et al., 2012). RIEDEL (Riedel et al., 2010) and MINTZ (Mintz et al., 2009).\nWSABIEM2R is comparable to, but slightly worse than, the MIMLRE and HOFFMANN methods, possibly due to its simplified assumptions (e.g. predicting a single relationship per entity pair). However, the addition of extra knowledge from other Freebase\n2There is an error in the plot from (Hoffmann et al., 2011), which we have corrected. The authors acknowledged the issue.\nentities in WSABIEM2R+FB provides superior performance to all other methods, by a wide margin, at least between 0 and 0.1 recall (see bottom plot)."}, {"heading": "4 Conclusion", "text": "In this paper we described a framework for leveraging large scale knowledge bases to improve relation extraction by training not only on (mention, relationship) pairs but using all other KB triples as well. Our modeling approach is general and should apply to other settings, e.g. for the task of entity linking."}], "references": [{"title": "Open information extraction from the web", "author": ["Banko et al.2007] Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni"], "venue": "In IJCAI,", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Label ranking under ambiguous supervision for learning semantic correspondences", "author": ["Nicolas Usunier", "Jason Weston"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Bordes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2010}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proc. of the 25th Conf. on Artif. Intel. (AAAI)", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "In Proc. of the 15th Intern. Conf. on Artif. Intel. and Stat.,", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Irreflexive and hierarchical relations as translations", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "arXiv preprint arXiv:1304.7158", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Craven et al.1999] Mark Craven", "Johan Kumlien"], "venue": "In ISMB,", "citeRegEx": "Craven and Kumlien,? \\Q1999\\E", "shortCiteRegEx": "Craven and Kumlien", "year": 1999}, {"title": "Incorporating nonlocal information into information extraction systems by gibbs sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguis-", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of the 49th Annual Meeting", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Learning language semantics from ambiguous supervision", "author": ["Kate", "Mooney2007] Rohit J Kate", "Raymond J Mooney"], "venue": "In AAAI,", "citeRegEx": "Kate et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kate et al\\.", "year": 2007}, {"title": "Learning semantic correspondences with less supervision", "author": ["Liang et al.2009] Percy Liang", "Michael I Jordan", "Dan Klein"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan-", "citeRegEx": "Liang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox"], "venue": "In Proceedings of the International Conference on Machine Learning", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M Marlin"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Weston et al.2010] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine learning,", "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}, {"title": "Autonomously semantifying wikipedia", "author": ["Wu", "Weld2007] Fei Wu", "Daniel S Weld"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2007}, {"title": "Open information extraction using wikipedia", "author": ["Wu", "Weld2010] Fei Wu", "Daniel S Weld"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Previous work Learning under weak supervision is common in Natural language processing, especially for tasks where the annotations costs are important such as semantic parsing (Kate and Mooney, 2007; Liang et al., 2009; Bordes et al., 2010; Matuszek et al., 2012).", "startOffset": 176, "endOffset": 263}, {"referenceID": 1, "context": "Previous work Learning under weak supervision is common in Natural language processing, especially for tasks where the annotations costs are important such as semantic parsing (Kate and Mooney, 2007; Liang et al., 2009; Bordes et al., 2010; Matuszek et al., 2012).", "startOffset": 176, "endOffset": 263}, {"referenceID": 11, "context": "Previous work Learning under weak supervision is common in Natural language processing, especially for tasks where the annotations costs are important such as semantic parsing (Kate and Mooney, 2007; Liang et al., 2009; Bordes et al., 2010; Matuszek et al., 2012).", "startOffset": 176, "endOffset": 263}, {"referenceID": 0, "context": "Large-scale open IE projects (Banko et al., 2007; Carlson et al., 2010) also rely on weak supervision, since they learn models from a seed KB in order to extend it.", "startOffset": 29, "endOffset": 71}, {"referenceID": 5, "context": "Large-scale open IE projects (Banko et al., 2007; Carlson et al., 2010) also rely on weak supervision, since they learn models from a seed KB in order to extend it.", "startOffset": 29, "endOffset": 71}, {"referenceID": 14, "context": "(2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multiinstance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 149, "endOffset": 216}, {"referenceID": 8, "context": "(2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multiinstance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 149, "endOffset": 216}, {"referenceID": 16, "context": "(2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multiinstance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 149, "endOffset": 216}, {"referenceID": 0, "context": "Large-scale open IE projects (Banko et al., 2007; Carlson et al., 2010) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multiinstance learning frameworks (Riedel et al.", "startOffset": 30, "endOffset": 235}, {"referenceID": 0, "context": "Large-scale open IE projects (Banko et al., 2007; Carlson et al., 2010) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multiinstance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these works only use textual information to perform extraction. Recently, Riedel et al. (2013) proposed an approach to model jointly KB data and text by relying on collaborative filtering.", "startOffset": 30, "endOffset": 545}, {"referenceID": 3, "context": "This approach is inspired by previous work designed to connect words and Wordnet (Bordes et al., 2012).", "startOffset": 81, "endOffset": 102}, {"referenceID": 17, "context": "Our approach is inspired by previous work for connecting word labels and images (Weston et al., 2010), which we adapted, replacing images by mentions and word labels by relationships.", "startOffset": 80, "endOffset": 101}, {"referenceID": 1, "context": "Since this learning problem is weakly supervised, Bordes et al. (2010) showed that a convenient way to train it is by using a ranking loss.", "startOffset": 50, "endOffset": 71}, {"referenceID": 14, "context": "as in (Riedel et al., 2010).", "startOffset": 6, "endOffset": 27}, {"referenceID": 17, "context": "See (Weston et al., 2010; Bordes et al., 2013) for details.", "startOffset": 4, "endOffset": 46}, {"referenceID": 4, "context": "See (Weston et al., 2010; Bordes et al., 2013) for details.", "startOffset": 4, "endOffset": 46}, {"referenceID": 13, "context": "in (Nickel et al., 2011; Bordes et al., 2011; Bordes et al., 2012)): we chose in this work to use the approach of (Bordes et al.", "startOffset": 3, "endOffset": 66}, {"referenceID": 2, "context": "in (Nickel et al., 2011; Bordes et al., 2011; Bordes et al., 2012)): we chose in this work to use the approach of (Bordes et al.", "startOffset": 3, "endOffset": 66}, {"referenceID": 3, "context": "in (Nickel et al., 2011; Bordes et al., 2011; Bordes et al., 2012)): we chose in this work to use the approach of (Bordes et al.", "startOffset": 3, "endOffset": 66}, {"referenceID": 4, "context": ", 2012)): we chose in this work to use the approach of (Bordes et al., 2013), which is simple, flexible and has shown very promising results on Freebase data.", "startOffset": 55, "endOffset": 76}, {"referenceID": 4, "context": "\u2200h,r ,t , ||h||2 \u2264 1, ||r ||2 \u2264 1, ||t||2 \u2264 1, and training is performed using SGD, as in (Bordes et al., 2013).", "startOffset": 90, "endOffset": 111}, {"referenceID": 14, "context": "We use the training and test data, evaluation framework and baselines from (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 75, "endOffset": 142}, {"referenceID": 8, "context": "We use the training and test data, evaluation framework and baselines from (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 75, "endOffset": 142}, {"referenceID": 16, "context": "We use the training and test data, evaluation framework and baselines from (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 75, "endOffset": 142}, {"referenceID": 14, "context": "NYT+FB This dataset, developed by (Riedel et al., 2010), aligns Freebase relations with the New York Times corpus.", "startOffset": 34, "endOffset": 55}, {"referenceID": 7, "context": "Entities were found using the Stanford named entity tagger (Finkel et al., 2005), and were matched to their name in Freebase.", "startOffset": 59, "endOffset": 80}, {"referenceID": 4, "context": "Modeling Following (Bordes et al., 2013) we set the embedding dimension k to 50.", "startOffset": 19, "endOffset": 40}, {"referenceID": 8, "context": "Results Figure 1 displays the aggregate precision / recall curves of our approach WSABIEM2R+FB which uses the combination of Sm2r+Skb, as well as WSABIEM2R , which only uses Sm2r , and state-ofthe-art: HOFFMANN (Hoffmann et al., 2011)2,", "startOffset": 211, "endOffset": 234}, {"referenceID": 16, "context": "MIMLRE (Surdeanu et al., 2012).", "startOffset": 7, "endOffset": 30}, {"referenceID": 14, "context": "RIEDEL (Riedel et al., 2010) and MINTZ (Mintz et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 12, "context": ", 2010) and MINTZ (Mintz et al., 2009).", "startOffset": 18, "endOffset": 38}, {"referenceID": 8, "context": "There is an error in the plot from (Hoffmann et al., 2011), which we have corrected.", "startOffset": 35, "endOffset": 58}], "year": 2013, "abstractText": "This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone.", "creator": "LaTeX with hyperref package"}}}