{"id": "1309.1973", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2013", "title": "Regret-Based Multi-Agent Coordination with Uncertain Task Rewards", "abstract": "1300gmt Many suwankhiri multi - chardonnays agent coordination nikes problems arseneault can be represented as DCOPs. k-lite Motivated by bakala task killbuck allocation headcorn in disaster response, we extend 1,504 standard DCOP models growers to desoer consider uncertain task gray-green rewards where the outcome 158.4 of kanematsu completing a knut task depends on 119.4 its current trifluoride state, which jing is strongarm randomly bronchodilator drawn from unknown distributions. midrace The goal of solving this ciotti problem texte is to devasahayam find fcx a balotelli solution for all mordru agents sugizo that others minimizes kosten the ascheberg overall worst - motionplus case euro990 loss. This institutional is a challenging problem for 4.20 centralized algorithms poon because the search wendat space grows exponentially fidayeen with the hudson number maheswari of jugonzalez agents and homeopathy is nontrivial uro\u0161 for standard greenshank DCOP ferland algorithms we humidities have. lepa To patate address 2003-2006 this, we qnx propose rensing a 150-bed novel decentralized algorithm tadley that time-scales incorporates picketing Max - Sum with pro-roman iterative lengthy constraint generation to solve fabregas the 4,050 problem hammill by passing 48-year-old messages among landon agents. By 1311 so axmann doing, s\u00fcdtirol our photographers approach scales foping well last-16 and can solve instances 80.0 of imagineers the egar task entendre allocation problem kohen with verraszto hundreds elkanah of agents glidrose and aberfan tasks.", "histories": [["v1", "Sun, 8 Sep 2013 16:20:06 GMT  (114kb,D)", "http://arxiv.org/abs/1309.1973v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["feng wu", "nicholas r jennings"], "accepted": true, "id": "1309.1973"}, "pdf": {"name": "1309.1973.pdf", "metadata": {"source": "CRF", "title": "Regret-Based Multi-Agent Coordination with Uncertain Task Rewards", "authors": ["Feng Wu", "Nicholas R. Jennings"], "emails": ["nrj}@ecs.soton.ac.uk"], "sections": [{"heading": "Introduction", "text": "Distributed constraint optimization problems (DCOPs) are a popular representation for many multi-agent coordination problems. In this model, agents are represented as decision variables and the tasks that they can be assigned are variable domains. The synergies between agents\u2019 joint assignment are specified as constraint values. Now, some tasks may require a subgroup of the team to work together, either because a single agent has insufficient capabilities to complete the task or the teamwork can substantially improve performance. In either case, the constraints are the utilities of the agents\u2019 joint behaviors. Once the DCOP model of the problem is obtained, we can solve it efficiently using optimal approaches such as ADOPT (Modi et al. 2005) and DPOP (Petcu & Faltings 2005) or approximate approaches such as DSA (Zhang et al. 2005), MGM (Maheswaran et al. 2004), and Max-Sum (Farinelli et al. 2008).\nIn DCOPs, the task rewards are often assumed to be completely known to the agents. However, this can make it difficult to model problems where the reward of completing a task depends on the task state, which is usually unobservable and uncontrollable by the agents. For example, in disaster response, a group of robots may be sent out to an unknown area to search for survivors. However, the success of the search tasks (task rewards) will depend on many fac-\ntors (task states) such as the local terrain, the weather condition, and the degree of damage. Initially, the robots may have very limited information about the task states, but must act quickly because time is critical for saving lives. In such cases, it is desirable to reason about the uncertainty of the task states (rewards) and assign the tasks to the agents in such a way that the worst-case loss 1 (compared to the unknown optimal solution) is minimized. The aim of this is to perform as closely as possible to the optimal solution given the uncertain task rewards (caused by unknown task states).\nOver recent years, a significant body of research has dealt with extending standard DCOPs to models with uncertainty. A common method is to introduce additional random variables (uncontrollable by the agents) to the constraint functions (Le\u0301aute\u0301 & Faltings 2011). Another way to reason about the uncertainty is to randomly select a constraint function from a predefined function set (Atlas & Decker 2010; Stranders et al. 2011; Nguyen et al. 2012). However, all the aforementioned approaches require the probability distributions of the random variables to be known (Atlas & Decker 2010; Le\u0301aute\u0301 & Faltings 2011) or the candidate functions to have certain properties (e.g., be Gaussian (Stranders et al. 2011) or concave (Nguyen et al. 2012)). Unfortunately, these assumptions are not common in our motivating domain because the agents have no or very limited information about the tasks as they start to respond to the crisis. Thus, the key challenge in our domain is to find a good solution (as close to the optimal as possible) given no or partial information about the associated task states (linked to the rewards).\nTo this end, we introduce a new model for multi-agent coordination with uncertain task rewards and propose an efficient algorithm for computing the robust solution (minimizing the worst-case loss) of this model. Our model, called uncertain reward DCOP (UR-DCOP), extends the standard DCOP to include random variables (task states), one for each constraint (task). We assume these random variables are independent from each other (the tasks are independent) and uncontrollable by the agents (e.g., the weather condition). Furthermore, we assume the choice of these variables are drawn from finite domains with unknown distributions. For each such variable, we define a belief as a probability\n1Here, we focus on the worst-cast loss but other robust optimization criterions (e.g., the maximin model) could be applied.\nar X\niv :1\n30 9.\n19 73\nv1 [\ncs .A\nI] 8\nS ep\n2 01\n3\ndistribution over its domain. Thus, minimizing the worstcase loss is equivalent to computing the minimax regret solution in the joint belief space. Intuitively, this process can be viewed as a game between the agents and nature where the agents select a solution to minimize the loss, while nature chooses a belief in the space to maximize it.\nFor large UR-DCOPs, it is intractable for a centralized solver to compute the minimax regret solution for all the agents due to the huge joint belief and solution space. Thus, we turn to decentralized approaches because they can exploit the interaction structure and distribute the computation locally to each agent. However, it is challenging to compute the minimax regret in a decentralized manner because intuitively all the agents need to find the worst case (a point in the belief space) before they can minimize the loss. To address this, we borrow ideas from iterative constraint generation, first introduced by (Benders 1962) and recently adopted by (Regan & Boutilier 2010; Regan & Boutilier 2011) for solving imprecise MDPs. Similar to their approaches, we decompose the overall problem into a master problem and a subproblem that are iteratively solved until they converge. The main contribution of our work lies in the development of two variations of Max-Sum (Farinelli et al. 2008) to solve the master and subproblems by passing messages among the agents. We adopt Max-Sum due to its performance and stability on large problems (i.e., hundreds of agents). For acyclic factor graphs, we prove our algorithm is optimal. In experiments, we show that our method can scale up to task allocation domains with hundreds of agents and tasks (intractable for centralized approaches) and can outperform the state-of-the-art decentralized approach by having much lower average regrets.\nThe reminder of the paper is organized as follows. First, we start with the background and introduce our UR-DCOP model. Then, we propose our algorithm and analyze its main properties. After that, we present our empirical results and conclude the paper with future work."}, {"heading": "The UR-DCOP Model", "text": "Formally, a distributed constraint optimization problem (DCOP) can be defined as a tuple M = \u3008I,X ,D,U\u3009, where:\n\u2022 I = {1, \u00b7 \u00b7 \u00b7 , n} is a set of agents indexed by 1, 2, \u00b7 \u00b7 \u00b7 , n;\n\u2022 X = {x1, \u00b7 \u00b7 \u00b7 , xn} is a set of decision variables where xi denotes the variable controlled by agent i;\n\u2022 D = {D1, \u00b7 \u00b7 \u00b7 , Dn} is a set of finite domains for the decision variables where domainDi is a set of possible values for decision variable xi;\n\u2022 U = {U1, \u00b7 \u00b7 \u00b7 , Um} is a set of soft constraints where each constraint Uj : Dj1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Djk \u2192 < defines the value of possible assignments to subsets of decision variables where Uj(xj1, \u00b7 \u00b7 \u00b7 , xjk) is the value function for variables xj1, \u00b7 \u00b7 \u00b7 , xjk \u2208 X .\nThe goal of solving a DCOP is to find an assignment x\u2217 of values in the domains of all decision variables xi \u2208 X that\nmaximizes the sum of all constraints:\nx\u2217 = argmax x m\u2211 j=1 Uj(xj1, \u00b7 \u00b7 \u00b7 , xjk) (1)\nTurning to Max-Sum, this is a decentralized messagepassing optimization approach for solving large DCOPs. To use Max-Sum, a DCOP needs to be encoded as a special bipartite graph, called a factor graph, where vertices represent variables xi and functions Uj , and edges the dependencies between them. Specifically, it defines two types of messages that are exchanged between variables and functions: \u2022 From variable xi to function Uj :\nqi\u2192j(xi) = \u03b1i\u2192j + \u2211\nk\u2208M(i)\\j\nrk\u2192i(xi) (2)\nwhere M(i) denotes the set of indices of the function nodes connected to variable xi and \u03b1i\u2192j is a scaler chosen such that \u2211 xi\u2208Di qi\u2192j(xi) = 0.\n\u2022 From function Uj to variable xi:\nrj\u2192i(xi) = max xj\\xi Uj(xj) + \u2211 k\u2208N(j)\\i qk\u2192j(xk)  (3) where N(j) denotes the set of indices of the variable nodes connected to Uj and xj is a variable vector \u3008xj1, \u00b7 \u00b7 \u00b7 , xjk\u3009.\nNotice that both qi\u2192j(xi) and rj\u2192i(xi) are scalar functions of variable xi \u2208 Di. Thus, the marginal function of each variable xi can be calculated by:\nzi(xi) = \u2211\nj\u2208M(i)\nrj\u2192i(xi) \u2248 max x\\xi m\u2211 j=1 Uj(xj) (4)\nafter which the assignment of xi can be selected by:\nx\u2217i = argmax xi\u2208Di zi(xi) (5)\nFrom this background, we now turn to the UR-DCOP model itself. In particular, our work is mainly motivated by the task allocation problem in disaster response scenarios 2, where a group of first responders need to be assigned to a set of tasks in order to maximize saved lives. This problem can be straightforwardly modeled as a DCOP where: I is a set of first responders, xi is the task assigned to responder i, Di is a set of tasks that can be performed by responder i, and Uj is the reward for the completion of task j. However, in our domains, the value of Uj does not only depend on the joint choice of the agents, but also on the uncontrollable events such as fires, hurricanes, floods, or debris flows in the disaster area. These events can be formally abstracted as task states, which are usually unknown to the first responders, but critical for the team performance. To model this, we introduce UR-DCOP \u2014 a new representation for multi-agent coordination with uncertain task rewards.\nIn more detail, UR-DCOP is an extension of the original DCOP model with two additional components:\n2Nevertheless, our results are broadly applicable to other domains that have uncertainty in task rewards.\n\u2022 E = {s1, \u00b7 \u00b7 \u00b7 , sm} is a set of random variables modeling uncontrollable stochastic events, e.g., fires in a building or weather in a disaster area, for each constraint Uj \u2208 U ;\n\u2022 S = {S1, \u00b7 \u00b7 \u00b7 , Sm} is a set of finite domains, e.g., levels of the fire damage or different weather conditions, for each random variable sj \u2208 Sj ;\nThe value functions are augmented to consider both decision variables and random variables (task states), i.e., Uj(sj ;xj1, \u00b7 \u00b7 \u00b7 , xjk). We assume each value function only associates with one random variable. If multiple random variables are associated with a value function, without loss of generality, they can be merged into a single variable. Furthermore, we assume the random variables are not under the control of the agents and they are independent of the decision variables. Specifically, their values are independently drawn from unknown probability distributions.\nGiven a random variable sj in UR-DCOPs, the probability distribution over domain Sj , denoted by bj \u2208 \u2206(Sj), is called a belief of the random variable, and b = \u3008b1, \u00b7 \u00b7 \u00b7 , bm\u3009 is a joint belief of all random variables. Similarly, a joint assignment of all decision variables is denoted by x = \u3008x1, \u00b7 \u00b7 \u00b7 , xn\u3009 and a partial joint assignment for the value function Uj is denoted by xj = \u3008xj1, \u00b7 \u00b7 \u00b7 , xjk\u3009. When the joint belief b is known, solving a UR-DCOP straightforwardly involves finding an assignment of all decision variables x that maximize the sum of the expected values:\nV (b,x) = m\u2211 j=1 \u2211 sj\u2208Sj\nbj(sj)Uj(sj ,xj)\ufe38 \ufe37\ufe37 \ufe38 Uj(bj ,xj)\n(6)\nThe key challenge in our problem is that the joint belief b is unknown. Therefore, we want to find a solution that is robust (minimizing the worst-case loss) to the uncertainty of the joint belief. As mentioned earlier, this objective is equivalent to the minimax regret given the belief space B:\nVregret(x) = min x\u2032 max b\u2208B max x\u2217 [V (b,x\u2217)\u2212 V (b,x\u2032)]\ufe38 \ufe37\ufe37 \ufe38 R1(x\u2032,b)\ufe38 \ufe37\ufe37 \ufe38\nR2(x\u2032)\n(7)\nwhere x\u2217 is the optimal solution given belief b. R1(x\u2032,b) is the regret or loss of solution x relative to b, i.e., the difference in expected value between x and the optimal solution x\u2217 under belief b. R2(x\u2032) is the maximum regret of x with respect to the feasible belief space. Thus, the value of minimax regret, Vregret(x), minimizes the worst-case loss over all possible belief points.\nAs mentioned, first responders usually have very limited information about the response tasks when the disaster happens and there is significant uncertainty in the environment. In such cases, minimax regret minimizes the difference between the optimal value V (b,x\u2217) and the actual value V (b,x) achieved by the current solution x in all possible beliefs b \u2208 B. Thus, it is a good solution for the first responders given the limited information."}, {"heading": "Solving UR-DCOPs", "text": "Generally, to compute the minimax regret in Equation 7, we first need to compute the optimal solution x\u2217 given a belief point b and the current solution of agents x\u2032. Then, the whole belief space B is searched to find the worst-case belief b. After that, we need to find the assignment x that minimizes the regret. On the one hand, it cannot be solved by standard DCOP algorithms. On the other hand, given a number of agents, it is very challenging for centralized algorithms to compute the minimax regret because the search space blows up exponentially with the number of agents.\nFollowing the ideas of Iterative Constraint Generation (ICG), two optimizations are alternatively solved at each iteration: the master problem and the subproblem. In more detail, the master problem solves a relaxation of Equation 7 by considering only a subset of all possible \u3008b,x\u2217\u3009 pairs G:\nminx,\u03b4 \u03b4 s. t. \u2200\u3008b,x\u2217\u3009 \u2208 G, V (b,x\u2217)\u2212 V (b,x) \u2264 \u03b4 (8)\nInitially, this set can be arbitrary (e.g., empty or randomly generated). By giving G, the master problem tries to minimize the loss for the worst case derived from G.\nThe subproblem generates the maximally violated constraint relative to x, the solution of the current master problem. More precisely, a new \u3008b,x\u2217\u3009 pair is found by the subproblem. This pair is called a witness point because it indicates that the current x is not the best solution in terms of the minimax regret. In more detail, a program is solved to determine the witness \u3008b,x\u2217\u3009 for the current solution x:\nmaxb,x\u2217,\u03b4\u2032 \u03b4 \u2032 s. t. V (b,x\u2217)\u2212 V (b,x) \u2265 \u03b4\u2032 (9)\nIf \u03b4\u2032 = \u03b4 then the constraint for \u3008b,x\u2217\u3009 in Equation 9 is satisfied at the current solution x, and indeed all unexpressed constraints must be satisfied as well. Otherwise, \u03b4\u2032 > \u03b4, implying that the constraint for \u3008b,x\u2217\u3009 is violated in the current relaxation. Thus, it is added to G and the master problem is solved again to compute a new x. This process repeats until no new witness point can be found by the subproblem and the master problem terminates with the best solution x.\nBased on the ideas of ICG, we propose Iterative Constraint Generation Max-Sum (ICG-Max-Sum) to solve URDCOPs. Similar to standard Max-Sum, our algorithm starts with encoding UR-DCOPs into a factor graph. Then, two Max-Sum algorithms are iteratively executed to solve the master and sub-problems. In the master problem, we run a Max-Sum to compute the current minimax solution x and minimax regret \u03b4 given the witness set G. In the subproblem, we run another Max-Sum to generate a new witness point \u3008b,x\u2217\u3009 and the corresponding minimax regret \u03b4\u2032 given the current solution x. Then, \u03b4 and \u03b4\u2032 are compared by each node in the factor graph: If \u03b4 > \u03b4\u2032, the newly generated witness point \u3008b,x\u2217\u3009 is added to G; otherwise it terminates and returns the current minimax solution x. These processes repeat until all nodes in the factor graph are terminated. Notice that in our algorithm the solutions xi \u2208 x and x\u2217i \u2208 x\u2217 are computed and stored locally by variable i and belief bj \u2208 b is computed and stored locally by function j. The main procedures are shown in Algorithm 1.\nAlgorithm 1: Iterative Constraint Generation Max-Sum Input:M: The UR-DCOP Model Create a factor graph based onM Initialize the witness set G \u2190 \u2205 repeat\n// The Master Problem Run Max-Sum on the factor graph with G Compute the current minimax solution x Save each xi \u2208 x in variable node i Compute the minimax regret \u03b4 // The Subproblem Run Max-Sum on the factor graph with x Compute the witness point \u3008b,x\u2217\u3009 Compute the minimax regret \u03b4\u2032 // G \u2190 G \u222a {\u3008b,x\u2217\u3009} if \u03b4\u2032 > \u03b4 foreach variable node i do\nif \u03b4\u2032 > \u03b4 then Save x\u2217i \u2208 x\u2217 in variable node i\nelse Terminate variable node i foreach function node j do\nif \u03b4\u2032 > \u03b4 then Save bj \u2208 b in function node j\nelse Terminate function node j until all nodes in the graph are terminated. return the current minimax solution x"}, {"heading": "The Master Problem", "text": "The master problem of Equation 8, given the witness set G, can be equivalently written as:\nx = arg min x\u2032 max \u3008b,x\u2217\u3009\u2208G m\u2211 j=1\n[Uj(bj ,x \u2217 j )\u2212 Uj(bj ,x\u2032j)]\ufe38 \ufe37\ufe37 \ufe38\n\u03b4\n(10)\nNote that the witness set G is known and the choice of \u3008bj ,x\u2217j \u3009 can be computed locally by function node j in MaxSum because bj is independent from other belief points and x\u2217j is only related to the variable nodes it connects. To do this, we consider the problem of minimizing a vector of regret functions for each witness point in G:\nV(x) = [ V\u0303 (x, \u3008b,x\u2217\u30091), \u00b7 \u00b7 \u00b7 , V\u0303 (x, \u3008b,x\u2217\u3009|G|) ] (11)\nwhere V\u0303 (x, \u3008b,x\u2217\u3009g) = V (b,x\u2217) \u2212 V (b,x) and \u3008b,x\u2217\u3009g is the gth element in G. Accordingly, instead of qj\u2192i(xi) and ri\u2192j(xi) being scalar functions of xi, these messages now map the domain of xi to a set of regret vectors: \u2200xi \u2208 Di, qj\u2192i(xi) = [q1, \u00b7 \u00b7 \u00b7 , q|G|], ri\u2192j(xi) = [r1, \u00b7 \u00b7 \u00b7 , r|G|].\nTo compute these messages, the two key operators required by Max-Sum (Equations 2 and 3) need to be redefined. In more detail, adding two messages is defined to add each corresponding element in the two vectors: q1j\u2192i(xi) + q 2 j\u2192i(xi) = [q 1 1 + q 2 1 , \u00b7 \u00b7 \u00b7 , q1|G| + q 2 |G|] and r1i\u2192j(xi)+r 2 i\u2192j(xi) = [r 1 1 +r 2 1, \u00b7 \u00b7 \u00b7 , r1|G|+r 2 |G|]. For Equation 3, we need to minimize the regret of function node j\nwith respect to its neighboring variables xj as: rj\u2192i(xi) = Uj(x\u0303j) + \u2211\nk\u2208N(j)\\i\nqk\u2192j(x\u0303k) (12)\nwhere Uj(x\u0303j) = [U\u0303j(x\u0303j, \u3008bj ,x\u2217j \u30091), \u00b7 \u00b7 \u00b7 , U\u0303j(x\u0303j, \u3008bj ,x\u2217j \u3009|G|], U\u0303j(x\u0303j, \u3008bj ,x\u2217j \u3009g) = Uj(bj ,x\u2217j )\u2212 Uj(bj ,x\u2032j), and\nx\u0303j = arg min xj\\xi max \u3008bj ,x\u2217j \u3009g\u2208G [U\u0303j(xj, \u3008bj ,x\u2217j \u3009g)+\u2211 k\u2208N(j)\\i qk\u2192j(xk, g)] (13)\nAt the end of the message-passing phase, each variable xi computes its marginal function zi(xi) according to Equation 4. Obviously, the value of the marginal function is also a vector: zi(xi) = [z1, \u00b7 \u00b7 \u00b7 , z|G|]. The best assignment of the variable xi can be computed by:\nxi = arg min x\u2032i\u2208Di max g\nzi(x \u2032 i, g) (14)\nwhere g is an index for the vector. After that, the minimax regret \u03b4 can be computed by propagating values in a (any) pre-defined tree structure of the factor graph: (1) Each variable node send its assignment to its neighboring nodes; (2) On received all the assignments from its neighboring nodes, each function node computes the regret value and sent the message to its neighboring nodes; (3) Each node propagates the regret values until all the regret values are computed and received by all the nodes. Then, \u03b4 can be computed by adding all the m messages in each node.\nAn example of the master problem with randomly generated V is shown in Figure 1. In this example, there are two variables with the domain {A,B} and {C,D} respectively and the witness set G is {G1, G2}. Clearly, the minimax solution is AD and the minimax regret is \u221296 since we have min{max{\u221257, 64}, max{\u221296,\u2212162}, max{54, 72}, max{\u22124, 55}} =\u221296. For our Max-Sum, according to Equation 12, the message r1(A) = V(AD) since AD = arg minAD,AC{max{\u221257, 64},max{\u221296,\u2212162}}. Similarly, we have the messages: r1(B) = V(BD), r1(C) = V(AC), and r1(D) = V(AD). After the messagepassing phase, the marginal functions z1(A) = V(AD), z1(B) = V(BD), z2(C) = V(AC), z2(D) = V(AD). Therefore, the best assignments of each variable are x1 = arg minA,B{max{\u221296,\u2212162, },max{\u22124, 55}} = A and x2 = arg maxC,D{max{\u221257, 64},max{\u221296,\u2212162}} =D. The joint solution is AD and the minimax regret is \u221296, which are equal to the minimax solution and regret that we computed earlier according to the definition."}, {"heading": "The Subproblem", "text": "The subproblem in Equation 9 given the current solution x can be written as:\n\u3008b,x\u2217\u3009 = arg max b\u2208B max x\u2032\u2217 m\u2211 j=1\n[Uj(bj ,x \u2032\u2217 j )\u2212 Uj(bj ,xj)]\ufe38 \ufe37\ufe37 \ufe38\n\u03b4\u2032\n.\nSince each belief bj is independent from each other and from the decision variables, the calculation of each belief can be moved inside the utility function, shown as:\n\u3008b,x\u2217\u3009 = arg max x\u2032\u2217 m\u2211 j=1 { max bj [Uj(bj ,x \u2032\u2217 j )\u2212 Uj(bj ,xj)] } \ufe38 \ufe37\ufe37 \ufe38\n\u03b4\u2032\nThus, we can define a new utility function as:\nUj(x \u2032\u2217 j ) = max\nbj [Uj(bj ,x\n\u2032\u2217 j )\u2212 Uj(bj ,xj)] (15)\nand rely on a linear program to compute the utility:\nmaxbj Uj(bj ,x \u2032\u2217 j )\u2212 Uj(bj ,xj)\ns. t. \u2200sj \u2208 Sj , bj(sj) \u2265 0\u2211 sj\u2208Sj bj(sj) = 1\n(16)\nThis can be done locally and thereby the subproblem can be solved by standard DCOP algorithms. For Max-Sum, we need to implement a linear program (Equation 16) in each function node to compute the belief bj when Uj(xj) is called in Equation 3. Once the optimal solution x\u2217 is found, we can propagate x and x\u2217 to the function nodes and apply Equation 16 for each function node j to compute the belief b. Similar to the master problem, the minimax regret \u03b4\u2032 can be computed by value propagation in the factor graph."}, {"heading": "Analysis and Discussion", "text": "Inherited from Max-Sum, the optimality of our algorithm depends on the structure of the factor graph. Specifically, for an acyclic factor graph, it is known that Max-Sum can converge to the optimal solution of the DCOPs in finite rounds of message passing (Farinelli et al. 2008). Lemma 1. The master problems in ICG-Max-Sum will converge to the optimal solution for acyclic factor graphs.\nProof (Sketch). The messages (vectors) in the master problems represent the regret values of all the witness points in |G|. The sum operator adds up all the regret components for each witness point, Uj(bj ,x\u2217j ) \u2212 Uj(bj ,xj), sent from its neighboring nodes. The max operator selects the current minimax solution x\u0303j and sends out the corresponding regret values. Specifically, this operator is over matrices [mij ] with the row i indexed by witness points and column j by assignments. It compares two matrices and outputs the one with smaller minj maxi[mij ] value. This operator is associative and commutative with an identity element (matrix) [\u221e] (i.e., the algebra is a commutative semi-ring). Thus, since MaxSum is a GDL algorithm (Aji & McEliece 2000), the results hold for acyclic factor graphs.\nLemma 2. The subproblems in ICG-Max-Sum will converge to the optimal solution for acyclic factor graphs.\nProof (Sketch). The subproblems are standard DCOPs given the utility function (Equation 15) that can be computed locally by each function node. Thus, Max-Sum will converge to the optimal solution for acyclic factor graphs.\nTheorem 1. ICG-Max-Sum will converge to the optimal minimax solution for acyclic factor graphs.\nProof (Sketch). According to Lemmas 1 and 2, the master problems and subproblems are optimal for acyclic factor graphs. Thus, this theorem can be proved by showing that the subproblem will enumerate all \u3008b,x\u2217\u3009 pairs if x is not the minimax optimal solution. This is equivalent to proving that in the subproblem, \u03b4\u2032 > \u03b4 is always true and the new witness \u3008b,x\u2217\u3009 6\u2208 G if x 6= x\u0304 where x\u0304 is the minimax optimal solution. Suppose \u03b4\u2032 = \u03b4 and x 6= x\u0304, then we have\n\u03b4\u2032 = max\u3008b,x\u2217\u3009[V (b,x \u2217)\u2212 V (b,x)]\n> minx\u2032 max\u3008b,x\u2217\u3009[V (b,x \u2217)\u2212 V (b,x\u2032)] = max\u3008b,x\u2217\u3009[V (b,x \u2217)\u2212 V (b, x\u0304)] =\u21d2 \u03b4 = max\u3008b,x\u2217\u3009\u2208G [V (b,x \u2217)\u2212 V (b,x)] = \u03b4\u2032\n> max\u3008b,x\u2217\u3009[V (b,x \u2217)\u2212 V (b, x\u0304)].\nBecause G is only a subset of the whole space, we have max \u3008b,x\u2217\u3009\u2208G [V (b,x\u2217)\u2212V (b,x)] > max \u3008b,x\u2217\u3009\u2208G [V (b,x\u2217)\u2212V (b, x\u0304)].\nThen, the current solution x computed by the master problem is x = arg minx\u2032 [max\u3008b,x\u2217\u3009\u2208G [V (b,x\u2217) \u2212 V (b,x\u2032)]] = x\u0304. This is contradictory to the assumption x 6= x\u0304. Furthermore, in the subproblem, the newly generated witness point must not be in G, otherwise \u03b4\u2032 = \u03b4 due to the same x and \u3008b,x\u2217\u3009 being in both problems because we have \u03b4\u2032 = max\u3008b,x\u2217\u3009[V (b,x\u2217) \u2212 V (b,x)] = max\u3008b,x\u2217\u3009\u2208G [V (b,x\n\u2217)\u2212 V (b,x)] = \u03b4. The algorithm will converge to the minimax optimal solution x\u0304 once all witness points \u3008b,x\u2217\u3009 are enumerated and added to G by the subproblems. Thus, the results hold.\nWhen the factor graph is cyclic, the straightforward application of Max-Sum is not guaranteed to converge optimally. However, in practice, Max-Sum can produce high quality solutions even on cyclic graphs (Farinelli et al. 2008). Moreover, it is straightforward for our algorithm to incorporate other approximate techniques to generate acyclic factor graphs by pruning edges (Rogers et al. 2011) or adding directions (Zivan & Peled 2012) in the cyclic factor graphs. The discussion of them is beyond the scope of this paper.\nFor the computation and communication complexity, the subproblem uses the standard Max-Sum except that a linear program is solved each time when the utility function is called in Equation 3. For the master problem, according to Equation 13, the computation is exponential only in the number of variables in the scope of Uj (similar to standard Max-Sum) but linear in the number of witness points in G. The messages in the master problem are vectors with the length of |G| while the messages in the sub-problems are normal Max-Sum messages. In experiments, we observed G is usually very small (<10) for the tested problems."}, {"heading": "Empirical Evaluation", "text": "We tested the performance of our algorithm on randomly generated instances of the task allocation problem that we used to motivate our work. We first generate a problem with a set of tasks T and agentsA. Each task has a set of states S, from which we randomly select one as its task state. We then create a random graph with links among agents and tasks. Each link represents the fact that the agent can perform the task. The utility function (a Gaussian function whose mean and variance are randomly generated between the ranges of 80 to 100 and 0 to 80 respectively) of each task depends on all connected agents and its current state. In the experiments, we set |A|=|T |/2 so that not all tasks can be performed at the same time because a task requires at least one agent. Thus, the agents must make a good choice to maximize the team performance. We run the algorithms to solve the problem and output its solution. Since the real states of tasks are hidden, we want the solution to be as close to the optimal solution as possible. To empirically evaluate this, we randomize the task states for 100 runs and compute the average regret value (the difference between the optimal value and the solution value given the states) for each algorithm. For acyclic graphs, the optimal value is computed by Max-Sum given the underlying task states.\nTo date, none of the existing algorithms in the literature can solve our model so a directed comparison is not possible. Therefore, to test the scalability and solution quality of our algorithm, we compared it with two baseline approaches: a centralized method based on ICG (Equations 8 and 9) and a decentralized method based on DSA (Zhang et al. 2005). Specifically, the two operators maxx\u2217 and minx\u2032 are alternatively solved by DSA and a linear program is used to solve the operator maxb\u2208B in Equation 7. We ran our experiments on a machine with a 2.66GHZ Intel Core 2 Duo and 4GB memory. All the algorithms including Max-Sum were implemented in Java 1.6, and the linear programs are solved by CPLEX 12.4. For each instance, we used the same random seeds so that all the randomized problems and task states are identical for all algorithms.\nIn more details, Table 1 shows the runtime of (centralized) ICG and ICG-Max-Sum. We can see from the table that the runtime of ICG increased dramatically with the problem size and ran out of time (>2h) for problems with |A|=10, while ICG-Max-Sum took less than 5 seconds to solve the same problems. For ICG-Max-Sum, the problems |A|=10 needed less time than problems |A|=7 because they took fewer iterations to converge. Clearly from the table, large UR-DCOPs are intractable for centralized ICG. The reason for the stability of ICG-Max-Sum is that it can exploit the interaction\nstructures of the task allocation problems (tasks usually require few agents to coordinate).\nTable 2 shows the average regrets (the mean) achieved by DSA and ICG-Max-Sum. Note that we feeded both methods with exactly the same problems and executed them until they converged. From acyclic graphs, we can see that ICG-Max-Sum produced much lower regrets than DSA in the tested instances. For large problems, DSA hardly converged to any meaningful results (behaving like random solutions) because the errors in the maxx\u2217 and minx\u2032 steps could accumulate over time. Moreover, we observed that DSA took more time (one order of magnitude) than ICGMax-Sum for large domains because it needs to solve maxx\u2217 for each minx\u2032 step. This confirms the advantage of ICGMax-Sum for solving large UR-DCOPs with lower average regret and faster runtime. For cyclic graphs, we only tested on small instances because it is hard to obtain optimal solutions (the ground truth) for large problems (Max-Sum is not optimal on cyclic graphs). In these tests, we can see from the table that ICG-Max-Sum performed similar to DSA (or a little worse especially for problems with more task states). Thus, the aforementioned approximation techniques is useful to run Max-Sum on cyclic graphs and bound the errors."}, {"heading": "Conclusions", "text": "We have presented the ICG-Max-Sum algorithm to find robust solutions for UR-DCOPs. Specifically, we assume the distributions of the task states are unknown and we use minimax regret to evaluate the worst-case loss. Building on the ideas of iterative constraint generation, we proposed a decentralized algorithm that can compute the minimax regret and solution using Max-Sum. Similar to Max-Sum, it can exploit the interaction structures among agents and scale up to problems with large number of agents. We empirically evaluated the performance of our algorithms on our motivating task allocation domains. The experimental results show that our algorithm has better scalability (e.g., 100 agents and 200 tasks) than the centralized method (ICG) and outperforms the state-of-the-art decentralized method (DSA) with much lower average regrets.\nThere are several future research directions that follow on from this work. First, it would be useful to bound the message size in the master problem when communication is very costly. Second, it would be interesting to extend our work to domains where the tasks are not completely independent."}], "references": [{"title": "R", "author": ["S.M. Aji", "McEliece"], "venue": "J.", "citeRegEx": "Aji . McEliece 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "and Decker", "author": ["J. Atlas"], "venue": "K.", "citeRegEx": "Atlas . Decker 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "J", "author": ["Benders"], "venue": "F.", "citeRegEx": "Benders 1962", "shortCiteRegEx": null, "year": 1962}, {"title": "N", "author": ["A. Farinelli", "A. Rogers", "A. Petcu", "Jennings"], "venue": "R.", "citeRegEx": "Farinelli et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Faltings", "author": ["T. L\u00e9aut\u00e9"], "venue": "B.", "citeRegEx": "L\u00e9aut\u00e9 . Faltings 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed algorithms for dcop: A graphical-game-based approach", "author": ["Maheswaran"], "venue": null, "citeRegEx": "Maheswaran,? \\Q2004\\E", "shortCiteRegEx": "Maheswaran", "year": 2004}, {"title": "P", "author": ["Modi"], "venue": "J.; Shen, W.-M.; Tambe, M.; and Yoko, M.", "citeRegEx": "Modi et al. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "H", "author": ["D.T. Nguyen", "W. Yeoh", "Lau"], "venue": "C.", "citeRegEx": "Nguyen et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Faltings", "author": ["A. Petcu"], "venue": "B.", "citeRegEx": "Petcu . Faltings 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "C", "author": ["K. Regan", "Boutilier"], "venue": "2010. Robust policy computation in reward-uncertain MDPs using nondominated policies. In Proceedings of the 24th AAAI Conference on Artificial Intelligence, 1127\u2013", "citeRegEx": "Regan . Boutilier 2010", "shortCiteRegEx": null, "year": 1133}, {"title": "and Boutilier", "author": ["K. Regan"], "venue": "C.", "citeRegEx": "Regan . Boutilier 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "N", "author": ["A. Rogers", "A. Farinelli", "R. Stranders", "Jennings"], "venue": "R.", "citeRegEx": "Rogers et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "N", "author": ["R. Stranders", "F.M.D. Fave", "A. Rogers", "Jennings"], "venue": "R.", "citeRegEx": "Stranders et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Peled", "author": ["R. Zivan"], "venue": "H.", "citeRegEx": "Zivan . Peled 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2013, "abstractText": "Many multi-agent coordination problems can be represented as DCOPs. Motivated by task allocation in disaster response, we extend standard DCOP models to consider uncertain task rewards where the outcome of completing a task depends on its current state, which is randomly drawn from unknown distributions. The goal of solving this problem is to find a solution for all agents that minimizes the overall worst-case loss. This is a challenging problem for centralized algorithms because the search space grows exponentially with the number of agents and is nontrivial for standard DCOP algorithms we have. To address this, we propose a novel decentralized algorithm that incorporates Max-Sum with iterative constraint generation to solve the problem by passing messages among agents. By so doing, our approach scales well and can solve instances of the task allocation problem with hundreds of agents and tasks.", "creator": "LaTeX with hyperref package"}}}