{"id": "1310.1949", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2013", "title": "Least Squares Revisited: Scalable Approaches for Multi-class Prediction", "abstract": "This work provides simple algorithms mossi for aydemir multi - periosteum class (whitefriars and multi - supernova label) 1933-34 prediction robberies in settings yibna where both 226-3347 the number 16-yards of kyran examples n passats and the data flouncing dimension d are relatively marvast large. 012 These rebuked robust kanhangad and ecologic parameter free nimatullah algorithms jazzercise are self-deprecation essentially winker iterative 64-59 least - squares updates and shalgi very disguised versatile carabanchel both in theory 1995-present and devar in corban practice. delaigue On the theoretical voltex front, we ramiah present heins several tanard variants holopainen with convergence coinages guarantees. m/z Owing tischer to their mawgan effective gorodets use al-qassam of second - order structure, these ba\u02bfal algorithms logis are wilk substantially better than first - sandalwood order methods in many practical scenarios. clarenville On the rivero empirical side, g\u00f6n\u00fcl we se-kyung present a 128-player scalable coman stagewise gybed variant of khaine our 3:59 approach, euro30 which achieves dramatic earned computational bobrza\u0144ski speedups unosom over sfpc popular sorriest optimization mulcahy packages fert such ambientalistas as Liblinear notification and iswahyudi Vowpal handprints Wabbit on standard datasets (MNIST a-porter and ahmet CIFAR - donetsk 10 ), impermissibly while ringland attaining state - 1976-1984 of - trumpeting the - dva art accuracies.", "histories": [["v1", "Mon, 7 Oct 2013 20:48:58 GMT  (359kb,D)", "https://arxiv.org/abs/1310.1949v1", null], ["v2", "Mon, 21 Oct 2013 15:18:37 GMT  (360kb,D)", "http://arxiv.org/abs/1310.1949v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alekh agarwal", "sham m kakade", "nikos karampatziakis", "le song", "gregory valiant"], "accepted": true, "id": "1310.1949"}, "pdf": {"name": "1310.1949.pdf", "metadata": {"source": "META", "title": "Least Squares Revisited:  Scalable Approaches for Multi-class Prediction", "authors": ["Alekh Agarwal"], "emails": ["alekha@microsoft.com", "skakade@microsoft.com", "nikosk@microsoft.com", "lsong@cc.gatech.edu", "valiant@stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "The aim of this paper is to develop robust and scalable algorithms for multi-class classification problems\nwith k classes, where the number of examples n and the number of features d is simultaneously quite large. Typically, such problems have been approached by the minimization of a convex surrogate loss, such as the multiclass hinge-loss or the multiclass logistic loss, or reduction to convex binary subproblems such as oneversus-rest. Given the size of the problem, (batch or online) first-order methods are typically the methods of choice to solve these underlying optimization problems. First-order updates, usually linear in the dimension in their computational complexity, easily scale to large d. To deal with the large number of examples, online methods are particularly appealing in the single machine setting, while batch methods are often preferred in distributed settings.\nEmpirically however, these first-order approaches are often found to be lacking. Many natural highdimensional data such as images, audio, and video typically result in ill-conditioned optimization problems. While each iteration of a first-order method is fast, the number of iterations needed unavoidably scale with the condition number of the data matrix (Nemirovsky & Yudin, 1983), even for simple generalized linear models (henceforth GLM). Hence, the convergence of these methods is still rather slow on many real-world datasets with decaying spectrum.\nA natural alternative in such scenarios is to use secondorder methods, which are robust to the conditioning of the data. In this paper, we present simple second-order methods for multiclass prediction in GLMs. Crucially,\nar X\niv :1\n31 0.\n19 49\nv2 [\ncs .L\nthe methods are parameter free, robust in practice and admit easy extensions. As an example, we show a more sophisticated variant which learns the unknown link function in the GLM simultaneously with the weights. Finally, we also present a practical variant to tackle the difficulties typically encountered in applying second-order methods to high-dimensional problems. We develop a block-coordinate descent style stagewise regression procedure that incrementally solves leastsquares problems on small batches of features. The result of this overall development is a suite of techniques that are simple, versatile and substantially faster than several other state-of-the-art optimization methods.\nOur Contributions: Our work has three main contributions. Working in the GLM framework: E[y | x] = g(Wx), where y is a vector of predictions, W is the weight matrix, and g is the vector valued link function, we present a simple second-order update rule. The update is based on a majorization of the Hessian, and uses a scaled version of the empirical second moment 1n \u2211 i xix T i as the preconditioner. Our algorithm is parameter-free and does not require a line search for convergence. Furthermore our computations only involve a d \u00d7 d matrix unlike IRLS and other Hessian related approaches where matrices are O(dk\u00d7dk) for multiclass problems1. Theoretically, the proposed method enjoys an iteration complexity independent of the condition number of the data matrix as an immediate observation.\nWe extend our algorithm to simultaneously estimate the weights as well as the link function in GLMs under a parametric assumption on the link function, building on ideas from isotonic regression (Kalai & Sastry, 2009; Kakade et al., 2011). We provide a global convergence guarantee for this algorithm despite the non-convexity of the problem. To the best of our knowledge, this is the first work to formulate and address the problem of isotonic regression in the multiclass classification setting. Practically this enables, for example, the use of our current predictions as features in order to improve the predictions in subsequent iterations. Similar procedures are common for binary SVMs (Platt, 1999) and for re-ranking(Collins & Koo, 2000).\nBoth the above algorithms, despite being metric free, still scale somewhat poorly with the dimensionality of the problem owing to the quadratic cost of the representation and updates. To address this problem, we take a cue from ideas in block-coordinate descent and stagewise regression literature. Specifically, we\n1This is a critical distinction as we focus on tasks involving increasingly complex class hierarchies, particularly in the context of computer vision problems.\nchoose a subset of the features and perform one of the above second-order updates on that subset only. We then repeat this process, successively fitting the residuals. We demonstrate excellent empirical performance of this procedure on two tasks: MNIST and CIFAR10. In settings where the second order information is relevant, such as MNIST and CIFAR-10, we find that stagewise variants can be highly effective, providing orders of magnitude speed-ups over online methods and other first-order approaches. This is particularly noteworthy since we compare a simple MATLAB implementation of our algorithms with sophisticated C software for the alternative approaches. In contrast, for certain text problems where the data matrix is well conditioned, online methods are highly effective. Notably, we also achieve state of the art accuracy results on MNIST and CIFAR-10, outperforming the \u201cdropout\u201d neural net (Hinton et al., 2012), where our underlying optimization procedures are entirely based on simple least squares approaches. These promising results highlight that this is a fruitful avenue for the development of further theory and algorithms, which we leave for future work.\nRelated Work: There is much work on scalable algorithms for large, high-dimensional datasets. A large chunk of this work builds on and around online learning and stochastic optimization, leveraging the ability of these algorithms to ensure a very rapid initial reduction of test error (see e.g. (Bottou & Bousquet, 2008; Shalev-Shwartz, 2012)). These methods can be somewhat unsuited though, when optimization to a relatively high precision is desired, for example, when the data matrix is ill-conditioned and small changes in the parameters can lead to large changes in the outputs. This has led to interesting works on hybrid methods that interpolate between an initial online and subsequent batch behavior (Shalev-Shwartz & Zhang, 2013; Roux et al., 2012). There has also been a renewed interest in Quasi-Newton methods scalable to statistical problems using stochastic approximation ideas (Byrd et al., 2011; Bordes et al., 2009). High-dimensional problems have also led to natural consideration of block coordinate descent style procedures, both in serial (Nesterov, 2012) and distributed (Richta\u0301rik & Taka\u0301c, 2012; Recht et al., 2011) settings. Indeed, in some of our text experiments, our stagewise procedure comes quite close to a block-coordinate descent type update. There are also related approaches for training SVMs that extract the most information out of a small subset of data before moving to the next batch (Chapelle, 2007; Matsushima et al., 2012; Yu et al., 2012).\nOn the statistical side, our work most directly gen-\neralizes past works on learning in generalized linear models for binary classification, when the link function is known or unknown (Kalai & Sastry, 2009; Kakade et al., 2011). A well-known case where squared loss was used in conjunction with a stagewise procedure to fit binary and multi-class GLMs is the gradient boosting machine (Friedman, 2001). In the statistics literature, the iteratively reweighed least squares algorithm (IRLS) is the workhorse for fitting GLMs and also works by recasting the optimization problem to a series of least squares problems. However, IRLS can (and does in some cases) diverge, while the proposed algorithms are guaranteed to make progress on each iteration. Another difficulty with IRLS (also shared by some majorization algorithms e.g., (Jebara & Choromanska, 2012)) is that each iteration needs to work with a new Hessian since it depends on the parameters. In contrast, our algorithms use the same matrix throughout their run."}, {"heading": "2. Setting and Algorithms", "text": "We begin with the simple case of binary GLMs, before addressing the more challenging multi-class setting."}, {"heading": "2.1. Warmup: Binary GLMs", "text": "The canonical definition of a GLM in binary classification (where y \u2208 {0, 1}) setup posits the probabilistic model E[y | x] = g(w\u2217Tx), (1) where g : R 7\u2192 R is a monotone increasing function, and w\u2217 \u2208 Rd. To facilitate the development of better algorithms, assume that g is a L-Lipschitz function of its univariate argument. Since g is a monotone increasing univariate function, there exists a convex function \u03a6 : R 7\u2192 R such that \u03a6\u2032 = g. Based on this convex function, let us define a convex loss function.\nDefinition 1 (Calibrated loss). Given the GLM (1), define the associated convex loss\n`(w; (x, y)) = \u03a6(wTx)\u2212 ywTx. (2)\nUp to constants independent of w, this definition yields the least-squares loss for the identity link function, g(u) = u, and the logistic loss for the logit link function, g(u) = eu/(1+eu). The loss is termed calibrated: for each x, minimizing the above loss yields a consistent estimate of the weights w\u2217. Precisely,\nLemma 1. Suppose g is a monotone function and that Eq. (1) holds. Then w\u2217 is a minimizer of E[`(w; (x, y))], where the expectation is with respect to the distribution on x and y. Furthermore, any other minimizer w\u0303 of E[`(w; (x, y))] (if one such exists) also satisfies E[y | x] = g(w\u0303Tx).\nProof. First, let us show that w\u2217 is a pointwise minimizer of E[`(w; (x, y))|x] (almost surely). E[`(w; (x, y))]. Observe for any point x,\nE[\u2207`(w; (x, y)) | x] = E[\u2207\u03a6(wTx)\u2212 xy|x]\n= E[g(wTx)x | x]\u2212 g(w\u2217Tx)x (3) where the second equality follows since \u03a6\u2032 = g and E[y | x] = g(w\u2217x) by the probabilistic model (1). Hence, w\u2217 is a global minimizer (since the loss function is convex).\nNow let us show that any other minimizer w\u0303 (if one exists) also satisfies E[y | x] = g(w\u0303Tx). Let S\u0303 be the set of x such that E[y | x] = g(w\u0303Tx). It suffices to show Pr(x /\u2208 S\u0303) = 0. Suppose this is not the case. We then have:\nE[`(w\u0303; (x, y))] = Pr(x \u2208 S\u0303) E[`(w\u0303; (x, y))|x \u2208 S\u0303] (4)\n+ Pr(x /\u2208 S\u0303) E[`(w\u0303; (x, y))|x /\u2208 S\u0303] (5) = Pr(x \u2208 S\u0303) E[`(w\u2217; (x, y))|x \u2208 S\u0303] (6)\n+ Pr(x /\u2208 S\u0303) E[`(w\u0303; (x, y))|x /\u2208 S\u0303] (7) > Pr(x \u2208 S\u0303) E[`(w\u2217; (x, y))|x \u2208 S\u0303] (8)\n+ Pr(x /\u2208 S\u0303) E[`(w\u2217; (x, y))|x /\u2208 S\u0303] (9) = E[`(w\u2217; (x, y))] (10)\n(11)\nwhere the second equality follows by (3) and the inequality follows since for x /\u2208 S\u0303, E[`(w\u0303; (x, y))|x] > E[`(w\u2217; (x, y))|x] (again by (3), since w\u2217 is a minimizer of E[`(w; (x, y))|x], almost surely). This contradicts the optimality of w\u0303.\nAs another intuition, this loss corresponds to the negative log-likelihood when the GLM specifies an exponential family with the sufficient statistic y. Similar observations have been noted for the binary case in some prior works as well (see Kakade et al. (2011); Ravikumar et al. (2008)). Computing the optimal w\u2217 simply amounts to using any standard convex optimization procedure. We now discuss these choices in the context of multi-class prediction."}, {"heading": "2.2. Multi-class GLMs and Minimization Algorithms", "text": "The first question in the multi-class case concerns the definition of a generalized linear model; monotonicity is not immediately extended in the multi-class setting. Following the definition in the recent work of Agarwal (2013), we extend the binary case by defining the model:\nE[y | x] = \u2207\u03a6(W \u2217x) := g(W \u2217x) (12)\nwhere W \u2217 \u2208 Rk\u00d7d is the weight matrix, \u03a6 : Rk 7\u2192 R is a proper and convex lower semicontinuous function of k variables and y \u2208 Rk is a vector with 1 for the correct class and zeros elsewhere. This definition essentially corresponds to the link function g = \u2207\u03a6 satisfying (maximal and cyclical) monotonicity (Rockafellar, 1966) (natural extensions of monotonicity to vector spaces). Furthermore, when the GLM (12) corresponds to an exponential family with sufficient statistics y, then \u03a6 corresponds to the log-partition function like the binary case, and is always convex (Lauritzen, 1996).\nThis formulation immediately yields an analogous definition for a calibrated multi-class loss.\nDefinition 2 (Calibrated multi-class loss). Given the GLM (12), define the associated convex loss\n`(W ; (x, y)) = \u03a6(Wx)\u2212 yTWx. (13)\nObserve that we obtain the multi-class logistic loss, when the probabilistic model (12) is a multinomial logit model.\nThe loss function is convex as before. It is Fisher consistent: the minimizer of the expected loss is W \u2217 (as in Equation 3). In particular,\nLemma 2. Suppose \u03a6 : Rk 7\u2192 R is a (proper and lower semicontinuous) convex function and that Eq. (12) holds. Then W \u2217 is a minimizer of E[`(W ; (x, y))], where the expectation is with respect to the distribution on x and y. Furthermore, any other minimizer W\u0303 of E[`(W ; (x, y))] (if one such exists) also satisfies E[y | x] = g(w\u0303Tx).\nThe proof is identical to that of before. Again, convexity only implies that all local minimizers are global minimizers.\nAs before, existing convex optimization algorithms can be utilized to estimate the weight matrix W . First-order methods applied to the problem have periteration complexity of O(dk), but can require a large number of iterations as discussed before. Here, the difficulty in utilizing second-order approaches is that the Hessian matrix is of size dk\u00d7dk (e.g. as in IRLS, for logistic regression); any direct matrix inversion method is now much more computationally expensive even for moderate sized k.\nAlgorithm 1 provides a simple variant of least squares regression \u2014 which repeatedly fits the residual error \u2014 that exploits the second order structure in x. Indeed, as shown in the appendix, the algorithm uses a block-diagonal upper bound on the Hessian matrix in order to preserve the correlations between the covariates x, but does not consider interactions across\nAlgorithm 1 Generalized Least Squares Input: Initial weight matrix W0, data {(xi, yi)}, Lipschitz constant L, link g = \u2207\u03a6.\nDefine the (vector valued) predictions y\u0302 (t) i = g(Wtxi) and the empirical expectations:\n\u03a3\u0302 = E\u0302[xixTi ] = 1\nn n\u2211 i=1 xix T i\nE\u0302[(y\u0302(t) \u2212 y)xT ] = 1 n n\u2211 i=1 (y\u0302 (t) i \u2212 yi)x T i\nrepeat Update the weight matrix Wt:\nWTt+1 = W T t \u2212\n1 L \u03a3\u0302\u22121 E\u0302[(y\u0302(t) \u2212 y)xT ] (14)\nuntil convergence\nthe different classes to have a more computationally tractable update. The algorithm has several attractive properties. Notably, (i) the algorithm is parameter free2 and (ii) the algorithm only inverts a d \u00d7 d matrix. Furthermore, this matrix is independent of the weights W (and the labels) and can be computed only once ahead of time. In that spirit, the algorithm can also be viewed as preconditioned gradient descent, with a block diagonal preconditioner whose diagonal blocks are identical and equal to the matrix \u03a3\u0302\u22121. At each step, we utilize the residual error E\u0302[(y\u0302 \u2212 y)xT ], akin to a gradient update on least-squares loss. Note the \u201cstepsize\u201d here is determined by L, a parameter entirely dependent on the loss function and not on the data. For the case of logistic regression, simply L = 1 satisfies this Lipchitz constraint3. Also observe that for the square loss, where L = 1, the generalized least squares algorithm reduces to least squares (and terminates in one iteration).\nWe now describe the convergence properties of Algorithm 1. The results are stated in terms of the sample loss\n`n(w) = 1\nn n\u2211 i=1 `(W ; (xi, yi)). (15)\n2Here and below we refer to parameter free algorithms from the point of view of optimization: no learning rates, backtracking constants etc. The overall learning algorithms may still require setting other parameters, such as the regularizer.\n3Using Gershgorin\u2019s circle theorem it is possible to show that L = 1/2 still leads to a valid upper bound on the Hessian. This is tight and achieved by an example whose class probabilities under the current model are (1/2, 1/2, 0, 0, . . .).\nThe following additional assumptions regarding the link function \u2207\u03a6 are natural for characterizing convergence rates. Assuming that the link function g = \u2207\u03a6 is L-Lipschitz amounts to the condition\n\u2016g(u)\u2212 g(v)\u20162 \u2264 L\u2016u\u2212 v\u20162, where u, v \u2208 Rk. (16) If we want a linear convergence rate, we must further assume \u00b5-strong monotonicity, meaning for all u, v \u2208 Rk: \u3008g(u)\u2212 g(v), u\u2212 v\u3009 \u2265 \u00b5\u2016u\u2212 v\u201622. (17) Theorem 1. Define W \u2217 = arg minW `n(W ). Suppose that the link function \u2207\u03a6 is L-Lipschitz (16). Using the generalized Least Squares updates (Algorithm 1) with W0 = 0, then for all t = 1, 2, . . .\n`n(Wt)\u2212 `n(W \u2217) \u2264 2L\u2016W \u2217\u20162\nt+ 4 .\nIf, in addition, the link function is \u00b5-strongly monotone (17) and let \u03ba\u03a6 = L/\u00b5. Then\n`n(Wt)\u2212 `n(W \u2217) \u2264 L\n2 ( \u03ba\u03a6 \u2212 1 \u03ba\u03a6 + 1 )t \u2016W \u2217\u20162F .\nThe proof rests on demonstrating that the blockdiagonal matrix formed by copies of L\u03a3\u0302 provides a majorization of the Hessian matrix, along with standard results in convex optimization (see e.g. (Nesterov, 2004)) and is deferred to the supplement. Also, observe that the convergence results in Theorem 1 are completely independent of the conditioning of the data matrix \u03a3\u0302. Indeed they depend only on the smoothness and strong convexity properties of \u03a6 which is a function we know ahead of time and control. This is the primary benefit of these updates over first-order updates.\nIn order to understand these issues better, let us quickly contrast these results to the analogous ones for gradient descent. In that case, we get qualitatively similar dependence on the number of iterations. However, in the case of Lipschitz \u2207\u03a6, the convergence rate is O ( L t \u03c3max ( XXT n ) \u2016W \u2217\u20162 ) . Under strong mono-\ntonicity, the rate improves to O ( L ( \u03ba\u03a6\u03baXXT \u2212 1 \u03ba\u03a6\u03baXXT + 1 )t) . That is, the convergence rate is slowed down by factors depending on the singular values of the empirical covariance in both the cases. Similar comparisons can also be made for accelerated versions of both our and vanilla gradient methods."}, {"heading": "2.3. Unknown Link Function for Multi-class", "text": "The more challenging case is when the link function is unknown. This setting has two main difficulties: the statistical one of how to restrict the complexity of\nAlgorithm 2 Calibrated Least Squares\nInput: Initial weight matrix W0, set of calibration functions G = {g1, . . . gm}\nInitialize the predictions: y\u0302 (0) i = W0xi repeat Fit the residual:\nWt = arg min W n\u2211 i=1 \u2016yi \u2212 y\u0302(t\u22121)i \u2212Wxi\u2016 2 2, and\ny\u0303 (t) i = y\u0302 (t\u22121) i +Wtxi. (18)\nCalibrate the predictions y\u0303(t):\nW\u0303t = arg min W\u0303 n\u2211 i=1 \u2016yi \u2212 W\u0303G(y\u0303(t)i )\u2016 2 2, and\ny\u0302 (t) i = clip(W\u0303tG(y\u0303 (t))), (19)\nwhere clip(v) is the Euclidean projection of v onto the probability simplex in Rk.\nuntil convergence\nthe class of link functions and the computational one of efficient estimation (as opposed to local search or other herutistic methods).\nWith regards to the former, a natural restriction is to consider the class of link functions realized as the derivative of a convex function in k-dimensions. This naturally extends the Isotron algorithm from the binary case (Kalai & Sastry, 2009). Unfortunately, this is an extremely rich class; the sample complexity of estimating a uniformly bounded convex, Lipschitz function in k dimensions grows exponentially with k (Bronshtein, 1976). In our setting, this would imply that the number of samples needed for a small error in link function estimation would necessarily scale exponentially in the number of classes, even with infinite computational resources at our disposal. To avoid this curse of dimensionality, assume that there is a finite basis G such that g\u22121 = (\u2207\u03a6)\u22121 \u2208 lin(G), (\u2207\u03a6)\u22121 is the funcional inverse of \u2207\u03a6. Without loss of generality, we also assume that G always contains the identity function. We do not consider the issue of approximation error here.\nBefore presenting the algorithm, let us provide some more intuition about our assumption g\u22121 = (\u2207\u03a6)\u22121 \u2208 lin(G). Clearly the case of G = g\u22121 for a fixed function g puts us in the setting of the previous section. More generally, let us consider that G is a dictionary of p functions so that g\u22121(y) = \u2211p i=1 w\u0303iGi(y). In the GLM (12), this means that we have an overall linear-\nlike model4 p\u2211 i=1 W\u0303iGi(E[Y |x]) = W \u2217x.\nIf we let p = k and Gi(y) be the ith class indicator yi, then the above equation boils down to W\u0303TE[Y |x] = W \u2217x, (20) meaning that an unknown linear combination of the class-conditional probabilities is a linear function of the data. More generally, we consider Gi to also have higher-order monomials such as y2i or y 3 i so that the LHS is some low-degree polynomial of the classconditional probability with unknown coefficients.\nNow, the computational issue is to efficiently form accurate predictions (as in the binary case (Kalai & Sastry, 2009), the problem is not convex). We now describe a simple strategy for simultaneously learning the weights as well as the link function, which not only improves the square loss at every step, but also converges to the optimal answer quickly. The strategy maintains two sets of weights, Wt \u2208 Rk\u00d7d and W\u0303t \u2208 Rk\u00d7|G| and maintains our current predictions y\u0302\n(t) i \u2208 Rk for each\ndata point i = 1, 2, . . . , n. After initializing all the predictions and weights to zero, the updates shown in Algorithm 2 involve two alternating least squares steps. The first step fits the residual error to x using the weights Wt. This The second step then fits y to the functions of y\u0302(t)\u2019s, i.e. to G(y\u0302(t)). Finally, we project onto the unit simplex in order to obtain the new predictions, which can only decrease the squared error and can be done in O(k) time (Duchi et al., 2008).\nIn the context of the examples of Gi mentioned above, the algorithm boils down to predicting the conditional probability of Y = i given x, based not only on x, but also on our current predictions for all the classes (and higher degree polynomials in these predictions)5.\nFor the analysis of Algorithm 2, we focus on the noiseless case to understand the optimization issues. Analyzing the statistical issues, where there is noise, can be handled using ideas in (Kalai & Sastry, 2009; Kakade et al., 2011).\nTheorem 2. Suppose that yi = g(W \u2217xi) and that the link function g = \u2207\u03a6 satisfies the Lipschitz and strong monotonicity conditions (16) and (17) with constants L and \u00b5 respectively. Suppose also that \u2207\u03a6(0) = 1 /k. Using the (calibrated) Least Squares updates (Algorithm 2) with W0 = 0, for all t = 1, 2, . . . we have\n4It is not a linear model since the statistical noise passes through the functions Gi rather than being additive.\n5The alternating least-squares update in this context are also quite reminiscent of CCA.\nAlgorithm 3 Stagewise Regression Input: data {(xi, yi)}, batch generator GEN, batch size p, iterations T\nInitialize predictions\ny\u0302 (1) i = 0\nfor t = 1, . . . , T do Generate p features from the original ones\n{x\u0303i} = GEN({xi}, p)\nLet Wt be the output of Algorithm 1 or 2 on the dataset {(x\u0303i, yi \u2212 y\u0302(t)i )} Update predictions\ny\u0302 (t+1) i = y\u0302 (t) i +Wtx\u0303i\nend for\nthe bound\n1\nn n\u2211 i=1 \u2016y\u0302(t)i \u2212 yi\u2016 2 2 \u2264 22\u03ba2\u03a6 t\nwhere \u03ba\u03a6 = L/\u00b5.\nWe again emphasize the fact that the updates (18) and (19) only require the solution of least-squares problems in a similar spirit as Algorithm 1. Finally, we note that the rules to compute predictions in our updates( (18) and (19)) require previous predictions (i.e. the learned model is not proper in that it does not actually estimate g, yet it is still guaranteed to make accurate predictions)."}, {"heading": "2.4. Scalable Variants", "text": "When the number of features is large, any optimization algorithm that scales superlinearly with the dimensionality faces serious computational issues. In such cases we can adopt a block coordinate descent approach. To keep the presentation fairly general, we assume that we have an algorithm GEN that returns a small set of m features, where m is small enough so that fitting models with m features is efficient (e.g. we typically use m \u2248 1000). The GEN procedure can be as simple as sampling m of the original features (with or without replacement) or more complex schemes such as random Fourier features (Rahimi & Recht, 2007). We call GEN and fit a model on the m features using either Algorithm 1 or Algorithm 2. We then compute residuals and repeat the process on a fresh batch of m features returned by GEN. In Algorithm 3 we provide pseudocode for this stagewise\nregression procedure. We stress that this algorithm is purely a computational convenience. It can be thought as the algorithm that would result by a block-diagonal approximation of the second moment matrix \u03a3 (not just across classes, but also groups of features). Algorithm 3 bears some resemblance to boosting and related coordinate descent methods, with the crucial difference that GEN is not restricted to searching for the best set of features. Indeed, in our experiments GEN is either sampling from the features without replacement or randomly projecting the data in m dimensions and transforming each of the m dimension by a simple non-linearity. Despite its simplicity, more work needs to be done to theoretically understand the properties of this variant as clearly as those of Algorithm 1 or Algorithm 2. Practically, stagewise regression can have useful regularization properties but these can be subtle and greatly depend on the GEN procedure. In text classification, for example, fitting the most frequent words first leads to better models than fitting the least frequent words first."}, {"heading": "3. Experiments", "text": "We consider four datasets MNIST, CIFAR-10, 20 Newsgroups, and RCV1 that capture many of the challenges encountered in real-world learning tasks. We believe that the lessons gleaned from our analysis and comparisons of performance on these datasets apply more broadly.\nFor MNIST, we compare our algorithms with a variety of standard algorithms. Both in terms of classification accuracy and optimization speed, we achieve close to state of the art performance among permutationinvariant methods (1.1% accuracy, improving upon methods such as the \u201cdropout\u201d neural net). For CIFAR-10, we also obtain nearly state of the art accuracy (> 85%) using standard features. Here, we emphasize that it is the computational efficiency of our algorithms which enables us to achieve higher accuracy without novel feature-generation.\nThe story is rather different for the two text datasets, where the performance of these stagewise methods is less competitive with online approaches, though we do demonstrate substantial reduction in error rate in one of the problems. As we discuss below, the statistical properties of these text datasets (which differ significantly from those of the image datasets) strongly favor online approaches."}, {"heading": "3.1. MNIST", "text": "Nonlinear classifiers are needed to achieve state-ofthe-art performance in MNIST dataset. Although MNIST dataset only contains 60K data points (small by modern standards), the requirement for nonlinear features make this dataset computationally challenging. For instance, a nonlinear support vector machine with a Gaussian RBF kernel needs to manipulate a 60K\u00d760K kernel matrix. This will require hundreds of Gigabytes of memory, not available on most modern desktop machines. Hence we use an explicit feature representation and train our classifiers in the primal space. Specifically we construct random fourier features which are known to approximate the Gaussian kernel k(x, x\u2032) = exp(\u2212\u2016x\u2212x\u2032\u20162/s) (Rahimi & Recht, 2007), though as discussed in the appendix various other methods to construct random low degree polynomials are also effective.\nWe start by comparing linear and logistic regression using Algorithm 1, as well as the calibration variant of Algorithm 2. For the calibration variant, we use a basis G(y) consisting of y, y2 and y3 (applied elementwise to vector y). We compare these algorithms on raw pixel features, as well as small number of random Fourier features described above. As seen in Table 1, the performance of logistic and calibrated variants seem similar and consistently superior to plain linear regression.\nNext, we move to improving accuracy by using the stagewise approach of Algorithm 3, which allows us to scale up to larger number of random Fourier features. Concretely, we fit blocks of features (either 512 and 1024) with Algorithm 3 with three alternative update rules on each stage: linear regression, calibrated linear regression, and logistic regression (with 50 inner loops for the logistic computations). Here, our calibrated linear regression is the simplest one: we only use the previous predictions as features in our new batch of features.\nOur next experiment demonstrates that all three (extremely simple and parameter free) algorithms quickly achieve state of the art performance. Figure 1(a) shows the relation between feature block size, classification test error, and runtime for these algorithm variants. Importantly, while the linear (and linear calibration) algorithms do not achieve as low an error for a fixed feature size, they are faster to optimize and are more effective overall.\nNotably, we find that (i) linear regression achieves better runtime and error trade-off, even though for a fixed number of features linear regressions are not as effective as logistic regression (as we see in Figure 2(a)).\n0.012\n0.014\n0.016\n0.018\n0.02\n0.022\n0.024\nC la\nss ifi\nca tio\nn er\nro r\ncalibrate block 512 calibrate block 1024 linear block 512 linear block 1024 logistic block 512 logistic block 1024\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n1\nno rm\nal iz\ned n\nuc le\nar n\nor m\nnews20 rcv1\nmnist\n(ii) relatively small size feature blocks provide better runtime and error trade-off (blocks of size 300 provide further improvements). (iii) the linearly calibrated regression works better than the vanilla linear regression.\nWe also compare these three variants of our approach to other state-of-the-art algorithms in terms of classification test error and runtime (Figure 2(a) and (b)). Note the logarithmic scaling of the runtime axes. The comparison includes VW 6, and six algorithms implemented in Liblinear (Fan et al., 2008) (see figure caption). We took care in our attempts to time these algorithms to reflect their actual computation time, rather than their loading of the features (which can be rather large, making it extremely time consuming to run these experiments); our stagewise algorithms generate new features on the fly so this is not an issue. See the appendix for further discussion.\nFrom Figure 2 (a) and (b), our logistic algorithm is competitive with all the other algorithms, in terms of it\u2019s accuracy (while for lower dimensions the naive linear methods fared a little worse). All of our algorithms were substantially faster.\nFinally, the models produced by our methods drive the classification test error down to 1.1% while none of the competitors achieve this test error. Runtime wise, our method is extremely fast for the linear regression and calibrated linear regression variants, which are consis-\n6http://hunch.net/~vw/\ntently at least 10 times faster than the other highly optimized algorithms. This is particularly notable given the simplicity of this approach."}, {"heading": "3.2. CIFAR-10", "text": "The CIFAR-10 dataset is a more challenging dataset, where many image recognition algorithms have been tested (primarily illustrating different methods of feature generation; our work instead focusses on the optimization component, given a choice of features). The neural net approaches of \u201cdropout\u201d and \u201cmaxout\u201d algorithms of (Hinton et al., 2012; Goodfellow et al., 2013) provide the best reported performance of 84% and 87%, without increasing the size of the dataset (through jitter or other transformations). We are able to robustly achieve over 85% accuracy with linear regression on standard convolution features (without increasing the size of the dataset through jitter, etc.), illustrating the advantage that improved optimization provides.\nFigure 3 illustrates the performance when we use two types of convolutional features: features generated by convolving the images by random masks, and features generated by convolving with K-means masks (as in (Coates et al., 2011), though we do not use contrast normalization).\nWe find that using only relatively few filters (say about 400), along with polynomial features, are sufficient to obtain over 80% accuracy extremely quickly. Hence, using the thousands of generated features, it is rather fast to build multiple models with disjoint features and model average them, obtaining extremely good performance.\n0 5 10 15 20 0.18 0.2\n0.22 0.24 0.26 0.28 0.3\n0.32 0.34 0.36\nPe rc\nen t T\nes t E\nrr or\nNumber Iterations of Least\u2212Squares\nPerformance vs # Iterations\nRandom Patches k\u2212means Patches Random and k\u2212means\n0 10 20 30 40\n0.15 0.16 0.17 0.18 0.19 0.2\n0.21 0.22 0.23 0.24 0.25\nPe rc\nen t T\nes t E\nrr or\nNumber Models Averaged\nPerformance vs # Models Averaged\nRandom Patches k\u2212means Patches Random and k\u2212means\n(a) (b)\nFigure 3. CIFAR-10 results using two types of convolutional features: (a) features generated by convolving the images by random masks, and (b) features generated by convolving with K-means masks."}, {"heading": "3.3. Well-Conditioned Problems", "text": "We now examine two popular multiclass text datasets: 20 newsgroups7 (henceforth NEWS20), which is a 20 class dataset and a four class version of Reuters Corpus Volume 1 (Lewis et al., 2004) (henceforth RCV1). We use a standard (log) term frequency representation of the data, discussed in the appendix. These data pose rather different challenges than our vision datasets; in addition to being sparse datasets, they are extremely well conditioned.\nThe ratio of the 2nd singular value to the 1000th one (as a proxy for the condition number) and is 19.8 for NEWS20 and 14 for RCV1. In contrast, for MNIST, this condition number is about 72000 (when computed with 3000 random Fourier features). Figure 1(b) shows the normalized spectrum for the three data matrices.\n7http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\nTable 2. Running times and test errors in text datasets for VW, Liblinear, and Stagewise regression\nNEWS20 RCV1 Method Time %Error Time %Error\nVW 2.5 12.4 2.5 2.75 Liblinear 27 13.8 120 2.73 Stagewise 40 11.7 240 2.77\nAs expected, online first-order methods (in particular VW) fare far more favorably in this setting, as seen in Table 2. We use a particular greedy procedure for a our stagewise ordering (as discussed in the appendix, though random also works well). Note that this data is well suited for online methods: it is sparse (making the online updates cheap) and well conditioned (making the gap in convergence between online methods and second order methods small).\nUltimately, an interesting direction is developing hybrid approaches applicable to both cases."}, {"heading": "4. Discussion", "text": "In this paper, we present a suite of fast and simple algorithms for tackling large-scale multiclass prediction problems. We stress that the key upshot of the methods developed in this work is their conceptual simplicity and ease of implementation. Indeed these properties make the methods quite versatile and easy to extend in various ways. We showed an instance of this in Algorithm 2. Similarly, it is straightforward to develop accelerated variants (Nesterov, 2009), by using\nthe distances defined by the matrix \u03a3\u0302 as the proxfunction in Nesterov\u2019s work. These variants enjoy the\nusual improvements of O(1/t2) iteration complexity in the smooth and \u221a \u03ba\u03a6 dependence in the strongly convex setting, while retaining the metric-free nature of Algorithm 1.\nIt is also quite easy to extend the algorithm to multilabel settings, with the only difference being that the vector y of labels now lives on the hypercube instead of the simplex. This only amounts to a minor modification of the projection step in Algorithm 2.\nOverall, we believe that our approach revisits many old and deep ideas to develop algorithms that are practically very effective. We believe that it will be quite fruitful to understand these methods better both theoretically and empirically in further research."}, {"heading": "A. Appendix", "text": "A.1. Proofs\nProof of Theorem 1\nWe start by noting that the Lipschitz and strong monotonicity conditions on\u2207\u03a6 imply the smoothness and strong convexity of the function \u03a6(u) as a function u \u2208 Rk. In particular, given any two matrices W1,W2 \u2208 Rk\u00d7d, we have as the following quadratic upper bound as a consequence of the Lipschitz condition (16)\n\u03a6(W1x) \u2264 \u03a6(W2x) + \u2329 \u2207\u03a6(W2xT )x,W1 \u2212W2 \u232a + L\n2 \u2016W1x\u2212W2x\u201622.\nThe strong monotonicity condition (17) yields an analogous lower bound\n\u03a6(W1x) \u2265 \u03a6(W2x) + \u2329 \u2207\u03a6(W2xT )x,W1 \u2212W2 \u232a + \u00b5\n2 \u2016W1x\u2212W2x\u201622.\nIn order to proceed further, we need one additional piece of notation. Given a positive semi-definite matrix M \u2208 Rd\u00d7d, let us define\n\u2016W\u2016M = k\u2211 i=1 W (i) T MW (i),\nwhere W (i) is the i-th column of W , to be a Mahalanobis norm on matrices. Let us also recall the definition of the matrix \u03a3\u0302 from Algorithm 1. Then adding the smoothness condition over the examples i = 1, 2, . . . , n yields the following conditions on the sample average loss under condition (16):\n`n(W1) \u2264 `n(W2) + \u3008\u2207`n(W2),W1 \u2212W2\u3009+ L\n2 \u2016W1 \u2212W2\u20162\u03a3\u0302.\nThis implies that our objective function `n is L-smooth in the metric induced by \u03a3\u0302, and the update rule (14) corresponds to gradient descent on `n under this metric with a step-size of 1/L. The first part of the theorem now follows from Corollary 2.1.2 of Nesterov (Nesterov, 2004).\nAs for the second part, we note that under the strong monotonicity condition, we have the lower bound\n`n(W1) \u2265 `n(W2) + \u3008\u2207`n(W2),W1 \u2212W2\u3009+ \u00b5\n2 \u2016W1 \u2212W2\u20162\u03a3\u0302.\nHence the objective `n is \u00b5-strongly convex and L-smooth in the metric induced by \u03a3\u0302. The result is now a consequence of Theorem 2.1.15 of Nesterov (Nesterov, 2004).\nWe now provide the proof of Theorem 2. First, a little more on our assumption on g\u22121. By convex duality, this inverse exists and if \u03a6 is a closed, convex function then (\u2207\u03a6)\u22121 = \u2207\u03a6\u2217, where \u03a6\u2217 is the Fenchel-Legendre conjugate of \u03a6. Throughout this section, assume that \u2207\u03a6 is L-Lipschitz continuous. By standard duality results regarding strong-convexity and smoothness, this implies that the conjugate \u03a6 is 1/L-strongly convex. Specifically, we have the useful inequality\n\u3008\u2207\u03a6\u2217(u)\u2212\u2207\u03a6\u2217(v), u\u2212 v\u3009 \u2265 1 L \u2016u\u2212 v\u201622, for all u, v \u2208 Rk. (21)\nSimilarly, due to our assumption about the strong monotonicity of \u2207\u03a6, it is the case that \u2207\u03a6\u2217 is Lipschitz continuous and satisfies\n\u3008\u2207\u03a6\u2217(u)\u2212\u2207\u03a6\u2217(v), u\u2212 v\u3009 \u2264 1 \u00b5 \u2016u\u2212 v\u201622, for all u, v \u2208 Rk. (22)\nAs a specific consequence, note that it is natural to assume that \u2207\u03a6(0) = 11/k, where 1 is the all ones vector. That is the expectation is uniform over all the labels when the weights are zero. Under this condition, it is easy\nto obtain as a consequence of Equation 22 that\n\u2016\u2207\u03a6\u2217(u)\u20162 = \u2016\u2207\u03a6\u2217(u)\u2212\u2207\u03a6\u2217(11/k)\u20162\n\u2264 1 \u00b5 \u2016u\u2212 11/k\u20162 \u2264 1 \u00b5\n( \u2016u\u20162 +\n1\u221a k\n) . (23)\nTogether with these facts, we now proceed to establish Theorem 2.\nProof of Theorem 2\nWe will use y\u0304ti = W\u0303tG(y\u0303 (t) i ) to denote the predictions at each iteration before the clipping operation. For brevity, we use Y\u0302 (t) \u2208 Rn\u00d7k to denote the matrix of all the predictions at iteration t, with a similar version Y\u0303 t for y\u0303(t)\nThe following basic properties of linear regression that are helpful. By the optimality conditions for Wi and W\u0303i,\nn\u2211 i=1 (y\u0303 (t) i \u2212 yi)x T i = 0k\u00d7d, and\nn\u2211 i=1 (y\u0304ti \u2212 yi)G(y\u0303 (t\u22121) i ) T = 0|G|\u00d7d. (24)\nIn particular, multiplying the first equality with the optimal weight matrix W \u2217 yields\n\u2329 n\u2211 i=1 (y\u0303 (t) i \u2212 yi)x T i ,W \u2217 \u232a = 0.\nRearranging terms and recalling that \u2207\u03a6(W \u2217xi) = yi due to the generative model (12) further allows us to rewrite\n\u2329 n\u2211 i=1 (y\u0303 (t) i \u2212 yi),\u2207\u03a6 \u2217(yi) \u232a = 0. (25)\nCombining this with our earlier inequality (21) further yields\nn\u2211 i=1 \u2329 y\u0303 (t) i \u2212 yi,\u2207\u03a6 \u2217(y\u0303 (t) i ) \u232a = n\u2211 i=1 \u2329 y\u0303 (t) i \u2212 yi,\u2207\u03a6 \u2217(y\u0303 (t) i )\u2212\u2207\u03a6 \u2217(yi) \u232a \u2265 1 L n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2. (26)\nHaving lower bounded this inner product term, we obtain an upper bound on it which will complete the proof for convergence of the algorithm. Note that W\u0303t minimizes the objective (19). Since \u2207\u03a6\u2217 \u2208 lin(G), we have for any constant \u03b2 \u2208 R\nn\u2211 i=1 \u2016y\u0304ti \u2212 yi\u201622 \u2264 n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi \u2212 \u03b2\u2207\u03a6 \u2217(y\u0303(t))\u201622.\nWe optimize over the choices of \u03b2 to obtain the best inequality above, which yields the error reduction as\nn\u2211 i=1 \u2016y\u0304ti \u2212 yi\u201622 \u2264 n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 \u2212\n\u2329 Y\u0303 t \u2212 Y,\u2207\u03a6\u2217(Y\u0303 t) \u232a2 \u2016\u2207\u03a6\u2217(Y\u0303 t)\u20162F . (27)\nWe now proceed to upper bound the denominator in the second term in the right hand side of the above bound. Note that from Equation 23, we have the upper bound\n\u2016\u2207\u03a6\u2217(y\u0303(t)i )\u2016 2 2 \u2264\n2\n\u00b52\n( \u2016y\u0302(t)i \u2016 2 2 + 1\nk ) \u2264 2 \u00b52k + 4 \u00b52 ( \u2016y\u0303(t)i \u2212 yi\u2016 2 2 + \u2016yi\u201622\n) \u2264 2 \u00b52k + 4 \u00b52 ( \u2016y\u0303(t)i \u2212 yi\u2016 2 2 + 1 ) ,\nwhere the final inequality follows since yi has a one in precisely one place and zeros elsewhere. Adding these inequalities over the examples, we further obtain\nn\u2211 i=1 \u2016\u2207\u03a6\u2217(y\u0303(t)i )\u2016 2 2 \u2264 2n \u00b52k + 4 \u00b52 ( n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 + n )\n\u2264 6n \u00b52 + 4 \u00b52 n\u2211 i=1 \u2016y\u0302(t)i \u2212 yi\u2016 2 2,\nwhere the last step is an outcome of solving the regression problem in the step (18). Finally, observe that y\u0302(t) is a probability vector in Rk as a result of the clipping operation, while yi is a basis vector as before. Taking these into account, we obtain the upper bound\nn\u2211 i=1 \u2016\u2207\u03a6\u2217(y\u0303(t)i )\u2016 2 2 \u2264 22nk \u00b52 . (28)\nWe are almost there now. Observe that we can substitute this upper bound into our earlier inequality (27) and obtain\nn\u2211 i=1 \u2016y\u0304ti \u2212 yi\u201622 \u2264 n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 \u2212 \u00b52 22nk \u2329 Y\u0303 t \u2212 Y,\u2207\u03a6\u2217(Y\u0303 t) \u232a2 . (29)\nWe can further combine this inequality with the lower bound (26) and obtain\nn\u2211 i=1 \u2016y\u0304ti \u2212 yi\u201622 \u2264 n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 \u2212 \u00b52 22nkL2 ( n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 )2 . (30)\nThis would yield a recursion if we could replace the term \u2211n i=1 \u2016y\u0304ti\u2212yi\u201622 with \u2211n i=1 \u2016y\u0303 (t+1) i \u2212yi\u201622. This requires the use of two critical facts. Note that y\u0302 (t) i is a Euclidean projection of y\u0304 t i onto the probability simplex and yi is an element of the simplex. Consequently, by Pythagoras theorem, it is easy to conclude that\n\u2016y\u0302(t)i \u2212 yi\u2016 2 \u2264 \u2016y\u0304ti \u2212 yi\u201622.\nFurthermore, the regression update (18) guarantees that we have\nn\u2211 i=1 \u2016y\u0303(t+1) \u2212 yi\u201622 \u2264 n\u2211 i=1 \u2016y\u0302(t)i \u2212 yi\u2016 2 2 \u2264 n\u2211 i=1 \u2016y\u0304ti \u2212 yi\u201622.\nCombining the update with earlier bound (30), we finally obtain the recursion we were after:\nn\u2211 i=1 \u2016y\u0303(t+1)i \u2212 yi\u2016 2 2 \u2264 n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 \u2212 \u00b52 22nkL2 ( n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 )2 . (31)\nLet us define t = 1 n \u2211n i=1 \u2016y\u0303 (t) i \u2212 yi\u201622. Then the above recursion can be simplified as\nt+1 \u2264 t \u2212 \u00b52\n22n2kL2 ( n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 )2\n= t \u2212 \u00b52\n22kL2 2t .\nIt is straightforward to verify that the recursion is satisfied by setting t = 22kL 2/(\u00b52t) = 22\u03ba2\u03a6/t. Lastly, observe that as a consequence of the update (19) and the contractivity of the projection operator, we also have\n1\nn n\u2211 i=1 \u2016y\u0302(t)i \u2212 yi\u2016 2 2 \u2264 1 n n\u2211 i=1 \u2016y\u0303(t)i \u2212 yi\u2016 2 2 \u2264 22\u03ba2\u03a6 t ,\nwhich completes our proof.\nA.2. Experimental Details\nA.2.1. MNIST\nWe utilize random Fourier features after PCA-ing the data down to 50 dimensions. This projection alone is a considerable speed improvement (for all algorithms) with essentially no loss in accuracy. We then applied the random features as described in (Rahimi & Recht, 2007). We should note that our reliance on these features is not critical; both random low degree polynomials or random logits (with weights chosen from Gaussian distribution of appropriate variance, so the features have reasonable sensitivity) give comparable performance. Notably, the random logits seem to need substantially fewer features to get to the same accuracy level (maybe by a factor of 2).\nThe kernel bandwidth s is set using the folklore \u201cmedian trick\u201d, i.e., s is the median pairwise distance between training points.\nFor VW, we first generated its native binary input format. VW uses a separate thread for loading data, so that the data loading time is negligible. For Liblinear, we used a modified version which can directly accept dense input features from MATLAB. As with the rest of the experiments, our methods are implemented in MATLAB. For all methods, the computation for model fitting is done in a single 2.4 GHz processor.\nA.2.2. 20 Newsgroups and RCV1\nOur theoretical understanding suggests that when the condition number is small then we expect first order methods to be highly effective. Furthermore, these methods enjoy an extra computational advantage when the data is sparse, as the computation of the gradient updates are linear time in the sparsity level. Here, as expected, methods that ignore second order information are faster than stagewise procedures.\nWe used unigram features with with log-transformed term frequencies as values. We also removed all words that appear fewer than 3 times on the training set. For RCV1 the task was to predict whether a news story should be classified as \u201ccorporate\u201d, \u201ceconomics\u201d, \u201cgovernment\u201d, or \u201cmarkets\u201d. Stories belonging to more than one category were removed. For RCV1, we switched the roles of training and test folds with 665 thousand and 20 thousand examples for training and testing respectively. Tokens in RCV1 had already been stemmed and stopwords had been removed resulting in about 20000 features. For NEWS20 we did not perform such preprocessing leading to\nabout 44000 features. We split the data into 15000 for training and the rest for testing.\nFor both datasets the bag of words representation is too \u201cverbose\u201d to handle efficiently with commodity hardware. Therefore, we exclusively used the stagewise approach here. In each stage, we picked a subset of the original features (i.e., no random projections). To speed up the algorithm, we ordered the features by the magnitude of the gradient.\nFor NEWS20 we used a batch size of 500, 2 passes over the data and regularization \u03bb = 30. We recomputed the ordering between the first and second pass. For RCV1 we used 2 batches of size 2000 and computed the ordering for each batch. The results are shown in Table 2(b). Even though the stagewise procedure can produce models with the same or better generalization error than Liblinear and VW, it is not as efficient. This is again no surprise since when the condition number is small, methods based on stochastic gradient, such as VW, are optimal (Bottou & Bousquet, 2008)."}], "references": [{"title": "Selective sampling algorithms for costsensitive multiclass prediction", "author": ["A. Agarwal"], "venue": "In ICML,", "citeRegEx": "Agarwal,? \\Q2013\\E", "shortCiteRegEx": "Agarwal", "year": 2013}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bordes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2009}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "In NIPS", "citeRegEx": "Bottou and Bousquet,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet", "year": 2008}, {"title": "entropy of convex sets and functions", "author": ["E.M. Bronshtein"], "venue": "Siberian Mathematical Journal,", "citeRegEx": "Bronshtein,? \\Q1976\\E", "shortCiteRegEx": "Bronshtein", "year": 1976}, {"title": "On the use of stochastic hessian information in optimization methods for machine learning", "author": ["R.H. Byrd", "G.M. Chin", "W. Neveitt", "J. Nocedal"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Byrd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 2011}, {"title": "Training a support vector machine in the primal", "author": ["O. Chapelle"], "venue": "Neural Comput.,", "citeRegEx": "Chapelle,? \\Q2007\\E", "shortCiteRegEx": "Chapelle", "year": 2007}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Discriminative reranking for natural language parsing", "author": ["M. Collins", "T. Koo"], "venue": "In ICML,", "citeRegEx": "Collins and Koo,? \\Q2000\\E", "shortCiteRegEx": "Collins and Koo", "year": 2000}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In ICML,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Greedy function approximation: a gradient boosting machine.(english summary)", "author": ["J.H. Friedman"], "venue": "Ann. Statist,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Majorization for crfs and latent likelihoods", "author": ["T. Jebara", "A. Choromanska"], "venue": "In NIPS,", "citeRegEx": "Jebara and Choromanska,? \\Q2012\\E", "shortCiteRegEx": "Jebara and Choromanska", "year": 2012}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["S.M. Kakade", "A. Kalai", "V. Kanade", "O. Shamir"], "venue": "In NIPS,", "citeRegEx": "Kakade et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2011}, {"title": "The isotron algorithm: Highdimensional isotonic regression", "author": ["A.T. Kalai", "R. Sastry"], "venue": "In COLT \u201909,", "citeRegEx": "Kalai and Sastry,? \\Q2009\\E", "shortCiteRegEx": "Kalai and Sastry", "year": 2009}, {"title": "Graphical Models", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Linear support vector machines via dual cached loops", "author": ["S. Matsushima", "S.V.N. Vishwanathan", "A.J. Smola"], "venue": "In KDD,", "citeRegEx": "Matsushima et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matsushima et al\\.", "year": 2012}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A.S. Nemirovsky", "D.B. Yudin"], "venue": "New York,", "citeRegEx": "Nemirovsky and Yudin,? \\Q1983\\E", "shortCiteRegEx": "Nemirovsky and Yudin", "year": 1983}, {"title": "Introductory Lectures on Convex Optimization", "author": ["Y. Nesterov"], "venue": "New York,", "citeRegEx": "Nesterov,? \\Q2004\\E", "shortCiteRegEx": "Nesterov", "year": 2004}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Y. Nesterov"], "venue": "Mathematical Programming A,", "citeRegEx": "Nesterov,? \\Q2009\\E", "shortCiteRegEx": "Nesterov", "year": 2009}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nesterov,? \\Q2012\\E", "shortCiteRegEx": "Nesterov", "year": 2012}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J.C. Platt"], "venue": "In Adavances in large margin classifiers,", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Single index convex experts: Efficient estimation via adapted bregman losses", "author": ["P. Ravikumar", "M. Wainwright", "B. Yu"], "venue": "Snowbird learning workshop,", "citeRegEx": "Ravikumar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2008}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S.J. Wright", "F. Niu"], "venue": "In NIPS, pp", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1c"], "venue": "URL http: //arxiv.org/abs/1212.0873", "citeRegEx": "Richt\u00e1rik and Tak\u00e1c,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1c", "year": 2012}, {"title": "Characterization of the subdifferentials of convex functions", "author": ["R.T. Rockafellar"], "venue": "Pac. J. Math.,", "citeRegEx": "Rockafellar,? \\Q1966\\E", "shortCiteRegEx": "Rockafellar", "year": 1966}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F. Bach"], "venue": "In NIPS,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz", "year": 2012}, {"title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Reearch,", "citeRegEx": "Shalev.Shwartz and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2013}, {"title": "Large linear classification when data cannot fit in memory", "author": ["Yu", "H.-F", "Hsieh", "C.-J", "Chang", "K.-W", "Lin"], "venue": "TKDD,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "We extend our algorithm to simultaneously estimate the weights as well as the link function in GLMs under a parametric assumption on the link function, building on ideas from isotonic regression (Kalai & Sastry, 2009; Kakade et al., 2011).", "startOffset": 195, "endOffset": 238}, {"referenceID": 22, "context": "Similar procedures are common for binary SVMs (Platt, 1999) and for re-ranking(Collins & Koo, 2000).", "startOffset": 46, "endOffset": 59}, {"referenceID": 11, "context": "Notably, we also achieve state of the art accuracy results on MNIST and CIFAR-10, outperforming the \u201cdropout\u201d neural net (Hinton et al., 2012), where our underlying optimization procedures are entirely based on simple least squares approaches.", "startOffset": 121, "endOffset": 142}, {"referenceID": 29, "context": "(Bottou & Bousquet, 2008; Shalev-Shwartz, 2012)).", "startOffset": 0, "endOffset": 47}, {"referenceID": 28, "context": "This has led to interesting works on hybrid methods that interpolate between an initial online and subsequent batch behavior (Shalev-Shwartz & Zhang, 2013; Roux et al., 2012).", "startOffset": 125, "endOffset": 174}, {"referenceID": 4, "context": "There has also been a renewed interest in Quasi-Newton methods scalable to statistical problems using stochastic approximation ideas (Byrd et al., 2011; Bordes et al., 2009).", "startOffset": 133, "endOffset": 173}, {"referenceID": 1, "context": "There has also been a renewed interest in Quasi-Newton methods scalable to statistical problems using stochastic approximation ideas (Byrd et al., 2011; Bordes et al., 2009).", "startOffset": 133, "endOffset": 173}, {"referenceID": 21, "context": "High-dimensional problems have also led to natural consideration of block coordinate descent style procedures, both in serial (Nesterov, 2012) and distributed (Richt\u00e1rik & Tak\u00e1c, 2012; Recht et al.", "startOffset": 126, "endOffset": 142}, {"referenceID": 25, "context": "High-dimensional problems have also led to natural consideration of block coordinate descent style procedures, both in serial (Nesterov, 2012) and distributed (Richt\u00e1rik & Tak\u00e1c, 2012; Recht et al., 2011) settings.", "startOffset": 159, "endOffset": 204}, {"referenceID": 5, "context": "There are also related approaches for training SVMs that extract the most information out of a small subset of data before moving to the next batch (Chapelle, 2007; Matsushima et al., 2012; Yu et al., 2012).", "startOffset": 148, "endOffset": 206}, {"referenceID": 17, "context": "There are also related approaches for training SVMs that extract the most information out of a small subset of data before moving to the next batch (Chapelle, 2007; Matsushima et al., 2012; Yu et al., 2012).", "startOffset": 148, "endOffset": 206}, {"referenceID": 31, "context": "There are also related approaches for training SVMs that extract the most information out of a small subset of data before moving to the next batch (Chapelle, 2007; Matsushima et al., 2012; Yu et al., 2012).", "startOffset": 148, "endOffset": 206}, {"referenceID": 13, "context": "eralizes past works on learning in generalized linear models for binary classification, when the link function is known or unknown (Kalai & Sastry, 2009; Kakade et al., 2011).", "startOffset": 131, "endOffset": 174}, {"referenceID": 10, "context": "A well-known case where squared loss was used in conjunction with a stagewise procedure to fit binary and multi-class GLMs is the gradient boosting machine (Friedman, 2001).", "startOffset": 156, "endOffset": 172}, {"referenceID": 13, "context": "Similar observations have been noted for the binary case in some prior works as well (see Kakade et al. (2011); Ravikumar et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 13, "context": "Similar observations have been noted for the binary case in some prior works as well (see Kakade et al. (2011); Ravikumar et al. (2008)).", "startOffset": 90, "endOffset": 136}, {"referenceID": 0, "context": "Following the definition in the recent work of Agarwal (2013), we extend the binary case by defining the model: E[y | x] = \u2207\u03a6(W \u2217x) := g(W \u2217x) (12)", "startOffset": 47, "endOffset": 62}, {"referenceID": 27, "context": "This definition essentially corresponds to the link function g = \u2207\u03a6 satisfying (maximal and cyclical) monotonicity (Rockafellar, 1966) (natural extensions of monotonicity to vector spaces).", "startOffset": 115, "endOffset": 134}, {"referenceID": 15, "context": "Furthermore, when the GLM (12) corresponds to an exponential family with sufficient statistics y, then \u03a6 corresponds to the log-partition function like the binary case, and is always convex (Lauritzen, 1996).", "startOffset": 190, "endOffset": 207}, {"referenceID": 19, "context": "(Nesterov, 2004)) and is deferred to the supplement.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "Unfortunately, this is an extremely rich class; the sample complexity of estimating a uniformly bounded convex, Lipschitz function in k dimensions grows exponentially with k (Bronshtein, 1976).", "startOffset": 174, "endOffset": 192}, {"referenceID": 8, "context": "Finally, we project onto the unit simplex in order to obtain the new predictions, which can only decrease the squared error and can be done in O(k) time (Duchi et al., 2008).", "startOffset": 153, "endOffset": 173}, {"referenceID": 13, "context": "Analyzing the statistical issues, where there is noise, can be handled using ideas in (Kalai & Sastry, 2009; Kakade et al., 2011).", "startOffset": 86, "endOffset": 129}, {"referenceID": 9, "context": "The comparison includes VW , and six algorithms implemented in Liblinear (Fan et al., 2008) (see figure caption).", "startOffset": 73, "endOffset": 91}, {"referenceID": 11, "context": "The neural net approaches of \u201cdropout\u201d and \u201cmaxout\u201d algorithms of (Hinton et al., 2012; Goodfellow et al., 2013) provide the best reported performance of 84% and 87%, without increasing the size of the dataset (through jitter or other transformations).", "startOffset": 66, "endOffset": 112}, {"referenceID": 6, "context": "Figure 3 illustrates the performance when we use two types of convolutional features: features generated by convolving the images by random masks, and features generated by convolving with K-means masks (as in (Coates et al., 2011), though we do not use contrast normalization).", "startOffset": 210, "endOffset": 231}, {"referenceID": 16, "context": "We now examine two popular multiclass text datasets: 20 newsgroups (henceforth NEWS20), which is a 20 class dataset and a four class version of Reuters Corpus Volume 1 (Lewis et al., 2004) (henceforth RCV1).", "startOffset": 168, "endOffset": 188}, {"referenceID": 20, "context": "Similarly, it is straightforward to develop accelerated variants (Nesterov, 2009), by using the distances defined by the matrix \u03a3\u0302 as the proxfunction in Nesterov\u2019s work.", "startOffset": 65, "endOffset": 81}], "year": 2013, "abstractText": "This work provides simple algorithms for multi-class (and multi-label) prediction in settings where both the number of examples n and the data dimension d are relatively large. These robust and parameter free algorithms are essentially iterative least-squares updates and very versatile both in theory and in practice. On the theoretical front, we present several variants with convergence guarantees. Owing to their effective use of second-order structure, these algorithms are substantially better than first-order methods in many practical scenarios. On the empirical side, we present a scalable stagewise variant of our approach, which achieves dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets (MNIST and CIFAR-10), while attaining state-of-theart accuracies.", "creator": "LaTeX with hyperref package"}}}