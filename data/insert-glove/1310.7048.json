{"id": "1310.7048", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2013", "title": "Scaling SVM and Least Absolute Deviations via Exact Data Reduction", "abstract": "The berms support vector vlaho machine (SVM) robbinsville is a llynfi widely used falls-windsor method 170-run for classification. Although bicalcarata many efforts 685.5 have ter\u00e1n been trigrams devoted 172,000 to develop efficient solvers, it remains challenging the-dream to nichifor apply 1,872 SVM ppk to channahon large - saguenay scale problems. A nice qwerty property fighter-bombers of papadopoulos SVM motiur is akumu that tastes the creekside non - support shash vectors goedert have no freema effect umunna on whitting the resulting classifier. 2.49 Motivated by durdin this waldrep observation, baronetcies we present rikiya fast vering and kadosh efficient screening 7.63 rules high-altitude to fallowing discard non - yutai support vectors by analyzing the dual samiha problem druk of svk SVM zoegirl via variational 40,417 inequalities (planter DVI ). As a moan result, the number spigelman of .242 data instances colbern to be entered kriangsak into the optimization parayre can be farraday substantially reduced. dunwich Some intergiro appealing co2e features of our screening feasibly method are: (delon 1) caenogastropoda DVI pavletic is safe serdyuk in the ncar sense that bottini the vectors discarded lebovic by khojavend DVI are shorja guaranteed to compacting be uii non - :11 support vectors; (overachieved 2) daggs the kefalas data set gitai needs nailon to be hatuel scanned only joppich once to 316.8 run greatest-ever the screening, sheahan whose computational endwar cost is negligible acyclic compared 1792 to przegl\u0105d that durg of solving hated the SVM problem; (3) DVI is independent of tejo the gelez solvers arom and arwel can marduk be integrated with any existing efficient solvers. dornbush We also show wanderers that the DVI gantemirov technique aldeburgh can seamlessly be fanolua extended planty to detect borzov non - seohyun support vectors in pierrick the least absolute deviations bar-ilan regression (ostro\u0142\u0119ka LAD ). wrenn To the winkelman best bellandi of our knowledge, http://www.nih.gov there hemoglobins are 17.49 currently no screening methods for moroso LAD. superego We ticketholders have holck evaluated DVI southesk on both ocellata synthetic p\u00e9ri and real viljo data 202-262-6061 sets. .424 Experiments akyuz indicate disjunction that 26.59 DVI ie significantly outperforms c1-ger the zenia existing sacca state - smithee of - mean-spirited the - henghai art screening rules for psychoanalyzed SVM, flota and pine-oak is very zlatna effective taldykorgan in garissa discarding non - ingratitude support enumeration vectors bleedings for hht LAD. lepyoshkin The eda speedup gained by leroi DVI .550 rules dahui can be kommt up kurbsky to two renois orders atms of magnitude.", "histories": [["v1", "Fri, 25 Oct 2013 23:01:52 GMT  (167kb,D)", "http://arxiv.org/abs/1310.7048v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jie wang", "peter wonka", "jieping ye"], "accepted": true, "id": "1310.7048"}, "pdf": {"name": "1310.7048.pdf", "metadata": {"source": "CRF", "title": "Scaling SVM and Least Absolute Deviations via Exact Data Reduction", "authors": ["Jie Wang", "Peter Wonka", "Jieping Ye"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The support vector machine is one of the most popular classification tools in machine learning. Many efforts have been devoted to develop efficient solvers for SVM [13, 18, 27, 16, 11]. However, the applications of SVM to large-scale problems still pose significant challenges. To address this issue, one promising approach is by \u201cscreening\u201d. The key idea of screening is motivated by the well known feature of SVM, that is, the resulting classifier is determined only by the so called \u201csupport vectors\u201d. If we first identify the non-support vectors via screening, and then remove them from the optimization, it may lead to substantial savings in the computational cost and memory. Another useful tool in machine learning and statistics is the least absolute deviations regression (LAD) [22, 30, 7, 24] or `1 method. When the protection against outliers is a major concern, LAD provides a useful and plausible alternative to the classical least squares or `2 method for linear regression. In this paper, we study both SVM and LAD under a unified framework.\nThe idea of screening has been successfully applied to a large class of `1-regularized problems [10, 31, 29, 28], including Lasso, `1-regularized logistic regression, elastic net, and more general convex problems. Those methods are able to discard a large portion of \u201cinactive\u201d features, which has 0 coefficients in the optimal solution, and the speedup can be several orders of magnitude.\nRecently, Ogawa et al. [20] proposed a \u201csafe screening\u201d rule to identify non-support vectors for SVM; in this paper, we refer to this method as SSNSV for convenience. Notice that, the former approaches for `1-regularized problems aim to discard inactive \u201cfeatures\u201d, while SSNSV is to identify the non-support \u201cvectors\u201d. This essential difference makes SSNSV a nontrivial extension of the existing feature screening\nar X\niv :1\n31 0.\n70 48\nv1 [\ncs .L\nG ]\n2 5\nO ct\nmethods. Although there exist many methods for data reduction for SVM [1, 33, 5], they are not safe, in the sense that the resulting classification model may be different. To the best of our knowledge, SSNSV is the only existing safe screening method [20] to identify the non-support vectors for SVM. However, in order to run the screening, SSNSV needs to iteratively determine an appropriate parameter value and an associated feasible solution, which can be very time consuming.\nIn this paper, we develop novel efficient and effective screening rules, called \u201cDVI\u201d, for a class of supervised learning problems including SVM and LAD [4, 17]. The proposed method, DVI, shares the same advantage as SSNSV [20], that is, both rules are safe in the sense that the discarded vectors are guaranteed to be non-support vectors. The proposed DVI identifies the non-support vectors by estimating a lower bound of the inner product between each vector and the optimal solution, which is unknown. The more accurate the estimation is, the more non-support vectors can be detected. However, the estimation turns out to be highly non-trivial since the optimal solution is not available. To overcome this difficulty, we propose a novel framework to accurately estimate the optimal solution via the estimation of the \u201cdual optimal solution\u201d, as the primal and dual optimal solutions can be related by the KKT conditions [12]. Our main technical contribution is to estimate the dual optimal solution via the so called \u201cvariational inequalities\u201d [12]. Our experiments on both synthetic and real data demonstrate that DVI can identify far more non-support vectors than SSNSV. Moreover, by using the same technique, that is, variational inequalities, we can strictly improve SSNSV in identifying the non-support vectors. Our results also show that DVI is very effective in discarding non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.\nThe rest of this paper is organized as follows. In Section 2, we study the SVM and LAD problems under a unified framework. We then introduce our DVI rules in detail for the general formulation in Sections 3 and 4. In Sections 5 and 6, we extend the DVI rules derived in Section 4 to SVM and LAD respectively. In Section 7, we evaluate our DVI rules for SVM and LAD using both synthetic and real data. We conclude this paper in Section 8.\nNotations: Throughout this paper, we use \u3008x,y\u3009 = \u2211 i xiyi to denote the inner product of vectors x and y, and \u2016x\u20162 = \u3008x,x\u3009. For vector x, let [x]i be the ith component of x. If M is a matrix, mi is the ith column of M and [M]i,j is the (i, j)\nth entry of M. Given a scalar x, we denote max{x, 0} by [x]+. For the index set I := {1, . . . , l}, let J := {j1, . . . , jk} \u2286 I and J c := I \\ J . For a vector x or a matrix M, let [x]J = ([x]j1 , . . . , [x]jk)\nT and [M]J = (mj1 , . . . ,mjk). Moreover, let \u03930(<n) be the class of proper and lower semicontinuous convex functions from <n to (\u2212\u221e,\u221e]. The conjugate of f \u2208 \u03930(<n) is the function f\u2217 \u2208 \u03930(<n) given by\nf\u2217 : <n \u2192 (\u2212\u221e,\u221e] : \u03b8 7\u2192 sup x\u2208<n xT \u03b8 \u2212 f(x). (1)\nThe biconjugate of f \u2208 \u03930(<n) is the function f\u2217\u2217 \u2208 \u03930(<n) given by\nf\u2217\u2217 : <n \u2192 (\u2212\u221e,\u221e] : x 7\u2192 sup \u03b8\u2208<n xT \u03b8 \u2212 f\u2217(\u03b8). (2)"}, {"heading": "2 Basics and Motivations", "text": "In this section, we study the SVM and LAD problems under a unified framework. Then, we motivate the general screening rules via the KKT conditions. Consider the convex optimization problems of the following form:\nmin w\u2208<n\n1 2 \u2016w\u20162 + C\u03a6(w), (3)\nwhere \u03a6 : <n \u2192 < is a convex function but not necessarily differentiable and C > 0 is a regularization parameter. Notice that, the function \u03a6 is generally referred to as the empirical loss. More specifically, suppose we have a set of observations {xi, yi}li=1, where xi \u2208 <n and yi \u2208 < are the ith data instance and the corresponding response. We focus on the following function class:\n\u03a6(w) = l\u2211 i=1 \u03d5 ( wT (aixi) + biyi ) , (4)\nwhere \u03d5 : < \u2192 <+ is a nonconstant continuous sublinear function, and ai, bi are scalars. We provide the definition of sublinear function as follows.\nDefinition 1. [15] A function \u03c3 : <n \u2192 (\u2212\u221e,\u221e] is said to be sublinear if it is convex, and positively homogeneous, i.e.,\n\u03c3(tx) = t\u03c3(x), \u2200x \u2208 <nand t > 0. (5)\nWe will see that SVM and LAD are both special cases of problem (3). A nice property of the function \u03d5 is that the biconjugate \u03d5\u2217\u2217 is exactly \u03d5 itself, as stated in Lemma 2.\nLemma 2. For the function \u03d5 : < \u2192 <+ which is continuous and sublinear, we have \u03d5 \u2208 \u03930(<), and thus \u03d5\u2217\u2217 = \u03d5.\nIt is straightforward to check the statement in Lemma 2 by verifying the requirements of the function class \u03930(<). For self-completeness, we provide a proof in the supplement. According to Lemma 2, problem (3) can be rewritten as\nmin w\u2208<n\n1 2 \u2016w\u20162 + C l\u2211 i=1 \u03d5\u2217\u2217 ( wT (aixi) + biyi ) (6)\n= min w\u2208<n\n1 2 \u2016w\u20162 + C l\u2211 i=1 { sup \u03b8i\u2208< \u03b8i [ wT (aixi) + biyi ] \u2212 \u03d5\u2217(\u03b8i) }\n= min w\u2208<n sup \u03b8i\u2208< i=1,...,l\n1 2 \u2016w\u20162 + C l\u2211 i=1 { \u03b8i [ wT (aixi) + biyi ] \u2212 \u03d5\u2217(\u03b8i) }\n= sup \u03b8\u2208<l \u2212C l\u2211 i=1 \u03d5\u2217(\u03b8i) + min w\u2208<n 1 2 \u2016w\u20162 + C\u3008Zw + y\u0304, \u03b8\u3009,\nwhere \u03b8 = (\u03b81, . . . , \u03b8l) T , Z = (aixi, . . . , alxl) T and y\u0304 = (b1y1, . . . , blyl) T . Let `(w) := 12\u2016w\u2016 2 + C\u3008Zw + y\u0304, \u03b8\u3009. The reason we can exchange the order of min and sup in Eq. (6) is due to the strong duality of problem (3) [3].\nBy setting \u2202`(w)\u2202w = 0, we have\nw\u2217 = \u2212CZT \u03b8, (7)\nand thus\nmin w\n`(w) = `(w\u2217) = \u2212C 2\n2 \u2016ZT \u03b8\u20162 + C\u3008y\u0304, \u03b8\u3009. (8)\nHence, Eq. (6) becomes\nsup \u03b8\u2208<l \u2212C l\u2211 i=1 \u03d5\u2217(\u03b8i)\u2212 C2 2 \u2016ZT \u03b8\u20162 + C\u3008y\u0304, \u03b8\u3009. (9)\nMoreover, because \u03d5 \u2208 \u03930(<) is sublinear by Lemma 2, we know that \u03d5\u2217 is the indicator function for a closed convex set. In fact, we have the following result:\nLemma 3. For the nonconstant continuous sublinear function \u03d5 : < \u2192 <+, there exists a nonempty closed interval I\u03d5 = [\u03b1, \u03b2] with \u03b1, \u03b2 \u2208 < and \u03b1 < \u03b2 such that\n\u03d5\u2217(t) := \u03b9[\u03b1,\u03b2] = { 0, if t \u2208 [\u03b1, \u03b2], \u221e, otherwise.\n(10)\nLet I l\u03d5 = [\u03b1, \u03b2] l. We can rewrite problem (9) as\nsup \u03b8\u2208Il\u03d5 \u2212C\n2\n2 \u2016ZT \u03b8\u20162 + C\u3008y\u0304, \u03b8\u3009. (11)\nProblem (11) is in fact the dual problem of (3). Moreover, the \u201csup\u201d in problem (11) can be replaced by \u201cmax\u201d due to the strong duality [3] of problem (3). Since C > 0, problem (11) is equivalent to\nmin \u03b8\u2208Il\u03d5\nC 2 \u2016ZT \u03b8\u20162 \u2212 \u3008y\u0304, \u03b8\u3009. (12)\nLet w\u2217(C) and \u03b8\u2217(C) be the optimal solutions of (3) and (11) respectively. Eq. (7) implies that\nw\u2217(C) = \u2212CZT \u03b8\u2217(C). (13)\nThe KKT conditions1 of problem (12) are\n[\u03b8\u2217(C)]i \u2208  \u03b2, if \u2212 \u3008w\u2217(C), aixi\u3009 < biyi; [\u03b1, \u03b2], if \u2212 \u3008w\u2217(C), aixi\u3009 = biyi; \u03b1, if \u2212 \u3008w\u2217(C), aixi\u3009 > biyi; i = 1, . . . , l. (14)\nFor notational convenience, let\nR = {i : \u2212\u3008w\u2217(C), aixi\u3009 > biyi}, E = {i : \u2212\u3008w\u2217(C), aixi\u3009 = biyi}, L = {i : \u2212\u3008w\u2217(C), aixi\u3009 < biyi}.\nWe call the vectors in the set E as \u201csupport vectors\u201d. All the other vectors in R and L are called \u201cnonsupport vectors\u201d. The KKT conditions in (14) imply that, if some of the data instances are known to be members of R and L, then the corresponding components of \u03b8\u2217(C) can be set accordingly and we only need the other components of \u03b8\u2217(C). More precisely, we have the following result:\nLemma 4. Given index sets R\u0302 \u2286 R and L\u0302 \u2286 L, we have 1. [\u03b8\u2217(C)]R\u0302 = \u03b1 and [\u03b8 \u2217(C)]L\u0302 = \u03b2.\n2. Let S\u0302 = R\u0302 \u22c3 L\u0302, |S\u0302c| be the cardinality of the set S\u0302c, G\u030211 = [ZT ]TS\u0302c [Z T ]S\u0302c , G\u030212 = [X T ]TS\u0302c [X T ]S\u0302 and\ny\u0302 = yS\u0302c \u2212 CG\u030212[\u03b8\u2217(C)]S\u0302 . Then, [\u03b8\u2217(C)]S\u0302c can be computed by solving the following problem:\nmin \u03b8\u0302\u2208<|S\u0302c|\nC 2 \u03b8\u0302T G\u030211\u03b8\u0302 \u2212 y\u0302T \u03b8\u0302, s.t. \u03b8\u0302 \u2208 [\u03b1, \u03b2]|S\u0302 c|. (15)\nClearly, if |S\u0302| is large compared to |I| = l, the computational cost for solving problem (15) can be much cheaper than solving the full problem (12). To determine the membership of the data instances, Eq. (13) and (14) imply that\nC\u3008ZT \u03b8\u2217(C), aixi\u3009 > biyi \u21d2 [\u03b8\u2217(C)]i = \u03b1\u21d4 i \u2208 R; (R1)\nC\u3008ZT \u03b8\u2217(C), aixi\u3009 < biyi \u21d2 [\u03b8\u2217(C)]i = \u03b2 \u21d4 i \u2208 L. (R2)\nHowever, (R1) and (R2) are generally not applicable since \u03b8\u2217(C) is unknown. To overcome this difficulty, we can estimate a region \u0398 such that \u03b8\u2217(C) \u2208 \u0398. As a result, we obtain the relaxed version of (R1) and (R2):\nmin \u03b8\u2208\u0398\nC\u3008ZT \u03b8, aixi\u3009 > biyi \u21d2 [\u03b8\u2217(C)]i = \u03b1\u21d4 i \u2208 R; (R1\u2032)\n1Please refer to the supplement for details.\nmax \u03b8\u2208\u0398\nC\u3008ZT \u03b8, aixi\u3009 < biyi \u21d2 [\u03b8\u2217(C)]i = \u03b2 \u21d4 i \u2208 L. (R2\u2032)\nNotice that, (R1\u2032) and (R2\u2032) serve as the foundation of the proposed DVI rules and the method in [20]. In the subsequent sections, we first estimate the region \u0398 which includes \u03b8\u2217(C), and then derive the screening rules based on (R1\u2032) and (R2\u2032). Method to solve problem (15)\nIt is known that, problem (15) can be efficiently solved by the dual coordinate descent method [16]. More precisely, the optimization procedure starts from an initial point \u03b8\u03020 \u2208 <|S\u0302c| and generates a sequence of points {\u03b8\u0302k}\u221ek=0. The process from \u03b8\u0302k to \u03b8\u0302k+1 is referred to as an outer iteration. In each outer iteration, we update the components of \u03b8\u0302k one at a time and thus get a sequence of points \u03b8\u0302k,i \u2208 <|S\u0302c|, i = 1, . . . , |S\u0302c|. Suppose we are at the kth outer iteration. To get \u03b8\u0302k,i from \u03b8\u0302k,i\u22121, we need to solve the following optimization problem:\nmin t\nC 2 (\u03b8\u0302k,i\u22121 + tei) T G\u030211(\u03b8\u0302 k,i\u22121 + tei)\u2212 y\u0302T (\u03b8\u0302k,i\u22121 + tei) (16)\ns.t. [\u03b8\u0302k,i\u22121]i + t \u2208 [\u03b1, \u03b2], i = 1, . . . , l,\nwhere ei = (0, . . . , 1, . . . , 0) T . Clearly, problem (16) is equivalent to the following 1D optimization\nproblem:\nmin t\nC 2 [G\u030211]i,it 2 + (CeTi G\u030211\u03b8\u0302 k,i\u22121 \u2212 [y\u0302]i)t (17)\ns.t. [\u03b8\u0302k,i\u22121]i + t \u2208 [\u03b1, \u03b2],\nwhich admits a closed form solution t\u2217. Once t\u2217 is available, we can set \u03b8\u0302k,i = \u03b8\u0302k,i\u22121 + t\u2217ei. For more details, please refer to [16].\nIn Section 3, we first give an accurate estimation of the set \u0398 which includes \u03b8\u2217(C) as in (R1\u2032) and (R2\u2032) via the variational inequalities. Then in Section 4, we present the novel DVI rules for problem (3) in detail."}, {"heading": "3 Estimation of the Dual Optimal Solution", "text": "For problem (12), suppose we are given two parameter values 0 < C0 < C and \u03b8 \u2217(C0) is known. Then, Theorem 6 shows that \u03b8\u2217(C) can be effectively bounded in terms of \u03b8\u2217(C0). The main technique we use is the so called variational inequalities. For self-completeness, we cite the definition of variational inequalities as follows.\nTheorem 5. [12] Let A \u2286 <n be a convex set, and let h be a Ga\u0302teaux differentiable function on an open set containing A. If x\u2217 is a local minimizer of h on A, then\n\u3008\u2207h(x\u2217),x\u2212 x\u2217\u3009 \u2265 0, \u2200x \u2208 A. (18)\nVia the variational inequalities, the following theorem shows that \u03b8\u2217(C) can estimated in terms of \u03b8\u2217(C0).\nTheorem 6. For problem (12), let C > C0 > 0. Then\n\u2016ZT \u03b8\u2217(C)\u2212 C0+C2C Z T \u03b8\u2217(C0)\u2016 \u2264 C\u2212C02C \u2016Z T \u03b8\u2217(C0)\u2016.\nProof. Let g(\u03b8) be the objective function of problem (12). The variational inequality implies that\n\u3008\u2207g(\u03b8\u2217(C0)), \u03b8 \u2212 \u03b8\u2217(C0)\u3009 \u2265 0, \u2200\u03b8 \u2208 [\u03b1, \u03b2]l; (19)\n\u3008\u2207g(\u03b8\u2217(C)), \u03b8 \u2212 \u03b8\u2217(C)\u3009 \u2265 0, \u2200\u03b8 \u2208 [\u03b1, \u03b2]l. (20)\nNotice that \u2207g(\u03b8) = CZZT \u03b8 \u2212 y\u0304, and \u03b8\u2217(C0) \u2208 [\u03b1, \u03b2]l and \u03b8\u2217(C) \u2208 [\u03b1, \u03b2]l. Plugging \u2207g(\u03b8\u2217(C)) and \u2207g(\u03b8\u2217(C0)) into (19) and (20) leads to\n\u3008C0ZZT \u03b8\u2217(C0)\u2212 y\u0304, \u03b8\u2217(C)\u2212 \u03b8\u2217(C0)\u3009 \u2265 0; (21)\n\u3008CZZT \u03b8\u2217(C)\u2212 y\u0304, \u03b8\u2217(C0)\u2212 \u03b8\u2217(C)\u3009 \u2265 0. (22)\nWe can see that the inequality in (22) is equivalent to\n\u3008y\u0304 \u2212 CZZT \u03b8\u2217(C), \u03b8\u2217(C)\u2212 \u03b8\u2217(C0)\u3009 \u2265 0. (23)\nThen the statement follows by adding the inequalities in (21) and (23) together."}, {"heading": "4 The Proposed DVI Rules", "text": "Given C > C0 > 0 and \u03b8 \u2217(C0), we can estimate \u03b8 \u2217(C) via Theorem 6. Combining (R1\u2032), (R2\u2032) and Theorem (6), we develop the basic screening rule for problem (3) as summarized in the following theorem:\nTheorem 7. (DVI) For problem (12), suppose we are given \u03b8\u2217(C0). Then, for any C > C0, we have [\u03b8\u2217(C)]i = \u03b1, i.e., i \u2208 R, if the following holds\nC+C0 2 \u3008Z T \u03b8\u2217(C0), aixi\u3009 \u2212 C\u2212C02 \u2016Z T \u03b8\u2217(C0)\u2016\u2016aixi\u2016 > biyi.\nSimilarly, we have [\u03b8\u2217(C)]i = \u03b2, i.e., i \u2208 L, if\nC+C0 2 \u3008Z T \u03b8\u2217(C0), aixi\u3009+ C\u2212C02 \u2016Z T \u03b8\u2217(C0)\u2016\u2016aixi\u2016 < biyi.\nProof. We will prove the first half of the statement. The second half can be proved analogously. To show [\u03b8\u2217(C)]i = \u03b1, i.e., i \u2208 R, (R1) implies that we only need to show C\u3008ZT \u03b8\u2217(C), aixi\u3009 > biyi. Thus, we can see that\nC\u3008ZT \u03b8\u2217(C), aixi\u3009 =C \u2329 ZT \u03b8\u2217(C)\u2212 C0+C2C Z T \u03b8\u2217(C0), aixi \u232a + C \u2329 C0+C 2C Z T \u03b8\u2217(C0), aixi \u232a \u2265C0+C2 \u3008Z T \u03b8\u2217(C0), aixi\u3009 \u2212 C \u2225\u2225ZT \u03b8\u2217(C)\u2212 C0+C2C XT \u03b8\u2217(C0)\u2225\u2225 \u2016aixi\u2016\n\u2265C0+C2 \u3008Z T \u03b8\u2217(C0), aixi\u3009 \u2212 C\u2212C02 \u2016Z T \u03b8\u2217(C0)\u2016\u2016aix\u2016 >biyi.\nNote that, the second inequality is due to Theorem 6, and the last line is due to the statement. This completes the proof.\nIn real applications, the optimal parameter value of C is unknown and we need to estimate it. Commonly used model selection strategies such as cross validation and stability selection need to solve the optimization problems over a grid of turning parameters 0 < C1 < C2 < . . . < CK to determine an appropriate value for C. This procedure is usually very time consuming, especially for large scale problems. To this end, we propose a sequential version of the proposed DVI below.\nCorollary 8. (DVI\u2217s) For problem (12), suppose we are given a sequence of parameters 0 < C1 < C2 < . . . < CK. Assume \u03b8\n\u2217(Ck) is known for an arbitrary integer 1 \u2264 k < K. Then, for Ck+1, we have [\u03b8\u2217(Ck+1)]i = \u03b1, i.e., i \u2208 R, if the following holds\nCk+1+Ck 2 \u3008Z T \u03b8\u2217(Ck), aixi\u3009 \u2212 Ck+1\u2212Ck2 \u2016Z T \u03b8\u2217(Ck)\u2016\u2016aixi\u2016 > biyi.\nSimilarly, we have [\u03b8\u2217(Ck+1)]i = \u03b2, i.e., i \u2208 L, if\nCk+1+Ck 2 \u3008Z T \u03b8\u2217(Ck), aixi\u3009+ Ck+1\u2212Ck2 \u2016Z T \u03b8\u2217(Ck)\u2016\u2016aixi\u2016 < biyi.\nThe main computational cost of DVI\u2217s is due to the evaluation of \u3008ZT \u03b8\u2217(Ck), aixi\u3009, \u2016ZT \u03b8\u2217(Ck)\u2016 and \u2016aixi\u2016. Let G = ZZT . It is easy to see that\n\u3008ZT \u03b8\u2217(Ck), aixi\u3009 = gTi \u03b8\u2217(Ck), \u2016ZT \u03b8\u2217(Ck)\u20162 = \u03b8\u2217(Ck)TG\u03b8\u2217(Ck), \u2016x\u0304i\u20162 = [G]i,i.\nwhere gi is the i th column of G. Since G is independent of Ck, it can be computed only once and thus the computational cost of DVI\u2217s reduces to O(l 2) to scan the entire data set. Indeed, by noting Eq. (13), we can reconstruct DVI rules without the explicit computation of G.\nCorollary 9. (DVIs) For problem (3), suppose we are given a sequence of parameters 0 < C1 < C2 < . . . < CK. Assume w\n\u2217(Ck) is known for an arbitrary integer 1 \u2264 k < K. Then, for Ck+1, we have [\u03b8\u2217(Ck+1)]i = \u03b1, i.e., i \u2208 R, if the following holds\n\u2212 Ck+Ck+12Ck \u3008w \u2217(Ck), aixi\u3009 \u2212 Ck+1\u2212Ck2Ck \u2016w \u2217(Ck)\u2016\u2016aixi\u2016 > biyi.\nSimilarly, we have [\u03b8\u2217(Ck+1)]i = \u03b2, i.e., i \u2208 L, if\n\u2212 Ck+Ck+12Ck \u3008w \u2217(Ck), aixi\u3009+ Ck+1\u2212Ck2Ck \u2016w \u2217(Ck)\u2016\u2016aixi\u2016 < biyi."}, {"heading": "5 Screening Rules for SVM", "text": "In Section 5.1, we first present the sequential DVI rules for SVM based on the results in Section 4. Then, in Section 5.2, we show how to strictly improve SSNSV [20] by the same technique used in DVI."}, {"heading": "5.1 DVI rules for SVM", "text": "Given a set of observations {xi, yi}li=1, where xi and yi \u2208 {1,\u22121} are the ith data instance and the corresponding class label, the SVM takes the form of:\nmin w\n1 2 \u2016w\u20162 + C l\u2211 i=1 [ 1\u2212wT (yixi) ] + . (24)\nIt is easy to see that, if we set \u03d5(t) = [t]+ and \u2212ai = bi = yi, problem (3) becomes the SVM problem. To construct the DVI rules for SVM by Corollaries 8 and 9, we only need to find \u03b1 and \u03b2. In fact, we have the following result:\nLemma 10. Let \u03d5(t) = [t]+, then \u03b1 = 0 and \u03b2 = 1, i.e.,\n\u03d5\u2217(s) = \u03b9[0,1]. (25)\nWe omit the proof of Lemma 10 since it is a direct application of Eq. (1). Then, we immediately have the following screening rules for the SVM problem. (For notational convenience, let x\u0304i = yixi and X = (x\u03041, . . . , x\u0304l) T .)\nCorollary 11. (DVI\u2217s for SVM) For problem (24), suppose we are given a sequence of parameters 0 < C1 < C2 < . . . < CK. Assume \u03b8\n\u2217(Ck) is known for an arbitrary integer 1 \u2264 k < K. Then, for Ck+1, we have [\u03b8\u2217(Ck+1)]i = 0, i.e., i \u2208 R, if the following holds\nCk+1+Ck 2 \u3008X T \u03b8\u2217(Ck), x\u0304i\u3009 \u2212 Ck+1\u2212Ck2 \u2016X T \u03b8\u2217(Ck)\u2016\u2016x\u0304i\u2016 > 1.\nSimilarly, we have [\u03b8\u2217(Ck+1)]i = 1, i.e., i \u2208 L, if\nCk+1+Ck 2 \u3008X T \u03b8\u2217(Ck), x\u0304i\u3009+ Ck+1\u2212Ck2 \u2016X T \u03b8\u2217(Ck)\u2016\u2016x\u0304i\u2016 < 1.\nCorollary 12. (DVIs for SVM) For problem (24), suppose we are given a sequence of parameters 0 < C1 < C2 < . . . < CK. Assume w\n\u2217(Ck) is known for an arbitrary integer 1 \u2264 k < K. Then, for Ck+1, we have [\u03b8\u2217(Ck+1)]i = 0, i.e., i \u2208 R, if the following holds\nCk+Ck+1 2Ck \u3008w\u2217(Ck), x\u0304i\u3009 \u2212 Ck+1\u2212Ck2Ck \u2016w \u2217(Ck)\u2016\u2016x\u0304i\u2016 > 1.\nSimilarly, we have [\u03b8\u2217(Ck+1)]i = 1, i.e., i \u2208 L, if Ck+Ck+1\n2Ck \u3008w\u2217(Ck), x\u0304i\u3009+ Ck+1\u2212Ck2Ck \u2016w \u2217(Ck)\u2016\u2016x\u0304i\u2016 < 1."}, {"heading": "5.2 Improving the existing method", "text": "In the rest of this section, we briefly describe how to strictly improve SSNSV [20] by using the same technique used in DVI rules (please refer to the supplement for more details). In view of Eq. (13), (R1\u2032) and (R2\u2032) can be rewritten as:\nmin w\u2208\u2126 \u3008w, x\u0304i\u3009 > 1\u21d2 [\u03b8\u2217(C)]i = 0\u21d4 i \u2208 R, (R1\u2032\u2032) max w\u2208\u2126 \u3008w, x\u0304i\u3009 < 1\u21d2 [\u03b8\u2217(C)]i = 1\u21d4 i \u2208 L, (R2\u2032\u2032)\nwhere \u2126 is a set which includes w\u2217(C) (notice that, we have already set \u2212ai = bi = yi, \u03b1 = 0 and \u03b2 = 1). It is easy to see that, the smaller \u2126 is, the tighter the bounds are in (R1\u2032\u2032) and (R2\u2032\u2032). Thus, more data instances\u2019 membership can be identified.\nEstimation of w\u2217 in SSNSV In [20], the authors consider the following equivalent formulation of SVM:\nmin w\n1 2 \u2016w\u20162, s.t. l\u2211 i=1 [1\u2212 yiwTxi]+ \u2264 s (26)\nLet Fs = {w : \u2211l i=1[1\u2212yiwTxi]+ \u2264 s}. Suppose we have two scalars sa > sb > 0, and Fsb 6= \u2205, w\u0302(sb) \u2208 Fsb . Then for s \u2208 [sb, sa], w\u2217(s) is inside the following region:\n\u2126[sb,sa] :=\n{ w :\n\u3008w\u2217(sa),w \u2212w\u2217(sa)\u3009 \u2265 0, \u2016w\u20162 \u2264 \u2016w\u0302(sb)\u20162\n} (27)\nEstimation of w\u2217 via VI By using the same technique as in DVI, we can conclude that w\u2217(s) is inside the region:\n\u2126\u2032[sb,sa] :=\n{ w :\n\u3008w\u2217(sa),w \u2212w\u2217(sa)\u3009 \u2265 0, \u2016w \u2212 12w\u0302(sb)\u2016 \u2264 1 2\u2016w\u0302(sb)\u2016\n} (28)\nWe can see that \u2126\u2032[sb,sa] \u2282 \u2126[sb,sa], and thus SSNSV can be strictly improved by the estimation in (28). The rule based on \u2126\u2032[sb, sa] is presented in Theorem 19 in the supplement, which is call the \u201cenhanced\u201d SSNSV (ESSNSV)."}, {"heading": "6 Screening Rules for LAD", "text": "In this section, we extend DVI rules in Section 4 to the least absolute deviations regression (LAD). Suppose we have a training set {xi, yi}li=1, where xi \u2208 <n and yi \u2208 <. The LAD problem takes the form of\nmin w\n1 2 \u2016w\u20162 + C l\u2211 i=1 |yi \u2212wTxi|. (29)\nWe can see that, if we set \u03d5(t) = |t| and \u2212ai = bi = 1, problem (3) becomes the LAD problem. To construct the DVI rules for LAD based on Corollaries 8 and 9, we need to find \u03b1 and \u03b2. Indeed, we have the following result:\nLemma 13. Let \u03d5(t) = |t|, then \u03b1 = \u22121 and \u03b2 = 1, i.e.,\n\u03d5\u2217(s) = \u03b9[\u22121,1]. (30)\nWe again omit the proof of Lemma 13 since it is a direct application of Eq. (1). Then, it is straightforward to derive the sequential DVI rules for the LAD problem.\nCorollary 14. (DVI\u2217s for LAD) For problem (29), suppose we are given a sequence of parameter values 0 < C1 < C2 < . . . < CK. Assume \u03b8\n\u2217(Ck) is known for an arbitrary integer 1 \u2264 k < K. Then, for Ck+1, we have [\u03b8\u2217(Ck+1)]i = \u22121 or 1, i.e., i \u2208 R or i \u2208 L, if the following holds respectively\n1. Ck+1+Ck2 \u3008X T \u03b8\u2217(Ck),xi\u3009 \u2212 Ck+1\u2212Ck2 \u2016X T \u03b8\u2217(Ck)\u2016\u2016xi\u2016 > yi. 2. Ck+1+Ck2 \u3008X T \u03b8\u2217(Ck),xi\u3009+ Ck+1\u2212Ck2 \u2016X T \u03b8\u2217(Ck)\u2016\u2016xi\u2016 < yi.\nCorollary 15. (DVIs for LAD) For problem (29), suppose we are given a sequence of parameter values 0 < C1 < C2 < . . . < CK. Assume w\n\u2217(Ck) is known for an arbitrary integer 1 \u2264 k < K. Then, for Ck+1, we have [\u03b8\u2217(Ck+1)]i = \u22121 or 1, i.e., i \u2208 R or i \u2208 L, if the following holds respectively\n1. Ck+1+Ck2Ck \u3008w \u2217(Ck),xi\u3009 \u2212 Ck+1\u2212Ck2Ck \u2016w \u2217(Ck)\u2016\u2016xi\u2016 > yi, 2. Ck+1+Ck2Ck \u3008w \u2217(Ck),xi\u3009+ Ck+1\u2212Ck2Ck \u2016w \u2217(Ck)\u2016\u2016xi\u2016 < yi.\nTo the best of our knowledge, ours are the first screening rules for LAD."}, {"heading": "7 Experiments", "text": "We evaluate DVI rules on both synthetic and real data sets. To measure the performance of the screening rules, we compute the rejection rate, that is, the ratio between the number of data instances whose membership can be identified by the rules and the total number of data instances. We test the rules along a sequence of 100 parameters of C \u2208 [10\u22122, 10] equally spaced in the logarithmic scale.\nIn Section 7.1, we compare the performance of DVI rules with SSNSV [20], which is the only existing method for identifying non-support vectors in SVM. Notice that, both of DVI rules and SSNSV are safe in the sense that no support vectors will be mistakenly discarded. We then evaluate DVI rules for LAD in Section 7.2."}, {"heading": "7.1 DVI for SVM", "text": "In this experiment, we first apply DVIs to three simple 2D synthetic data sets to illustrate the effectiveness of the proposed screening methods. Then we compare the performance of DVIs, SSNSV and ESSNSV on: (a) IJCNN1 data set [23]; (b) Wine Quality data set [8]; (c) Forest Covertype data set [14]. The original Forest Covertype data set includes 7 classes. We randomly pick two of the seven classes to construct the data set used in this paper.\nSynthetic Data Sets In this experiment, we show that DVIs are very effective in discarding nonsupport vectors even for largely overlapping classes. We evaluate DVIs rules on three synthetic data sets, i.e., Toy1, Toy2 and Toy3, plotted in the first row of Fig. 1. For each data set, we generate two classes. Each class has 1000 data points and is generated from N({\u00b5, \u00b5}T , 0.752I), where I \u2208 <2\u00d72 is the identity matrix. For the positive classes (the red dots), \u00b5 = 1.5, 0.75, 0.5, for Toy1, Toy2 and Toy 3, respectively; and \u00b5 = \u22121.5,\u22120.75,\u22120.5, for the negative classes (the blue dots). From the plots, we can observe that when |\u00b5| decreases, the two classes increasingly overlap and thus the number of data instances belong to the set L increases.\nThe second row of Fig. 1 presents the stacked area charts of the rejection rates. For convenience, let R\u0303 and L\u0303 be the indices of data instances which are identified by DVIs as members of R and L, respectively. Then, the blue and red regions present the ratios of |R\u0303|/l and |L\u0303|/l (recall that, l is the number of data\ninstances, which is 2000 for this experiment). We can see that, for Toy1, the two classes are clearly apart from each other and thus most of the data instances belong to the set R. The first chart in the second row of Fig. 1 indicates that the proposed DVIs can identify almost all of the non-support vectors and thus the speedup is almost 60 times compared to the solver without screening (please refer to Table 1). When the two classes have a large overlap, e.g., Toy3, the number of data instances in L significantly increases. This will generally impose great challenge for the solver. But even for this challenging case, DVIs is still able to identify a large portion of the non-support vectors as indicated by the last charts in the second row of Fig. 1. Notice that, for Toy3, |L\u0303| is comparable to |R\u0303|. Table 1 shows that the speedup gained by DVIs is about 25 times for this challenging case. It is worthwhile to mention that the running time of \u201cSolver+DVIs\u201d in Table 1 includes the running time (the 5th column of Table 1) for solving SVM with the smallest parameter value.\nReal Data Sets In this experiment, we compare the performance of SSNSV, ESSNSV and DVIs in terms of the rejection ratio, that is, the ratio between the number of data instances identified as members\nof R or L by the screening rules and the number of total data instances. Fig. 2 shows the rejection ratios of the three screening rules on three real data sets. We can observe that DVIs rules identify far more nonsupport vectors than SSNSV and ESSNSV. For IJCNN1, about 80% of the data instances are identified as non-support vectors by DVIs. Therefore, as indicated by Table 2 the speedup gained by DVIs is about 5 times. For the Wine data set, more than 80% of the data instances are identified to belong to R or L by DVIs. As indicated in Table 2, the speedup is about 6 times gained by DVIs. For the Forest Covertype data set, almost all of data instances\u2019 membership can be determined by DVIs. Table 2 shows that the speedup gained by DVIs is almost 80 times, which is much higher than that of SSNSV and ESSNSV. Moreover, Fig. 2 demonstrates that ESSNSV is more effective in identifying non-support vectors than SSNSV, which is consistent with our analysis."}, {"heading": "7.2 DVI for LAD", "text": "In this experiment, we evaluate the performance of DVIs for LAD on three real data sets: (a) Magic Gamma Telescope data set [2]; (b) Computer data set [25]; (c) Houses data set [21]. Fig. 3 shows the rejection ratio of DVIs rules for the three data sets. We can observe that the rejection ratio of DVIs on Magic Gamma Telescope data set is about 90%, leading to a 10 times speedup as indicated in Table 3. For the Computer and Houses data sets, we can see that the rejection rates are very close to 100%, i.e., almost all of the data instances\u2019 membership can be determined by the DVIs rules. As expected, Table 3 shows that the resulting speedup are about 20 and 115 times, respectively. Notice that, the speedup for the Houses data set is more than two orders of magnitude. These results demonstrate the effectiveness of the proposed DVI rules."}, {"heading": "8 Conclusion", "text": "In this paper, we develop new screening rules for a class of supervised learning problems by studying their dual formulation with the variational inequalities. Our framework includes two well known models, i.e., SVM and LAD, as special cases. The proposed DVI rules are very effective in identifying non-support vectors for both SVM and LAD, and thus result in substantial savings in the computational cost and memory. Extensive experiments on both synthetic and real data sets demonstrate the effectiveness of the proposed DVI rules. We plan to extend the framework of DVI to other supervised learning problems, e.g., weighted SVM [32], RWLS (robust weighted least squres) [6], robust PCA [9], robust matrix factorization [19]."}, {"heading": "A Proof of Lemma 2", "text": "Before we prove Lemma 2, let us cite the following technical lemma.\nLemma 16. [15] The function f is equal to its biconjugate f\u2217\u2217 if and only if f \u2208 \u03930(<n). We are now ready to derive a simple proof of Lemma 2 based on Lemma 16.\nProof. In order to show \u03d5\u2217\u2217 = \u03d5, it is enough to show \u03d5 \u2208 \u03930(<) according to Lemma 16. Therefore we only to check the following three conditions:\n1). Properness: because \u03d5 : < \u2192 <+, i.e., there exists t \u2208 < such that \u03d5(t) is finite, \u03d5 is proper. 2). Lower semi-continuality: \u03d5 is lower semicontinuous because it is continuous. 3). Convexity: the convexity of \u03d5 is due to the its sublinearity, see Definition 1. Thus, we have \u03d5 \u2208 \u03930(<), which completes the proof."}, {"heading": "B Proof of Lemma 3", "text": "To prove Lemma 3, we need to following results.\nLemma 17. [26] Let Z \u2286 <n be a convex and closed set. Let us define the support function of Z as\n\u03c3Z(s) := sup x\u2208Z\nsTx, (31)\nand the indicator function \u03b9Z as\n\u03b9Z(x) = { 0, if x \u2208 Z, \u221e, otherwise.\n(32)\nThen\n\u03c3\u2217Z = \u03b9Z , amd \u03b9 \u2217 Z = \u03c3Z . (33)\nTheorem 18. [15] Let \u03c3 \u2208 \u03930(<n) be a sublinear function, then \u03c3 is the support function of the nonempty closed convex set\nS\u03c3 := {s \u2208 <n : sTd \u2264 \u03c3(d), \u2200d \u2208 <n}. (34)\nWe are now ready to prove Lemma 3.\nProof. Due to Lemma 17 and Theorem 18, we can see that, there is a nonempty closed convex set Z \u2286 < such that\n\u03d5(t) = sup s\u2208Z\nst, \u2200t \u2208 <, (35)\nwhere\nZ := {s : st \u2264 \u03d5(t), \u2200t \u2208 <}. (36)\nLet t = 1 and \u22121 respectively, Eq. (36) implies that\nsup s\u2208Z s \u2264 \u03d5(1) and inf s\u2208Z s \u2265 \u03d5(\u22121). (37)\nTherefore, Z is a closed and bounded interval, i.e., Z = [\u03b1, \u03b2] with \u03b1, \u03b2 \u2208 <. Next, let us show that \u03b1 6= \u03b2. In fact, in view of the nonnegativity of \u03d5 and Eq. (36), it is easy to see that 0 \u2208 Z. Therefore, if \u03b1 = \u03b2, we must have Z = {0}. Thus, Lemma 17 implies that\n\u03d5 = \u03b9\u2217Z \u2261 0, (38)\nwhich contradicts the fact that \u03d5 is a nonconstant function. Hence, we can conclude that \u03b1 < \u03b2, which completes the proof."}, {"heading": "C Derivation of the KKT Condition in Eq. (14)", "text": "The problem in (12) can be written as follows:\nmin \u03b8\nC 2 \u2016ZT \u03b8\u20162 \u2212 \u3008y\u0304, \u03b8\u3009, (39)\ns.t. \u03b8i \u2208 [\u03b1, \u03b2], i = 1, . . . , l.\nTherefore, we can see that the Lagrangian is\nL(\u03b8, \u00b5, \u03bd) = C\n2 \u2016ZT \u03b8\u20162 \u2212 \u3008y\u0304, \u03b8\u3009+ l\u2211 i=1 \u00b5i(\u03b1\u2212 \u03b8i) + l\u2211 i=1 \u03bdi(\u03b8i \u2212 \u03b2), (40)\nwhere \u00b5 = (\u00b51, . . . , \u00b5l) T , \u03bd = (\u03bd1, . . . , \u03bdl) T , and \u00b5i \u2265 0, \u03bdi \u2265 0 for all i = 1, . . . , l. \u00b5 and \u03bd are in fact the vector of Lagrangian multipliers.\nFor simplicity, let us denote \u03b8\u2217(C) by \u03b8\u2217. Then the KKT conditions [3] are\n\u2202L(\u03b8, \u00b5, \u03bd)\n\u2202\u03b8 |\u03b8\u2217 = 0\u21d2 CZZT \u03b8\u2217 \u2212 y\u0304 \u2212 \u00b5+ \u03bd = 0, (41)\n\u00b5i(\u03b1\u2212 \u03b8\u2217i ) = 0, \u03bdi(\u03b8 \u2217 i \u2212 \u03b2) = 0, i = 1, . . . , l. (42)\nEq. (42) is known as the complementary slackness condition. The equation in (41) actually involves l equations. We can write down the ith equation as follows:\nC\u3008ZT \u03b8\u2217, aixi\u3009 \u2212 \u00b5i + \u03bdi = biyi. (43)\nRecall that the ith column of Z is aixi. In view of Eq. (42) and Eq. (43), we can see that: 1. if \u03b8\u2217i = \u03b1, then \u03bdi = 0 and Eq. (43) results in\nC\u3008ZT \u03b8\u2217, aixi\u3009 \u2265 biyi; (44)\n2. if \u03b8\u2217i \u2208 (\u03b1, \u03b2), then \u00b5i = \u03bdi = 0 and Eq. (43) results in\nC\u3008ZT \u03b8\u2217, aixi\u3009 = biyi; (45)\n3. if \u03b8\u2217i = \u03b2, then \u00b5i = 0 and Eq. (43) results in\nC\u3008ZT \u03b8\u2217, aixi\u3009 \u2264 biyi. (46)\nThen, in view of the inequalities in (44), (45) and (46), and Eq. (13), it is straightforward to derive the KKT condition in (14)."}, {"heading": "D Proof of Lemma 4", "text": "Proof. The first part of the statement is trivial by the definition of R\u0302 and L\u0302. Therefore, we only consider the second part of the statement.\nLet G = ZZT . By permuting the columns and rows of G, we have\nG\u0302 = ( G\u030211 G\u030212 G\u030221 G\u030222 ) = ( [XT ]TS\u0302c [X T ]S\u0302c [X T ]TS\u0302c [X T ]S\u0302\n[XT ]TS\u0302 [X T ]S\u0302c [X T ]TS\u0302 [X T ]S\u0302\n) .\nAs a result, the objective function of problem (12) can be rewritten as\nC 2 [\u03b8]TS\u0302cG\u030211[\u03b8]S\u0302c \u2212 y\u0302 T [\u03b8]S\u0302c +R([\u03b8]S\u0302) (47)\nwhere\ny\u0302 = yS\u0302c \u2212 CG\u030212[\u03b8]S\u0302 , , (48)\nR([\u03b8]S\u0302) = C\n2 [\u03b8]TS\u0302 G\u030222[\u03b8]S\u0302 \u2212 y T S\u0302 [\u03b8]S\u0302 (49)\nDue to the assumption that [\u03b8\u2217(C)]S\u0302 is known, y\u0302 and R([\u03b8]S\u0302) can be treated as constants, and thus problem (12) reduces to problem (15).\nE Improving SSNSV via VI\nIn this section, we describe how to strictly improve SSNSV by using the same technique used in DVI rules in a detailed manner.\nEstimation of w\u2217 via VI We show that \u2126[sb,sa] in Eq. (27) can be strictly improved by the variational inequalities. Consider Fsa .\nBecause sa > sb, we can see that w \u2217(sb) \u2208 Fsa . Therefore, by Theorem 5, we have\n\u3008w\u2217(sa),w\u2217(s)\u2212w\u2217(sa)\u3009 \u2265 0, (50)\nwhich is the first constraint in (27). Similarly, consider Fsb . Since w\u0302(sb) \u2208 Fsb , Theorem 5 implies that\n\u3008w\u2217(s), w\u0302(sb)\u2212w\u2217(s)\u3009 \u2265 0,\nwhich is equivalent to\n\u2016w\u2217(s)\u2212 12w\u0302(sb)\u2016 \u2264 1 2\u2016w\u0302(sb)\u2016. (51)\nClearly, the radius determined by the inequality (51) is only a half of the radius determined by the second constraint in (27). In view of the inequalities in (50) and (51), we can see that w\u2217(s) can be bounded inside the following region:\n\u2126\u2032[sb,sa] :=\n{ w :\n\u3008w\u2217(sa),w \u2212w\u2217(sa)\u3009 \u2265 0, \u2016w \u2212 12w\u0302(sb)\u2016 \u2264 1 2\u2016w\u0302(sb)\u2016 } It is easy to see that \u2126\u2032[sb,sa] \u2282 \u2126[sb,sa]. As a result, the bounds in (R1 \u2032) and (R2\u2032) with \u2126\u2032[sb,sa] are tighter than that of \u2126[sb,sa]. Thus, SSNSV [20] can be strictly improved by the estimation in (28). In fact, we have the following theorem:\nTheorem 19. Suppose we are given two parameters sa > sb > 0, and let w \u2217(sa) and w\u0302(sb) be the optimal solution at s = sa and a feasible solution at s = sb, respectively. Moreover, let us define\n\u03c1 = \u2212\u2016w\u2217(sa)\u20162 + 12 \u3008w \u2217(sa), w\u0302(sb)\u3009\nv\u22a5 = v \u2212 v Tw\u2217(sa) \u2016w\u2217(sa)\u20162 w \u2217(sa),\u2200v \u2208 <n.\nThen, for all s \u2208 [sb, sa],\n\u3008w\u2217(sa), x\u0304i\u3009 > 2\u2016x\u0304i\u2016\u2016w\u0302(sb)\u2016\u03c1 and `i > 1\u21d2 i \u2208 R \u21d4 \u03b1i = 0, (52)\nwhere\n`i = \u2212 \u3008w \u2217(sa),x\u0304i\u3009 \u2016w\u2217(sa)\u20162 \u03c1+ 1 2 \u3008w\u0302(sb), x\u0304i\u3009 \u2212 \u2016x\u0304 \u22a5 i \u2016 \u221a 1 4 \u2016w\u0302(sb)\u20162 \u2212 \u03c1 2 \u2016w\u2217(sa)\u20162 . (53)\nSimilarly,\nui < 1\u21d2 i \u2208 L \u21d4 \u03b1i = c, (54)\nwhere\nui =  1 2 (\u3008w\u0302(sb), x\u0304i\u3009+ \u2016w\u0302(sb)\u2016\u2016x\u0304i\u2016) , if \u3008w\u2217(sa), x\u0304i\u3009 \u2265 \u2212 2\u2016x\u0304i\u2016\u2016w\u0302(sb)\u2016\u03c1 \u2212 \u3008w \u2217(sa),x\u0304i\u3009 \u2016w\u2217(sa)\u20162 \u03c1+ 1 2 \u3008w\u0302(sb), x\u0304i\u3009 +\u2016x\u0304\u22a5i \u2016 \u221a 1 4\u2016w\u0302(sb)\u20162 \u2212 \u03c12 \u2016w\u2217(sa)\u20162 ,\notherwise.\n(55)\nFor convenience, we call the screening rule presented in Theorem 19 as the \u201cenhanced\u201d SSNSV (ESSNSV). To prove Theorem 19, we first establish the following technical lemma.\nLemma 20. Consider the problem as follows:\nmin w\nf(w) = vTw, s.t. uTw \u2264 d, \u2016w \u2212 o\u2016 \u2264 r, (56)\nwhere r > 0. Let d\u2032 = d\u2212 uTo and the optimal solution of problem (56) be f\u2217. Then we have\n1. If vTu + \u2016v\u2016d \u2032\nr \u2265 0, then\nf\u2217 = vTo\u2212 r\u2016v\u2016.\n2. Otherwise,\nf\u2217 = vTo\u2212 \u2016v\u22a5\u2016 \u221a r2 \u2212 (d \u2032)2\n\u2016u\u20162 +\nvTud\u2032\n\u2016u\u20162 ,\nwhere v\u22a5 = v \u2212 v Tu \u2016u\u20162 u.\nNotice that, we assume problem (56) is feasible, i.e., |u T o\u2212d| \u2016u\u2016 \u2264 r.\nProof. Let z = w \u2212 o, problem (56) can be rewritten as:\nmin z\nvT z + vTo, s.t. uT z \u2264 d\u2212 uTo, \u2016z\u2016 \u2264 r. (57)\nProblem (57) reduces to\nmin z\nvT z, s.t. uT z \u2264 d\u2032, \u2016z\u2016 \u2264 r. (58)\nTo solve problem (58), we make use of the Lagrangian multiplier method. For notational convenience, let F := {z : uT z \u2264 d\u2032, \u2016z\u2016 \u2264 r}.\nmin z\u2208F vT z = min z max \u00b5\u22650, \u03bd\u22650\nvT z + \u03bd(uT z\u2212 d\u2032) + \u00b5 2 (\u2016z\u20162 \u2212 r2)\n= max \u00b5\u22650, \u03bd\u22650 min z\nvT z + \u03bd(uT z\u2212 d\u2032) + \u00b5 2 (\u2016z\u20162 \u2212 r2)\n= max \u00b5\u22650, \u03bd\u22650\n\u2212 1 2\u00b5 \u2016v + \u03bdu\u20162 \u2212 \u03bdd\u2032 \u2212 \u00b5r\n2\n2 . (59)\nNotice that, in Eq. (59), we make the assumption that \u00b5 > 0. However, we can not simply exclude this possibility. In fact, if \u00b5 = 0, we must have\nv + \u03bdu = 0, (60)\nsince otherwise the function value of vT z + \u03bd(uT z\u2212 d\u2032)\nin the second line of Eq. (59) can be made arbitrarily small. As a result, we will have\ng(\u00b5, \u03bd) = \u2212\u221e,\nwhich contradicts the strong duality of problem (56) [3]. [Problem (56) is clearly lower bounded since the feasible set is compact.] Therefore, in view of Eq. (60), we can conclude that \u00b5 = 0 only if v point in the opposite direction of u.\nLet us first consider the general case, i.e., v does not point in the opposite direction of u. In view of\nEq. (59), let g(\u00b5, \u03bd) = \u2212 12\u00b5\u2016v + \u03bdu\u2016 2 \u2212 \u03bdd\u2032 \u2212 \u00b5r\n2\n2 . It is easy to see that\n\u2202g(\u00b5, \u03bd) \u2202\u03bd = 0\u21d4 \u03bd = \u2212v\nTu + \u00b5d\u2032\n\u2016u\u20162 . (61)\nSince \u03bd has to be nonnegative, we have\n\u03bd = max { 0,\u2212v Tu + \u00b5d\u2032\n\u2016u\u20162\n} . (62)\nCase 1. If \u2212v Tu+\u00b5d\u2032\n\u2016u\u20162 \u2264 0, then \u03bd = 0 and thus\n\u2202g(\u00b5, \u03bd) \u2202\u00b5 = 0\u21d4 \u00b5 = \u2016v\u2016 r . (63)\nThen g(\u00b5, \u03bd) = \u2212r\u2016v\u2016 and the optimal value of problem (56) is given by\nvTo\u2212 r\u2016v\u2016. (64)\nCase 2. If \u2212v Tu+\u00b5d\u2032 \u2016u\u20162 > 0, then \u03bd = \u2212 vTu+\u00b5d\u2032 \u2016u\u20162 and\ng(\u00b5, \u03bd) = \u2212 1 2\u00b5 \u2016v\u22a5\u20162 \u2212 \u00b5 2\n( r2 \u2212 (d \u2032)2\n\u2016u\u20162\n) + vTud\u2032\n\u2016u\u20162 , (65)\nThus,\n\u2202g(\u00b5, \u03bd) \u2202\u00b5 = 0\u21d4 \u00b5 = \u2016v \u22a5\u2016\u221a r2 \u2212 (d \u2032)2\n\u2016u\u20162\n(66)\nThen g(\u00b5, \u03bd) = \u2212\u2016v\u22a5\u2016 \u221a r2 \u2212 (d \u2032)2\n\u2016u\u20162 + vTud\u2032 \u2016u\u20162 and the optimal value of problem (56) is given by\nvTo\u2212 \u2016v\u22a5\u2016 \u221a r2 \u2212 (d \u2032)2\n\u2016u\u20162 +\nvTud\u2032\n\u2016u\u20162 . (67)\nNow let us consider the case with v pointing in the opposite direction of u. We can see that there exists \u03b3 = \u2212 u Tv \u2016u\u20162 > 0 such that v = \u2212\u03b3u. By plugging v = \u2212\u03b3u in problem (56) and following an analogous argument as before, we can see that the statement in Lemma 20 is also applicable to this case. Therefore, the proof of the statement is completed.\nWe are now ready to prove Theorem 19.\nProof. To prove the statements in (52) and (53), we only need to set\nv := x\u0304i, u : \u2212w\u2217(sa), d := \u2212\u2016w\u2217(sa)\u20162, o := 12w\u0302, r := 1 2\u2016w\u0302\u2016,\nd\u2032 := \u03c1 = \u2212\u2016w\u2217(sa)\u20162 + 1\n2 \u3008w\u2217(sa), w\u0302\u3009,\nand then apply Lemma 20. Notice that, for case 1, the optimal value\nf\u2217 = 1\n2 (\u3008w\u0302(sb), x\u0304i\u3009 \u2212 \u2016w\u0302(sb)\u2016\u2016x\u0304i\u2016) \u2264 0,\nand thus none of the non-support vectors can be identified [recall that, according to (R1\u2032), f\u2217 has to be larger than 1 such that x\u0304i can be detected as a non-support vector]. As a result, we only need to consider case 2.\nThe statement in (54) and (55) follows with an analogous argument by noting that\nmax w\u2208\u0398\u2032\n[sb,sa]\n\u3008w, x\u0304i\u3009 = \u2212 min w\u2208\u0398\u2032\n[sb,sa]\n\u2212\u3008w, x\u0304i\u3009."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>The support vector machine (SVM) is a widely used method for classification. Although many<lb>efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-<lb>scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting<lb>classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-<lb>support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result,<lb>the number of data instances to be entered into the optimization can be substantially reduced. Some<lb>appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded<lb>by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run<lb>the screening, whose computational cost is negligible compared to that of solving the SVM problem; (3)<lb>DVI is independent of the solvers and can be integrated with any existing efficient solvers. We also show<lb>that the DVI technique can be extended to detect non-support vectors in the least absolute deviations<lb>regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD. We<lb>have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly<lb>outperforms the existing state-of-the-art screening rules for SVM, and is very effective in discarding<lb>non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.", "creator": "LaTeX with hyperref package"}}}