{"id": "1311.4296", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2013", "title": "Reflection methods for user-friendly submodular optimization", "abstract": "parasitise Recently, it alemka has zapolski become sistiaga evident folkways that submodularity pzkpfw naturally captures widely berriedale occurring 875 concepts in machine 101.31 learning, signal cena processing screamingly and neese computer vision. Consequently, backtracks there illinova is shoutin need for transcriber efficient optimization procedures egoistic for postfeminist submodular functions, albenga especially 767-200 for minimization problems. While butlins general all-rounders submodular minimization is whitwam challenging, wedbush we alkan propose gwydir a new plasticizer method masker that exploits existing decomposability enlight of vergakis submodular voivodship functions. premiering In contrast to platja previous approaches, our method is 5440 neither re-married approximate, modrica nor impractical, nor terashima does af1 it firhill need makhosini any pyrokinetic cumbersome fci parameter tuning. Moreover, nipissing it joffre is u.c. easy relaid to implement hap and \u0161m\u00edd parallelize. composure A a2000 key then-ruling component bridgen of shipu our warchus method is a formulation traub of the discrete smythson submodular minimization problem as a conquistador continuous best test-tube approximation problem that sametime is thackwell solved through rvm a acftu sequence of reflections, dyakov and standardizing its kakul solution can stubbings be cornwallis easily hel101 thresholded kiya to obtain transparencia an georges-louis optimal secher discrete initiations solution. This method solves weight both the kentuckians continuous nishadham and discrete hadba formulations of goateed the mausi problem, and therefore has applications in peevishness learning, inference, and gullible reconstruction. bahjat In our ifad experiments, neruda we illustrate dekom the treman benefits cariocas of scorsese our method on two busier image massereene segmentation tasks.", "histories": [["v1", "Mon, 18 Nov 2013 08:48:13 GMT  (1583kb)", "http://arxiv.org/abs/1311.4296v1", "Neural Information Processing Systems (NIPS), \\'Etats-Unis (2013)"]], "COMMENTS": "Neural Information Processing Systems (NIPS), \\'Etats-Unis (2013)", "reviews": [], "SUBJECTS": "cs.LG cs.NA cs.RO math.OC", "authors": ["stefanie jegelka", "francis r bach", "suvrit sra"], "accepted": true, "id": "1311.4296"}, "pdf": {"name": "1311.4296.pdf", "metadata": {"source": "CRF", "title": "Reflection methods for user-friendly submodular optimization", "authors": ["Stefanie Jegelka"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n31 1.\n42 96\nv1 [\ncs .L\nG ]\n1 8\nN ov"}, {"heading": "1 Introduction", "text": "Submodularity is a rich combinatorial concept that expresses widely occurring phenomena such as diminishing marginal costs and preferences for grouping. A set function F : 2V \u2192 R on a set V is submodular if for all subsets S, T \u2286 V , we have F (S \u222a T ) + F (S \u2229 T ) \u2264 F (S) + F (T ). Submodular functions underlie the goals of numerous problems in machine learning, computer vision and signal processing [1]. Several problems in these areas can be phrased as submodular optimization tasks: notable examples include graph cut-based image segmentation [9], sensor placement [32], or document summarization [34]. A longer list of examples may be found in [1].\nThe theoretical complexity of submodular optimization is well-understood: unconstrained minimization of submodular set functions is polynomial-time [21] while submodular maximization is NPhard. Algorithmically, however, the picture is different. Generic submodular maximization admits efficient algorithms that can attain approximate optima with global guarantees; these algorithms are typically based on local search techniques [18, 38]. In contrast, although polynomial-time solvable, submodular function minimization (SFM) which seeks to solve\nmin S\u2286V F (S), (1)\nposes substantial algorithmic difficulties. This is partly due to the fact that one is commonly interested in an exact solution (or an arbitrarily close approximation thereof), and \u201cpolynomial-time\u201d is not necessarily equivalent to \u201cpractically fast\u201d.\nSubmodular minimization algorithms may be obtained from two main perspectives: combinatorial and continuous. Combinatorial algorithms for SFM typically use close connections to matroid and\nmaximum flow methods; the currently theoretically fastest combinatorial algorithm for SFM scales as O(n6 + n5\u03c4), where \u03c4 is the time to evaluate the function oracle [40] (for an overview of other algorithms, see e.g., [36]). These combinatorial algorithms are typically nontrivial to implement.\nContinuous methods offer an alternative by instead minimizing a convex extension. This idea exploits the fundamental connection between a submodular function F and its Lova\u0301sz extension f [35], which is continuous and convex. The SFM problem (1) is then equivalent to\nmin x\u2208[0,1]n f(x). (2)\nThe Lova\u0301sz extension f is nonsmooth, so we might have to resort to subgradient methods. While a fundamental result of Edmonds [17] demonstrates that a subgradient of f can be computed in O(n log n) time, subgradient methods can be sensitive to choices of the step size, and can be slow. They theoretically converge at a rate of O(1/ \u221a t) (after t iterations). The \u201csmoothing technique\u201d of [39] does not in general apply here because computing a smoothed gradient is equivalent to solving the submodular minimization problem. We discuss this issue further in Section 2.\nAn alternative to minimizing the Lova\u0301sz extension directly on [0, 1]n is to consider a slightly modified convex problem. Specifically, the exact solution of the discrete problem minS\u2286V F (S) and of its nonsmooth convex relaxation minx\u2208[0,1]n f(x) may be found as a level set S0 = {k | x\u2217k > 0} of the unique point x\u2217 that minimizes the strongly convex function [1, 12]:\nf(x) + 12\u2016x\u20162. (3) We will refer to the minimization of (3) as the proximal problem due to its close similarity to proximity operators used in convex optimization [14]. When F is a cut function, (3) becomes a total variation problem (see, e.g., [11] and references therein) that also occurs in other regularization problems [1]. Two noteworthy points about (3) are: (i) addition of the strongly convex component 1 2\u2016x\u20162; (ii) the ensuing removal of the box-constraints x \u2208 [0, 1]n. These changes allow us to consider a convex dual which is amenable to smooth optimization techniques.\nTypical approaches to generic SFM include Frank-Wolfe methods [19] that have cheap iterations and O(1/t) convergence, but can be quite slow in practice (Section 5); or the minimum-normpoint/Fujishige-Wolfe algorithm [22] that has expensive iterations but finite convergence. Other recent methods are approximate [26]. In contrast to several iterative methods based on convex relaxations, we seek to obtain exact discrete solutions.\nTo the best of our knowledge, all generic algorithms that use only submodularity are several orders of magnitude slower than specialized algorithms when they exist (e.g., for graph cuts). However, the submodular function is not always generic and given via a black-box, but has known structure. Following [29, 31, 41, 44], we make the assumption that F (S) =\n\u2211r i=1 Fi(S) is a sum of sufficiently\n\u201csimple\u201d functions (see Sec. 3). This structure allows the use of (parallelizable) dual decomposition techniques for the problem in Eq. (2), with [13, 41] or without [31] Nesterov\u2019s smoothing technique, or with direct smoothing [44] techniques. But existing approaches typically have two drawbacks: (a) they use smoothing or step-size parameters whose selection may be critical and quite tedious; and (b) they still exhibit slow convergence (see Section 5).\nThese drawbacks arise from working with formulation (2). Our main insight is that, despite seemingly counter-intuitive, the proximal problem (3) offers a much more user-friendly tool for solving (1) than its natural convex counterpart (2), both in implementation and running time. We approach Problem (3) via its dual. This allows decomposition techniques which combine well with orthogonal projection and reflection methods that (a) exhibit faster convergence, (b) are easily parallelizable, (c) require no extra hyperparameters, and (d) are extremely easy to implement.\nThe main three algorithms that we consider are: (i) dual block-coordinate descent (equivalently, primal-dual proximal-Dykstra), which was already shown to be extremely efficient for total variation problems [3] that are special cases of Problem (3); (ii) Douglas-Rachford splitting using the careful variant of [5], which for our formulation (Section 4.2) requires no hyper-parameters; and (iii) accelerated projected gradient [6]. We will see these alternative algorithms can offer speedups beyond known efficiencies. Our observations have two implications: first, from the viewpoint of solving Problem (3), they offers speedups for often occurring denoising and reconstruction problems that employ total variation. Second, our experiments suggest that projection and reflection methods can work very well for solving the combinatorial problem (1).\nIn summary, we make the following contributions:\n(1) In Section 3, we cast the problem of minimizing decomposable submodular functions as an orthogonal projection problem and show how existing optimization techniques may be brought to bear on this problem, to obtain fast, easy-to-code and easily parallelizable algorithms. In addition, we show examples of classes of functions amenable to our approach. In particular, for simple functions, i.e., those for which minimizing F (S)\u2212 a(S) is easy for all vectors1 a \u2208 Rn, the problem in Eq. (3) may be solved in O(log 1\u03b5 ) calls to such minimization routines, to reach a precision \u03b5 (Section 2, 3, Appendix B). (2) In Section 5, we demonstrate the empirical gains of using accelerated proximal methods, Douglas-Rachford and block coordinate descent methods over existing approaches: fewer hyperparameters and faster convergence."}, {"heading": "2 Review of relevant results from submodular analysis", "text": "The relevant concepts we review here are the Lova\u0301sz extension, base polytopes of submodular functions, and relationships between proximal and discrete problems. For more details, see [1, 21].\nLova\u0301sz extension and convexity. The power set 2V may be naturally identified with the vertices of the hypercube, i.e., {0, 1}n. The Lova\u0301sz extension f of any set function is defined by linear interpolation, so that for any S \u2282 V , F (S) = f(1S). It may be computed in closed form once the components of x are sorted: if x\u03c3(1) > \u00b7 \u00b7 \u00b7 > x\u03c3(n), then f(x) = \u2211n k=1 x\u03c3(k) [\nF ({\u03c3(1), . . . , \u03c3(k)})\u2212 F ({\u03c3(1), . . . , \u03c3(k \u2212 1)}) ] [35]. For the graph cut function, f is the total variation.\nIn this paper, we are going to use two important results: (a) if the set function F is submodular, then its Lova\u0301sz extension f is convex, and (b) minimizing the set function F is equivalent to minimizing f(x) with respect to x \u2208 [0, 1]n. Given x \u2208 [0, 1]n, all of its level sets may be considered and the function may be evaluated (at most n times) to obtain a set S. Moreover, for a submodular function, the Lova\u0301sz extension happens to be the support function of the base polytope B(F ) defined as B(F ) = {y \u2208 Rn | \u2200S \u2282 V, y(S) 6 F (S) and y(V ) = F (V )}, that is f(x) = maxy\u2208B(F ) y\u22a4x [17]. A maximizer of y\u22a4x (and hence the value of f(x)), may be computed by the \u201cgreedy algorithm\u201d, which first sorts the components of w in decreasing order x\u03c3(1) > \u00b7 \u00b7 \u00b7 > x\u03c3(n), and then compute y\u03c3(k) = F ({\u03c3(1), . . . , \u03c3(k)})\u2212 F ({\u03c3(1), . . . , \u03c3(k \u2212 1)}). In other words, a linear function can be maximized over B(F ) in time O(n log n + n\u03c4) (note that the term n\u03c4 may be improved in many special cases). This is crucial for exploiting convex duality.\nDual of discrete problem. We may derive a dual problem to the discrete problem in Eq. (1) and the convex nonsmooth problem in Eq. (2), as follows:\nmin S\u2286V F (S) = min x\u2208[0,1]n f(x) = min x\u2208[0,1]n max y\u2208B(F ) y\u22a4x = max y\u2208B(F ) min x\u2208[0,1]n y\u22a4x = max y\u2208B(F ) (y)\u2212(V ), (4)\nwhere (y)\u2212 = min{y, 0} applied elementwise. This allows to obtain dual certificates of optimality from any y \u2208 B(F ) and x \u2208 [0, 1]n. Proximal problem. The optimization problem (3), i.e., minx\u2208Rn f(x) + 12\u2016x\u20162, has intricate relations to the SFM problem [12]. Given the unique optimal solution x\u2217 of (3), the maximal (resp. minimal) optimizer of the SFM problem is the set S\u2217 of nonnegative (resp. positive) elements of x\u2217. More precisely, solving (3) is equivalent to minimizing F (S) + \u00b5|S| for all \u00b5 \u2208 R. A solution S\u00b5 \u2286 V is obtained from a solution x\u2217 as S\u2217\u00b5 = {i | x\u2217i > \u00b5}. Conversely, x\u2217 may be obtained from all S\u2217\u00b5 as x \u2217 k = sup{\u00b5 \u2208 R | k \u2208 S\u2217\u00b5} for all k \u2208 V . Moreover, if x is an \u03b5-optimal solution\nof Eq. (3), then we may construct \u221a \u03b5n-optimal solutions for all S\u00b5 [1; Prop. 10.5]. In practice, the duality gap of the discrete problem is usually much lower than that of the proximal version of the same problem, as we will see in Section 5. Note that the problem in Eq. (3) provides much more information than Eq. (2), as all \u00b5-parameterized discrete problems are solved.\nThe dual problem of Problem (3) reads as follows:\nmin x\u2208Rn f(x)+ 12\u2016x\u201622 = minx\u2208Rn maxy\u2208B(F ) y \u22a4x+ 12\u2016x\u201622 = maxy\u2208B(F ) minx\u2208Rn y \u22a4x+ 12\u2016x\u201622 = maxy\u2208B(F )\u2212 1 2\u2016y\u201622, where primal and dual variables are linked as x = \u2212y. Observe that this dual problem is equivalent to finding the orthogonal projection of 0 onto B(F ).\n1Every vector a \u2208 Rn may be viewed as a modular (linear) set function: a(S) , \u2211\ni\u2208S a(i).\nDivide-and-conquer strategies for the proximal problems. Given a solution x\u2217 of the proximal problem, we have seen how to get S\u2217\u00b5 for any \u00b5 by simply thresholding x\n\u2217 at \u00b5. Conversely, one can recover x\u2217 exactly from at most n well-chosen values of \u00b5. A known divide-and-conquer strategy [21, 23] hinges upon the fact that for any \u00b5, one can easily see which components of x\u2217 are greater or smaller than \u00b5 by computing S\u2217\u00b5. The resulting algorithm makes O(n) calls to the submodular function oracle. In Appendix B, we extend an alternative approach by Tarjan et al. [45] for cuts to general submodular functions and obtain a solution to (3) up to precision \u03b5 in O(min{n, log 1\u03b5}) iterations. This result is particularly useful if our function F is a sum of functions for each of which by itself the SFM problem is easy. Beyond squared \u21132-norms, our algorithm equally applies to computing all minimizers of f(x) +\n\u2211p j=1 hj(xj) for arbitrary smooth strictly convex functions hj ,\nj = 1, . . . , n."}, {"heading": "3 Decomposition of submodular functions", "text": "Following [29, 31, 41, 44], we assume that our function F may be decomposed as the sum F (S) = \u2211r\nj=1 Fj(S) of r \u201csimple\u201d functions. In this paper, by \u201csimple\u201d we mean functions G for which G(S) \u2212 a(S) can be minimized efficiently for all vectors a \u2208 Rn (more precisely, we require that S 7\u2192 G(S\u222aT )\u2212a(S) can be minimized efficiently over all subsets of V \\T , for any T \u2286 V and a). Efficiency may arise from the functional form of G, or from the fact that G has small support. For such functions, Problems (1) and (3) become\nmin S\u2286V\n\u2211r\nj=1 Fj(S) = min x\u2208[0,1]n\n\u2211r\nj=1 fj(x) min x\u2208Rn\n\u2211r\nj=1 fj(x) +\n1 2\u2016x\u201622. (5)\nThe key to the algorithms presented here is to be able to minimize 12\u2016x\u2212z\u201622+fj(x), or equivalently, to orthogonally project z onto B(Fj): min 12\u2016y \u2212 z\u201622 subject to y \u2208 B(Fj). We next sketch some examples of functions F and their decompositions into simple functions Fj . As shown at the end of Section 2, projecting onto B(Fj) is easy as soon as the corresponding submodular minimization problems are easy. Here we outline some cases for which specialized fast algorithms are known.\nGraph cuts. A widely used class of submodular functions are graph cuts. Graphs may be decomposed into substructures such as trees, simple paths or single edges. Message passing algorithms apply to trees, while the proximal problem for paths is very efficiently solved by [3]. For single edges, it is solvable in closed form. Tree decompositions are common in graphical models, whereas path decompositions are frequently used for TV problems [3].\nConcave functions. Another important class of submodular functions is that of concave functions of cardinality, i.e., Fj(S) = h(|S|) for a concave function h. Problem (3) for such functions may be solved in O(n log n) time (see [20] and Appendix B). Functions of this class have been used in [26, 28, 44]. Such functions also include covering functions [44].\nHierarchical functions. Here, the ground set corresponds to the leaves of a rooted, undirected tree. Each node has a weight, and the cost of a set of nodes S \u2286 V is the sum of the weights of all nodes in the smallest subtree (including the root) that spans S. This class of functions too admits to solve the proximal problem in O(n logn) time [24, 25]. Related tree functions have been considered in [27], where the elements v of the ground set are arranged in a tree of height d and each have a weight w(v). Let desc(v) be the set of descendants of v in the tree. Then F (S) = \u2211\nv\u2208V w(v)1[dec(v) \u2229 S 6= \u2205]. Jenatton et al. [27] show how to solve the proximal problem for such a function in time O(nd).\nSmall support. Any general, potentially slower algorithm such as the minimum-norm-point algorithm can be applied if the support of each Fj is only a small subset of the ground set."}, {"heading": "3.1 Dual decomposition of the nonsmooth problem", "text": "We first review existing dual decomposition techniques for the nonsmooth problem (1). We always assume that F =\n\u2211r j=1 Fj , and define Hr := \u220fr j=1 R n \u2243 Rn\u00d7r. We follow [31] to derive a dual formulation (see Appendix A):\nLemma 1. The dual of Problem (1) may be written in terms of variables \u03bb1, . . . , \u03bbr \u2208 Rn as\nmax \u2211r\nj=1 gj(\u03bbj) s.t. \u03bb \u2208\n{ (\u03bb1, . . . , \u03bbr) \u2208 Hr | \u2211r\nj=1 \u03bbj = 0\n}\n(6)\nwhere gj(\u03bbj) = minS\u2282V Fj(S)\u2212 \u03bbj(S) is a nonsmooth concave function.\nThe dual is the maximization of a nonsmooth concave function over a convex set, onto which it is easy to project: the projection of a vector y has j-th block equal to yj \u2212 1r \u2211r k=1 yk. Moreover, in our setup, functions gj and their subgradients may be computed efficiently through SFM.\nWe consider several existing alternatives for the minimization of f(x) on x \u2208 [0, 1]n, most of which use Lemma 1. Computing subgradients for any fj means calling the greedy algorithm, which runs in time O(n logn). All of the following algorithms require the tuning of an appropriate step size.\nPrimal subgradient descent (primal-sgd): Agnostic to any decomposition properties, we may apply a standard simple subgradient method to f . A subgradient of f may be obtained from the subgradients of the components fj . This algorithm converges at rate O(1/ \u221a t).\nDual subgradient descent (dual-sgd) [31]: Applying a subgradient method to the nonsmooth dual in Lemma 1 leads to a convergence rate of O(1/ \u221a t). Computing a subgradient requires minimizing the submodular functions Fj individually. In simulations, following [31], we consider a step-size rule similar to Polyak\u2019s rule (dual-sgd-P) [7], as well as a decaying step-size (dual-sgd-F), and use discrete optimization for all Fj .\nPrimal smoothing (primal-smooth) [44]: The nonsmooth primal may be smoothed in several ways by smoothing the fj individually; one example is f\u0303 \u03b5j (xj) = maxyj\u2208B(Fj) y \u22a4 j xj\u2212 \u03b52\u2016yj\u20162. This leads to a function that is (1/\u03b5)-smooth. Computing f\u0303 \u03b5j means solving the proximal problem for Fj . The convergence rate is O(1/t), but, apart from the step size which may be set relatively easily, the smoothing constant \u03b5 needs to be defined.\nDual smoothing (dual-smooth): Instead of the primal, the dual (6) may be smoothed, e.g., by entropy [10, 41] applied to each gj as g\u0303\u03b5j (\u03bbj) = minx\u2208[0,1]n fj(x)+ \u03b5h(x) where h(x) is a negative entropy. Again, the convergence rate is O(1/t) but there are two free parameters (in particular the smoothing constant \u03b5 which is hard to tune). This method too requires solving proximal problems for all Fj in each iteration.\nDual smoothing with entropy also admits coordinate descent methods [37] that exploit the decomposition, but we do not compare to those here."}, {"heading": "3.2 Dual decomposition methods for proximal problems", "text": "We may also consider Eq. (3) and first derive a dual problem using the same technique as in Section 3.1. Lemma 2 (proved in Appendix A) formally presents our dual formulation as a best approximation problem. The primal variable can be recovered as x = \u2212\u2211j yj . Lemma 2. The dual of Eq. (3) may be written as the best approximation problem\nmin \u03bb,y\n\u2016y \u2212 \u03bb\u201622 s.t. \u03bb \u2208 { (\u03bb1, . . . , \u03bbr) \u2208 Hr | \u2211r\nj=1 \u03bbj = 0\n} , y \u2208 \u220fr\nj=1 B(Fj). (7)\nWe can actually eliminate the \u03bbj and obtain the simpler looking dual problem\nmax y \u22121 2\n\u2225 \u2225 \u2225 \u2211r\nj=1 yj\n\u2225 \u2225 \u2225 2\n2 s.t. yj \u2208 B(Fj), j \u2208 {1, . . . , r} (8)\nSuch a dual was also used in [43]. In Section 5, we will see the effect of solving one of these duals or the other. For the simpler dual (8) the case r = 2 is of special interest; it reads\nmax y1\u2208B(F1), y2\u2208B(F2) \u22121 2 \u2016y1 + y2\u201622 \u21d0\u21d2 min y1\u2208B(F1),\u2212y2\u2208\u2212B(F2) \u2016y1 \u2212 (\u2212y2)\u20162. (9)\nWe write Problem (9) in this suggestive form to highlight its key geometric structure: it is, like (7), a best approximation problem: i.e., the problem of finding the closest point between the polytopes B(F1) and \u2212B(F2). Notice, however, that (7) is very different from (9)\u2014the former operates in a product space while the latter does not, a difference that can have impact in practice (see Section 5). We are now ready to present algorithms that exploit our dual formulations."}, {"heading": "4 Algorithms", "text": "We describe a few competing methods for solving our smooth dual formulations. We describe the details for the special 2-block case (9); the same arguments apply to the block dual from Lemma 2."}, {"heading": "4.1 Block coordinate descent or proximal-Dykstra", "text": "Perhaps the simplest approach to solving (9) (viewed as a minimization problem) is to use a block coordinate descent (BCD) procedure, which in this case performs the alternating projections:\nyk+11 \u2190 argminy1\u2208B(F1) \u2016y1 \u2212 (\u2212yk2 )\u201622; yk+12 \u2190 argminy2\u2208B(F2) \u2016y2 \u2212 (\u2212yk+11 )\u20162. (10) The iterations for solving (8) are analogous. This BCD method (applied to (9)) is equivalent to applying the so-called proximal-Dykstra method [14] to the primal problem. This may be seen by comparing the iterates. Notice that the BCD iteration (10) is nothing but alternating projections onto the convex polyhedra B(F1) and B(F2). There exists a large body of literature studying method of alternating projections\u2014we refer the interested reader to the monograph [15] for further details.\nHowever, despite its attractive simplicity, it is known that BCD (in its alternating projections form), can converge arbitrarily slowly [5] depending on the relative orientation of the convex sets onto which one projects. Thus, we turn to a potentially more effective method."}, {"heading": "4.2 Douglas-Rachford splitting", "text": "The Douglas-Rachford (DR) splitting method [16] includes algorithms like ADMM as a special case [14]. It avoids the slowdowns alluded to above by replacing alternating projections with alternating \u201creflections\u201d. Formally, DR applies to convex problems of the form [4, 14]\nminx \u03c61(x) + \u03c62(x), (11)\nsubject to the qualification ri(dom\u03c61) \u2229 ri(dom\u03c62) 6= \u2205. To solve (11), DR starts with some z0, and performs the three-step iteration (for k \u2265 0): 1. xk = prox\u03c62(zk); 2. vk = prox\u03c61(2xk \u2212 zk); 3. zk+1 = zk + \u03b3k(vk \u2212 zk), (12) where \u03b3k \u2208 [0, 2] is a sequence of scalars that satisfy \u2211\nk \u03b3k(2 \u2212 \u03b3k) = \u221e. The sequence {xk} produced by iteration (12) can be shown to converge to a solution of (11) [4; Thm. 25.6].\nIntroducing the reflection operator R\u03c6 := 2 prox\u03c6 \u2212 I, and setting \u03b3k = 1, the DR iteration (12) may be written in a more symmetric form as\nxk = prox\u03c62(zk), zk+1 = 1 2 [R\u03c61R\u03c62 + I]zk, k \u2265 0. (13)\nApplying DR to the duals (7) or (9), requires first putting them in the form (11), either by introducing extra variables or by going back to the primal, which is unnecessary. This is where the special structure of our dual problem proves crucial, a recognition that is subtle yet remarkably important.\nInstead of applying DR to (9), consider the closely related problem\nminy \u03b41(y) + \u03b4 \u2212 2 (y), (14)\nwhere \u03b41, \u03b4 \u2212 2 are indicator functions for B(F1) and \u2212B(F2), respectively. Applying DR directly to (14) does not work because usually ri(dom \u03b41) \u2229 ri(dom \u03b42) = \u2205. Indeed, applying DR to (14) generates iterates that diverge to infinity [5; Thm. 3.13(ii)]. Fortunately, even though the DR iterates for (14) may diverge, Bauschke et al. [5] show how to extract convergent sequences from these iterates, which actually solve the corresponding best approximation problem; for us this is nothing but the dual (9) that we wanted to solve in the first place. Theorem 3, which is a simplified version of [5; Thm. 3.13], formalizes the above discussion. Theorem 3. [5] Let A and B be nonempty polyhedral convex sets. Let \u03a0A (\u03a0B) denote orthogonal projection onto A (B), and let RA := 2\u03a0A \u2212 I (similarly RB) be the corresponding reflection operator. Let {zk} be the sequence generated by the DR method (13) applied to (14). If A\u2229B 6= \u2205, then {zk}k\u22650 converges weakly to a fixed-point of the operator T := 12 [RARB + I]; otherwise\u2016zk\u20162 \u2192 \u221e. The sequences {xk} and {\u03a0A\u03a0Bzk} are bounded; the weak cluster points of either of the two sequences {(\u03a0ARBzk, xk)}k\u22650 {(\u03a0Axk, xk)}k\u22650, (15) are solutions best approximation problem mina,b \u2016a\u2212 b\u2016 such that a \u2208 A and b \u2208 B.\nThe key consequence of Theorem 3 is that we can apply DR with impunity to (14), and extract from its iterates the optimal solution to problem (9) (from which recovering the primal is trivial). The most important feature of solving the dual (9) in this way is that absolutely no stepsize tuning is required, making the method very practical and user friendly (see also Appendix D)."}, {"heading": "5 Experiments", "text": "We empirically compare the proposed projection methods2 to the (smoothed) subgradient methods discussed in Section 3.1. For solving the proximal problem, we apply block coordinate descent (BCD) and Douglas-Rachford (DR) to Problem (8) if applicable, and also to (7) (BCD-para, DRpara). In addition, we use acceleration to solve (8) or (9) [6]. The main iteration cost of all methods except for the primal subgradient method is the orthogonal projection onto polytopes B(Fj), and therefore the number of iterations is a suitable criterion for comparisons. The primal subgradient method uses the greedy algorithm in each iteration, which runs in O(n log n). However, as we will see, its convergence is so slow to counteract any benefit that may arise from not using projections. We do not include Frank-Wolfe methods here, since FW is equivalent to a subgradient descent on the primal and converges correspondingly slowly.\nAs benchmark problems, we use (i) graph cut problems for segmentation, or MAP inference in a 4-neighborhood grid-structured MRF, and (ii) concave functions similar to those used in [44], but together with graph cut functions. The segmentation problems (i) are set up in a fairly standard way on a 4-neighbor grid graph, with unary potentials derived from Gaussian Mixture Models of color features. The weight of graph edge (i, j) is a function of exp(\u2212\u2016yi \u2212 yj\u20162), where yi is the RGB color vector of pixel i. The functions in (i) decompose as sums over vertical and horizontal paths. All horizontal paths are independent and can be solved together in parallel, and similarly all vertical paths. The functions in (ii) are constructed by extracting regions Rj via superpixels [33] and, for each Rj , defining the function Fj(S) = |S||Rj \\ S|. We use 200 and 500 regions. The problems have size 640 \u00d7 427. Hence, for (i) we have r = 640 + 427 (but solve it as r = 2) and for (ii) r = 640 + 427 + 500 (solved as r = 3).\nFor algorithms working with formulation (7), we compute an improved smooth duality gap of a current primary solution x = \u2212\u2211j yj as follows: find y\u2032 \u2208 argmaxy\u2208B(F ) x\u22a4y (then f(x) = x\u22a4y\u2032) and find an improved x\u2032 by minimizing minz z\u22a4y\u2032 + 12\u2016z\u20162 subject to the constraint that z has the same ordering as x [1]. The constraint ensures that (x\u2032)\u22a4y\u2032 = f(x\u2032). This is an isotonic regression problem and can be solved in time O(n) using the \u201cpool adjacent violators\u201d algorithm [1]. The gap is then f(x\u2032) + 12\u2016x\u2032\u20162 \u2212 (\u2212 12\u2016y\u2032\u20162). For computing the discrete gap, we find the best level set Si of x and, using y\u2032 = \u2212x, compute mini F (Si)\u2212 y\u2032\u2212(V ). Two functions (r = 2). Figure 2 shows the duality gaps for the discrete and smooth (where applicable) problems for two instances of segmentation problems. The algorithms working with the proximal problems are much faster than the ones directly solving the nonsmooth problem. In particular DR converges extremely fast, faster even than BCD which is known to be a state-of-the-art algorithms for this problem [3]. This, in itself, is a new insight for solving TV. We also see that the discrete gap shrinks faster than the smooth gap, i.e., the optimal discrete solution does not require to solve the smooth problem to extremely high accuracy. Figure 1 illustrates example results for different gaps.\nMore functions (r > 2). Figure 3 shows example results for four problems of sums of concave and cut functions. Here, we can only run DR-para. Overall, BCD, DR-para and the accelerated gradient method perform very well.\n2Code and data corresponding to this paper are available at https://sites.google.com/site/mloptstat/drsubmod\nParallel speedups If we aim for parallel methods, then again DR outperforms BCD. Figure 4 (right) shows the speedup gained from parallel processing for r = 2. Using 8 cores, we obtain a 5-fold speed-up.\nRunning time compared to graph cuts Table 1 shows the running times of our DR method (implemented in Matlab/C++) and the Maxflow code of [8, 9, 30] (using the wrapper [2]) for the four graph cut (segmentation) instances above on a MacBook Air with a 2 GHz Intel Core i7. The running times are averages over 5 repetitions. DR was run for 10, 10, 21, and 20 iterations, respectively.\nDR is by a factor of 2-9 slower than the specialized code. Given that, as opposed to the combinatorial algorithm, DR solves the full regularization path, is parallelizable, generic and straightforwardly extends to a variety of functions, this is remarkable.\nIn summary, our experiments suggest that projection methods can be extremely useful for solving the combinatorial submodular minimization problem. Of the tested methods, DR, cyclic BCD and accelerated gradient perform very well. For parallelism, applying DR on (9) converges much faster than BCD on the same problem."}, {"heading": "6 Conclusion", "text": "We have presented a novel approach to submodular function minimization based on the equivalence with a best approximation problem. The use of reflection methods avoids any hyperparameters and reduce the number of iterations significantly, suggesting the suitability of reflection methods for combinatorial problems. Given the natural parallelization abilities of our approach, it would be interesting to perform detailed empirical comparisons with existing parallel implementations of graph cuts (e.g., [42]). Moreover, a generalization beyond submodular functions of the relationships between combinatorial optimization problems and convex problems would enable the application of our framework to other common situations such as multiple labels (see, e.g., [31]).\nAcknowledgments. This research was in part funded by the Office of Naval Research under contract/grant number N00014-11-1-0688, by NSF CISE Expeditions award CCF-1139158, by DARPA XData Award FA8750-12-2-0331, and the European Research Council (SIERRA project), as well as gifts from Amazon Web Services, Google, SAP, Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Facebook, General Electric, Hortonworks, Intel, Microsoft, NetApp, Oracle, Samsung, Splunk, VMware and Yahoo!. We would like to thank Martin Jaggi, Simon Lacoste-Julien and Mark Schmidt for discussions."}, {"heading": "A Derivations of Dual Problems", "text": ""}, {"heading": "A.1 Proof of Lemma 1", "text": "Proof. To derive the non-smooth dual problem, we follow [31] and use Lagrangian duality:\nmin x\u2208[0,1]n f(x) = min x\u2208[0,1]n\n\u2211r\nj=1 fj(x) = min\nx1,...,xr\u2208[0,1]n\n\u2211r\nj=1 fj(xj) such that x1 = \u00b7 \u00b7 \u00b7 = xr\n= min x\u2208Rn, x1,...,xr\u2208[0,1]n max (\u03bbj)\n\u2211r\nj=1 fj(xj) +\n\u2211r\nj=1 \u03bb\u22a4j (x\u2212 xj)\n= max\u2211r j=1 \u03bbj=0\n\u2211r\nj=1 min\nxj\u2208[0,1]n\n{ fj(xj)\u2212 \u03bb\u22a4j xj }\n= max\u2211r j=1 \u03bbj=0\n\u2211r\nj=1 max yj\u2208B(Fj) (yj \u2212 \u03bbj)\u2212(V ) = max\u2211r\nj=1 \u03bbj=0\n\u2211r\nj=1 gj(\u03bbj),\nwhere gj(\u03bbj) = minA\u2282V Fj(A)\u2212\u03bbj(A) is a nonsmooth concave function, which may be computed efficiently through submodular function minimization."}, {"heading": "A.2 Proof of Lemma 2", "text": "Proof. The proof follows a similar saddle-point approach.\nmin x\u2208Rn f(x) + 12\u2016x\u201622 = minx\u2208Rn \u2211r j=1 fj(x) + 1 2\u2016x\u201622\n= min x1,...,xr\u2208Rn\n\u2211r\nj=1\n{ fj(xj) + 1\n2r \u2016xj\u201622\n}\nsuch that x1 = \u00b7 \u00b7 \u00b7 = xr\n= min x\u2208Rn, x1,...,xr\u2208Rn max \u03bbj\n\u2211r\nj=1\n{ fj(xj) + 1\n2r \u2016xj\u201622 + \u03bb\u22a4j (x\u2212 xj)\n}\n= max\u2211 r j=1 \u03bbj=0\n\u2211r\nj=1 min xj\u2208Rn\n{ fj(xj)\u2212 \u03bb\u22a4j xj + 12r\u2016xj\u201622 }\n= max\u2211 r j=1 \u03bbj=0\n\u2211r\nj=1 min xj\u2208Rn\n{\nmax yj\u2208B(Fj)\nx\u22a4j yj \u2212 \u03bb\u22a4j xj + 12r\u2016xj\u201622 }\n= max\u2211r j=1 \u03bbj=0\n\u2211r\nj=1 max yj\u2208B(Fj) \u2212 r2\u2016yj \u2212 \u03bbj\u201622\n= max\u2211r j=1 \u03bbj=0 max yj\u2208B(Fj) \u2212 r 2\nr \u2211\nj=1\n\u2016yj \u2212 \u03bbj\u201622. (16)\nWriting (16) as a minimization problem and ignoring constants completes the proof."}, {"heading": "B Divide-and-conquer algorithm for parametric submodular minimization", "text": ""}, {"heading": "B.1 Description of the algorithm", "text": "The optimal solution x\u2217 of our proximal problem minx\u2208Rn f(x) + \u2016x\u20162 indicates the minimizers of F (S) \u2212 \u03bb|S| for all \u03bb \u2208 R. Those minimizers form a chain S\u2205 \u2282 S1 \u2282 . . . \u2282 Sk = V . The solutions are the level sets of the optimal solution x\u2217.\nHere, we extend the approach of Tarjan et al. [45] for parametric max-flow to all submodular functions and all monotone strictly convex functions beyond the square functions used in the main paper. More precisely, we consider a submodular function F defined on V = {1, . . . , n} and n differentiable strictly convex functions hi such that their Fenchel-conjugates h\u2217i have full domain, for\ni \u2208 {1, . . . , n}. The functions h\u2217i are then differentiable. We consider the following problem:\nmin x\u2208Rn f(x) +\nn \u2211\ni=1\nhi(xi) = min x\u2208Rn max y\u2208B(F )\ny\u22a4x+\nn \u2211\ni=1\nhi(xi) (17)\n= max y\u2208B(F ) min x\u2208Rn\ny\u22a4x+\nn \u2211\ni=1\nhi(xi) (18)\n= max y\u2208B(F )\n\u2212 n \u2211\ni=1\nh\u2217i (\u2212yi). (19)\nThe optimality conditions are\n1. y \u2208 B(F ), 2. y\u22a4x = f(x),\n3. \u2212yi = h\u2032i(xi) \u21d4 xi = (h\u2217i )\u2032(\u2212yi).\nLet \u03c4(V ) be the time for minimizing the submodular function F (S) + a(S) on the ground set V (for any a \u2208 Rn). For our complexity analysis, we make the assumption that minimizing the (contracted) function FS,a(T ) , F (S \u222a T )\u2212 F (S) + a(T ) on the smaller ground set U \u2286 V \\ S (for any a \u2208 Rn, S \u2286 V , U \u2286 V \\S) takes time at most |U||V |\u03c4(V ). This is a reasonable assumption, because it essentially says that \u03c4(V ) grows at least linearly in the size of V . To our knowledge, even fast algorithms for special submodular functions take at least linear time.\nWe will also use the notation F (S | T ) , F (S \u222a T )\u2212 F (S). For recursions, we use the restriction FS : 2\nS \u2192 R, FS(T ) = F (T ) of F to S and the contraction FS : 2V \\S \u2192 R, FS(T ) = F (T | S) of F on S.\nAlgorithm 1: Recursive Divide-and-Conquer"}, {"heading": "SplitInterval (\u03bbmin, \u03bbmax, V , F , i)", "text": "if i even then\n// unbalanced split \u03bb \u2190 argmin\u03bb \u2211\ni hi(\u03bb) \u2212 \u03bbF (V ) A \u2190 argminT\u2286V F (T ) + \u2211 i\u2208T h \u2032 i(\u03bb) if S = \u2205 or S = V then return x = \u03bb1V\nend else\n// balanced split \u03bb \u2190 (\u03bbmin + \u03bbmax)/2 S \u2190 argminT\u2286V F (T ) + \u2211 i\u2208T h \u2032 i(\u03bb) if S = \u2205 then x \u2190 SplitInterval (\u03bbmin, \u03bb, V , F , i+ 1) return x\nend if S = V then\nx \u2190 SplitInterval (\u03bb, \u03bbmax, V , F , i+ 1) return x\nend end // S 6= \u2205 and S 6= V xS \u2190 SplitInterval (\u03bbmin, \u03bb, S, FA, i+ 1) xV \\S \u2190 SplitInterval (\u03bb, \u03bbmax, V \\ S, FS , i+ 1) return [xS , xV \\S ]\nAlgorithm 1 is a divide-and-conquer algorithm. In each recursive call, it takes an interval [\u03bbmin, \u03bbmax] in which all components of the optimal solution lie and either (a) shortens the search\ninterval for any break point, (b) finds the optimal (constant) value of x on a range of elements, or (c) recursively splits the problem into a set S and V \\ S with corresponding ranges for the values of x\u2217 and finds the optimal values of x on the two subsets."}, {"heading": "B.2 Review of related results", "text": "The goal of this appendix is to show Proposition 4 below. We first start by reviewing existing results regarding separable problems on the base polyhedron (see [1] for details).\nIt is known that if y \u2208 B(F ), then yk \u2208 [ F (V )\u2212F (V \\{k}), F ({k}) ]\n; thus, the optimal solution x is such that xk \u2208 [ (h\u2217k) \u2032(\u2212F ({k})), (h\u2217k)\u2032(F (V \\{k})\u2212F (V )) ]\n. We therefore set the initial search range to\n\u03bbmin = min k\u2208V\n(h\u2217k) \u2032(\u2212F ({k})) and \u03bbmax = max k\u2208V (h\u2217k) \u2032(F (V \\{k})\u2212 F (V )).\nThe algorithm relies on the following facts (see [1] for a proof). For all three propositions, we assume that the hi are strictly convex, continuously differentiable functions on R such that sup\u03bb\u2208R h\n\u2032(\u03bb) = +\u221e and inf\u03bb\u2208R h\u2032(\u03bb) = \u2212\u221e. Proposition 1 (Monotonicity of optimizing sets). Let \u03b1 < \u03b2 and S\u03b1 be any minimizer of F (S) + h\u2032(\u03b1)(S) and S\u03b2 any minimizer of F (S) + h\u2032(\u03b2)(S). Then S\u03b2 \u2286 S\u03b1. Proposition 2 (Characterization of x\u2217). The coordinates x\u2217j (j \u2208 V ) of the unique optimal solution x\u2217 of Problem 17 are x\u2217j = max{\u03bb | j \u2208 S\u03bb}, where S\u03bb is any minimizer of F (S) + h\u2032(\u03bb)(S).\nPropositions 1 and 2 imply that the level sets of x\u2217 form a chain \u2205 = S0 \u2282 S1 \u2282 . . . \u2282 Sk = V of maximal minimizers for the critical values of \u03bb (which are the entries of x\u2217). (Each Si = S\u03bb for some \u03bb = x\u2217j .)\nProposition 3 (Splits). Let T = Si be a level set of x\u2217 and let y \u2208 RT , z \u2208 RV \\T be the minimizers of the subproblems\ny = argmin x\nfT (x) + \u2211\ni\u2208T\nhi(xi)\nz = argmin x\nfT (x) + \u2211\ni/\u2208T\nhi(xi)"}, {"heading": "Then x\u2217j = yj for j \u2208 T and x\u2217j = zj for j \u2208 V \\ T .", "text": "The algorithm uses Proposition 3 recursively.\nProof. Let \u03bb be the value in x\u2217 defining Si = S\u03bb. It is easy to see that the restriction FT and the contraction FT are both submodular. Hence, Propositions 1 and 2 hold for them.\nSince the restriction on T is equivalent to the original function for any S \u2286 T , F (S) + h(\u03bb)(S) = FT (S) + hT (\u03bb)(S) for any S \u2286 T . With this, Propositions 1 and 2 imply that for any \u03b1 > \u03bb, F (S) + h(\u03bb)(S) = FT (S) + hT (\u03bb)(S) and therefore x\u2217j = yj for j \u2208 T .\nSimilarly, for any S \u2208 V \\ T , it holds that F (S \u222a T ) + h(\u03bb)(S \u222a T ) = FT (S) + (h\u2032(\u03bb))T (S) + F (T ) + h\u2032(\u03bb)(T ). Due to the monotonicity property of the optimizing sets, S\u03b1 \u2287 T for all \u03b1 < \u03bb, and therefore the maximal minimizer U\u03b1 of FT (S) + (h\u2032(\u03b1))T (S) satisfies U\u03b1 \u222a T = S\u03b1 (the terms F (T ) + h\u2032(\u03bb)(T ) are constant with respect to U ). Hence Poposition 2 implies that x\u2217j = zj for j \u2208 V \\ T .\nThese propositions imply that there is a set of at most n values of \u03bb = \u03b1 that define the level sets S\u03b1 of the optimal solution x\u2217. If we know these break point values, then we know x\u2217. Algorithm 1 interleaves an unbalanced split strategy that may split the seach interval in an unbalanced way but converges in O(n) recursive calls, and a balanced split strategy that always halves the search intervals but is not finitely convergent."}, {"heading": "B.3 Proof of convergence", "text": "We now prove the convergence rate for Algorithm 1. Proposition 4. The minimum of f(x) +\n\u2211n i=1 hi(xi) may be obtained up to coordinate-wise accu-\nracy \u01eb within O (\nmin{n, log 1\u01eb} )\n(20)\nsubmodular function minimizations. If hi(xi) = 12x 2 i , then \u01eb = \u2206min n2\u21130 is sufficient to recover the exact solution, where \u2206min = min{|F (S | T )| | S \u2286 V \\ T, F (S | T ) 6= 0} and \u21130 is the length of the initial interval [\u03bbmin, \u03bbmax].\nProof. The proof relies on Propositions 1, 2 and 3.\nWe first argue for the correctness of the balanced splitting strategy. Propositions 1 and 2 imply that for any \u03bb \u2208 R, if S is a minimizer of F (S) + h\u2032(\u03bb)(S), then the unique minimum of f(x) + \u2211n\ni=1 hi(xi) satisfies that for all k \u2208 S, xk > h\u2032k(\u03bb) and for all k \u2208 V \\S, xk 6 h\u2032k(\u03bb). In particular, if S = \u2205, then this means that for all k \u2208 V , xk 6 h\u2032k(\u03bb). Similarly, if S = V , then for all k \u2208 V , xk > h \u2032 k(\u03bb). The limits of the interval are set accordingly. The correctness of the recursive call follows from Proposition 3.\nIn each iteration, the size of the search interval [\u03bbmin, \u03bbmax] for any break point is at least halved. Hence, within d recursions, the length of each interval is at most 2\u2212d\u21130.\nThe choice of \u03bb in the unbalanced splitting strategy corresponds to solving a simplified version of the dual problem. Indeed, by convex duality, the following two problems are dual to each other:\nmax y\n\u2212 \u2211\ni\nh\u2217i (\u2212yi) s.t. y(V ) = F (V ) (21)\nmin \u03bb\u2208R\n\u2211\ni\u2208V\nhi(\u03bb)\u2212 \u03bbF (V ). (22)\nProblem (21) replaces the constraint that y \u2208 B(F ) by y(V ) = F (V ), dropping the constraint that y(S) \u2264 F (S) for all S \u2286 V . Testing whether y satisfies all constraints of (19), i.e., y \u2208 B(F ) is equivalent to testing whether F (S) \u2212 y(S) \u2265 0. We do this implicitly by our choice of \u03bb: Convex duality implies that the the optimal solutions of Problems (21) and (22) satisfy yi = \u2212h\u2032i(\u03bb). This holds in particular for the chosen (unique optimal) \u03bb in the algorithm.\nLet T be a minimizer of F (S) + h\u2032(\u03bb)(S) = F (S) \u2212 y(S). If T = \u2205 or T = V , then y \u2208 B(F ) and an optimal solution for the full dual problem (19). Hence, y and x = \u03bb1V = (h\u2217)\u2032(\u2212y) form a primal/dual optimal pair for (19).\nIf \u2205 \u2282 T \u2282 V and F (T ) \u2212 y(T ) < 0, then y /\u2208 B(F ), and we perform a split with the same argumentation as above. This splitting strategy is exactly that of [1, 23] and splits at most n times. Hence, this strategy yields the global optimum (to machine precision) in the time of O(n) times solving a submodular minimization on V . If n is large, this may be computationally expensive.\nIf we only do balanced splits, we end up approaching the break points more and more closely (but typically never exactly). Unbalanced splits always find an exact break point, but with potentially little progress in reducing the intervals. Algorithm 1 thus interleaves both strategies where we store intervals of allowed values for subsets of components of A. At step d there are at most min{n, 2d} different intervals (as there cannot be more intervals than elements of V ). To split these intervals, submodular function minimization problems have to be solved on each of these intervals, with total complexity less than a single submodular function optimization problem on the full set. At each iteration, intervals corresponding to a singleton may be trivially completely solved, and components which are already found are discarded. Hence, at each recursive level, the total computation time is bounded above by \u03c4(V ).\nWhile balanced splits always substantially shrink the intervals, they are not finitely convergent. Unbalanced splits converge after at most n recursions. Following the argumentation of Tarjan et al. [45], who considered the special case of flows, alternating the two types of splits gives the best of both worlds: (a) all components are estimated up to precision \u21130\n2d/2 , and (b) the algorithm is finitely con-\nvergent, and will stop when \u21130 2d/2 is less than the minimal distance between two different components of x.\nFinally, we adress the precision for the special case that hi(xi) = 12x 2 i for all i \u2208 V . If the interval lengths are smaller than the smallest gap between any two break points (components of x\u2217), then each interval contains at most one break point and the algorithm converges after at most two unbalanced splits. Hence, we here consider \u01eb to be one half times the smallest gap between any two break points. Let \u2205 = S0 \u2282 S1 \u2282 . . . \u2282 Sk = V be the chain of level sets of x\u2217. By the optimality conditions discussed above for unbalanced splits, any constant part T = Si \\ Si\u22121 of x\u2217 takes value \u03bb1 = \u2212yj1 (j \u2208 T ), where y(T ) = FSi\u22121(T ), and hence\n\u03bb = \u2212F (Si \\ Si\u22121|Si\u22121)|Si \\ Si\u22121| . (23)\nTherefore, the (absolute) difference between any two such values is loosely lower bounded by\nmin i\n\u2223 \u2223 \u2223 \u2223 F (Si \\ Si\u22121|Si\u22121) |Si \\ Si\u22121| \u2212 F (Si+1 \\ Si|Si)|Si+1 \\ Si| \u2223 \u2223 \u2223 \u2223 \u2265 \u2206min ( 2 n\u2212 1 \u2212 2 n ) \u2265 2\u2206min n2 . (24)\nThis implies O(log(\u21130n2/\u2206min)) iterations.\nNote that in the case of flows, the algorithm is not exactly equivalent to the flow algorithm of [45], which updates flows directly."}, {"heading": "C BCD and proximal Dykstra", "text": "We consider the best approximation problem\nmin 12\u2016x\u2212 y\u201622 s.t. x \u2208 C1 \u2229 C2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Cm.\nLet us show the details for only the two block case. The general case follows similarly.\nConsider the more general problem\nmin 12\u2016x\u2212 y\u201622 + f(x) + h(x). (25) Clearly, this problem contains the two-block best approximation problem as a special case (by setting f and h to be suitable indicator functions). Now introduce two variables z, w that equal x; then the corresponding Lagrangian is\nL(x, z, w, \u03bd, \u00b5) := 12\u2016x\u2212 y\u201622 + f(z) + h(w) + \u03bdT (x\u2212 z) + \u00b5T (x\u2212 w). From this Lagrangian, a brief calculation yields the dual optimization problem\nmin g(\u03bd, \u00b5) := 12\u2016\u03bd + \u00b5\u2212 y\u201622 + f\u2217(\u03bd) + h\u2217(\u00b5). We solve this dual problem via BCD, which has the updates\n\u03bdk+1 = argmin\u03bd g(\u03bd, \u00b5k), \u00b5k+1 = argmin\u00b5 g(\u03bdk+1, \u00b5).\nThus, 0 \u2208 \u03bdk+1 +\u00b5k \u2212 y+ \u2202f\u2217(\u03bdk+1) and 0 \u2208 \u03bdk+1 +\u00b5k+1\u2212 y+ \u2202h\u2217(\u00b5k+1). The first optimality condition may be rewritten as\ny\u2212\u00b5k \u2208 \u03bdk+1+\u2202f\u2217(\u03bdk+1) =\u21d2 \u03bdk+1 = proxf\u2217(y\u2212\u00b5k) =\u21d2 \u03bdk+1 = y\u2212\u00b5k\u2212proxf (y\u2212\u00b5k). Similarly, we second condition yields \u00b5k+1 = y \u2212 \u03bdk+1 \u2212 proxh(y \u2212 \u03bdk+1). Now use Lagrangian stationarity x = y \u2212 \u03bd \u2212 \u00b5 =\u21d2 y \u2212 \u00b5 = x+ \u03bd to rewrite BCD using primal and dual variables to obtain the so-called proximal-Dykstra method:\ntk \u2190 proxf (xk + \u03bdk) \u03bdk+1 \u2190 xk + \u03bdk \u2212 tk xk+1 \u2190 proxh(\u00b5k + tk) \u00b5k+1 \u2190 \u00b5k + tk \u2212 xk+1\nWe discussed the more general problem (25) because it contains the smoothed primal as a special case, namely with y = 0 in (25), f = f1, and h = f2, we obtain\nmin f1(x) + f2(x) + 1 2\u2016x\u201622,\nfor which BCD yields the proximal-Dykstra method that was previously used in [3] for twodimensional TV optimization."}, {"heading": "D Recipe: Submodular minimization via reflections", "text": "To be precise, we summarize here how to solve Problem (17) via reflections. As we showed above, the dual is of the form\nmin \u03bb,y\n\u2016y \u2212 \u03bb\u201622 s.t. \u03bb \u2208 A = { (\u03bb1, . . . , \u03bbr) \u2208 Hr | \u2211r\nj=1 \u03bbj = 0\n} , y \u2208 B , \u220fr\nj=1 B(Fj).\n(26)\nThe vector y consists of r parts yj \u2208 B(Fj). We first solve the dual by starting with any z(0) \u2208 Hr, and iterate\nz(k+1) = 12 (z k +RARB(z (k))). (27)\nUpon convergence to a point z\u2217, we extract the components\nyj = \u03a0B(Fj)(z \u2217 j ). (28)\nThe final primal solution is x = \u2212\u2211j yj \u2208 Rn."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Recently, it has become evident that submodularity naturally captures widely oc-<lb>curring concepts in machine learning, signal processing and computer vision. Con-<lb>sequently, there is need for efficient optimization procedures for submodular func-<lb>tions, especially for minimization problems. While general submodular minimiza-<lb>tion is challenging, we propose a new method that exploits existing decomposabil-<lb>ity of submodular functions. In contrast to previous approaches, our method is<lb>neither approximate, nor impractical, nor does it need any cumbersome parame-<lb>ter tuning. Moreover, it is easy to implement and parallelize. A key component<lb>of our method is a formulation of the discrete submodular minimization problem<lb>as a continuous best approximation problem that is solved through a sequence of<lb>reflections, and its solution can be easily thresholded to obtain an optimal discrete<lb>solution. This method solves both the continuous and discrete formulations of<lb>the problem, and therefore has applications in learning, inference, and reconstruc-<lb>tion. In our experiments, we illustrate the benefits of our method on two image<lb>segmentation tasks.", "creator": "LaTeX with hyperref package"}}}