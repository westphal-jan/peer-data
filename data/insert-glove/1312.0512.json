{"id": "1312.0512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2013", "title": "Sensing-Aware Kernel SVM", "abstract": "illiterates We leakers propose laestadius a novel approach for kindlmann designing cross-examine kernels grapico for support zito vector gr machines (SVMs) coalitions when basionym the boehme class label pehla is eds. linked to 318-2300 the dabholkar observation through sanjaks a latent auditoriums state lechmere and the likelihood function of the autapomorphies observation tavira given characidae the assise state (290-pound the dudu sensing foxen model) is triband available. preda We show that gryder the bellmer Bayes - glitches optimum voronoi decision 1,800-seat boundary is a ritterbusch hyperplane wafic under a supression mapping v6-powered defined catomine by asoc the teardown likelihood function. Combining panduranga this with the kankakee maximum margin prevents principle 1955-58 yields tinsel kernels for SVMs that wights leverage kci knowledge of joshu the harnett sensing model hbe in an optimal way. We derive the attiya optimum kernel workboats for delain the haec bag - samphan of - words (palmsource BoWs) shorja sensing tulfo model banishment and demonstrate its brocades superior performance over other recirculates kernels agoa in document durn and image self-image classification tasks. holmenkolbanen These results aerotaxi indicate headhunting that such optimum sensing - boutiette aware acornsoft kernel SVMs piyush can match the houtart performance icemen of kame rather 81.36 sophisticated state - of - raunchiness the - art approaches.", "histories": [["v1", "Mon, 2 Dec 2013 16:47:10 GMT  (15kb)", "https://arxiv.org/abs/1312.0512v1", null], ["v2", "Thu, 13 Mar 2014 12:02:10 GMT  (15kb)", "http://arxiv.org/abs/1312.0512v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weicong ding", "prakash ishwar", "venkatesh saligrama", "w clem karl"], "accepted": false, "id": "1312.0512"}, "pdf": {"name": "1312.0512.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["dingwc@bu.edu", "pi@bu.edu", "srv@bu.edu", "wckarl@bu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n05 12\nv2 [\ncs .L\nG ]\n1 3\nM ar\n2 01\nIndex Terms\u2014 Sensing model, Kernel method, SVM, Bag of Words, Supervised Classification"}, {"heading": "1. INTRODUCTION", "text": "This paper presents a new provably optimum method for designing kernels for SVMs that explicitly incorporates information about the structure of the underlying data generating process. We consider the typical classification task where the observed data point x \u2208 X and label y follow some joint distribution p(x, y). In many real world problems, however, observations x are only indirectly related through some generative process to latent variables z \u2208 Z via a sensing model p(x|z) and the latent variables have different distributions conditioned on the underlying label y. An example of such a situation is medical tomography, where the acquired data x consists of X-ray based projections but the diagnosis is done on reconstructed cross-sections of the body z to determine the presence or absence of a disease y. Another example is the classic \u201cBag-of-Word\u201d (BoW) modeling paradigm widely-used to model text documents, images, etc. [1][2], where the observed document x is modeled as a collection of iid drawings of words from a latent document-specific distribution z over the vocabulary. This z is, in turn, related to the document category y through an unknown p(z|y).\nThis article is based upon work supported by the U.S. AFOSR and the U.S. NSF under award numbers \u266fFA9550-10-1-0458 (subaward \u266fA1795) and \u266f1218992 respectively.\nFor supervised classification, a generative approach would make further assumptions on p(z|y) which may either not hold or lead to an intractable posterior inference problem [3]. On the other hand, the classical distribution-free discriminative paradigm would ignore knowledge of the sensing model p(x|z) and build classifiers using labeled training data [4, 5]. Yet another approach is a decoupled two-step process where the latent states are first estimated as z\u0302(x) using the sensing model and then a classifier is built based on (z\u0302, y) pairs. Computing estimates, however, can be costly and can introduce confounding artifacts when data is limited or noisy, and may be unnecessary if the ultimate goal is a simple decision.\nMotivated by such problems, we start by examining the structure of the Bayes-optimum classifier, focusing on binary classification, and then connect it to kernel SVMs via the max-margin principle. Our proposed \u201csensing-aware\u201d kerneldesign arises as the natural consequence of such a procedure. We illustrate the approach on the BoW model and demonstrate its merits on document and image classification tasks."}, {"heading": "2. OPTIMUM SENSING-AWARE KERNEL", "text": "Let y \u2208 {\u22121,+1}, z \u2208 Z , and x \u2208 X denote the class label, latent variable, and observation respectively. x and y are independent conditioned on z and the sensing model p(x|z) is known. We make the following technical assumptions on the joint distribution which are satisfied in many applications. Assumption 1: (Square-integrability)\u2200x, y, p(x|z), p(z, y) \u2208 L2(Z) as functions of z. Assumption 2: (Uniform boundedness) There exists R \u2208 [0,\u221e) such that supx,z |p(x|z)| \u2264 R. Further, let \u3008f, g\u3009Z := \u222b\nZ f(z)g(z)dz denote the usual inner\nproduct in L2(Z) which induces the norm \u2016f\u20162 Z , \u3008f, f\u3009Z . Let \u2016f\u20161 , \u222b Z |f(z)|dz denote the \u21131-norm in L2(Z).\nWhen full knowledge of the generative model p(x, y) is available, it is well known that the Maximum Aposteriori Probability (MAP) estimator of the class label minimizes the error probability, i.e., the Bayes risk. For binary classification, this reduces to a simple likelihood ratio test (LRT):\np(y = 1|x) 1 \u2277 \u22121 p(y = \u22121|x) (1)\nUsing p(x, z, y) = p(x|z)p(z, y) and marginalizing over z the LRT (1) reduces to,\n\u3008p(x|z), w(z)\u3009Z 1\n\u2277 \u22121 0 (2)\nwhere w(z) := p(z, y = 1)\u2212 p(z, y = \u22121). Note that w(z) satisfies the following constraint\n\u2016w(z)\u20161 \u2264 \u2016p(z, y = 1)\u20161 + \u2016p(z, y = \u22121)\u20161 = 1.\nTheorem: If we interpret p(x|z) and w(z) as vectors (functions of z with x held fixed) in L2(Z), then the Bayes-optimal rule (2) is a linear classifier in L2(Z) defined by the separating hyperplane w(z).\nWhen a set of training samples {xi, yi}Mi=1 that are generated from p(x, y) in an iid fashion are given, and w(z) is unknown, the popular max-margin principle advocates using the hyperplane w\u2217(z) that maximizes the separation, i.e., a (soft) margin, between the two classes. This max-margin hyperplane w\u2217(z) is the optimal solution of the following (infinite-dimensional) constrained optimization problem\nw\u2217(z) = argmin w(z)\u2208L2(Z),\u03bei\n1 2 \u2016w(z)\u20162Z + C\nN \u2211\ni=1\n\u03bei\nsuch that yi\u3008p(xi|z), w(z)\u3009 \u2265 1\u2212 \u03bei, \u03bei \u2265 0\n\u2016w(z)\u20161 \u2264 1 (3)\nAnd the optimum classifier is then given by\nf\u2217(x) = sign (\u3008p(x|z), w\u2217(z)\u3009) .\nIf we write w\u2217(z) = M \u2211\ni=1\n\u03b2ip(x i|z) + w\u22a5(z), where w\u22a5 is\northogonal to the linear span of {p(xi|z)}Mi=1, and ignore the constraint \u2016w(z)\u2016 \u2264 1, the optimization problem (3) reduces to\nmin \u03b2i,\u03bei,w\u22a5\n1\n2\nM \u2211\ni,j=1\n\u03b2i\u03b2jK(x i,xj) + \u2016w\u22a5\u201622 + C\nN \u2211\ni=1\n\u03bei\nsuch that yi M \u2211\nj=1\n\u03b2jK(x i,xj) \u2265 1\u2212 \u03bei, \u03bei \u2265 0\n(4) where\nK(x,x\u2032) , \u3008p(x|z), p(x\u2032|z)\u3009 = \u222b\nZ\np(x|z)p(x\u2032|z)dz. (5)\nNote that the optimal solution of (4) has w\u22a5 = 0 making it a finite-dimensional constrained optimization problem. Equation (4) is, in fact, a kernel SVM problem with the kernel defined by equation (5). We will refer to (5) as the \u201csensingaware kernel\u201d in what follows. The optimal classifier f\u2217(x) corresponding to the solution to 4 has the following form\nf(x) = sign\n(\nn \u2211\ni=1\n\u03b2\u2217i K(x,x i)\n)\n. (6)\nThe sensing-aware kernel thus provides a principled way to incorporate the partial information about the generative model p(x|z). For a wide range of p(x|z), K(x,x\u2032) has a\nclosed form expression and can be calculated efficiently. In this paper, we focus on the BoW generative model.\nRemark 1: If t(x) is a sufficient statistic for z, i.e., x \u2212 t(x) \u2212 z is a Markov chain, then t(x) can replace x in the above analysis and the resulting kernel SVM will have the kernel K(x,x\u2032) = \u222b\nZ p(t(x)|z)p(t(x\u2032)|z)dz.\nRemark 2: There is a vast literature dedicated to kernel methods for classification[5]. While the standard RBF kernel works well in many problems, various kernels have been designed for specific tasks. Among them, a line of work on model-based kernels is related to our approach in the sense that knowledge of a generative model is utilized. The Probability Product Kernel (PPK) proposed in [6] first estimates latent variable z\u0302i using observation xi and defines the kernel in terms of the estimate as K(xi,xj) := \u222b\nX p(x|z\u0302i)p(x|z\u0302j)dx.\nHeat or Diffusion kernels (Diff) that exploit the Fisher information metric on the probability manifold were proposed in [7]. Kernels based on the KL divergence were proposed in [8]. Although each approach has its own strength, unlike ours, the aforementioned probabilistic kernels are not designed to directly minimize the classification error. Moreover, they require full model knowledge which is unreasonable for large and complex datasets like text or images."}, {"heading": "3. KERNEL FOR BAG OF WORDS MODELS", "text": "In the \u201cBag of words\u201d (BoW), or \u201cBag of Features\u201d (BoF) model, there is a collection of documents composed of words from a vocabulary of size W . Each document is modeled as being generated by N i.i.d drawings of words from a latent W \u00d7 1 document word-distribution vector z. Since the ordering is ignored, a document is represented as a W\u00d71 empirical word-count vector x = (x1, . . . , xW )T . Therefore, p(x|z) is a multinomial distribution\np(x|z) =\n(\nN\nx1, . . . , xW\n) W \u220f\nw=1\nzxww\nFrom this it can be shown that the sensing kernel for two documents xi,xj defined in 5 has the following form,\nK(xi,xj) =\nW \u220f\nw=1\n\u0393(xiw + x j w + 1)\n\u0393(xiw + 1)\u0393(x j w + 1)\n\u0393(Ni + 1)\u0393(Nj + 1)\n\u0393(Ni +Nj +W )\n= W \u220f\nw=1\n(xiw + x j w)! (xiw)!(x j w)!\nNi!Nj!\n(Ni +Nj +W \u2212 1)!\n(7) where Ni is the number of words in the i-th document and \u0393(t) is the ordinary Gamma function. One practical concern is the size of the factorials. Typically, W is a large constant and Ni varies across documents. Therefore, the product of factorials will have a large dynamic range and lead to overflow errors (even for small W ). The product is sensitive to the differences in the Ni values across documents. These issues can be addressed by using one of the following approximations to the original kernel 7.\n\u2022 Sensing 0 kernel: K0(xi,xj) := log(K(xi,xj)).\n\u2022 Sensing 1 kernel: Following (7),\nK1n(x i,xj) := log\n{\nW \u220f\nw=1\n\u0393(nx\u0304iw + nx\u0304 j w + 1)\n\u0393(nx\u0304iw + 1)\u0393(nx\u0304 j w + 1)\n}\nwhere x\u0304i := xi/Ni is the word-frequency (normalized word-count) and n is set to be some constant.\n\u2022 Sensing 2 kernel: K2N(x i,xj) = log(K(x\u0302iN , x\u0302 j N ))\nwhere x\u0302iN are the words counts obtained by randomly resampling the i-th document, uniformly, N times.\nIn general, these approximations balance the number of words of documents and avoid numerical issues by a log-mapping that compresses the dynamic range. In typical experimental settings, we have found that these approximations yield similar performance."}, {"heading": "4. DOCUMENT CLASSIFICATION", "text": "In this section, we consider the problem of text document classification. In this application context, the terms \u201cword\u201d, \u201cvocabulary\u201d, and \u201cdocument\u201d have their natural meanings. We selected the 20 Newsgourps dataset which contains 18774 news postings from 20 newsgroups (classes) to test performance. The average number of words per document in this dataset is 117. Following standard practice [9], we removed a standard list of stop words from the vocabulary."}, {"heading": "4.1. Binary Classification", "text": "We chose to distinguish postings in the newsgroup alt.atheism from talk.religion.misc. This is a difficult task due to the similarity of content in these two groups [3, 10]. The training set contains 856 documents and the test set contains 569 documents. The average number of words per document is 132.\nWe used LIBSVM [11] to train our kernel SVMs. We report results only for the Sensing 1 and Sensing 2 approximations of our sensing-aware kernel since in this dataset, the number of words Ni varies significantly across documents. Table 1 compares the Correct Classification Rate (CCR %) of the proposed sensing-aware kernels against two modelbased kernels, PPK and Diff, and the baseline RBF kernel. Table 1 also shows results for a discriminative Latent Dirichlet Allocation method (denoted by DiscLDA) [3] and a recent method that is based on a max-margin supervised topic model (denoted by G-MedLDA) [10]. Both of these methods posit more complex models and represent the current stateof-the-art. The free model parameters such as n and N in\nsensing kernels 1 and 2, t in Diff, and \u03c3 in the RBF kernel, were tuned using a 5 fold cross-validation. The CCR for the Gibbs MedLDA method is for the best number of topics k as in [10]. Words in the test set that were not in the training set were dropped.\nWe can see that the proposed Sensing 1 and 2 kernels outperform the RBF kernel and the model-based kernels PPK and Diff. Surprisingly, the performance of our very simple method is better than that of DiscLDA and is almost the same as that of G-MedLDA (with only 1 less document correctly classified). These state-of-the-art methods make use of fairly complex models and require sophisticated inference algorithms while our sensing-aware kernel SVM makes minimal modeling assumptions and has much lower time complexity."}, {"heading": "4.2. Multi-class Classification", "text": "We next studied multi-class classification in the 20 Newsgroups dataset with all 20 classes. We adopted a widely used training/test split where the training set consists of 11,269 documents, and the test set consists of 7,505 documents. We used the \u201cone-versus-all\u201d strategy following [10] to do multiclass classification with binary classifiers. We followed the same settings as in the binary case. For the Sensing 1 and 2 kernels, we used n = N = 150. Table 2 shows the CCRs for the proposed sensing-aware kernels, the RBF baseline, the two model-based kernels PPK and Diff, and the G-MedLDA [10] algorithm. Also shown is the CCR for a recently developed deep learning method based on a Restricted Boltzmann Machine (RBM) [12] which outperforms the standard RBF SVM. Deep learning has recently emerged as a powerful generic approach and has attained state-of-the-art performance in many applications. Since [12] uses the same training settings, we simply quote the results reported in that paper. Observe that the approximate Sensing kernels 1 and 2\noutperform all other kernel SVMs and the RBM. As in binary classification, their performance is close to the G-MedLDA method that uses complex models."}, {"heading": "5. IMAGE CLASSIFICATION", "text": "In this section, we consider the problem of recognizing image scene categories. We use the Natural Scene category dataset first introduced in [2, 13]. This dataset consists of 15 scene categories, e.g., office, street view, forests, etc., with 200-400 grayscale images in each category (total 4485 images) and an average image size of 300\u00d7 250 pixels."}, {"heading": "5.1. Modeling Images as Bags of Features", "text": "There is extensive literature on BOF models for images. A typical model consists of 1) local feature extraction, 2) visual-vocabulary construction, and 3) image representation as BoFs. In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14]. 1) Local feature extraction. For each image, the SIFT descriptors of all the 16 \u00d7 16 image patches centered at grid points spaced 8 pixels apart are computed. 2) Visual vocabulary construction. D patches are randomly selected from training images. W -means clustering is performed over the corresponding D SIFT descriptors. The W mean vectors form a visual vocabulary, labeled as 1, . . . ,W . 3) BoF representation. For each image, the SIFT descriptor of each patch is assigned the label of its nearest neighbor in the visual vocabulary. An image is then a collection of visual words from the vocabulary and the spatial correlation is ignored.\nTo incorporate spatial structure, we use the pyramid matching scheme proposed in [13]. This uses a sequence of coarse-to-fine grids. At level L, an image is split into 2L \u00d7 2L non-overlapping cells, and each cell is treated as a separate \u201cdocument\u201d. Hence, at level L, each image xi is represented as a collection of 22L word-frequency vectors {xiL,c, c = 1, . . . , 2\n2L}, ordered according to the spatial position of the cell. The kernel at level L for two images xi,xj is defined as KL(xi,xj) = \u221122L\nc=1 \u03ba(x i L,c,x j L,c), where \u03ba is\nthe kernel function for two BoF documents. The above kernel is only for a single level. The pyramid kernel K for two images is defined as a weighted sum of single-level kernels: K(xi,xj) = \u2211L\nl=0 \u03b1 lK l(xi,xj). We\nuse the same weighting factors \u03b1l as in [13]."}, {"heading": "5.2. Experimental Results", "text": "We follow the settings in [13, 2]. We set the vocabulary size as W = 400. We randomly select 100 images per class for training and leave the rest for testing. We repeat experiments 10 times with a randomly selected training/testing split for each run. We use the \u201cone-versus-all\u201d strategy for multi-class classification and report the average CCR over 10 random runs.\nFollowing [13], we consider three setups. \u201cL = 0, single\u201d uses K0 as the kernel for classification. This simply views the entire image as a document. Similarly, \u201cL = 2, single\u201d uses K2 as the kernel for classification. In this case, the image is divided into 22L = 16 regions. \u201cL = 2, Pyramid\u201d uses K =\n\u22112 l=0 \u03b1 lK l as the kernel value. We compare our Sensing 0 and Sensing 2 kernels with the probabilistic model-based PPK and Diff kernels and the standard RBF kernel. We also compare with the classic Spatial Pyramid (SP) kernel of [13]. This kernel is specially crafted for this problem and has good empirical performance in several Computer Vision tasks. We also consider a recent work called Reconfiguration BoW (RBoW), which makes additional modeling assumptions on p(z|y). Finally, we con-\nsider a recently developed deep learning algorithm [15] (denoted by MRF) which achieves state-of-the-art performance on the same task.\nFor the Sensing 2 kernel, we simply set N = 500. As in text classification, all the other free parameters are tuned by a 5-fold cross-validation. We measure performance using the overall classification accuracy (CCR %). The results are summarized in Table 3. Since the settings are identical, we simply quote the results for MRF and RBoW as reported in [15] and [16] respectively.\nWe can see that in the \u201cL = 2, single-level\u201d setting, the proposed Sensing kernel 2 clearly improves over the classic kernel SP and outperforms other kernels (RBF, PPK and Diff) as well as the more complex RBoW method. However in the \u201cL = 0 single-level\u201d setting, the sensing-aware kernels do perform worse than PPK, Diff, and SP. This could be because L = 0 corresponds to the whole image. It has been strongly suggested in the Computer Vision literature that modeling the entire image as a BoF is unrealistic in many datasets [14] [13]. Our proposed sensing-aware approach only makes sense if the data is actually generated from the sensing structure. Our experimental results accord with the fact that BoF is a reasonable model for local regions, but not for the entire image.\nWe also did experiments for the \u201cL = 2, pyramid\u201d setting. We can see that the Sensing 2 kernel achieves almost the same performance as the state-of-the-art SP and MRF. The improvement is not as significant as in the \u201cL = 2 singlelevel\u201d case. This could be because it involves the L = 0 level kernel values, which does not fit the BoF model well."}, {"heading": "6. CONCLUSION", "text": "In this paper, we proposed a novel kernel design principle to incorporate partial model information. On two distinct types of data, we showed that with minimal modeling assumptions, the sensing-aware kernel improves upon other standard kernels and handcrafted kernels for specific domains. We also observed that the sensing-aware kernel can match the performance of the state-of-the-art approaches."}, {"heading": "7. REFERENCES", "text": "[1] D. Blei, \u201cProbabilistic topic models,\u201d Commun. of the ACM, vol. 55, no. 4, pp. 77\u201384, 2012.\n[2] Fei-Fei Li and P. Perona, \u201cA bayesian hierarchical model for learning natural scene categories,\u201d in Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), San Diego, CA, Jun. 2005, pp. 524\u2013531.\n[3] S. Lacoste-Julien, F. Sha, and M. Jordan, \u201cDisclda: Discriminative learning for dimensionality reduction and classification,\u201d in Advances in Neural Information Processing Systems 21(NIPS\u201909), pp. 897\u2013904. 2009.\n[4] V. N. Vapnik, The nature of statistical learning theory, Springer-Verlag New York, Inc., New York, NY, USA, 1995.\n[5] T. Hastie, R. Tibshirani, and J. Friedman, Elements of Statistical Learning, Springer-Verlag New York, Inc., New York, NY, USA, 2009.\n[6] T. Jebara, R. Kondor, and A. Howard, \u201cProbability product kernels,\u201d J. Mach. Learn. Res., vol. 5, pp. 819\u2013844, Dec. 2004.\n[7] J. Lafferty and G. Lebanon, \u201cDiffusion kernels on statistical manifolds,\u201d J. Mach. Learn. Res., vol. 6, pp. 129\u2013163, Dec. 2005.\n[8] Pedro J. Moreno, Purdy P. Ho, and Nuno Vasconcelos, \u201cA kullback-leibler divergence based kernel for svm classification in multimedia applications,\u201d in Advances in Neural Information Processing Systems 16 (NIPS\u201903). 2004.\n[9] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, \u201cRcv1: A new benchmark collection for text categorization research,\u201d J. Mach. Learn. Res., vol. 5, pp. 361\u2013397, Dec. 2004.\n[10] J. Zhu, N. Chen, H. Perkins, and B. Zhang, \u201cGibbs maxmargin topic models with fast sampling algorithms,\u201d in the 30th Int. Conf. on Machine Learning (ICML\u201913), Atlanta, GA, Jun. 2013.\n[11] C. Chang and C. Lin, \u201cLIBSVM: A library for support vector machines,\u201d ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011.\n[12] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio, \u201cLearning algorithms for the classification restricted boltzmann machine,\u201d J. Mach. Learn. Res., vol. 13, pp. 643\u2013669, Mar. 2012.\n[13] S. Lazebnik, C. Schmid, and J. Ponce, \u201cBeyond bags of features: Spatial pyramid matching for recognizing natural scene categories,\u201d in Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), New York, NY, Jun. 2006, pp. 2169\u20132178.\n[14] K. Grauman and T. Darrell, \u201cThe pyramid match kernel: Discriminative classification with sets of image features,\u201d in Proc. the 10th IEEE International Conference on Computer Vision(ICCV\u201905), Beijing, China, Oct. 2005, pp. 1458\u20131465.\n[15] M. Ranzato, V. Mnih, J.M. Susskind, and G.E. Hinton, \u201cModeling natural images using gated mrfs,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 9, pp. 2206\u20132222, 2013.\n[16] Sobhan Naderi Parizi, John Oberlin, and Pedro F. Felzenszwalb, \u201cReconfigurable models for scene recognition,\u201d in Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2775\u20132782."}], "references": [{"title": "Probabilistic topic models", "author": ["D. Blei"], "venue": "Commun. of the ACM, vol. 55, no. 4, pp. 77\u201384, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["Fei-Fei Li", "P. Perona"], "venue": "Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), San Diego, CA, Jun. 2005, pp. 524\u2013531.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Disclda: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M. Jordan"], "venue": "Advances in Neural Information Processing Systems 21(NIPS\u201909), pp. 897\u2013904. 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "The nature of statistical learning theory, Springer-Verlag", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "J. Mach. Learn. Res., vol. 5, pp. 819\u2013844, Dec. 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Diffusion kernels on statistical manifolds", "author": ["J. Lafferty", "G. Lebanon"], "venue": "J. Mach. Learn. Res., vol. 6, pp. 129\u2013163, Dec. 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "A kullback-leibler divergence based kernel for svm classification in multimedia applications", "author": ["Pedro J. Moreno", "Purdy P. Ho", "Nuno Vasconcelos"], "venue": "Advances in Neural Information Processing Systems 16 (NIPS\u201903). 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "J. Mach. Learn. Res., vol. 5, pp. 361\u2013397, Dec. 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Gibbs maxmargin topic models with fast sampling algorithms", "author": ["J. Zhu", "N. Chen", "H. Perkins", "B. Zhang"], "venue": "the 30th Int. Conf. on Machine Learning (ICML\u201913), Atlanta, GA, Jun. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 643\u2013669, Mar. 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), New York, NY, Jun. 2006, pp. 2169\u20132178.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "The pyramid match kernel: Discriminative classification with sets of image features", "author": ["K. Grauman", "T. Darrell"], "venue": "Proc. the 10th IEEE International Conference on Computer Vision(ICCV\u201905), Beijing, China, Oct. 2005, pp. 1458\u20131465.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Modeling natural images using gated mrfs", "author": ["M. Ranzato", "V. Mnih", "J.M. Susskind", "G.E. Hinton"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 9, pp. 2206\u20132222, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Reconfigurable models for scene recognition", "author": ["Sobhan Naderi Parizi", "John Oberlin", "Pedro F. Felzenszwalb"], "venue": "Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2775\u20132782.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "[1][2], where the observed document x is modeled as a collection of iid drawings of words from a latent document-specific distribution z over the vocabulary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1][2], where the observed document x is modeled as a collection of iid drawings of words from a latent document-specific distribution z over the vocabulary.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "For supervised classification, a generative approach would make further assumptions on p(z|y) which may either not hold or lead to an intractable posterior inference problem [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "On the other hand, the classical distribution-free discriminative paradigm would ignore knowledge of the sensing model p(x|z) and build classifiers using labeled training data [4, 5].", "startOffset": 176, "endOffset": 182}, {"referenceID": 4, "context": "The Probability Product Kernel (PPK) proposed in [6] first estimates latent variable \u1e91 using observation x and defines the kernel in terms of the estimate as K(x,x) := \u222b", "startOffset": 49, "endOffset": 52}, {"referenceID": 5, "context": "Heat or Diffusion kernels (Diff) that exploit the Fisher information metric on the probability manifold were proposed in [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "Kernels based on the KL divergence were proposed in [8].", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "Following standard practice [9], we removed a standard list of stop words from the vocabulary.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "This is a difficult task due to the similarity of content in these two groups [3, 10].", "startOffset": 78, "endOffset": 85}, {"referenceID": 8, "context": "This is a difficult task due to the similarity of content in these two groups [3, 10].", "startOffset": 78, "endOffset": 85}, {"referenceID": 2, "context": "35 DiscLDA [3] 83.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "00 PPK SVM[6] 81.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "02 G-MedLDA [10] 83.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "57 Diff SVM [7] 79.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "We used LIBSVM [11] to train our kernel SVMs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "Table 1 also shows results for a discriminative Latent Dirichlet Allocation method (denoted by DiscLDA) [3] and a recent method that is based on a max-margin supervised topic model (denoted by G-MedLDA) [10].", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "Table 1 also shows results for a discriminative Latent Dirichlet Allocation method (denoted by DiscLDA) [3] and a recent method that is based on a max-margin supervised topic model (denoted by G-MedLDA) [10].", "startOffset": 203, "endOffset": 207}, {"referenceID": 8, "context": "The CCR for the Gibbs MedLDA method is for the best number of topics k as in [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "We used the \u201cone-versus-all\u201d strategy following [10] to do multiclass classification with binary classifiers.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "Table 2 shows the CCRs for the proposed sensing-aware kernels, the RBF baseline, the two model-based kernels PPK and Diff, and the G-MedLDA [10] algorithm.", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "Also shown is the CCR for a recently developed deep learning method based on a Restricted Boltzmann Machine (RBM) [12] which outperforms the standard RBF SVM.", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "Since [12] uses the same training settings, we simply quote the results reported in that paper.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "5 RBM [12] 76.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "2 PPK SVM[6] 78.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "2 G-MedLDA [10] 80.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "9 Diff SVM [7] 78.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "We use the Natural Scene category dataset first introduced in [2, 13].", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": "We use the Natural Scene category dataset first introduced in [2, 13].", "startOffset": 62, "endOffset": 69}, {"referenceID": 1, "context": "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].", "startOffset": 89, "endOffset": 100}, {"referenceID": 11, "context": "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].", "startOffset": 89, "endOffset": 100}, {"referenceID": 12, "context": "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].", "startOffset": 89, "endOffset": 100}, {"referenceID": 11, "context": "To incorporate spatial structure, we use the pyramid matching scheme proposed in [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "We use the same weighting factors \u03b1 as in [13].", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "We follow the settings in [13, 2].", "startOffset": 26, "endOffset": 33}, {"referenceID": 1, "context": "We follow the settings in [13, 2].", "startOffset": 26, "endOffset": 33}, {"referenceID": 11, "context": "Following [13], we consider three setups.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "We also compare with the classic Spatial Pyramid (SP) kernel of [13].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Finally, we consider a recently developed deep learning algorithm [15] (denoted by MRF) which achieves state-of-the-art performance on the same task.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Since the settings are identical, we simply quote the results for MRF and RBoW as reported in [15] and [16] respectively.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Since the settings are identical, we simply quote the results for MRF and RBoW as reported in [15] and [16] respectively.", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "3 PPK [6] 73.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "6 Diff [7] 74.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "5 SP [13] 74.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "4 RBOW [16] N/A 78.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "MRF [15] 81.", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "It has been strongly suggested in the Computer Vision literature that modeling the entire image as a BoF is unrealistic in many datasets [14] [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "It has been strongly suggested in the Computer Vision literature that modeling the entire image as a BoF is unrealistic in many datasets [14] [13].", "startOffset": 142, "endOffset": 146}], "year": 2014, "abstractText": "We propose a novel approach for designing kernels for support vector machines (SVMs) when the class label is linked to the observation through a latent state and the likelihood function of the observation given the state (the sensing model) is available. We show that the Bayes-optimum decision boundary is a hyperplane under a mapping defined by the likelihood function. Combining this with the maximum margin principle yields kernels for SVMs that leverage knowledge of the sensing model in an optimal way. We derive the optimum kernel for the bag-of-words (BoWs) sensing model and demonstrate its superior performance over other kernels in document and image classification tasks. These results indicate that such optimum sensing-aware kernel SVMs can match the performance of rather sophisticated state-of-the-art approaches.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}